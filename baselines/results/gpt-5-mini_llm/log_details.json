[
    {
        "sha_fail": "793505ce699f22c642aff91b686b142129ff6410",
        "error_context": [
            "The CI run failed during the test stage when pytest reported a failing test: test/nn/test_model_hub.py::test_from_pretrained. The test calls model.from_pretrained(...) and the call chain enters torch_geometric/nn/model_hub.py -> huggingface_hub (validators wrapper) -> huggingface_hub/hub_mixin.py and raises a TypeError: \"PyGModelHubMixin._from_pretrained() missing 2 required positional arguments: 'proxies' and 'resume_download'\" (log evidence: \".venv/lib/python3.9/site-packages/huggingface_hub/hub_mixin.py:559: TypeError: PyGModelHubMixin._from_pretrained() missing 2 required positional arguments: 'proxies' and 'resume_download'\"). This indicates a runtime API/signature mismatch when the repository's PyG hub mixin delegates to HuggingFace Hub internals, so the test cannot instantiate the model from the saved local directory and fails the test job.",
            "Concurrently the test run emitted repeated critical logging errors while running other tests (notably test_onnx invoked by test/nn/models/test_basic_gnn.py). During torch.onnx.export -> onnxscript optimizer passes, logger.info() calls (e.g. \"Skipping constant folding for node %r...\", \"Removed %s unused nodes\", \"Replaced initializer '%s'...\") triggered Python logging.Handler.emit to raise ValueError: \"I/O operation on closed file.\" (log evidence: \"File .../logging/__init__.py, line 1086, in emit stream.write(msg + self.terminator) ValueError: I/O operation on closed file.\" and several '--- Logging error ---' blocks). These are runtime logging-emission errors originating from ONNX conversion/optimizer passes invoked inside tests; they generated large tracebacks through pytest and the onnxscript/onnx_ir pipelines but did not replace the failing test as the immediate exit cause \u2014 they are critical runtime errors/warnings that polluted the test output and indicate logging/stream handling problems in the test environment.",
            "The run also produced multiprocessing/thread warnings: a QueueFeederThread encountered an OSError (Bad file descriptor) and then a ValueError in the thread (\"semaphore or lock released too many times\"), surfaced as a PytestUnhandledThreadExceptionWarning (log evidence: traceback into multiprocessing/queues.py and connection.py and the warning text). These thread/multiprocessing exceptions are additional runtime issues during fixture teardown or multi-process test helpers and are likely environment-related but were not the primary failing assertion \u2014 the TypeError in the HuggingFace hub call produced the failing pytest exit code.",
            "Summarized sequence: (1) Workflow step 'Run tests' invoked pytest (uv run --no-project pytest ...). (2) Most tests ran and many passed, but test/nn/test_model_hub.py::test_from_pretrained attempted to call model.from_pretrained(save_directory) which delegated to huggingface_hub and failed with a TypeError because _from_pretrained's signature did not match (missing 'proxies' and 'resume_download'). (3) Separately, ONNX export/optimization code invoked by test_onnx emitted logger.info messages which triggered repeated logging emit ValueError: I/O operation on closed file errors, and multiprocessing fixtures raised thread warnings during setup/teardown. The TypeError caused pytest to exit non-zero (Process completed with exit code 1)."
        ],
        "relevant_files": [
            {
                "file": "test/nn/test_model_hub.py",
                "line_number": 91,
                "reason": "Failing test location. The failing invocation is shown in the trace: 'test/nn/test_model_hub.py:91' with the failing statement 'model = model.from_pretrained(save_directory)'. The pytest short summary lists this test as the single failing test."
            },
            {
                "file": "torch_geometric/nn/model_hub.py",
                "line_number": 239,
                "reason": "Project wrapper that delegates to HuggingFace Hub. Trace shows 'torch_geometric/nn/model_hub.py:239: in from_pretrained' immediately before huggingface_hub was invoked; the project's mixin is on the failing call path."
            },
            {
                "file": ".venv/lib/python3.9/site-packages/huggingface_hub/hub_mixin.py",
                "line_number": 559,
                "reason": "Where HuggingFace Hub attempted to call cls._from_pretrained(...) and the TypeError was raised: \"_from_pretrained() missing 2 required positional arguments: 'proxies' and 'resume_download'\" (log evidence points to this file/line in the traceback)."
            },
            {
                "file": ".venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py",
                "line_number": 89,
                "reason": "Validators wrapper invoked around HuggingFace Hub from_pretrained (appears in the traceback). Also emits a UserWarning about deprecated 'resume_download' which is relevant to the signature/deprecation context."
            },
            {
                "file": "torch_geometric/_onnx.py",
                "line_number": 74,
                "reason": "safe_onnx_export helper in repository calls torch.onnx.export (log: 'torch_geometric/_onnx.py:74 in safe_onnx_export torch.onnx.export(...)'); ONNX conversion/optimizer passes invoked here emitted logger.info messages that preceded logging emit errors."
            },
            {
                "file": "onnxscript/optimizer/_constant_folding.py",
                "line_number": 1159,
                "reason": "An optimizer pass that emitted info-level logs such as 'Skipping constant folding for node %r...' which immediately preceded repeated '--- Logging error ---' blocks (the logger.info calls here are the origin of many logging emission exceptions shown in the logs)."
            },
            {
                "file": "onnx_ir/passes/common/unused_removal.py",
                "line_number": 116,
                "reason": "Pass that logged 'Removed %s unused nodes' (Message: 'Removed %s unused nodes' Arguments: (8,)) and its logger.info triggered a logging emit ValueError: I/O operation on closed file (one of the repeated logging error blocks)."
            },
            {
                "file": "onnx_ir/passes/common/initializer_deduplication.py",
                "line_number": 106,
                "reason": "Initializer dedup pass that logged 'Replaced initializer '%s' with existing initializer '%s'' for various initializers; these logger.info calls also preceded logging emit failures reported in the log."
            },
            {
                "file": "test/conftest.py",
                "line_number": 97,
                "reason": "Fixture spawn_context calls logging.info(\"Setting torch.multiprocessing context to 'spawn'\") \u2014 the logging.info here is the origin of repeated 'Logging error' blocks on macOS/ubuntu runs where logging.emit attempted to write to a closed stream (shown in the call stacks)."
            },
            {
                "file": "multiprocessing/queues.py",
                "line_number": 239,
                "reason": "Appears in a PytestUnhandledThreadExceptionWarning: Queue feeder thread raised OSError: [Errno 9] Bad file descriptor and then ValueError: semaphore or lock released too many times. These thread/multiprocessing exceptions were printed in the log and indicate runtime issues during fixture teardown."
            },
            {
                "file": "logging/__init__.py",
                "line_number": 1086,
                "reason": "Immediate origin of the logging runtime error: logging.Handler.emit attempted stream.write(...) and raised 'ValueError: I/O operation on closed file.' This line appears in every '--- Logging error ---' block."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "TypeError in test (HuggingFace hub instantiation)",
                "evidence": "\"E       TypeError: PyGModelHubMixin._from_pretrained() missing 2 required positional arguments: 'proxies' and 'resume_download'\" (stack points to huggingface_hub/hub_mixin.py:559 and test/nn/test_model_hub.py:91)."
            },
            {
                "category": "Dependency / API Mismatch",
                "subcategory": "Signature mismatch between local PyG mixin and huggingface_hub",
                "evidence": "Trace shows torch_geometric/nn/model_hub.py delegates to huggingface_hub.from_pretrained and huggingface_hub then calls cls._from_pretrained but fails with missing 'proxies' and 'resume_download' arguments \u2014 indicates incompatible expected parameters between implementations (log: .venv/lib/.../huggingface_hub/hub_mixin.py:559: TypeError)."
            },
            {
                "category": "Runtime Error",
                "subcategory": "Logging emission error (I/O on closed file)",
                "evidence": "\"ValueError: I/O operation on closed file.\" raised inside logging/__init__.py emit() and repeated '--- Logging error ---' blocks tied to logger.info calls in ONNX optimizer passes (e.g., onnx_ir/passes/common/unused_removal.py and onnxscript/optimizer/_constant_folding.py)."
            },
            {
                "category": "Runtime Warning / Threading Error",
                "subcategory": "Multiprocessing thread exception (QueueFeeder thread / Bad file descriptor)",
                "evidence": "PytestUnhandledThreadExceptionWarning shows traceback into multiprocessing/queues.py and connection.py: 'OSError: [Errno 9] Bad file descriptor' followed by 'ValueError: semaphore or lock released too many times' in thread, printed as a pytest warning in the logs."
            }
        ],
        "failed_job": [
            {
                "job": "pytest",
                "step": "Run tests",
                "command": "uv run --no-project pytest --cov --cov-report=xml --durations 10"
            },
            {
                "job": "pytest",
                "step": "Run tests",
                "command": "uv run --no-project pytest --cov --cov-report=xml --durations 10"
            }
        ]
    },
    {
        "sha_fail": "9fe0f4cb42b835e5b50db536ba6902c3822b0e21",
        "error_context": [
            "The CI job failed during the first 'Run tests' step after dependencies were installed. The workflow set up Python 3.8 and pip installation of requirements-min.txt completed successfully, but running 'python -m etc.unittest' raised an import-time SyntaxError that aborted the run. The Python traceback shows an import chain (etc/unittest/__main__ -> g4f -> g4f/models.py -> g4f/Provider -> g4f/Provider/qwen/QwenCode.py -> g4f/Provider/qwen/qwenContentGenerator.py -> g4f/Provider/qwen/sharedTokenManager.py) and terminates with \"SyntaxError: 'await' outside async function\" at g4f/Provider/qwen/sharedTokenManager.py line 121. Because the SyntaxError occurs during module import, the test runner exited with code 1 and none of the unit tests executed; subsequent workflow steps (including the second Python 3.12 test run) were not reached. Post-job cleanup then ran."
        ],
        "relevant_files": [
            {
                "file": "g4f/Provider/qwen/sharedTokenManager.py",
                "line_number": 121,
                "reason": "Immediate source of failure: traceback shows \"await self.reloadCredentialsFromFile()\" at line 121 with SyntaxError: 'await' outside async function; this line caused the interpreter to fail at import time."
            },
            {
                "file": "g4f/Provider/qwen/qwenContentGenerator.py",
                "line_number": 3,
                "reason": "Imports sharedTokenManager (traceback: 'from .sharedTokenManager import SharedTokenManager'), placing this file on the import chain that triggered the SyntaxError in sharedTokenManager.py."
            },
            {
                "file": "g4f/Provider/qwen/QwenCode.py",
                "line_number": 6,
                "reason": "Imported qwenContentGenerator (traceback: 'from .qwenContentGenerator import QwenContentGenerator'), part of the import chain leading to the failing module."
            },
            {
                "file": "g4f/Provider/__init__.py",
                "line_number": 17,
                "reason": "Imports QwenCode (traceback: 'from .qwen.QwenCode import QwenCode'), included in the import chain that led to the SyntaxError."
            },
            {
                "file": "g4f/models.py",
                "line_number": 6,
                "reason": "Imported by g4f/__init__.py (traceback shows 'from .Provider import IterListProvider, ProviderType'), contributing to the module import sequence that reached the failing file."
            },
            {
                "file": "g4f/__init__.py",
                "line_number": 9,
                "reason": "Entry in the traceback as the top-level package import ('from .models import Model') triggered by running the unittest module, initiating the failing import chain."
            },
            {
                "file": "etc/unittest/__main__.py",
                "line_number": 3,
                "reason": "The test runner entrypoint executed with 'python -m etc.unittest' and performed 'import g4f.debug' (traceback shows this as the module run that started the failing import sequence)."
            },
            {
                "file": "lib/python3.8/runpy.py",
                "line_number": 194,
                "reason": "Standard library entry used to run the module as __main__ (traceback shows runpy._run_module_as_main -> _run_code), indicating how the module execution reached the import that failed."
            }
        ],
        "error_types": [
            {
                "category": "Runtime Error",
                "subcategory": "SyntaxError: 'await' outside async function",
                "evidence": "Traceback ends with: \"File \\\".../g4f/Provider/qwen/sharedTokenManager.py\\\", line 121\\n    await self.reloadCredentialsFromFile()\\n    ^\\nSyntaxError: 'await' outside async function\" \u2014 this directly caused process exit code 1."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Run tests",
                "command": "python -m etc.unittest"
            }
        ]
    },
    {
        "sha_fail": "00dccf59f5829fd11d234d62a13bfa0fa4a38932",
        "error_context": [
            "In step \"test / test-linux (3.9)\": the job successfully set up Python and installed development/runtime dependencies, then ran tests with the command \"python -m pytest ./tests/test_core.py\". Pytest collected 37 tests and reported 7 failures (7 failed, 30 passed). The failing tests show that stats.get_plugin(...) returned None for several plugin names (cpu, network, diskio), causing AttributeError when the tests attempt to call methods like get_raw(), reset(), or get_views(); one test also asserts that mandatory plugin names appear in the plugins list and fails (AssertionError: False is not true). Representative evidence: \"AttributeError: 'NoneType' object has no attribute 'get_raw'\" and the pytest short summary \"7 failed, 30 passed in 8.19s\" as well as the failing test trace for tests/test_core.py. These failures indicate tests expected plugin instances to be registered/available but they were missing or not returned by stats.get_plugin().",
            "In step \"test / test-windows (3.9)\": the provided step/chunk in the input contained a parsing error (\"Expected a '}' but saw '']'\"), indicating the log/step data for that Windows run is malformed or truncated in the analysis input. No runtime/test command or further test output for the Windows step is available in the logs, so its execution status cannot be determined from the provided data."
        ],
        "relevant_files": [
            {
                "file": "tests/test_core.py",
                "line_number": 216,
                "reason": "All failing traces and assertions originate in tests/test_core.py (examples: AssertionError at line 216; AttributeError traces at lines 232, 282, 289, 71, 640, 681) where tests expect stats.get_plugin(...) to return plugin instances but receive None."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError",
                "evidence": "\"AssertionError: False is not true\" from tests/test_core.py::TestGlances::test_001_plugins (checks for mandatory plugin names)."
            },
            {
                "category": "Runtime Error",
                "subcategory": "AttributeError (NoneType)",
                "evidence": "Multiple traces: \"AttributeError: 'NoneType' object has no attribute 'get_raw'\", \"...has no attribute 'reset'\", \"...has no attribute 'get_views'\" when tests call methods on stats.get_plugin(...) results."
            },
            {
                "category": "Configuration Error",
                "subcategory": "Malformed step/log input",
                "evidence": "Log/details parsing error for step 'test / test-windows (3.9)': \"Expected a '}' but saw ']'\" (provided as the step's error field)."
            }
        ],
        "failed_job": [
            {
                "job": "test",
                "step": "test / test-linux (3.9)",
                "command": "python -m pytest ./tests/test_core.py"
            },
            {
                "job": "test",
                "step": "test / test-windows (3.9)",
                "command": null
            }
        ]
    },
    {
        "sha_fail": "6f22be309c840ceb16a17fe01c4d42a94fc2f07f",
        "error_context": [
            "The CI run failed because the repository's code formatting check detected an unformatted file. In the step 'test / source-code-checks' the chartboost/ruff-action was run with arguments 'format --check' over the repository (INPUT_ARGS: format --check, INPUT_SRC: .). Ruff reported that one file would be reformatted: \"Would reformat: glances/exports/glances_prometheus/__init__.py\" and returned a non-zero exit code, causing the job to terminate ('##[error]Process completed with exit code 1.'). The checkout and git setup steps completed successfully; the single failing point recorded in the logs is the ruff formatting check detecting a formatting mismatch."
        ],
        "relevant_files": [
            {
                "file": "glances/exports/glances_prometheus/__init__.py",
                "line_number": null,
                "reason": "Ruff reported this file would be reformatted: \"Would reformat: glances/exports/glances_prometheus/__init__.py\" and the action was run with 'format --check', so this formatting mismatch directly produced the non-zero exit code that failed the job."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Formatter check failed (ruff would reformat file)",
                "evidence": "\"Would reformat: glances/exports/glances_prometheus/__init__.py\"; \"1 file would be reformatted, 151 files already formatted\" and the action was invoked with 'format --check', producing a non-zero exit (Process completed with exit code 1)."
            }
        ],
        "failed_job": [
            {
                "job": "test",
                "step": "test / source-code-checks",
                "command": "chartboost/ruff-action@v1 (args: format --check, src: .)"
            }
        ]
    },
    {
        "sha_fail": "1db61eccedfe77f232e7c118d45e36201a84f598",
        "error_context": [
            "The CI run failed during the 'linting' job when the 'Lint with pylint' step completed with a non-zero exit code. The logs show pylint was executed (pylint --fail-under=10 deepface/) and produced two configuration UserWarning messages about invalid values for the 'overgeneral-exceptions' option and one lint message (W0611: unused-import) in deepface/models/face_detection/TinaFace.py. Despite printing \"Your code has been rated at 10.00/10\", the step terminated with \"Process completed with exit code 4\", which caused the job to fail. The failure is therefore tied to the lint step's process exit (exit code 4) after pylint output and configuration warnings."
        ],
        "relevant_files": [
            {
                "file": "deepface/models/face_detection/TinaFace.py",
                "line_number": 12,
                "reason": "Pylint reported an unused-import in this file: \"deepface/models/face_detection/TinaFace.py:12:0: W0611: Unused folder_utils imported from deepface.commons (unused-import)\", which is referenced directly in the lint step output and is tied to the linting result."
            }
        ],
        "error_types": [
            {
                "category": "Code Quality",
                "subcategory": "Unused Import",
                "evidence": "Log: \"deepface/models/face_detection/TinaFace.py:12:0: W0611: Unused folder_utils imported from deepface.commons (unused-import)\""
            },
            {
                "category": "Configuration Error",
                "subcategory": "Invalid pylint configuration value",
                "evidence": "Log: \"pylint: Command line or configuration file:1: UserWarning: 'BaseException' is not a proper value for the 'overgeneral-exceptions' option...\" and similarly for 'Exception'."
            },
            {
                "category": "Runtime Error",
                "subcategory": "Process exited with non-zero status",
                "evidence": "Log: \"##[error]Process completed with exit code 4.\" indicating the lint step terminated with exit code 4."
            }
        ],
        "failed_job": [
            {
                "job": "linting",
                "step": "Lint with pylint",
                "command": "pylint --fail-under=10 deepface/"
            }
        ]
    },
    {
        "sha_fail": "814ab44479570f4759b7eeedeb7864d3f622f695",
        "error_context": [
            "In job 'run-tests', step 'Run pytest' (command: `pytest -v tests/`) failed during test collection because importing the package raised a ModuleNotFoundError for the third\u2011party package 'requests'. Evidence: pytest collection shows three identical import errors where faster_whisper/utils.py line 8 attempts `import requests` and the log records `ModuleNotFoundError: No module named 'requests'`, which aborted collection for tests/test_tokenizer.py, tests/test_transcribe.py, and tests/test_utils.py.",
            "In job 'check-code-format', step 'Check code style with Flake8' (command: `flake8 .`) failed the linters due to a naming error in the source: flake8 reported `./faster_whisper/vad.py:323:45: F821 undefined name 'num_samples'`, causing the flake8 step to exit with code 1 and fail that job. Both errors are independent: one is a missing dependency at import/runtime, the other is a static code style/name error detected by flake8."
        ],
        "relevant_files": [
            {
                "file": "tests/test_tokenizer.py",
                "line_number": 1,
                "reason": "Test import failed during collection: log shows `tests/test_tokenizer.py:1: in <module> from faster_whisper import WhisperModel` which triggered the import chain that encountered `ModuleNotFoundError: No module named 'requests'`."
            },
            {
                "file": "tests/test_transcribe.py",
                "line_number": 6,
                "reason": "Test import failed during collection: log shows `tests/test_transcribe.py:6: in <module> from faster_whisper import BatchedInferencePipeline, WhisperModel, decode_audio` which propagated the missing 'requests' error from faster_whisper.utils."
            },
            {
                "file": "tests/test_utils.py",
                "line_number": 3,
                "reason": "Test import failed during collection: log shows `tests/test_utils.py:3: in <module> from faster_whisper import available_models, download_model` which triggered the import of faster_whisper.utils that raised `ModuleNotFoundError: No module named 'requests'`."
            },
            {
                "file": "faster_whisper/__init__.py",
                "line_number": 2,
                "reason": "Package __init__ imports from faster_whisper.transcribe (`faster_whisper/__init__.py:2`), starting the import chain that reached faster_whisper.utils and failed due to the missing 'requests' module."
            },
            {
                "file": "faster_whisper/transcribe.py",
                "line_number": 22,
                "reason": "transcribe.py imports functions from faster_whisper.utils (`faster_whisper/transcribe.py:22`); that import triggered faster_whisper/utils.py to execute `import requests` which raised the ModuleNotFoundError recorded in the log."
            },
            {
                "file": "faster_whisper/utils.py",
                "line_number": 8,
                "reason": "Direct cause of pytest collection failure: `faster_whisper/utils.py:8` executes `import requests` and the log shows `E   ModuleNotFoundError: No module named 'requests'`."
            },
            {
                "file": "faster_whisper/vad.py",
                "line_number": 323,
                "reason": "Cause of flake8 failure: linter output reports `./faster_whisper/vad.py:323:45: F821 undefined name 'num_samples'`, which made the flake8 step exit with code 1."
            },
            {
                "file": "importlib/__init__.py",
                "line_number": 127,
                "reason": "Appears in the traceback during test import: `importlib.__init__.py:127` is the import machinery shown in stack traces when pytest attempted to import test modules and failed due to the missing dependency."
            }
        ],
        "error_types": [
            {
                "category": "Dependency Error",
                "subcategory": "ImportError: No module named 'requests'",
                "evidence": "Pytest import traceback: `faster_whisper/utils.py:8: in <module> import requests` followed by `E   ModuleNotFoundError: No module named 'requests'` (caused pytest collection to fail: '3 errors during collection')."
            },
            {
                "category": "Code Formatting",
                "subcategory": "Flake8 F821 undefined name 'num_samples'",
                "evidence": "Flake8 output: `./faster_whisper/vad.py:323:45: F821 undefined name 'num_samples'` and the step terminated with `Process completed with exit code 1`."
            }
        ],
        "failed_job": [
            {
                "job": "run-tests",
                "step": "Run pytest",
                "command": "pytest -v tests/"
            },
            {
                "job": "check-code-format",
                "step": "Check code style with Flake8",
                "command": "flake8 ."
            }
        ]
    },
    {
        "sha_fail": "a2d2c49083f4454c8ec6ccb9eaa7ec12567ae285",
        "error_context": [
            "Two independent failures caused the CI run to fail. In job 'check-code-format' the style checks failed: the Black step returned a non-zero exit because Black reported \"3 files would be reformatted\" (files: faster_whisper/__init__.py, faster_whisper/transcribe.py, tests/test_transcribe.py) and the Flake8 step returned errors (e.g. \"./faster_whisper/__init__.py:19:2: W292 no newline at end of file\" and multiple \"E302 expected 2 blank lines, found 1\" entries). Those non-zero exits caused the check-code-format job to fail. Separately, in job 'run-tests' pytest failed during collection because an import in faster_whisper.utils raised ModuleNotFoundError: \"No module named 'requests'\" (traceback shows faster_whisper/utils.py:8 -> import requests). The pip install -e .[dev] step completed successfully in logs, but tests could not be collected because the runtime dependency 'requests' was not available to Python, producing three collection errors (tests/test_tokenizer.py, tests/test_transcribe.py, tests/test_utils.py) and pytest exited with code 2."
        ],
        "relevant_files": [
            {
                "file": "faster_whisper/__init__.py",
                "line_number": 19,
                "reason": "Flagged by both style tools: Flake8 reported \"./faster_whisper/__init__.py:19:2: W292 no newline at end of file\" and Black listed this file among the three that \"would be reformatted\"."
            },
            {
                "file": "faster_whisper/transcribe.py",
                "line_number": 1126,
                "reason": "Black reported the file would be reformatted and Flake8 reported a style error at line 1126: \"./faster_whisper/transcribe.py:1126:1: E302 expected 2 blank lines, found 1\"; transcribe.py is also on the import chain shown in test collection errors."
            },
            {
                "file": "faster_whisper/utils.py",
                "line_number": 8,
                "reason": "Root cause of test collection failure: import at line 8 attempts \"import requests\" and the traceback shows \"E   ModuleNotFoundError: No module named 'requests'\", preventing tests from being collected."
            },
            {
                "file": "tests/test_transcribe.py",
                "line_number": 96,
                "reason": "Both Black and Flake8 flagged this test file: Black would reformat it and Flake8 reported blank-line errors at lines 96 and 125 (e.g. \"./tests/test_transcribe.py:96:1: E302 expected 2 blank lines, found 1\"). The file also failed to be collected by pytest due to the import error originating in faster_whisper.utils."
            },
            {
                "file": "tests/test_tokenizer.py",
                "line_number": null,
                "reason": "Pytest collection error: \"ERROR collecting tests/test_tokenizer.py\" with traceback showing import chain into faster_whisper -> faster_whisper.utils which raised ModuleNotFoundError for 'requests'."
            },
            {
                "file": "tests/test_utils.py",
                "line_number": null,
                "reason": "Pytest collection error: \"ERROR collecting tests/test_utils.py\" with traceback showing the same ModuleNotFoundError for 'requests' originating in faster_whisper/utils.py."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Black formatting violations",
                "evidence": "\"would reformat ... __init__.py\", \"would reformat ... transcribe.py\", \"3 files would be reformatted\" (Black step output) and exit code 1."
            },
            {
                "category": "Code Style",
                "subcategory": "Flake8 E302 (missing blank lines)",
                "evidence": "\"./faster_whisper/transcribe.py:1126:1: E302 expected 2 blank lines, found 1\" and \"./tests/test_transcribe.py:96:1: E302 expected 2 blank lines, found 1\" (Flake8 output)."
            },
            {
                "category": "Code Style",
                "subcategory": "Flake8 W292 (no newline at end of file)",
                "evidence": "\"./faster_whisper/__init__.py:19:2: W292 no newline at end of file\" (Flake8 output)."
            },
            {
                "category": "Dependency Error",
                "subcategory": "Missing dependency (ImportError / ModuleNotFoundError)",
                "evidence": "\"faster_whisper/utils.py:8: in <module>     import requests\" followed by \"E   ModuleNotFoundError: No module named 'requests'\" (pytest collection traceback)."
            },
            {
                "category": "Test Failure",
                "subcategory": "Import error during test collection",
                "evidence": "\"collected 0 items / 3 errors\" and \"Interrupted: 3 errors during collection\" with each error showing an ImportError/ModuleNotFoundError for 'requests' while importing test modules."
            }
        ],
        "failed_job": [
            {
                "job": "check-code-format",
                "step": "Check code format with Black",
                "command": "black --check ."
            },
            {
                "job": "check-code-format",
                "step": "Check code style with Flake8",
                "command": "flake8 ."
            },
            {
                "job": "run-tests",
                "step": "Run pytest",
                "command": "pytest -v tests/"
            }
        ]
    },
    {
        "sha_fail": "88bc3d3b4e9e235b5451b7450563a196e5da5a85",
        "error_context": [
            "The CI run failed because pytest could not import the package due to a missing runtime dependency: the 'requests' module. During the 'Install module' step (pip install -e .[dev]) many dependencies were installed (the 'Successfully installed ...' list), but 'requests' is not present in that list. When pytest tried to collect tests in the 'Run pytest' step, importing the package triggered faster_whisper/utils.py which performs 'import requests' and raised ModuleNotFoundError: \"No module named 'requests'\", causing 3 import errors during collection and the job to exit with code 2.",
            "Sequence of involved steps: the workflow ran 'Install module' (pip install -e .[dev]) which completed successfully without installing 'requests'; then in 'Run pytest' the test collection failed immediately with ModuleNotFoundError in faster_whisper/utils.py (line 8). Pytest reported 'collected 0 items / 3 errors' and '3 errors in 0.46s'."
        ],
        "relevant_files": [
            {
                "file": "faster_whisper/utils.py",
                "line_number": 8,
                "reason": "Contains the failing import: traceback shows 'faster_whisper/utils.py:8: in <module> import requests' followed by 'ModuleNotFoundError: No module named \\\"requests\\\"' \u2014 this is the root cause."
            },
            {
                "file": "faster_whisper/transcribe.py",
                "line_number": 22,
                "reason": "Imported by faster_whisper.__init__.py and imports faster_whisper.utils (traceback: 'faster_whisper/transcribe.py:22: in <module     from faster_whisper.utils ...') \u2014 part of the import chain that triggers the missing 'requests' error."
            },
            {
                "file": "faster_whisper/__init__.py",
                "line_number": 2,
                "reason": "Top-level package import used by tests: traceback shows 'faster_whisper/__init__.py:2: in <module> from faster_whisper.transcribe ...' \u2014 it starts the import chain that reaches utils.py and fails."
            },
            {
                "file": "tests/test_tokenizer.py",
                "line_number": 1,
                "reason": "Test module shown in pytest traceback: 'tests/test_tokenizer.py:1: in <module> from faster_whisper import WhisperModel' \u2014 importing the package causes the ModuleNotFoundError during collection."
            },
            {
                "file": "tests/test_transcribe.py",
                "line_number": 6,
                "reason": "Test module shown in pytest traceback: 'tests/test_transcribe.py:6: in <module> from faster_whisper import BatchedInferencePipeline, WhisperModel, decode_audio' \u2014 triggers the same import chain that fails."
            },
            {
                "file": "tests/test_utils.py",
                "line_number": 3,
                "reason": "Test module shown in pytest traceback: 'tests/test_utils.py:3: in <module> from faster_whisper import available_models, download_model' \u2014 also triggers the failing import chain."
            }
        ],
        "error_types": [
            {
                "category": "Dependency Error",
                "subcategory": "Missing dependency (ModuleNotFoundError: No module named 'requests')",
                "evidence": "'faster_whisper/utils.py:8: in <module> import requests' followed by 'E   ModuleNotFoundError: No module named \"requests\"' and 'Successfully installed ...' list from pip does not include 'requests'."
            },
            {
                "category": "Test Failure",
                "subcategory": "ImportError during test collection",
                "evidence": "pytest collected 0 items and reported '3 errors during collection' with ERRORS showing ImportError/ModuleNotFoundError for each test: 'collected 0 items / 3 errors' and '============================== 3 errors in 0.46s ==============================='."
            }
        ],
        "failed_job": [
            {
                "job": "run-tests",
                "step": "Run pytest",
                "command": "pytest -v tests/"
            }
        ]
    },
    {
        "sha_fail": "5f3a5691eef3f0165b735bfd5a183b32c02a5f5c",
        "error_context": [
            "The CI run failed because the repository style check for trailing whitespace reported matches and was written to exit non\u2011zero when any matches are found. In the job 'quick-checks', the step 'Ensure no trailing whitespace' executed a git grep for trailing spaces and printed four matches (e.g. \"runtime/triton_trtllm/model_repo/audio_tokenizer/config.pbtxt:23:   key: \\\"model_dir\\\", \"). The step then printed \"The above files have trailing whitespace; please remove them\" and the command returned exit code 1, causing the job to fail. The run also emitted a non-fatal deprecation warning for the use of the deprecated set-output command in the 'Checkout PR tip' step, but that warning did not cause the failure."
        ],
        "relevant_files": [
            {
                "file": "runtime/triton_trtllm/model_repo/audio_tokenizer/config.pbtxt",
                "line_number": 23,
                "reason": "Log shows a trailing-whitespace match at line 23: 'runtime/triton_trtllm/model_repo/audio_tokenizer/config.pbtxt:23:   key: \"model_dir\",' which was included in the message 'The above files have trailing whitespace; please remove them' that caused the trailing-whitespace check to fail."
            },
            {
                "file": "runtime/triton_trtllm/model_repo/cosyvoice2/config.pbtxt",
                "line_number": 26,
                "reason": "Log shows trailing-whitespace matches for this file at lines 26 and 30 (e.g. '...cosyvoice2/config.pbtxt:26:   key: \"llm_tokenizer_dir\",') and these matches were reported by the trailing-whitespace check that exited with code 1."
            },
            {
                "file": "runtime/triton_trtllm/model_repo/token2wav/config.pbtxt",
                "line_number": 23,
                "reason": "Log shows a trailing-whitespace match at line 23: 'runtime/triton_trtllm/model_repo/token2wav/config.pbtxt:23:   key: \"model_dir\",' which was reported by the trailing-whitespace check that failed the job."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Trailing Whitespace",
                "evidence": "Output from trailing-whitespace check: 'runtime/triton_trtllm/model_repo/audio_tokenizer/config.pbtxt:23:   key: \"model_dir\",' ... 'The above files have trailing whitespace; please remove them' followed by '##[error]Process completed with exit code 1.'"
            },
            {
                "category": "Configuration Error",
                "subcategory": "Deprecated Workflow Command (warning)",
                "evidence": "Runner emitted: '##[warning]The `set-output` command is deprecated and will be disabled soon. Please upgrade to using Environment Files.' from the 'Checkout PR tip' step; this is a warning and did not cause the failure."
            }
        ],
        "failed_job": [
            {
                "job": "quick-checks",
                "step": "Ensure no trailing whitespace",
                "command": "(! git grep -I -n $' $' -- . ':(exclude)*.txt' ':(exclude)third_party' ':(exclude).gitattributes' ':(exclude).gitmodules' || (echo \"The above files have trailing whitespace; please remove them\"; false))"
            }
        ]
    },
    {
        "sha_fail": "fa65b8800ad462580a5d1e9fd7f51dd2d9bbea86",
        "error_context": [
            "The CI job failed during the 'Install pyflyby' step when attempting to build editable metadata for the local pyflyby package. In that step the workflow ran pip install --no-build-isolation -ve .[test] in the pyflyby checkout; Meson was invoked during metadata preparation and ran the command `python -m setuptools_scm`, which exited with status 1 because setuptools_scm was not installed. Evidence: Meson reported \"ERROR: Command `/.../python -m setuptools_scm` failed with status 1.\" and the meson log stderr shows \"/Library/Frameworks/Python.framework/Versions/3.13/bin/python: No module named setuptools_scm\". pip then reported \"Preparing editable metadata (pyproject.toml) did not run successfully... exit code: 1\" and \"error: metadata-generation-failed\" for file:///Users/runner/work/ipython/pyflyby. Earlier steps (installing ipykernel and running its pytest suite) completed successfully \u2014 pytest reported \"188 passed, 22 skipped, 12 warnings\" \u2014 and the ipykernel warnings were PendingDeprecationWarning entries (non-fatal). The missing setuptools_scm dependency in the environment therefore caused Meson to fail during pyflyby packaging, which caused pip's metadata generation to fail and the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "pyflyby/meson.build",
                "line_number": 4,
                "reason": "Meson reported the failure at '../../meson.build:4:11' while running `python -m setuptools_scm`; this file/line is the location where Meson invoked setuptools_scm and thus is directly implicated in the build failure."
            },
            {
                "file": "pyflyby/build/cp313/meson-logs/meson-log.txt",
                "line_number": null,
                "reason": "Meson log contains the concrete stderr showing the root cause: \"/Library/Frameworks/Python.framework/Versions/3.13/bin/python: No module named setuptools_scm\" and notes the failing command and status."
            },
            {
                "file": "pyflyby/pyproject.toml",
                "line_number": null,
                "reason": "pip failed while 'Preparing editable metadata (pyproject.toml)' for the pyflyby package (cwd: /Users/runner/work/ipython/pyflyby), so the pyproject.toml metadata build step is the packaging context that failed."
            },
            {
                "file": "pyflyby",
                "line_number": null,
                "reason": "The failing pip invocation targeted the local pyflyby package (file:///Users/runner/work/ipython/pyflyby); the package root is therefore the subject of the metadata-generation error."
            },
            {
                "file": "ipykernel/ipykernel/kernelbase.py",
                "line_number": null,
                "reason": "Multiple PendingDeprecationWarning messages during the ipykernel pytest run reference this file (lines 891, 925, 950, 1117). These warnings were recorded in the test output (12 warnings) but are non-fatal and not the cause of the job failure."
            }
        ],
        "error_types": [
            {
                "category": "Dependency Error",
                "subcategory": "Missing dependency (setuptools_scm)",
                "evidence": "Meson invoked `python -m setuptools_scm` and stderr: \"No module named setuptools_scm\"; Meson then reported the command failed with status 1."
            },
            {
                "category": "Build/Packaging Error",
                "subcategory": "metadata-generation-failed (pyproject/PEP 517 editable build)",
                "evidence": "pip error: \"Preparing editable metadata (pyproject.toml) did not run successfully. exit code: 1\" and \"error: metadata-generation-failed\" from file:///Users/runner/work/ipython/pyflyby."
            },
            {
                "category": "Test Outcome / Warnings",
                "subcategory": "PendingDeprecationWarning in ipykernel",
                "evidence": "pytest output shows multiple PendingDeprecationWarning entries referencing ipykernel/ipykernel/kernelbase.py (e.g. line 891) and the summary shows \"12 warnings\"; tests themselves passed (188 passed)."
            }
        ],
        "failed_job": [
            {
                "job": "test",
                "step": "Install pyflyby",
                "command": "pip install --no-build-isolation -ve .[test]"
            }
        ]
    },
    {
        "sha_fail": "275119c05eb95555e52b5c4b25ee27baf150fbe6",
        "error_context": [
            "The run failed during the job 'musllinux_x86_64' (display name: 'musl Ubuntu-latest, fast, py3.11/npAny, spin') in the 'test' step when running the test suite (invoked via `spin test`). The pytest invocation finished with a non-zero exit because one test failed: scipy/_lib/tests/test_public_api.py::test_private_but_present_deprecation[scipy.linalg.decomp_svd-None]. The failure is an AttributeError raised while the deprecation helper attempted to delegate access of get_lapack_funcs to a private replacement module: \"AttributeError: module 'scipy.linalg._decomp_svd' has no attribute 'get_lapack_funcs'\" (log trace shows the chain: getattr(module, attr_name) -> scipy/linalg/decomp_svd.py:19 __getattr__ -> scipy/_lib/deprecation.py:72-77 where the AttributeError is raised).",
            "Secondary non-fatal issues were observed earlier in the same job during build: Meson emitted a configuration warning recommending using the built-in option instead of \"-std=legacy\" (meson.build:94), and the C/C++/Cython build produced compiler warnings about potentially uninitialized uses (e.g. -Wuninitialized in object.h when compiling scipy/_lib/_uarray/_uarray_dispatch.cxx and -Wmaybe-uninitialized in generated _zeros.c). These build warnings are recorded in the logs but did not directly cause the CI to exit with code 1\u2014the immediate cause is the pytest failure described above.",
            "An environment warning repeatedly appeared during dependency installation in the 'prep build environment' step: pip printed that '/github/home/.cache/pip' is not writable and the cache was disabled. This is an environment permission warning and not the proximate cause of the test failure."
        ],
        "relevant_files": [
            {
                "file": "scipy/_lib/tests/test_public_api.py",
                "line_number": 429,
                "reason": "This test function is where the failing assertion is executed. The log shows the failing test id and traceback pointing to line 429 and the call getattr(module, attr_name) that triggered the failure."
            },
            {
                "file": "scipy/linalg/decomp_svd.py",
                "line_number": 19,
                "reason": "__getattr__ in this module is invoked by the test when accessing get_lapack_funcs on scipy.linalg.decomp_svd; the traceback shows decomp_svd.py:19 delegating to the deprecation helper."
            },
            {
                "file": "scipy/_lib/deprecation.py",
                "line_number": 72,
                "reason": "The deprecation helper _sub_module_deprecation attempts to locate the attribute on private modules and raises the AttributeError. The traceback shows the exception raised at the getattr(import_module(...), attribute) call in this file."
            },
            {
                "file": "scipy/linalg/_decomp_svd.py",
                "line_number": null,
                "reason": "This private replacement module was imported by the deprecation helper but does not contain get_lapack_funcs, causing the AttributeError: \"module 'scipy.linalg._decomp_svd' has no attribute 'get_lapack_funcs'\"."
            },
            {
                "file": "meson.build",
                "line_number": 94,
                "reason": "Meson emitted a configuration-time warning at meson.build:94 recommending replacing an explicit \"-std=legacy\" usage with a built-in option (evidence: 'meson.build:94: WARNING: Consider using the built-in option for language standard version instead of using \"-std=legacy\"')."
            },
            {
                "file": "scipy/_lib/_uarray/_uarray_dispatch.cxx",
                "line_number": 316,
                "reason": "Compiler inlining diagnostics cite this generated source as the site of inlined functions that triggered -Wuninitialized warnings referencing CPython's object.h (multiple inlined locations reported, first at line 316)."
            },
            {
                "file": "/opt/_internal/cpython-3.11.14/include/python3.11/object.h",
                "line_number": 133,
                "reason": "Compiler diagnostics reference object.h:133 as the location where ob->ob_type is reported as used uninitialized when inlined from the C++ code (evidence: 'object.h:133:16: warning: ... used uninitialized [-Wuninitialized]')."
            },
            {
                "file": "scipy/optimize/cython_optimize/_zeros.cpython-311-x86_64-linux-musl.so.p/_zeros.c",
                "line_number": 3376,
                "reason": "Generated C file triggered a maybe-uninitialized warning for '__pyx_v_solver_stats.iterations' at line 3376 (evidence: 'warning: '__pyx_v_solver_stats.iterations' may be used uninitialized [-Wmaybe-uninitialized]')."
            },
            {
                "file": "/github/home/.cache/pip",
                "line_number": null,
                "reason": "Pip printed repeated permission warnings that this cache directory is not owned/writable and disabled the cache; appears during dependency installation in 'prep build environment'."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AttributeError during test (deprecation delegation)",
                "evidence": "Pytest summary: 1 failed. Traceback shows: \"AttributeError: module 'scipy.linalg._decomp_svd' has no attribute 'get_lapack_funcs'\" raised from scipy/_lib/deprecation.py when getattr(import_module(...), attribute) was attempted."
            },
            {
                "category": "Compilation Warning",
                "subcategory": "Uninitialized / Maybe-uninitialized warnings",
                "evidence": "Compiler warnings: '/opt/_internal/.../object.h:133:16: warning: ... used uninitialized [-Wuninitialized]' originating from compiling scipy/_lib/_uarray/_uarray_dispatch.cxx; and '_zeros.c:3376:37: warning: '__pyx_v_solver_stats.iterations' may be used uninitialized [-Wmaybe-uninitialized]'."
            },
            {
                "category": "Configuration Warning",
                "subcategory": "Meson language standard option",
                "evidence": "Meson printed: 'meson.build:94: WARNING: Consider using the built-in option for language standard version instead of using \"-std=legacy\".'"
            },
            {
                "category": "Environment Warning",
                "subcategory": "pip cache permission",
                "evidence": "Repeated pip warnings: \"WARNING: The directory '/github/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled.\""
            }
        ],
        "failed_job": [
            {
                "job": "musllinux_x86_64",
                "step": "test",
                "command": "set -xe\ncd \"$RUNNER_TEMP\"\n. test_env/bin/activate\ncd \"$GITHUB_WORKSPACE\"\nexport PKG_CONFIG_PATH=\"$PWD\"\nspin test"
            }
        ]
    },
    {
        "sha_fail": "204e413e26b928e07629584dee4ac15a051533b2",
        "error_context": [
            "The CI run completed dependency installation and built Pillow successfully, but failed during testing. The failure occurred in the 'Test Pillow' step (workflow step: \"Test Pillow\"), where pytest reported a single failing test: Tests/test_file_cur.py::test_largest_cursor. The test constructs an in-memory ICO/CUR structure and calls Image.open(BytesIO(data)). During decoding, a BMP/DIB tile read encountered an unexpected EOF and raised OSError: \"image file is truncated (0 bytes not processed)\", which caused the test to fail and the job to exit with code 1. The Python stack trace shows the error propagated through PIL's CurImagePlugin -> IcoImagePlugin -> DibImageFile/BmpImagePlugin -> ImageFile load logic, demonstrating the failure is triggered while decoding an ICO/CUR frame to RGBA from an in-memory bytes buffer. Non-fatal warnings appeared earlier (pacman dependency-cycle warnings and pip PEP 668 \"externally-managed-environment\" warnings) but these did not stop the build; the explicit cause of CI failure is the pytest test failure caused by the raised OSError."
        ],
        "relevant_files": [
            {
                "file": "Tests/test_file_cur.py",
                "line_number": 41,
                "reason": "Location of the failing test: pytest reported 'Tests/test_file_cur.py::test_largest_cursor FAILED' and the traceback shows the failure originated at Tests/test_file_cur.py:41 when calling Image.open(BytesIO(data))."
            },
            {
                "file": "PIL/ImageFile.py",
                "line_number": 391,
                "reason": "OSError was raised here: traceback shows 'PIL/ImageFile.py:391: OSError' with message 'image file is truncated (0 bytes not processed)', indicating the decoder detected truncated tile data during read."
            },
            {
                "file": "PIL/CurImagePlugin.py",
                "line_number": 223,
                "reason": "CurImagePlugin._open called self.load() which entered the failing code path; traceback includes 'PIL/CurImagePlugin.py:223: in _open', linking the CUR wrapper to the ICO/BMP decoding that failed."
            },
            {
                "file": "PIL/IcoImagePlugin.py",
                "line_number": 294,
                "reason": "IcoImagePlugin.frame calls convert('RGBA') on the extracted image and appears in the call chain (trace frames at lines 205, 294, 349), showing ICO handling forwarded the image data into the BMP/DIB loader that raised the error."
            },
            {
                "file": "PIL/Image.py",
                "line_number": 3539,
                "reason": "Image.open -> _open_core is the entry point shown in the traceback ('PIL/Image.py:3539' and related frames), confirming the image open/convert path that triggered the decoding error."
            },
            {
                "file": "PIL/BmpImagePlugin.py",
                "line_number": null,
                "reason": "The BMP/DIB loader's DibImageFile.load implementation performed tile reads and raised/traced the truncated-file OSError; traceback references code in the BMP plugin (decoder read/tiles logic) as the root of the raised exception."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Failing test (exception)",
                "evidence": "Pytest summary: '1 failed, 365 passed, 6 skipped' and failure block: 'Tests/test_file_cur.py::test_largest_cursor FAILED'."
            },
            {
                "category": "Runtime Error",
                "subcategory": "OSError: truncated image data",
                "evidence": "Exception message from runtime: 'OSError: image file is truncated (0 bytes not processed)' raised in PIL/ImageFile.py:391 during decoder read."
            },
            {
                "category": "Dependency Warning",
                "subcategory": "Package manager dependency cycle",
                "evidence": "pacman emitted warnings: 'warning: dependency cycle detected: mingw-w64-x86_64-harfbuzz will be installed before its ... freetype dependency' and '... libwebp will be installed before its ... libtiff dependency'."
            },
            {
                "category": "Environment Warning",
                "subcategory": "PEP 668 externally-managed-environment",
                "evidence": "pip printed 'WARNING: The script was installed in ... which is not on PATH.' and 'WARNING: The environment is externally managed (PEP 668): see https://pip.pypa.io/en/stable/topics/dependency-management/' in the install logs."
            },
            {
                "category": "Configuration / Missing optional feature",
                "subcategory": "Optional XCB support not installed",
                "evidence": "Selftest output: '*** XCB (X protocol) support not installed' (informational, not fatal to the failing test)."
            }
        ],
        "failed_job": [
            {
                "job": "MinGW",
                "step": "Test Pillow",
                "command": "python3 selftest.py --installed\n.ci/test.sh"
            }
        ]
    },
    {
        "sha_fail": "f11df47284ea65b6491ea8cd86dd31acd98059ec",
        "error_context": [
            "The CI run failed because a unit test assertion failed during the 'Tests (ubuntu-latest, 3.10, 4, 3)' job while running the project's pytest suite. The environment setup steps completed successfully (venv created, dependencies installed, open_clip_torch built). The failure occurred when pytest executed tests in the 'Unit tests' step: pytest collected tests, ran a test group (pytest-split reported 'Running group 3/4') and immediately reported one failed test: tests/test_download_pretrained.py::DownloadPretrainedTests::test_download_pretrained_from_hfh. The test computes normalized image and text features and compares the resulting softmax probabilities to a hard-coded expected tensor via self.assertTrue(torch.allclose(..., 1e-3)), which failed (AssertionError: False is not true). Pytest stopped after this single failure and returned exit code 1, causing the job to fail. A separate PytestCollectionWarning (cannot collect test class 'TestWrapper' because it has a __init__ constructor) was recorded from tests/util_test.py but is a warning and did not directly cause the job failure."
        ],
        "relevant_files": [
            {
                "file": "tests/test_download_pretrained.py",
                "line_number": 111,
                "reason": "Contains the failing assertion: 'self.assertTrue(torch.allclose(text_probs, torch.tensor([[0.0597, 0.6349, 0.3053]]), 1e-3))' \u2014 pytest shows this line failed and the traceback cites tests/test_download_pretrained.py:111."
            },
            {
                "file": "tests/util_test.py",
                "line_number": 203,
                "reason": "Emits a PytestCollectionWarning: 'cannot collect test class \"TestWrapper\" because it has a __init__ constructor' (logs quote the warning referencing tests/util_test.py:203)."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError",
                "evidence": "Pytest failure: 'tests/test_download_pretrained.py::DownloadPretrainedTests::test_download_pretrained_from_hfh FAILED' with traceback showing 'self.assertTrue(torch.allclose(...))' and 'E       AssertionError: False is not true'."
            },
            {
                "category": "Test Collection",
                "subcategory": "PytestCollectionWarning",
                "evidence": "Warning logged: '/home/runner/work/open_clip/open_clip/tests/util_test.py:203: PytestCollectionWarning: cannot collect test class \"TestWrapper\" because it has a __init__ constructor (from: tests/util_test.py)'."
            }
        ],
        "failed_job": [
            {
                "job": "Tests",
                "step": "Unit tests",
                "command": "source .env/bin/activate\nif [[ -f .test_durations ]]\nthen\n  cp .test_durations durations_1\n  mv .test_durations durations_2\nfi\npython -m pytest           -x -s -v           --splitting-algorithm least_duration           --splits ${ matrix.job_num }           --group ${ matrix.job }           --store-durations           --durations-path durations_1           --clean-durations           -m \"not regression_test\"           tests"
            }
        ]
    },
    {
        "sha_fail": "e705426fd12039f22f442486440eeb8d4c76fe9f",
        "error_context": [
            "The CI job 'Code health check' failed because the flake8 linter reported a code-style violation (line too long) and exited with a non-zero status. In the 'Run flake8 tests' step the workflow executed 'tox -e flake8', which ran the command 'flake8 scapy/'. Flake8 reported: \"scapy/layers/smb2.py:1513:89: E501 line too long (116 > 88 characters)\", causing flake8 to exit with code 1 and tox to mark the flake8 environment as FAIL. The 'Install tox' step completed successfully; the failure occurred during linting."
        ],
        "relevant_files": [
            {
                "file": "scapy/layers/smb2.py",
                "line_number": 1513,
                "reason": "flake8 reported an E501 (line too long) violation at this file and line: \"scapy/layers/smb2.py:1513:89: E501 line too long (116 > 88 characters)\", which caused flake8 to exit with code 1 and the job to fail."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Line Length Exceeded",
                "evidence": "\"scapy/layers/smb2.py:1513:89: E501 line too long (116 > 88 characters)\" reported by flake8."
            },
            {
                "category": "CI Job Failure",
                "subcategory": "Non-zero exit code from linting",
                "evidence": "\"flake8: exit 1\" and \"##[error]Process completed with exit code 1.\" shown after tox -e flake8 ran."
            }
        ],
        "failed_job": [
            {
                "job": "Code health check",
                "step": "Run flake8 tests",
                "command": "tox -e flake8"
            }
        ]
    },
    {
        "sha_fail": "f4dd72cde157a24001167e51cd9bcc32bb69f455",
        "error_context": [
            "The CI run failed during the 'Code health check' job because the 'Run flake8 tests' step (command: `tox -e flake8`) reported a flake8 linting violation and exited non-zero. Tox invoked flake8 which produced an E501 error (line too long) for a specific source file, causing flake8 to exit with code 1 and tox to record FAIL, and the workflow process completed with exit code 1.",
            "The 'Install tox' step completed successfully (pip installed tox and dependencies), so the failure is a code-style/linting issue detected by flake8 rather than an installation or runtime error."
        ],
        "relevant_files": [
            {
                "file": "scapy/layers/smb2.py",
                "line_number": 1513,
                "reason": "Flake8 reported a line-length violation at this file and line: \"scapy/layers/smb2.py:1513:89: E501 line too long (116 > 88 characters)\", which caused flake8 (and therefore tox) to exit with code 1."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Line Length Exceeded",
                "evidence": "\"scapy/layers/smb2.py:1513:89: E501 line too long (116 > 88 characters)\" and flake8 exited with code 1"
            }
        ],
        "failed_job": [
            {
                "job": "Code health check",
                "step": "Run flake8 tests",
                "command": "tox -e flake8"
            }
        ]
    }
]