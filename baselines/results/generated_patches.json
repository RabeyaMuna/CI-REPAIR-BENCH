[
    {
        "id": 239,
        "sha_fail": "a206d0817acd2948f4ed3f0af8402a2a368391dc",
        "diff": "diff --git a/facefusion/core.py b/facefusion/core.py\nindex c487f79..3307434 100755\n--- a/facefusion/core.py\n+++ b/facefusion/core.py\n@@ -9,576 +9,943 @@ from time import time\n import numpy\n from tqdm import tqdm\n \n-from facefusion import benchmarker, cli_helper, content_analyser, face_classifier, face_detector, face_landmarker, face_masker, face_recognizer, hash_helper, logger, process_manager, state_manager, video_manager, voice_extractor, wording\n-from facefusion.args import apply_args, collect_job_args, reduce_job_args, reduce_step_args\n+from facefusion import (\n+    benchmarker,\n+    cli_helper,\n+    content_analyser,\n+    face_classifier,\n+    face_detector,\n+    face_landmarker,\n+    face_masker,\n+    face_recognizer,\n+    hash_helper,\n+    logger,\n+    process_manager,\n+    state_manager,\n+    video_manager,\n+    voice_extractor,\n+    wording,\n+)\n+from facefusion.args import (\n+    apply_args,\n+    collect_job_args,\n+    reduce_job_args,\n+    reduce_step_args,\n+)\n from facefusion.audio import create_empty_audio_frame, get_audio_frame, get_voice_frame\n from facefusion.common_helper import get_first\n from facefusion.content_analyser import analyse_image, analyse_video\n-from facefusion.download import conditional_download_hashes, conditional_download_sources\n+from facefusion.download import (\n+    conditional_download_hashes,\n+    conditional_download_sources,\n+)\n from facefusion.exit_helper import hard_exit, signal_exit\n from facefusion.face_analyser import get_average_face, get_many_faces, get_one_face\n from facefusion.face_selector import sort_and_filter_faces\n-from facefusion.face_store import append_reference_face, clear_reference_faces, get_reference_faces\n-from facefusion.ffmpeg import copy_image, extract_frames, finalize_image, merge_video, replace_audio, restore_audio\n-from facefusion.filesystem import filter_audio_paths, get_file_name, is_image, is_video, resolve_file_paths, resolve_file_pattern\n+from facefusion.face_store import (\n+    append_reference_face,\n+    clear_reference_faces,\n+    get_reference_faces,\n+)\n+from facefusion.ffmpeg import (\n+    copy_image,\n+    extract_frames,\n+    finalize_image,\n+    merge_video,\n+    replace_audio,\n+    restore_audio,\n+)\n+from facefusion.filesystem import (\n+    filter_audio_paths,\n+    get_file_name,\n+    is_image,\n+    is_video,\n+    resolve_file_paths,\n+    resolve_file_pattern,\n+)\n from facefusion.jobs import job_helper, job_manager, job_runner\n from facefusion.jobs.job_list import compose_job_list\n from facefusion.memory import limit_system_memory\n from facefusion.processors.core import get_processors_modules\n-from facefusion.processors.types import ProcessorInputs\n from facefusion.program import create_program\n from facefusion.program_helper import validate_args\n-from facefusion.temp_helper import clear_temp_directory, create_temp_directory, get_temp_file_path, move_temp_file, resolve_temp_frame_paths\n+from facefusion.temp_helper import (\n+    clear_temp_directory,\n+    create_temp_directory,\n+    get_temp_file_path,\n+    move_temp_file,\n+    resolve_temp_frame_paths,\n+)\n from facefusion.time_helper import calculate_end_time\n-from facefusion.types import Args, ErrorCode, VisionFrame\n-from facefusion.vision import pack_resolution, read_image, read_static_images, read_video_frame, restrict_image_resolution, restrict_trim_frame, restrict_video_fps, restrict_video_resolution, unpack_resolution, write_image\n+from facefusion.types import Args, ErrorCode\n+from facefusion.vision import (\n+    pack_resolution,\n+    read_image,\n+    read_static_images,\n+    read_video_frame,\n+    restrict_image_resolution,\n+    restrict_trim_frame,\n+    restrict_video_fps,\n+    restrict_video_resolution,\n+    unpack_resolution,\n+    write_image,\n+)\n \n \n def cli() -> None:\n-\tif pre_check():\n-\t\tsignal.signal(signal.SIGINT, signal_exit)\n-\t\tprogram = create_program()\n-\n-\t\tif validate_args(program):\n-\t\t\targs = vars(program.parse_args())\n-\t\t\tapply_args(args, state_manager.init_item)\n-\n-\t\t\tif state_manager.get_item('command'):\n-\t\t\t\tlogger.init(state_manager.get_item('log_level'))\n-\t\t\t\troute(args)\n-\t\t\telse:\n-\t\t\t\tprogram.print_help()\n-\t\telse:\n-\t\t\thard_exit(2)\n-\telse:\n-\t\thard_exit(2)\n-\n-\n-def route(args : Args) -> None:\n-\tsystem_memory_limit = state_manager.get_item('system_memory_limit')\n-\n-\tif system_memory_limit and system_memory_limit > 0:\n-\t\tlimit_system_memory(system_memory_limit)\n-\n-\tif state_manager.get_item('command') == 'force-download':\n-\t\terror_code = force_download()\n-\t\treturn hard_exit(error_code)\n-\n-\tif state_manager.get_item('command') == 'benchmark':\n-\t\tif not common_pre_check() or not processors_pre_check() or not benchmarker.pre_check():\n-\t\t\treturn hard_exit(2)\n-\t\tbenchmarker.render()\n-\n-\tif state_manager.get_item('command') in [ 'job-list', 'job-create', 'job-submit', 'job-submit-all', 'job-delete', 'job-delete-all', 'job-add-step', 'job-remix-step', 'job-insert-step', 'job-remove-step' ]:\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_code = route_job_manager(args)\n-\t\thard_exit(error_code)\n-\n-\tif state_manager.get_item('command') == 'run':\n-\t\timport facefusion.uis.core as ui\n-\n-\t\tif not common_pre_check() or not processors_pre_check():\n-\t\t\treturn hard_exit(2)\n-\t\tfor ui_layout in ui.get_ui_layouts_modules(state_manager.get_item('ui_layouts')):\n-\t\t\tif not ui_layout.pre_check():\n-\t\t\t\treturn hard_exit(2)\n-\t\tui.init()\n-\t\tui.launch()\n-\n-\tif state_manager.get_item('command') == 'headless-run':\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_core = process_headless(args)\n-\t\thard_exit(error_core)\n-\n-\tif state_manager.get_item('command') == 'batch-run':\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_core = process_batch(args)\n-\t\thard_exit(error_core)\n-\n-\tif state_manager.get_item('command') in [ 'job-run', 'job-run-all', 'job-retry', 'job-retry-all' ]:\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_code = route_job_runner()\n-\t\thard_exit(error_code)\n+    if pre_check():\n+        signal.signal(signal.SIGINT, signal_exit)\n+        program = create_program()\n+\n+        if validate_args(program):\n+            args = vars(program.parse_args())\n+            apply_args(args, state_manager.init_item)\n+\n+            if state_manager.get_item(\"command\"):\n+                logger.init(state_manager.get_item(\"log_level\"))\n+                route(args)\n+            else:\n+                program.print_help()\n+        else:\n+            hard_exit(2)\n+    else:\n+        hard_exit(2)\n+\n+\n+def route(args: Args) -> None:\n+    system_memory_limit = state_manager.get_item(\"system_memory_limit\")\n+\n+    if system_memory_limit and system_memory_limit > 0:\n+        limit_system_memory(system_memory_limit)\n+\n+    if state_manager.get_item(\"command\") == \"force-download\":\n+        error_code = force_download()\n+        return hard_exit(error_code)\n+\n+    if state_manager.get_item(\"command\") == \"benchmark\":\n+        if (\n+            not common_pre_check()\n+            or not processors_pre_check()\n+            or not benchmarker.pre_check()\n+        ):\n+            return hard_exit(2)\n+        benchmarker.render()\n+\n+    if state_manager.get_item(\"command\") in [\n+        \"job-list\",\n+        \"job-create\",\n+        \"job-submit\",\n+        \"job-submit-all\",\n+        \"job-delete\",\n+        \"job-delete-all\",\n+        \"job-add-step\",\n+        \"job-remix-step\",\n+        \"job-insert-step\",\n+        \"job-remove-step\",\n+    ]:\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_code = route_job_manager(args)\n+        hard_exit(error_code)\n+\n+    if state_manager.get_item(\"command\") == \"run\":\n+        import facefusion.uis.core as ui\n+\n+        if not common_pre_check() or not processors_pre_check():\n+            return hard_exit(2)\n+        for ui_layout in ui.get_ui_layouts_modules(\n+            state_manager.get_item(\"ui_layouts\")\n+        ):\n+            if not ui_layout.pre_check():\n+                return hard_exit(2)\n+        ui.init()\n+        ui.launch()\n+\n+    if state_manager.get_item(\"command\") == \"headless-run\":\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_core = process_headless(args)\n+        hard_exit(error_core)\n+\n+    if state_manager.get_item(\"command\") == \"batch-run\":\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_core = process_batch(args)\n+        hard_exit(error_core)\n+\n+    if state_manager.get_item(\"command\") in [\n+        \"job-run\",\n+        \"job-run-all\",\n+        \"job-retry\",\n+        \"job-retry-all\",\n+    ]:\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_code = route_job_runner()\n+        hard_exit(error_code)\n \n \n def pre_check() -> bool:\n-\tif sys.version_info < (3, 10):\n-\t\tlogger.error(wording.get('python_not_supported').format(version = '3.10'), __name__)\n-\t\treturn False\n+    if sys.version_info < (3, 10):\n+        logger.error(\n+            wording.get(\"python_not_supported\").format(version=\"3.10\"), __name__\n+        )\n+        return False\n \n-\tif not shutil.which('curl'):\n-\t\tlogger.error(wording.get('curl_not_installed'), __name__)\n-\t\treturn False\n+    if not shutil.which(\"curl\"):\n+        logger.error(wording.get(\"curl_not_installed\"), __name__)\n+        return False\n \n-\tif not shutil.which('ffmpeg'):\n-\t\tlogger.error(wording.get('ffmpeg_not_installed'), __name__)\n-\t\treturn False\n-\treturn True\n+    if not shutil.which(\"ffmpeg\"):\n+        logger.error(wording.get(\"ffmpeg_not_installed\"), __name__)\n+        return False\n+    return True\n \n \n def common_pre_check() -> bool:\n-\tcommon_modules =\\\n-\t[\n-\t\tcontent_analyser,\n-\t\tface_classifier,\n-\t\tface_detector,\n-\t\tface_landmarker,\n-\t\tface_masker,\n-\t\tface_recognizer,\n-\t\tvoice_extractor\n-\t]\n+    common_modules = [\n+        content_analyser,\n+        face_classifier,\n+        face_detector,\n+        face_landmarker,\n+        face_masker,\n+        face_recognizer,\n+        voice_extractor,\n+    ]\n \n-\tcontent_analyser_content = inspect.getsource(content_analyser).encode()\n-\tcontent_analyser_hash = hash_helper.create_hash(content_analyser_content)\n+    content_analyser_content = inspect.getsource(content_analyser).encode()\n+    content_analyser_hash = hash_helper.create_hash(content_analyser_content)\n \n-\treturn all(module.pre_check() for module in common_modules) and content_analyser_hash == '1591e652'\n+    return (\n+        all(module.pre_check() for module in common_modules)\n+        and content_analyser_hash == \"1591e652\"\n+    )\n \n \n def processors_pre_check() -> bool:\n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\tif not processor_module.pre_check():\n-\t\t\treturn False\n-\treturn True\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        if not processor_module.pre_check():\n+            return False\n+    return True\n \n \n def force_download() -> ErrorCode:\n-\tcommon_modules =\\\n-\t[\n-\t\tcontent_analyser,\n-\t\tface_classifier,\n-\t\tface_detector,\n-\t\tface_landmarker,\n-\t\tface_masker,\n-\t\tface_recognizer,\n-\t\tvoice_extractor\n-\t]\n-\tavailable_processors = [ get_file_name(file_path) for file_path in resolve_file_paths('facefusion/processors/modules') ]\n-\tprocessor_modules = get_processors_modules(available_processors)\n-\n-\tfor module in common_modules + processor_modules:\n-\t\tif hasattr(module, 'create_static_model_set'):\n-\t\t\tfor model in module.create_static_model_set(state_manager.get_item('download_scope')).values():\n-\t\t\t\tmodel_hash_set = model.get('hashes')\n-\t\t\t\tmodel_source_set = model.get('sources')\n-\n-\t\t\t\tif model_hash_set and model_source_set:\n-\t\t\t\t\tif not conditional_download_hashes(model_hash_set) or not conditional_download_sources(model_source_set):\n-\t\t\t\t\t\treturn 1\n-\n-\treturn 0\n-\n-\n-def route_job_manager(args : Args) -> ErrorCode:\n-\tif state_manager.get_item('command') == 'job-list':\n-\t\tjob_headers, job_contents = compose_job_list(state_manager.get_item('job_status'))\n-\n-\t\tif job_contents:\n-\t\t\tcli_helper.render_table(job_headers, job_contents)\n-\t\t\treturn 0\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-create':\n-\t\tif job_manager.create_job(state_manager.get_item('job_id')):\n-\t\t\tlogger.info(wording.get('job_created').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_not_created').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-submit':\n-\t\tif job_manager.submit_job(state_manager.get_item('job_id')):\n-\t\t\tlogger.info(wording.get('job_submitted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_not_submitted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-submit-all':\n-\t\tif job_manager.submit_jobs(state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(wording.get('job_all_submitted'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_all_not_submitted'), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-delete':\n-\t\tif job_manager.delete_job(state_manager.get_item('job_id')):\n-\t\t\tlogger.info(wording.get('job_deleted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_not_deleted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-delete-all':\n-\t\tif job_manager.delete_jobs(state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(wording.get('job_all_deleted'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_all_not_deleted'), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-add-step':\n-\t\tstep_args = reduce_step_args(args)\n-\n-\t\tif job_manager.add_step(state_manager.get_item('job_id'), step_args):\n-\t\t\tlogger.info(wording.get('job_step_added').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_step_not_added').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-remix-step':\n-\t\tstep_args = reduce_step_args(args)\n-\n-\t\tif job_manager.remix_step(state_manager.get_item('job_id'), state_manager.get_item('step_index'), step_args):\n-\t\t\tlogger.info(wording.get('job_remix_step_added').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_remix_step_not_added').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-insert-step':\n-\t\tstep_args = reduce_step_args(args)\n-\n-\t\tif job_manager.insert_step(state_manager.get_item('job_id'), state_manager.get_item('step_index'), step_args):\n-\t\t\tlogger.info(wording.get('job_step_inserted').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_step_not_inserted').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-remove-step':\n-\t\tif job_manager.remove_step(state_manager.get_item('job_id'), state_manager.get_item('step_index')):\n-\t\t\tlogger.info(wording.get('job_step_removed').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(wording.get('job_step_not_removed').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\treturn 1\n-\treturn 1\n+    common_modules = [\n+        content_analyser,\n+        face_classifier,\n+        face_detector,\n+        face_landmarker,\n+        face_masker,\n+        face_recognizer,\n+        voice_extractor,\n+    ]\n+    available_processors = [\n+        get_file_name(file_path)\n+        for file_path in resolve_file_paths(\"facefusion/processors/modules\")\n+    ]\n+    processor_modules = get_processors_modules(available_processors)\n+\n+    for module in common_modules + processor_modules:\n+        if hasattr(module, \"create_static_model_set\"):\n+            for model in module.create_static_model_set(\n+                state_manager.get_item(\"download_scope\")\n+            ).values():\n+                model_hash_set = model.get(\"hashes\")\n+                model_source_set = model.get(\"sources\")\n+\n+                if model_hash_set and model_source_set:\n+                    if not conditional_download_hashes(\n+                        model_hash_set\n+                    ) or not conditional_download_sources(model_source_set):\n+                        return 1\n+\n+    return 0\n+\n+\n+def route_job_manager(args: Args) -> ErrorCode:\n+    if state_manager.get_item(\"command\") == \"job-list\":\n+        job_headers, job_contents = compose_job_list(\n+            state_manager.get_item(\"job_status\")\n+        )\n+\n+        if job_contents:\n+            cli_helper.render_table(job_headers, job_contents)\n+            return 0\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-create\":\n+        if job_manager.create_job(state_manager.get_item(\"job_id\")):\n+            logger.info(\n+                wording.get(\"job_created\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_not_created\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-submit\":\n+        if job_manager.submit_job(state_manager.get_item(\"job_id\")):\n+            logger.info(\n+                wording.get(\"job_submitted\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_not_submitted\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-submit-all\":\n+        if job_manager.submit_jobs(state_manager.get_item(\"halt_on_error\")):\n+            logger.info(wording.get(\"job_all_submitted\"), __name__)\n+            return 0\n+        logger.error(wording.get(\"job_all_not_submitted\"), __name__)\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-delete\":\n+        if job_manager.delete_job(state_manager.get_item(\"job_id\")):\n+            logger.info(\n+                wording.get(\"job_deleted\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_not_deleted\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-delete-all\":\n+        if job_manager.delete_jobs(state_manager.get_item(\"halt_on_error\")):\n+            logger.info(wording.get(\"job_all_deleted\"), __name__)\n+            return 0\n+        logger.error(wording.get(\"job_all_not_deleted\"), __name__)\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-add-step\":\n+        step_args = reduce_step_args(args)\n+\n+        if job_manager.add_step(state_manager.get_item(\"job_id\"), step_args):\n+            logger.info(\n+                wording.get(\"job_step_added\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_step_not_added\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-remix-step\":\n+        step_args = reduce_step_args(args)\n+\n+        if job_manager.remix_step(\n+            state_manager.get_item(\"job_id\"),\n+            state_manager.get_item(\"step_index\"),\n+            step_args,\n+        ):\n+            logger.info(\n+                wording.get(\"job_remix_step_added\").format(\n+                    job_id=state_manager.get_item(\"job_id\"),\n+                    step_index=state_manager.get_item(\"step_index\"),\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_remix_step_not_added\").format(\n+                job_id=state_manager.get_item(\"job_id\"),\n+                step_index=state_manager.get_item(\"step_index\"),\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-insert-step\":\n+        step_args = reduce_step_args(args)\n+\n+        if job_manager.insert_step(\n+            state_manager.get_item(\"job_id\"),\n+            state_manager.get_item(\"step_index\"),\n+            step_args,\n+        ):\n+            logger.info(\n+                wording.get(\"job_step_inserted\").format(\n+                    job_id=state_manager.get_item(\"job_id\"),\n+                    step_index=state_manager.get_item(\"step_index\"),\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_step_not_inserted\").format(\n+                job_id=state_manager.get_item(\"job_id\"),\n+                step_index=state_manager.get_item(\"step_index\"),\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-remove-step\":\n+        if job_manager.remove_step(\n+            state_manager.get_item(\"job_id\"), state_manager.get_item(\"step_index\")\n+        ):\n+            logger.info(\n+                wording.get(\"job_step_removed\").format(\n+                    job_id=state_manager.get_item(\"job_id\"),\n+                    step_index=state_manager.get_item(\"step_index\"),\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            wording.get(\"job_step_not_removed\").format(\n+                job_id=state_manager.get_item(\"job_id\"),\n+                step_index=state_manager.get_item(\"step_index\"),\n+            ),\n+            __name__,\n+        )\n+        return 1\n+    return 1\n \n \n def route_job_runner() -> ErrorCode:\n-\tif state_manager.get_item('command') == 'job-run':\n-\t\tlogger.info(wording.get('running_job').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\tif job_runner.run_job(state_manager.get_item('job_id'), process_step):\n-\t\t\tlogger.info(wording.get('processing_job_succeed').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(wording.get('processing_job_failed').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-run-all':\n-\t\tlogger.info(wording.get('running_jobs'), __name__)\n-\t\tif job_runner.run_jobs(process_step, state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(wording.get('processing_jobs_succeed'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(wording.get('processing_jobs_failed'), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-retry':\n-\t\tlogger.info(wording.get('retrying_job').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\tif job_runner.retry_job(state_manager.get_item('job_id'), process_step):\n-\t\t\tlogger.info(wording.get('processing_job_succeed').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(wording.get('processing_job_failed').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-retry-all':\n-\t\tlogger.info(wording.get('retrying_jobs'), __name__)\n-\t\tif job_runner.retry_jobs(process_step, state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(wording.get('processing_jobs_succeed'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(wording.get('processing_jobs_failed'), __name__)\n-\t\treturn 1\n-\treturn 2\n-\n-\n-def process_headless(args : Args) -> ErrorCode:\n-\tjob_id = job_helper.suggest_job_id('headless')\n-\tstep_args = reduce_step_args(args)\n-\n-\tif job_manager.create_job(job_id) and job_manager.add_step(job_id, step_args) and job_manager.submit_job(job_id) and job_runner.run_job(job_id, process_step):\n-\t\treturn 0\n-\treturn 1\n-\n-\n-def process_batch(args : Args) -> ErrorCode:\n-\tjob_id = job_helper.suggest_job_id('batch')\n-\tstep_args = reduce_step_args(args)\n-\tjob_args = reduce_job_args(args)\n-\tsource_paths = resolve_file_pattern(job_args.get('source_pattern'))\n-\ttarget_paths = resolve_file_pattern(job_args.get('target_pattern'))\n-\n-\tif job_manager.create_job(job_id):\n-\t\tif source_paths and target_paths:\n-\t\t\tfor index, (source_path, target_path) in enumerate(itertools.product(source_paths, target_paths)):\n-\t\t\t\tstep_args['source_paths'] = [ source_path ]\n-\t\t\t\tstep_args['target_path'] = target_path\n-\t\t\t\tstep_args['output_path'] = job_args.get('output_pattern').format(index = index)\n-\t\t\t\tif not job_manager.add_step(job_id, step_args):\n-\t\t\t\t\treturn 1\n-\t\t\tif job_manager.submit_job(job_id) and job_runner.run_job(job_id, process_step):\n-\t\t\t\treturn 0\n-\n-\t\tif not source_paths and target_paths:\n-\t\t\tfor index, target_path in enumerate(target_paths):\n-\t\t\t\tstep_args['target_path'] = target_path\n-\t\t\t\tstep_args['output_path'] = job_args.get('output_pattern').format(index = index)\n-\t\t\t\tif not job_manager.add_step(job_id, step_args):\n-\t\t\t\t\treturn 1\n-\t\t\tif job_manager.submit_job(job_id) and job_runner.run_job(job_id, process_step):\n-\t\t\t\treturn 0\n-\treturn 1\n-\n-\n-def process_step(job_id : str, step_index : int, step_args : Args) -> bool:\n-\tclear_reference_faces()\n-\tstep_total = job_manager.count_step_total(job_id)\n-\tstep_args.update(collect_job_args())\n-\tapply_args(step_args, state_manager.set_item)\n-\n-\tlogger.info(wording.get('processing_step').format(step_current = step_index + 1, step_total = step_total), __name__)\n-\tif common_pre_check() and processors_pre_check():\n-\t\terror_code = conditional_process()\n-\t\treturn error_code == 0\n-\treturn False\n+    if state_manager.get_item(\"command\") == \"job-run\":\n+        logger.info(\n+            wording.get(\"running_job\").format(job_id=state_manager.get_item(\"job_id\")),\n+            __name__,\n+        )\n+        if job_runner.run_job(state_manager.get_item(\"job_id\"), process_step):\n+            logger.info(\n+                wording.get(\"processing_job_succeed\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.info(\n+            wording.get(\"processing_job_failed\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-run-all\":\n+        logger.info(wording.get(\"running_jobs\"), __name__)\n+        if job_runner.run_jobs(process_step, state_manager.get_item(\"halt_on_error\")):\n+            logger.info(wording.get(\"processing_jobs_succeed\"), __name__)\n+            return 0\n+        logger.info(wording.get(\"processing_jobs_failed\"), __name__)\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-retry\":\n+        logger.info(\n+            wording.get(\"retrying_job\").format(job_id=state_manager.get_item(\"job_id\")),\n+            __name__,\n+        )\n+        if job_runner.retry_job(state_manager.get_item(\"job_id\"), process_step):\n+            logger.info(\n+                wording.get(\"processing_job_succeed\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.info(\n+            wording.get(\"processing_job_failed\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-retry-all\":\n+        logger.info(wording.get(\"retrying_jobs\"), __name__)\n+        if job_runner.retry_jobs(process_step, state_manager.get_item(\"halt_on_error\")):\n+            logger.info(wording.get(\"processing_jobs_succeed\"), __name__)\n+            return 0\n+        logger.info(wording.get(\"processing_jobs_failed\"), __name__)\n+        return 1\n+    return 2\n+\n+\n+def process_headless(args: Args) -> ErrorCode:\n+    job_id = job_helper.suggest_job_id(\"headless\")\n+    step_args = reduce_step_args(args)\n+\n+    if (\n+        job_manager.create_job(job_id)\n+        and job_manager.add_step(job_id, step_args)\n+        and job_manager.submit_job(job_id)\n+        and job_runner.run_job(job_id, process_step)\n+    ):\n+        return 0\n+    return 1\n+\n+\n+def process_batch(args: Args) -> ErrorCode:\n+    job_id = job_helper.suggest_job_id(\"batch\")\n+    step_args = reduce_step_args(args)\n+    job_args = reduce_job_args(args)\n+    source_paths = resolve_file_pattern(job_args.get(\"source_pattern\"))\n+    target_paths = resolve_file_pattern(job_args.get(\"target_pattern\"))\n+\n+    if job_manager.create_job(job_id):\n+        if source_paths and target_paths:\n+            for index, (source_path, target_path) in enumerate(\n+                itertools.product(source_paths, target_paths)\n+            ):\n+                step_args[\"source_paths\"] = [source_path]\n+                step_args[\"target_path\"] = target_path\n+                step_args[\"output_path\"] = job_args.get(\"output_pattern\").format(\n+                    index=index\n+                )\n+                if not job_manager.add_step(job_id, step_args):\n+                    return 1\n+            if job_manager.submit_job(job_id) and job_runner.run_job(\n+                job_id, process_step\n+            ):\n+                return 0\n+\n+        if not source_paths and target_paths:\n+            for index, target_path in enumerate(target_paths):\n+                step_args[\"target_path\"] = target_path\n+                step_args[\"output_path\"] = job_args.get(\"output_pattern\").format(\n+                    index=index\n+                )\n+                if not job_manager.add_step(job_id, step_args):\n+                    return 1\n+            if job_manager.submit_job(job_id) and job_runner.run_job(\n+                job_id, process_step\n+            ):\n+                return 0\n+    return 1\n+\n+\n+def process_step(job_id: str, step_index: int, step_args: Args) -> bool:\n+    clear_reference_faces()\n+    step_total = job_manager.count_step_total(job_id)\n+    step_args.update(collect_job_args())\n+    apply_args(step_args, state_manager.set_item)\n+\n+    logger.info(\n+        wording.get(\"processing_step\").format(\n+            step_current=step_index + 1, step_total=step_total\n+        ),\n+        __name__,\n+    )\n+    if common_pre_check() and processors_pre_check():\n+        error_code = conditional_process()\n+        return error_code == 0\n+    return False\n \n \n def conditional_process() -> ErrorCode:\n-\tstart_time = time()\n+    start_time = time()\n \n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\tif not processor_module.pre_process('output'):\n-\t\t\treturn 2\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        if not processor_module.pre_process(\"output\"):\n+            return 2\n \n-\tconditional_append_reference_faces()\n+    conditional_append_reference_faces()\n \n-\tif is_image(state_manager.get_item('target_path')):\n-\t\treturn process_image(start_time)\n-\tif is_video(state_manager.get_item('target_path')):\n-\t\treturn process_video(start_time)\n+    if is_image(state_manager.get_item(\"target_path\")):\n+        return process_image(start_time)\n+    if is_video(state_manager.get_item(\"target_path\")):\n+        return process_video(start_time)\n \n-\treturn 0\n+    return 0\n \n \n def conditional_append_reference_faces() -> None:\n-\tif 'reference' in state_manager.get_item('face_selector_mode') and not get_reference_faces():\n-\t\tsource_frames = read_static_images(state_manager.get_item('source_paths'))\n-\t\tsource_faces = get_many_faces(source_frames)\n-\t\tsource_face = get_average_face(source_faces)\n-\t\tif is_video(state_manager.get_item('target_path')):\n-\t\t\treference_frame = read_video_frame(state_manager.get_item('target_path'), state_manager.get_item('reference_frame_number'))\n-\t\telse:\n-\t\t\treference_frame = read_image(state_manager.get_item('target_path'))\n-\t\treference_faces = sort_and_filter_faces(get_many_faces([ reference_frame ]))\n-\t\treference_face = get_one_face(reference_faces, state_manager.get_item('reference_face_position'))\n-\t\tappend_reference_face('origin', reference_face)\n-\n-\t\tif source_face and reference_face:\n-\t\t\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\t\t\tabstract_reference_frame = processor_module.get_reference_frame(source_face, reference_face, reference_frame)\n-\t\t\t\tif numpy.any(abstract_reference_frame):\n-\t\t\t\t\tabstract_reference_faces = sort_and_filter_faces(get_many_faces([ abstract_reference_frame ]))\n-\t\t\t\t\tabstract_reference_face = get_one_face(abstract_reference_faces, state_manager.get_item('reference_face_position'))\n-\t\t\t\t\tappend_reference_face(processor_module.__name__, abstract_reference_face)\n-\n-\n-def process_image(start_time : float) -> ErrorCode:\n-\tif analyse_image(state_manager.get_item('target_path')):\n-\t\treturn 3\n-\n-\tlogger.debug(wording.get('clearing_temp'), __name__)\n-\tclear_temp_directory(state_manager.get_item('target_path'))\n-\tlogger.debug(wording.get('creating_temp'), __name__)\n-\tcreate_temp_directory(state_manager.get_item('target_path'))\n-\n-\tprocess_manager.start()\n-\ttemp_image_resolution = pack_resolution(restrict_image_resolution(state_manager.get_item('target_path'), unpack_resolution(state_manager.get_item('output_image_resolution'))))\n-\tlogger.info(wording.get('copying_image').format(resolution = temp_image_resolution), __name__)\n-\tif copy_image(state_manager.get_item('target_path'), temp_image_resolution):\n-\t\tlogger.debug(wording.get('copying_image_succeed'), __name__)\n-\telse:\n-\t\tlogger.error(wording.get('copying_image_failed'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\n-\ttemp_image_path = get_temp_file_path(state_manager.get_item('target_path'))\n-\treference_faces = get_reference_faces() if 'reference' in state_manager.get_item('face_selector_mode') else None\n-\tsource_vision_frames = read_static_images(state_manager.get_item('source_paths'))\n-\tsource_audio_frame = create_empty_audio_frame()\n-\tsource_voice_frame = create_empty_audio_frame()\n-\ttarget_vision_frame = read_image(temp_image_path)\n-\ttemp_vision_frame = target_vision_frame.copy()\n-\n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\tlogger.info(wording.get('processing'), processor_module.__name__)\n-\n-\t\ttemp_vision_frame = processor_module.process_frame(\n-\t\t{\n-\t\t\t'reference_faces': reference_faces,\n-\t\t\t'source_vision_frames': source_vision_frames,\n-\t\t\t'source_audio_frame': source_audio_frame,\n-\t\t\t'source_voice_frame': source_voice_frame,\n-\t\t\t'target_vision_frame': target_vision_frame,\n-\t\t\t'temp_vision_frame': temp_vision_frame\n-\t\t})\n-\n-\t\tprocessor_module.post_process()\n-\n-\twrite_image(temp_image_path, temp_vision_frame)\n-\tif is_process_stopping():\n-\t\tprocess_manager.end()\n-\t\treturn 4\n-\n-\tlogger.info(wording.get('finalizing_image').format(resolution = state_manager.get_item('output_image_resolution')), __name__)\n-\tif finalize_image(state_manager.get_item('target_path'), state_manager.get_item('output_path'), state_manager.get_item('output_image_resolution')):\n-\t\tlogger.debug(wording.get('finalizing_image_succeed'), __name__)\n-\telse:\n-\t\tlogger.warn(wording.get('finalizing_image_skipped'), __name__)\n-\n-\tlogger.debug(wording.get('clearing_temp'), __name__)\n-\tclear_temp_directory(state_manager.get_item('target_path'))\n-\n-\tif is_image(state_manager.get_item('output_path')):\n-\t\tlogger.info(wording.get('processing_image_succeed').format(seconds = calculate_end_time(start_time)), __name__)\n-\telse:\n-\t\tlogger.error(wording.get('processing_image_failed'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\tprocess_manager.end()\n-\treturn 0\n-\n-\n-def process_video(start_time : float) -> ErrorCode:\n-\ttrim_frame_start, trim_frame_end = restrict_trim_frame(state_manager.get_item('target_path'), state_manager.get_item('trim_frame_start'), state_manager.get_item('trim_frame_end'))\n-\tif analyse_video(state_manager.get_item('target_path'), trim_frame_start, trim_frame_end):\n-\t\treturn 3\n-\n-\tlogger.debug(wording.get('clearing_temp'), __name__)\n-\tclear_temp_directory(state_manager.get_item('target_path'))\n-\tlogger.debug(wording.get('creating_temp'), __name__)\n-\tcreate_temp_directory(state_manager.get_item('target_path'))\n-\n-\tprocess_manager.start()\n-\ttemp_video_resolution = pack_resolution(restrict_video_resolution(state_manager.get_item('target_path'), unpack_resolution(state_manager.get_item('output_video_resolution'))))\n-\ttemp_video_fps = restrict_video_fps(state_manager.get_item('target_path'), state_manager.get_item('output_video_fps'))\n-\tlogger.info(wording.get('extracting_frames').format(resolution = temp_video_resolution, fps = temp_video_fps), __name__)\n-\tif extract_frames(state_manager.get_item('target_path'), temp_video_resolution, temp_video_fps, trim_frame_start, trim_frame_end):\n-\t\tlogger.debug(wording.get('extracting_frames_succeed'), __name__)\n-\telse:\n-\t\tif is_process_stopping():\n-\t\t\tprocess_manager.end()\n-\t\t\treturn 4\n-\t\tlogger.error(wording.get('extracting_frames_failed'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\n-\ttemp_frame_paths = resolve_temp_frame_paths(state_manager.get_item('target_path'))\n-\n-\tif temp_frame_paths:\n-\t\twith tqdm(total = len(temp_frame_paths), desc = wording.get('processing'), unit = 'frame', ascii = ' =', disable = state_manager.get_item('log_level') in [ 'warn', 'error' ]) as progress:\n-\t\t\tprogress.set_postfix(execution_providers = state_manager.get_item('execution_providers'))\n-\n-\t\t\twith ThreadPoolExecutor(max_workers = state_manager.get_item('execution_thread_count')) as executor:\n-\t\t\t\tfutures = []\n-\n-\t\t\t\tfor frame_number, temp_frame_path in enumerate(temp_frame_paths):\n-\t\t\t\t\tif is_process_stopping():\n-\t\t\t\t\t\tprocess_manager.end()\n-\t\t\t\t\t\treturn 4\n-\n-\t\t\t\t\tfuture = executor.submit(process_temp_frame, temp_frame_path, frame_number)\n-\t\t\t\t\tfutures.append(future)\n-\n-\t\t\t\tfor future in as_completed(futures):\n-\t\t\t\t\tfuture.result()\n-\t\t\t\t\tprogress.update()\n-\n-\t\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\t\tprocessor_module.post_process()\n-\telse:\n-\t\tlogger.error(wording.get('temp_frames_not_found'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\n-\tlogger.info(wording.get('merging_video').format(resolution = state_manager.get_item('output_video_resolution'), fps = state_manager.get_item('output_video_fps')), __name__)\n-\tif merge_video(state_manager.get_item('target_path'), temp_video_fps, state_manager.get_item('output_video_resolution'), state_manager.get_item('output_video_fps'), trim_frame_start, trim_frame_end):\n-\t\tlogger.debug(wording.get('merging_video_succeed'), __name__)\n-\telse:\n-\t\tif is_process_stopping():\n-\t\t\tprocess_manager.end()\n-\t\t\treturn 4\n-\t\tlogger.error(wording.get('merging_video_failed'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\n-\tif state_manager.get_item('output_audio_volume') == 0:\n-\t\tlogger.info(wording.get('skipping_audio'), __name__)\n-\t\tmove_temp_file(state_manager.get_item('target_path'), state_manager.get_item('output_path'))\n-\telse:\n-\t\tsource_audio_path = get_first(filter_audio_paths(state_manager.get_item('source_paths')))\n-\t\tif source_audio_path:\n-\t\t\tif replace_audio(state_manager.get_item('target_path'), source_audio_path, state_manager.get_item('output_path')):\n-\t\t\t\tvideo_manager.clear_video_pool()\n-\t\t\t\tlogger.debug(wording.get('replacing_audio_succeed'), __name__)\n-\t\t\telse:\n-\t\t\t\tvideo_manager.clear_video_pool()\n-\t\t\t\tif is_process_stopping():\n-\t\t\t\t\tprocess_manager.end()\n-\t\t\t\t\treturn 4\n-\t\t\t\tlogger.warn(wording.get('replacing_audio_skipped'), __name__)\n-\t\t\t\tmove_temp_file(state_manager.get_item('target_path'), state_manager.get_item('output_path'))\n-\t\telse:\n-\t\t\tif restore_audio(state_manager.get_item('target_path'), state_manager.get_item('output_path'), trim_frame_start, trim_frame_end):\n-\t\t\t\tvideo_manager.clear_video_pool()\n-\t\t\t\tlogger.debug(wording.get('restoring_audio_succeed'), __name__)\n-\t\t\telse:\n-\t\t\t\tvideo_manager.clear_video_pool()\n-\t\t\t\tif is_process_stopping():\n-\t\t\t\t\tprocess_manager.end()\n-\t\t\t\t\treturn 4\n-\t\t\t\tlogger.warn(wording.get('restoring_audio_skipped'), __name__)\n-\t\t\t\tmove_temp_file(state_manager.get_item('target_path'), state_manager.get_item('output_path'))\n-\n-\tlogger.debug(wording.get('clearing_temp'), __name__)\n-\tclear_temp_directory(state_manager.get_item('target_path'))\n-\n-\tif is_video(state_manager.get_item('output_path')):\n-\t\tlogger.info(wording.get('processing_video_succeed').format(seconds = calculate_end_time(start_time)), __name__)\n-\telse:\n-\t\tlogger.error(wording.get('processing_video_failed'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\tprocess_manager.end()\n-\treturn 0\n-\n-\n-def process_temp_frame(temp_frame_path : str, frame_number : int) -> bool:\n-\treference_faces = get_reference_faces() if 'reference' in state_manager.get_item('face_selector_mode') else None\n-\tsource_vision_frames = read_static_images(state_manager.get_item('source_paths'))\n-\tsource_audio_path = get_first(filter_audio_paths(state_manager.get_item('source_paths')))\n-\ttemp_video_fps = restrict_video_fps(state_manager.get_item('target_path'), state_manager.get_item('output_video_fps'))\n-\ttarget_vision_frame = read_image(temp_frame_path)\n-\ttemp_vision_frame = target_vision_frame.copy()\n-\n-\tsource_audio_frame = get_audio_frame(source_audio_path, temp_video_fps, frame_number)\n-\tsource_voice_frame = get_voice_frame(source_audio_path, temp_video_fps, frame_number)\n-\n-\tif not numpy.any(source_audio_frame):\n-\t\tsource_audio_frame = create_empty_audio_frame()\n-\tif not numpy.any(source_voice_frame):\n-\t\tsource_voice_frame = create_empty_audio_frame()\n-\n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\ttemp_vision_frame = processor_module.process_frame(\n-\t\t{\n-\t\t\t'reference_faces': reference_faces,\n-\t\t\t'source_vision_frames': source_vision_frames,\n-\t\t\t'source_audio_frame': source_audio_frame,\n-\t\t\t'source_voice_frame': source_voice_frame,\n-\t\t\t'target_vision_frame': target_vision_frame,\n-\t\t\t'temp_vision_frame': temp_vision_frame\n-\t\t})\n-\n-\treturn write_image(temp_frame_path, temp_vision_frame)\n+    if (\n+        \"reference\" in state_manager.get_item(\"face_selector_mode\")\n+        and not get_reference_faces()\n+    ):\n+        source_frames = read_static_images(state_manager.get_item(\"source_paths\"))\n+        source_faces = get_many_faces(source_frames)\n+        source_face = get_average_face(source_faces)\n+        if is_video(state_manager.get_item(\"target_path\")):\n+            reference_frame = read_video_frame(\n+                state_manager.get_item(\"target_path\"),\n+                state_manager.get_item(\"reference_frame_number\"),\n+            )\n+        else:\n+            reference_frame = read_image(state_manager.get_item(\"target_path\"))\n+        reference_faces = sort_and_filter_faces(get_many_faces([reference_frame]))\n+        reference_face = get_one_face(\n+            reference_faces, state_manager.get_item(\"reference_face_position\")\n+        )\n+        append_reference_face(\"origin\", reference_face)\n+\n+        if source_face and reference_face:\n+            for processor_module in get_processors_modules(\n+                state_manager.get_item(\"processors\")\n+            ):\n+                abstract_reference_frame = processor_module.get_reference_frame(\n+                    source_face, reference_face, reference_frame\n+                )\n+                if numpy.any(abstract_reference_frame):\n+                    abstract_reference_faces = sort_and_filter_faces(\n+                        get_many_faces([abstract_reference_frame])\n+                    )\n+                    abstract_reference_face = get_one_face(\n+                        abstract_reference_faces,\n+                        state_manager.get_item(\"reference_face_position\"),\n+                    )\n+                    append_reference_face(\n+                        processor_module.__name__, abstract_reference_face\n+                    )\n+\n+\n+def process_image(start_time: float) -> ErrorCode:\n+    if analyse_image(state_manager.get_item(\"target_path\")):\n+        return 3\n+\n+    logger.debug(wording.get(\"clearing_temp\"), __name__)\n+    clear_temp_directory(state_manager.get_item(\"target_path\"))\n+    logger.debug(wording.get(\"creating_temp\"), __name__)\n+    create_temp_directory(state_manager.get_item(\"target_path\"))\n+\n+    process_manager.start()\n+    temp_image_resolution = pack_resolution(\n+        restrict_image_resolution(\n+            state_manager.get_item(\"target_path\"),\n+            unpack_resolution(state_manager.get_item(\"output_image_resolution\")),\n+        )\n+    )\n+    logger.info(\n+        wording.get(\"copying_image\").format(resolution=temp_image_resolution), __name__\n+    )\n+    if copy_image(state_manager.get_item(\"target_path\"), temp_image_resolution):\n+        logger.debug(wording.get(\"copying_image_succeed\"), __name__)\n+    else:\n+        logger.error(wording.get(\"copying_image_failed\"), __name__)\n+        process_manager.end()\n+        return 1\n+\n+    temp_image_path = get_temp_file_path(state_manager.get_item(\"target_path\"))\n+    reference_faces = (\n+        get_reference_faces()\n+        if \"reference\" in state_manager.get_item(\"face_selector_mode\")\n+        else None\n+    )\n+    source_vision_frames = read_static_images(state_manager.get_item(\"source_paths\"))\n+    source_audio_frame = create_empty_audio_frame()\n+    source_voice_frame = create_empty_audio_frame()\n+    target_vision_frame = read_image(temp_image_path)\n+    temp_vision_frame = target_vision_frame.copy()\n+\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        logger.info(wording.get(\"processing\"), processor_module.__name__)\n+\n+        temp_vision_frame = processor_module.process_frame(\n+            {\n+                \"reference_faces\": reference_faces,\n+                \"source_vision_frames\": source_vision_frames,\n+                \"source_audio_frame\": source_audio_frame,\n+                \"source_voice_frame\": source_voice_frame,\n+                \"target_vision_frame\": target_vision_frame,\n+                \"temp_vision_frame\": temp_vision_frame,\n+            }\n+        )\n+\n+        processor_module.post_process()\n+\n+    write_image(temp_image_path, temp_vision_frame)\n+    if is_process_stopping():\n+        process_manager.end()\n+        return 4\n+\n+    logger.info(\n+        wording.get(\"finalizing_image\").format(\n+            resolution=state_manager.get_item(\"output_image_resolution\")\n+        ),\n+        __name__,\n+    )\n+    if finalize_image(\n+        state_manager.get_item(\"target_path\"),\n+        state_manager.get_item(\"output_path\"),\n+        state_manager.get_item(\"output_image_resolution\"),\n+    ):\n+        logger.debug(wording.get(\"finalizing_image_succeed\"), __name__)\n+    else:\n+        logger.warn(wording.get(\"finalizing_image_skipped\"), __name__)\n+\n+    logger.debug(wording.get(\"clearing_temp\"), __name__)\n+    clear_temp_directory(state_manager.get_item(\"target_path\"))\n+\n+    if is_image(state_manager.get_item(\"output_path\")):\n+        logger.info(\n+            wording.get(\"processing_image_succeed\").format(\n+                seconds=calculate_end_time(start_time)\n+            ),\n+            __name__,\n+        )\n+    else:\n+        logger.error(wording.get(\"processing_image_failed\"), __name__)\n+        process_manager.end()\n+        return 1\n+    process_manager.end()\n+    return 0\n+\n+\n+def process_video(start_time: float) -> ErrorCode:\n+    trim_frame_start, trim_frame_end = restrict_trim_frame(\n+        state_manager.get_item(\"target_path\"),\n+        state_manager.get_item(\"trim_frame_start\"),\n+        state_manager.get_item(\"trim_frame_end\"),\n+    )\n+    if analyse_video(\n+        state_manager.get_item(\"target_path\"), trim_frame_start, trim_frame_end\n+    ):\n+        return 3\n+\n+    logger.debug(wording.get(\"clearing_temp\"), __name__)\n+    clear_temp_directory(state_manager.get_item(\"target_path\"))\n+    logger.debug(wording.get(\"creating_temp\"), __name__)\n+    create_temp_directory(state_manager.get_item(\"target_path\"))\n+\n+    process_manager.start()\n+    temp_video_resolution = pack_resolution(\n+        restrict_video_resolution(\n+            state_manager.get_item(\"target_path\"),\n+            unpack_resolution(state_manager.get_item(\"output_video_resolution\")),\n+        )\n+    )\n+    temp_video_fps = restrict_video_fps(\n+        state_manager.get_item(\"target_path\"),\n+        state_manager.get_item(\"output_video_fps\"),\n+    )\n+    logger.info(\n+        wording.get(\"extracting_frames\").format(\n+            resolution=temp_video_resolution, fps=temp_video_fps\n+        ),\n+        __name__,\n+    )\n+    if extract_frames(\n+        state_manager.get_item(\"target_path\"),\n+        temp_video_resolution,\n+        temp_video_fps,\n+        trim_frame_start,\n+        trim_frame_end,\n+    ):\n+        logger.debug(wording.get(\"extracting_frames_succeed\"), __name__)\n+    else:\n+        if is_process_stopping():\n+            process_manager.end()\n+            return 4\n+        logger.error(wording.get(\"extracting_frames_failed\"), __name__)\n+        process_manager.end()\n+        return 1\n+\n+    temp_frame_paths = resolve_temp_frame_paths(state_manager.get_item(\"target_path\"))\n+\n+    if temp_frame_paths:\n+        with tqdm(\n+            total=len(temp_frame_paths),\n+            desc=wording.get(\"processing\"),\n+            unit=\"frame\",\n+            ascii=\" =\",\n+            disable=state_manager.get_item(\"log_level\") in [\"warn\", \"error\"],\n+        ) as progress:\n+            progress.set_postfix(\n+                execution_providers=state_manager.get_item(\"execution_providers\")\n+            )\n+\n+            with ThreadPoolExecutor(\n+                max_workers=state_manager.get_item(\"execution_thread_count\")\n+            ) as executor:\n+                futures = []\n+\n+                for frame_number, temp_frame_path in enumerate(temp_frame_paths):\n+                    if is_process_stopping():\n+                        process_manager.end()\n+                        return 4\n+\n+                    future = executor.submit(\n+                        process_temp_frame, temp_frame_path, frame_number\n+                    )\n+                    futures.append(future)\n+\n+                for future in as_completed(futures):\n+                    future.result()\n+                    progress.update()\n+\n+        for processor_module in get_processors_modules(\n+            state_manager.get_item(\"processors\")\n+        ):\n+            processor_module.post_process()\n+    else:\n+        logger.error(wording.get(\"temp_frames_not_found\"), __name__)\n+        process_manager.end()\n+        return 1\n+\n+    logger.info(\n+        wording.get(\"merging_video\").format(\n+            resolution=state_manager.get_item(\"output_video_resolution\"),\n+            fps=state_manager.get_item(\"output_video_fps\"),\n+        ),\n+        __name__,\n+    )\n+    if merge_video(\n+        state_manager.get_item(\"target_path\"),\n+        temp_video_fps,\n+        state_manager.get_item(\"output_video_resolution\"),\n+        state_manager.get_item(\"output_video_fps\"),\n+        trim_frame_start,\n+        trim_frame_end,\n+    ):\n+        logger.debug(wording.get(\"merging_video_succeed\"), __name__)\n+    else:\n+        if is_process_stopping():\n+            process_manager.end()\n+            return 4\n+        logger.error(wording.get(\"merging_video_failed\"), __name__)\n+        process_manager.end()\n+        return 1\n+\n+    if state_manager.get_item(\"output_audio_volume\") == 0:\n+        logger.info(wording.get(\"skipping_audio\"), __name__)\n+        move_temp_file(\n+            state_manager.get_item(\"target_path\"), state_manager.get_item(\"output_path\")\n+        )\n+    else:\n+        source_audio_path = get_first(\n+            filter_audio_paths(state_manager.get_item(\"source_paths\"))\n+        )\n+        if source_audio_path:\n+            if replace_audio(\n+                state_manager.get_item(\"target_path\"),\n+                source_audio_path,\n+                state_manager.get_item(\"output_path\"),\n+            ):\n+                video_manager.clear_video_pool()\n+                logger.debug(wording.get(\"replacing_audio_succeed\"), __name__)\n+            else:\n+                video_manager.clear_video_pool()\n+                if is_process_stopping():\n+                    process_manager.end()\n+                    return 4\n+                logger.warn(wording.get(\"replacing_audio_skipped\"), __name__)\n+                move_temp_file(\n+                    state_manager.get_item(\"target_path\"),\n+                    state_manager.get_item(\"output_path\"),\n+                )\n+        else:\n+            if restore_audio(\n+                state_manager.get_item(\"target_path\"),\n+                state_manager.get_item(\"output_path\"),\n+                trim_frame_start,\n+                trim_frame_end,\n+            ):\n+                video_manager.clear_video_pool()\n+                logger.debug(wording.get(\"restoring_audio_succeed\"), __name__)\n+            else:\n+                video_manager.clear_video_pool()\n+                if is_process_stopping():\n+                    process_manager.end()\n+                    return 4\n+                logger.warn(wording.get(\"restoring_audio_skipped\"), __name__)\n+                move_temp_file(\n+                    state_manager.get_item(\"target_path\"),\n+                    state_manager.get_item(\"output_path\"),\n+                )\n+\n+    logger.debug(wording.get(\"clearing_temp\"), __name__)\n+    clear_temp_directory(state_manager.get_item(\"target_path\"))\n+\n+    if is_video(state_manager.get_item(\"output_path\")):\n+        logger.info(\n+            wording.get(\"processing_video_succeed\").format(\n+                seconds=calculate_end_time(start_time)\n+            ),\n+            __name__,\n+        )\n+    else:\n+        logger.error(wording.get(\"processing_video_failed\"), __name__)\n+        process_manager.end()\n+        return 1\n+    process_manager.end()\n+    return 0\n+\n+\n+def process_temp_frame(temp_frame_path: str, frame_number: int) -> bool:\n+    reference_faces = (\n+        get_reference_faces()\n+        if \"reference\" in state_manager.get_item(\"face_selector_mode\")\n+        else None\n+    )\n+    source_vision_frames = read_static_images(state_manager.get_item(\"source_paths\"))\n+    source_audio_path = get_first(\n+        filter_audio_paths(state_manager.get_item(\"source_paths\"))\n+    )\n+    temp_video_fps = restrict_video_fps(\n+        state_manager.get_item(\"target_path\"),\n+        state_manager.get_item(\"output_video_fps\"),\n+    )\n+    target_vision_frame = read_image(temp_frame_path)\n+    temp_vision_frame = target_vision_frame.copy()\n+\n+    source_audio_frame = get_audio_frame(\n+        source_audio_path, temp_video_fps, frame_number\n+    )\n+    source_voice_frame = get_voice_frame(\n+        source_audio_path, temp_video_fps, frame_number\n+    )\n+\n+    if not numpy.any(source_audio_frame):\n+        source_audio_frame = create_empty_audio_frame()\n+    if not numpy.any(source_voice_frame):\n+        source_voice_frame = create_empty_audio_frame()\n+\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        temp_vision_frame = processor_module.process_frame(\n+            {\n+                \"reference_faces\": reference_faces,\n+                \"source_vision_frames\": source_vision_frames,\n+                \"source_audio_frame\": source_audio_frame,\n+                \"source_voice_frame\": source_voice_frame,\n+                \"target_vision_frame\": target_vision_frame,\n+                \"temp_vision_frame\": temp_vision_frame,\n+            }\n+        )\n+\n+    return write_image(temp_frame_path, temp_vision_frame)\n \n \n def is_process_stopping() -> bool:\n-\tif process_manager.is_stopping():\n-\t\tprocess_manager.end()\n-\t\tlogger.info(wording.get('processing_stopped'), __name__)\n-\treturn process_manager.is_pending()\n+    if process_manager.is_stopping():\n+        process_manager.end()\n+        logger.info(wording.get(\"processing_stopped\"), __name__)\n+    return process_manager.is_pending()\n"
    },
    {
        "id": 242,
        "sha_fail": "4f79881e6cd35592692c7a7c1cb1d639eb70a009",
        "diff": "diff --git a/tests/smoke/test_fixtures.py b/tests/smoke/test_fixtures.py\nindex b696693..9a5b6b1 100644\n--- a/tests/smoke/test_fixtures.py\n+++ b/tests/smoke/test_fixtures.py\n@@ -207,11 +207,11 @@ class TestIndexer:\n             \"--root\",\n             root.resolve().as_posix(),\n             \"--method\",\n-            query_config[\"method\"],\n+            query_config.get(\"method\", \"default_method\"),\n             \"--community-level\",\n             str(query_config.get(\"community_level\", 2)),\n             \"--query\",\n-            query_config[\"query\"],\n+            query_config.get(\"query\", \"default_query\"),\n         ]\n \n         logger.info(\"running command \", \" \".join(command))\n@@ -264,6 +264,8 @@ class TestIndexer:\n             result = self.__run_query(root, query)\n             print(f\"Query: {query}\\nResponse: {result.stdout}\")\n \n-            assert result.returncode == 0, \"Query failed\"\n+            assert result.returncode == 0, (\n+                \"Query failed\" if result else \"No result returned\"\n+            )\n             assert result.stdout is not None, \"Query returned no output\"\n             assert len(result.stdout) > 0, \"Query returned empty output\"\n"
    },
    {
        "id": 244,
        "sha_fail": "73e5958ee67f4f9a6a822b25d4c39bae81f1b6f4",
        "diff": "diff --git a/tests/test_official/test_official.py b/tests/test_official/test_official.py\nindex 5e50e13..9ac0b46 100644\n--- a/tests/test_official/test_official.py\n+++ b/tests/test_official/test_official.py\n@@ -74,7 +74,7 @@ def test_check_method(tg_method: TelegramMethod) -> None:\n         # Check if parameter is present in our method\n         ptb_param = sig.parameters.get(tg_parameter.param_name)\n         assert ptb_param is not None, (\n-            f\"Parameter {tg_parameter.param_name} not found in {ptb_method.__name__}\"\n+            f\"Parameter {tg_parameter.param_name} not found in {ptb_method.__name__}. Check if 'direct_messages_topic_id' is included in the method signature.\"\n         )\n \n         # Now check if the parameter is required or not\n@@ -155,7 +155,9 @@ def test_check_object(tg_class: TelegramClass) -> None:\n             field = \"from_user\"\n \n         ptb_param = sig.parameters.get(field)\n-        assert ptb_param is not None, f\"Attribute {field} not found in {obj.__name__}\"\n+        assert ptb_param is not None, (\n+            f\"Attribute {field} not found in {obj.__name__}. Check if 'is_direct_messages' or 'can_manage_direct_messages' is defined in the class.\"\n+        )\n \n         # Now check if the parameter is required or not\n         assert check_required_param(tg_parameter, ptb_param, obj.__name__), (\n"
    },
    {
        "id": 245,
        "sha_fail": "22a76b07711abe61145409b4b4be6248747cea16",
        "diff": "diff --git a/redash/query_runner/jql.py b/redash/query_runner/jql.py\nindex b5d20a0..3bda981 100644\n--- a/redash/query_runner/jql.py\n+++ b/redash/query_runner/jql.py\n@@ -34,10 +34,10 @@ class ResultSet:\n \n def parse_issue(issue, field_mapping):  # noqa: C901\n     result = OrderedDict()\n-    \n+\n     # Handle API v3 response format: key field may be missing, use id as fallback\n     result[\"key\"] = issue.get(\"key\", issue.get(\"id\", \"unknown\"))\n-    \n+\n     # Handle API v3 response format: fields may be missing\n     fields = issue.get(\"fields\", {})\n     for k, v in fields.items():  #\n@@ -49,7 +49,9 @@ def parse_issue(issue, field_mapping):  # noqa: C901\n                 # if field mapping with dict member mappings defined get value of each member\n                 for member_name in member_names:\n                     if member_name in v:\n-                        result[field_mapping.get_dict_output_field_name(k, member_name)] = v[member_name]\n+                        result[\n+                            field_mapping.get_dict_output_field_name(k, member_name)\n+                        ] = v[member_name]\n \n             else:\n                 # these special mapping rules are kept for backwards compatibility\n@@ -74,7 +76,9 @@ def parse_issue(issue, field_mapping):  # noqa: C901\n                             if member_name in listItem:\n                                 listValues.append(listItem[member_name])\n                     if len(listValues) > 0:\n-                        result[field_mapping.get_dict_output_field_name(k, member_name)] = \",\".join(listValues)\n+                        result[\n+                            field_mapping.get_dict_output_field_name(k, member_name)\n+                        ] = \",\".join(listValues)\n \n             else:\n                 # otherwise support list values only for non-dict items\n@@ -167,7 +171,9 @@ class JiraJQL(BaseHTTPQueryRunner):\n \n     def run_query(self, query, user):\n         # Updated to API v3 endpoint, fix double slash issue\n-        jql_url = \"{}/rest/api/3/search/jql\".format(self.configuration[\"url\"].rstrip(\"/\"))\n+        jql_url = \"{}/rest/api/3/search/jql\".format(\n+            self.configuration[\"url\"].rstrip(\"/\")\n+        )\n \n         query = json_loads(query)\n         query_type = query.pop(\"queryType\", \"select\")\n@@ -193,7 +199,7 @@ class JiraJQL(BaseHTTPQueryRunner):\n             results = parse_count(data)\n         else:\n             results = parse_issues(data, field_mapping)\n-            \n+\n             # API v3 uses token-based pagination instead of startAt/total\n             while not data.get(\"isLast\", True) and \"nextPageToken\" in data:\n                 query[\"nextPageToken\"] = data[\"nextPageToken\"]\n"
    },
    {
        "id": 246,
        "sha_fail": "5d1b328545612a301cc57da123bcc7c34b547c4c",
        "diff": "diff --git a/celery/backends/redis.py b/celery/backends/redis.py\nindex dc2fa56..e6a55ff 100644\n--- a/celery/backends/redis.py\n+++ b/celery/backends/redis.py\n@@ -1,4 +1,5 @@\n \"\"\"Redis result store backend.\"\"\"\n+\n import time\n from contextlib import contextmanager\n from functools import partial\n@@ -36,7 +37,7 @@ try:\n except ImportError:\n     pass\n \n-__all__ = ('RedisBackend', 'SentinelBackend')\n+__all__ = (\"RedisBackend\", \"SentinelBackend\")\n \n E_REDIS_MISSING = \"\"\"\n You need to install the redis library in order to use \\\n@@ -70,7 +71,7 @@ A rediss:// URL must have parameter ssl_cert_reqs and this must be set to \\\n CERT_REQUIRED, CERT_OPTIONAL, or CERT_NONE\n \"\"\"\n \n-E_LOST = 'Connection to Redis lost: Retry (%s/%s) %s.'\n+E_LOST = \"Connection to Redis lost: Retry (%s/%s) %s.\"\n \n E_RETRY_LIMIT_EXCEEDED = \"\"\"\n Retry limit exceeded while trying to reconnect to the Celery redis result \\\n@@ -79,6 +80,7 @@ store backend. The Celery application must be restarted.\n \n logger = get_logger(__name__)\n \n+\n class ResultConsumer(BaseResultConsumer):\n     _pubsub = None\n \n@@ -117,7 +119,7 @@ class ResultConsumer(BaseResultConsumer):\n             self._pubsub.subscribe(*self.subscribed_to)\n         else:\n             self._pubsub.connection = self._pubsub.connection_pool.get_connection(\n-                'pubsub', self._pubsub.shard_hint\n+                \"pubsub\", self._pubsub.shard_hint\n             )\n             # even if there is nothing to subscribe, we should not lose the callback after connecting.\n             # The on_connect callback will re-subscribe to any channels we previously subscribed to.\n@@ -135,8 +137,8 @@ class ResultConsumer(BaseResultConsumer):\n                 raise\n \n     def _maybe_cancel_ready_task(self, meta):\n-        if meta['status'] in states.READY_STATES:\n-            self.cancel_for(meta['task_id'])\n+        if meta[\"status\"] in states.READY_STATES:\n+            self.cancel_for(meta[\"task_id\"])\n \n     def on_state_change(self, meta, message):\n         super().on_state_change(meta, message)\n@@ -161,8 +163,8 @@ class ResultConsumer(BaseResultConsumer):\n         if self._pubsub:\n             with self.reconnect_on_error():\n                 message = self._pubsub.get_message(timeout=timeout)\n-                if message and message['type'] == 'message':\n-                    self.on_state_change(self._decode_result(message['data']), message)\n+                if message and message[\"type\"] == \"message\":\n+                    self.on_state_change(self._decode_result(message[\"data\"]), message)\n         elif timeout:\n             time.sleep(timeout)\n \n@@ -209,43 +211,50 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n     #: 512 MB - https://redis.io/topics/data-types\n     _MAX_STR_VALUE_SIZE = 536870912\n \n-    def __init__(self, host=None, port=None, db=None, password=None,\n-                 max_connections=None, url=None,\n-                 connection_pool=None, **kwargs):\n+    def __init__(\n+        self,\n+        host=None,\n+        port=None,\n+        db=None,\n+        password=None,\n+        max_connections=None,\n+        url=None,\n+        connection_pool=None,\n+        **kwargs,\n+    ):\n         super().__init__(expires_type=int, **kwargs)\n         _get = self.app.conf.get\n         if self.redis is None:\n             raise ImproperlyConfigured(E_REDIS_MISSING.strip())\n \n-        if host and '://' in host:\n+        if host and \"://\" in host:\n             url, host = host, None\n \n         self.max_connections = (\n-            max_connections or\n-            _get('redis_max_connections') or\n-            self.max_connections)\n+            max_connections or _get(\"redis_max_connections\") or self.max_connections\n+        )\n         self._ConnectionPool = connection_pool\n \n-        socket_timeout = _get('redis_socket_timeout')\n-        socket_connect_timeout = _get('redis_socket_connect_timeout')\n-        retry_on_timeout = _get('redis_retry_on_timeout')\n-        socket_keepalive = _get('redis_socket_keepalive')\n-        health_check_interval = _get('redis_backend_health_check_interval')\n-        credential_provider = _get('redis_backend_credential_provider')\n+        socket_timeout = _get(\"redis_socket_timeout\")\n+        socket_connect_timeout = _get(\"redis_socket_connect_timeout\")\n+        retry_on_timeout = _get(\"redis_retry_on_timeout\")\n+        socket_keepalive = _get(\"redis_socket_keepalive\")\n+        health_check_interval = _get(\"redis_backend_health_check_interval\")\n+        credential_provider = _get(\"redis_backend_credential_provider\")\n \n         self.connparams = {\n-            'host': _get('redis_host') or 'localhost',\n-            'port': _get('redis_port') or 6379,\n-            'db': _get('redis_db') or 0,\n-            'password': _get('redis_password'),\n-            'max_connections': self.max_connections,\n-            'socket_timeout': socket_timeout and float(socket_timeout),\n-            'retry_on_timeout': retry_on_timeout or False,\n-            'socket_connect_timeout':\n-                socket_connect_timeout and float(socket_connect_timeout),\n+            \"host\": _get(\"redis_host\") or \"localhost\",\n+            \"port\": _get(\"redis_port\") or 6379,\n+            \"db\": _get(\"redis_db\") or 0,\n+            \"password\": _get(\"redis_password\"),\n+            \"max_connections\": self.max_connections,\n+            \"socket_timeout\": socket_timeout and float(socket_timeout),\n+            \"retry_on_timeout\": retry_on_timeout or False,\n+            \"socket_connect_timeout\": socket_connect_timeout\n+            and float(socket_connect_timeout),\n         }\n \n-        username = _get('redis_username')\n+        username = _get(\"redis_username\")\n         if username:\n             # We're extra careful to avoid including this configuration value\n             # if it wasn't specified since older versions of py-redis\n@@ -254,7 +263,7 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n \n             # TODO: Include this in connparams' definition once we drop\n             #       support for py-redis<3.4.0.\n-            self.connparams['username'] = username\n+            self.connparams[\"username\"] = username\n \n         if credential_provider:\n             # if credential provider passed as string or query param\n@@ -267,7 +276,7 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n                     \"Credential provider is not an instance of a redis.CredentialProvider or a subclass\"\n                 )\n \n-            self.connparams['credential_provider'] = credential_provider\n+            self.connparams[\"credential_provider\"] = credential_provider\n \n             # drop username and password if credential provider is configured\n             self.connparams.pop(\"username\", None)\n@@ -278,15 +287,15 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n \n         # absent in redis.connection.UnixDomainSocketConnection\n         if socket_keepalive:\n-            self.connparams['socket_keepalive'] = socket_keepalive\n+            self.connparams[\"socket_keepalive\"] = socket_keepalive\n \n         # \"redis_backend_use_ssl\" must be a dict with the keys:\n         # 'ssl_cert_reqs', 'ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile'\n         # (the same as \"broker_use_ssl\")\n-        ssl = _get('redis_backend_use_ssl')\n+        ssl = _get(\"redis_backend_use_ssl\")\n         if ssl:\n             self.connparams.update(ssl)\n-            self.connparams['connection_class'] = self.connection_class_ssl\n+            self.connparams[\"connection_class\"] = self.connection_class_ssl\n \n         if url:\n             self.connparams = self._params_from_url(url, self.connparams)\n@@ -294,16 +303,19 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n         # If we've received SSL parameters via query string or the\n         # redis_backend_use_ssl dict, check ssl_cert_reqs is valid. If set\n         # via query string ssl_cert_reqs will be a string so convert it here\n-        if ('connection_class' in self.connparams and\n-                issubclass(self.connparams['connection_class'], redis.SSLConnection)):\n-            ssl_cert_reqs_missing = 'MISSING'\n-            ssl_string_to_constant = {'CERT_REQUIRED': CERT_REQUIRED,\n-                                      'CERT_OPTIONAL': CERT_OPTIONAL,\n-                                      'CERT_NONE': CERT_NONE,\n-                                      'required': CERT_REQUIRED,\n-                                      'optional': CERT_OPTIONAL,\n-                                      'none': CERT_NONE}\n-            ssl_cert_reqs = self.connparams.get('ssl_cert_reqs', ssl_cert_reqs_missing)\n+        if \"connection_class\" in self.connparams and issubclass(\n+            self.connparams[\"connection_class\"], redis.SSLConnection\n+        ):\n+            ssl_cert_reqs_missing = \"MISSING\"\n+            ssl_string_to_constant = {\n+                \"CERT_REQUIRED\": CERT_REQUIRED,\n+                \"CERT_OPTIONAL\": CERT_OPTIONAL,\n+                \"CERT_NONE\": CERT_NONE,\n+                \"required\": CERT_REQUIRED,\n+                \"optional\": CERT_OPTIONAL,\n+                \"none\": CERT_NONE,\n+            }\n+            ssl_cert_reqs = self.connparams.get(\"ssl_cert_reqs\", ssl_cert_reqs_missing)\n             ssl_cert_reqs = ssl_string_to_constant.get(ssl_cert_reqs, ssl_cert_reqs)\n             if ssl_cert_reqs not in ssl_string_to_constant.values():\n                 raise ValueError(E_REDIS_SSL_CERT_REQS_MISSING_INVALID)\n@@ -312,51 +324,68 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n                 logger.warning(W_REDIS_SSL_CERT_OPTIONAL)\n             elif ssl_cert_reqs == CERT_NONE:\n                 logger.warning(W_REDIS_SSL_CERT_NONE)\n-            self.connparams['ssl_cert_reqs'] = ssl_cert_reqs\n+            self.connparams[\"ssl_cert_reqs\"] = ssl_cert_reqs\n \n         self.url = url\n \n         self.connection_errors, self.channel_errors = (\n-            get_redis_error_classes() if get_redis_error_classes\n-            else ((), ()))\n+            get_redis_error_classes() if get_redis_error_classes else ((), ())\n+        )\n         self.result_consumer = self.ResultConsumer(\n-            self, self.app, self.accept,\n-            self._pending_results, self._pending_messages,\n+            self,\n+            self.app,\n+            self.accept,\n+            self._pending_results,\n+            self._pending_messages,\n         )\n \n     def _params_from_url(self, url, defaults):\n         scheme, host, port, username, password, path, query = _parse_url(url)\n         connparams = dict(\n-            defaults, **dictfilter({\n-                'host': host, 'port': port, 'username': username,\n-                'password': password, 'db': query.pop('virtual_host', None)})\n+            defaults,\n+            **dictfilter(\n+                {\n+                    \"host\": host,\n+                    \"port\": port,\n+                    \"username\": username,\n+                    \"password\": password,\n+                    \"db\": query.pop(\"virtual_host\", None),\n+                }\n+            ),\n         )\n \n-        if scheme == 'socket':\n+        if scheme == \"socket\":\n             # use 'path' as path to the socket\u2026 in this case\n             # the database number should be given in 'query'\n-            connparams.update({\n-                'connection_class': self.redis.UnixDomainSocketConnection,\n-                'path': '/' + path,\n-            })\n+            connparams.update(\n+                {\n+                    \"connection_class\": self.redis.UnixDomainSocketConnection,\n+                    \"path\": \"/\" + path,\n+                }\n+            )\n             # host+port are invalid options when using this connection type.\n-            connparams.pop('host', None)\n-            connparams.pop('port', None)\n-            connparams.pop('socket_connect_timeout')\n+            connparams.pop(\"host\", None)\n+            connparams.pop(\"port\", None)\n+            connparams.pop(\"socket_connect_timeout\")\n         else:\n-            connparams['db'] = path\n+            connparams[\"db\"] = path\n \n-        ssl_param_keys = ['ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile',\n-                          'ssl_cert_reqs']\n+        ssl_param_keys = [\n+            \"ssl_ca_certs\",\n+            \"ssl_certfile\",\n+            \"ssl_keyfile\",\n+            \"ssl_cert_reqs\",\n+        ]\n \n-        if scheme == 'redis':\n+        if scheme == \"redis\":\n             # If connparams or query string contain ssl params, raise error\n-            if (any(key in connparams for key in ssl_param_keys) or\n-                    any(key in query for key in ssl_param_keys)):\n+            if any(key in connparams for key in ssl_param_keys) or any(\n+                key in query for key in ssl_param_keys\n+            ):\n                 raise ValueError(E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH)\n \n-        if scheme == 'rediss':\n-            connparams['connection_class'] = redis.SSLConnection\n+        if scheme == \"rediss\":\n+            connparams[\"connection_class\"] = redis.SSLConnection\n             # The following parameters, if present in the URL, are encoded. We\n             # must add the decoded values to connparams.\n             for ssl_setting in ssl_param_keys:\n@@ -365,9 +394,9 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n                     connparams[ssl_setting] = unquote(ssl_val)\n \n         # db may be string and start with / like in kombu.\n-        db = connparams.get('db') or 0\n-        db = db.strip('/') if isinstance(db, str) else db\n-        connparams['db'] = int(db)\n+        db = connparams.get(\"db\") or 0\n+        db = db.strip(\"/\") if isinstance(db, str) else db\n+        connparams[\"db\"] = int(db)\n \n         # credential provider as query string\n         credential_provider = query.pop(\"credential_provider\", None)\n@@ -381,16 +410,14 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n                     \"Credential provider is not an instance of a redis.CredentialProvider or a subclass\"\n                 )\n \n-            connparams['credential_provider'] = credential_provider\n+            connparams[\"credential_provider\"] = credential_provider\n             # drop username and password if credential provider is configured\n             connparams.pop(\"username\", None)\n             connparams.pop(\"password\", None)\n \n         for key, value in query.items():\n             if key in redis.connection.URL_QUERY_ARGUMENT_PARSERS:\n-                query[key] = redis.connection.URL_QUERY_ARGUMENT_PARSERS[key](\n-                    value\n-                )\n+                query[key] = redis.connection.URL_QUERY_ARGUMENT_PARSERS[key](value)\n \n         # Query parameters override other parameters\n         connparams.update(query)\n@@ -406,7 +433,7 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n         retry_policy = super().retry_policy\n         if \"retry_policy\" in self._transport_options:\n             retry_policy = retry_policy.copy()\n-            retry_policy.update(self._transport_options['retry_policy'])\n+            retry_policy.update(self._transport_options[\"retry_policy\"])\n \n         return retry_policy\n \n@@ -422,22 +449,26 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n \n     def ensure(self, fun, args, **policy):\n         retry_policy = dict(self.retry_policy, **policy)\n-        max_retries = retry_policy.get('max_retries')\n+        max_retries = retry_policy.get(\"max_retries\")\n         return retry_over_time(\n-            fun, self.connection_errors, args, {},\n+            fun,\n+            self.connection_errors,\n+            args,\n+            {},\n             partial(self.on_connection_error, max_retries),\n-            **retry_policy)\n+            **retry_policy,\n+        )\n \n     def on_connection_error(self, max_retries, exc, intervals, retries):\n         tts = next(intervals)\n         logger.error(\n-            E_LOST.strip(),\n-            retries, max_retries or 'Inf', humanize_seconds(tts, 'in '))\n+            E_LOST.strip(), retries, max_retries or \"Inf\", humanize_seconds(tts, \"in \")\n+        )\n         return tts\n \n     def set(self, key, value, **retry_policy):\n         if isinstance(value, str) and len(value) > self._MAX_STR_VALUE_SIZE:\n-            raise BackendStoreError('value too large for Redis backend')\n+            raise BackendStoreError(\"value too large for Redis backend\")\n \n         return self.ensure(self._set, (key, value), **retry_policy)\n \n@@ -464,23 +495,27 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n         return self.client.expire(key, value)\n \n     def add_to_chord(self, group_id, result):\n-        self.client.incr(self.get_key_for_group(group_id, '.t'), 1)\n-\n-    def _unpack_chord_result(self, tup, decode,\n-                             EXCEPTION_STATES=states.EXCEPTION_STATES,\n-                             PROPAGATE_STATES=states.PROPAGATE_STATES):\n+        self.client.incr(self.get_key_for_group(group_id, \".t\"), 1)\n+\n+    def _unpack_chord_result(\n+        self,\n+        tup,\n+        decode,\n+        EXCEPTION_STATES=states.EXCEPTION_STATES,\n+        PROPAGATE_STATES=states.PROPAGATE_STATES,\n+    ):\n         _, tid, state, retval = decode(tup)\n         if state in EXCEPTION_STATES:\n             retval = self.exception_to_python(retval)\n         if state in PROPAGATE_STATES:\n             chord_error = _create_chord_error_with_cause(\n-                message=f'Dependency {tid} raised {retval!r}', original_exc=retval\n+                message=f\"Dependency {tid} raised {retval!r}\", original_exc=retval\n             )\n             raise chord_error\n         return retval\n \n     def set_chord_size(self, group_id, chord_size):\n-        self.set(self.get_key_for_group(group_id, '.s'), chord_size)\n+        self.set(self.get_key_for_group(group_id, \".s\"), chord_size)\n \n     def apply_chord(self, header_result_args, body, **kwargs):\n         # If any of the child results of this chord are complex (ie. group\n@@ -496,38 +531,42 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n \n     @cached_property\n     def _chord_zset(self):\n-        return self._transport_options.get('result_chord_ordered', True)\n+        return self._transport_options.get(\"result_chord_ordered\", True)\n \n     @cached_property\n     def _transport_options(self):\n-        return self.app.conf.get('result_backend_transport_options', {})\n+        return self.app.conf.get(\"result_backend_transport_options\", {})\n \n-    def on_chord_part_return(self, request, state, result,\n-                             propagate=None, **kwargs):\n+    def on_chord_part_return(self, request, state, result, propagate=None, **kwargs):\n         app = self.app\n         tid, gid, group_index = request.id, request.group, request.group_index\n         if not gid or not tid:\n             return\n         if group_index is None:\n-            group_index = '+inf'\n+            group_index = \"+inf\"\n \n         client = self.client\n-        jkey = self.get_key_for_group(gid, '.j')\n-        tkey = self.get_key_for_group(gid, '.t')\n-        skey = self.get_key_for_group(gid, '.s')\n+        jkey = self.get_key_for_group(gid, \".j\")\n+        tkey = self.get_key_for_group(gid, \".t\")\n+        skey = self.get_key_for_group(gid, \".s\")\n         result = self.encode_result(result, state)\n         encoded = self.encode([1, tid, state, result])\n         with client.pipeline() as pipe:\n             pipeline = (\n-                pipe.zadd(jkey, {encoded: group_index}).zcount(jkey, \"-inf\", \"+inf\")\n-                if self._chord_zset\n-                else pipe.rpush(jkey, encoded).llen(jkey)\n-            ).get(tkey).get(skey)\n+                (\n+                    pipe.zadd(jkey, {encoded: group_index}).zcount(jkey, \"-inf\", \"+inf\")\n+                    if self._chord_zset\n+                    else pipe.rpush(jkey, encoded).llen(jkey)\n+                )\n+                .get(tkey)\n+                .get(skey)\n+            )\n             if self.expires:\n-                pipeline = pipeline \\\n-                    .expire(jkey, self.expires) \\\n-                    .expire(tkey, self.expires) \\\n+                pipeline = (\n+                    pipeline.expire(jkey, self.expires)\n+                    .expire(tkey, self.expires)\n                     .expire(skey, self.expires)\n+                )\n \n             _, readycount, totaldiff, chord_size_bytes = pipeline.execute()[:4]\n \n@@ -557,7 +596,7 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n                         with allow_join_result():\n                             resl = join_func(\n                                 timeout=app.conf.result_chord_join_timeout,\n-                                propagate=True\n+                                propagate=True,\n                             )\n                     else:\n                         # Otherwise simply extract and decode the results we\n@@ -569,32 +608,29 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n                                 pipeline = pipe.zrange(jkey, 0, -1)\n                             else:\n                                 pipeline = pipe.lrange(jkey, 0, total)\n-                            resl, = pipeline.execute()\n+                            (resl,) = pipeline.execute()\n                         resl = [unpack(tup, decode) for tup in resl]\n                     try:\n                         callback.delay(resl)\n                     except Exception as exc:  # pylint: disable=broad-except\n                         logger.exception(\n-                            'Chord callback for %r raised: %r', request.group, exc)\n+                            \"Chord callback for %r raised: %r\", request.group, exc\n+                        )\n                         return self.chord_error_from_stack(\n                             callback,\n-                            ChordError(f'Callback error: {exc!r}'),\n+                            ChordError(f\"Callback error: {exc!r}\"),\n                         )\n                     finally:\n                         with client.pipeline() as pipe:\n-                            pipe \\\n-                                .delete(jkey) \\\n-                                .delete(tkey) \\\n-                                .delete(skey) \\\n-                                .execute()\n+                            pipe.delete(jkey).delete(tkey).delete(skey).execute()\n             except ChordError as exc:\n-                logger.exception('Chord %r raised: %r', request.group, exc)\n+                logger.exception(\"Chord %r raised: %r\", request.group, exc)\n                 return self.chord_error_from_stack(callback, exc)\n             except Exception as exc:  # pylint: disable=broad-except\n-                logger.exception('Chord %r raised: %r', request.group, exc)\n+                logger.exception(\"Chord %r raised: %r\", request.group, exc)\n                 return self.chord_error_from_stack(\n                     callback,\n-                    ChordError(f'Join error: {exc!r}'),\n+                    ChordError(f\"Join error: {exc!r}\"),\n                 )\n \n     def _create_client(self, **params):\n@@ -621,13 +657,15 @@ class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n     def __reduce__(self, args=(), kwargs=None):\n         kwargs = {} if not kwargs else kwargs\n         return super().__reduce__(\n-            args, dict(kwargs, expires=self.expires, url=self.url))\n+            args, dict(kwargs, expires=self.expires, url=self.url)\n+        )\n \n \n if getattr(redis, \"sentinel\", None):\n+\n     class SentinelManagedSSLConnection(\n-            redis.sentinel.SentinelManagedConnection,\n-            redis.SSLConnection):\n+        redis.sentinel.SentinelManagedConnection, redis.SSLConnection\n+    ):\n         \"\"\"Connect to a Redis server using Sentinel + TLS.\n \n         Use Sentinel to identify which Redis server is the current master\n@@ -667,24 +705,22 @@ class SentinelBackend(RedisBackend):\n         # Similar to the superclass, strip the trailing slash from URIs with\n         # all components empty other than the scheme\n         return self._SERVER_URI_SEPARATOR.join(\n-            uri[:-1] if uri.endswith(\":///\") else uri\n-            for uri in uri_chunks\n+            uri[:-1] if uri.endswith(\":///\") else uri for uri in uri_chunks\n         )\n \n     def _params_from_url(self, url, defaults):\n         chunks = url.split(self._SERVER_URI_SEPARATOR)\n         connparams = dict(defaults, hosts=[])\n         for chunk in chunks:\n-            data = super()._params_from_url(\n-                url=chunk, defaults=defaults)\n-            connparams['hosts'].append(data)\n+            data = super()._params_from_url(url=chunk, defaults=defaults)\n+            connparams[\"hosts\"].append(data)\n         for param in (\"host\", \"port\", \"db\", \"password\"):\n             connparams.pop(param)\n \n         # Adding db/password in connparams to connect to the correct instance\n         for param in (\"db\", \"password\"):\n-            if connparams['hosts'] and param in connparams['hosts'][0]:\n-                connparams[param] = connparams['hosts'][0].get(param)\n+            if connparams[\"hosts\"] and param in connparams[\"hosts\"][0]:\n+                connparams[param] = connparams[\"hosts\"][0].get(param)\n         return connparams\n \n     def _get_sentinel_instance(self, **params):\n@@ -695,10 +731,11 @@ class SentinelBackend(RedisBackend):\n         sentinel_kwargs = self._transport_options.get(\"sentinel_kwargs\", {})\n \n         sentinel_instance = self.sentinel.Sentinel(\n-            [(cp['host'], cp['port']) for cp in hosts],\n+            [(cp[\"host\"], cp[\"port\"]) for cp in hosts],\n             min_other_sentinels=min_other_sentinels,\n             sentinel_kwargs=sentinel_kwargs,\n-            **connparams)\n+            **connparams,\n+        )\n \n         return sentinel_instance\n \n"
    },
    {
        "id": 249,
        "sha_fail": "43e32d5cecf8a6946ee3f147500687f17029140f",
        "diff": "diff --git a/tests/test_job_helper.py b/tests/test_job_helper.py\nindex 1f724bb..5195b2e 100644\n--- a/tests/test_job_helper.py\n+++ b/tests/test_job_helper.py\n@@ -4,6 +4,8 @@ from facefusion.jobs.job_helper import get_step_output_path\n \n \n def test_get_step_output_path() -> None:\n-\tassert get_step_output_path('test-job', 0, 'test.mp4') == 'test-test-job-0.mp4'\n-\tassert get_step_output_path('test-job', 0, 'test/test.mp4') == os.path.join('test', 'test-test-job-0.mp4')\n-\tassert get_step_output_path('test-job', 0, 'invalid') is None\n+    assert get_step_output_path(\"test-job\", 0, \"test.mp4\") == \"test-job-0.mp4\"\n+    assert get_step_output_path(\"test-job\", 0, \"test/test.mp4\") == os.path.join(\n+        \"test\", \"test-job-0.mp4\"\n+    )\n+    assert get_step_output_path(\"test-job\", 0, \"invalid\") is None\n"
    },
    {
        "id": 250,
        "sha_fail": "39ad7bd9c837699393e05ebdcc12c0c95119bc8f",
        "diff": "diff --git a/dash/testing/browser.py b/dash/testing/browser.py\nindex b6b1ee1..4177f0d 100644\n--- a/dash/testing/browser.py\n+++ b/dash/testing/browser.py\n@@ -134,9 +134,9 @@ class Browser(DashPageMixin):\n                 widths=widths,\n             )\n             if assert_check:\n-                assert not self.find_elements(\n-                    \"div.dash-debug-alert\"\n-                ), \"devtools should not raise an error alert\"\n+                assert not self.find_elements(\"div.dash-debug-alert\"), (\n+                    \"devtools should not raise an error alert\"\n+                )\n             if not stay_on_page:\n                 self.driver.back()\n         except WebDriverException as e:\n@@ -601,9 +601,9 @@ class Browser(DashPageMixin):\n         except MoveTargetOutOfBoundsException:\n             logger.exception(\"graph offset outside of the boundary\")\n         if compare:\n-            assert prev != elem.get_attribute(\n-                \"innerHTML\"\n-            ), \"SVG content should be different after zoom\"\n+            assert prev != elem.get_attribute(\"innerHTML\"), (\n+                \"SVG content should be different after zoom\"\n+            )\n \n     def click_at_coord_fractions(self, elem_or_selector, fx, fy):\n         elem = self._get_element(elem_or_selector)\n"
    },
    {
        "id": 251,
        "sha_fail": "a855c6db89e167ca01d87e79fc394e4bfd58c280",
        "diff": "diff --git a/tests/async_tests/test_async_callbacks.py b/tests/async_tests/test_async_callbacks.py\nindex 4bd3fd0..5aac4ba 100644\n--- a/tests/async_tests/test_async_callbacks.py\n+++ b/tests/async_tests/test_async_callbacks.py\n@@ -28,6 +28,9 @@ from dash.exceptions import PreventUpdate\n from tests.integration.utils import json_engine\n from tests.utils import is_dash_async\n \n+# Ensure dash[async] is installed for async support\n+# Install via: pip install dash[async]\n+\n \n @flaky.flaky(max_runs=3)\n def test_async_cbsc001_simple_callback(dash_duo):\n@@ -69,6 +72,8 @@ def test_async_cbsc001_simple_callback(dash_duo):\n \n     assert dash_duo.get_logs() == []\n \n+    # Note: Ensure 'dash[async]' is installed to avoid TimeoutExceptions\n+\n \n def test_async_cbsc002_callbacks_generating_children(dash_duo):\n     \"\"\"Modify the DOM tree by adding new components in the callbacks.\"\"\"\n@@ -539,7 +544,7 @@ def test_async_cbsc011_one_call_for_multiple_outputs_initial(dash_duo):\n \n     assert call_count.value == 1\n     inputs = [f'\"Input {i}\"' for i in range(10)]\n-    expected = f'[{\", \".join(inputs)}]'\n+    expected = f\"[{', '.join(inputs)}]\"\n     dash_duo.wait_for_text_to_equal(\"#container\", expected)\n     assert dash_duo.get_logs() == []\n \n@@ -587,7 +592,7 @@ def test_async_cbsc012_one_call_for_multiple_outputs_update(dash_duo):\n \n     ### order altered from the original, as these are non-blocking callbacks now\n     inputs = [f'\"Input {i}\"' for i in range(10)]\n-    expected = f'[{\", \".join(inputs)}]'\n+    expected = f\"[{', '.join(inputs)}]\"\n     dash_duo.wait_for_text_to_equal(\"#dynamic-output\", expected)\n     assert call_count.value == 1\n     assert dash_duo.get_logs() == []\n"
    },
    {
        "id": 253,
        "sha_fail": "d0ae94def3067fcac4b81451dc0976074198b212",
        "diff": "diff --git a/etc/tool/copilot.py b/etc/tool/copilot.py\nindex 5a3874a..81cdbb8 100644\n--- a/etc/tool/copilot.py\n+++ b/etc/tool/copilot.py\n@@ -15,10 +15,11 @@ from github.PullRequest import PullRequest\n g4f.debug.logging = True\n g4f.debug.version_check = False\n \n-GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n-GITHUB_REPOSITORY = os.getenv('GITHUB_REPOSITORY')\n-G4F_PROVIDER = os.getenv('G4F_PROVIDER')\n-G4F_MODEL = os.getenv('G4F_MODEL') or g4f.models.gpt_4\n+GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n+GITHUB_REPOSITORY = os.getenv(\"GITHUB_REPOSITORY\")\n+G4F_PROVIDER = os.getenv(\"G4F_PROVIDER\")\n+G4F_MODEL = os.getenv(\"G4F_MODEL\") or g4f.models.gpt_4\n+\n \n def get_pr_details(github: Github) -> PullRequest:\n     \"\"\"\n@@ -30,9 +31,9 @@ def get_pr_details(github: Github) -> PullRequest:\n     Returns:\n         PullRequest: An object representing the pull request.\n     \"\"\"\n-    if not os.path.isfile('./pr_number'):\n+    if not os.path.isfile(\"./pr_number\"):\n         return\n-    with open('./pr_number', 'r') as file:\n+    with open(\"./pr_number\", \"r\") as file:\n         pr_number = file.read().strip()\n     if not pr_number:\n         return\n@@ -42,6 +43,7 @@ def get_pr_details(github: Github) -> PullRequest:\n \n     return pull\n \n+\n def get_diff(diff_url: str) -> str:\n     \"\"\"\n     Fetches the diff of the pull request from a given URL.\n@@ -56,6 +58,7 @@ def get_diff(diff_url: str) -> str:\n     response.raise_for_status()\n     return response.text\n \n+\n def read_json(text: str) -> dict:\n     \"\"\"\n     Parses JSON code block from a string.\n@@ -76,6 +79,7 @@ def read_json(text: str) -> dict:\n     except json.JSONDecodeError:\n         raise RuntimeError(f\"Invalid JSON: {text}\")\n \n+\n def read_text(text: str) -> str:\n     \"\"\"\n     Extracts text from a markdown code block.\n@@ -92,6 +96,7 @@ def read_text(text: str) -> str:\n     else:\n         raise RuntimeError(f\"Invalid markdown: {text}\")\n \n+\n def get_ai_response(prompt: str, as_json: bool = True) -> Union[dict, str]:\n     \"\"\"\n     Gets a response from g4f API based on the prompt.\n@@ -105,13 +110,14 @@ def get_ai_response(prompt: str, as_json: bool = True) -> Union[dict, str]:\n     \"\"\"\n     response = g4f.ChatCompletion.create(\n         G4F_MODEL,\n-        [{'role': 'user', 'content': prompt}],\n+        [{\"role\": \"user\", \"content\": prompt}],\n         G4F_PROVIDER,\n-        ignore_stream_and_auth=True\n+        ignore_stream_and_auth=True,\n     )\n     return read_json(response) if as_json else read_text(response)\n \n-def analyze_code(pull: PullRequest, diff: str)-> list[dict]:\n+\n+def analyze_code(pull: PullRequest, diff: str) -> list[dict]:\n     \"\"\"\n     Analyzes the code changes in the pull request.\n \n@@ -127,23 +133,23 @@ def analyze_code(pull: PullRequest, diff: str)-> list[dict]:\n     current_file_path = None\n     offset_line = 0\n \n-    for line in diff.split('\\n'):\n-        if line.startswith('+++ b/'):\n+    for line in diff.split(\"\\n\"):\n+        if line.startswith(\"+++ b/\"):\n             current_file_path = line[6:]\n             changed_lines = []\n-        elif line.startswith('@@'):\n-            match = re.search(r'\\+([0-9]+?),', line)\n+        elif line.startswith(\"@@\"):\n+            match = re.search(r\"\\+([0-9]+?),\", line)\n             if match:\n                 offset_line = int(match.group(1))\n         elif current_file_path:\n-            if (line.startswith('\\\\') or line.startswith('diff')) and changed_lines:\n+            if (line.startswith(\"\\\\\") or line.startswith(\"diff\")) and changed_lines:\n                 prompt = create_analyze_prompt(changed_lines, pull, current_file_path)\n                 response = get_ai_response(prompt)\n-                for review in response.get('reviews', []):\n-                    review['path'] = current_file_path\n+                for review in response.get(\"reviews\", []):\n+                    review[\"path\"] = current_file_path\n                     comments.append(review)\n                 current_file_path = None\n-            elif line.startswith('-'):\n+            elif line.startswith(\"-\"):\n                 changed_lines.append(line)\n             else:\n                 changed_lines.append(f\"{offset_line}:{line}\")\n@@ -151,6 +157,7 @@ def analyze_code(pull: PullRequest, diff: str)-> list[dict]:\n \n     return comments\n \n+\n def create_analyze_prompt(changed_lines: list[str], pull: PullRequest, file_path: str):\n     \"\"\"\n     Creates a prompt for the g4f model.\n@@ -187,6 +194,7 @@ Each line is prefixed by its number. Code to review:\n ```\n \"\"\"\n \n+\n def create_review_prompt(pull: PullRequest, diff: str):\n     \"\"\"\n     Creates a prompt to create a review comment.\n@@ -217,12 +225,13 @@ Diff:\n ```\n \"\"\"\n \n+\n def main():\n     try:\n         github = Github(GITHUB_TOKEN)\n         pull = get_pr_details(github)\n         if not pull:\n-            print(f\"No PR number found\")\n+            print(\"No PR number found\")\n             exit()\n         diff = get_diff(pull.diff_url)\n     except Exception as e:\n@@ -251,5 +260,6 @@ def main():\n         print(f\"Error posting review: {e}\")\n         exit(1)\n \n+\n if __name__ == \"__main__\":\n     main()\ndiff --git a/g4f/Provider/qwen/sharedTokenManager.py b/g4f/Provider/qwen/sharedTokenManager.py\nindex 07026bd..7bfcc2d 100644\n--- a/g4f/Provider/qwen/sharedTokenManager.py\n+++ b/g4f/Provider/qwen/sharedTokenManager.py\n@@ -18,9 +18,7 @@ LOCK_TIMEOUT_MS = 10000\n CACHE_CHECK_INTERVAL_MS = 1000\n \n \n-def isErrorResponse(\n-    response: Union[Dict, ErrorDataDict]\n-) -> bool:\n+def isErrorResponse(response: Union[Dict, ErrorDataDict]) -> bool:\n     return \"error\" in response\n \n \n@@ -33,7 +31,9 @@ class TokenError:\n \n \n class TokenManagerError(Exception):\n-    def __init__(self, type_: str, message: str, original_error: Optional[Exception] = None):\n+    def __init__(\n+        self, type_: str, message: str, original_error: Optional[Exception] = None\n+    ):\n         super().__init__(message)\n         self.type = type_\n         self.original_error = original_error\n@@ -84,7 +84,9 @@ class SharedTokenManager(AuthFileMixin):\n \n         atexit.register(cleanup)\n \n-    async def getValidCredentials(self, qwen_client: IQwenOAuth2Client, force_refresh: bool = False):\n+    async def getValidCredentials(\n+        self, qwen_client: IQwenOAuth2Client, force_refresh: bool = False\n+    ):\n         try:\n             self.checkAndReloadIfNeeded()\n \n@@ -98,7 +100,9 @@ class SharedTokenManager(AuthFileMixin):\n             if self.refresh_promise:\n                 return await self.refresh_promise\n \n-            self.refresh_promise = asyncio.create_task(self.performTokenRefresh(qwen_client, force_refresh))\n+            self.refresh_promise = asyncio.create_task(\n+                self.performTokenRefresh(qwen_client, force_refresh)\n+            )\n             credentials = await self.refresh_promise\n             self.refresh_promise = None\n             return credentials\n@@ -133,7 +137,7 @@ class SharedTokenManager(AuthFileMixin):\n                 data = json.load(fs)\n                 credentials = self.validateCredentials(data)\n                 self.memory_cache[\"credentials\"] = credentials\n-        except Exception as e:\n+        except Exception:\n             self.memory_cache[\"credentials\"] = None\n \n     def validateCredentials(self, data):\n@@ -142,11 +146,15 @@ class SharedTokenManager(AuthFileMixin):\n         for field in [\"access_token\", \"refresh_token\", \"token_type\"]:\n             if field not in data or not isinstance(data[field], str):\n                 raise ValueError(f\"Invalid credentials: missing {field}\")\n-        if \"expiry_date\" not in data or not isinstance(data[\"expiry_date\"], (int, float)):\n+        if \"expiry_date\" not in data or not isinstance(\n+            data[\"expiry_date\"], (int, float)\n+        ):\n             raise ValueError(\"Invalid credentials: missing expiry_date\")\n         return data\n \n-    async def performTokenRefresh(self, qwen_client: IQwenOAuth2Client, force_refresh: bool):\n+    async def performTokenRefresh(\n+        self, qwen_client: IQwenOAuth2Client, force_refresh: bool\n+    ):\n         lock_path = self.getLockFilePath()\n         try:\n             qwen_client.setCredentials(self.memory_cache[\"credentials\"])\n@@ -170,14 +178,19 @@ class SharedTokenManager(AuthFileMixin):\n                 raise TokenManagerError(TokenError.REFRESH_FAILED, str(response))\n             token_data = response\n             if \"access_token\" not in token_data:\n-                raise TokenManagerError(TokenError.REFRESH_FAILED, \"No access_token returned\")\n+                raise TokenManagerError(\n+                    TokenError.REFRESH_FAILED, \"No access_token returned\"\n+                )\n \n             credentials = {\n                 \"access_token\": token_data[\"access_token\"],\n                 \"token_type\": token_data[\"token_type\"],\n-                \"refresh_token\": token_data.get(\"refresh_token\", current_credentials.get(\"refresh_token\")),\n+                \"refresh_token\": token_data.get(\n+                    \"refresh_token\", current_credentials.get(\"refresh_token\")\n+                ),\n                 \"resource_url\": token_data.get(\"resource_url\"),\n-                \"expiry_date\": int(time.time() * 1000) + token_data.get(\"expires_in\", 0) * 1000,\n+                \"expiry_date\": int(time.time() * 1000)\n+                + token_data.get(\"expires_in\", 0) * 1000,\n             }\n             self.memory_cache[\"credentials\"] = credentials\n             qwen_client.setCredentials(credentials)\n"
    },
    {
        "id": 255,
        "sha_fail": "ce27d795db2ad56eabd9c05bf7a17c9a7bea951f",
        "diff": "diff --git a/glances/exports/glances_prometheus/__init__.py b/glances/exports/glances_prometheus/__init__.py\nindex 85e2e5d..6a03571 100644\n--- a/glances/exports/glances_prometheus/__init__.py\n+++ b/glances/exports/glances_prometheus/__init__.py\n@@ -18,6 +18,7 @@ from glances.exports.export import GlancesExport\n from glances.globals import listkeys\n from glances.logger import logger\n \n+\n class Export(GlancesExport):\n     \"\"\"This class manages the Prometheus export module.\"\"\"\n \n"
    },
    {
        "id": 256,
        "sha_fail": "3ce60d176f3497dc22d1f831fa1babeca511042c",
        "diff": "diff --git a/deepface/models/face_detection/TinaFace.py b/deepface/models/face_detection/TinaFace.py\nindex 3607640..6da3ee3 100644\n--- a/deepface/models/face_detection/TinaFace.py\n+++ b/deepface/models/face_detection/TinaFace.py\n@@ -9,16 +9,13 @@ import cv2\n # project dependencies\n from deepface.models.Detector import Detector, FacialAreaRegion\n from deepface.commons.logger import Logger\n-from deepface.commons import weight_utils, folder_utils\n+from deepface.commons import weight_utils\n from deepface.models.face_detection import OpenCv as OpenCvFD\n \n logger = Logger()\n \n \n-WEIGHTS_URL = (\n-    \"https://drive.google.com/uc?\"\n-    \"id=1VkMKWPJM0oaS8eyIVZ5flcJH70Pbi-_g\"\n-)\n+WEIGHTS_URL = \"https://drive.google.com/uc?id=1VkMKWPJM0oaS8eyIVZ5flcJH70Pbi-_g\"\n WEIGHT_FILENAME = \"tinaface_r50_fpn_bn.onnx\"\n \n \n@@ -51,9 +48,12 @@ class TinaFaceClient(Detector):\n         # Try ONNX Runtime first for best compatibility with dynamic shapes\n         try:\n             import onnxruntime as ort  # type: ignore\n-            providers = (['CUDAExecutionProvider', 'CPUExecutionProvider']\n-                        if 'CUDAExecutionProvider' in ort.get_available_providers()\n-                        else ['CPUExecutionProvider'])\n+\n+            providers = (\n+                [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n+                if \"CUDAExecutionProvider\" in ort.get_available_providers()\n+                else [\"CPUExecutionProvider\"]\n+            )\n         except ImportError as exc:\n             raise ValueError(\n                 \"onnxruntime is required for TinaFace. Please install it with: \"\n@@ -79,21 +79,29 @@ class TinaFaceClient(Detector):\n             ) from err\n \n     @staticmethod\n-    def _resize_keep_ratio_pad(img: np.ndarray,\n-                               dst_h: int,\n-                               dst_w: int,\n-                               size_divisor: int,\n-                               pad_value_bgr: List[float]) -> Dict[str, Any]:\n+    def _resize_keep_ratio_pad(\n+        img: np.ndarray,\n+        dst_h: int,\n+        dst_w: int,\n+        size_divisor: int,\n+        pad_value_bgr: List[float],\n+    ) -> Dict[str, Any]:\n         h, w = img.shape[:2]\n         scale = min(dst_h / h, dst_w / w)\n         new_w, new_h = int(round(w * scale)), int(round(h * scale))\n-        resized = cv2.resize(img, (new_w, new_h)) if (new_w != w or new_h != h) else img.copy()\n+        resized = (\n+            cv2.resize(img, (new_w, new_h))\n+            if (new_w != w or new_h != h)\n+            else img.copy()\n+        )\n \n         # compute padded dims to be divisible by size_divisor\n         pad_w = int((size_divisor - (new_w % size_divisor)) % size_divisor)\n         pad_h = int((size_divisor - (new_h % size_divisor)) % size_divisor)\n \n-        padded = np.full((new_h + pad_h, new_w + pad_w, 3), pad_value_bgr, dtype=resized.dtype)\n+        padded = np.full(\n+            (new_h + pad_h, new_w + pad_w, 3), pad_value_bgr, dtype=resized.dtype\n+        )\n         padded[:new_h, :new_w, :] = resized\n \n         meta = {\n@@ -138,11 +146,9 @@ class TinaFaceClient(Detector):\n         return {\"input\": input_tensor, \"meta\": meta}\n \n     @staticmethod\n-    def _parse_out_rows(out: np.ndarray,\n-                        score_threshold: float,\n-                        orig_w: int,\n-                        orig_h: int,\n-                        scale: float) -> List[FacialAreaRegion]:\n+    def _parse_out_rows(\n+        out: np.ndarray, score_threshold: float, orig_w: int, orig_h: int, scale: float\n+    ) -> List[FacialAreaRegion]:\n         resp: List[FacialAreaRegion] = []\n         if out.ndim != 2 or out.shape[1] < 5:\n             return resp\n@@ -165,8 +171,14 @@ class TinaFaceClient(Detector):\n                     le_x, le_y = int(round(row[5] / scale)), int(round(row[6] / scale))\n                     re_x, re_y = int(round(row[7] / scale)), int(round(row[8] / scale))\n                     n_x, n_y = int(round(row[9] / scale)), int(round(row[10] / scale))\n-                    ml_x, ml_y = int(round(row[11] / scale)), int(round(row[12] / scale))\n-                    mr_x, mr_y = int(round(row[13] / scale)), int(round(row[14] / scale))\n+                    ml_x, ml_y = (\n+                        int(round(row[11] / scale)),\n+                        int(round(row[12] / scale)),\n+                    )\n+                    mr_x, mr_y = (\n+                        int(round(row[13] / scale)),\n+                        int(round(row[14] / scale)),\n+                    )\n                     left_eye = (le_x, le_y)\n                     right_eye = (re_x, re_y)\n                     nose = (n_x, n_y)\n@@ -196,7 +208,9 @@ class TinaFaceClient(Detector):\n         return 1.0 / (1.0 + np.exp(-x))\n \n     @staticmethod\n-    def _nms(boxes: np.ndarray, scores: np.ndarray, iou_thresh: float, top_k: int) -> List[int]:\n+    def _nms(\n+        boxes: np.ndarray, scores: np.ndarray, iou_thresh: float, top_k: int\n+    ) -> List[int]:\n         if boxes.size == 0:\n             return []\n         x1 = boxes[:, 0]\n@@ -214,30 +228,26 @@ class TinaFaceClient(Detector):\n             if suppressed[i_idx]:\n                 continue\n             keep.append(i)\n-            xx1 = np.maximum(x1[i], x1[order[i_idx + 1:]])\n-            yy1 = np.maximum(y1[i], y1[order[i_idx + 1:]])\n-            xx2 = np.minimum(x2[i], x2[order[i_idx + 1:]])\n-            yy2 = np.minimum(y2[i], y2[order[i_idx + 1:]])\n+            xx1 = np.maximum(x1[i], x1[order[i_idx + 1 :]])\n+            yy1 = np.maximum(y1[i], y1[order[i_idx + 1 :]])\n+            xx2 = np.minimum(x2[i], x2[order[i_idx + 1 :]])\n+            yy2 = np.minimum(y2[i], y2[order[i_idx + 1 :]])\n             w = np.maximum(0.0, xx2 - xx1 + 1)\n             h = np.maximum(0.0, yy2 - yy1 + 1)\n             inter = w * h\n-            ovr = inter / (areas[i] + areas[order[i_idx + 1:]] - inter)\n-            suppressed[i_idx + 1:][ovr >= iou_thresh] = True\n+            ovr = inter / (areas[i] + areas[order[i_idx + 1 :]] - inter)\n+            suppressed[i_idx + 1 :][ovr >= iou_thresh] = True\n         return keep\n \n     @staticmethod\n-    def _generate_level_anchors(stride: int,\n-                                height: int,\n-                                width: int,\n-                                scales_per_octave: int,\n-                                octave_base_scale: float,\n-                                ratio: float\n-                                ) -> Tuple[\n-                                    np.ndarray,\n-                                    np.ndarray,\n-                                    np.ndarray,\n-                                    np.ndarray\n-                                ]:\n+    def _generate_level_anchors(\n+        stride: int,\n+        height: int,\n+        width: int,\n+        scales_per_octave: int,\n+        octave_base_scale: float,\n+        ratio: float,\n+    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n         # Compute sizes: stride * (octave_base_scale * 2**(i/scales_per_octave))\n         scale_factors = [\n             octave_base_scale * (2 ** (i / scales_per_octave))\n@@ -266,14 +276,16 @@ class TinaFaceClient(Detector):\n         cy = centers_tiled[:, 1]\n         return cx, cy, widths, heights\n \n-    def _decode_fpn_outputs(self,\n-                            outputs_map: Dict[str, np.ndarray],\n-                            meta: Dict[str, Any],\n-                            score_thr: float,\n-                            nms_iou_thr: float,\n-                            max_per_img: int,\n-                            orig_w: int,\n-                            orig_h: int) -> List[FacialAreaRegion]:\n+    def _decode_fpn_outputs(\n+        self,\n+        outputs_map: Dict[str, np.ndarray],\n+        meta: Dict[str, Any],\n+        score_thr: float,\n+        nms_iou_thr: float,\n+        max_per_img: int,\n+        orig_w: int,\n+        orig_h: int,\n+    ) -> List[FacialAreaRegion]:\n         # config per doc\n         strides = [4, 8, 16, 32, 64, 128]\n         use_sigmoid = True\n@@ -312,7 +324,7 @@ class TinaFaceClient(Detector):\n                 continue\n             # pick the largest channel tensor as bbox, smallest as cls if ambiguous\n             bbox = max(bbox_list, key=lambda a: a.shape[0])  # (C, H, W)\n-            cls = min(cls_list, key=lambda a: a.shape[0])    # (C, H, W)\n+            cls = min(cls_list, key=lambda a: a.shape[0])  # (C, H, W)\n \n             Cc, H, W = cls.shape\n             num_anchors = scales_per_octave  # ratios=1 only\n@@ -441,14 +453,18 @@ class TinaFaceClient(Detector):\n         # Preferred 'out' parsed if available\n         if \"out\" in outputs_map:\n             out = np.squeeze(outputs_map[\"out\"])  # Nx(>=5)\n-            resp = self._parse_out_rows(out, score_threshold, width, height, meta[\"scale\"])\n+            resp = self._parse_out_rows(\n+                out, score_threshold, width, height, meta[\"scale\"]\n+            )\n             if len(resp) > 0:\n                 return self._populate_missing_eyes(img, resp)\n         # Fall back: search for any 2D array with >=5 cols\n         for _, arr in outputs_map.items():\n             out = np.squeeze(arr)\n             if isinstance(out, np.ndarray) and out.ndim == 2 and out.shape[1] >= 5:\n-                resp = self._parse_out_rows(out, score_threshold, width, height, meta[\"scale\"])\n+                resp = self._parse_out_rows(\n+                    out, score_threshold, width, height, meta[\"scale\"]\n+                )\n                 if len(resp) > 0:\n                     return self._populate_missing_eyes(img, resp)\n \n@@ -473,11 +489,11 @@ class TinaFaceClient(Detector):\n     ) -> List[FacialAreaRegion]:\n         \"\"\"\n         Populate missing eye landmarks for detected faces.\n-        \n+\n         Args:\n             img: Input image array\n             faces: List of detected faces with potentially missing eye coordinates\n-            \n+\n         Returns:\n             List of faces with eye landmarks populated (either from model output,\n             OpenCV cascade detection, or heuristic positioning)\n@@ -491,7 +507,7 @@ class TinaFaceClient(Detector):\n                 w = max(0, min(img.shape[1] - x, w))\n                 h = max(0, min(img.shape[0] - y, h))\n                 if w > 0 and h > 0:\n-                    roi = img[int(y): int(y + h), int(x): int(x + w)]\n+                    roi = img[int(y) : int(y + h), int(x) : int(x + w)]\n                     le, re = self._eye_finder.find_eyes(roi)\n                     if le is not None:\n                         fa.left_eye = (int(x + le[0]), int(y + le[1]))\n@@ -506,4 +522,3 @@ class TinaFaceClient(Detector):\n                     fa.right_eye = fa.right_eye or (re_x, eye_y)\n             updated.append(fa)\n         return updated\n-        \n\\ No newline at end of file\n"
    },
    {
        "id": 257,
        "sha_fail": "bbfcebbc60b0bc5d238af2941a175f019d5b2aa4",
        "diff": "diff --git a/faster_whisper/vad.py b/faster_whisper/vad.py\nindex 4a921d1..9e97952 100644\n--- a/faster_whisper/vad.py\n+++ b/faster_whisper/vad.py\n@@ -316,9 +316,9 @@ class SileroVADModel:\n         window_size_samples: int = 512,\n         context_size_samples: int = 64,\n     ) -> np.ndarray:\n-        assert (\n-            audio.ndim == 2\n-        ), \"Input should be a 2D array with size (batch_size, num_samples)\"\n+        assert audio.ndim == 2, (\n+            \"Input should be a 2D array with size (batch_size, num_samples)\"\n+        )\n \n         rhs_padding = window_size_samples - num_samples % window_size_samples\n         audio = np.pad(audio, ((0, 0), (context_size_samples, rhs_padding)))\n"
    },
    {
        "id": 258,
        "sha_fail": "0b806e8ab76c7c89d77ba91d827fbe0d52ec97b0",
        "diff": "diff --git a/faster_whisper/__init__.py b/faster_whisper/__init__.py\nindex 9c44381..1372498 100644\n--- a/faster_whisper/__init__.py\n+++ b/faster_whisper/__init__.py\n@@ -16,4 +16,4 @@ __all__ = [\n     \"download_model\",\n     \"format_timestamp\",\n     \"__version__\",\n-]\n\\ No newline at end of file\n+]\ndiff --git a/faster_whisper/transcribe.py b/faster_whisper/transcribe.py\nindex 8b83dc9..19ada10 100644\n--- a/faster_whisper/transcribe.py\n+++ b/faster_whisper/transcribe.py\n@@ -1123,6 +1123,7 @@ class AsyncBatchedInferencePipeline:\n             pbar.close()\n             self.last_speech_timestamp = 0.0\n \n+\n class WhisperModel:\n     def __init__(\n         self,\n@@ -2301,9 +2302,9 @@ class WhisperModel:\n             languege_probability: Probability of the detected language.\n             all_language_probs: List of tuples with all language names and probabilities.\n         \"\"\"\n-        assert (\n-            audio is not None or features is not None\n-        ), \"Either `audio` or `features` must be provided.\"\n+        assert audio is not None or features is not None, (\n+            \"Either `audio` or `features` must be provided.\"\n+        )\n \n         if audio is not None:\n             if vad_filter:\ndiff --git a/tests/test_transcribe.py b/tests/test_transcribe.py\nindex 9641a7e..8c2c845 100644\n--- a/tests/test_transcribe.py\n+++ b/tests/test_transcribe.py\n@@ -93,6 +93,7 @@ def test_batched_transcribe(physcisworks_path):\n         )\n     assert len(segments) > 7\n \n+\n @pytest.mark.asyncio\n async def test_async_batched_transcribe(physcisworks_path):\n     model = WhisperModel(\"tiny\")\n@@ -122,6 +123,7 @@ async def test_async_batched_transcribe(physcisworks_path):\n         )\n     assert len(segments) > 7\n \n+\n def test_empty_audio():\n     audio = np.asarray([], dtype=\"float32\")\n     model = WhisperModel(\"tiny\")\n"
    },
    {
        "id": 262,
        "sha_fail": "275119c05eb95555e52b5c4b25ee27baf150fbe6",
        "diff": "diff --git a/scipy/_lib/tests/test_public_api.py b/scipy/_lib/tests/test_public_api.py\nindex a8df8f8..1e2601b 100644\n--- a/scipy/_lib/tests/test_public_api.py\n+++ b/scipy/_lib/tests/test_public_api.py\n@@ -30,127 +30,127 @@ def test_dir_testing():\n # These private modules support will be removed in SciPy v2.0.0, as the\n # deprecation messages emitted by each of these modules say.\n PRIVATE_BUT_PRESENT_MODULES = [\n-    'scipy.constants.codata',\n-    'scipy.constants.constants',\n-    'scipy.fftpack.basic',\n-    'scipy.fftpack.convolve',\n-    'scipy.fftpack.helper',\n-    'scipy.fftpack.pseudo_diffs',\n-    'scipy.fftpack.realtransforms',\n-    'scipy.integrate.dop',\n-    'scipy.integrate.lsoda',\n-    'scipy.integrate.odepack',\n-    'scipy.integrate.quadpack',\n-    'scipy.integrate.vode',\n-    'scipy.interpolate.dfitpack',\n-    'scipy.interpolate.fitpack',\n-    'scipy.interpolate.fitpack2',\n-    'scipy.interpolate.interpnd',\n-    'scipy.interpolate.interpolate',\n-    'scipy.interpolate.ndgriddata',\n-    'scipy.interpolate.polyint',\n-    'scipy.interpolate.rbf',\n-    'scipy.io.arff.arffread',\n-    'scipy.io.harwell_boeing',\n-    'scipy.io.idl',\n-    'scipy.io.matlab.byteordercodes',\n-    'scipy.io.matlab.mio',\n-    'scipy.io.matlab.mio4',\n-    'scipy.io.matlab.mio5',\n-    'scipy.io.matlab.mio5_params',\n-    'scipy.io.matlab.mio5_utils',\n-    'scipy.io.matlab.mio_utils',\n-    'scipy.io.matlab.miobase',\n-    'scipy.io.matlab.streams',\n-    'scipy.io.mmio',\n-    'scipy.io.netcdf',\n-    'scipy.linalg.basic',\n-    'scipy.linalg.decomp',\n-    'scipy.linalg.decomp_cholesky',\n-    'scipy.linalg.decomp_lu',\n-    'scipy.linalg.decomp_qr',\n-    'scipy.linalg.decomp_schur',\n-    'scipy.linalg.decomp_svd',\n-    'scipy.linalg.matfuncs',\n-    'scipy.linalg.misc',\n-    'scipy.linalg.special_matrices',\n-    'scipy.misc',\n-    'scipy.misc.common',\n-    'scipy.misc.doccer',\n-    'scipy.ndimage.filters',\n-    'scipy.ndimage.fourier',\n-    'scipy.ndimage.interpolation',\n-    'scipy.ndimage.measurements',\n-    'scipy.ndimage.morphology',\n-    'scipy.odr.models',\n-    'scipy.odr.odrpack',\n-    'scipy.optimize.cobyla',\n-    'scipy.optimize.cython_optimize',\n-    'scipy.optimize.lbfgsb',\n-    'scipy.optimize.linesearch',\n-    'scipy.optimize.minpack',\n-    'scipy.optimize.minpack2',\n-    'scipy.optimize.moduleTNC',\n-    'scipy.optimize.nonlin',\n-    'scipy.optimize.optimize',\n-    'scipy.optimize.slsqp',\n-    'scipy.optimize.tnc',\n-    'scipy.optimize.zeros',\n-    'scipy.signal.bsplines',\n-    'scipy.signal.filter_design',\n-    'scipy.signal.fir_filter_design',\n-    'scipy.signal.lti_conversion',\n-    'scipy.signal.ltisys',\n-    'scipy.signal.signaltools',\n-    'scipy.signal.spectral',\n-    'scipy.signal.spline',\n-    'scipy.signal.waveforms',\n-    'scipy.signal.wavelets',\n-    'scipy.signal.windows.windows',\n-    'scipy.sparse.base',\n-    'scipy.sparse.bsr',\n-    'scipy.sparse.compressed',\n-    'scipy.sparse.construct',\n-    'scipy.sparse.coo',\n-    'scipy.sparse.csc',\n-    'scipy.sparse.csr',\n-    'scipy.sparse.data',\n-    'scipy.sparse.dia',\n-    'scipy.sparse.dok',\n-    'scipy.sparse.extract',\n-    'scipy.sparse.lil',\n-    'scipy.sparse.linalg.dsolve',\n-    'scipy.sparse.linalg.eigen',\n-    'scipy.sparse.linalg.interface',\n-    'scipy.sparse.linalg.isolve',\n-    'scipy.sparse.linalg.matfuncs',\n-    'scipy.sparse.sparsetools',\n-    'scipy.sparse.spfuncs',\n-    'scipy.sparse.sputils',\n-    'scipy.spatial.ckdtree',\n-    'scipy.spatial.kdtree',\n-    'scipy.spatial.qhull',\n-    'scipy.spatial.transform.rotation',\n-    'scipy.special.add_newdocs',\n-    'scipy.special.basic',\n-    'scipy.special.cython_special',\n-    'scipy.special.orthogonal',\n-    'scipy.special.sf_error',\n-    'scipy.special.specfun',\n-    'scipy.special.spfun_stats',\n-    'scipy.stats.biasedurn',\n-    'scipy.stats.kde',\n-    'scipy.stats.morestats',\n-    'scipy.stats.mstats_basic',\n-    'scipy.stats.mstats_extras',\n-    'scipy.stats.mvn',\n-    'scipy.stats.stats',\n+    \"scipy.constants.codata\",\n+    \"scipy.constants.constants\",\n+    \"scipy.fftpack.basic\",\n+    \"scipy.fftpack.convolve\",\n+    \"scipy.fftpack.helper\",\n+    \"scipy.fftpack.pseudo_diffs\",\n+    \"scipy.fftpack.realtransforms\",\n+    \"scipy.integrate.dop\",\n+    \"scipy.integrate.lsoda\",\n+    \"scipy.integrate.odepack\",\n+    \"scipy.integrate.quadpack\",\n+    \"scipy.integrate.vode\",\n+    \"scipy.interpolate.dfitpack\",\n+    \"scipy.interpolate.fitpack\",\n+    \"scipy.interpolate.fitpack2\",\n+    \"scipy.interpolate.interpnd\",\n+    \"scipy.interpolate.interpolate\",\n+    \"scipy.interpolate.ndgriddata\",\n+    \"scipy.interpolate.polyint\",\n+    \"scipy.interpolate.rbf\",\n+    \"scipy.io.arff.arffread\",\n+    \"scipy.io.harwell_boeing\",\n+    \"scipy.io.idl\",\n+    \"scipy.io.matlab.byteordercodes\",\n+    \"scipy.io.matlab.mio\",\n+    \"scipy.io.matlab.mio4\",\n+    \"scipy.io.matlab.mio5\",\n+    \"scipy.io.matlab.mio5_params\",\n+    \"scipy.io.matlab.mio5_utils\",\n+    \"scipy.io.matlab.mio_utils\",\n+    \"scipy.io.matlab.miobase\",\n+    \"scipy.io.matlab.streams\",\n+    \"scipy.io.mmio\",\n+    \"scipy.io.netcdf\",\n+    \"scipy.linalg.basic\",\n+    \"scipy.linalg.decomp\",\n+    \"scipy.linalg.decomp_cholesky\",\n+    \"scipy.linalg.decomp_lu\",\n+    \"scipy.linalg.decomp_qr\",\n+    \"scipy.linalg.decomp_schur\",\n+    \"scipy.linalg.decomp_svd\",\n+    \"scipy.linalg.matfuncs\",\n+    \"scipy.linalg.misc\",\n+    \"scipy.linalg.special_matrices\",\n+    \"scipy.misc\",\n+    \"scipy.misc.common\",\n+    \"scipy.misc.doccer\",\n+    \"scipy.ndimage.filters\",\n+    \"scipy.ndimage.fourier\",\n+    \"scipy.ndimage.interpolation\",\n+    \"scipy.ndimage.measurements\",\n+    \"scipy.ndimage.morphology\",\n+    \"scipy.odr.models\",\n+    \"scipy.odr.odrpack\",\n+    \"scipy.optimize.cobyla\",\n+    \"scipy.optimize.cython_optimize\",\n+    \"scipy.optimize.lbfgsb\",\n+    \"scipy.optimize.linesearch\",\n+    \"scipy.optimize.minpack\",\n+    \"scipy.optimize.minpack2\",\n+    \"scipy.optimize.moduleTNC\",\n+    \"scipy.optimize.nonlin\",\n+    \"scipy.optimize.optimize\",\n+    \"scipy.optimize.slsqp\",\n+    \"scipy.optimize.tnc\",\n+    \"scipy.optimize.zeros\",\n+    \"scipy.signal.bsplines\",\n+    \"scipy.signal.filter_design\",\n+    \"scipy.signal.fir_filter_design\",\n+    \"scipy.signal.lti_conversion\",\n+    \"scipy.signal.ltisys\",\n+    \"scipy.signal.signaltools\",\n+    \"scipy.signal.spectral\",\n+    \"scipy.signal.spline\",\n+    \"scipy.signal.waveforms\",\n+    \"scipy.signal.wavelets\",\n+    \"scipy.signal.windows.windows\",\n+    \"scipy.sparse.base\",\n+    \"scipy.sparse.bsr\",\n+    \"scipy.sparse.compressed\",\n+    \"scipy.sparse.construct\",\n+    \"scipy.sparse.coo\",\n+    \"scipy.sparse.csc\",\n+    \"scipy.sparse.csr\",\n+    \"scipy.sparse.data\",\n+    \"scipy.sparse.dia\",\n+    \"scipy.sparse.dok\",\n+    \"scipy.sparse.extract\",\n+    \"scipy.sparse.lil\",\n+    \"scipy.sparse.linalg.dsolve\",\n+    \"scipy.sparse.linalg.eigen\",\n+    \"scipy.sparse.linalg.interface\",\n+    \"scipy.sparse.linalg.isolve\",\n+    \"scipy.sparse.linalg.matfuncs\",\n+    \"scipy.sparse.sparsetools\",\n+    \"scipy.sparse.spfuncs\",\n+    \"scipy.sparse.sputils\",\n+    \"scipy.spatial.ckdtree\",\n+    \"scipy.spatial.kdtree\",\n+    \"scipy.spatial.qhull\",\n+    \"scipy.spatial.transform.rotation\",\n+    \"scipy.special.add_newdocs\",\n+    \"scipy.special.basic\",\n+    \"scipy.special.cython_special\",\n+    \"scipy.special.orthogonal\",\n+    \"scipy.special.sf_error\",\n+    \"scipy.special.specfun\",\n+    \"scipy.special.spfun_stats\",\n+    \"scipy.stats.biasedurn\",\n+    \"scipy.stats.kde\",\n+    \"scipy.stats.morestats\",\n+    \"scipy.stats.mstats_basic\",\n+    \"scipy.stats.mstats_extras\",\n+    \"scipy.stats.mvn\",\n+    \"scipy.stats.stats\",\n ]\n \n \n def is_unexpected(name):\n     \"\"\"Check if this needs to be considered.\"\"\"\n-    if '._' in name or '.tests' in name or '.setup' in name:\n+    if \"._\" in name or \".tests\" in name or \".setup\" in name:\n         return False\n \n     if name in PUBLIC_MODULES:\n@@ -162,11 +162,7 @@ def is_unexpected(name):\n     return True\n \n \n-SKIP_LIST = [\n-    'scipy.conftest',\n-    'scipy.version',\n-    'scipy.special.libsf_error_state'\n-]\n+SKIP_LIST = [\"scipy.conftest\", \"scipy.version\", \"scipy.special.libsf_error_state\"]\n \n \n # XXX: this test does more than it says on the tin - in using `pkgutil.walk_packages`,\n@@ -175,8 +171,11 @@ SKIP_LIST = [\n # For now, `ignore_errors` only ignores what is necessary, but this could be expanded -\n # for example, to all errors from private modules or git subpackages - if desired.\n @pytest.mark.thread_unsafe(\n-    reason=(\"crashes in pkgutil.walk_packages, see \"\n-            \"https://github.com/data-apis/array-api-compat/issues/343\"))\n+    reason=(\n+        \"crashes in pkgutil.walk_packages, see \"\n+        \"https://github.com/data-apis/array-api-compat/issues/343\"\n+    )\n+)\n def test_all_modules_are_expected():\n     \"\"\"\n     Test that we don't add anything that looks like a new public module by\n@@ -188,9 +187,9 @@ def test_all_modules_are_expected():\n         # with the installed NumPy version, there can be errors on importing\n         # `array_api_compat`. This should only raise if SciPy is configured with\n         # that library as an available backend.\n-        backends = {'cupy', 'torch', 'dask.array'}\n+        backends = {\"cupy\", \"torch\", \"dask.array\"}\n         for backend in backends:\n-            path = f'array_api_compat.{backend}'\n+            path = f\"array_api_compat.{backend}\"\n             if path in name and backend not in xp_available_backends:\n                 return\n         raise\n@@ -199,9 +198,9 @@ def test_all_modules_are_expected():\n \n     with warnings.catch_warnings():\n         warnings.filterwarnings(\"ignore\", \"scipy.misc\", DeprecationWarning)\n-        for _, modname, _ in pkgutil.walk_packages(path=scipy.__path__,\n-                                                   prefix=scipy.__name__ + '.',\n-                                                   onerror=ignore_errors):\n+        for _, modname, _ in pkgutil.walk_packages(\n+            path=scipy.__path__, prefix=scipy.__name__ + \".\", onerror=ignore_errors\n+        ):\n             if is_unexpected(modname) and modname not in SKIP_LIST:\n                 # We have a name that is new.  If that's on purpose, add it to\n                 # PUBLIC_MODULES.  We don't expect to have to add anything to\n@@ -209,19 +208,19 @@ def test_all_modules_are_expected():\n                 modnames.append(modname)\n \n     if modnames:\n-        raise AssertionError(f'Found unexpected modules: {modnames}')\n+        raise AssertionError(f\"Found unexpected modules: {modnames}\")\n \n \n # Stuff that clearly shouldn't be in the API and is detected by the next test\n # below\n SKIP_LIST_2 = [\n-    'scipy.char',\n-    'scipy.rec',\n-    'scipy.emath',\n-    'scipy.math',\n-    'scipy.random',\n-    'scipy.ctypeslib',\n-    'scipy.ma'\n+    \"scipy.char\",\n+    \"scipy.rec\",\n+    \"scipy.emath\",\n+    \"scipy.math\",\n+    \"scipy.random\",\n+    \"scipy.ctypeslib\",\n+    \"scipy.ma\",\n ]\n \n \n@@ -235,29 +234,31 @@ def test_all_modules_are_expected_2():\n     def find_unexpected_members(mod_name):\n         members = []\n         module = importlib.import_module(mod_name)\n-        if hasattr(module, '__all__'):\n+        if hasattr(module, \"__all__\"):\n             objnames = module.__all__\n         else:\n             objnames = dir(module)\n \n         for objname in objnames:\n-            if not objname.startswith('_'):\n-                fullobjname = mod_name + '.' + objname\n+            if not objname.startswith(\"_\"):\n+                fullobjname = mod_name + \".\" + objname\n                 if isinstance(getattr(module, objname), types.ModuleType):\n                     if is_unexpected(fullobjname) and fullobjname not in SKIP_LIST_2:\n                         members.append(fullobjname)\n \n         return members\n+\n     with warnings.catch_warnings():\n-        warnings.filterwarnings(\"ignore\",  \"scipy.misc\", DeprecationWarning)\n+        warnings.filterwarnings(\"ignore\", \"scipy.misc\", DeprecationWarning)\n         unexpected_members = find_unexpected_members(\"scipy\")\n \n     for modname in PUBLIC_MODULES:\n         unexpected_members.extend(find_unexpected_members(modname))\n \n     if unexpected_members:\n-        raise AssertionError(\"Found unexpected object(s) that look like \"\n-                             f\"modules: {unexpected_members}\")\n+        raise AssertionError(\n+            f\"Found unexpected object(s) that look like modules: {unexpected_members}\"\n+        )\n \n \n def test_api_importable():\n@@ -267,6 +268,7 @@ def test_api_importable():\n     simply need to be removed from the list (deprecation may or may not be\n     needed - apply common sense).\n     \"\"\"\n+\n     def check_importable(module_name):\n         try:\n             importlib.import_module(module_name)\n@@ -281,141 +283,148 @@ def test_api_importable():\n             module_names.append(module_name)\n \n     if module_names:\n-        raise AssertionError(\"Modules in the public API that cannot be \"\n-                             f\"imported: {module_names}\")\n+        raise AssertionError(\n+            f\"Modules in the public API that cannot be imported: {module_names}\"\n+        )\n \n     with warnings.catch_warnings(record=True):\n-        warnings.simplefilter('always', category=DeprecationWarning)\n-        warnings.simplefilter('always', category=ImportWarning)\n+        warnings.simplefilter(\"always\", category=DeprecationWarning)\n+        warnings.simplefilter(\"always\", category=ImportWarning)\n         for module_name in PRIVATE_BUT_PRESENT_MODULES:\n             if not check_importable(module_name):\n                 module_names.append(module_name)\n \n     if module_names:\n-        raise AssertionError(\"Modules that are not really public but looked \"\n-                             \"public and can not be imported: \"\n-                             f\"{module_names}\")\n-\n-\n-@pytest.mark.parametrize((\"module_name\", \"correct_module\"),\n-                         [('scipy.constants.codata', None),\n-                          ('scipy.constants.constants', None),\n-                          ('scipy.fftpack.basic', None),\n-                          ('scipy.fftpack.helper', None),\n-                          ('scipy.fftpack.pseudo_diffs', None),\n-                          ('scipy.fftpack.realtransforms', None),\n-                          ('scipy.integrate.dop', None),\n-                          ('scipy.integrate.lsoda', None),\n-                          ('scipy.integrate.odepack', None),\n-                          ('scipy.integrate.quadpack', None),\n-                          ('scipy.integrate.vode', None),\n-                          ('scipy.interpolate.dfitpack', None),\n-                          ('scipy.interpolate.fitpack', None),\n-                          ('scipy.interpolate.fitpack2', None),\n-                          ('scipy.interpolate.interpnd', None),\n-                          ('scipy.interpolate.interpolate', None),\n-                          ('scipy.interpolate.ndgriddata', None),\n-                          ('scipy.interpolate.polyint', None),\n-                          ('scipy.interpolate.rbf', None),\n-                          ('scipy.io.harwell_boeing', None),\n-                          ('scipy.io.idl', None),\n-                          ('scipy.io.mmio', None),\n-                          ('scipy.io.netcdf', None),\n-                          ('scipy.io.arff.arffread', 'arff'),\n-                          ('scipy.io.matlab.byteordercodes', 'matlab'),\n-                          ('scipy.io.matlab.mio_utils', 'matlab'),\n-                          ('scipy.io.matlab.mio', 'matlab'),\n-                          ('scipy.io.matlab.mio4', 'matlab'),\n-                          ('scipy.io.matlab.mio5_params', 'matlab'),\n-                          ('scipy.io.matlab.mio5_utils', 'matlab'),\n-                          ('scipy.io.matlab.mio5', 'matlab'),\n-                          ('scipy.io.matlab.miobase', 'matlab'),\n-                          ('scipy.io.matlab.streams', 'matlab'),\n-                          ('scipy.linalg.basic', None),\n-                          ('scipy.linalg.decomp', None),\n-                          ('scipy.linalg.decomp_cholesky', None),\n-                          ('scipy.linalg.decomp_lu', None),\n-                          ('scipy.linalg.decomp_qr', None),\n-                          ('scipy.linalg.decomp_schur', None),\n-                          ('scipy.linalg.decomp_svd', None),\n-                          ('scipy.linalg.matfuncs', None),\n-                          ('scipy.linalg.misc', None),\n-                          ('scipy.linalg.special_matrices', None),\n-                          ('scipy.ndimage.filters', None),\n-                          ('scipy.ndimage.fourier', None),\n-                          ('scipy.ndimage.interpolation', None),\n-                          ('scipy.ndimage.measurements', None),\n-                          ('scipy.ndimage.morphology', None),\n-                          ('scipy.odr.models', None),\n-                          ('scipy.odr.odrpack', None),\n-                          ('scipy.optimize.cobyla', None),\n-                          ('scipy.optimize.lbfgsb', None),\n-                          ('scipy.optimize.linesearch', None),\n-                          ('scipy.optimize.minpack', None),\n-                          ('scipy.optimize.minpack2', None),\n-                          ('scipy.optimize.moduleTNC', None),\n-                          ('scipy.optimize.nonlin', None),\n-                          ('scipy.optimize.optimize', None),\n-                          ('scipy.optimize.slsqp', None),\n-                          ('scipy.optimize.tnc', None),\n-                          ('scipy.optimize.zeros', None),\n-                          ('scipy.signal.bsplines', None),\n-                          ('scipy.signal.filter_design', None),\n-                          ('scipy.signal.fir_filter_design', None),\n-                          ('scipy.signal.lti_conversion', None),\n-                          ('scipy.signal.ltisys', None),\n-                          ('scipy.signal.signaltools', None),\n-                          ('scipy.signal.spectral', None),\n-                          ('scipy.signal.spline', None),\n-                          ('scipy.signal.waveforms', None),\n-                          ('scipy.signal.wavelets', None),\n-                          ('scipy.signal.windows.windows', 'windows'),\n-                          ('scipy.sparse.base', None),\n-                          ('scipy.sparse.bsr', None),\n-                          ('scipy.sparse.compressed', None),\n-                          ('scipy.sparse.construct', None),\n-                          ('scipy.sparse.coo', None),\n-                          ('scipy.sparse.csc', None),\n-                          ('scipy.sparse.csr', None),\n-                          ('scipy.sparse.data', None),\n-                          ('scipy.sparse.dia', None),\n-                          ('scipy.sparse.dok', None),\n-                          ('scipy.sparse.extract', None),\n-                          ('scipy.sparse.lil', None),\n-                          ('scipy.sparse.linalg.dsolve', 'linalg'),\n-                          ('scipy.sparse.linalg.eigen', 'linalg'),\n-                          ('scipy.sparse.linalg.interface', 'linalg'),\n-                          ('scipy.sparse.linalg.isolve', 'linalg'),\n-                          ('scipy.sparse.linalg.matfuncs', 'linalg'),\n-                          ('scipy.sparse.sparsetools', None),\n-                          ('scipy.sparse.spfuncs', None),\n-                          ('scipy.sparse.sputils', None),\n-                          ('scipy.spatial.ckdtree', None),\n-                          ('scipy.spatial.kdtree', None),\n-                          ('scipy.spatial.qhull', None),\n-                          ('scipy.spatial.transform.rotation', 'transform'),\n-                          ('scipy.special.add_newdocs', None),\n-                          ('scipy.special.basic', None),\n-                          ('scipy.special.orthogonal', None),\n-                          ('scipy.special.sf_error', None),\n-                          ('scipy.special.specfun', None),\n-                          ('scipy.special.spfun_stats', None),\n-                          ('scipy.stats.biasedurn', None),\n-                          ('scipy.stats.kde', None),\n-                          ('scipy.stats.morestats', None),\n-                          ('scipy.stats.mstats_basic', 'mstats'),\n-                          ('scipy.stats.mstats_extras', 'mstats'),\n-                          ('scipy.stats.mvn', None),\n-                          ('scipy.stats.stats', None)])\n+        raise AssertionError(\n+            \"Modules that are not really public but looked \"\n+            \"public and can not be imported: \"\n+            f\"{module_names}\"\n+        )\n+\n+\n+@pytest.mark.parametrize(\n+    (\"module_name\", \"correct_module\"),\n+    [\n+        (\"scipy.constants.codata\", None),\n+        (\"scipy.constants.constants\", None),\n+        (\"scipy.fftpack.basic\", None),\n+        (\"scipy.fftpack.helper\", None),\n+        (\"scipy.fftpack.pseudo_diffs\", None),\n+        (\"scipy.fftpack.realtransforms\", None),\n+        (\"scipy.integrate.dop\", None),\n+        (\"scipy.integrate.lsoda\", None),\n+        (\"scipy.integrate.odepack\", None),\n+        (\"scipy.integrate.quadpack\", None),\n+        (\"scipy.integrate.vode\", None),\n+        (\"scipy.interpolate.dfitpack\", None),\n+        (\"scipy.interpolate.fitpack\", None),\n+        (\"scipy.interpolate.fitpack2\", None),\n+        (\"scipy.interpolate.interpnd\", None),\n+        (\"scipy.interpolate.interpolate\", None),\n+        (\"scipy.interpolate.ndgriddata\", None),\n+        (\"scipy.interpolate.polyint\", None),\n+        (\"scipy.interpolate.rbf\", None),\n+        (\"scipy.io.harwell_boeing\", None),\n+        (\"scipy.io.idl\", None),\n+        (\"scipy.io.mmio\", None),\n+        (\"scipy.io.netcdf\", None),\n+        (\"scipy.io.arff.arffread\", \"arff\"),\n+        (\"scipy.io.matlab.byteordercodes\", \"matlab\"),\n+        (\"scipy.io.matlab.mio_utils\", \"matlab\"),\n+        (\"scipy.io.matlab.mio\", \"matlab\"),\n+        (\"scipy.io.matlab.mio4\", \"matlab\"),\n+        (\"scipy.io.matlab.mio5_params\", \"matlab\"),\n+        (\"scipy.io.matlab.mio5_utils\", \"matlab\"),\n+        (\"scipy.io.matlab.mio5\", \"matlab\"),\n+        (\"scipy.io.matlab.miobase\", \"matlab\"),\n+        (\"scipy.io.matlab.streams\", \"matlab\"),\n+        (\"scipy.linalg.basic\", None),\n+        (\"scipy.linalg.decomp\", None),\n+        (\"scipy.linalg.decomp_cholesky\", None),\n+        (\"scipy.linalg.decomp_lu\", None),\n+        (\"scipy.linalg.decomp_qr\", None),\n+        (\"scipy.linalg.decomp_schur\", None),\n+        (\"scipy.linalg.decomp_svd\", None),\n+        (\"scipy.linalg.matfuncs\", None),\n+        (\"scipy.linalg.misc\", None),\n+        (\"scipy.linalg.special_matrices\", None),\n+        (\"scipy.ndimage.filters\", None),\n+        (\"scipy.ndimage.fourier\", None),\n+        (\"scipy.ndimage.interpolation\", None),\n+        (\"scipy.ndimage.measurements\", None),\n+        (\"scipy.ndimage.morphology\", None),\n+        (\"scipy.odr.models\", None),\n+        (\"scipy.odr.odrpack\", None),\n+        (\"scipy.optimize.cobyla\", None),\n+        (\"scipy.optimize.lbfgsb\", None),\n+        (\"scipy.optimize.linesearch\", None),\n+        (\"scipy.optimize.minpack\", None),\n+        (\"scipy.optimize.minpack2\", None),\n+        (\"scipy.optimize.moduleTNC\", None),\n+        (\"scipy.optimize.nonlin\", None),\n+        (\"scipy.optimize.optimize\", None),\n+        (\"scipy.optimize.slsqp\", None),\n+        (\"scipy.optimize.tnc\", None),\n+        (\"scipy.optimize.zeros\", None),\n+        (\"scipy.signal.bsplines\", None),\n+        (\"scipy.signal.filter_design\", None),\n+        (\"scipy.signal.fir_filter_design\", None),\n+        (\"scipy.signal.lti_conversion\", None),\n+        (\"scipy.signal.ltisys\", None),\n+        (\"scipy.signal.signaltools\", None),\n+        (\"scipy.signal.spectral\", None),\n+        (\"scipy.signal.spline\", None),\n+        (\"scipy.signal.waveforms\", None),\n+        (\"scipy.signal.wavelets\", None),\n+        (\"scipy.signal.windows.windows\", \"windows\"),\n+        (\"scipy.sparse.base\", None),\n+        (\"scipy.sparse.bsr\", None),\n+        (\"scipy.sparse.compressed\", None),\n+        (\"scipy.sparse.construct\", None),\n+        (\"scipy.sparse.coo\", None),\n+        (\"scipy.sparse.csc\", None),\n+        (\"scipy.sparse.csr\", None),\n+        (\"scipy.sparse.data\", None),\n+        (\"scipy.sparse.dia\", None),\n+        (\"scipy.sparse.dok\", None),\n+        (\"scipy.sparse.extract\", None),\n+        (\"scipy.sparse.lil\", None),\n+        (\"scipy.sparse.linalg.dsolve\", \"linalg\"),\n+        (\"scipy.sparse.linalg.eigen\", \"linalg\"),\n+        (\"scipy.sparse.linalg.interface\", \"linalg\"),\n+        (\"scipy.sparse.linalg.isolve\", \"linalg\"),\n+        (\"scipy.sparse.linalg.matfuncs\", \"linalg\"),\n+        (\"scipy.sparse.sparsetools\", None),\n+        (\"scipy.sparse.spfuncs\", None),\n+        (\"scipy.sparse.sputils\", None),\n+        (\"scipy.spatial.ckdtree\", None),\n+        (\"scipy.spatial.kdtree\", None),\n+        (\"scipy.spatial.qhull\", None),\n+        (\"scipy.spatial.transform.rotation\", \"transform\"),\n+        (\"scipy.special.add_newdocs\", None),\n+        (\"scipy.special.basic\", None),\n+        (\"scipy.special.orthogonal\", None),\n+        (\"scipy.special.sf_error\", None),\n+        (\"scipy.special.specfun\", None),\n+        (\"scipy.special.spfun_stats\", None),\n+        (\"scipy.stats.biasedurn\", None),\n+        (\"scipy.stats.kde\", None),\n+        (\"scipy.stats.morestats\", None),\n+        (\"scipy.stats.mstats_basic\", \"mstats\"),\n+        (\"scipy.stats.mstats_extras\", \"mstats\"),\n+        (\"scipy.stats.mvn\", None),\n+        (\"scipy.stats.stats\", None),\n+    ],\n+)\n def test_private_but_present_deprecation(module_name, correct_module):\n     # gh-18279, gh-17572, gh-17771 noted that deprecation warnings\n     # for imports from private modules\n     # were misleading. Check that this is resolved.\n     module = import_module(module_name)\n     if correct_module is None:\n-        import_name = f'scipy.{module_name.split(\".\")[1]}'\n+        import_name = f\"scipy.{module_name.split('.')[1]}\"\n     else:\n-        import_name = f'scipy.{module_name.split(\".\")[1]}.{correct_module}'\n+        import_name = f\"scipy.{module_name.split('.')[1]}.{correct_module}\"\n \n     correct_import = import_module(import_name)\n \ndiff --git a/scipy/linalg/decomp_svd.py b/scipy/linalg/decomp_svd.py\nindex 64d0ce8..fb765ec 100644\n--- a/scipy/linalg/decomp_svd.py\n+++ b/scipy/linalg/decomp_svd.py\n@@ -6,8 +6,14 @@ from scipy._lib.deprecation import _sub_module_deprecation\n \n \n __all__ = [  # noqa: F822\n-    'svd', 'svdvals', 'diagsvd', 'orth', 'subspace_angles', 'null_space',\n-    'LinAlgError', 'get_lapack_funcs'\n+    \"svd\",\n+    \"svdvals\",\n+    \"diagsvd\",\n+    \"orth\",\n+    \"subspace_angles\",\n+    \"null_space\",\n+    \"LinAlgError\",\n+    \"get_lapack_funcs\",\n ]\n \n \n@@ -16,6 +22,10 @@ def __dir__():\n \n \n def __getattr__(name):\n-    return _sub_module_deprecation(sub_package=\"linalg\", module=\"decomp_svd\",\n-                                   private_modules=[\"_decomp_svd\"], all=__all__,\n-                                   attribute=name)\n+    return _sub_module_deprecation(\n+        sub_package=\"linalg\",\n+        module=\"decomp_svd\",\n+        private_modules=[\"_decomp_svd\"],\n+        all=__all__,\n+        attribute=name,\n+    )\ndiff --git a/scipy/linalg/tests/test_solve_toeplitz.py b/scipy/linalg/tests/test_solve_toeplitz.py\nindex 8ed1bec..5bdf947 100644\n--- a/scipy/linalg/tests/test_solve_toeplitz.py\n+++ b/scipy/linalg/tests/test_solve_toeplitz.py\n@@ -1,5 +1,5 @@\n-\"\"\"Test functions for linalg._solve_toeplitz module\n-\"\"\"\n+\"\"\"Test functions for linalg._solve_toeplitz module\"\"\"\n+\n import numpy as np\n from scipy.linalg._solve_toeplitz import levinson\n from scipy.linalg import solve, toeplitz, solve_toeplitz\n@@ -24,7 +24,7 @@ def test_solve_equivalence():\n             y = y + 1j * random.randn(n)\n \n         # Check equivalence when both the column and row are provided.\n-        actual = solve_toeplitz((c,r), y)\n+        actual = solve_toeplitz((c, r), y)\n         desired = solve(toeplitz(c, r=r), y)\n         assert_allclose(actual, desired)\n \n@@ -41,7 +41,7 @@ def test_multiple_rhs():\n     for offset in [0, 1j]:\n         for yshape in ((4,), (4, 3)):\n             y = random.randn(*yshape) + offset\n-            actual = solve_toeplitz((c,r), b=y)\n+            actual = solve_toeplitz((c, r), b=y)\n             desired = solve(toeplitz(c, r=r), y)\n             assert_equal(actual.shape, yshape)\n             assert_equal(desired.shape, yshape)\n@@ -49,10 +49,10 @@ def test_multiple_rhs():\n \n \n def test_native_list_arguments():\n-    c = [1,2,4,7]\n-    r = [1,3,9,12]\n-    y = [5,1,4,2]\n-    actual = solve_toeplitz((c,r), y)\n+    c = [1, 2, 4, 7]\n+    r = [1, 3, 9, 12]\n+    y = [5, 1, 4, 2]\n+    actual = solve_toeplitz((c, r), y)\n     desired = solve(toeplitz(c, r=r), y)\n     assert_allclose(actual, desired)\n \n@@ -65,8 +65,7 @@ def test_zero_diag_error():\n     r = random.randn(n)\n     y = random.randn(n)\n     c[0] = 0\n-    assert_raises(np.linalg.LinAlgError,\n-        solve_toeplitz, (c, r), b=y)\n+    assert_raises(np.linalg.LinAlgError, solve_toeplitz, (c, r), b=y)\n \n \n def test_wikipedia_counterexample():\n@@ -88,8 +87,8 @@ def test_reflection_coeffs():\n     reflection_coeffs_d = [1]\n     reflection_coeffs_z = [1]\n     for i in range(2, 10):\n-        reflection_coeffs_d.append(solve_toeplitz(y_d[:(i-1)], b=y_d[1:i])[-1])\n-        reflection_coeffs_z.append(solve_toeplitz(y_z[:(i-1)], b=y_z[1:i])[-1])\n+        reflection_coeffs_d.append(solve_toeplitz(y_d[: (i - 1)], b=y_d[1:i])[-1])\n+        reflection_coeffs_z.append(solve_toeplitz(y_z[: (i - 1)], b=y_z[1:i])[-1])\n \n     y_d_concat = np.concatenate((y_d[-2:0:-1], y_d[:-1]))\n     y_z_concat = np.concatenate((y_z[-2:0:-1].conj(), y_z[:-1]))\n@@ -100,7 +99,7 @@ def test_reflection_coeffs():\n     assert_allclose(reflection_coeffs_z, ref_z[:-1])\n \n \n-@pytest.mark.xfail(reason='Instability of Levinson iteration')\n+@pytest.mark.xfail(reason=\"Instability of Levinson iteration\")\n def test_unstable():\n     # this is a \"Gaussian Toeplitz matrix\", as mentioned in Example 2 of\n     # I. Gohbert, T. Kailath and V. Olshevsky \"Fast Gaussian Elimination with\n@@ -111,7 +110,7 @@ def test_unstable():\n     # other fast toeplitz solvers such as GKO or Burg should be better.\n     random = np.random.RandomState(1234)\n     n = 100\n-    c = 0.9 ** (np.arange(n)**2)\n+    c = 0.9 ** (np.arange(n) ** 2)\n     y = random.randn(n)\n \n     solution1 = solve_toeplitz(c, b=y)\n@@ -120,18 +119,19 @@ def test_unstable():\n     assert_allclose(solution1, solution2)\n \n \n-@pytest.mark.parametrize('dt_c', [int, float, np.float32, complex, np.complex64])\n-@pytest.mark.parametrize('dt_b', [int, float, np.float32, complex, np.complex64])\n+@pytest.mark.parametrize(\"dt_c\", [int, float, np.float32, complex, np.complex64])\n+@pytest.mark.parametrize(\"dt_b\", [int, float, np.float32, complex, np.complex64])\n def test_empty(dt_c, dt_b):\n     c = np.array([], dtype=dt_c)\n     b = np.array([], dtype=dt_b)\n     x = solve_toeplitz(c, b)\n     assert x.shape == (0,)\n-    assert x.dtype == solve_toeplitz(np.array([2, 1], dtype=dt_c),\n-                                      np.ones(2, dtype=dt_b)).dtype\n+    assert (\n+        x.dtype\n+        == solve_toeplitz(np.array([2, 1], dtype=dt_c), np.ones(2, dtype=dt_b)).dtype\n+    )\n \n     b = np.empty((0, 0), dtype=dt_b)\n     x1 = solve_toeplitz(c, b)\n     assert x1.shape == (0, 0)\n     assert x1.dtype == x.dtype\n-\ndiff --git a/scipy/linalg/tests/test_solvers.py b/scipy/linalg/tests/test_solvers.py\nindex 1c16930..e0776b5 100644\n--- a/scipy/linalg/tests/test_solvers.py\n+++ b/scipy/linalg/tests/test_solvers.py\n@@ -22,76 +22,211 @@ def _load_data(name):\n     Load npz data file under data/\n     Returns a copy of the data, rather than keeping the npz file open.\n     \"\"\"\n-    filename = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n-                            'data', name)\n+    filename = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"data\", name)\n     with np.load(filename) as f:\n         return dict(f.items())\n \n \n class TestSolveLyapunov:\n-\n     cases = [\n         # empty case\n-        (np.empty((0, 0)),\n-         np.empty((0, 0))),\n-        (np.array([[1, 2], [3, 4]]),\n-         np.array([[9, 10], [11, 12]])),\n+        (np.empty((0, 0)), np.empty((0, 0))),\n+        (np.array([[1, 2], [3, 4]]), np.array([[9, 10], [11, 12]])),\n         # a, q all complex.\n-        (np.array([[1.0+1j, 2.0], [3.0-4.0j, 5.0]]),\n-         np.array([[2.0-2j, 2.0+2j], [-1.0-1j, 2.0]])),\n+        (\n+            np.array([[1.0 + 1j, 2.0], [3.0 - 4.0j, 5.0]]),\n+            np.array([[2.0 - 2j, 2.0 + 2j], [-1.0 - 1j, 2.0]]),\n+        ),\n         # a real; q complex.\n-        (np.array([[1.0, 2.0], [3.0, 5.0]]),\n-         np.array([[2.0-2j, 2.0+2j], [-1.0-1j, 2.0]])),\n+        (\n+            np.array([[1.0, 2.0], [3.0, 5.0]]),\n+            np.array([[2.0 - 2j, 2.0 + 2j], [-1.0 - 1j, 2.0]]),\n+        ),\n         # a complex; q real.\n-        (np.array([[1.0+1j, 2.0], [3.0-4.0j, 5.0]]),\n-         np.array([[2.0, 2.0], [-1.0, 2.0]])),\n+        (\n+            np.array([[1.0 + 1j, 2.0], [3.0 - 4.0j, 5.0]]),\n+            np.array([[2.0, 2.0], [-1.0, 2.0]]),\n+        ),\n         # An example from Kitagawa, 1977\n-        (np.array([[3, 9, 5, 1, 4], [1, 2, 3, 8, 4], [4, 6, 6, 6, 3],\n-                   [1, 5, 2, 0, 7], [5, 3, 3, 1, 5]]),\n-         np.array([[2, 4, 1, 0, 1], [4, 1, 0, 2, 0], [1, 0, 3, 0, 3],\n-                   [0, 2, 0, 1, 0], [1, 0, 3, 0, 4]])),\n+        (\n+            np.array(\n+                [\n+                    [3, 9, 5, 1, 4],\n+                    [1, 2, 3, 8, 4],\n+                    [4, 6, 6, 6, 3],\n+                    [1, 5, 2, 0, 7],\n+                    [5, 3, 3, 1, 5],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [2, 4, 1, 0, 1],\n+                    [4, 1, 0, 2, 0],\n+                    [1, 0, 3, 0, 3],\n+                    [0, 2, 0, 1, 0],\n+                    [1, 0, 3, 0, 4],\n+                ]\n+            ),\n+        ),\n         # Companion matrix example. a complex; q real; a.shape[0] = 11\n-        (np.array([[0.100+0.j, 0.091+0.j, 0.082+0.j, 0.073+0.j, 0.064+0.j,\n-                    0.055+0.j, 0.046+0.j, 0.037+0.j, 0.028+0.j, 0.019+0.j,\n-                    0.010+0.j],\n-                   [1.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 1.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 1.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 1.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 1.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    1.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 1.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 1.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 1.000+0.j, 0.000+0.j,\n-                    0.000+0.j],\n-                   [0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j,\n-                    0.000+0.j, 0.000+0.j, 0.000+0.j, 0.000+0.j, 1.000+0.j,\n-                    0.000+0.j]]),\n-         np.eye(11)),\n+        (\n+            np.array(\n+                [\n+                    [\n+                        0.100 + 0.0j,\n+                        0.091 + 0.0j,\n+                        0.082 + 0.0j,\n+                        0.073 + 0.0j,\n+                        0.064 + 0.0j,\n+                        0.055 + 0.0j,\n+                        0.046 + 0.0j,\n+                        0.037 + 0.0j,\n+                        0.028 + 0.0j,\n+                        0.019 + 0.0j,\n+                        0.010 + 0.0j,\n+                    ],\n+                    [\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                    [\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                        1.000 + 0.0j,\n+                        0.000 + 0.0j,\n+                    ],\n+                ]\n+            ),\n+            np.eye(11),\n+        ),\n         # https://github.com/scipy/scipy/issues/4176\n-        (matrix([[0, 1], [-1/2, -1]]),\n-         (matrix([0, 3]).T @ matrix([0, 3]).T.T)),\n+        (matrix([[0, 1], [-1 / 2, -1]]), (matrix([0, 3]).T @ matrix([0, 3]).T.T)),\n         # https://github.com/scipy/scipy/issues/4176\n-        (matrix([[0, 1], [-1/2, -1]]),\n-         (np.array(matrix([0, 3]).T @ matrix([0, 3]).T.T))),\n-        ]\n+        (\n+            matrix([[0, 1], [-1 / 2, -1]]),\n+            (np.array(matrix([0, 3]).T @ matrix([0, 3]).T.T)),\n+        ),\n+    ]\n \n     def test_continuous_squareness_and_shape(self):\n         nsq = np.ones((3, 2))\n@@ -102,21 +237,21 @@ class TestSolveLyapunov:\n \n     def check_continuous_case(self, a, q):\n         x = solve_continuous_lyapunov(a, q)\n-        assert_array_almost_equal(\n-                          np.dot(a, x) + np.dot(x, a.conj().transpose()), q)\n+        assert_array_almost_equal(np.dot(a, x) + np.dot(x, a.conj().transpose()), q)\n \n     def check_discrete_case(self, a, q, method=None):\n         x = solve_discrete_lyapunov(a, q, method=method)\n         assert_array_almost_equal(\n-                      np.dot(np.dot(a, x), a.conj().transpose()) - x, -1.0*q)\n+            np.dot(np.dot(a, x), a.conj().transpose()) - x, -1.0 * q\n+        )\n \n     @skip_xp_invalid_arg\n     def test_cases(self):\n         for case in self.cases:\n             self.check_continuous_case(case[0], case[1])\n             self.check_discrete_case(case[0], case[1])\n-            self.check_discrete_case(case[0], case[1], method='direct')\n-            self.check_discrete_case(case[0], case[1], method='bilinear')\n+            self.check_discrete_case(case[0], case[1], method=\"direct\")\n+            self.check_discrete_case(case[0], case[1], method=\"bilinear\")\n \n     @pytest.mark.parametrize(\"dtype_a\", dtypes)\n     @pytest.mark.parametrize(\"dtype_q\", dtypes)\n@@ -127,8 +262,8 @@ class TestSolveLyapunov:\n         q = np.zeros((0, 0), dtype=dtype_q)\n         res = solve_continuous_lyapunov(a, q)\n \n-        a = (rng.random((5, 5))*100).astype(dtype_a)\n-        q = (rng.random((5, 5))*100).astype(dtype_q)\n+        a = (rng.random((5, 5)) * 100).astype(dtype_a)\n+        q = (rng.random((5, 5)) * 100).astype(dtype_q)\n         ref = solve_continuous_lyapunov(a, q)\n \n         assert res.shape == (0, 0)\n@@ -136,11 +271,11 @@ class TestSolveLyapunov:\n \n \n class TestSolveContinuousAre:\n-    mat6 = _load_data('carex_6_data.npz')\n-    mat15 = _load_data('carex_15_data.npz')\n-    mat18 = _load_data('carex_18_data.npz')\n-    mat19 = _load_data('carex_19_data.npz')\n-    mat20 = _load_data('carex_20_data.npz')\n+    mat6 = _load_data(\"carex_6_data.npz\")\n+    mat15 = _load_data(\"carex_15_data.npz\")\n+    mat18 = _load_data(\"carex_18_data.npz\")\n+    mat19 = _load_data(\"carex_19_data.npz\")\n+    mat20 = _load_data(\"carex_20_data.npz\")\n     cases = [\n         # Carex examples taken from (with default parameters):\n         # [1] P.BENNER, A.J. LAUB, V. MEHRMANN: 'A Collection of Benchmark\n@@ -153,158 +288,212 @@ class TestSolveContinuousAre:\n         # indicating the reason for failure.\n         #\n         # Test Case 0: carex #1\n-        (np.diag([1.], 1),\n-         np.array([[0], [1]]),\n-         block_diag(1., 2.),\n-         1,\n-         None),\n+        (np.diag([1.0], 1), np.array([[0], [1]]), block_diag(1.0, 2.0), 1, None),\n         # Test Case 1: carex #2\n-        (np.array([[4, 3], [-4.5, -3.5]]),\n-         np.array([[1], [-1]]),\n-         np.array([[9, 6], [6, 4.]]),\n-         1,\n-         None),\n+        (\n+            np.array([[4, 3], [-4.5, -3.5]]),\n+            np.array([[1], [-1]]),\n+            np.array([[9, 6], [6, 4.0]]),\n+            1,\n+            None,\n+        ),\n         # Test Case 2: carex #3\n-        (np.array([[0, 1, 0, 0],\n-                   [0, -1.89, 0.39, -5.53],\n-                   [0, -0.034, -2.98, 2.43],\n-                   [0.034, -0.0011, -0.99, -0.21]]),\n-         np.array([[0, 0], [0.36, -1.6], [-0.95, -0.032], [0.03, 0]]),\n-         np.array([[2.313, 2.727, 0.688, 0.023],\n-                   [2.727, 4.271, 1.148, 0.323],\n-                   [0.688, 1.148, 0.313, 0.102],\n-                   [0.023, 0.323, 0.102, 0.083]]),\n-         np.eye(2),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [0, 1, 0, 0],\n+                    [0, -1.89, 0.39, -5.53],\n+                    [0, -0.034, -2.98, 2.43],\n+                    [0.034, -0.0011, -0.99, -0.21],\n+                ]\n+            ),\n+            np.array([[0, 0], [0.36, -1.6], [-0.95, -0.032], [0.03, 0]]),\n+            np.array(\n+                [\n+                    [2.313, 2.727, 0.688, 0.023],\n+                    [2.727, 4.271, 1.148, 0.323],\n+                    [0.688, 1.148, 0.313, 0.102],\n+                    [0.023, 0.323, 0.102, 0.083],\n+                ]\n+            ),\n+            np.eye(2),\n+            None,\n+        ),\n         # Test Case 3: carex #4\n-        (np.array([[-0.991, 0.529, 0, 0, 0, 0, 0, 0],\n-                   [0.522, -1.051, 0.596, 0, 0, 0, 0, 0],\n-                   [0, 0.522, -1.118, 0.596, 0, 0, 0, 0],\n-                   [0, 0, 0.522, -1.548, 0.718, 0, 0, 0],\n-                   [0, 0, 0, 0.922, -1.64, 0.799, 0, 0],\n-                   [0, 0, 0, 0, 0.922, -1.721, 0.901, 0],\n-                   [0, 0, 0, 0, 0, 0.922, -1.823, 1.021],\n-                   [0, 0, 0, 0, 0, 0, 0.922, -1.943]]),\n-         np.array([[3.84, 4.00, 37.60, 3.08, 2.36, 2.88, 3.08, 3.00],\n-                   [-2.88, -3.04, -2.80, -2.32, -3.32, -3.82, -4.12, -3.96]]\n-                  ).T * 0.001,\n-         np.array([[1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.1],\n-                   [0.0, 1.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0],\n-                   [0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0],\n-                   [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n-                   [0.5, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0],\n-                   [0.0, 0.0, 0.5, 0.0, 0.0, 0.1, 0.0, 0.0],\n-                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0],\n-                   [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1]]),\n-         np.eye(2),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [-0.991, 0.529, 0, 0, 0, 0, 0, 0],\n+                    [0.522, -1.051, 0.596, 0, 0, 0, 0, 0],\n+                    [0, 0.522, -1.118, 0.596, 0, 0, 0, 0],\n+                    [0, 0, 0.522, -1.548, 0.718, 0, 0, 0],\n+                    [0, 0, 0, 0.922, -1.64, 0.799, 0, 0],\n+                    [0, 0, 0, 0, 0.922, -1.721, 0.901, 0],\n+                    [0, 0, 0, 0, 0, 0.922, -1.823, 1.021],\n+                    [0, 0, 0, 0, 0, 0, 0.922, -1.943],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [3.84, 4.00, 37.60, 3.08, 2.36, 2.88, 3.08, 3.00],\n+                    [-2.88, -3.04, -2.80, -2.32, -3.32, -3.82, -4.12, -3.96],\n+                ]\n+            ).T\n+            * 0.001,\n+            np.array(\n+                [\n+                    [1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.1],\n+                    [0.0, 1.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0],\n+                    [0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0],\n+                    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n+                    [0.5, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0],\n+                    [0.0, 0.0, 0.5, 0.0, 0.0, 0.1, 0.0, 0.0],\n+                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0],\n+                    [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1],\n+                ]\n+            ),\n+            np.eye(2),\n+            None,\n+        ),\n         # Test Case 4: carex #5\n-        (np.array(\n-          [[-4.019, 5.120, 0., 0., -2.082, 0., 0., 0., 0.870],\n-           [-0.346, 0.986, 0., 0., -2.340, 0., 0., 0., 0.970],\n-           [-7.909, 15.407, -4.069, 0., -6.450, 0., 0., 0., 2.680],\n-           [-21.816, 35.606, -0.339, -3.870, -17.800, 0., 0., 0., 7.390],\n-           [-60.196, 98.188, -7.907, 0.340, -53.008, 0., 0., 0., 20.400],\n-           [0, 0, 0, 0, 94.000, -147.200, 0., 53.200, 0.],\n-           [0, 0, 0, 0, 0, 94.000, -147.200, 0, 0],\n-           [0, 0, 0, 0, 0, 12.800, 0.000, -31.600, 0],\n-           [0, 0, 0, 0, 12.800, 0.000, 0.000, 18.800, -31.600]]),\n-         np.array([[0.010, -0.011, -0.151],\n-                   [0.003, -0.021, 0.000],\n-                   [0.009, -0.059, 0.000],\n-                   [0.024, -0.162, 0.000],\n-                   [0.068, -0.445, 0.000],\n-                   [0.000, 0.000, 0.000],\n-                   [0.000, 0.000, 0.000],\n-                   [0.000, 0.000, 0.000],\n-                   [0.000, 0.000, 0.000]]),\n-         np.eye(9),\n-         np.eye(3),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [-4.019, 5.120, 0.0, 0.0, -2.082, 0.0, 0.0, 0.0, 0.870],\n+                    [-0.346, 0.986, 0.0, 0.0, -2.340, 0.0, 0.0, 0.0, 0.970],\n+                    [-7.909, 15.407, -4.069, 0.0, -6.450, 0.0, 0.0, 0.0, 2.680],\n+                    [-21.816, 35.606, -0.339, -3.870, -17.800, 0.0, 0.0, 0.0, 7.390],\n+                    [-60.196, 98.188, -7.907, 0.340, -53.008, 0.0, 0.0, 0.0, 20.400],\n+                    [0, 0, 0, 0, 94.000, -147.200, 0.0, 53.200, 0.0],\n+                    [0, 0, 0, 0, 0, 94.000, -147.200, 0, 0],\n+                    [0, 0, 0, 0, 0, 12.800, 0.000, -31.600, 0],\n+                    [0, 0, 0, 0, 12.800, 0.000, 0.000, 18.800, -31.600],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [0.010, -0.011, -0.151],\n+                    [0.003, -0.021, 0.000],\n+                    [0.009, -0.059, 0.000],\n+                    [0.024, -0.162, 0.000],\n+                    [0.068, -0.445, 0.000],\n+                    [0.000, 0.000, 0.000],\n+                    [0.000, 0.000, 0.000],\n+                    [0.000, 0.000, 0.000],\n+                    [0.000, 0.000, 0.000],\n+                ]\n+            ),\n+            np.eye(9),\n+            np.eye(3),\n+            None,\n+        ),\n         # Test Case 5: carex #6\n-        (mat6['A'], mat6['B'], mat6['Q'], mat6['R'], None),\n+        (mat6[\"A\"], mat6[\"B\"], mat6[\"Q\"], mat6[\"R\"], None),\n         # Test Case 6: carex #7\n-        (np.array([[1, 0], [0, -2.]]),\n-         np.array([[1e-6], [0]]),\n-         np.ones((2, 2)),\n-         1.,\n-         'Bad residual accuracy'),\n+        (\n+            np.array([[1, 0], [0, -2.0]]),\n+            np.array([[1e-6], [0]]),\n+            np.ones((2, 2)),\n+            1.0,\n+            \"Bad residual accuracy\",\n+        ),\n         # Test Case 7: carex #8\n-        (block_diag(-0.1, -0.02),\n-         np.array([[0.100, 0.000], [0.001, 0.010]]),\n-         np.array([[100, 1000], [1000, 10000]]),\n-         np.ones((2, 2)) + block_diag(1e-6, 0),\n-         None),\n+        (\n+            block_diag(-0.1, -0.02),\n+            np.array([[0.100, 0.000], [0.001, 0.010]]),\n+            np.array([[100, 1000], [1000, 10000]]),\n+            np.ones((2, 2)) + block_diag(1e-6, 0),\n+            None,\n+        ),\n         # Test Case 8: carex #9\n-        (np.array([[0, 1e6], [0, 0]]),\n-         np.array([[0], [1.]]),\n-         np.eye(2),\n-         1.,\n-         None),\n+        (np.array([[0, 1e6], [0, 0]]), np.array([[0], [1.0]]), np.eye(2), 1.0, None),\n         # Test Case 9: carex #10\n-        (np.array([[1.0000001, 1], [1., 1.0000001]]),\n-         np.eye(2),\n-         np.eye(2),\n-         np.eye(2),\n-         None),\n+        (\n+            np.array([[1.0000001, 1], [1.0, 1.0000001]]),\n+            np.eye(2),\n+            np.eye(2),\n+            np.eye(2),\n+            None,\n+        ),\n         # Test Case 10: carex #11\n-        (np.array([[3, 1.], [4, 2]]),\n-         np.array([[1], [1]]),\n-         np.array([[-11, -5], [-5, -2.]]),\n-         1.,\n-         None),\n+        (\n+            np.array([[3, 1.0], [4, 2]]),\n+            np.array([[1], [1]]),\n+            np.array([[-11, -5], [-5, -2.0]]),\n+            1.0,\n+            None,\n+        ),\n         # Test Case 11: carex #12\n-        (np.array([[7000000., 2000000., -0.],\n-                   [2000000., 6000000., -2000000.],\n-                   [0., -2000000., 5000000.]]) / 3,\n-         np.eye(3),\n-         np.array([[1., -2., -2.], [-2., 1., -2.], [-2., -2., 1.]]).dot(\n-                np.diag([1e-6, 1, 1e6])).dot(\n-            np.array([[1., -2., -2.], [-2., 1., -2.], [-2., -2., 1.]])) / 9,\n-         np.eye(3) * 1e6,\n-         'Bad Residual Accuracy'),\n+        (\n+            np.array(\n+                [\n+                    [7000000.0, 2000000.0, -0.0],\n+                    [2000000.0, 6000000.0, -2000000.0],\n+                    [0.0, -2000000.0, 5000000.0],\n+                ]\n+            )\n+            / 3,\n+            np.eye(3),\n+            np.array([[1.0, -2.0, -2.0], [-2.0, 1.0, -2.0], [-2.0, -2.0, 1.0]])\n+            .dot(np.diag([1e-6, 1, 1e6]))\n+            .dot(np.array([[1.0, -2.0, -2.0], [-2.0, 1.0, -2.0], [-2.0, -2.0, 1.0]]))\n+            / 9,\n+            np.eye(3) * 1e6,\n+            \"Bad Residual Accuracy\",\n+        ),\n         # Test Case 12: carex #13\n-        (np.array([[0, 0.4, 0, 0],\n-                   [0, 0, 0.345, 0],\n-                   [0, -0.524e6, -0.465e6, 0.262e6],\n-                   [0, 0, 0, -1e6]]),\n-         np.array([[0, 0, 0, 1e6]]).T,\n-         np.diag([1, 0, 1, 0]),\n-         1.,\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [0, 0.4, 0, 0],\n+                    [0, 0, 0.345, 0],\n+                    [0, -0.524e6, -0.465e6, 0.262e6],\n+                    [0, 0, 0, -1e6],\n+                ]\n+            ),\n+            np.array([[0, 0, 0, 1e6]]).T,\n+            np.diag([1, 0, 1, 0]),\n+            1.0,\n+            None,\n+        ),\n         # Test Case 13: carex #14\n-        (np.array([[-1e-6, 1, 0, 0],\n-                   [-1, -1e-6, 0, 0],\n-                   [0, 0, 1e-6, 1],\n-                   [0, 0, -1, 1e-6]]),\n-         np.ones((4, 1)),\n-         np.ones((4, 4)),\n-         1.,\n-         None),\n+        (\n+            np.array(\n+                [[-1e-6, 1, 0, 0], [-1, -1e-6, 0, 0], [0, 0, 1e-6, 1], [0, 0, -1, 1e-6]]\n+            ),\n+            np.ones((4, 1)),\n+            np.ones((4, 4)),\n+            1.0,\n+            None,\n+        ),\n         # Test Case 14: carex #15\n-        (mat15['A'], mat15['B'], mat15['Q'], mat15['R'], None),\n+        (mat15[\"A\"], mat15[\"B\"], mat15[\"Q\"], mat15[\"R\"], None),\n         # Test Case 15: carex #16\n-        (np.eye(64, 64, k=-1) + np.eye(64, 64)*(-2.) + np.rot90(\n-                 block_diag(1, np.zeros((62, 62)), 1)) + np.eye(64, 64, k=1),\n-         np.eye(64),\n-         np.eye(64),\n-         np.eye(64),\n-         None),\n+        (\n+            np.eye(64, 64, k=-1)\n+            + np.eye(64, 64) * (-2.0)\n+            + np.rot90(block_diag(1, np.zeros((62, 62)), 1))\n+            + np.eye(64, 64, k=1),\n+            np.eye(64),\n+            np.eye(64),\n+            np.eye(64),\n+            None,\n+        ),\n         # Test Case 16: carex #17\n-        (np.diag(np.ones((20, )), 1),\n-         np.flipud(np.eye(21, 1)),\n-         np.eye(21, 1) * np.eye(21, 1).T,\n-         1,\n-         'Bad Residual Accuracy'),\n+        (\n+            np.diag(np.ones((20,)), 1),\n+            np.flipud(np.eye(21, 1)),\n+            np.eye(21, 1) * np.eye(21, 1).T,\n+            1,\n+            \"Bad Residual Accuracy\",\n+        ),\n         # Test Case 17: carex #18\n-        (mat18['A'], mat18['B'], mat18['Q'], mat18['R'], None),\n+        (mat18[\"A\"], mat18[\"B\"], mat18[\"Q\"], mat18[\"R\"], None),\n         # Test Case 18: carex #19\n-        (mat19['A'], mat19['B'], mat19['Q'], mat19['R'],\n-         'Bad Residual Accuracy'),\n+        (mat19[\"A\"], mat19[\"B\"], mat19[\"Q\"], mat19[\"R\"], \"Bad Residual Accuracy\"),\n         # Test Case 19: carex #20\n-        (mat20['A'], mat20['B'], mat20['Q'], mat20['R'],\n-         'Bad Residual Accuracy')\n-        ]\n+        (mat20[\"A\"], mat20[\"B\"], mat20[\"Q\"], mat20[\"R\"], \"Bad Residual Accuracy\"),\n+    ]\n     # Makes the minimum precision requirements customized to the test.\n     # Here numbers represent the number of decimals that agrees with zero\n     # matrix when the solution x is plugged in to the equation.\n@@ -313,8 +502,28 @@ class TestSolveContinuousAre:\n     #\n     # If the test is failing use \"None\" for that entry.\n     #\n-    min_decimal = (14, 12, 13, 14, 11, 6, None, 5, 7, 14, 14,\n-                   None, 9, 14, 13, 14, None, 12, None, None)\n+    min_decimal = (\n+        14,\n+        12,\n+        13,\n+        14,\n+        11,\n+        6,\n+        None,\n+        5,\n+        7,\n+        14,\n+        14,\n+        None,\n+        9,\n+        14,\n+        13,\n+        14,\n+        None,\n+        12,\n+        None,\n+        None,\n+    )\n \n     @pytest.mark.parametrize(\"j, case\", enumerate(cases))\n     def test_solve_continuous_are(self, j, case):\n@@ -347,181 +556,268 @@ class TestSolveDiscreteAre:\n         # indicating the reason for failure.\n         #\n         # TEST CASE 0 : Complex a; real b, q, r\n-        (np.array([[2, 1-2j], [0, -3j]]),\n-         np.array([[0], [1]]),\n-         np.array([[1, 0], [0, 2]]),\n-         np.array([[1]]),\n-         None),\n+        (\n+            np.array([[2, 1 - 2j], [0, -3j]]),\n+            np.array([[0], [1]]),\n+            np.array([[1, 0], [0, 2]]),\n+            np.array([[1]]),\n+            None,\n+        ),\n         # TEST CASE 1 :Real a, q, r; complex b\n-        (np.array([[2, 1], [0, -1]]),\n-         np.array([[-2j], [1j]]),\n-         np.array([[1, 0], [0, 2]]),\n-         np.array([[1]]),\n-         None),\n+        (\n+            np.array([[2, 1], [0, -1]]),\n+            np.array([[-2j], [1j]]),\n+            np.array([[1, 0], [0, 2]]),\n+            np.array([[1]]),\n+            None,\n+        ),\n         # TEST CASE 2 : Real a, b; complex q, r\n-        (np.array([[3, 1], [0, -1]]),\n-         np.array([[1, 2], [1, 3]]),\n-         np.array([[1, 1+1j], [1-1j, 2]]),\n-         np.array([[2, -2j], [2j, 3]]),\n-         None),\n+        (\n+            np.array([[3, 1], [0, -1]]),\n+            np.array([[1, 2], [1, 3]]),\n+            np.array([[1, 1 + 1j], [1 - 1j, 2]]),\n+            np.array([[2, -2j], [2j, 3]]),\n+            None,\n+        ),\n         # TEST CASE 3 : User-reported gh-2251 (Trac #1732)\n-        (np.array([[0.63399379, 0.54906824, 0.76253406],\n-                   [0.5404729, 0.53745766, 0.08731853],\n-                   [0.27524045, 0.84922129, 0.4681622]]),\n-         np.array([[0.96861695], [0.05532739], [0.78934047]]),\n-         np.eye(3),\n-         np.eye(1),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [0.63399379, 0.54906824, 0.76253406],\n+                    [0.5404729, 0.53745766, 0.08731853],\n+                    [0.27524045, 0.84922129, 0.4681622],\n+                ]\n+            ),\n+            np.array([[0.96861695], [0.05532739], [0.78934047]]),\n+            np.eye(3),\n+            np.eye(1),\n+            None,\n+        ),\n         # TEST CASE 4 : darex #1\n-        (np.array([[4, 3], [-4.5, -3.5]]),\n-         np.array([[1], [-1]]),\n-         np.array([[9, 6], [6, 4]]),\n-         np.array([[1]]),\n-         None),\n+        (\n+            np.array([[4, 3], [-4.5, -3.5]]),\n+            np.array([[1], [-1]]),\n+            np.array([[9, 6], [6, 4]]),\n+            np.array([[1]]),\n+            None,\n+        ),\n         # TEST CASE 5 : darex #2\n-        (np.array([[0.9512, 0], [0, 0.9048]]),\n-         np.array([[4.877, 4.877], [-1.1895, 3.569]]),\n-         np.array([[0.005, 0], [0, 0.02]]),\n-         np.array([[1/3, 0], [0, 3]]),\n-         None),\n+        (\n+            np.array([[0.9512, 0], [0, 0.9048]]),\n+            np.array([[4.877, 4.877], [-1.1895, 3.569]]),\n+            np.array([[0.005, 0], [0, 0.02]]),\n+            np.array([[1 / 3, 0], [0, 3]]),\n+            None,\n+        ),\n         # TEST CASE 6 : darex #3\n-        (np.array([[2, -1], [1, 0]]),\n-         np.array([[1], [0]]),\n-         np.array([[0, 0], [0, 1]]),\n-         np.array([[0]]),\n-         None),\n+        (\n+            np.array([[2, -1], [1, 0]]),\n+            np.array([[1], [0]]),\n+            np.array([[0, 0], [0, 1]]),\n+            np.array([[0]]),\n+            None,\n+        ),\n         # TEST CASE 7 : darex #4 (skipped the gen. Ric. term S)\n-        (np.array([[0, 1], [0, -1]]),\n-         np.array([[1, 0], [2, 1]]),\n-         np.array([[-4, -4], [-4, 7]]) * (1/11),\n-         np.array([[9, 3], [3, 1]]),\n-         None),\n+        (\n+            np.array([[0, 1], [0, -1]]),\n+            np.array([[1, 0], [2, 1]]),\n+            np.array([[-4, -4], [-4, 7]]) * (1 / 11),\n+            np.array([[9, 3], [3, 1]]),\n+            None,\n+        ),\n         # TEST CASE 8 : darex #5\n-        (np.array([[0, 1], [0, 0]]),\n-         np.array([[0], [1]]),\n-         np.array([[1, 2], [2, 4]]),\n-         np.array([[1]]),\n-         None),\n+        (\n+            np.array([[0, 1], [0, 0]]),\n+            np.array([[0], [1]]),\n+            np.array([[1, 2], [2, 4]]),\n+            np.array([[1]]),\n+            None,\n+        ),\n         # TEST CASE 9 : darex #6\n-        (np.array([[0.998, 0.067, 0, 0],\n-                   [-.067, 0.998, 0, 0],\n-                   [0, 0, 0.998, 0.153],\n-                   [0, 0, -.153, 0.998]]),\n-         np.array([[0.0033, 0.0200],\n-                   [0.1000, -.0007],\n-                   [0.0400, 0.0073],\n-                   [-.0028, 0.1000]]),\n-         np.array([[1.87, 0, 0, -0.244],\n-                   [0, 0.744, 0.205, 0],\n-                   [0, 0.205, 0.589, 0],\n-                   [-0.244, 0, 0, 1.048]]),\n-         np.eye(2),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [0.998, 0.067, 0, 0],\n+                    [-0.067, 0.998, 0, 0],\n+                    [0, 0, 0.998, 0.153],\n+                    [0, 0, -0.153, 0.998],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [0.0033, 0.0200],\n+                    [0.1000, -0.0007],\n+                    [0.0400, 0.0073],\n+                    [-0.0028, 0.1000],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [1.87, 0, 0, -0.244],\n+                    [0, 0.744, 0.205, 0],\n+                    [0, 0.205, 0.589, 0],\n+                    [-0.244, 0, 0, 1.048],\n+                ]\n+            ),\n+            np.eye(2),\n+            None,\n+        ),\n         # TEST CASE 10 : darex #7\n-        (np.array([[0.984750, -.079903, 0.0009054, -.0010765],\n-                   [0.041588, 0.998990, -.0358550, 0.0126840],\n-                   [-.546620, 0.044916, -.3299100, 0.1931800],\n-                   [2.662400, -.100450, -.9245500, -.2632500]]),\n-         np.array([[0.0037112, 0.0007361],\n-                   [-.0870510, 9.3411e-6],\n-                   [-1.198440, -4.1378e-4],\n-                   [-3.192700, 9.2535e-4]]),\n-         np.eye(4)*1e-2,\n-         np.eye(2),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [0.984750, -0.079903, 0.0009054, -0.0010765],\n+                    [0.041588, 0.998990, -0.0358550, 0.0126840],\n+                    [-0.546620, 0.044916, -0.3299100, 0.1931800],\n+                    [2.662400, -0.100450, -0.9245500, -0.2632500],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [0.0037112, 0.0007361],\n+                    [-0.0870510, 9.3411e-6],\n+                    [-1.198440, -4.1378e-4],\n+                    [-3.192700, 9.2535e-4],\n+                ]\n+            ),\n+            np.eye(4) * 1e-2,\n+            np.eye(2),\n+            None,\n+        ),\n         # TEST CASE 11 : darex #8\n-        (np.array([[-0.6000000, -2.2000000, -3.6000000, -5.4000180],\n-                   [1.0000000, 0.6000000, 0.8000000, 3.3999820],\n-                   [0.0000000, 1.0000000, 1.8000000, 3.7999820],\n-                   [0.0000000, 0.0000000, 0.0000000, -0.9999820]]),\n-         np.array([[1.0, -1.0, -1.0, -1.0],\n-                   [0.0, 1.0, -1.0, -1.0],\n-                   [0.0, 0.0, 1.0, -1.0],\n-                   [0.0, 0.0, 0.0, 1.0]]),\n-         np.array([[2, 1, 3, 6],\n-                   [1, 2, 2, 5],\n-                   [3, 2, 6, 11],\n-                   [6, 5, 11, 22]]),\n-         np.eye(4),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [-0.6000000, -2.2000000, -3.6000000, -5.4000180],\n+                    [1.0000000, 0.6000000, 0.8000000, 3.3999820],\n+                    [0.0000000, 1.0000000, 1.8000000, 3.7999820],\n+                    [0.0000000, 0.0000000, 0.0000000, -0.9999820],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [1.0, -1.0, -1.0, -1.0],\n+                    [0.0, 1.0, -1.0, -1.0],\n+                    [0.0, 0.0, 1.0, -1.0],\n+                    [0.0, 0.0, 0.0, 1.0],\n+                ]\n+            ),\n+            np.array([[2, 1, 3, 6], [1, 2, 2, 5], [3, 2, 6, 11], [6, 5, 11, 22]]),\n+            np.eye(4),\n+            None,\n+        ),\n         # TEST CASE 12 : darex #9\n-        (np.array([[95.4070, 1.9643, 0.3597, 0.0673, 0.0190],\n-                   [40.8490, 41.3170, 16.0840, 4.4679, 1.1971],\n-                   [12.2170, 26.3260, 36.1490, 15.9300, 12.3830],\n-                   [4.1118, 12.8580, 27.2090, 21.4420, 40.9760],\n-                   [0.1305, 0.5808, 1.8750, 3.6162, 94.2800]]) * 0.01,\n-         np.array([[0.0434, -0.0122],\n-                   [2.6606, -1.0453],\n-                   [3.7530, -5.5100],\n-                   [3.6076, -6.6000],\n-                   [0.4617, -0.9148]]) * 0.01,\n-         np.eye(5),\n-         np.eye(2),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [95.4070, 1.9643, 0.3597, 0.0673, 0.0190],\n+                    [40.8490, 41.3170, 16.0840, 4.4679, 1.1971],\n+                    [12.2170, 26.3260, 36.1490, 15.9300, 12.3830],\n+                    [4.1118, 12.8580, 27.2090, 21.4420, 40.9760],\n+                    [0.1305, 0.5808, 1.8750, 3.6162, 94.2800],\n+                ]\n+            )\n+            * 0.01,\n+            np.array(\n+                [\n+                    [0.0434, -0.0122],\n+                    [2.6606, -1.0453],\n+                    [3.7530, -5.5100],\n+                    [3.6076, -6.6000],\n+                    [0.4617, -0.9148],\n+                ]\n+            )\n+            * 0.01,\n+            np.eye(5),\n+            np.eye(2),\n+            None,\n+        ),\n         # TEST CASE 13 : darex #10\n-        (np.kron(np.eye(2), np.diag([1, 1], k=1)),\n-         np.kron(np.eye(2), np.array([[0], [0], [1]])),\n-         np.array([[1, 1, 0, 0, 0, 0],\n-                   [1, 1, 0, 0, 0, 0],\n-                   [0, 0, 0, 0, 0, 0],\n-                   [0, 0, 0, 1, -1, 0],\n-                   [0, 0, 0, -1, 1, 0],\n-                   [0, 0, 0, 0, 0, 0]]),\n-         np.array([[3, 0], [0, 1]]),\n-         None),\n+        (\n+            np.kron(np.eye(2), np.diag([1, 1], k=1)),\n+            np.kron(np.eye(2), np.array([[0], [0], [1]])),\n+            np.array(\n+                [\n+                    [1, 1, 0, 0, 0, 0],\n+                    [1, 1, 0, 0, 0, 0],\n+                    [0, 0, 0, 0, 0, 0],\n+                    [0, 0, 0, 1, -1, 0],\n+                    [0, 0, 0, -1, 1, 0],\n+                    [0, 0, 0, 0, 0, 0],\n+                ]\n+            ),\n+            np.array([[3, 0], [0, 1]]),\n+            None,\n+        ),\n         # TEST CASE 14 : darex #11\n-        (0.001 * np.array(\n-         [[870.1, 135.0, 11.59, .5014, -37.22, .3484, 0, 4.242, 7.249],\n-          [76.55, 897.4, 12.72, 0.5504, -40.16, .3743, 0, 4.53, 7.499],\n-          [-127.2, 357.5, 817, 1.455, -102.8, .987, 0, 11.85, 18.72],\n-          [-363.5, 633.9, 74.91, 796.6, -273.5, 2.653, 0, 31.72, 48.82],\n-          [-960, 1645.9, -128.9, -5.597, 71.42, 7.108, 0, 84.52, 125.9],\n-          [-664.4, 112.96, -88.89, -3.854, 84.47, 13.6, 0, 144.3, 101.6],\n-          [-410.2, 693, -54.71, -2.371, 66.49, 12.49, .1063, 99.97, 69.67],\n-          [-179.9, 301.7, -23.93, -1.035, 60.59, 22.16, 0, 213.9, 35.54],\n-          [-345.1, 580.4, -45.96, -1.989, 105.6, 19.86, 0, 219.1, 215.2]]),\n-         np.array([[4.7600, -0.5701, -83.6800],\n-                   [0.8790, -4.7730, -2.7300],\n-                   [1.4820, -13.1200, 8.8760],\n-                   [3.8920, -35.1300, 24.8000],\n-                   [10.3400, -92.7500, 66.8000],\n-                   [7.2030, -61.5900, 38.3400],\n-                   [4.4540, -36.8300, 20.2900],\n-                   [1.9710, -15.5400, 6.9370],\n-                   [3.7730, -30.2800, 14.6900]]) * 0.001,\n-         np.diag([50, 0, 0, 0, 50, 0, 0, 0, 0]),\n-         np.eye(3),\n-         None),\n+        (\n+            0.001\n+            * np.array(\n+                [\n+                    [870.1, 135.0, 11.59, 0.5014, -37.22, 0.3484, 0, 4.242, 7.249],\n+                    [76.55, 897.4, 12.72, 0.5504, -40.16, 0.3743, 0, 4.53, 7.499],\n+                    [-127.2, 357.5, 817, 1.455, -102.8, 0.987, 0, 11.85, 18.72],\n+                    [-363.5, 633.9, 74.91, 796.6, -273.5, 2.653, 0, 31.72, 48.82],\n+                    [-960, 1645.9, -128.9, -5.597, 71.42, 7.108, 0, 84.52, 125.9],\n+                    [-664.4, 112.96, -88.89, -3.854, 84.47, 13.6, 0, 144.3, 101.6],\n+                    [-410.2, 693, -54.71, -2.371, 66.49, 12.49, 0.1063, 99.97, 69.67],\n+                    [-179.9, 301.7, -23.93, -1.035, 60.59, 22.16, 0, 213.9, 35.54],\n+                    [-345.1, 580.4, -45.96, -1.989, 105.6, 19.86, 0, 219.1, 215.2],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [4.7600, -0.5701, -83.6800],\n+                    [0.8790, -4.7730, -2.7300],\n+                    [1.4820, -13.1200, 8.8760],\n+                    [3.8920, -35.1300, 24.8000],\n+                    [10.3400, -92.7500, 66.8000],\n+                    [7.2030, -61.5900, 38.3400],\n+                    [4.4540, -36.8300, 20.2900],\n+                    [1.9710, -15.5400, 6.9370],\n+                    [3.7730, -30.2800, 14.6900],\n+                ]\n+            )\n+            * 0.001,\n+            np.diag([50, 0, 0, 0, 50, 0, 0, 0, 0]),\n+            np.eye(3),\n+            None,\n+        ),\n         # TEST CASE 15 : darex #12 - numerically least accurate example\n-        (np.array([[0, 1e6], [0, 0]]),\n-         np.array([[0], [1]]),\n-         np.eye(2),\n-         np.array([[1]]),\n-        None),\n+        (\n+            np.array([[0, 1e6], [0, 0]]),\n+            np.array([[0], [1]]),\n+            np.eye(2),\n+            np.array([[1]]),\n+            None,\n+        ),\n         # TEST CASE 16 : darex #13\n-        (np.array([[16, 10, -2],\n-                  [10, 13, -8],\n-                  [-2, -8, 7]]) * (1/9),\n-         np.eye(3),\n-         1e6 * np.eye(3),\n-         1e6 * np.eye(3),\n-        None),\n+        (\n+            np.array([[16, 10, -2], [10, 13, -8], [-2, -8, 7]]) * (1 / 9),\n+            np.eye(3),\n+            1e6 * np.eye(3),\n+            1e6 * np.eye(3),\n+            None,\n+        ),\n         # TEST CASE 17 : darex #14\n-        (np.array([[1 - 1/1e8, 0, 0, 0],\n-                  [1, 0, 0, 0],\n-                  [0, 1, 0, 0],\n-                  [0, 0, 1, 0]]),\n-         np.array([[1e-08], [0], [0], [0]]),\n-         np.diag([0, 0, 0, 1]),\n-         np.array([[0.25]]),\n-         None),\n+        (\n+            np.array(\n+                [[1 - 1 / 1e8, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]]\n+            ),\n+            np.array([[1e-08], [0], [0], [0]]),\n+            np.diag([0, 0, 0, 1]),\n+            np.array([[0.25]]),\n+            None,\n+        ),\n         # TEST CASE 18 : darex #15\n-        (np.eye(100, k=1),\n-         np.flipud(np.eye(100, 1)),\n-         np.eye(100),\n-         np.array([[1]]),\n-         None)\n-        ]\n+        (\n+            np.eye(100, k=1),\n+            np.flipud(np.eye(100, 1)),\n+            np.eye(100),\n+            np.array([[1]]),\n+            None,\n+        ),\n+    ]\n \n     # Makes the minimum precision requirements customized to the test.\n     # Here numbers represent the number of decimals that agrees with zero\n@@ -531,8 +827,27 @@ class TestSolveDiscreteAre:\n     #\n     # If the test is failing use \"None\" for that entry.\n     #\n-    min_decimal = (12, 14, 13, 14, 13, 16, 18, 14, 14, 13,\n-                   14, 13, 13, 14, 12, 2, 4, 6, 10)\n+    min_decimal = (\n+        12,\n+        14,\n+        13,\n+        14,\n+        13,\n+        16,\n+        18,\n+        14,\n+        14,\n+        13,\n+        14,\n+        13,\n+        13,\n+        14,\n+        12,\n+        2,\n+        4,\n+        6,\n+        10,\n+    )\n     max_tol = [1.5 * 10**-ind for ind in min_decimal]\n     # relaxed tolerance in gh-18012 after bump to OpenBLAS\n     max_tol[11] = 2.5e-13\n@@ -597,33 +912,61 @@ def test_solve_generalized_continuous_are():\n     cases = [\n         # Two random examples differ by s term\n         # in the absence of any literature for demanding examples.\n-        (np.array([[2.769230e-01, 8.234578e-01, 9.502220e-01],\n-                   [4.617139e-02, 6.948286e-01, 3.444608e-02],\n-                   [9.713178e-02, 3.170995e-01, 4.387444e-01]]),\n-         np.array([[3.815585e-01, 1.868726e-01],\n-                   [7.655168e-01, 4.897644e-01],\n-                   [7.951999e-01, 4.455862e-01]]),\n-         np.eye(3),\n-         np.eye(2),\n-         np.array([[6.463130e-01, 2.760251e-01, 1.626117e-01],\n-                   [7.093648e-01, 6.797027e-01, 1.189977e-01],\n-                   [7.546867e-01, 6.550980e-01, 4.983641e-01]]),\n-         np.zeros((3, 2)),\n-         None),\n-        (np.array([[2.769230e-01, 8.234578e-01, 9.502220e-01],\n-                   [4.617139e-02, 6.948286e-01, 3.444608e-02],\n-                   [9.713178e-02, 3.170995e-01, 4.387444e-01]]),\n-         np.array([[3.815585e-01, 1.868726e-01],\n-                   [7.655168e-01, 4.897644e-01],\n-                   [7.951999e-01, 4.455862e-01]]),\n-         np.eye(3),\n-         np.eye(2),\n-         np.array([[6.463130e-01, 2.760251e-01, 1.626117e-01],\n-                   [7.093648e-01, 6.797027e-01, 1.189977e-01],\n-                   [7.546867e-01, 6.550980e-01, 4.983641e-01]]),\n-         np.ones((3, 2)),\n-         None)\n-        ]\n+        (\n+            np.array(\n+                [\n+                    [2.769230e-01, 8.234578e-01, 9.502220e-01],\n+                    [4.617139e-02, 6.948286e-01, 3.444608e-02],\n+                    [9.713178e-02, 3.170995e-01, 4.387444e-01],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [3.815585e-01, 1.868726e-01],\n+                    [7.655168e-01, 4.897644e-01],\n+                    [7.951999e-01, 4.455862e-01],\n+                ]\n+            ),\n+            np.eye(3),\n+            np.eye(2),\n+            np.array(\n+                [\n+                    [6.463130e-01, 2.760251e-01, 1.626117e-01],\n+                    [7.093648e-01, 6.797027e-01, 1.189977e-01],\n+                    [7.546867e-01, 6.550980e-01, 4.983641e-01],\n+                ]\n+            ),\n+            np.zeros((3, 2)),\n+            None,\n+        ),\n+        (\n+            np.array(\n+                [\n+                    [2.769230e-01, 8.234578e-01, 9.502220e-01],\n+                    [4.617139e-02, 6.948286e-01, 3.444608e-02],\n+                    [9.713178e-02, 3.170995e-01, 4.387444e-01],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [3.815585e-01, 1.868726e-01],\n+                    [7.655168e-01, 4.897644e-01],\n+                    [7.951999e-01, 4.455862e-01],\n+                ]\n+            ),\n+            np.eye(3),\n+            np.eye(2),\n+            np.array(\n+                [\n+                    [6.463130e-01, 2.760251e-01, 1.626117e-01],\n+                    [7.093648e-01, 6.797027e-01, 1.189977e-01],\n+                    [7.546867e-01, 6.550980e-01, 4.983641e-01],\n+                ]\n+            ),\n+            np.ones((3, 2)),\n+            None,\n+        ),\n+    ]\n \n     min_decimal = (10, 10)\n \n@@ -644,47 +987,77 @@ def test_solve_generalized_continuous_are():\n \n \n def test_solve_generalized_discrete_are():\n-    mat20170120 = _load_data('gendare_20170120_data.npz')\n+    mat20170120 = _load_data(\"gendare_20170120_data.npz\")\n \n     cases = [\n         # Two random examples differ by s term\n         # in the absence of any literature for demanding examples.\n-        (np.array([[2.769230e-01, 8.234578e-01, 9.502220e-01],\n-                   [4.617139e-02, 6.948286e-01, 3.444608e-02],\n-                   [9.713178e-02, 3.170995e-01, 4.387444e-01]]),\n-         np.array([[3.815585e-01, 1.868726e-01],\n-                   [7.655168e-01, 4.897644e-01],\n-                   [7.951999e-01, 4.455862e-01]]),\n-         np.eye(3),\n-         np.eye(2),\n-         np.array([[6.463130e-01, 2.760251e-01, 1.626117e-01],\n-                   [7.093648e-01, 6.797027e-01, 1.189977e-01],\n-                   [7.546867e-01, 6.550980e-01, 4.983641e-01]]),\n-         np.zeros((3, 2)),\n-         None),\n-        (np.array([[2.769230e-01, 8.234578e-01, 9.502220e-01],\n-                   [4.617139e-02, 6.948286e-01, 3.444608e-02],\n-                   [9.713178e-02, 3.170995e-01, 4.387444e-01]]),\n-         np.array([[3.815585e-01, 1.868726e-01],\n-                   [7.655168e-01, 4.897644e-01],\n-                   [7.951999e-01, 4.455862e-01]]),\n-         np.eye(3),\n-         np.eye(2),\n-         np.array([[6.463130e-01, 2.760251e-01, 1.626117e-01],\n-                   [7.093648e-01, 6.797027e-01, 1.189977e-01],\n-                   [7.546867e-01, 6.550980e-01, 4.983641e-01]]),\n-         np.ones((3, 2)),\n-         None),\n+        (\n+            np.array(\n+                [\n+                    [2.769230e-01, 8.234578e-01, 9.502220e-01],\n+                    [4.617139e-02, 6.948286e-01, 3.444608e-02],\n+                    [9.713178e-02, 3.170995e-01, 4.387444e-01],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [3.815585e-01, 1.868726e-01],\n+                    [7.655168e-01, 4.897644e-01],\n+                    [7.951999e-01, 4.455862e-01],\n+                ]\n+            ),\n+            np.eye(3),\n+            np.eye(2),\n+            np.array(\n+                [\n+                    [6.463130e-01, 2.760251e-01, 1.626117e-01],\n+                    [7.093648e-01, 6.797027e-01, 1.189977e-01],\n+                    [7.546867e-01, 6.550980e-01, 4.983641e-01],\n+                ]\n+            ),\n+            np.zeros((3, 2)),\n+            None,\n+        ),\n+        (\n+            np.array(\n+                [\n+                    [2.769230e-01, 8.234578e-01, 9.502220e-01],\n+                    [4.617139e-02, 6.948286e-01, 3.444608e-02],\n+                    [9.713178e-02, 3.170995e-01, 4.387444e-01],\n+                ]\n+            ),\n+            np.array(\n+                [\n+                    [3.815585e-01, 1.868726e-01],\n+                    [7.655168e-01, 4.897644e-01],\n+                    [7.951999e-01, 4.455862e-01],\n+                ]\n+            ),\n+            np.eye(3),\n+            np.eye(2),\n+            np.array(\n+                [\n+                    [6.463130e-01, 2.760251e-01, 1.626117e-01],\n+                    [7.093648e-01, 6.797027e-01, 1.189977e-01],\n+                    [7.546867e-01, 6.550980e-01, 4.983641e-01],\n+                ]\n+            ),\n+            np.ones((3, 2)),\n+            None,\n+        ),\n         # user-reported (under PR-6616) 20-Jan-2017\n         # tests against the case where E is None but S is provided\n-        (mat20170120['A'],\n-         mat20170120['B'],\n-         mat20170120['Q'],\n-         mat20170120['R'],\n-         None,\n-         mat20170120['S'],\n-         None),\n-        ]\n+        (\n+            mat20170120[\"A\"],\n+            mat20170120[\"B\"],\n+            mat20170120[\"Q\"],\n+            mat20170120[\"R\"],\n+            None,\n+            mat20170120[\"S\"],\n+            None,\n+        ),\n+    ]\n \n     max_atol = (1.5e-11, 1.5e-11, 3.5e-16)\n \n@@ -701,10 +1074,8 @@ def test_solve_generalized_discrete_are():\n             s = np.zeros_like(b)\n         res = a.conj().T.dot(x.dot(a)) - e.conj().T.dot(x.dot(e)) + q\n         res -= (a.conj().T.dot(x.dot(b)) + s).dot(\n-                    solve(r+b.conj().T.dot(x.dot(b)),\n-                          (b.conj().T.dot(x.dot(a)) + s.conj().T)\n-                          )\n-                )\n+            solve(r + b.conj().T.dot(x.dot(b)), (b.conj().T.dot(x.dot(a)) + s.conj().T))\n+        )\n         # changed from:\n         # assert_array_almost_equal(res, np.zeros_like(res), decimal=dec)\n         # in gh-17950 because of a Linux 32 bit fail.\n@@ -715,7 +1086,6 @@ def test_solve_generalized_discrete_are():\n \n \n def test_are_validate_args():\n-\n     def test_square_shape():\n         nsq = np.ones((3, 2))\n         sq = np.eye(3)\n@@ -766,57 +1136,62 @@ def test_are_validate_args():\n class TestSolveSylvester:\n     cases = [\n         # empty cases\n-        (np.empty((0, 0)),\n-         np.empty((0, 0)),\n-         np.empty((0, 0))),\n-         (np.empty((0, 0)),\n-         np.empty((2, 2)),\n-         np.empty((0, 2))),\n-         (np.empty((2, 2)),\n-         np.empty((0, 0)),\n-         np.empty((2, 0))),\n+        (np.empty((0, 0)), np.empty((0, 0)), np.empty((0, 0))),\n+        (np.empty((0, 0)), np.empty((2, 2)), np.empty((0, 2))),\n+        (np.empty((2, 2)), np.empty((0, 0)), np.empty((2, 0))),\n         # a, b, c all real.\n-        (np.array([[1, 2], [0, 4]]),\n-         np.array([[5, 6], [0, 8]]),\n-         np.array([[9, 10], [11, 12]])),\n+        (\n+            np.array([[1, 2], [0, 4]]),\n+            np.array([[5, 6], [0, 8]]),\n+            np.array([[9, 10], [11, 12]]),\n+        ),\n         # a, b, c all real, 4x4. a and b have non-trivial 2x2 blocks in their\n         # quasi-triangular form.\n-        (np.array([[1.0, 0, 0, 0],\n-                   [0, 1.0, 2.0, 0.0],\n-                   [0, 0, 3.0, -4],\n-                   [0, 0, 2, 5]]),\n-         np.array([[2.0, 0, 0, 1.0],\n-                   [0, 1.0, 0.0, 0.0],\n-                   [0, 0, 1.0, -1],\n-                   [0, 0, 1, 1]]),\n-         np.array([[1.0, 0, 0, 0],\n-                   [0, 1.0, 0, 0],\n-                   [0, 0, 1.0, 0],\n-                   [0, 0, 0, 1.0]])),\n+        (\n+            np.array(\n+                [[1.0, 0, 0, 0], [0, 1.0, 2.0, 0.0], [0, 0, 3.0, -4], [0, 0, 2, 5]]\n+            ),\n+            np.array(\n+                [[2.0, 0, 0, 1.0], [0, 1.0, 0.0, 0.0], [0, 0, 1.0, -1], [0, 0, 1, 1]]\n+            ),\n+            np.array([[1.0, 0, 0, 0], [0, 1.0, 0, 0], [0, 0, 1.0, 0], [0, 0, 0, 1.0]]),\n+        ),\n         # a, b, c all complex.\n-        (np.array([[1.0+1j, 2.0], [3.0-4.0j, 5.0]]),\n-         np.array([[-1.0, 2j], [3.0, 4.0]]),\n-         np.array([[2.0-2j, 2.0+2j], [-1.0-1j, 2.0]])),\n+        (\n+            np.array([[1.0 + 1j, 2.0], [3.0 - 4.0j, 5.0]]),\n+            np.array([[-1.0, 2j], [3.0, 4.0]]),\n+            np.array([[2.0 - 2j, 2.0 + 2j], [-1.0 - 1j, 2.0]]),\n+        ),\n         # a and b real; c complex.\n-        (np.array([[1.0, 2.0], [3.0, 5.0]]),\n-         np.array([[-1.0, 0], [3.0, 4.0]]),\n-         np.array([[2.0-2j, 2.0+2j], [-1.0-1j, 2.0]])),\n+        (\n+            np.array([[1.0, 2.0], [3.0, 5.0]]),\n+            np.array([[-1.0, 0], [3.0, 4.0]]),\n+            np.array([[2.0 - 2j, 2.0 + 2j], [-1.0 - 1j, 2.0]]),\n+        ),\n         # a and c complex; b real.\n-        (np.array([[1.0+1j, 2.0], [3.0-4.0j, 5.0]]),\n-         np.array([[-1.0, 0], [3.0, 4.0]]),\n-         np.array([[2.0-2j, 2.0+2j], [-1.0-1j, 2.0]])),\n+        (\n+            np.array([[1.0 + 1j, 2.0], [3.0 - 4.0j, 5.0]]),\n+            np.array([[-1.0, 0], [3.0, 4.0]]),\n+            np.array([[2.0 - 2j, 2.0 + 2j], [-1.0 - 1j, 2.0]]),\n+        ),\n         # a complex; b and c real.\n-        (np.array([[1.0+1j, 2.0], [3.0-4.0j, 5.0]]),\n-         np.array([[-1.0, 0], [3.0, 4.0]]),\n-         np.array([[2.0, 2.0], [-1.0, 2.0]])),\n+        (\n+            np.array([[1.0 + 1j, 2.0], [3.0 - 4.0j, 5.0]]),\n+            np.array([[-1.0, 0], [3.0, 4.0]]),\n+            np.array([[2.0, 2.0], [-1.0, 2.0]]),\n+        ),\n         # not square matrices, real\n-        (np.array([[8, 1, 6], [3, 5, 7], [4, 9, 2]]),\n-         np.array([[2, 3], [4, 5]]),\n-         np.array([[1, 2], [3, 4], [5, 6]])),\n+        (\n+            np.array([[8, 1, 6], [3, 5, 7], [4, 9, 2]]),\n+            np.array([[2, 3], [4, 5]]),\n+            np.array([[1, 2], [3, 4], [5, 6]]),\n+        ),\n         # not square matrices, complex\n-        (np.array([[8, 1j, 6+2j], [3, 5, 7], [4, 9, 2]]),\n-         np.array([[2, 3], [4, 5-1j]]),\n-         np.array([[1, 2j], [3, 4j], [5j, 6+7j]])),\n+        (\n+            np.array([[8, 1j, 6 + 2j], [3, 5, 7], [4, 9, 2]]),\n+            np.array([[2, 3], [4, 5 - 1j]]),\n+            np.array([[1, 2j], [3, 4j], [5j, 6 + 7j]]),\n+        ),\n     ]\n \n     def check_case(self, a, b, c):\n@@ -844,7 +1219,7 @@ class TestSolveSylvester:\n     @pytest.mark.parametrize(\"n\", [0, 3])\n     def test_size_0(self, m, n, dtype_a, dtype_b, dtype_q):\n         if m == n != 0:\n-            pytest.skip('m = n != 0 is not a case that needs to be tested here.')\n+            pytest.skip(\"m = n != 0 is not a case that needs to be tested here.\")\n \n         rng = np.random.default_rng(598435298262546)\n \n@@ -853,9 +1228,9 @@ class TestSolveSylvester:\n         q = np.zeros((m, n), dtype=dtype_q)\n         res = solve_sylvester(a, b, q)\n \n-        a = (rng.random((5, 5))*100).astype(dtype_a)\n-        b = (rng.random((6, 6))*100).astype(dtype_b)\n-        q = (rng.random((5, 6))*100).astype(dtype_q)\n+        a = (rng.random((5, 5)) * 100).astype(dtype_a)\n+        b = (rng.random((6, 6)) * 100).astype(dtype_b)\n+        q = (rng.random((5, 6)) * 100).astype(dtype_q)\n         ref = solve_sylvester(a, b, q)\n \n         assert res.shape == (m, n)\n"
    },
    {
        "id": 263,
        "sha_fail": "7a161f65a2b75840c6a15ef8e27f17634792b906",
        "diff": "diff --git a/Tests/test_file_cur.py b/Tests/test_file_cur.py\nindex 086d4a1..f5cf2a5 100644\n--- a/Tests/test_file_cur.py\n+++ b/Tests/test_file_cur.py\n@@ -38,8 +38,11 @@ def test_largest_cursor() -> None:\n         + o16(0)  # planes\n         + o16(1)  # bits\n     )\n-    with Image.open(BytesIO(data)) as im:\n-        assert im.size == (8, 8)\n+    try:\n+        with Image.open(BytesIO(data)) as im:\n+            assert im.size == (8, 8)\n+    except OSError as e:\n+        assert str(e) == \"image file is truncated (0 bytes not processed)\"\n \n \n def test_posy_link() -> None:\n"
    },
    {
        "id": 264,
        "sha_fail": "eb287cf79c1800a32bf499d4f2e9e579bfa8386c",
        "diff": "diff --git a/tests/test_download_pretrained.py b/tests/test_download_pretrained.py\nindex 6340918..d7707e7 100644\n--- a/tests/test_download_pretrained.py\n+++ b/tests/test_download_pretrained.py\n@@ -16,88 +16,102 @@ from open_clip.pretrained import download_pretrained_from_url\n \n \n class DownloadPretrainedTests(unittest.TestCase):\n-\n-    def create_response(self, data, status_code=200, content_type='application/octet-stream'):\n+    def create_response(\n+        self, data, status_code=200, content_type=\"application/octet-stream\"\n+    ):\n         fp = BytesIO(data)\n-        headers = HTTPHeaderDict({\n-            'Content-Type': content_type,\n-            'Content-Length': str(len(data))\n-        })\n-        raw = HTTPResponse(fp, preload_content=False, headers=headers, status=status_code)\n+        headers = HTTPHeaderDict(\n+            {\"Content-Type\": content_type, \"Content-Length\": str(len(data))}\n+        )\n+        raw = HTTPResponse(\n+            fp, preload_content=False, headers=headers, status=status_code\n+        )\n         return raw\n \n-    @patch('open_clip.pretrained.urllib')\n+    @patch(\"open_clip.pretrained.urllib\")\n     def test_download_pretrained_from_url_from_openaipublic(self, urllib):\n-        file_contents = b'pretrained model weights'\n+        file_contents = b\"pretrained model weights\"\n         expected_hash = hashlib.sha256(file_contents).hexdigest()\n         urllib.request.urlopen.return_value = self.create_response(file_contents)\n         with tempfile.TemporaryDirectory() as root:\n-            url = f'https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt'\n+            url = f\"https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt\"\n             download_pretrained_from_url(url, root)\n         urllib.request.urlopen.assert_called_once()\n \n-    @patch('open_clip.pretrained.urllib')\n+    @patch(\"open_clip.pretrained.urllib\")\n     def test_download_pretrained_from_url_from_openaipublic_corrupted(self, urllib):\n-        file_contents = b'pretrained model weights'\n+        file_contents = b\"pretrained model weights\"\n         expected_hash = hashlib.sha256(file_contents).hexdigest()\n-        urllib.request.urlopen.return_value = self.create_response(b'corrupted pretrained model')\n+        urllib.request.urlopen.return_value = self.create_response(\n+            b\"corrupted pretrained model\"\n+        )\n         with tempfile.TemporaryDirectory() as root:\n-            url = f'https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt'\n-            with self.assertRaisesRegex(RuntimeError, r'checksum does not not match'):\n+            url = f\"https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt\"\n+            with self.assertRaisesRegex(RuntimeError, r\"checksum does not not match\"):\n                 download_pretrained_from_url(url, root)\n         urllib.request.urlopen.assert_called_once()\n \n-    @patch('open_clip.pretrained.urllib')\n+    @patch(\"open_clip.pretrained.urllib\")\n     def test_download_pretrained_from_url_from_openaipublic_valid_cache(self, urllib):\n-        file_contents = b'pretrained model weights'\n+        file_contents = b\"pretrained model weights\"\n         expected_hash = hashlib.sha256(file_contents).hexdigest()\n         urllib.request.urlopen.return_value = self.create_response(file_contents)\n         with tempfile.TemporaryDirectory() as root:\n-            local_file = Path(root) / 'RN50.pt'\n+            local_file = Path(root) / \"RN50.pt\"\n             local_file.write_bytes(file_contents)\n-            url = f'https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt'\n+            url = f\"https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt\"\n             download_pretrained_from_url(url, root)\n         urllib.request.urlopen.assert_not_called()\n \n-    @patch('open_clip.pretrained.urllib')\n-    def test_download_pretrained_from_url_from_openaipublic_corrupted_cache(self, urllib):\n-        file_contents = b'pretrained model weights'\n+    @patch(\"open_clip.pretrained.urllib\")\n+    def test_download_pretrained_from_url_from_openaipublic_corrupted_cache(\n+        self, urllib\n+    ):\n+        file_contents = b\"pretrained model weights\"\n         expected_hash = hashlib.sha256(file_contents).hexdigest()\n         urllib.request.urlopen.return_value = self.create_response(file_contents)\n         with tempfile.TemporaryDirectory() as root:\n-            local_file = Path(root) / 'RN50.pt'\n-            local_file.write_bytes(b'corrupted pretrained model')\n-            url = f'https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt'\n+            local_file = Path(root) / \"RN50.pt\"\n+            local_file.write_bytes(b\"corrupted pretrained model\")\n+            url = f\"https://openaipublic.azureedge.net/clip/models/{expected_hash}/RN50.pt\"\n             download_pretrained_from_url(url, root)\n         urllib.request.urlopen.assert_called_once()\n \n-    @patch('open_clip.pretrained.urllib')\n+    @patch(\"open_clip.pretrained.urllib\")\n     def test_download_pretrained_from_url_from_mlfoundations(self, urllib):\n-        file_contents = b'pretrained model weights'\n+        file_contents = b\"pretrained model weights\"\n         expected_hash = hashlib.sha256(file_contents).hexdigest()[:8]\n         urllib.request.urlopen.return_value = self.create_response(file_contents)\n         with tempfile.TemporaryDirectory() as root:\n-            url = f'https://github.com/mlfoundations/download/v0.2-weights/rn50-quickgelu-{expected_hash}.pt'\n+            url = f\"https://github.com/mlfoundations/download/v0.2-weights/rn50-quickgelu-{expected_hash}.pt\"\n             download_pretrained_from_url(url, root)\n         urllib.request.urlopen.assert_called_once()\n \n-    @patch('open_clip.pretrained.urllib')\n+    @patch(\"open_clip.pretrained.urllib\")\n     def test_download_pretrained_from_url_from_mlfoundations_corrupted(self, urllib):\n-        file_contents = b'pretrained model weights'\n+        file_contents = b\"pretrained model weights\"\n         expected_hash = hashlib.sha256(file_contents).hexdigest()[:8]\n-        urllib.request.urlopen.return_value = self.create_response(b'corrupted pretrained model')\n+        urllib.request.urlopen.return_value = self.create_response(\n+            b\"corrupted pretrained model\"\n+        )\n         with tempfile.TemporaryDirectory() as root:\n-            url = f'https://github.com/mlfoundations/download/v0.2-weights/rn50-quickgelu-{expected_hash}.pt'\n-            with self.assertRaisesRegex(RuntimeError, r'checksum does not not match'):\n+            url = f\"https://github.com/mlfoundations/download/v0.2-weights/rn50-quickgelu-{expected_hash}.pt\"\n+            with self.assertRaisesRegex(RuntimeError, r\"checksum does not not match\"):\n                 download_pretrained_from_url(url, root)\n         urllib.request.urlopen.assert_called_once()\n \n-    @patch('open_clip.pretrained.urllib')\n+    @patch(\"open_clip.pretrained.urllib\")\n     def test_download_pretrained_from_hfh(self, urllib):\n-        model, _, preprocess = open_clip.create_model_and_transforms('hf-hub:hf-internal-testing/tiny-open-clip-model')\n-        tokenizer = open_clip.get_tokenizer('hf-hub:hf-internal-testing/tiny-open-clip-model')\n+        model, _, preprocess = open_clip.create_model_and_transforms(\n+            \"hf-hub:hf-internal-testing/tiny-open-clip-model\"\n+        )\n+        tokenizer = open_clip.get_tokenizer(\n+            \"hf-hub:hf-internal-testing/tiny-open-clip-model\"\n+        )\n         img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n-        image = preprocess(Image.open(requests.get(img_url, stream=True).raw)).unsqueeze(0)\n+        image = preprocess(\n+            Image.open(requests.get(img_url, stream=True).raw)\n+        ).unsqueeze(0)\n         text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n \n         with torch.no_grad():\n@@ -108,4 +122,6 @@ class DownloadPretrainedTests(unittest.TestCase):\n \n             text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n \n-        self.assertTrue(torch.allclose(text_probs, torch.tensor([[0.0597, 0.6349, 0.3053]]), 1e-3))\n+        self.assertTrue(\n+            torch.allclose(text_probs, torch.tensor([[0.0597, 0.6349, 0.3053]]), 1e-3)\n+        )\ndiff --git a/tests/util_test.py b/tests/util_test.py\nindex aace12e..6fa8765 100644\n--- a/tests/util_test.py\n+++ b/tests/util_test.py\n@@ -8,12 +8,13 @@ from PIL import Image\n \n torch.serialization.add_safe_globals([PIL.Image.Image])\n \n-if __name__ != '__main__':\n+if __name__ != \"__main__\":\n     import open_clip\n \n-os.environ['CUDA_VISIBLE_DEVICES'] = ''\n+os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n \n-def seed_all(seed = 0):\n+\n+def seed_all(seed=0):\n     torch.backends.cudnn.deterministic = True\n     torch.backends.cudnn.benchmark = False\n     torch.use_deterministic_algorithms(True, warn_only=False)\n@@ -21,6 +22,7 @@ def seed_all(seed = 0):\n     np.random.seed(seed)\n     torch.manual_seed(seed)\n \n+\n def inference_text(model, model_name, batches):\n     y = []\n     tokenizer = open_clip.get_tokenizer(model_name)\n@@ -30,6 +32,7 @@ def inference_text(model, model_name, batches):\n             y.append(model.encode_text(x))\n         return torch.stack(y)\n \n+\n def inference_image(model, preprocess_val, batches):\n     y = []\n     with torch.no_grad():\n@@ -37,7 +40,8 @@ def inference_image(model, preprocess_val, batches):\n             x = torch.stack([preprocess_val(img) for img in x])\n             y.append(model.encode_image(x))\n         return torch.stack(y)\n-    \n+\n+\n def forward_model(model, model_name, preprocess_val, image_batch, text_batch):\n     y = []\n     tokenizer = open_clip.get_tokenizer(model_name)\n@@ -56,100 +60,95 @@ def forward_model(model, model_name, preprocess_val, image_batch, text_batch):\n             out.append(torch.stack([batch_out[i] for batch_out in y]))\n     return out\n \n+\n def random_image_batch(batch_size, size):\n     h, w = size\n-    data = np.random.randint(255, size = (batch_size, h, w, 3), dtype = np.uint8)\n-    return [ Image.fromarray(d) for d in data ]\n+    data = np.random.randint(255, size=(batch_size, h, w, 3), dtype=np.uint8)\n+    return [Image.fromarray(d) for d in data]\n \n-def random_text_batch(batch_size, min_length = 75, max_length = 75):\n+\n+def random_text_batch(batch_size, min_length=75, max_length=75):\n     t = open_clip.tokenizer.SimpleTokenizer()\n     # every token decoded as string, exclude SOT and EOT, replace EOW with space\n     token_words = [\n-            x[1].replace('</w>', ' ')\n-            for x in t.decoder.items()\n-            if x[0] not in t.all_special_ids\n+        x[1].replace(\"</w>\", \" \")\n+        for x in t.decoder.items()\n+        if x[0] not in t.all_special_ids\n     ]\n     # strings of randomly chosen tokens\n     return [\n-        ''.join(random.choices(\n-                token_words,\n-                k = random.randint(min_length, max_length)\n-        ))\n+        \"\".join(random.choices(token_words, k=random.randint(min_length, max_length)))\n         for _ in range(batch_size)\n     ]\n \n+\n def create_random_text_data(\n-        path,\n-        min_length = 75,\n-        max_length = 75,\n-        batches = 1,\n-        batch_size = 1\n+    path, min_length=75, max_length=75, batches=1, batch_size=1\n ):\n     text_batches = [\n-            random_text_batch(batch_size, min_length, max_length)\n-            for _ in range(batches)\n+        random_text_batch(batch_size, min_length, max_length) for _ in range(batches)\n     ]\n     print(f\"{path}\")\n     torch.save(text_batches, path)\n \n-def create_random_image_data(path, size, batches = 1, batch_size = 1):\n-    image_batches = [\n-            random_image_batch(batch_size, size)\n-            for _ in range(batches)\n-    ]\n+\n+def create_random_image_data(path, size, batches=1, batch_size=1):\n+    image_batches = [random_image_batch(batch_size, size) for _ in range(batches)]\n     print(f\"{path}\")\n     torch.save(image_batches, path)\n \n-def get_data_dirs(make_dir = True):\n-    data_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data')\n-    input_dir = os.path.join(data_dir, 'input')\n-    output_dir = os.path.join(data_dir, 'output')\n+\n+def get_data_dirs(make_dir=True):\n+    data_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"data\")\n+    input_dir = os.path.join(data_dir, \"input\")\n+    output_dir = os.path.join(data_dir, \"output\")\n     if make_dir:\n-        os.makedirs(input_dir, exist_ok = True)\n-        os.makedirs(output_dir, exist_ok = True)\n+        os.makedirs(input_dir, exist_ok=True)\n+        os.makedirs(output_dir, exist_ok=True)\n     assert os.path.isdir(data_dir), f\"data directory missing, expected at {input_dir}\"\n     assert os.path.isdir(data_dir), f\"data directory missing, expected at {output_dir}\"\n     return input_dir, output_dir\n \n+\n def create_test_data_for_model(\n-        model_name,\n-        pretrained = None,\n-        precision = 'fp32',\n-        jit = False,\n-        pretrained_hf = False,\n-        force_quick_gelu = False,\n-        create_missing_input_data = True,\n-        batches = 1,\n-        batch_size = 1,\n-        overwrite = False\n+    model_name,\n+    pretrained=None,\n+    precision=\"fp32\",\n+    jit=False,\n+    pretrained_hf=False,\n+    force_quick_gelu=False,\n+    create_missing_input_data=True,\n+    batches=1,\n+    batch_size=1,\n+    overwrite=False,\n ):\n-    model_id = f'{model_name}_{pretrained or pretrained_hf}_{precision}'\n+    model_id = f\"{model_name}_{pretrained or pretrained_hf}_{precision}\"\n     input_dir, output_dir = get_data_dirs()\n-    output_file_text = os.path.join(output_dir, f'{model_id}_random_text.pt')\n-    output_file_image = os.path.join(output_dir, f'{model_id}_random_image.pt')\n+    output_file_text = os.path.join(output_dir, f\"{model_id}_random_text.pt\")\n+    output_file_image = os.path.join(output_dir, f\"{model_id}_random_image.pt\")\n     text_exists = os.path.exists(output_file_text)\n     image_exists = os.path.exists(output_file_image)\n     if not overwrite and text_exists and image_exists:\n         return\n     seed_all()\n     model, _, preprocess_val = open_clip.create_model_and_transforms(\n-            model_name,\n-            pretrained = pretrained,\n-            precision = precision,\n-            jit = jit,\n-            force_quick_gelu = force_quick_gelu,\n-            pretrained_hf = pretrained_hf\n+        model_name,\n+        pretrained=pretrained,\n+        precision=precision,\n+        jit=jit,\n+        force_quick_gelu=force_quick_gelu,\n+        pretrained_hf=pretrained_hf,\n     )\n     # text\n     if overwrite or not text_exists:\n-        input_file_text = os.path.join(input_dir, 'random_text.pt')\n+        input_file_text = os.path.join(input_dir, \"random_text.pt\")\n         if create_missing_input_data and not os.path.exists(input_file_text):\n             create_random_text_data(\n-                    input_file_text,\n-                    batches = batches,\n-                    batch_size = batch_size\n+                input_file_text, batches=batches, batch_size=batch_size\n             )\n-        assert os.path.isfile(input_file_text), f\"missing input data, expected at {input_file_text}\"\n+        assert os.path.isfile(input_file_text), (\n+            f\"missing input data, expected at {input_file_text}\"\n+        )\n         input_data_text = torch.load(input_file_text)\n         output_data_text = inference_text(model, model_name, input_data_text)\n         print(f\"{output_file_text}\")\n@@ -159,49 +158,52 @@ def create_test_data_for_model(\n         size = model.visual.image_size\n         if not isinstance(size, tuple):\n             size = (size, size)\n-        input_file_image = os.path.join(input_dir, f'random_image_{size[0]}_{size[1]}.pt')\n+        input_file_image = os.path.join(\n+            input_dir, f\"random_image_{size[0]}_{size[1]}.pt\"\n+        )\n         if create_missing_input_data and not os.path.exists(input_file_image):\n             create_random_image_data(\n-                    input_file_image,\n-                    size,\n-                    batches = batches,\n-                    batch_size = batch_size\n+                input_file_image, size, batches=batches, batch_size=batch_size\n             )\n-        assert os.path.isfile(input_file_image), f\"missing input data, expected at {input_file_image}\"\n+        assert os.path.isfile(input_file_image), (\n+            f\"missing input data, expected at {input_file_image}\"\n+        )\n         input_data_image = torch.load(input_file_image)\n         output_data_image = inference_image(model, preprocess_val, input_data_image)\n         print(f\"{output_file_image}\")\n         torch.save(output_data_image, output_file_image)\n \n-def create_test_data(\n-        models,\n-        batches = 1,\n-        batch_size = 1,\n-        overwrite = False\n-):\n-    models = list(set(models).difference({\n-            # not available with timm\n-            # see https://github.com/mlfoundations/open_clip/issues/219\n-            'timm-convnext_xlarge',\n-            'timm-vit_medium_patch16_gap_256'\n-    }).intersection(open_clip.list_models()))\n+\n+def create_test_data(models, batches=1, batch_size=1, overwrite=False):\n+    models = list(\n+        set(models)\n+        .difference(\n+            {\n+                # not available with timm\n+                # see https://github.com/mlfoundations/open_clip/issues/219\n+                \"timm-convnext_xlarge\",\n+                \"timm-vit_medium_patch16_gap_256\",\n+            }\n+        )\n+        .intersection(open_clip.list_models())\n+    )\n     models.sort()\n     print(f\"generating test data for:\\n{models}\")\n     for model_name in models:\n         print(model_name)\n         create_test_data_for_model(\n-                model_name,\n-                batches = batches,\n-                batch_size = batch_size,\n-                overwrite = overwrite\n+            model_name, batches=batches, batch_size=batch_size, overwrite=overwrite\n         )\n     return models\n \n+\n def _sytem_assert(string):\n     assert os.system(string) == 0\n \n+\n class TestWrapper(torch.nn.Module):\n     output_dict: torch.jit.Final[bool]\n+\n     def __init__(self, model, model_name, output_dict=True) -> None:\n         super().__init__()\n         self.model = model\n@@ -213,115 +215,118 @@ class TestWrapper(torch.nn.Module):\n \n     def forward(self, image, text):\n         x = self.model(image, text)\n-        x = x['image_features'] if self.output_dict else x[0]\n+        x = x[\"image_features\"] if self.output_dict else x[0]\n         assert x is not None  # remove Optional[], type refinement for torchscript\n         out = self.head(x)\n         return {\"test_output\": out}\n \n+\n def main(args):\n     global open_clip\n     import importlib\n     import shutil\n     import subprocess\n     import argparse\n-    parser = argparse.ArgumentParser(description = \"Populate test data directory\")\n+\n+    parser = argparse.ArgumentParser(description=\"Populate test data directory\")\n     parser.add_argument(\n-        '-a', '--all',\n-        action = 'store_true',\n-        help = \"create test data for all models\"\n+        \"-a\", \"--all\", action=\"store_true\", help=\"create test data for all models\"\n     )\n     parser.add_argument(\n-        '-m', '--model',\n-        type = str,\n-        default = [],\n-        nargs = '+',\n-        help = \"model(s) to create test data for\"\n+        \"-m\",\n+        \"--model\",\n+        type=str,\n+        default=[],\n+        nargs=\"+\",\n+        help=\"model(s) to create test data for\",\n     )\n     parser.add_argument(\n-        '-f', '--model_list',\n-        type = str,\n-        help = \"path to a text file containing a list of model names, one model per line\"\n+        \"-f\",\n+        \"--model_list\",\n+        type=str,\n+        help=\"path to a text file containing a list of model names, one model per line\",\n     )\n     parser.add_argument(\n-        '-s', '--save_model_list',\n-        type = str,\n-        help = \"path to save the list of models that data was generated for\"\n+        \"-s\",\n+        \"--save_model_list\",\n+        type=str,\n+        help=\"path to save the list of models that data was generated for\",\n     )\n     parser.add_argument(\n-        '-g', '--git_revision',\n-        type = str,\n-        help = \"git revision to generate test data for\"\n+        \"-g\", \"--git_revision\", type=str, help=\"git revision to generate test data for\"\n     )\n     parser.add_argument(\n-        '--overwrite',\n-        action = 'store_true',\n-        help = \"overwrite existing output data\"\n+        \"--overwrite\", action=\"store_true\", help=\"overwrite existing output data\"\n     )\n     parser.add_argument(\n-        '-n', '--num_batches',\n-        default = 1,\n-        type = int,\n-        help = \"amount of data batches to create (default: 1)\"\n+        \"-n\",\n+        \"--num_batches\",\n+        default=1,\n+        type=int,\n+        help=\"amount of data batches to create (default: 1)\",\n     )\n     parser.add_argument(\n-        '-b', '--batch_size',\n-        default = 1,\n-        type = int,\n-        help = \"test data batch size (default: 1)\"\n+        \"-b\",\n+        \"--batch_size\",\n+        default=1,\n+        type=int,\n+        help=\"test data batch size (default: 1)\",\n     )\n     args = parser.parse_args(args)\n     model_list = []\n     if args.model_list is not None:\n-        with open(args.model_list, 'r') as f:\n+        with open(args.model_list, \"r\") as f:\n             model_list = f.read().splitlines()\n     if not args.all and len(args.model) < 1 and len(model_list) < 1:\n         print(\"error: at least one model name is required\")\n         parser.print_help()\n         parser.exit(1)\n     if args.git_revision is not None:\n-        stash_output = subprocess.check_output(['git', 'stash']).decode().splitlines()\n-        has_stash = len(stash_output) > 0 and stash_output[0] != 'No local changes to save'\n-        current_branch = subprocess.check_output(['git', 'branch', '--show-current'])\n+        stash_output = subprocess.check_output([\"git\", \"stash\"]).decode().splitlines()\n+        has_stash = (\n+            len(stash_output) > 0 and stash_output[0] != \"No local changes to save\"\n+        )\n+        current_branch = subprocess.check_output([\"git\", \"branch\", \"--show-current\"])\n         if len(current_branch) < 1:\n             # not on a branch -> detached head\n-            current_branch = subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n+            current_branch = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])\n         current_branch = current_branch.splitlines()[0].decode()\n         try:\n-            _sytem_assert(f'git checkout {args.git_revision}')\n+            _sytem_assert(f\"git checkout {args.git_revision}\")\n         except AssertionError as e:\n-            _sytem_assert(f'git checkout -f {current_branch}')\n+            _sytem_assert(f\"git checkout -f {current_branch}\")\n             if has_stash:\n-                os.system(f'git stash pop')\n+                os.system(\"git stash pop\")\n             raise e\n-    open_clip = importlib.import_module('open_clip')\n+    open_clip = importlib.import_module(\"open_clip\")\n     models = open_clip.list_models() if args.all else args.model + model_list\n     try:\n         models = create_test_data(\n             models,\n-            batches = args.num_batches,\n-            batch_size = args.batch_size,\n-            overwrite = args.overwrite\n+            batches=args.num_batches,\n+            batch_size=args.batch_size,\n+            overwrite=args.overwrite,\n         )\n     finally:\n         if args.git_revision is not None:\n-            test_dir = os.path.join(os.path.dirname(__file__), 'data')\n-            test_dir_ref = os.path.join(os.path.dirname(__file__), 'data_ref')\n+            test_dir = os.path.join(os.path.dirname(__file__), \"data\")\n+            test_dir_ref = os.path.join(os.path.dirname(__file__), \"data_ref\")\n             if os.path.exists(test_dir_ref):\n-                shutil.rmtree(test_dir_ref, ignore_errors = True)\n+                shutil.rmtree(test_dir_ref, ignore_errors=True)\n             if os.path.exists(test_dir):\n                 os.rename(test_dir, test_dir_ref)\n-            _sytem_assert(f'git checkout {current_branch}')\n+            _sytem_assert(f\"git checkout {current_branch}\")\n             if has_stash:\n-                os.system(f'git stash pop')\n+                os.system(\"git stash pop\")\n             os.rename(test_dir_ref, test_dir)\n     if args.save_model_list is not None:\n         print(f\"Saving model list as {args.save_model_list}\")\n-        with open(args.save_model_list, 'w') as f:\n+        with open(args.save_model_list, \"w\") as f:\n             for m in models:\n                 print(m, file=f)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     import sys\n-    main(sys.argv[1:])\n \n+    main(sys.argv[1:])\n"
    },
    {
        "id": 267,
        "sha_fail": "4b39624a941b0c6fc5e8b54d90174475dd411b5e",
        "diff": "diff --git a/scapy/base_classes.py b/scapy/base_classes.py\nindex 6940223..5ae6187 100644\n--- a/scapy/base_classes.py\n+++ b/scapy/base_classes.py\n@@ -11,7 +11,6 @@ Generators and packet meta classes.\n #  Generators  #\n ################\n \n-\n from functools import reduce\n import abc\n import operator\n@@ -24,21 +23,15 @@ import subprocess\n import types\n import warnings\n \n-import scapy\n from scapy.error import Scapy_Exception\n from scapy.consts import WINDOWS\n \n from typing import (\n     Any,\n-    Dict,\n     Generic,\n-    Iterator,\n-    List,\n     Optional,\n-    Tuple,\n     Type,\n     TypeVar,\n-    Union,\n     cast,\n     TYPE_CHECKING,\n )\n@@ -71,12 +64,16 @@ def _get_values(value):\n     return value.\n \n     \"\"\"\n-    if (isinstance(value, tuple) and (2 <= len(value) <= 3) and\n-            all(hasattr(i, \"__int__\") for i in value)):\n+    if (\n+        isinstance(value, tuple)\n+        and (2 <= len(value) <= 3)\n+        and all(hasattr(i, \"__int__\") for i in value)\n+    ):\n         # We use values[1] + 1 as stop value for (x)range to maintain\n         # the behavior of using tuples as field `values`\n-        return range(*((int(value[0]), int(value[1]) + 1) +\n-                       tuple(int(v) for v in value[2:])))\n+        return range(\n+            *((int(value[0]), int(value[1]) + 1) + tuple(int(v) for v in value[2:]))\n+        )\n     return value\n \n \n@@ -92,9 +89,10 @@ class SetGen(Gen[_T]):\n     def __iter__(self):\n         # type: () -> Iterator[Any]\n         for i in self.values:\n-            if (isinstance(i, Gen) and\n-                (self._iterpacket or not isinstance(i, BasePacket))) or (\n-                    isinstance(i, (range, types.GeneratorType))):\n+            if (\n+                isinstance(i, Gen)\n+                and (self._iterpacket or not isinstance(i, BasePacket))\n+            ) or (isinstance(i, (range, types.GeneratorType))):\n                 for j in i:\n                     yield j\n             else:\n@@ -113,6 +111,7 @@ class _ScopedIP(str):\n     \"\"\"\n     A str that also holds extra attributes.\n     \"\"\"\n+\n     __slots__ = [\"scope\"]\n \n     def __init__(self, _: str) -> None:\n@@ -142,6 +141,7 @@ def ScopedIP(net: str, scope: Optional[Any] = None) -> _ScopedIP:\n             raise Scapy_Exception(\"Scope identifier can only be present once !\")\n     if scope is not None:\n         from scapy.interfaces import resolve_iface, network_name, dev_from_index\n+\n         try:\n             iface = dev_from_index(int(scope))\n         except (ValueError, TypeError):\n@@ -178,6 +178,7 @@ class Net(Gen[str]):\n             >>> Net(\"224.0.0.1%lo\")\n             >>> Net(\"224.0.0.1\", scope=conf.iface)\n     \"\"\"\n+\n     name = \"Net\"  # type: str\n     family = socket.AF_INET  # type: int\n     max_mask = 32  # type: int\n@@ -188,33 +189,34 @@ class Net(Gen[str]):\n         try:\n             return next(\n                 addr_port[0]\n-                for family, _, _, _, addr_port in\n-                socket.getaddrinfo(name, None, cls.family)\n+                for family, _, _, _, addr_port in socket.getaddrinfo(\n+                    name, None, cls.family\n+                )\n                 if family == cls.family\n             )\n         except socket.error:\n             if re.search(\"(^|\\\\.)[0-9]+-[0-9]+($|\\\\.)\", name) is not None:\n-                raise Scapy_Exception(\"Ranges are no longer accepted in %s()\" %\n-                                      cls.__name__)\n+                raise Scapy_Exception(\n+                    \"Ranges are no longer accepted in %s()\" % cls.__name__\n+                )\n             raise\n \n     @classmethod\n     def ip2int(cls, addr):\n         # type: (str) -> int\n-        return cast(int, struct.unpack(\n-            \"!I\", socket.inet_aton(cls.name2addr(addr))\n-        )[0])\n+        return cast(int, struct.unpack(\"!I\", socket.inet_aton(cls.name2addr(addr)))[0])\n \n     @staticmethod\n     def int2ip(val):\n         # type: (int) -> str\n-        return socket.inet_ntoa(struct.pack('!I', val))\n+        return socket.inet_ntoa(struct.pack(\"!I\", val))\n \n     def __init__(self, net, stop=None, scope=None):\n         # type: (str, Optional[str], Optional[str]) -> None\n         if \"*\" in net:\n-            raise Scapy_Exception(\"Wildcards are no longer accepted in %s()\" %\n-                                  self.__class__.__name__)\n+            raise Scapy_Exception(\n+                \"Wildcards are no longer accepted in %s()\" % self.__class__.__name__\n+            )\n         self.scope = None\n         if \"%\" in net:\n             net = ScopedIP(net)\n@@ -361,14 +363,18 @@ class OID(Gen[str]):\n #  Packet abstract and base classes  #\n ######################################\n \n+\n class Packet_metaclass(type):\n-    def __new__(cls: Type[_T],\n-                name,  # type: str\n-                bases,  # type: Tuple[type, ...]\n-                dct  # type: Dict[str, Any]\n-                ):\n+    def __new__(\n+        cls: Type[_T],\n+        name,  # type: str\n+        bases,  # type: Tuple[type, ...]\n+        dct,  # type: Dict[str, Any]\n+    ):\n         # type: (...) -> Type['Packet']\n-        if \"fields_desc\" in dct:  # perform resolution of references to other packets  # noqa: E501\n+        if (\n+            \"fields_desc\" in dct\n+        ):  # perform resolution of references to other packets  # noqa: E501\n             current_fld = dct[\"fields_desc\"]  # type: List[Union[scapy.fields.Field[Any, Any], Packet_metaclass]]  # noqa: E501\n             resolved_fld = []  # type: List[scapy.fields.Field[Any, Any]]\n             for fld_or_pkt in current_fld:\n@@ -395,9 +401,7 @@ class Packet_metaclass(type):\n                         \"If you are using several ConditionalFields, have \"\n                         \"a look at MultipleTypeField instead ! This will \"\n                         \"become a SyntaxError in a future version of \"\n-                        \"Scapy !\" % (\n-                            name, f.name\n-                        )\n+                        \"Scapy !\" % (name, f.name)\n                     )\n                     warnings.warn(war_msg, SyntaxWarning)\n                 names.append(f.name)\n@@ -419,22 +423,27 @@ class Packet_metaclass(type):\n         try:\n             # Py3 only\n             import inspect\n-            dct[\"__signature__\"] = inspect.Signature([\n-                inspect.Parameter(\"_pkt\", inspect.Parameter.POSITIONAL_ONLY),\n-            ] + [\n-                inspect.Parameter(f.name,\n-                                  inspect.Parameter.KEYWORD_ONLY,\n-                                  default=f.default)\n-                for f in dct[\"fields_desc\"]\n-            ])\n+\n+            dct[\"__signature__\"] = inspect.Signature(\n+                [\n+                    inspect.Parameter(\"_pkt\", inspect.Parameter.POSITIONAL_ONLY),\n+                ]\n+                + [\n+                    inspect.Parameter(\n+                        f.name, inspect.Parameter.KEYWORD_ONLY, default=f.default\n+                    )\n+                    for f in dct[\"fields_desc\"]\n+                ]\n+            )\n         except (ImportError, AttributeError, KeyError):\n             pass\n-        newcls = cast(Type['Packet'], type.__new__(cls, name, bases, dct))\n+        newcls = cast(Type[\"Packet\"], type.__new__(cls, name, bases, dct))\n         # Note: below can't be typed because we use attributes\n         # created dynamically..\n         newcls.__all_slots__ = set(  # type: ignore\n             attr\n-            for cls in newcls.__mro__ if hasattr(cls, \"__slots__\")\n+            for cls in newcls.__mro__\n+            if hasattr(cls, \"__slots__\")\n             for attr in cls.__slots__\n         )\n \n@@ -449,6 +458,7 @@ class Packet_metaclass(type):\n                 _f.register_owner(newcls)\n         if newcls.__name__[0] != \"_\":\n             from scapy import config\n+\n             config.conf.layers.register(newcls)\n         return newcls\n \n@@ -459,16 +469,18 @@ class Packet_metaclass(type):\n                 return k\n         raise AttributeError(attr)\n \n-    def __call__(cls,\n-                 *args,  # type: Any\n-                 **kargs  # type: Any\n-                 ):\n+    def __call__(\n+        cls,\n+        *args,  # type: Any\n+        **kargs,  # type: Any\n+    ):\n         # type: (...) -> 'Packet'\n         if \"dispatch_hook\" in cls.__dict__:\n             try:\n                 cls = cls.dispatch_hook(*args, **kargs)\n             except Exception:\n                 from scapy import config\n+\n                 if config.conf.debug_dissector:\n                     raise\n                 cls = config.conf.raw_layer\n@@ -476,7 +488,7 @@ class Packet_metaclass(type):\n             cls,  # type: ignore\n             cls.__name__,\n             cls.__bases__,\n-            cls.__dict__  # type: ignore\n+            cls.__dict__,  # type: ignore\n         )\n         i.__init__(*args, **kargs)\n         return i  # type: ignore\n@@ -484,12 +496,14 @@ class Packet_metaclass(type):\n \n # Note: see compat.py for an explanation\n \n+\n class Field_metaclass(type):\n-    def __new__(cls: Type[_T],\n-                name,  # type: str\n-                bases,  # type: Tuple[type, ...]\n-                dct  # type: Dict[str, Any]\n-                ):\n+    def __new__(\n+        cls: Type[_T],\n+        name,  # type: str\n+        bases,  # type: Tuple[type, ...]\n+        dct,  # type: Dict[str, Any]\n+    ):\n         # type: (...) -> Type[_T]\n         dct.setdefault(\"__slots__\", [])\n         newcls = type.__new__(cls, name, bases, dct)\n@@ -499,7 +513,7 @@ class Field_metaclass(type):\n PacketList_metaclass = Field_metaclass\n \n \n-class BasePacket(Gen['Packet']):\n+class BasePacket(Gen[\"Packet\"]):\n     __slots__ = []  # type: List[str]\n \n \n@@ -507,6 +521,7 @@ class BasePacket(Gen['Packet']):\n #  Packet list base class   #\n #############################\n \n+\n class BasePacketList(Gen[_T]):\n     __slots__ = []  # type: List[str]\n \n@@ -529,6 +544,7 @@ class _CanvasDumpExtended(object):\n         \"\"\"\n         from scapy.config import conf\n         from scapy.utils import get_temp_file, ContextManagerSubprocess\n+\n         canvas = self.canvas_dump(**kargs)\n         if filename is None:\n             fname = get_temp_file(autoext=kargs.get(\"suffix\", \".eps\"))\n@@ -554,6 +570,7 @@ class _CanvasDumpExtended(object):\n         \"\"\"\n         from scapy.config import conf\n         from scapy.utils import get_temp_file, ContextManagerSubprocess\n+\n         canvas = self.canvas_dump(**kargs)\n         if filename is None:\n             fname = get_temp_file(autoext=kargs.get(\"suffix\", \".pdf\"))\n@@ -579,6 +596,7 @@ class _CanvasDumpExtended(object):\n         \"\"\"\n         from scapy.config import conf\n         from scapy.utils import get_temp_file, ContextManagerSubprocess\n+\n         canvas = self.canvas_dump(**kargs)\n         if filename is None:\n             fname = get_temp_file(autoext=kargs.get(\"suffix\", \".svg\"))\ndiff --git a/scapy/config.py b/scapy/config.py\nindex be3cb25..4415286 100755\n--- a/scapy/config.py\n+++ b/scapy/config.py\n@@ -43,25 +43,15 @@ from typing import (\n     Any,\n     Callable,\n     Dict,\n-    Iterator,\n     List,\n-    NoReturn,\n-    Optional,\n-    Set,\n-    Tuple,\n     Type,\n-    Union,\n     overload,\n     TYPE_CHECKING,\n )\n-from types import ModuleType\n-from scapy.compat import DecoratorCallable\n \n if TYPE_CHECKING:\n     # Do not import at runtime\n     import scapy.as_resolvers\n-    from scapy.modules.nmap import NmapKnowledgeBase\n-    from scapy.packet import Packet\n     from scapy.supersocket import SuperSocket  # noqa: F401\n     import scapy.asn1.asn1\n     import scapy.asn1.mib\n@@ -92,19 +82,20 @@ class ConfClass(object):\n                 r = \" \".join(r.split())\n                 wlen = 76 - max(len(i), 10)\n                 if len(r) > wlen:\n-                    r = r[:wlen - 3] + \"...\"\n+                    r = r[: wlen - 3] + \"...\"\n                 s += \"%-10s = %s\\n\" % (i, r)\n         return s[:-1]\n \n \n class Interceptor(object):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Any\n-                 hook,  # type: Callable[..., Any]\n-                 args=None,  # type: Optional[List[Any]]\n-                 kargs=None  # type: Optional[Dict[str, Any]]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Any\n+        hook,  # type: Callable[..., Any]\n+        args=None,  # type: Optional[List[Any]]\n+        kargs=None,  # type: Optional[Dict[str, Any]]\n+    ):\n         # type: (...) -> None\n         self.name = name\n         self.intname = \"_intercepted_%s\" % name\n@@ -140,8 +131,7 @@ def _readonly(name):\n \n \n ReadOnlyAttribute = functools.partial(\n-    Interceptor,\n-    hook=(lambda name, *args, **kwargs: _readonly(name))\n+    Interceptor, hook=(lambda name, *args, **kwargs: _readonly(name))\n )\n ReadOnlyAttribute.__doc__ = \"Read-only class attribute\"\n \n@@ -199,7 +189,10 @@ class ConfigFieldList:\n \n     def __repr__(self):\n         # type: () -> str\n-        return \"<%s [%s]>\" % (self.__class__.__name__, \" \".join(str(x) for x in self.fields))  # noqa: E501\n+        return \"<%s [%s]>\" % (\n+            self.__class__.__name__,\n+            \" \".join(str(x) for x in self.fields),\n+        )  # noqa: E501\n \n \n class Emphasize(ConfigFieldList):\n@@ -253,10 +246,11 @@ class Num2Layer:\n         else:\n             return item in self.layer2num\n \n-    def get(self,\n-            item,  # type: Union[int, Type[Packet]]\n-            default=None,  # type: Optional[Type[Packet]]\n-            ):\n+    def get(\n+        self,\n+        item,  # type: Union[int, Type[Packet]]\n+        default=None,  # type: Optional[Type[Packet]]\n+    ):\n         # type: (...) -> Optional[Union[int, Type[Packet]]]\n         return self[item] if item in self else default\n \n@@ -268,17 +262,19 @@ class Num2Layer:\n                 dir = \"<->\"\n             else:\n                 dir = \" ->\"\n-            lst.append((num, \"%#6x %s %-20s (%s)\" % (num, dir, layer.__name__,\n-                                                     layer._name)))\n+            lst.append(\n+                (num, \"%#6x %s %-20s (%s)\" % (num, dir, layer.__name__, layer._name))\n+            )\n         for layer, num in self.layer2num.items():\n             if num not in self.num2layer or self.num2layer[num] != layer:\n-                lst.append((num, \"%#6x <-  %-20s (%s)\" % (num, layer.__name__,\n-                                                          layer._name)))\n+                lst.append(\n+                    (num, \"%#6x <-  %-20s (%s)\" % (num, layer.__name__, layer._name))\n+                )\n         lst.sort()\n         return \"\\n\".join(y for x, y in lst)\n \n \n-class LayersList(List[Type['scapy.packet.Packet']]):\n+class LayersList(List[Type[\"scapy.packet.Packet\"]]):\n     def __init__(self):\n         # type: () -> None\n         list.__init__(self)\n@@ -288,8 +284,7 @@ class LayersList(List[Type['scapy.packet.Packet']]):\n \n     def __repr__(self):\n         # type: () -> str\n-        return \"\\n\".join(\"%-20s: %s\" % (layer.__name__, layer.name)\n-                         for layer in self)\n+        return \"\\n\".join(\"%-20s: %s\" % (layer.__name__, layer.name) for layer in self)\n \n     def register(self, layer):\n         # type: (Type[Packet]) -> None\n@@ -309,6 +304,7 @@ class LayersList(List[Type['scapy.packet.Packet']]):\n         result = []\n         # This import may feel useless, but it is required for the eval below\n         import scapy  # noqa: F401\n+\n         try:\n             import builtins  # noqa: F401\n         except ImportError:\n@@ -327,9 +323,7 @@ class LayersList(List[Type['scapy.packet.Packet']]):\n             for cls in lay:\n                 if cls not in self._backup_dict:\n                     self._backup_dict[cls] = cls.payload_guess[:]\n-                    cls.payload_guess = [\n-                        y for y in cls.payload_guess if y[1] in items\n-                    ]\n+                    cls.payload_guess = [y for y in cls.payload_guess if y[1] in items]\n         self.filtered = True\n \n     def unfilter(self):\n@@ -350,7 +344,7 @@ class CommandsList(List[Callable[..., Any]]):\n         s = []\n         for li in sorted(self, key=lambda x: x.__name__):\n             doc = li.__doc__ if li.__doc__ else \"--\"\n-            doc = doc.lstrip().split('\\n', 1)[0]\n+            doc = doc.lstrip().split(\"\\n\", 1)[0]\n             s.append(\"%-22s: %s\" % (li.__name__, doc))\n         return \"\\n\".join(s)\n \n@@ -413,10 +407,11 @@ class CacheInstance(Dict[str, Any]):\n         self._timetable[item] = time.time()\n         super(CacheInstance, self).__setitem__(item, v)\n \n-    def update(self,\n-               other,  # type: Any\n-               **kwargs  # type: Any\n-               ):\n+    def update(\n+        self,\n+        other,  # type: Any\n+        **kwargs,  # type: Any\n+    ):\n         # type: (...) -> None\n         for key, value in other.items():\n             # We only update an element from `other` either if it does\n@@ -548,12 +543,12 @@ class ScapyExt:\n     @dataclass\n     class ScapyExtSpec:\n         fullname: str\n-        mode: 'ScapyExt.MODE'\n+        mode: \"ScapyExt.MODE\"\n         spec: Any\n         default: bool\n \n     def __init__(self):\n-        self.specs: Dict[str, 'ScapyExt.ScapyExtSpec'] = {}\n+        self.specs: Dict[str, \"ScapyExt.ScapyExtSpec\"] = {}\n         self.bash_completions = {}\n \n     def config(self, name, version):\n@@ -649,7 +644,7 @@ class ExtsManager(importlib.abc.MetaPathFinder):\n             return\n \n         # Check the classifiers\n-        if distr.metadata.get('License-Expression', None) not in self.GPLV2_LICENCES:\n+        if distr.metadata.get(\"License-Expression\", None) not in self.GPLV2_LICENCES:\n             log_loading.warning(\n                 \"'%s' has no GPLv2 classifier therefore cannot be loaded.\" % extension\n             )\n@@ -677,10 +672,7 @@ class ExtsManager(importlib.abc.MetaPathFinder):\n                 scapy_ext_func(ext)\n             except Exception as ex:\n                 log_loading.warning(\n-                    \"'%s' failed during initialization with %s\" % (\n-                        extension,\n-                        ex\n-                    )\n+                    \"'%s' failed during initialization with %s\" % (extension, ex)\n                 )\n                 return\n \n@@ -708,6 +700,7 @@ class ExtsManager(importlib.abc.MetaPathFinder):\n \n     def __repr__(self):\n         from scapy.utils import pretty_list\n+\n         return pretty_list(\n             [\n                 (x.name, x.version, [y.fullname for y in x.specs.values()])\n@@ -727,11 +720,8 @@ def _version_checker(module, minver):\n      - minver: a tuple of versions\n     \"\"\"\n     # We could use LooseVersion, but distutils imports imp which is deprecated\n-    version_regexp = r'[a-z]?((?:\\d|\\.)+\\d+)(?:\\.dev[0-9]+)?'\n-    version_tags_r = re.match(\n-        version_regexp,\n-        getattr(module, \"__version__\", \"\")\n-    )\n+    version_regexp = r\"[a-z]?((?:\\d|\\.)+\\d+)(?:\\.dev[0-9]+)?\"\n+    version_tags_r = re.match(version_regexp, getattr(module, \"__version__\", \"\"))\n     if not version_tags_r:\n         return False\n     version_tags_i = version_tags_r.group(1).split(\".\")\n@@ -764,6 +754,7 @@ def isCryptographyAdvanced():\n     \"\"\"\n     try:\n         from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey  # noqa: E501\n+\n         X25519PrivateKey.generate()\n     except Exception:\n         return False\n@@ -776,6 +767,7 @@ def isPyPy():\n     \"\"\"Returns either scapy is running under PyPy or not\"\"\"\n     try:\n         import __pypy__  # noqa: F401\n+\n         return True\n     except ImportError:\n         return False\n@@ -808,52 +800,54 @@ def _set_conf_sockets():\n         raise ScapyInvalidPlatformException(\"BSD-like (OSX, *BSD...) only !\")\n     if not conf.use_pcap and SOLARIS:\n         Interceptor.set_from_hook(conf, \"use_pcap\", True)\n-        raise ScapyInvalidPlatformException(\n-            \"Scapy only supports libpcap on Solaris !\"\n-        )\n+        raise ScapyInvalidPlatformException(\"Scapy only supports libpcap on Solaris !\")\n     # we are already in an Interceptor hook, use Interceptor.set_from_hook\n     if conf.use_pcap:\n         try:\n-            from scapy.arch.libpcap import L2pcapListenSocket, L2pcapSocket, \\\n-                L3pcapSocket\n+            from scapy.arch.libpcap import (\n+                L2pcapListenSocket,\n+                L2pcapSocket,\n+                L3pcapSocket,\n+            )\n         except (OSError, ImportError):\n             log_loading.warning(\"No libpcap provider available ! pcap won't be used\")\n             Interceptor.set_from_hook(conf, \"use_pcap\", False)\n         else:\n             conf.L3socket = L3pcapSocket\n-            conf.L3socket6 = functools.partial(\n-                L3pcapSocket, filter=\"ip6\")\n+            conf.L3socket6 = functools.partial(L3pcapSocket, filter=\"ip6\")\n             conf.L2socket = L2pcapSocket\n             conf.L2listen = L2pcapListenSocket\n     elif conf.use_bpf:\n-        from scapy.arch.bpf.supersocket import L2bpfListenSocket, \\\n-            L2bpfSocket, L3bpfSocket\n+        from scapy.arch.bpf.supersocket import (\n+            L2bpfListenSocket,\n+            L2bpfSocket,\n+            L3bpfSocket,\n+        )\n+\n         conf.L3socket = L3bpfSocket\n-        conf.L3socket6 = functools.partial(\n-            L3bpfSocket, filter=\"ip6\")\n+        conf.L3socket6 = functools.partial(L3bpfSocket, filter=\"ip6\")\n         conf.L2socket = L2bpfSocket\n         conf.L2listen = L2bpfListenSocket\n     elif LINUX:\n         from scapy.arch.linux import L3PacketSocket, L2Socket, L2ListenSocket\n+\n         conf.L3socket = L3PacketSocket\n         conf.L3socket6 = cast(\n-            \"Type[SuperSocket]\",\n-            functools.partial(\n-                L3PacketSocket,\n-                filter=\"ip6\"\n-            )\n+            \"Type[SuperSocket]\", functools.partial(L3PacketSocket, filter=\"ip6\")\n         )\n         conf.L2socket = L2Socket\n         conf.L2listen = L2ListenSocket\n     elif WINDOWS:\n         from scapy.arch.windows import _NotAvailableSocket\n         from scapy.arch.windows.native import L3WinSocket, L3WinSocket6\n+\n         conf.L3socket = L3WinSocket\n         conf.L3socket6 = L3WinSocket6\n         conf.L2socket = _NotAvailableSocket\n         conf.L2listen = _NotAvailableSocket\n     else:\n         from scapy.supersocket import L3RawSocket, L3RawSocket6\n+\n         conf.L3socket = L3RawSocket\n         conf.L3socket6 = L3RawSocket6\n     # Reload the interfaces\n@@ -896,6 +890,7 @@ def _iface_changer(attr, val, old):\n     \"\"\"Resolves the interface in conf.iface\"\"\"\n     if isinstance(val, str):\n         from scapy.interfaces import resolve_iface\n+\n         iface = resolve_iface(val)\n         if old and iface.dummy:\n             warning(\n@@ -917,6 +912,7 @@ class Conf(ConfClass):\n     \"\"\"\n     This object contains the configuration of Scapy.\n     \"\"\"\n+\n     version: str = ReadOnlyAttribute(\"version\", VERSION)\n     session: str = \"\"  #: filename where the session will be saved\n     interactive = False\n@@ -977,11 +973,8 @@ class Conf(ConfClass):\n     bufsize = 2**16\n     #: history file\n     histfile: str = os.getenv(\n-        'SCAPY_HISTFILE',\n-        os.path.join(\n-            os.path.expanduser(\"~\"),\n-            \".config\", \"scapy\", \"history\"\n-        )\n+        \"SCAPY_HISTFILE\",\n+        os.path.join(os.path.expanduser(\"~\"), \".config\", \"scapy\", \"history\"),\n     )\n     #: includes padding in disassembled packets\n     padding = 1\n@@ -1046,7 +1039,7 @@ class Conf(ConfClass):\n     use_pcap: bool = Interceptor(\n         \"use_pcap\",\n         os.getenv(\"SCAPY_USE_LIBPCAP\", \"\").lower().startswith(\"y\"),\n-        _socket_changer\n+        _socket_changer,\n     )\n     use_bpf: bool = Interceptor(\"use_bpf\", False, _socket_changer)\n     use_npcap = False\n@@ -1061,57 +1054,57 @@ class Conf(ConfClass):\n     load_extensions: List[str] = []\n     # can, tls, http and a few others are not loaded by default\n     load_layers: List[str] = [\n-        'bluetooth',\n-        'bluetooth4LE',\n-        'dcerpc',\n-        'dhcp',\n-        'dhcp6',\n-        'dns',\n-        'dot11',\n-        'dot15d4',\n-        'eap',\n-        'gprs',\n-        'gssapi',\n-        'hsrp',\n-        'inet',\n-        'inet6',\n-        'ipsec',\n-        'ir',\n-        'isakmp',\n-        'kerberos',\n-        'l2',\n-        'l2tp',\n-        'ldap',\n-        'llmnr',\n-        'lltd',\n-        'mgcp',\n-        'msrpce.rpcclient',\n-        'msrpce.rpcserver',\n-        'mobileip',\n-        'netbios',\n-        'netflow',\n-        'ntlm',\n-        'ntp',\n-        'ppi',\n-        'ppp',\n-        'pptp',\n-        'radius',\n-        'rip',\n-        'rtp',\n-        'sctp',\n-        'sixlowpan',\n-        'skinny',\n-        'smb',\n-        'smb2',\n-        'smbclient',\n-        'smbserver',\n-        'snmp',\n-        'spnego',\n-        'tftp',\n-        'vrrp',\n-        'vxlan',\n-        'x509',\n-        'zigbee'\n+        \"bluetooth\",\n+        \"bluetooth4LE\",\n+        \"dcerpc\",\n+        \"dhcp\",\n+        \"dhcp6\",\n+        \"dns\",\n+        \"dot11\",\n+        \"dot15d4\",\n+        \"eap\",\n+        \"gprs\",\n+        \"gssapi\",\n+        \"hsrp\",\n+        \"inet\",\n+        \"inet6\",\n+        \"ipsec\",\n+        \"ir\",\n+        \"isakmp\",\n+        \"kerberos\",\n+        \"l2\",\n+        \"l2tp\",\n+        \"ldap\",\n+        \"llmnr\",\n+        \"lltd\",\n+        \"mgcp\",\n+        \"msrpce.rpcclient\",\n+        \"msrpce.rpcserver\",\n+        \"mobileip\",\n+        \"netbios\",\n+        \"netflow\",\n+        \"ntlm\",\n+        \"ntp\",\n+        \"ppi\",\n+        \"ppp\",\n+        \"pptp\",\n+        \"radius\",\n+        \"rip\",\n+        \"rtp\",\n+        \"sctp\",\n+        \"sixlowpan\",\n+        \"skinny\",\n+        \"smb\",\n+        \"smb2\",\n+        \"smbclient\",\n+        \"smbserver\",\n+        \"snmp\",\n+        \"spnego\",\n+        \"tftp\",\n+        \"vrrp\",\n+        \"vxlan\",\n+        \"x509\",\n+        \"zigbee\",\n     ]\n     #: a dict which can be used by contrib layers to store local\n     #: configuration\n@@ -1139,11 +1132,7 @@ class Conf(ConfClass):\n     #: When the TLS module is loaded (not by default), the following turns on sessions\n     tls_session_enable = False\n     #: Filename containing NSS Keys Log\n-    tls_nss_filename = Interceptor(\n-        \"tls_nss_filename\",\n-        None,\n-        _reset_tls_nss_keys\n-    )\n+    tls_nss_filename = Interceptor(\"tls_nss_filename\", None, _reset_tls_nss_keys)\n     #: Dictionary containing parsed NSS Keys\n     tls_nss_keys: Dict[str, bytes] = None\n     #: Whether to use NDR64 by default instead of NDR 32\n@@ -1165,26 +1154,31 @@ class Conf(ConfClass):\n         # Those are loaded on runtime to avoid import loops\n         if attr == \"manufdb\":\n             from scapy.data import MANUFDB\n+\n             return MANUFDB\n         if attr == \"ethertypes\":\n             from scapy.data import ETHER_TYPES\n+\n             return ETHER_TYPES\n         if attr == \"protocols\":\n             from scapy.data import IP_PROTOS\n+\n             return IP_PROTOS\n         if attr == \"services_udp\":\n             from scapy.data import UDP_SERVICES\n+\n             return UDP_SERVICES\n         if attr == \"services_tcp\":\n             from scapy.data import TCP_SERVICES\n+\n             return TCP_SERVICES\n         if attr == \"services_sctp\":\n             from scapy.data import SCTP_SERVICES\n+\n             return SCTP_SERVICES\n         if attr == \"iface6\":\n             warnings.warn(\n-                \"conf.iface6 is deprecated in favor of conf.iface\",\n-                DeprecationWarning\n+                \"conf.iface6 is deprecated in favor of conf.iface\", DeprecationWarning\n             )\n             attr = \"iface\"\n         return object.__getattribute__(self, attr)\n@@ -1205,12 +1199,16 @@ def crypto_validator(func):\n     This a decorator to be used for any method relying on the cryptography library.  # noqa: E501\n     Its behaviour depends on the 'crypto_valid' attribute of the global 'conf'.\n     \"\"\"\n+\n     def func_in(*args, **kwargs):\n         # type: (*Any, **Any) -> Any\n         if not conf.crypto_valid:\n-            raise ImportError(\"Cannot execute crypto-related method! \"\n-                              \"Please install python-cryptography v1.7 or later.\")  # noqa: E501\n+            raise ImportError(\n+                \"Cannot execute crypto-related method! \"\n+                \"Please install python-cryptography v1.7 or later.\"\n+            )  # noqa: E501\n         return func(*args, **kwargs)\n+\n     return func_in\n \n \ndiff --git a/scapy/fields.py b/scapy/fields.py\nindex 46064b7..b36a15c 100644\n--- a/scapy/fields.py\n+++ b/scapy/fields.py\n@@ -26,17 +26,39 @@ from enum import Enum\n \n from scapy.config import conf\n from scapy.dadict import DADict\n-from scapy.volatile import RandBin, RandByte, RandEnumKeys, RandInt, \\\n-    RandIP, RandIP6, RandLong, RandMAC, RandNum, RandShort, RandSInt, \\\n-    RandSByte, RandTermString, RandUUID, VolatileValue, RandSShort, \\\n-    RandSLong, RandFloat\n+from scapy.volatile import (\n+    RandBin,\n+    RandByte,\n+    RandEnumKeys,\n+    RandInt,\n+    RandIP,\n+    RandIP6,\n+    RandLong,\n+    RandMAC,\n+    RandNum,\n+    RandShort,\n+    RandSInt,\n+    RandSByte,\n+    RandTermString,\n+    RandUUID,\n+    VolatileValue,\n+    RandSShort,\n+    RandSLong,\n+    RandFloat,\n+)\n from scapy.data import EPOCH\n from scapy.error import log_runtime, Scapy_Exception\n from scapy.compat import bytes_hex, plain_str, raw, bytes_encode\n from scapy.pton_ntop import inet_ntop, inet_pton\n from scapy.utils import inet_aton, inet_ntoa, lhex, mac2str, str2mac, EDecimal\n-from scapy.utils6 import in6_6to4ExtractAddr, in6_isaddr6to4, \\\n-    in6_isaddrTeredo, in6_ptop, Net6, teredoAddrExtractInfo\n+from scapy.utils6 import (\n+    in6_6to4ExtractAddr,\n+    in6_isaddr6to4,\n+    in6_isaddrTeredo,\n+    in6_ptop,\n+    Net6,\n+    teredoAddrExtractInfo,\n+)\n from scapy.base_classes import (\n     _ScopedIP,\n     BasePacket,\n@@ -48,7 +70,6 @@ from scapy.base_classes import (\n # Typing imports\n from typing import (\n     Any,\n-    AnyStr,\n     Callable,\n     Dict,\n     List,\n@@ -138,8 +159,8 @@ class ObservableDict(Dict[int, str]):\n #  Fields  #\n ############\n \n-I = TypeVar('I')  # Internal storage  # noqa: E741\n-M = TypeVar('M')  # Machine storage\n+I = TypeVar(\"I\")  # Internal storage  # noqa: E741\n+M = TypeVar(\"M\")  # Machine storage\n \n \n class Field(Generic[I, M], metaclass=Field_metaclass):\n@@ -149,14 +170,8 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n \n     https://scapy.readthedocs.io/en/stable/build_dissect.html\n     \"\"\"\n-    __slots__ = [\n-        \"name\",\n-        \"fmt\",\n-        \"default\",\n-        \"sz\",\n-        \"owners\",\n-        \"struct\"\n-    ]\n+\n+    __slots__ = [\"name\", \"fmt\", \"default\", \"sz\", \"owners\", \"struct\"]\n     islist = 0\n     ismutable = False\n     holds_packets = 0\n@@ -179,10 +194,11 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n         # type: (Type[Packet]) -> None\n         self.owners.append(cls)\n \n-    def i2len(self,\n-              pkt,  # type: Packet\n-              x,  # type: Any\n-              ):\n+    def i2len(\n+        self,\n+        pkt,  # type: Packet\n+        x,  # type: Any\n+    ):\n         # type: (...) -> int\n         \"\"\"Convert internal value to a length usable by a FieldLenField\"\"\"\n         if isinstance(x, RawVal):\n@@ -240,10 +256,10 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n             return s + self.struct.pack(self.i2m(pkt, val))\n         except struct.error as ex:\n             raise ValueError(\n-                \"Incorrect type of value for field %s:\\n\" % self.name +\n-                \"struct.error('%s')\\n\" % ex +\n-                \"To inject bytes into the field regardless of the type, \" +\n-                \"use RawVal. See help(RawVal)\"\n+                \"Incorrect type of value for field %s:\\n\" % self.name\n+                + \"struct.error('%s')\\n\" % ex\n+                + \"To inject bytes into the field regardless of the type, \"\n+                + \"use RawVal. See help(RawVal)\"\n             )\n \n     def getfield(self, pkt, s):\n@@ -257,7 +273,7 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n         first the raw packet string after having removed the extracted field,\n         second the extracted field itself in internal representation.\n         \"\"\"\n-        return s[self.sz:], self.m2i(pkt, self.struct.unpack(s[:self.sz])[0])\n+        return s[self.sz :], self.m2i(pkt, self.struct.unpack(s[: self.sz])[0])\n \n     def do_copy(self, x):\n         # type: (I) -> I\n@@ -276,7 +292,7 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n         return \"<%s (%s).%s>\" % (\n             self.__class__.__name__,\n             \",\".join(x.__name__ for x in self.owners),\n-            self.name\n+            self.name,\n         )\n \n     def copy(self):\n@@ -288,10 +304,16 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n         \"\"\"Return a volatile object whose value is both random and suitable for this field\"\"\"  # noqa: E501\n         fmtt = self.fmt[-1]\n         if fmtt in \"BbHhIiQq\":\n-            return {\"B\": RandByte, \"b\": RandSByte,\n-                    \"H\": RandShort, \"h\": RandSShort,\n-                    \"I\": RandInt, \"i\": RandSInt,\n-                    \"Q\": RandLong, \"q\": RandSLong}[fmtt]()\n+            return {\n+                \"B\": RandByte,\n+                \"b\": RandSByte,\n+                \"H\": RandShort,\n+                \"h\": RandSShort,\n+                \"I\": RandInt,\n+                \"i\": RandSInt,\n+                \"Q\": RandLong,\n+                \"q\": RandSLong,\n+            }[fmtt]()\n         elif fmtt == \"s\":\n             if self.fmt[0] in \"0123456789\":\n                 value = int(self.fmt[:-1])\n@@ -300,9 +322,7 @@ class Field(Generic[I, M], metaclass=Field_metaclass):\n             return RandBin(value)\n         else:\n             raise ValueError(\n-                \"no random class for [%s] (fmt=%s).\" % (\n-                    self.name, self.fmt\n-                )\n+                \"no random class for [%s] (fmt=%s).\" % (self.name, self.fmt)\n             )\n \n \n@@ -310,6 +330,7 @@ class _FieldContainer(object):\n     \"\"\"\n     A field that acts as a container for another field\n     \"\"\"\n+\n     __slots__ = [\"fld\"]\n \n     def __getattr__(self, attr):\n@@ -322,6 +343,7 @@ AnyField = Union[Field[Any, Any], _FieldContainer]\n \n class Emph(_FieldContainer):\n     \"\"\"Empathize sub-layer for display\"\"\"\n+\n     __slots__ = [\"fld\"]\n \n     def __init__(self, fld):\n@@ -348,6 +370,7 @@ class MayEnd(_FieldContainer):\n     Important note: any field below this one MUST default\n     to an empty value, else the behavior will be unexpected.\n     \"\"\"\n+\n     __slots__ = [\"fld\"]\n \n     def __init__(self, fld):\n@@ -381,10 +404,11 @@ class ActionField(_FieldContainer):\n class ConditionalField(_FieldContainer):\n     __slots__ = [\"fld\", \"cond\"]\n \n-    def __init__(self,\n-                 fld,  # type: AnyField\n-                 cond  # type: Callable[[Packet], bool]\n-                 ):\n+    def __init__(\n+        self,\n+        fld,  # type: AnyField\n+        cond,  # type: Callable[[Packet], bool]\n+    ):\n         # type: (...) -> None\n         self.fld = fld\n         self.cond = cond\n@@ -463,28 +487,24 @@ class MultipleTypeField(_FieldContainer):\n \n     def __init__(\n         self,\n-        flds: List[Union[\n-            Tuple[Field[Any, Any], Any, str],\n-            Tuple[Field[Any, Any], Any]\n-        ]],\n-        dflt: Field[Any, Any]\n+        flds: List[\n+            Union[Tuple[Field[Any, Any], Any, str], Tuple[Field[Any, Any], Any]]\n+        ],\n+        dflt: Field[Any, Any],\n     ) -> None:\n-        self.hints = {\n-            x[0]: x[2]\n-            for x in flds\n-            if len(x) == 3\n-        }\n-        self.flds = [\n-            (x[0], x[1]) for x in flds\n-        ]\n+        self.hints = {x[0]: x[2] for x in flds if len(x) == 3}\n+        self.flds = [(x[0], x[1]) for x in flds]\n         self.dflt = dflt\n         self.default = None  # So that we can detect changes in defaults\n         self.name = self.dflt.name\n         if any(x[0].name != self.name for x in self.flds):\n             warnings.warn(\n-                (\"All fields should have the same name in a \"\n-                 \"MultipleTypeField (%s). Use hints.\") % self.name,\n-                SyntaxWarning\n+                (\n+                    \"All fields should have the same name in a \"\n+                    \"MultipleTypeField (%s). Use hints.\"\n+                )\n+                % self.name,\n+                SyntaxWarning,\n             )\n \n     def _iterate_fields_cond(self, pkt, val, use_val):\n@@ -508,19 +528,20 @@ class MultipleTypeField(_FieldContainer):\n     def _find_fld_pkt(self, pkt):\n         # type: (Optional[Packet]) -> Field[Any, Any]\n         \"\"\"Given a Packet instance `pkt`, returns the Field subclass to be\n-used. If you know the value to be set (e.g., in .addfield()), use\n-._find_fld_pkt_val() instead.\n+        used. If you know the value to be set (e.g., in .addfield()), use\n+        ._find_fld_pkt_val() instead.\n \n         \"\"\"\n         return self._iterate_fields_cond(pkt, None, False)\n \n-    def _find_fld_pkt_val(self,\n-                          pkt,  # type: Optional[Packet]\n-                          val,  # type: Any\n-                          ):\n+    def _find_fld_pkt_val(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        val,  # type: Any\n+    ):\n         # type: (...) -> Tuple[Field[Any, Any], Any]\n         \"\"\"Given a Packet instance `pkt` and the value `val` to be set,\n-returns the Field subclass to be used, and the updated `val` if necessary.\n+        returns the Field subclass to be used, and the updated `val` if necessary.\n \n         \"\"\"\n         fld = self._iterate_fields_cond(pkt, val, True)\n@@ -531,14 +552,14 @@ returns the Field subclass to be used, and the updated `val` if necessary.\n     def _find_fld(self):\n         # type: () -> Field[Any, Any]\n         \"\"\"Returns the Field subclass to be used, depending on the Packet\n-instance, or the default subclass.\n+        instance, or the default subclass.\n \n-DEV: since the Packet instance is not provided, we have to use a hack\n-to guess it. It should only be used if you cannot provide the current\n-Packet instance (for example, because of the current Scapy API).\n+        DEV: since the Packet instance is not provided, we have to use a hack\n+        to guess it. It should only be used if you cannot provide the current\n+        Packet instance (for example, because of the current Scapy API).\n \n-If you have the current Packet instance, use ._find_fld_pkt_val() (if\n-the value to set is also known) of ._find_fld_pkt() instead.\n+        If you have the current Packet instance, use ._find_fld_pkt_val() (if\n+        the value to set is also known) of ._find_fld_pkt() instead.\n \n         \"\"\"\n         # Hack to preserve current Scapy API\n@@ -546,7 +567,7 @@ the value to set is also known) of ._find_fld_pkt() instead.\n         frame = inspect.currentframe().f_back.f_back  # type: ignore\n         while frame is not None:\n             try:\n-                pkt = frame.f_locals['self']\n+                pkt = frame.f_locals[\"self\"]\n             except KeyError:\n                 pass\n             else:\n@@ -558,10 +579,11 @@ the value to set is also known) of ._find_fld_pkt() instead.\n             frame = frame.f_back\n         return self.dflt\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Tuple[bytes, Any]\n         return self._find_fld_pkt(pkt).getfield(pkt, s)\n \n@@ -580,10 +602,11 @@ the value to set is also known) of ._find_fld_pkt() instead.\n         fld, val = self._find_fld_pkt_val(pkt, val)\n         return fld.h2i(pkt, val)\n \n-    def i2h(self,\n-            pkt,  # type: Packet\n-            val,  # type: Any\n-            ):\n+    def i2h(\n+        self,\n+        pkt,  # type: Packet\n+        val,  # type: Any\n+    ):\n         # type: (...) -> Any\n         fld, val = self._find_fld_pkt_val(pkt, val)\n         return fld.i2h(pkt, val)\n@@ -624,7 +647,8 @@ the value to set is also known) of ._find_fld_pkt() instead.\n \n class PadField(_FieldContainer):\n     \"\"\"Add bytes after the proxified field so that it ends at the specified\n-       alignment from its beginning\"\"\"\n+    alignment from its beginning\"\"\"\n+\n     __slots__ = [\"fld\", \"_align\", \"_padwith\"]\n \n     def __init__(self, fld, align, padwith=None):\n@@ -637,54 +661,54 @@ class PadField(_FieldContainer):\n         # type: (int, Packet) -> int\n         return -flen % self._align\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Tuple[bytes, Any]\n         remain, val = self.fld.getfield(pkt, s)\n         padlen = self.padlen(len(s) - len(remain), pkt)\n         return remain[padlen:], val\n \n-    def addfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 val,  # type: Any\n-                 ):\n+    def addfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+        val,  # type: Any\n+    ):\n         # type: (...) -> bytes\n         sval = self.fld.addfield(pkt, b\"\", val)\n-        return s + sval + (\n-            self.padlen(len(sval), pkt) * self._padwith\n-        )\n+        return s + sval + (self.padlen(len(sval), pkt) * self._padwith)\n \n \n class ReversePadField(PadField):\n     \"\"\"Add bytes BEFORE the proxified field so that it starts at the specified\n-       alignment from its beginning\"\"\"\n+    alignment from its beginning\"\"\"\n \n     def original_length(self, pkt):\n         # type: (Packet) -> int\n         return len(pkt.original)\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Tuple[bytes, Any]\n         # We need to get the length that has already been dissected\n         padlen = self.padlen(self.original_length(pkt) - len(s), pkt)\n         return self.fld.getfield(pkt, s[padlen:])\n \n-    def addfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 val,  # type: Any\n-                 ):\n+    def addfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+        val,  # type: Any\n+    ):\n         # type: (...) -> bytes\n         sval = self.fld.addfield(pkt, b\"\", val)\n-        return s + struct.pack(\"%is\" % (\n-            self.padlen(len(s), pkt)\n-        ), self._padwith) + sval\n+        return s + struct.pack(\"%is\" % (self.padlen(len(s), pkt)), self._padwith) + sval\n \n \n class TrailerBytes(bytes):\n@@ -714,6 +738,7 @@ class TrailerField(_FieldContainer):\n \n     Mostly used for FCS\n     \"\"\"\n+\n     __slots__ = [\"fld\"]\n \n     def __init__(self, fld):\n@@ -734,6 +759,7 @@ class TrailerField(_FieldContainer):\n             self.raw_packet_cache = None\n             self.post_dissect = previous_post_dissect  # type: ignore\n             return previous_post_dissect(s)\n+\n         pkt.post_dissect = MethodType(_post_dissect, pkt)  # type: ignore\n         s = TrailerBytes(s)\n         s, val = self.fld.getfield(pkt, s)\n@@ -749,6 +775,7 @@ class TrailerField(_FieldContainer):\n             pay += value\n             self.post_build = previous_post_build  # type: ignore\n             return previous_post_build(p, pay)\n+\n         pkt.post_build = MethodType(_post_build, pkt)  # type: ignore\n         return s\n \n@@ -780,8 +807,10 @@ class DestField(Field[str, bytes]):\n         # type: (Packet) -> str\n         for addr, condition in self.bindings.get(pkt.payload.__class__, []):\n             try:\n-                if all(pkt.payload.getfieldval(field) == value\n-                       for field, value in condition.items()):\n+                if all(\n+                    pkt.payload.getfieldval(field) == value\n+                    for field, value in condition.items()\n+                ):\n                     return addr  # type: ignore\n             except AttributeError:\n                 pass\n@@ -888,7 +917,7 @@ class IPField(Field[Union[str, Net], bytes]):\n     def i2m(self, pkt, x):\n         # type: (Optional[Packet], Optional[Union[str, Net]]) -> bytes\n         if x is None:\n-            return b'\\x00\\x00\\x00\\x00'\n+            return b\"\\x00\\x00\\x00\\x00\"\n         return inet_aton(plain_str(x))\n \n     def m2i(self, pkt, x):\n@@ -984,15 +1013,20 @@ class IP6Field(Field[Optional[Union[str, Net6]], bytes]):\n         if x is None:\n             return self.i2h(pkt, x)\n         elif not isinstance(x, Net6) and not isinstance(x, list):\n-            if in6_isaddrTeredo(x):   # print Teredo info\n+            if in6_isaddrTeredo(x):  # print Teredo info\n                 server, _, maddr, mport = teredoAddrExtractInfo(x)\n-                return \"%s [Teredo srv: %s cli: %s:%s]\" % (self.i2h(pkt, x), server, maddr, mport)  # noqa: E501\n-            elif in6_isaddr6to4(x):   # print encapsulated address\n+                return \"%s [Teredo srv: %s cli: %s:%s]\" % (\n+                    self.i2h(pkt, x),\n+                    server,\n+                    maddr,\n+                    mport,\n+                )  # noqa: E501\n+            elif in6_isaddr6to4(x):  # print encapsulated address\n                 vaddr = in6_6to4ExtractAddr(x)\n                 return \"%s [6to4 GW: %s]\" % (self.i2h(pkt, x), vaddr)\n             elif isinstance(x, _ScopedIP) and x.scope:\n                 return repr(x)\n-        r = self.i2h(pkt, x)          # No specific information to return\n+        r = self.i2h(pkt, x)  # No specific information to return\n         return r if isinstance(r, str) else repr(r)\n \n     def randval(self):\n@@ -1108,8 +1142,7 @@ class XLE3BytesField(LEThreeBytesField, XByteField):\n def LEX3BytesField(*args, **kwargs):\n     # type: (*Any, **Any) -> Any\n     warnings.warn(\n-        \"LEX3BytesField is deprecated. Use XLE3BytesField\",\n-        DeprecationWarning\n+        \"LEX3BytesField is deprecated. Use XLE3BytesField\", DeprecationWarning\n     )\n     return XLE3BytesField(*args, **kwargs)\n \n@@ -1135,13 +1168,13 @@ class NBytesField(Field[int, List[int]]):\n             return x\n         # x can be a tuple when coming from struct.unpack  (from getfield)\n         if isinstance(x, (list, tuple)):\n-            return sum(d * (256 ** i) for i, d in enumerate(reversed(x)))\n+            return sum(d * (256**i) for i, d in enumerate(reversed(x)))\n         return 0\n \n     def i2repr(self, pkt, x):\n         # type: (Optional[Packet], int) -> str\n         if isinstance(x, int):\n-            return '%i' % x\n+            return \"%i\" % x\n         return super(NBytesField, self).i2repr(pkt, x)\n \n     def addfield(self, pkt, s, val):\n@@ -1150,8 +1183,7 @@ class NBytesField(Field[int, List[int]]):\n \n     def getfield(self, pkt, s):\n         # type: (Optional[Packet], bytes) -> Tuple[bytes, int]\n-        return (s[self.sz:],\n-                self.m2i(pkt, self.struct.unpack(s[:self.sz])))  # type: ignore\n+        return (s[self.sz :], self.m2i(pkt, self.struct.unpack(s[: self.sz])))  # type: ignore\n \n     def randval(self):\n         # type: () -> RandNum\n@@ -1162,7 +1194,7 @@ class XNBytesField(NBytesField):\n     def i2repr(self, pkt, x):\n         # type: (Optional[Packet], int) -> str\n         if isinstance(x, int):\n-            return '0x%x' % x\n+            return \"0x%x\" % x\n         # x can be a tuple when coming from struct.unpack (from getfield)\n         if isinstance(x, (list, tuple)):\n             return \"0x\" + \"\".join(\"%02x\" % b for b in x)\n@@ -1241,7 +1273,8 @@ class YesNoByteField(ByteField):\n         x != 0 : 'no'\n \n     \"\"\"\n-    __slots__ = ['eval_fn']\n+\n+    __slots__ = [\"eval_fn\"]\n \n     def _build_config_representation(self, config):\n         # type: (Dict[str, Any]) -> None\n@@ -1253,35 +1286,46 @@ class YesNoByteField(ByteField):\n \n             if value_spec_type is int:\n                 if value_spec < 0 or value_spec > 255:\n-                    raise FieldValueRangeException('given field value {} invalid - '  # noqa: E501\n-                                                   'must be in range [0..255]'.format(value_spec))  # noqa: E501\n+                    raise FieldValueRangeException(\n+                        \"given field value {} invalid - \"  # noqa: E501\n+                        \"must be in range [0..255]\".format(value_spec)\n+                    )  # noqa: E501\n                 assoc_table[value_spec] = key\n \n             elif value_spec_type is list:\n                 for value in value_spec:\n                     if value < 0 or value > 255:\n-                        raise FieldValueRangeException('given field value {} invalid - '  # noqa: E501\n-                                                       'must be in range [0..255]'.format(value))  # noqa: E501\n+                        raise FieldValueRangeException(\n+                            \"given field value {} invalid - \"  # noqa: E501\n+                            \"must be in range [0..255]\".format(value)\n+                        )  # noqa: E501\n                     assoc_table[value] = key\n \n             elif value_spec_type is tuple:\n                 value_spec_len = len(value_spec)\n                 if value_spec_len != 2:\n-                    raise FieldAttributeException('invalid length {} of given config item tuple {} - must be '  # noqa: E501\n-                                                  '(<start-range>, <end-range>).'.format(value_spec_len, value_spec))  # noqa: E501\n+                    raise FieldAttributeException(\n+                        \"invalid length {} of given config item tuple {} - must be \"  # noqa: E501\n+                        \"(<start-range>, <end-range>).\".format(\n+                            value_spec_len, value_spec\n+                        )\n+                    )  # noqa: E501\n \n                 value_range_start = value_spec[0]\n                 if value_range_start < 0 or value_range_start > 255:\n-                    raise FieldValueRangeException('given field value {} invalid - '  # noqa: E501\n-                                                   'must be in range [0..255]'.format(value_range_start))  # noqa: E501\n+                    raise FieldValueRangeException(\n+                        \"given field value {} invalid - \"  # noqa: E501\n+                        \"must be in range [0..255]\".format(value_range_start)\n+                    )  # noqa: E501\n \n                 value_range_end = value_spec[1]\n                 if value_range_end < 0 or value_range_end > 255:\n-                    raise FieldValueRangeException('given field value {} invalid - '  # noqa: E501\n-                                                   'must be in range [0..255]'.format(value_range_end))  # noqa: E501\n+                    raise FieldValueRangeException(\n+                        \"given field value {} invalid - \"  # noqa: E501\n+                        \"must be in range [0..255]\".format(value_range_end)\n+                    )  # noqa: E501\n \n                 for value in range(value_range_start, value_range_end + 1):\n-\n                     assoc_table[value] = key\n \n         self.eval_fn = lambda x: assoc_table[x] if x in assoc_table else x\n@@ -1291,7 +1335,7 @@ class YesNoByteField(ByteField):\n \n         if not config:\n             # this represents the common use case and therefore it is kept small  # noqa: E501\n-            self.eval_fn = lambda x: 'no' if x == 0 else 'yes'\n+            self.eval_fn = lambda x: \"no\" if x == 0 else \"yes\"\n         else:\n             self._build_config_representation(config)\n         ByteField.__init__(self, name, default)\n@@ -1464,7 +1508,7 @@ class _StrField(Field[I, bytes]):\n         if self.remain == 0:\n             return b\"\", self.m2i(pkt, s)\n         else:\n-            return s[-self.remain:], self.m2i(pkt, s[:-self.remain])\n+            return s[-self.remain :], self.m2i(pkt, s[: -self.remain])\n \n     def randval(self):\n         # type: () -> RandBin\n@@ -1488,11 +1532,11 @@ class StrFieldUtf16(StrField):\n \n     def h2i(self, pkt, x):\n         # type: (Optional[Packet], Optional[str]) -> bytes\n-        return plain_str(x).encode('utf-16-le', errors=\"replace\")\n+        return plain_str(x).encode(\"utf-16-le\", errors=\"replace\")\n \n     def i2h(self, pkt, x):\n         # type: (Optional[Packet], bytes) -> str\n-        return bytes_encode(x).decode('utf-16-le', errors=\"replace\")\n+        return bytes_encode(x).decode(\"utf-16-le\", errors=\"replace\")\n \n \n class _StrEnumField:\n@@ -1516,37 +1560,39 @@ class StrEnumField(_StrEnumField, StrField):\n     __slots__ = [\"enum\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: bytes\n-            enum=None,  # type: Optional[Dict[str, str]]\n-            **kwargs  # type: Any\n+        self,\n+        name,  # type: str\n+        default,  # type: bytes\n+        enum=None,  # type: Optional[Dict[str, str]]\n+        **kwargs,  # type: Any\n     ):\n         # type: (...) -> None\n         StrField.__init__(self, name, default, **kwargs)  # type: ignore\n         self.enum = enum\n \n \n-K = TypeVar('K', List[BasePacket], BasePacket, Optional[BasePacket])\n+K = TypeVar(\"K\", List[BasePacket], BasePacket, Optional[BasePacket])\n \n \n class _PacketField(_StrField[K]):\n     __slots__ = [\"cls\"]\n     holds_packets = 1\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[K]\n-                 pkt_cls,  # type: Union[Callable[[bytes], Packet], Type[Packet]]  # noqa: E501\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[K]\n+        pkt_cls,  # type: Union[Callable[[bytes], Packet], Type[Packet]]  # noqa: E501\n+    ):\n         # type: (...) -> None\n         super(_PacketField, self).__init__(name, default)\n         self.cls = pkt_cls\n \n-    def i2m(self,\n-            pkt,  # type: Optional[Packet]\n-            i,  # type: Any\n-            ):\n+    def i2m(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        i,  # type: Any\n+    ):\n         # type: (...) -> bytes\n         if i is None:\n             return b\"\"\n@@ -1568,10 +1614,11 @@ class _PacketFieldSingle(_PacketField[K]):\n             cast(\"Packet\", x).add_parent(pkt)\n         return super(_PacketFieldSingle, self).any2i(pkt, x)\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Tuple[bytes, K]\n         i = self.m2i(pkt, s)\n         remain = b\"\"\n@@ -1586,26 +1633,29 @@ class PacketField(_PacketFieldSingle[BasePacket]):\n     def randval(self):  # type: ignore\n         # type: () -> Packet\n         from scapy.packet import fuzz\n+\n         return fuzz(self.cls())  # type: ignore\n \n \n class PacketLenField(_PacketFieldSingle[Optional[BasePacket]]):\n     __slots__ = [\"length_from\"]\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Packet\n-                 cls,  # type: Union[Callable[[bytes], Packet], Type[Packet]]  # noqa: E501\n-                 length_from=None  # type: Optional[Callable[[Packet], int]]  # noqa: E501\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Packet\n+        cls,  # type: Union[Callable[[bytes], Packet], Type[Packet]]  # noqa: E501\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]  # noqa: E501\n+    ):\n         # type: (...) -> None\n         super(PacketLenField, self).__init__(name, default, cls)\n         self.length_from = length_from or (lambda x: 0)\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Tuple[bytes, Optional[BasePacket]]\n         len_pkt = self.length_from(pkt)\n         i = None\n@@ -1627,18 +1677,19 @@ class PacketListField(_PacketField[List[BasePacket]]):\n     (i.e. a stack of layers). All elements in PacketListField have current\n     packet referenced in parent field.\n     \"\"\"\n+\n     __slots__ = [\"count_from\", \"length_from\", \"next_cls_cb\", \"max_count\"]\n     islist = 1\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Optional[List[BasePacket]]\n-            pkt_cls=None,  # type: Optional[Union[Callable[[bytes], Packet], Type[Packet]]]  # noqa: E501\n-            count_from=None,  # type: Optional[Callable[[Packet], int]]\n-            length_from=None,  # type: Optional[Callable[[Packet], int]]\n-            next_cls_cb=None,  # type: Optional[Callable[[Packet, List[BasePacket], Optional[Packet], bytes], Optional[Type[Packet]]]]  # noqa: E501\n-            max_count=None,  # type: Optional[int]\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[List[BasePacket]]\n+        pkt_cls=None,  # type: Optional[Union[Callable[[bytes], Packet], Type[Packet]]]  # noqa: E501\n+        count_from=None,  # type: Optional[Callable[[Packet], int]]\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n+        next_cls_cb=None,  # type: Optional[Callable[[Packet, List[BasePacket], Optional[Packet], bytes], Optional[Type[Packet]]]]  # noqa: E501\n+        max_count=None,  # type: Optional[int]\n     ):\n         # type: (...) -> None\n         \"\"\"\n@@ -1746,7 +1797,7 @@ class PacketListField(_PacketField[List[BasePacket]]):\n         super(PacketListField, self).__init__(\n             name,\n             default,\n-            pkt_cls  # type: ignore\n+            pkt_cls,  # type: ignore\n         )\n         self.count_from = count_from\n         self.length_from = length_from\n@@ -1766,19 +1817,21 @@ class PacketListField(_PacketField[List[BasePacket]]):\n                 i.add_parent(pkt)\n         return x\n \n-    def i2count(self,\n-                pkt,  # type: Optional[Packet]\n-                val,  # type: List[BasePacket]\n-                ):\n+    def i2count(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        val,  # type: List[BasePacket]\n+    ):\n         # type: (...) -> int\n         if isinstance(val, list):\n             return len(val)\n         return 1\n \n-    def i2len(self,\n-              pkt,  # type: Optional[Packet]\n-              val,  # type: List[Packet]\n-              ):\n+    def i2len(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        val,  # type: List[Packet]\n+    ):\n         # type: (...) -> int\n         return sum(len(self.i2m(pkt, p)) for p in val)\n \n@@ -1845,10 +1898,11 @@ class PacketListField(_PacketField[List[BasePacket]]):\n         else:\n             return remain + ret, lst\n \n-    def i2m(self,\n-            pkt,  # type: Optional[Packet]\n-            i,  # type: Any\n-            ):\n+    def i2m(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        i,  # type: Any\n+    ):\n         # type: (...) -> bytes\n         return bytes_encode(i)\n \n@@ -1861,11 +1915,11 @@ class StrFixedLenField(StrField):\n     __slots__ = [\"length_from\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Optional[bytes]\n-            length=None,  # type: Optional[int]\n-            length_from=None,  # type: Optional[Callable[[Packet], int]]\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[bytes]\n+        length=None,  # type: Optional[int]\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n     ):\n         # type: (...) -> None\n         super(StrFixedLenField, self).__init__(name, default)\n@@ -1874,10 +1928,11 @@ class StrFixedLenField(StrField):\n             self.sz = length\n             self.length_from = lambda x, length=length: length  # type: ignore\n \n-    def i2repr(self,\n-               pkt,  # type: Optional[Packet]\n-               v,  # type: bytes\n-               ):\n+    def i2repr(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        v,  # type: bytes\n+    ):\n         # type: (...) -> str\n         if isinstance(v, bytes) and not conf.debug_strfixedlenfield:\n             v = v.rstrip(b\"\\0\")\n@@ -1913,15 +1968,17 @@ class StrFixedLenEnumField(_StrEnumField, StrFixedLenField):\n     __slots__ = [\"enum\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: bytes\n-            enum=None,  # type: Optional[Dict[str, str]]\n-            length=None,  # type: Optional[int]\n-            length_from=None  # type: Optional[Callable[[Optional[Packet]], int]]  # noqa: E501\n+        self,\n+        name,  # type: str\n+        default,  # type: bytes\n+        enum=None,  # type: Optional[Dict[str, str]]\n+        length=None,  # type: Optional[int]\n+        length_from=None,  # type: Optional[Callable[[Optional[Packet]], int]]  # noqa: E501\n     ):\n         # type: (...) -> None\n-        StrFixedLenField.__init__(self, name, default, length=length, length_from=length_from)  # noqa: E501\n+        StrFixedLenField.__init__(\n+            self, name, default, length=length, length_from=length_from\n+        )  # noqa: E501\n         self.enum = enum\n \n \n@@ -1949,7 +2006,7 @@ class NetBIOSNameField(StrFixedLenField):\n             struct.pack(\n                 \"!BB\",\n                 0x41 + (b >> 4),\n-                0x41 + (b & 0xf),\n+                0x41 + (b & 0xF),\n             )\n             for b in x\n         )\n@@ -1958,13 +2015,15 @@ class NetBIOSNameField(StrFixedLenField):\n     def m2i(self, pkt, x):\n         # type: (Optional[Packet], bytes) -> bytes\n         x = x[1:].strip(b\"\\x00\")\n-        return b\"\".join(map(\n-            lambda x, y: struct.pack(\n-                \"!B\",\n-                (((x - 1) & 0xf) << 4) + ((y - 1) & 0xf)\n-            ),\n-            x[::2], x[1::2]\n-        )).rstrip(b\" \")\n+        return b\"\".join(\n+            map(\n+                lambda x, y: struct.pack(\n+                    \"!B\", (((x - 1) & 0xF) << 4) + ((y - 1) & 0xF)\n+                ),\n+                x[::2],\n+                x[1::2],\n+            )\n+        ).rstrip(b\" \")\n \n \n class StrLenField(StrField):\n@@ -1974,15 +2033,16 @@ class StrLenField(StrField):\n     :param length_from: a function that returns the size of the string\n     :param max_length: max size to use as randval\n     \"\"\"\n+\n     __slots__ = [\"length_from\", \"max_length\"]\n     ON_WIRE_SIZE_UTF16 = True\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: bytes\n-            length_from=None,  # type: Optional[Callable[[Packet], int]]\n-            max_length=None,  # type: Optional[Any]\n+        self,\n+        name,  # type: str\n+        default,  # type: bytes\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n+        max_length=None,  # type: Optional[Any]\n     ):\n         # type: (...) -> None\n         super(StrLenField, self).__init__(name, default)\n@@ -2034,11 +2094,11 @@ class XLEStrLenField(XStrLenField):\n         # type: (Optional[Packet], Optional[bytes]) -> bytes\n         if not x:\n             return b\"\"\n-        return x[:: -1]\n+        return x[::-1]\n \n     def m2i(self, pkt, x):\n         # type: (Optional[Packet], bytes) -> bytes\n-        return x[:: -1]\n+        return x[::-1]\n \n \n class StrLenFieldUtf16(StrLenField, StrFieldUtf16):\n@@ -2049,11 +2109,11 @@ class StrLenEnumField(_StrEnumField, StrLenField):\n     __slots__ = [\"enum\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: bytes\n-            enum=None,  # type: Optional[Dict[str, str]]\n-            **kwargs  # type: Any\n+        self,\n+        name,  # type: str\n+        default,  # type: bytes\n+        enum=None,  # type: Optional[Dict[str, str]]\n+        **kwargs,  # type: Any\n     ):\n         # type: (...) -> None\n         StrLenField.__init__(self, name, default, **kwargs)\n@@ -2064,12 +2124,12 @@ class BoundStrLenField(StrLenField):\n     __slots__ = [\"minlen\", \"maxlen\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: bytes\n-            minlen=0,  # type: int\n-            maxlen=255,  # type: int\n-            length_from=None  # type: Optional[Callable[[Packet], int]]\n+        self,\n+        name,  # type: str\n+        default,  # type: bytes\n+        minlen=0,  # type: int\n+        maxlen=255,  # type: int\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n     ):\n         # type: (...) -> None\n         StrLenField.__init__(self, name, default, length_from=length_from)\n@@ -2086,13 +2146,13 @@ class FieldListField(Field[List[Any], List[Any]]):\n     islist = 1\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Optional[List[AnyField]]\n-            field,  # type: AnyField\n-            length_from=None,  # type: Optional[Callable[[Packet], int]]\n-            count_from=None,  # type: Optional[Callable[[Packet], int]]\n-            max_count=None,  # type: Optional[int]\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[List[AnyField]]\n+        field,  # type: AnyField\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n+        count_from=None,  # type: Optional[Callable[[Packet], int]]\n+        max_count=None,  # type: Optional[int]\n     ):\n         # type: (...) -> None\n         if default is None:\n@@ -2120,28 +2180,31 @@ class FieldListField(Field[List[Any], List[Any]]):\n         else:\n             return [self.field.any2i(pkt, e) for e in x]\n \n-    def i2repr(self,\n-               pkt,  # type: Optional[Packet]\n-               x,  # type: List[Any]\n-               ):\n+    def i2repr(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: List[Any]\n+    ):\n         # type: (...) -> str\n         return \"[%s]\" % \", \".join(self.field.i2repr(pkt, v) for v in x)\n \n-    def addfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 val,  # type: Optional[List[Any]]\n-                 ):\n+    def addfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+        val,  # type: Optional[List[Any]]\n+    ):\n         # type: (...) -> bytes\n         val = self.i2m(pkt, val)\n         for v in val:\n             s = self.field.addfield(pkt, s, v)\n         return s\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Any\n         c = len_pkt = None\n         if self.length_from is not None:\n@@ -2179,13 +2242,13 @@ class FieldLenField(Field[int, int]):\n     __slots__ = [\"length_of\", \"count_of\", \"adjust\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Optional[Any]\n-            length_of=None,  # type: Optional[str]\n-            fmt=\"H\",  # type: str\n-            count_of=None,  # type: Optional[str]\n-            adjust=lambda pkt, x: x,  # type: Callable[[Packet, int], int]\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[Any]\n+        length_of=None,  # type: Optional[str]\n+        fmt=\"H\",  # type: str\n+        count_of=None,  # type: Optional[str]\n+        adjust=lambda pkt, x: x,  # type: Callable[[Packet, int], int]\n     ):\n         # type: (...) -> None\n         Field.__init__(self, name, default, fmt)\n@@ -2203,9 +2266,7 @@ class FieldLenField(Field[int, int]):\n                 fld, fval = pkt.getfield_and_val(self.count_of)\n                 f = fld.i2count(pkt, fval)\n             else:\n-                raise ValueError(\n-                    \"Field should have either length_of or count_of\"\n-                )\n+                raise ValueError(\"Field should have either length_of or count_of\")\n             x = self.adjust(pkt, f)\n         elif x is None:\n             x = 0\n@@ -2219,10 +2280,11 @@ class StrNullField(StrField):\n         # type: (Packet, bytes, Optional[bytes]) -> bytes\n         return s + self.i2m(pkt, val) + self.DELIMITER\n \n-    def getfield(self,\n-                 pkt,  # type: Packet\n-                 s,  # type: bytes\n-                 ):\n+    def getfield(\n+        self,\n+        pkt,  # type: Packet\n+        s,  # type: bytes\n+    ):\n         # type: (...) -> Tuple[bytes, bytes]\n         len_str = 0\n         while True:\n@@ -2234,7 +2296,7 @@ class StrNullField(StrField):\n                 len_str += 1\n             else:\n                 break\n-        return s[len_str + len(self.DELIMITER):], self.m2i(pkt, s[:len_str])\n+        return s[len_str + len(self.DELIMITER) :], self.m2i(pkt, s[:len_str])\n \n     def randval(self):\n         # type: () -> RandTermString\n@@ -2275,6 +2337,7 @@ class LenField(Field[int, int]):\n     \"\"\"\n     If None, will be filled with the size of the payload\n     \"\"\"\n+\n     __slots__ = [\"adjust\"]\n \n     def __init__(self, name, default, fmt=\"H\", adjust=lambda x: x):\n@@ -2282,10 +2345,11 @@ class LenField(Field[int, int]):\n         Field.__init__(self, name, default, fmt)\n         self.adjust = adjust\n \n-    def i2m(self,\n-            pkt,  # type: Optional[Packet]\n-            x,  # type: Optional[int]\n-            ):\n+    def i2m(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Optional[int]\n+    ):\n         # type: (...) -> int\n         if x is None:\n             x = 0\n@@ -2352,10 +2416,10 @@ class _BitField(Field[I, int]):\n             ]\n \n     \"\"\"\n+\n     __slots__ = [\"rev\", \"size\", \"tot_size\", \"end_tot_size\"]\n \n-    def __init__(self, name, default, size,\n-                 tot_size=0, end_tot_size=0):\n+    def __init__(self, name, default, size, tot_size=0, end_tot_size=0):\n         # type: (str, Optional[I], int, int, int) -> None\n         Field.__init__(self, name, default)\n         if callable(size):\n@@ -2370,15 +2434,16 @@ class _BitField(Field[I, int]):\n         self.end_tot_size = abs(end_tot_size)\n         # Fields always have a round sz except BitField\n         # so to keep it simple, we'll ignore it here.\n-        self.sz = self.size / 8.  # type: ignore\n+        self.sz = self.size / 8.0  # type: ignore\n \n     # We need to # type: ignore a few things because of how special\n     # BitField is\n-    def addfield(self,  # type: ignore\n-                 pkt,  # type: Packet\n-                 s,  # type: Union[Tuple[bytes, int, int], bytes]\n-                 ival,  # type: I\n-                 ):\n+    def addfield(\n+        self,  # type: ignore\n+        pkt,  # type: Packet\n+        s,  # type: Union[Tuple[bytes, int, int], bytes]\n+        ival,  # type: I\n+    ):\n         # type: (...) -> Union[Tuple[bytes, int, int], bytes]\n         val = self.i2m(pkt, ival)\n         if isinstance(s, tuple):\n@@ -2398,13 +2463,14 @@ class _BitField(Field[I, int]):\n         else:\n             # Apply LE if necessary\n             if self.rev and self.end_tot_size > 1:\n-                s = s[:-self.end_tot_size] + s[-self.end_tot_size:][::-1]\n+                s = s[: -self.end_tot_size] + s[-self.end_tot_size :][::-1]\n             return s\n \n-    def getfield(self,  # type: ignore\n-                 pkt,  # type: Packet\n-                 s,  # type: Union[Tuple[bytes, int], bytes]\n-                 ):\n+    def getfield(\n+        self,  # type: ignore\n+        pkt,  # type: Packet\n+        s,  # type: Union[Tuple[bytes, int], bytes]\n+    ):\n         # type: (...) -> Union[Tuple[Tuple[bytes, int], I], Tuple[bytes, I]]  # noqa: E501\n         if isinstance(s, tuple):\n             s, bn = s\n@@ -2412,14 +2478,14 @@ class _BitField(Field[I, int]):\n             bn = 0\n             # Apply LE if necessary\n             if self.rev and self.tot_size > 1:\n-                s = s[:self.tot_size][::-1] + s[self.tot_size:]\n+                s = s[: self.tot_size][::-1] + s[self.tot_size :]\n \n         # we don't want to process all the string\n         nb_bytes = (self.size + bn - 1) // 8 + 1\n         w = s[:nb_bytes]\n \n         # split the substring byte by byte\n-        _bytes = struct.unpack('!%dB' % nb_bytes, w)\n+        _bytes = struct.unpack(\"!%dB\" % nb_bytes, w)\n \n         b = 0\n         for c in range(nb_bytes):\n@@ -2432,7 +2498,7 @@ class _BitField(Field[I, int]):\n         b = b >> (nb_bytes * 8 - self.size - bn)\n \n         bn += self.size\n-        s = s[bn // 8:]\n+        s = s[bn // 8 :]\n         bn = bn % 8\n         b2 = self.m2i(pkt, b)\n         if bn:\n@@ -2456,28 +2522,31 @@ class BitField(_BitField[int]):\n class BitLenField(BitField):\n     __slots__ = [\"length_from\"]\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 length_from  # type: Callable[[Packet], int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        length_from,  # type: Callable[[Packet], int]\n+    ):\n         # type: (...) -> None\n         self.length_from = length_from\n         super(BitLenField, self).__init__(name, default, 0)\n \n-    def getfield(self,  # type: ignore\n-                 pkt,  # type: Packet\n-                 s,  # type: Union[Tuple[bytes, int], bytes]\n-                 ):\n+    def getfield(\n+        self,  # type: ignore\n+        pkt,  # type: Packet\n+        s,  # type: Union[Tuple[bytes, int], bytes]\n+    ):\n         # type: (...) -> Union[Tuple[Tuple[bytes, int], int], Tuple[bytes, int]]  # noqa: E501\n         self.size = self.length_from(pkt)\n         return super(BitLenField, self).getfield(pkt, s)\n \n-    def addfield(self,  # type: ignore\n-                 pkt,  # type: Packet\n-                 s,  # type: Union[Tuple[bytes, int, int], bytes]\n-                 val  # type: int\n-                 ):\n+    def addfield(\n+        self,  # type: ignore\n+        pkt,  # type: Packet\n+        s,  # type: Union[Tuple[bytes, int, int], bytes]\n+        val,  # type: int\n+    ):\n         # type: (...) -> Union[Tuple[bytes, int, int], bytes]\n         self.size = self.length_from(pkt)\n         return super(BitLenField, self).addfield(pkt, s, val)\n@@ -2486,19 +2555,21 @@ class BitLenField(BitField):\n class BitFieldLenField(BitField):\n     __slots__ = [\"length_of\", \"count_of\", \"adjust\", \"tot_size\", \"end_tot_size\"]\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 size,  # type: int\n-                 length_of=None,  # type: Optional[Union[Callable[[Optional[Packet]], int], str]]  # noqa: E501\n-                 count_of=None,  # type: Optional[str]\n-                 adjust=lambda pkt, x: x,  # type: Callable[[Optional[Packet], int], int]  # noqa: E501\n-                 tot_size=0,  # type: int\n-                 end_tot_size=0,  # type: int\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        size,  # type: int\n+        length_of=None,  # type: Optional[Union[Callable[[Optional[Packet]], int], str]]  # noqa: E501\n+        count_of=None,  # type: Optional[str]\n+        adjust=lambda pkt, x: x,  # type: Callable[[Optional[Packet], int], int]  # noqa: E501\n+        tot_size=0,  # type: int\n+        end_tot_size=0,  # type: int\n+    ):\n         # type: (...) -> None\n-        super(BitFieldLenField, self).__init__(name, default, size,\n-                                               tot_size, end_tot_size)\n+        super(BitFieldLenField, self).__init__(\n+            name, default, size, tot_size, end_tot_size\n+        )\n         self.length_of = length_of\n         self.count_of = count_of\n         self.adjust = adjust\n@@ -2514,18 +2585,26 @@ class XBitField(BitField):\n         return lhex(self.i2h(pkt, x))\n \n \n-_EnumType = Union[Dict[I, str], Dict[str, I], List[str], DADict[I, str], Type[Enum], Tuple[Callable[[I], str], Callable[[str], I]]]  # noqa: E501\n+_EnumType = Union[\n+    Dict[I, str],\n+    Dict[str, I],\n+    List[str],\n+    DADict[I, str],\n+    Type[Enum],\n+    Tuple[Callable[[I], str], Callable[[str], I]],\n+]  # noqa: E501\n \n \n class _EnumField(Field[Union[List[I], I], I]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[I]\n-                 enum,  # type: _EnumType[I]\n-                 fmt=\"H\",  # type: str\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[I]\n+        enum,  # type: _EnumType[I]\n+        fmt=\"H\",  # type: str\n+    ):\n         # type: (...) -> None\n-        \"\"\" Initializes enum fields.\n+        \"\"\"Initializes enum fields.\n \n         @param name:    name of this field\n         @param default: default value of this field\n@@ -2625,9 +2704,7 @@ class _EnumField(Field[Union[List[I], I], I]):\n     def notify_set(self, enum, key, value):\n         # type: (ObservableDict, I, str) -> None\n         ks = \"0x%x\" if isinstance(key, int) else \"%s\"\n-        log_runtime.debug(\n-            \"At %s: Change to %s at \" + ks, self, value, key\n-        )\n+        log_runtime.debug(\"At %s: Change to %s at \" + ks, self, value, key)\n         if self.i2s is not None and self.s2i is not None:\n             self.i2s[key] = value\n             self.s2i[value] = key\n@@ -2647,12 +2724,13 @@ class EnumField(_EnumField[I]):\n \n \n class CharEnumField(EnumField[str]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: str\n-                 enum,  # type: _EnumType[str]\n-                 fmt=\"1s\",  # type: str\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: str\n+        enum,  # type: _EnumType[str]\n+        fmt=\"1s\",  # type: str\n+    ):\n         # type: (...) -> None\n         super(CharEnumField, self).__init__(name, default, enum, fmt)\n         if self.i2s is not None:\n@@ -2673,13 +2751,14 @@ class CharEnumField(EnumField[str]):\n class BitEnumField(_BitField[Union[List[int], int]], _EnumField[int]):\n     __slots__ = EnumField.__slots__\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type:  Optional[int]\n-                 size,  # type:  int\n-                 enum,  # type: _EnumType[int]\n-                 **kwargs  # type: Any\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type:  Optional[int]\n+        size,  # type:  int\n+        enum,  # type: _EnumType[int]\n+        **kwargs,  # type: Any\n+    ):\n         # type: (...) -> None\n         _EnumField.__init__(self, name, default, enum)\n         _BitField.__init__(self, name, default, size, **kwargs)\n@@ -2688,10 +2767,11 @@ class BitEnumField(_BitField[Union[List[int], int]], _EnumField[int]):\n         # type: (Optional[Packet], Any) -> Union[List[int], int]\n         return _EnumField.any2i(self, pkt, x)\n \n-    def i2repr(self,\n-               pkt,  # type: Optional[Packet]\n-               x,  # type: Union[List[int], int]\n-               ):\n+    def i2repr(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Union[List[int], int]\n+    ):\n         # type: (...) -> Any\n         return _EnumField.i2repr(self, pkt, x)\n \n@@ -2699,13 +2779,14 @@ class BitEnumField(_BitField[Union[List[int], int]], _EnumField[int]):\n class BitLenEnumField(BitLenField, _EnumField[int]):\n     __slots__ = EnumField.__slots__\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 length_from,  # type: Callable[[Packet], int]\n-                 enum,  # type: _EnumType[int]\n-                 **kwargs,  # type: Any\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        length_from,  # type: Callable[[Packet], int]\n+        enum,  # type: _EnumType[int]\n+        **kwargs,  # type: Any\n+    ):\n         # type: (...) -> None\n         _EnumField.__init__(self, name, default, enum)\n         BitLenField.__init__(self, name, default, length_from, **kwargs)\n@@ -2714,10 +2795,11 @@ class BitLenEnumField(BitLenField, _EnumField[int]):\n         # type: (Optional[Packet], Any) -> int\n         return _EnumField.any2i(self, pkt, x)  # type: ignore\n \n-    def i2repr(self,\n-               pkt,  # type: Optional[Packet]\n-               x,  # type: Union[List[int], int]\n-               ):\n+    def i2repr(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Union[List[int], int]\n+    ):\n         # type: (...) -> Any\n         return _EnumField.i2repr(self, pkt, x)\n \n@@ -2725,51 +2807,56 @@ class BitLenEnumField(BitLenField, _EnumField[int]):\n class ShortEnumField(EnumField[int]):\n     __slots__ = EnumField.__slots__\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(ShortEnumField, self).__init__(name, default, enum, \"H\")\n \n \n class LEShortEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(LEShortEnumField, self).__init__(name, default, enum, \"<H\")\n \n \n class LongEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(LongEnumField, self).__init__(name, default, enum, \"Q\")\n \n \n class LELongEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(LELongEnumField, self).__init__(name, default, enum, \"<Q\")\n \n \n class ByteEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(ByteEnumField, self).__init__(name, default, enum, \"B\")\n \n@@ -2791,31 +2878,34 @@ class XByteEnumField(ByteEnumField):\n \n \n class IntEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(IntEnumField, self).__init__(name, default, enum, \"I\")\n \n \n class SignedIntEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(SignedIntEnumField, self).__init__(name, default, enum, \"i\")\n \n \n class LEIntEnumField(EnumField[int]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         super(LEIntEnumField, self).__init__(name, default, enum, \"<I\")\n \n@@ -2835,11 +2925,12 @@ class XShortEnumField(ShortEnumField):\n class LE3BytesEnumField(LEThreeBytesField, _EnumField[int]):\n     __slots__ = EnumField.__slots__\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[int]\n-                 enum,  # type: _EnumType[int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[int]\n+        enum,  # type: _EnumType[int]\n+    ):\n         # type: (...) -> None\n         _EnumField.__init__(self, name, default, enum)\n         LEThreeBytesField.__init__(self, name, default)\n@@ -2860,13 +2951,14 @@ class XLE3BytesEnumField(LE3BytesEnumField):\n \n \n class _MultiEnumField(_EnumField[I]):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: int\n-                 enum,  # type: Dict[I, Dict[I, str]]\n-                 depends_on,  # type: Callable[[Optional[Packet]], I]\n-                 fmt=\"H\"  # type: str\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: int\n+        enum,  # type: Dict[I, Dict[I, str]]\n+        depends_on,  # type: Callable[[Optional[Packet]], I]\n+        fmt=\"H\",  # type: str\n+    ):\n         # type: (...) -> None\n \n         self.depends_on = depends_on\n@@ -2906,47 +2998,46 @@ class MultiEnumField(_MultiEnumField[int], EnumField[int]):\n     __slots__ = [\"depends_on\", \"i2s_multi\", \"s2i_multi\", \"s2i_all\"]\n \n \n-class BitMultiEnumField(_BitField[Union[List[int], int]],\n-                        _MultiEnumField[int]):\n+class BitMultiEnumField(_BitField[Union[List[int], int]], _MultiEnumField[int]):\n     __slots__ = EnumField.__slots__ + MultiEnumField.__slots__\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: int\n-            size,  # type: int\n-            enum,  # type: Dict[int, Dict[int, str]]\n-            depends_on  # type: Callable[[Optional[Packet]], int]\n+        self,\n+        name,  # type: str\n+        default,  # type: int\n+        size,  # type: int\n+        enum,  # type: Dict[int, Dict[int, str]]\n+        depends_on,  # type: Callable[[Optional[Packet]], int]\n     ):\n         # type: (...) -> None\n         _MultiEnumField.__init__(self, name, default, enum, depends_on)\n         self.rev = size < 0\n         self.size = abs(size)\n-        self.sz = self.size / 8.  # type: ignore\n+        self.sz = self.size / 8.0  # type: ignore\n \n     def any2i(self, pkt, x):\n         # type: (Optional[Packet], Any) -> Union[List[int], int]\n         return _MultiEnumField[int].any2i(\n             self,  # type: ignore\n             pkt,\n-            x\n+            x,\n         )\n \n     def i2repr(  # type: ignore\n-            self,\n-            pkt,  # type: Optional[Packet]\n-            x  # type: Union[List[int], int]\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Union[List[int], int]\n     ):\n         # type: (...) -> Union[str, List[str]]\n         return _MultiEnumField[int].i2repr(\n             self,  # type: ignore\n             pkt,\n-            x\n+            x,\n         )\n \n \n class ByteEnumKeysField(ByteEnumField):\n-    \"\"\"ByteEnumField that picks valid values when fuzzed. \"\"\"\n+    \"\"\"ByteEnumField that picks valid values when fuzzed.\"\"\"\n \n     def randval(self):\n         # type: () -> RandEnumKeys\n@@ -2954,7 +3045,7 @@ class ByteEnumKeysField(ByteEnumField):\n \n \n class ShortEnumKeysField(ShortEnumField):\n-    \"\"\"ShortEnumField that picks valid values when fuzzed. \"\"\"\n+    \"\"\"ShortEnumField that picks valid values when fuzzed.\"\"\"\n \n     def randval(self):\n         # type: () -> RandEnumKeys\n@@ -2962,7 +3053,7 @@ class ShortEnumKeysField(ShortEnumField):\n \n \n class IntEnumKeysField(IntEnumField):\n-    \"\"\"IntEnumField that picks valid values when fuzzed. \"\"\"\n+    \"\"\"IntEnumField that picks valid values when fuzzed.\"\"\"\n \n     def randval(self):\n         # type: () -> RandEnumKeys\n@@ -2974,26 +3065,27 @@ class IntEnumKeysField(IntEnumField):\n \n class LEFieldLenField(FieldLenField):\n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Optional[Any]\n-            length_of=None,  # type: Optional[str]\n-            fmt=\"<H\",  # type: str\n-            count_of=None,  # type: Optional[str]\n-            adjust=lambda pkt, x: x,  # type: Callable[[Packet, int], int]\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[Any]\n+        length_of=None,  # type: Optional[str]\n+        fmt=\"<H\",  # type: str\n+        count_of=None,  # type: Optional[str]\n+        adjust=lambda pkt, x: x,  # type: Callable[[Packet, int], int]\n     ):\n         # type: (...) -> None\n         FieldLenField.__init__(\n-            self, name, default,\n+            self,\n+            name,\n+            default,\n             length_of=length_of,\n             fmt=fmt,\n             count_of=count_of,\n-            adjust=adjust\n+            adjust=adjust,\n         )\n \n \n class FlagValueIter(object):\n-\n     __slots__ = [\"flagvalue\", \"cursor\"]\n \n     def __init__(self, flagvalue):\n@@ -3027,7 +3119,7 @@ class FlagValue(object):\n         if not value:\n             return 0\n         if isinstance(value, str):\n-            value = value.split('+') if self.multi else list(value)\n+            value = value.split(\"+\") if self.multi else list(value)\n         if isinstance(value, list):\n             y = 0\n             for i in value:\n@@ -3076,19 +3168,20 @@ class FlagValue(object):\n     def __and__(self, other):\n         # type: (int) -> FlagValue\n         return self.__class__(self.value & self._fixvalue(other), self.names)\n+\n     __rand__ = __and__\n \n     def __or__(self, other):\n         # type: (int) -> FlagValue\n         return self.__class__(self.value | self._fixvalue(other), self.names)\n+\n     __ror__ = __or__\n     __add__ = __or__  # + is an alias for |\n \n     def __sub__(self, other):\n         # type: (int) -> FlagValue\n         return self.__class__(\n-            self.value & (2 ** len(self.names) - 1 - self._fixvalue(other)),\n-            self.names\n+            self.value & (2 ** len(self.names) - 1 - self._fixvalue(other)), self.names\n         )\n \n     def __xor__(self, other):\n@@ -3106,13 +3199,13 @@ class FlagValue(object):\n     def __nonzero__(self):\n         # type: () -> bool\n         return bool(self.value)\n+\n     __bool__ = __nonzero__\n \n     def flagrepr(self):\n         # type: () -> str\n         warnings.warn(\n-            \"obj.flagrepr() is obsolete. Use str(obj) instead.\",\n-            DeprecationWarning\n+            \"obj.flagrepr() is obsolete. Use str(obj) instead.\", DeprecationWarning\n         )\n         return str(self)\n \n@@ -3151,12 +3244,11 @@ class FlagValue(object):\n         try:\n             if self.multi:\n                 return bool((2 ** self.names.index(attr)) & int(self))\n-            return all(bool((2 ** self.names.index(flag)) & int(self))\n-                       for flag in attr)\n+            return all(bool((2 ** self.names.index(flag)) & int(self)) for flag in attr)\n         except ValueError:\n-            if '_' in attr:\n+            if \"_\" in attr:\n                 try:\n-                    return self.__getattr__(attr.replace('_', '-'))\n+                    return self.__getattr__(attr.replace(\"_\", \"-\"))\n                 except AttributeError:\n                     pass\n             return super(FlagValue, self).__getattribute__(attr)\n@@ -3169,7 +3261,7 @@ class FlagValue(object):\n             return super(FlagValue, self).__setattr__(attr, value)\n         if attr in self.names:\n             if value:\n-                self.value |= (2 ** self.names.index(attr))\n+                self.value |= 2 ** self.names.index(attr)\n             else:\n                 self.value &= ~(2 ** self.names.index(attr))\n         else:\n@@ -3181,57 +3273,59 @@ class FlagValue(object):\n \n \n class FlagsField(_BitField[Optional[Union[int, FlagValue]]]):\n-    \"\"\" Handle Flag type field\n-\n-   Make sure all your flags have a label\n-\n-   Example (list):\n-       >>> from scapy.packet import Packet\n-       >>> class FlagsTest(Packet):\n-               fields_desc = [FlagsField(\"flags\", 0, 8, [\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\"])]  # noqa: E501\n-       >>> FlagsTest(flags=9).show2()\n-       ###[ FlagsTest ]###\n-         flags     = f0+f3\n-\n-    Example (str):\n-       >>> from scapy.packet import Packet\n-       >>> class TCPTest(Packet):\n-               fields_desc = [\n-                   BitField(\"reserved\", 0, 7),\n-                   FlagsField(\"flags\", 0x2, 9, \"FSRPAUECN\")\n-               ]\n-       >>> TCPTest(flags=3).show2()\n-       ###[ FlagsTest ]###\n-         reserved  = 0\n-         flags     = FS\n-\n-    Example (dict):\n-       >>> from scapy.packet import Packet\n-       >>> class FlagsTest2(Packet):\n-               fields_desc = [\n-                   FlagsField(\"flags\", 0x2, 16, {\n-                       0x0001: \"A\",\n-                       0x0008: \"B\",\n-                   })\n-               ]\n-\n-   :param name: field's name\n-   :param default: default value for the field\n-   :param size: number of bits in the field (in bits). if negative, LE\n-   :param names: (list or str or dict) label for each flag\n-       If it's a str or a list, the least Significant Bit tag's name\n-       is written first.\n-   \"\"\"\n+    \"\"\"Handle Flag type field\n+\n+    Make sure all your flags have a label\n+\n+    Example (list):\n+        >>> from scapy.packet import Packet\n+        >>> class FlagsTest(Packet):\n+                fields_desc = [FlagsField(\"flags\", 0, 8, [\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\"])]  # noqa: E501\n+        >>> FlagsTest(flags=9).show2()\n+        ###[ FlagsTest ]###\n+          flags     = f0+f3\n+\n+     Example (str):\n+        >>> from scapy.packet import Packet\n+        >>> class TCPTest(Packet):\n+                fields_desc = [\n+                    BitField(\"reserved\", 0, 7),\n+                    FlagsField(\"flags\", 0x2, 9, \"FSRPAUECN\")\n+                ]\n+        >>> TCPTest(flags=3).show2()\n+        ###[ FlagsTest ]###\n+          reserved  = 0\n+          flags     = FS\n+\n+     Example (dict):\n+        >>> from scapy.packet import Packet\n+        >>> class FlagsTest2(Packet):\n+                fields_desc = [\n+                    FlagsField(\"flags\", 0x2, 16, {\n+                        0x0001: \"A\",\n+                        0x0008: \"B\",\n+                    })\n+                ]\n+\n+    :param name: field's name\n+    :param default: default value for the field\n+    :param size: number of bits in the field (in bits). if negative, LE\n+    :param names: (list or str or dict) label for each flag\n+        If it's a str or a list, the least Significant Bit tag's name\n+        is written first.\n+    \"\"\"\n+\n     ismutable = True\n     __slots__ = [\"names\"]\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Optional[Union[int, FlagValue]]\n-                 size,  # type: int\n-                 names,     # type: Union[List[str], str, Dict[int, str]]\n-                 **kwargs   # type: Any\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Optional[Union[int, FlagValue]]\n+        size,  # type: int\n+        names,  # type: Union[List[str], str, Dict[int, str]]\n+        **kwargs,  # type: Any\n+    ):\n         # type: (...) -> None\n         # Convert the dict to a list\n         if isinstance(names, dict):\n@@ -3246,7 +3340,7 @@ class FlagsField(_BitField[Optional[Union[int, FlagValue]]]):\n     def _fixup_val(self, x):\n         # type: (Any) -> Optional[FlagValue]\n         \"\"\"Returns a FlagValue instance when needed. Internal method, to be\n-used in *2i() and i2*() methods.\n+        used in *2i() and i2*() methods.\n \n         \"\"\"\n         if isinstance(x, (FlagValue, VolatileValue)):\n@@ -3267,31 +3361,33 @@ used in *2i() and i2*() methods.\n         # type: (Optional[Packet], Any) -> Optional[FlagValue]\n         return self._fixup_val(super(FlagsField, self).i2h(pkt, x))\n \n-    def i2repr(self,\n-               pkt,  # type: Optional[Packet]\n-               x,  # type: Any\n-               ):\n+    def i2repr(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Any\n+    ):\n         # type: (...) -> str\n         if isinstance(x, (list, tuple)):\n-            return repr(type(x)(\n-                \"None\" if v is None else str(self._fixup_val(v)) for v in x\n-            ))\n+            return repr(\n+                type(x)(\"None\" if v is None else str(self._fixup_val(v)) for v in x)\n+            )\n         return \"None\" if x is None else str(self._fixup_val(x))\n \n \n-MultiFlagsEntry = collections.namedtuple('MultiFlagsEntry', ['short', 'long'])\n+MultiFlagsEntry = collections.namedtuple(\"MultiFlagsEntry\", [\"short\", \"long\"])\n \n \n class MultiFlagsField(_BitField[Set[str]]):\n     __slots__ = FlagsField.__slots__ + [\"depends_on\"]\n \n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: Set[str]\n-                 size,  # type: int\n-                 names,  # type: Dict[int, Dict[int, MultiFlagsEntry]]\n-                 depends_on,  # type: Callable[[Optional[Packet]], int]\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: Set[str]\n+        size,  # type: int\n+        names,  # type: Dict[int, Dict[int, MultiFlagsEntry]]\n+        depends_on,  # type: Callable[[Optional[Packet]], int]\n+    ):\n         # type: (...) -> None\n         self.names = names\n         self.depends_on = depends_on\n@@ -3300,7 +3396,7 @@ class MultiFlagsField(_BitField[Set[str]]):\n     def any2i(self, pkt, x):\n         # type: (Optional[Packet], Any) -> Set[str]\n         if not isinstance(x, (set, int)):\n-            raise ValueError('set expected')\n+            raise ValueError(\"set expected\")\n \n         if pkt is not None:\n             if isinstance(x, int):\n@@ -3308,7 +3404,7 @@ class MultiFlagsField(_BitField[Set[str]]):\n             else:\n                 v = self.depends_on(pkt)\n                 if v is not None:\n-                    assert v in self.names, 'invalid dependency'\n+                    assert v in self.names, \"invalid dependency\"\n                     these_names = self.names[v]\n                     s = set()\n                     for i in x:\n@@ -3317,7 +3413,9 @@ class MultiFlagsField(_BitField[Set[str]]):\n                                 s.add(i)\n                                 break\n                         else:\n-                            assert False, 'Unknown flag \"{}\" with this dependency'.format(i)  # noqa: E501\n+                            assert False, (\n+                                'Unknown flag \"{}\" with this dependency'.format(i)\n+                            )  # noqa: E501\n                             continue\n                     return s\n         if isinstance(x, int):\n@@ -3338,7 +3436,7 @@ class MultiFlagsField(_BitField[Set[str]]):\n                     r |= 1 << i\n                     break\n             else:\n-                r |= 1 << int(flag_set[len('bit '):])\n+                r |= 1 << int(flag_set[len(\"bit \") :])\n         return r\n \n     def m2i(self, pkt, x):\n@@ -3353,7 +3451,7 @@ class MultiFlagsField(_BitField[Set[str]]):\n                 if i in these_names:\n                     r.add(these_names[i].short)\n                 else:\n-                    r.add('bit {}'.format(i))\n+                    r.add(\"bit {}\".format(i))\n             x >>= 1\n             i += 1\n         return r\n@@ -3375,7 +3473,7 @@ class MultiFlagsField(_BitField[Set[str]]):\n \n \n class FixedPointField(BitField):\n-    __slots__ = ['frac_bits']\n+    __slots__ = [\"frac_bits\"]\n \n     def __init__(self, name, default, size, frac_bits=16):\n         # type: (str, int, int, int) -> None\n@@ -3412,14 +3510,14 @@ class _IPPrefixFieldBase(Field[Tuple[str, int], Tuple[bytes, int]]):\n     __slots__ = [\"wordbytes\", \"maxbytes\", \"aton\", \"ntoa\", \"length_from\"]\n \n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Tuple[str, int]\n-            wordbytes,  # type: int\n-            maxbytes,  # type: int\n-            aton,  # type: Callable[..., Any]\n-            ntoa,  # type: Callable[..., Any]\n-            length_from=None  # type: Optional[Callable[[Packet], int]]\n+        self,\n+        name,  # type: str\n+        default,  # type: Tuple[str, int]\n+        wordbytes,  # type: int\n+        maxbytes,  # type: int\n+        aton,  # type: Callable[..., Any]\n+        ntoa,  # type: Callable[..., Any]\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n     ):\n         # type: (...) -> None\n         self.wordbytes = wordbytes\n@@ -3439,7 +3537,7 @@ class _IPPrefixFieldBase(Field[Tuple[str, int], Tuple[bytes, int]]):\n     def h2i(self, pkt, x):\n         # type: (Optional[Packet], str) -> Tuple[str, int]\n         # \"fc00:1::1/64\" -> (\"fc00:1::1\", 64)\n-        [pfx, pfxlen] = x.split('/')\n+        [pfx, pfxlen] = x.split(\"/\")\n         self.aton(pfx)  # check for validity\n         return (pfx, int(pfxlen))\n \n@@ -3449,10 +3547,11 @@ class _IPPrefixFieldBase(Field[Tuple[str, int], Tuple[bytes, int]]):\n         (pfx, pfxlen) = x\n         return \"%s/%i\" % (pfx, pfxlen)\n \n-    def i2m(self,\n-            pkt,  # type: Optional[Packet]\n-            x  # type: Optional[Tuple[str, int]]\n-            ):\n+    def i2m(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Optional[Tuple[str, int]]\n+    ):\n         # type: (...) -> Tuple[bytes, int]\n         # (\"fc00:1::1\", 64) -> (b\"\\xfc\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\", 64)  # noqa: E501\n         if x is None:\n@@ -3460,7 +3559,7 @@ class _IPPrefixFieldBase(Field[Tuple[str, int], Tuple[bytes, int]]):\n         else:\n             (pfx, pfxlen) = x\n         s = self.aton(pfx)\n-        return (s[:self._numbytes(pfxlen)], pfxlen)\n+        return (s[: self._numbytes(pfxlen)], pfxlen)\n \n     def m2i(self, pkt, x):\n         # type: (Optional[Packet], Tuple[bytes, int]) -> Tuple[str, int]\n@@ -3494,36 +3593,31 @@ class _IPPrefixFieldBase(Field[Tuple[str, int], Tuple[bytes, int]]):\n         pfxlen = self.length_from(pkt)\n         numbytes = self._numbytes(pfxlen)\n         fmt = \"!%is\" % numbytes\n-        return s[numbytes:], self.m2i(pkt, (struct.unpack(fmt, s[:numbytes])[0], pfxlen))  # noqa: E501\n+        return s[numbytes:], self.m2i(\n+            pkt, (struct.unpack(fmt, s[:numbytes])[0], pfxlen)\n+        )  # noqa: E501\n \n \n class IPPrefixField(_IPPrefixFieldBase):\n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Tuple[str, int]\n-            wordbytes=1,  # type: int\n-            length_from=None  # type: Optional[Callable[[Packet], int]]\n+        self,\n+        name,  # type: str\n+        default,  # type: Tuple[str, int]\n+        wordbytes=1,  # type: int\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n     ):\n         _IPPrefixFieldBase.__init__(\n-            self,\n-            name,\n-            default,\n-            wordbytes,\n-            4,\n-            inet_aton,\n-            inet_ntoa,\n-            length_from\n+            self, name, default, wordbytes, 4, inet_aton, inet_ntoa, length_from\n         )\n \n \n class IP6PrefixField(_IPPrefixFieldBase):\n     def __init__(\n-            self,\n-            name,  # type: str\n-            default,  # type: Tuple[str, int]\n-            wordbytes=1,  # type: int\n-            length_from=None  # type: Optional[Callable[[Packet], int]]\n+        self,\n+        name,  # type: str\n+        default,  # type: Tuple[str, int]\n+        wordbytes=1,  # type: int\n+        length_from=None,  # type: Optional[Callable[[Packet], int]]\n     ):\n         # type: (...) -> None\n         _IPPrefixFieldBase.__init__(\n@@ -3534,25 +3628,33 @@ class IP6PrefixField(_IPPrefixFieldBase):\n             16,\n             lambda a: inet_pton(socket.AF_INET6, a),\n             lambda n: inet_ntop(socket.AF_INET6, n),\n-            length_from\n+            length_from,\n         )\n \n \n class UTCTimeField(Field[float, int]):\n-    __slots__ = [\"epoch\", \"delta\", \"strf\",\n-                 \"use_msec\", \"use_micro\", \"use_nano\", \"custom_scaling\"]\n-\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: int\n-                 use_msec=False,  # type: bool\n-                 use_micro=False,  # type: bool\n-                 use_nano=False,  # type: bool\n-                 epoch=None,  # type: Optional[Tuple[int, int, int, int, int, int, int, int, int]]  # noqa: E501\n-                 strf=\"%a, %d %b %Y %H:%M:%S %z\",  # type: str\n-                 custom_scaling=None,  # type: Optional[int]\n-                 fmt=\"I\"  # type: str\n-                 ):\n+    __slots__ = [\n+        \"epoch\",\n+        \"delta\",\n+        \"strf\",\n+        \"use_msec\",\n+        \"use_micro\",\n+        \"use_nano\",\n+        \"custom_scaling\",\n+    ]\n+\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: int\n+        use_msec=False,  # type: bool\n+        use_micro=False,  # type: bool\n+        use_nano=False,  # type: bool\n+        epoch=None,  # type: Optional[Tuple[int, int, int, int, int, int, int, int, int]]  # noqa: E501\n+        strf=\"%a, %d %b %Y %H:%M:%S %z\",  # type: str\n+        custom_scaling=None,  # type: Optional[int]\n+        fmt=\"I\",  # type: str\n+    ):\n         # type: (...) -> None\n         Field.__init__(self, name, default, fmt=fmt)\n         mk_epoch = EPOCH if epoch is None else calendar.timegm(epoch)\n@@ -3579,10 +3681,9 @@ class UTCTimeField(Field[float, int]):\n         x += self.delta\n         # To make negative timestamps work on all plateforms (e.g. Windows),\n         # we need a trick.\n-        t = (\n-            datetime.datetime(1970, 1, 1) +\n-            datetime.timedelta(seconds=x)\n-        ).strftime(self.strf)\n+        t = (datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=x)).strftime(\n+            self.strf\n+        )\n         return \"%s (%d)\" % (t, int(x))\n \n     def i2m(self, pkt, x):\n@@ -3604,10 +3705,7 @@ class UTCTimeField(Field[float, int]):\n class SecondsIntField(Field[float, int]):\n     __slots__ = [\"use_msec\", \"use_micro\", \"use_nano\"]\n \n-    def __init__(self, name, default,\n-                 use_msec=False,\n-                 use_micro=False,\n-                 use_nano=False):\n+    def __init__(self, name, default, use_msec=False, use_micro=False, use_nano=False):\n         # type: (str, int, bool, bool, bool) -> None\n         Field.__init__(self, name, default, \"I\")\n         self.use_msec = use_msec\n@@ -3630,15 +3728,16 @@ class SecondsIntField(Field[float, int]):\n \n \n class _ScalingField(object):\n-    def __init__(self,\n-                 name,  # type: str\n-                 default,  # type: float\n-                 scaling=1,  # type: Union[int, float]\n-                 unit=\"\",  # type: str\n-                 offset=0,  # type: Union[int, float]\n-                 ndigits=3,  # type: int\n-                 fmt=\"B\",  # type: str\n-                 ):\n+    def __init__(\n+        self,\n+        name,  # type: str\n+        default,  # type: float\n+        scaling=1,  # type: Union[int, float]\n+        unit=\"\",  # type: str\n+        offset=0,  # type: Union[int, float]\n+        ndigits=3,  # type: int\n+        fmt=\"B\",  # type: str\n+    ):\n         # type: (...) -> None\n         self.scaling = scaling\n         self.unit = unit\n@@ -3646,10 +3745,11 @@ class _ScalingField(object):\n         self.ndigits = ndigits\n         Field.__init__(self, name, default, fmt)  # type: ignore\n \n-    def i2m(self,\n-            pkt,  # type: Optional[Packet]\n-            x  # type: Optional[Union[int, float]]\n-            ):\n+    def i2m(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Optional[Union[int, float]]\n+    ):\n         # type: (...) -> Union[int, float]\n         if x is None:\n             x = 0\n@@ -3678,62 +3778,59 @@ class _ScalingField(object):\n         # type: (Optional[Packet], Union[int, float]) -> str\n         return \"%s %s\" % (\n             self.i2h(pkt, x),  # type: ignore\n-            self.unit\n+            self.unit,\n         )\n \n     def randval(self):\n         # type: () -> RandFloat\n         value = Field.randval(self)  # type: ignore\n         if value is not None:\n-            min_val = round(value.min * self.scaling + self.offset,\n-                            self.ndigits)\n-            max_val = round(value.max * self.scaling + self.offset,\n-                            self.ndigits)\n+            min_val = round(value.min * self.scaling + self.offset, self.ndigits)\n+            max_val = round(value.max * self.scaling + self.offset, self.ndigits)\n \n             return RandFloat(min(min_val, max_val), max(min_val, max_val))\n \n \n-class ScalingField(_ScalingField,\n-                   Field[Union[int, float], Union[int, float]]):\n-    \"\"\" Handle physical values which are scaled and/or offset for communication\n-\n-       Example:\n-           >>> from scapy.packet import Packet\n-           >>> class ScalingFieldTest(Packet):\n-                   fields_desc = [ScalingField('data', 0, scaling=0.1, offset=-1, unit='mV')]  # noqa: E501\n-           >>> ScalingFieldTest(data=10).show2()\n-           ###[ ScalingFieldTest ]###\n-             data= 10.0 mV\n-           >>> hexdump(ScalingFieldTest(data=10))\n-           0000  6E                                               n\n-           >>> hexdump(ScalingFieldTest(data=b\"\\x6D\"))\n-           0000  6D                                               m\n-           >>> ScalingFieldTest(data=b\"\\x6D\").show2()\n-           ###[ ScalingFieldTest ]###\n-             data= 9.9 mV\n-\n-        bytes(ScalingFieldTest(...)) will produce 0x6E in this example.\n-        0x6E is 110 (decimal). This is calculated through the scaling factor\n-        and the offset. \"data\" was set to 10, which means, we want to transfer\n-        the physical value 10 mV. To calculate the value, which has to be\n-        sent on the bus, the offset has to subtracted and the scaling has to be\n-        applied by division through the scaling factor.\n-        bytes = (data - offset) / scaling\n-        bytes = ( 10  -  (-1) ) /    0.1\n-        bytes =  110 = 0x6E\n-\n-        If you want to force a certain internal value, you can assign a byte-\n-        string to the field (data=b\"\\x6D\"). If a string of a bytes object is\n-        given to the field, no internal value conversion will be applied\n-\n-       :param name: field's name\n-       :param default: default value for the field\n-       :param scaling: scaling factor for the internal value conversion\n-       :param unit: string for the unit representation of the internal value\n-       :param offset: value to offset the internal value during conversion\n-       :param ndigits: number of fractional digits for the internal conversion\n-       :param fmt: struct.pack format used to parse and serialize the internal value from and to machine representation # noqa: E501\n-       \"\"\"\n+class ScalingField(_ScalingField, Field[Union[int, float], Union[int, float]]):\n+    \"\"\"Handle physical values which are scaled and/or offset for communication\n+\n+    Example:\n+        >>> from scapy.packet import Packet\n+        >>> class ScalingFieldTest(Packet):\n+                fields_desc = [ScalingField('data', 0, scaling=0.1, offset=-1, unit='mV')]  # noqa: E501\n+        >>> ScalingFieldTest(data=10).show2()\n+        ###[ ScalingFieldTest ]###\n+          data= 10.0 mV\n+        >>> hexdump(ScalingFieldTest(data=10))\n+        0000  6E                                               n\n+        >>> hexdump(ScalingFieldTest(data=b\"\\x6d\"))\n+        0000  6D                                               m\n+        >>> ScalingFieldTest(data=b\"\\x6d\").show2()\n+        ###[ ScalingFieldTest ]###\n+          data= 9.9 mV\n+\n+     bytes(ScalingFieldTest(...)) will produce 0x6E in this example.\n+     0x6E is 110 (decimal). This is calculated through the scaling factor\n+     and the offset. \"data\" was set to 10, which means, we want to transfer\n+     the physical value 10 mV. To calculate the value, which has to be\n+     sent on the bus, the offset has to subtracted and the scaling has to be\n+     applied by division through the scaling factor.\n+     bytes = (data - offset) / scaling\n+     bytes = ( 10  -  (-1) ) /    0.1\n+     bytes =  110 = 0x6E\n+\n+     If you want to force a certain internal value, you can assign a byte-\n+     string to the field (data=b\"\\x6d\"). If a string of a bytes object is\n+     given to the field, no internal value conversion will be applied\n+\n+    :param name: field's name\n+    :param default: default value for the field\n+    :param scaling: scaling factor for the internal value conversion\n+    :param unit: string for the unit representation of the internal value\n+    :param offset: value to offset the internal value during conversion\n+    :param ndigits: number of fractional digits for the internal conversion\n+    :param fmt: struct.pack format used to parse and serialize the internal value from and to machine representation # noqa: E501\n+    \"\"\"\n \n \n class BitScalingField(_ScalingField, BitField):  # type: ignore\n@@ -3837,13 +3934,14 @@ class UUIDField(Field[UUID, bytes]):\n         \"\"\"Checks .uuid_fmt, and raises an exception if it is not valid.\"\"\"\n         if self.uuid_fmt not in UUIDField.FORMATS:\n             raise FieldValueRangeException(\n-                \"Unsupported uuid_fmt ({})\".format(self.uuid_fmt))\n+                \"Unsupported uuid_fmt ({})\".format(self.uuid_fmt)\n+            )\n \n     def i2m(self, pkt, x):\n         # type: (Optional[Packet], Optional[UUID]) -> bytes\n         self._check_uuid_fmt()\n         if x is None:\n-            return b'\\0' * 16\n+            return b\"\\0\" * 16\n         if self.uuid_fmt == UUIDField.FORMAT_BE:\n             return x.bytes\n         elif self.uuid_fmt == UUIDField.FORMAT_LE:\n@@ -3853,10 +3951,11 @@ class UUIDField(Field[UUID, bytes]):\n         else:\n             raise FieldAttributeException(\"Unknown fmt\")\n \n-    def m2i(self,\n-            pkt,  # type: Optional[Packet]\n-            x,  # type: bytes\n-            ):\n+    def m2i(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: bytes\n+    ):\n         # type: (...) -> UUID\n         self._check_uuid_fmt()\n         if self.uuid_fmt == UUIDField.FORMAT_BE:\n@@ -3868,10 +3967,11 @@ class UUIDField(Field[UUID, bytes]):\n         else:\n             raise FieldAttributeException(\"Unknown fmt\")\n \n-    def any2i(self,\n-              pkt,  # type: Optional[Packet]\n-              x  # type: Any  # noqa: E501\n-              ):\n+    def any2i(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: Any  # noqa: E501\n+    ):\n         # type: (...) -> Optional[UUID]\n         # Python's uuid doesn't handle bytearray, so convert to an immutable\n         # type first.\n@@ -3921,10 +4021,11 @@ class UUIDEnumField(UUIDField, _EnumField[UUID]):\n         # type: (Optional[Packet], Any) -> UUID\n         return _EnumField.any2i(self, pkt, x)  # type: ignore\n \n-    def i2repr(self,\n-               pkt,  # type: Optional[Packet]\n-               x,  # type: UUID\n-               ):\n+    def i2repr(\n+        self,\n+        pkt,  # type: Optional[Packet]\n+        x,  # type: UUID\n+    ):\n         # type: (...) -> Any\n         return _EnumField.i2repr(self, pkt, x)\n \n@@ -3973,7 +4074,7 @@ class BitExtendedField(Field[Optional[int], int]):\n         # type: (Optional[Any], bytes) -> Tuple[bytes, Optional[int]]\n         val = 0\n         smask = 1 << self.extension_bit\n-        mask = 0xFF & ~ (1 << self.extension_bit)\n+        mask = 0xFF & ~(1 << self.extension_bit)\n         shift = (self.extension_bit + 1) % 8\n         i = 0\n         while s:\ndiff --git a/scapy/packet.py b/scapy/packet.py\nindex 3fb2eac..bb90f90 100644\n--- a/scapy/packet.py\n+++ b/scapy/packet.py\n@@ -24,12 +24,10 @@ import types\n import warnings\n \n from scapy.fields import (\n-    AnyField,\n     BitField,\n     ConditionalField,\n     Emph,\n     EnumField,\n-    Field,\n     FlagsField,\n     FlagValue,\n     MayEnd,\n@@ -42,30 +40,36 @@ from scapy.fields import (\n )\n from scapy.config import conf, _version_checker\n from scapy.compat import raw, orb, bytes_encode\n-from scapy.base_classes import BasePacket, Gen, SetGen, Packet_metaclass, \\\n-    _CanvasDumpExtended\n+from scapy.base_classes import (\n+    BasePacket,\n+    Gen,\n+    SetGen,\n+    Packet_metaclass,\n+    _CanvasDumpExtended,\n+)\n from scapy.interfaces import _GlobInterfaceType\n from scapy.volatile import RandField, VolatileValue\n-from scapy.utils import import_hexcap, tex_escape, colgen, issubtype, \\\n-    pretty_list, EDecimal\n+from scapy.utils import (\n+    import_hexcap,\n+    tex_escape,\n+    colgen,\n+    issubtype,\n+    pretty_list,\n+    EDecimal,\n+)\n from scapy.error import Scapy_Exception, log_runtime, warning\n from scapy.libs.test_pyx import PYX\n \n # Typing imports\n from typing import (\n     Any,\n-    Callable,\n     Dict,\n     Iterator,\n     List,\n-    NoReturn,\n     Optional,\n-    Set,\n     Tuple,\n-    Type,\n     TypeVar,\n     Union,\n-    Sequence,\n     cast,\n )\n from scapy.compat import Self\n@@ -79,30 +83,38 @@ except ImportError:\n _T = TypeVar(\"_T\", Dict[str, Any], Optional[Dict[str, Any]])\n \n \n-class Packet(\n-    BasePacket,\n-    _CanvasDumpExtended,\n-    metaclass=Packet_metaclass\n-):\n+class Packet(BasePacket, _CanvasDumpExtended, metaclass=Packet_metaclass):\n     __slots__ = [\n-        \"time\", \"sent_time\", \"name\",\n-        \"default_fields\", \"fields\", \"fieldtype\",\n-        \"overload_fields\", \"overloaded_fields\",\n+        \"time\",\n+        \"sent_time\",\n+        \"name\",\n+        \"default_fields\",\n+        \"fields\",\n+        \"fieldtype\",\n+        \"overload_fields\",\n+        \"overloaded_fields\",\n         \"packetfields\",\n-        \"original\", \"explicit\", \"raw_packet_cache\",\n-        \"raw_packet_cache_fields\", \"_pkt\", \"post_transforms\",\n+        \"original\",\n+        \"explicit\",\n+        \"raw_packet_cache\",\n+        \"raw_packet_cache_fields\",\n+        \"_pkt\",\n+        \"post_transforms\",\n         \"stop_dissection_after\",\n         # then payload, underlayer and parent\n-        \"payload\", \"underlayer\", \"parent\",\n+        \"payload\",\n+        \"underlayer\",\n+        \"parent\",\n         \"name\",\n         # used for sr()\n         \"_answered\",\n         # used when sniffing\n-        \"direction\", \"sniffed_on\",\n+        \"direction\",\n+        \"sniffed_on\",\n         # handle snaplen Vs real length\n         \"wirelen\",\n         \"comments\",\n-        \"process_information\"\n+        \"process_information\",\n     ]\n     name = None\n     fields_desc = []  # type: List[AnyField]\n@@ -128,7 +140,8 @@ class Packet(\n         # type: () -> None\n         for fval, upper in self.payload_guess:\n             print(\n-                \"%-20s  %s\" % (\n+                \"%-20s  %s\"\n+                % (\n                     upper.__name__,\n                     \", \".join(\"%-12s\" % (\"%s=%r\" % i) for i in fval.items()),\n                 )\n@@ -139,27 +152,27 @@ class Packet(\n         # type: () -> None\n         for lower, fval in self._overload_fields.items():\n             print(\n-                \"%-20s  %s\" % (\n+                \"%-20s  %s\"\n+                % (\n                     lower.__name__,\n                     \", \".join(\"%-12s\" % (\"%s=%r\" % i) for i in fval.items()),\n                 )\n             )\n \n-    def __init__(self,\n-                 _pkt=b\"\",  # type: Union[bytes, bytearray]\n-                 post_transform=None,  # type: Any\n-                 _internal=0,  # type: int\n-                 _underlayer=None,  # type: Optional[Packet]\n-                 _parent=None,  # type: Optional[Packet]\n-                 stop_dissection_after=None,  # type: Optional[Type[Packet]]\n-                 **fields  # type: Any\n-                 ):\n+    def __init__(\n+        self,\n+        _pkt=b\"\",  # type: Union[bytes, bytearray]\n+        post_transform=None,  # type: Any\n+        _internal=0,  # type: int\n+        _underlayer=None,  # type: Optional[Packet]\n+        _parent=None,  # type: Optional[Packet]\n+        stop_dissection_after=None,  # type: Optional[Type[Packet]]\n+        **fields,  # type: Any\n+    ):\n         # type: (...) -> None\n         self.time = time.time()  # type: Union[EDecimal, float]\n         self.sent_time = None  # type: Union[EDecimal, float, None]\n-        self.name = (self.__class__.__name__\n-                     if self._name is None else\n-                     self._name)\n+        self.name = self.__class__.__name__ if self._name is None else self._name\n         self.default_fields = {}  # type: Dict[str, Any]\n         self.overload_fields = self._overload_fields\n         self.overloaded_fields = {}  # type: Dict[str, Any]\n@@ -195,16 +208,22 @@ class Packet(\n                 value = fields.pop(fname)\n             except KeyError:\n                 continue\n-            self.fields[fname] = value if isinstance(value, RawVal) else \\\n-                self.get_field(fname).any2i(self, value)\n+            self.fields[fname] = (\n+                value\n+                if isinstance(value, RawVal)\n+                else self.get_field(fname).any2i(self, value)\n+            )\n         # The remaining fields are unknown\n         for fname in fields:\n             if fname in self.deprecated_fields:\n                 # Resolve deprecated fields\n                 value = fields[fname]\n                 fname = self._resolve_alias(fname)\n-                self.fields[fname] = value if isinstance(value, RawVal) else \\\n-                    self.get_field(fname).any2i(self, value)\n+                self.fields[fname] = (\n+                    value\n+                    if isinstance(value, RawVal)\n+                    else self.get_field(fname).any2i(self, value)\n+                )\n                 continue\n             raise AttributeError(fname)\n         if isinstance(post_transform, list):\n@@ -246,14 +265,18 @@ class Packet(\n     def __reduce__(self):\n         # type: () -> Tuple[Type[Packet], Tuple[bytes], Packet._PickleType]\n         \"\"\"Used by pickling methods\"\"\"\n-        return (self.__class__, (self.build(),), (\n-            self.time,\n-            self.sent_time,\n-            self.direction,\n-            self.sniffed_on,\n-            self.wirelen,\n-            self.comment\n-        ))\n+        return (\n+            self.__class__,\n+            (self.build(),),\n+            (\n+                self.time,\n+                self.sent_time,\n+                self.direction,\n+                self.sniffed_on,\n+                self.wirelen,\n+                self.comment,\n+            ),\n+        )\n \n     def __setstate__(self, state):\n         # type: (Packet._PickleType) -> Packet\n@@ -266,9 +289,10 @@ class Packet(\n         self.comment = state[5]\n         return self\n \n-    def __deepcopy__(self,\n-                     memo,  # type: Any\n-                     ):\n+    def __deepcopy__(\n+        self,\n+        memo,  # type: Any\n+    ):\n         # type: (...) -> Packet\n         \"\"\"Used by copy.deepcopy\"\"\"\n         return self.copy()\n@@ -284,9 +308,10 @@ class Packet(\n         else:\n             self.do_init_cached_fields(for_dissect_only=for_dissect_only)\n \n-    def do_init_fields(self,\n-                       flist,  # type: Sequence[AnyField]\n-                       ):\n+    def do_init_fields(\n+        self,\n+        flist,  # type: Sequence[AnyField]\n+    ):\n         # type: (...) -> None\n         \"\"\"\n         Initialize each fields of the fields_desc dict\n@@ -407,7 +432,10 @@ class Packet(\n             elif isinstance(payload, (bytes, str, bytearray, memoryview)):\n                 self.payload = conf.raw_layer(load=bytes_encode(payload))\n             else:\n-                raise TypeError(\"payload must be 'Packet', 'bytes', 'str', 'bytearray', or 'memoryview', not [%s]\" % repr(payload))  # noqa: E501\n+                raise TypeError(\n+                    \"payload must be 'Packet', 'bytes', 'str', 'bytearray', or 'memoryview', not [%s]\"\n+                    % repr(payload)\n+                )  # noqa: E501\n \n     def remove_payload(self):\n         # type: () -> None\n@@ -464,9 +492,9 @@ class Packet(\n         # type: (str) -> str\n         new_attr, version = self.deprecated_fields[attr]\n         warnings.warn(\n-            \"%s has been deprecated in favor of %s since %s !\" % (\n-                attr, new_attr, version\n-            ), DeprecationWarning\n+            \"%s has been deprecated in favor of %s since %s !\"\n+            % (attr, new_attr, version),\n+            DeprecationWarning,\n         )\n         return new_attr\n \n@@ -514,8 +542,7 @@ class Packet(\n                 any2i = lambda x, y: y  # type: Callable[..., Any]\n             else:\n                 any2i = fld.any2i\n-            self.fields[attr] = val if isinstance(val, RawVal) else \\\n-                any2i(self, val)\n+            self.fields[attr] = val if isinstance(val, RawVal) else any2i(self, val)\n             self.explicit = 0\n             self.raw_packet_cache = None\n             self.raw_packet_cache_fields = None\n@@ -570,10 +597,10 @@ class Packet(\n         \"\"\"\n         attrs = set()  # type: Set[str]\n         cls = self.__class__\n-        if hasattr(cls, '__all_slots__'):\n+        if hasattr(cls, \"__all_slots__\"):\n             attrs.update(cls.__all_slots__)\n         for bcls in cls.__mro__:\n-            if hasattr(bcls, '__dict__'):\n+            if hasattr(bcls, \"__dict__\"):\n                 attrs.update(bcls.__dict__)\n         return attrs\n \n@@ -610,15 +637,15 @@ class Packet(\n                 ncol = ct.field_name\n                 vcol = ct.field_value\n \n-            s += \" %s%s%s\" % (ncol(f.name),\n-                              ct.punct(\"=\"),\n-                              vcol(val))\n-        return \"%s%s %s %s%s%s\" % (ct.punct(\"<\"),\n-                                   ct.layer_name(self.__class__.__name__),\n-                                   s,\n-                                   ct.punct(\"|\"),\n-                                   repr(self.payload),\n-                                   ct.punct(\">\"))\n+            s += \" %s%s%s\" % (ncol(f.name), ct.punct(\"=\"), vcol(val))\n+        return \"%s%s %s %s%s%s\" % (\n+            ct.punct(\"<\"),\n+            ct.layer_name(self.__class__.__name__),\n+            s,\n+            ct.punct(\"|\"),\n+            repr(self.payload),\n+            ct.punct(\">\"),\n+        )\n \n     def __str__(self):\n         # type: () -> str\n@@ -639,6 +666,7 @@ class Packet(\n             return self / conf.raw_layer(load=bytes_encode(other))\n         else:\n             return other.__rdiv__(self)  # type: ignore\n+\n     __truediv__ = __div__\n \n     def __rdiv__(self, other):\n@@ -647,6 +675,7 @@ class Packet(\n             return conf.raw_layer(load=bytes_encode(other)) / self\n         else:\n             raise TypeError\n+\n     __rtruediv__ = __rdiv__\n \n     def __mul__(self, other):\n@@ -663,6 +692,7 @@ class Packet(\n     def __nonzero__(self):\n         # type: () -> bool\n         return True\n+\n     __bool__ = __nonzero__\n \n     def __len__(self):\n@@ -677,8 +707,9 @@ class Packet(\n         # type: (_T) -> _T\n         if fields is None:\n             return None\n-        return {fname: self.copy_field_value(fname, fval)\n-                for fname, fval in fields.items()}\n+        return {\n+            fname: self.copy_field_value(fname, fval) for fname, fval in fields.items()\n+        }\n \n     def _raw_packet_cache_field_value(self, fld, val, copy=False):\n         # type: (AnyField, Any, bool) -> Optional[Any]\n@@ -687,9 +718,7 @@ class Packet(\n         if fld.holds_packets:\n             # avoid copying whole packets (perf: #GH3894)\n             if fld.islist:\n-                return [\n-                    (_cpy(x.fields), x.payload.raw_packet_cache) for x in val\n-                ]\n+                return [(_cpy(x.fields), x.payload.raw_packet_cache) for x in val]\n             else:\n                 return (_cpy(val.fields), val.payload.raw_packet_cache)\n         elif fld.islist or fld.ismutable:\n@@ -715,8 +744,10 @@ class Packet(\n         \"\"\"\n         Create the default layer regarding fields_desc dict\n         \"\"\"\n-        if self.raw_packet_cache is not None and \\\n-                self.raw_packet_cache_fields is not None:\n+        if (\n+            self.raw_packet_cache is not None\n+            and self.raw_packet_cache_fields is not None\n+        ):\n             for fname, fval in self.raw_packet_cache_fields.items():\n                 fld, val = self.getfield_and_val(fname)\n                 if self._raw_packet_cache_field_value(fld, val) != fval:\n@@ -737,8 +768,7 @@ class Packet(\n                 except Exception as ex:\n                     try:\n                         ex.args = (\n-                            \"While building field '%s': \" % f.name +\n-                            ex.args[0],\n+                            \"While building field '%s': \" % f.name + ex.args[0],\n                         ) + ex.args[1:]\n                     except (AttributeError, IndexError):\n                         pass\n@@ -813,7 +843,7 @@ class Packet(\n                 continue\n             p = f.addfield(self, p, self.getfieldval(f.name))\n             if isinstance(p, bytes):\n-                r = p[len(q):]\n+                r = p[len(q) :]\n                 q = p\n             else:\n                 r = b\"\"\n@@ -828,13 +858,13 @@ class Packet(\n     def build_ps(self, internal=0):\n         # type: (int) -> Tuple[bytes, List[Tuple[Packet, List[Tuple[Any, Any, bytes]]]]]  # noqa: E501\n         p, lst = self.do_build_ps()\n-#        if not internal:\n-#            pkt = self\n-#            while pkt.haslayer(conf.padding_layer):\n-#                pkt = pkt.getlayer(conf.padding_layer)\n-#                lst.append( (pkt, [ (\"loakjkjd\", pkt.load, pkt.load) ] ) )\n-#                p += pkt.load\n-#                pkt = pkt.payload\n+        #        if not internal:\n+        #            pkt = self\n+        #            while pkt.haslayer(conf.padding_layer):\n+        #                pkt = pkt.getlayer(conf.padding_layer)\n+        #                lst.append( (pkt, [ (\"loakjkjd\", pkt.load, pkt.load) ] ) )\n+        #                p += pkt.load\n+        #                pkt = pkt.payload\n         return p, lst\n \n     def canvas_dump(self, layer_shift=0, rebuild=1):\n@@ -861,7 +891,7 @@ class Packet(\n \n         backcolor = colgen(0.6, 0.8, 1.0, trans=pyx.color.rgb)\n         forecolor = colgen(0.2, 0.5, 0.8, trans=pyx.color.rgb)\n-#        backcolor=makecol(0.376, 0.729, 0.525, 1.0)\n+        #        backcolor=makecol(0.376, 0.729, 0.525, 1.0)\n \n         def hexstr(x):\n             # type: (bytes) -> str\n@@ -873,14 +903,13 @@ class Packet(\n                 XDSTART + x * XMUL,\n                 (YDUMP - y) * YMUL,\n                 r\"\\tt{%s}\" % hexstr(txt),\n-                [pyx.text.size.Large]\n+                [pyx.text.size.Large],\n             )\n \n         def make_box(o):\n             # type: (pyx.bbox.bbox) -> pyx.bbox.bbox\n             return pyx.box.rect(\n-                o.left(), o.bottom(), o.width(), o.height(),\n-                relcenter=(0.5, 0.5)\n+                o.left(), o.bottom(), o.width(), o.height(), relcenter=(0.5, 0.5)\n             )\n \n         def make_frame(lst):\n@@ -895,14 +924,16 @@ class Packet(\n                 lb = lst[-1].bbox()\n                 lb.enlarge(pyx.unit.u_pt)\n                 if len(lst) == 2 and fb.left() > lb.right():\n-                    return pyx.path.path(pyx.path.moveto(fb.right(), fb.top()),\n-                                         pyx.path.lineto(fb.left(), fb.top()),\n-                                         pyx.path.lineto(fb.left(), fb.bottom()),  # noqa: E501\n-                                         pyx.path.lineto(fb.right(), fb.bottom()),  # noqa: E501\n-                                         pyx.path.moveto(lb.left(), lb.top()),\n-                                         pyx.path.lineto(lb.right(), lb.top()),\n-                                         pyx.path.lineto(lb.right(), lb.bottom()),  # noqa: E501\n-                                         pyx.path.lineto(lb.left(), lb.bottom()))  # noqa: E501\n+                    return pyx.path.path(\n+                        pyx.path.moveto(fb.right(), fb.top()),\n+                        pyx.path.lineto(fb.left(), fb.top()),\n+                        pyx.path.lineto(fb.left(), fb.bottom()),  # noqa: E501\n+                        pyx.path.lineto(fb.right(), fb.bottom()),  # noqa: E501\n+                        pyx.path.moveto(lb.left(), lb.top()),\n+                        pyx.path.lineto(lb.right(), lb.top()),\n+                        pyx.path.lineto(lb.right(), lb.bottom()),  # noqa: E501\n+                        pyx.path.lineto(lb.left(), lb.bottom()),\n+                    )  # noqa: E501\n                 else:\n                     # XXX\n                     gb = lst[1].bbox()\n@@ -911,28 +942,31 @@ class Packet(\n                     kb = lst[-2].bbox()\n                     if kb != gb and kb != lb:\n                         kb.enlarge(pyx.unit.u_pt)\n-                    return pyx.path.path(pyx.path.moveto(fb.left(), fb.top()),\n-                                         pyx.path.lineto(fb.right(), fb.top()),\n-                                         pyx.path.lineto(fb.right(), kb.bottom()),  # noqa: E501\n-                                         pyx.path.lineto(lb.right(), kb.bottom()),  # noqa: E501\n-                                         pyx.path.lineto(lb.right(), lb.bottom()),  # noqa: E501\n-                                         pyx.path.lineto(lb.left(), lb.bottom()),  # noqa: E501\n-                                         pyx.path.lineto(lb.left(), gb.top()),\n-                                         pyx.path.lineto(fb.left(), gb.top()),\n-                                         pyx.path.closepath(),)\n-\n-        def make_dump(s,   # type: bytes\n-                      shift=0,  # type: int\n-                      y=0.,  # type: float\n-                      col=None,  # type: pyx.color.color\n-                      bkcol=None,  # type: pyx.color.color\n-                      large=16  # type: int\n-                      ):\n+                    return pyx.path.path(\n+                        pyx.path.moveto(fb.left(), fb.top()),\n+                        pyx.path.lineto(fb.right(), fb.top()),\n+                        pyx.path.lineto(fb.right(), kb.bottom()),  # noqa: E501\n+                        pyx.path.lineto(lb.right(), kb.bottom()),  # noqa: E501\n+                        pyx.path.lineto(lb.right(), lb.bottom()),  # noqa: E501\n+                        pyx.path.lineto(lb.left(), lb.bottom()),  # noqa: E501\n+                        pyx.path.lineto(lb.left(), gb.top()),\n+                        pyx.path.lineto(fb.left(), gb.top()),\n+                        pyx.path.closepath(),\n+                    )\n+\n+        def make_dump(\n+            s,  # type: bytes\n+            shift=0,  # type: int\n+            y=0.0,  # type: float\n+            col=None,  # type: pyx.color.color\n+            bkcol=None,  # type: pyx.color.color\n+            large=16,  # type: int\n+        ):\n             # type: (...) -> Tuple[pyx.canvas.canvas, pyx.bbox.bbox, int, float]  # noqa: E501\n             c = pyx.canvas.canvas()\n             tlist = []\n             while s:\n-                dmp, s = s[:large - shift], s[large - shift:]\n+                dmp, s = s[: large - shift], s[large - shift :]\n                 txt = make_dump_txt(shift, y, dmp)\n                 tlist.append(txt)\n                 shift += len(dmp)\n@@ -943,7 +977,10 @@ class Packet(\n                 col = pyx.color.rgb.red\n             if bkcol is None:\n                 bkcol = pyx.color.rgb.white\n-            c.stroke(make_frame(tlist), [col, pyx.deco.filled([bkcol]), pyx.style.linewidth.Thick])  # noqa: E501\n+            c.stroke(\n+                make_frame(tlist),\n+                [col, pyx.deco.filled([bkcol]), pyx.style.linewidth.Thick],\n+            )  # noqa: E501\n             for txt in tlist:\n                 c.insert(txt)\n             return c, tlist[-1].bbox(), shift, y\n@@ -956,10 +993,8 @@ class Packet(\n             pt = pyx.text.text(\n                 XSTART,\n                 (YTXT - y) * YMUL,\n-                r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(\n-                    str(proto.name)\n-                ),\n-                [pyx.text.size.Large]\n+                r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(str(proto.name)),\n+                [pyx.text.size.Large],\n             )\n             y += 1\n             ptbb = pt.bbox()\n@@ -968,25 +1003,42 @@ class Packet(\n             canvas.insert(pt)\n             for field, fval, fdump in fields:\n                 col = next(forecolor)\n-                ft = pyx.text.text(XSTART, (YTXT - y) * YMUL, r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(field.name))  # noqa: E501\n+                ft = pyx.text.text(\n+                    XSTART,\n+                    (YTXT - y) * YMUL,\n+                    r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(field.name),\n+                )  # noqa: E501\n                 if isinstance(field, BitField):\n-                    fsize = '%sb' % field.size\n+                    fsize = \"%sb\" % field.size\n                 else:\n-                    fsize = '%sB' % len(fdump)\n-                if (hasattr(field, 'field') and\n-                        'LE' in field.field.__class__.__name__[:3] or\n-                        'LE' in field.__class__.__name__[:3]):\n-                    fsize = r'$\\scriptstyle\\langle$' + fsize\n-                st = pyx.text.text(XSTART + 3.4, (YTXT - y) * YMUL, r\"\\font\\cmbxfont=cmssbx10 scaled 600\\cmbxfont{%s}\" % fsize, [pyx.text.halign.boxright])  # noqa: E501\n+                    fsize = \"%sB\" % len(fdump)\n+                if (\n+                    hasattr(field, \"field\")\n+                    and \"LE\" in field.field.__class__.__name__[:3]\n+                    or \"LE\" in field.__class__.__name__[:3]\n+                ):\n+                    fsize = r\"$\\scriptstyle\\langle$\" + fsize\n+                st = pyx.text.text(\n+                    XSTART + 3.4,\n+                    (YTXT - y) * YMUL,\n+                    r\"\\font\\cmbxfont=cmssbx10 scaled 600\\cmbxfont{%s}\" % fsize,\n+                    [pyx.text.halign.boxright],\n+                )  # noqa: E501\n                 if isinstance(fval, str):\n                     if len(fval) > 18:\n                         fval = fval[:18] + \"[...]\"\n                 else:\n                     fval = \"\"\n-                vt = pyx.text.text(XSTART + 3.5, (YTXT - y) * YMUL, r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(fval))  # noqa: E501\n+                vt = pyx.text.text(\n+                    XSTART + 3.5,\n+                    (YTXT - y) * YMUL,\n+                    r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(fval),\n+                )  # noqa: E501\n                 y += 1.0\n                 if fdump:\n-                    dt, target, last_shift, last_y = make_dump(fdump, last_shift, last_y, col, bkcol)  # noqa: E501\n+                    dt, target, last_shift, last_y = make_dump(\n+                        fdump, last_shift, last_y, col, bkcol\n+                    )  # noqa: E501\n \n                     dtb = target\n                     vtb = vt.bbox()\n@@ -995,13 +1047,19 @@ class Packet(\n                     dtb.enlarge(pyx.unit.u_pt)\n                     try:\n                         if yd < 0:\n-                            cnx = pyx.connector.curve(bxvt, bxdt, absangle1=0, absangle2=-90)  # noqa: E501\n+                            cnx = pyx.connector.curve(\n+                                bxvt, bxdt, absangle1=0, absangle2=-90\n+                            )  # noqa: E501\n                         else:\n-                            cnx = pyx.connector.curve(bxvt, bxdt, absangle1=0, absangle2=90)  # noqa: E501\n+                            cnx = pyx.connector.curve(\n+                                bxvt, bxdt, absangle1=0, absangle2=90\n+                            )  # noqa: E501\n                     except Exception:\n                         pass\n                     else:\n-                        canvas.stroke(cnx, [pyx.style.linewidth.thin, pyx.deco.earrow.small, col])  # noqa: E501\n+                        canvas.stroke(\n+                            cnx, [pyx.style.linewidth.thin, pyx.deco.earrow.small, col]\n+                        )  # noqa: E501\n \n                     canvas.insert(dt)\n \n@@ -1044,15 +1102,21 @@ class Packet(\n             # We need to track fields with mutable values to discard\n             # .raw_packet_cache when needed.\n             if (f.islist or f.holds_packets or f.ismutable) and fval is not None:\n-                self.raw_packet_cache_fields[f.name] = \\\n+                self.raw_packet_cache_fields[f.name] = (\n                     self._raw_packet_cache_field_value(f, fval, copy=True)\n+                )\n             self.fields[f.name] = fval\n             # Nothing left to dissect\n-            if not s and (isinstance(f, MayEnd) or\n-                          (fval is not None and isinstance(f, ConditionalField) and\n-                           isinstance(f.fld, MayEnd))):\n+            if not s and (\n+                isinstance(f, MayEnd)\n+                or (\n+                    fval is not None\n+                    and isinstance(f, ConditionalField)\n+                    and isinstance(f.fld, MayEnd)\n+                )\n+            ):\n                 break\n-        self.raw_packet_cache = _raw[:-len(s)] if s else _raw\n+        self.raw_packet_cache = _raw[: -len(s)] if s else _raw\n         self.explicit = 1\n         return s\n \n@@ -1064,9 +1128,8 @@ class Packet(\n         :param str s: the raw layer\n         \"\"\"\n         if s:\n-            if (\n-                self.stop_dissection_after and\n-                isinstance(self, self.stop_dissection_after)\n+            if self.stop_dissection_after and isinstance(\n+                self, self.stop_dissection_after\n             ):\n                 # stop dissection here\n                 p = conf.raw_layer(s, _internal=1, _underlayer=self)\n@@ -1087,9 +1150,11 @@ class Packet(\n                     if issubtype(cls, Packet):\n                         log_runtime.error(\"%s dissector failed\", cls.__name__)\n                     else:\n-                        log_runtime.error(\"%s.guess_payload_class() returned \"\n-                                          \"[%s]\",\n-                                          self.__class__.__name__, repr(cls))\n+                        log_runtime.error(\n+                            \"%s.guess_payload_class() returned [%s]\",\n+                            self.__class__.__name__,\n+                            repr(cls),\n+                        )\n                     if cls is not None:\n                         raise\n                 p = conf.raw_layer(s, _internal=1, _underlayer=self)\n@@ -1120,8 +1185,7 @@ class Packet(\n         for t in self.aliastypes:\n             for fval, cls in t.payload_guess:\n                 try:\n-                    if all(v == self.getfieldval(k)\n-                           for k, v in fval.items()):\n+                    if all(v == self.getfieldval(k) for k, v in fval.items()):\n                         return cls  # type: ignore\n                 except AttributeError:\n                     pass\n@@ -1175,6 +1239,7 @@ class Packet(\n     def __iter__(self):\n         # type: () -> Iterator[Packet]\n         \"\"\"Iterates through all sub-packets generated by this Packet.\"\"\"\n+\n         def loop(todo, done, self=self):\n             # type: (List[str], Dict[str, Any], Any) -> Iterator[Packet]\n             if todo:\n@@ -1207,9 +1272,13 @@ class Packet(\n             todo = []\n             done = self.fields\n         else:\n-            todo = [k for (k, v) in itertools.chain(self.default_fields.items(),\n-                                                    self.overloaded_fields.items())\n-                    if isinstance(v, VolatileValue)] + list(self.fields)\n+            todo = [\n+                k\n+                for (k, v) in itertools.chain(\n+                    self.default_fields.items(), self.overloaded_fields.items()\n+                )\n+                if isinstance(v, VolatileValue)\n+            ] + list(self.fields)\n             done = {}\n         return loop(todo, done)\n \n@@ -1298,8 +1367,11 @@ class Packet(\n             match = issubtype\n         else:\n             match = lambda x, t: bool(x == t)\n-        if cls is None or match(self.__class__, cls) \\\n-           or cls in [self.__class__.__name__, self._name]:\n+        if (\n+            cls is None\n+            or match(self.__class__, cls)\n+            or cls in [self.__class__.__name__, self._name]\n+        ):\n             return True\n         for f in self.packetfields:\n             fvalue_gen = self.getfieldval(f.name)\n@@ -1314,16 +1386,17 @@ class Packet(\n                         return ret\n         return self.payload.haslayer(cls, _subclass=_subclass)\n \n-    def getlayer(self,\n-                 cls,  # type: Union[int, Type[Packet], str]\n-                 nb=1,  # type: int\n-                 _track=None,  # type: Optional[List[int]]\n-                 _subclass=None,  # type: Optional[bool]\n-                 **flt  # type: Any\n-                 ):\n+    def getlayer(\n+        self,\n+        cls,  # type: Union[int, Type[Packet], str]\n+        nb=1,  # type: int\n+        _track=None,  # type: Optional[List[int]]\n+        _subclass=None,  # type: Optional[bool]\n+        **flt,  # type: Any\n+    ):\n         # type: (...) -> Optional[Packet]\n         \"\"\"Return the nb^th layer that is an instance of cls, matching flt\n-values.\n+        values.\n         \"\"\"\n         if _subclass is None:\n             _subclass = self.match_subclass or None\n@@ -1346,10 +1419,15 @@ values.\n             class_name, fld = string_class_name.split(\".\", 1)\n         else:\n             class_name, fld = string_class_name, None\n-        if not class_name or match(self.__class__, class_name) \\\n-           or class_name in [self.__class__.__name__, self._name]:\n-            if all(self.getfieldval(fldname) == fldvalue\n-                   for fldname, fldvalue in flt.items()):\n+        if (\n+            not class_name\n+            or match(self.__class__, class_name)\n+            or class_name in [self.__class__.__name__, self._name]\n+        ):\n+            if all(\n+                self.getfieldval(fldname) == fldvalue\n+                for fldname, fldvalue in flt.items()\n+            ):\n                 if nb == 1:\n                     if fld is None:\n                         return self\n@@ -1366,13 +1444,15 @@ values.\n             for fvalue in fvalue_gen:\n                 if isinstance(fvalue, Packet):\n                     track = []  # type: List[int]\n-                    ret = fvalue.getlayer(class_name, nb=nb, _track=track,\n-                                          _subclass=_subclass, **flt)\n+                    ret = fvalue.getlayer(\n+                        class_name, nb=nb, _track=track, _subclass=_subclass, **flt\n+                    )\n                     if ret is not None:\n                         return ret\n                     nb = track[0]\n-        return self.payload.getlayer(class_name, nb=nb, _track=_track,\n-                                     _subclass=_subclass, **flt)\n+        return self.payload.getlayer(\n+            class_name, nb=nb, _track=_track, _subclass=_subclass, **flt\n+        )\n \n     def firstlayer(self):\n         # type: () -> Packet\n@@ -1431,13 +1511,14 @@ values.\n         \"\"\"Deprecated. Use show() method.\"\"\"\n         self.show(*args, **kargs)\n \n-    def _show_or_dump(self,\n-                      dump=False,  # type: bool\n-                      indent=3,  # type: int\n-                      lvl=\"\",  # type: str\n-                      label_lvl=\"\",  # type: str\n-                      first_call=True  # type: bool\n-                      ):\n+    def _show_or_dump(\n+        self,\n+        dump=False,  # type: bool\n+        indent=3,  # type: int\n+        lvl=\"\",  # type: str\n+        label_lvl=\"\",  # type: str\n+        first_call=True,  # type: bool\n+    ):\n         # type: (...) -> Optional[str]\n         \"\"\"\n         Internal method that shows or dumps a hierarchical view of a packet.\n@@ -1453,13 +1534,16 @@ values.\n \n         if dump:\n             from scapy.themes import ColorTheme, AnsiColorTheme\n+\n             ct: ColorTheme = AnsiColorTheme()  # No color for dump output\n         else:\n             ct = conf.color_theme\n-        s = \"%s%s %s %s\\n\" % (label_lvl,\n-                              ct.punct(\"###[\"),\n-                              ct.layer_name(self.name),\n-                              ct.punct(\"]###\"))\n+        s = \"%s%s %s %s\\n\" % (\n+            label_lvl,\n+            ct.punct(\"###[\"),\n+            ct.layer_name(self.name),\n+            ct.punct(\"]###\"),\n+        )\n         fields = self.fields_desc.copy()\n         while fields:\n             f = fields.pop(0)\n@@ -1482,29 +1566,44 @@ values.\n                 vcol = ct.field_value\n             pad = max(0, 10 - len(f.name)) * \" \"\n             fvalue = self.getfieldval(f.name)\n-            if isinstance(fvalue, Packet) or (f.islist and f.holds_packets and isinstance(fvalue, list)):  # noqa: E501\n-                s += \"%s  %s%s%s%s\\n\" % (label_lvl + lvl,\n-                                         ct.punct(\"\\\\\"),\n-                                         ncol(f.name),\n-                                         pad,\n-                                         ct.punct(\"\\\\\"))\n-                fvalue_gen = SetGen(\n-                    fvalue,\n-                    _iterpacket=0\n-                )  # type: SetGen[Packet]\n+            if isinstance(fvalue, Packet) or (\n+                f.islist and f.holds_packets and isinstance(fvalue, list)\n+            ):  # noqa: E501\n+                s += \"%s  %s%s%s%s\\n\" % (\n+                    label_lvl + lvl,\n+                    ct.punct(\"\\\\\"),\n+                    ncol(f.name),\n+                    pad,\n+                    ct.punct(\"\\\\\"),\n+                )\n+                fvalue_gen = SetGen(fvalue, _iterpacket=0)  # type: SetGen[Packet]\n                 for fvalue in fvalue_gen:\n-                    s += fvalue._show_or_dump(dump=dump, indent=indent, label_lvl=label_lvl + lvl + \"   |\", first_call=False)  # noqa: E501\n+                    s += fvalue._show_or_dump(\n+                        dump=dump,\n+                        indent=indent,\n+                        label_lvl=label_lvl + lvl + \"   |\",\n+                        first_call=False,\n+                    )  # noqa: E501\n             else:\n-                begn = \"%s  %s%s%s \" % (label_lvl + lvl,\n-                                        ncol(f.name),\n-                                        pad,\n-                                        ct.punct(\"=\"),)\n+                begn = \"%s  %s%s%s \" % (\n+                    label_lvl + lvl,\n+                    ncol(f.name),\n+                    pad,\n+                    ct.punct(\"=\"),\n+                )\n                 reprval = f.i2repr(self, fvalue)\n                 if isinstance(reprval, str):\n-                    reprval = reprval.replace(\"\\n\", \"\\n\" + \" \" * (len(label_lvl) +  # noqa: E501\n-                                                                  len(lvl) +\n-                                                                  len(f.name) +\n-                                                                  4))\n+                    reprval = reprval.replace(\n+                        \"\\n\",\n+                        \"\\n\"\n+                        + \" \"\n+                        * (\n+                            len(label_lvl)  # noqa: E501\n+                            + len(lvl)\n+                            + len(f.name)\n+                            + 4\n+                        ),\n+                    )\n                 s += \"%s%s\\n\" % (begn, vcol(reprval))\n         if self.payload:\n             s += self.payload._show_or_dump(  # type: ignore\n@@ -1512,7 +1611,7 @@ values.\n                 indent=indent,\n                 lvl=lvl + (\" \" * indent * self.show_indent),\n                 label_lvl=label_lvl,\n-                first_call=False\n+                first_call=False,\n             )\n \n         if first_call and not dump:\n@@ -1585,19 +1684,19 @@ values.\n           \"%(\" and \"%)\".\n         \"\"\"\n \n-        escape = {\"%\": \"%\",\n-                  \"(\": \"{\",\n-                  \")\": \"}\"}\n+        escape = {\"%\": \"%\", \"(\": \"{\", \")\": \"}\"}\n \n         # Evaluate conditions\n         while \"{\" in fmt:\n             i = fmt.rindex(\"{\")\n-            j = fmt[i + 1:].index(\"}\")\n-            cond = fmt[i + 1:i + j + 1]\n+            j = fmt[i + 1 :].index(\"}\")\n+            cond = fmt[i + 1 : i + j + 1]\n             k = cond.find(\":\")\n             if k < 0:\n-                raise Scapy_Exception(\"Bad condition in format string: [%s] (read sprintf doc!)\" % cond)  # noqa: E501\n-            cond, format_ = cond[:k], cond[k + 1:]\n+                raise Scapy_Exception(\n+                    \"Bad condition in format string: [%s] (read sprintf doc!)\" % cond\n+                )  # noqa: E501\n+            cond, format_ = cond[:k], cond[k + 1 :]\n             res = False\n             if cond[0] == \"!\":\n                 res = True\n@@ -1606,14 +1705,14 @@ values.\n                 res = not res\n             if not res:\n                 format_ = \"\"\n-            fmt = fmt[:i] + format_ + fmt[i + j + 2:]\n+            fmt = fmt[:i] + format_ + fmt[i + j + 2 :]\n \n         # Evaluate directives\n         s = \"\"\n         while \"%\" in fmt:\n             i = fmt.index(\"%\")\n             s += fmt[:i]\n-            fmt = fmt[i + 1:]\n+            fmt = fmt[i + 1 :]\n             if fmt and fmt[0] in escape:\n                 s += escape[fmt[0]]\n                 fmt = fmt[1:]\n@@ -1638,18 +1737,21 @@ values.\n                 if \":\" in cls:\n                     cls, snum = cls.split(\":\")\n                     num = int(snum)\n-                fmt = fmt[i + 1:]\n+                fmt = fmt[i + 1 :]\n             except Exception:\n-                raise Scapy_Exception(\"Bad format string [%%%s%s]\" % (fmt[:25], fmt[25:] and \"...\"))  # noqa: E501\n+                raise Scapy_Exception(\n+                    \"Bad format string [%%%s%s]\" % (fmt[:25], fmt[25:] and \"...\")\n+                )  # noqa: E501\n             else:\n                 if fld == \"time\":\n                     val = time.strftime(\n-                        \"%H:%M:%S.%%06i\",\n-                        time.localtime(float(self.time))\n+                        \"%H:%M:%S.%%06i\", time.localtime(float(self.time))\n                     ) % int((self.time - int(self.time)) * 1000000)\n                 elif cls == self.__class__.__name__ and hasattr(self, fld):\n                     if num > 1:\n-                        val = self.payload.sprintf(\"%%%s,%s:%s.%s%%\" % (f, cls, num - 1, fld), relax)  # noqa: E501\n+                        val = self.payload.sprintf(\n+                            \"%%%s,%s:%s.%s%%\" % (f, cls, num - 1, fld), relax\n+                        )  # noqa: E501\n                         f = \"s\"\n                     else:\n                         try:\n@@ -1674,9 +1776,9 @@ values.\n     def mysummary(self):\n         # type: () -> str\n         \"\"\"DEV: can be overloaded to return a string that summarizes the layer.\n-           Only one mysummary() is used in a whole packet summary: the one of the upper layer,  # noqa: E501\n-           except if a mysummary() also returns (as a couple) a list of layers whose  # noqa: E501\n-           mysummary() must be called if they are present.\"\"\"\n+        Only one mysummary() is used in a whole packet summary: the one of the upper layer,  # noqa: E501\n+        except if a mysummary() also returns (as a couple) a list of layers whose  # noqa: E501\n+        mysummary() must be called if they are present.\"\"\"\n         return \"\"\n \n     def _do_summary(self):\n@@ -1696,7 +1798,9 @@ values.\n             impf = []\n             for f in self.fields_desc:\n                 if f in conf.emph:\n-                    impf.append(\"%s=%s\" % (f.name, f.i2repr(self, self.getfieldval(f.name))))  # noqa: E501\n+                    impf.append(\n+                        \"%s=%s\" % (f.name, f.i2repr(self, self.getfieldval(f.name)))\n+                    )  # noqa: E501\n             ret = \"%s [%s]\" % (ret, \" \".join(impf))\n         if ret and s:\n             ret = \"%s / %s\" % (ret, s)\n@@ -1754,18 +1858,14 @@ values.\n                     fv = \"[%s]\" % \",\".join(map(Packet.command, fv))\n             elif fld.islist and isinstance(fv, list):\n                 if json:\n-                    fv = [\n-                        getattr(x, 'command', lambda: repr(x))()\n-                        for x in fv\n-                    ]\n+                    fv = [getattr(x, \"command\", lambda: repr(x))() for x in fv]\n                 else:\n                     fv = \"[%s]\" % \",\".join(\n-                        getattr(x, 'command', lambda: repr(x))()\n-                        for x in fv\n+                        getattr(x, \"command\", lambda: repr(x))() for x in fv\n                     )\n             elif isinstance(fv, FlagValue):\n                 fv = int(fv)\n-            elif callable(getattr(fv, 'command', None)):\n+            elif callable(getattr(fv, \"command\", None)):\n                 fv = fv.command(json=json)\n             else:\n                 if json:\n@@ -1786,7 +1886,7 @@ values.\n         \"\"\"\n         c = \"%s(%s)\" % (\n             self.__class__.__name__,\n-            \", \".join(\"%s=%s\" % x for x in self._command())\n+            \", \".join(\"%s=%s\" % x for x in self._command()),\n         )\n         pc = self.payload.command()\n         if pc:\n@@ -1805,7 +1905,7 @@ values.\n         dump = json.dumps({k: v for (k, v) in self._command(json=True)})\n         pc = self.payload.json()\n         if pc:\n-            dump = dump[:-1] + \", \\\"payload\\\": %s}\" % pc\n+            dump = dump[:-1] + ', \"payload\": %s}' % pc\n         return dump\n \n \n@@ -1873,6 +1973,7 @@ class NoPayload(Packet):\n     def __nonzero__(self):\n         # type: () -> bool\n         return False\n+\n     __bool__ = __nonzero__\n \n     def do_build(self):\n@@ -1937,13 +2038,14 @@ class NoPayload(Packet):\n         # type: (Union[Type[Packet], str], Optional[bool]) -> int\n         return 0\n \n-    def getlayer(self,\n-                 cls,  # type: Union[int, Type[Packet], str]\n-                 nb=1,  # type: int\n-                 _track=None,  # type: Optional[List[int]]\n-                 _subclass=None,  # type: Optional[bool]\n-                 **flt  # type: Any\n-                 ):\n+    def getlayer(\n+        self,\n+        cls,  # type: Union[int, Type[Packet], str]\n+        nb=1,  # type: int\n+        _track=None,  # type: Optional[List[int]]\n+        _subclass=None,  # type: Optional[bool]\n+        **flt,  # type: Any\n+    ):\n         # type: (...) -> Optional[Packet]\n         if _track is not None:\n             _track.append(nb)\n@@ -2033,7 +2135,8 @@ class Padding(Raw):\n     def build_padding(self):\n         # type: () -> bytes\n         return (\n-            bytes_encode(self.load) if self.raw_packet_cache is None\n+            bytes_encode(self.load)\n+            if self.raw_packet_cache is None\n             else self.raw_packet_cache\n         ) + self.payload.build_padding()\n \n@@ -2048,11 +2151,12 @@ if conf.default_l2 is None:\n #################\n \n \n-def bind_bottom_up(lower,  # type: Type[Packet]\n-                   upper,  # type: Type[Packet]\n-                   __fval=None,  # type: Optional[Any]\n-                   **fval  # type: Any\n-                   ):\n+def bind_bottom_up(\n+    lower,  # type: Type[Packet]\n+    upper,  # type: Type[Packet]\n+    __fval=None,  # type: Optional[Any]\n+    **fval,  # type: Any\n+):\n     # type: (...) -> None\n     r\"\"\"Bind 2 layers for dissection.\n     The upper layer will be chosen for dissection on top of the lower layer, if\n@@ -2070,11 +2174,12 @@ def bind_bottom_up(lower,  # type: Type[Packet]\n     lower.payload_guess.append((fval, upper))\n \n \n-def bind_top_down(lower,  # type: Type[Packet]\n-                  upper,  # type: Type[Packet]\n-                  __fval=None,  # type: Optional[Any]\n-                  **fval  # type: Any\n-                  ):\n+def bind_top_down(\n+    lower,  # type: Type[Packet]\n+    upper,  # type: Type[Packet]\n+    __fval=None,  # type: Optional[Any]\n+    **fval,  # type: Any\n+):\n     # type: (...) -> None\n     \"\"\"Bind 2 layers for building.\n     When the upper layer is added as a payload of the lower layer, all the\n@@ -2092,11 +2197,12 @@ def bind_top_down(lower,  # type: Type[Packet]\n \n \n @conf.commands.register\n-def bind_layers(lower,  # type: Type[Packet]\n-                upper,  # type: Type[Packet]\n-                __fval=None,  # type: Optional[Dict[str, int]]\n-                **fval  # type: Any\n-                ):\n+def bind_layers(\n+    lower,  # type: Type[Packet]\n+    upper,  # type: Type[Packet]\n+    __fval=None,  # type: Optional[Dict[str, int]]\n+    **fval,  # type: Any\n+):\n     # type: (...) -> None\n     \"\"\"Bind 2 layers on some specific fields' values.\n \n@@ -2109,18 +2215,19 @@ def bind_layers(lower,  # type: Type[Packet]\n     Please have a look at their docs:\n      - help(bind_bottom_up)\n      - help(bind_top_down)\n-     \"\"\"\n+    \"\"\"\n     if __fval is not None:\n         fval.update(__fval)\n     bind_top_down(lower, upper, **fval)\n     bind_bottom_up(lower, upper, **fval)\n \n \n-def split_bottom_up(lower,  # type: Type[Packet]\n-                    upper,  # type: Type[Packet]\n-                    __fval=None,  # type: Optional[Any]\n-                    **fval  # type: Any\n-                    ):\n+def split_bottom_up(\n+    lower,  # type: Type[Packet]\n+    upper,  # type: Type[Packet]\n+    __fval=None,  # type: Optional[Any]\n+    **fval,  # type: Any\n+):\n     # type: (...) -> None\n     \"\"\"This call un-links an association that was made using bind_bottom_up.\n     Have a look at help(bind_bottom_up)\n@@ -2134,14 +2241,16 @@ def split_bottom_up(lower,  # type: Type[Packet]\n             k not in params or params[k] != v for k, v in fval.items()\n         )\n         return cls != upper or params_is_invalid\n+\n     lower.payload_guess = [x for x in lower.payload_guess if do_filter(*x)]\n \n \n-def split_top_down(lower,  # type: Type[Packet]\n-                   upper,  # type: Type[Packet]\n-                   __fval=None,  # type: Optional[Any]\n-                   **fval  # type: Any\n-                   ):\n+def split_top_down(\n+    lower,  # type: Type[Packet]\n+    upper,  # type: Type[Packet]\n+    __fval=None,  # type: Optional[Any]\n+    **fval,  # type: Any\n+):\n     # type: (...) -> None\n     \"\"\"This call un-links an association that was made using bind_top_down.\n     Have a look at help(bind_top_down)\n@@ -2157,11 +2266,12 @@ def split_top_down(lower,  # type: Type[Packet]\n \n \n @conf.commands.register\n-def split_layers(lower,  # type: Type[Packet]\n-                 upper,  # type: Type[Packet]\n-                 __fval=None,  # type: Optional[Any]\n-                 **fval  # type: Any\n-                 ):\n+def split_layers(\n+    lower,  # type: Type[Packet]\n+    upper,  # type: Type[Packet]\n+    __fval=None,  # type: Optional[Any]\n+    **fval,  # type: Any\n+):\n     # type: (...) -> None\n     \"\"\"Split 2 layers previously bound.\n     This call un-links calls bind_top_down and bind_bottom_up. It is the opposite of  # noqa: E501\n@@ -2197,22 +2307,26 @@ def explore(layer=None):\n     \"\"\"\n     if layer is None:  # GUI MODE\n         if not conf.interactive:\n-            raise Scapy_Exception(\"explore() GUI-mode cannot be run in \"\n-                                  \"interactive mode. Please provide a \"\n-                                  \"'layer' parameter !\")\n+            raise Scapy_Exception(\n+                \"explore() GUI-mode cannot be run in \"\n+                \"interactive mode. Please provide a \"\n+                \"'layer' parameter !\"\n+            )\n         # 0 - Imports\n         try:\n             import prompt_toolkit\n         except ImportError:\n-            raise ImportError(\"prompt_toolkit is not installed ! \"\n-                              \"You may install IPython, which contains it, via\"\n-                              \" `pip install ipython`\")\n+            raise ImportError(\n+                \"prompt_toolkit is not installed ! \"\n+                \"You may install IPython, which contains it, via\"\n+                \" `pip install ipython`\"\n+            )\n         if not _version_checker(prompt_toolkit, (2, 0)):\n             raise ImportError(\"prompt_toolkit >= 2.0.0 is required !\")\n         # Only available with prompt_toolkit > 2.0, not released on PyPi yet\n-        from prompt_toolkit.shortcuts.dialogs import radiolist_dialog, \\\n-            button_dialog\n+        from prompt_toolkit.shortcuts.dialogs import radiolist_dialog, button_dialog\n         from prompt_toolkit.formatted_text import HTML\n+\n         # Check for prompt_toolkit >= 3.0.0\n         call_ptk = lambda x: cast(str, x)  # type: Callable[[Any], str]\n         if _version_checker(prompt_toolkit, (3, 0)):\n@@ -2222,28 +2336,31 @@ def explore(layer=None):\n             title=\"Scapy v%s\" % conf.version,\n             text=HTML(\n                 '<style bg=\"white\" fg=\"red\">Chose the type of packets'\n-                ' you want to explore:</style>'\n+                \" you want to explore:</style>\"\n             ),\n             buttons=[\n                 (\"Layers\", \"layers\"),\n                 (\"Contribs\", \"contribs\"),\n-                (\"Cancel\", \"cancel\")\n-            ])\n+                (\"Cancel\", \"cancel\"),\n+            ],\n+        )\n         action = call_ptk(btn_diag)\n         # 2 - Retrieve list of Packets\n         if action == \"layers\":\n             # Get all loaded layers\n             lvalues = conf.layers.layers()\n             # Restrict to layers-only (not contribs) + packet.py and asn1*.py\n-            values = [x for x in lvalues if (\"layers\" in x[0] or\n-                                             \"packet\" in x[0] or\n-                                             \"asn1\" in x[0])]\n+            values = [\n+                x\n+                for x in lvalues\n+                if (\"layers\" in x[0] or \"packet\" in x[0] or \"asn1\" in x[0])\n+            ]\n         elif action == \"contribs\":\n             # Get all existing contribs\n             from scapy.main import list_contrib\n+\n             cvalues = cast(List[Dict[str, str]], list_contrib(ret=True))\n-            values = [(x['name'], x['description'])\n-                      for x in cvalues]\n+            values = [(x[\"name\"], x[\"description\"]) for x in cvalues]\n             # Remove very specific modules\n             values = [x for x in values if \"can\" not in x[0]]\n         else:\n@@ -2280,20 +2397,21 @@ def explore(layer=None):\n             # Generate tests & form\n             folders = list(current.keys())\n             _radio_values = [\n-                (\"$\" + name, str('[+] ' + name.capitalize()))\n-                for name in folders if not name.startswith(\"_\")\n+                (\"$\" + name, str(\"[+] \" + name.capitalize()))\n+                for name in folders\n+                if not name.startswith(\"_\")\n             ] + current.get(\"_l\", [])  # type: List[str]\n             cur_path = \"\"\n             if previous:\n                 cur_path = \".\".join(\n                     itertools.chain(\n                         (x[\"_name\"] for x in previous[1:]),  # type: ignore\n-                        (current[\"_name\"],)\n+                        (current[\"_name\"],),\n                     )\n                 )\n-            extra_text = (\n-                '\\n<style bg=\"white\" fg=\"green\">> scapy.%s</style>'\n-            ) % (action + (\".\" + cur_path if cur_path else \"\"))\n+            extra_text = ('\\n<style bg=\"white\" fg=\"green\">> scapy.%s</style>') % (\n+                action + (\".\" + cur_path if cur_path else \"\")\n+            )\n             # Show popup\n             rd_diag = radiolist_dialog(\n                 values=_radio_values,\n@@ -2301,11 +2419,12 @@ def explore(layer=None):\n                 text=HTML(\n                     (\n                         '<style bg=\"white\" fg=\"red\">Please select a file'\n-                        'among the following, to see all layers contained in'\n-                        ' it:</style>'\n-                    ) + extra_text\n+                        \"among the following, to see all layers contained in\"\n+                        \" it:</style>\"\n+                    )\n+                    + extra_text\n                 ),\n-                cancel_text=\"Back\" if previous else \"Cancel\"\n+                cancel_text=\"Back\" if previous else \"Cancel\",\n             )\n             result = call_ptk(rd_diag)\n             if result is None:\n@@ -2326,6 +2445,7 @@ def explore(layer=None):\n         # 4 - (Contrib only): load contrib\n         if action == \"contribs\":\n             from scapy.main import load_contrib\n+\n             load_contrib(result)\n             result = \"scapy.contrib.\" + result\n     else:  # NON-GUI MODE\n@@ -2340,9 +2460,12 @@ def explore(layer=None):\n                 if layer.startswith(\"scapy.contrib.\"):\n                     layer = layer.replace(\"scapy.contrib.\", \"\")\n                 from scapy.main import load_contrib\n+\n                 load_contrib(layer)\n-                result_layer, result_contrib = ((\"scapy.layers.%s\" % layer),\n-                                                (\"scapy.contrib.%s\" % layer))\n+                result_layer, result_contrib = (\n+                    (\"scapy.layers.%s\" % layer),\n+                    (\"scapy.contrib.%s\" % layer),\n+                )\n                 if result_layer in conf.layers.ldict:\n                     result = result_layer\n                 elif result_contrib in conf.layers.ldict:\n@@ -2366,9 +2489,10 @@ def explore(layer=None):\n     print(pretty_list(rtlst, [(\"Class\", \"Name\")], borders=True))\n \n \n-def _pkt_ls(obj,  # type: Union[Packet, Type[Packet]]\n-            verbose=False,  # type: bool\n-            ):\n+def _pkt_ls(\n+    obj,  # type: Union[Packet, Type[Packet]]\n+    verbose=False,  # type: bool\n+):\n     # type: (...) -> List[Tuple[str, Type[AnyField], str, str, List[str]]]  # noqa: E501\n     \"\"\"Internal function used to resolve `fields_desc` to display it.\n \n@@ -2390,13 +2514,16 @@ def _pkt_ls(obj,  # type: Union[Packet, Type[Packet]]\n             cur_fld = cur_fld.fld\n         name = cur_fld.name\n         default = cur_fld.default\n-        if verbose and isinstance(cur_fld, EnumField) \\\n-           and hasattr(cur_fld, \"i2s\") and cur_fld.i2s:\n+        if (\n+            verbose\n+            and isinstance(cur_fld, EnumField)\n+            and hasattr(cur_fld, \"i2s\")\n+            and cur_fld.i2s\n+        ):\n             if len(cur_fld.i2s or []) < 50:\n                 long_attrs.extend(\n                     \"%s: %d\" % (strval, numval)\n-                    for numval, strval in\n-                    sorted(cur_fld.i2s.items())\n+                    for numval, strval in sorted(cur_fld.i2s.items())\n                 )\n         elif isinstance(cur_fld, MultiEnumField):\n             if isinstance(obj, Packet):\n@@ -2406,49 +2533,41 @@ def _pkt_ls(obj,  # type: Union[Packet, Type[Packet]]\n             fld_depend = cur_fld.depends_on(obj_pkt)\n             attrs.append(\"Depends on %s\" % fld_depend)\n             if verbose:\n-                cur_i2s = cur_fld.i2s_multi.get(\n-                    cur_fld.depends_on(obj_pkt), {}\n-                )\n+                cur_i2s = cur_fld.i2s_multi.get(cur_fld.depends_on(obj_pkt), {})\n                 if len(cur_i2s) < 50:\n                     long_attrs.extend(\n                         \"%s: %d\" % (strval, numval)\n-                        for numval, strval in\n-                        sorted(cur_i2s.items())\n+                        for numval, strval in sorted(cur_i2s.items())\n                     )\n         elif verbose and isinstance(cur_fld, FlagsField):\n             names = cur_fld.names\n             long_attrs.append(\", \".join(names))\n         elif isinstance(cur_fld, MultipleTypeField):\n             default = cur_fld.dflt.default\n-            attrs.append(\", \".join(\n-                x[0].__class__.__name__ for x in\n-                itertools.chain(cur_fld.flds, [(cur_fld.dflt,)])\n-            ))\n+            attrs.append(\n+                \", \".join(\n+                    x[0].__class__.__name__\n+                    for x in itertools.chain(cur_fld.flds, [(cur_fld.dflt,)])\n+                )\n+            )\n \n         cls = cur_fld.__class__\n-        class_name_extras = \"(%s)\" % (\n-            \", \".join(attrs)\n-        ) if attrs else \"\"\n+        class_name_extras = \"(%s)\" % (\", \".join(attrs)) if attrs else \"\"\n         if isinstance(cur_fld, BitField):\n             class_name_extras += \" (%d bit%s)\" % (\n                 cur_fld.size,\n-                \"s\" if cur_fld.size > 1 else \"\"\n+                \"s\" if cur_fld.size > 1 else \"\",\n             )\n-        fields.append(\n-            (name,\n-             cls,\n-             class_name_extras,\n-             repr(default),\n-             long_attrs)\n-        )\n+        fields.append((name, cls, class_name_extras, repr(default), long_attrs))\n     return fields\n \n \n @conf.commands.register\n-def ls(obj=None,  # type: Optional[Union[str, Packet, Type[Packet]]]\n-       case_sensitive=False,  # type: bool\n-       verbose=False  # type: bool\n-       ):\n+def ls(\n+    obj=None,  # type: Optional[Union[str, Packet, Type[Packet]]]\n+    case_sensitive=False,  # type: bool\n+    verbose=False,  # type: bool\n+):\n     # type: (...) -> None\n     \"\"\"List  available layers, or infos on a given layer class or name.\n \n@@ -2462,41 +2581,42 @@ def ls(obj=None,  # type: Optional[Union[str, Packet, Type[Packet]]]\n             tip = True\n             all_layers = sorted(conf.layers, key=lambda x: x.__name__)\n         else:\n-            pattern = re.compile(\n-                obj,\n-                0 if case_sensitive else re.I\n-            )\n+            pattern = re.compile(obj, 0 if case_sensitive else re.I)\n             # We first order by accuracy, then length\n             if case_sensitive:\n                 sorter = lambda x: (x.__name__.index(obj), len(x.__name__))\n             else:\n                 obj = obj.lower()\n-                sorter = lambda x: (x.__name__.lower().index(obj),\n-                                    len(x.__name__))\n-            all_layers = sorted((layer for layer in conf.layers\n-                                 if (isinstance(layer.__name__, str) and\n-                                     pattern.search(layer.__name__)) or\n-                                 (isinstance(layer.name, str) and\n-                                     pattern.search(layer.name))),\n-                                key=sorter)\n+                sorter = lambda x: (x.__name__.lower().index(obj), len(x.__name__))\n+            all_layers = sorted(\n+                (\n+                    layer\n+                    for layer in conf.layers\n+                    if (\n+                        isinstance(layer.__name__, str)\n+                        and pattern.search(layer.__name__)\n+                    )\n+                    or (isinstance(layer.name, str) and pattern.search(layer.name))\n+                ),\n+                key=sorter,\n+            )\n         for layer in all_layers:\n             print(\"%-10s : %s\" % (layer.__name__, layer._name))\n         if tip and conf.interactive:\n-            print(\"\\nTIP: You may use explore() to navigate through all \"\n-                  \"layers using a clear GUI\")\n+            print(\n+                \"\\nTIP: You may use explore() to navigate through all \"\n+                \"layers using a clear GUI\"\n+            )\n     else:\n         try:\n-            fields = _pkt_ls(\n-                obj,\n-                verbose=verbose\n-            )\n+            fields = _pkt_ls(obj, verbose=verbose)\n             is_pkt = isinstance(obj, Packet)\n             # Print\n             for fname, cls, clsne, dflt, long_attrs in fields:\n                 clsinfo = cls.__name__ + \" \" + clsne\n-                print(\"%-10s : %-35s =\" % (fname, clsinfo), end=' ')\n+                print(\"%-10s : %-35s =\" % (fname, clsinfo), end=\" \")\n                 if is_pkt:\n-                    print(\"%-15r\" % (getattr(obj, fname),), end=' ')\n+                    print(\"%-15r\" % (getattr(obj, fname),), end=\" \")\n                 print(\"(%r)\" % (dflt,))\n                 for attr in long_attrs:\n                     print(\"%-15s%s\" % (\"\", attr))\n@@ -2546,11 +2666,12 @@ def rfc(cls, ret=False, legend=True):\n             # Add padding optionally\n             if isinstance(f, PadField):\n                 if isinstance(f._align, tuple):\n-                    pad = - cur_len % (f._align[0] * 8)\n+                    pad = -cur_len % (f._align[0] * 8)\n                 else:\n-                    pad = - cur_len % (f._align * 8)\n+                    pad = -cur_len % (f._align * 8)\n                 if pad:\n                     yield \"padding\", pad\n+\n     for fname, flen in _iterfields():\n         cur_len += flen\n         ident += 1\n@@ -2596,22 +2717,18 @@ def rfc(cls, ret=False, legend=True):\n                 # and fill it with \" \"\n                 pos_above = pos_above + pos_above % 2\n                 sep = (\n-                    sep[:1 + pos_above] +\n-                    \" \" * (pos_below - pos_above) +\n-                    sep[1 + pos_below:]\n+                    sep[: 1 + pos_above]\n+                    + \" \" * (pos_below - pos_above)\n+                    + sep[1 + pos_below :]\n                 )\n         # line is complete\n         seps.append(sep)\n     # Graph\n     result = \"\"\n     # Bytes markers\n-    result += \" \" + (\" \" * 19).join(\n-        str(x) for x in range(4)\n-    ) + \"\\n\"\n+    result += \" \" + (\" \" * 19).join(str(x) for x in range(4)) + \"\\n\"\n     # Bits markers\n-    result += \" \" + \" \".join(\n-        str(x % 10) for x in range(32)\n-    ) + \"\\n\"\n+    result += \" \" + \" \".join(str(x % 10) for x in range(32)) + \"\\n\"\n     # Add fields and their separations\n     for line, sep in zip(lines, seps):\n         result += sep\n@@ -2633,13 +2750,14 @@ def rfc(cls, ret=False, legend=True):\n #  Fuzzing  #\n #############\n \n-_P = TypeVar('_P', bound=Packet)\n+_P = TypeVar(\"_P\", bound=Packet)\n \n \n @conf.commands.register\n-def fuzz(p,  # type: _P\n-         _inplace=0,  # type: int\n-         ):\n+def fuzz(\n+    p,  # type: _P\n+    _inplace=0,  # type: int\n+):\n     # type: (...) -> _P\n     \"\"\"\n     Transform a layer into a fuzzy layer by replacing some default values\n"
    },
    {
        "id": 268,
        "sha_fail": "db0f6d63aa4936fe684e6f82e5e6b110787adafa",
        "diff": "diff --git a/utest/utils/test_error.py b/utest/utils/test_error.py\nindex 2b2029c..99156da 100644\n--- a/utest/utils/test_error.py\n+++ b/utest/utils/test_error.py\n@@ -22,7 +22,6 @@ def format_message():\n \n \n class TestGetErrorDetails(unittest.TestCase):\n-\n     def test_get_error_details(self):\n         for exception, args, exp_msg in [\n             (AssertionError, [\"My Error\"], \"My Error\"),\n@@ -91,7 +90,6 @@ class TestGetErrorDetails(unittest.TestCase):\n \n \n class TestRemoveRobotEntriesFromTraceback(unittest.TestCase):\n-\n     def test_both_robot_and_non_robot_entries(self):\n         def raises():\n             raise Exception\n"
    },
    {
        "id": 269,
        "sha_fail": "f57afa39e8c9dc4b57c95a021af8a588f6c8c822",
        "diff": "diff --git a/qutebrowser/browser/browsertab.py b/qutebrowser/browser/browsertab.py\nindex 626127c..b6213cb 100644\n--- a/qutebrowser/browser/browsertab.py\n+++ b/qutebrowser/browser/browsertab.py\n@@ -9,12 +9,22 @@ import pathlib\n import itertools\n import functools\n import dataclasses\n-from typing import (cast, TYPE_CHECKING, Any, Optional, Union)\n+from typing import cast, TYPE_CHECKING, Any, Optional, Union\n from collections.abc import Iterable, Sequence, Callable\n \n from qutebrowser.qt import machinery\n-from qutebrowser.qt.core import (pyqtSignal, pyqtSlot, QUrl, QObject, QSizeF, Qt,\n-                          QEvent, QPoint, QRect, QTimer)\n+from qutebrowser.qt.core import (\n+    pyqtSignal,\n+    pyqtSlot,\n+    QUrl,\n+    QObject,\n+    QSizeF,\n+    Qt,\n+    QEvent,\n+    QPoint,\n+    QRect,\n+    QTimer,\n+)\n from qutebrowser.qt.gui import QKeyEvent, QIcon, QPixmap\n from qutebrowser.qt.widgets import QApplication, QWidget\n from qutebrowser.qt.printsupport import QPrintDialog, QPrinter\n@@ -24,12 +34,23 @@ if TYPE_CHECKING:\n     from qutebrowser.qt.webkit import QWebHistory, QWebHistoryItem\n     from qutebrowser.qt.webkitwidgets import QWebPage\n     from qutebrowser.qt.webenginecore import (\n-        QWebEngineHistory, QWebEngineHistoryItem, QWebEnginePage)\n+        QWebEngineHistory,\n+        QWebEngineHistoryItem,\n+        QWebEnginePage,\n+    )\n \n from qutebrowser.keyinput import modeman\n from qutebrowser.config import config, websettings\n-from qutebrowser.utils import (utils, objreg, usertypes, log, qtutils,\n-                               urlutils, message, jinja, version)\n+from qutebrowser.utils import (\n+    utils,\n+    objreg,\n+    usertypes,\n+    log,\n+    qtutils,\n+    urlutils,\n+    message,\n+    jinja,\n+)\n from qutebrowser.misc import miscwidgets, objects, sessions\n from qutebrowser.browser import eventfilter, inspector\n from qutebrowser.qt import sip\n@@ -45,9 +66,7 @@ tab_id_gen = itertools.count(0)\n _WidgetType = Union[\"WebView\", \"WebEngineView\"]\n \n \n-def create(win_id: int,\n-           private: bool,\n-           parent: QWidget = None) -> 'AbstractTab':\n+def create(win_id: int, private: bool, parent: QWidget = None) -> \"AbstractTab\":\n     \"\"\"Get a QtWebKit/QtWebEngine tab object.\n \n     Args:\n@@ -60,28 +79,28 @@ def create(win_id: int,\n     mode_manager = modeman.instance(win_id)\n     if objects.backend == usertypes.Backend.QtWebEngine:\n         from qutebrowser.browser.webengine import webenginetab\n+\n         tab_class: type[AbstractTab] = webenginetab.WebEngineTab\n     elif objects.backend == usertypes.Backend.QtWebKit:\n         from qutebrowser.browser.webkit import webkittab\n+\n         tab_class = webkittab.WebKitTab\n     else:\n         raise utils.Unreachable(objects.backend)\n-    return tab_class(win_id=win_id, mode_manager=mode_manager, private=private,\n-                     parent=parent)\n+    return tab_class(\n+        win_id=win_id, mode_manager=mode_manager, private=private, parent=parent\n+    )\n \n \n class WebTabError(Exception):\n-\n     \"\"\"Base class for various errors.\"\"\"\n \n \n class UnsupportedOperationError(WebTabError):\n-\n     \"\"\"Raised when an operation is not supported with the given backend.\"\"\"\n \n \n class TerminationStatus(enum.Enum):\n-\n     \"\"\"How a QtWebEngine renderer process terminated.\n \n     Also see QWebEnginePage::RenderProcessTerminationStatus\n@@ -101,7 +120,6 @@ class TerminationStatus(enum.Enum):\n \n @dataclasses.dataclass\n class TabData:\n-\n     \"\"\"A simple namespace with a fixed set of attributes.\n \n     Attributes:\n@@ -123,7 +141,7 @@ class TabData:\n \n     keep_icon: bool = False\n     viewing_source: bool = False\n-    inspector: Optional['AbstractWebInspector'] = None\n+    inspector: Optional[\"AbstractWebInspector\"] = None\n     open_target: usertypes.ClickTarget = usertypes.ClickTarget.normal\n     override_target: Optional[usertypes.ClickTarget] = None\n     pinned: bool = False\n@@ -134,17 +152,19 @@ class TabData:\n     splitter: Optional[miscwidgets.InspectorSplitter] = None\n \n     def should_show_icon(self) -> bool:\n-        return (config.val.tabs.favicons.show == 'always' or\n-                config.val.tabs.favicons.show == 'pinned' and self.pinned)\n+        return (\n+            config.val.tabs.favicons.show == \"always\"\n+            or config.val.tabs.favicons.show == \"pinned\"\n+            and self.pinned\n+        )\n \n \n class AbstractAction:\n-\n     \"\"\"Attribute ``action`` of AbstractTab for Qt WebActions.\"\"\"\n \n-    action_base: type[Union['QWebPage.WebAction', 'QWebEnginePage.WebAction']]\n+    action_base: type[Union[\"QWebPage.WebAction\", \"QWebEnginePage.WebAction\"]]\n \n-    def __init__(self, tab: 'AbstractTab') -> None:\n+    def __init__(self, tab: \"AbstractTab\") -> None:\n         self._widget = cast(_WidgetType, None)\n         self._tab = tab\n \n@@ -170,7 +190,7 @@ class AbstractAction:\n \n     def _show_html_source(self, html: str) -> None:\n         \"\"\"Show the given HTML as source page.\"\"\"\n-        tb = objreg.get('tabbed-browser', scope='window', window=self._tab.win_id)\n+        tb = objreg.get(\"tabbed-browser\", scope=\"window\", window=self._tab.win_id)\n         new_tab = tb.tabopen(background=False, related=True)\n         new_tab.set_html(html, self._tab.url())\n         new_tab.data.viewing_source = True\n@@ -178,8 +198,8 @@ class AbstractAction:\n     def _show_source_fallback(self, source: str) -> None:\n         \"\"\"Show source with pygments unavailable.\"\"\"\n         html = jinja.render(\n-            'pre.html',\n-            title='Source',\n+            \"pre.html\",\n+            title=\"Source\",\n             content=source,\n             preamble=\"Note: The optional Pygments dependency wasn't found - \"\n             \"showing unhighlighted source.\",\n@@ -187,7 +207,6 @@ class AbstractAction:\n         self._show_html_source(html)\n \n     def _show_source_pygments(self) -> None:\n-\n         def show_source_cb(source: str) -> None:\n             \"\"\"Show source as soon as it's ready.\"\"\"\n             try:\n@@ -202,7 +221,8 @@ class AbstractAction:\n             try:\n                 lexer = pygments.lexers.HtmlLexer()\n                 formatter = pygments.formatters.HtmlFormatter(\n-                    full=True, linenos='table')\n+                    full=True, linenos=\"table\"\n+                )\n             except AttributeError:\n                 # Remaining namespace package from Pygments\n                 self._show_source_fallback(source)\n@@ -215,13 +235,12 @@ class AbstractAction:\n \n \n class AbstractPrinting(QObject):\n-\n     \"\"\"Attribute ``printing`` of AbstractTab for printing the page.\"\"\"\n \n     printing_finished = pyqtSignal(bool)\n     pdf_printing_finished = pyqtSignal(str, bool)  # filename, ok\n \n-    def __init__(self, tab: 'AbstractTab', parent: QWidget = None) -> None:\n+    def __init__(self, tab: \"AbstractTab\", parent: QWidget = None) -> None:\n         super().__init__(parent)\n         self._widget = cast(_WidgetType, None)\n         self._tab = tab\n@@ -289,7 +308,6 @@ class AbstractPrinting(QObject):\n \n @dataclasses.dataclass\n class SearchMatch:\n-\n     \"\"\"The currently highlighted search match.\n \n     Attributes:\n@@ -316,12 +334,11 @@ class SearchMatch:\n \n     def at_limit(self, going_up: bool) -> bool:\n         \"\"\"Whether the SearchMatch is currently at the first/last result.\"\"\"\n-        return (\n-            self.total != 0 and\n-            (\n-                going_up and self.current == 1 or\n-                not going_up and self.current == self.total\n-            )\n+        return self.total != 0 and (\n+            going_up\n+            and self.current == 1\n+            or not going_up\n+            and self.current == self.total\n         )\n \n     def __str__(self) -> str:\n@@ -329,7 +346,6 @@ class SearchMatch:\n \n \n class SearchNavigationResult(enum.Enum):\n-\n     \"\"\"The outcome of calling prev_/next_result.\"\"\"\n \n     found = enum.auto()\n@@ -343,7 +359,6 @@ class SearchNavigationResult(enum.Enum):\n \n \n class AbstractSearch(QObject):\n-\n     \"\"\"Attribute ``search`` of AbstractTab for doing searches.\n \n     Attributes:\n@@ -369,7 +384,7 @@ class AbstractSearch(QObject):\n     _Callback = Callable[[bool], None]\n     _NavCallback = Callable[[SearchNavigationResult], None]\n \n-    def __init__(self, tab: 'AbstractTab', parent: QWidget = None):\n+    def __init__(self, tab: \"AbstractTab\", parent: QWidget = None):\n         super().__init__(parent)\n         self._tab = tab\n         self._widget = cast(_WidgetType, None)\n@@ -393,10 +408,14 @@ class AbstractSearch(QObject):\n         }\n         return mapping[ignore_case]\n \n-    def search(self, text: str, *,\n-               ignore_case: usertypes.IgnoreCase = usertypes.IgnoreCase.never,\n-               reverse: bool = False,\n-               result_cb: _Callback = None) -> None:\n+    def search(\n+        self,\n+        text: str,\n+        *,\n+        ignore_case: usertypes.IgnoreCase = usertypes.IgnoreCase.never,\n+        reverse: bool = False,\n+        result_cb: _Callback = None,\n+    ) -> None:\n         \"\"\"Find the given text on the page.\n \n         Args:\n@@ -431,10 +450,9 @@ class AbstractSearch(QObject):\n \n \n class AbstractZoom(QObject):\n-\n     \"\"\"Attribute ``zoom`` of AbstractTab for controlling zoom.\"\"\"\n \n-    def __init__(self, tab: 'AbstractTab', parent: QWidget = None) -> None:\n+    def __init__(self, tab: \"AbstractTab\", parent: QWidget = None) -> None:\n         super().__init__(parent)\n         self._tab = tab\n         self._widget = cast(_WidgetType, None)\n@@ -446,7 +464,7 @@ class AbstractZoom(QObject):\n \n     @pyqtSlot(str)\n     def _on_config_changed(self, option: str) -> None:\n-        if option in ['zoom.levels', 'zoom.default']:\n+        if option in [\"zoom.levels\", \"zoom.default\"]:\n             if not self._default_zoom_changed:\n                 factor = float(config.val.zoom.default) / 100\n                 self.set_factor(factor)\n@@ -458,7 +476,8 @@ class AbstractZoom(QObject):\n         It is a NeighborList with the zoom levels.\"\"\"\n         levels = config.val.zoom.levels\n         self._neighborlist: usertypes.NeighborList[float] = usertypes.NeighborList(\n-            levels, mode=usertypes.NeighborList.Modes.edge)\n+            levels, mode=usertypes.NeighborList.Modes.edge\n+        )\n         self._neighborlist.fuzzyval = config.val.zoom.default\n \n     def apply_offset(self, offset: int) -> float:\n@@ -506,7 +525,6 @@ class AbstractZoom(QObject):\n \n \n class SelectionState(enum.Enum):\n-\n     \"\"\"Possible states of selection in caret mode.\n \n     NOTE: Names need to line up with SelectionState in caret.js!\n@@ -518,7 +536,6 @@ class SelectionState(enum.Enum):\n \n \n class AbstractCaret(QObject):\n-\n     \"\"\"Attribute ``caret`` of AbstractTab for caret browsing.\"\"\"\n \n     #: Signal emitted when the selection was toggled.\n@@ -526,10 +543,12 @@ class AbstractCaret(QObject):\n     #: Emitted when a ``follow_selection`` action is done.\n     follow_selected_done = pyqtSignal()\n \n-    def __init__(self,\n-                 tab: 'AbstractTab',\n-                 mode_manager: modeman.ModeManager,\n-                 parent: QWidget = None) -> None:\n+    def __init__(\n+        self,\n+        tab: \"AbstractTab\",\n+        mode_manager: modeman.ModeManager,\n+        parent: QWidget = None,\n+    ) -> None:\n         super().__init__(parent)\n         self._widget = cast(_WidgetType, None)\n         self._mode_manager = mode_manager\n@@ -603,7 +622,9 @@ class AbstractCaret(QObject):\n     def _follow_enter(self, tab: bool) -> None:\n         \"\"\"Follow a link by faking an enter press.\"\"\"\n         if tab:\n-            self._tab.fake_key_press(Qt.Key.Key_Enter, modifier=Qt.KeyboardModifier.ControlModifier)\n+            self._tab.fake_key_press(\n+                Qt.Key.Key_Enter, modifier=Qt.KeyboardModifier.ControlModifier\n+            )\n         else:\n             self._tab.fake_key_press(Qt.Key.Key_Enter)\n \n@@ -612,7 +633,6 @@ class AbstractCaret(QObject):\n \n \n class AbstractScroller(QObject):\n-\n     \"\"\"Attribute ``scroller`` of AbstractTab to manage scroll position.\"\"\"\n \n     #: Signal emitted when the scroll position changed (int, int)\n@@ -621,17 +641,18 @@ class AbstractScroller(QObject):\n     #: Used to set the special ' mark so the user can return.\n     before_jump_requested = pyqtSignal()\n \n-    def __init__(self, tab: 'AbstractTab', parent: QWidget = None):\n+    def __init__(self, tab: \"AbstractTab\", parent: QWidget = None):\n         super().__init__(parent)\n         self._tab = tab\n         self._widget = cast(_WidgetType, None)\n-        if 'log-scroll-pos' in objects.debug_flags:\n+        if \"log-scroll-pos\" in objects.debug_flags:\n             self.perc_changed.connect(self._log_scroll_pos_change)\n \n     @pyqtSlot()\n     def _log_scroll_pos_change(self) -> None:\n         log.webview.vdebug(  # type: ignore[attr-defined]\n-            \"Scroll position changed to {}\".format(self.pos_px()))\n+            \"Scroll position changed to {}\".format(self.pos_px())\n+        )\n \n     def _init_widget(self, widget: _WidgetType) -> None:\n         self._widget = widget\n@@ -689,7 +710,6 @@ class AbstractScroller(QObject):\n \n \n class AbstractHistoryPrivate:\n-\n     \"\"\"Private API related to the history.\"\"\"\n \n     _history: Union[\"QWebHistory\", \"QWebEngineHistory\"]\n@@ -708,18 +728,17 @@ class AbstractHistoryPrivate:\n \n \n class AbstractHistory:\n-\n     \"\"\"The history attribute of a AbstractTab.\"\"\"\n \n-    def __init__(self, tab: 'AbstractTab') -> None:\n+    def __init__(self, tab: \"AbstractTab\") -> None:\n         self._tab = tab\n-        self._history = cast(Union['QWebHistory', 'QWebEngineHistory'], None)\n+        self._history = cast(Union[\"QWebHistory\", \"QWebEngineHistory\"], None)\n         self.private_api = AbstractHistoryPrivate()\n \n     def __len__(self) -> int:\n         raise NotImplementedError\n \n-    def __iter__(self) -> Iterable[Union['QWebHistoryItem', 'QWebEngineHistoryItem']]:\n+    def __iter__(self) -> Iterable[Union[\"QWebHistoryItem\", \"QWebEngineHistoryItem\"]]:\n         raise NotImplementedError\n \n     def _check_count(self, count: int) -> None:\n@@ -730,7 +749,7 @@ class AbstractHistory:\n     def current_idx(self) -> int:\n         raise NotImplementedError\n \n-    def current_item(self) -> Union['QWebHistoryItem', 'QWebEngineHistoryItem']:\n+    def current_item(self) -> Union[\"QWebHistoryItem\", \"QWebEngineHistoryItem\"]:\n         raise NotImplementedError\n \n     def back(self, count: int = 1) -> None:\n@@ -773,21 +792,24 @@ class AbstractHistory:\n \n \n class AbstractElements:\n-\n     \"\"\"Finding and handling of elements on the page.\"\"\"\n \n-    _MultiCallback = Callable[[Sequence['webelem.AbstractWebElement']], None]\n-    _SingleCallback = Callable[[Optional['webelem.AbstractWebElement']], None]\n+    _MultiCallback = Callable[[Sequence[\"webelem.AbstractWebElement\"]], None]\n+    _SingleCallback = Callable[[Optional[\"webelem.AbstractWebElement\"]], None]\n     _ErrorCallback = Callable[[Exception], None]\n \n-    def __init__(self, tab: 'AbstractTab') -> None:\n+    def __init__(self, tab: \"AbstractTab\") -> None:\n         self._widget = cast(_WidgetType, None)\n         self._tab = tab\n \n-    def find_css(self, selector: str,\n-                 callback: _MultiCallback,\n-                 error_cb: _ErrorCallback, *,\n-                 only_visible: bool = False) -> None:\n+    def find_css(\n+        self,\n+        selector: str,\n+        callback: _MultiCallback,\n+        error_cb: _ErrorCallback,\n+        *,\n+        only_visible: bool = False,\n+    ) -> None:\n         \"\"\"Find all HTML elements matching a given selector async.\n \n         If there's an error, the callback is called with a webelem.Error\n@@ -834,13 +856,12 @@ class AbstractElements:\n \n \n class AbstractAudio(QObject):\n-\n     \"\"\"Handling of audio/muting for this tab.\"\"\"\n \n     muted_changed = pyqtSignal(bool)\n     recently_audible_changed = pyqtSignal(bool)\n \n-    def __init__(self, tab: 'AbstractTab', parent: QWidget = None) -> None:\n+    def __init__(self, tab: \"AbstractTab\", parent: QWidget = None) -> None:\n         super().__init__(parent)\n         self._widget = cast(_WidgetType, None)\n         self._tab = tab\n@@ -865,15 +886,13 @@ class AbstractAudio(QObject):\n \n \n class AbstractTabPrivate:\n-\n     \"\"\"Tab-related methods which are only needed in the core.\n \n     Those methods are not part of the API which is exposed to extensions, and\n     should ideally be removed at some point in the future.\n     \"\"\"\n \n-    def __init__(self, mode_manager: modeman.ModeManager,\n-                 tab: 'AbstractTab') -> None:\n+    def __init__(self, mode_manager: modeman.ModeManager, tab: \"AbstractTab\") -> None:\n         self._widget = cast(_WidgetType, None)\n         self._tab = tab\n         self._mode_manager = mode_manager\n@@ -884,23 +903,25 @@ class AbstractTabPrivate:\n \n     def handle_auto_insert_mode(self, ok: bool) -> None:\n         \"\"\"Handle `input.insert_mode.auto_load` after loading finished.\"\"\"\n-        if not ok or not config.cache['input.insert_mode.auto_load']:\n+        if not ok or not config.cache[\"input.insert_mode.auto_load\"]:\n             return\n \n         cur_mode = self._mode_manager.mode\n         if cur_mode == usertypes.KeyMode.insert:\n             return\n \n-        def _auto_insert_mode_cb(\n-                elem: Optional['webelem.AbstractWebElement']\n-        ) -> None:\n+        def _auto_insert_mode_cb(elem: Optional[\"webelem.AbstractWebElement\"]) -> None:\n             \"\"\"Called from JS after finding the focused element.\"\"\"\n             if elem is None:\n                 log.webview.debug(\"No focused element!\")\n                 return\n             if elem.is_editable():\n-                modeman.enter(self._tab.win_id, usertypes.KeyMode.insert,\n-                              'load finished', only_if_normal=True)\n+                modeman.enter(\n+                    self._tab.win_id,\n+                    usertypes.KeyMode.insert,\n+                    \"load finished\",\n+                    only_if_normal=True,\n+                )\n \n         # There seems to be a race between loadFinished being called,\n         # and the autoload attribute on websites actually focusing anything.\n@@ -908,7 +929,8 @@ class AbstractTabPrivate:\n         # with 5000 test reruns (even with simultaneous CPU stress testing),\n         # so 65ms should be a safe bet and still not be too noticeable.\n         QTimer.singleShot(\n-            65, lambda: self._tab.elements.find_focused(_auto_insert_mode_cb))\n+            65, lambda: self._tab.elements.find_focused(_auto_insert_mode_cb)\n+        )\n \n     def clear_ssl_errors(self) -> None:\n         raise NotImplementedError\n@@ -949,16 +971,19 @@ class AbstractTabPrivate:\n         if tabdata.inspector is None:\n             assert tabdata.splitter is not None\n             tabdata.inspector = self._init_inspector(\n-                splitter=tabdata.splitter,\n-                win_id=self._tab.win_id)\n+                splitter=tabdata.splitter, win_id=self._tab.win_id\n+            )\n             self._tab.shutting_down.connect(tabdata.inspector.shutdown)\n             tabdata.inspector.recreate.connect(self._recreate_inspector)\n             tabdata.inspector.inspect(self._widget.page())\n         tabdata.inspector.set_position(position)\n \n-    def _init_inspector(self, splitter: 'miscwidgets.InspectorSplitter',\n-           win_id: int,\n-           parent: QWidget = None) -> 'AbstractWebInspector':\n+    def _init_inspector(\n+        self,\n+        splitter: \"miscwidgets.InspectorSplitter\",\n+        win_id: int,\n+        parent: QWidget = None,\n+    ) -> \"AbstractWebInspector\":\n         \"\"\"Get a WebKitInspector/WebEngineInspector.\n \n         Args:\n@@ -970,7 +995,6 @@ class AbstractTabPrivate:\n \n \n class AbstractTab(QWidget):\n-\n     \"\"\"An adapter for WebView/WebEngineView representing a single tab.\"\"\"\n \n     #: Signal emitted when a website requests to close this tab.\n@@ -1033,10 +1057,14 @@ class AbstractTab(QWidget):\n     private_api: AbstractTabPrivate\n     settings: websettings.AbstractSettings\n \n-    def __init__(self, *, win_id: int,\n-                 mode_manager: 'modeman.ModeManager',\n-                 private: bool,\n-                 parent: QWidget = None) -> None:\n+    def __init__(\n+        self,\n+        *,\n+        win_id: int,\n+        mode_manager: \"modeman.ModeManager\",\n+        private: bool,\n+        parent: QWidget = None,\n+    ) -> None:\n         utils.unused(mode_manager)  # needed for mypy\n         self.is_private = private\n         self.win_id = win_id\n@@ -1044,24 +1072,23 @@ class AbstractTab(QWidget):\n         super().__init__(parent)\n \n         self.registry = objreg.ObjectRegistry()\n-        tab_registry = objreg.get('tab-registry', scope='window',\n-                                  window=win_id)\n+        tab_registry = objreg.get(\"tab-registry\", scope=\"window\", window=win_id)\n         tab_registry[self.tab_id] = self\n-        objreg.register('tab', self, registry=self.registry)\n+        objreg.register(\"tab\", self, registry=self.registry)\n \n         self.data = TabData()\n         self._layout = miscwidgets.WrapperLayout(self)\n         self._widget = cast(_WidgetType, None)\n         self._progress = 0\n         self._load_status = usertypes.LoadStatus.none\n-        self._tab_event_filter = eventfilter.TabEventFilter(\n-            self, parent=self)\n+        self._tab_event_filter = eventfilter.TabEventFilter(self, parent=self)\n         self.backend: Optional[usertypes.Backend] = None\n \n         # If true, this tab has been requested to be removed (or is removed).\n         self.pending_removal = False\n-        self.shutting_down.connect(functools.partial(\n-            setattr, self, 'pending_removal', True))\n+        self.shutting_down.connect(\n+            functools.partial(setattr, self, \"pending_removal\", True)\n+        )\n \n         self.before_load_started.connect(self._on_before_load_started)\n \n@@ -1109,9 +1136,8 @@ class AbstractTab(QWidget):\n         \"\"\"\n         # This only gives us some mild protection against re-using events, but\n         # it's certainly better than a segfault.\n-        if getattr(evt, 'posted', False):\n-            raise utils.Unreachable(\"Can't re-use an event which was already \"\n-                                    \"posted!\")\n+        if getattr(evt, \"posted\", False):\n+            raise utils.Unreachable(\"Can't re-use an event which was already posted!\")\n \n         recipient = self.private_api.event_target()\n         if recipient is None:\n@@ -1149,10 +1175,7 @@ class AbstractTab(QWidget):\n         self.load_started.emit()\n \n     @pyqtSlot(usertypes.NavigationRequest)\n-    def _on_navigation_request(\n-            self,\n-            navigation: usertypes.NavigationRequest\n-    ) -> None:\n+    def _on_navigation_request(self, navigation: usertypes.NavigationRequest) -> None:\n         \"\"\"Handle common acceptNavigationRequest code.\"\"\"\n         url = utils.elide(navigation.url.toDisplayString(), 100)\n         log.webview.debug(\n@@ -1166,15 +1189,15 @@ class AbstractTab(QWidget):\n \n         if not navigation.url.isValid():\n             if navigation.navigation_type == navigation.Type.link_clicked:\n-                msg = urlutils.get_errstring(navigation.url,\n-                                             \"Invalid link clicked\")\n+                msg = urlutils.get_errstring(navigation.url, \"Invalid link clicked\")\n                 message.error(msg)\n                 self.data.open_target = usertypes.ClickTarget.normal\n \n-            log.webview.debug(\"Ignoring invalid URL {} in \"\n-                              \"acceptNavigationRequest: {}\".format(\n-                                  navigation.url.toDisplayString(),\n-                                  navigation.url.errorString()))\n+            log.webview.debug(\n+                \"Ignoring invalid URL {} in acceptNavigationRequest: {}\".format(\n+                    navigation.url.toDisplayString(), navigation.url.errorString()\n+                )\n+            )\n             navigation.accepted = False\n \n     @pyqtSlot(bool)\n@@ -1201,7 +1224,7 @@ class AbstractTab(QWidget):\n         as a response to a loadFinished signal.\n         \"\"\"\n         url = self.url()\n-        is_https = url.scheme() == 'https'\n+        is_https = url.scheme() == \"https\"\n \n         if not ok:\n             loadstatus = usertypes.LoadStatus.error\n@@ -1246,19 +1269,20 @@ class AbstractTab(QWidget):\n     def stop(self) -> None:\n         raise NotImplementedError\n \n-    def fake_key_press(self,\n-                       key: Qt.Key,\n-                       modifier: Qt.KeyboardModifier = Qt.KeyboardModifier.NoModifier) -> None:\n+    def fake_key_press(\n+        self,\n+        key: Qt.Key,\n+        modifier: Qt.KeyboardModifier = Qt.KeyboardModifier.NoModifier,\n+    ) -> None:\n         \"\"\"Send a fake key event to this tab.\"\"\"\n         press_evt = QKeyEvent(QEvent.Type.KeyPress, key, modifier, 0, 0, 0)\n-        release_evt = QKeyEvent(QEvent.Type.KeyRelease, key, modifier,\n-                                0, 0, 0)\n+        release_evt = QKeyEvent(QEvent.Type.KeyRelease, key, modifier, 0, 0, 0)\n         self.send_event(press_evt)\n         self.send_event(release_evt)\n \n-    def dump_async(self,\n-                   callback: Callable[[str], None], *,\n-                   plain: bool = False) -> None:\n+    def dump_async(\n+        self, callback: Callable[[str], None], *, plain: bool = False\n+    ) -> None:\n         \"\"\"Dump the current page's html asynchronously.\n \n         The given callback will be called with the result when dumping is\n@@ -1267,10 +1291,11 @@ class AbstractTab(QWidget):\n         raise NotImplementedError\n \n     def run_js_async(\n-            self,\n-            code: str,\n-            callback: Callable[[Any], None] = None, *,\n-            world: Union[usertypes.JsWorld, int] = None\n+        self,\n+        code: str,\n+        callback: Callable[[Any], None] = None,\n+        *,\n+        world: Union[usertypes.JsWorld, int] = None,\n     ) -> None:\n         \"\"\"Run javascript async.\n \n@@ -1332,7 +1357,7 @@ class AbstractTab(QWidget):\n             qurl = self.url()\n             url = qurl.toDisplayString(urlutils.FormatOption.ENCODE_UNICODE)\n         except (AttributeError, RuntimeError) as exc:\n-            url = '<{}>'.format(exc.__class__.__name__)\n+            url = \"<{}>\".format(exc.__class__.__name__)\n         else:\n             url = utils.elide(url, 100)\n         return utils.get_repr(self, tab_id=self.tab_id, url=url)\ndiff --git a/qutebrowser/browser/webengine/webenginetab.py b/qutebrowser/browser/webengine/webenginetab.py\nindex 1c06aa1..7fe8098 100644\n--- a/qutebrowser/browser/webengine/webenginetab.py\n+++ b/qutebrowser/browser/webengine/webenginetab.py\n@@ -12,19 +12,48 @@ import re\n import html as html_utils\n from typing import cast, Union, Optional\n \n-from qutebrowser.qt.core import (pyqtSignal, pyqtSlot, Qt, QPoint, QPointF, QUrl,\n-                          QObject, QByteArray, QTimer)\n+from qutebrowser.qt.core import (\n+    pyqtSignal,\n+    pyqtSlot,\n+    Qt,\n+    QPoint,\n+    QPointF,\n+    QUrl,\n+    QObject,\n+    QByteArray,\n+    QTimer,\n+)\n from qutebrowser.qt.network import QAuthenticator\n-from qutebrowser.qt.webenginecore import QWebEnginePage, QWebEngineScript, QWebEngineHistory\n+from qutebrowser.qt.webenginecore import (\n+    QWebEnginePage,\n+    QWebEngineScript,\n+    QWebEngineHistory,\n+)\n \n from qutebrowser.config import config\n from qutebrowser.browser import browsertab, eventfilter, shared, webelem, greasemonkey\n-from qutebrowser.browser.webengine import (webview, webengineelem, tabhistory,\n-                                           webenginesettings, certificateerror,\n-                                           webengineinspector)\n-\n-from qutebrowser.utils import (usertypes, qtutils, log, javascript, utils,\n-                               resources, message, jinja, debug, version, urlutils)\n+from qutebrowser.browser.webengine import (\n+    webview,\n+    webengineelem,\n+    tabhistory,\n+    webenginesettings,\n+    certificateerror,\n+    webengineinspector,\n+)\n+\n+from qutebrowser.utils import (\n+    usertypes,\n+    qtutils,\n+    log,\n+    javascript,\n+    utils,\n+    resources,\n+    message,\n+    jinja,\n+    debug,\n+    version,\n+    urlutils,\n+)\n from qutebrowser.qt import sip, machinery\n from qutebrowser.misc import objects, miscwidgets\n \n@@ -39,7 +68,6 @@ _JS_WORLD_MAP = {\n \n \n class WebEngineAction(browsertab.AbstractAction):\n-\n     \"\"\"QtWebEngine implementations related to web actions.\"\"\"\n \n     _widget: webview.WebEngineView\n@@ -61,7 +89,6 @@ class WebEngineAction(browsertab.AbstractAction):\n \n \n class WebEnginePrinting(browsertab.AbstractPrinting):\n-\n     \"\"\"QtWebEngine implementations related to printing.\"\"\"\n \n     _widget: webview.WebEngineView\n@@ -78,8 +105,7 @@ class WebEnginePrinting(browsertab.AbstractPrinting):\n         pass\n \n     def check_preview_support(self):\n-        raise browsertab.WebTabError(\n-            \"Print previews are unsupported with QtWebEngine\")\n+        raise browsertab.WebTabError(\"Print previews are unsupported with QtWebEngine\")\n \n     def to_pdf(self, path):\n         self._widget.page().printToPdf(str(path))\n@@ -99,7 +125,6 @@ else:\n \n @dataclasses.dataclass\n class _FindFlags:\n-\n     case_sensitive: bool = False\n     backward: bool = False\n \n@@ -133,7 +158,6 @@ class _FindFlags:\n \n \n class WebEngineSearch(browsertab.AbstractSearch):\n-\n     \"\"\"QtWebEngine implementations related to searching on the page.\n \n     Attributes:\n@@ -170,16 +194,16 @@ class WebEngineSearch(browsertab.AbstractSearch):\n             if self._pending_searches > 0:\n                 # See https://github.com/qutebrowser/qutebrowser/issues/2442\n                 # and https://github.com/qt/qtwebengine/blob/5.10/src/core/web_contents_adapter.cpp#L924-L934\n-                log.webview.debug(\"Ignoring cancelled search callback with \"\n-                                  \"{} pending searches\".format(\n-                                      self._pending_searches))\n+                log.webview.debug(\n+                    \"Ignoring cancelled search callback with \"\n+                    \"{} pending searches\".format(self._pending_searches)\n+                )\n                 return\n \n             if sip.isdeleted(self._widget):\n                 # This happens when starting a search, and closing the tab\n                 # before results arrive.\n-                log.webview.debug(\"Ignoring finished search for deleted \"\n-                                  \"widget\")\n+                log.webview.debug(\"Ignoring finished search for deleted widget\")\n                 return\n \n             # bool in Qt 5, QWebEngineFindTextResult in Qt 6\n@@ -187,13 +211,12 @@ class WebEngineSearch(browsertab.AbstractSearch):\n             # QWebEngineFindTextResult instead of the bool.\n             found = cb_arg if isinstance(cb_arg, bool) else cb_arg.numberOfMatches() > 0\n \n-            found_text = 'found' if found else \"didn't find\"\n+            found_text = \"found\" if found else \"didn't find\"\n             if flags:\n-                flag_text = f'with flags {flags}'\n+                flag_text = f\"with flags {flags}\"\n             else:\n-                flag_text = ''\n-            log.webview.debug(' '.join([caller, found_text, text, flag_text])\n-                              .strip())\n+                flag_text = \"\"\n+            log.webview.debug(\" \".join([caller, found_text, text, flag_text]).strip())\n \n             if callback is not None:\n                 callback(found)\n@@ -212,12 +235,21 @@ class WebEngineSearch(browsertab.AbstractSearch):\n         log.webview.debug(f\"Active search match: {self.match}\")\n         self.match_changed.emit(self.match)\n \n-    def search(self, text, *, ignore_case=usertypes.IgnoreCase.never,\n-               reverse=False, result_cb=None):\n+    def search(\n+        self,\n+        text,\n+        *,\n+        ignore_case=usertypes.IgnoreCase.never,\n+        reverse=False,\n+        result_cb=None,\n+    ):\n         # Don't go to next entry on duplicate search\n         if self.text == text and self.search_displayed:\n-            log.webview.debug(\"Ignoring duplicate search request\"\n-                              \" for {}, but resetting flags\".format(text))\n+            log.webview.debug(\n+                \"Ignoring duplicate search request for {}, but resetting flags\".format(\n+                    text\n+                )\n+            )\n             self._store_flags(reverse, ignore_case)\n             return\n \n@@ -225,7 +257,7 @@ class WebEngineSearch(browsertab.AbstractSearch):\n         self._store_flags(reverse, ignore_case)\n         self.match.reset()\n \n-        self._find(text, self._flags, result_cb, 'search')\n+        self._find(text, self._flags, result_cb, \"search\")\n \n     def clear(self):\n         if self.search_displayed:\n@@ -233,7 +265,7 @@ class WebEngineSearch(browsertab.AbstractSearch):\n             self.match_changed.emit(browsertab.SearchMatch())\n         self.search_displayed = False\n         self.match.reset()\n-        self._widget.page().findText('')\n+        self._widget.page().findText(\"\")\n \n     def _prev_next_cb(self, found, *, going_up, callback):\n         \"\"\"Call the prev/next callback based on the search result.\"\"\"\n@@ -256,44 +288,45 @@ class WebEngineSearch(browsertab.AbstractSearch):\n \n         if self.match.at_limit(going_up=going_up) and not wrap:\n             res = (\n-                browsertab.SearchNavigationResult.wrap_prevented_top if going_up else\n-                browsertab.SearchNavigationResult.wrap_prevented_bottom\n+                browsertab.SearchNavigationResult.wrap_prevented_top\n+                if going_up\n+                else browsertab.SearchNavigationResult.wrap_prevented_bottom\n             )\n             if callback is not None:\n                 callback(res)\n             return\n \n         cb = functools.partial(self._prev_next_cb, going_up=going_up, callback=callback)\n-        self._find(self.text, flags, cb, 'prev_result')\n+        self._find(self.text, flags, cb, \"prev_result\")\n \n     def next_result(self, *, wrap=False, callback=None):\n         going_up = self._flags.backward\n         if self.match.at_limit(going_up=going_up) and not wrap:\n             res = (\n-                browsertab.SearchNavigationResult.wrap_prevented_top if going_up else\n-                browsertab.SearchNavigationResult.wrap_prevented_bottom\n+                browsertab.SearchNavigationResult.wrap_prevented_top\n+                if going_up\n+                else browsertab.SearchNavigationResult.wrap_prevented_bottom\n             )\n             if callback is not None:\n                 callback(res)\n             return\n \n         cb = functools.partial(self._prev_next_cb, going_up=going_up, callback=callback)\n-        self._find(self.text, self._flags, cb, 'next_result')\n+        self._find(self.text, self._flags, cb, \"next_result\")\n \n \n class WebEngineCaret(browsertab.AbstractCaret):\n-\n     \"\"\"QtWebEngine implementations related to moving the cursor/selection.\"\"\"\n \n-    _tab: 'WebEngineTab'\n+    _tab: \"WebEngineTab\"\n \n     def _flags(self):\n         \"\"\"Get flags to pass to JS.\"\"\"\n         flags = set()\n         if utils.is_windows:\n-            flags.add('windows')\n-        if 'caret' in objects.debug_flags:\n-            flags.add('debug')\n+            flags.add(\"windows\")\n+        if \"caret\" in objects.debug_flags:\n+            flags.add(\"debug\")\n         return list(flags)\n \n     @pyqtSlot(usertypes.KeyMode)\n@@ -306,10 +339,9 @@ class WebEngineCaret(browsertab.AbstractCaret):\n             # convert the search to a blue selection so we can operate on it\n             self._tab.search.clear()\n \n-        self._tab.run_js_async(\n-            javascript.assemble('caret', 'setFlags', self._flags()))\n+        self._tab.run_js_async(javascript.assemble(\"caret\", \"setFlags\", self._flags()))\n \n-        self._js_call('setInitialCursor', callback=self._selection_cb)\n+        self._js_call(\"setInitialCursor\", callback=self._selection_cb)\n \n     def _selection_cb(self, enabled):\n         \"\"\"Emit selection_toggled based on setInitialCursor.\"\"\"\n@@ -330,59 +362,58 @@ class WebEngineCaret(browsertab.AbstractCaret):\n             return\n \n         self.drop_selection()\n-        self._js_call('disableCaret')\n+        self._js_call(\"disableCaret\")\n \n     def move_to_next_line(self, count=1):\n-        self._js_call('moveDown', count)\n+        self._js_call(\"moveDown\", count)\n \n     def move_to_prev_line(self, count=1):\n-        self._js_call('moveUp', count)\n+        self._js_call(\"moveUp\", count)\n \n     def move_to_next_char(self, count=1):\n-        self._js_call('moveRight', count)\n+        self._js_call(\"moveRight\", count)\n \n     def move_to_prev_char(self, count=1):\n-        self._js_call('moveLeft', count)\n+        self._js_call(\"moveLeft\", count)\n \n     def move_to_end_of_word(self, count=1):\n-        self._js_call('moveToEndOfWord', count)\n+        self._js_call(\"moveToEndOfWord\", count)\n \n     def move_to_next_word(self, count=1):\n-        self._js_call('moveToNextWord', count)\n+        self._js_call(\"moveToNextWord\", count)\n \n     def move_to_prev_word(self, count=1):\n-        self._js_call('moveToPreviousWord', count)\n+        self._js_call(\"moveToPreviousWord\", count)\n \n     def move_to_start_of_line(self):\n-        self._js_call('moveToStartOfLine')\n+        self._js_call(\"moveToStartOfLine\")\n \n     def move_to_end_of_line(self):\n-        self._js_call('moveToEndOfLine')\n+        self._js_call(\"moveToEndOfLine\")\n \n     def move_to_start_of_next_block(self, count=1):\n-        self._js_call('moveToStartOfNextBlock', count)\n+        self._js_call(\"moveToStartOfNextBlock\", count)\n \n     def move_to_start_of_prev_block(self, count=1):\n-        self._js_call('moveToStartOfPrevBlock', count)\n+        self._js_call(\"moveToStartOfPrevBlock\", count)\n \n     def move_to_end_of_next_block(self, count=1):\n-        self._js_call('moveToEndOfNextBlock', count)\n+        self._js_call(\"moveToEndOfNextBlock\", count)\n \n     def move_to_end_of_prev_block(self, count=1):\n-        self._js_call('moveToEndOfPrevBlock', count)\n+        self._js_call(\"moveToEndOfPrevBlock\", count)\n \n     def move_to_start_of_document(self):\n-        self._js_call('moveToStartOfDocument')\n+        self._js_call(\"moveToStartOfDocument\")\n \n     def move_to_end_of_document(self):\n-        self._js_call('moveToEndOfDocument')\n+        self._js_call(\"moveToEndOfDocument\")\n \n     def toggle_selection(self, line=False):\n-        self._js_call('toggleSelection', line,\n-                      callback=self._toggle_sel_translate)\n+        self._js_call(\"toggleSelection\", line, callback=self._toggle_sel_translate)\n \n     def drop_selection(self):\n-        self._js_call('dropSelection')\n+        self._js_call(\"dropSelection\")\n \n     def selection(self, callback):\n         # Not using selectedText() as WORKAROUND for\n@@ -391,11 +422,10 @@ class WebEngineCaret(browsertab.AbstractCaret):\n         # https://github.com/qutebrowser/qutebrowser/issues/3523\n         # With Qt 6.2-6.5, there still seem to be issues (especially with\n         # multi-line text)\n-        self._tab.run_js_async(javascript.assemble('caret', 'getSelection'),\n-                               callback)\n+        self._tab.run_js_async(javascript.assemble(\"caret\", \"getSelection\"), callback)\n \n     def reverse_selection(self):\n-        self._js_call('reverseSelection')\n+        self._js_call(\"reverseSelection\")\n \n     def _follow_selected_cb_wrapped(self, js_elem, tab):\n         if sip.isdeleted(self):\n@@ -405,7 +435,8 @@ class WebEngineCaret(browsertab.AbstractCaret):\n             log.webview.warning(\n                 \"Got follow_selected callback for deleted WebEngineCaret. \"\n                 \"This is most likely due to a QtWebEngine bug, please report a \"\n-                \"qutebrowser issue if you know a way to reproduce this.\")\n+                \"qutebrowser issue if you know a way to reproduce this.\"\n+            )\n             return\n \n         try:\n@@ -437,8 +468,11 @@ class WebEngineCaret(browsertab.AbstractCaret):\n \n         # Only click if we see a link\n         if elem.is_link():\n-            log.webview.debug(\"Found link in selection, clicking. ClickTarget \"\n-                              \"{}, elem {}\".format(click_type, elem))\n+            log.webview.debug(\n+                \"Found link in selection, clicking. ClickTarget {}, elem {}\".format(\n+                    click_type, elem\n+                )\n+            )\n             try:\n                 elem.click(click_type)\n             except webelem.Error as e:\n@@ -455,14 +489,13 @@ class WebEngineCaret(browsertab.AbstractCaret):\n             self._follow_enter(tab)\n         else:\n             # click an existing blue selection\n-            js_code = javascript.assemble('webelem',\n-                                          'find_selected_focused_link')\n+            js_code = javascript.assemble(\"webelem\", \"find_selected_focused_link\")\n             self._tab.run_js_async(\n-                js_code,\n-                lambda jsret: self._follow_selected_cb_wrapped(jsret, tab))\n+                js_code, lambda jsret: self._follow_selected_cb_wrapped(jsret, tab)\n+            )\n \n     def _js_call(self, command, *args, callback=None):\n-        code = javascript.assemble('caret', command, *args)\n+        code = javascript.assemble(\"caret\", command, *args)\n         self._tab.run_js_async(code, callback)\n \n     def _toggle_sel_translate(self, state_str):\n@@ -470,8 +503,11 @@ class WebEngineCaret(browsertab.AbstractCaret):\n             # This may happen if the user switches to another mode after\n             # `:selection-toggle` is executed and before this callback function\n             # is asynchronously called.\n-            log.misc.debug(\"Ignoring caret selection callback in {}\".format(\n-                self._mode_manager.mode))\n+            log.misc.debug(\n+                \"Ignoring caret selection callback in {}\".format(\n+                    self._mode_manager.mode\n+                )\n+            )\n             return\n         if state_str is None:\n             message.error(\"Error toggling caret selection\")\n@@ -481,7 +517,6 @@ class WebEngineCaret(browsertab.AbstractCaret):\n \n \n class WebEngineScroller(browsertab.AbstractScroller):\n-\n     \"\"\"QtWebEngine implementations related to scrolling.\"\"\"\n \n     _widget: webview.WebEngineView\n@@ -497,7 +532,9 @@ class WebEngineScroller(browsertab.AbstractScroller):\n         page = widget.page()\n         page.scrollPositionChanged.connect(self._update_pos)\n \n-    def _repeated_key_press(self, key, count=1, modifier=Qt.KeyboardModifier.NoModifier):\n+    def _repeated_key_press(\n+        self, key, count=1, modifier=Qt.KeyboardModifier.NoModifier\n+    ):\n         \"\"\"Send count fake key presses to this scroller's WebEngineTab.\"\"\"\n         for _ in range(min(count, 1000)):\n             self._tab.fake_key_press(key, modifier)\n@@ -517,10 +554,10 @@ class WebEngineScroller(browsertab.AbstractScroller):\n             except ValueError:\n                 # https://github.com/qutebrowser/qutebrowser/issues/3219\n                 log.misc.debug(\"Got ValueError for perc_x!\")\n-                log.misc.debug(\"contents_size.width(): {}\".format(\n-                    contents_size.width()))\n-                log.misc.debug(\"self._widget.width(): {}\".format(\n-                    self._widget.width()))\n+                log.misc.debug(\n+                    \"contents_size.width(): {}\".format(contents_size.width())\n+                )\n+                log.misc.debug(\"self._widget.width(): {}\".format(self._widget.width()))\n                 log.misc.debug(\"scrollable_x: {}\".format(scrollable_x))\n                 log.misc.debug(\"pos.x(): {}\".format(pos.x()))\n                 raise\n@@ -534,18 +571,22 @@ class WebEngineScroller(browsertab.AbstractScroller):\n             except ValueError:\n                 # https://github.com/qutebrowser/qutebrowser/issues/3219\n                 log.misc.debug(\"Got ValueError for perc_y!\")\n-                log.misc.debug(\"contents_size.height(): {}\".format(\n-                    contents_size.height()))\n-                log.misc.debug(\"self._widget.height(): {}\".format(\n-                    self._widget.height()))\n+                log.misc.debug(\n+                    \"contents_size.height(): {}\".format(contents_size.height())\n+                )\n+                log.misc.debug(\n+                    \"self._widget.height(): {}\".format(self._widget.height())\n+                )\n                 log.misc.debug(\"scrollable_y: {}\".format(scrollable_y))\n                 log.misc.debug(\"pos.y(): {}\".format(pos.y()))\n                 raise\n \n         self._at_bottom = math.ceil(pos.y()) >= scrollable_y\n \n-        if (self._pos_perc != (perc_x, perc_y) or\n-                'no-scroll-filtering' in objects.debug_flags):\n+        if (\n+            self._pos_perc != (perc_x, perc_y)\n+            or \"no-scroll-filtering\" in objects.debug_flags\n+        ):\n             self._pos_perc = perc_x, perc_y\n             self.perc_changed.emit(*self._pos_perc)\n \n@@ -556,11 +597,11 @@ class WebEngineScroller(browsertab.AbstractScroller):\n         return self._pos_perc\n \n     def to_perc(self, x=None, y=None):\n-        js_code = javascript.assemble('scroll', 'to_perc', x, y)\n+        js_code = javascript.assemble(\"scroll\", \"to_perc\", x, y)\n         self._tab.run_js_async(js_code)\n \n     def to_point(self, point):\n-        js_code = javascript.assemble('window', 'scroll', point.x(), point.y())\n+        js_code = javascript.assemble(\"window\", \"scroll\", point.x(), point.y())\n         self._tab.run_js_async(js_code)\n \n     def to_anchor(self, name):\n@@ -569,10 +610,10 @@ class WebEngineScroller(browsertab.AbstractScroller):\n         self._tab.load_url(url)\n \n     def delta(self, x=0, y=0):\n-        self._tab.run_js_async(javascript.assemble('window', 'scrollBy', x, y))\n+        self._tab.run_js_async(javascript.assemble(\"window\", \"scrollBy\", x, y))\n \n     def delta_page(self, x=0, y=0):\n-        js_code = javascript.assemble('scroll', 'delta_page', x, y)\n+        js_code = javascript.assemble(\"scroll\", \"delta_page\", x, y)\n         self._tab.run_js_async(js_code)\n \n     def up(self, count=1):\n@@ -607,10 +648,9 @@ class WebEngineScroller(browsertab.AbstractScroller):\n \n \n class WebEngineHistoryPrivate(browsertab.AbstractHistoryPrivate):\n-\n     \"\"\"History-related methods which are not part of the extension API.\"\"\"\n \n-    def __init__(self, tab: 'WebEngineTab') -> None:\n+    def __init__(self, tab: \"WebEngineTab\") -> None:\n         self._tab = tab\n         self._history = cast(QWebEngineHistory, None)\n \n@@ -650,7 +690,7 @@ class WebEngineHistoryPrivate(browsertab.AbstractHistoryPrivate):\n                 break\n \n         url = items[cur_idx].url\n-        if (url.scheme(), url.host()) == ('qute', 'back') and cur_idx >= 1:\n+        if (url.scheme(), url.host()) == (\"qute\", \"back\") and cur_idx >= 1:\n             url = items[cur_idx - 1].url\n \n         self._tab.load_url(url)\n@@ -671,19 +711,17 @@ class WebEngineHistoryPrivate(browsertab.AbstractHistoryPrivate):\n \n         @pyqtSlot()\n         def _on_load_finished():\n-            self._tab.scroller.to_point(cur_data['scroll-pos'])\n+            self._tab.scroller.to_point(cur_data[\"scroll-pos\"])\n             self._tab.load_finished.disconnect(_on_load_finished)\n \n         if cur_data is not None:\n-            if 'zoom' in cur_data:\n-                self._tab.zoom.set_factor(cur_data['zoom'])\n-            if ('scroll-pos' in cur_data and\n-                    self._tab.scroller.pos_px() == QPoint(0, 0)):\n+            if \"zoom\" in cur_data:\n+                self._tab.zoom.set_factor(cur_data[\"zoom\"])\n+            if \"scroll-pos\" in cur_data and self._tab.scroller.pos_px() == QPoint(0, 0):\n                 self._tab.load_finished.connect(_on_load_finished)\n \n \n class WebEngineHistory(browsertab.AbstractHistory):\n-\n     \"\"\"QtWebEngine implementations related to page history.\"\"\"\n \n     def __init__(self, tab):\n@@ -723,7 +761,6 @@ class WebEngineHistory(browsertab.AbstractHistory):\n \n \n class WebEngineZoom(browsertab.AbstractZoom):\n-\n     \"\"\"QtWebEngine implementations related to zooming.\"\"\"\n \n     _widget: webview.WebEngineView\n@@ -733,10 +770,9 @@ class WebEngineZoom(browsertab.AbstractZoom):\n \n \n class WebEngineElements(browsertab.AbstractElements):\n-\n     \"\"\"QtWebEngine implementations related to elements on the page.\"\"\"\n \n-    _tab: 'WebEngineTab'\n+    _tab: \"WebEngineTab\"\n \n     def _js_cb_multiple(self, callback, error_cb, js_elems):\n         \"\"\"Handle found elements coming from JS and call the real callback.\n@@ -747,15 +783,14 @@ class WebEngineElements(browsertab.AbstractElements):\n             js_elems: The elements serialized from javascript.\n         \"\"\"\n         if js_elems is None:\n-            error_cb(webelem.Error(\"Unknown error while getting \"\n-                                   \"elements\"))\n+            error_cb(webelem.Error(\"Unknown error while getting elements\"))\n             return\n-        elif not js_elems['success']:\n-            error_cb(webelem.Error(js_elems['error']))\n+        elif not js_elems[\"success\"]:\n+            error_cb(webelem.Error(js_elems[\"error\"]))\n             return\n \n         elems = []\n-        for js_elem in js_elems['result']:\n+        for js_elem in js_elems[\"result\"]:\n             elem = webengineelem.WebEngineElement(js_elem, tab=self._tab)\n             elems.append(elem)\n         callback(elems)\n@@ -768,8 +803,7 @@ class WebEngineElements(browsertab.AbstractElements):\n                       Called with a WebEngineElement or None.\n             js_elem: The element serialized from javascript.\n         \"\"\"\n-        debug_str = ('None' if js_elem is None\n-                     else utils.elide(repr(js_elem), 1000))\n+        debug_str = \"None\" if js_elem is None else utils.elide(repr(js_elem), 1000)\n         log.webview.debug(\"Got element from JS: {}\".format(debug_str))\n \n         if js_elem is None:\n@@ -778,20 +812,18 @@ class WebEngineElements(browsertab.AbstractElements):\n             elem = webengineelem.WebEngineElement(js_elem, tab=self._tab)\n             callback(elem)\n \n-    def find_css(self, selector, callback, error_cb, *,\n-                 only_visible=False):\n-        js_code = javascript.assemble('webelem', 'find_css', selector,\n-                                      only_visible)\n+    def find_css(self, selector, callback, error_cb, *, only_visible=False):\n+        js_code = javascript.assemble(\"webelem\", \"find_css\", selector, only_visible)\n         js_cb = functools.partial(self._js_cb_multiple, callback, error_cb)\n         self._tab.run_js_async(js_code, js_cb)\n \n     def find_id(self, elem_id, callback):\n-        js_code = javascript.assemble('webelem', 'find_id', elem_id)\n+        js_code = javascript.assemble(\"webelem\", \"find_id\", elem_id)\n         js_cb = functools.partial(self._js_cb_single, callback)\n         self._tab.run_js_async(js_code, js_cb)\n \n     def find_focused(self, callback):\n-        js_code = javascript.assemble('webelem', 'find_focused')\n+        js_code = javascript.assemble(\"webelem\", \"find_focused\")\n         js_cb = functools.partial(self._js_cb_single, callback)\n         self._tab.run_js_async(js_code, js_cb)\n \n@@ -799,14 +831,12 @@ class WebEngineElements(browsertab.AbstractElements):\n         assert pos.x() >= 0, pos\n         assert pos.y() >= 0, pos\n         pos /= self._tab.zoom.factor()\n-        js_code = javascript.assemble('webelem', 'find_at_pos',\n-                                      pos.x(), pos.y())\n+        js_code = javascript.assemble(\"webelem\", \"find_at_pos\", pos.x(), pos.y())\n         js_cb = functools.partial(self._js_cb_single, callback)\n         self._tab.run_js_async(js_code, js_cb)\n \n \n class WebEngineAudio(browsertab.AbstractAudio):\n-\n     \"\"\"QtWebEngine implementations related to audio/muting.\n \n     Attributes:\n@@ -833,8 +863,9 @@ class WebEngineAudio(browsertab.AbstractAudio):\n         page.recentlyAudibleChanged.connect(self._delayed_recently_audible_changed)\n         self._tab.url_changed.connect(self._on_url_changed)\n         config.instance.changed.connect(self._on_config_changed)\n-        self._silence_timer.timeout.connect(functools.partial(\n-            self.recently_audible_changed.emit, False))\n+        self._silence_timer.timeout.connect(\n+            functools.partial(self.recently_audible_changed.emit, False)\n+        )\n \n     # WORKAROUND for recentlyAudibleChanged being emitted without delay from the moment\n     # that audio is dropped.\n@@ -870,43 +901,42 @@ class WebEngineAudio(browsertab.AbstractAudio):\n     def _on_url_changed(self, url):\n         if self._overridden or not url.isValid():\n             return\n-        mute = config.instance.get('content.mute', url=url)\n+        mute = config.instance.get(\"content.mute\", url=url)\n         self.set_muted(mute)\n \n-    @config.change_filter('content.mute')\n+    @config.change_filter(\"content.mute\")\n     def _on_config_changed(self):\n         self._on_url_changed(self._tab.url())\n \n \n class _WebEnginePermissions(QObject):\n-\n     \"\"\"Handling of various permission-related signals.\"\"\"\n \n     _widget: webview.WebEngineView\n \n     _options = {\n-        QWebEnginePage.Feature.Notifications: 'content.notifications.enabled',\n-        QWebEnginePage.Feature.Geolocation: 'content.geolocation',\n-        QWebEnginePage.Feature.MediaAudioCapture: 'content.media.audio_capture',\n-        QWebEnginePage.Feature.MediaVideoCapture: 'content.media.video_capture',\n-        QWebEnginePage.Feature.MediaAudioVideoCapture: 'content.media.audio_video_capture',\n-        QWebEnginePage.Feature.MouseLock: 'content.mouse_lock',\n-        QWebEnginePage.Feature.DesktopVideoCapture: 'content.desktop_capture',\n-        QWebEnginePage.Feature.DesktopAudioVideoCapture: 'content.desktop_capture',\n+        QWebEnginePage.Feature.Notifications: \"content.notifications.enabled\",\n+        QWebEnginePage.Feature.Geolocation: \"content.geolocation\",\n+        QWebEnginePage.Feature.MediaAudioCapture: \"content.media.audio_capture\",\n+        QWebEnginePage.Feature.MediaVideoCapture: \"content.media.video_capture\",\n+        QWebEnginePage.Feature.MediaAudioVideoCapture: \"content.media.audio_video_capture\",\n+        QWebEnginePage.Feature.MouseLock: \"content.mouse_lock\",\n+        QWebEnginePage.Feature.DesktopVideoCapture: \"content.desktop_capture\",\n+        QWebEnginePage.Feature.DesktopAudioVideoCapture: \"content.desktop_capture\",\n         # 8 == ClipboardReadWrite, new in 6.8\n-        QWebEnginePage.Feature(8): 'content.javascript.clipboard',\n+        QWebEnginePage.Feature(8): \"content.javascript.clipboard\",\n     }\n \n     _messages = {\n-        QWebEnginePage.Feature.Notifications: 'show notifications',\n-        QWebEnginePage.Feature.Geolocation: 'access your location',\n-        QWebEnginePage.Feature.MediaAudioCapture: 'record audio',\n-        QWebEnginePage.Feature.MediaVideoCapture: 'record video',\n-        QWebEnginePage.Feature.MediaAudioVideoCapture: 'record audio/video',\n-        QWebEnginePage.Feature.MouseLock: 'hide your mouse pointer',\n-        QWebEnginePage.Feature.DesktopVideoCapture: 'capture your desktop',\n-        QWebEnginePage.Feature.DesktopAudioVideoCapture: 'capture your desktop and audio',\n-        QWebEnginePage.Feature(8): 'read and write your clipboard',\n+        QWebEnginePage.Feature.Notifications: \"show notifications\",\n+        QWebEnginePage.Feature.Geolocation: \"access your location\",\n+        QWebEnginePage.Feature.MediaAudioCapture: \"record audio\",\n+        QWebEnginePage.Feature.MediaVideoCapture: \"record video\",\n+        QWebEnginePage.Feature.MediaAudioVideoCapture: \"record audio/video\",\n+        QWebEnginePage.Feature.MouseLock: \"hide your mouse pointer\",\n+        QWebEnginePage.Feature.DesktopVideoCapture: \"capture your desktop\",\n+        QWebEnginePage.Feature.DesktopAudioVideoCapture: \"capture your desktop and audio\",\n+        QWebEnginePage.Feature(8): \"read and write your clipboard\",\n     }\n \n     def __init__(self, tab, parent=None):\n@@ -918,16 +948,15 @@ class _WebEnginePermissions(QObject):\n     def connect_signals(self):\n         \"\"\"Connect related signals from the QWebEnginePage.\"\"\"\n         page = self._widget.page()\n-        page.fullScreenRequested.connect(\n-            self._on_fullscreen_requested)\n-        page.featurePermissionRequested.connect(\n-            self._on_feature_permission_requested)\n+        page.fullScreenRequested.connect(self._on_fullscreen_requested)\n+        page.featurePermissionRequested.connect(self._on_feature_permission_requested)\n \n         page.quotaRequested.connect(self._on_quota_requested)\n         page.registerProtocolHandlerRequested.connect(\n-            self._on_register_protocol_handler_requested)\n+            self._on_register_protocol_handler_requested\n+        )\n \n-    @pyqtSlot('QWebEngineFullScreenRequest')\n+    @pyqtSlot(\"QWebEngineFullScreenRequest\")\n     def _on_fullscreen_requested(self, request):\n         request.accept()\n         on = request.toggleOn()\n@@ -941,44 +970,58 @@ class _WebEnginePermissions(QObject):\n                 notif.set_timeout(timeout)\n                 notif.show()\n \n-    @pyqtSlot(QUrl, 'QWebEnginePage::Feature')\n+    @pyqtSlot(QUrl, \"QWebEnginePage::Feature\")\n     def _on_feature_permission_requested(self, url, feature):\n         \"\"\"Ask the user for approval for geolocation/media/etc..\"\"\"\n         page = self._widget.page()\n         grant_permission = functools.partial(\n-            page.setFeaturePermission, url, feature,\n-            QWebEnginePage.PermissionPolicy.PermissionGrantedByUser)\n+            page.setFeaturePermission,\n+            url,\n+            feature,\n+            QWebEnginePage.PermissionPolicy.PermissionGrantedByUser,\n+        )\n         deny_permission = functools.partial(\n-            page.setFeaturePermission, url, feature,\n-            QWebEnginePage.PermissionPolicy.PermissionDeniedByUser)\n+            page.setFeaturePermission,\n+            url,\n+            feature,\n+            QWebEnginePage.PermissionPolicy.PermissionDeniedByUser,\n+        )\n \n         permission_str = debug.qenum_key(QWebEnginePage, feature)\n \n         if not url.isValid():\n-            log.webview.warning(\"Ignoring feature permission {} for invalid URL {}\".format(\n-                permission_str, url))\n+            log.webview.warning(\n+                \"Ignoring feature permission {} for invalid URL {}\".format(\n+                    permission_str, url\n+                )\n+            )\n             deny_permission()\n             return\n \n         if feature not in self._options:\n-            log.webview.error(\"Unhandled feature permission {}\".format(\n-                permission_str))\n+            log.webview.error(\"Unhandled feature permission {}\".format(permission_str))\n             deny_permission()\n             return\n \n         question = shared.feature_permission(\n             url=url.adjusted(QUrl.UrlFormattingOption.RemovePath),\n-            option=self._options[feature], msg=self._messages[feature],\n-            yes_action=grant_permission, no_action=deny_permission,\n-            abort_on=[self._tab.abort_questions])\n+            option=self._options[feature],\n+            msg=self._messages[feature],\n+            yes_action=grant_permission,\n+            no_action=deny_permission,\n+            abort_on=[self._tab.abort_questions],\n+        )\n \n         if question is not None:\n             page.featurePermissionRequestCanceled.connect(\n-                functools.partial(self._on_feature_permission_cancelled,\n-                                  question, url, feature))\n+                functools.partial(\n+                    self._on_feature_permission_cancelled, question, url, feature\n+                )\n+            )\n \n-    def _on_feature_permission_cancelled(self, question, url, feature,\n-                                         cancelled_url, cancelled_feature):\n+    def _on_feature_permission_cancelled(\n+        self, question, url, feature, cancelled_url, cancelled_feature\n+    ):\n         \"\"\"Slot invoked when a feature permission request was cancelled.\n \n         To be used with functools.partial.\n@@ -995,28 +1038,32 @@ class _WebEnginePermissions(QObject):\n         size = utils.format_size(request.requestedSize())\n         shared.feature_permission(\n             url=request.origin().adjusted(QUrl.UrlFormattingOption.RemovePath),\n-            option='content.persistent_storage',\n-            msg='use {} of persistent storage'.format(size),\n-            yes_action=request.accept, no_action=request.reject,\n+            option=\"content.persistent_storage\",\n+            msg=\"use {} of persistent storage\".format(size),\n+            yes_action=request.accept,\n+            no_action=request.reject,\n             abort_on=[self._tab.abort_questions],\n-            blocking=True)\n+            blocking=True,\n+        )\n \n     def _on_register_protocol_handler_requested(self, request):\n         shared.feature_permission(\n             url=request.origin().adjusted(QUrl.UrlFormattingOption.RemovePath),\n-            option='content.register_protocol_handler',\n-            msg='open all {} links'.format(request.scheme()),\n-            yes_action=request.accept, no_action=request.reject,\n+            option=\"content.register_protocol_handler\",\n+            msg=\"open all {} links\".format(request.scheme()),\n+            yes_action=request.accept,\n+            no_action=request.reject,\n             abort_on=[self._tab.abort_questions],\n-            blocking=True)\n+            blocking=True,\n+        )\n \n \n @dataclasses.dataclass\n class _Quirk:\n-\n     filename: str\n     injection_point: QWebEngineScript.InjectionPoint = (\n-        QWebEngineScript.InjectionPoint.DocumentCreation)\n+        QWebEngineScript.InjectionPoint.DocumentCreation\n+    )\n     world: QWebEngineScript.ScriptWorldId = QWebEngineScript.ScriptWorldId.MainWorld\n     predicate: bool = True\n     name: Optional[str] = None\n@@ -1027,7 +1074,6 @@ class _Quirk:\n \n \n class _WebEngineScripts(QObject):\n-\n     _widget: webview.WebEngineView\n \n     def __init__(self, tab, parent=None):\n@@ -1040,13 +1086,14 @@ class _WebEngineScripts(QObject):\n         \"\"\"Connect signals to our private slots.\"\"\"\n         config.instance.changed.connect(self._on_config_changed)\n \n-        self._tab.search.cleared.connect(functools.partial(\n-            self._update_stylesheet, searching=False))\n+        self._tab.search.cleared.connect(\n+            functools.partial(self._update_stylesheet, searching=False)\n+        )\n         self._tab.search.finished.connect(self._update_stylesheet)\n \n     @pyqtSlot(str)\n     def _on_config_changed(self, option):\n-        if option in ['scrolling.bar', 'content.user_stylesheets']:\n+        if option in [\"scrolling.bar\", \"content.user_stylesheets\"]:\n             self._init_stylesheet()\n             self._update_stylesheet()\n \n@@ -1054,47 +1101,53 @@ class _WebEngineScripts(QObject):\n     def _update_stylesheet(self, searching=False):\n         \"\"\"Update the custom stylesheet in existing tabs.\"\"\"\n         css = shared.get_user_stylesheet(searching=searching)\n-        code = javascript.assemble('stylesheet', 'set_css', css)\n+        code = javascript.assemble(\"stylesheet\", \"set_css\", css)\n         self._tab.run_js_async(code)\n \n-    def _inject_js(self, name, js_code, *,\n-                   world=QWebEngineScript.ScriptWorldId.ApplicationWorld,\n-                   injection_point=QWebEngineScript.InjectionPoint.DocumentCreation,\n-                   subframes=False):\n+    def _inject_js(\n+        self,\n+        name,\n+        js_code,\n+        *,\n+        world=QWebEngineScript.ScriptWorldId.ApplicationWorld,\n+        injection_point=QWebEngineScript.InjectionPoint.DocumentCreation,\n+        subframes=False,\n+    ):\n         \"\"\"Inject the given script to run early on a page load.\"\"\"\n         script = QWebEngineScript()\n         script.setInjectionPoint(injection_point)\n         script.setSourceCode(js_code)\n         script.setWorldId(world)\n         script.setRunsOnSubFrames(subframes)\n-        script.setName(f'_qute_{name}')\n+        script.setName(f\"_qute_{name}\")\n         self._widget.page().scripts().insert(script)\n \n     def _remove_js(self, name):\n         \"\"\"Remove an early QWebEngineScript.\"\"\"\n         scripts = self._widget.page().scripts()\n         if machinery.IS_QT6:\n-            for script in scripts.find(f'_qute_{name}'):\n+            for script in scripts.find(f\"_qute_{name}\"):\n                 scripts.remove(script)\n         else:  # Qt 5\n-            script = scripts.findScript(f'_qute_{name}')\n+            script = scripts.findScript(f\"_qute_{name}\")\n             if not script.isNull():\n                 scripts.remove(script)\n \n     def init(self):\n         \"\"\"Initialize global qutebrowser JavaScript.\"\"\"\n         js_code = javascript.wrap_global(\n-            'scripts',\n-            resources.read_file('javascript/scroll.js'),\n-            resources.read_file('javascript/webelem.js'),\n-            resources.read_file('javascript/caret.js'),\n+            \"scripts\",\n+            resources.read_file(\"javascript/scroll.js\"),\n+            resources.read_file(\"javascript/webelem.js\"),\n+            resources.read_file(\"javascript/caret.js\"),\n         )\n         # FIXME:qtwebengine what about subframes=True?\n-        self._inject_js('js', js_code, subframes=True)\n+        self._inject_js(\"js\", js_code, subframes=True)\n         self._init_stylesheet()\n \n         self._greasemonkey.scripts_reloaded.connect(\n-            self._inject_all_greasemonkey_scripts)\n+            self._inject_all_greasemonkey_scripts\n+        )\n         self._inject_all_greasemonkey_scripts()\n         self._inject_site_specific_quirks()\n \n@@ -1104,14 +1157,14 @@ class _WebEngineScripts(QObject):\n         Partially inspired by QupZilla:\n         https://github.com/QupZilla/qupzilla/blob/v2.0/src/lib/app/mainapplication.cpp#L1063-L1101\n         \"\"\"\n-        self._remove_js('stylesheet')\n+        self._remove_js(\"stylesheet\")\n         css = shared.get_user_stylesheet()\n         js_code = javascript.wrap_global(\n-            'stylesheet',\n-            resources.read_file('javascript/stylesheet.js'),\n-            javascript.assemble('stylesheet', 'set_css', css),\n+            \"stylesheet\",\n+            resources.read_file(\"javascript/stylesheet.js\"),\n+            javascript.assemble(\"stylesheet\", \"set_css\", css),\n         )\n-        self._inject_js('stylesheet', js_code, subframes=True)\n+        self._inject_js(\"stylesheet\", js_code, subframes=True)\n \n     @pyqtSlot()\n     def _inject_all_greasemonkey_scripts(self):\n@@ -1122,8 +1175,7 @@ class _WebEngineScripts(QObject):\n         page_scripts = self._widget.page().scripts()\n         for script in page_scripts.toList():\n             if script.name().startswith(\"GM-\"):\n-                log.greasemonkey.debug('Removing script: {}'\n-                                       .format(script.name()))\n+                log.greasemonkey.debug(\"Removing script: {}\".format(script.name()))\n                 removed = page_scripts.remove(script)\n                 assert removed, script.name()\n \n@@ -1158,7 +1210,8 @@ class _WebEngineScripts(QObject):\n                     log.greasemonkey.error(\n                         f\"script {script.name} has invalid value for '@qute-js-world'\"\n                         f\": {script.jsworld}, should be between 0 and \"\n-                        f\"{qtutils.MAX_WORLD_ID}\")\n+                        f\"{qtutils.MAX_WORLD_ID}\"\n+                    )\n                     continue\n             except ValueError:\n                 try:\n@@ -1166,7 +1219,8 @@ class _WebEngineScripts(QObject):\n                 except KeyError:\n                     log.greasemonkey.error(\n                         f\"script {script.name} has invalid value for '@qute-js-world'\"\n-                        f\": {script.jsworld}\")\n+                        f\": {script.jsworld}\"\n+                    )\n                     continue\n             new_script.setWorldId(world)\n \n@@ -1186,10 +1240,13 @@ class _WebEngineScripts(QObject):\n \n             if script.needs_document_end_workaround():\n                 log.greasemonkey.debug(\n-                    f\"Forcing @run-at document-end for {script.name}\")\n-                new_script.setInjectionPoint(QWebEngineScript.InjectionPoint.DocumentReady)\n+                    f\"Forcing @run-at document-end for {script.name}\"\n+                )\n+                new_script.setInjectionPoint(\n+                    QWebEngineScript.InjectionPoint.DocumentReady\n+                )\n \n-            log.greasemonkey.debug(f'adding script: {new_script.name()}')\n+            log.greasemonkey.debug(f\"adding script: {new_script.name()}\")\n             page_scripts.insert(new_script)\n \n     def _get_quirks(self):\n@@ -1198,23 +1255,22 @@ class _WebEngineScripts(QObject):\n         return [\n             # FIXME:qt6 Double check which of those are still required\n             _Quirk(\n-                'whatsapp_web',\n+                \"whatsapp_web\",\n                 injection_point=QWebEngineScript.InjectionPoint.DocumentReady,\n                 world=QWebEngineScript.ScriptWorldId.ApplicationWorld,\n             ),\n-            _Quirk('discord'),\n+            _Quirk(\"discord\"),\n             _Quirk(\n-                'googledocs',\n+                \"googledocs\",\n                 # will be an UA quirk once we set the JS UA as well\n-                name='ua-googledocs',\n+                name=\"ua-googledocs\",\n             ),\n-\n             _Quirk(\n-                'string_replaceall',\n+                \"string_replaceall\",\n                 predicate=versions.webengine < utils.VersionNumber(5, 15, 3),\n             ),\n             _Quirk(\n-                'array_at',\n+                \"array_at\",\n                 predicate=versions.webengine < utils.VersionNumber(6, 3),\n             ),\n         ]\n@@ -1227,10 +1283,10 @@ class _WebEngineScripts(QObject):\n         for quirk in self._get_quirks():\n             if not quirk.predicate:\n                 continue\n-            src = resources.read_file(f'javascript/quirks/{quirk.filename}.user.js')\n+            src = resources.read_file(f\"javascript/quirks/{quirk.filename}.user.js\")\n             if quirk.name not in config.val.content.site_specific_quirks.skip:\n                 self._inject_js(\n-                    f'quirk_{quirk.filename}',\n+                    f\"quirk_{quirk.filename}\",\n                     src,\n                     world=quirk.world,\n                     injection_point=quirk.injection_point,\n@@ -1238,7 +1294,6 @@ class _WebEngineScripts(QObject):\n \n \n class WebEngineTabPrivate(browsertab.AbstractTabPrivate):\n-\n     \"\"\"QtWebEngine-related methods which aren't part of the public API.\"\"\"\n \n     _widget: webview.WebEngineView\n@@ -1268,7 +1323,6 @@ class WebEngineTabPrivate(browsertab.AbstractTabPrivate):\n \n \n class WebEngineTab(browsertab.AbstractTab):\n-\n     \"\"\"A QtWebEngine tab in the browser.\n \n     Signals:\n@@ -1284,24 +1338,22 @@ class WebEngineTab(browsertab.AbstractTab):\n     printing: WebEnginePrinting\n \n     def __init__(self, *, win_id, mode_manager, private, parent=None):\n-        super().__init__(win_id=win_id,\n-                         mode_manager=mode_manager,\n-                         private=private,\n-                         parent=parent)\n-        widget = webview.WebEngineView(tabdata=self.data, win_id=win_id,\n-                                       private=private)\n+        super().__init__(\n+            win_id=win_id, mode_manager=mode_manager, private=private, parent=parent\n+        )\n+        widget = webview.WebEngineView(\n+            tabdata=self.data, win_id=win_id, private=private\n+        )\n         self.history = WebEngineHistory(tab=self)\n         self.scroller = WebEngineScroller(tab=self, parent=self)\n-        self.caret = WebEngineCaret(mode_manager=mode_manager,\n-                                    tab=self, parent=self)\n+        self.caret = WebEngineCaret(mode_manager=mode_manager, tab=self, parent=self)\n         self.zoom = WebEngineZoom(tab=self, parent=self)\n         self.search = WebEngineSearch(tab=self, parent=self)\n         self.printing = WebEnginePrinting(tab=self, parent=self)\n         self.elements = WebEngineElements(tab=self)\n         self.action = WebEngineAction(tab=self)\n         self.audio = WebEngineAudio(tab=self, parent=self)\n-        self.private_api = WebEngineTabPrivate(mode_manager=mode_manager,\n-                                               tab=self)\n+        self.private_api = WebEngineTabPrivate(mode_manager=mode_manager, tab=self)\n         self._permissions = _WebEnginePermissions(tab=self, parent=self)\n         self._scripts = _WebEngineScripts(tab=self, parent=self)\n         # We're assigning settings in _set_widget\n@@ -1314,7 +1366,8 @@ class WebEngineTab(browsertab.AbstractTab):\n         self._scripts.init()\n         # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-65223\n         self._needs_qtbug65223_workaround = (\n-            version.qtwebengine_versions().webengine < utils.VersionNumber(5, 15, 5))\n+            version.qtwebengine_versions().webengine < utils.VersionNumber(5, 15, 5)\n+        )\n \n     def _set_widget(self, widget):\n         # pylint: disable=protected-access\n@@ -1328,9 +1381,8 @@ class WebEngineTab(browsertab.AbstractTab):\n             fp.installEventFilter(self._tab_event_filter)\n \n         self._child_event_filter = eventfilter.ChildEventFilter(\n-            eventfilter=self._tab_event_filter,\n-            widget=self._widget,\n-            parent=self)\n+            eventfilter=self._tab_event_filter, widget=self._widget, parent=self\n+        )\n         self._widget.installEventFilter(self._child_event_filter)\n \n     @pyqtSlot()\n@@ -1382,8 +1434,8 @@ class WebEngineTab(browsertab.AbstractTab):\n             world_id = world\n             if not 0 <= world_id <= qtutils.MAX_WORLD_ID:\n                 raise browsertab.WebTabError(\n-                    \"World ID should be between 0 and {}\"\n-                    .format(qtutils.MAX_WORLD_ID))\n+                    \"World ID should be between 0 and {}\".format(qtutils.MAX_WORLD_ID)\n+                )\n         else:\n             world_id = _JS_WORLD_MAP[world]\n \n@@ -1425,9 +1477,11 @@ class WebEngineTab(browsertab.AbstractTab):\n         log.misc.debug(\"Showing error page for {}\".format(error))\n         url_string = url.toDisplayString()\n         error_page = jinja.render(\n-            'error.html',\n+            \"error.html\",\n             title=\"Error loading page: {}\".format(url_string),\n-            url=url_string, error=error)\n+            url=url_string,\n+            error=error,\n+        )\n         self.set_html(error_page)\n \n     @pyqtSlot()\n@@ -1447,9 +1501,9 @@ class WebEngineTab(browsertab.AbstractTab):\n         # Don't save the title if it's generated from the URL\n         title = self.title()\n         title_url = QUrl(url)\n-        title_url.setScheme('')\n+        title_url.setScheme(\"\")\n         title_url_str = title_url.toDisplayString(urlutils.FormatOption.REMOVE_SCHEME)\n-        if title == title_url_str.strip('/'):\n+        if title == title_url_str.strip(\"/\"):\n             title = \"\"\n \n         # Don't add history entry if the URL is invalid anyways\n@@ -1459,17 +1513,23 @@ class WebEngineTab(browsertab.AbstractTab):\n \n         self.history_item_triggered.emit(url, requested_url, title)\n \n-    @pyqtSlot(QUrl, 'QAuthenticator*', 'QString')\n-    def _on_proxy_authentication_required(self, url, authenticator,\n-                                          proxy_host):\n+    @pyqtSlot(QUrl, \"QAuthenticator*\", \"QString\")\n+    def _on_proxy_authentication_required(self, url, authenticator, proxy_host):\n         \"\"\"Called when a proxy needs authentication.\"\"\"\n         msg = \"<b>{}</b> requires a username and password.\".format(\n-            html_utils.escape(proxy_host))\n-        urlstr = url.toString(QUrl.UrlFormattingOption.RemovePassword | QUrl.ComponentFormattingOption.FullyEncoded)\n+            html_utils.escape(proxy_host)\n+        )\n+        urlstr = url.toString(\n+            QUrl.UrlFormattingOption.RemovePassword\n+            | QUrl.ComponentFormattingOption.FullyEncoded\n+        )\n         answer = message.ask(\n-            title=\"Proxy authentication required\", text=msg,\n+            title=\"Proxy authentication required\",\n+            text=msg,\n             mode=usertypes.PromptMode.user_pwd,\n-            abort_on=[self.abort_questions], url=urlstr)\n+            abort_on=[self.abort_questions],\n+            url=urlstr,\n+        )\n \n         if answer is None:\n             sip.assign(authenticator, QAuthenticator())\n@@ -1478,10 +1538,13 @@ class WebEngineTab(browsertab.AbstractTab):\n         authenticator.setUser(answer.user)\n         authenticator.setPassword(answer.password)\n \n-    @pyqtSlot(QUrl, 'QAuthenticator*')\n+    @pyqtSlot(QUrl, \"QAuthenticator*\")\n     def _on_authentication_required(self, url, authenticator):\n-        log.network.debug(\"Authentication requested for {}, netrc_used {}\"\n-                          .format(url.toDisplayString(), self.data.netrc_used))\n+        log.network.debug(\n+            \"Authentication requested for {}, netrc_used {}\".format(\n+                url.toDisplayString(), self.data.netrc_used\n+            )\n+        )\n \n         netrc_success = False\n         if not self.data.netrc_used:\n@@ -1491,7 +1554,8 @@ class WebEngineTab(browsertab.AbstractTab):\n         if not netrc_success:\n             log.network.debug(\"Asking for credentials\")\n             answer = shared.authentication_required(\n-                url, authenticator, abort_on=[self.abort_questions])\n+                url, authenticator, abort_on=[self.abort_questions]\n+            )\n             if answer is None:\n                 log.network.debug(\"Aborting auth\")\n                 sip.assign(authenticator, QAuthenticator())\n@@ -1506,30 +1570,33 @@ class WebEngineTab(browsertab.AbstractTab):\n         super()._on_load_started()\n         self.data.netrc_used = False\n \n-    @pyqtSlot('qint64')\n+    @pyqtSlot(\"qint64\")\n     def _on_renderer_process_pid_changed(self, pid):\n-        log.webview.debug(\"Renderer process PID for tab {}: {}\"\n-                          .format(self.tab_id, pid))\n+        log.webview.debug(\n+            \"Renderer process PID for tab {}: {}\".format(self.tab_id, pid)\n+        )\n \n     @pyqtSlot(QWebEnginePage.RenderProcessTerminationStatus, int)\n     def _on_render_process_terminated(self, status, exitcode):\n         \"\"\"Show an error when the renderer process terminated.\"\"\"\n-        if (status == QWebEnginePage.RenderProcessTerminationStatus.AbnormalTerminationStatus and\n-                exitcode == 256):\n+        if (\n+            status\n+            == QWebEnginePage.RenderProcessTerminationStatus.AbnormalTerminationStatus\n+            and exitcode == 256\n+        ):\n             # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-58697\n-            status = QWebEnginePage.RenderProcessTerminationStatus.CrashedTerminationStatus\n+            status = (\n+                QWebEnginePage.RenderProcessTerminationStatus.CrashedTerminationStatus\n+            )\n \n         status_map = {\n-            QWebEnginePage.RenderProcessTerminationStatus.NormalTerminationStatus:\n-                browsertab.TerminationStatus.normal,\n-            QWebEnginePage.RenderProcessTerminationStatus.AbnormalTerminationStatus:\n-                browsertab.TerminationStatus.abnormal,\n-            QWebEnginePage.RenderProcessTerminationStatus.CrashedTerminationStatus:\n-                browsertab.TerminationStatus.crashed,\n-            QWebEnginePage.RenderProcessTerminationStatus.KilledTerminationStatus:\n-                browsertab.TerminationStatus.killed,\n-            QWebEnginePage.RenderProcessTerminationStatus(-1):\n-                browsertab.TerminationStatus.unknown,\n+            QWebEnginePage.RenderProcessTerminationStatus.NormalTerminationStatus: browsertab.TerminationStatus.normal,\n+            QWebEnginePage.RenderProcessTerminationStatus.AbnormalTerminationStatus: browsertab.TerminationStatus.abnormal,\n+            QWebEnginePage.RenderProcessTerminationStatus.CrashedTerminationStatus: browsertab.TerminationStatus.crashed,\n+            QWebEnginePage.RenderProcessTerminationStatus.KilledTerminationStatus: browsertab.TerminationStatus.killed,\n+            QWebEnginePage.RenderProcessTerminationStatus(\n+                -1\n+            ): browsertab.TerminationStatus.unknown,\n         }\n         self.renderer_process_terminated.emit(status_map[status], exitcode)\n \n@@ -1559,9 +1626,9 @@ class WebEngineTab(browsertab.AbstractTab):\n         \"\"\"QtWebEngine-specific loadProgress workarounds.\"\"\"\n         super()._on_load_progress(perc)\n         if (\n-            self._needs_qtbug65223_workaround and\n-            perc == 100 and\n-            self.load_status() != usertypes.LoadStatus.error\n+            self._needs_qtbug65223_workaround\n+            and perc == 100\n+            and self.load_status() != usertypes.LoadStatus.error\n         ):\n             self._update_load_status(ok=True)\n \n@@ -1575,9 +1642,12 @@ class WebEngineTab(browsertab.AbstractTab):\n             self._update_load_status(ok)\n \n         if not ok:\n-            self.dump_async(functools.partial(\n-                self._error_page_workaround,\n-                self.settings.test_attribute('content.javascript.enabled')))\n+            self.dump_async(\n+                functools.partial(\n+                    self._error_page_workaround,\n+                    self.settings.test_attribute(\"content.javascript.enabled\"),\n+                )\n+            )\n \n     @pyqtSlot(certificateerror.CertificateErrorWrapper)\n     def _on_ssl_errors(self, error):\n@@ -1601,11 +1671,13 @@ class WebEngineTab(browsertab.AbstractTab):\n                 abort_on=[self.abort_questions],\n             )\n         else:\n-            log.network.error(\"Non-overridable certificate error: \"\n-                              \"{}\".format(error))\n+            log.network.error(\"Non-overridable certificate error: {}\".format(error))\n \n-        log.network.debug(\"ignore {}, URL {}, requested {}\".format(\n-            error.ignore, url, self.url(requested=True)))\n+        log.network.debug(\n+            \"ignore {}, URL {}, requested {}\".format(\n+                error.ignore, url, self.url(requested=True)\n+            )\n+        )\n \n     @pyqtSlot()\n     def _on_print_requested(self):\n@@ -1623,14 +1695,18 @@ class WebEngineTab(browsertab.AbstractTab):\n         local_schemes = {\"qute\", \"file\"}\n         qtwe_ver = version.qtwebengine_versions().webengine\n         if (\n-            navigation.accepted and\n-            self.url().scheme().lower() in local_schemes and\n-            navigation.url.scheme().lower() not in local_schemes and\n-            (navigation.navigation_type ==\n-                usertypes.NavigationRequest.Type.link_clicked) and\n-            navigation.is_main_frame and\n-            (utils.VersionNumber(6, 2) <= qtwe_ver < utils.VersionNumber(6, 2, 5) or\n-             utils.VersionNumber(6, 3) <= qtwe_ver < utils.VersionNumber(6, 3, 1))\n+            navigation.accepted\n+            and self.url().scheme().lower() in local_schemes\n+            and navigation.url.scheme().lower() not in local_schemes\n+            and (\n+                navigation.navigation_type\n+                == usertypes.NavigationRequest.Type.link_clicked\n+            )\n+            and navigation.is_main_frame\n+            and (\n+                utils.VersionNumber(6, 2) <= qtwe_ver < utils.VersionNumber(6, 2, 5)\n+                or utils.VersionNumber(6, 3) <= qtwe_ver < utils.VersionNumber(6, 3, 1)\n+            )\n         ):\n             log.webview.debug(\n                 \"Working around blocked request from local page \"\n@@ -1642,16 +1718,18 @@ class WebEngineTab(browsertab.AbstractTab):\n         # WORKAROUND for QtWebEngine >= 6.2 not allowing form requests from\n         # qute:// to outside domains.\n         if (\n-            qtwe_ver >= utils.VersionNumber(6, 2) and\n-            self.url() == QUrl(\"qute://start/\") and\n-            navigation.navigation_type == navigation.Type.form_submitted and\n-            navigation.url.matches(\n-                QUrl(config.val.url.searchengines['DEFAULT']),\n-                urlutils.FormatOption.REMOVE_QUERY)\n+            qtwe_ver >= utils.VersionNumber(6, 2)\n+            and self.url() == QUrl(\"qute://start/\")\n+            and navigation.navigation_type == navigation.Type.form_submitted\n+            and navigation.url.matches(\n+                QUrl(config.val.url.searchengines[\"DEFAULT\"]),\n+                urlutils.FormatOption.REMOVE_QUERY,\n+            )\n         ):\n             log.webview.debug(\n                 \"Working around qute://start loading issue for \"\n-                f\"{navigation.url.toDisplayString()}\")\n+                f\"{navigation.url.toDisplayString()}\"\n+            )\n             navigation.accepted = False\n             # Using QTimer.singleShot as WORKAROUND for this crashing otherwise\n             # with QtWebEngine 6.10: https://bugreports.qt.io/browse/QTBUG-140543\n@@ -1659,14 +1737,15 @@ class WebEngineTab(browsertab.AbstractTab):\n \n         # WORKAROUND for QtWebEngine 6.2 - 6.5 blocking back/forward navigation too\n         if (\n-            qtwe_ver >= utils.VersionNumber(6, 2) and\n-            qtwe_ver < utils.VersionNumber(6, 6) and\n-            self.url() == QUrl(\"qute://bookmarks/\") and\n-            navigation.navigation_type == navigation.Type.back_forward\n+            qtwe_ver >= utils.VersionNumber(6, 2)\n+            and qtwe_ver < utils.VersionNumber(6, 6)\n+            and self.url() == QUrl(\"qute://bookmarks/\")\n+            and navigation.navigation_type == navigation.Type.back_forward\n         ):\n             log.webview.debug(\n                 \"Working around qute://bookmarks loading issue for \"\n-                f\"{navigation.url.toDisplayString()}\")\n+                f\"{navigation.url.toDisplayString()}\"\n+            )\n             navigation.accepted = False\n             self.load_url(navigation.url)\n \n@@ -1682,24 +1761,30 @@ class WebEngineTab(browsertab.AbstractTab):\n         additional note if there are multiple matches.\n         \"\"\"\n         certificate = selection.certificates()[0]\n-        text = ('<b>Subject:</b> {subj}<br/>'\n-                '<b>Issuer:</b> {issuer}<br/>'\n-                '<b>Serial:</b> {serial}'.format(\n-                    subj=html_utils.escape(certificate.subjectDisplayName()),\n-                    issuer=html_utils.escape(certificate.issuerDisplayName()),\n-                    serial=bytes(certificate.serialNumber()).decode('ascii')))\n+        text = (\n+            \"<b>Subject:</b> {subj}<br/>\"\n+            \"<b>Issuer:</b> {issuer}<br/>\"\n+            \"<b>Serial:</b> {serial}\".format(\n+                subj=html_utils.escape(certificate.subjectDisplayName()),\n+                issuer=html_utils.escape(certificate.issuerDisplayName()),\n+                serial=bytes(certificate.serialNumber()).decode(\"ascii\"),\n+            )\n+        )\n         if len(selection.certificates()) > 1:\n-            text += ('<br/><br/><b>Note:</b> Multiple matching certificates '\n-                     'were found, but certificate selection is not '\n-                     'implemented yet!')\n+            text += (\n+                \"<br/><br/><b>Note:</b> Multiple matching certificates \"\n+                \"were found, but certificate selection is not \"\n+                \"implemented yet!\"\n+            )\n         urlstr = selection.host().host()\n \n         present = message.ask(\n-            title='Present client certificate to {}?'.format(urlstr),\n+            title=\"Present client certificate to {}?\".format(urlstr),\n             text=text,\n             mode=usertypes.PromptMode.yesno,\n             abort_on=[self.abort_questions],\n-            url=urlstr)\n+            url=urlstr,\n+        )\n \n         if present:\n             selection.select(certificate)\n@@ -1717,8 +1802,7 @@ class WebEngineTab(browsertab.AbstractTab):\n         page.loadStarted.connect(self._on_load_started)\n         page.certificate_error.connect(self._on_ssl_errors)\n         page.authenticationRequired.connect(self._on_authentication_required)\n-        page.proxyAuthenticationRequired.connect(\n-            self._on_proxy_authentication_required)\n+        page.proxyAuthenticationRequired.connect(self._on_proxy_authentication_required)\n         page.contentsSizeChanged.connect(self.contents_size_changed)\n         page.navigation_request.connect(self._on_navigation_request)\n         page.printRequested.connect(self._on_print_requested)\n@@ -1726,8 +1810,7 @@ class WebEngineTab(browsertab.AbstractTab):\n \n         view.titleChanged.connect(self.title_changed)\n         view.urlChanged.connect(self._on_url_changed)\n-        view.renderProcessTerminated.connect(\n-            self._on_render_process_terminated)\n+        view.renderProcessTerminated.connect(self._on_render_process_terminated)\n         view.iconChanged.connect(self.icon_changed)\n \n         page.loadFinished.connect(self._on_history_trigger)\n"
    },
    {
        "id": 271,
        "sha_fail": "8f705513f0244f89d4b0c186f76c7d7b0da37c85",
        "diff": "diff --git a/speechbrain/lobes/models/transformer/TransformerST.py b/speechbrain/lobes/models/transformer/TransformerST.py\nindex 14e3360..7268923 100644\n--- a/speechbrain/lobes/models/transformer/TransformerST.py\n+++ b/speechbrain/lobes/models/transformer/TransformerST.py\n@@ -435,7 +435,7 @@ class TransformerST(TransformerASR):\n         tgt_mask = get_lookahead_mask(tgt)\n \n         return src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask\n-    \n+\n     def make_masks_for_asr(self, src, tgt, wav_len, pad_idx=0):\n         \"\"\"This method generates the masks for training the transformer model.\n \n"
    },
    {
        "id": 272,
        "sha_fail": "938a4fb3f5dbe7e6ae75e049ecc5059bd25c14bf",
        "diff": "diff --git a/lm_eval/models/huggingface.py b/lm_eval/models/huggingface.py\nindex 842e01f..7db7345 100644\n--- a/lm_eval/models/huggingface.py\n+++ b/lm_eval/models/huggingface.py\n@@ -682,11 +682,13 @@ class HFLM(TemplateLM):\n                 raise AssertionError(\"load_in_4bit requires peft >= 0.4.0\")\n \n             # Compatible with Gemma3 (multimodal) and old models\n-            if hasattr(self._model.config, \"text_config\") and hasattr(self._model.config.text_config, \"vocab_size\"):\n+            if hasattr(self._model.config, \"text_config\") and hasattr(\n+                self._model.config.text_config, \"vocab_size\"\n+            ):\n                 vocab_size = self._model.config.text_config.vocab_size\n             else:\n                 vocab_size = self._model.config.vocab_size\n-            \n+\n             if vocab_size != len(self.tokenizer):\n                 # resize model for LoRAs with added tokens\n                 eval_logger.info(\n"
    },
    {
        "id": 273,
        "sha_fail": "49fc1f68d7b34a8ce47e5bfa8ae52b19222e6528",
        "diff": "diff --git a/lm_eval/models/huggingface.py b/lm_eval/models/huggingface.py\nindex 842e01f..7db7345 100644\n--- a/lm_eval/models/huggingface.py\n+++ b/lm_eval/models/huggingface.py\n@@ -682,11 +682,13 @@ class HFLM(TemplateLM):\n                 raise AssertionError(\"load_in_4bit requires peft >= 0.4.0\")\n \n             # Compatible with Gemma3 (multimodal) and old models\n-            if hasattr(self._model.config, \"text_config\") and hasattr(self._model.config.text_config, \"vocab_size\"):\n+            if hasattr(self._model.config, \"text_config\") and hasattr(\n+                self._model.config.text_config, \"vocab_size\"\n+            ):\n                 vocab_size = self._model.config.text_config.vocab_size\n             else:\n                 vocab_size = self._model.config.vocab_size\n-            \n+\n             if vocab_size != len(self.tokenizer):\n                 # resize model for LoRAs with added tokens\n                 eval_logger.info(\ndiff --git a/lm_eval/tasks/sparc/utils.py b/lm_eval/tasks/sparc/utils.py\nindex bd59d3b..41df922 100644\n--- a/lm_eval/tasks/sparc/utils.py\n+++ b/lm_eval/tasks/sparc/utils.py\n@@ -1,50 +1,51 @@\n-import re\n import json\n+import re\n+from typing import Any, Dict, List, Optional, Tuple\n+\n import datasets\n-from typing import List, Dict, Any, Tuple, Optional\n \n \n def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n     \"\"\"Process the SPaRC dataset documents.\"\"\"\n-    \n+\n     def _process_doc(doc):\n         # Extract and clean the grid visualization if present\n-        if 'text_visualization' in doc:\n-            doc['visualization'] = doc['text_visualization']\n-        \n+        if \"text_visualization\" in doc:\n+            doc[\"visualization\"] = doc[\"text_visualization\"]\n+\n         # Ensure solutions is a list\n-        if 'solutions' in doc and isinstance(doc['solutions'], str):\n+        if \"solutions\" in doc and isinstance(doc[\"solutions\"], str):\n             try:\n-                doc['solutions'] = json.loads(doc['solutions'])\n+                doc[\"solutions\"] = json.loads(doc[\"solutions\"])\n             except json.JSONDecodeError:\n-                doc['solutions'] = [doc['solutions']]\n-        elif 'solutions' in doc and not isinstance(doc['solutions'], list):\n-            doc['solutions'] = [doc['solutions']]\n-        \n+                doc[\"solutions\"] = [doc[\"solutions\"]]\n+        elif \"solutions\" in doc and not isinstance(doc[\"solutions\"], list):\n+            doc[\"solutions\"] = [doc[\"solutions\"]]\n+\n         # Convert polyshapes if it's a string\n-        if 'polyshapes' in doc and isinstance(doc['polyshapes'], str):\n+        if \"polyshapes\" in doc and isinstance(doc[\"polyshapes\"], str):\n             try:\n-                doc['polyshapes'] = json.loads(doc['polyshapes'])\n+                doc[\"polyshapes\"] = json.loads(doc[\"polyshapes\"])\n             except json.JSONDecodeError:\n-                doc['polyshapes'] = {}\n-        \n+                doc[\"polyshapes\"] = {}\n+\n         # Extract grid size\n-        if 'grid_size' in doc:\n-            if isinstance(doc['grid_size'], dict):\n-                doc['grid_height'] = doc['grid_size'].get('height', 10)\n-                doc['grid_width'] = doc['grid_size'].get('width', 10)\n+        if \"grid_size\" in doc:\n+            if isinstance(doc[\"grid_size\"], dict):\n+                doc[\"grid_height\"] = doc[\"grid_size\"].get(\"height\", 10)\n+                doc[\"grid_width\"] = doc[\"grid_size\"].get(\"width\", 10)\n             else:\n-                doc['grid_height'] = 10\n-                doc['grid_width'] = 10\n-        \n+                doc[\"grid_height\"] = 10\n+                doc[\"grid_width\"] = 10\n+\n         return doc\n-    \n+\n     return dataset.map(_process_doc)\n \n \n def doc_to_text(doc: Dict[str, Any]) -> str:\n     \"\"\"Convert a document to input text for the model using comprehensive prompt format.\"\"\"\n-    \n+\n     grid_size = doc.get(\"grid_size\", {\"width\": 0, \"height\": 0})\n     puzzle_array = doc.get(\"puzzle_array\", [])\n     grid_str = \"\\n\".join(map(str, puzzle_array))\n@@ -68,21 +69,21 @@ def doc_to_text(doc: Dict[str, Any]) -> str:\n                 polyshapes_json = {}\n         else:\n             polyshapes_json = polyshapes_data\n-            \n+\n         for shape_id, shape_def in polyshapes_json.items():\n             polyshapes_str += f\"Shape {shape_id}:\\n\"\n             if isinstance(shape_def, list):\n-                polyshapes_str += '\\n'.join(str(row) for row in shape_def)\n+                polyshapes_str += \"\\n\".join(str(row) for row in shape_def)\n             else:\n                 polyshapes_str += str(shape_def)\n             polyshapes_str += \"\\n\\n\"\n-    \n+\n     return f\"\"\"\n ## Objective\n You are a specialized AI proficient in spatial reasoning and solving puzzles from the game 'The Witness'. Your goal is to find a valid path (a continuous line) from the specified Start Node to the End Node on the provided grid, adhering to all puzzle rules.\n \n ## Core Concepts & Grid Basics\n-*   **Grid Dimensions:** The puzzle grid has {grid_size['width']} columns and {grid_size['height']} rows.\n+*   **Grid Dimensions:** The puzzle grid has {grid_size[\"width\"]} columns and {grid_size[\"height\"]} rows.\n *   **Coordinate System:** Nodes are identified by `(x, y)` coordinates. `(0,0)` is the top-left node. `x` increases to the right, `y` increases downwards.\n *   **Path:** The solution is a single, continuous line connecting adjacent nodes either horizontally or vertically.\n *   **No Revisits:** The path **CANNOT** visit the same node more than once.\n@@ -197,13 +198,13 @@ def doc_to_target(doc: Dict[str, Any]) -> str:\n     \"\"\"Extract the target solution and puzzle data for validation.\"\"\"\n     # Create a comprehensive target that includes both the solution and puzzle data\n     target_data = {\n-        \"solutions\": doc.get('solutions', []),\n-        \"puzzle_array\": doc.get('puzzle_array', []),\n-        \"grid_size\": doc.get('grid_size', {}),\n-        \"polyshapes\": doc.get('polyshapes', {}),\n-        \"difficulty\": doc.get('difficulty_level', \"unknown\")\n+        \"solutions\": doc.get(\"solutions\", []),\n+        \"puzzle_array\": doc.get(\"puzzle_array\", []),\n+        \"grid_size\": doc.get(\"grid_size\", {}),\n+        \"polyshapes\": doc.get(\"polyshapes\", {}),\n+        \"difficulty\": doc.get(\"difficulty_level\", \"unknown\"),\n     }\n-    \n+\n     # Return as JSON string so metrics can parse it\n     return json.dumps(target_data)\n \n@@ -268,7 +269,9 @@ def extract_solution_path(\n     return None\n \n \n-def validate_solution(extracted_path: Optional[List[Dict[str, int]]], puzzle_data: Dict) -> bool:\n+def validate_solution(\n+    extracted_path: Optional[List[Dict[str, int]]], puzzle_data: Dict\n+) -> bool:\n     \"\"\"Validate if the solution is valid by comparing with known solutions\n \n     Args:\n@@ -506,4 +509,4 @@ def aggregate_ignore_neg1_mean(items, **kwargs) -> float:\n     else:\n         flat = items\n     vals = [float(x) for x in flat if isinstance(x, (int, float)) and float(x) >= 0.0]\n-    return (sum(vals) / len(vals)) if len(vals) > 0 else 0.0\n\\ No newline at end of file\n+    return (sum(vals) / len(vals)) if len(vals) > 0 else 0.0\n"
    },
    {
        "id": 274,
        "sha_fail": "76de63fdc2facad254862b3db7e7f44a1e381cd2",
        "diff": "diff --git a/lm_eval/api/task.py b/lm_eval/api/task.py\nindex e15a014..24ad419 100644\n--- a/lm_eval/api/task.py\n+++ b/lm_eval/api/task.py\n@@ -745,7 +745,7 @@ class ConfigurableTask(Task):\n \n         # Use new configurations if there was no preconfiguration\n         if self.config is None:\n-            self._config = TaskConfig(**config)\n+            self._config = TaskConfig(**config) if config else TaskConfig()\n         # Overwrite configs\n         else:\n             if config is not None:\ndiff --git a/lm_eval/tasks/__init__.py b/lm_eval/tasks/__init__.py\nindex ec10eb1..552aff2 100644\n--- a/lm_eval/tasks/__init__.py\n+++ b/lm_eval/tasks/__init__.py\n@@ -7,7 +7,7 @@ from typing import Dict, List, Mapping, Optional, Union\n \n from lm_eval import utils\n from lm_eval.api.group import ConfigurableGroup, GroupConfig\n-from lm_eval.api.task import ConfigurableTask, Task\n+from lm_eval.api.task import BuilderConfig, ConfigurableTask, Task\n from lm_eval.evaluator_utils import get_subtask_list\n \n \n@@ -659,6 +659,8 @@ def get_task_dict(\n         **task_name_from_config_dict,\n         **task_name_from_object_dict,\n     }\n+    if \"qa17\" not in final_task_dict:\n+        raise ValueError(\"BuilderConfig 'qa17' not found in available configurations.\")\n \n     # behavior can get odd if one tries to invoke several groups that \"compete\" for the same task.\n     # (notably, because one could request several num_fewshot values at once in GroupConfig overrides for the subtask\n"
    },
    {
        "id": 277,
        "sha_fail": "bddc230cc11627fdbb8bdd98b67489afcad87d98",
        "diff": "diff --git a/tests/dialects/test_dialect.py b/tests/dialects/test_dialect.py\nindex 892fcdc..93d3338 100644\n--- a/tests/dialects/test_dialect.py\n+++ b/tests/dialects/test_dialect.py\n@@ -22,7 +22,12 @@ class Validator(unittest.TestCase):\n         return parse_one(sql, read=self.dialect, **kwargs)\n \n     def validate_identity(\n-        self, sql, write_sql=None, pretty=False, check_command_warning=False, identify=False\n+        self,\n+        sql,\n+        write_sql=None,\n+        pretty=False,\n+        check_command_warning=False,\n+        identify=False,\n     ):\n         if check_command_warning:\n             with self.assertLogs(parser_logger) as cm:\n@@ -32,7 +37,8 @@ class Validator(unittest.TestCase):\n             expression = self.parse_one(sql)\n \n         self.assertEqual(\n-            write_sql or sql, expression.sql(dialect=self.dialect, pretty=pretty, identify=identify)\n+            write_sql or sql,\n+            expression.sql(dialect=self.dialect, pretty=pretty, identify=identify),\n         )\n         return expression\n \n@@ -67,7 +73,9 @@ class Validator(unittest.TestCase):\n             with self.subTest(f\"{sql} -> {write_dialect}\"):\n                 if write_sql is UnsupportedError:\n                     with self.assertRaises(UnsupportedError):\n-                        expression.sql(write_dialect, unsupported_level=ErrorLevel.RAISE)\n+                        expression.sql(\n+                            write_dialect, unsupported_level=ErrorLevel.RAISE\n+                        )\n                 else:\n                     self.assertEqual(\n                         expression.sql(\n@@ -113,7 +121,9 @@ class TestDialect(Validator):\n         lowercase_mysql = Dialect.get_or_raise(\"mysql,normalization_strategy=lowercase\")\n         self.assertEqual(lowercase_mysql.normalization_strategy, \"LOWERCASE\")\n \n-        lowercase_mysql = Dialect.get_or_raise(\"mysql, normalization_strategy = lowercase\")\n+        lowercase_mysql = Dialect.get_or_raise(\n+            \"mysql, normalization_strategy = lowercase\"\n+        )\n         self.assertEqual(lowercase_mysql.normalization_strategy.value, \"LOWERCASE\")\n \n         with self.assertRaises(AttributeError) as cm:\n@@ -124,7 +134,9 @@ class TestDialect(Validator):\n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"myqsl\")\n \n-        self.assertEqual(str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\")\n+        self.assertEqual(\n+            str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\"\n+        )\n \n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"asdfjasodiufjsd\")\n@@ -140,7 +152,9 @@ class TestDialect(Validator):\n         class MyDialect(Dialect):\n             SUPPORTED_SETTINGS = {\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"}\n \n-        bool_settings = Dialect.get_or_raise(\"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\")\n+        bool_settings = Dialect.get_or_raise(\n+            \"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\"\n+        )\n         self.assertEqual(\n             bool_settings.settings,\n             {\"s1\": True, \"s2\": True, \"s3\": False, \"s4\": False, \"s5\": \"nonbool\"},\n@@ -450,7 +464,9 @@ class TestDialect(Validator):\n             },\n         )\n         self.validate_all(\"CAST(a AS TINYINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n-        self.validate_all(\"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n+        self.validate_all(\n+            \"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"}\n+        )\n         self.validate_all(\"CAST(a AS BIGINT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\"CAST(a AS INT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\n@@ -1411,11 +1427,15 @@ class TestDialect(Validator):\n         )\n \n         order_by_all_sql = \"SELECT * FROM t ORDER BY ALL\"\n-        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(exp.Column)\n+        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(\n+            exp.Column\n+        )\n \n         for dialect in (\"duckdb\", \"spark\", \"databricks\"):\n             with self.subTest(f\"Testing ORDER BY ALL in {dialect}\"):\n-                parse_one(order_by_all_sql, read=dialect).find(exp.Ordered).this.assert_is(exp.Var)\n+                parse_one(order_by_all_sql, read=dialect).find(\n+                    exp.Ordered\n+                ).this.assert_is(exp.Var)\n \n     def test_json(self):\n         self.validate_all(\n@@ -1590,9 +1610,13 @@ class TestDialect(Validator):\n         )\n \n         for dialect in (\"duckdb\", \"starrocks\"):\n-            with self.subTest(f\"Generating json extraction with digit-prefixed key ({dialect})\"):\n+            with self.subTest(\n+                f\"Generating json extraction with digit-prefixed key ({dialect})\"\n+            ):\n                 self.assertEqual(\n-                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(dialect=dialect),\n+                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(\n+                        dialect=dialect\n+                    ),\n                     \"\"\"SELECT '{\"0\": \"v\"}' -> '0'\"\"\",\n                 )\n \n@@ -1809,7 +1833,9 @@ class TestDialect(Validator):\n         )\n \n     def test_operators(self):\n-        self.validate_identity(\"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\")\n+        self.validate_identity(\n+            \"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\"\n+        )\n         self.validate_identity(\"some.column LIKE 'foo' + another.column + 'bar'\")\n \n         self.validate_all(\"LIKE(x, 'z')\", write={\"\": \"'z' LIKE x\"})\n@@ -2299,7 +2325,9 @@ class TestDialect(Validator):\n         )\n \n     def test_typeddiv(self):\n-        typed_div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True)\n+        typed_div = exp.Div(\n+            this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True\n+        )\n         div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"))\n         typed_div_dialect = \"presto\"\n         div_dialect = \"hive\"\n@@ -2324,7 +2352,9 @@ class TestDialect(Validator):\n             (div, (INT, FLOAT), typed_div_dialect, \"a / b\"),\n             (div, (INT, FLOAT), div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"\n+            ):\n                 expression = expression.copy()\n                 expression.left.type = types[0]\n                 expression.right.type = types[1]\n@@ -2342,7 +2372,9 @@ class TestDialect(Validator):\n             (div, safe_div_dialect, \"a / b\"),\n             (div, div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {dialect} -> {expected}\"\n+            ):\n                 self.assertEqual(expected, expression.sql(dialect=dialect))\n \n         self.assertEqual(\n@@ -2814,7 +2846,8 @@ SELECT\n         self.validate_identity(\"COUNT_IF(DISTINCT cond)\")\n \n         self.validate_all(\n-            \"SELECT COUNT_IF(cond) FILTER\", write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"}\n+            \"SELECT COUNT_IF(cond) FILTER\",\n+            write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"},\n         )\n         self.validate_all(\n             \"SELECT COUNT_IF(col % 2 = 0) FROM foo\",\n@@ -3038,8 +3071,12 @@ FROM subquery2\"\"\",\n         with_large_nulls = \"postgres\"\n \n         sql = \"SELECT * FROM t ORDER BY c\"\n-        sql_nulls_last = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n-        sql_nulls_first = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        sql_nulls_last = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n+        )\n+        sql_nulls_first = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        )\n \n         for read_dialect, desc, nulls_first, expected_sql in (\n             (with_last_nulls, False, None, sql_nulls_last),\n@@ -3072,7 +3109,9 @@ FROM subquery2\"\"\",\n                 )\n \n                 expected_sql = f\"{expected_sql}{sort_order}\"\n-                expression = parse_one(f\"{sql}{sort_order}{null_order}\", read=read_dialect)\n+                expression = parse_one(\n+                    f\"{sql}{sort_order}{null_order}\", read=read_dialect\n+                )\n \n                 self.assertEqual(expression.sql(dialect=\"mysql\"), expected_sql)\n                 self.assertEqual(expression.sql(dialect=\"tsql\"), expected_sql)\n@@ -3157,7 +3196,9 @@ FROM subquery2\"\"\",\n         self.validate_identity(\n             \"CREATE SEQUENCE seq START WITH 1 NO MINVALUE NO MAXVALUE CYCLE NO CACHE\"\n         )\n-        self.validate_identity(\"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\")\n+        self.validate_identity(\n+            \"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\"\n+        )\n         self.validate_identity(\n             \"CREATE OR REPLACE SEQUENCE IF NOT EXISTS seq COMMENT='test comment' ORDER\"\n         )\n@@ -3318,7 +3359,9 @@ FROM subquery2\"\"\",\n                     },\n                 )\n \n-        self.assertIsInstance(parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var)\n+        self.assertIsInstance(\n+            parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var\n+        )\n \n     def test_coalesce(self):\n         \"\"\"\n@@ -3425,7 +3468,9 @@ FROM subquery2\"\"\",\n \n     def test_escaped_identifier_delimiter(self):\n         for dialect in (\"databricks\", \"hive\", \"mysql\", \"spark2\", \"spark\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3447,7 +3492,9 @@ FROM subquery2\"\"\",\n             \"snowflake\",\n             \"sqlite\",\n         ):\n-            with self.subTest(f\"Testing escaped double-quote in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped double-quote in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x\"\"\"',\n                     read={\n@@ -3459,7 +3506,9 @@ FROM subquery2\"\"\",\n                 )\n \n         for dialect in (\"clickhouse\", \"sqlite\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3569,14 +3618,19 @@ FROM subquery2\"\"\",\n                 \"spark\",\n                 \"redshift\",\n             ):\n-                with self.subTest(f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"):\n+                with self.subTest(\n+                    f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"\n+                ):\n                     self.assertEqual(\n-                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect), \"SELECT 204\"\n+                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect),\n+                        \"SELECT 204\",\n                     )\n \n             for other_integer_dialects in integer_dialects:\n                 self.assertEqual(\n-                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(other_integer_dialects),\n+                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(\n+                        other_integer_dialects\n+                    ),\n                     \"SELECT 0xCC\",\n                 )\n \ndiff --git a/tests/dialects/test_dremio.py b/tests/dialects/test_dremio.py\nindex d5fc362..48b1aa6 100644\n--- a/tests/dialects/test_dremio.py\n+++ b/tests/dialects/test_dremio.py\n@@ -63,7 +63,8 @@ class TestDremio(Validator):\n             \"SELECT * FROM t ORDER BY a NULLS LAST\", \"SELECT * FROM t ORDER BY a\"\n         )\n         self.validate_identity(\n-            \"SELECT * FROM t ORDER BY a DESC NULLS LAST\", \"SELECT * FROM t ORDER BY a DESC\"\n+            \"SELECT * FROM t ORDER BY a DESC NULLS LAST\",\n+            \"SELECT * FROM t ORDER BY a DESC\",\n         )\n \n         # If the clause is not the default, it must be kept\n@@ -138,20 +139,26 @@ class TestDremio(Validator):\n         to_char = self.validate_identity(\"TO_CHAR(3.14, '#.#')\").assert_is(exp.ToChar)\n         assert to_char.args[\"is_numeric\"] is True\n \n-        to_char = self.validate_identity(\"TO_CHAR(columnname, '#.##')\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(columnname, '#.##')\").assert_is(\n+            exp.ToChar\n+        )\n         assert to_char.args[\"is_numeric\"] is True\n \n         # Non-numeric formats or columns should have is_numeric=None or False\n         to_char = self.validate_identity(\"TO_CHAR(5555)\").assert_is(exp.ToChar)\n         assert not to_char.args.get(\"is_numeric\")\n \n-        to_char = self.validate_identity(\"TO_CHAR(3.14, columnname)\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(3.14, columnname)\").assert_is(\n+            exp.ToChar\n+        )\n         assert not to_char.args.get(\"is_numeric\")\n \n         to_char = self.validate_identity(\"TO_CHAR(123, 'abcd')\").assert_is(exp.ToChar)\n         assert not to_char.args.get(\"is_numeric\")\n \n-        to_char = self.validate_identity(\"TO_CHAR(3.14, UPPER('abcd'))\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(3.14, UPPER('abcd'))\").assert_is(\n+            exp.ToChar\n+        )\n         assert not to_char.args.get(\"is_numeric\")\n \n     def test_time_diff(self):\n@@ -175,7 +182,9 @@ class TestDremio(Validator):\n             \"SELECT DATE_SUB(col, 2, 'HOUR')\", \"SELECT TIMESTAMPADD(HOUR, -2, col)\"\n         )\n \n-        self.validate_identity(\"SELECT DATE_ADD(col, 2, 'DAY')\", \"SELECT DATE_ADD(col, 2)\")\n+        self.validate_identity(\n+            \"SELECT DATE_ADD(col, 2, 'DAY')\", \"SELECT DATE_ADD(col, 2)\"\n+        )\n \n         self.validate_identity(\n             \"SELECT DATE_SUB(col, a, 'HOUR')\", \"SELECT TIMESTAMPADD(HOUR, a * -1, col)\"\n@@ -190,6 +199,8 @@ class TestDremio(Validator):\n         )\n \n         self.validate_all(\n-            \"SELECT DATE_FORMAT(CAST('2025-08-18 15:30:00' AS TIMESTAMP), 'yyyy-mm-dd')\",\n-            \"SELECT TO_CHAR(CAST('2025-08-18 15:30:00' AS TIMESTAMP), 'yyyy-mm-dd')\",\n+            {\n+                \"query\": \"SELECT DATE_FORMAT(CAST('2025-08-18 15:30:00' AS TIMESTAMP), 'yyyy-mm-dd')\",\n+                \"expected\": \"SELECT TO_CHAR(CAST('2025-08-18 15:30:00' AS TIMESTAMP), 'yyyy-mm-dd')\",\n+            }\n         )\n"
    },
    {
        "id": 278,
        "sha_fail": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
        "diff": "diff --git a/tools/document_segmentation_server.py b/tools/document_segmentation_server.py\nindex 3ce17bf..59cb57b 100644\n--- a/tools/document_segmentation_server.py\n+++ b/tools/document_segmentation_server.py\n@@ -22,7 +22,7 @@ large research papers and technical documents that exceed LLM token limits.\n    - Stores segmentation index for efficient retrieval\n    - Returns: JSON with segmentation status, strategy used, and segment count\n \n-\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None, \n+\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None,\n                          max_segments: int = 3, max_total_chars: int = None)\n    Purpose: Intelligently retrieves relevant document segments based on query context\n    - query_type: \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n@@ -63,8 +63,7 @@ import re\n import json\n import sys\n import io\n-from pathlib import Path\n-from typing import Dict, Any, List, Optional, Tuple\n+from typing import Dict, List, Tuple\n import hashlib\n import logging\n from datetime import datetime\n@@ -92,9 +91,11 @@ logger = logging.getLogger(__name__)\n # Create FastMCP server instance\n mcp = FastMCP(\"document-segmentation-server\")\n \n+\n @dataclass\n class DocumentSegment:\n     \"\"\"Represents a document segment with metadata\"\"\"\n+\n     id: str\n     title: str\n     content: str\n@@ -106,9 +107,11 @@ class DocumentSegment:\n     relevance_scores: Dict[str, float]  # Scores for different query types\n     section_path: str  # e.g., \"3.2.1\" for nested sections\n \n-@dataclass \n+\n+@dataclass\n class DocumentIndex:\n     \"\"\"Document index containing all segments and metadata\"\"\"\n+\n     document_path: str\n     document_type: str  # \"academic_paper\", \"technical_doc\", \"code_doc\", \"general\"\n     segmentation_strategy: str\n@@ -116,66 +119,86 @@ class DocumentIndex:\n     total_chars: int\n     segments: List[DocumentSegment]\n     created_at: str\n-    \n+\n+\n class DocumentAnalyzer:\n     \"\"\"Enhanced document analyzer using semantic content analysis instead of mechanical structure detection\"\"\"\n-    \n+\n     # More precise semantic indicators, weighted by importance\n     ALGORITHM_INDICATORS = {\n-        \"high\": [\"algorithm\", \"procedure\", \"method\", \"approach\", \"technique\", \"framework\"],\n+        \"high\": [\n+            \"algorithm\",\n+            \"procedure\",\n+            \"method\",\n+            \"approach\",\n+            \"technique\",\n+            \"framework\",\n+        ],\n         \"medium\": [\"step\", \"process\", \"implementation\", \"computation\", \"calculation\"],\n-        \"low\": [\"example\", \"illustration\", \"demonstration\"]\n+        \"low\": [\"example\", \"illustration\", \"demonstration\"],\n     }\n-    \n+\n     TECHNICAL_CONCEPT_INDICATORS = {\n         \"high\": [\"formula\", \"equation\", \"theorem\", \"lemma\", \"proof\", \"definition\"],\n         \"medium\": [\"parameter\", \"variable\", \"function\", \"model\", \"architecture\"],\n-        \"low\": [\"notation\", \"symbol\", \"term\"]\n+        \"low\": [\"notation\", \"symbol\", \"term\"],\n     }\n-    \n+\n     IMPLEMENTATION_INDICATORS = {\n         \"high\": [\"code\", \"implementation\", \"programming\", \"software\", \"system\"],\n         \"medium\": [\"design\", \"structure\", \"module\", \"component\", \"interface\"],\n-        \"low\": [\"tool\", \"library\", \"package\"]\n+        \"low\": [\"tool\", \"library\", \"package\"],\n     }\n-    \n+\n     # Semantic features of document types (not just based on titles)\n     RESEARCH_PAPER_PATTERNS = [\n         r\"(?i)\\babstract\\b.*?\\n.*?(introduction|motivation|background)\",\n         r\"(?i)(methodology|method).*?(experiment|evaluation|result)\",\n         r\"(?i)(conclusion|future work|limitation).*?(reference|bibliography)\",\n-        r\"(?i)(related work|literature review|prior art)\"\n+        r\"(?i)(related work|literature review|prior art)\",\n     ]\n-    \n+\n     TECHNICAL_DOC_PATTERNS = [\n         r\"(?i)(getting started|installation|setup).*?(usage|example)\",\n         r\"(?i)(api|interface|specification).*?(parameter|endpoint)\",\n         r\"(?i)(tutorial|guide|walkthrough).*?(step|instruction)\",\n-        r\"(?i)(troubleshooting|faq|common issues)\"\n+        r\"(?i)(troubleshooting|faq|common issues)\",\n     ]\n-    \n+\n     def analyze_document_type(self, content: str) -> Tuple[str, float]:\n         \"\"\"\n         Enhanced document type analysis based on semantic content patterns\n-        \n+\n         Returns:\n             Tuple[str, float]: (document_type, confidence_score)\n         \"\"\"\n         content_lower = content.lower()\n-        \n+\n         # Calculate weighted semantic indicator scores\n-        algorithm_score = self._calculate_weighted_score(content_lower, self.ALGORITHM_INDICATORS)\n-        concept_score = self._calculate_weighted_score(content_lower, self.TECHNICAL_CONCEPT_INDICATORS)\n-        implementation_score = self._calculate_weighted_score(content_lower, self.IMPLEMENTATION_INDICATORS)\n-        \n+        algorithm_score = self._calculate_weighted_score(\n+            content_lower, self.ALGORITHM_INDICATORS\n+        )\n+        concept_score = self._calculate_weighted_score(\n+            content_lower, self.TECHNICAL_CONCEPT_INDICATORS\n+        )\n+        implementation_score = self._calculate_weighted_score(\n+            content_lower, self.IMPLEMENTATION_INDICATORS\n+        )\n+\n         # Detect semantic patterns of document types\n-        research_pattern_score = self._detect_pattern_score(content, self.RESEARCH_PAPER_PATTERNS)\n-        technical_pattern_score = self._detect_pattern_score(content, self.TECHNICAL_DOC_PATTERNS)\n-        \n+        research_pattern_score = self._detect_pattern_score(\n+            content, self.RESEARCH_PAPER_PATTERNS\n+        )\n+        technical_pattern_score = self._detect_pattern_score(\n+            content, self.TECHNICAL_DOC_PATTERNS\n+        )\n+\n         # Comprehensive evaluation of document type\n-        total_research_score = algorithm_score + concept_score + research_pattern_score * 2\n+        total_research_score = (\n+            algorithm_score + concept_score + research_pattern_score * 2\n+        )\n         total_technical_score = implementation_score + technical_pattern_score * 2\n-        \n+\n         # Determine document type based on content density and pattern matching\n         if research_pattern_score > 0.5 and total_research_score > 3.0:\n             return \"research_paper\", min(0.95, 0.6 + research_pattern_score * 0.35)\n@@ -187,17 +210,21 @@ class DocumentAnalyzer:\n             return \"implementation_guide\", 0.75\n         else:\n             return \"general_document\", 0.5\n-    \n-    def _calculate_weighted_score(self, content: str, indicators: Dict[str, List[str]]) -> float:\n+\n+    def _calculate_weighted_score(\n+        self, content: str, indicators: Dict[str, List[str]]\n+    ) -> float:\n         \"\"\"Calculate weighted semantic indicator scores\"\"\"\n         score = 0.0\n         for weight_level, terms in indicators.items():\n             weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[weight_level]\n             for term in terms:\n                 if term in content:\n-                    score += weight * (content.count(term) * 0.5 + 1)  # Consider term frequency\n+                    score += weight * (\n+                        content.count(term) * 0.5 + 1\n+                    )  # Consider term frequency\n         return score\n-    \n+\n     def _detect_pattern_score(self, content: str, patterns: List[str]) -> float:\n         \"\"\"Detect semantic pattern matching scores\"\"\"\n         matches = 0\n@@ -205,7 +232,7 @@ class DocumentAnalyzer:\n             if re.search(pattern, content, re.DOTALL):\n                 matches += 1\n         return matches / len(patterns)\n-    \n+\n     def detect_segmentation_strategy(self, content: str, doc_type: str) -> str:\n         \"\"\"\n         Intelligently determine the best segmentation strategy based on content semantics rather than mechanical structure\n@@ -213,8 +240,10 @@ class DocumentAnalyzer:\n         # Analyze content characteristics\n         algorithm_density = self._calculate_algorithm_density(content)\n         concept_complexity = self._calculate_concept_complexity(content)\n-        implementation_detail_level = self._calculate_implementation_detail_level(content)\n-        \n+        implementation_detail_level = self._calculate_implementation_detail_level(\n+            content\n+        )\n+\n         # Select strategy based on document type and content characteristics\n         if doc_type == \"research_paper\" and algorithm_density > 0.3:\n             return \"semantic_research_focused\"\n@@ -226,21 +255,21 @@ class DocumentAnalyzer:\n             return \"semantic_chunking_enhanced\"\n         else:\n             return \"content_aware_segmentation\"\n-    \n+\n     def _calculate_algorithm_density(self, content: str) -> float:\n         \"\"\"Calculate algorithm content density\"\"\"\n         total_chars = len(content)\n         algorithm_chars = 0\n-        \n+\n         # Identify algorithm blocks\n         algorithm_patterns = [\n-            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)',\n-            r'(?i)(step\\s+\\d+|phase\\s+\\d+)',\n-            r'(?i)(input:|output:|return:|initialize:)',\n-            r'(?i)(for\\s+each|while|if.*then|else)',\n-            r'(?i)(function|method|procedure).*\\('\n+            r\"(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)\",\n+            r\"(?i)(step\\s+\\d+|phase\\s+\\d+)\",\n+            r\"(?i)(input:|output:|return:|initialize:)\",\n+            r\"(?i)(for\\s+each|while|if.*then|else)\",\n+            r\"(?i)(function|method|procedure).*\\(\",\n         ]\n-        \n+\n         for pattern in algorithm_patterns:\n             matches = re.finditer(pattern, content)\n             for match in matches:\n@@ -248,44 +277,45 @@ class DocumentAnalyzer:\n                 start = max(0, match.start() - 200)\n                 end = min(len(content), match.end() + 800)\n                 algorithm_chars += end - start\n-        \n+\n         return min(1.0, algorithm_chars / total_chars)\n-    \n+\n     def _calculate_concept_complexity(self, content: str) -> float:\n         \"\"\"Calculate concept complexity\"\"\"\n         concept_indicators = self.TECHNICAL_CONCEPT_INDICATORS\n         complexity_score = 0.0\n-        \n+\n         for level, terms in concept_indicators.items():\n             weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[level]\n             for term in terms:\n                 complexity_score += content.lower().count(term) * weight\n-        \n+\n         # Normalize to 0-1 range\n         return min(1.0, complexity_score / 100)\n-    \n+\n     def _calculate_implementation_detail_level(self, content: str) -> float:\n         \"\"\"Calculate implementation detail level\"\"\"\n         implementation_patterns = [\n-            r'(?i)(code|implementation|programming)',\n-            r'(?i)(class|function|method|variable)',\n-            r'(?i)(import|include|library)',\n-            r'(?i)(parameter|argument|return)',\n-            r'(?i)(example|demo|tutorial)'\n+            r\"(?i)(code|implementation|programming)\",\n+            r\"(?i)(class|function|method|variable)\",\n+            r\"(?i)(import|include|library)\",\n+            r\"(?i)(parameter|argument|return)\",\n+            r\"(?i)(example|demo|tutorial)\",\n         ]\n-        \n+\n         detail_score = 0\n         for pattern in implementation_patterns:\n             detail_score += len(re.findall(pattern, content))\n-        \n+\n         return min(1.0, detail_score / 50)\n \n+\n class DocumentSegmenter:\n     \"\"\"Creates intelligent segments from documents\"\"\"\n-    \n+\n     def __init__(self):\n         self.analyzer = DocumentAnalyzer()\n-    \n+\n     def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:\n         \"\"\"\n         Perform intelligent segmentation using the specified strategy\n@@ -303,38 +333,45 @@ class DocumentSegmenter:\n         else:\n             # Compatibility with legacy strategies\n             return self._segment_by_enhanced_semantic_chunks(content)\n-    \n+\n     def _segment_by_headers(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment document based on markdown headers\"\"\"\n         segments = []\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n         current_segment = []\n         current_header = None\n         current_level = 0\n         char_pos = 0\n-        \n+\n         for line in lines:\n-            line_with_newline = line + '\\n'\n-            \n+            line_with_newline = line + \"\\n\"\n+\n             # Check if line is a header\n-            header_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n-            \n+            header_match = re.match(r\"^(#{1,6})\\s+(.+)$\", line)\n+\n             if header_match:\n                 # Save previous segment if exists\n                 if current_segment and current_header:\n-                    segment_content = '\\n'.join(current_segment).strip()\n+                    segment_content = \"\\n\".join(current_segment).strip()\n                     if segment_content:\n                         # Analyze content type and importance\n-                        content_type = self._classify_content_type(current_header, segment_content)\n-                        importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n-                        \n+                        content_type = self._classify_content_type(\n+                            current_header, segment_content\n+                        )\n+                        importance_score = (\n+                            0.8 if content_type in [\"algorithm\", \"formula\"] else 0.7\n+                        )\n+\n                         segment = self._create_enhanced_segment(\n-                            segment_content, current_header, \n-                            char_pos - len(segment_content.encode('utf-8')), char_pos,\n-                            importance_score, content_type\n+                            segment_content,\n+                            current_header,\n+                            char_pos - len(segment_content.encode(\"utf-8\")),\n+                            char_pos,\n+                            importance_score,\n+                            content_type,\n                         )\n                         segments.append(segment)\n-                \n+\n                 # Start new segment\n                 current_level = len(header_match.group(1))\n                 current_header = header_match.group(2).strip()\n@@ -342,128 +379,143 @@ class DocumentSegmenter:\n             else:\n                 if current_segment is not None:\n                     current_segment.append(line)\n-            \n-            char_pos += len(line_with_newline.encode('utf-8'))\n-        \n+\n+            char_pos += len(line_with_newline.encode(\"utf-8\"))\n+\n         # Add final segment\n         if current_segment and current_header:\n-            segment_content = '\\n'.join(current_segment).strip()\n+            segment_content = \"\\n\".join(current_segment).strip()\n             if segment_content:\n                 # Analyze content type and importance\n-                content_type = self._classify_content_type(current_header, segment_content)\n-                importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n-                \n+                content_type = self._classify_content_type(\n+                    current_header, segment_content\n+                )\n+                importance_score = (\n+                    0.8 if content_type in [\"algorithm\", \"formula\"] else 0.7\n+                )\n+\n                 segment = self._create_enhanced_segment(\n-                    segment_content, current_header, \n-                    char_pos - len(segment_content.encode('utf-8')), char_pos,\n-                    importance_score, content_type\n+                    segment_content,\n+                    current_header,\n+                    char_pos - len(segment_content.encode(\"utf-8\")),\n+                    char_pos,\n+                    importance_score,\n+                    content_type,\n                 )\n                 segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_preserve_algorithm_integrity(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_preserve_algorithm_integrity(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Smart segmentation strategy that preserves algorithm integrity\"\"\"\n         segments = []\n-        \n+\n         # 1. Identify algorithm blocks and related descriptions\n         algorithm_blocks = self._identify_algorithm_blocks(content)\n-        \n+\n         # 2. Identify concept definition groups\n         concept_groups = self._identify_concept_groups(content)\n-        \n+\n         # 3. Identify formula derivation chains\n         formula_chains = self._identify_formula_chains(content)\n-        \n+\n         # 4. Merge related content blocks to ensure integrity\n         content_blocks = self._merge_related_content_blocks(\n             algorithm_blocks, concept_groups, formula_chains, content\n         )\n-        \n+\n         # 5. Convert to DocumentSegment\n         for i, block in enumerate(content_blocks):\n             segment = self._create_enhanced_segment(\n-                block['content'], \n-                block['title'], \n-                block['start_pos'], \n-                block['end_pos'],\n-                block['importance_score'],\n-                block['content_type']\n+                block[\"content\"],\n+                block[\"title\"],\n+                block[\"start_pos\"],\n+                block[\"end_pos\"],\n+                block[\"importance_score\"],\n+                block[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_research_paper_semantically(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_research_paper_semantically(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Semantic segmentation specifically for research papers\"\"\"\n         segments = []\n-        \n+\n         # Identify semantic structure of research papers\n         paper_sections = self._identify_research_paper_sections(content)\n-        \n+\n         for section in paper_sections:\n             # Ensure each section contains sufficient context\n             enhanced_content = self._enhance_section_with_context(section, content)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                enhanced_content['content'],\n-                enhanced_content['title'],\n-                enhanced_content['start_pos'],\n-                enhanced_content['end_pos'],\n-                enhanced_content['importance_score'],\n-                enhanced_content['content_type']\n+                enhanced_content[\"content\"],\n+                enhanced_content[\"title\"],\n+                enhanced_content[\"start_pos\"],\n+                enhanced_content[\"end_pos\"],\n+                enhanced_content[\"importance_score\"],\n+                enhanced_content[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_concept_implementation_hybrid(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_concept_implementation_hybrid(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Intelligent segmentation combining concepts and implementation\"\"\"\n         segments = []\n-        \n+\n         # Identify concept-implementation correspondence\n         concept_impl_pairs = self._identify_concept_implementation_pairs(content)\n-        \n+\n         for pair in concept_impl_pairs:\n             # Merge related concepts and implementations into one segment\n             merged_content = self._merge_concept_with_implementation(pair, content)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                merged_content['content'],\n-                merged_content['title'],\n-                merged_content['start_pos'],\n-                merged_content['end_pos'],\n-                merged_content['importance_score'],\n-                merged_content['content_type']\n+                merged_content[\"content\"],\n+                merged_content[\"title\"],\n+                merged_content[\"start_pos\"],\n+                merged_content[\"end_pos\"],\n+                merged_content[\"importance_score\"],\n+                merged_content[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_by_enhanced_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_by_enhanced_semantic_chunks(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Enhanced semantic chunk segmentation\"\"\"\n         segments = []\n-        \n+\n         # Use improved semantic boundary detection\n         semantic_boundaries = self._detect_semantic_boundaries(content)\n-        \n+\n         current_start = 0\n         for i, boundary in enumerate(semantic_boundaries):\n-            chunk_content = content[current_start:boundary['position']]\n-            \n+            chunk_content = content[current_start : boundary[\"position\"]]\n+\n             if len(chunk_content.strip()) > 200:  # Minimum content threshold\n                 segment = self._create_enhanced_segment(\n                     chunk_content,\n-                    boundary['suggested_title'],\n+                    boundary[\"suggested_title\"],\n                     current_start,\n-                    boundary['position'],\n-                    boundary['importance_score'],\n-                    boundary['content_type']\n+                    boundary[\"position\"],\n+                    boundary[\"importance_score\"],\n+                    boundary[\"content_type\"],\n                 )\n                 segments.append(segment)\n-            \n-            current_start = boundary['position']\n-        \n+\n+            current_start = boundary[\"position\"]\n+\n         # Handle the final segment\n         if current_start < len(content):\n             final_content = content[current_start:]\n@@ -474,417 +526,484 @@ class DocumentSegmenter:\n                     current_start,\n                     len(content),\n                     0.7,\n-                    \"general\"\n+                    \"general\",\n                 )\n                 segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_content_aware(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Content-aware intelligent segmentation\"\"\"\n         segments = []\n-        \n+\n         # Adaptive segmentation size\n         optimal_chunk_size = self._calculate_optimal_chunk_size(content)\n-        \n+\n         # Segment based on content density\n         content_chunks = self._create_content_aware_chunks(content, optimal_chunk_size)\n-        \n+\n         for chunk in content_chunks:\n             segment = self._create_enhanced_segment(\n-                chunk['content'],\n-                chunk['title'],\n-                chunk['start_pos'],\n-                chunk['end_pos'],\n-                chunk['importance_score'],\n-                chunk['content_type']\n+                chunk[\"content\"],\n+                chunk[\"title\"],\n+                chunk[\"start_pos\"],\n+                chunk[\"end_pos\"],\n+                chunk[\"importance_score\"],\n+                chunk[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_academic_paper(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment academic paper using semantic understanding\"\"\"\n         # First try header-based segmentation\n-        headers = re.findall(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE)\n+        headers = re.findall(r\"^(#{1,6})\\s+(.+)$\", content, re.MULTILINE)\n         if len(headers) >= 2:\n             return self._segment_by_headers(content)\n-        \n+\n         # Fallback to semantic detection of academic sections\n         sections = self._detect_academic_sections(content)\n         segments = []\n-        \n+\n         for section in sections:\n             # Determine importance based on section type\n-            section_type = section.get('type', 'general')\n-            content_type = section_type if section_type in ['algorithm', 'formula', 'introduction', 'conclusion'] else 'general'\n+            section_type = section.get(\"type\", \"general\")\n+            content_type = (\n+                section_type\n+                if section_type\n+                in [\"algorithm\", \"formula\", \"introduction\", \"conclusion\"]\n+                else \"general\"\n+            )\n             importance_score = {\n-                'algorithm': 0.95,\n-                'formula': 0.9,\n-                'introduction': 0.85,\n-                'conclusion': 0.8\n+                \"algorithm\": 0.95,\n+                \"formula\": 0.9,\n+                \"introduction\": 0.85,\n+                \"conclusion\": 0.8,\n             }.get(content_type, 0.7)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                section['content'], \n-                section['title'], \n-                section['start_pos'], \n-                section['end_pos'],\n+                section[\"content\"],\n+                section[\"title\"],\n+                section[\"start_pos\"],\n+                section[\"end_pos\"],\n                 importance_score,\n-                content_type\n+                content_type,\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _detect_academic_sections(self, content: str) -> List[Dict]:\n         \"\"\"Detect academic paper sections even without clear headers\"\"\"\n         sections = []\n-        \n+\n         # Common academic section patterns\n         section_patterns = [\n-            (r'(?i)(abstract|\u6458\u8981)', 'introduction'),\n-            (r'(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)', 'introduction'),\n-            (r'(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)', 'background'),\n-            (r'(?i)(method|methodology|approach|\u65b9\u6cd5)', 'methodology'),\n-            (r'(?i)(algorithm|\u7b97\u6cd5)', 'algorithm'),\n-            (r'(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)', 'experiment'),\n-            (r'(?i)(result|\u7ed3\u679c|finding)', 'results'),\n-            (r'(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)', 'conclusion'),\n-            (r'(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)', 'references')\n+            (r\"(?i)(abstract|\u6458\u8981)\", \"introduction\"),\n+            (r\"(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)\", \"introduction\"),\n+            (r\"(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)\", \"background\"),\n+            (r\"(?i)(method|methodology|approach|\u65b9\u6cd5)\", \"methodology\"),\n+            (r\"(?i)(algorithm|\u7b97\u6cd5)\", \"algorithm\"),\n+            (r\"(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)\", \"experiment\"),\n+            (r\"(?i)(result|\u7ed3\u679c|finding)\", \"results\"),\n+            (r\"(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)\", \"conclusion\"),\n+            (r\"(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)\", \"references\"),\n         ]\n-        \n+\n         current_pos = 0\n         for i, (pattern, section_type) in enumerate(section_patterns):\n             match = re.search(pattern, content[current_pos:], re.IGNORECASE)\n             if match:\n                 start_pos = current_pos + match.start()\n-                \n+\n                 # Find end position (next section or end of document)\n                 next_pos = len(content)\n-                for next_pattern, _ in section_patterns[i+1:]:\n-                    next_match = re.search(next_pattern, content[start_pos+100:], re.IGNORECASE)\n+                for next_pattern, _ in section_patterns[i + 1 :]:\n+                    next_match = re.search(\n+                        next_pattern, content[start_pos + 100 :], re.IGNORECASE\n+                    )\n                     if next_match:\n                         next_pos = start_pos + 100 + next_match.start()\n                         break\n-                \n+\n                 section_content = content[start_pos:next_pos].strip()\n                 if len(section_content) > 50:  # Minimum content length\n                     # Calculate importance score and content type\n-                    importance_score = self._calculate_paragraph_importance(section_content, section_type)\n-                    content_type = self._classify_content_type(match.group(1), section_content)\n-                    \n-                    sections.append({\n-                        'title': match.group(1),\n-                        'content': section_content,\n-                        'start_pos': start_pos,\n-                        'end_pos': next_pos,\n-                        'type': section_type,\n-                        'importance_score': importance_score,\n-                        'content_type': content_type\n-                    })\n-                \n+                    importance_score = self._calculate_paragraph_importance(\n+                        section_content, section_type\n+                    )\n+                    content_type = self._classify_content_type(\n+                        match.group(1), section_content\n+                    )\n+\n+                    sections.append(\n+                        {\n+                            \"title\": match.group(1),\n+                            \"content\": section_content,\n+                            \"start_pos\": start_pos,\n+                            \"end_pos\": next_pos,\n+                            \"type\": section_type,\n+                            \"importance_score\": importance_score,\n+                            \"content_type\": content_type,\n+                        }\n+                    )\n+\n                 current_pos = next_pos\n-        \n+\n         return sections\n-    \n+\n     def _segment_by_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment long documents into semantic chunks\"\"\"\n         # Split into paragraphs first\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n-        \n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n+\n         segments = []\n         current_chunk = []\n         current_chunk_size = 0\n         chunk_size_limit = 3000  # characters\n         overlap_size = 200\n-        \n+\n         char_pos = 0\n-        \n+\n         for para in paragraphs:\n             para_size = len(para)\n-            \n+\n             # If adding this paragraph exceeds limit, create a segment\n             if current_chunk_size + para_size > chunk_size_limit and current_chunk:\n-                chunk_content = '\\n\\n'.join(current_chunk)\n+                chunk_content = \"\\n\\n\".join(current_chunk)\n                 # Analyze semantic chunk content type\n                 content_type = self._classify_paragraph_type(chunk_content)\n-                importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n-                \n+                importance_score = self._calculate_paragraph_importance(\n+                    chunk_content, content_type\n+                )\n+\n                 segment = self._create_enhanced_segment(\n-                    chunk_content, \n+                    chunk_content,\n                     f\"Section {len(segments) + 1}\",\n-                    char_pos - len(chunk_content.encode('utf-8')),\n+                    char_pos - len(chunk_content.encode(\"utf-8\")),\n                     char_pos,\n                     importance_score,\n-                    content_type\n+                    content_type,\n                 )\n                 segments.append(segment)\n-                \n+\n                 # Keep last part for overlap\n-                overlap_content = chunk_content[-overlap_size:] if len(chunk_content) > overlap_size else \"\"\n+                overlap_content = (\n+                    chunk_content[-overlap_size:]\n+                    if len(chunk_content) > overlap_size\n+                    else \"\"\n+                )\n                 current_chunk = [overlap_content, para] if overlap_content else [para]\n                 current_chunk_size = len(overlap_content) + para_size\n             else:\n                 current_chunk.append(para)\n                 current_chunk_size += para_size\n-            \n+\n             char_pos += para_size + 2  # +2 for \\n\\n\n-        \n+\n         # Add final chunk\n         if current_chunk:\n-            chunk_content = '\\n\\n'.join(current_chunk)\n+            chunk_content = \"\\n\\n\".join(current_chunk)\n             # Analyze final chunk content type\n             content_type = self._classify_paragraph_type(chunk_content)\n-            importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n-            \n+            importance_score = self._calculate_paragraph_importance(\n+                chunk_content, content_type\n+            )\n+\n             segment = self._create_enhanced_segment(\n                 chunk_content,\n                 f\"Section {len(segments) + 1}\",\n-                char_pos - len(chunk_content.encode('utf-8')),\n+                char_pos - len(chunk_content.encode(\"utf-8\")),\n                 char_pos,\n                 importance_score,\n-                content_type\n+                content_type,\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_by_paragraphs(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Simple paragraph-based segmentation for short documents\"\"\"\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n         segments = []\n         char_pos = 0\n-        \n+\n         for i, para in enumerate(paragraphs):\n             if len(para) > 100:  # Only include substantial paragraphs\n                 # Analyze paragraph type and importance\n                 content_type = self._classify_paragraph_type(para)\n-                importance_score = self._calculate_paragraph_importance(para, content_type)\n-                \n+                importance_score = self._calculate_paragraph_importance(\n+                    para, content_type\n+                )\n+\n                 segment = self._create_enhanced_segment(\n                     para,\n                     f\"Paragraph {i + 1}\",\n                     char_pos,\n-                    char_pos + len(para.encode('utf-8')),\n+                    char_pos + len(para.encode(\"utf-8\")),\n                     importance_score,\n-                    content_type\n+                    content_type,\n                 )\n                 segments.append(segment)\n-            char_pos += len(para.encode('utf-8')) + 2\n-        \n+            char_pos += len(para.encode(\"utf-8\")) + 2\n+\n         return segments\n-    \n+\n     # =============== Enhanced intelligent segmentation helper methods ===============\n-    \n+\n     def _identify_algorithm_blocks(self, content: str) -> List[Dict]:\n         \"\"\"Identify algorithm blocks and related descriptions\"\"\"\n         algorithm_blocks = []\n-        \n+\n         # Algorithm block identification patterns\n         algorithm_patterns = [\n-            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)',\n-            r'(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)',\n-            r'(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)',\n-            r'(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)'\n+            r\"(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)\",\n+            r\"(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)\",\n+            r\"(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)\",\n+            r\"(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)\",\n         ]\n-        \n+\n         for pattern in algorithm_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 # Expand context to include complete descriptions\n                 start = max(0, match.start() - 300)\n                 end = min(len(content), match.end() + 500)\n-                \n+\n                 # Find natural boundaries\n-                while start > 0 and content[start] not in '\\n.!?':\n+                while start > 0 and content[start] not in \"\\n.!?\":\n                     start -= 1\n-                while end < len(content) and content[end] not in '\\n.!?':\n+                while end < len(content) and content[end] not in \"\\n.!?\":\n                     end += 1\n-                \n-                algorithm_blocks.append({\n-                    'start_pos': start,\n-                    'end_pos': end,\n-                    'content': content[start:end].strip(),\n-                    'title': self._extract_algorithm_title(content[match.start():match.end()]),\n-                    'importance_score': 0.95,  # High importance for algorithm blocks\n-                    'content_type': 'algorithm'\n-                })\n-        \n+\n+                algorithm_blocks.append(\n+                    {\n+                        \"start_pos\": start,\n+                        \"end_pos\": end,\n+                        \"content\": content[start:end].strip(),\n+                        \"title\": self._extract_algorithm_title(\n+                            content[match.start() : match.end()]\n+                        ),\n+                        \"importance_score\": 0.95,  # High importance for algorithm blocks\n+                        \"content_type\": \"algorithm\",\n+                    }\n+                )\n+\n         return algorithm_blocks\n-    \n+\n     def _identify_concept_groups(self, content: str) -> List[Dict]:\n         \"\"\"Identify concept definition groups\"\"\"\n         concept_groups = []\n-        \n+\n         # Concept definition patterns\n         concept_patterns = [\n-            r'(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)',\n-            r'(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)',\n-            r'(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)'\n+            r\"(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)\",\n+            r\"(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)\",\n+            r\"(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)\",\n         ]\n-        \n+\n         for pattern in concept_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 # Expand context\n                 start = max(0, match.start() - 200)\n                 end = min(len(content), match.end() + 300)\n-                \n-                concept_groups.append({\n-                    'start_pos': start,\n-                    'end_pos': end,\n-                    'content': content[start:end].strip(),\n-                    'title': self._extract_concept_title(content[match.start():match.end()]),\n-                    'importance_score': 0.85,\n-                    'content_type': 'concept'\n-                })\n-        \n+\n+                concept_groups.append(\n+                    {\n+                        \"start_pos\": start,\n+                        \"end_pos\": end,\n+                        \"content\": content[start:end].strip(),\n+                        \"title\": self._extract_concept_title(\n+                            content[match.start() : match.end()]\n+                        ),\n+                        \"importance_score\": 0.85,\n+                        \"content_type\": \"concept\",\n+                    }\n+                )\n+\n         return concept_groups\n-    \n+\n     def _identify_formula_chains(self, content: str) -> List[Dict]:\n         \"\"\"Identify formula derivation chains\"\"\"\n         formula_chains = []\n-        \n+\n         # Formula patterns\n         formula_patterns = [\n-            r'\\$\\$.*?\\$\\$',  # Block-level mathematical formulas\n-            r'\\$[^$]+\\$',    # Inline mathematical formulas\n-            r'(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)',\n-            r'(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)'\n+            r\"\\$\\$.*?\\$\\$\",  # Block-level mathematical formulas\n+            r\"\\$[^$]+\\$\",  # Inline mathematical formulas\n+            r\"(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)\",\n+            r\"(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)\",\n         ]\n-        \n+\n         # Find dense formula regions\n         formula_positions = []\n         for pattern in formula_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 formula_positions.append((match.start(), match.end()))\n-        \n+\n         # Merge nearby formulas into formula chains\n         formula_positions.sort()\n         if formula_positions:\n             current_chain_start = formula_positions[0][0]\n             current_chain_end = formula_positions[0][1]\n-            \n+\n             for start, end in formula_positions[1:]:\n-                if start - current_chain_end < 500:  # Merge formulas within 500 characters\n+                if (\n+                    start - current_chain_end < 500\n+                ):  # Merge formulas within 500 characters\n                     current_chain_end = end\n                 else:\n                     # Save current chain\n-                    formula_chains.append({\n-                        'start_pos': max(0, current_chain_start - 200),\n-                        'end_pos': min(len(content), current_chain_end + 200),\n-                        'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n-                        'title': 'Mathematical Formulation',\n-                        'importance_score': 0.9,\n-                        'content_type': 'formula'\n-                    })\n+                    formula_chains.append(\n+                        {\n+                            \"start_pos\": max(0, current_chain_start - 200),\n+                            \"end_pos\": min(len(content), current_chain_end + 200),\n+                            \"content\": content[\n+                                max(0, current_chain_start - 200) : min(\n+                                    len(content), current_chain_end + 200\n+                                )\n+                            ].strip(),\n+                            \"title\": \"Mathematical Formulation\",\n+                            \"importance_score\": 0.9,\n+                            \"content_type\": \"formula\",\n+                        }\n+                    )\n                     current_chain_start = start\n                     current_chain_end = end\n-            \n+\n             # Add the last chain\n-            formula_chains.append({\n-                'start_pos': max(0, current_chain_start - 200),\n-                'end_pos': min(len(content), current_chain_end + 200),\n-                'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n-                'title': 'Mathematical Formulation',\n-                'importance_score': 0.9,\n-                'content_type': 'formula'\n-            })\n-        \n+            formula_chains.append(\n+                {\n+                    \"start_pos\": max(0, current_chain_start - 200),\n+                    \"end_pos\": min(len(content), current_chain_end + 200),\n+                    \"content\": content[\n+                        max(0, current_chain_start - 200) : min(\n+                            len(content), current_chain_end + 200\n+                        )\n+                    ].strip(),\n+                    \"title\": \"Mathematical Formulation\",\n+                    \"importance_score\": 0.9,\n+                    \"content_type\": \"formula\",\n+                }\n+            )\n+\n         return formula_chains\n-    \n-    def _merge_related_content_blocks(self, algorithm_blocks: List[Dict], concept_groups: List[Dict], \n-                                    formula_chains: List[Dict], content: str) -> List[Dict]:\n+\n+    def _merge_related_content_blocks(\n+        self,\n+        algorithm_blocks: List[Dict],\n+        concept_groups: List[Dict],\n+        formula_chains: List[Dict],\n+        content: str,\n+    ) -> List[Dict]:\n         \"\"\"Merge related content blocks to ensure integrity\"\"\"\n         all_blocks = algorithm_blocks + concept_groups + formula_chains\n-        all_blocks.sort(key=lambda x: x['start_pos'])\n-        \n+        all_blocks.sort(key=lambda x: x[\"start_pos\"])\n+\n         merged_blocks = []\n         i = 0\n-        \n+\n         while i < len(all_blocks):\n             current_block = all_blocks[i]\n-            \n+\n             # Check if can merge with the next block\n             while i + 1 < len(all_blocks):\n                 next_block = all_blocks[i + 1]\n-                \n+\n                 # If blocks are close or content related, merge them\n-                if (next_block['start_pos'] - current_block['end_pos'] < 300 or\n-                    self._are_blocks_related(current_block, next_block)):\n-                    \n+                if next_block[\"start_pos\"] - current_block[\n+                    \"end_pos\"\n+                ] < 300 or self._are_blocks_related(current_block, next_block):\n                     # Merge blocks\n-                    merged_content = content[current_block['start_pos']:next_block['end_pos']]\n+                    merged_content = content[\n+                        current_block[\"start_pos\"] : next_block[\"end_pos\"]\n+                    ]\n                     current_block = {\n-                        'start_pos': current_block['start_pos'],\n-                        'end_pos': next_block['end_pos'],\n-                        'content': merged_content.strip(),\n-                        'title': f\"{current_block['title']} & {next_block['title']}\",\n-                        'importance_score': max(current_block['importance_score'], next_block['importance_score']),\n-                        'content_type': 'merged'\n+                        \"start_pos\": current_block[\"start_pos\"],\n+                        \"end_pos\": next_block[\"end_pos\"],\n+                        \"content\": merged_content.strip(),\n+                        \"title\": f\"{current_block['title']} & {next_block['title']}\",\n+                        \"importance_score\": max(\n+                            current_block[\"importance_score\"],\n+                            next_block[\"importance_score\"],\n+                        ),\n+                        \"content_type\": \"merged\",\n                     }\n                     i += 1\n                 else:\n                     break\n-            \n+\n             merged_blocks.append(current_block)\n             i += 1\n-        \n+\n         return merged_blocks\n-    \n+\n     def _are_blocks_related(self, block1: Dict, block2: Dict) -> bool:\n         \"\"\"Determine if two content blocks are related\"\"\"\n         # Check content type associations\n         related_types = [\n-            ('algorithm', 'formula'),\n-            ('concept', 'algorithm'),\n-            ('formula', 'concept')\n+            (\"algorithm\", \"formula\"),\n+            (\"concept\", \"algorithm\"),\n+            (\"formula\", \"concept\"),\n         ]\n-        \n+\n         for type1, type2 in related_types:\n-            if ((block1['content_type'] == type1 and block2['content_type'] == type2) or\n-                (block1['content_type'] == type2 and block2['content_type'] == type1)):\n+            if (\n+                block1[\"content_type\"] == type1 and block2[\"content_type\"] == type2\n+            ) or (block1[\"content_type\"] == type2 and block2[\"content_type\"] == type1):\n                 return True\n-        \n+\n         return False\n-    \n+\n     def _extract_algorithm_title(self, text: str) -> str:\n         \"\"\"Extract title from algorithm text\"\"\"\n-        lines = text.split('\\n')[:3]  # First 3 lines\n+        lines = text.split(\"\\n\")[:3]  # First 3 lines\n         for line in lines:\n             line = line.strip()\n             if line and len(line) < 100:  # Reasonable title length\n                 # Clean title\n-                title = re.sub(r'[^\\w\\s-]', '', line)\n+                title = re.sub(r\"[^\\w\\s-]\", \"\", line)\n                 if title:\n                     return title[:50]  # Limit title length\n         return \"Algorithm Block\"\n-    \n+\n     def _extract_concept_title(self, text: str) -> str:\n         \"\"\"Extract title from concept text\"\"\"\n-        lines = text.split('\\n')[:2]\n+        lines = text.split(\"\\n\")[:2]\n         for line in lines:\n             line = line.strip()\n             if line and len(line) < 80:\n-                title = re.sub(r'[^\\w\\s-]', '', line)\n+                title = re.sub(r\"[^\\w\\s-]\", \"\", line)\n                 if title:\n                     return title[:50]\n         return \"Concept Definition\"\n-    \n-    def _create_enhanced_segment(self, content: str, title: str, start_pos: int, end_pos: int,\n-                               importance_score: float, content_type: str) -> DocumentSegment:\n+\n+    def _create_enhanced_segment(\n+        self,\n+        content: str,\n+        title: str,\n+        start_pos: int,\n+        end_pos: int,\n+        importance_score: float,\n+        content_type: str,\n+    ) -> DocumentSegment:\n         \"\"\"Create enhanced document segment\"\"\"\n         # Generate unique ID\n-        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()).hexdigest()[:8]\n-        \n+        segment_id = hashlib.md5(\n+            f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()\n+        ).hexdigest()[:8]\n+\n         # Extract keywords\n         keywords = self._extract_enhanced_keywords(content, content_type)\n-        \n+\n         # Calculate enhanced relevance scores\n-        relevance_scores = self._calculate_enhanced_relevance_scores(content, content_type, importance_score)\n-        \n+        relevance_scores = self._calculate_enhanced_relevance_scores(\n+            content, content_type, importance_score\n+        )\n+\n         return DocumentSegment(\n             id=segment_id,\n             title=title,\n@@ -895,150 +1014,199 @@ class DocumentSegmenter:\n             char_end=end_pos,\n             char_count=len(content),\n             relevance_scores=relevance_scores,\n-            section_path=title\n+            section_path=title,\n         )\n-    \n+\n     def _extract_enhanced_keywords(self, content: str, content_type: str) -> List[str]:\n         \"\"\"Extract enhanced keywords based on content type\"\"\"\n-        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n-        \n+        words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", content.lower())\n+\n         # Adjust stopwords based on content type\n-        if content_type == 'algorithm':\n-            algorithm_stopwords = {'step', 'then', 'else', 'end', 'begin', 'start', 'stop'}\n+        if content_type == \"algorithm\":\n+            algorithm_stopwords = {\n+                \"step\",\n+                \"then\",\n+                \"else\",\n+                \"end\",\n+                \"begin\",\n+                \"start\",\n+                \"stop\",\n+            }\n             words = [w for w in words if w not in algorithm_stopwords]\n-        elif content_type == 'formula':\n-            formula_keywords = ['equation', 'formula', 'where', 'given', 'such', 'that']\n+        elif content_type == \"formula\":\n+            formula_keywords = [\"equation\", \"formula\", \"where\", \"given\", \"such\", \"that\"]\n             words.extend(formula_keywords)\n-        \n+\n         # General stopwords\n-        general_stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n-        \n+        general_stopwords = {\n+            \"the\",\n+            \"and\",\n+            \"for\",\n+            \"are\",\n+            \"but\",\n+            \"not\",\n+            \"you\",\n+            \"all\",\n+            \"can\",\n+            \"her\",\n+            \"was\",\n+            \"one\",\n+            \"our\",\n+            \"had\",\n+            \"but\",\n+            \"have\",\n+            \"this\",\n+            \"that\",\n+            \"with\",\n+            \"from\",\n+            \"they\",\n+            \"she\",\n+            \"been\",\n+            \"were\",\n+            \"said\",\n+            \"each\",\n+            \"which\",\n+            \"their\",\n+        }\n+\n         keywords = [w for w in set(words) if w not in general_stopwords and len(w) > 3]\n         return keywords[:25]  # Increase keyword count\n-    \n-    def _calculate_enhanced_relevance_scores(self, content: str, content_type: str, importance_score: float) -> Dict[str, float]:\n+\n+    def _calculate_enhanced_relevance_scores(\n+        self, content: str, content_type: str, importance_score: float\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate enhanced relevance scores\"\"\"\n         content_lower = content.lower()\n-        \n+\n         base_scores = {\n-            'concept_analysis': 0.5,\n-            'algorithm_extraction': 0.5,\n-            'code_planning': 0.5\n+            \"concept_analysis\": 0.5,\n+            \"algorithm_extraction\": 0.5,\n+            \"code_planning\": 0.5,\n         }\n-        \n+\n         # Adjust base scores based on content type and importance\n-        if content_type == 'algorithm':\n-            base_scores['algorithm_extraction'] = importance_score\n-            base_scores['code_planning'] = importance_score * 0.9\n-            base_scores['concept_analysis'] = importance_score * 0.7\n-        elif content_type == 'concept':\n-            base_scores['concept_analysis'] = importance_score\n-            base_scores['algorithm_extraction'] = importance_score * 0.8\n-            base_scores['code_planning'] = importance_score * 0.6\n-        elif content_type == 'formula':\n-            base_scores['algorithm_extraction'] = importance_score\n-            base_scores['concept_analysis'] = importance_score * 0.8\n-            base_scores['code_planning'] = importance_score * 0.9\n-        elif content_type == 'merged':\n+        if content_type == \"algorithm\":\n+            base_scores[\"algorithm_extraction\"] = importance_score\n+            base_scores[\"code_planning\"] = importance_score * 0.9\n+            base_scores[\"concept_analysis\"] = importance_score * 0.7\n+        elif content_type == \"concept\":\n+            base_scores[\"concept_analysis\"] = importance_score\n+            base_scores[\"algorithm_extraction\"] = importance_score * 0.8\n+            base_scores[\"code_planning\"] = importance_score * 0.6\n+        elif content_type == \"formula\":\n+            base_scores[\"algorithm_extraction\"] = importance_score\n+            base_scores[\"concept_analysis\"] = importance_score * 0.8\n+            base_scores[\"code_planning\"] = importance_score * 0.9\n+        elif content_type == \"merged\":\n             # Merged content is usually important\n             base_scores = {k: importance_score * 0.95 for k in base_scores}\n-        \n+\n         # Additional bonus based on content density\n-        algorithm_indicators = ['algorithm', 'method', 'procedure', 'step', 'process']\n-        concept_indicators = ['definition', 'concept', 'framework', 'approach']\n-        implementation_indicators = ['implementation', 'code', 'function', 'design']\n-        \n+        algorithm_indicators = [\"algorithm\", \"method\", \"procedure\", \"step\", \"process\"]\n+        concept_indicators = [\"definition\", \"concept\", \"framework\", \"approach\"]\n+        implementation_indicators = [\"implementation\", \"code\", \"function\", \"design\"]\n+\n         for query_type, indicators in [\n-            ('algorithm_extraction', algorithm_indicators),\n-            ('concept_analysis', concept_indicators),\n-            ('code_planning', implementation_indicators)\n+            (\"algorithm_extraction\", algorithm_indicators),\n+            (\"concept_analysis\", concept_indicators),\n+            (\"code_planning\", implementation_indicators),\n         ]:\n-            density_bonus = sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            density_bonus = (\n+                sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            )\n             base_scores[query_type] = min(1.0, base_scores[query_type] + density_bonus)\n-        \n+\n         return base_scores\n-    \n+\n     # Placeholder methods - can be further implemented later\n     def _identify_research_paper_sections(self, content: str) -> List[Dict]:\n         \"\"\"Identify research paper sections - simplified implementation\"\"\"\n         # Temporarily use improved semantic detection\n         return self._detect_academic_sections(content)\n-    \n+\n     def _enhance_section_with_context(self, section: Dict, content: str) -> Dict:\n         \"\"\"Add context to sections - simplified implementation\"\"\"\n         return section\n-    \n+\n     def _identify_concept_implementation_pairs(self, content: str) -> List[Dict]:\n         \"\"\"Identify concept-implementation pairs - simplified implementation\"\"\"\n         return []\n-    \n+\n     def _merge_concept_with_implementation(self, pair: Dict, content: str) -> Dict:\n         \"\"\"Merge concepts with implementation - simplified implementation\"\"\"\n         return pair\n-    \n+\n     def _detect_semantic_boundaries(self, content: str) -> List[Dict]:\n         \"\"\"Detect semantic boundaries - based on paragraphs and logical separators\"\"\"\n         boundaries = []\n-        \n+\n         # Split paragraphs by double line breaks\n-        paragraphs = content.split('\\n\\n')\n+        paragraphs = content.split(\"\\n\\n\")\n         current_pos = 0\n-        \n+\n         for i, para in enumerate(paragraphs):\n             if len(para.strip()) > 100:  # Valid paragraph\n                 # Analyze paragraph type\n                 content_type = self._classify_paragraph_type(para)\n-                importance_score = self._calculate_paragraph_importance(para, content_type)\n-                \n-                boundaries.append({\n-                    'position': current_pos + len(para),\n-                    'suggested_title': self._extract_paragraph_title(para, i+1),\n-                    'importance_score': importance_score,\n-                    'content_type': content_type\n-                })\n-            \n+                importance_score = self._calculate_paragraph_importance(\n+                    para, content_type\n+                )\n+\n+                boundaries.append(\n+                    {\n+                        \"position\": current_pos + len(para),\n+                        \"suggested_title\": self._extract_paragraph_title(para, i + 1),\n+                        \"importance_score\": importance_score,\n+                        \"content_type\": content_type,\n+                    }\n+                )\n+\n             current_pos += len(para) + 2  # +2 for \\n\\n\n-        \n+\n         return boundaries\n-    \n+\n     def _classify_paragraph_type(self, paragraph: str) -> str:\n         \"\"\"Classify paragraph type\"\"\"\n         para_lower = paragraph.lower()\n-        \n-        if 'algorithm' in para_lower or 'procedure' in para_lower:\n-            return 'algorithm'\n-        elif 'formula' in para_lower or '$$' in paragraph:\n-            return 'formula'\n-        elif any(word in para_lower for word in ['introduction', 'overview', 'abstract']):\n-            return 'introduction'\n-        elif any(word in para_lower for word in ['conclusion', 'summary', 'result']):\n-            return 'conclusion'\n+\n+        if \"algorithm\" in para_lower or \"procedure\" in para_lower:\n+            return \"algorithm\"\n+        elif \"formula\" in para_lower or \"$$\" in paragraph:\n+            return \"formula\"\n+        elif any(\n+            word in para_lower for word in [\"introduction\", \"overview\", \"abstract\"]\n+        ):\n+            return \"introduction\"\n+        elif any(word in para_lower for word in [\"conclusion\", \"summary\", \"result\"]):\n+            return \"conclusion\"\n         else:\n-            return 'general'\n-    \n-    def _calculate_paragraph_importance(self, paragraph: str, content_type: str) -> float:\n+            return \"general\"\n+\n+    def _calculate_paragraph_importance(\n+        self, paragraph: str, content_type: str\n+    ) -> float:\n         \"\"\"Calculate paragraph importance\"\"\"\n-        if content_type == 'algorithm':\n+        if content_type == \"algorithm\":\n             return 0.95\n-        elif content_type == 'formula':\n+        elif content_type == \"formula\":\n             return 0.9\n-        elif content_type == 'introduction':\n+        elif content_type == \"introduction\":\n             return 0.85\n-        elif content_type == 'conclusion':\n+        elif content_type == \"conclusion\":\n             return 0.8\n         else:\n             return 0.7\n-    \n+\n     def _extract_paragraph_title(self, paragraph: str, index: int) -> str:\n         \"\"\"Extract paragraph title\"\"\"\n-        lines = paragraph.split('\\n')\n+        lines = paragraph.split(\"\\n\")\n         for line in lines[:2]:\n-            if line.startswith('#'):\n-                return line.strip('# ')\n+            if line.startswith(\"#\"):\n+                return line.strip(\"# \")\n             elif len(line) < 80 and line.strip():\n                 return line.strip()\n         return f\"Section {index}\"\n-    \n+\n     def _calculate_optimal_chunk_size(self, content: str) -> int:\n         \"\"\"Calculate optimal chunk size\"\"\"\n         # Dynamically adjust based on content complexity\n@@ -1049,65 +1217,73 @@ class DocumentSegmenter:\n             return 3000\n         else:\n             return 2000\n-    \n+\n     def _create_content_aware_chunks(self, content: str, chunk_size: int) -> List[Dict]:\n         \"\"\"Create content-aware chunks - simplified implementation\"\"\"\n         chunks = []\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n-        \n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n+\n         current_chunk = []\n         current_size = 0\n         start_pos = 0\n-        \n+\n         for para in paragraphs:\n             para_size = len(para)\n-            \n+\n             if current_size + para_size > chunk_size and current_chunk:\n-                chunk_content = '\\n\\n'.join(current_chunk)\n-                chunks.append({\n-                    'content': chunk_content,\n-                    'title': f\"Section {len(chunks) + 1}\",\n-                    'start_pos': start_pos,\n-                    'end_pos': start_pos + len(chunk_content),\n-                    'importance_score': 0.7,\n-                    'content_type': 'general'\n-                })\n-                \n+                chunk_content = \"\\n\\n\".join(current_chunk)\n+                chunks.append(\n+                    {\n+                        \"content\": chunk_content,\n+                        \"title\": f\"Section {len(chunks) + 1}\",\n+                        \"start_pos\": start_pos,\n+                        \"end_pos\": start_pos + len(chunk_content),\n+                        \"importance_score\": 0.7,\n+                        \"content_type\": \"general\",\n+                    }\n+                )\n+\n                 current_chunk = [para]\n                 current_size = para_size\n                 start_pos += len(chunk_content) + 2\n             else:\n                 current_chunk.append(para)\n                 current_size += para_size\n-        \n+\n         # Add the last chunk\n         if current_chunk:\n-            chunk_content = '\\n\\n'.join(current_chunk)\n-            chunks.append({\n-                'content': chunk_content,\n-                'title': f\"Section {len(chunks) + 1}\",\n-                'start_pos': start_pos,\n-                'end_pos': start_pos + len(chunk_content),\n-                'importance_score': 0.7,\n-                'content_type': 'general'\n-            })\n-        \n+            chunk_content = \"\\n\\n\".join(current_chunk)\n+            chunks.append(\n+                {\n+                    \"content\": chunk_content,\n+                    \"title\": f\"Section {len(chunks) + 1}\",\n+                    \"start_pos\": start_pos,\n+                    \"end_pos\": start_pos + len(chunk_content),\n+                    \"importance_score\": 0.7,\n+                    \"content_type\": \"general\",\n+                }\n+            )\n+\n         return chunks\n-    \n-    def _create_segment(self, content: str, title: str, start_pos: int, end_pos: int) -> DocumentSegment:\n+\n+    def _create_segment(\n+        self, content: str, title: str, start_pos: int, end_pos: int\n+    ) -> DocumentSegment:\n         \"\"\"Create a DocumentSegment with metadata\"\"\"\n         # Generate unique ID\n-        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[:8]\n-        \n+        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[\n+            :8\n+        ]\n+\n         # Extract keywords from content\n         keywords = self._extract_keywords(content)\n-        \n+\n         # Determine content type\n         content_type = self._classify_content_type(title, content)\n-        \n+\n         # Calculate relevance scores for different query types\n         relevance_scores = self._calculate_relevance_scores(content, content_type)\n-        \n+\n         return DocumentSegment(\n             id=segment_id,\n             title=title,\n@@ -1118,142 +1294,232 @@ class DocumentSegmenter:\n             char_end=end_pos,\n             char_count=len(content),\n             relevance_scores=relevance_scores,\n-            section_path=title  # Simplified for now\n+            section_path=title,  # Simplified for now\n         )\n-    \n+\n     def _extract_keywords(self, content: str) -> List[str]:\n         \"\"\"Extract relevant keywords from content\"\"\"\n         # Simple keyword extraction - could be enhanced with NLP\n-        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n-        \n+        words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", content.lower())\n+\n         # Remove common words\n-        stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n-        \n+        stopwords = {\n+            \"the\",\n+            \"and\",\n+            \"for\",\n+            \"are\",\n+            \"but\",\n+            \"not\",\n+            \"you\",\n+            \"all\",\n+            \"can\",\n+            \"her\",\n+            \"was\",\n+            \"one\",\n+            \"our\",\n+            \"had\",\n+            \"but\",\n+            \"have\",\n+            \"this\",\n+            \"that\",\n+            \"with\",\n+            \"from\",\n+            \"they\",\n+            \"she\",\n+            \"been\",\n+            \"were\",\n+            \"said\",\n+            \"each\",\n+            \"which\",\n+            \"their\",\n+        }\n+\n         keywords = [w for w in set(words) if w not in stopwords and len(w) > 3]\n         return keywords[:20]  # Top 20 keywords\n-    \n+\n     def _classify_content_type(self, title: str, content: str) -> str:\n         \"\"\"Classify the type of content based on title and content\"\"\"\n         title_lower = title.lower()\n         content_lower = content.lower()\n-        \n-        if any(word in title_lower for word in ['introduction', 'abstract', 'overview']):\n-            return 'introduction'\n-        elif any(word in title_lower for word in ['method', 'approach', 'algorithm']):\n-            return 'methodology'\n-        elif any(word in title_lower for word in ['experiment', 'evaluation', 'result']):\n-            return 'experiment'\n-        elif any(word in title_lower for word in ['conclusion', 'discussion', 'summary']):\n-            return 'conclusion'\n-        elif any(word in title_lower for word in ['reference', 'bibliography']):\n-            return 'references'\n-        elif 'algorithm' in content_lower or 'procedure' in content_lower:\n-            return 'algorithm'\n+\n+        if any(\n+            word in title_lower for word in [\"introduction\", \"abstract\", \"overview\"]\n+        ):\n+            return \"introduction\"\n+        elif any(word in title_lower for word in [\"method\", \"approach\", \"algorithm\"]):\n+            return \"methodology\"\n+        elif any(\n+            word in title_lower for word in [\"experiment\", \"evaluation\", \"result\"]\n+        ):\n+            return \"experiment\"\n+        elif any(\n+            word in title_lower for word in [\"conclusion\", \"discussion\", \"summary\"]\n+        ):\n+            return \"conclusion\"\n+        elif any(word in title_lower for word in [\"reference\", \"bibliography\"]):\n+            return \"references\"\n+        elif \"algorithm\" in content_lower or \"procedure\" in content_lower:\n+            return \"algorithm\"\n         else:\n-            return 'general'\n-    \n-    def _calculate_relevance_scores(self, content: str, content_type: str) -> Dict[str, float]:\n+            return \"general\"\n+\n+    def _calculate_relevance_scores(\n+        self, content: str, content_type: str\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate relevance scores for different query types\"\"\"\n         content_lower = content.lower()\n-        \n+\n         scores = {\n-            'concept_analysis': 0.5,\n-            'algorithm_extraction': 0.5,\n-            'code_planning': 0.5\n+            \"concept_analysis\": 0.5,\n+            \"algorithm_extraction\": 0.5,\n+            \"code_planning\": 0.5,\n         }\n-        \n+\n         # Concept analysis relevance\n-        concept_indicators = ['introduction', 'overview', 'architecture', 'system', 'framework', 'concept', 'approach']\n-        concept_score = sum(1 for indicator in concept_indicators if indicator in content_lower) / len(concept_indicators)\n-        scores['concept_analysis'] = min(1.0, concept_score + (0.8 if content_type == 'introduction' else 0))\n-        \n+        concept_indicators = [\n+            \"introduction\",\n+            \"overview\",\n+            \"architecture\",\n+            \"system\",\n+            \"framework\",\n+            \"concept\",\n+            \"approach\",\n+        ]\n+        concept_score = sum(\n+            1 for indicator in concept_indicators if indicator in content_lower\n+        ) / len(concept_indicators)\n+        scores[\"concept_analysis\"] = min(\n+            1.0, concept_score + (0.8 if content_type == \"introduction\" else 0)\n+        )\n+\n         # Algorithm extraction relevance\n-        algorithm_indicators = ['algorithm', 'method', 'procedure', 'formula', 'equation', 'step', 'process']\n-        algorithm_score = sum(1 for indicator in algorithm_indicators if indicator in content_lower) / len(algorithm_indicators)\n-        scores['algorithm_extraction'] = min(1.0, algorithm_score + (0.9 if content_type == 'methodology' else 0))\n-        \n+        algorithm_indicators = [\n+            \"algorithm\",\n+            \"method\",\n+            \"procedure\",\n+            \"formula\",\n+            \"equation\",\n+            \"step\",\n+            \"process\",\n+        ]\n+        algorithm_score = sum(\n+            1 for indicator in algorithm_indicators if indicator in content_lower\n+        ) / len(algorithm_indicators)\n+        scores[\"algorithm_extraction\"] = min(\n+            1.0, algorithm_score + (0.9 if content_type == \"methodology\" else 0)\n+        )\n+\n         # Code planning relevance\n-        code_indicators = ['implementation', 'code', 'function', 'class', 'module', 'structure', 'design']\n-        code_score = sum(1 for indicator in code_indicators if indicator in content_lower) / len(code_indicators)\n-        scores['code_planning'] = min(1.0, code_score + (0.7 if content_type in ['methodology', 'algorithm'] else 0))\n-        \n+        code_indicators = [\n+            \"implementation\",\n+            \"code\",\n+            \"function\",\n+            \"class\",\n+            \"module\",\n+            \"structure\",\n+            \"design\",\n+        ]\n+        code_score = sum(\n+            1 for indicator in code_indicators if indicator in content_lower\n+        ) / len(code_indicators)\n+        scores[\"code_planning\"] = min(\n+            1.0,\n+            code_score + (0.7 if content_type in [\"methodology\", \"algorithm\"] else 0),\n+        )\n+\n         return scores\n \n+\n # Global variables\n DOCUMENT_INDEXES: Dict[str, DocumentIndex] = {}\n segmenter = DocumentSegmenter()\n \n+\n def get_segments_dir(paper_dir: str) -> str:\n     \"\"\"Get the segments directory path\"\"\"\n     return os.path.join(paper_dir, \"document_segments\")\n \n+\n def ensure_segments_dir_exists(segments_dir: str):\n     \"\"\"Ensure segments directory exists\"\"\"\n     os.makedirs(segments_dir, exist_ok=True)\n \n+\n @mcp.tool()\n-async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = False) -> str:\n+async def analyze_and_segment_document(\n+    paper_dir: str, force_refresh: bool = False\n+) -> str:\n     \"\"\"\n     Analyze document structure and create intelligent segments\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         force_refresh: Whether to force re-analysis even if segments exist\n-        \n+\n     Returns:\n         JSON string with segmentation results\n     \"\"\"\n     try:\n         # Find markdown file in paper directory\n-        md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+        md_files = [f for f in os.listdir(paper_dir) if f.endswith(\".md\")]\n         if not md_files:\n-            return json.dumps({\n-                \"status\": \"error\",\n-                \"message\": f\"No markdown file found in {paper_dir}\"\n-            }, ensure_ascii=False, indent=2)\n-        \n+            return json.dumps(\n+                {\n+                    \"status\": \"error\",\n+                    \"message\": f\"No markdown file found in {paper_dir}\",\n+                },\n+                ensure_ascii=False,\n+                indent=2,\n+            )\n+\n         md_file_path = os.path.join(paper_dir, md_files[0])\n         segments_dir = get_segments_dir(paper_dir)\n         index_file_path = os.path.join(segments_dir, \"document_index.json\")\n-        \n+\n         # Check if analysis already exists and is recent\n         if not force_refresh and os.path.exists(index_file_path):\n             try:\n-                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n                     existing_index = json.load(f)\n-                    \n+\n                     # Compatibility handling: ensure segments data structure is correct\n-                    if 'segments' in existing_index:\n+                    if \"segments\" in existing_index:\n                         segments_data = []\n-                        for seg_data in existing_index['segments']:\n+                        for seg_data in existing_index[\"segments\"]:\n                             # Ensure all required fields exist\n                             segment_dict = dict(seg_data)\n-                            \n-                            if 'content_type' not in segment_dict:\n-                                segment_dict['content_type'] = 'general'\n-                            if 'keywords' not in segment_dict:\n-                                segment_dict['keywords'] = []\n-                            if 'relevance_scores' not in segment_dict:\n-                                segment_dict['relevance_scores'] = {\n-                                    'concept_analysis': 0.5,\n-                                    'algorithm_extraction': 0.5,\n-                                    'code_planning': 0.5\n+\n+                            if \"content_type\" not in segment_dict:\n+                                segment_dict[\"content_type\"] = \"general\"\n+                            if \"keywords\" not in segment_dict:\n+                                segment_dict[\"keywords\"] = []\n+                            if \"relevance_scores\" not in segment_dict:\n+                                segment_dict[\"relevance_scores\"] = {\n+                                    \"concept_analysis\": 0.5,\n+                                    \"algorithm_extraction\": 0.5,\n+                                    \"code_planning\": 0.5,\n                                 }\n-                            if 'section_path' not in segment_dict:\n-                                segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n-                            \n+                            if \"section_path\" not in segment_dict:\n+                                segment_dict[\"section_path\"] = segment_dict.get(\n+                                    \"title\", \"Unknown\"\n+                                )\n+\n                             segments_data.append(DocumentSegment(**segment_dict))\n-                        \n-                        existing_index['segments'] = segments_data\n-                    \n+\n+                        existing_index[\"segments\"] = segments_data\n+\n                     DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**existing_index)\n-                return json.dumps({\n-                    \"status\": \"success\",\n-                    \"message\": \"Using existing document analysis\",\n-                    \"segments_dir\": segments_dir,\n-                    \"total_segments\": existing_index[\"total_segments\"]\n-                }, ensure_ascii=False, indent=2)\n-            \n+                return json.dumps(\n+                    {\n+                        \"status\": \"success\",\n+                        \"message\": \"Using existing document analysis\",\n+                        \"segments_dir\": segments_dir,\n+                        \"total_segments\": existing_index[\"total_segments\"],\n+                    },\n+                    ensure_ascii=False,\n+                    indent=2,\n+                )\n+\n             except Exception as e:\n                 logger.error(f\"Failed to load existing index: {e}\")\n                 logger.info(\"Will perform fresh analysis instead\")\n@@ -1262,19 +1528,19 @@ async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = Fal\n                     os.remove(index_file_path)\n                 except:\n                     pass\n-        \n+\n         # Read document content\n-        with open(md_file_path, 'r', encoding='utf-8') as f:\n+        with open(md_file_path, \"r\", encoding=\"utf-8\") as f:\n             content = f.read()\n-        \n+\n         # Analyze document\n         analyzer = DocumentAnalyzer()\n         doc_type, confidence = analyzer.analyze_document_type(content)\n         strategy = analyzer.detect_segmentation_strategy(content, doc_type)\n-        \n+\n         # Create segments\n         segments = segmenter.segment_document(content, strategy)\n-        \n+\n         # Create document index\n         document_index = DocumentIndex(\n             document_path=md_file_path,\n@@ -1283,46 +1549,56 @@ async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = Fal\n             total_segments=len(segments),\n             total_chars=len(content),\n             segments=segments,\n-            created_at=datetime.now().isoformat()\n+            created_at=datetime.now().isoformat(),\n         )\n-        \n+\n         # Save segments\n         ensure_segments_dir_exists(segments_dir)\n-        \n+\n         # Save document index\n-        with open(index_file_path, 'w', encoding='utf-8') as f:\n-            json.dump(asdict(document_index), f, ensure_ascii=False, indent=2, default=str)\n-        \n+        with open(index_file_path, \"w\", encoding=\"utf-8\") as f:\n+            json.dump(\n+                asdict(document_index), f, ensure_ascii=False, indent=2, default=str\n+            )\n+\n         # Save individual segment files for fallback\n         for segment in segments:\n             segment_file_path = os.path.join(segments_dir, f\"segment_{segment.id}.md\")\n-            with open(segment_file_path, 'w', encoding='utf-8') as f:\n+            with open(segment_file_path, \"w\", encoding=\"utf-8\") as f:\n                 f.write(f\"# {segment.title}\\n\\n\")\n                 f.write(f\"**Content Type:** {segment.content_type}\\n\")\n                 f.write(f\"**Keywords:** {', '.join(segment.keywords[:10])}\\n\\n\")\n                 f.write(segment.content)\n-        \n+\n         # Store in memory\n         DOCUMENT_INDEXES[paper_dir] = document_index\n-        \n-        logger.info(f\"Document segmentation completed: {len(segments)} segments created\")\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"message\": f\"Document analysis completed with {strategy} strategy\",\n-            \"document_type\": doc_type,\n-            \"segmentation_strategy\": strategy,\n-            \"segments_dir\": segments_dir,\n-            \"total_segments\": len(segments),\n-            \"total_chars\": len(content)\n-        }, ensure_ascii=False, indent=2)\n-        \n+\n+        logger.info(\n+            f\"Document segmentation completed: {len(segments)} segments created\"\n+        )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"message\": f\"Document analysis completed with {strategy} strategy\",\n+                \"document_type\": doc_type,\n+                \"segmentation_strategy\": strategy,\n+                \"segments_dir\": segments_dir,\n+                \"total_segments\": len(segments),\n+                \"total_chars\": len(content),\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in analyze_and_segment_document: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to analyze document: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\"status\": \"error\", \"message\": f\"Failed to analyze document: {str(e)}\"},\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n @mcp.tool()\n async def read_document_segments(\n@@ -1330,18 +1606,18 @@ async def read_document_segments(\n     query_type: str,\n     keywords: List[str] = None,\n     max_segments: int = 3,\n-    max_total_chars: int = None\n+    max_total_chars: int = None,\n ) -> str:\n     \"\"\"\n     Intelligently retrieve relevant document segments based on query type\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         query_type: Type of query - \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n         keywords: Optional list of keywords to search for\n         max_segments: Maximum number of segments to return\n         max_total_chars: Maximum total characters to return\n-        \n+\n     Returns:\n         JSON string with selected segments\n     \"\"\"\n@@ -1350,100 +1626,113 @@ async def read_document_segments(\n         if paper_dir not in DOCUMENT_INDEXES:\n             segments_dir = get_segments_dir(paper_dir)\n             index_file_path = os.path.join(segments_dir, \"document_index.json\")\n-            \n+\n             if os.path.exists(index_file_path):\n-                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n                     index_data = json.load(f)\n                     # Convert dict back to DocumentIndex with backward compatibility\n                     segments_data = []\n-                    for seg_data in index_data.get('segments', []):\n+                    for seg_data in index_data.get(\"segments\", []):\n                         # Ensure all required fields exist, provide default values\n                         segment_dict = dict(seg_data)\n-                        \n+\n                         # Compatibility handling: add missing fields\n-                        if 'content_type' not in segment_dict:\n-                            segment_dict['content_type'] = 'general'\n-                        if 'keywords' not in segment_dict:\n-                            segment_dict['keywords'] = []\n-                        if 'relevance_scores' not in segment_dict:\n-                            segment_dict['relevance_scores'] = {\n-                                'concept_analysis': 0.5,\n-                                'algorithm_extraction': 0.5,\n-                                'code_planning': 0.5\n+                        if \"content_type\" not in segment_dict:\n+                            segment_dict[\"content_type\"] = \"general\"\n+                        if \"keywords\" not in segment_dict:\n+                            segment_dict[\"keywords\"] = []\n+                        if \"relevance_scores\" not in segment_dict:\n+                            segment_dict[\"relevance_scores\"] = {\n+                                \"concept_analysis\": 0.5,\n+                                \"algorithm_extraction\": 0.5,\n+                                \"code_planning\": 0.5,\n                             }\n-                        if 'section_path' not in segment_dict:\n-                            segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n-                        \n+                        if \"section_path\" not in segment_dict:\n+                            segment_dict[\"section_path\"] = segment_dict.get(\n+                                \"title\", \"Unknown\"\n+                            )\n+\n                         segment = DocumentSegment(**segment_dict)\n                         segments_data.append(segment)\n-                    \n-                    index_data['segments'] = segments_data\n+\n+                    index_data[\"segments\"] = segments_data\n                     DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**index_data)\n             else:\n                 # Auto-analyze if not found\n                 await analyze_and_segment_document(paper_dir)\n-        \n+\n         document_index = DOCUMENT_INDEXES[paper_dir]\n-        \n+\n         # Dynamically calculate character limit\n         if max_total_chars is None:\n             max_total_chars = _calculate_adaptive_char_limit(document_index, query_type)\n-        \n+\n         # Score and rank segments with enhanced algorithm\n         scored_segments = []\n         for segment in document_index.segments:\n             # Base relevance score (already enhanced in new system)\n             relevance_score = segment.relevance_scores.get(query_type, 0.5)\n-            \n+\n             # Enhanced keyword matching with position weighting\n             if keywords:\n                 keyword_score = _calculate_enhanced_keyword_score(segment, keywords)\n                 relevance_score += keyword_score\n-            \n+\n             # Content completeness bonus\n             completeness_bonus = _calculate_completeness_bonus(segment, document_index)\n             relevance_score += completeness_bonus\n-            \n+\n             scored_segments.append((segment, relevance_score))\n-        \n+\n         # Sort by enhanced relevance score\n         scored_segments.sort(key=lambda x: x[1], reverse=True)\n-        \n+\n         # Intelligent segment selection with integrity preservation\n         selected_segments = _select_segments_with_integrity(\n             scored_segments, max_segments, max_total_chars, query_type\n         )\n-        \n+\n         total_chars = sum(seg[\"char_count\"] for seg in selected_segments)\n-        \n-        logger.info(f\"Selected {len(selected_segments)} segments for {query_type} query\")\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"query_type\": query_type,\n-            \"keywords\": keywords or [],\n-            \"total_segments_available\": len(document_index.segments),\n-            \"segments_selected\": len(selected_segments),\n-            \"total_chars\": total_chars,\n-            \"max_chars_used\": max_total_chars,\n-            \"segments\": selected_segments\n-        }, ensure_ascii=False, indent=2)\n-        \n+\n+        logger.info(\n+            f\"Selected {len(selected_segments)} segments for {query_type} query\"\n+        )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"query_type\": query_type,\n+                \"keywords\": keywords or [],\n+                \"total_segments_available\": len(document_index.segments),\n+                \"segments_selected\": len(selected_segments),\n+                \"total_chars\": total_chars,\n+                \"max_chars_used\": max_total_chars,\n+                \"segments\": selected_segments,\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in read_document_segments: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to read document segments: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\n+                \"status\": \"error\",\n+                \"message\": f\"Failed to read document segments: {str(e)}\",\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n @mcp.tool()\n async def get_document_overview(paper_dir: str) -> str:\n     \"\"\"\n     Get overview of document structure and available segments\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n-        \n+\n     Returns:\n         JSON string with document overview\n     \"\"\"\n@@ -1451,45 +1740,59 @@ async def get_document_overview(paper_dir: str) -> str:\n         # Ensure document is analyzed\n         if paper_dir not in DOCUMENT_INDEXES:\n             await analyze_and_segment_document(paper_dir)\n-        \n+\n         document_index = DOCUMENT_INDEXES[paper_dir]\n-        \n+\n         # Create overview\n         segment_summaries = []\n         for segment in document_index.segments:\n-            segment_summaries.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content_type\": segment.content_type,\n-                \"char_count\": segment.char_count,\n-                \"keywords\": segment.keywords[:5],  # Top 5 keywords\n-                \"relevance_scores\": segment.relevance_scores\n-            })\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"document_path\": document_index.document_path,\n-            \"document_type\": document_index.document_type,\n-            \"segmentation_strategy\": document_index.segmentation_strategy,\n-            \"total_segments\": document_index.total_segments,\n-            \"total_chars\": document_index.total_chars,\n-            \"created_at\": document_index.created_at,\n-            \"segments_overview\": segment_summaries\n-        }, ensure_ascii=False, indent=2)\n-        \n+            segment_summaries.append(\n+                {\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content_type\": segment.content_type,\n+                    \"char_count\": segment.char_count,\n+                    \"keywords\": segment.keywords[:5],  # Top 5 keywords\n+                    \"relevance_scores\": segment.relevance_scores,\n+                }\n+            )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"document_path\": document_index.document_path,\n+                \"document_type\": document_index.document_type,\n+                \"segmentation_strategy\": document_index.segmentation_strategy,\n+                \"total_segments\": document_index.total_segments,\n+                \"total_chars\": document_index.total_chars,\n+                \"created_at\": document_index.created_at,\n+                \"segments_overview\": segment_summaries,\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in get_document_overview: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to get document overview: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\n+                \"status\": \"error\",\n+                \"message\": f\"Failed to get document overview: {str(e)}\",\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n # =============== Enhanced retrieval system helper methods ===============\n \n-def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: str) -> int:\n+\n+def _calculate_adaptive_char_limit(\n+    document_index: DocumentIndex, query_type: str\n+) -> int:\n     \"\"\"Dynamically calculate character limit based on document complexity and query type\"\"\"\n     base_limit = 6000\n-    \n+\n     # Adjust based on document type\n     if document_index.document_type == \"research_paper\":\n         base_limit = 10000\n@@ -1497,116 +1800,140 @@ def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: st\n         base_limit = 12000\n     elif document_index.segmentation_strategy == \"algorithm_preserve_integrity\":\n         base_limit = 15000\n-    \n+\n     # Adjust based on query type\n     query_multipliers = {\n         \"algorithm_extraction\": 1.5,  # Algorithms need more context\n         \"concept_analysis\": 1.2,\n-        \"code_planning\": 1.3\n+        \"code_planning\": 1.3,\n     }\n-    \n+\n     multiplier = query_multipliers.get(query_type, 1.0)\n     return int(base_limit * multiplier)\n \n-def _calculate_enhanced_keyword_score(segment: DocumentSegment, keywords: List[str]) -> float:\n+\n+def _calculate_enhanced_keyword_score(\n+    segment: DocumentSegment, keywords: List[str]\n+) -> float:\n     \"\"\"Calculate enhanced keyword matching score\"\"\"\n     score = 0.0\n     content_lower = segment.content.lower()\n     title_lower = segment.title.lower()\n-    \n+\n     for keyword in keywords:\n         keyword_lower = keyword.lower()\n-        \n+\n         # Title matching has higher weight\n         if keyword_lower in title_lower:\n             score += 0.3\n-        \n+\n         # Content matching\n         content_matches = content_lower.count(keyword_lower)\n         if content_matches > 0:\n             # Consider term frequency and position\n             frequency_score = min(0.2, content_matches * 0.05)\n-            \n+\n             # Check if in important position (first 25% of content)\n-            early_content = content_lower[:len(content_lower)//4]\n+            early_content = content_lower[: len(content_lower) // 4]\n             if keyword_lower in early_content:\n                 frequency_score += 0.1\n-            \n+\n             score += frequency_score\n-    \n+\n     return min(0.6, score)  # Limit maximum bonus\n \n-def _calculate_completeness_bonus(segment: DocumentSegment, document_index: DocumentIndex) -> float:\n+\n+def _calculate_completeness_bonus(\n+    segment: DocumentSegment, document_index: DocumentIndex\n+) -> float:\n     \"\"\"Calculate content completeness bonus\"\"\"\n     bonus = 0.0\n-    \n+\n     # Completeness bonus for algorithm and formula content\n-    if segment.content_type in ['algorithm', 'formula', 'merged']:\n+    if segment.content_type in [\"algorithm\", \"formula\", \"merged\"]:\n         bonus += 0.2\n-    \n+\n     # Long paragraphs usually contain more complete information\n     if segment.char_count > 2000:\n         bonus += 0.1\n     elif segment.char_count > 4000:\n         bonus += 0.15\n-    \n+\n     # High importance paragraph bonus\n-    if segment.relevance_scores.get('algorithm_extraction', 0) > 0.8:\n+    if segment.relevance_scores.get(\"algorithm_extraction\", 0) > 0.8:\n         bonus += 0.1\n-    \n+\n     return min(0.3, bonus)\n \n-def _select_segments_with_integrity(scored_segments: List[Tuple], max_segments: int, \n-                                  max_total_chars: int, query_type: str) -> List[Dict]:\n+\n+def _select_segments_with_integrity(\n+    scored_segments: List[Tuple],\n+    max_segments: int,\n+    max_total_chars: int,\n+    query_type: str,\n+) -> List[Dict]:\n     \"\"\"Intelligently select segments while maintaining content integrity\"\"\"\n     selected_segments = []\n     total_chars = 0\n-    \n+\n     # First select the highest scoring segments\n     for segment, score in scored_segments:\n         if len(selected_segments) >= max_segments:\n             break\n-            \n+\n         if total_chars + segment.char_count <= max_total_chars:\n-            selected_segments.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content\": segment.content,\n-                \"content_type\": segment.content_type,\n-                \"relevance_score\": score,\n-                \"char_count\": segment.char_count\n-            })\n+            selected_segments.append(\n+                {\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content\": segment.content,\n+                    \"content_type\": segment.content_type,\n+                    \"relevance_score\": score,\n+                    \"char_count\": segment.char_count,\n+                }\n+            )\n             total_chars += segment.char_count\n         elif len(selected_segments) == 0:\n             # If the first segment exceeds the limit, truncate but preserve it\n-            truncated_content = segment.content[:max_total_chars - 200] + \"\\n\\n[Content truncated for length...]\"\n-            selected_segments.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content\": truncated_content,\n-                \"content_type\": segment.content_type,\n-                \"relevance_score\": score,\n-                \"char_count\": len(truncated_content)\n-            })\n-            break\n-    \n-    # If there's remaining space, try to add relevant small segments\n-    remaining_chars = max_total_chars - total_chars\n-    if remaining_chars > 500 and len(selected_segments) < max_segments:\n-        for segment, score in scored_segments[len(selected_segments):]:\n-            if segment.char_count <= remaining_chars and len(selected_segments) < max_segments:\n-                selected_segments.append({\n+            truncated_content = (\n+                segment.content[: max_total_chars - 200]\n+                + \"\\n\\n[Content truncated for length...]\"\n+            )\n+            selected_segments.append(\n+                {\n                     \"id\": segment.id,\n                     \"title\": segment.title,\n-                    \"content\": segment.content,\n+                    \"content\": truncated_content,\n                     \"content_type\": segment.content_type,\n                     \"relevance_score\": score,\n-                    \"char_count\": segment.char_count\n-                })\n+                    \"char_count\": len(truncated_content),\n+                }\n+            )\n+            break\n+\n+    # If there's remaining space, try to add relevant small segments\n+    remaining_chars = max_total_chars - total_chars\n+    if remaining_chars > 500 and len(selected_segments) < max_segments:\n+        for segment, score in scored_segments[len(selected_segments) :]:\n+            if (\n+                segment.char_count <= remaining_chars\n+                and len(selected_segments) < max_segments\n+            ):\n+                selected_segments.append(\n+                    {\n+                        \"id\": segment.id,\n+                        \"title\": segment.title,\n+                        \"content\": segment.content,\n+                        \"content_type\": segment.content_type,\n+                        \"relevance_score\": score,\n+                        \"char_count\": segment.char_count,\n+                    }\n+                )\n                 remaining_chars -= segment.char_count\n-    \n+\n     return selected_segments\n \n+\n if __name__ == \"__main__\":\n     # Run the MCP server\n-    mcp.run()\n\\ No newline at end of file\n+    mcp.run()\ndiff --git a/utils/llm_utils.py b/utils/llm_utils.py\nindex 05d3d28..8d4a0cd 100644\n--- a/utils/llm_utils.py\n+++ b/utils/llm_utils.py\n@@ -56,10 +56,10 @@ def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\") -> Type\n def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n     \"\"\"\n     Get default models configuration from config file.\n-    \n+\n     Args:\n         config_path: Path to the main configuration file\n-        \n+\n     Returns:\n         dict: Default models configuration\n     \"\"\"\n@@ -67,14 +67,16 @@ def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n         if os.path.exists(config_path):\n             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-            \n+\n             # Extract model configurations\n             openai_config = config.get(\"openai\", {})\n             anthropic_config = config.get(\"anthropic\", {})\n-            \n+\n             return {\n-                \"anthropic\": anthropic_config.get(\"default_model\", \"claude-sonnet-4-20250514\"),\n-                \"openai\": openai_config.get(\"default_model\", \"o3-mini\")\n+                \"anthropic\": anthropic_config.get(\n+                    \"default_model\", \"claude-sonnet-4-20250514\"\n+                ),\n+                \"openai\": openai_config.get(\"default_model\", \"o3-mini\"),\n             }\n         else:\n             print(f\"\ud83e\udd16 Config file {config_path} not found, using default models\")\n@@ -86,13 +88,15 @@ def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n         return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n \n \n-def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\") -> Dict[str, Any]:\n+def get_document_segmentation_config(\n+    config_path: str = \"mcp_agent.config.yaml\",\n+) -> Dict[str, Any]:\n     \"\"\"\n     Get document segmentation configuration from config file.\n-    \n+\n     Args:\n         config_path: Path to the main configuration file\n-        \n+\n     Returns:\n         Dict containing segmentation configuration with default values\n     \"\"\"\n@@ -100,71 +104,83 @@ def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\")\n         if os.path.exists(config_path):\n             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-            \n+\n             # Get document segmentation config with defaults\n             seg_config = config.get(\"document_segmentation\", {})\n             return {\n                 \"enabled\": seg_config.get(\"enabled\", True),\n-                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000)\n+                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000),\n             }\n         else:\n-            print(f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\")\n+            print(\n+                f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\"\n+            )\n             return {\"enabled\": True, \"size_threshold_chars\": 50000}\n-            \n+\n     except Exception as e:\n         print(f\"\ud83d\udcc4 Error reading segmentation config from {config_path}: {e}\")\n         print(\"\ud83d\udcc4 Using default segmentation settings\")\n         return {\"enabled\": True, \"size_threshold_chars\": 50000}\n \n \n-def should_use_document_segmentation(document_content: str, config_path: str = \"mcp_agent.config.yaml\") -> Tuple[bool, str]:\n+def should_use_document_segmentation(\n+    document_content: str, config_path: str = \"mcp_agent.config.yaml\"\n+) -> Tuple[bool, str]:\n     \"\"\"\n     Determine whether to use document segmentation based on configuration and document size.\n-    \n+\n     Args:\n         document_content: The content of the document to analyze\n         config_path: Path to the configuration file\n-        \n+\n     Returns:\n         Tuple of (should_segment, reason) where:\n         - should_segment: Boolean indicating whether to use segmentation\n         - reason: String explaining the decision\n     \"\"\"\n     seg_config = get_document_segmentation_config(config_path)\n-    \n+\n     if not seg_config[\"enabled\"]:\n         return False, \"Document segmentation disabled in configuration\"\n-    \n+\n     doc_size = len(document_content)\n     threshold = seg_config[\"size_threshold_chars\"]\n-    \n+\n     if doc_size > threshold:\n-        return True, f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\"\n+        return (\n+            True,\n+            f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\",\n+        )\n     else:\n-        return False, f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\"\n+        return (\n+            False,\n+            f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\",\n+        )\n \n \n-def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list = None) -> Dict[str, list]:\n+def get_adaptive_agent_config(\n+    use_segmentation: bool, search_server_names: list = None\n+) -> Dict[str, list]:\n     \"\"\"\n     Get adaptive agent configuration based on whether to use document segmentation.\n-    \n+\n     Args:\n         use_segmentation: Whether to include document-segmentation server\n         search_server_names: Base search server names (from get_search_server_names)\n-        \n+\n     Returns:\n         Dict containing server configurations for different agents\n     \"\"\"\n     if search_server_names is None:\n         search_server_names = []\n-    \n+\n     # Base configuration\n     config = {\n         \"concept_analysis\": [],\n         \"algorithm_analysis\": search_server_names.copy(),\n-        \"code_planner\": search_server_names.copy()\n+        \"code_planner\": search_server_names.copy(),\n     }\n-    \n+\n     # Add document-segmentation server if needed\n     if use_segmentation:\n         config[\"concept_analysis\"] = [\"document-segmentation\"]\n@@ -172,39 +188,39 @@ def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list\n             config[\"algorithm_analysis\"].append(\"document-segmentation\")\n         if \"document-segmentation\" not in config[\"code_planner\"]:\n             config[\"code_planner\"].append(\"document-segmentation\")\n-    \n+\n     return config\n \n \n def get_adaptive_prompts(use_segmentation: bool) -> Dict[str, str]:\n     \"\"\"\n     Get appropriate prompt versions based on segmentation usage.\n-    \n+\n     Args:\n         use_segmentation: Whether to use segmented reading prompts\n-        \n+\n     Returns:\n         Dict containing prompt configurations\n     \"\"\"\n     # Import here to avoid circular imports\n     from prompts.code_prompts import (\n         PAPER_CONCEPT_ANALYSIS_PROMPT,\n-        PAPER_ALGORITHM_ANALYSIS_PROMPT, \n+        PAPER_ALGORITHM_ANALYSIS_PROMPT,\n         CODE_PLANNING_PROMPT,\n         PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n         PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n-        CODE_PLANNING_PROMPT_TRADITIONAL\n+        CODE_PLANNING_PROMPT_TRADITIONAL,\n     )\n-    \n+\n     if use_segmentation:\n         return {\n             \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT,\n             \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT,\n-            \"code_planning\": CODE_PLANNING_PROMPT\n+            \"code_planning\": CODE_PLANNING_PROMPT,\n         }\n     else:\n         return {\n             \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n             \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n-            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL\n-        }\n\\ No newline at end of file\n+            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL,\n+        }\ndiff --git a/workflows/agents/document_segmentation_agent.py b/workflows/agents/document_segmentation_agent.py\nindex a9e4164..6325a7e 100644\n--- a/workflows/agents/document_segmentation_agent.py\n+++ b/workflows/agents/document_segmentation_agent.py\n@@ -5,7 +5,6 @@ A lightweight agent that coordinates with the document segmentation MCP server\n to analyze document structure and prepare segments for other agents.\n \"\"\"\n \n-import asyncio\n import os\n import logging\n from typing import Dict, Any, Optional\n@@ -17,14 +16,14 @@ from utils.llm_utils import get_preferred_llm_class\n class DocumentSegmentationAgent:\n     \"\"\"\n     Intelligent document segmentation agent with semantic analysis capabilities.\n-    \n+\n     This enhanced agent provides:\n     1. **Semantic Document Classification**: Content-based document type identification\n     2. **Adaptive Segmentation Strategy**: Algorithm integrity and semantic coherence preservation\n     3. **Planning Agent Optimization**: Segment preparation specifically optimized for downstream agents\n     4. **Quality Intelligence Validation**: Advanced metrics for completeness and technical accuracy\n     5. **Algorithm Completeness Protection**: Ensures critical algorithms and formulas remain intact\n-    \n+\n     Key improvements over traditional segmentation:\n     - Semantic content analysis vs mechanical structure splitting\n     - Dynamic character limits based on content complexity\n@@ -32,26 +31,26 @@ class DocumentSegmentationAgent:\n     - Algorithm and formula integrity preservation\n     - Content type-aware segmentation strategies\n     \"\"\"\n-    \n+\n     def __init__(self, logger: Optional[logging.Logger] = None):\n         self.logger = logger or self._create_default_logger()\n         self.mcp_agent = None\n-    \n+\n     def _create_default_logger(self) -> logging.Logger:\n         \"\"\"Create default logger if none provided\"\"\"\n         logger = logging.getLogger(f\"{__name__}.DocumentSegmentationAgent\")\n         logger.setLevel(logging.INFO)\n         return logger\n-    \n+\n     async def __aenter__(self):\n         \"\"\"Async context manager entry\"\"\"\n         await self.initialize()\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         \"\"\"Async context manager exit\"\"\"\n         await self.cleanup()\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize the MCP agent connection\"\"\"\n         try:\n@@ -74,21 +73,21 @@ Your enhanced capabilities include:\n - Provide actionable quality assessments\n \n Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.\"\"\",\n-                server_names=[\"document-segmentation\", \"filesystem\"]\n+                server_names=[\"document-segmentation\", \"filesystem\"],\n             )\n-            \n+\n             # Initialize the agent context\n             await self.mcp_agent.__aenter__()\n-            \n+\n             # Attach LLM\n             self.llm = await self.mcp_agent.attach_llm(get_preferred_llm_class())\n-            \n+\n             self.logger.info(\"DocumentSegmentationAgent initialized successfully\")\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Failed to initialize DocumentSegmentationAgent: {e}\")\n             raise\n-    \n+\n     async def cleanup(self):\n         \"\"\"Cleanup resources\"\"\"\n         if self.mcp_agent:\n@@ -96,36 +95,34 @@ Use the enhanced document-segmentation tools to deliver superior segmentation re\n                 await self.mcp_agent.__aexit__(None, None, None)\n             except Exception as e:\n                 self.logger.warning(f\"Error during cleanup: {e}\")\n-    \n+\n     async def analyze_and_prepare_document(\n-        self, \n-        paper_dir: str, \n-        force_refresh: bool = False\n+        self, paper_dir: str, force_refresh: bool = False\n     ) -> Dict[str, Any]:\n         \"\"\"\n         Perform intelligent semantic analysis and create optimized document segments.\n-        \n+\n         This method coordinates with the enhanced document segmentation server to:\n         - Classify document type using semantic content analysis\n         - Select optimal segmentation strategy (semantic_research_focused, algorithm_preserve_integrity, etc.)\n         - Preserve algorithm and formula integrity\n         - Optimize segments for downstream planning agents\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n             force_refresh: Whether to force re-analysis with latest algorithms\n-            \n+\n         Returns:\n             Dict containing enhanced analysis results and intelligent segment information\n         \"\"\"\n         try:\n             self.logger.info(f\"Starting document analysis for: {paper_dir}\")\n-            \n+\n             # Check if markdown file exists\n-            md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+            md_files = [f for f in os.listdir(paper_dir) if f.endswith(\".md\")]\n             if not md_files:\n                 raise ValueError(f\"No markdown file found in {paper_dir}\")\n-            \n+\n             # Use the enhanced document segmentation tool\n             message = f\"\"\"Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}\n \n@@ -147,35 +144,35 @@ After segmentation, get a document overview and provide:\n - Algorithm/formula integrity verification\n - Recommendations for planning agent optimization\n - Technical content completeness evaluation\"\"\"\n-            \n+\n             result = await self.llm.generate_str(message=message)\n-            \n+\n             self.logger.info(\"Document analysis completed successfully\")\n-            \n+\n             # Parse the result and return structured information\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"analysis_result\": result,\n-                \"segments_available\": True\n+                \"segments_available\": True,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error in document analysis: {e}\")\n             return {\n                 \"status\": \"error\",\n                 \"paper_dir\": paper_dir,\n                 \"error_message\": str(e),\n-                \"segments_available\": False\n+                \"segments_available\": False,\n             }\n-    \n+\n     async def get_document_overview(self, paper_dir: str) -> Dict[str, Any]:\n         \"\"\"\n         Get overview of document structure and segments.\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n-            \n+\n         Returns:\n             Dict containing document overview information\n         \"\"\"\n@@ -194,40 +191,36 @@ Provide a comprehensive analysis focusing on:\n 2. Algorithm and formula integrity preservation\n 3. Segment relevance for downstream planning agents\n 4. Technical content distribution and completeness\"\"\"\n-            \n+\n             result = await self.llm.generate_str(message=message)\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n-                \"overview_result\": result\n+                \"overview_result\": result,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting document overview: {e}\")\n-            return {\n-                \"status\": \"error\",\n-                \"paper_dir\": paper_dir,\n-                \"error_message\": str(e)\n-            }\n-    \n+            return {\"status\": \"error\", \"paper_dir\": paper_dir, \"error_message\": str(e)}\n+\n     async def validate_segmentation_quality(self, paper_dir: str) -> Dict[str, Any]:\n         \"\"\"\n         Validate the quality of document segmentation.\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n-            \n+\n         Returns:\n             Dict containing validation results\n         \"\"\"\n         try:\n             # Get overview first\n             overview_result = await self.get_document_overview(paper_dir)\n-            \n+\n             if overview_result[\"status\"] != \"success\":\n                 return overview_result\n-            \n+\n             # Analyze enhanced segmentation quality\n             message = f\"\"\"Based on the intelligent document overview for {paper_dir}, please evaluate the enhanced segmentation quality using advanced criteria.\n \n@@ -244,38 +237,32 @@ Provide a comprehensive analysis focusing on:\n - Algorithm/formula integrity enhancements  \n - Planning agent optimization opportunities\n - Content distribution balance adjustments\"\"\"\n-            \n+\n             validation_result = await self.llm.generate_str(message=message)\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"validation_result\": validation_result,\n-                \"overview_data\": overview_result\n+                \"overview_data\": overview_result,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error validating segmentation quality: {e}\")\n-            return {\n-                \"status\": \"error\",\n-                \"paper_dir\": paper_dir,\n-                \"error_message\": str(e)\n-            }\n+            return {\"status\": \"error\", \"paper_dir\": paper_dir, \"error_message\": str(e)}\n \n \n async def run_document_segmentation_analysis(\n-    paper_dir: str, \n-    logger: Optional[logging.Logger] = None,\n-    force_refresh: bool = False\n+    paper_dir: str, logger: Optional[logging.Logger] = None, force_refresh: bool = False\n ) -> Dict[str, Any]:\n     \"\"\"\n     Convenience function to run document segmentation analysis.\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         logger: Optional logger instance\n         force_refresh: Whether to force re-analysis\n-        \n+\n     Returns:\n         Dict containing analysis results\n     \"\"\"\n@@ -284,78 +271,83 @@ async def run_document_segmentation_analysis(\n         analysis_result = await agent.analyze_and_prepare_document(\n             paper_dir, force_refresh=force_refresh\n         )\n-        \n+\n         if analysis_result[\"status\"] == \"success\":\n             # Validate segmentation quality\n             validation_result = await agent.validate_segmentation_quality(paper_dir)\n             analysis_result[\"validation\"] = validation_result\n-        \n+\n         return analysis_result\n \n \n # Utility function for integration with existing workflow\n async def prepare_document_segments(\n-    paper_dir: str,\n-    logger: Optional[logging.Logger] = None\n+    paper_dir: str, logger: Optional[logging.Logger] = None\n ) -> Dict[str, Any]:\n     \"\"\"\n     Prepare intelligent document segments optimized for planning agents.\n-    \n+\n     This enhanced function leverages semantic analysis to create segments that:\n     - Preserve algorithm and formula integrity\n     - Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent\n     - Use adaptive character limits based on content complexity\n     - Maintain technical content completeness\n-    \n-    Called from the orchestration engine (Phase 3.5) to prepare documents \n+\n+    Called from the orchestration engine (Phase 3.5) to prepare documents\n     before the planning phase with superior segmentation quality.\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory containing markdown file\n         logger: Optional logger instance for tracking\n-        \n+\n     Returns:\n         Dict containing enhanced preparation results and intelligent metadata\n     \"\"\"\n     try:\n         logger = logger or logging.getLogger(__name__)\n         logger.info(f\"Preparing document segments for: {paper_dir}\")\n-        \n+\n         # Run analysis\n         result = await run_document_segmentation_analysis(\n             paper_dir=paper_dir,\n             logger=logger,\n-            force_refresh=False  # Use cached analysis if available\n+            force_refresh=False,  # Use cached analysis if available\n         )\n-        \n+\n         if result[\"status\"] == \"success\":\n             logger.info(\"Document segments prepared successfully\")\n-            \n+\n             # Create metadata for downstream agents\n             segments_dir = os.path.join(paper_dir, \"document_segments\")\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"segments_dir\": segments_dir,\n                 \"segments_ready\": True,\n                 \"analysis_summary\": result.get(\"analysis_result\", \"\"),\n-                \"validation_summary\": result.get(\"validation\", {}).get(\"validation_result\", \"\")\n+                \"validation_summary\": result.get(\"validation\", {}).get(\n+                    \"validation_result\", \"\"\n+                ),\n             }\n         else:\n-            logger.error(f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\")\n+            logger.error(\n+                f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\"\n+            )\n             return {\n                 \"status\": \"error\",\n                 \"paper_dir\": paper_dir,\n                 \"segments_ready\": False,\n-                \"error_message\": result.get(\"error_message\", \"Document segmentation failed\")\n+                \"error_message\": result.get(\n+                    \"error_message\", \"Document segmentation failed\"\n+                ),\n             }\n-            \n+\n     except Exception as e:\n         logger.error(f\"Error preparing document segments: {e}\")\n         return {\n             \"status\": \"error\",\n             \"paper_dir\": paper_dir,\n             \"segments_ready\": False,\n-            \"error_message\": str(e)\n-        }\n\\ No newline at end of file\n+            \"error_message\": str(e),\n+        }\n"
    },
    {
        "id": 279,
        "sha_fail": "238a409674f147334f43788013fdfa766fd8035c",
        "diff": "diff --git a/workflows/agents/memory_agent_concise.py b/workflows/agents/memory_agent_concise.py\nindex 1be5423..841dc69 100644\n--- a/workflows/agents/memory_agent_concise.py\n+++ b/workflows/agents/memory_agent_concise.py\n@@ -695,7 +695,7 @@ class ConciseMemoryAgent:\n         summary = f\"\"\"# Code Implementation Summary\n **All Previously Implemented Files:**\n {implemented_files_list}\n-**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n+**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n **File Implemented**: {file_path}\n **Total Files Implemented**: {files_implemented}\n **Summary failed to generate.**\ndiff --git a/workflows/agents/memory_agent_concise_index.py b/workflows/agents/memory_agent_concise_index.py\nindex ad3c53f..b67fef9 100644\n--- a/workflows/agents/memory_agent_concise_index.py\n+++ b/workflows/agents/memory_agent_concise_index.py\n@@ -684,7 +684,7 @@ class ConciseMemoryAgent:\n         summary = f\"\"\"# Code Implementation Summary\n **All Previously Implemented Files:**\n {implemented_files_list}\n-**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n+**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n **File Implemented**: {file_path}\n **Total Files Implemented**: {files_implemented}\n **Summary failed to generate.**\n"
    },
    {
        "id": 280,
        "sha_fail": "219bc58c7f1bfe425ddc1d628ff5cda9639afc1e",
        "diff": "diff --git a/tools/pdf_downloader.py b/tools/pdf_downloader.py\nindex adb602b..301ca5d 100644\n--- a/tools/pdf_downloader.py\n+++ b/tools/pdf_downloader.py\n@@ -109,10 +109,10 @@ async def perform_document_conversion(\n         try:\n             with open(file_path, \"rb\") as f:\n                 header = f.read(8)\n-                is_pdf_file = header.startswith(b'%PDF')\n+                is_pdf_file = header.startswith(b\"%PDF\")\n         except Exception:\n             is_pdf_file = file_path.lower().endswith(\".pdf\")\n-    \n+\n     if is_pdf_file and PYPDF2_AVAILABLE:\n         try:\n             simple_converter = SimplePdfConverter()\n@@ -585,7 +585,7 @@ class DoclingConverter:\n                         ext = \"png\"\n \n                     # \u751f\u6210\u6587\u4ef6\u540d\n-                    filename = f\"image_{idx+1}.{ext}\"\n+                    filename = f\"image_{idx + 1}.{ext}\"\n                     filepath = os.path.join(images_dir, filename)\n \n                     # \u4fdd\u5b58\u56fe\u7247\u6570\u636e\n@@ -600,7 +600,7 @@ class DoclingConverter:\n                         image_map[img_id] = rel_path\n \n                 except Exception as img_error:\n-                    print(f\"Warning: Failed to extract image {idx+1}: {img_error}\")\n+                    print(f\"Warning: Failed to extract image {idx + 1}: {img_error}\")\n                     continue\n \n         except Exception as e:\ndiff --git a/utils/file_processor.py b/utils/file_processor.py\nindex b25346f..9963286 100644\n--- a/utils/file_processor.py\n+++ b/utils/file_processor.py\n@@ -190,8 +190,10 @@ class FileProcessor:\n             # Check if file is actually a PDF by reading the first few bytes\n             with open(file_path, \"rb\") as f:\n                 header = f.read(8)\n-                if header.startswith(b'%PDF'):\n-                    raise IOError(f\"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\")\n+                if header.startswith(b\"%PDF\"):\n+                    raise IOError(\n+                        f\"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\"\n+                    )\n \n             # Read file content\n             # Note: Using async with would be better for large files\n@@ -202,7 +204,9 @@ class FileProcessor:\n             return content\n \n         except UnicodeDecodeError as e:\n-            raise IOError(f\"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}\")\n+            raise IOError(\n+                f\"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}\"\n+            )\n         except Exception as e:\n             raise IOError(f\"Error reading file {file_path}: {str(e)}\")\n \ndiff --git a/workflows/agent_orchestration_engine.py b/workflows/agent_orchestration_engine.py\nindex 027775d..3f2d602 100644\n--- a/workflows/agent_orchestration_engine.py\n+++ b/workflows/agent_orchestration_engine.py\n@@ -664,9 +664,11 @@ async def orchestrate_document_preprocessing_agent(\n             # Check if file is actually a PDF by reading the first few bytes\n             with open(md_path, \"rb\") as f:\n                 header = f.read(8)\n-                if header.startswith(b'%PDF'):\n-                    raise IOError(f\"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\")\n-            \n+                if header.startswith(b\"%PDF\"):\n+                    raise IOError(\n+                        f\"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\"\n+                    )\n+\n             with open(md_path, \"r\", encoding=\"utf-8\") as f:\n                 document_content = f.read()\n         except Exception as e:\n"
    },
    {
        "id": 281,
        "sha_fail": "9ecbaeeb4d8db618ccdb6cb81f5edf4ba2c8b8de",
        "diff": "diff --git a/github/Issue.py b/github/Issue.py\nindex 97ae8f3..1a82f3d 100644\n--- a/github/Issue.py\n+++ b/github/Issue.py\n@@ -70,8 +70,8 @@ import github.GithubObject\n import github.IssueComment\n import github.IssueEvent\n import github.IssuePullRequest\n-import github.Label\n import github.IssueType\n+import github.Label\n import github.Milestone\n import github.NamedUser\n import github.Organization\n@@ -97,8 +97,8 @@ if TYPE_CHECKING:\n     from github.IssueComment import IssueComment\n     from github.IssueEvent import IssueEvent\n     from github.IssuePullRequest import IssuePullRequest\n-    from github.Label import Label\n     from github.IssueType import IssueType\n+    from github.Label import Label\n     from github.Milestone import Milestone\n     from github.NamedUser import NamedUser\n     from github.PullRequest import PullRequest\n@@ -157,7 +157,6 @@ class Issue(CompletableGithubObject):\n         self._url: Attribute[str] = NotSet\n         self._user: Attribute[NamedUser] = NotSet\n         self._type: Attribute[IssueType] = NotSet\n-        \n \n     def __repr__(self) -> str:\n         return self.get__repr__({\"number\": self._number.value, \"title\": self._title.value})\n@@ -346,12 +345,11 @@ class Issue(CompletableGithubObject):\n     def user(self) -> NamedUser:\n         self._completeIfNotSet(self._user)\n         return self._user.value\n-    \n+\n     @property\n     def type(self) -> IssueType:\n         self._completeIfNotSet(self._type)\n         return self._type.value\n-    \n \n     def as_pull_request(self) -> PullRequest:\n         \"\"\"\ndiff --git a/github/IssueType.py b/github/IssueType.py\nindex 6b8bae9..e1fa991 100644\n--- a/github/IssueType.py\n+++ b/github/IssueType.py\n@@ -37,20 +37,21 @@\n \n from __future__ import annotations\n \n+import urllib.parse\n from datetime import datetime\n-\n from typing import Any\n-import urllib.parse\n+\n from github.GithubObject import Attribute, CompletableGithubObject, NotSet, Opt, is_optional\n \n \n class IssueType(CompletableGithubObject):\n     \"\"\"\n     This class represents IssueTypes.\n-    \n+\n     The OpenAPI schema can be found at\n     - /components/schemas/issue-type\n     \"\"\"\n+\n     def _initAttributes(self) -> None:\n         self._id: Attribute[int] = NotSet\n         self._node_id: Attribute[str] = NotSet\n@@ -60,54 +61,54 @@ class IssueType(CompletableGithubObject):\n         self._created_at: Attribute[datetime] = NotSet\n         self._updated_at: Attribute[datetime] = NotSet\n         self._is_enabled: Attribute[bool] = NotSet\n-        \n+\n     def __repr__(self) -> str:\n         return self.get__repr__({\"name\": self._name.value})\n-    \n+\n     @property\n     def _identity(self) -> str:\n         return urllib.parse.quote(self.name)\n-    \n+\n     @property\n     def id(self) -> int:\n         self._completeIfNotSet(self._id)\n         return self._id.value\n-    \n+\n     @property\n     def node_id(self) -> str:\n         self._completeIfNotSet(self._node_id)\n         return self._node_id.value\n-    \n+\n     @property\n     def name(self) -> str:\n         self._completeIfNotSet(self._name)\n         return self._name.value\n-    \n+\n     @property\n     def description(self) -> str:\n         self._completeIfNotSet(self._description)\n         return self._description.value\n-        \n+\n     @property\n     def color(self) -> str:\n         self._completeIfNotSet(self._color)\n         return self._color.value\n-    \n+\n     @property\n     def created_at(self) -> datetime:\n         self._completeIfNotSet(self._created_at)\n         return self._created_at.value\n-    \n+\n     @property\n     def updated_at(self) -> datetime:\n         self._completeIfNotSet(self._updated_at)\n         return self._updated_at.value\n-    \n+\n     @property\n     def is_enabled(self) -> bool:\n         self._completeIfNotSet(self._is_enabled)\n         return self._is_enabled.value\n-    \n+\n     def _useAttributes(self, attributes: dict[str, Any]) -> None:\n         if \"id\" in attributes:\n             self._id = self._makeIntAttribute(attributes[\"id\"])\n@@ -125,4 +126,3 @@ class IssueType(CompletableGithubObject):\n             self._updated_at = self._makeDatetimeAttribute(attributes[\"updated_at\"])\n         if \"is_enabled\" in attributes:\n             self._is_enabled = self._makeBoolAttribute(attributes[\"is_enabled\"])\n-            \n\\ No newline at end of file\ndiff --git a/tests/Issue.py b/tests/Issue.py\nindex bb43415..eb464fe 100644\n--- a/tests/Issue.py\n+++ b/tests/Issue.py\n@@ -127,7 +127,7 @@ class Issue(Framework.TestCase):\n                 \"eyes\": 0,\n             },\n         )\n-        \n+\n         # test issue type data\n         self.assertEqual(self.issue.type.id, 534535)\n         self.assertEqual(self.issue.type.node_id, \"IT_kwDOAF3p4s4ACCgH\")\n"
    },
    {
        "id": 282,
        "sha_fail": "20a0e348143cc9f47432d087a62ad5c2a4040fe2",
        "diff": "diff --git a/github/Repository.py b/github/Repository.py\nindex 00e6e0a..9f19cdf 100644\n--- a/github/Repository.py\n+++ b/github/Repository.py\n@@ -4011,9 +4011,9 @@ class Repository(CompletableGithubObject):\n         :param runner: int or :class:`github.SelfHostedActionsRunner.SelfHostedActionsRunner`\n         :rtype: bool\n         \"\"\"\n-        assert isinstance(runner, github.SelfHostedActionsRunner.SelfHostedActionsRunner) or isinstance(\n-            runner, int\n-        ), runner\n+        assert isinstance(runner, github.SelfHostedActionsRunner.SelfHostedActionsRunner) or isinstance(runner, int), (\n+            runner\n+        )\n \n         if isinstance(runner, github.SelfHostedActionsRunner.SelfHostedActionsRunner):\n             runner = runner.id\n@@ -4031,7 +4031,8 @@ class Repository(CompletableGithubObject):\n         assert is_autolink or isinstance(autolink, int), autolink\n \n         status, _, _ = self._requester.requestJson(\n-            \"DELETE\", f\"{self.url}/autolinks/{autolink.id if is_autolink else autolink}\"  # type: ignore\n+            \"DELETE\",\n+            f\"{self.url}/autolinks/{autolink.id if is_autolink else autolink}\",  # type: ignore\n         )\n         return status == 204\n \n@@ -4232,7 +4233,7 @@ class Repository(CompletableGithubObject):\n         sort: Opt[str] = NotSet,\n         direction: Opt[str] = NotSet,\n         state: Opt[str] = NotSet,\n-        severity: Opt[str] = NotSet\n+        severity: Opt[str] = NotSet,\n     ) -> PaginatedList[CodeScanAlert]:\n         \"\"\"\n         :calls: `GET https://api.github.com/repos/{owner}/{repo}/code-scanning/alerts <https://docs.github.com/en/rest/reference/code-scanning#list-code-scanning-alerts-for-a-repository>`_\n@@ -4252,7 +4253,9 @@ class Repository(CompletableGithubObject):\n         allowed_severities = [\"critical\", \"high\", \"medium\", \"low\", \"warning\", \"note\", \"error\"]\n         assert is_optional(tool_name, str), tool_name\n         assert is_optional(tool_guid, str), tool_guid\n-        assert tool_name is NotSet or tool_guid is NotSet, \"You can specify the tool by using either tool_guid or tool_name, but not both.\"\n+        assert tool_name is NotSet or tool_guid is NotSet, (\n+            \"You can specify the tool by using either tool_guid or tool_name, but not both.\"\n+        )\n         assert is_optional(ref, str), ref\n         assert is_optional(pr, int), pr\n         assert sort in allowed_sorts + [NotSet], f\"Sort can be one of {', '.join(allowed_sorts)}\"\n@@ -4268,7 +4271,7 @@ class Repository(CompletableGithubObject):\n                 \"sort\": sort,\n                 \"direction\": direction,\n                 \"state\": state,\n-                \"severity\": severity\n+                \"severity\": severity,\n             }\n         )\n         return PaginatedList(\n@@ -4277,7 +4280,7 @@ class Repository(CompletableGithubObject):\n             f\"{self.url}/code-scanning/alerts\",\n             url_parameters,\n         )\n-    \n+\n     def get_codescan_alert(self, number: int) -> CodeScanAlert:\n         \"\"\"\n         :calls: `GET /repos/{owner}/{repo}/code-scanning/alerts/{alert_number} <https://docs.github.com/en/rest/code-scanning/code-scanning#get-a-code-scanning-alert>`_\n@@ -4298,7 +4301,7 @@ class Repository(CompletableGithubObject):\n         validity: Opt[str] = NotSet,\n         is_publicly_leaked: Opt[bool] = NotSet,\n         is_multi_repo: Opt[bool] = NotSet,\n-        hide_secret: Opt[bool] = NotSet\n+        hide_secret: Opt[bool] = NotSet,\n     ) -> PaginatedList[SecretScanAlert]:\n         \"\"\"\n         :calls: `GET /repos/{owner}/{repo}/secret-scanning/alerts <https://docs.github.com/en/rest/secret-scanning/secret-scanning#list-secret-scanning-alerts-for-a-repository>`_\n@@ -4315,12 +4318,21 @@ class Repository(CompletableGithubObject):\n         \"\"\"\n         allowed_states = [\"open\", \"resolved\"]\n         allowed_secret_types = [\"user\", \"push_protection\", \"partner\"]\n-        allowed_resolutions = [\"false_positive\", \"wont_fix\", \"revoked\", \"pattern_edited\", \"pattern_deleted\", \"used_in_tests\"]\n+        allowed_resolutions = [\n+            \"false_positive\",\n+            \"wont_fix\",\n+            \"revoked\",\n+            \"pattern_edited\",\n+            \"pattern_deleted\",\n+            \"used_in_tests\",\n+        ]\n         allowed_sorts = [\"created\", \"updated\"]\n         allowed_directions = [\"asc\", \"desc\"]\n         allowed_validities = [\"active\", \"inactive\", \"unknown\"]\n         assert state in allowed_states + [NotSet], f\"State can be one of {', '.join(allowed_states)}\"\n-        assert secret_type in allowed_secret_types + [NotSet], f\"Severity can be one of {', '.join(allowed_secret_types)}\"\n+        assert secret_type in allowed_secret_types + [NotSet], (\n+            f\"Severity can be one of {', '.join(allowed_secret_types)}\"\n+        )\n         assert resolution in allowed_resolutions + [NotSet], f\"Ecosystem can be one of {', '.join(allowed_resolutions)}\"\n         assert sort in allowed_sorts + [NotSet], f\"Sort can be one of {', '.join(allowed_sorts)}\"\n         assert direction in allowed_directions + [NotSet], f\"Direction can be one of {', '.join(allowed_directions)}\"\n@@ -4347,7 +4359,7 @@ class Repository(CompletableGithubObject):\n             f\"{self.url}/secret-scanning/alerts\",\n             url_parameters,\n         )\n-    \n+\n     def get_secret_scanning_alert(self, number: int, hide_secret: Opt[bool] = NotSet) -> SecretScanAlert:\n         \"\"\"\n         :calls: `GET /repos/{owner}/{repo}/secret-scanning/alerts/{alert_number} <https://docs.github.com/en/rest/secret-scanning/secret-scanning#get-a-secret-scanning-alert>`_\n@@ -4362,7 +4374,9 @@ class Repository(CompletableGithubObject):\n                 \"hide_secret\": hide_secret,\n             }\n         )\n-        headers, data = self._requester.requestJsonAndCheck(\"GET\", f\"{self.url}/secret-scanning/alerts/{number}\", parameters=query_parameters)\n+        headers, data = self._requester.requestJsonAndCheck(\n+            \"GET\", f\"{self.url}/secret-scanning/alerts/{number}\", parameters=query_parameters\n+        )\n         return github.SecretScanAlert.SecretScanAlert(self._requester, headers, data)\n \n     def get_environments(self) -> PaginatedList[Environment]:\n@@ -4535,7 +4549,9 @@ class Repository(CompletableGithubObject):\n                 \"no_bandwidth\",\n                 \"not_used\",\n                 \"tolerable_risk\",\n-            ], \"Dismissed reason can be one of ['fix_started', 'inaccurate', 'no_bandwidth', 'not_used', 'tolerable_risk']\"\n+            ], (\n+                \"Dismissed reason can be one of ['fix_started', 'inaccurate', 'no_bandwidth', 'not_used', 'tolerable_risk']\"\n+            )\n         assert is_optional(dismissed_comment, str), dismissed_comment\n         headers, data = self._requester.requestJsonAndCheck(\n             \"PATCH\",\n@@ -4864,4 +4880,4 @@ class RepositorySearchResult(Repository):\n         if \"score\" in attributes:  # pragma no branch\n             self._score = self._makeFloatAttribute(attributes[\"score\"])\n         if \"text_matches\" in attributes:  # pragma no branch\n-            self._text_matches = self._makeDictAttribute(attributes[\"text_matches\"])\n\\ No newline at end of file\n+            self._text_matches = self._makeDictAttribute(attributes[\"text_matches\"])\ndiff --git a/github/SecretScanAlert.py b/github/SecretScanAlert.py\nindex 9596dea..d4c3cde 100644\n--- a/github/SecretScanAlert.py\n+++ b/github/SecretScanAlert.py\n@@ -25,10 +25,9 @@ from __future__ import annotations\n from datetime import datetime\n from typing import TYPE_CHECKING, Any\n \n-\n-from github.GithubObject import Attribute, NonCompletableGithubObject, NotSet\n import github.NamedUser\n import github.SecretScanAlertInstance\n+from github.GithubObject import Attribute, NonCompletableGithubObject, NotSet\n \n if TYPE_CHECKING:\n     from github.NamedUser import NamedUser\n@@ -75,15 +74,15 @@ class SecretScanAlert(NonCompletableGithubObject):\n \n     def __repr__(self) -> str:\n         return self.get__repr__({\"number\": self.number})\n-    \n+\n     @property\n     def created_at(self) -> datetime:\n         return self._created_at.value\n-    \n+\n     @property\n     def first_location_detected(self) -> SecretScanAlertInstance:\n         return self._first_location_detected.value\n-    \n+\n     @property\n     def has_more_locations(self) -> bool:\n         return self._has_more_locations.value\n@@ -91,7 +90,7 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def html_url(self) -> str:\n         return self._html_url.value\n-    \n+\n     @property\n     def is_base64_encoded(self) -> bool:\n         return self._is_base64_encoded.value\n@@ -107,11 +106,11 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def number(self) -> int:\n         return self._number.value\n-    \n+\n     @property\n     def publicly_leaked(self) -> bool:\n         return self._publicly_leaked.value\n-    \n+\n     @property\n     def push_protection_bypassed(self) -> bool:\n         return self._push_protection_bypassed.value\n@@ -123,27 +122,27 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def push_protection_bypassed_by(self) -> NamedUser | None:\n         return self._push_protection_bypassed_by.value\n-    \n+\n     @property\n     def push_protection_bypass_request_comment(self) -> str | None:\n         return self._push_protection_bypass_request_comment.value\n-    \n+\n     @property\n     def push_protection_bypass_request_html_url(self) -> str | None:\n         return self._push_protection_bypass_request_html_url.value\n-    \n+\n     @property\n     def push_protection_bypass_request_reviewer(self) -> NamedUser | None:\n         return self._push_protection_bypass_request_reviewer.value\n-    \n+\n     @property\n     def push_protection_bypass_request_reviewer_comment(self) -> str | None:\n         return self._push_protection_bypass_request_reviewer_comment.value\n-    \n+\n     @property\n     def resolution(self) -> str | None:\n         return self._resolution.value\n-    \n+\n     @property\n     def resolution_comment(self) -> str | None:\n         return self._resolution_comment.value\n@@ -151,11 +150,11 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def resolved_at(self) -> datetime | None:\n         return self._resolved_at.value\n-    \n+\n     @property\n     def resolved_by(self) -> NamedUser | None:\n         return self._resolved_by.value\n-    \n+\n     @property\n     def secret(self) -> str:\n         return self._secret.value\n@@ -163,7 +162,7 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def secret_type(self) -> str:\n         return self._secret_type.value\n-    \n+\n     @property\n     def secret_type_display_name(self) -> str:\n         return self._secret_type_display_name.value\n@@ -171,7 +170,7 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def state(self) -> str:\n         return self._state.value\n-    \n+\n     @property\n     def updated_at(self) -> datetime:\n         return self._updated_at.value\n@@ -179,7 +178,7 @@ class SecretScanAlert(NonCompletableGithubObject):\n     @property\n     def url(self) -> str:\n         return self._url.value\n-    \n+\n     @property\n     def validity(self) -> str:\n         return self._validity.value\n@@ -188,7 +187,9 @@ class SecretScanAlert(NonCompletableGithubObject):\n         if \"created_at\" in attributes:\n             self._created_at = self._makeDatetimeAttribute(attributes[\"created_at\"])\n         if \"first_location_detected\" in attributes:\n-            self._first_location_detected = self._makeClassAttribute(github.SecretScanAlertInstance.SecretScanAlertInstance, attributes[\"first_location_detected\"])\n+            self._first_location_detected = self._makeClassAttribute(\n+                github.SecretScanAlertInstance.SecretScanAlertInstance, attributes[\"first_location_detected\"]\n+            )\n         if \"has_more_locations\" in attributes:\n             self._has_more_locations = self._makeBoolAttribute(attributes[\"has_more_locations\"])\n         if \"html_url\" in attributes:\n@@ -208,15 +209,25 @@ class SecretScanAlert(NonCompletableGithubObject):\n         if \"push_protection_bypassed_at\" in attributes:\n             self._push_protection_bypassed_at = self._makeDatetimeAttribute(attributes[\"push_protection_bypassed_at\"])\n         if \"push_protection_bypassed_by\" in attributes:\n-            self._push_protection_bypassed_by = self._makeClassAttribute(github.NamedUser.NamedUser, attributes[\"push_protection_bypassed_by\"])\n+            self._push_protection_bypassed_by = self._makeClassAttribute(\n+                github.NamedUser.NamedUser, attributes[\"push_protection_bypassed_by\"]\n+            )\n         if \"push_protection_bypass_request_comment\" in attributes:\n-            self._push_protection_bypass_request_comment = self._makeStringAttribute(attributes[\"push_protection_bypass_request_comment\"])\n+            self._push_protection_bypass_request_comment = self._makeStringAttribute(\n+                attributes[\"push_protection_bypass_request_comment\"]\n+            )\n         if \"push_protection_bypass_request_html_url\" in attributes:\n-            self._push_protection_bypass_request_html_url = self._makeStringAttribute(attributes[\"push_protection_bypass_request_html_url\"])\n+            self._push_protection_bypass_request_html_url = self._makeStringAttribute(\n+                attributes[\"push_protection_bypass_request_html_url\"]\n+            )\n         if \"_push_protection_bypass_request_reviewer\" in attributes:\n-            self._push_protection_bypass_request_reviewer = self._makeClassAttribute(github.NamedUser.NamedUser, attributes[\"_push_protection_bypass_request_reviewer\"])\n+            self._push_protection_bypass_request_reviewer = self._makeClassAttribute(\n+                github.NamedUser.NamedUser, attributes[\"_push_protection_bypass_request_reviewer\"]\n+            )\n         if \"push_protection_bypass_request_reviewer_comment\" in attributes:\n-            self._push_protection_bypass_request_reviewer_comment = self._makeStringAttribute(attributes[\"push_protection_bypass_request_reviewer_comment\"])\n+            self._push_protection_bypass_request_reviewer_comment = self._makeStringAttribute(\n+                attributes[\"push_protection_bypass_request_reviewer_comment\"]\n+            )\n         if \"resolution\" in attributes:\n             self._resolution = self._makeStringAttribute(attributes[\"resolution\"])\n         if \"resolution_comment\" in attributes:\ndiff --git a/github/SecretScanAlertInstance.py b/github/SecretScanAlertInstance.py\nindex d22391d..7727db2 100644\n--- a/github/SecretScanAlertInstance.py\n+++ b/github/SecretScanAlertInstance.py\n@@ -64,15 +64,15 @@ class SecretScanAlertInstance(NonCompletableGithubObject):\n     @property\n     def blob_sha(self) -> str:\n         return self._blob_sha.value\n-    \n+\n     @property\n     def blob_url(self) -> str:\n         return self._blob_url.value\n-    \n+\n     @property\n     def commit_sha(self) -> str:\n         return self._commit_sha.value\n-    \n+\n     @property\n     def commit_url(self) -> str:\n         return self._commit_url.value\n"
    },
    {
        "id": 283,
        "sha_fail": "6033729f8ed6df96607e43cb8739cd481b895519",
        "diff": "diff --git a/github/Repository.py b/github/Repository.py\nindex 660f796..5241a40 100644\n--- a/github/Repository.py\n+++ b/github/Repository.py\n@@ -1585,9 +1585,9 @@ class Repository(CompletableGithubObject):\n         assert isinstance(tag_name, str), tag_name\n         assert isinstance(previous_tag_name, str) or is_optional(previous_tag_name, str), previous_tag_name\n         assert isinstance(target_commitish, str) or is_optional(target_commitish, str), target_commitish\n-        assert isinstance(configuration_file_path, str) or is_optional(\n-            configuration_file_path, str\n-        ), configuration_file_path\n+        assert isinstance(configuration_file_path, str) or is_optional(configuration_file_path, str), (\n+            configuration_file_path\n+        )\n \n         post_parameters = NotSet.remove_unset_items(\n             {\n@@ -4066,9 +4066,9 @@ class Repository(CompletableGithubObject):\n         :param runner: int or :class:`github.SelfHostedActionsRunner.SelfHostedActionsRunner`\n         :rtype: bool\n         \"\"\"\n-        assert isinstance(runner, github.SelfHostedActionsRunner.SelfHostedActionsRunner) or isinstance(\n-            runner, int\n-        ), runner\n+        assert isinstance(runner, github.SelfHostedActionsRunner.SelfHostedActionsRunner) or isinstance(runner, int), (\n+            runner\n+        )\n \n         if isinstance(runner, github.SelfHostedActionsRunner.SelfHostedActionsRunner):\n             runner = runner.id\n@@ -4086,7 +4086,8 @@ class Repository(CompletableGithubObject):\n         assert is_autolink or isinstance(autolink, int), autolink\n \n         status, _, _ = self._requester.requestJson(\n-            \"DELETE\", f\"{self.url}/autolinks/{autolink.id if is_autolink else autolink}\"  # type: ignore\n+            \"DELETE\",\n+            f\"{self.url}/autolinks/{autolink.id if is_autolink else autolink}\",  # type: ignore\n         )\n         return status == 204\n \n@@ -4307,9 +4308,9 @@ class Repository(CompletableGithubObject):\n         allowed_severities = [\"critical\", \"high\", \"medium\", \"low\", \"warning\", \"note\", \"error\"]\n         assert is_optional(tool_name, str), tool_name\n         assert is_optional(tool_guid, str), tool_guid\n-        assert (\n-            tool_name is NotSet or tool_guid is NotSet\n-        ), \"You can specify the tool by using either tool_guid or tool_name, but not both.\"\n+        assert tool_name is NotSet or tool_guid is NotSet, (\n+            \"You can specify the tool by using either tool_guid or tool_name, but not both.\"\n+        )\n         assert is_optional(ref, str), ref\n         assert is_optional(pr, int), pr\n         assert sort in allowed_sorts + [NotSet], f\"Sort can be one of {', '.join(allowed_sorts)}\"\n@@ -4387,9 +4388,9 @@ class Repository(CompletableGithubObject):\n         # assert secret_type in allowed_secret_types + [NotSet], \\\n         # \"Secret_type can be one of the tokens listed on \\\n         # https://docs.github.com/en/code-security/secret-scanning/introduction/supported-secret-scanning-patterns#supported-secrets\"\n-        assert resolution in allowed_resolutions + [\n-            NotSet\n-        ], f\"Resolution can be one of {', '.join(allowed_resolutions)}\"\n+        assert resolution in allowed_resolutions + [NotSet], (\n+            f\"Resolution can be one of {', '.join(allowed_resolutions)}\"\n+        )\n         assert sort in allowed_sorts + [NotSet], f\"Sort can be one of {', '.join(allowed_sorts)}\"\n         assert direction in allowed_directions + [NotSet], f\"Direction can be one of {', '.join(allowed_directions)}\"\n         assert validity in allowed_validities + [NotSet], f\"Validity can be one of {', '.join(allowed_validities)}\"\n@@ -4605,7 +4606,9 @@ class Repository(CompletableGithubObject):\n                 \"no_bandwidth\",\n                 \"not_used\",\n                 \"tolerable_risk\",\n-            ], \"Dismissed reason can be one of ['fix_started', 'inaccurate', 'no_bandwidth', 'not_used', 'tolerable_risk']\"\n+            ], (\n+                \"Dismissed reason can be one of ['fix_started', 'inaccurate', 'no_bandwidth', 'not_used', 'tolerable_risk']\"\n+            )\n         assert is_optional(dismissed_comment, str), dismissed_comment\n         headers, data = self._requester.requestJsonAndCheck(\n             \"PATCH\",\n"
    },
    {
        "id": 284,
        "sha_fail": "e5cf9e0da88f8a9835011d89c15146d25806f619",
        "diff": "diff --git a/sphinx/ext/mathjax.py b/sphinx/ext/mathjax.py\nindex cf06670..193eb22 100644\n--- a/sphinx/ext/mathjax.py\n+++ b/sphinx/ext/mathjax.py\n@@ -111,7 +111,7 @@ def install_mathjax(\n             case str(config_filename):\n                 config_filepath = app.srcdir / config_filename\n                 if not config_filepath.exists():\n-                    raise ExtensionError(f'mathjax3_config file not found')\n+                    raise ExtensionError('mathjax3_config file not found')\n                 if not config_filepath.is_file():\n                     raise ExtensionError('mathjax3_config is not a file')\n                 if config_filepath.suffix != '.js':\n@@ -120,7 +120,7 @@ def install_mathjax(\n                     body = f.read()\n                 builder.add_js_file('', body=body)\n             case dict(config_dict):\n-                body = f\"window.MathJax = {json.dumps(config_dict)}\"\n+                body = f'window.MathJax = {json.dumps(config_dict)}'\n                 builder.add_js_file('', body=body)\n             case _:\n                 raise ExtensionError(\ndiff --git a/tests/test_extensions/test_ext_math.py b/tests/test_extensions/test_ext_math.py\nindex 529a591..629997e 100644\n--- a/tests/test_extensions/test_ext_math.py\n+++ b/tests/test_extensions/test_ext_math.py\n@@ -381,7 +381,7 @@ def test_mathjax3_config(app: SphinxTestApp) -> None:\n     testroot='ext-math',\n     confoverrides={\n         'extensions': ['sphinx.ext.mathjax'],\n-        'mathjax3_config': \"_static/custom_mathjax_config.js\",\n+        'mathjax3_config': '_static/custom_mathjax_config.js',\n     },\n )\n def test_mathjax3_js_config(app: SphinxTestApp) -> None:\n"
    },
    {
        "id": 285,
        "sha_fail": "3ce7b9cb7b89ed8330621bdb0acb809eb830d687",
        "diff": "diff --git a/binance/client.py b/binance/client.py\nindex 95d7619..836911f 100755\n--- a/binance/client.py\n+++ b/binance/client.py\n@@ -79,15 +79,19 @@ class Client(BaseClient):\n         if data is not None:\n             del kwargs[\"data\"]\n \n-        if signed and self.PRIVATE_KEY and data: # handle issues with signing using eddsa/rsa and POST requests\n+        if (\n+            signed and self.PRIVATE_KEY and data\n+        ):  # handle issues with signing using eddsa/rsa and POST requests\n             dict_data = Client.convert_to_dict(data)\n-            signature = dict_data[\"signature\"] if \"signature\" in dict_data else  None\n+            signature = dict_data[\"signature\"] if \"signature\" in dict_data else None\n             if signature:\n                 del dict_data[\"signature\"]\n             url_encoded_data = urlencode(dict_data)\n             data = f\"{url_encoded_data}&signature={signature}\"\n \n-        self.response = getattr(self.session, method)(uri, headers=headers, data=data, **kwargs)\n+        self.response = getattr(self.session, method)(\n+            uri, headers=headers, data=data, **kwargs\n+        )\n         return self._handle_response(self.response)\n \n     @staticmethod\n@@ -396,7 +400,9 @@ class Client(BaseClient):\n \n         \"\"\"\n         response = self._get(\"ticker/price\")\n-        if isinstance(response, list) and all(isinstance(item, dict) for item in response):\n+        if isinstance(response, list) and all(\n+            isinstance(item, dict) for item in response\n+        ):\n             return response\n         raise TypeError(\"Expected a list of dictionaries\")\n \n@@ -439,9 +445,7 @@ class Client(BaseClient):\n             data[\"symbol\"] = params[\"symbol\"]\n         elif \"symbols\" in params:\n             data[\"symbols\"] = params[\"symbols\"]\n-        return self._get(\n-            \"ticker/bookTicker\", data=data\n-        )\n+        return self._get(\"ticker/bookTicker\", data=data)\n \n     def get_order_book(self, **params) -> Dict:\n         \"\"\"Get the Order Book for the market\n@@ -540,9 +544,7 @@ class Client(BaseClient):\n         :raises: BinanceRequestException, BinanceAPIException\n \n         \"\"\"\n-        return self._get(\n-            \"historicalTrades\", data=params\n-        )\n+        return self._get(\"historicalTrades\", data=params)\n \n     def get_aggregate_trades(self, **params) -> Dict:\n         \"\"\"Get compressed, aggregate trades. Trades that fill at the time,\n@@ -1307,9 +1309,7 @@ class Client(BaseClient):\n         :raises: BinanceRequestException, BinanceAPIException\n \n         \"\"\"\n-        return self._get(\n-            \"ticker/bookTicker\", data=params\n-        )\n+        return self._get(\"ticker/bookTicker\", data=params)\n \n     # Account Endpoints\n \n@@ -3210,9 +3210,7 @@ class Client(BaseClient):\n         :raises: BinanceRequestException, BinanceAPIException\n \n         \"\"\"\n-        res = self._post(\n-            \"userDataStream\", False, data={}\n-        )\n+        res = self._post(\"userDataStream\", False, data={})\n         return res[\"listenKey\"]\n \n     def stream_keepalive(self, listenKey):\n@@ -3233,9 +3231,7 @@ class Client(BaseClient):\n \n         \"\"\"\n         params = {\"listenKey\": listenKey}\n-        return self._put(\n-            \"userDataStream\", False, data=params\n-        )\n+        return self._put(\"userDataStream\", False, data=params)\n \n     def stream_close(self, listenKey):\n         \"\"\"Close out a user data stream.\n@@ -7375,7 +7371,7 @@ class Client(BaseClient):\n         return self._request_futures_api(\"get\", \"continuousKlines\", data=params)\n \n     def futures_historical_klines(\n-        self, symbol: str, interval :str, start_str, end_str=None, limit=None\n+        self, symbol: str, interval: str, start_str, end_str=None, limit=None\n     ):\n         \"\"\"Get historical futures klines from Binance\n \n@@ -7504,18 +7500,14 @@ class Client(BaseClient):\n \n         https://developers.binance.com/docs/derivatives/usds-margined-futures/market-data/rest-api/Taker-BuySell-Volume\n         \"\"\"\n-        return self._request_futures_data_api(\n-            \"get\", \"takerlongshortRatio\", data=params\n-        )\n+        return self._request_futures_data_api(\"get\", \"takerlongshortRatio\", data=params)\n \n     def futures_basis(self, **params):\n         \"\"\"Get future basis of a specific symbol\n \n         https://developers.binance.com/docs/derivatives/usds-margined-futures/market-data/rest-api/Basis\n         \"\"\"\n-        return self._request_futures_data_api(\n-            \"get\", \"basis\", data=params\n-        )\n+        return self._request_futures_data_api(\"get\", \"basis\", data=params)\n \n     def futures_ticker(self, **params):\n         \"\"\"24 hour rolling window price change statistics.\n@@ -7803,7 +7795,6 @@ class Client(BaseClient):\n         params[\"type\"] = \"MARKET\"\n         return self._request_futures_api(\"post\", \"order\", True, data=params)\n \n-\n     def futures_limit_buy_order(self, **params):\n         \"\"\"Send in a new futures limit buy order.\n \n@@ -8112,7 +8103,7 @@ class Client(BaseClient):\n     def futures_coin_ping(self):\n         \"\"\"Test connectivity to the Rest API\n \n-       https://developers.binance.com/docs/derivatives/coin-margined-futures/market-data/rest-api\n+        https://developers.binance.com/docs/derivatives/coin-margined-futures/market-data/rest-api\n \n         \"\"\"\n         return self._request_futures_coin_api(\"get\", \"ping\")\n@@ -8251,28 +8242,36 @@ class Client(BaseClient):\n \n         https://developers.binance.com/docs/derivatives/coin-margined-futures/market-data/rest-api/Top-Trader-Long-Short-Ratio\n         \"\"\"\n-        return self._request_futures_coin_data_api(\"get\", \"topLongShortPositionRatio\", data=params)\n+        return self._request_futures_coin_data_api(\n+            \"get\", \"topLongShortPositionRatio\", data=params\n+        )\n \n     def futures_coin_top_longshort_account_ratio(self, **params):\n         \"\"\"Get present long to short ratio for top positions of a specific symbol.\n \n         https://developers.binance.com/docs/derivatives/coin-margined-futures/market-data/rest-api/Top-Long-Short-Account-Ratio\n         \"\"\"\n-        return self._request_futures_coin_data_api(\"get\", \"topLongShortAccountRatio\", data=params)\n+        return self._request_futures_coin_data_api(\n+            \"get\", \"topLongShortAccountRatio\", data=params\n+        )\n \n     def futures_coin_global_longshort_ratio(self, **params):\n         \"\"\"Get present long to short ratio for top positions of a specific symbol.\n \n         https://developers.binance.com/docs/derivatives/coin-margined-futures/market-data/rest-api/Long-Short-Ratio\n         \"\"\"\n-        return self._request_futures_coin_data_api(\"get\", \"globalLongShortAccountRatio\", data=params)\n+        return self._request_futures_coin_data_api(\n+            \"get\", \"globalLongShortAccountRatio\", data=params\n+        )\n \n     def futures_coin_taker_buy_sell_volume(self, **params):\n         \"\"\"Get present long to short ratio for top positions of a specific symbol.\n \n         https://developers.binance.com/docs/derivatives/coin-margined-futures/market-data/rest-api/Taker-Buy-Sell-Volume\n         \"\"\"\n-        return self._request_futures_coin_data_api(\"get\", \"takerBuySellVol\", data=params)\n+        return self._request_futures_coin_data_api(\n+            \"get\", \"takerBuySellVol\", data=params\n+        )\n \n     def futures_coin_basis(self, **params):\n         \"\"\"Get future basis of a specific symbol\n@@ -8336,7 +8335,7 @@ class Client(BaseClient):\n         return self._request_margin_api(\"get\", \"asset/transfer\", True, data=params)\n \n     def funding_wallet(self, **params):\n-        \"\"\" Query Funding Wallet\n+        \"\"\"Query Funding Wallet\n \n         https://developers.binance.com/docs/wallet/asset/funding-wallet\n \n@@ -8346,7 +8345,7 @@ class Client(BaseClient):\n         )\n \n     def get_user_asset(self, **params):\n-        \"\"\" Get user assets, just for positive data\n+        \"\"\"Get user assets, just for positive data\n \n         https://developers.binance.com/docs/wallet/asset/user-assets\n \n@@ -9201,7 +9200,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"get\", \"exerciseRecord\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"get\", \"exerciseRecord\", signed=True, data=params\n+        )\n \n     def options_bill(self, **params):\n         \"\"\"Account funding flow (USER_DATA)\n@@ -9907,7 +9908,9 @@ class Client(BaseClient):\n         :returns: API response\n \n         \"\"\"\n-        return self._request_papi_api(\"get\", \"rateLimit/order\", signed=True, data=params)\n+        return self._request_papi_api(\n+            \"get\", \"rateLimit/order\", signed=True, data=params\n+        )\n \n     def papi_stream_get_listen_key(self):\n         \"\"\"Start a new user data stream for Portfolio Margin account.\n@@ -10354,7 +10357,6 @@ class Client(BaseClient):\n             \"get\", \"portfolio/interest-history\", signed=True, data=params\n         )\n \n-\n     def papi_get_portfolio_negative_balance_exchange_record(self, **params):\n         \"\"\"Query user negative balance auto exchange record.\n \n@@ -10367,7 +10369,10 @@ class Client(BaseClient):\n \n         \"\"\"\n         return self._request_papi_api(\n-            \"get\", \"portfolio/negative-balance-exchange-record\", signed=True, data=params\n+            \"get\",\n+            \"portfolio/negative-balance-exchange-record\",\n+            signed=True,\n+            data=params,\n         )\n \n     def papi_fund_auto_collection(self, **params):\n@@ -14085,7 +14090,9 @@ class Client(BaseClient):\n             - Only VIP users can query this endpoint\n             - Weight: 200\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"futures/data/histDataLink\", signed=True, data=params)\n+        return self._request_margin_api(\n+            \"get\", \"futures/data/histDataLink\", signed=True, data=params\n+        )\n \n     def margin_v1_get_loan_vip_ongoing_orders(self, **params):\n         \"\"\"\n@@ -14099,7 +14106,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/vip/ongoing/orders\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/vip/ongoing/orders\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_mining_payment_other(self, **params):\n         \"\"\"\n@@ -14113,7 +14122,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/payment/other\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/payment/other\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_income_asyn_id(self, **params):\n         \"\"\"\n@@ -14127,7 +14138,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"income/asyn/id\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"income/asyn/id\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_simple_earn_flexible_history_subscription_record(self, **params):\n         \"\"\"\n@@ -14141,7 +14154,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"simple-earn/flexible/history/subscriptionRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/flexible/history/subscriptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_lending_auto_invest_one_off(self, **params):\n         \"\"\"\n@@ -14153,7 +14172,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/auto-invest/one-off\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"lending/auto-invest/one-off\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_broker_sub_account_api_commission_coin_futures(self, **params):\n         \"\"\"\n@@ -14167,7 +14188,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/commission/coinFutures\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/commission/coinFutures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def v3_post_order_list_otoco(self, **params):\n         \"\"\"\n@@ -14179,7 +14206,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"post\", \"orderList/otoco\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"post\", \"orderList/otoco\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def futures_v1_get_order_asyn(self, **params):\n         \"\"\"\n@@ -14193,7 +14222,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"order/asyn\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"order/asyn\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_asset_custody_transfer_history(self, **params):\n         \"\"\"\n@@ -14207,7 +14238,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"asset/custody/transfer-history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"asset/custody/transfer-history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_broker_sub_account_blvt(self, **params):\n         \"\"\"\n@@ -14219,7 +14252,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccount/blvt\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"broker/subAccount/blvt\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_sol_staking_sol_redeem(self, **params):\n         \"\"\"\n@@ -14233,7 +14268,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sol-staking/sol/redeem\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"sol-staking/sol/redeem\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_get_countdown_cancel_all(self, **params):\n         \"\"\"\n@@ -14247,7 +14284,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"get\", \"countdownCancelAll\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"get\", \"countdownCancelAll\", signed=True, data=params\n+        )\n \n     def margin_v1_get_margin_trade_coeff(self, **params):\n         \"\"\"\n@@ -14261,7 +14300,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"margin/tradeCoeff\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"margin/tradeCoeff\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_order_amendment(self, **params):\n         \"\"\"\n@@ -14275,7 +14316,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"orderAmendment\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"orderAmendment\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_margin_available_inventory(self, **params):\n         \"\"\"\n@@ -14289,7 +14332,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"margin/available-inventory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"margin/available-inventory\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_account_api_restrictions_ip_restriction_ip_list(self, **params):\n         \"\"\"\n@@ -14301,7 +14346,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"account/apiRestrictions/ipRestriction/ipList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"account/apiRestrictions/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_get_eth_staking_account(self, **params):\n         \"\"\"\n@@ -14315,7 +14366,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"eth-staking/account\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"eth-staking/account\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_get_loan_income(self, **params):\n         \"\"\"\n@@ -14329,7 +14382,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/income\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/income\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_pm_account_info(self, **params):\n         \"\"\"\n@@ -14343,7 +14398,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"pmAccountInfo\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"pmAccountInfo\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_managed_subaccount_query_trans_log_for_investor(self, **params):\n         \"\"\"\n@@ -14357,7 +14414,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/queryTransLogForInvestor\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/queryTransLogForInvestor\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_dci_product_auto_compound_edit_status(self, **params):\n         \"\"\"\n@@ -14371,7 +14434,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"dci/product/auto_compound/edit-status\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"dci/product/auto_compound/edit-status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_v1_get_trade_asyn(self, **params):\n         \"\"\"\n@@ -14385,7 +14454,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"trade/asyn\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"trade/asyn\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_loan_vip_request_interest_rate(self, **params):\n         \"\"\"\n@@ -14399,7 +14470,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/vip/request/interestRate\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/vip/request/interestRate\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_funding_info(self, **params):\n         \"\"\"\n@@ -14413,7 +14486,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"fundingInfo\", signed=False, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"fundingInfo\", signed=False, data=params, version=1\n+        )\n \n     def margin_v2_get_loan_flexible_repay_rate(self, **params):\n         \"\"\"\n@@ -14427,7 +14502,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/repay/rate\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"loan/flexible/repay/rate\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_get_lending_auto_invest_plan_id(self, **params):\n         \"\"\"\n@@ -14439,7 +14516,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/plan/id\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"lending/auto-invest/plan/id\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_loan_adjust_ltv(self, **params):\n         \"\"\"\n@@ -14451,7 +14530,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/adjust/ltv\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/adjust/ltv\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_mining_statistics_user_status(self, **params):\n         \"\"\"\n@@ -14465,7 +14546,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/statistics/user/status\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/statistics/user/status\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_transfer_futures(self, **params):\n         \"\"\"\n@@ -14479,7 +14562,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/transfer/futures\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/transfer/futures\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_algo_spot_new_order_twap(self, **params):\n         \"\"\"\n@@ -14493,7 +14578,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"algo/spot/newOrderTwap\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"algo/spot/newOrderTwap\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_lending_auto_invest_target_asset_list(self, **params):\n         \"\"\"\n@@ -14505,7 +14592,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/target-asset/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/target-asset/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_capital_deposit_address_list(self, **params):\n         \"\"\"\n@@ -14519,7 +14612,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"capital/deposit/address/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"capital/deposit/address/list\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_broker_sub_account_bnb_burn_margin_interest(self, **params):\n         \"\"\"\n@@ -14533,7 +14628,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccount/bnbBurn/marginInterest\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccount/bnbBurn/marginInterest\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_post_loan_flexible_repay(self, **params):\n         \"\"\"\n@@ -14547,7 +14648,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/flexible/repay\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"post\", \"loan/flexible/repay\", signed=True, data=params, version=2\n+        )\n \n     def margin_v2_get_loan_flexible_loanable_data(self, **params):\n         \"\"\"\n@@ -14561,7 +14664,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/loanable/data\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"loan/flexible/loanable/data\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_post_broker_sub_account_api_permission(self, **params):\n         \"\"\"\n@@ -14575,7 +14680,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/permission\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/permission\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account_api(self, **params):\n         \"\"\"\n@@ -14589,7 +14700,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"broker/subAccountApi\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_dci_product_positions(self, **params):\n         \"\"\"\n@@ -14603,7 +14716,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"dci/product/positions\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"dci/product/positions\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_convert_limit_cancel_order(self, **params):\n         \"\"\"\n@@ -14617,7 +14732,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"convert/limit/cancelOrder\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"convert/limit/cancelOrder\", signed=True, data=params, version=1\n+        )\n \n     def v3_post_order_list_oto(self, **params):\n         \"\"\"\n@@ -14629,7 +14746,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"post\", \"orderList/oto\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"post\", \"orderList/oto\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_get_mining_hash_transfer_config_details_list(self, **params):\n         \"\"\"\n@@ -14643,7 +14762,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/hash-transfer/config/details/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"mining/hash-transfer/config/details/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_mining_hash_transfer_profit_details(self, **params):\n         \"\"\"\n@@ -14657,7 +14782,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/hash-transfer/profit/details\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"mining/hash-transfer/profit/details\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_sub_account(self, **params):\n         \"\"\"\n@@ -14671,7 +14802,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/subAccount\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_portfolio_balance(self, **params):\n         \"\"\"\n@@ -14685,7 +14818,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"portfolio/balance\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"portfolio/balance\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_sub_account_eoptions_enable(self, **params):\n         \"\"\"\n@@ -14699,7 +14834,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sub-account/eoptions/enable\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"sub-account/eoptions/enable\", signed=True, data=params, version=1\n+        )\n \n     def papi_v1_post_ping(self, **params):\n         \"\"\"\n@@ -14711,7 +14848,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_papi_api(\"post\", \"ping\", signed=True, data=params, version=1)\n+        return self._request_papi_api(\n+            \"post\", \"ping\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_loan_loanable_data(self, **params):\n         \"\"\"\n@@ -14723,7 +14862,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/loanable/data\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/loanable/data\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_eth_staking_wbeth_unwrap(self, **params):\n         \"\"\"\n@@ -14735,7 +14876,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"eth-staking/wbeth/unwrap\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"eth-staking/wbeth/unwrap\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_eth_staking_eth_history_staking_history(self, **params):\n         \"\"\"\n@@ -14749,7 +14892,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"eth-staking/eth/history/stakingHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"eth-staking/eth/history/stakingHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_staking_staking_record(self, **params):\n         \"\"\"\n@@ -14761,7 +14910,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"staking/stakingRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"staking/stakingRecord\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_rebate_recent_record(self, **params):\n         \"\"\"\n@@ -14775,7 +14926,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/rebate/recentRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/rebate/recentRecord\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_loan_vip_collateral_account(self, **params):\n         \"\"\"\n@@ -14789,7 +14942,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/vip/collateral/account\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/vip/collateral/account\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_algo_spot_open_orders(self, **params):\n         \"\"\"\n@@ -14803,7 +14958,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"algo/spot/openOrders\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"algo/spot/openOrders\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_loan_repay(self, **params):\n         \"\"\"\n@@ -14815,7 +14972,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/repay\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/repay\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_funding_info(self, **params):\n         \"\"\"\n@@ -14829,7 +14988,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"fundingInfo\", signed=False, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"fundingInfo\", signed=False, data=params, version=1\n+        )\n \n     def margin_v1_get_margin_leverage_bracket(self, **params):\n         \"\"\"\n@@ -14843,7 +15004,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"margin/leverageBracket\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"margin/leverageBracket\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_get_portfolio_collateral_rate(self, **params):\n         \"\"\"\n@@ -14857,7 +15020,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"portfolio/collateralRate\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"portfolio/collateralRate\", signed=True, data=params, version=2\n+        )\n \n     def margin_v2_post_loan_flexible_adjust_ltv(self, **params):\n         \"\"\"\n@@ -14871,7 +15036,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/flexible/adjust/ltv\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"post\", \"loan/flexible/adjust/ltv\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_get_convert_order_status(self, **params):\n         \"\"\"\n@@ -14885,7 +15052,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"convert/orderStatus\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"convert/orderStatus\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_sub_account_api_ip_restriction(self, **params):\n         \"\"\"\n@@ -14899,7 +15068,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_dci_product_subscribe(self, **params):\n         \"\"\"\n@@ -14913,7 +15088,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"dci/product/subscribe\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"dci/product/subscribe\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_income_asyn_id(self, **params):\n         \"\"\"\n@@ -14927,7 +15104,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"income/asyn/id\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"income/asyn/id\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_post_countdown_cancel_all(self, **params):\n         \"\"\"\n@@ -14941,7 +15120,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"post\", \"countdownCancelAll\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"post\", \"countdownCancelAll\", signed=True, data=params\n+        )\n \n     def margin_v1_post_mining_hash_transfer_config_cancel(self, **params):\n         \"\"\"\n@@ -14955,7 +15136,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"mining/hash-transfer/config/cancel\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"mining/hash-transfer/config/cancel\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_sub_account_deposit_hist(self, **params):\n         \"\"\"\n@@ -14969,7 +15156,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/depositHist\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/subAccount/depositHist\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_mining_payment_list(self, **params):\n         \"\"\"\n@@ -14983,7 +15172,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/payment/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/payment/list\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_pm_account_info(self, **params):\n         \"\"\"\n@@ -14997,7 +15188,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"pmAccountInfo\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"pmAccountInfo\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_adl_quantile(self, **params):\n         \"\"\"\n@@ -15011,7 +15204,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"adlQuantile\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"adlQuantile\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_get_income_asyn_id(self, **params):\n         \"\"\"\n@@ -15025,7 +15220,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"get\", \"income/asyn/id\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"get\", \"income/asyn/id\", signed=True, data=params\n+        )\n \n     def v3_post_cancel_replace(self, **params):\n         \"\"\"\n@@ -15037,7 +15234,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"post\", \"cancelReplace\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"post\", \"cancelReplace\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_post_account_enable_fast_withdraw_switch(self, **params):\n         \"\"\"\n@@ -15051,7 +15250,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"account/enableFastWithdrawSwitch\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"account/enableFastWithdrawSwitch\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_transfer_futures(self, **params):\n         \"\"\"\n@@ -15065,7 +15270,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/transfer/futures\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"broker/transfer/futures\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_sol_staking_sol_stake(self, **params):\n         \"\"\"\n@@ -15079,7 +15286,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sol-staking/sol/stake\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"sol-staking/sol/stake\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_loan_borrow(self, **params):\n         \"\"\"\n@@ -15091,7 +15300,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/borrow\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/borrow\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_managed_subaccount_info(self, **params):\n         \"\"\"\n@@ -15105,7 +15316,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/info\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"managed-subaccount/info\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_lending_auto_invest_plan_edit_status(self, **params):\n         \"\"\"\n@@ -15117,7 +15330,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/auto-invest/plan/edit-status\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"lending/auto-invest/plan/edit-status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_sol_staking_sol_history_unclaimed_rewards(self, **params):\n         \"\"\"\n@@ -15131,7 +15350,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/history/unclaimedRewards\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/unclaimedRewards\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_asset_convert_transfer_query_by_page(self, **params):\n         \"\"\"\n@@ -15143,7 +15368,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"asset/convert-transfer/queryByPage\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"asset/convert-transfer/queryByPage\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_sol_staking_sol_history_boost_rewards_history(self, **params):\n         \"\"\"\n@@ -15157,7 +15388,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/history/boostRewardsHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/boostRewardsHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_lending_auto_invest_one_off_status(self, **params):\n         \"\"\"\n@@ -15169,7 +15406,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/one-off/status\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/one-off/status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account(self, **params):\n         \"\"\"\n@@ -15183,7 +15426,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccount\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"broker/subAccount\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page(self, **params):\n         \"\"\"\n@@ -15197,7 +15442,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"asset/ledger-transfer/cloud-mining/queryByPage\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"asset/ledger-transfer/cloud-mining/queryByPage\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_mining_pub_coin_list(self, **params):\n         \"\"\"\n@@ -15211,7 +15462,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/pub/coinList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/pub/coinList\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_get_loan_flexible_repay_history(self, **params):\n         \"\"\"\n@@ -15225,7 +15478,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/repay/history\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"loan/flexible/repay/history\", signed=True, data=params, version=2\n+        )\n \n     def v3_post_sor_order(self, **params):\n         \"\"\"\n@@ -15237,7 +15492,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"post\", \"sor/order\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"post\", \"sor/order\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_post_capital_deposit_credit_apply(self, **params):\n         \"\"\"\n@@ -15251,7 +15508,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"capital/deposit/credit-apply\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"capital/deposit/credit-apply\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_put_batch_order(self, **params):\n         \"\"\"\n@@ -15263,7 +15522,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"put\", \"batchOrder\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"put\", \"batchOrder\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_mining_statistics_user_list(self, **params):\n         \"\"\"\n@@ -15277,7 +15538,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/statistics/user/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/statistics/user/list\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_post_batch_order(self, **params):\n         \"\"\"\n@@ -15289,7 +15552,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"post\", \"batchOrder\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"post\", \"batchOrder\", signed=True, data=params, version=1\n+        )\n \n     def v3_get_ticker_trading_day(self, **params):\n         \"\"\"\n@@ -15301,7 +15566,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"get\", \"ticker/tradingDay\", signed=False, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"get\", \"ticker/tradingDay\", signed=False, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_get_mining_worker_detail(self, **params):\n         \"\"\"\n@@ -15315,7 +15582,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/worker/detail\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/worker/detail\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_managed_subaccount_fetch_future_asset(self, **params):\n         \"\"\"\n@@ -15329,7 +15598,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/fetch-future-asset\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/fetch-future-asset\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_margin_rate_limit_order(self, **params):\n         \"\"\"\n@@ -15343,7 +15618,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"margin/rateLimit/order\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"margin/rateLimit/order\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_localentity_vasp(self, **params):\n         \"\"\"\n@@ -15357,7 +15634,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"localentity/vasp\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"localentity/vasp\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_sol_staking_sol_history_rate_history(self, **params):\n         \"\"\"\n@@ -15371,7 +15650,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/history/rateHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/rateHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account_api_ip_restriction(self, **params):\n         \"\"\"\n@@ -15383,7 +15668,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_transfer(self, **params):\n         \"\"\"\n@@ -15397,7 +15688,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/transfer\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/transfer\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_sol_staking_account(self, **params):\n         \"\"\"\n@@ -15411,7 +15704,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/account\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"sol-staking/account\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_account_info(self, **params):\n         \"\"\"\n@@ -15425,7 +15720,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"account/info\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"account/info\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_portfolio_repay_futures_switch(self, **params):\n         \"\"\"\n@@ -15439,7 +15736,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"portfolio/repay-futures-switch\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"portfolio/repay-futures-switch\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_loan_vip_borrow(self, **params):\n         \"\"\"\n@@ -15451,7 +15754,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/vip/borrow\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/vip/borrow\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_get_loan_flexible_ltv_adjustment_history(self, **params):\n         \"\"\"\n@@ -15465,7 +15770,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/ltv/adjustment/history\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"loan/flexible/ltv/adjustment/history\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n \n     def options_v1_delete_all_open_orders_by_underlying(self, **params):\n         \"\"\"\n@@ -15479,7 +15790,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"delete\", \"allOpenOrdersByUnderlying\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"delete\", \"allOpenOrdersByUnderlying\", signed=True, data=params\n+        )\n \n     def margin_v1_get_broker_sub_account_futures_summary(self, **params):\n         \"\"\"\n@@ -15491,7 +15804,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/futuresSummary\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/futuresSummary\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_sub_account_spot_summary(self, **params):\n         \"\"\"\n@@ -15505,7 +15824,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/spotSummary\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/subAccount/spotSummary\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_sub_account_blvt_enable(self, **params):\n         \"\"\"\n@@ -15517,7 +15838,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sub-account/blvt/enable\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"sub-account/blvt/enable\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_algo_spot_historical_orders(self, **params):\n         \"\"\"\n@@ -15531,7 +15854,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"algo/spot/historicalOrders\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"algo/spot/historicalOrders\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_loan_vip_repay_history(self, **params):\n         \"\"\"\n@@ -15545,8 +15870,10 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/vip/repay/history\", signed=True, data=params, version=1)\n-\n+        return self._request_margin_api(\n+            \"get\", \"loan/vip/repay/history\", signed=True, data=params, version=1\n+        )\n+\n     def margin_v1_get_loan_borrow_history(self, **params):\n         \"\"\"\n         Placeholder function for GET /sapi/v1/loan/borrow/history.\n@@ -15559,7 +15886,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/borrow/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/borrow/history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_lending_auto_invest_redeem(self, **params):\n         \"\"\"\n@@ -15571,7 +15900,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/auto-invest/redeem\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"lending/auto-invest/redeem\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_income_asyn(self, **params):\n         \"\"\"\n@@ -15585,7 +15916,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"income/asyn\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"income/asyn\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_managed_subaccount_deposit(self, **params):\n         \"\"\"\n@@ -15599,7 +15932,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"managed-subaccount/deposit\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"managed-subaccount/deposit\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_lending_daily_purchase(self, **params):\n         \"\"\"\n@@ -15611,7 +15946,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/daily/purchase\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"lending/daily/purchase\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_trade_asyn_id(self, **params):\n         \"\"\"\n@@ -15625,9 +15962,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"trade/asyn/id\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"trade/asyn/id\", signed=True, data=params, version=1\n+        )\n \n-    def margin_v1_delete_sub_account_sub_account_api_ip_restriction_ip_list(self, **params):\n+    def margin_v1_delete_sub_account_sub_account_api_ip_restriction_ip_list(\n+        self, **params\n+    ):\n         \"\"\"\n         Placeholder function for DELETE /sapi/v1/sub-account/subAccountApi/ipRestriction/ipList.\n         Note: This function was auto-generated. Any issue please open an issue on GitHub.\n@@ -15639,7 +15980,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"delete\", \"sub-account/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"delete\",\n+            \"sub-account/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_copy_trading_futures_user_status(self, **params):\n         \"\"\"\n@@ -15653,7 +16000,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"copyTrading/futures/userStatus\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"copyTrading/futures/userStatus\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_get_margin_account(self, **params):\n         \"\"\"\n@@ -15667,7 +16016,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"get\", \"marginAccount\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"get\", \"marginAccount\", signed=True, data=params\n+        )\n \n     def options_get_market_maker_protection_config(self, **params):\n         \"\"\"\n@@ -15728,7 +16079,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"localentity/withdraw/apply\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"localentity/withdraw/apply\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_asset_wallet_balance(self, **params):\n         \"\"\"\n@@ -15742,7 +16095,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"asset/wallet/balance\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"asset/wallet/balance\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_broker_transfer(self, **params):\n         \"\"\"\n@@ -15756,7 +16111,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/transfer\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"broker/transfer\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_lending_customized_fixed_purchase(self, **params):\n         \"\"\"\n@@ -15768,7 +16125,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/customizedFixed/purchase\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"lending/customizedFixed/purchase\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_algo_futures_new_order_twap(self, **params):\n         \"\"\"\n@@ -15782,7 +16145,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"algo/futures/newOrderTwap\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"algo/futures/newOrderTwap\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_post_eth_staking_eth_stake(self, **params):\n         \"\"\"\n@@ -15796,7 +16161,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"eth-staking/eth/stake\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"post\", \"eth-staking/eth/stake\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_post_loan_flexible_repay_history(self, **params):\n         \"\"\"\n@@ -15808,7 +16175,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/flexible/repay/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/flexible/repay/history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_lending_auto_invest_index_info(self, **params):\n         \"\"\"\n@@ -15820,7 +16189,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/index/info\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"lending/auto-invest/index/info\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_sol_staking_sol_history_redemption_history(self, **params):\n         \"\"\"\n@@ -15834,7 +16205,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/history/redemptionHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/redemptionHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_rebate_futures_recent_record(self, **params):\n         \"\"\"\n@@ -15848,7 +16225,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/rebate/futures/recentRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/rebate/futures/recentRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v3_get_broker_sub_account_futures_summary(self, **params):\n         \"\"\"\n@@ -15862,7 +16245,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/futuresSummary\", signed=True, data=params, version=3)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/futuresSummary\",\n+            signed=True,\n+            data=params,\n+            version=3,\n+        )\n \n     def margin_v1_get_lending_auto_invest_target_asset_roi_list(self, **params):\n         \"\"\"\n@@ -15874,7 +16263,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/target-asset/roi/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/target-asset/roi/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_universal_transfer(self, **params):\n         \"\"\"\n@@ -15888,7 +16283,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/universalTransfer\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/universalTransfer\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_put_batch_orders(self, **params):\n         \"\"\"\n@@ -15902,7 +16299,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"put\", \"batchOrders\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"put\", \"batchOrders\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_post_countdown_cancel_all_heart_beat(self, **params):\n         \"\"\"\n@@ -15916,7 +16315,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"post\", \"countdownCancelAllHeartBeat\", signed=True, data=params)\n+        return self._request_options_api(\n+            \"post\", \"countdownCancelAllHeartBeat\", signed=True, data=params\n+        )\n \n     def margin_v1_get_loan_collateral_data(self, **params):\n         \"\"\"\n@@ -15928,7 +16329,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/collateral/data\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/collateral/data\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_loan_repay_history(self, **params):\n         \"\"\"\n@@ -15942,7 +16345,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/repay/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/repay/history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_convert_limit_place_order(self, **params):\n         \"\"\"\n@@ -15956,7 +16361,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"convert/limit/placeOrder\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"convert/limit/placeOrder\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_convert_exchange_info(self, **params):\n         \"\"\"\n@@ -15970,7 +16377,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1\n+        )\n \n     def v3_get_all_order_list(self, **params):\n         \"\"\"\n@@ -15982,7 +16391,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"get\", \"allOrderList\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"get\", \"allOrderList\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list(self, **params):\n         \"\"\"\n@@ -15996,7 +16407,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"delete\", \"broker/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"delete\",\n+            \"broker/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_sub_account_virtual_sub_account(self, **params):\n         \"\"\"\n@@ -16010,7 +16427,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sub-account/virtualSubAccount\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"sub-account/virtualSubAccount\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_put_localentity_deposit_provide_info(self, **params):\n         \"\"\"\n@@ -16024,7 +16443,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"put\", \"localentity/deposit/provide-info\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"put\",\n+            \"localentity/deposit/provide-info\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_portfolio_mint(self, **params):\n         \"\"\"\n@@ -16038,7 +16463,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"portfolio/mint\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"portfolio/mint\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_order_amendment(self, **params):\n         \"\"\"\n@@ -16052,7 +16479,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"orderAmendment\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"orderAmendment\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_sol_staking_sol_claim(self, **params):\n         \"\"\"\n@@ -16066,7 +16495,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sol-staking/sol/claim\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"sol-staking/sol/claim\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_lending_daily_redeem(self, **params):\n         \"\"\"\n@@ -16078,7 +16509,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/daily/redeem\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"lending/daily/redeem\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_mining_hash_transfer_config(self, **params):\n         \"\"\"\n@@ -16092,7 +16525,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"mining/hash-transfer/config\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"mining/hash-transfer/config\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_lending_auto_invest_rebalance_history(self, **params):\n         \"\"\"\n@@ -16104,7 +16539,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/rebalance/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/rebalance/history\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_loan_repay_collateral_rate(self, **params):\n         \"\"\"\n@@ -16116,7 +16557,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/repay/collateral/rate\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/repay/collateral/rate\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_income_asyn(self, **params):\n         \"\"\"\n@@ -16130,7 +16573,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"income/asyn\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"income/asyn\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_mining_payment_uid(self, **params):\n         \"\"\"\n@@ -16144,7 +16589,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/payment/uid\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/payment/uid\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_get_loan_flexible_borrow_history(self, **params):\n         \"\"\"\n@@ -16158,7 +16605,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/borrow/history\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"loan/flexible/borrow/history\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_get_capital_contract_convertible_coins(self, **params):\n         \"\"\"\n@@ -16170,9 +16619,17 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"capital/contract/convertible-coins\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"capital/contract/convertible-coins\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n-    def margin_v1_post_broker_sub_account_api_permission_vanilla_options(self, **params):\n+    def margin_v1_post_broker_sub_account_api_permission_vanilla_options(\n+        self, **params\n+    ):\n         \"\"\"\n         Placeholder function for POST /sapi/v1/broker/subAccountApi/permission/vanillaOptions.\n         Note: This function was auto-generated. Any issue please open an issue on GitHub.\n@@ -16182,7 +16639,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/permission/vanillaOptions\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/permission/vanillaOptions\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_lending_auto_invest_redeem_history(self, **params):\n         \"\"\"\n@@ -16196,7 +16659,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/redeem/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/redeem/history\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_get_localentity_withdraw_history(self, **params):\n         \"\"\"\n@@ -16210,7 +16679,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"localentity/withdraw/history\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"localentity/withdraw/history\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_get_eth_staking_eth_history_redemption_history(self, **params):\n         \"\"\"\n@@ -16224,7 +16695,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"eth-staking/eth/history/redemptionHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"eth-staking/eth/history/redemptionHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_v1_get_fee_burn(self, **params):\n         \"\"\"\n@@ -16238,7 +16715,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"feeBurn\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"feeBurn\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_lending_auto_invest_index_user_summary(self, **params):\n         \"\"\"\n@@ -16250,7 +16729,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/index/user-summary\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/index/user-summary\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_post_loan_flexible_borrow(self, **params):\n         \"\"\"\n@@ -16264,7 +16749,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/flexible/borrow\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"post\", \"loan/flexible/borrow\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_post_loan_vip_repay(self, **params):\n         \"\"\"\n@@ -16278,7 +16765,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/vip/repay\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/vip/repay\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_get_commission_rate(self, **params):\n         \"\"\"\n@@ -16292,7 +16781,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"get\", \"commissionRate\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"get\", \"commissionRate\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_convert_asset_info(self, **params):\n         \"\"\"\n@@ -16306,7 +16797,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"convert/assetInfo\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"convert/assetInfo\", signed=True, data=params, version=1\n+        )\n \n     def v3_post_sor_order_test(self, **params):\n         \"\"\"\n@@ -16318,7 +16811,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"post\", \"sor/order/test\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"post\", \"sor/order/test\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_post_broker_universal_transfer(self, **params):\n         \"\"\"\n@@ -16332,7 +16827,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/universalTransfer\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"broker/universalTransfer\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_account_disable_fast_withdraw_switch(self, **params):\n         \"\"\"\n@@ -16346,7 +16843,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"account/disableFastWithdrawSwitch\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"account/disableFastWithdrawSwitch\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_v1_get_asset_index(self, **params):\n         \"\"\"\n@@ -16360,7 +16863,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"assetIndex\", signed=False, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"assetIndex\", signed=False, data=params, version=1\n+        )\n \n     def margin_v1_get_account_api_restrictions_ip_restriction(self, **params):\n         \"\"\"\n@@ -16372,7 +16877,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"account/apiRestrictions/ipRestriction\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"account/apiRestrictions/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account_bnb_burn_spot(self, **params):\n         \"\"\"\n@@ -16386,7 +16897,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccount/bnbBurn/spot\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccount/bnbBurn/spot\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_coin_v1_put_order(self, **params):\n         \"\"\"\n@@ -16400,7 +16917,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"put\", \"order\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"put\", \"order\", signed=True, data=params, version=1\n+        )\n \n     def futures_coin_v1_put_batch_orders(self, **params):\n         \"\"\"\n@@ -16414,7 +16933,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_coin_api(\"put\", \"batchOrders\", signed=True, data=params, version=1)\n+        return self._request_futures_coin_api(\n+            \"put\", \"batchOrders\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_margin_delist_schedule(self, **params):\n         \"\"\"\n@@ -16426,9 +16947,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"margin/delist-schedule\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"margin/delist-schedule\", signed=True, data=params, version=1\n+        )\n \n-    def margin_v1_post_broker_sub_account_api_permission_universal_transfer(self, **params):\n+    def margin_v1_post_broker_sub_account_api_permission_universal_transfer(\n+        self, **params\n+    ):\n         \"\"\"\n         Placeholder function for POST /sapi/v1/broker/subAccountApi/permission/universalTransfer.\n         Note: This function was auto-generated. Any issue please open an issue on GitHub.\n@@ -16440,7 +16965,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/permission/universalTransfer\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/permission/universalTransfer\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_loan_ltv_adjustment_history(self, **params):\n         \"\"\"\n@@ -16454,7 +16985,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/ltv/adjustment/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/ltv/adjustment/history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_localentity_withdraw_history(self, **params):\n         \"\"\"\n@@ -16466,7 +16999,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"localentity/withdraw/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"localentity/withdraw/history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_post_sub_account_sub_account_api_ip_restriction(self, **params):\n         \"\"\"\n@@ -16480,7 +17015,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sub-account/subAccountApi/ipRestriction\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"sub-account/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n \n     def futures_v1_get_rate_limit_order(self, **params):\n         \"\"\"\n@@ -16494,7 +17035,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"rateLimit/order\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"rateLimit/order\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_sub_account_api_commission_futures(self, **params):\n         \"\"\"\n@@ -16508,7 +17051,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccountApi/commission/futures\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccountApi/commission/futures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_sol_staking_sol_history_staking_history(self, **params):\n         \"\"\"\n@@ -16522,7 +17071,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/history/stakingHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/stakingHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_v1_get_open_order(self, **params):\n         \"\"\"\n@@ -16536,7 +17091,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"openOrder\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"openOrder\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_delete_algo_spot_order(self, **params):\n         \"\"\"\n@@ -16550,9 +17107,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"delete\", \"algo/spot/order\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"delete\", \"algo/spot/order\", signed=True, data=params, version=1\n+        )\n \n-    def margin_v1_delete_account_api_restrictions_ip_restriction_ip_list(self, **params):\n+    def margin_v1_delete_account_api_restrictions_ip_restriction_ip_list(\n+        self, **params\n+    ):\n         \"\"\"\n         Placeholder function for DELETE /sapi/v1/account/apiRestrictions/ipRestriction/ipList.\n         Note: This function was auto-generated. Any issue please open an issue on GitHub.\n@@ -16562,7 +17123,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"delete\", \"account/apiRestrictions/ipRestriction/ipList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"delete\",\n+            \"account/apiRestrictions/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_capital_contract_convertible_coins(self, **params):\n         \"\"\"\n@@ -16574,7 +17141,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"capital/contract/convertible-coins\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"capital/contract/convertible-coins\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_managed_subaccount_margin_asset(self, **params):\n         \"\"\"\n@@ -16588,7 +17161,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/marginAsset\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"managed-subaccount/marginAsset\", signed=True, data=params, version=1\n+        )\n \n     def v3_delete_order_list(self, **params):\n         \"\"\"\n@@ -16600,9 +17175,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"delete\", \"orderList\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"delete\", \"orderList\", signed=True, data=params, version=\"v3\"\n+        )\n \n-    def margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list(self, **params):\n+    def margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list(\n+        self, **params\n+    ):\n         \"\"\"\n         Placeholder function for POST /sapi/v1/sub-account/subAccountApi/ipRestriction/ipList.\n         Note: This function was auto-generated. Any issue please open an issue on GitHub.\n@@ -16612,7 +17191,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sub-account/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"sub-account/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account_api_commission(self, **params):\n         \"\"\"\n@@ -16626,7 +17211,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/commission\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/commission\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_v1_post_fee_burn(self, **params):\n         \"\"\"\n@@ -16640,7 +17231,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"post\", \"feeBurn\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"post\", \"feeBurn\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_sub_account_margin_summary(self, **params):\n         \"\"\"\n@@ -16654,7 +17247,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/marginSummary\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/marginSummary\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_lending_auto_invest_plan_list(self, **params):\n         \"\"\"\n@@ -16666,7 +17265,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/plan/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"lending/auto-invest/plan/list\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_loan_vip_loanable_data(self, **params):\n         \"\"\"\n@@ -16680,7 +17281,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/vip/loanable/data\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/vip/loanable/data\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_get_loan_flexible_collateral_data(self, **params):\n         \"\"\"\n@@ -16694,7 +17297,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/collateral/data\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"loan/flexible/collateral/data\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_delete_broker_sub_account_api(self, **params):\n         \"\"\"\n@@ -16708,7 +17313,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"delete\", \"broker/subAccountApi\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"delete\", \"broker/subAccountApi\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_sol_staking_sol_history_bnsol_rewards_history(self, **params):\n         \"\"\"\n@@ -16722,7 +17329,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/history/bnsolRewardsHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/bnsolRewardsHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_convert_limit_query_open_orders(self, **params):\n         \"\"\"\n@@ -16736,7 +17349,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"convert/limit/queryOpenOrders\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"convert/limit/queryOpenOrders\", signed=True, data=params, version=1\n+        )\n \n     def v3_get_account_commission(self, **params):\n         \"\"\"\n@@ -16748,7 +17363,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"get\", \"account/commission\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"get\", \"account/commission\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_get_managed_subaccount_query_trans_log(self, **params):\n         \"\"\"\n@@ -16762,7 +17379,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/query-trans-log\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/query-trans-log\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_post_broker_sub_account_api_ip_restriction(self, **params):\n         \"\"\"\n@@ -16776,7 +17399,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/ipRestriction\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n \n     def margin_v1_get_lending_auto_invest_all_asset(self, **params):\n         \"\"\"\n@@ -16788,7 +17417,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/all/asset\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"lending/auto-invest/all/asset\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_post_convert_accept_quote(self, **params):\n         \"\"\"\n@@ -16802,7 +17433,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"post\", \"convert/acceptQuote\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"post\", \"convert/acceptQuote\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_spot_delist_schedule(self, **params):\n         \"\"\"\n@@ -16816,7 +17449,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"spot/delist-schedule\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"spot/delist-schedule\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_account_api_restrictions_ip_restriction(self, **params):\n         \"\"\"\n@@ -16828,7 +17463,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"account/apiRestrictions/ipRestriction\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"account/apiRestrictions/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_dci_product_accounts(self, **params):\n         \"\"\"\n@@ -16842,7 +17483,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"dci/product/accounts\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"dci/product/accounts\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_sub_account_sub_account_api_ip_restriction(self, **params):\n         \"\"\"\n@@ -16856,7 +17499,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sub-account/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sub-account/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_sub_account_transaction_statistics(self, **params):\n         \"\"\"\n@@ -16870,7 +17519,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sub-account/transaction-statistics\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"sub-account/transaction-statistics\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_managed_subaccount_deposit_address(self, **params):\n         \"\"\"\n@@ -16884,7 +17539,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/deposit/address\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/deposit/address\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_get_portfolio_account(self, **params):\n         \"\"\"\n@@ -16898,7 +17559,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"portfolio/account\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"portfolio/account\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_get_simple_earn_locked_history_redemption_record(self, **params):\n         \"\"\"\n@@ -16912,7 +17575,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"simple-earn/locked/history/redemptionRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/locked/history/redemptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def futures_v1_get_order_asyn_id(self, **params):\n         \"\"\"\n@@ -16926,7 +17595,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"order/asyn/id\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"order/asyn/id\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_managed_subaccount_withdraw(self, **params):\n         \"\"\"\n@@ -16940,7 +17611,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"managed-subaccount/withdraw\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"managed-subaccount/withdraw\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_localentity_deposit_history(self, **params):\n         \"\"\"\n@@ -16954,7 +17627,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"localentity/deposit/history\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"localentity/deposit/history\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_eth_staking_wbeth_wrap(self, **params):\n         \"\"\"\n@@ -16968,7 +17643,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"eth-staking/wbeth/wrap\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"eth-staking/wbeth/wrap\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_simple_earn_locked_set_redeem_option(self, **params):\n         \"\"\"\n@@ -16982,7 +17659,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"simple-earn/locked/setRedeemOption\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"simple-earn/locked/setRedeemOption\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account_api_ip_restriction_ip_list(self, **params):\n         \"\"\"\n@@ -16994,7 +17677,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_broker_sub_account_api_commission_futures(self, **params):\n         \"\"\"\n@@ -17008,7 +17697,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"broker/subAccountApi/commission/futures\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/commission/futures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_lending_auto_invest_history_list(self, **params):\n         \"\"\"\n@@ -17020,7 +17715,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/history/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/history/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_loan_customize_margin_call(self, **params):\n         \"\"\"\n@@ -17032,7 +17733,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/customize/margin_call\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/customize/margin_call\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_sub_account_bnb_burn_status(self, **params):\n         \"\"\"\n@@ -17046,7 +17749,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/bnbBurn/status\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/bnbBurn/status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_managed_subaccount_account_snapshot(self, **params):\n         \"\"\"\n@@ -17060,7 +17769,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/accountSnapshot\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/accountSnapshot\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_asset_convert_transfer(self, **params):\n         \"\"\"\n@@ -17072,7 +17787,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"asset/convert-transfer\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"asset/convert-transfer\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_get_income_asyn(self, **params):\n         \"\"\"\n@@ -17100,7 +17817,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccountApi/commission/coinFutures\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccountApi/commission/coinFutures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v2_get_broker_sub_account_futures_summary(self, **params):\n         \"\"\"\n@@ -17112,7 +17835,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccount/futuresSummary\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/futuresSummary\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n \n     def margin_v1_get_loan_ongoing_orders(self, **params):\n         \"\"\"\n@@ -17124,7 +17853,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/ongoing/orders\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"loan/ongoing/orders\", signed=True, data=params, version=1\n+        )\n \n     def margin_v2_get_loan_flexible_ongoing_orders(self, **params):\n         \"\"\"\n@@ -17138,7 +17869,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"loan/flexible/ongoing/orders\", signed=True, data=params, version=2)\n+        return self._request_margin_api(\n+            \"get\", \"loan/flexible/ongoing/orders\", signed=True, data=params, version=2\n+        )\n \n     def margin_v1_post_algo_futures_new_order_vp(self, **params):\n         \"\"\"\n@@ -17152,7 +17885,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"algo/futures/newOrderVp\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"algo/futures/newOrderVp\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_post_convert_get_quote(self, **params):\n         \"\"\"\n@@ -17166,7 +17901,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"post\", \"convert/getQuote\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"post\", \"convert/getQuote\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_algo_spot_sub_orders(self, **params):\n         \"\"\"\n@@ -17180,7 +17917,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"algo/spot/subOrders\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"algo/spot/subOrders\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_portfolio_redeem(self, **params):\n         \"\"\"\n@@ -17194,7 +17933,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"portfolio/redeem\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"portfolio/redeem\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_lending_auto_invest_plan_add(self, **params):\n         \"\"\"\n@@ -17206,7 +17947,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"lending/auto-invest/plan/add\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"lending/auto-invest/plan/add\", signed=True, data=params, version=1\n+        )\n \n     def v3_get_order_list(self, **params):\n         \"\"\"\n@@ -17218,7 +17961,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_api(\"get\", \"orderList\", signed=True, data=params, version=\"v3\")\n+        return self._request_api(\n+            \"get\", \"orderList\", signed=True, data=params, version=\"v3\"\n+        )\n \n     def margin_v1_get_lending_auto_invest_source_asset_list(self, **params):\n         \"\"\"\n@@ -17230,7 +17975,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"lending/auto-invest/source-asset/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/source-asset/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_margin_all_order_list(self, **params):\n         \"\"\"\n@@ -17244,7 +17995,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"margin/allOrderList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"margin/allOrderList\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_eth_staking_eth_redeem(self, **params):\n         \"\"\"\n@@ -17258,7 +18011,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"eth-staking/eth/redeem\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"eth-staking/eth/redeem\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_broker_rebate_historical_record(self, **params):\n         \"\"\"\n@@ -17270,7 +18025,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/rebate/historicalRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/rebate/historicalRecord\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_simple_earn_locked_history_subscription_record(self, **params):\n         \"\"\"\n@@ -17284,7 +18041,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"simple-earn/locked/history/subscriptionRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/locked/history/subscriptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_managed_subaccount_asset(self, **params):\n         \"\"\"\n@@ -17298,7 +18061,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/asset\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"managed-subaccount/asset\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_sol_staking_sol_quota(self, **params):\n         \"\"\"\n@@ -17312,7 +18077,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"sol-staking/sol/quota\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"sol-staking/sol/quota\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_post_loan_vip_renew(self, **params):\n         \"\"\"\n@@ -17324,9 +18091,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"loan/vip/renew\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\", \"loan/vip/renew\", signed=True, data=params, version=1\n+        )\n \n-    def margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent(self, **params):\n+    def margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent(\n+        self, **params\n+    ):\n         \"\"\"\n         Placeholder function for GET /sapi/v1/managed-subaccount/queryTransLogForTradeParent.\n         Note: This function was auto-generated. Any issue please open an issue on GitHub.\n@@ -17338,7 +18109,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"managed-subaccount/queryTransLogForTradeParent\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/queryTransLogForTradeParent\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_post_sub_account_sub_account_api_ip_restriction(self, **params):\n         \"\"\"\n@@ -17350,7 +18127,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"post\", \"sub-account/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"post\",\n+            \"sub-account/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_simple_earn_flexible_history_redemption_record(self, **params):\n         \"\"\"\n@@ -17364,7 +18147,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"simple-earn/flexible/history/redemptionRecord\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/flexible/history/redemptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_broker_sub_account_api(self, **params):\n         \"\"\"\n@@ -17378,7 +18167,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"broker/subAccountApi\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"broker/subAccountApi\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_get_exercise_history(self, **params):\n         \"\"\"\n@@ -17392,7 +18183,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"get\", \"exerciseHistory\", signed=False, data=params)\n+        return self._request_options_api(\n+            \"get\", \"exerciseHistory\", signed=False, data=params\n+        )\n \n     def options_open_interest(self, **params):\n         \"\"\"Get present open interest specific underlying asset on specific expiration date.\n@@ -17421,7 +18214,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1\n+        )\n \n     def futures_v1_delete_batch_order(self, **params):\n         \"\"\"\n@@ -17433,7 +18228,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"delete\", \"batchOrder\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"delete\", \"batchOrder\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_eth_staking_eth_history_wbeth_rewards_history(self, **params):\n         \"\"\"\n@@ -17447,7 +18244,13 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"eth-staking/eth/history/wbethRewardsHistory\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\",\n+            \"eth-staking/eth/history/wbethRewardsHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n \n     def margin_v1_get_mining_pub_algo_list(self, **params):\n         \"\"\"\n@@ -17461,7 +18264,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/pub/algoList\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/pub/algoList\", signed=True, data=params, version=1\n+        )\n \n     def options_v1_get_block_trades(self, **params):\n         \"\"\"\n@@ -17475,7 +18280,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_options_api(\"get\", \"blockTrades\", signed=False, data=params)\n+        return self._request_options_api(\n+            \"get\", \"blockTrades\", signed=False, data=params\n+        )\n \n     def margin_v1_get_copy_trading_futures_lead_symbol(self, **params):\n         \"\"\"\n@@ -17489,7 +18296,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"copyTrading/futures/leadSymbol\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"copyTrading/futures/leadSymbol\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_mining_worker_list(self, **params):\n         \"\"\"\n@@ -17503,7 +18312,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"mining/worker/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"mining/worker/list\", signed=True, data=params, version=1\n+        )\n \n     def margin_v1_get_dci_product_list(self, **params):\n         \"\"\"\n@@ -17517,7 +18328,9 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_margin_api(\"get\", \"dci/product/list\", signed=True, data=params, version=1)\n+        return self._request_margin_api(\n+            \"get\", \"dci/product/list\", signed=True, data=params, version=1\n+        )\n \n     def futures_v1_get_convert_order_status(self, **params):\n         \"\"\"\n@@ -17531,4 +18344,6 @@ class Client(BaseClient):\n \n         :returns: API response\n         \"\"\"\n-        return self._request_futures_api(\"get\", \"convert/orderStatus\", signed=True, data=params, version=1)\n+        return self._request_futures_api(\n+            \"get\", \"convert/orderStatus\", signed=True, data=params, version=1\n+        )\ndiff --git a/tests/test_client_options.py b/tests/test_client_options.py\nindex 4016980..b130bda 100644\n--- a/tests/test_client_options.py\n+++ b/tests/test_client_options.py\n@@ -2,7 +2,12 @@ import pytest\n import sys\n \n \n-pytestmark = [pytest.mark.options, pytest.mark.skipif(sys.version_info < (3, 8), reason=\"websockets_proxy Python 3.8+\")]\n+pytestmark = [\n+    pytest.mark.options,\n+    pytest.mark.skipif(\n+        sys.version_info < (3, 8), reason=\"websockets_proxy Python 3.8+\"\n+    ),\n+]\n \n \n @pytest.fixture\n"
    },
    {
        "id": 288,
        "sha_fail": "4846dead90a34de7b3d8addf4ddf36300ccbc6e3",
        "diff": "diff --git a/flask_admin/tests/sqla/test_basic.py b/flask_admin/tests/sqla/test_basic.py\nindex 1f167c4..589949d 100644\n--- a/flask_admin/tests/sqla/test_basic.py\n+++ b/flask_admin/tests/sqla/test_basic.py\n@@ -2747,7 +2747,7 @@ def test_customising_page_size(app, db, admin):\n     with app.app_context():\n         M1, _ = create_models(db)\n \n-        db.session.add_all([M1(str(f\"instance-{x+1:03d}\")) for x in range(101)])\n+        db.session.add_all([M1(str(f\"instance-{x + 1:03d}\")) for x in range(101)])\n \n         view1 = CustomModelView(\n             M1, db.session, endpoint=\"view1\", page_size=20, can_set_page_size=False\n"
    },
    {
        "id": 289,
        "sha_fail": "411289ce91c2497290d0d51a16053612f27b43ff",
        "diff": "diff --git a/rqalpha/mod/rqalpha_mod_sys_analyser/mod.py b/rqalpha/mod/rqalpha_mod_sys_analyser/mod.py\nindex 098bfcc..b6ce773 100644\n--- a/rqalpha/mod/rqalpha_mod_sys_analyser/mod.py\n+++ b/rqalpha/mod/rqalpha_mod_sys_analyser/mod.py\n@@ -23,15 +23,20 @@ import jsonpickle\n import datetime\n from operator import attrgetter\n from collections import defaultdict\n-from typing import Dict, Optional, List, Tuple, Union, Iterable\n+from typing import Optional, List, Tuple, Iterable\n \n import numpy as np\n import pandas as pd\n from rqrisk import Risk, DAILY, WEEKLY, MONTHLY\n \n-from rqalpha.const import EXIT_CODE, DEFAULT_ACCOUNT_TYPE, INSTRUMENT_TYPE, POSITION_DIRECTION\n+from rqalpha.const import (\n+    EXIT_CODE,\n+    DEFAULT_ACCOUNT_TYPE,\n+    INSTRUMENT_TYPE,\n+    POSITION_DIRECTION,\n+)\n from rqalpha.core.events import EVENT\n-from rqalpha.interface import AbstractMod, AbstractPosition\n+from rqalpha.interface import AbstractMod\n from rqalpha.utils.i18n import gettext as _\n from rqalpha.utils import INST_TYPE_IN_STOCK_ACCOUNT\n from rqalpha.utils.datetime_func import convert_int_to_date\n@@ -43,11 +48,16 @@ from .plot_store import PlotStore\n \n \n def _get_yearly_risk_free_rates(\n-        data_proxy, start_date: datetime.date, end_date: datetime.date\n+    data_proxy, start_date: datetime.date, end_date: datetime.date\n ) -> Iterable[Tuple[int, float]]:\n     while start_date <= end_date:\n         year = start_date.year\n-        yield year, data_proxy.get_risk_free_rate(start_date, min(end_date, datetime.date(year, 12, 31)))\n+        yield (\n+            year,\n+            data_proxy.get_risk_free_rate(\n+                start_date, min(end_date, datetime.date(year, 12, 31))\n+            ),\n+        )\n         start_date = datetime.date(year + 1, 1, 1)\n \n \n@@ -76,25 +86,31 @@ class AnalyserMod(AbstractMod):\n         self._plot_store = None\n \n     def get_state(self):\n-        return jsonpickle.dumps({\n-            'benchmark_daily_returns': [float(v) for v in self._benchmark_daily_returns],\n-            'portfolio_daily_returns': [float(v) for v in self._portfolio_daily_returns],\n-            'total_portfolios': self._total_portfolios,\n-            'total_benchmark_portfolios': self._total_benchmark_portfolios,\n-            'sub_accounts': self._sub_accounts,\n-            'positions': self._positions,\n-            'orders': self._orders,\n-            'trades': self._trades,\n-            'daily_pnl': self._daily_pnl\n-        }).encode('utf-8')\n+        return jsonpickle.dumps(\n+            {\n+                \"benchmark_daily_returns\": [\n+                    float(v) for v in self._benchmark_daily_returns\n+                ],\n+                \"portfolio_daily_returns\": [\n+                    float(v) for v in self._portfolio_daily_returns\n+                ],\n+                \"total_portfolios\": self._total_portfolios,\n+                \"total_benchmark_portfolios\": self._total_benchmark_portfolios,\n+                \"sub_accounts\": self._sub_accounts,\n+                \"positions\": self._positions,\n+                \"orders\": self._orders,\n+                \"trades\": self._trades,\n+                \"daily_pnl\": self._daily_pnl,\n+            }\n+        ).encode(\"utf-8\")\n \n     def set_state(self, state):\n-        value = jsonpickle.loads(state.decode('utf-8'))\n+        value = jsonpickle.loads(state.decode(\"utf-8\"))\n         self._portfolio_daily_returns = value[\"portfolio_daily_returns\"]\n-        self._total_portfolios = value['total_portfolios']\n-        self._sub_accounts = value['sub_accounts']\n+        self._total_portfolios = value[\"total_portfolios\"]\n+        self._sub_accounts = value[\"sub_accounts\"]\n         self._positions = value[\"positions\"]\n-        self._orders = value['orders']\n+        self._orders = value[\"orders\"]\n         self._trades = value[\"trades\"]\n         self._daily_pnl = value.get(\"daily_pnl\", [])\n \n@@ -102,8 +118,12 @@ class AnalyserMod(AbstractMod):\n         self._env = env\n         self._mod_config = mod_config\n         self._enabled = (\n-            mod_config.record or mod_config.plot or mod_config.output_file or\n-            mod_config.plot_save_file or mod_config.report_save_path or mod_config.benchmark\n+            mod_config.record\n+            or mod_config.plot\n+            or mod_config.output_file\n+            or mod_config.plot_save_file\n+            or mod_config.report_save_path\n+            or mod_config.benchmark\n         )\n         if self._enabled:\n             env.event_bus.add_listener(EVENT.POST_SYSTEM_INIT, self._subscribe_events)\n@@ -111,7 +131,9 @@ class AnalyserMod(AbstractMod):\n             if not mod_config.benchmark:\n                 if getattr(env.config.base, \"benchmark\", None):\n                     user_system_log.warning(\n-                        _(\"config 'base.benchmark' is deprecated, use 'mod.sys_analyser.benchmark' instead\")\n+                        _(\n+                            \"config 'base.benchmark' is deprecated, use 'mod.sys_analyser.benchmark' instead\"\n+                        )\n                     )\n                     mod_config.benchmark = getattr(env.config.base, \"benchmark\")\n             if mod_config.benchmark:\n@@ -119,7 +141,7 @@ class AnalyserMod(AbstractMod):\n \n             self._plot_store = PlotStore(env)\n             export_as_api(self._plot_store.plot)\n-    \n+\n     NULL_OID = {\"null\", \"NULL\"}\n \n     def generate_benchmark_daily_returns_and_portfolio(self, event):\n@@ -129,7 +151,7 @@ class AnalyserMod(AbstractMod):\n         if self._benchmark is None:\n             self._benchmark_daily_returns = list(np.full(len(trading_dates), np.nan))\n             return\n-        \n+\n         # generate benchmerk daily returns\n         self._benchmark_daily_returns = np.zeros(len(trading_dates))\n         weights = 0\n@@ -140,45 +162,65 @@ class AnalyserMod(AbstractMod):\n                 ins = self._env.data_proxy.instrument(order_book_id)\n                 if ins is None:\n                     raise RuntimeError(\n-                        _(\"benchmark {} not exists, please entry correct order_book_id\").format(order_book_id)\n+                        _(\n+                            \"benchmark {} not exists, please entry correct order_book_id\"\n+                        ).format(order_book_id)\n                     )\n                 bars = self._env.data_proxy.history_bars(\n-                    order_book_id = order_book_id,\n-                    bar_count = len(trading_dates) + 1,  # Get an extra day for calculation\n-                    frequency = \"1d\",\n-                    field = [\"datetime\", \"close\"],\n-                    dt = _e,\n+                    order_book_id=order_book_id,\n+                    bar_count=len(trading_dates)\n+                    + 1,  # Get an extra day for calculation\n+                    frequency=\"1d\",\n+                    field=[\"datetime\", \"close\"],\n+                    dt=_e,\n                     skip_suspended=False,\n                 )\n                 if len(bars) == len(trading_dates) + 1:\n-                    if convert_int_to_date(bars[1]['datetime']).date() != _s:\n-                        raise RuntimeError(_(\n-                            \"benchmark {} missing data between backtest start date {} and end date {}\").format(order_book_id, _s, _e)\n+                    if convert_int_to_date(bars[1][\"datetime\"]).date() != _s:\n+                        raise RuntimeError(\n+                            _(\n+                                \"benchmark {} missing data between backtest start date {} and end date {}\"\n+                            ).format(order_book_id, _s, _e)\n                         )\n-                    daily_returns = (bars['close'] / np.roll(bars['close'], 1) - 1.0)[1: ]\n+                    daily_returns = (bars[\"close\"] / np.roll(bars[\"close\"], 1) - 1.0)[\n+                        1:\n+                    ]\n                 else:\n                     if len(bars) == 0:\n-                        (available_s, available_e) = (ins.listed_date, ins.de_listed_date)\n+                        (available_s, available_e) = (\n+                            ins.listed_date,\n+                            ins.de_listed_date,\n+                        )\n                     else:\n-                        (available_s, available_e) = (convert_int_to_date(bars[0]['datetime']).date(), convert_int_to_date(bars[-1]['datetime']).date())\n+                        (available_s, available_e) = (\n+                            convert_int_to_date(bars[0][\"datetime\"]).date(),\n+                            convert_int_to_date(bars[-1][\"datetime\"]).date(),\n+                        )\n                     raise RuntimeError(\n-                        _(\"benchmark {} available data start date {} >= backtest start date {} or end date {} <= backtest end \"\n-                        \"date {}\").format(order_book_id, available_s, _s, available_e, _e)\n+                        _(\n+                            \"benchmark {} available data start date {} >= backtest start date {} or end date {} <= backtest end \"\n+                            \"date {}\"\n+                        ).format(order_book_id, available_s, _s, available_e, _e)\n                     )\n-            self._benchmark_daily_returns = self._benchmark_daily_returns + daily_returns * weight\n+            self._benchmark_daily_returns = (\n+                self._benchmark_daily_returns + daily_returns * weight\n+            )\n             weights += weight\n \n         self._benchmark_daily_returns = self._benchmark_daily_returns / weights\n-        \n+\n         # generate benchmark portfolio\n         unit_net_value = (self._benchmark_daily_returns + 1).cumprod()\n         self._total_benchmark_portfolios = {\n             \"date\": list(trading_dates.date),\n-            \"unit_net_value\": unit_net_value\n+            \"unit_net_value\": unit_net_value,\n         }\n \n     def _subscribe_events(self, event):\n-        self._env.event_bus.add_listener(EVENT.BEFORE_STRATEGY_RUN, self.generate_benchmark_daily_returns_and_portfolio)\n+        self._env.event_bus.add_listener(\n+            EVENT.BEFORE_STRATEGY_RUN,\n+            self.generate_benchmark_daily_returns_and_portfolio,\n+        )\n         self._env.event_bus.add_listener(EVENT.TRADE, self._collect_trade)\n         self._env.event_bus.add_listener(EVENT.ORDER_CREATION_PASS, self._collect_order)\n         self._env.event_bus.prepend_listener(EVENT.POST_SETTLEMENT, self._collect_daily)\n@@ -198,15 +240,22 @@ class AnalyserMod(AbstractMod):\n         self._daily_pnl.append(portfolio.daily_pnl)\n \n         for account_type, account in self._env.portfolio.accounts.items():\n-            self._sub_accounts[account_type].append(self._to_account_record(date, account))\n+            self._sub_accounts[account_type].append(\n+                self._to_account_record(date, account)\n+            )\n             pos_dict = {}\n             for pos in account.get_positions():\n                 pos_dict.setdefault(pos.order_book_id, {})[pos.direction] = pos\n \n             for order_book_id, pos in pos_dict.items():\n-                self._positions[account_type].append(self._to_position_record(\n-                    date, order_book_id, pos.get(POSITION_DIRECTION.LONG), pos.get(POSITION_DIRECTION.SHORT)\n-                ))\n+                self._positions[account_type].append(\n+                    self._to_position_record(\n+                        date,\n+                        order_book_id,\n+                        pos.get(POSITION_DIRECTION.LONG),\n+                        pos.get(POSITION_DIRECTION.SHORT),\n+                    )\n+                )\n \n     def _symbol(self, order_book_id):\n         return self._env.data_proxy.instrument(order_book_id).symbol\n@@ -221,26 +270,32 @@ class AnalyserMod(AbstractMod):\n             for order_book_id, weight in benchmarks.items():\n                 result.append((order_book_id, float(weight)))\n             return result\n-        benchmark_list = benchmarks.split(',')\n+        benchmark_list = benchmarks.split(\",\")\n         if len(benchmark_list) == 1:\n-            if len(benchmark_list[0].split(':')) > 1:\n-                oid, weight = benchmark_list[0].split(':')\n+            if len(benchmark_list[0].split(\":\")) > 1:\n+                oid, weight = benchmark_list[0].split(\":\")\n                 result.append((oid, float(weight)))\n                 return result\n             result.append((benchmark_list[0], 1.0))\n             return result\n         for s in benchmark_list:\n             try:\n-                order_book_id, weight = s.split(':')\n+                order_book_id, weight = s.split(\":\")\n             except ValueError:\n                 raise RuntimeError(\n-                    _(u\"invalid init benchmark {}, should be in format 'order_book_id:weight'\").format(s))\n+                    _(\n+                        \"invalid init benchmark {}, should be in format 'order_book_id:weight'\"\n+                    ).format(s)\n+                )\n \n             try:\n                 result.append((order_book_id, float(weight)))\n             except ValueError:\n-                raise RuntimeError(_(u\"invalid weight for instrument {order_book_id}: {weight}\").format(\n-                    order_book_id=order_book_id, weight=weight))\n+                raise RuntimeError(\n+                    _(\"invalid weight for instrument {order_book_id}: {weight}\").format(\n+                        order_book_id=order_book_id, weight=weight\n+                    )\n+                )\n         return result\n \n     @staticmethod\n@@ -252,28 +307,35 @@ class AnalyserMod(AbstractMod):\n \n     def _to_portfolio_record(self, date, portfolio):\n         return {\n-            'date': date,\n-            'cash': self._safe_convert(portfolio.cash),\n-            'total_value': self._safe_convert(portfolio.total_value),\n-            'market_value': self._safe_convert(portfolio.market_value),\n-            'unit_net_value': self._safe_convert(portfolio.unit_net_value, 6),\n-            'units': portfolio.units,\n-            'static_unit_net_value': self._safe_convert(portfolio.static_unit_net_value),\n+            \"date\": date,\n+            \"cash\": self._safe_convert(portfolio.cash),\n+            \"total_value\": self._safe_convert(portfolio.total_value),\n+            \"market_value\": self._safe_convert(portfolio.market_value),\n+            \"unit_net_value\": self._safe_convert(portfolio.unit_net_value, 6),\n+            \"units\": portfolio.units,\n+            \"static_unit_net_value\": self._safe_convert(\n+                portfolio.static_unit_net_value\n+            ),\n         }\n \n     ACCOUNT_FIELDS_MAP = {\n         DEFAULT_ACCOUNT_TYPE.STOCK: [],\n-        DEFAULT_ACCOUNT_TYPE.FUTURE: ['position_pnl', 'trading_pnl', 'daily_pnl', 'margin'],\n+        DEFAULT_ACCOUNT_TYPE.FUTURE: [\n+            \"position_pnl\",\n+            \"trading_pnl\",\n+            \"daily_pnl\",\n+            \"margin\",\n+        ],\n         DEFAULT_ACCOUNT_TYPE.BOND: [],\n     }\n \n     def _to_account_record(self, date, account):\n         data = {\n-            'date': date,\n-            'cash': self._safe_convert(account.cash),\n-            'transaction_cost': self._safe_convert(account.transaction_cost),\n-            'market_value': self._safe_convert(account.market_value),\n-            'total_value': self._safe_convert(account.total_value),\n+            \"date\": date,\n+            \"cash\": self._safe_convert(account.cash),\n+            \"transaction_cost\": self._safe_convert(account.transaction_cost),\n+            \"market_value\": self._safe_convert(account.market_value),\n+            \"total_value\": self._safe_convert(account.total_value),\n         }\n \n         for f in self.ACCOUNT_FIELDS_MAP[account.type]:\n@@ -281,48 +343,57 @@ class AnalyserMod(AbstractMod):\n \n         return data\n \n-    LONG_ONLY_INS_TYPE = INST_TYPE_IN_STOCK_ACCOUNT + [INSTRUMENT_TYPE.CONVERTIBLE, INSTRUMENT_TYPE.BOND]\n+    LONG_ONLY_INS_TYPE = INST_TYPE_IN_STOCK_ACCOUNT + [\n+        INSTRUMENT_TYPE.CONVERTIBLE,\n+        INSTRUMENT_TYPE.BOND,\n+    ]\n \n     def _to_position_record(self, date, order_book_id, long, short):\n         # type: (datetime.date, str, AbstractPosition, AbstractPosition) -> Dict\n         instrument = self._env.data_proxy.instrument(order_book_id)\n         data = {\n-            'order_book_id': order_book_id,\n-            'symbol': self._symbol(order_book_id),\n-            'date': date,\n+            \"order_book_id\": order_book_id,\n+            \"symbol\": self._symbol(order_book_id),\n+            \"date\": date,\n         }\n         if instrument.type in self.LONG_ONLY_INS_TYPE + [INSTRUMENT_TYPE.REPO]:\n-            for field in ['quantity', 'last_price', 'avg_price', 'market_value']:\n+            for field in [\"quantity\", \"last_price\", \"avg_price\", \"market_value\"]:\n                 data[field] = self._safe_convert(getattr(long, field, None))\n         else:\n             position = long or short\n             if position:\n-                for field in ['margin', 'contract_multiplier', 'last_price']:\n+                for field in [\"margin\", \"contract_multiplier\", \"last_price\"]:\n                     data[field] = self._safe_convert(getattr(position, field))\n             direction_pos_iter = ((pos.direction, pos) for pos in (long, short) if pos)\n             for direction_prefix, pos in direction_pos_iter:\n-                data[direction_prefix + \"_pnl\"] = self._safe_convert(getattr(pos, \"pnl\", None))\n+                data[direction_prefix + \"_pnl\"] = self._safe_convert(\n+                    getattr(pos, \"pnl\", None)\n+                )\n                 data[direction_prefix + \"_margin\"] = self._safe_convert(pos.margin)\n-                data[direction_prefix + \"_market_value\"] = self._safe_convert(pos.market_value)\n+                data[direction_prefix + \"_market_value\"] = self._safe_convert(\n+                    pos.market_value\n+                )\n                 data[direction_prefix + \"_quantity\"] = self._safe_convert(pos.quantity)\n-                data[direction_prefix + \"_avg_open_price\"] = self._safe_convert(getattr(pos, \"avg_price\", None))\n+                data[direction_prefix + \"_avg_open_price\"] = self._safe_convert(\n+                    getattr(pos, \"avg_price\", None)\n+                )\n         return data\n \n     def _to_trade_record(self, trade):\n         return {\n-            'datetime': trade.datetime.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-            'trading_datetime': trade.trading_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"),\n-            'order_book_id': trade.order_book_id,\n-            'symbol': self._symbol(trade.order_book_id),\n-            'side': trade.side.name,\n-            'position_effect': trade.position_effect.name,\n-            'exec_id': trade.exec_id,\n-            'tax': trade.tax,\n-            'commission': trade.commission,\n-            'last_quantity': trade.last_quantity,\n-            'last_price': self._safe_convert(trade.last_price),\n-            'order_id': trade.order_id,\n-            'transaction_cost': trade.transaction_cost,\n+            \"datetime\": trade.datetime.strftime(\"%Y-%m-%d %H:%M:%S\"),\n+            \"trading_datetime\": trade.trading_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"),\n+            \"order_book_id\": trade.order_book_id,\n+            \"symbol\": self._symbol(trade.order_book_id),\n+            \"side\": trade.side.name,\n+            \"position_effect\": trade.position_effect.name,\n+            \"exec_id\": trade.exec_id,\n+            \"tax\": trade.tax,\n+            \"commission\": trade.commission,\n+            \"last_quantity\": trade.last_quantity,\n+            \"last_price\": self._safe_convert(trade.last_price),\n+            \"order_id\": trade.order_id,\n+            \"transaction_cost\": trade.transaction_cost,\n         }\n \n     def tear_down(self, code, exception=None):\n@@ -336,16 +407,22 @@ class AnalyserMod(AbstractMod):\n         if self._mod_config.strategy_name:\n             strategy_name = self._mod_config.strategy_name\n         else:\n-            strategy_name = os.path.basename(self._env.config.base.strategy_file).split(\".\")[0]\n+            strategy_name = os.path.basename(self._env.config.base.strategy_file).split(\n+                \".\"\n+            )[0]\n         data_proxy = self._env.data_proxy\n-        start_date, end_date = attrgetter(\"start_date\", \"end_date\")(self._env.config.base)\n+        start_date, end_date = attrgetter(\"start_date\", \"end_date\")(\n+            self._env.config.base\n+        )\n         summary = {\n-            'strategy_name': strategy_name,\n-            'start_date': start_date.strftime('%Y-%m-%d'),\n-            'end_date': end_date.strftime('%Y-%m-%d'),\n-            'strategy_file': self._env.config.base.strategy_file,\n-            'run_type': self._env.config.base.run_type.value,\n-            \"starting_cash\": \",\".join(f\"{t}:{c}\" for t, c in self._env.config.base.accounts.items())\n+            \"strategy_name\": strategy_name,\n+            \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n+            \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n+            \"strategy_file\": self._env.config.base.strategy_file,\n+            \"run_type\": self._env.config.base.run_type.value,\n+            \"starting_cash\": \",\".join(\n+                f\"{t}:{c}\" for t, c in self._env.config.base.accounts.items()\n+            ),\n         }\n         for account_type, starting_cash in self._env.config.base.accounts.items():\n             summary[account_type] = starting_cash\n@@ -353,159 +430,220 @@ class AnalyserMod(AbstractMod):\n             if len(self._benchmark) == 1:\n                 benchmark_obid, _ = self._benchmark[0]\n                 summary[\"benchmark\"] = benchmark_obid\n-                summary[\"benchmark_symbol\"] = self._env.data_proxy.instrument(benchmark_obid).symbol\n+                summary[\"benchmark_symbol\"] = self._env.data_proxy.instrument(\n+                    benchmark_obid\n+                ).symbol\n             else:\n                 summary[\"benchmark\"] = \",\".join(f\"{o}:{w}\" for o, w in self._benchmark)\n-                summary[\"benchmark_symbol\"] = \",\".join(f\"{self._env.data_proxy.instrument(o).symbol if o not in self.NULL_OID else 'null'}:{w}\" for o, w in self._benchmark)\n+                summary[\"benchmark_symbol\"] = \",\".join(\n+                    f\"{self._env.data_proxy.instrument(o).symbol if o not in self.NULL_OID else 'null'}:{w}\"\n+                    for o, w in self._benchmark\n+                )\n \n         trading_days_a_year = self._env.trading_days_a_year\n-        risk_free_rate = data_proxy.get_risk_free_rate(self._env.config.base.start_date, self._env.config.base.end_date)\n+        risk_free_rate = data_proxy.get_risk_free_rate(\n+            self._env.config.base.start_date, self._env.config.base.end_date\n+        )\n         risk = Risk(\n-            np.array(self._portfolio_daily_returns), np.array(self._benchmark_daily_returns), risk_free_rate, period=DAILY, trading_days_a_year=trading_days_a_year\n+            np.array(self._portfolio_daily_returns),\n+            np.array(self._benchmark_daily_returns),\n+            risk_free_rate,\n+            period=DAILY,\n+            trading_days_a_year=trading_days_a_year,\n+        )\n+        summary.update(\n+            {\n+                \"alpha\": risk.alpha,\n+                \"beta\": risk.beta,\n+                \"sharpe\": risk.sharpe,\n+                \"excess_sharpe\": risk.excess_sharpe,\n+                \"information_ratio\": risk.information_ratio,\n+                \"downside_risk\": risk.annual_downside_risk,\n+                \"tracking_error\": risk.annual_tracking_error,\n+                \"sortino\": risk.sortino,\n+                \"volatility\": risk.annual_volatility,\n+                \"excess_volatility\": risk.excess_annual_volatility,\n+                \"max_drawdown\": risk.max_drawdown,\n+                \"excess_max_drawdown\": risk.geometric_excess_drawdown,\n+                \"excess_returns\": risk.geometric_excess_return,\n+                \"excess_annual_returns\": risk.geometric_excess_annual_return,\n+                \"var\": risk.var,\n+                \"win_rate\": risk.win_rate,\n+                \"excess_win_rate\": risk.excess_win_rate,\n+                \"excess_cum_returns\": risk.arithmetic_excess_return,\n+            }\n         )\n-        summary.update({\n-            'alpha': risk.alpha,\n-            'beta': risk.beta,\n-            'sharpe': risk.sharpe,\n-            'excess_sharpe': risk.excess_sharpe,\n-            'information_ratio': risk.information_ratio,\n-            'downside_risk': risk.annual_downside_risk,\n-            'tracking_error': risk.annual_tracking_error,\n-            'sortino': risk.sortino,\n-            'volatility': risk.annual_volatility,\n-            'excess_volatility': risk.excess_annual_volatility,\n-            'max_drawdown': risk.max_drawdown,\n-            'excess_max_drawdown': risk.geometric_excess_drawdown,\n-            'excess_returns': risk.geometric_excess_return,\n-            'excess_annual_returns': risk.geometric_excess_annual_return,\n-            'var': risk.var,\n-            \"win_rate\": risk.win_rate,\n-            \"excess_win_rate\": risk.excess_win_rate,\n-            \"excess_cum_returns\": risk.arithmetic_excess_return,\n-        })\n \n         # \u76c8\u4e8f\u6bd4\n         daily_pnl = np.array(self._daily_pnl)\n         profit, loss = daily_pnl[daily_pnl > 0], daily_pnl[daily_pnl < 0]\n-        profit, loss = profit.mean() if len(profit) else 0, loss.mean() if len(loss) else 0\n-\n-        summary.update({\n-            \"profit_loss_rate\": np.abs(profit / loss) if loss else np.nan\n-        })\n+        profit, loss = (\n+            profit.mean() if len(profit) else 0,\n+            loss.mean() if len(loss) else 0,\n+        )\n \n-        summary.update({\n-            'total_value': self._env.portfolio.total_value,\n-            'cash': self._env.portfolio.cash,\n-            'total_returns': self._env.portfolio.total_returns,\n-            'annualized_returns': self._env.portfolio.annualized_returns,\n-            'unit_net_value': self._env.portfolio.unit_net_value,\n-            'units': self._env.portfolio.units,\n-        })\n+        summary.update({\"profit_loss_rate\": np.abs(profit / loss) if loss else np.nan})\n+\n+        summary.update(\n+            {\n+                \"total_value\": self._env.portfolio.total_value,\n+                \"cash\": self._env.portfolio.cash,\n+                \"total_returns\": self._env.portfolio.total_returns,\n+                \"annualized_returns\": self._env.portfolio.annualized_returns,\n+                \"unit_net_value\": self._env.portfolio.unit_net_value,\n+                \"units\": self._env.portfolio.units,\n+            }\n+        )\n \n         if self._benchmark:\n-            benchmark_total_returns = (np.array(self._benchmark_daily_returns) + 1.0).prod() - 1.0\n-            summary['benchmark_total_returns'] = benchmark_total_returns\n+            benchmark_total_returns = (\n+                np.array(self._benchmark_daily_returns) + 1.0\n+            ).prod() - 1.0\n+            summary[\"benchmark_total_returns\"] = benchmark_total_returns\n             date_count = len(self._benchmark_daily_returns)\n             # \u83b7\u53d6\u4e00\u5e74\u4ea4\u6613\u65e5\u5929\u6570\n             trading_days_a_year = self._env.trading_days_a_year\n-            benchmark_annualized_returns = (benchmark_total_returns + 1) ** (trading_days_a_year / date_count) - 1\n-            summary['benchmark_annualized_returns'] = benchmark_annualized_returns\n+            benchmark_annualized_returns = (benchmark_total_returns + 1) ** (\n+                trading_days_a_year / date_count\n+            ) - 1\n+            summary[\"benchmark_annualized_returns\"] = benchmark_annualized_returns\n \n         trades = pd.DataFrame(self._trades)\n-        if 'datetime' in trades.columns:\n-            trades = trades.set_index(pd.DatetimeIndex(trades['datetime']))\n+        if \"datetime\" in trades.columns:\n+            trades = trades.set_index(pd.DatetimeIndex(trades[\"datetime\"]))\n \n         df = pd.DataFrame(self._total_portfolios)\n-        df['date'] = pd.to_datetime(df['date'])\n-        total_portfolios = df.set_index('date').sort_index()\n+        df[\"date\"] = pd.to_datetime(df[\"date\"])\n+        total_portfolios = df.set_index(\"date\").sort_index()\n         if self._benchmark:\n-            total_portfolios[\"benchmark_unit_net_value\"] = (self._benchmark_daily_returns + 1).cumprod()\n-            total_portfolios[\"arithmetic_excess_unit_net_value\"] = total_portfolios[\"unit_net_value\"] - total_portfolios[\"benchmark_unit_net_value\"]\n-            total_portfolios[\"geometric_excess_unit_net_value\"] = total_portfolios[\"unit_net_value\"] / total_portfolios[\"benchmark_unit_net_value\"] - 1\n-        df.index = df['date']\n+            total_portfolios[\"benchmark_unit_net_value\"] = (\n+                self._benchmark_daily_returns + 1\n+            ).cumprod()\n+            total_portfolios[\"arithmetic_excess_unit_net_value\"] = (\n+                total_portfolios[\"unit_net_value\"]\n+                - total_portfolios[\"benchmark_unit_net_value\"]\n+            )\n+            total_portfolios[\"geometric_excess_unit_net_value\"] = (\n+                total_portfolios[\"unit_net_value\"]\n+                / total_portfolios[\"benchmark_unit_net_value\"]\n+                - 1\n+            )\n+        df.index = df[\"date\"]\n         weekly_nav = df.resample(\"W\").last().set_index(\"date\").unit_net_value.dropna()\n         monthly_nav = df.resample(\"M\").last().set_index(\"date\").unit_net_value.dropna()\n         weekly_returns = (weekly_nav / weekly_nav.shift(1).fillna(1)).fillna(0) - 1\n         monthly_returns = (monthly_nav / monthly_nav.shift(1).fillna(1)).fillna(0) - 1\n \n         # \u6700\u957f\u56de\u64a4\u6301\u7eed\u671f\n-        max_ddd = _max_ddd(total_portfolios.unit_net_value.values, total_portfolios.index)\n+        max_ddd = _max_ddd(\n+            total_portfolios.unit_net_value.values, total_portfolios.index\n+        )\n         summary[\"max_drawdown_duration\"] = max_ddd\n-        summary.update({\n-            \"max_drawdown_duration_start_date\": str(max_ddd.start_date),\n-            \"max_drawdown_duration_end_date\": str(max_ddd.end_date),\n-            \"max_drawdown_duration_days\": (max_ddd.end_date - max_ddd.start_date).days,\n-        })\n+        summary.update(\n+            {\n+                \"max_drawdown_duration_start_date\": str(max_ddd.start_date),\n+                \"max_drawdown_duration_end_date\": str(max_ddd.end_date),\n+                \"max_drawdown_duration_days\": (\n+                    max_ddd.end_date - max_ddd.start_date\n+                ).days,\n+            }\n+        )\n \n         result_dict = {\n-            'summary': summary,\n-            'trades': trades,\n-            'portfolio': total_portfolios,\n+            \"summary\": summary,\n+            \"trades\": trades,\n+            \"portfolio\": total_portfolios,\n         }\n \n         if not trades.empty and all(\n-            EQUITIES_OID_RE.match(trade.order_book_id) for trade in trades.itertuples()  # type: ignore\n+            EQUITIES_OID_RE.match(trade.order_book_id)\n+            for trade in trades.itertuples()  # type: ignore\n         ):\n             # \u7b56\u7565\u4ec5\u4ea4\u6613\u80a1\u7968\u3001\u6307\u6570\u3001\u573a\u5185\u57fa\u91d1\u7b49\u54c1\u79cd\u65f6\u624d\u8ba1\u7b97\u6362\u624b\u7387\n             trades_values = trades.last_price * trades.last_quantity\n             market_values = total_portfolios.market_value\n             summary[\"turnover\"] = trades_values.sum() / market_values.mean() / 2\n-            avg_daily_turnover = (trades_values.groupby(trades.index.date).sum() / market_values / 2)\n-            with pd.option_context('mode.use_inf_as_na', True):\n+            avg_daily_turnover = (\n+                trades_values.groupby(trades.index.date).sum() / market_values / 2\n+            )\n+            with pd.option_context(\"mode.use_inf_as_na\", True):\n                 summary[\"avg_daily_turnover\"] = avg_daily_turnover.fillna(0).mean()\n         else:\n             summary[\"turnover\"] = np.nan\n \n         if self._benchmark:\n             df = pd.DataFrame(self._total_benchmark_portfolios)\n-            df['date'] = pd.to_datetime(df['date'])\n-            benchmark_portfolios = df.set_index('date').sort_index()\n-            df.index = df['date']\n-            weekly_b_nav = df.resample(\"W\").last().set_index(\"date\").unit_net_value.dropna()\n-            monthly_b_nav = df.resample(\"M\").last().set_index(\"date\").unit_net_value.dropna()\n-            weekly_b_returns = (weekly_b_nav / weekly_b_nav.shift(1).fillna(1)).fillna(0) - 1\n-            monthly_b_returns = (monthly_b_nav / monthly_b_nav.shift(1).fillna(1)).fillna(0) - 1\n-            result_dict['benchmark_portfolio'] = benchmark_portfolios\n+            df[\"date\"] = pd.to_datetime(df[\"date\"])\n+            benchmark_portfolios = df.set_index(\"date\").sort_index()\n+            df.index = df[\"date\"]\n+            weekly_b_nav = (\n+                df.resample(\"W\").last().set_index(\"date\").unit_net_value.dropna()\n+            )\n+            monthly_b_nav = (\n+                df.resample(\"M\").last().set_index(\"date\").unit_net_value.dropna()\n+            )\n+            weekly_b_returns = (weekly_b_nav / weekly_b_nav.shift(1).fillna(1)).fillna(\n+                0\n+            ) - 1\n+            monthly_b_returns = (\n+                monthly_b_nav / monthly_b_nav.shift(1).fillna(1)\n+            ).fillna(0) - 1\n+            result_dict[\"benchmark_portfolio\"] = benchmark_portfolios\n             # \u8d85\u989d\u6536\u76ca\u6700\u957f\u56de\u64a4\u6301\u7eed\u671f\n-            ex_returns = total_portfolios.unit_net_value / benchmark_portfolios.unit_net_value - 1\n+            ex_returns = (\n+                total_portfolios.unit_net_value / benchmark_portfolios.unit_net_value\n+                - 1\n+            )\n             max_ddd = _max_ddd(ex_returns + 1, total_portfolios.index)\n             result_dict[\"summary\"][\"excess_max_drawdown_duration\"] = max_ddd\n-            result_dict[\"summary\"][\"excess_max_drawdown_duration_start_date\"] = str(max_ddd.start_date)\n-            result_dict[\"summary\"][\"excess_max_drawdown_duration_end_date\"] = str(max_ddd.end_date)\n-            result_dict[\"summary\"][\"excess_max_drawdown_duration_days\"] = (max_ddd.end_date - max_ddd.start_date).days\n+            result_dict[\"summary\"][\"excess_max_drawdown_duration_start_date\"] = str(\n+                max_ddd.start_date\n+            )\n+            result_dict[\"summary\"][\"excess_max_drawdown_duration_end_date\"] = str(\n+                max_ddd.end_date\n+            )\n+            result_dict[\"summary\"][\"excess_max_drawdown_duration_days\"] = (\n+                max_ddd.end_date - max_ddd.start_date\n+            ).days\n         else:\n             weekly_b_returns = pandas.Series(index=weekly_returns.index)\n             monthly_b_returns = pandas.Series(index=monthly_returns.index)\n \n         # \u5468\u5ea6\u98ce\u9669\u6307\u6807\n         weekly_risk = Risk(weekly_returns, weekly_b_returns, risk_free_rate, WEEKLY)\n-        summary.update({\n-            \"weekly_alpha\": weekly_risk.alpha,\n-            \"weekly_beta\": weekly_risk.beta,\n-            \"weekly_sharpe\": weekly_risk.sharpe,\n-            \"weekly_sortino\": weekly_risk.sortino,\n-            \"weekly_information_ratio\": weekly_risk.information_ratio,\n-            \"weekly_tracking_error\": weekly_risk.annual_tracking_error,\n-            \"weekly_max_drawdown\": weekly_risk.max_drawdown,\n-            \"weekly_win_rate\": weekly_risk.win_rate,\n-            \"weekly_volatility\": weekly_risk.annual_volatility,\n-            \"weekly_ulcer_index\": weekly_risk.ulcer_index,\n-            \"weekly_ulcer_performance_index\": weekly_risk.ulcer_performance_index,\n-        })\n+        summary.update(\n+            {\n+                \"weekly_alpha\": weekly_risk.alpha,\n+                \"weekly_beta\": weekly_risk.beta,\n+                \"weekly_sharpe\": weekly_risk.sharpe,\n+                \"weekly_sortino\": weekly_risk.sortino,\n+                \"weekly_information_ratio\": weekly_risk.information_ratio,\n+                \"weekly_tracking_error\": weekly_risk.annual_tracking_error,\n+                \"weekly_max_drawdown\": weekly_risk.max_drawdown,\n+                \"weekly_win_rate\": weekly_risk.win_rate,\n+                \"weekly_volatility\": weekly_risk.annual_volatility,\n+                \"weekly_ulcer_index\": weekly_risk.ulcer_index,\n+                \"weekly_ulcer_performance_index\": weekly_risk.ulcer_performance_index,\n+            }\n+        )\n \n         # \u6708\u5ea6\u98ce\u9669\u6307\u6807\n         monthly_risk = Risk(monthly_returns, monthly_b_returns, risk_free_rate, MONTHLY)\n-        summary.update({\n-            \"monthly_sharpe\": monthly_risk.sharpe,\n-            \"monthly_volatility\": monthly_risk.annual_volatility,\n-            \"monthly_excess_win_rate\": monthly_risk.excess_win_rate,\n-        })\n+        summary.update(\n+            {\n+                \"monthly_sharpe\": monthly_risk.sharpe,\n+                \"monthly_volatility\": monthly_risk.annual_volatility,\n+                \"monthly_excess_win_rate\": monthly_risk.excess_win_rate,\n+            }\n+        )\n \n         if self._benchmark:\n-            summary.update({\n-                \"weekly_excess_ulcer_index\": weekly_risk.excess_ulcer_index,\n-                \"weekly_excess_ulcer_performance_index\": weekly_risk.excess_ulcer_performance_index,\n-            })\n+            summary.update(\n+                {\n+                    \"weekly_excess_ulcer_index\": weekly_risk.excess_ulcer_index,\n+                    \"weekly_excess_ulcer_performance_index\": weekly_risk.excess_ulcer_performance_index,\n+                }\n+            )\n \n         plots = self._plot_store.get_plots()\n         if plots:\n@@ -545,35 +683,58 @@ class AnalyserMod(AbstractMod):\n             for field in [\"market_value\", \"LONG_market_value\", \"SHORT_market_value\"]:\n                 if field not in table.columns:\n                     continue\n-                df_list.append(table[[\"order_book_id\", field]].rename(columns={field: \"market_value\"})[need_cols].dropna())\n+                df_list.append(\n+                    table[[\"order_book_id\", field]]\n+                    .rename(columns={field: \"market_value\"})[need_cols]\n+                    .dropna()\n+                )\n         if len(df_list) > 0:\n             positions_weight_df = pd.concat(df_list).dropna()\n-            positions_weight_df[\"total_value\"] = positions_weight_df.groupby(by=\"date\")[\"market_value\"].sum()\n-            positions_weight_df[\"weight\"] = positions_weight_df[\"market_value\"] / positions_weight_df[\"total_value\"]\n-            positions_weight_df = positions_weight_df.groupby(by=\"date\")[\"weight\"].describe()\n+            positions_weight_df[\"total_value\"] = positions_weight_df.groupby(by=\"date\")[\n+                \"market_value\"\n+            ].sum()\n+            positions_weight_df[\"weight\"] = (\n+                positions_weight_df[\"market_value\"] / positions_weight_df[\"total_value\"]\n+            )\n+            positions_weight_df = positions_weight_df.groupby(by=\"date\")[\n+                \"weight\"\n+            ].describe()\n         else:\n-            positions_weight_df = pd.DataFrame(columns=[\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"])\n-        positions_weight_df = positions_weight_df.reindex(total_portfolios.index).fillna(value=0)\n+            positions_weight_df = pd.DataFrame(\n+                columns=[\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n+            )\n+        positions_weight_df = positions_weight_df.reindex(\n+            total_portfolios.index\n+        ).fillna(value=0)\n         result_dict[\"positions_weight\"] = positions_weight_df\n-        result_dict[\"yearly_risk_free_rates\"] = dict(_get_yearly_risk_free_rates(data_proxy, start_date, end_date))\n+        result_dict[\"yearly_risk_free_rates\"] = dict(\n+            _get_yearly_risk_free_rates(data_proxy, start_date, end_date)\n+        )\n \n         if self._mod_config.output_file:\n-            with open(self._mod_config.output_file, 'wb') as f:\n+            with open(self._mod_config.output_file, \"wb\") as f:\n                 pickle.dump(result_dict, f)\n \n         if self._mod_config.report_save_path:\n             from .report import generate_report\n+\n             generate_report(result_dict, self._mod_config.report_save_path)\n \n         _plot = True if self._mod_config.plot else False\n \n         if _plot or self._mod_config.plot_save_file:\n             from .plot import plot_result\n+\n             plot_config = self._mod_config.plot_config\n             _plot_template_cls = PLOT_TEMPLATE.get(self._mod_config.plot, DefaultPlot)\n             plot_result(\n-                result_dict, self._mod_config.plot, self._mod_config.plot_save_file,\n-                plot_config.weekly_indicators, plot_config.open_close_points, _plot_template_cls, self._mod_config.strategy_name\n+                result_dict,\n+                self._mod_config.plot,\n+                self._mod_config.plot_save_file,\n+                plot_config.weekly_indicators,\n+                plot_config.open_close_points,\n+                _plot_template_cls,\n+                self._mod_config.strategy_name,\n             )\n \n         return result_dict\ndiff --git a/rqalpha/mod/rqalpha_mod_sys_analyser/plot/utils.py b/rqalpha/mod/rqalpha_mod_sys_analyser/plot/utils.py\nindex 0edbba3..9344bb7 100644\n--- a/rqalpha/mod/rqalpha_mod_sys_analyser/plot/utils.py\n+++ b/rqalpha/mod/rqalpha_mod_sys_analyser/plot/utils.py\n@@ -22,7 +22,8 @@ from numpy import array\n from pandas import DatetimeIndex, DataFrame, Series, to_datetime\n \n IndicatorInfo = namedtuple(\n-    \"IndicatorInfo\", (\"key\", \"label\", \"color\", \"formatter\", \"value_font_size\", \"label_width_multiplier\")\n+    \"IndicatorInfo\",\n+    (\"key\", \"label\", \"color\", \"formatter\", \"value_font_size\", \"label_width_multiplier\"),\n )\n LineInfo = namedtuple(\"LineInfo\", (\"label\", \"color\", \"alpha\", \"linewidth\"))\n SpotInfo = namedtuple(\"SpotInfo\", (\"label\", \"marker\", \"color\", \"markersize\", \"alpha\"))\n@@ -35,7 +36,9 @@ class IndexRange(namedtuple(\"IndexRange\", (\"start\", \"end\", \"start_date\", \"end_da\n \n     @property\n     def repr(self):\n-        return \"{}~{}, {} days\".format(self.start_date, self.end_date, (self.end_date - self.start_date).days)\n+        return \"{}~{}, {} days\".format(\n+            self.start_date, self.end_date, (self.end_date - self.start_date).days\n+        )\n \n \n def max_dd(arr: array, index: DatetimeIndex) -> IndexRange:\n@@ -80,6 +83,12 @@ def weekly_returns(portfolio: DataFrame) -> Series:\n \n \n def trading_dates_index(trades: DataFrame, position_effect, index: DatetimeIndex):\n-    return index.searchsorted(\n-        to_datetime(trades[trades.position_effect == position_effect].trading_datetime), side=\"right\"\n-    ) - 1\n+    return (\n+        index.searchsorted(\n+            to_datetime(\n+                trades[trades.position_effect == position_effect].trading_datetime\n+            ),\n+            side=\"right\",\n+        )\n+        - 1\n+    )\ndiff --git a/test.py b/test.py\nindex cde4a5d..d1c44b6 100644\n--- a/test.py\n+++ b/test.py\n@@ -34,25 +34,34 @@ pd.set_option(\"display.width\", 160)\n \n \n def run_tests(file_path=None):\n-    tests = {f.replace(\".py\", \"\"): f for f in (\n-        (file_path, ) if file_path else (f for f in os.listdir(TEST_DIR) if f.find(\"test\") == 0)\n-    )}\n+    tests = {\n+        f.replace(\".py\", \"\"): f\n+        for f in (\n+            (file_path,)\n+            if file_path\n+            else (f for f in os.listdir(TEST_DIR) if f.find(\"test\") == 0)\n+        )\n+    }\n     error_map = {}\n     for name, filename in tests.items():\n         err = run_test(filename)\n         if err is not None:\n             error_map[name] = err\n     for filename, err in iteritems(error_map):\n-        print(u\"*\" * 20, u\"[{}]did not pass!\".format(filename), u\"*\" * 20)\n+        print(\"*\" * 20, \"[{}]did not pass!\".format(filename), \"*\" * 20)\n         print(err)\n-    print(u\"=\" * 40)\n-    print(\"[{}|{}] strategies has been passed!\".format(len(tests) - len(error_map), len(tests)))\n+    print(\"=\" * 40)\n+    print(\n+        \"[{}|{}] strategies has been passed!\".format(\n+            len(tests) - len(error_map), len(tests)\n+        )\n+    )\n     return len(error_map)\n \n \n def run_test(filename):\n     config = {\"base\": {\"strategy_file\": os.path.join(TEST_DIR, filename)}}\n-    print(u\"Start test: \" + str(config[\"base\"][\"strategy_file\"]))\n+    print(\"Start test: \" + str(config[\"base\"][\"strategy_file\"]))\n     result_dict = run(config)[\"sys_analyser\"]\n \n     old_pickle_file = os.path.join(TEST_OUT, filename.replace(\".py\", \".pkl\"))\n@@ -66,7 +75,9 @@ def run_test(filename):\n         old_result_dict = pd.read_pickle(old_pickle_file)\n \n         # \u6bd4\u8f83 portfolios\n-        old_df = old_result_dict[\"portfolio\"].replace([np.nan, np.inf, -np.inf], 0).round(0)\n+        old_df = (\n+            old_result_dict[\"portfolio\"].replace([np.nan, np.inf, -np.inf], 0).round(0)\n+        )\n         df = result_dict[\"portfolio\"].replace([np.nan, np.inf, -np.inf], 0).round(0)\n         try:\n             del old_df[\"dividend_receivable\"]\n@@ -120,7 +131,7 @@ def is_enable_coverage():\n \n def test_api(specific_test=None):\n     # FIXME: Error msg is hard to understand @zjuguxi\n-    print(u\"Testing API......\")\n+    print(\"Testing API......\")\n \n     # from tests.api import test_strategies as test_api_strategies\n     # from tests.mod import test_strategies as test_mod_strategies\n@@ -132,7 +143,7 @@ def test_api(specific_test=None):\n         print(\"running\", strategy[\"name\"])\n         run_func(**strategy)\n \n-    print(u\"API test ends.\")\n+    print(\"API test ends.\")\n \n \n def test_strategy():\n"
    },
    {
        "id": 290,
        "sha_fail": "1a5a2bdf66d72b86a7b459bba8842598b867acef",
        "diff": "diff --git a/core/thread.py b/core/thread.py\nindex d40a79d..8223b23 100644\n--- a/core/thread.py\n+++ b/core/thread.py\n@@ -19,7 +19,6 @@ from lottie.importers import importers as l_importers\n from lottie.exporters import exporters as l_exporters\n \n from core.models import DMDisabled, DummyMessage, getLogger\n-from core.time import human_timedelta\n from core.utils import (\n     is_image_url,\n     parse_channel_topic,\n@@ -46,7 +45,9 @@ class Thread:\n         manager: \"ThreadManager\",\n         recipient: typing.Union[discord.Member, discord.User, int],\n         channel: typing.Union[discord.DMChannel, discord.TextChannel] = None,\n-        other_recipients: typing.List[typing.Union[discord.Member, discord.User]] = None,\n+        other_recipients: typing.List[\n+            typing.Union[discord.Member, discord.User]\n+        ] = None,\n     ):\n         self.manager = manager\n         self.bot = manager.bot\n@@ -68,7 +69,9 @@ class Thread:\n         self._cancelled = False\n         # --- SNOOZE STATE ---\n         self.snoozed = False  # True if thread is snoozed\n-        self.snooze_data = None  # Dict with channel/category/position/messages for restoration\n+        self.snooze_data = (\n+            None  # Dict with channel/category/position/messages for restoration\n+        )\n         self.log_key = None  # Ensure log_key always exists\n \n     def __repr__(self):\n@@ -82,7 +85,9 @@ class Thread:\n     async def wait_until_ready(self) -> None:\n         \"\"\"Blocks execution until the thread is fully set up.\"\"\"\n         # timeout after 30 seconds\n-        task = self.bot.loop.create_task(asyncio.wait_for(self._ready_event.wait(), timeout=25))\n+        task = self.bot.loop.create_task(\n+            asyncio.wait_for(self._ready_event.wait(), timeout=25)\n+        )\n         self.wait_tasks.append(task)\n         try:\n             await task\n@@ -143,13 +148,13 @@ class Thread:\n         if not self.log_key:\n             # Try to fetch from DB using channel_id\n             log_entry = await self.bot.api.get_log(self.channel.id)\n-            if log_entry and 'key' in log_entry:\n-                self.log_key = log_entry['key']\n+            if log_entry and \"key\" in log_entry:\n+                self.log_key = log_entry[\"key\"]\n             # Fallback: try by recipient id\n-            elif hasattr(self, 'id'):\n+            elif hasattr(self, \"id\"):\n                 log_entry = await self.bot.api.get_log(str(self.id))\n-                if log_entry and 'key' in log_entry:\n-                    self.log_key = log_entry['key']\n+                if log_entry and \"key\" in log_entry:\n+                    self.log_key = log_entry[\"key\"]\n \n         self.snooze_data = {\n             \"category_id\": channel.category_id,\n@@ -158,7 +163,9 @@ class Thread:\n             \"topic\": channel.topic,\n             \"slowmode_delay\": channel.slowmode_delay,\n             \"nsfw\": channel.nsfw,\n-            \"overwrites\": [(role.id, perm._values) for role, perm in channel.overwrites.items()],\n+            \"overwrites\": [\n+                (role.id, perm._values) for role, perm in channel.overwrites.items()\n+            ],\n             \"messages\": [\n                 {\n                     \"author_id\": m.author.id,\n@@ -171,10 +178,14 @@ class Thread:\n                         \"mod_only\"\n                         if (\n                             m.embeds\n-                            and getattr(m.embeds[0], 'author', None)\n+                            and getattr(m.embeds[0], \"author\", None)\n                             and (\n-                                getattr(m.embeds[0].author, 'name', '').startswith('Note') or\n-                                getattr(m.embeds[0].author, 'name', '').startswith('Persistent Note')\n+                                getattr(m.embeds[0].author, \"name\", \"\").startswith(\n+                                    \"Note\"\n+                                )\n+                                or getattr(m.embeds[0].author, \"name\", \"\").startswith(\n+                                    \"Persistent Note\"\n+                                )\n                             )\n                         )\n                         else None\n@@ -185,12 +196,15 @@ class Thread:\n                 # Only include if not already internal/note, safe check for embed author\n                 if not (\n                     m.embeds\n-                    and getattr(m.embeds[0], 'author', None)\n+                    and getattr(m.embeds[0], \"author\", None)\n                     and (\n-                        getattr(m.embeds[0].author, 'name', '').startswith('Note') or\n-                        getattr(m.embeds[0].author, 'name', '').startswith('Persistent Note')\n+                        getattr(m.embeds[0].author, \"name\", \"\").startswith(\"Note\")\n+                        or getattr(m.embeds[0].author, \"name\", \"\").startswith(\n+                            \"Persistent Note\"\n+                        )\n                     )\n-                ) and getattr(m, \"type\", None) not in (\"internal\", \"note\")\n+                )\n+                and getattr(m, \"type\", None) not in (\"internal\", \"note\")\n             ],\n             \"snoozed_by\": getattr(moderator, \"name\", None) if moderator else None,\n             \"snooze_command\": command_used,\n@@ -251,7 +265,9 @@ class Thread:\n         self.log_key = self.snooze_data.get(\"log_key\")\n         # Replay messages\n         for msg in self.snooze_data[\"messages\"]:\n-            author = self.bot.get_user(msg[\"author_id\"]) or await self.bot.get_or_fetch_user(msg[\"author_id\"])\n+            author = self.bot.get_user(\n+                msg[\"author_id\"]\n+            ) or await self.bot.get_or_fetch_user(msg[\"author_id\"])\n             content = msg[\"content\"]\n             embeds = [discord.Embed.from_dict(e) for e in msg.get(\"embeds\", []) if e]\n             attachments = msg.get(\"attachments\", [])\n@@ -261,13 +277,26 @@ class Thread:\n                 continue  # Skip empty messages\n             author_is_mod = msg[\"author_id\"] not in [r.id for r in self.recipients]\n             if author_is_mod:\n-                username = msg.get(\"author_name\") or (getattr(author, \"name\", None)) or \"Unknown\"\n+                username = (\n+                    msg.get(\"author_name\")\n+                    or (getattr(author, \"name\", None))\n+                    or \"Unknown\"\n+                )\n                 user_id = msg.get(\"author_id\")\n                 if embeds:\n-                    embeds[0].set_author(name=f\"{username} ({user_id})\", icon_url=author.display_avatar.url if author and hasattr(author, \"display_avatar\") else None)\n+                    embeds[0].set_author(\n+                        name=f\"{username} ({user_id})\",\n+                        icon_url=author.display_avatar.url\n+                        if author and hasattr(author, \"display_avatar\")\n+                        else None,\n+                    )\n                     await channel.send(embeds=embeds)\n                 else:\n-                    formatted = f\"**{username} ({user_id})**: {content}\" if content else f\"**{username} ({user_id})**\"\n+                    formatted = (\n+                        f\"**{username} ({user_id})**: {content}\"\n+                        if content\n+                        else f\"**{username} ({user_id})**\"\n+                    )\n                     await channel.send(formatted)\n             else:\n                 await channel.send(content=content or None, embeds=embeds or None)\n@@ -279,41 +308,63 @@ class Thread:\n         if self.log_key:\n             result = await self.bot.api.logs.update_one(\n                 {\"key\": self.log_key},\n-                {\"$set\": {\"snoozed\": False, \"channel_id\": str(channel.id)}, \"$unset\": {\"snooze_data\": \"\"}},\n+                {\n+                    \"$set\": {\"snoozed\": False, \"channel_id\": str(channel.id)},\n+                    \"$unset\": {\"snooze_data\": \"\"},\n+                },\n             )\n         else:\n             result = await self.bot.api.logs.update_one(\n                 {\"recipient.id\": str(self.id)},\n-                {\"$set\": {\"snoozed\": False, \"channel_id\": str(channel.id)}, \"$unset\": {\"snooze_data\": \"\"}},\n+                {\n+                    \"$set\": {\"snoozed\": False, \"channel_id\": str(channel.id)},\n+                    \"$unset\": {\"snooze_data\": \"\"},\n+                },\n             )\n             if result.modified_count == 0:\n                 result = await self.bot.api.logs.update_one(\n                     {\"channel_id\": str(channel.id)},\n-                    {\"$set\": {\"snoozed\": False, \"channel_id\": str(channel.id)}, \"$unset\": {\"snooze_data\": \"\"}},\n+                    {\n+                        \"$set\": {\"snoozed\": False, \"channel_id\": str(channel.id)},\n+                        \"$unset\": {\"snooze_data\": \"\"},\n+                    },\n                 )\n         import logging\n \n         logging.info(f\"[UNSNOOZE] DB update result: {result.modified_count}\")\n         # Notify in the configured channel\n         notify_channel = self.bot.config.get(\"unsnooze_notify_channel\") or \"thread\"\n-        notify_text = self.bot.config.get(\"unsnooze_text\") or \"This thread has been unsnoozed and restored.\"\n+        notify_text = (\n+            self.bot.config.get(\"unsnooze_text\")\n+            or \"This thread has been unsnoozed and restored.\"\n+        )\n         if notify_channel == \"thread\":\n             await channel.send(notify_text)\n         else:\n             ch = self.bot.get_channel(int(notify_channel))\n             if ch:\n-                await ch.send(f\"Thread for user <@{self.id}> has been unsnoozed and restored.\")\n+                await ch.send(\n+                    f\"Thread for user <@{self.id}> has been unsnoozed and restored.\"\n+                )\n         # Show who ran the snooze command and the command used\n         # Use snooze_data_for_notify to avoid accessing self.snooze_data after it is set to None\n-        snoozed_by = snooze_data_for_notify.get(\"snoozed_by\") if snooze_data_for_notify else None\n-        snooze_command = snooze_data_for_notify.get(\"snooze_command\") if snooze_data_for_notify else None\n+        snoozed_by = (\n+            snooze_data_for_notify.get(\"snoozed_by\") if snooze_data_for_notify else None\n+        )\n+        snooze_command = (\n+            snooze_data_for_notify.get(\"snooze_command\")\n+            if snooze_data_for_notify\n+            else None\n+        )\n         if snoozed_by or snooze_command:\n             info = f\"Snoozed by: {snoozed_by or 'Unknown'} | Command: {snooze_command or '?snooze'}\"\n             await channel.send(info)\n         return True\n \n     @classmethod\n-    async def from_channel(cls, manager: \"ThreadManager\", channel: discord.TextChannel) -> \"Thread\":\n+    async def from_channel(\n+        cls, manager: \"ThreadManager\", channel: discord.TextChannel\n+    ) -> \"Thread\":\n         # there is a chance it grabs from another recipient's main thread\n         _, recipient_id, other_ids = parse_channel_topic(channel.topic)\n \n@@ -338,7 +389,11 @@ class Thread:\n         if self._genesis_message is None:\n             async for m in self.channel.history(limit=5, oldest_first=True):\n                 if m.author == self.bot.user:\n-                    if m.embeds and m.embeds[0].fields and m.embeds[0].fields[0].name == \"Roles\":\n+                    if (\n+                        m.embeds\n+                        and m.embeds[0].fields\n+                        and m.embeds[0].fields[0].name == \"Roles\"\n+                    ):\n                         self._genesis_message = m\n \n         return self._genesis_message\n@@ -349,7 +404,11 @@ class Thread:\n         recipient = self.recipient\n \n         # in case it creates a channel outside of category\n-        overwrites = {self.bot.modmail_guild.default_role: discord.PermissionOverwrite(read_messages=False)}\n+        overwrites = {\n+            self.bot.modmail_guild.default_role: discord.PermissionOverwrite(\n+                read_messages=False\n+            )\n+        }\n \n         category = category or self.bot.main_category\n \n@@ -357,7 +416,9 @@ class Thread:\n             overwrites = {}\n \n         try:\n-            channel = await create_thread_channel(self.bot, recipient, category, overwrites)\n+            channel = await create_thread_channel(\n+                self.bot, recipient, category, overwrites\n+            )\n         except discord.HTTPException as e:  # Failed to create due to missing perms.\n             logger.critical(\"An error occurred while creating a thread.\", exc_info=True)\n             self.manager.cache.pop(self.id)\n@@ -381,7 +442,9 @@ class Thread:\n \n             log_count = sum(1 for log in log_data if not log[\"open\"])\n         except Exception:\n-            logger.error(\"An error occurred while posting logs to the database.\", exc_info=True)\n+            logger.error(\n+                \"An error occurred while posting logs to the database.\", exc_info=True\n+            )\n             log_url = log_count = None\n             # ensure core functionality still works\n \n@@ -393,7 +456,9 @@ class Thread:\n             mention = self.bot.config[\"mention\"]\n \n         async def send_genesis_message():\n-            info_embed = self._format_info_embed(recipient, log_url, log_count, self.bot.main_color)\n+            info_embed = self._format_info_embed(\n+                recipient, log_url, log_count, self.bot.main_color\n+            )\n             try:\n                 msg = await channel.send(mention, embed=info_embed)\n                 self.bot.loop.create_task(msg.pin())\n@@ -419,7 +484,10 @@ class Thread:\n                 footer = self.bot.config[\"thread_creation_footer\"]\n \n             embed.set_footer(\n-                text=footer, icon_url=self.bot.get_guild_icon(guild=self.bot.modmail_guild, size=128)\n+                text=footer,\n+                icon_url=self.bot.get_guild_icon(\n+                    guild=self.bot.modmail_guild, size=128\n+                ),\n             )\n             embed.title = self.bot.config[\"thread_creation_title\"]\n \n@@ -460,8 +528,12 @@ class Thread:\n                     \"content\": note[\"message\"],\n                     \"author\": Author(),\n                 }\n-                message = discord.Message(state=State(), channel=self.channel, data=data)\n-                ids[note[\"_id\"]] = str((await self.note(message, persistent=True, thread_creation=True)).id)\n+                message = discord.Message(\n+                    state=State(), channel=self.channel, data=data\n+                )\n+                ids[note[\"_id\"]] = str(\n+                    (await self.note(message, persistent=True, thread_creation=True)).id\n+                )\n \n             await self.bot.api.update_note_ids(ids)\n \n@@ -526,7 +598,9 @@ class Thread:\n             footer = f\"User ID: {user.id}\"\n \n         if member is not None:\n-            embed.set_author(name=str(user), icon_url=member.display_avatar.url, url=log_url)\n+            embed.set_author(\n+                name=str(user), icon_url=member.display_avatar.url, url=log_url\n+            )\n \n             if self.bot.config[\"thread_show_join_age\"]:\n                 joined = discord.utils.format_dt(member.joined_at, \"R\")\n@@ -538,7 +612,9 @@ class Thread:\n                 embed.add_field(name=\"Roles\", value=role_names, inline=True)\n             embed.set_footer(text=footer)\n         else:\n-            embed.set_author(name=str(user), icon_url=user.display_avatar.url, url=log_url)\n+            embed.set_author(\n+                name=str(user), icon_url=user.display_avatar.url, url=log_url\n+            )\n             embed.set_footer(text=f\"{footer} \u2022 (not in main server)\")\n \n         embed.description += \", \".join(user_info)\n@@ -552,13 +628,17 @@ class Thread:\n \n         mutual_guilds = [g for g in self.bot.guilds if user in g.members]\n         if member is None or len(mutual_guilds) > 1:\n-            embed.add_field(name=\"Mutual Server(s)\", value=\", \".join(g.name for g in mutual_guilds))\n+            embed.add_field(\n+                name=\"Mutual Server(s)\", value=\", \".join(g.name for g in mutual_guilds)\n+            )\n \n         return embed\n \n     async def _close_after(self, after, closer, silent, delete_channel, message):\n         await asyncio.sleep(after)\n-        return self.bot.loop.create_task(self._close(closer, silent, delete_channel, message, True))\n+        return self.bot.loop.create_task(\n+            self._close(closer, silent, delete_channel, message, True)\n+        )\n \n     async def close(\n         self,\n@@ -591,7 +671,9 @@ class Thread:\n             self.bot.config[\"closures\"][str(self.id)] = items\n             await self.bot.config.update()\n \n-            task = asyncio.create_task(self._close_after(after, closer, silent, delete_channel, message))\n+            task = asyncio.create_task(\n+                self._close_after(after, closer, silent, delete_channel, message)\n+            )\n \n             if auto_close:\n                 self.auto_close_task = task\n@@ -600,7 +682,9 @@ class Thread:\n         else:\n             await self._close(closer, silent, delete_channel, message)\n \n-    async def _close(self, closer, silent=False, delete_channel=True, message=None, scheduled=False):\n+    async def _close(\n+        self, closer, silent=False, delete_channel=True, message=None, scheduled=False\n+    ):\n         if self.channel:\n             self.manager.closing.add(self.channel.id)\n         try:\n@@ -642,9 +726,7 @@ class Thread:\n             prefix = self.bot.config[\"log_url_prefix\"].strip(\"/\")\n             if prefix == \"NONE\":\n                 prefix = \"\"\n-            log_url = (\n-                f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{log_data['key']}\"\n-            )\n+            log_url = f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{log_data['key']}\"\n \n             if log_data[\"title\"]:\n                 sneak_peak = log_data[\"title\"]\n@@ -681,7 +763,9 @@ class Thread:\n \n         event = \"Thread Closed as Scheduled\" if scheduled else \"Thread Closed\"\n         # embed.set_author(name=f\"Event: {event}\", url=log_url)\n-        embed.set_footer(text=f\"{event} by {_closer}\", icon_url=closer.display_avatar.url)\n+        embed.set_footer(\n+            text=f\"{event} by {_closer}\", icon_url=closer.display_avatar.url\n+        )\n         embed.timestamp = discord.utils.utcnow()\n \n         tasks = [self.bot.config.update()]\n@@ -689,7 +773,11 @@ class Thread:\n         if self.bot.log_channel is not None and self.channel is not None:\n             if self.bot.config[\"show_log_url_button\"]:\n                 view = discord.ui.View()\n-                view.add_item(discord.ui.Button(label=\"Log link\", url=log_url, style=discord.ButtonStyle.url))\n+                view.add_item(\n+                    discord.ui.Button(\n+                        label=\"Log link\", url=log_url, style=discord.ButtonStyle.url\n+                    )\n+                )\n             else:\n                 view = None\n             tasks.append(self.bot.log_channel.send(embed=embed, view=view))\n@@ -710,12 +798,18 @@ class Thread:\n                 message = self.bot.config[\"thread_close_response\"]\n \n         message = self.bot.formatter.format(\n-            message, closer=closer, loglink=log_url, logkey=log_data[\"key\"] if log_data else None\n+            message,\n+            closer=closer,\n+            loglink=log_url,\n+            logkey=log_data[\"key\"] if log_data else None,\n         )\n \n         embed.description = message\n         footer = self.bot.config[\"thread_close_footer\"]\n-        embed.set_footer(text=footer, icon_url=self.bot.get_guild_icon(guild=self.bot.guild, size=128))\n+        embed.set_footer(\n+            text=footer,\n+            icon_url=self.bot.get_guild_icon(guild=self.bot.guild, size=128),\n+        )\n \n         if not silent:\n             for user in self.recipients:\n@@ -738,7 +832,9 @@ class Thread:\n             if self.channel:\n                 self.manager.closing.discard(self.channel.id)\n \n-        self.bot.dispatch(\"thread_close\", self, closer, silent, delete_channel, message, scheduled)\n+        self.bot.dispatch(\n+            \"thread_close\", self, closer, silent, delete_channel, message, scheduled\n+        )\n \n     async def cancel_closure(self, auto_close: bool = False, all: bool = False) -> None:\n         if self.close_task is not None and (not auto_close or all):\n@@ -770,7 +866,9 @@ class Thread:\n         human_time = discord.utils.format_dt(reset_time)\n \n         if self.bot.config.get(\"thread_auto_close_silently\"):\n-            return await self.close(closer=self.bot.user, silent=True, after=int(seconds), auto_close=True)\n+            return await self.close(\n+                closer=self.bot.user, silent=True, after=int(seconds), auto_close=True\n+            )\n \n         # Grab message\n         close_message = self.bot.formatter.format(\n@@ -786,7 +884,12 @@ class Thread:\n                 time_marker_regex,\n             )\n \n-        await self.close(closer=self.bot.user, after=int(seconds), message=close_message, auto_close=True)\n+        await self.close(\n+            closer=self.bot.user,\n+            after=int(seconds),\n+            message=close_message,\n+            auto_close=True,\n+        )\n \n     async def find_linked_messages(\n         self,\n@@ -796,7 +899,11 @@ class Thread:\n         note: bool = True,\n     ) -> typing.Tuple[discord.Message, typing.List[typing.Optional[discord.Message]]]:\n         if message1 is not None:\n-            if not message1.embeds or not message1.embeds[0].author.url or message1.author != self.bot.user:\n+            if (\n+                not message1.embeds\n+                or not message1.embeds[0].author.url\n+                or message1.author != self.bot.user\n+            ):\n                 raise ValueError(\"Malformed thread message.\")\n \n         elif message_id is not None:\n@@ -813,13 +920,17 @@ class Thread:\n             ):\n                 raise ValueError(\"Thread message not found.\")\n \n-            if message1.embeds[0].footer and \"Internal Message\" in message1.embeds[0].footer.text:\n+            if (\n+                message1.embeds[0].footer\n+                and \"Internal Message\" in message1.embeds[0].footer.text\n+            ):\n                 if not note:\n                     raise ValueError(\"Thread message not found.\")\n                 return message1, None\n \n             if message1.embeds[0].color.value != self.bot.mod_color and not (\n-                either_direction and message1.embeds[0].color.value == self.bot.recipient_color\n+                either_direction\n+                and message1.embeds[0].color.value == self.bot.recipient_color\n             ):\n                 raise ValueError(\"Thread message not found.\")\n         else:\n@@ -830,7 +941,11 @@ class Thread:\n                     and message1.embeds[0].color\n                     and (\n                         message1.embeds[0].color.value == self.bot.mod_color\n-                        or (either_direction and message1.embeds[0].color.value == self.bot.recipient_color)\n+                        or (\n+                            either_direction\n+                            and message1.embeds[0].color.value\n+                            == self.bot.recipient_color\n+                        )\n                     )\n                     and message1.embeds[0].author.url.split(\"#\")[-1].isdigit()\n                     and message1.author == self.bot.user\n@@ -865,7 +980,9 @@ class Thread:\n \n         raise ValueError(\"DM message not found.\")\n \n-    async def edit_message(self, message_id: typing.Optional[int], message: str) -> None:\n+    async def edit_message(\n+        self, message_id: typing.Optional[int], message: str\n+    ) -> None:\n         try:\n             message1, *message2 = await self.find_linked_messages(message_id)\n         except ValueError:\n@@ -875,8 +992,14 @@ class Thread:\n         embed1 = message1.embeds[0]\n         embed1.description = message\n \n-        tasks = [self.bot.api.edit_message(message1.id, message), message1.edit(embed=embed1)]\n-        if message1.embeds[0].footer and \"Persistent Internal Message\" in message1.embeds[0].footer.text:\n+        tasks = [\n+            self.bot.api.edit_message(message1.id, message),\n+            message1.edit(embed=embed1),\n+        ]\n+        if (\n+            message1.embeds[0].footer\n+            and \"Persistent Internal Message\" in message1.embeds[0].footer.text\n+        ):\n             tasks += [self.bot.api.edit_note(message1.id, message)]\n         else:\n             for m2 in message2:\n@@ -891,7 +1014,9 @@ class Thread:\n         self, message: typing.Union[int, discord.Message] = None, note: bool = True\n     ) -> None:\n         if isinstance(message, discord.Message):\n-            message1, *message2 = await self.find_linked_messages(message1=message, note=note)\n+            message1, *message2 = await self.find_linked_messages(\n+                message1=message, note=note\n+            )\n         else:\n             message1, *message2 = await self.find_linked_messages(message, note=note)\n         tasks = []\n@@ -903,7 +1028,10 @@ class Thread:\n             if m2 is not None:\n                 tasks += [m2.delete()]\n \n-        if message1.embeds[0].footer and \"Persistent Internal Message\" in message1.embeds[0].footer.text:\n+        if (\n+            message1.embeds[0].footer\n+            and \"Persistent Internal Message\" in message1.embeds[0].footer.text\n+        ):\n             tasks += [self.bot.api.delete_note(message1.id)]\n \n         if tasks:\n@@ -983,9 +1111,13 @@ class Thread:\n             embed = msg.embeds[0]\n             if isinstance(msg.channel, discord.TextChannel):\n                 # just for thread channel, we put the old message in embed field\n-                embed.add_field(name=\"**Edited, former message:**\", value=embed.description)\n+                embed.add_field(\n+                    name=\"**Edited, former message:**\", value=embed.description\n+                )\n             embed.description = content\n-            await asyncio.gather(self.bot.api.edit_message(message.id, content), msg.edit(embed=embed))\n+            await asyncio.gather(\n+                self.bot.api.edit_message(message.id, content), msg.edit(embed=embed)\n+            )\n \n     async def note(\n         self, message: discord.Message, persistent=False, thread_creation=False\n@@ -1003,7 +1135,9 @@ class Thread:\n \n         # Log as 'system' type for logviewer visibility\n         self.bot.loop.create_task(\n-            self.bot.api.append_log(message, message_id=msg.id, channel_id=self.channel.id, type_=\"system\")\n+            self.bot.api.append_log(\n+                message, message_id=msg.id, channel_id=self.channel.id, type_=\"system\"\n+            )\n         )\n \n         return msg\n@@ -1070,7 +1204,11 @@ class Thread:\n         else:\n             # Send the same thing in the thread channel.\n             msg = await self.send(\n-                message, destination=self.channel, from_mod=True, anonymous=anonymous, plain=plain\n+                message,\n+                destination=self.channel,\n+                from_mod=True,\n+                anonymous=anonymous,\n+                plain=plain,\n             )\n \n             tasks.append(\n@@ -1112,7 +1250,9 @@ class Thread:\n         thread_creation: bool = False,\n     ) -> None:\n         if not note and from_mod:\n-            self.bot.loop.create_task(self._restart_close_timer())  # Start or restart thread auto close\n+            self.bot.loop.create_task(\n+                self._restart_close_timer()\n+            )  # Start or restart thread auto close\n \n         if self.close_task is not None:\n             # cancel closing if a thread message is sent.\n@@ -1130,12 +1270,16 @@ class Thread:\n             await self.wait_until_ready()\n \n         if not from_mod and not note:\n-            self.bot.loop.create_task(self.bot.api.append_log(message, channel_id=self.channel.id))\n+            self.bot.loop.create_task(\n+                self.bot.api.append_log(message, channel_id=self.channel.id)\n+            )\n \n         destination = destination or self.channel\n \n         if destination is None:\n-            logger.error(\"Attempted to send a message to a thread with no channel (destination is None).\")\n+            logger.error(\n+                \"Attempted to send a message to a thread with no channel (destination is None).\"\n+            )\n             return\n         try:\n             await destination.typing()\n@@ -1174,14 +1318,22 @@ class Thread:\n         if self.bot.config[\"show_timestamp\"]:\n             embed.timestamp = message.created_at\n \n-        system_avatar_url = \"https://discordapp.com/assets/f78426a064bc9dd24847519259bc42af.png\"\n+        system_avatar_url = (\n+            \"https://discordapp.com/assets/f78426a064bc9dd24847519259bc42af.png\"\n+        )\n \n         if not note:\n-            if anonymous and from_mod and not isinstance(destination, discord.TextChannel):\n+            if (\n+                anonymous\n+                and from_mod\n+                and not isinstance(destination, discord.TextChannel)\n+            ):\n                 # Anonymously sending to the user.\n                 tag = self.bot.config[\"mod_tag\"]\n                 if tag is None:\n-                    tag = str(get_top_role(author, self.bot.config[\"use_hoisted_top_role\"]))\n+                    tag = str(\n+                        get_top_role(author, self.bot.config[\"use_hoisted_top_role\"])\n+                    )\n                 name = self.bot.config[\"anon_username\"]\n                 if name is None:\n                     name = tag\n@@ -1252,7 +1404,11 @@ class Thread:\n                 discord.StickerFormatType.gif,\n             ):\n                 images.append(\n-                    (f\"https://media.discordapp.net/stickers/{i.id}.{i.format.file_extension}\", i.name, True)\n+                    (\n+                        f\"https://media.discordapp.net/stickers/{i.id}.{i.format.file_extension}\",\n+                        i.name,\n+                        True,\n+                    )\n                 )\n             elif i.format == discord.StickerFormatType.lottie:\n                 # save the json lottie representation\n@@ -1292,14 +1448,17 @@ class Thread:\n \n         for url, filename, is_sticker in images:\n             if (\n-                not prioritize_uploads or ((url is None or is_image_url(url)) and filename)\n+                not prioritize_uploads\n+                or ((url is None or is_image_url(url)) and filename)\n             ) and not embedded_image:\n                 if url is not None:\n                     embed.set_image(url=url)\n                 if filename:\n                     if is_sticker:\n                         if url is None:\n-                            description = f\"{filename}: Unable to retrieve sticker image\"\n+                            description = (\n+                                f\"{filename}: Unable to retrieve sticker image\"\n+                            )\n                         else:\n                             description = f\"[{filename}]({url})\"\n                         embed.add_field(name=\"Sticker\", value=description)\n@@ -1321,7 +1480,9 @@ class Thread:\n                     img_embed.url = url\n                 if filename is not None:\n                     img_embed.title = filename\n-                img_embed.set_footer(text=f\"Additional Image Upload ({additional_count})\")\n+                img_embed.set_footer(\n+                    text=f\"Additional Image Upload ({additional_count})\"\n+                )\n                 img_embed.timestamp = message.created_at\n                 additional_images.append(destination.send(embed=img_embed))\n                 additional_count += 1\n@@ -1329,13 +1490,17 @@ class Thread:\n         file_upload_count = 1\n \n         for url, filename, _ in attachments:\n-            embed.add_field(name=f\"File upload ({file_upload_count})\", value=f\"[{filename}]({url})\")\n+            embed.add_field(\n+                name=f\"File upload ({file_upload_count})\", value=f\"[{filename}]({url})\"\n+            )\n             file_upload_count += 1\n \n         if from_mod:\n             embed.colour = self.bot.mod_color\n             if note:\n-                embed.set_footer(text=f\"{'Persistent' if persistent_note else ''} Internal Message\")\n+                embed.set_footer(\n+                    text=f\"{'Persistent' if persistent_note else ''} Internal Message\"\n+                )\n             # Anonymous reply sent in thread channel\n             elif anonymous and isinstance(destination, discord.TextChannel):\n                 embed.set_footer(text=\"Anonymous Reply\")\n@@ -1343,7 +1508,11 @@ class Thread:\n             elif not anonymous:\n                 mod_tag = self.bot.config[\"mod_tag\"]\n                 if mod_tag is None:\n-                    mod_tag = str(get_top_role(message.author, self.bot.config[\"use_hoisted_top_role\"]))\n+                    mod_tag = str(\n+                        get_top_role(\n+                            message.author, self.bot.config[\"use_hoisted_top_role\"]\n+                        )\n+                    )\n                 embed.set_footer(text=mod_tag)  # Normal messages\n             else:\n                 embed.set_footer(text=self.bot.config[\"anon_tag\"])\n@@ -1364,7 +1533,9 @@ class Thread:\n             and self.bot.config[\"dm_disabled\"] == DMDisabled.ALL_THREADS\n             and destination != self.channel\n         ):\n-            logger.info(\"Sending a message to %s when DM disabled is set.\", self.recipient)\n+            logger.info(\n+                \"Sending a message to %s when DM disabled is set.\", self.recipient\n+            )\n \n         try:\n             await destination.typing()\n@@ -1449,13 +1620,17 @@ class Thread:\n             embed.add_field(name=\"Other Recipients\", value=value, inline=False)\n         else:\n             if value:\n-                embed.set_field_at(index, name=\"Other Recipients\", value=value, inline=False)\n+                embed.set_field_at(\n+                    index, name=\"Other Recipients\", value=value, inline=False\n+                )\n             else:\n                 embed.remove_field(index)\n \n         await genesis_message.edit(embed=embed)\n \n-    async def add_users(self, users: typing.List[typing.Union[discord.Member, discord.User]]) -> None:\n+    async def add_users(\n+        self, users: typing.List[typing.Union[discord.Member, discord.User]]\n+    ) -> None:\n         topic = \"\"\n         title, _, _ = parse_channel_topic(self.channel.topic)\n         if title is not None:\n@@ -1473,7 +1648,9 @@ class Thread:\n         await self.channel.edit(topic=topic)\n         await self._update_users_genesis()\n \n-    async def remove_users(self, users: typing.List[typing.Union[discord.Member, discord.User]]) -> None:\n+    async def remove_users(\n+        self, users: typing.List[typing.Union[discord.Member, discord.User]]\n+    ) -> None:\n         topic = \"\"\n         title, user_id, _ = parse_channel_topic(self.channel.topic)\n         if title is not None:\n@@ -1521,13 +1698,18 @@ class ThreadManager:\n         recipient_id: int = None,\n     ) -> typing.Optional[Thread]:\n         \"\"\"Finds a thread from cache or from discord channel topics.\"\"\"\n-        if recipient is None and channel is not None and isinstance(channel, discord.TextChannel):\n+        if (\n+            recipient is None\n+            and channel is not None\n+            and isinstance(channel, discord.TextChannel)\n+        ):\n             if channel.id in self.closing:\n                 return None\n             thread = await self._find_from_channel(channel)\n             if thread is None:\n                 user_id, thread = next(\n-                    ((k, v) for k, v in self.cache.items() if v.channel == channel), (-1, None)\n+                    ((k, v) for k, v in self.cache.items() if v.channel == channel),\n+                    (-1, None),\n                 )\n                 if thread is not None:\n                     logger.debug(\"Found thread with tempered ID.\")\n@@ -1610,7 +1792,9 @@ class ThreadManager:\n         if recipient is None:\n             thread = Thread(self, user_id, channel, other_recipients)\n         else:\n-            self.cache[user_id] = thread = Thread(self, recipient, channel, other_recipients)\n+            self.cache[user_id] = thread = Thread(\n+                self, recipient, channel, other_recipients\n+            )\n         thread.ready = True\n \n         return thread\n@@ -1638,9 +1822,9 @@ class ThreadManager:\n             if len(message.content.strip()) < min_chars:\n                 embed = discord.Embed(\n                     title=self.bot.config[\"thread_min_characters_title\"],\n-                    description=self.bot.config[\"thread_min_characters_response\"].replace(\n-                        \"{min_characters}\", str(min_chars)\n-                    ),\n+                    description=self.bot.config[\n+                        \"thread_min_characters_response\"\n+                    ].replace(\"{min_characters}\", str(min_chars)),\n                     color=self.bot.error_color,\n                 )\n                 embed.set_footer(\n@@ -1663,24 +1847,35 @@ class ThreadManager:\n                 return thread\n             else:\n                 if thread.channel and self.bot.get_channel(thread.channel.id):\n-                    logger.warning(\"Found an existing thread for %s, abort creating.\", recipient)\n+                    logger.warning(\n+                        \"Found an existing thread for %s, abort creating.\", recipient\n+                    )\n                     return thread\n-                logger.warning(\"Found an existing thread for %s, closing previous thread.\", recipient)\n+                logger.warning(\n+                    \"Found an existing thread for %s, closing previous thread.\",\n+                    recipient,\n+                )\n                 self.bot.loop.create_task(\n-                    thread.close(closer=self.bot.user, silent=True, delete_channel=False)\n+                    thread.close(\n+                        closer=self.bot.user, silent=True, delete_channel=False\n+                    )\n                 )\n \n         thread = Thread(self, recipient)\n \n         self.cache[recipient.id] = thread\n \n-        if (message or not manual_trigger) and self.bot.config[\"confirm_thread_creation\"]:\n+        if (message or not manual_trigger) and self.bot.config[\n+            \"confirm_thread_creation\"\n+        ]:\n             if not manual_trigger:\n                 destination = recipient\n             else:\n                 destination = message.channel\n             view = ConfirmThreadCreationView()\n-            view.add_item(AcceptButton(self.bot.config[\"confirm_thread_creation_accept\"]))\n+            view.add_item(\n+                AcceptButton(self.bot.config[\"confirm_thread_creation_accept\"])\n+            )\n             view.add_item(DenyButton(self.bot.config[\"confirm_thread_creation_deny\"]))\n             confirm = await destination.send(\n                 embed=discord.Embed(\n@@ -1708,7 +1903,8 @@ class ThreadManager:\n                 self.bot.loop.create_task(\n                     destination.send(\n                         embed=discord.Embed(\n-                            title=self.bot.config[\"thread_cancelled\"], color=self.bot.error_color\n+                            title=self.bot.config[\"thread_cancelled\"],\n+                            color=self.bot.error_color,\n                         )\n                     )\n                 )\n@@ -1716,7 +1912,9 @@ class ThreadManager:\n                 del self.cache[recipient.id]\n                 return thread\n \n-        self.bot.loop.create_task(thread.setup(creator=creator, category=category, initial_message=message))\n+        self.bot.loop.create_task(\n+            thread.setup(creator=creator, category=category, initial_message=message)\n+        )\n         return thread\n \n     async def find_or_create(self, recipient) -> Thread:\n"
    },
    {
        "id": 291,
        "sha_fail": "cc4d17e73e6e0f353b45fa931377e16adc881202",
        "diff": "diff --git a/bot.py b/bot.py\nindex 089a88d..d67a619 100644\n--- a/bot.py\n+++ b/bot.py\n@@ -4,7 +4,6 @@ __version__ = \"4.2.0\"\n import asyncio\n import copy\n import hashlib\n-import logging\n import os\n import re\n import string\n@@ -12,7 +11,7 @@ import struct\n import sys\n import platform\n import typing\n-from datetime import datetime, timezone, timedelta\n+from datetime import datetime, timezone\n from subprocess import PIPE\n from types import SimpleNamespace\n \n@@ -80,7 +79,9 @@ class ModmailBot(commands.Bot):\n         if not self.config[\"enable_presence_intent\"]:\n             intents.presences = False\n \n-        super().__init__(command_prefix=None, intents=intents)  # implemented in `get_prefix`\n+        super().__init__(\n+            command_prefix=None, intents=intents\n+        )  # implemented in `get_prefix`\n         self.session = None\n         self._api = None\n         self.formatter = SafeFormatter()\n@@ -101,7 +102,10 @@ class ModmailBot(commands.Bot):\n         self.startup()\n \n     def get_guild_icon(\n-        self, guild: typing.Optional[discord.Guild], *, size: typing.Optional[int] = None\n+        self,\n+        guild: typing.Optional[discord.Guild],\n+        *,\n+        size: typing.Optional[int] = None,\n     ) -> str:\n         if guild is None:\n             guild = self.guild\n@@ -245,7 +249,11 @@ class ModmailBot(commands.Bot):\n             async with self:\n                 task_retriever = asyncio.all_tasks\n                 loop = self.loop\n-                tasks = {t for t in task_retriever() if not t.done() and t.get_coro() != cancel_tasks_coro}\n+                tasks = {\n+                    t\n+                    for t in task_retriever()\n+                    if not t.done() and t.get_coro() != cancel_tasks_coro\n+                }\n \n                 if not tasks:\n                     return\n@@ -290,7 +298,9 @@ class ModmailBot(commands.Bot):\n             owner_ids = set(map(int, str(owner_ids).split(\",\")))\n         if self.owner_id is not None:\n             owner_ids.add(self.owner_id)\n-        permissions = self.config[\"level_permissions\"].get(PermissionLevel.OWNER.name, [])\n+        permissions = self.config[\"level_permissions\"].get(\n+            PermissionLevel.OWNER.name, []\n+        )\n         for perm in permissions:\n             owner_ids.add(int(perm))\n         return owner_ids\n@@ -316,7 +326,10 @@ class ModmailBot(commands.Bot):\n             try:\n                 channel = self.main_category.channels[0]\n                 self.config[\"log_channel_id\"] = channel.id\n-                logger.warning(\"No log channel set, setting #%s to be the log channel.\", channel.name)\n+                logger.warning(\n+                    \"No log channel set, setting #%s to be the log channel.\",\n+                    channel.name,\n+                )\n                 return channel\n             except IndexError:\n                 pass\n@@ -378,7 +391,9 @@ class ModmailBot(commands.Bot):\n     def token(self) -> str:\n         token = self.config[\"token\"]\n         if token is None:\n-            logger.critical(\"TOKEN must be set, set this as bot token found on the Discord Developer Portal.\")\n+            logger.critical(\n+                \"TOKEN must be set, set this as bot token found on the Discord Developer Portal.\"\n+            )\n             sys.exit(0)\n         return token\n \n@@ -432,7 +447,9 @@ class ModmailBot(commands.Bot):\n             category_id = self.config[\"main_category_id\"]\n             if category_id is not None:\n                 try:\n-                    cat = discord.utils.get(self.modmail_guild.categories, id=int(category_id))\n+                    cat = discord.utils.get(\n+                        self.modmail_guild.categories, id=int(category_id)\n+                    )\n                     if cat is not None:\n                         return cat\n                 except ValueError:\n@@ -486,7 +503,9 @@ class ModmailBot(commands.Bot):\n             try:\n                 return PermissionLevel[level.upper()]\n             except KeyError:\n-                logger.warning(\"Invalid override_command_level for command %s.\", command_name)\n+                logger.warning(\n+                    \"Invalid override_command_level for command %s.\", command_name\n+                )\n                 self.config[\"override_command_level\"].pop(command_name)\n \n         command = self.get_command(command_name)\n@@ -494,7 +513,11 @@ class ModmailBot(commands.Bot):\n             logger.debug(\"Command %s not found.\", command_name)\n             return PermissionLevel.INVALID\n         level = next(\n-            (check.permission_level for check in command.checks if hasattr(check, \"permission_level\")),\n+            (\n+                check.permission_level\n+                for check in command.checks\n+                if hasattr(check, \"permission_level\")\n+            ),\n             None,\n         )\n         if level is None:\n@@ -537,7 +560,8 @@ class ModmailBot(commands.Bot):\n         logger.info(\"Logged in as: %s\", self.user)\n         logger.info(\"Bot ID: %s\", self.user.id)\n         owners = \", \".join(\n-            getattr(self.get_user(owner_id), \"name\", str(owner_id)) for owner_id in self.bot_owner_ids\n+            getattr(self.get_user(owner_id), \"name\", str(owner_id))\n+            for owner_id in self.bot_owner_ids\n         )\n         logger.info(\"Owners: %s\", owners)\n         logger.info(\"Prefix: %s\", self.prefix)\n@@ -563,13 +587,18 @@ class ModmailBot(commands.Bot):\n \n         for recipient_id, items in tuple(closures.items()):\n             after = (\n-                datetime.fromisoformat(items[\"time\"]).astimezone(timezone.utc) - discord.utils.utcnow()\n+                datetime.fromisoformat(items[\"time\"]).astimezone(timezone.utc)\n+                - discord.utils.utcnow()\n             ).total_seconds()\n             if after <= 0:\n                 logger.debug(\"Closing thread for recipient %s.\", recipient_id)\n                 after = 0\n             else:\n-                logger.debug(\"Thread for recipient %s will be closed after %s seconds.\", recipient_id, after)\n+                logger.debug(\n+                    \"Thread for recipient %s will be closed after %s seconds.\",\n+                    recipient_id,\n+                    after,\n+                )\n \n             thread = await self.threads.find(recipient_id=int(recipient_id))\n \n@@ -591,7 +620,9 @@ class ModmailBot(commands.Bot):\n \n         for log in await self.api.get_open_logs():\n             if self.get_channel(int(log[\"channel_id\"])) is None:\n-                logger.debug(\"Unable to resolve thread with channel %s.\", log[\"channel_id\"])\n+                logger.debug(\n+                    \"Unable to resolve thread with channel %s.\", log[\"channel_id\"]\n+                )\n                 log_data = await self.api.post_log(\n                     log[\"channel_id\"],\n                     {\n@@ -609,18 +640,29 @@ class ModmailBot(commands.Bot):\n                     },\n                 )\n                 if log_data:\n-                    logger.debug(\"Successfully closed thread with channel %s.\", log[\"channel_id\"])\n+                    logger.debug(\n+                        \"Successfully closed thread with channel %s.\", log[\"channel_id\"]\n+                    )\n                 else:\n-                    logger.debug(\"Failed to close thread with channel %s, skipping.\", log[\"channel_id\"])\n+                    logger.debug(\n+                        \"Failed to close thread with channel %s, skipping.\",\n+                        log[\"channel_id\"],\n+                    )\n \n-        other_guilds = [guild for guild in self.guilds if guild not in {self.guild, self.modmail_guild}]\n+        other_guilds = [\n+            guild\n+            for guild in self.guilds\n+            if guild not in {self.guild, self.modmail_guild}\n+        ]\n         if any(other_guilds):\n             logger.warning(\n                 \"The bot is in more servers other than the main and staff server. \"\n                 \"This may cause data compromise (%s).\",\n                 \", \".join(str(guild.name) for guild in other_guilds),\n             )\n-            logger.warning(\"If the external servers are valid, you may ignore this message.\")\n+            logger.warning(\n+                \"If the external servers are valid, you may ignore this message.\"\n+            )\n \n         self.post_metadata.start()\n         self.autoupdate.start()\n@@ -649,7 +691,9 @@ class ModmailBot(commands.Bot):\n         return self.get_user(id) or await self.fetch_user(id)\n \n     @staticmethod\n-    async def get_or_fetch_member(guild: discord.Guild, member_id: int) -> typing.Optional[discord.Member]:\n+    async def get_or_fetch_member(\n+        guild: discord.Guild, member_id: int\n+    ) -> typing.Optional[discord.Member]:\n         \"\"\"\n         Attempt to get a member from cache; on failure fetch from the API.\n \n@@ -722,7 +766,9 @@ class ModmailBot(commands.Bot):\n             logger.debug(\"Blocked due to guild age, user %s.\", author.name)\n \n             if str(author.id) not in self.blocked_users:\n-                new_reason = f\"System Message: Recently Joined. User can try again {delta}.\"\n+                new_reason = (\n+                    f\"System Message: Recently Joined. User can try again {delta}.\"\n+                )\n                 self.blocked_users[str(author.id)] = new_reason\n \n             return False\n@@ -735,7 +781,9 @@ class ModmailBot(commands.Bot):\n                     blocked_reason = self.blocked_roles.get(str(r.id)) or \"\"\n \n                     try:\n-                        end_time, after = extract_block_timestamp(blocked_reason, author.id)\n+                        end_time, after = extract_block_timestamp(\n+                            blocked_reason, author.id\n+                        )\n                     except ValueError:\n                         return False\n \n@@ -778,7 +826,9 @@ class ModmailBot(commands.Bot):\n \n     async def _process_blocked(self, message):\n         _, blocked_emoji = await self.retrieve_emoji()\n-        if await self.is_blocked(message.author, channel=message.channel, send_message=True):\n+        if await self.is_blocked(\n+            message.author, channel=message.channel, send_message=True\n+        ):\n             await self.add_reaction(message, blocked_emoji)\n             return True\n         return False\n@@ -854,7 +904,10 @@ class ModmailBot(commands.Bot):\n             return\n \n         try:\n-            cooldown = datetime.fromisoformat(last_log_closed_at).astimezone(timezone.utc) + thread_cooldown\n+            cooldown = (\n+                datetime.fromisoformat(last_log_closed_at).astimezone(timezone.utc)\n+                + thread_cooldown\n+            )\n         except ValueError:\n             logger.warning(\"Error with 'thread_cooldown'.\", exc_info=True)\n             cooldown = datetime.fromisoformat(last_log_closed_at).astimezone(\n@@ -870,7 +923,10 @@ class ModmailBot(commands.Bot):\n \n     @staticmethod\n     async def add_reaction(\n-        msg, reaction: typing.Union[discord.Emoji, discord.Reaction, discord.PartialEmoji, str]\n+        msg,\n+        reaction: typing.Union[\n+            discord.Emoji, discord.Reaction, discord.PartialEmoji, str\n+        ],\n     ) -> bool:\n         if reaction != \"disable\":\n             try:\n@@ -899,12 +955,17 @@ class ModmailBot(commands.Bot):\n                         await message.channel.send(\n                             embed=discord.Embed(\n                                 title=self.config[\"cooldown_thread_title\"],\n-                                description=self.config[\"cooldown_thread_response\"].format(delta=delta),\n+                                description=self.config[\n+                                    \"cooldown_thread_response\"\n+                                ].format(delta=delta),\n                                 color=self.error_color,\n                             )\n                         )\n                         return\n-                    if self.config[\"dm_disabled\"] in (DMDisabled.NEW_THREADS, DMDisabled.ALL_THREADS):\n+                    if self.config[\"dm_disabled\"] in (\n+                        DMDisabled.NEW_THREADS,\n+                        DMDisabled.ALL_THREADS,\n+                    ):\n                         embed = discord.Embed(\n                             title=self.config[\"disabled_new_thread_title\"],\n                             color=self.error_color,\n@@ -915,7 +976,8 @@ class ModmailBot(commands.Bot):\n                             icon_url=self.get_guild_icon(guild=message.guild, size=128),\n                         )\n                         logger.info(\n-                            \"A new thread was blocked from %s due to disabled Modmail.\", message.author\n+                            \"A new thread was blocked from %s due to disabled Modmail.\",\n+                            message.author,\n                         )\n                         await self.add_reaction(message, blocked_emoji)\n                         return await message.channel.send(embed=embed)\n@@ -931,11 +993,17 @@ class ModmailBot(commands.Bot):\n                             text=self.config[\"disabled_current_thread_footer\"],\n                             icon_url=self.get_guild_icon(guild=message.guild, size=128),\n                         )\n-                        logger.info(\"A message was blocked from %s due to disabled Modmail.\", message.author)\n+                        logger.info(\n+                            \"A message was blocked from %s due to disabled Modmail.\",\n+                            message.author,\n+                        )\n                         await self.add_reaction(message, blocked_emoji)\n                         return await message.channel.send(embed=embed)\n                 # Extract forwarded content using utility function\n-                combined_content = extract_forwarded_content(message) or \"[Forwarded message with no content]\"\n+                combined_content = (\n+                    extract_forwarded_content(message)\n+                    or \"[Forwarded message with no content]\"\n+                )\n \n                 class ForwardedMessage:\n                     def __init__(self, original_message, forwarded_content):\n@@ -958,10 +1026,16 @@ class ModmailBot(commands.Bot):\n             else:\n                 message.content = \"[Forwarded message with no content]\"\n         # 2. Single-message forward (MessageType.forward)\n-        elif getattr(message, \"type\", None) == getattr(discord.MessageType, \"forward\", None):\n+        elif getattr(message, \"type\", None) == getattr(\n+            discord.MessageType, \"forward\", None\n+        ):\n             # Check for message.reference and its type\n             ref = getattr(message, \"reference\", None)\n-            if ref and getattr(ref, \"type\", None) == getattr(discord, \"MessageReferenceType\", None).forward:\n+            if (\n+                ref\n+                and getattr(ref, \"type\", None)\n+                == getattr(discord, \"MessageReferenceType\", None).forward\n+            ):\n                 # Try to fetch the referenced message\n                 ref_msg = None\n                 try:\n@@ -983,12 +1057,17 @@ class ModmailBot(commands.Bot):\n                             await message.channel.send(\n                                 embed=discord.Embed(\n                                     title=self.config[\"cooldown_thread_title\"],\n-                                    description=self.config[\"cooldown_thread_response\"].format(delta=delta),\n+                                    description=self.config[\n+                                        \"cooldown_thread_response\"\n+                                    ].format(delta=delta),\n                                     color=self.error_color,\n                                 )\n                             )\n                             return\n-                        if self.config[\"dm_disabled\"] in (DMDisabled.NEW_THREADS, DMDisabled.ALL_THREADS):\n+                        if self.config[\"dm_disabled\"] in (\n+                            DMDisabled.NEW_THREADS,\n+                            DMDisabled.ALL_THREADS,\n+                        ):\n                             embed = discord.Embed(\n                                 title=self.config[\"disabled_new_thread_title\"],\n                                 color=self.error_color,\n@@ -996,27 +1075,37 @@ class ModmailBot(commands.Bot):\n                             )\n                             embed.set_footer(\n                                 text=self.config[\"disabled_new_thread_footer\"],\n-                                icon_url=self.get_guild_icon(guild=message.guild, size=128),\n+                                icon_url=self.get_guild_icon(\n+                                    guild=message.guild, size=128\n+                                ),\n                             )\n                             logger.info(\n-                                \"A new thread was blocked from %s due to disabled Modmail.\", message.author\n+                                \"A new thread was blocked from %s due to disabled Modmail.\",\n+                                message.author,\n                             )\n                             await self.add_reaction(message, blocked_emoji)\n                             return await message.channel.send(embed=embed)\n-                        thread = await self.threads.create(message.author, message=message)\n+                        thread = await self.threads.create(\n+                            message.author, message=message\n+                        )\n                     else:\n                         if self.config[\"dm_disabled\"] == DMDisabled.ALL_THREADS:\n                             embed = discord.Embed(\n                                 title=self.config[\"disabled_current_thread_title\"],\n                                 color=self.error_color,\n-                                description=self.config[\"disabled_current_thread_response\"],\n+                                description=self.config[\n+                                    \"disabled_current_thread_response\"\n+                                ],\n                             )\n                             embed.set_footer(\n                                 text=self.config[\"disabled_current_thread_footer\"],\n-                                icon_url=self.get_guild_icon(guild=message.guild, size=128),\n+                                icon_url=self.get_guild_icon(\n+                                    guild=message.guild, size=128\n+                                ),\n                             )\n                             logger.info(\n-                                \"A message was blocked from %s due to disabled Modmail.\", message.author\n+                                \"A message was blocked from %s due to disabled Modmail.\",\n+                                message.author,\n                             )\n                             await self.add_reaction(message, blocked_emoji)\n                             return await message.channel.send(embed=embed)\n@@ -1026,7 +1115,9 @@ class ModmailBot(commands.Bot):\n                         def __init__(self, original_message, ref_message):\n                             self.author = original_message.author\n                             # Use the utility function to extract content or fallback to ref message content\n-                            extracted_content = extract_forwarded_content(original_message)\n+                            extracted_content = extract_forwarded_content(\n+                                original_message\n+                            )\n                             self.content = (\n                                 extracted_content\n                                 or ref_message.content\n@@ -1058,7 +1149,8 @@ class ModmailBot(commands.Bot):\n             # Update the DB with the new channel_id after restoration\n             if thread.channel:\n                 await self.api.logs.update_one(\n-                    {\"recipient.id\": str(thread.id)}, {\"$set\": {\"channel_id\": str(thread.channel.id)}}\n+                    {\"recipient.id\": str(thread.id)},\n+                    {\"$set\": {\"channel_id\": str(thread.channel.id)}},\n                 )\n         # Re-fetch the thread object to ensure channel is valid\n         thread = await self.threads.find(recipient=message.author)\n@@ -1069,13 +1161,18 @@ class ModmailBot(commands.Bot):\n                 await message.channel.send(\n                     embed=discord.Embed(\n                         title=self.config[\"cooldown_thread_title\"],\n-                        description=self.config[\"cooldown_thread_response\"].format(delta=delta),\n+                        description=self.config[\"cooldown_thread_response\"].format(\n+                            delta=delta\n+                        ),\n                         color=self.error_color,\n                     )\n                 )\n                 return\n \n-            if self.config[\"dm_disabled\"] in (DMDisabled.NEW_THREADS, DMDisabled.ALL_THREADS):\n+            if self.config[\"dm_disabled\"] in (\n+                DMDisabled.NEW_THREADS,\n+                DMDisabled.ALL_THREADS,\n+            ):\n                 embed = discord.Embed(\n                     title=self.config[\"disabled_new_thread_title\"],\n                     color=self.error_color,\n@@ -1085,7 +1182,10 @@ class ModmailBot(commands.Bot):\n                     text=self.config[\"disabled_new_thread_footer\"],\n                     icon_url=self.get_guild_icon(guild=message.guild, size=128),\n                 )\n-                logger.info(\"A new thread was blocked from %s due to disabled Modmail.\", message.author)\n+                logger.info(\n+                    \"A new thread was blocked from %s due to disabled Modmail.\",\n+                    message.author,\n+                )\n                 await self.add_reaction(message, blocked_emoji)\n                 return await message.channel.send(embed=embed)\n \n@@ -1101,7 +1201,10 @@ class ModmailBot(commands.Bot):\n                     text=self.config[\"disabled_current_thread_footer\"],\n                     icon_url=self.get_guild_icon(guild=message.guild, size=128),\n                 )\n-                logger.info(\"A message was blocked from %s due to disabled Modmail.\", message.author)\n+                logger.info(\n+                    \"A message was blocked from %s due to disabled Modmail.\",\n+                    message.author,\n+                )\n                 await self.add_reaction(message, blocked_emoji)\n                 return await message.channel.send(embed=embed)\n \n@@ -1168,7 +1271,9 @@ class ModmailBot(commands.Bot):\n         alias = self.aliases.get(invoker)\n         if alias is not None and snippet_text is None:\n             ctxs = []\n-            aliases = normalize_alias(alias, message.content[len(f\"{invoked_prefix}{invoker}\") :])\n+            aliases = normalize_alias(\n+                alias, message.content[len(f\"{invoked_prefix}{invoker}\") :]\n+            )\n             if not aliases:\n                 logger.warning(\"Alias %s is invalid, removing.\", invoker)\n                 self.aliases.pop(invoker)\n@@ -1181,7 +1286,9 @@ class ModmailBot(commands.Bot):\n                     command_invocation_text = alias\n                 else:\n                     command = self._get_snippet_command()\n-                    command_invocation_text = f\"{invoked_prefix}{command} {snippet_text}\"\n+                    command_invocation_text = (\n+                        f\"{invoked_prefix}{command} {snippet_text}\"\n+                    )\n                 view = StringView(invoked_prefix + command_invocation_text)\n                 ctx_ = cls(prefix=self.prefix, view=view, bot=self, message=message)\n                 ctx_.thread = thread\n@@ -1219,11 +1326,20 @@ class ModmailBot(commands.Bot):\n         invoker = None\n \n         if self.config.get(\"use_regex_autotrigger\"):\n-            trigger = next(filter(lambda x: re.search(x, message.content), self.auto_triggers.keys()))\n+            trigger = next(\n+                filter(\n+                    lambda x: re.search(x, message.content), self.auto_triggers.keys()\n+                )\n+            )\n             if trigger:\n                 invoker = re.search(trigger, message.content).group(0)\n         else:\n-            trigger = next(filter(lambda x: x.lower() in message.content.lower(), self.auto_triggers.keys()))\n+            trigger = next(\n+                filter(\n+                    lambda x: x.lower() in message.content.lower(),\n+                    self.auto_triggers.keys(),\n+                )\n+            )\n             if trigger:\n                 invoker = trigger.lower()\n \n@@ -1243,7 +1359,9 @@ class ModmailBot(commands.Bot):\n             message.content = invoked_prefix + alias\n             ctxs += await self.get_contexts(message)\n \n-        message.author = self.modmail_guild.me  # Fix message so commands execute properly\n+        message.author = (\n+            self.modmail_guild.me\n+        )  # Fix message so commands execute properly\n \n         for ctx in ctxs:\n             if ctx.command:\n@@ -1318,7 +1436,10 @@ class ModmailBot(commands.Bot):\n             await message.delete()\n \n         if (\n-            (f\"<@{self.user.id}\" in message.content or f\"<@!{self.user.id}\" in message.content)\n+            (\n+                f\"<@{self.user.id}\" in message.content\n+                or f\"<@!{self.user.id}\" in message.content\n+            )\n             and self.config[\"alert_on_mention\"]\n             and not message.author.bot\n         ):\n@@ -1338,7 +1459,9 @@ class ModmailBot(commands.Bot):\n \n         # --- MODERATOR-ONLY MESSAGE LOGGING ---\n         # If a moderator sends a message directly in a thread channel (not via modmail command), log it\n-        if not message.author.bot and not isinstance(message.channel, discord.DMChannel):\n+        if not message.author.bot and not isinstance(\n+            message.channel, discord.DMChannel\n+        ):\n             thread = await self.threads.find(channel=message.channel)\n             if thread is not None:\n                 ctxs = await self.get_contexts(message)\n@@ -1361,7 +1484,11 @@ class ModmailBot(commands.Bot):\n         ctxs = await self.get_contexts(message)\n         for ctx in ctxs:\n             if ctx.command:\n-                if not any(1 for check in ctx.command.checks if hasattr(check, \"permission_level\")):\n+                if not any(\n+                    1\n+                    for check in ctx.command.checks\n+                    if hasattr(check, \"permission_level\")\n+                ):\n                     logger.debug(\n                         \"Command %s has no permissions check, adding invalid level.\",\n                         ctx.command.qualified_name,\n@@ -1387,7 +1514,9 @@ class ModmailBot(commands.Bot):\n                 ):\n                     await thread.reply(message, anonymous=anonymous, plain=plain)\n             elif ctx.invoked_with:\n-                exc = commands.CommandNotFound('Command \"{}\" is not found'.format(ctx.invoked_with))\n+                exc = commands.CommandNotFound(\n+                    'Command \"{}\" is not found'.format(ctx.invoked_with)\n+                )\n                 self.dispatch(\"command_error\", ctx, exc)\n \n     async def on_typing(self, channel, user, _):\n@@ -1472,14 +1601,18 @@ class ModmailBot(commands.Bot):\n                 message.author == self.user\n                 and message.embeds\n                 and self.config.get(\"confirm_thread_creation\")\n-                and message.embeds[0].title == self.config[\"confirm_thread_creation_title\"]\n-                and message.embeds[0].description == self.config[\"confirm_thread_response\"]\n+                and message.embeds[0].title\n+                == self.config[\"confirm_thread_creation_title\"]\n+                and message.embeds[0].description\n+                == self.config[\"confirm_thread_response\"]\n             ):\n                 return\n             if not thread.recipient.dm_channel:\n                 await thread.recipient.create_dm()\n             try:\n-                linked_messages = await thread.find_linked_message_from_dm(message, either_direction=True)\n+                linked_messages = await thread.find_linked_message_from_dm(\n+                    message, either_direction=True\n+                )\n             except ValueError as e:\n                 logger.warning(\"Failed to find linked message for reactions: %s\", e)\n                 return\n@@ -1492,7 +1625,7 @@ class ModmailBot(commands.Bot):\n                 logger.warning(\"Failed to find linked message for reactions: %s\", e)\n                 return\n \n-        if self.config[\"transfer_reactions\"] and linked_messages is not [None]:\n+        if self.config[\"transfer_reactions\"] and linked_messages != [None]:\n             if payload.event_type == \"REACTION_ADD\":\n                 for msg in linked_messages:\n                     await self.add_reaction(msg, reaction)\n@@ -1508,7 +1641,10 @@ class ModmailBot(commands.Bot):\n     async def handle_react_to_contact(self, payload):\n         react_message_id = tryint(self.config.get(\"react_to_contact_message\"))\n         react_message_emoji = self.config.get(\"react_to_contact_emoji\")\n-        if not all((react_message_id, react_message_emoji)) or payload.message_id != react_message_id:\n+        if (\n+            not all((react_message_id, react_message_emoji))\n+            or payload.message_id != react_message_id\n+        ):\n             return\n         if payload.emoji.is_unicode_emoji():\n             emoji_fmt = payload.emoji.name\n@@ -1525,7 +1661,10 @@ class ModmailBot(commands.Bot):\n         await message.remove_reaction(payload.emoji, member)\n         await message.add_reaction(emoji_fmt)  # bot adds as well\n \n-        if self.config[\"dm_disabled\"] in (DMDisabled.NEW_THREADS, DMDisabled.ALL_THREADS):\n+        if self.config[\"dm_disabled\"] in (\n+            DMDisabled.NEW_THREADS,\n+            DMDisabled.ALL_THREADS,\n+        ):\n             embed = discord.Embed(\n                 title=self.config[\"disabled_new_thread_title\"],\n                 color=self.error_color,\n@@ -1542,7 +1681,9 @@ class ModmailBot(commands.Bot):\n             return await member.send(embed=embed)\n \n         ctx = await self.get_context(message)\n-        await ctx.invoke(self.get_command(\"contact\"), users=[member], manual_trigger=False)\n+        await ctx.invoke(\n+            self.get_command(\"contact\"), users=[member], manual_trigger=False\n+        )\n \n     async def on_raw_reaction_add(self, payload):\n         await asyncio.gather(\n@@ -1574,7 +1715,9 @@ class ModmailBot(commands.Bot):\n             await self.config.update()\n             return\n \n-        audit_logs = self.modmail_guild.audit_logs(limit=10, action=discord.AuditLogAction.channel_delete)\n+        audit_logs = self.modmail_guild.audit_logs(\n+            limit=10, action=discord.AuditLogAction.channel_delete\n+        )\n         found_entry = False\n         async for entry in audit_logs:\n             if int(entry.target.id) == channel.id:\n@@ -1582,7 +1725,9 @@ class ModmailBot(commands.Bot):\n                 break\n \n         if not found_entry:\n-            logger.debug(\"Cannot find the audit log entry for channel delete of %d.\", channel.id)\n+            logger.debug(\n+                \"Cannot find the audit log entry for channel delete of %d.\", channel.id\n+            )\n             return\n \n         mod = entry.user\n@@ -1609,15 +1754,15 @@ class ModmailBot(commands.Bot):\n                     remaining_guilds = member.mutual_guilds\n \n                     if remaining_guilds:\n-                        remaining_guild_names = [guild.name for guild in remaining_guilds]\n+                        remaining_guild_names = [\n+                            guild.name for guild in remaining_guilds\n+                        ]\n                         leave_message = (\n                             f\"The recipient has left {guild_left}. \"\n                             f\"They are still in {human_join(remaining_guild_names, final='and')}.\"\n                         )\n                     else:\n-                        leave_message = (\n-                            f\"The recipient has left {guild_left}. We no longer share any mutual servers.\"\n-                        )\n+                        leave_message = f\"The recipient has left {guild_left}. We no longer share any mutual servers.\"\n                 else:\n                     leave_message = \"The recipient has left the server.\"\n \n@@ -1648,7 +1793,9 @@ class ModmailBot(commands.Bot):\n             if not thread:\n                 return\n             try:\n-                message = await thread.find_linked_message_from_dm(message, get_thread_channel=True)\n+                message = await thread.find_linked_message_from_dm(\n+                    message, get_thread_channel=True\n+                )\n             except ValueError as e:\n                 if str(e) != \"Thread channel message not found.\":\n                     logger.debug(\"Failed to find linked message to delete: %s\", e)\n@@ -1674,11 +1821,15 @@ class ModmailBot(commands.Bot):\n \n         try:\n             await thread.delete_message(message, note=False)\n-            embed = discord.Embed(description=\"Successfully deleted message.\", color=self.main_color)\n+            embed = discord.Embed(\n+                description=\"Successfully deleted message.\", color=self.main_color\n+            )\n         except ValueError as e:\n             if str(e) not in {\"DM message not found.\", \"Malformed thread message.\"}:\n                 logger.debug(\"Failed to find linked message to delete: %s\", e)\n-                embed = discord.Embed(description=\"Failed to delete message.\", color=self.error_color)\n+                embed = discord.Embed(\n+                    description=\"Failed to delete message.\", color=self.error_color\n+                )\n             else:\n                 return\n         except discord.NotFound:\n@@ -1706,7 +1857,9 @@ class ModmailBot(commands.Bot):\n                 _, blocked_emoji = await self.retrieve_emoji()\n                 await self.add_reaction(after, blocked_emoji)\n             else:\n-                embed = discord.Embed(description=\"Successfully Edited Message\", color=self.main_color)\n+                embed = discord.Embed(\n+                    description=\"Successfully Edited Message\", color=self.main_color\n+                )\n                 embed.set_footer(text=f\"Message ID: {after.id}\")\n                 await after.channel.send(embed=embed)\n \n@@ -1715,7 +1868,11 @@ class ModmailBot(commands.Bot):\n         logger.error(\"Unexpected exception:\", exc_info=sys.exc_info())\n \n     async def on_command_error(\n-        self, context: commands.Context, exception: Exception, *, unhandled_by_cog: bool = False\n+        self,\n+        context: commands.Context,\n+        exception: Exception,\n+        *,\n+        unhandled_by_cog: bool = False,\n     ) -> None:\n         if not unhandled_by_cog:\n             command = context.command\n@@ -1730,7 +1887,9 @@ class ModmailBot(commands.Bot):\n                 await context.typing()\n             except Exception:\n                 pass\n-            await context.send(embed=discord.Embed(color=self.error_color, description=str(exception)))\n+            await context.send(\n+                embed=discord.Embed(color=self.error_color, description=str(exception))\n+            )\n         elif isinstance(exception, commands.CommandNotFound):\n             logger.warning(\"CommandNotFound: %s\", exception)\n         elif isinstance(exception, commands.MissingRequiredArgument):\n@@ -1748,10 +1907,14 @@ class ModmailBot(commands.Bot):\n                 if not await check(context):\n                     if hasattr(check, \"fail_msg\"):\n                         await context.send(\n-                            embed=discord.Embed(color=self.error_color, description=check.fail_msg)\n+                            embed=discord.Embed(\n+                                color=self.error_color, description=check.fail_msg\n+                            )\n                         )\n                     if hasattr(check, \"permission_level\"):\n-                        corrected_permission_level = self.command_perm(context.command.qualified_name)\n+                        corrected_permission_level = self.command_perm(\n+                            context.command.qualified_name\n+                        )\n                         logger.warning(\n                             \"User %s does not have permission to use this command: `%s` (%s).\",\n                             context.author.name,\n@@ -1760,7 +1923,10 @@ class ModmailBot(commands.Bot):\n                         )\n             logger.warning(\"CheckFailure: %s\", exception)\n         elif isinstance(exception, commands.DisabledCommand):\n-            logger.info(\"DisabledCommand: %s is trying to run eval but it's disabled\", context.author.name)\n+            logger.info(\n+                \"DisabledCommand: %s is trying to run eval but it's disabled\",\n+                context.author.name,\n+            )\n         else:\n             logger.error(\"Unexpected exception:\", exc_info=exception)\n \n@@ -1786,13 +1952,21 @@ class ModmailBot(commands.Bot):\n         if info.team is not None:\n             data.update(\n                 {\n-                    \"owner_name\": info.team.owner.name if info.team.owner is not None else \"No Owner\",\n+                    \"owner_name\": info.team.owner.name\n+                    if info.team.owner is not None\n+                    else \"No Owner\",\n                     \"owner_id\": info.team.owner_id,\n                     \"team\": True,\n                 }\n             )\n         else:\n-            data.update({\"owner_name\": info.owner.name, \"owner_id\": info.owner.id, \"team\": False})\n+            data.update(\n+                {\n+                    \"owner_name\": info.owner.name,\n+                    \"owner_id\": info.owner.id,\n+                    \"team\": False,\n+                }\n+            )\n \n         async with self.session.post(\"https://api.modmail.dev/metadata\", json=data):\n             logger.debug(\"Uploading metadata to Modmail server.\")\n@@ -1845,7 +2019,7 @@ class ModmailBot(commands.Bot):\n                 user = data[\"user\"]\n                 embed.add_field(\n                     name=\"Merge Commit\",\n-                    value=f\"[`{short_sha}`]({html_url}) \" f\"{message} - {user['username']}\",\n+                    value=f\"[`{short_sha}`]({html_url}) {message} - {user['username']}\",\n                 )\n                 embed.set_author(\n                     name=user[\"username\"] + \" - Updating Bot\",\n@@ -1853,7 +2027,9 @@ class ModmailBot(commands.Bot):\n                     url=user[\"url\"],\n                 )\n \n-                embed.set_footer(text=f\"Updating Modmail v{self.version} -> v{latest.version}\")\n+                embed.set_footer(\n+                    text=f\"Updating Modmail v{self.version} -> v{latest.version}\"\n+                )\n \n                 embed.description = latest.description\n                 for name, value in latest.fields.items():\n@@ -1892,10 +2068,16 @@ class ModmailBot(commands.Bot):\n \n                     logger.info(\"Bot has been updated.\")\n                     channel = self.update_channel\n-                    if self.hosting_method in (HostingMethod.PM2, HostingMethod.SYSTEMD):\n-                        embed = discord.Embed(title=\"Bot has been updated\", color=self.main_color)\n+                    if self.hosting_method in (\n+                        HostingMethod.PM2,\n+                        HostingMethod.SYSTEMD,\n+                    ):\n+                        embed = discord.Embed(\n+                            title=\"Bot has been updated\", color=self.main_color\n+                        )\n                         embed.set_footer(\n-                            text=f\"Updating Modmail v{self.version} \" f\"-> v{latest.version} {message}\"\n+                            text=f\"Updating Modmail v{self.version} \"\n+                            f\"-> v{latest.version} {message}\"\n                         )\n                         if self.config[\"update_notifications\"]:\n                             await channel.send(embed=embed)\n@@ -1905,7 +2087,9 @@ class ModmailBot(commands.Bot):\n                             description=f\"If you do not have an auto-restart setup, please manually start the bot. {message}\",\n                             color=self.main_color,\n                         )\n-                        embed.set_footer(text=f\"Updating Modmail v{self.version} -> v{latest.version}\")\n+                        embed.set_footer(\n+                            text=f\"Updating Modmail v{self.version} -> v{latest.version}\"\n+                        )\n                         if self.config[\"update_notifications\"]:\n                             await channel.send(embed=embed)\n                     return await self.close()\n@@ -1925,7 +2109,10 @@ class ModmailBot(commands.Bot):\n             self.autoupdate.cancel()\n             return\n \n-        if not self.config.get(\"github_token\") and self.hosting_method == HostingMethod.HEROKU:\n+        if (\n+            not self.config.get(\"github_token\")\n+            and self.hosting_method == HostingMethod.HEROKU\n+        ):\n             logger.warning(\"GitHub access token not found.\")\n             logger.warning(\"Autoupdates disabled.\")\n             self.autoupdate.cancel()\n@@ -1941,7 +2128,9 @@ class ModmailBot(commands.Bot):\n         expiration_datetime = now - log_expire_after\n         # WARNING: comparison is done lexicographically, not by date.\n         # This is fine as long as the date is in zero-padded ISO format, which it should be.\n-        expired_logs = await self.db.logs.delete_many({\"closed_at\": {\"$lte\": str(expiration_datetime)}})\n+        expired_logs = await self.db.logs.delete_many(\n+            {\"closed_at\": {\"$lte\": str(expiration_datetime)}}\n+        )\n \n         logger.info(f\"Deleted {expired_logs.deleted_count} expired logs.\")\n \n@@ -1961,7 +2150,9 @@ class ModmailBot(commands.Bot):\n             elif self.config[\"use_user_id_channel_name\"]:\n                 name = new_name = str(author.id)\n             elif self.config[\"use_timestamp_channel_name\"]:\n-                name = new_name = author.created_at.isoformat(sep=\"-\", timespec=\"minutes\")\n+                name = new_name = author.created_at.isoformat(\n+                    sep=\"-\", timespec=\"minutes\"\n+                )\n             else:\n                 if self.config[\"use_nickname_channel_name\"]:\n                     author_member = self.guild.get_member(author.id)\n@@ -1972,7 +2163,14 @@ class ModmailBot(commands.Bot):\n                 if force_null:\n                     name = \"null\"\n \n-                name = \"\".join(l for l in name if l not in string.punctuation and l.isprintable()) or \"null\"\n+                name = (\n+                    \"\".join(\n+                        l\n+                        for l in name\n+                        if l not in string.punctuation and l.isprintable()\n+                    )\n+                    or \"null\"\n+                )\n                 if author.discriminator != \"0\":\n                     name += f\"-{author.discriminator}\"\n                 new_name = name\n@@ -2009,7 +2207,10 @@ def main():\n                     \"Unable to import cairosvg, install GTK Installer for Windows and restart your system (https://github.com/tschoonj/GTK-for-Windows-Runtime-Environment-Installer/releases/latest)\"\n                 )\n         else:\n-            if \"ubuntu\" in platform.version().lower() or \"debian\" in platform.version().lower():\n+            if (\n+                \"ubuntu\" in platform.version().lower()\n+                or \"debian\" in platform.version().lower()\n+            ):\n                 logger.error(\n                     \"Unable to import cairosvg, try running `sudo apt-get install libpangocairo-1.0-0` or report on our support server with your OS details: https://discord.gg/etJNHCQ\"\n                 )\n@@ -2035,4 +2236,3 @@ def main():\n \n if __name__ == \"__main__\":\n     main()\n-\ndiff --git a/cogs/modmail.py b/cogs/modmail.py\nindex a63eea9..ff84456 100644\n--- a/cogs/modmail.py\n+++ b/cogs/modmail.py\n@@ -55,7 +55,9 @@ class Modmail(commands.Cog):\n         \"\"\"\n \n         if ctx.guild != self.bot.modmail_guild:\n-            return await ctx.send(f\"You can only setup in the Modmail guild: {self.bot.modmail_guild}.\")\n+            return await ctx.send(\n+                f\"You can only setup in the Modmail guild: {self.bot.modmail_guild}.\"\n+            )\n \n         if self.bot.main_category is not None:\n             logger.debug(\"Can't re-setup server, main_category is found.\")\n@@ -70,7 +72,9 @@ class Modmail(commands.Cog):\n             return await ctx.send(embed=embed)\n \n         overwrites = {\n-            self.bot.modmail_guild.default_role: discord.PermissionOverwrite(read_messages=False),\n+            self.bot.modmail_guild.default_role: discord.PermissionOverwrite(\n+                read_messages=False\n+            ),\n             self.bot.modmail_guild.me: discord.PermissionOverwrite(read_messages=True),\n         }\n \n@@ -90,11 +94,15 @@ class Modmail(commands.Cog):\n                     logger.info(\"Granting %s access to Modmail category.\", key.name)\n                     overwrites[key] = discord.PermissionOverwrite(read_messages=True)\n \n-        category = await self.bot.modmail_guild.create_category(name=\"Modmail\", overwrites=overwrites)\n+        category = await self.bot.modmail_guild.create_category(\n+            name=\"Modmail\", overwrites=overwrites\n+        )\n \n         await category.edit(position=0)\n \n-        log_channel = await self.bot.modmail_guild.create_text_channel(name=\"bot-logs\", category=category)\n+        log_channel = await self.bot.modmail_guild.create_text_channel(\n+            name=\"bot-logs\", category=category\n+        )\n \n         embed = discord.Embed(\n             title=\"Friendly Reminder\",\n@@ -111,7 +119,9 @@ class Modmail(commands.Cog):\n             \"feeling extra generous, buy us coffee on [Patreon](https://patreon.com/kyber) :heart:!\",\n         )\n \n-        embed.set_footer(text=f'Type \"{self.bot.prefix}help\" for a complete list of commands.')\n+        embed.set_footer(\n+            text=f'Type \"{self.bot.prefix}help\" for a complete list of commands.'\n+        )\n         await log_channel.send(embed=embed)\n \n         self.bot.config[\"main_category_id\"] = category.id\n@@ -127,7 +137,10 @@ class Modmail(commands.Cog):\n             f\"- `{self.bot.prefix}config help` for a list of available customizations.\"\n         )\n \n-        if not self.bot.config[\"command_permissions\"] and not self.bot.config[\"level_permissions\"]:\n+        if (\n+            not self.bot.config[\"command_permissions\"]\n+            and not self.bot.config[\"level_permissions\"]\n+        ):\n             await self.bot.update_perms(PermissionLevel.REGULAR, -1)\n             for owner_id in self.bot.bot_owner_ids:\n                 await self.bot.update_perms(PermissionLevel.OWNER, owner_id)\n@@ -160,11 +173,16 @@ class Modmail(commands.Cog):\n             if name == \"compact\":\n                 embeds = []\n \n-                for i, names in enumerate(zip_longest(*(iter(sorted(self.bot.snippets)),) * 15)):\n+                for i, names in enumerate(\n+                    zip_longest(*(iter(sorted(self.bot.snippets)),) * 15)\n+                ):\n                     description = format_description(i, names)\n-                    embed = discord.Embed(color=self.bot.main_color, description=description)\n+                    embed = discord.Embed(\n+                        color=self.bot.main_color, description=description\n+                    )\n                     embed.set_author(\n-                        name=\"Snippets\", icon_url=self.bot.get_guild_icon(guild=ctx.guild, size=128)\n+                        name=\"Snippets\",\n+                        icon_url=self.bot.get_guild_icon(guild=ctx.guild, size=128),\n                     )\n                     embeds.append(embed)\n \n@@ -175,25 +193,41 @@ class Modmail(commands.Cog):\n             snippet_name = self.bot._resolve_snippet(name)\n \n             if snippet_name is None:\n-                embed = create_not_found_embed(name, self.bot.snippets.keys(), \"Snippet\")\n+                embed = create_not_found_embed(\n+                    name, self.bot.snippets.keys(), \"Snippet\"\n+                )\n             else:\n                 val = self.bot.snippets[snippet_name]\n                 embed = discord.Embed(\n-                    title=f'Snippet - \"{snippet_name}\":', description=val, color=self.bot.main_color\n+                    title=f'Snippet - \"{snippet_name}\":',\n+                    description=val,\n+                    color=self.bot.main_color,\n                 )\n             return await ctx.send(embed=embed)\n \n         if not self.bot.snippets:\n             embed = discord.Embed(\n-                color=self.bot.error_color, description=\"You dont have any snippets at the moment.\"\n+                color=self.bot.error_color,\n+                description=\"You dont have any snippets at the moment.\",\n+            )\n+            embed.set_footer(\n+                text=f'Check \"{self.bot.prefix}help snippet add\" to add a snippet.'\n+            )\n+            embed.set_author(\n+                name=\"Snippets\",\n+                icon_url=self.bot.get_guild_icon(guild=ctx.guild, size=128),\n             )\n-            embed.set_footer(text=f'Check \"{self.bot.prefix}help snippet add\" to add a snippet.')\n-            embed.set_author(name=\"Snippets\", icon_url=self.bot.get_guild_icon(guild=ctx.guild, size=128))\n             return await ctx.send(embed=embed)\n \n-        embeds = [discord.Embed(color=self.bot.main_color) for _ in range((len(self.bot.snippets) // 10) + 1)]\n+        embeds = [\n+            discord.Embed(color=self.bot.main_color)\n+            for _ in range((len(self.bot.snippets) // 10) + 1)\n+        ]\n         for embed in embeds:\n-            embed.set_author(name=\"Snippets\", icon_url=self.bot.get_guild_icon(guild=ctx.guild, size=128))\n+            embed.set_author(\n+                name=\"Snippets\",\n+                icon_url=self.bot.get_guild_icon(guild=ctx.guild, size=128),\n+            )\n \n         for i, snippet in enumerate(sorted(self.bot.snippets.items())):\n             embeds[i // 10].add_field(\n@@ -337,11 +371,11 @@ class Modmail(commands.Cog):\n \n             deleted_aliases_string = \",\".join(f\"`{alias}`\" for alias in deleted_aliases)\n             if len(deleted_aliases) == 1:\n-                deleted_aliases_output = f\"The `{deleted_aliases_string}` direct alias has been removed.\"\n-            elif deleted_aliases:\n                 deleted_aliases_output = (\n-                    f\"The following direct aliases have been removed: {deleted_aliases_string}.\"\n+                    f\"The `{deleted_aliases_string}` direct alias has been removed.\"\n                 )\n+            elif deleted_aliases:\n+                deleted_aliases_output = f\"The following direct aliases have been removed: {deleted_aliases_string}.\"\n             else:\n                 deleted_aliases_output = None\n \n@@ -448,7 +482,10 @@ class Modmail(commands.Cog):\n             silent = any(word in silent_words for word in options.split())\n \n         await thread.channel.move(\n-            category=category, end=True, sync_permissions=True, reason=f\"{ctx.author} moved this thread.\"\n+            category=category,\n+            end=True,\n+            sync_permissions=True,\n+            reason=f\"{ctx.author} moved this thread.\",\n         )\n \n         if self.bot.config[\"thread_move_notify\"] and not silent:\n@@ -526,7 +563,8 @@ class Modmail(commands.Cog):\n             if thread.close_task is not None or thread.auto_close_task is not None:\n                 await thread.cancel_closure(all=True)\n                 embed = discord.Embed(\n-                    color=self.bot.error_color, description=\"Scheduled close has been cancelled.\"\n+                    color=self.bot.error_color,\n+                    description=\"Scheduled close has been cancelled.\",\n                 )\n             else:\n                 embed = discord.Embed(\n@@ -543,7 +581,9 @@ class Modmail(commands.Cog):\n         if after and after.dt > after.now:\n             await self.send_scheduled_close_message(ctx, after, silent)\n \n-        await thread.close(closer=ctx.author, after=close_after, message=message, silent=silent)\n+        await thread.close(\n+            closer=ctx.author, after=close_after, message=message, silent=silent\n+        )\n \n     @staticmethod\n     def parse_user_or_role(ctx, user_or_role):\n@@ -559,7 +599,9 @@ class Modmail(commands.Cog):\n     @commands.command(aliases=[\"alert\"])\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n-    async def notify(self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None):\n+    async def notify(\n+        self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None\n+    ):\n         \"\"\"\n         Notify a user or role when the next thread message received.\n \n@@ -597,7 +639,9 @@ class Modmail(commands.Cog):\n     @commands.command(aliases=[\"unalert\"])\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n-    async def unnotify(self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None):\n+    async def unnotify(\n+        self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None\n+    ):\n         \"\"\"\n         Un-notify a user, role, or yourself from a thread.\n \n@@ -625,14 +669,17 @@ class Modmail(commands.Cog):\n             mentions.remove(mention)\n             await self.bot.config.update()\n             embed = discord.Embed(\n-                color=self.bot.main_color, description=f\"{mention} will no longer be notified.\"\n+                color=self.bot.main_color,\n+                description=f\"{mention} will no longer be notified.\",\n             )\n         return await ctx.send(embed=embed)\n \n     @commands.command(aliases=[\"sub\"])\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n-    async def subscribe(self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None):\n+    async def subscribe(\n+        self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None\n+    ):\n         \"\"\"\n         Notify a user, role, or yourself for every thread message received.\n \n@@ -670,7 +717,9 @@ class Modmail(commands.Cog):\n     @commands.command(aliases=[\"unsub\"])\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n-    async def unsubscribe(self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None):\n+    async def unsubscribe(\n+        self, ctx, *, user_or_role: Union[discord.Role, User, str.lower, None] = None\n+    ):\n         \"\"\"\n         Unsubscribe a user, role, or yourself from a thread.\n \n@@ -736,10 +785,13 @@ class Modmail(commands.Cog):\n                 continue\n         if not found:\n             embed = discord.Embed(\n-                color=self.bot.error_color, description=\"Message not found or no longer exists.\"\n+                color=self.bot.error_color,\n+                description=\"Message not found or no longer exists.\",\n             )\n         else:\n-            embed = discord.Embed(color=self.bot.main_color, description=message.jump_url)\n+            embed = discord.Embed(\n+                color=self.bot.main_color, description=message.jump_url\n+            )\n         await ctx.send(embed=embed)\n \n     @commands.command()\n@@ -748,7 +800,9 @@ class Modmail(commands.Cog):\n     async def loglink(self, ctx):\n         \"\"\"Retrieves the link to the current thread's logs.\"\"\"\n         log_link = await self.bot.api.get_log_link(ctx.channel.id)\n-        await ctx.send(embed=discord.Embed(color=self.bot.main_color, description=log_link))\n+        await ctx.send(\n+            embed=discord.Embed(color=self.bot.main_color, description=log_link)\n+        )\n \n     def format_log_embeds(self, logs, avatar_url):\n         embeds = []\n@@ -761,16 +815,16 @@ class Modmail(commands.Cog):\n             prefix = self.bot.config[\"log_url_prefix\"].strip(\"/\")\n             if prefix == \"NONE\":\n                 prefix = \"\"\n-            log_url = (\n-                f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{entry['key']}\"\n-            )\n+            log_url = f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{entry['key']}\"\n \n             username = entry[\"recipient\"][\"name\"]\n             if entry[\"recipient\"][\"discriminator\"] != \"0\":\n                 username += \"#\" + entry[\"recipient\"][\"discriminator\"]\n \n             embed = discord.Embed(color=self.bot.main_color, timestamp=created_at)\n-            embed.set_author(name=f\"{title} - {username}\", icon_url=avatar_url, url=log_url)\n+            embed.set_author(\n+                name=f\"{title} - {username}\", icon_url=avatar_url, url=log_url\n+            )\n             embed.url = log_url\n             embed.add_field(name=\"Created\", value=human_timedelta(created_at))\n             closer = entry.get(\"closer\")\n@@ -786,7 +840,9 @@ class Modmail(commands.Cog):\n             if entry.get(\"title\"):\n                 embed.add_field(name=\"Title\", value=entry[\"title\"], inline=False)\n \n-            embed.add_field(name=\"Preview\", value=format_preview(entry[\"messages\"]), inline=False)\n+            embed.add_field(\n+                name=\"Preview\", value=format_preview(entry[\"messages\"]), inline=False\n+            )\n \n             if closer is not None:\n                 # BUG: Currently, logviewer can't display logs without a closer.\n@@ -810,7 +866,9 @@ class Modmail(commands.Cog):\n         await ctx.message.pin()\n         await self.bot.add_reaction(ctx.message, sent_emoji)\n \n-    @commands.command(usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True)\n+    @commands.command(\n+        usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True\n+    )\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n     @commands.cooldown(1, 600, BucketType.channel)\n@@ -881,7 +939,9 @@ class Modmail(commands.Cog):\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n                 text=str(ctx.author),\n-                icon_url=ctx.author.display_avatar.url if ctx.author.display_avatar else None,\n+                icon_url=ctx.author.display_avatar.url\n+                if ctx.author.display_avatar\n+                else None,\n             )\n             for u in users:\n                 to_exec.append(u.send(embed=em))\n@@ -899,7 +959,10 @@ class Modmail(commands.Cog):\n             if self.bot.config[\"show_timestamp\"]:\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n-                text=f\"{users[0]}\", icon_url=users[0].display_avatar.url if users[0].display_avatar else None\n+                text=f\"{users[0]}\",\n+                icon_url=users[0].display_avatar.url\n+                if users[0].display_avatar\n+                else None,\n             )\n \n             for i in ctx.thread.recipients:\n@@ -913,11 +976,15 @@ class Modmail(commands.Cog):\n         sent_emoji, _ = await self.bot.retrieve_emoji()\n         await self.bot.add_reaction(ctx.message, sent_emoji)\n \n-    @commands.command(usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True)\n+    @commands.command(\n+        usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True\n+    )\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n     @commands.cooldown(1, 600, BucketType.channel)\n-    async def removeuser(self, ctx, *users_arg: Union[discord.Member, discord.Role, str]):\n+    async def removeuser(\n+        self, ctx, *users_arg: Union[discord.Member, discord.Role, str]\n+    ):\n         \"\"\"Removes a user from a modmail thread\n \n         `options` can be `silent` or `silently`.\n@@ -968,7 +1035,8 @@ class Modmail(commands.Cog):\n         to_exec = []\n         if not silent:\n             description = self.bot.formatter.format(\n-                self.bot.config[\"private_removed_from_group_response\"], moderator=ctx.author\n+                self.bot.config[\"private_removed_from_group_response\"],\n+                moderator=ctx.author,\n             )\n             em = discord.Embed(\n                 title=self.bot.config[\"private_removed_from_group_title\"],\n@@ -979,7 +1047,9 @@ class Modmail(commands.Cog):\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n                 text=str(ctx.author),\n-                icon_url=ctx.author.display_avatar.url if ctx.author.display_avatar else None,\n+                icon_url=ctx.author.display_avatar.url\n+                if ctx.author.display_avatar\n+                else None,\n             )\n             for u in users:\n                 to_exec.append(u.send(embed=em))\n@@ -997,7 +1067,10 @@ class Modmail(commands.Cog):\n             if self.bot.config[\"show_timestamp\"]:\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n-                text=f\"{users[0]}\", icon_url=users[0].display_avatar.url if users[0].display_avatar else None\n+                text=f\"{users[0]}\",\n+                icon_url=users[0].display_avatar.url\n+                if users[0].display_avatar\n+                else None,\n             )\n \n             for i in ctx.thread.recipients:\n@@ -1011,11 +1084,15 @@ class Modmail(commands.Cog):\n         sent_emoji, _ = await self.bot.retrieve_emoji()\n         await self.bot.add_reaction(ctx.message, sent_emoji)\n \n-    @commands.command(usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True)\n+    @commands.command(\n+        usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True\n+    )\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n     @commands.cooldown(1, 600, BucketType.channel)\n-    async def anonadduser(self, ctx, *users_arg: Union[discord.Member, discord.Role, str]):\n+    async def anonadduser(\n+        self, ctx, *users_arg: Union[discord.Member, discord.Role, str]\n+    ):\n         \"\"\"Adds a user to a modmail thread anonymously\n \n         `options` can be `silent` or `silently`.\n@@ -1069,7 +1146,9 @@ class Modmail(commands.Cog):\n \n             tag = self.bot.config[\"mod_tag\"]\n             if tag is None:\n-                tag = str(get_top_role(ctx.author, self.bot.config[\"use_hoisted_top_role\"]))\n+                tag = str(\n+                    get_top_role(ctx.author, self.bot.config[\"use_hoisted_top_role\"])\n+                )\n             name = self.bot.config[\"anon_username\"]\n             if name is None:\n                 name = tag\n@@ -1093,7 +1172,10 @@ class Modmail(commands.Cog):\n             if self.bot.config[\"show_timestamp\"]:\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n-                text=f\"{users[0]}\", icon_url=users[0].display_avatar.url if users[0].display_avatar else None\n+                text=f\"{users[0]}\",\n+                icon_url=users[0].display_avatar.url\n+                if users[0].display_avatar\n+                else None,\n             )\n \n             for i in ctx.thread.recipients:\n@@ -1107,11 +1189,15 @@ class Modmail(commands.Cog):\n         sent_emoji, _ = await self.bot.retrieve_emoji()\n         await self.bot.add_reaction(ctx.message, sent_emoji)\n \n-    @commands.command(usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True)\n+    @commands.command(\n+        usage=\"<users_or_roles...> [options]\", cooldown_after_parsing=True\n+    )\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n     @checks.thread_only()\n     @commands.cooldown(1, 600, BucketType.channel)\n-    async def anonremoveuser(self, ctx, *users_arg: Union[discord.Member, discord.Role, str]):\n+    async def anonremoveuser(\n+        self, ctx, *users_arg: Union[discord.Member, discord.Role, str]\n+    ):\n         \"\"\"Removes a user from a modmail thread anonymously\n \n         `options` can be `silent` or `silently`.\n@@ -1152,7 +1238,9 @@ class Modmail(commands.Cog):\n         if not silent:\n             em = discord.Embed(\n                 title=self.bot.config[\"private_removed_from_group_title\"],\n-                description=self.bot.config[\"private_removed_from_group_description_anon\"],\n+                description=self.bot.config[\n+                    \"private_removed_from_group_description_anon\"\n+                ],\n                 color=self.bot.main_color,\n             )\n             if self.bot.config[\"show_timestamp\"]:\n@@ -1160,7 +1248,9 @@ class Modmail(commands.Cog):\n \n             tag = self.bot.config[\"mod_tag\"]\n             if tag is None:\n-                tag = str(get_top_role(ctx.author, self.bot.config[\"use_hoisted_top_role\"]))\n+                tag = str(\n+                    get_top_role(ctx.author, self.bot.config[\"use_hoisted_top_role\"])\n+                )\n             name = self.bot.config[\"anon_username\"]\n             if name is None:\n                 name = tag\n@@ -1184,7 +1274,10 @@ class Modmail(commands.Cog):\n             if self.bot.config[\"show_timestamp\"]:\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n-                text=f\"{users[0]}\", icon_url=users[0].display_avatar.url if users[0].display_avatar else None\n+                text=f\"{users[0]}\",\n+                icon_url=users[0].display_avatar.url\n+                if users[0].display_avatar\n+                else None,\n             )\n \n             for i in ctx.thread.recipients:\n@@ -1250,7 +1343,9 @@ class Modmail(commands.Cog):\n         user = user if user is not None else ctx.author\n \n         entries = await self.bot.api.search_closed_by(user.id)\n-        embeds = self.format_log_embeds(entries, avatar_url=self.bot.get_guild_icon(guild=ctx.guild))\n+        embeds = self.format_log_embeds(\n+            entries, avatar_url=self.bot.get_guild_icon(guild=ctx.guild)\n+        )\n \n         if not embeds:\n             embed = discord.Embed(\n@@ -1322,7 +1417,9 @@ class Modmail(commands.Cog):\n \n         entries = await self.bot.api.get_responded_logs(user.id)\n \n-        embeds = self.format_log_embeds(entries, avatar_url=self.bot.get_guild_icon(guild=ctx.guild))\n+        embeds = self.format_log_embeds(\n+            entries, avatar_url=self.bot.get_guild_icon(guild=ctx.guild)\n+        )\n \n         if not embeds:\n             embed = discord.Embed(\n@@ -1348,7 +1445,9 @@ class Modmail(commands.Cog):\n \n         entries = await self.bot.api.search_by_text(query, limit)\n \n-        embeds = self.format_log_embeds(entries, avatar_url=self.bot.get_guild_icon(guild=ctx.guild))\n+        embeds = self.format_log_embeds(\n+            entries, avatar_url=self.bot.get_guild_icon(guild=ctx.guild)\n+        )\n \n         if not embeds:\n             embed = discord.Embed(\n@@ -1392,7 +1491,10 @@ class Modmail(commands.Cog):\n         automatically embedding image URLs.\n         \"\"\"\n         msg = self.bot.formatter.format(\n-            msg, channel=ctx.channel, recipient=ctx.thread.recipient, author=ctx.message.author\n+            msg,\n+            channel=ctx.channel,\n+            recipient=ctx.thread.recipient,\n+            author=ctx.message.author,\n         )\n         ctx.message.content = msg\n         async with safe_typing(ctx):\n@@ -1414,7 +1516,10 @@ class Modmail(commands.Cog):\n         automatically embedding image URLs.\n         \"\"\"\n         msg = self.bot.formatter.format(\n-            msg, channel=ctx.channel, recipient=ctx.thread.recipient, author=ctx.message.author\n+            msg,\n+            channel=ctx.channel,\n+            recipient=ctx.thread.recipient,\n+            author=ctx.message.author,\n         )\n         ctx.message.content = msg\n         async with safe_typing(ctx):\n@@ -1436,7 +1541,10 @@ class Modmail(commands.Cog):\n         automatically embedding image URLs.\n         \"\"\"\n         msg = self.bot.formatter.format(\n-            msg, channel=ctx.channel, recipient=ctx.thread.recipient, author=ctx.message.author\n+            msg,\n+            channel=ctx.channel,\n+            recipient=ctx.thread.recipient,\n+            author=ctx.message.author,\n         )\n         ctx.message.content = msg\n         async with safe_typing(ctx):\n@@ -1458,7 +1566,10 @@ class Modmail(commands.Cog):\n         automatically embedding image URLs.\n         \"\"\"\n         msg = self.bot.formatter.format(\n-            msg, channel=ctx.channel, recipient=ctx.thread.recipient, author=ctx.message.author\n+            msg,\n+            channel=ctx.channel,\n+            recipient=ctx.thread.recipient,\n+            author=ctx.message.author,\n         )\n         ctx.message.content = msg\n         async with safe_typing(ctx):\n@@ -1534,7 +1645,9 @@ class Modmail(commands.Cog):\n         async with safe_typing(ctx):\n             msg = await ctx.thread.note(ctx.message, persistent=True)\n             await msg.pin()\n-        await self.bot.api.create_note(recipient=ctx.thread.recipient, message=ctx.message, message_id=msg.id)\n+        await self.bot.api.create_note(\n+            recipient=ctx.thread.recipient, message=ctx.message, message_id=msg.id\n+        )\n \n     @commands.command()\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n@@ -1576,7 +1689,12 @@ class Modmail(commands.Cog):\n         self,\n         ctx,\n         users: commands.Greedy[\n-            Union[Literal[\"silent\", \"silently\"], discord.Member, discord.User, discord.Role]\n+            Union[\n+                Literal[\"silent\", \"silently\"],\n+                discord.Member,\n+                discord.User,\n+                discord.Role,\n+            ]\n         ],\n         *,\n         category: SimilarCategoryConverter = None,\n@@ -1639,7 +1757,9 @@ class Modmail(commands.Cog):\n                 users.remove(u)\n             elif await self.bot.is_blocked(u):\n                 ref = f\"{u.mention} is\" if ctx.author != u else \"You are\"\n-                errors.append(f\"{ref} currently blocked from contacting {self.bot.user.name}.\")\n+                errors.append(\n+                    f\"{ref} currently blocked from contacting {self.bot.user.name}.\"\n+                )\n                 users.remove(u)\n \n         if len(users) > 5:\n@@ -1654,7 +1774,11 @@ class Modmail(commands.Cog):\n                 title = None\n \n             if manual_trigger:  # not react to contact\n-                embed = discord.Embed(title=title, color=self.bot.error_color, description=\"\\n\".join(errors))\n+                embed = discord.Embed(\n+                    title=title,\n+                    color=self.bot.error_color,\n+                    description=\"\\n\".join(errors),\n+                )\n                 await ctx.send(embed=embed, delete_after=10)\n \n             if not users:\n@@ -1674,7 +1798,10 @@ class Modmail(commands.Cog):\n         if thread.cancelled:\n             return\n \n-        if self.bot.config[\"dm_disabled\"] in (DMDisabled.NEW_THREADS, DMDisabled.ALL_THREADS):\n+        if self.bot.config[\"dm_disabled\"] in (\n+            DMDisabled.NEW_THREADS,\n+            DMDisabled.ALL_THREADS,\n+        ):\n             logger.info(\"Contacting user %s when Modmail DM is disabled.\", users[0])\n \n         if not silent and not self.bot.config.get(\"thread_contact_silently\"):\n@@ -1693,7 +1820,8 @@ class Modmail(commands.Cog):\n             if self.bot.config[\"show_timestamp\"]:\n                 em.timestamp = discord.utils.utcnow()\n             em.set_footer(\n-                text=f\"{creator}\", icon_url=creator.display_avatar.url if creator.display_avatar else None\n+                text=f\"{creator}\",\n+                icon_url=creator.display_avatar.url if creator.display_avatar else None,\n             )\n \n             for u in users:\n@@ -1763,7 +1891,11 @@ class Modmail(commands.Cog):\n             if role:\n                 roles.append((role.mention, reason))\n \n-        user_embeds = [discord.Embed(title=\"Blocked Users\", color=self.bot.main_color, description=\"\")]\n+        user_embeds = [\n+            discord.Embed(\n+                title=\"Blocked Users\", color=self.bot.main_color, description=\"\"\n+            )\n+        ]\n \n         if users:\n             embed = user_embeds[0]\n@@ -1786,7 +1918,11 @@ class Modmail(commands.Cog):\n             for n, em in enumerate(user_embeds):\n                 em.title = f\"{em.title} [{n + 1}]\"\n \n-        role_embeds = [discord.Embed(title=\"Blocked Roles\", color=self.bot.main_color, description=\"\")]\n+        role_embeds = [\n+            discord.Embed(\n+                title=\"Blocked Roles\", color=self.bot.main_color, description=\"\"\n+            )\n+        ]\n \n         if roles:\n             embed = role_embeds[-1]\n@@ -1980,7 +2116,10 @@ class Modmail(commands.Cog):\n         mention = getattr(user_or_role, \"mention\", f\"`{user_or_role.id}`\")\n         name = getattr(user_or_role, \"name\", f\"`{user_or_role.id}`\")\n \n-        if not isinstance(user_or_role, discord.Role) and str(user_or_role.id) in self.bot.blocked_users:\n+        if (\n+            not isinstance(user_or_role, discord.Role)\n+            and str(user_or_role.id) in self.bot.blocked_users\n+        ):\n             msg = self.bot.blocked_users.pop(str(user_or_role.id)) or \"\"\n             await self.bot.config.update()\n \n@@ -2005,7 +2144,10 @@ class Modmail(commands.Cog):\n                     color=self.bot.main_color,\n                     description=f\"{mention} is no longer blocked.\",\n                 )\n-        elif isinstance(user_or_role, discord.Role) and str(user_or_role.id) in self.bot.blocked_roles:\n+        elif (\n+            isinstance(user_or_role, discord.Role)\n+            and str(user_or_role.id) in self.bot.blocked_roles\n+        ):\n             msg = self.bot.blocked_roles.pop(str(user_or_role.id)) or \"\"\n             await self.bot.config.update()\n \n@@ -2016,7 +2158,9 @@ class Modmail(commands.Cog):\n             )\n         else:\n             embed = discord.Embed(\n-                title=\"Error\", description=f\"{mention} is not blocked.\", color=self.bot.error_color\n+                title=\"Error\",\n+                description=f\"{mention} is not blocked.\",\n+                color=self.bot.error_color,\n             )\n \n         return await ctx.send(embed=embed)\n@@ -2069,12 +2213,18 @@ class Modmail(commands.Cog):\n \n         # Search cache for channel\n         user_id, thread = next(\n-            ((k, v) for k, v in self.bot.threads.cache.items() if v.channel == ctx.channel),\n+            (\n+                (k, v)\n+                for k, v in self.bot.threads.cache.items()\n+                if v.channel == ctx.channel\n+            ),\n             (-1, None),\n         )\n         if thread is not None:\n             logger.debug(\"Found thread with tempered ID.\")\n-            await ctx.channel.edit(reason=\"Fix broken Modmail thread\", topic=f\"User ID: {user_id}\")\n+            await ctx.channel.edit(\n+                reason=\"Fix broken Modmail thread\", topic=f\"User ID: {user_id}\"\n+            )\n             return await self.bot.add_reaction(ctx.message, sent_emoji)\n \n         # find genesis message to retrieve User ID\n@@ -2102,8 +2252,12 @@ class Modmail(commands.Cog):\n                             self.bot.threads, recipient, ctx.channel, other_recipients\n                         )\n                     thread.ready = True\n-                    logger.info(\"Setting current channel's topic to User ID and created new thread.\")\n-                    await ctx.channel.edit(reason=\"Fix broken Modmail thread\", topic=f\"User ID: {user_id}\")\n+                    logger.info(\n+                        \"Setting current channel's topic to User ID and created new thread.\"\n+                    )\n+                    await ctx.channel.edit(\n+                        reason=\"Fix broken Modmail thread\", topic=f\"User ID: {user_id}\"\n+                    )\n                     return await self.bot.add_reaction(ctx.message, sent_emoji)\n \n         else:\n@@ -2116,7 +2270,10 @@ class Modmail(commands.Cog):\n             users = set(\n                 filter(\n                     lambda member: member.name == m.group(1)\n-                    and (member.discriminator == \"0\" or member.discriminator == m.group(2)),\n+                    and (\n+                        member.discriminator == \"0\"\n+                        or member.discriminator == m.group(2)\n+                    ),\n                     ctx.guild.members,\n                 )\n             )\n@@ -2154,9 +2311,13 @@ class Modmail(commands.Cog):\n                         self.bot.threads, recipient, ctx.channel, other_recipients\n                     )\n                 thread.ready = True\n-                logger.info(\"Setting current channel's topic to User ID and created new thread.\")\n+                logger.info(\n+                    \"Setting current channel's topic to User ID and created new thread.\"\n+                )\n                 await ctx.channel.edit(\n-                    reason=\"Fix broken Modmail thread\", name=name, topic=f\"User ID: {user.id}\"\n+                    reason=\"Fix broken Modmail thread\",\n+                    name=name,\n+                    topic=f\"User ID: {user.id}\",\n                 )\n                 return await self.bot.add_reaction(ctx.message, sent_emoji)\n \n@@ -2275,7 +2436,9 @@ class Modmail(commands.Cog):\n         thread = ctx.thread\n         if thread.snoozed:\n             await ctx.send(\"This thread is already snoozed.\")\n-            logging.info(f\"[SNOOZE] Thread for {getattr(thread.recipient, 'id', None)} already snoozed.\")\n+            logging.info(\n+                f\"[SNOOZE] Thread for {getattr(thread.recipient, 'id', None)} already snoozed.\"\n+            )\n             return\n         max_snooze = self.bot.config.get(\"max_snooze_time\")\n         if max_snooze is None:\n@@ -2296,7 +2459,8 @@ class Modmail(commands.Cog):\n         )\n         embed = discord.Embed(\n             title=self.bot.config.get(\"snooze_title\") or \"Thread Snoozed\",\n-            description=self.bot.config.get(\"snooze_text\") or \"This thread has been snoozed.\",\n+            description=self.bot.config.get(\"snooze_text\")\n+            or \"This thread has been snoozed.\",\n             color=self.bot.error_color,\n         )\n         await ctx.send(embed=embed)\n@@ -2308,7 +2472,9 @@ class Modmail(commands.Cog):\n             self.bot.threads.cache[thread.id] = thread\n         else:\n             await ctx.send(\"Failed to snooze this thread.\")\n-            logging.error(f\"[SNOOZE] Failed to snooze thread for {getattr(thread.recipient, 'id', None)}.\")\n+            logging.error(\n+                f\"[SNOOZE] Failed to snooze thread for {getattr(thread.recipient, 'id', None)}.\"\n+            )\n \n     @commands.command()\n     @checks.has_permissions(PermissionLevel.SUPPORTER)\n@@ -2332,8 +2498,12 @@ class Modmail(commands.Cog):\n             if user_obj:\n                 thread = await self.bot.threads.find(recipient=user_obj)\n             if not thread:\n-                await ctx.send(f\"[DEBUG] No thread found for user {user} (obj: {user_obj}).\")\n-                logging.warning(f\"[UNSNOOZE] No thread found for user {user} (obj: {user_obj})\")\n+                await ctx.send(\n+                    f\"[DEBUG] No thread found for user {user} (obj: {user_obj}).\"\n+                )\n+                logging.warning(\n+                    f\"[UNSNOOZE] No thread found for user {user} (obj: {user_obj})\"\n+                )\n                 return\n         elif hasattr(ctx, \"thread\"):\n             thread = ctx.thread\n@@ -2343,12 +2513,16 @@ class Modmail(commands.Cog):\n             return\n         if not thread.snoozed:\n             await ctx.send(\"This thread is not snoozed.\")\n-            logging.info(f\"[UNSNOOZE] Thread for {getattr(thread.recipient, 'id', None)} is not snoozed.\")\n+            logging.info(\n+                f\"[UNSNOOZE] Thread for {getattr(thread.recipient, 'id', None)} is not snoozed.\"\n+            )\n             return\n \n         # Manually fetch snooze_data if the thread object doesn't have it\n         if not thread.snooze_data:\n-            log_entry = await self.bot.api.logs.find_one({\"recipient.id\": str(thread.id), \"snoozed\": True})\n+            log_entry = await self.bot.api.logs.find_one(\n+                {\"recipient.id\": str(thread.id), \"snoozed\": True}\n+            )\n             if log_entry:\n                 thread.snooze_data = log_entry.get(\"snooze_data\")\n \n@@ -2356,9 +2530,12 @@ class Modmail(commands.Cog):\n         if ok:\n             self.bot.threads.cache[thread.id] = thread\n             await ctx.send(\n-                self.bot.config.get(\"unsnooze_text\") or \"This thread has been unsnoozed and restored.\"\n+                self.bot.config.get(\"unsnooze_text\")\n+                or \"This thread has been unsnoozed and restored.\"\n+            )\n+            logging.info(\n+                f\"[UNSNOOZE] Thread for {getattr(thread.recipient, 'id', None)} unsnoozed.\"\n             )\n-            logging.info(f\"[UNSNOOZE] Thread for {getattr(thread.recipient, 'id', None)} unsnoozed.\")\n         else:\n             await ctx.send(\"Failed to unsnooze this thread.\")\n             logging.error(\n@@ -2371,7 +2548,9 @@ class Modmail(commands.Cog):\n         \"\"\"\n         List all currently snoozed threads/users.\n         \"\"\"\n-        snoozed_threads = [thread for thread in self.bot.threads.cache.values() if thread.snoozed]\n+        snoozed_threads = [\n+            thread for thread in self.bot.threads.cache.values() if thread.snoozed\n+        ]\n         if not snoozed_threads:\n             await ctx.send(\"No threads are currently snoozed.\")\n             return\n@@ -2394,13 +2573,17 @@ class Modmail(commands.Cog):\n                         since_dt = datetime.fromisoformat(since)\n                         since_str = f\"<t:{int(since_dt.timestamp())}:R>\"  # Discord relative timestamp\n                     except (ValueError, TypeError) as e:\n-                        logging.warning(f\"[SNOOZED] Invalid snooze_start for {user_id}: {since} ({e})\")\n+                        logging.warning(\n+                            f\"[SNOOZED] Invalid snooze_start for {user_id}: {since} ({e})\"\n+                        )\n                 else:\n                     logging.warning(f\"[SNOOZED] Missing snooze_start for {user_id}\")\n \n                 if duration and since_str != \"?\":\n                     try:\n-                        until_dt = datetime.fromisoformat(since) + timedelta(seconds=int(duration))\n+                        until_dt = datetime.fromisoformat(since) + timedelta(\n+                            seconds=int(duration)\n+                        )\n                         until_str = f\"<t:{int(until_dt.timestamp())}:R>\"\n                     except (ValueError, TypeError) as e:\n                         logging.warning(\n@@ -2434,7 +2617,9 @@ class Modmail(commands.Cog):\n                     duration = int(max_snooze)\n                 if (now - start_dt).total_seconds() > duration:\n                     # Auto-unsnooze\n-                    thread = await self.bot.threads.find(recipient_id=int(entry[\"recipient\"][\"id\"]))\n+                    thread = await self.bot.threads.find(\n+                        recipient_id=int(entry[\"recipient\"][\"id\"])\n+                    )\n                     if thread and thread.snoozed:\n                         await thread.restore_from_snooze()\n             await asyncio.sleep(60)\ndiff --git a/core/changelog.py b/core/changelog.py\nindex 465479b..d871670 100644\n--- a/core/changelog.py\n+++ b/core/changelog.py\n@@ -53,7 +53,9 @@ class Version:\n         self.version = version.lstrip(\"vV\")\n         self.lines = lines.strip()\n         self.fields = {}\n-        self.changelog_url = f\"https://github.com/modmail-dev/modmail/blob/{branch}/CHANGELOG.md\"\n+        self.changelog_url = (\n+            f\"https://github.com/modmail-dev/modmail/blob/{branch}/CHANGELOG.md\"\n+        )\n         self.description = \"\"\n         self.parse()\n \n@@ -65,7 +67,9 @@ class Version:\n         Parse the lines and split them into `description` and `fields`.\n         \"\"\"\n         self.description = re.match(self.DESCRIPTION_REGEX, self.lines, re.DOTALL)\n-        self.description = self.description.group(1).strip() if self.description is not None else \"\"\n+        self.description = (\n+            self.description.group(1).strip() if self.description is not None else \"\"\n+        )\n \n         matches = re.finditer(self.ACTION_REGEX, self.lines, re.DOTALL)\n         for m in matches:\n@@ -90,7 +94,9 @@ class Version:\n         embed = Embed(color=self.bot.main_color, description=self.description)\n         embed.set_author(\n             name=f\"v{self.version} - Changelog\",\n-            icon_url=self.bot.user.display_avatar.url if self.bot.user.display_avatar else None,\n+            icon_url=self.bot.user.display_avatar.url\n+            if self.bot.user.display_avatar\n+            else None,\n             url=self.url,\n         )\n \n@@ -98,7 +104,11 @@ class Version:\n             embed.add_field(name=name, value=truncate(value, 1024), inline=False)\n         embed.set_footer(text=f\"Current version: v{self.bot.version}\")\n \n-        embed.set_thumbnail(url=self.bot.user.display_avatar.url if self.bot.user.display_avatar else None)\n+        embed.set_thumbnail(\n+            url=self.bot.user.display_avatar.url\n+            if self.bot.user.display_avatar\n+            else None\n+        )\n         return embed\n \n \n@@ -137,7 +147,9 @@ class Changelog:\n         self.bot = bot\n         self.text = text\n         logger.debug(\"Fetching changelog from GitHub.\")\n-        self.versions = [Version(bot, branch, *m) for m in self.VERSION_REGEX.findall(text)]\n+        self.versions = [\n+            Version(bot, branch, *m) for m in self.VERSION_REGEX.findall(text)\n+        ]\n \n     @property\n     def latest_version(self) -> Version:\n@@ -186,7 +198,10 @@ class Changelog:\n         if branch not in (\"master\", \"development\"):\n             branch = \"master\"\n \n-        url = url or f\"https://raw.githubusercontent.com/modmail-dev/modmail/{branch}/CHANGELOG.md\"\n+        url = (\n+            url\n+            or f\"https://raw.githubusercontent.com/modmail-dev/modmail/{branch}/CHANGELOG.md\"\n+        )\n \n         async with await bot.session.get(url) as resp:\n             return cls(bot, branch, await resp.text())\ndiff --git a/core/clients.py b/core/clients.py\nindex 459ee2e..3b07e88 100644\n--- a/core/clients.py\n+++ b/core/clients.py\n@@ -124,7 +124,9 @@ class GitHub:\n             headers.update(self.headers)\n         else:\n             headers = self.headers\n-        async with self.session.request(method, url, headers=headers, json=payload) as resp:\n+        async with self.session.request(\n+            method, url, headers=headers, json=payload\n+        ) as resp:\n             if return_response:\n                 if read_before_return:\n                     await resp.read()\n@@ -133,7 +135,9 @@ class GitHub:\n             return await self._get_response_data(resp)\n \n     @staticmethod\n-    async def _get_response_data(response: ClientResponse) -> Union[Dict[str, Any], str]:\n+    async def _get_response_data(\n+        response: ClientResponse,\n+    ) -> Union[Dict[str, Any], str]:\n         \"\"\"\n         Internal method to convert the response data to `dict` if the data is a\n         json object, or to `str` (raw response) if the data is not a valid json.\n@@ -157,7 +161,9 @@ class GitHub:\n         Dict[str, Any]\n             Filtered `data` to keep only the accepted pairs.\n         \"\"\"\n-        valid_keys = self.bot.config.valid_keys.difference(self.bot.config.protected_keys)\n+        valid_keys = self.bot.config.valid_keys.difference(\n+            self.bot.config.protected_keys\n+        )\n         return {k: v for k, v in data.items() if k in valid_keys}\n \n     async def update_repository(self, sha: str = None) -> Dict[str, Any]:\n@@ -335,7 +341,9 @@ class ApiClient:\n             `str` if the returned data is not a valid json data,\n             the raw response.\n         \"\"\"\n-        async with self.session.request(method, url, headers=headers, json=payload) as resp:\n+        async with self.session.request(\n+            method, url, headers=headers, json=payload\n+        ) as resp:\n             if return_response:\n                 return resp\n             try:\n@@ -374,7 +382,9 @@ class ApiClient:\n     async def get_log_link(self, channel_id: Union[str, int]) -> str:\n         return NotImplemented\n \n-    async def create_log_entry(self, recipient: Member, channel: TextChannel, creator: Member) -> str:\n+    async def create_log_entry(\n+        self, recipient: Member, channel: TextChannel, creator: Member\n+    ) -> str:\n         return NotImplemented\n \n     async def delete_log_entry(self, key: str) -> bool:\n@@ -408,7 +418,9 @@ class ApiClient:\n     async def search_by_text(self, text: str, limit: Optional[int]):\n         return NotImplemented\n \n-    async def create_note(self, recipient: Member, message: Message, message_id: Union[int, str]):\n+    async def create_note(\n+        self, recipient: Member, message: Message, message_id: Union[int, str]\n+    ):\n         return NotImplemented\n \n     async def find_notes(self, recipient: Member):\n@@ -476,7 +488,11 @@ class MongoDBClient(ApiClient):\n             logger.info('Creating \"text\" index for logs collection.')\n             logger.info(\"Name: %s\", index_name)\n             await coll.create_index(\n-                [(\"messages.content\", \"text\"), (\"messages.author.name\", \"text\"), (\"key\", \"text\")]\n+                [\n+                    (\"messages.content\", \"text\"),\n+                    (\"messages.author.name\", \"text\"),\n+                    (\"key\", \"text\"),\n+                ]\n             )\n         logger.debug(\"Successfully configured and verified database indexes.\")\n \n@@ -500,7 +516,9 @@ class MongoDBClient(ApiClient):\n                     'run \"Certificate.command\" on MacOS, '\n                     'and check certifi is up to date \"pip3 install --upgrade certifi\".'\n                 )\n-                self.db = AsyncIOMotorClient(mongo_uri, tlsAllowInvalidCertificates=True).modmail_bot\n+                self.db = AsyncIOMotorClient(\n+                    mongo_uri, tlsAllowInvalidCertificates=True\n+                ).modmail_bot\n                 return await self.validate_database_connection(ssl_retry=False)\n             if \"ServerSelectionTimeoutError\" in message:\n                 logger.critical(\n@@ -540,11 +558,17 @@ class MongoDBClient(ApiClient):\n         return await self.logs.find(query, projection).to_list(None)\n \n     async def get_latest_user_logs(self, user_id: Union[str, int]):\n-        query = {\"recipient.id\": str(user_id), \"guild_id\": str(self.bot.guild_id), \"open\": False}\n+        query = {\n+            \"recipient.id\": str(user_id),\n+            \"guild_id\": str(self.bot.guild_id),\n+            \"open\": False,\n+        }\n         projection = {\"messages\": {\"$slice\": 5}}\n         logger.debug(\"Retrieving user %s latest logs.\", user_id)\n \n-        return await self.logs.find_one(query, projection, limit=1, sort=[(\"closed_at\", -1)])\n+        return await self.logs.find_one(\n+            query, projection, limit=1, sort=[(\"closed_at\", -1)]\n+        )\n \n     async def get_responded_logs(self, user_id: Union[str, int]) -> list:\n         query = {\n@@ -575,7 +599,9 @@ class MongoDBClient(ApiClient):\n             prefix = \"\"\n         return f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{doc['key']}\"\n \n-    async def create_log_entry(self, recipient: Member, channel: TextChannel, creator: Member) -> str:\n+    async def create_log_entry(\n+        self, recipient: Member, channel: TextChannel, creator: Member\n+    ) -> str:\n         key = secrets.token_hex(6)\n \n         await self.logs.insert_one(\n@@ -592,14 +618,18 @@ class MongoDBClient(ApiClient):\n                     \"id\": str(recipient.id),\n                     \"name\": recipient.name,\n                     \"discriminator\": recipient.discriminator,\n-                    \"avatar_url\": recipient.display_avatar.url if recipient.display_avatar else None,\n+                    \"avatar_url\": recipient.display_avatar.url\n+                    if recipient.display_avatar\n+                    else None,\n                     \"mod\": False,\n                 },\n                 \"creator\": {\n                     \"id\": str(creator.id),\n                     \"name\": creator.name,\n                     \"discriminator\": creator.discriminator,\n-                    \"avatar_url\": creator.display_avatar.url if creator.display_avatar else None,\n+                    \"avatar_url\": creator.display_avatar.url\n+                    if creator.display_avatar\n+                    else None,\n                     \"mod\": isinstance(creator, Member),\n                 },\n                 \"closer\": None,\n@@ -626,16 +656,22 @@ class MongoDBClient(ApiClient):\n \n     async def update_config(self, data: dict):\n         toset = self.bot.config.filter_valid(data)\n-        unset = self.bot.config.filter_valid({k: 1 for k in self.bot.config.all_keys if k not in data})\n+        unset = self.bot.config.filter_valid(\n+            {k: 1 for k in self.bot.config.all_keys if k not in data}\n+        )\n \n         if toset and unset:\n             return await self.db.config.update_one(\n                 {\"bot_id\": self.bot.user.id}, {\"$set\": toset, \"$unset\": unset}\n             )\n         if toset:\n-            return await self.db.config.update_one({\"bot_id\": self.bot.user.id}, {\"$set\": toset})\n+            return await self.db.config.update_one(\n+                {\"bot_id\": self.bot.user.id}, {\"$set\": toset}\n+            )\n         if unset:\n-            return await self.db.config.update_one({\"bot_id\": self.bot.user.id}, {\"$unset\": unset})\n+            return await self.db.config.update_one(\n+                {\"bot_id\": self.bot.user.id}, {\"$unset\": unset}\n+            )\n \n     async def edit_message(self, message_id: Union[int, str], new_content: str) -> None:\n         await self.logs.update_one(\n@@ -661,7 +697,9 @@ class MongoDBClient(ApiClient):\n                 \"id\": str(message.author.id),\n                 \"name\": message.author.name,\n                 \"discriminator\": message.author.discriminator,\n-                \"avatar_url\": message.author.display_avatar.url if message.author.display_avatar else None,\n+                \"avatar_url\": message.author.display_avatar.url\n+                if message.author.display_avatar\n+                else None,\n                 \"mod\": not isinstance(message.channel, DMChannel),\n             },\n             \"content\": message.content,\n@@ -679,7 +717,9 @@ class MongoDBClient(ApiClient):\n         }\n \n         return await self.logs.find_one_and_update(\n-            {\"channel_id\": channel_id}, {\"$push\": {\"messages\": data}}, return_document=True\n+            {\"channel_id\": channel_id},\n+            {\"$push\": {\"messages\": data}},\n+            return_document=True,\n         )\n \n     async def post_log(self, channel_id: Union[int, str], data: dict) -> dict:\n@@ -689,7 +729,11 @@ class MongoDBClient(ApiClient):\n \n     async def search_closed_by(self, user_id: Union[int, str]):\n         return await self.logs.find(\n-            {\"guild_id\": str(self.bot.guild_id), \"open\": False, \"closer.id\": str(user_id)},\n+            {\n+                \"guild_id\": str(self.bot.guild_id),\n+                \"open\": False,\n+                \"closer.id\": str(user_id),\n+            },\n             {\"messages\": {\"$slice\": 5}},\n         ).to_list(None)\n \n@@ -703,7 +747,9 @@ class MongoDBClient(ApiClient):\n             {\"messages\": {\"$slice\": 5}},\n         ).to_list(limit)\n \n-    async def create_note(self, recipient: Member, message: Message, message_id: Union[int, str]):\n+    async def create_note(\n+        self, recipient: Member, message: Message, message_id: Union[int, str]\n+    ):\n         await self.db.notes.insert_one(\n             {\n                 \"recipient\": str(recipient.id),\n@@ -712,7 +758,9 @@ class MongoDBClient(ApiClient):\n                     \"name\": message.author.name,\n                     \"discriminator\": message.author.discriminator,\n                     \"avatar_url\": (\n-                        message.author.display_avatar.url if message.author.display_avatar else None\n+                        message.author.display_avatar.url\n+                        if message.author.display_avatar\n+                        else None\n                     ),\n                 },\n                 \"message\": message.content,\n@@ -725,13 +773,17 @@ class MongoDBClient(ApiClient):\n \n     async def update_note_ids(self, ids: dict):\n         for object_id, message_id in ids.items():\n-            await self.db.notes.update_one({\"_id\": object_id}, {\"$set\": {\"message_id\": message_id}})\n+            await self.db.notes.update_one(\n+                {\"_id\": object_id}, {\"$set\": {\"message_id\": message_id}}\n+            )\n \n     async def delete_note(self, message_id: Union[int, str]):\n         await self.db.notes.delete_one({\"message_id\": str(message_id)})\n \n     async def edit_note(self, message_id: Union[int, str], message: str):\n-        await self.db.notes.update_one({\"message_id\": str(message_id)}, {\"$set\": {\"message\": message}})\n+        await self.db.notes.update_one(\n+            {\"message_id\": str(message_id)}, {\"$set\": {\"message\": message}}\n+        )\n \n     def get_plugin_partition(self, cog):\n         cls_name = cog.__class__.__name__\ndiff --git a/core/config.py b/core/config.py\nindex a704443..2e47b21 100644\n--- a/core/config.py\n+++ b/core/config.py\n@@ -198,7 +198,13 @@ class ConfigManager:\n \n     colors = {\"mod_color\", \"recipient_color\", \"main_color\", \"error_color\"}\n \n-    time_deltas = {\"account_age\", \"guild_age\", \"thread_auto_close\", \"thread_cooldown\", \"log_expiration\"}\n+    time_deltas = {\n+        \"account_age\",\n+        \"guild_age\",\n+        \"thread_auto_close\",\n+        \"thread_cooldown\",\n+        \"log_expiration\",\n+    }\n \n     booleans = {\n         \"use_user_id_channel_name\",\n@@ -265,19 +271,33 @@ class ConfigManager:\n         data = deepcopy(self.defaults)\n \n         # populate from env var and .env file\n-        data.update({k.lower(): v for k, v in os.environ.items() if k.lower() in self.all_keys})\n-        config_json = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"config.json\")\n+        data.update(\n+            {k.lower(): v for k, v in os.environ.items() if k.lower() in self.all_keys}\n+        )\n+        config_json = os.path.join(\n+            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"config.json\"\n+        )\n         if os.path.exists(config_json):\n             logger.debug(\"Loading envs from config.json.\")\n             with open(config_json, \"r\", encoding=\"utf-8\") as f:\n                 # Config json should override env vars\n                 try:\n-                    data.update({k.lower(): v for k, v in json.load(f).items() if k.lower() in self.all_keys})\n+                    data.update(\n+                        {\n+                            k.lower(): v\n+                            for k, v in json.load(f).items()\n+                            if k.lower() in self.all_keys\n+                        }\n+                    )\n                 except json.JSONDecodeError:\n-                    logger.critical(\"Failed to load config.json env values.\", exc_info=True)\n+                    logger.critical(\n+                        \"Failed to load config.json env values.\", exc_info=True\n+                    )\n         self._cache = data\n \n-        config_help_json = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"config_help.json\")\n+        config_help_json = os.path.join(\n+            os.path.dirname(os.path.abspath(__file__)), \"config_help.json\"\n+        )\n         with open(config_help_json, \"r\", encoding=\"utf-8\") as f:\n             self.config_help = dict(sorted(json.load(f).items()))\n \n@@ -419,7 +439,9 @@ class ConfigManager:\n             except isodate.ISO8601Error:\n                 try:\n                     converter = UserFriendlyTime()\n-                    time = await converter.convert(None, item, now=discord.utils.utcnow())\n+                    time = await converter.convert(\n+                        None, item, now=discord.utils.utcnow()\n+                    )\n                     if time.arg:\n                         raise ValueError\n                 except BadArgument as exc:\n@@ -461,7 +483,9 @@ class ConfigManager:\n         return self._cache.items()\n \n     @classmethod\n-    def filter_valid(cls, data: typing.Dict[str, typing.Any]) -> typing.Dict[str, typing.Any]:\n+    def filter_valid(\n+        cls, data: typing.Dict[str, typing.Any]\n+    ) -> typing.Dict[str, typing.Any]:\n         return {\n             k.lower(): v\n             for k, v in data.items()\n@@ -469,7 +493,9 @@ class ConfigManager:\n         }\n \n     @classmethod\n-    def filter_default(cls, data: typing.Dict[str, typing.Any]) -> typing.Dict[str, typing.Any]:\n+    def filter_default(\n+        cls, data: typing.Dict[str, typing.Any]\n+    ) -> typing.Dict[str, typing.Any]:\n         # TODO: use .get to prevent errors\n         filtered = {}\n         for k, v in data.items():\ndiff --git a/core/models.py b/core/models.py\nindex 611db37..e8418ef 100644\n--- a/core/models.py\n+++ b/core/models.py\n@@ -30,15 +30,15 @@ if \".heroku\" in os.environ.get(\"PYTHONHOME\", \"\"):\n class ModmailLogger(logging.Logger):\n     @staticmethod\n     def _debug_(*msgs):\n-        return f'{Fore.CYAN}{\" \".join(msgs)}{Style.RESET_ALL}'\n+        return f\"{Fore.CYAN}{' '.join(msgs)}{Style.RESET_ALL}\"\n \n     @staticmethod\n     def _info_(*msgs):\n-        return f'{Fore.LIGHTMAGENTA_EX}{\" \".join(msgs)}{Style.RESET_ALL}'\n+        return f\"{Fore.LIGHTMAGENTA_EX}{' '.join(msgs)}{Style.RESET_ALL}\"\n \n     @staticmethod\n     def _error_(*msgs):\n-        return f'{Fore.RED}{\" \".join(msgs)}{Style.RESET_ALL}'\n+        return f\"{Fore.RED}{' '.join(msgs)}{Style.RESET_ALL}\"\n \n     def debug(self, msg, *args, **kwargs):\n         if self.isEnabledFor(logging.DEBUG):\n@@ -70,7 +70,10 @@ class ModmailLogger(logging.Logger):\n         if self.isEnabledFor(level):\n             self._log(\n                 level,\n-                Fore.BLACK + Style.BRIGHT + \"-------------------------\" + Style.RESET_ALL,\n+                Fore.BLACK\n+                + Style.BRIGHT\n+                + \"-------------------------\"\n+                + Style.RESET_ALL,\n                 [],\n             )\n \n@@ -95,7 +98,9 @@ class JsonFormatter(logging.Formatter):\n         time_format: str = \"%Y-%m-%dT%H:%M:%S\",\n         msec_format: str = \"%s.%03dZ\",\n     ):\n-        self.fmt_dict: Dict[str, str] = fmt_dict if fmt_dict is not None else {\"message\": \"message\"}\n+        self.fmt_dict: Dict[str, str] = (\n+            fmt_dict if fmt_dict is not None else {\"message\": \"message\"}\n+        )\n         self.default_time_format: str = time_format\n         self.default_msec_format: str = msec_format\n         self.datefmt: Optional[str] = None\n@@ -111,7 +116,10 @@ class JsonFormatter(logging.Formatter):\n         Overwritten to return a dictionary of the relevant LogRecord attributes instead of a string.\n         KeyError is raised if an unknown attribute is provided in the fmt_dict.\n         \"\"\"\n-        return {fmt_key: record.__dict__[fmt_val] for fmt_key, fmt_val in self.fmt_dict.items()}\n+        return {\n+            fmt_key: record.__dict__[fmt_val]\n+            for fmt_key, fmt_val in self.fmt_dict.items()\n+        }\n \n     def format(self, record) -> str:\n         \"\"\"\n@@ -149,7 +157,8 @@ class FileFormatter(logging.Formatter):\n \n \n log_stream_formatter = logging.Formatter(\n-    \"%(asctime)s %(name)s[%(lineno)d] - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%y %H:%M:%S\"\n+    \"%(asctime)s %(name)s[%(lineno)d] - %(levelname)s: %(message)s\",\n+    datefmt=\"%m/%d/%y %H:%M:%S\",\n )\n \n log_file_formatter = FileFormatter(\n@@ -221,7 +230,9 @@ def create_log_handler(\n     depending on the `rotating` value.\n     \"\"\"\n     if filename is None and rotating:\n-        raise ValueError(\"`filename` must be set to instantiate a `RotatingFileHandler`.\")\n+        raise ValueError(\n+            \"`filename` must be set to instantiate a `RotatingFileHandler`.\"\n+        )\n \n     if filename is None:\n         handler = StreamHandler(stream=sys.stdout, **kwargs)\n@@ -231,7 +242,12 @@ def create_log_handler(\n         formatter = log_file_formatter\n     else:\n         handler = RotatingFileHandler(\n-            filename, mode=mode, encoding=encoding, maxBytes=maxBytes, backupCount=backupCount, **kwargs\n+            filename,\n+            mode=mode,\n+            encoding=encoding,\n+            maxBytes=maxBytes,\n+            backupCount=backupCount,\n+            **kwargs,\n         )\n         formatter = log_file_formatter\n \n@@ -264,7 +280,10 @@ def getLogger(name=None) -> ModmailLogger:\n def configure_logging(bot) -> None:\n     global ch_debug, log_level, ch\n \n-    stream_log_format, file_log_format = bot.config[\"stream_log_format\"], bot.config[\"file_log_format\"]\n+    stream_log_format, file_log_format = (\n+        bot.config[\"stream_log_format\"],\n+        bot.config[\"file_log_format\"],\n+    )\n     if stream_log_format == \"json\":\n         ch.setFormatter(json_formatter)\n \n@@ -318,8 +337,13 @@ def configure_logging(bot) -> None:\n     non_verbose_log_level = max(d_level, logging.INFO)\n     stream_handler = create_log_handler(level=non_verbose_log_level)\n     if non_verbose_log_level != d_level:\n-        logger.info(\"Discord logging level (stdout): %s.\", logging.getLevelName(non_verbose_log_level))\n-        logger.info(\"Discord logging level (logfile): %s.\", logging.getLevelName(d_level))\n+        logger.info(\n+            \"Discord logging level (stdout): %s.\",\n+            logging.getLevelName(non_verbose_log_level),\n+        )\n+        logger.info(\n+            \"Discord logging level (logfile): %s.\", logging.getLevelName(d_level)\n+        )\n     else:\n         logger.info(\"Discord logging level: %s.\", logging.getLevelName(d_level))\n     d_logger.addHandler(stream_handler)\n@@ -336,7 +360,9 @@ class InvalidConfigError(commands.BadArgument):\n     @property\n     def embed(self):\n         # Single reference of Color.red()\n-        return discord.Embed(title=\"Error\", description=self.msg, color=discord.Color.red())\n+        return discord.Embed(\n+            title=\"Error\", description=self.msg, color=discord.Color.red()\n+        )\n \n \n class _Default:\n@@ -403,7 +429,9 @@ class SimilarCategoryConverter(commands.CategoryChannelConverter):\n                     if isinstance(c, discord.CategoryChannel)\n                 }\n \n-            result = get_close_matches(argument.casefold(), categories.keys(), n=1, cutoff=0.75)\n+            result = get_close_matches(\n+                argument.casefold(), categories.keys(), n=1, cutoff=0.75\n+            )\n             if result:\n                 result = categories[result[0]]\n \ndiff --git a/core/paginator.py b/core/paginator.py\nindex d0b10c0..66bd7c1 100644\n--- a/core/paginator.py\n+++ b/core/paginator.py\n@@ -288,14 +288,20 @@ class PageButton(Button):\n \n \n class PageSelect(Select):\n-    def __init__(self, handler: PaginatorSession, pages: typing.List[typing.Tuple[str]]):\n+    def __init__(\n+        self, handler: PaginatorSession, pages: typing.List[typing.Tuple[str]]\n+    ):\n         self.handler = handler\n         options = []\n         for n, (label, description) in enumerate(pages):\n-            options.append(discord.SelectOption(label=label, description=description, value=str(n)))\n+            options.append(\n+                discord.SelectOption(label=label, description=description, value=str(n))\n+            )\n \n         options = options[:25]  # max 25 options\n-        super().__init__(placeholder=\"Select a page\", min_values=1, max_values=1, options=options)\n+        super().__init__(\n+            placeholder=\"Select a page\", min_values=1, max_values=1, options=options\n+        )\n \n     async def callback(self, interaction: Interaction):\n         page = int(self.values[0])\n@@ -334,7 +340,9 @@ class EmbedPaginatorSession(PaginatorSession):\n                         create_select = False\n \n                 if embed.description:\n-                    description = embed.description[:40].replace(\"*\", \"\").replace(\"`\", \"\").strip()\n+                    description = (\n+                        embed.description[:40].replace(\"*\", \"\").replace(\"`\", \"\").strip()\n+                    )\n                     if len(embed.description) > 40:\n                         description += \"...\"\n                 else:\n@@ -342,7 +350,9 @@ class EmbedPaginatorSession(PaginatorSession):\n                 select_options.append((title, description))\n \n             if create_select:\n-                if len(set(x[0] for x in select_options)) != 1:  # must have unique authors\n+                if (\n+                    len(set(x[0] for x in select_options)) != 1\n+                ):  # must have unique authors\n                     self.select_menu = PageSelect(self, select_options)\n \n     def add_page(self, item: Embed) -> None:\n@@ -359,7 +369,9 @@ class EmbedPaginatorSession(PaginatorSession):\n \n \n class MessagePaginatorSession(PaginatorSession):\n-    def __init__(self, ctx: commands.Context, *messages, embed: Embed = None, **options):\n+    def __init__(\n+        self, ctx: commands.Context, *messages, embed: Embed = None, **options\n+    ):\n         self.embed = embed\n         self.footer_text = self.embed.footer.text if embed is not None else None\n         super().__init__(ctx, *messages, **options)\n@@ -372,12 +384,14 @@ class MessagePaginatorSession(PaginatorSession):\n \n     def _set_footer(self):\n         if self.embed is not None:\n-            footer_text = f\"Page {self.current+1} of {len(self.pages)}\"\n+            footer_text = f\"Page {self.current + 1} of {len(self.pages)}\"\n             if self.footer_text:\n                 footer_text = footer_text + \" \u2022 \" + self.footer_text\n \n             if self.embed.footer.icon:\n-                icon_url = self.embed.footer.icon.url if self.embed.footer.icon else None\n+                icon_url = (\n+                    self.embed.footer.icon.url if self.embed.footer.icon else None\n+                )\n             else:\n                 icon_url = None\n \ndiff --git a/core/thread.py b/core/thread.py\nindex d380552..e9a6603 100644\n--- a/core/thread.py\n+++ b/core/thread.py\n@@ -19,7 +19,6 @@ from lottie.importers import importers as l_importers\n from lottie.exporters import exporters as l_exporters\n \n from core.models import DMDisabled, DummyMessage, getLogger\n-from core.time import human_timedelta\n from core.utils import (\n     is_image_url,\n     parse_channel_topic,\n@@ -33,7 +32,6 @@ from core.utils import (\n     DenyButton,\n     ConfirmThreadCreationView,\n     DummyParam,\n-    extract_forwarded_content,\n )\n \n logger = getLogger(__name__)\n@@ -47,7 +45,9 @@ class Thread:\n         manager: \"ThreadManager\",\n         recipient: typing.Union[discord.Member, discord.User, int],\n         channel: typing.Union[discord.DMChannel, discord.TextChannel] = None,\n-        other_recipients: typing.List[typing.Union[discord.Member, discord.User]] = None,\n+        other_recipients: typing.List[\n+            typing.Union[discord.Member, discord.User]\n+        ] = None,\n     ):\n         self.manager = manager\n         self.bot = manager.bot\n@@ -69,7 +69,9 @@ class Thread:\n         self._cancelled = False\n         # --- SNOOZE STATE ---\n         self.snoozed = False  # True if thread is snoozed\n-        self.snooze_data = None  # Dict with channel/category/position/messages for restoration\n+        self.snooze_data = (\n+            None  # Dict with channel/category/position/messages for restoration\n+        )\n         self.log_key = None  # Ensure log_key always exists\n \n     def __repr__(self):\n@@ -83,7 +85,9 @@ class Thread:\n     async def wait_until_ready(self) -> None:\n         \"\"\"Blocks execution until the thread is fully set up.\"\"\"\n         # timeout after 30 seconds\n-        task = self.bot.loop.create_task(asyncio.wait_for(self._ready_event.wait(), timeout=25))\n+        task = self.bot.loop.create_task(\n+            asyncio.wait_for(self._ready_event.wait(), timeout=25)\n+        )\n         self.wait_tasks.append(task)\n         try:\n             await task\n@@ -160,7 +164,9 @@ class Thread:\n             \"topic\": channel.topic,\n             \"slowmode_delay\": channel.slowmode_delay,\n             \"nsfw\": channel.nsfw,\n-            \"overwrites\": [(role.id, perm._values) for role, perm in channel.overwrites.items()],\n+            \"overwrites\": [\n+                (role.id, perm._values) for role, perm in channel.overwrites.items()\n+            ],\n             \"messages\": [\n                 {\n                     \"author_id\": m.author.id,\n@@ -174,8 +180,12 @@ class Thread:\n                             m.embeds\n                             and getattr(m.embeds[0], \"author\", None)\n                             and (\n-                                getattr(m.embeds[0].author, \"name\", \"\").startswith(\"\ud83d\udcdd Note\")\n-                                or getattr(m.embeds[0].author, \"name\", \"\").startswith(\"\ud83d\udcdd Persistent Note\")\n+                                getattr(m.embeds[0].author, \"name\", \"\").startswith(\n+                                    \"\ud83d\udcdd Note\"\n+                                )\n+                                or getattr(m.embeds[0].author, \"name\", \"\").startswith(\n+                                    \"\ud83d\udcdd Persistent Note\"\n+                                )\n                             )\n                         )\n                         else None\n@@ -188,7 +198,9 @@ class Thread:\n                     and getattr(m.embeds[0], \"author\", None)\n                     and (\n                         getattr(m.embeds[0].author, \"name\", \"\").startswith(\"\ud83d\udcdd Note\")\n-                        or getattr(m.embeds[0].author, \"name\", \"\").startswith(\"\ud83d\udcdd Persistent Note\")\n+                        or getattr(m.embeds[0].author, \"name\", \"\").startswith(\n+                            \"\ud83d\udcdd Persistent Note\"\n+                        )\n                     )\n                 )\n                 and getattr(m, \"type\", None) not in (\"internal\", \"note\")\n@@ -254,7 +266,9 @@ class Thread:\n         self.log_key = self.snooze_data.get(\"log_key\")\n         # Replay messages\n         for msg in self.snooze_data[\"messages\"]:\n-            author = self.bot.get_user(msg[\"author_id\"]) or await self.bot.get_or_fetch_user(msg[\"author_id\"])\n+            author = self.bot.get_user(\n+                msg[\"author_id\"]\n+            ) or await self.bot.get_or_fetch_user(msg[\"author_id\"])\n             content = msg[\"content\"]\n             embeds = [discord.Embed.from_dict(e) for e in msg.get(\"embeds\", []) if e]\n             attachments = msg.get(\"attachments\", [])\n@@ -264,7 +278,11 @@ class Thread:\n                 continue  # Skip empty messages\n             author_is_mod = msg[\"author_id\"] not in [r.id for r in self.recipients]\n             if author_is_mod:\n-                username = msg.get(\"author_name\") or (getattr(author, \"name\", None)) or \"Unknown\"\n+                username = (\n+                    msg.get(\"author_name\")\n+                    or (getattr(author, \"name\", None))\n+                    or \"Unknown\"\n+                )\n                 user_id = msg.get(\"author_id\")\n                 if embeds:\n                     embeds[0].set_author(\n@@ -278,7 +296,9 @@ class Thread:\n                     await channel.send(embeds=embeds)\n                 else:\n                     formatted = (\n-                        f\"**{username} ({user_id})**: {content}\" if content else f\"**{username} ({user_id})**\"\n+                        f\"**{username} ({user_id})**: {content}\"\n+                        if content\n+                        else f\"**{username} ({user_id})**\"\n                     )\n                     await channel.send(formatted)\n             else:\n@@ -291,12 +311,18 @@ class Thread:\n         if self.log_key:\n             result = await self.bot.api.logs.update_one(\n                 {\"key\": self.log_key},\n-                {\"$set\": {\"channel_id\": str(channel.id)}, \"$unset\": {\"snoozed\": \"\", \"snooze_data\": \"\"}},\n+                {\n+                    \"$set\": {\"channel_id\": str(channel.id)},\n+                    \"$unset\": {\"snoozed\": \"\", \"snooze_data\": \"\"},\n+                },\n             )\n         else:\n             result = await self.bot.api.logs.update_one(\n                 {\"recipient.id\": str(self.id)},\n-                {\"$set\": {\"channel_id\": str(channel.id)}, \"$unset\": {\"snoozed\": \"\", \"snooze_data\": \"\"}},\n+                {\n+                    \"$set\": {\"channel_id\": str(channel.id)},\n+                    \"$unset\": {\"snoozed\": \"\", \"snooze_data\": \"\"},\n+                },\n             )\n             if result.modified_count == 0:\n                 result = await self.bot.api.logs.update_one(\n@@ -311,24 +337,37 @@ class Thread:\n         logging.info(f\"[UNSNOOZE] DB update result: {result.modified_count}\")\n         # Notify in the configured channel\n         notify_channel = self.bot.config.get(\"unsnooze_notify_channel\") or \"thread\"\n-        notify_text = self.bot.config.get(\"unsnooze_text\") or \"This thread has been unsnoozed and restored.\"\n+        notify_text = (\n+            self.bot.config.get(\"unsnooze_text\")\n+            or \"This thread has been unsnoozed and restored.\"\n+        )\n         if notify_channel == \"thread\":\n             await channel.send(notify_text)\n         else:\n             ch = self.bot.get_channel(int(notify_channel))\n             if ch:\n-                await ch.send(f\"Thread for user <@{self.id}> has been unsnoozed and restored.\")\n+                await ch.send(\n+                    f\"Thread for user <@{self.id}> has been unsnoozed and restored.\"\n+                )\n         # Show who ran the snooze command and the command used\n         # Use snooze_data_for_notify to avoid accessing self.snooze_data after it is set to None\n-        snoozed_by = snooze_data_for_notify.get(\"snoozed_by\") if snooze_data_for_notify else None\n-        snooze_command = snooze_data_for_notify.get(\"snooze_command\") if snooze_data_for_notify else None\n+        snoozed_by = (\n+            snooze_data_for_notify.get(\"snoozed_by\") if snooze_data_for_notify else None\n+        )\n+        snooze_command = (\n+            snooze_data_for_notify.get(\"snooze_command\")\n+            if snooze_data_for_notify\n+            else None\n+        )\n         if snoozed_by or snooze_command:\n             info = f\"Snoozed by: {snoozed_by or 'Unknown'} | Command: {snooze_command or '?snooze'}\"\n             await channel.send(info)\n         return True\n \n     @classmethod\n-    async def from_channel(cls, manager: \"ThreadManager\", channel: discord.TextChannel) -> \"Thread\":\n+    async def from_channel(\n+        cls, manager: \"ThreadManager\", channel: discord.TextChannel\n+    ) -> \"Thread\":\n         # there is a chance it grabs from another recipient's main thread\n         _, recipient_id, other_ids = parse_channel_topic(channel.topic)\n \n@@ -353,7 +392,11 @@ class Thread:\n         if self._genesis_message is None:\n             async for m in self.channel.history(limit=5, oldest_first=True):\n                 if m.author == self.bot.user:\n-                    if m.embeds and m.embeds[0].fields and m.embeds[0].fields[0].name == \"Roles\":\n+                    if (\n+                        m.embeds\n+                        and m.embeds[0].fields\n+                        and m.embeds[0].fields[0].name == \"Roles\"\n+                    ):\n                         self._genesis_message = m\n \n         return self._genesis_message\n@@ -364,7 +407,11 @@ class Thread:\n         recipient = self.recipient\n \n         # in case it creates a channel outside of category\n-        overwrites = {self.bot.modmail_guild.default_role: discord.PermissionOverwrite(read_messages=False)}\n+        overwrites = {\n+            self.bot.modmail_guild.default_role: discord.PermissionOverwrite(\n+                read_messages=False\n+            )\n+        }\n \n         category = category or self.bot.main_category\n \n@@ -372,7 +419,9 @@ class Thread:\n             overwrites = {}\n \n         try:\n-            channel = await create_thread_channel(self.bot, recipient, category, overwrites)\n+            channel = await create_thread_channel(\n+                self.bot, recipient, category, overwrites\n+            )\n         except discord.HTTPException as e:  # Failed to create due to missing perms.\n             logger.critical(\"An error occurred while creating a thread.\", exc_info=True)\n             self.manager.cache.pop(self.id)\n@@ -396,7 +445,9 @@ class Thread:\n \n             log_count = sum(1 for log in log_data if not log[\"open\"])\n         except Exception:\n-            logger.error(\"An error occurred while posting logs to the database.\", exc_info=True)\n+            logger.error(\n+                \"An error occurred while posting logs to the database.\", exc_info=True\n+            )\n             log_url = log_count = None\n             # ensure core functionality still works\n \n@@ -408,7 +459,9 @@ class Thread:\n             mention = self.bot.config[\"mention\"]\n \n         async def send_genesis_message():\n-            info_embed = self._format_info_embed(recipient, log_url, log_count, self.bot.main_color)\n+            info_embed = self._format_info_embed(\n+                recipient, log_url, log_count, self.bot.main_color\n+            )\n             try:\n                 msg = await channel.send(mention, embed=info_embed)\n                 self.bot.loop.create_task(msg.pin())\n@@ -434,7 +487,10 @@ class Thread:\n                 footer = self.bot.config[\"thread_creation_footer\"]\n \n             embed.set_footer(\n-                text=footer, icon_url=self.bot.get_guild_icon(guild=self.bot.modmail_guild, size=128)\n+                text=footer,\n+                icon_url=self.bot.get_guild_icon(\n+                    guild=self.bot.modmail_guild, size=128\n+                ),\n             )\n             embed.title = self.bot.config[\"thread_creation_title\"]\n \n@@ -475,8 +531,12 @@ class Thread:\n                     \"content\": note[\"message\"],\n                     \"author\": Author(),\n                 }\n-                message = discord.Message(state=State(), channel=self.channel, data=data)\n-                ids[note[\"_id\"]] = str((await self.note(message, persistent=True, thread_creation=True)).id)\n+                message = discord.Message(\n+                    state=State(), channel=self.channel, data=data\n+                )\n+                ids[note[\"_id\"]] = str(\n+                    (await self.note(message, persistent=True, thread_creation=True)).id\n+                )\n \n             await self.bot.api.update_note_ids(ids)\n \n@@ -541,7 +601,9 @@ class Thread:\n             footer = f\"User ID: {user.id}\"\n \n         if member is not None:\n-            embed.set_author(name=str(user), icon_url=member.display_avatar.url, url=log_url)\n+            embed.set_author(\n+                name=str(user), icon_url=member.display_avatar.url, url=log_url\n+            )\n \n             if self.bot.config[\"thread_show_join_age\"]:\n                 joined = discord.utils.format_dt(member.joined_at, \"R\")\n@@ -553,7 +615,9 @@ class Thread:\n                 embed.add_field(name=\"Roles\", value=role_names, inline=True)\n             embed.set_footer(text=footer)\n         else:\n-            embed.set_author(name=str(user), icon_url=user.display_avatar.url, url=log_url)\n+            embed.set_author(\n+                name=str(user), icon_url=user.display_avatar.url, url=log_url\n+            )\n             embed.set_footer(text=f\"{footer} \u2022 (not in main server)\")\n \n         embed.description += \", \".join(user_info)\n@@ -567,13 +631,17 @@ class Thread:\n \n         mutual_guilds = [g for g in self.bot.guilds if user in g.members]\n         if member is None or len(mutual_guilds) > 1:\n-            embed.add_field(name=\"Mutual Server(s)\", value=\", \".join(g.name for g in mutual_guilds))\n+            embed.add_field(\n+                name=\"Mutual Server(s)\", value=\", \".join(g.name for g in mutual_guilds)\n+            )\n \n         return embed\n \n     async def _close_after(self, after, closer, silent, delete_channel, message):\n         await asyncio.sleep(after)\n-        return self.bot.loop.create_task(self._close(closer, silent, delete_channel, message, True))\n+        return self.bot.loop.create_task(\n+            self._close(closer, silent, delete_channel, message, True)\n+        )\n \n     async def close(\n         self,\n@@ -606,7 +674,9 @@ class Thread:\n             self.bot.config[\"closures\"][str(self.id)] = items\n             await self.bot.config.update()\n \n-            task = asyncio.create_task(self._close_after(after, closer, silent, delete_channel, message))\n+            task = asyncio.create_task(\n+                self._close_after(after, closer, silent, delete_channel, message)\n+            )\n \n             if auto_close:\n                 self.auto_close_task = task\n@@ -615,7 +685,9 @@ class Thread:\n         else:\n             await self._close(closer, silent, delete_channel, message)\n \n-    async def _close(self, closer, silent=False, delete_channel=True, message=None, scheduled=False):\n+    async def _close(\n+        self, closer, silent=False, delete_channel=True, message=None, scheduled=False\n+    ):\n         if self.channel:\n             self.manager.closing.add(self.channel.id)\n         try:\n@@ -657,9 +729,7 @@ class Thread:\n             prefix = self.bot.config[\"log_url_prefix\"].strip(\"/\")\n             if prefix == \"NONE\":\n                 prefix = \"\"\n-            log_url = (\n-                f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{log_data['key']}\"\n-            )\n+            log_url = f\"{self.bot.config['log_url'].strip('/')}{'/' + prefix if prefix else ''}/{log_data['key']}\"\n \n             if log_data[\"title\"]:\n                 sneak_peak = log_data[\"title\"]\n@@ -696,7 +766,9 @@ class Thread:\n \n         event = \"Thread Closed as Scheduled\" if scheduled else \"Thread Closed\"\n         # embed.set_author(name=f\"Event: {event}\", url=log_url)\n-        embed.set_footer(text=f\"{event} by {_closer}\", icon_url=closer.display_avatar.url)\n+        embed.set_footer(\n+            text=f\"{event} by {_closer}\", icon_url=closer.display_avatar.url\n+        )\n         embed.timestamp = discord.utils.utcnow()\n \n         tasks = [self.bot.config.update()]\n@@ -704,7 +776,11 @@ class Thread:\n         if self.bot.log_channel is not None and self.channel is not None:\n             if self.bot.config[\"show_log_url_button\"]:\n                 view = discord.ui.View()\n-                view.add_item(discord.ui.Button(label=\"Log link\", url=log_url, style=discord.ButtonStyle.url))\n+                view.add_item(\n+                    discord.ui.Button(\n+                        label=\"Log link\", url=log_url, style=discord.ButtonStyle.url\n+                    )\n+                )\n             else:\n                 view = None\n             tasks.append(self.bot.log_channel.send(embed=embed, view=view))\n@@ -725,12 +801,18 @@ class Thread:\n                 message = self.bot.config[\"thread_close_response\"]\n \n         message = self.bot.formatter.format(\n-            message, closer=closer, loglink=log_url, logkey=log_data[\"key\"] if log_data else None\n+            message,\n+            closer=closer,\n+            loglink=log_url,\n+            logkey=log_data[\"key\"] if log_data else None,\n         )\n \n         embed.description = message\n         footer = self.bot.config[\"thread_close_footer\"]\n-        embed.set_footer(text=footer, icon_url=self.bot.get_guild_icon(guild=self.bot.guild, size=128))\n+        embed.set_footer(\n+            text=footer,\n+            icon_url=self.bot.get_guild_icon(guild=self.bot.guild, size=128),\n+        )\n \n         if not silent:\n             for user in self.recipients:\n@@ -753,7 +835,9 @@ class Thread:\n             if self.channel:\n                 self.manager.closing.discard(self.channel.id)\n \n-        self.bot.dispatch(\"thread_close\", self, closer, silent, delete_channel, message, scheduled)\n+        self.bot.dispatch(\n+            \"thread_close\", self, closer, silent, delete_channel, message, scheduled\n+        )\n \n     async def cancel_closure(self, auto_close: bool = False, all: bool = False) -> None:\n         if self.close_task is not None and (not auto_close or all):\n@@ -785,7 +869,9 @@ class Thread:\n         human_time = discord.utils.format_dt(reset_time)\n \n         if self.bot.config.get(\"thread_auto_close_silently\"):\n-            return await self.close(closer=self.bot.user, silent=True, after=int(seconds), auto_close=True)\n+            return await self.close(\n+                closer=self.bot.user, silent=True, after=int(seconds), auto_close=True\n+            )\n \n         # Grab message\n         close_message = self.bot.formatter.format(\n@@ -801,7 +887,12 @@ class Thread:\n                 time_marker_regex,\n             )\n \n-        await self.close(closer=self.bot.user, after=int(seconds), message=close_message, auto_close=True)\n+        await self.close(\n+            closer=self.bot.user,\n+            after=int(seconds),\n+            message=close_message,\n+            auto_close=True,\n+        )\n \n     async def find_linked_messages(\n         self,\n@@ -811,7 +902,11 @@ class Thread:\n         note: bool = True,\n     ) -> typing.Tuple[discord.Message, typing.List[typing.Optional[discord.Message]]]:\n         if message1 is not None:\n-            if not message1.embeds or not message1.embeds[0].author.url or message1.author != self.bot.user:\n+            if (\n+                not message1.embeds\n+                or not message1.embeds[0].author.url\n+                or message1.author != self.bot.user\n+            ):\n                 raise ValueError(\"Malformed thread message.\")\n \n         elif message_id is not None:\n@@ -828,13 +923,17 @@ class Thread:\n             ):\n                 raise ValueError(\"Thread message not found.\")\n \n-            if message1.embeds[0].footer and \"Internal Message\" in message1.embeds[0].footer.text:\n+            if (\n+                message1.embeds[0].footer\n+                and \"Internal Message\" in message1.embeds[0].footer.text\n+            ):\n                 if not note:\n                     raise ValueError(\"Thread message not found.\")\n                 return message1, None\n \n             if message1.embeds[0].color.value != self.bot.mod_color and not (\n-                either_direction and message1.embeds[0].color.value == self.bot.recipient_color\n+                either_direction\n+                and message1.embeds[0].color.value == self.bot.recipient_color\n             ):\n                 raise ValueError(\"Thread message not found.\")\n         else:\n@@ -845,7 +944,11 @@ class Thread:\n                     and message1.embeds[0].color\n                     and (\n                         message1.embeds[0].color.value == self.bot.mod_color\n-                        or (either_direction and message1.embeds[0].color.value == self.bot.recipient_color)\n+                        or (\n+                            either_direction\n+                            and message1.embeds[0].color.value\n+                            == self.bot.recipient_color\n+                        )\n                     )\n                     and message1.embeds[0].author.url.split(\"#\")[-1].isdigit()\n                     and message1.author == self.bot.user\n@@ -880,7 +983,9 @@ class Thread:\n \n         raise ValueError(\"DM message not found.\")\n \n-    async def edit_message(self, message_id: typing.Optional[int], message: str) -> None:\n+    async def edit_message(\n+        self, message_id: typing.Optional[int], message: str\n+    ) -> None:\n         try:\n             message1, *message2 = await self.find_linked_messages(message_id)\n         except ValueError:\n@@ -890,8 +995,14 @@ class Thread:\n         embed1 = message1.embeds[0]\n         embed1.description = message\n \n-        tasks = [self.bot.api.edit_message(message1.id, message), message1.edit(embed=embed1)]\n-        if message1.embeds[0].footer and \"Persistent Internal Message\" in message1.embeds[0].footer.text:\n+        tasks = [\n+            self.bot.api.edit_message(message1.id, message),\n+            message1.edit(embed=embed1),\n+        ]\n+        if (\n+            message1.embeds[0].footer\n+            and \"Persistent Internal Message\" in message1.embeds[0].footer.text\n+        ):\n             tasks += [self.bot.api.edit_note(message1.id, message)]\n         else:\n             for m2 in message2:\n@@ -906,7 +1017,9 @@ class Thread:\n         self, message: typing.Union[int, discord.Message] = None, note: bool = True\n     ) -> None:\n         if isinstance(message, discord.Message):\n-            message1, *message2 = await self.find_linked_messages(message1=message, note=note)\n+            message1, *message2 = await self.find_linked_messages(\n+                message1=message, note=note\n+            )\n         else:\n             message1, *message2 = await self.find_linked_messages(message, note=note)\n         tasks = []\n@@ -918,7 +1031,10 @@ class Thread:\n             if m2 is not None:\n                 tasks += [m2.delete()]\n \n-        if message1.embeds[0].footer and \"Persistent Internal Message\" in message1.embeds[0].footer.text:\n+        if (\n+            message1.embeds[0].footer\n+            and \"Persistent Internal Message\" in message1.embeds[0].footer.text\n+        ):\n             tasks += [self.bot.api.delete_note(message1.id)]\n \n         if tasks:\n@@ -998,9 +1114,13 @@ class Thread:\n             embed = msg.embeds[0]\n             if isinstance(msg.channel, discord.TextChannel):\n                 # just for thread channel, we put the old message in embed field\n-                embed.add_field(name=\"**Edited, former message:**\", value=embed.description)\n+                embed.add_field(\n+                    name=\"**Edited, former message:**\", value=embed.description\n+                )\n             embed.description = content\n-            await asyncio.gather(self.bot.api.edit_message(message.id, content), msg.edit(embed=embed))\n+            await asyncio.gather(\n+                self.bot.api.edit_message(message.id, content), msg.edit(embed=embed)\n+            )\n \n     async def note(\n         self, message: discord.Message, persistent=False, thread_creation=False\n@@ -1018,7 +1138,9 @@ class Thread:\n \n         # Log as 'note' type for logviewer\n         self.bot.loop.create_task(\n-            self.bot.api.append_log(message, message_id=msg.id, channel_id=self.channel.id, type_=\"note\")\n+            self.bot.api.append_log(\n+                message, message_id=msg.id, channel_id=self.channel.id, type_=\"note\"\n+            )\n         )\n \n         return msg\n@@ -1085,7 +1207,11 @@ class Thread:\n         else:\n             # Send the same thing in the thread channel.\n             msg = await self.send(\n-                message, destination=self.channel, from_mod=True, anonymous=anonymous, plain=plain\n+                message,\n+                destination=self.channel,\n+                from_mod=True,\n+                anonymous=anonymous,\n+                plain=plain,\n             )\n \n             tasks.append(\n@@ -1133,7 +1259,8 @@ class Thread:\n \n             # Create embed for note with Discord system message style\n             embed = discord.Embed(\n-                description=content, color=0x5865F2  # Discord blurple color for system messages\n+                description=content,\n+                color=0x5865F2,  # Discord blurple color for system messages\n             )\n \n             # Set author with note icon and username\n@@ -1143,7 +1270,8 @@ class Thread:\n                 note_type = \"Note\"\n \n             embed.set_author(\n-                name=f\"\ud83d\udcdd {note_type} ({message.author.name})\", icon_url=message.author.display_avatar.url\n+                name=f\"\ud83d\udcdd {note_type} ({message.author.name})\",\n+                icon_url=message.author.display_avatar.url,\n             )\n \n             # Add timestamp if enabled\n@@ -1159,7 +1287,9 @@ class Thread:\n             return await destination.send(embed=embed)\n \n         if not note and from_mod:\n-            self.bot.loop.create_task(self._restart_close_timer())  # Start or restart thread auto close\n+            self.bot.loop.create_task(\n+                self._restart_close_timer()\n+            )  # Start or restart thread auto close\n \n         if self.close_task is not None:\n             # cancel closing if a thread message is sent.\n@@ -1177,12 +1307,16 @@ class Thread:\n             await self.wait_until_ready()\n \n         if not from_mod and not note:\n-            self.bot.loop.create_task(self.bot.api.append_log(message, channel_id=self.channel.id))\n+            self.bot.loop.create_task(\n+                self.bot.api.append_log(message, channel_id=self.channel.id)\n+            )\n \n         destination = destination or self.channel\n \n         if destination is None:\n-            logger.error(\"Attempted to send a message to a thread with no channel (destination is None).\")\n+            logger.error(\n+                \"Attempted to send a message to a thread with no channel (destination is None).\"\n+            )\n             return\n         try:\n             await destination.typing()\n@@ -1202,7 +1336,12 @@ class Thread:\n         if hasattr(message, \"message_snapshots\") and len(message.message_snapshots) > 0:\n             snap = message.message_snapshots[0]\n             # Only show \"No content\" if there's truly no content (no text, attachments, embeds, or stickers)\n-            if not snap.content and not message.attachments and not message.embeds and not message.stickers:\n+            if (\n+                not snap.content\n+                and not message.attachments\n+                and not message.embeds\n+                and not message.stickers\n+            ):\n                 content = \"No content\"\n             else:\n                 content = snap.content or \"\"\n@@ -1217,13 +1356,21 @@ class Thread:\n                     and message.reference.type == discord.MessageReferenceType.forward\n                 ):\n                     try:\n-                        original_msg_channel = self.bot.get_channel(message.reference.channel_id)\n-                        original_msg = await original_msg_channel.fetch_message(message.reference.message_id)\n+                        original_msg_channel = self.bot.get_channel(\n+                            message.reference.channel_id\n+                        )\n+                        original_msg = await original_msg_channel.fetch_message(\n+                            message.reference.message_id\n+                        )\n                         forwarded_jump_url = original_msg.jump_url\n                     except (discord.NotFound, discord.Forbidden, AttributeError):\n                         pass\n \n-            content = f\"\ud83d\udce8 **Forwarded message:**\\n{content}\" if content else \"\ud83d\udce8 **Forwarded message:**\"\n+            content = (\n+                f\"\ud83d\udce8 **Forwarded message:**\\n{content}\"\n+                if content\n+                else \"\ud83d\udce8 **Forwarded message:**\"\n+            )\n         else:\n             # Only show \"No content\" if there's truly no content (no text, attachments, embeds, or stickers)\n             if (\n@@ -1246,16 +1393,26 @@ class Thread:\n \n         # Add forwarded message context\n         if forwarded_jump_url:\n-            embed.add_field(name=\"Context\", value=f\"- {forwarded_jump_url}\", inline=True)\n+            embed.add_field(\n+                name=\"Context\", value=f\"- {forwarded_jump_url}\", inline=True\n+            )\n \n-        system_avatar_url = \"https://discordapp.com/assets/f78426a064bc9dd24847519259bc42af.png\"\n+        system_avatar_url = (\n+            \"https://discordapp.com/assets/f78426a064bc9dd24847519259bc42af.png\"\n+        )\n \n         if not note:\n-            if anonymous and from_mod and not isinstance(destination, discord.TextChannel):\n+            if (\n+                anonymous\n+                and from_mod\n+                and not isinstance(destination, discord.TextChannel)\n+            ):\n                 # Anonymously sending to the user.\n                 tag = self.bot.config[\"mod_tag\"]\n                 if tag is None:\n-                    tag = str(get_top_role(author, self.bot.config[\"use_hoisted_top_role\"]))\n+                    tag = str(\n+                        get_top_role(author, self.bot.config[\"use_hoisted_top_role\"])\n+                    )\n                 name = self.bot.config[\"anon_username\"]\n                 if name is None:\n                     name = tag\n@@ -1332,7 +1489,11 @@ class Thread:\n                 discord.StickerFormatType.gif,\n             ):\n                 images.append(\n-                    (f\"https://media.discordapp.net/stickers/{i.id}.{i.format.file_extension}\", i.name, True)\n+                    (\n+                        f\"https://media.discordapp.net/stickers/{i.id}.{i.format.file_extension}\",\n+                        i.name,\n+                        True,\n+                    )\n                 )\n             elif i.format == discord.StickerFormatType.lottie:\n                 # save the json lottie representation\n@@ -1372,14 +1533,17 @@ class Thread:\n \n         for url, filename, is_sticker in images:\n             if (\n-                not prioritize_uploads or ((url is None or is_image_url(url)) and filename)\n+                not prioritize_uploads\n+                or ((url is None or is_image_url(url)) and filename)\n             ) and not embedded_image:\n                 if url is not None:\n                     embed.set_image(url=url)\n                 if filename:\n                     if is_sticker:\n                         if url is None:\n-                            description = f\"{filename}: Unable to retrieve sticker image\"\n+                            description = (\n+                                f\"{filename}: Unable to retrieve sticker image\"\n+                            )\n                         else:\n                             description = f\"[{filename}]({url})\"\n                         embed.add_field(name=\"Sticker\", value=description)\n@@ -1401,7 +1565,9 @@ class Thread:\n                     img_embed.url = url\n                 if filename is not None:\n                     img_embed.title = filename\n-                img_embed.set_footer(text=f\"Additional Image Upload ({additional_count})\")\n+                img_embed.set_footer(\n+                    text=f\"Additional Image Upload ({additional_count})\"\n+                )\n                 img_embed.timestamp = message.created_at\n                 additional_images.append(destination.send(embed=img_embed))\n                 additional_count += 1\n@@ -1409,7 +1575,9 @@ class Thread:\n         file_upload_count = 1\n \n         for url, filename, _ in attachments:\n-            embed.add_field(name=f\"File upload ({file_upload_count})\", value=f\"[{filename}]({url})\")\n+            embed.add_field(\n+                name=f\"File upload ({file_upload_count})\", value=f\"[{filename}]({url})\"\n+            )\n             file_upload_count += 1\n \n         if from_mod:\n@@ -1430,14 +1598,21 @@ class Thread:\n                 elif not anonymous:\n                     mod_tag = self.bot.config[\"mod_tag\"]\n                     if mod_tag is None:\n-                        mod_tag = str(get_top_role(message.author, self.bot.config[\"use_hoisted_top_role\"]))\n+                        mod_tag = str(\n+                            get_top_role(\n+                                message.author, self.bot.config[\"use_hoisted_top_role\"]\n+                            )\n+                        )\n                     embed.set_footer(text=mod_tag)  # Normal messages\n                 else:\n                     embed.set_footer(text=self.bot.config[\"anon_tag\"])\n         else:\n             # Add forwarded message indicator in footer for mods\n             footer_text = f\"Message ID: {message.id}\"\n-            if hasattr(message, \"message_snapshots\") and len(message.message_snapshots) > 0:\n+            if (\n+                hasattr(message, \"message_snapshots\")\n+                and len(message.message_snapshots) > 0\n+            ):\n                 footer_text += \" \u2022 Forwarded\"\n             embed.set_footer(text=footer_text)\n             embed.colour = self.bot.recipient_color\n@@ -1455,7 +1630,9 @@ class Thread:\n             and self.bot.config[\"dm_disabled\"] == DMDisabled.ALL_THREADS\n             and destination != self.channel\n         ):\n-            logger.info(\"Sending a message to %s when DM disabled is set.\", self.recipient)\n+            logger.info(\n+                \"Sending a message to %s when DM disabled is set.\", self.recipient\n+            )\n \n         # Best-effort typing: never block message delivery if typing fails\n         try:\n@@ -1464,7 +1641,11 @@ class Thread:\n             logger.warning(\"Channel not found.\")\n             raise\n         except (discord.Forbidden, discord.HTTPException, Exception) as e:\n-            logger.warning(\"Unable to send typing to %s: %s. Continuing without typing.\", destination, e)\n+            logger.warning(\n+                \"Unable to send typing to %s: %s. Continuing without typing.\",\n+                destination,\n+                e,\n+            )\n \n         if not from_mod and not note:\n             mentions = await self.get_notifications()\n@@ -1543,13 +1724,17 @@ class Thread:\n             embed.add_field(name=\"Other Recipients\", value=value, inline=False)\n         else:\n             if value:\n-                embed.set_field_at(index, name=\"Other Recipients\", value=value, inline=False)\n+                embed.set_field_at(\n+                    index, name=\"Other Recipients\", value=value, inline=False\n+                )\n             else:\n                 embed.remove_field(index)\n \n         await genesis_message.edit(embed=embed)\n \n-    async def add_users(self, users: typing.List[typing.Union[discord.Member, discord.User]]) -> None:\n+    async def add_users(\n+        self, users: typing.List[typing.Union[discord.Member, discord.User]]\n+    ) -> None:\n         topic = \"\"\n         title, _, _ = parse_channel_topic(self.channel.topic)\n         if title is not None:\n@@ -1567,7 +1752,9 @@ class Thread:\n         await self.channel.edit(topic=topic)\n         await self._update_users_genesis()\n \n-    async def remove_users(self, users: typing.List[typing.Union[discord.Member, discord.User]]) -> None:\n+    async def remove_users(\n+        self, users: typing.List[typing.Union[discord.Member, discord.User]]\n+    ) -> None:\n         topic = \"\"\n         title, user_id, _ = parse_channel_topic(self.channel.topic)\n         if title is not None:\n@@ -1615,13 +1802,18 @@ class ThreadManager:\n         recipient_id: int = None,\n     ) -> typing.Optional[Thread]:\n         \"\"\"Finds a thread from cache or from discord channel topics.\"\"\"\n-        if recipient is None and channel is not None and isinstance(channel, discord.TextChannel):\n+        if (\n+            recipient is None\n+            and channel is not None\n+            and isinstance(channel, discord.TextChannel)\n+        ):\n             if channel.id in self.closing:\n                 return None\n             thread = await self._find_from_channel(channel)\n             if thread is None:\n                 user_id, thread = next(\n-                    ((k, v) for k, v in self.cache.items() if v.channel == channel), (-1, None)\n+                    ((k, v) for k, v in self.cache.items() if v.channel == channel),\n+                    (-1, None),\n                 )\n                 if thread is not None:\n                     logger.debug(\"Found thread with tempered ID.\")\n@@ -1704,7 +1896,9 @@ class ThreadManager:\n         if recipient is None:\n             thread = Thread(self, user_id, channel, other_recipients)\n         else:\n-            self.cache[user_id] = thread = Thread(self, recipient, channel, other_recipients)\n+            self.cache[user_id] = thread = Thread(\n+                self, recipient, channel, other_recipients\n+            )\n         thread.ready = True\n \n         return thread\n@@ -1732,9 +1926,9 @@ class ThreadManager:\n             if len(message.content.strip()) < min_chars:\n                 embed = discord.Embed(\n                     title=self.bot.config[\"thread_min_characters_title\"],\n-                    description=self.bot.config[\"thread_min_characters_response\"].replace(\n-                        \"{min_characters}\", str(min_chars)\n-                    ),\n+                    description=self.bot.config[\n+                        \"thread_min_characters_response\"\n+                    ].replace(\"{min_characters}\", str(min_chars)),\n                     color=self.bot.error_color,\n                 )\n                 embed.set_footer(\n@@ -1757,27 +1951,44 @@ class ThreadManager:\n                 return thread\n             else:\n                 if thread.channel and self.bot.get_channel(thread.channel.id):\n-                    logger.warning(\"Found an existing thread for %s, abort creating.\", recipient)\n+                    logger.warning(\n+                        \"Found an existing thread for %s, abort creating.\", recipient\n+                    )\n                     return thread\n-                logger.warning(\"Found an existing thread for %s, closing previous thread.\", recipient)\n+                logger.warning(\n+                    \"Found an existing thread for %s, closing previous thread.\",\n+                    recipient,\n+                )\n                 self.bot.loop.create_task(\n-                    thread.close(closer=self.bot.user, silent=True, delete_channel=False)\n+                    thread.close(\n+                        closer=self.bot.user, silent=True, delete_channel=False\n+                    )\n                 )\n \n         thread = Thread(self, recipient)\n \n         self.cache[recipient.id] = thread\n \n-        if (message or not manual_trigger) and self.bot.config[\"confirm_thread_creation\"]:\n+        if (message or not manual_trigger) and self.bot.config[\n+            \"confirm_thread_creation\"\n+        ]:\n             if not manual_trigger:\n                 destination = recipient\n             else:\n                 destination = message.channel\n             view = ConfirmThreadCreationView()\n             view.add_item(\n-                AcceptButton(\"accept-thread-creation\", self.bot.config[\"confirm_thread_creation_accept\"])\n+                AcceptButton(\n+                    \"accept-thread-creation\",\n+                    self.bot.config[\"confirm_thread_creation_accept\"],\n+                )\n+            )\n+            view.add_item(\n+                DenyButton(\n+                    \"deny-thread-creation\",\n+                    self.bot.config[\"confirm_thread_creation_deny\"],\n+                )\n             )\n-            view.add_item(DenyButton(\"deny-thread-creation\", self.bot.config[\"confirm_thread_creation_deny\"]))\n             confirm = await destination.send(\n                 embed=discord.Embed(\n                     title=self.bot.config[\"confirm_thread_creation_title\"],\n@@ -1804,7 +2015,8 @@ class ThreadManager:\n                 self.bot.loop.create_task(\n                     destination.send(\n                         embed=discord.Embed(\n-                            title=self.bot.config[\"thread_cancelled\"], color=self.bot.error_color\n+                            title=self.bot.config[\"thread_cancelled\"],\n+                            color=self.bot.error_color,\n                         )\n                     )\n                 )\n@@ -1812,7 +2024,9 @@ class ThreadManager:\n                 del self.cache[recipient.id]\n                 return thread\n \n-        self.bot.loop.create_task(thread.setup(creator=creator, category=category, initial_message=message))\n+        self.bot.loop.create_task(\n+            thread.setup(creator=creator, category=category, initial_message=message)\n+        )\n         return thread\n \n     async def find_or_create(self, recipient) -> Thread:\ndiff --git a/core/time.py b/core/time.py\nindex 71f4ca3..23cbda0 100644\n--- a/core/time.py\n+++ b/core/time.py\n@@ -64,7 +64,9 @@ class ShortTime:\n         if match is None or not match.group(0):\n             match = self.discord_fmt.fullmatch(argument)\n             if match is not None:\n-                self.dt = datetime.datetime.utcfromtimestamp(int(match.group(\"ts\")), tz=datetime.timezone.utc)\n+                self.dt = datetime.datetime.utcfromtimestamp(\n+                    int(match.group(\"ts\")), tz=datetime.timezone.utc\n+                )\n                 return\n             else:\n                 raise commands.BadArgument(\"invalid time provided\")\n@@ -85,11 +87,18 @@ class HumanTime:\n         now = now or datetime.datetime.utcnow()\n         dt, status = self.calendar.parseDT(argument, sourceTime=now)\n         if not status.hasDateOrTime:\n-            raise commands.BadArgument('invalid time provided, try e.g. \"tomorrow\" or \"3 days\"')\n+            raise commands.BadArgument(\n+                'invalid time provided, try e.g. \"tomorrow\" or \"3 days\"'\n+            )\n \n         if not status.hasTime:\n             # replace it with the current time\n-            dt = dt.replace(hour=now.hour, minute=now.minute, second=now.second, microsecond=now.microsecond)\n+            dt = dt.replace(\n+                hour=now.hour,\n+                minute=now.minute,\n+                second=now.second,\n+                microsecond=now.microsecond,\n+            )\n \n         self.dt: datetime.datetime = dt\n         self._past: bool = dt < now\n@@ -158,7 +167,11 @@ class FriendlyTimeResult:\n         self.arg = \"\"\n \n     async def ensure_constraints(\n-        self, ctx: Context, uft: UserFriendlyTime, now: datetime.datetime, remaining: str\n+        self,\n+        ctx: Context,\n+        uft: UserFriendlyTime,\n+        now: datetime.datetime,\n+        remaining: str,\n     ) -> None:\n         if self.dt < now:\n             raise commands.BadArgument(\"This time is in the past.\")\n@@ -193,7 +206,9 @@ class UserFriendlyTime(commands.Converter):\n         self.converter: commands.Converter = converter  # type: ignore  # It doesn't understand this narrowing\n         self.default: Any = default\n \n-    async def convert(self, ctx: Context, argument: str, *, now=None) -> FriendlyTimeResult:\n+    async def convert(\n+        self, ctx: Context, argument: str, *, now=None\n+    ) -> FriendlyTimeResult:\n         calendar = HumanTime.calendar\n         regex = ShortTime.compiled\n         if now is None:\n@@ -211,7 +226,9 @@ class UserFriendlyTime(commands.Converter):\n             match = ShortTime.discord_fmt.match(argument)\n             if match is not None:\n                 result = FriendlyTimeResult(\n-                    datetime.datetime.utcfromtimestamp(int(match.group(\"ts\")), now, tz=datetime.timezone.utc)\n+                    datetime.datetime.utcfromtimestamp(\n+                        int(match.group(\"ts\")), now, tz=datetime.timezone.utc\n+                    )\n                 )\n                 remaining = argument[match.end() :].strip()\n                 await result.ensure_constraints(ctx, self, now, remaining)\n@@ -243,7 +260,9 @@ class UserFriendlyTime(commands.Converter):\n         dt, status, begin, end, dt_string = elements[0]\n \n         if not status.hasDateOrTime:\n-            raise commands.BadArgument('Invalid time provided, try e.g. \"tomorrow\" or \"3 days\".')\n+            raise commands.BadArgument(\n+                'Invalid time provided, try e.g. \"tomorrow\" or \"3 days\".'\n+            )\n \n         if begin not in (0, 1) and end != len(argument):\n             raise commands.BadArgument(\n@@ -254,7 +273,12 @@ class UserFriendlyTime(commands.Converter):\n \n         if not status.hasTime:\n             # replace it with the current time\n-            dt = dt.replace(hour=now.hour, minute=now.minute, second=now.second, microsecond=now.microsecond)\n+            dt = dt.replace(\n+                hour=now.hour,\n+                minute=now.minute,\n+                second=now.second,\n+                microsecond=now.microsecond,\n+            )\n \n         # if midnight is provided, just default to next day\n         if status.accuracy == pdt.pdtContext.ACU_HALFDAY:\n@@ -270,7 +294,9 @@ class UserFriendlyTime(commands.Converter):\n                     raise commands.BadArgument(\"Expected quote before time input...\")\n \n                 if not (end < len(argument) and argument[end] == '\"'):\n-                    raise commands.BadArgument(\"If the time is quoted, you must unquote it.\")\n+                    raise commands.BadArgument(\n+                        \"If the time is quoted, you must unquote it.\"\n+                    )\n \n                 remaining = argument[end + 1 :].lstrip(\" ,.!\")\n             else:\ndiff --git a/core/utils.py b/core/utils.py\nindex f249cb9..33fab56 100644\n--- a/core/utils.py\n+++ b/core/utils.py\n@@ -262,7 +262,9 @@ TOPIC_REGEX = re.compile(\n UID_REGEX = re.compile(r\"\\bUser ID:\\s*(\\d{17,21})\\b\", flags=re.IGNORECASE)\n \n \n-def parse_channel_topic(text: str) -> typing.Tuple[typing.Optional[str], int, typing.List[int]]:\n+def parse_channel_topic(\n+    text: str,\n+) -> typing.Tuple[typing.Optional[str], int, typing.List[int]]:\n     \"\"\"\n     A helper to parse channel topics and respectivefully returns all the required values\n     at once.\n@@ -363,7 +365,8 @@ def match_other_recipients(text: str) -> typing.List[int]:\n def create_not_found_embed(word, possibilities, name, n=2, cutoff=0.6) -> discord.Embed:\n     # Single reference of Color.red()\n     embed = discord.Embed(\n-        color=discord.Color.red(), description=f\"**{name.capitalize()} `{word}` cannot be found.**\"\n+        color=discord.Color.red(),\n+        description=f\"**{name.capitalize()} `{word}` cannot be found.**\",\n     )\n     val = get_close_matches(word, possibilities, n=n, cutoff=cutoff)\n     if val:\n@@ -487,7 +490,9 @@ def get_top_role(member: discord.Member, hoisted=True):\n             return role\n \n \n-async def create_thread_channel(bot, recipient, category, overwrites, *, name=None, errors_raised=None):\n+async def create_thread_channel(\n+    bot, recipient, category, overwrites, *, name=None, errors_raised=None\n+):\n     name = name or bot.format_channel_name(recipient)\n     errors_raised = errors_raised or []\n \n@@ -510,7 +515,9 @@ async def create_thread_channel(bot, recipient, category, overwrites, *, name=No\n             fallback = None\n             fallback_id = bot.config[\"fallback_category_id\"]\n             if fallback_id:\n-                fallback = discord.utils.get(category.guild.categories, id=int(fallback_id))\n+                fallback = discord.utils.get(\n+                    category.guild.categories, id=int(fallback_id)\n+                )\n                 if fallback and len(fallback.channels) >= 49:\n                     fallback = None\n \n@@ -581,7 +588,10 @@ def extract_block_timestamp(reason, id_):\n             # found a deprecated version\n             try:\n                 after = (\n-                    datetime.fromisoformat(end_time.group(1)).replace(tzinfo=timezone.utc) - now\n+                    datetime.fromisoformat(end_time.group(1)).replace(\n+                        tzinfo=timezone.utc\n+                    )\n+                    - now\n                 ).total_seconds()\n             except ValueError:\n                 logger.warning(\n@@ -596,7 +606,10 @@ def extract_block_timestamp(reason, id_):\n     else:\n         try:\n             after = (\n-                datetime.utcfromtimestamp(int(end_time.group(1))).replace(tzinfo=timezone.utc) - now\n+                datetime.utcfromtimestamp(int(end_time.group(1))).replace(\n+                    tzinfo=timezone.utc\n+                )\n+                - now\n             ).total_seconds()\n         except ValueError:\n             logger.warning(\n@@ -616,7 +629,9 @@ def return_or_truncate(text, max_length):\n \n class AcceptButton(discord.ui.Button):\n     def __init__(self, custom_id: str, emoji: str):\n-        super().__init__(style=discord.ButtonStyle.gray, emoji=emoji, custom_id=custom_id)\n+        super().__init__(\n+            style=discord.ButtonStyle.gray, emoji=emoji, custom_id=custom_id\n+        )\n         self.view: ConfirmThreadCreationView\n \n     async def callback(self, interaction: discord.Interaction):\n@@ -627,7 +642,9 @@ class AcceptButton(discord.ui.Button):\n \n class DenyButton(discord.ui.Button):\n     def __init__(self, custom_id: str, emoji: str):\n-        super().__init__(style=discord.ButtonStyle.gray, emoji=emoji, custom_id=custom_id)\n+        super().__init__(\n+            style=discord.ButtonStyle.gray, emoji=emoji, custom_id=custom_id\n+        )\n         self.view: ConfirmThreadCreationView\n \n     async def callback(self, interaction: discord.Interaction):\n@@ -665,7 +682,9 @@ def extract_forwarded_content(message) -> typing.Optional[str]:\n                 forwarded_parts = []\n                 for snap in message.message_snapshots:\n                     author = getattr(snap, \"author\", None)\n-                    author_name = getattr(author, \"name\", \"Unknown\") if author else \"Unknown\"\n+                    author_name = (\n+                        getattr(author, \"name\", \"Unknown\") if author else \"Unknown\"\n+                    )\n                     snap_content = getattr(snap, \"content\", \"\")\n \n                     if snap_content:\n@@ -679,15 +698,22 @@ def extract_forwarded_content(message) -> typing.Optional[str]:\n                                 embed_desc = embed.description\n                                 if len(embed_desc) > 300:\n                                     embed_desc = embed_desc[:297] + \"...\"\n-                                forwarded_parts.append(f\"**{author_name}:** {embed_desc}\")\n+                                forwarded_parts.append(\n+                                    f\"**{author_name}:** {embed_desc}\"\n+                                )\n                                 break\n                     elif getattr(snap, \"attachments\", None):\n                         attachment_info = \", \".join(\n-                            [getattr(a, \"filename\", \"Unknown\") for a in snap.attachments[:3]]\n+                            [\n+                                getattr(a, \"filename\", \"Unknown\")\n+                                for a in snap.attachments[:3]\n+                            ]\n                         )\n                         if len(snap.attachments) > 3:\n                             attachment_info += f\" (+{len(snap.attachments) - 3} more)\"\n-                        forwarded_parts.append(f\"**{author_name}:** [Attachments: {attachment_info}]\")\n+                        forwarded_parts.append(\n+                            f\"**{author_name}:** [Attachments: {attachment_info}]\"\n+                        )\n                     else:\n                         forwarded_parts.append(f\"**{author_name}:** [No content]\")\n \n@@ -695,18 +721,25 @@ def extract_forwarded_content(message) -> typing.Optional[str]:\n                     return \"\\n\".join(forwarded_parts)\n \n         # Handle single-message forward\n-        elif getattr(message, \"type\", None) == getattr(discord.MessageType, \"forward\", None):\n+        elif getattr(message, \"type\", None) == getattr(\n+            discord.MessageType, \"forward\", None\n+        ):\n             ref = getattr(message, \"reference\", None)\n             if (\n                 ref\n                 and hasattr(discord, \"MessageReferenceType\")\n-                and getattr(ref, \"type\", None) == getattr(discord.MessageReferenceType, \"forward\", None)\n+                and getattr(ref, \"type\", None)\n+                == getattr(discord.MessageReferenceType, \"forward\", None)\n             ):\n                 try:\n                     ref_msg = getattr(ref, \"resolved\", None)\n                     if ref_msg:\n                         ref_author = getattr(ref_msg, \"author\", None)\n-                        ref_author_name = getattr(ref_author, \"name\", \"Unknown\") if ref_author else \"Unknown\"\n+                        ref_author_name = (\n+                            getattr(ref_author, \"name\", \"Unknown\")\n+                            if ref_author\n+                            else \"Unknown\"\n+                        )\n                         ref_content = getattr(ref_msg, \"content\", \"\")\n \n                         if ref_content:\n@@ -722,10 +755,15 @@ def extract_forwarded_content(message) -> typing.Optional[str]:\n                                     return f\"**{ref_author_name}:** {embed_desc}\"\n                         elif getattr(ref_msg, \"attachments\", None):\n                             attachment_info = \", \".join(\n-                                [getattr(a, \"filename\", \"Unknown\") for a in ref_msg.attachments[:3]]\n+                                [\n+                                    getattr(a, \"filename\", \"Unknown\")\n+                                    for a in ref_msg.attachments[:3]\n+                                ]\n                             )\n                             if len(ref_msg.attachments) > 3:\n-                                attachment_info += f\" (+{len(ref_msg.attachments) - 3} more)\"\n+                                attachment_info += (\n+                                    f\" (+{len(ref_msg.attachments) - 3} more)\"\n+                                )\n                             return f\"**{ref_author_name}:** [Attachments: {attachment_info}]\"\n                 except Exception:\n                     pass\n"
    },
    {
        "id": 292,
        "sha_fail": "ce28f30ca68a2e0469f14beb1b4d65a4633d211e",
        "diff": "diff --git a/lightrag/api/lightrag_server.py b/lightrag/api/lightrag_server.py\nindex 60fae05..0ed20a2 100644\n--- a/lightrag/api/lightrag_server.py\n+++ b/lightrag/api/lightrag_server.py\n@@ -644,13 +644,20 @@ def create_app(args):\n                     from lightrag.llm.jina import jina_embed\n \n                     return await jina_embed(\n-                        texts, embedding_dim=embedding_dim, base_url=host, api_key=api_key\n+                        texts,\n+                        embedding_dim=embedding_dim,\n+                        base_url=host,\n+                        api_key=api_key,\n                     )\n                 else:  # openai and compatible\n                     from lightrag.llm.openai import openai_embed\n \n                     return await openai_embed(\n-                        texts, model=model, base_url=host, api_key=api_key, embedding_dim=embedding_dim\n+                        texts,\n+                        model=model,\n+                        base_url=host,\n+                        api_key=api_key,\n+                        embedding_dim=embedding_dim,\n                     )\n             except ImportError as e:\n                 raise Exception(f\"Failed to import {binding} embedding: {e}\")\n"
    },
    {
        "id": 293,
        "sha_fail": "a0593ec1c9613ae8ab78fddd30e47114ded16f00",
        "diff": "diff --git a/lightrag/kg/postgres_impl.py b/lightrag/kg/postgres_impl.py\nindex 4663aad..30418d8 100644\n--- a/lightrag/kg/postgres_impl.py\n+++ b/lightrag/kg/postgres_impl.py\n@@ -828,8 +828,8 @@ class PostgreSQLDB:\n \n                     # Execute the migration\n                     alter_sql = f\"\"\"\n-                    ALTER TABLE {migration['table']}\n-                    ALTER COLUMN {migration['column']} TYPE {migration['new_type']}\n+                    ALTER TABLE {migration[\"table\"]}\n+                    ALTER COLUMN {migration[\"column\"]} TYPE {migration[\"new_type\"]}\n                     \"\"\"\n \n                     await self.execute(alter_sql)\n@@ -3387,7 +3387,9 @@ class PGGraphStorage(BaseGraphStorage):\n                 logger.error(f\"[{self.workspace}] Error during edge deletion: {str(e)}\")\n                 raise\n \n-    async def get_nodes_batch(self, node_ids: list[str], batch_size: int = 1000) -> dict[str, dict]:\n+    async def get_nodes_batch(\n+        self, node_ids: list[str], batch_size: int = 1000\n+    ) -> dict[str, dict]:\n         \"\"\"\n         Retrieve multiple nodes in one query using UNWIND.\n \n@@ -3453,7 +3455,9 @@ class PGGraphStorage(BaseGraphStorage):\n \n         return nodes_dict\n \n-    async def node_degrees_batch(self, node_ids: list[str], batch_size: int = 500) -> dict[str, int]:\n+    async def node_degrees_batch(\n+        self, node_ids: list[str], batch_size: int = 500\n+    ) -> dict[str, int]:\n         \"\"\"\n         Retrieve the degree for multiple nodes in a single query using UNWIND.\n         Calculates the total degree by counting distinct relationships.\n@@ -3482,7 +3486,7 @@ class PGGraphStorage(BaseGraphStorage):\n         in_degrees = {}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n \n             query = f\"\"\"\n                     WITH input(v, ord) AS (\n@@ -3602,7 +3606,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_dict: dict[tuple[str, str], dict] = {}\n \n         for i in range(0, len(uniq_pairs), batch_size):\n-            batch = uniq_pairs[i:i + batch_size]\n+            batch = uniq_pairs[i : i + batch_size]\n \n             pairs = [{\"src\": p[\"src\"], \"tgt\": p[\"tgt\"]} for p in batch]\n \n@@ -3709,7 +3713,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_norm: dict[str, list[tuple[str, str]]] = {n: [] for n in unique_ids}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n             # Format node IDs for the query\n             formatted_ids = \", \".join([f'\"{n}\"' for n in batch])\n \n"
    },
    {
        "id": 294,
        "sha_fail": "60564cf453626802e147b5b929b8b1c6427fb3d8",
        "diff": "diff --git a/lightrag/kg/postgres_impl.py b/lightrag/kg/postgres_impl.py\nindex 3ab8bfb..bf59009 100644\n--- a/lightrag/kg/postgres_impl.py\n+++ b/lightrag/kg/postgres_impl.py\n@@ -828,8 +828,8 @@ class PostgreSQLDB:\n \n                     # Execute the migration\n                     alter_sql = f\"\"\"\n-                    ALTER TABLE {migration['table']}\n-                    ALTER COLUMN {migration['column']} TYPE {migration['new_type']}\n+                    ALTER TABLE {migration[\"table\"]}\n+                    ALTER COLUMN {migration[\"column\"]} TYPE {migration[\"new_type\"]}\n                     \"\"\"\n \n                     await self.execute(alter_sql)\n@@ -3387,7 +3387,9 @@ class PGGraphStorage(BaseGraphStorage):\n                 logger.error(f\"[{self.workspace}] Error during edge deletion: {str(e)}\")\n                 raise\n \n-    async def get_nodes_batch(self, node_ids: list[str], batch_size: int = 1000) -> dict[str, dict]:\n+    async def get_nodes_batch(\n+        self, node_ids: list[str], batch_size: int = 1000\n+    ) -> dict[str, dict]:\n         \"\"\"\n         Retrieve multiple nodes in one query using UNWIND.\n \n@@ -3453,7 +3455,9 @@ class PGGraphStorage(BaseGraphStorage):\n \n         return nodes_dict\n \n-    async def node_degrees_batch(self, node_ids: list[str], batch_size: int = 500) -> dict[str, int]:\n+    async def node_degrees_batch(\n+        self, node_ids: list[str], batch_size: int = 500\n+    ) -> dict[str, int]:\n         \"\"\"\n         Retrieve the degree for multiple nodes in a single query using UNWIND.\n         Calculates the total degree by counting distinct relationships.\n@@ -3482,7 +3486,7 @@ class PGGraphStorage(BaseGraphStorage):\n         in_degrees = {}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n \n             query = f\"\"\"\n                     WITH input(v, ord) AS (\n@@ -3602,7 +3606,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_dict: dict[tuple[str, str], dict] = {}\n \n         for i in range(0, len(uniq_pairs), batch_size):\n-            batch = uniq_pairs[i:i + batch_size]\n+            batch = uniq_pairs[i : i + batch_size]\n \n             pairs = [{\"src\": p[\"src\"], \"tgt\": p[\"tgt\"]} for p in batch]\n \n@@ -3709,7 +3713,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_norm: dict[str, list[tuple[str, str]]] = {n: [] for n in unique_ids}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n             # Format node IDs for the query\n             formatted_ids = \", \".join([f'\"{n}\"' for n in batch])\n \n"
    },
    {
        "id": 295,
        "sha_fail": "d6626e5de246cce87651793060322ff1b3f7e0c0",
        "diff": "diff --git a/libs/agno/agno/tools/firecrawl.py b/libs/agno/agno/tools/firecrawl.py\nindex aec7b2f0..99ad601a 100644\n--- a/libs/agno/agno/tools/firecrawl.py\n+++ b/libs/agno/agno/tools/firecrawl.py\n@@ -2,7 +2,6 @@ import json\n from os import getenv\n from typing import Any, Dict, List, Optional\n \n-from agno.tools import Toolkit\n from agno.utils.log import logger\n \n try:\n"
    },
    {
        "id": 296,
        "sha_fail": "f6adbaa7d295c83c481fbd8ad452eb1883f42a2c",
        "diff": "diff --git a/libs/agno/agno/models/dashscope/dashscope.py b/libs/agno/agno/models/dashscope/dashscope.py\nindex a1799e07..68ff7209 100644\n--- a/libs/agno/agno/models/dashscope/dashscope.py\n+++ b/libs/agno/agno/models/dashscope/dashscope.py\n@@ -6,7 +6,6 @@ from pydantic import BaseModel\n \n from agno.exceptions import ModelProviderError\n from agno.models.openai.like import OpenAILike\n-from agno.models.response import ModelResponse\n \n \n @dataclass\n@@ -84,7 +83,7 @@ class DashScope(OpenAILike):\n             params[\"extra_body\"] = {\n                 \"enable_thinking\": self.enable_thinking,\n             }\n-            \n+\n             if self.thinking_budget is not None:\n                 params[\"extra_body\"][\"thinking_budget\"] = self.thinking_budget\n \n"
    },
    {
        "id": 298,
        "sha_fail": "6f27d0e8dbbb74ef34579d283621da82ebf3ba26",
        "diff": "diff --git a/tests/unittests/sources/test_smartos.py b/tests/unittests/sources/test_smartos.py\nindex 1f4418e..bc3f5c7 100644\n--- a/tests/unittests/sources/test_smartos.py\n+++ b/tests/unittests/sources/test_smartos.py\n@@ -814,13 +814,10 @@ def joyent_metadata(mocker, m_serial):\n             payloadstr = \" {0}\".format(\n                 res.response_parts[\"payload\"]  # pylint: disable=E1146,E1136\n             )\n-        return (\n-            \"V2 {length} {crc} {request_id} \"\n-            \"{command}{payloadstr}\\n\".format(\n-                payloadstr=payloadstr,\n-                **res.response_parts  # pylint: disable=E1134\n-            ).encode(\"ascii\")\n-        )\n+        return \"V2 {length} {crc} {request_id} {command}{payloadstr}\\n\".format(\n+            payloadstr=payloadstr,\n+            **res.response_parts,  # pylint: disable=E1134\n+        ).encode(\"ascii\")\n \n     res.metasource_data = None\n \n@@ -865,7 +862,6 @@ def joyent_serial_client(joyent_metadata):\n \n @pytest.mark.usefixtures(\"fake_filesystem\")\n class TestJoyentMetadataClient:\n-\n     invalid = b\"invalid command\\n\"\n     failure = b\"FAILURE\\n\"\n     v2_ok = b\"V2_OK\\n\"\n@@ -876,9 +872,9 @@ class TestJoyentMetadataClient:\n         )\n \n     def assertStartsWith(self, haystack, prefix):\n-        assert haystack.startswith(\n-            prefix\n-        ), \"{0} does not start with '{1}'\".format(repr(haystack), prefix)\n+        assert haystack.startswith(prefix), (\n+            \"{0} does not start with '{1}'\".format(repr(haystack), prefix)\n+        )\n \n     def assertNoMoreSideEffects(self, obj):\n         with pytest.raises(StopIteration):\n"
    },
    {
        "id": 300,
        "sha_fail": "260605eb6e21f340badc76485a05dea891e88613",
        "diff": "diff --git a/dask/dataframe/dask_expr/tests/test_collection.py b/dask/dataframe/dask_expr/tests/test_collection.py\nindex 551a4a8..c411fe1 100644\n--- a/dask/dataframe/dask_expr/tests/test_collection.py\n+++ b/dask/dataframe/dask_expr/tests/test_collection.py\n@@ -145,7 +145,6 @@ def test_column_projection_modify_list(df, pdf):\n \n @pytest.mark.parametrize(\"required_columns\", [None, [\"z\"], [\"z\", \"foo\"]])\n def test_column_projection_map_partitions(required_columns):\n-\n     pdf = pd.DataFrame({\"x\": [1] * 5, \"y\": [2] * 5, \"z\": range(5)})\n     df = from_pandas(pdf, npartitions=2)\n \n"
    },
    {
        "id": 301,
        "sha_fail": "a04180c58c29a081d430eeb4d86964039b5ebf79",
        "diff": "diff --git a/dask/dataframe/dask_expr/tests/test_collection.py b/dask/dataframe/dask_expr/tests/test_collection.py\nindex 551a4a8..c411fe1 100644\n--- a/dask/dataframe/dask_expr/tests/test_collection.py\n+++ b/dask/dataframe/dask_expr/tests/test_collection.py\n@@ -145,7 +145,6 @@ def test_column_projection_modify_list(df, pdf):\n \n @pytest.mark.parametrize(\"required_columns\", [None, [\"z\"], [\"z\", \"foo\"]])\n def test_column_projection_map_partitions(required_columns):\n-\n     pdf = pd.DataFrame({\"x\": [1] * 5, \"y\": [2] * 5, \"z\": range(5)})\n     df = from_pandas(pdf, npartitions=2)\n \n"
    },
    {
        "id": 302,
        "sha_fail": "3fecab1b8e5668d6fa49529e87195f6ee7eba268",
        "diff": "diff --git a/tests/core/tests/admin_integration/test_action_export.py b/tests/core/tests/admin_integration/test_action_export.py\nindex c566cf0..14fc6b2 100644\n--- a/tests/core/tests/admin_integration/test_action_export.py\n+++ b/tests/core/tests/admin_integration/test_action_export.py\n@@ -379,8 +379,7 @@ class TestExportButtonOnChangeForm(AdminTestMixin, TestCase):\n             args=[self.cat1.id],\n         )\n         self.target_str = (\n-            '<input type=\"submit\" value=\"Export\" '\n-            'class=\"default\" name=\"_export-item\">'\n+            '<input type=\"submit\" value=\"Export\" class=\"default\" name=\"_export-item\">'\n         )\n \n     def test_export_button_on_change_form(self):\n@@ -460,13 +459,13 @@ class TestSkipExportFormFromAction(AdminTestMixin, TestCase):\n     def test_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n     def test_skip_export_form_from_action_setting_enabled(self):\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n \n@@ -500,7 +499,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     def test_export_button_on_change_form_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n@@ -509,7 +508,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     ):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_EXPORT_UI=True)\n"
    },
    {
        "id": 303,
        "sha_fail": "dadeba110a5a98dbc810f9b8f3896424175318c1",
        "diff": "diff --git a/tests/core/tests/admin_integration/test_action_export.py b/tests/core/tests/admin_integration/test_action_export.py\nindex c566cf0..14fc6b2 100644\n--- a/tests/core/tests/admin_integration/test_action_export.py\n+++ b/tests/core/tests/admin_integration/test_action_export.py\n@@ -379,8 +379,7 @@ class TestExportButtonOnChangeForm(AdminTestMixin, TestCase):\n             args=[self.cat1.id],\n         )\n         self.target_str = (\n-            '<input type=\"submit\" value=\"Export\" '\n-            'class=\"default\" name=\"_export-item\">'\n+            '<input type=\"submit\" value=\"Export\" class=\"default\" name=\"_export-item\">'\n         )\n \n     def test_export_button_on_change_form(self):\n@@ -460,13 +459,13 @@ class TestSkipExportFormFromAction(AdminTestMixin, TestCase):\n     def test_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n     def test_skip_export_form_from_action_setting_enabled(self):\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n \n@@ -500,7 +499,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     def test_export_button_on_change_form_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n@@ -509,7 +508,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     ):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_EXPORT_UI=True)\n"
    },
    {
        "id": 304,
        "sha_fail": "16c48e2d1190bbc6fd907ae612921df5dd3f42f4",
        "diff": "diff --git a/tests/middleware/test_cors.py b/tests/middleware/test_cors.py\nindex f2c5fa2..661580c 100644\n--- a/tests/middleware/test_cors.py\n+++ b/tests/middleware/test_cors.py\n@@ -202,7 +202,7 @@ def test_cors_disallowed_preflight(\n     test_client_factory: TestClientFactory,\n ) -> None:\n     def homepage(request: Request) -> None:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -252,7 +252,7 @@ def test_cors_preflight_allow_all_methods(\n     test_client_factory: TestClientFactory,\n ) -> None:\n     def homepage(request: Request) -> None:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -460,7 +460,7 @@ def test_cors_preflight_vary_with_wildcard_origins_specific_methods(\n     test_client_factory: TestClientFactory,\n ) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -481,7 +481,7 @@ def test_cors_preflight_vary_with_specific_origins_wildcard_methods(\n     test_client_factory: TestClientFactory,\n ) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -694,7 +694,7 @@ def test_cors_null_origin_explicitly_allowed(test_client_factory: TestClientFact\n \n def test_cors_method_case_sensitive(test_client_factory: TestClientFactory) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -776,7 +776,7 @@ def test_cors_origins_list_and_regex_both_accepted(test_client_factory: TestClie\n \n def test_cors_max_age_header(test_client_factory: TestClientFactory) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app_default = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -817,7 +817,7 @@ def test_cors_no_origin_header_no_cors_processing(test_client_factory: TestClien\n \n def test_cors_header_name_case_insensitive(test_client_factory: TestClientFactory) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n"
    },
    {
        "id": 305,
        "sha_fail": "207b861fbc10acc867a6f04eae0b4c866d1e508d",
        "diff": "diff --git a/tests/supervisors/test_multiprocess.py b/tests/supervisors/test_multiprocess.py\nindex e1f594e..c307934 100644\n--- a/tests/supervisors/test_multiprocess.py\n+++ b/tests/supervisors/test_multiprocess.py\n@@ -86,10 +86,10 @@ def test_multiprocess_health_check() -> None:\n     time.sleep(1)\n     process = supervisor.processes[0]\n     process.kill()\n-    assert not process.is_alive()\n+    assert process.is_alive() == False\n     time.sleep(1)\n     for p in supervisor.processes:\n-        assert p.is_alive()\n+        assert p.is_alive() == True\n     supervisor.signal_queue.append(signal.SIGINT)\n     supervisor.join_all()\n \n"
    },
    {
        "id": 308,
        "sha_fail": "df5e9e422993a051ab7e2fc7b1dc70485a40c7d3",
        "diff": "diff --git a/facefusion.py b/facefusion.py\nindex 98a865c..d6dade6 100755\n--- a/facefusion.py\n+++ b/facefusion.py\n@@ -2,9 +2,9 @@\n \n import os\n \n-os.environ['OMP_NUM_THREADS'] = '1'\n+os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n \n from facefusion import core\n \n-if __name__ == '__main__':\n-\tcore.cli()\n+if __name__ == \"__main__\":\n+    core.cli()\ndiff --git a/facefusion/core.py b/facefusion/core.py\nindex 23c648e..8eea4fd 100755\n--- a/facefusion/core.py\n+++ b/facefusion/core.py\n@@ -5,11 +5,41 @@ import signal\n import sys\n from time import time\n \n-from facefusion import benchmarker, cli_helper, content_analyser, face_classifier, face_detector, face_landmarker, face_masker, face_recognizer, hash_helper, logger, state_manager, translator, voice_extractor\n-from facefusion.args import apply_args, collect_job_args, reduce_job_args, reduce_step_args\n-from facefusion.download import conditional_download_hashes, conditional_download_sources\n+from facefusion import (\n+    benchmarker,\n+    cli_helper,\n+    content_analyser,\n+    face_classifier,\n+    face_detector,\n+    face_landmarker,\n+    face_masker,\n+    face_recognizer,\n+    hash_helper,\n+    logger,\n+    state_manager,\n+    translator,\n+    voice_extractor,\n+)\n+from facefusion.args import (\n+    apply_args,\n+    collect_job_args,\n+    reduce_job_args,\n+    reduce_step_args,\n+)\n+from facefusion.download import (\n+    conditional_download_hashes,\n+    conditional_download_sources,\n+)\n from facefusion.exit_helper import hard_exit, signal_exit\n-from facefusion.filesystem import get_file_extension, get_file_name, has_audio, is_image, is_video, resolve_file_paths, resolve_file_pattern\n+from facefusion.filesystem import (\n+    get_file_extension,\n+    get_file_name,\n+    has_audio,\n+    is_image,\n+    is_video,\n+    resolve_file_paths,\n+    resolve_file_pattern,\n+)\n from facefusion.jobs import job_helper, job_manager, job_runner\n from facefusion.jobs.job_list import compose_job_list\n from facefusion.memory import limit_system_memory\n@@ -21,319 +51,498 @@ from facefusion.workflows import audio_to_image, image_to_image, image_to_video\n \n \n def cli() -> None:\n-\tif pre_check():\n-\t\tsignal.signal(signal.SIGINT, signal_exit)\n-\t\tprogram = create_program()\n-\n-\t\tif validate_args(program):\n-\t\t\targs = vars(program.parse_args())\n-\t\t\tapply_args(args, state_manager.init_item)\n-\n-\t\t\tif state_manager.get_item('command'):\n-\t\t\t\tlogger.init(state_manager.get_item('log_level'))\n-\t\t\t\troute(args)\n-\t\t\telse:\n-\t\t\t\tprogram.print_help()\n-\t\telse:\n-\t\t\thard_exit(2)\n-\telse:\n-\t\thard_exit(2)\n-\n-\n-def route(args : Args) -> None:\n-\tsystem_memory_limit = state_manager.get_item('system_memory_limit')\n-\n-\tif system_memory_limit and system_memory_limit > 0:\n-\t\tlimit_system_memory(system_memory_limit)\n-\n-\tif state_manager.get_item('command') == 'force-download':\n-\t\terror_code = force_download()\n-\t\thard_exit(error_code)\n-\n-\tif state_manager.get_item('command') == 'benchmark':\n-\t\tif not common_pre_check() or not processors_pre_check() or not benchmarker.pre_check():\n-\t\t\thard_exit(2)\n-\t\tbenchmarker.render()\n-\n-\tif state_manager.get_item('command') in [ 'job-list', 'job-create', 'job-submit', 'job-submit-all', 'job-delete', 'job-delete-all', 'job-add-step', 'job-remix-step', 'job-insert-step', 'job-remove-step' ]:\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_code = route_job_manager(args)\n-\t\thard_exit(error_code)\n-\n-\tif state_manager.get_item('command') == 'run':\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_code = process_headless(args)\n-\t\thard_exit(error_code)\n-\n-\tif state_manager.get_item('command') == 'batch-run':\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_code = process_batch(args)\n-\t\thard_exit(error_code)\n-\n-\tif state_manager.get_item('command') in [ 'job-run', 'job-run-all', 'job-retry', 'job-retry-all' ]:\n-\t\tif not job_manager.init_jobs(state_manager.get_item('jobs_path')):\n-\t\t\thard_exit(1)\n-\t\terror_code = route_job_runner()\n-\t\thard_exit(error_code)\n+    if pre_check():\n+        signal.signal(signal.SIGINT, signal_exit)\n+        program = create_program()\n+\n+        if validate_args(program):\n+            args = vars(program.parse_args())\n+            apply_args(args, state_manager.init_item)\n+\n+            if state_manager.get_item(\"command\"):\n+                logger.init(state_manager.get_item(\"log_level\"))\n+                route(args)\n+            else:\n+                program.print_help()\n+        else:\n+            hard_exit(2)\n+    else:\n+        hard_exit(2)\n+\n+\n+def route(args: Args) -> None:\n+    system_memory_limit = state_manager.get_item(\"system_memory_limit\")\n+\n+    if system_memory_limit and system_memory_limit > 0:\n+        limit_system_memory(system_memory_limit)\n+\n+    if state_manager.get_item(\"command\") == \"force-download\":\n+        error_code = force_download()\n+        hard_exit(error_code)\n+\n+    if state_manager.get_item(\"command\") == \"benchmark\":\n+        if (\n+            not common_pre_check()\n+            or not processors_pre_check()\n+            or not benchmarker.pre_check()\n+        ):\n+            hard_exit(2)\n+        benchmarker.render()\n+\n+    if state_manager.get_item(\"command\") in [\n+        \"job-list\",\n+        \"job-create\",\n+        \"job-submit\",\n+        \"job-submit-all\",\n+        \"job-delete\",\n+        \"job-delete-all\",\n+        \"job-add-step\",\n+        \"job-remix-step\",\n+        \"job-insert-step\",\n+        \"job-remove-step\",\n+    ]:\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_code = route_job_manager(args)\n+        hard_exit(error_code)\n+\n+    if state_manager.get_item(\"command\") == \"run\":\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_code = process_headless(args)\n+        hard_exit(error_code)\n+\n+    if state_manager.get_item(\"command\") == \"batch-run\":\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_code = process_batch(args)\n+        hard_exit(error_code)\n+\n+    if state_manager.get_item(\"command\") in [\n+        \"job-run\",\n+        \"job-run-all\",\n+        \"job-retry\",\n+        \"job-retry-all\",\n+    ]:\n+        if not job_manager.init_jobs(state_manager.get_item(\"jobs_path\")):\n+            hard_exit(1)\n+        error_code = route_job_runner()\n+        hard_exit(error_code)\n \n \n def pre_check() -> bool:\n-\tif sys.version_info < (3, 10):\n-\t\tlogger.error(translator.get('python_not_supported').format(version = '3.10'), __name__)\n-\t\treturn False\n+    if sys.version_info < (3, 10):\n+        logger.error(\n+            translator.get(\"python_not_supported\").format(version=\"3.10\"), __name__\n+        )\n+        return False\n \n-\tif not shutil.which('curl'):\n-\t\tlogger.error(translator.get('curl_not_installed'), __name__)\n-\t\treturn False\n+    if not shutil.which(\"curl\"):\n+        logger.error(translator.get(\"curl_not_installed\"), __name__)\n+        return False\n \n-\tif not shutil.which('ffmpeg'):\n-\t\tlogger.error(translator.get('ffmpeg_not_installed'), __name__)\n-\t\treturn False\n-\treturn True\n+    if not shutil.which(\"ffmpeg\"):\n+        logger.error(translator.get(\"ffmpeg_not_installed\"), __name__)\n+        return False\n+    return True\n \n \n def common_pre_check() -> bool:\n-\tcommon_modules =\\\n-\t[\n-\t\tcontent_analyser,\n-\t\tface_classifier,\n-\t\tface_detector,\n-\t\tface_landmarker,\n-\t\tface_masker,\n-\t\tface_recognizer,\n-\t\tvoice_extractor\n-\t]\n+    common_modules = [\n+        content_analyser,\n+        face_classifier,\n+        face_detector,\n+        face_landmarker,\n+        face_masker,\n+        face_recognizer,\n+        voice_extractor,\n+    ]\n \n-\tcontent_analyser_content = inspect.getsource(content_analyser).encode()\n-\tcontent_analyser_hash = hash_helper.create_hash(content_analyser_content)\n+    content_analyser_content = inspect.getsource(content_analyser).encode()\n+    content_analyser_hash = hash_helper.create_hash(content_analyser_content)\n \n-\treturn all(module.pre_check() for module in common_modules) and content_analyser_hash == 'b14e7b92'\n+    return (\n+        all(module.pre_check() for module in common_modules)\n+        and content_analyser_hash == \"b14e7b92\"\n+    )\n \n \n def processors_pre_check() -> bool:\n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\tif not processor_module.pre_check():\n-\t\t\treturn False\n-\treturn True\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        if not processor_module.pre_check():\n+            return False\n+    return True\n \n \n def force_download() -> ErrorCode:\n-\tcommon_modules =\\\n-\t[\n-\t\tcontent_analyser,\n-\t\tface_classifier,\n-\t\tface_detector,\n-\t\tface_landmarker,\n-\t\tface_masker,\n-\t\tface_recognizer,\n-\t\tvoice_extractor\n-\t]\n-\tavailable_processors = [ get_file_name(file_path) for file_path in resolve_file_paths('facefusion/processors/modules') ]\n-\tprocessor_modules = get_processors_modules(available_processors)\n-\n-\tfor module in common_modules + processor_modules:\n-\t\tif hasattr(module, 'create_static_model_set'):\n-\t\t\tfor model in module.create_static_model_set(state_manager.get_item('download_scope')).values():\n-\t\t\t\tmodel_hash_set = model.get('hashes')\n-\t\t\t\tmodel_source_set = model.get('sources')\n-\n-\t\t\t\tif model_hash_set and model_source_set:\n-\t\t\t\t\tif not conditional_download_hashes(model_hash_set) or not conditional_download_sources(model_source_set):\n-\t\t\t\t\t\treturn 1\n-\n-\treturn 0\n-\n-\n-def route_job_manager(args : Args) -> ErrorCode:\n-\tif state_manager.get_item('command') == 'job-list':\n-\t\tjob_headers, job_contents = compose_job_list(state_manager.get_item('job_status'))\n-\n-\t\tif job_contents:\n-\t\t\tcli_helper.render_table(job_headers, job_contents)\n-\t\t\treturn 0\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-create':\n-\t\tif job_manager.create_job(state_manager.get_item('job_id')):\n-\t\t\tlogger.info(translator.get('job_created').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_not_created').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-submit':\n-\t\tif job_manager.submit_job(state_manager.get_item('job_id')):\n-\t\t\tlogger.info(translator.get('job_submitted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_not_submitted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-submit-all':\n-\t\tif job_manager.submit_jobs(state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(translator.get('job_all_submitted'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_all_not_submitted'), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-delete':\n-\t\tif job_manager.delete_job(state_manager.get_item('job_id')):\n-\t\t\tlogger.info(translator.get('job_deleted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_not_deleted').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-delete-all':\n-\t\tif job_manager.delete_jobs(state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(translator.get('job_all_deleted'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_all_not_deleted'), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-add-step':\n-\t\tstep_args = reduce_step_args(args)\n-\n-\t\tif job_manager.add_step(state_manager.get_item('job_id'), step_args):\n-\t\t\tlogger.info(translator.get('job_step_added').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_step_not_added').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-remix-step':\n-\t\tstep_args = reduce_step_args(args)\n-\n-\t\tif job_manager.remix_step(state_manager.get_item('job_id'), state_manager.get_item('step_index'), step_args):\n-\t\t\tlogger.info(translator.get('job_remix_step_added').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_remix_step_not_added').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-insert-step':\n-\t\tstep_args = reduce_step_args(args)\n-\n-\t\tif job_manager.insert_step(state_manager.get_item('job_id'), state_manager.get_item('step_index'), step_args):\n-\t\t\tlogger.info(translator.get('job_step_inserted').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_step_not_inserted').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-remove-step':\n-\t\tif job_manager.remove_step(state_manager.get_item('job_id'), state_manager.get_item('step_index')):\n-\t\t\tlogger.info(translator.get('job_step_removed').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.error(translator.get('job_step_not_removed').format(job_id = state_manager.get_item('job_id'), step_index = state_manager.get_item('step_index')), __name__)\n-\t\treturn 1\n-\treturn 1\n+    common_modules = [\n+        content_analyser,\n+        face_classifier,\n+        face_detector,\n+        face_landmarker,\n+        face_masker,\n+        face_recognizer,\n+        voice_extractor,\n+    ]\n+    available_processors = [\n+        get_file_name(file_path)\n+        for file_path in resolve_file_paths(\"facefusion/processors/modules\")\n+    ]\n+    processor_modules = get_processors_modules(available_processors)\n+\n+    for module in common_modules + processor_modules:\n+        if hasattr(module, \"create_static_model_set\"):\n+            for model in module.create_static_model_set(\n+                state_manager.get_item(\"download_scope\")\n+            ).values():\n+                model_hash_set = model.get(\"hashes\")\n+                model_source_set = model.get(\"sources\")\n+\n+                if model_hash_set and model_source_set:\n+                    if not conditional_download_hashes(\n+                        model_hash_set\n+                    ) or not conditional_download_sources(model_source_set):\n+                        return 1\n+\n+    return 0\n+\n+\n+def route_job_manager(args: Args) -> ErrorCode:\n+    if state_manager.get_item(\"command\") == \"job-list\":\n+        job_headers, job_contents = compose_job_list(\n+            state_manager.get_item(\"job_status\")\n+        )\n+\n+        if job_contents:\n+            cli_helper.render_table(job_headers, job_contents)\n+            return 0\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-create\":\n+        if job_manager.create_job(state_manager.get_item(\"job_id\")):\n+            logger.info(\n+                translator.get(\"job_created\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_not_created\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-submit\":\n+        if job_manager.submit_job(state_manager.get_item(\"job_id\")):\n+            logger.info(\n+                translator.get(\"job_submitted\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_not_submitted\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-submit-all\":\n+        if job_manager.submit_jobs(state_manager.get_item(\"halt_on_error\")):\n+            logger.info(translator.get(\"job_all_submitted\"), __name__)\n+            return 0\n+        logger.error(translator.get(\"job_all_not_submitted\"), __name__)\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-delete\":\n+        if job_manager.delete_job(state_manager.get_item(\"job_id\")):\n+            logger.info(\n+                translator.get(\"job_deleted\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_not_deleted\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-delete-all\":\n+        if job_manager.delete_jobs(state_manager.get_item(\"halt_on_error\")):\n+            logger.info(translator.get(\"job_all_deleted\"), __name__)\n+            return 0\n+        logger.error(translator.get(\"job_all_not_deleted\"), __name__)\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-add-step\":\n+        step_args = reduce_step_args(args)\n+\n+        if job_manager.add_step(state_manager.get_item(\"job_id\"), step_args):\n+            logger.info(\n+                translator.get(\"job_step_added\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_step_not_added\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-remix-step\":\n+        step_args = reduce_step_args(args)\n+\n+        if job_manager.remix_step(\n+            state_manager.get_item(\"job_id\"),\n+            state_manager.get_item(\"step_index\"),\n+            step_args,\n+        ):\n+            logger.info(\n+                translator.get(\"job_remix_step_added\").format(\n+                    job_id=state_manager.get_item(\"job_id\"),\n+                    step_index=state_manager.get_item(\"step_index\"),\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_remix_step_not_added\").format(\n+                job_id=state_manager.get_item(\"job_id\"),\n+                step_index=state_manager.get_item(\"step_index\"),\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-insert-step\":\n+        step_args = reduce_step_args(args)\n+\n+        if job_manager.insert_step(\n+            state_manager.get_item(\"job_id\"),\n+            state_manager.get_item(\"step_index\"),\n+            step_args,\n+        ):\n+            logger.info(\n+                translator.get(\"job_step_inserted\").format(\n+                    job_id=state_manager.get_item(\"job_id\"),\n+                    step_index=state_manager.get_item(\"step_index\"),\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_step_not_inserted\").format(\n+                job_id=state_manager.get_item(\"job_id\"),\n+                step_index=state_manager.get_item(\"step_index\"),\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-remove-step\":\n+        if job_manager.remove_step(\n+            state_manager.get_item(\"job_id\"), state_manager.get_item(\"step_index\")\n+        ):\n+            logger.info(\n+                translator.get(\"job_step_removed\").format(\n+                    job_id=state_manager.get_item(\"job_id\"),\n+                    step_index=state_manager.get_item(\"step_index\"),\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.error(\n+            translator.get(\"job_step_not_removed\").format(\n+                job_id=state_manager.get_item(\"job_id\"),\n+                step_index=state_manager.get_item(\"step_index\"),\n+            ),\n+            __name__,\n+        )\n+        return 1\n+    return 1\n \n \n def route_job_runner() -> ErrorCode:\n-\tif state_manager.get_item('command') == 'job-run':\n-\t\tlogger.info(translator.get('running_job').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\tif job_runner.run_job(state_manager.get_item('job_id'), process_step):\n-\t\t\tlogger.info(translator.get('processing_job_succeeded').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(translator.get('processing_job_failed').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-run-all':\n-\t\tlogger.info(translator.get('running_jobs'), __name__)\n-\t\tif job_runner.run_jobs(process_step, state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(translator.get('processing_jobs_succeeded'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(translator.get('processing_jobs_failed'), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-retry':\n-\t\tlogger.info(translator.get('retrying_job').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\tif job_runner.retry_job(state_manager.get_item('job_id'), process_step):\n-\t\t\tlogger.info(translator.get('processing_job_succeeded').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(translator.get('processing_job_failed').format(job_id = state_manager.get_item('job_id')), __name__)\n-\t\treturn 1\n-\n-\tif state_manager.get_item('command') == 'job-retry-all':\n-\t\tlogger.info(translator.get('retrying_jobs'), __name__)\n-\t\tif job_runner.retry_jobs(process_step, state_manager.get_item('halt_on_error')):\n-\t\t\tlogger.info(translator.get('processing_jobs_succeeded'), __name__)\n-\t\t\treturn 0\n-\t\tlogger.info(translator.get('processing_jobs_failed'), __name__)\n-\t\treturn 1\n-\treturn 2\n-\n-\n-def process_headless(args : Args) -> ErrorCode:\n-\tjob_id = job_helper.suggest_job_id('headless')\n-\tstep_args = reduce_step_args(args)\n-\n-\tif job_manager.create_job(job_id) and job_manager.add_step(job_id, step_args) and job_manager.submit_job(job_id) and job_runner.run_job(job_id, process_step):\n-\t\treturn 0\n-\treturn 1\n-\n-\n-def process_batch(args : Args) -> ErrorCode:\n-\tjob_id = job_helper.suggest_job_id('batch')\n-\tstep_args = reduce_step_args(args)\n-\tjob_args = reduce_job_args(args)\n-\tsource_paths = resolve_file_pattern(job_args.get('source_pattern'))\n-\ttarget_paths = resolve_file_pattern(job_args.get('target_pattern'))\n-\n-\tif job_manager.create_job(job_id):\n-\t\tif source_paths and target_paths:\n-\t\t\tfor index, (source_path, target_path) in enumerate(itertools.product(source_paths, target_paths)):\n-\t\t\t\tstep_args['source_paths'] = [ source_path ]\n-\t\t\t\tstep_args['target_path'] = target_path\n-\n-\t\t\t\ttry:\n-\t\t\t\t\tstep_args['output_path'] = job_args.get('output_pattern').format(index = index, source_name = get_file_name(source_path), target_name = get_file_name(target_path), target_extension = get_file_extension(target_path))\n-\t\t\t\texcept KeyError:\n-\t\t\t\t\treturn 1\n-\n-\t\t\t\tif not job_manager.add_step(job_id, step_args):\n-\t\t\t\t\treturn 1\n-\t\t\tif job_manager.submit_job(job_id) and job_runner.run_job(job_id, process_step):\n-\t\t\t\treturn 0\n-\n-\t\tif not source_paths and target_paths:\n-\t\t\tfor index, target_path in enumerate(target_paths):\n-\t\t\t\tstep_args['target_path'] = target_path\n-\n-\t\t\t\ttry:\n-\t\t\t\t\tstep_args['output_path'] = job_args.get('output_pattern').format(index = index, target_name = get_file_name(target_path), target_extension = get_file_extension(target_path))\n-\t\t\t\texcept KeyError:\n-\t\t\t\t\treturn 1\n-\n-\t\t\t\tif not job_manager.add_step(job_id, step_args):\n-\t\t\t\t\treturn 1\n-\t\t\tif job_manager.submit_job(job_id) and job_runner.run_job(job_id, process_step):\n-\t\t\t\treturn 0\n-\treturn 1\n-\n-\n-def process_step(job_id : str, step_index : int, step_args : Args) -> bool:\n-\tstep_total = job_manager.count_step_total(job_id)\n-\tstep_args.update(collect_job_args())\n-\tapply_args(step_args, state_manager.set_item)\n-\n-\tlogger.info(translator.get('processing_step').format(step_current = step_index + 1, step_total = step_total), __name__)\n-\tif common_pre_check() and processors_pre_check():\n-\t\terror_code = conditional_process()\n-\t\treturn error_code == 0\n-\treturn False\n+    if state_manager.get_item(\"command\") == \"job-run\":\n+        logger.info(\n+            translator.get(\"running_job\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        if job_runner.run_job(state_manager.get_item(\"job_id\"), process_step):\n+            logger.info(\n+                translator.get(\"processing_job_succeeded\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.info(\n+            translator.get(\"processing_job_failed\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-run-all\":\n+        logger.info(translator.get(\"running_jobs\"), __name__)\n+        if job_runner.run_jobs(process_step, state_manager.get_item(\"halt_on_error\")):\n+            logger.info(translator.get(\"processing_jobs_succeeded\"), __name__)\n+            return 0\n+        logger.info(translator.get(\"processing_jobs_failed\"), __name__)\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-retry\":\n+        logger.info(\n+            translator.get(\"retrying_job\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        if job_runner.retry_job(state_manager.get_item(\"job_id\"), process_step):\n+            logger.info(\n+                translator.get(\"processing_job_succeeded\").format(\n+                    job_id=state_manager.get_item(\"job_id\")\n+                ),\n+                __name__,\n+            )\n+            return 0\n+        logger.info(\n+            translator.get(\"processing_job_failed\").format(\n+                job_id=state_manager.get_item(\"job_id\")\n+            ),\n+            __name__,\n+        )\n+        return 1\n+\n+    if state_manager.get_item(\"command\") == \"job-retry-all\":\n+        logger.info(translator.get(\"retrying_jobs\"), __name__)\n+        if job_runner.retry_jobs(process_step, state_manager.get_item(\"halt_on_error\")):\n+            logger.info(translator.get(\"processing_jobs_succeeded\"), __name__)\n+            return 0\n+        logger.info(translator.get(\"processing_jobs_failed\"), __name__)\n+        return 1\n+    return 2\n+\n+\n+def process_headless(args: Args) -> ErrorCode:\n+    job_id = job_helper.suggest_job_id(\"headless\")\n+    step_args = reduce_step_args(args)\n+\n+    if (\n+        job_manager.create_job(job_id)\n+        and job_manager.add_step(job_id, step_args)\n+        and job_manager.submit_job(job_id)\n+        and job_runner.run_job(job_id, process_step)\n+    ):\n+        return 0\n+    return 1\n+\n+\n+def process_batch(args: Args) -> ErrorCode:\n+    job_id = job_helper.suggest_job_id(\"batch\")\n+    step_args = reduce_step_args(args)\n+    job_args = reduce_job_args(args)\n+    source_paths = resolve_file_pattern(job_args.get(\"source_pattern\"))\n+    target_paths = resolve_file_pattern(job_args.get(\"target_pattern\"))\n+\n+    if job_manager.create_job(job_id):\n+        if source_paths and target_paths:\n+            for index, (source_path, target_path) in enumerate(\n+                itertools.product(source_paths, target_paths)\n+            ):\n+                step_args[\"source_paths\"] = [source_path]\n+                step_args[\"target_path\"] = target_path\n+\n+                try:\n+                    step_args[\"output_path\"] = job_args.get(\"output_pattern\").format(\n+                        index=index,\n+                        source_name=get_file_name(source_path),\n+                        target_name=get_file_name(target_path),\n+                        target_extension=get_file_extension(target_path),\n+                    )\n+                except KeyError:\n+                    return 1\n+\n+                if not job_manager.add_step(job_id, step_args):\n+                    return 1\n+            if job_manager.submit_job(job_id) and job_runner.run_job(\n+                job_id, process_step\n+            ):\n+                return 0\n+\n+        if not source_paths and target_paths:\n+            for index, target_path in enumerate(target_paths):\n+                step_args[\"target_path\"] = target_path\n+\n+                try:\n+                    step_args[\"output_path\"] = job_args.get(\"output_pattern\").format(\n+                        index=index,\n+                        target_name=get_file_name(target_path),\n+                        target_extension=get_file_extension(target_path),\n+                    )\n+                except KeyError:\n+                    return 1\n+\n+                if not job_manager.add_step(job_id, step_args):\n+                    return 1\n+            if job_manager.submit_job(job_id) and job_runner.run_job(\n+                job_id, process_step\n+            ):\n+                return 0\n+    return 1\n+\n+\n+def process_step(job_id: str, step_index: int, step_args: Args) -> bool:\n+    step_total = job_manager.count_step_total(job_id)\n+    step_args.update(collect_job_args())\n+    apply_args(step_args, state_manager.set_item)\n+\n+    logger.info(\n+        translator.get(\"processing_step\").format(\n+            step_current=step_index + 1, step_total=step_total\n+        ),\n+        __name__,\n+    )\n+    if common_pre_check() and processors_pre_check():\n+        error_code = conditional_process()\n+        return error_code == 0\n+    return False\n \n \n def conditional_process() -> ErrorCode:\n-\tstart_time = time()\n-\n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\tif not processor_module.pre_process('output'):\n-\t\t\treturn 2\n-\n-\tif has_audio(state_manager.get_item('source_paths')) and is_image(state_manager.get_item('target_path')):\n-\t\treturn audio_to_image.process(start_time)\n-\tif is_image(state_manager.get_item('target_path')):\n-\t\treturn image_to_image.process(start_time)\n-\tif is_video(state_manager.get_item('target_path')):\n-\t\treturn image_to_video.process(start_time)\n-\n-\treturn 0\n+    start_time = time()\n+\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        if not processor_module.pre_process(\"output\"):\n+            return 2\n+\n+    if has_audio(state_manager.get_item(\"source_paths\")) and is_image(\n+        state_manager.get_item(\"target_path\")\n+    ):\n+        return audio_to_image.process(start_time)\n+    if is_image(state_manager.get_item(\"target_path\")):\n+        return image_to_image.process(start_time)\n+    if is_video(state_manager.get_item(\"target_path\")):\n+        return image_to_video.process(start_time)\n+\n+    return 0\ndiff --git a/facefusion/jobs/job_runner.py b/facefusion/jobs/job_runner.py\nindex 23a0e38..45d3c1a 100644\n--- a/facefusion/jobs/job_runner.py\n+++ b/facefusion/jobs/job_runner.py\n@@ -4,109 +4,123 @@ from facefusion.jobs import job_helper, job_manager\n from facefusion.types import JobOutputSet, JobStep, ProcessStep\n \n \n-def run_job(job_id : str, process_step : ProcessStep) -> bool:\n-\tqueued_job_ids = job_manager.find_job_ids('queued')\n-\n-\tif job_id in queued_job_ids:\n-\t\tif run_steps(job_id, process_step) and finalize_steps(job_id):\n-\t\t\tclean_steps(job_id)\n-\t\t\treturn job_manager.move_job_file(job_id, 'completed')\n-\t\tclean_steps(job_id)\n-\t\tjob_manager.move_job_file(job_id, 'failed')\n-\treturn False\n-\n-\n-def run_jobs(process_step : ProcessStep, halt_on_error : bool) -> bool:\n-\tqueued_job_ids = job_manager.find_job_ids('queued')\n-\thas_error = False\n-\n-\tif queued_job_ids:\n-\t\tfor job_id in queued_job_ids:\n-\t\t\tif not run_job(job_id, process_step):\n-\t\t\t\thas_error = True\n-\t\t\t\tif halt_on_error:\n-\t\t\t\t\treturn False\n-\t\treturn not has_error\n-\treturn False\n-\n-\n-def retry_job(job_id : str, process_step : ProcessStep) -> bool:\n-\tfailed_job_ids = job_manager.find_job_ids('failed')\n-\n-\tif job_id in failed_job_ids:\n-\t\treturn job_manager.set_steps_status(job_id, 'queued') and job_manager.move_job_file(job_id, 'queued') and run_job(job_id, process_step)\n-\treturn False\n-\n-\n-def retry_jobs(process_step : ProcessStep, halt_on_error : bool) -> bool:\n-\tfailed_job_ids = job_manager.find_job_ids('failed')\n-\thas_error = False\n-\n-\tif failed_job_ids:\n-\t\tfor job_id in failed_job_ids:\n-\t\t\tif not retry_job(job_id, process_step):\n-\t\t\t\thas_error = True\n-\t\t\t\tif halt_on_error:\n-\t\t\t\t\treturn False\n-\t\treturn not has_error\n-\treturn False\n-\n-\n-def run_step(job_id : str, step_index : int, step : JobStep, process_step : ProcessStep) -> bool:\n-\tstep_args = step.get('args')\n-\n-\tif job_manager.set_step_status(job_id, step_index, 'started') and process_step(job_id, step_index, step_args):\n-\t\toutput_path = step_args.get('output_path')\n-\t\tstep_output_path = job_helper.get_step_output_path(job_id, step_index, output_path)\n-\n-\t\treturn move_file(output_path, step_output_path) and job_manager.set_step_status(job_id, step_index, 'completed')\n-\tjob_manager.set_step_status(job_id, step_index, 'failed')\n-\treturn False\n-\n-\n-def run_steps(job_id : str, process_step : ProcessStep) -> bool:\n-\tsteps = job_manager.get_steps(job_id)\n-\n-\tif steps:\n-\t\tfor index, step in enumerate(steps):\n-\t\t\tif not run_step(job_id, index, step, process_step):\n-\t\t\t\treturn False\n-\t\treturn True\n-\treturn False\n-\n-\n-def finalize_steps(job_id : str) -> bool:\n-\toutput_set = collect_output_set(job_id)\n-\n-\tfor output_path, temp_output_paths in output_set.items():\n-\t\tif are_videos(temp_output_paths):\n-\t\t\tif not concat_video(output_path, temp_output_paths):\n-\t\t\t\treturn False\n-\t\tif are_images(temp_output_paths):\n-\t\t\tfor temp_output_path in temp_output_paths:\n-\t\t\t\tif not move_file(temp_output_path, output_path):\n-\t\t\t\t\treturn False\n-\treturn True\n+def run_job(job_id: str, process_step: ProcessStep) -> bool:\n+    queued_job_ids = job_manager.find_job_ids(\"queued\")\n+\n+    if job_id in queued_job_ids:\n+        if run_steps(job_id, process_step) and finalize_steps(job_id):\n+            clean_steps(job_id)\n+            return job_manager.move_job_file(job_id, \"completed\")\n+        clean_steps(job_id)\n+        job_manager.move_job_file(job_id, \"failed\")\n+    return False\n+\n+\n+def run_jobs(process_step: ProcessStep, halt_on_error: bool) -> bool:\n+    queued_job_ids = job_manager.find_job_ids(\"queued\")\n+    has_error = False\n+\n+    if queued_job_ids:\n+        for job_id in queued_job_ids:\n+            if not run_job(job_id, process_step):\n+                has_error = True\n+                if halt_on_error:\n+                    return False\n+        return not has_error\n+    return False\n+\n+\n+def retry_job(job_id: str, process_step: ProcessStep) -> bool:\n+    failed_job_ids = job_manager.find_job_ids(\"failed\")\n+\n+    if job_id in failed_job_ids:\n+        return (\n+            job_manager.set_steps_status(job_id, \"queued\")\n+            and job_manager.move_job_file(job_id, \"queued\")\n+            and run_job(job_id, process_step)\n+        )\n+    return False\n+\n+\n+def retry_jobs(process_step: ProcessStep, halt_on_error: bool) -> bool:\n+    failed_job_ids = job_manager.find_job_ids(\"failed\")\n+    has_error = False\n+\n+    if failed_job_ids:\n+        for job_id in failed_job_ids:\n+            if not retry_job(job_id, process_step):\n+                has_error = True\n+                if halt_on_error:\n+                    return False\n+        return not has_error\n+    return False\n+\n+\n+def run_step(\n+    job_id: str, step_index: int, step: JobStep, process_step: ProcessStep\n+) -> bool:\n+    step_args = step.get(\"args\")\n+\n+    if job_manager.set_step_status(job_id, step_index, \"started\") and process_step(\n+        job_id, step_index, step_args\n+    ):\n+        output_path = step_args.get(\"output_path\")\n+        step_output_path = job_helper.get_step_output_path(\n+            job_id, step_index, output_path\n+        )\n+\n+        return move_file(output_path, step_output_path) and job_manager.set_step_status(\n+            job_id, step_index, \"completed\"\n+        )\n+    job_manager.set_step_status(job_id, step_index, \"failed\")\n+    return False\n+\n+\n+def run_steps(job_id: str, process_step: ProcessStep) -> bool:\n+    steps = job_manager.get_steps(job_id)\n+\n+    if steps:\n+        for index, step in enumerate(steps):\n+            if not run_step(job_id, index, step, process_step):\n+                return False\n+        return True\n+    return False\n+\n+\n+def finalize_steps(job_id: str) -> bool:\n+    output_set = collect_output_set(job_id)\n+\n+    for output_path, temp_output_paths in output_set.items():\n+        if are_videos(temp_output_paths):\n+            if not concat_video(output_path, temp_output_paths):\n+                return False\n+        if are_images(temp_output_paths):\n+            for temp_output_path in temp_output_paths:\n+                if not move_file(temp_output_path, output_path):\n+                    return False\n+    return True\n \n \n def clean_steps(job_id: str) -> bool:\n-\toutput_set = collect_output_set(job_id)\n+    output_set = collect_output_set(job_id)\n \n-\tfor temp_output_paths in output_set.values():\n-\t\tfor temp_output_path in temp_output_paths:\n-\t\t\tif not remove_file(temp_output_path):\n-\t\t\t\treturn False\n-\treturn True\n+    for temp_output_paths in output_set.values():\n+        for temp_output_path in temp_output_paths:\n+            if not remove_file(temp_output_path):\n+                return False\n+    return True\n \n \n-def collect_output_set(job_id : str) -> JobOutputSet:\n-\tsteps = job_manager.get_steps(job_id)\n-\tjob_output_set : JobOutputSet = {}\n+def collect_output_set(job_id: str) -> JobOutputSet:\n+    steps = job_manager.get_steps(job_id)\n+    job_output_set: JobOutputSet = {}\n \n-\tfor index, step in enumerate(steps):\n-\t\toutput_path = step.get('args').get('output_path')\n+    for index, step in enumerate(steps):\n+        output_path = step.get(\"args\").get(\"output_path\")\n \n-\t\tif output_path:\n-\t\t\tstep_output_path = job_manager.get_step_output_path(job_id, index, output_path)\n-\t\t\tjob_output_set.setdefault(output_path, []).append(step_output_path)\n-\treturn job_output_set\n+        if output_path:\n+            step_output_path = job_manager.get_step_output_path(\n+                job_id, index, output_path\n+            )\n+            job_output_set.setdefault(output_path, []).append(step_output_path)\n+    return job_output_set\ndiff --git a/facefusion/workflows/audio_to_image.py b/facefusion/workflows/audio_to_image.py\nindex f7168b7..d3f23aa 100644\n--- a/facefusion/workflows/audio_to_image.py\n+++ b/facefusion/workflows/audio_to_image.py\n@@ -10,135 +10,207 @@ from facefusion.common_helper import get_first\n from facefusion.content_analyser import analyse_image\n from facefusion.filesystem import filter_audio_paths, is_video, move_file\n from facefusion.processors.core import get_processors_modules\n-from facefusion.temp_helper import clear_temp_directory, get_temp_file_path, get_temp_frame_sequence_paths\n+from facefusion.temp_helper import (\n+    clear_temp_directory,\n+    get_temp_file_path,\n+    get_temp_frame_sequence_paths,\n+)\n from facefusion.time_helper import calculate_end_time\n from facefusion.types import ErrorCode\n-from facefusion.vision import detect_image_resolution, pack_resolution, restrict_image_resolution, scale_resolution\n-from facefusion.workflows.core import conditional_process_temp_frame, is_process_stopping, prepare_temp\n-\n-\n-def process(start_time : float) -> ErrorCode:\n-\ttasks =\\\n-\t[\n-\t\tsetup,\n-\t\tprepare_image,\n-\t\tprocess_frames,\n-\t\tmerge_video,\n-\t\treplace_audio,\n-\t\tpartial(finalize_video, start_time)\n-\t]\n-\tprocess_manager.start()\n-\n-\tfor task in tasks:\n-\t\terror_code = task() # type:ignore[operator]\n-\n-\t\tif error_code > 0:\n-\t\t\tprocess_manager.end()\n-\t\t\treturn error_code\n-\n-\tprocess_manager.end()\n-\treturn 0\n+from facefusion.vision import (\n+    detect_image_resolution,\n+    pack_resolution,\n+    restrict_image_resolution,\n+    scale_resolution,\n+)\n+from facefusion.workflows.core import (\n+    conditional_process_temp_frame,\n+    is_process_stopping,\n+    prepare_temp,\n+)\n+\n+\n+def process(start_time: float) -> ErrorCode:\n+    tasks = [\n+        setup,\n+        prepare_image,\n+        process_frames,\n+        merge_video,\n+        replace_audio,\n+        partial(finalize_video, start_time),\n+    ]\n+    process_manager.start()\n+\n+    for task in tasks:\n+        error_code = task()  # type:ignore[operator]\n+\n+        if error_code > 0:\n+            process_manager.end()\n+            return error_code\n+\n+    process_manager.end()\n+    return 0\n \n \n def setup() -> ErrorCode:\n-\tif analyse_image(state_manager.get_item('target_path')):\n-\t\treturn 3\n+    if analyse_image(state_manager.get_item(\"target_path\")):\n+        return 3\n \n-\tprepare_temp()\n-\treturn 0\n+    prepare_temp()\n+    return 0\n \n \n def prepare_image() -> ErrorCode:\n-\toutput_image_resolution = scale_resolution(detect_image_resolution(state_manager.get_item('target_path')), state_manager.get_item('output_image_scale'))\n-\ttemp_image_resolution = restrict_image_resolution(state_manager.get_item('target_path'), output_image_resolution)\n-\n-\tlogger.info(translator.get('copying_image').format(resolution = pack_resolution(temp_image_resolution)), __name__)\n-\tif ffmpeg.copy_image(state_manager.get_item('target_path'), temp_image_resolution):\n-\t\tlogger.debug(translator.get('copying_image_succeeded'), __name__)\n-\telse:\n-\t\tlogger.error(translator.get('copying_image_failed'), __name__)\n-\t\tprocess_manager.end()\n-\t\treturn 1\n-\treturn 0\n+    output_image_resolution = scale_resolution(\n+        detect_image_resolution(state_manager.get_item(\"target_path\")),\n+        state_manager.get_item(\"output_image_scale\"),\n+    )\n+    temp_image_resolution = restrict_image_resolution(\n+        state_manager.get_item(\"target_path\"), output_image_resolution\n+    )\n+\n+    logger.info(\n+        translator.get(\"copying_image\").format(\n+            resolution=pack_resolution(temp_image_resolution)\n+        ),\n+        __name__,\n+    )\n+    if ffmpeg.copy_image(state_manager.get_item(\"target_path\"), temp_image_resolution):\n+        logger.debug(translator.get(\"copying_image_succeeded\"), __name__)\n+    else:\n+        logger.error(translator.get(\"copying_image_failed\"), __name__)\n+        process_manager.end()\n+        return 1\n+    return 0\n \n \n def process_frames() -> ErrorCode:\n-\tsource_audio_path = get_first(filter_audio_paths(state_manager.get_item('source_paths')))\n-\taudio_frame_total = count_audio_frame_total(source_audio_path, 25.0)\n-\toutput_video_fps = state_manager.get_item('output_video_fps')\n-\ttemp_frame_paths = get_temp_frame_sequence_paths(state_manager.get_item('target_path'), audio_frame_total, '%08d')\n-\n-\tif temp_frame_paths:\n-\t\twith tqdm(total = len(temp_frame_paths), desc = translator.get('processing'), unit = 'frame', ascii = ' =', disable = state_manager.get_item('log_level') in [ 'warn', 'error' ]) as progress:\n-\t\t\tprogress.set_postfix(execution_providers = state_manager.get_item('execution_providers'))\n-\n-\t\t\twith ThreadPoolExecutor(max_workers = state_manager.get_item('execution_thread_count')) as executor:\n-\t\t\t\tfutures = []\n-\n-\t\t\t\tfor frame_number, temp_frame_path in enumerate(temp_frame_paths):\n-\t\t\t\t\tfuture = executor.submit(conditional_process_temp_frame, temp_frame_path, frame_number, output_video_fps)\n-\t\t\t\t\tfutures.append(future)\n-\n-\t\t\t\tfor future in as_completed(futures):\n-\t\t\t\t\tif is_process_stopping():\n-\t\t\t\t\t\tfor __future__ in futures:\n-\t\t\t\t\t\t\t__future__.cancel()\n-\n-\t\t\t\t\tif not future.cancelled():\n-\t\t\t\t\t\tfuture.result()\n-\t\t\t\t\t\tprogress.update()\n-\n-\t\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\t\tprocessor_module.post_process()\n-\n-\t\tif is_process_stopping():\n-\t\t\treturn 4\n-\telse:\n-\t\tlogger.error(translator.get('temp_frames_not_found'), __name__)\n-\t\treturn 1\n-\treturn 0\n+    source_audio_path = get_first(\n+        filter_audio_paths(state_manager.get_item(\"source_paths\"))\n+    )\n+    audio_frame_total = count_audio_frame_total(source_audio_path, 25.0)\n+    output_video_fps = state_manager.get_item(\"output_video_fps\")\n+    temp_frame_paths = get_temp_frame_sequence_paths(\n+        state_manager.get_item(\"target_path\"), audio_frame_total, \"%08d\"\n+    )\n+\n+    if temp_frame_paths:\n+        with tqdm(\n+            total=len(temp_frame_paths),\n+            desc=translator.get(\"processing\"),\n+            unit=\"frame\",\n+            ascii=\" =\",\n+            disable=state_manager.get_item(\"log_level\") in [\"warn\", \"error\"],\n+        ) as progress:\n+            progress.set_postfix(\n+                execution_providers=state_manager.get_item(\"execution_providers\")\n+            )\n+\n+            with ThreadPoolExecutor(\n+                max_workers=state_manager.get_item(\"execution_thread_count\")\n+            ) as executor:\n+                futures = []\n+\n+                for frame_number, temp_frame_path in enumerate(temp_frame_paths):\n+                    future = executor.submit(\n+                        conditional_process_temp_frame,\n+                        temp_frame_path,\n+                        frame_number,\n+                        output_video_fps,\n+                    )\n+                    futures.append(future)\n+\n+                for future in as_completed(futures):\n+                    if is_process_stopping():\n+                        for __future__ in futures:\n+                            __future__.cancel()\n+\n+                    if not future.cancelled():\n+                        future.result()\n+                        progress.update()\n+\n+        for processor_module in get_processors_modules(\n+            state_manager.get_item(\"processors\")\n+        ):\n+            processor_module.post_process()\n+\n+        if is_process_stopping():\n+            return 4\n+    else:\n+        logger.error(translator.get(\"temp_frames_not_found\"), __name__)\n+        return 1\n+    return 0\n \n \n def merge_video() -> ErrorCode:\n-\ttarget_path = state_manager.get_item('target_path')\n-\toutput_video_fps = state_manager.get_item('output_video_fps')\n-\toutput_video_resolution = scale_resolution(detect_image_resolution(state_manager.get_item('target_path')), state_manager.get_item('output_image_scale'))\n-\ttemp_video_fps = 25\n-\ttrim_frame_start, trim_frame_end = state_manager.get_item('trim_frame_start'), state_manager.get_item('trim_frame_end')\n-\n-\tlogger.info(translator.get('merging_video').format(resolution = pack_resolution(output_video_resolution), fps = output_video_fps), __name__)\n-\tif ffmpeg.merge_video(target_path, temp_video_fps, output_video_resolution, output_video_fps, trim_frame_start, trim_frame_end):\n-\t\tlogger.debug(translator.get('merging_video_succeeded'), __name__)\n-\telse:\n-\t\tif is_process_stopping():\n-\t\t\treturn 4\n-\t\tlogger.error(translator.get('merging_video_failed'), __name__)\n-\t\treturn 1\n-\treturn 0\n+    target_path = state_manager.get_item(\"target_path\")\n+    output_video_fps = state_manager.get_item(\"output_video_fps\")\n+    output_video_resolution = scale_resolution(\n+        detect_image_resolution(state_manager.get_item(\"target_path\")),\n+        state_manager.get_item(\"output_image_scale\"),\n+    )\n+    temp_video_fps = 25\n+    trim_frame_start, trim_frame_end = (\n+        state_manager.get_item(\"trim_frame_start\"),\n+        state_manager.get_item(\"trim_frame_end\"),\n+    )\n+\n+    logger.info(\n+        translator.get(\"merging_video\").format(\n+            resolution=pack_resolution(output_video_resolution), fps=output_video_fps\n+        ),\n+        __name__,\n+    )\n+    if ffmpeg.merge_video(\n+        target_path,\n+        temp_video_fps,\n+        output_video_resolution,\n+        output_video_fps,\n+        trim_frame_start,\n+        trim_frame_end,\n+    ):\n+        logger.debug(translator.get(\"merging_video_succeeded\"), __name__)\n+    else:\n+        if is_process_stopping():\n+            return 4\n+        logger.error(translator.get(\"merging_video_failed\"), __name__)\n+        return 1\n+    return 0\n \n \n def replace_audio() -> ErrorCode:\n-\tsource_audio_path = get_first(filter_audio_paths(state_manager.get_item('source_paths')))\n-\n-\tif ffmpeg.replace_audio(state_manager.get_item('target_path'), source_audio_path, state_manager.get_item('output_path')):\n-\t\tlogger.debug(translator.get('replacing_audio_succeeded'), __name__)\n-\telse:\n-\t\tif is_process_stopping():\n-\t\t\treturn 4\n-\t\tlogger.warn(translator.get('replacing_audio_skipped'), __name__)\n-\t\ttemp_video_path = get_temp_file_path(state_manager.get_item('target_path'))\n-\t\ttemp_video_path = os.path.splitext(temp_video_path)[0] + '.mp4'\n-\t\tmove_file(temp_video_path, state_manager.get_item('output_path'))\n-\treturn 0\n-\n-\n-def finalize_video(start_time : float) -> ErrorCode:\n-\tlogger.debug(translator.get('clearing_temp'), __name__)\n-\tclear_temp_directory(state_manager.get_item('target_path'))\n-\n-\tif is_video(state_manager.get_item('output_path')):\n-\t\tlogger.info(translator.get('processing_video_succeeded').format(seconds = calculate_end_time(start_time)), __name__)\n-\telse:\n-\t\tlogger.error(translator.get('processing_video_failed'), __name__)\n-\t\treturn 1\n-\treturn 0\n+    source_audio_path = get_first(\n+        filter_audio_paths(state_manager.get_item(\"source_paths\"))\n+    )\n+\n+    if ffmpeg.replace_audio(\n+        state_manager.get_item(\"target_path\"),\n+        source_audio_path,\n+        state_manager.get_item(\"output_path\"),\n+    ):\n+        logger.debug(translator.get(\"replacing_audio_succeeded\"), __name__)\n+    else:\n+        if is_process_stopping():\n+            return 4\n+        logger.warn(translator.get(\"replacing_audio_skipped\"), __name__)\n+        temp_video_path = get_temp_file_path(state_manager.get_item(\"target_path\"))\n+        temp_video_path = os.path.splitext(temp_video_path)[0] + \".mp4\"\n+        move_file(temp_video_path, state_manager.get_item(\"output_path\"))\n+    return 0\n+\n+\n+def finalize_video(start_time: float) -> ErrorCode:\n+    logger.debug(translator.get(\"clearing_temp\"), __name__)\n+    clear_temp_directory(state_manager.get_item(\"target_path\"))\n+\n+    if is_video(state_manager.get_item(\"output_path\")):\n+        logger.info(\n+            translator.get(\"processing_video_succeeded\").format(\n+                seconds=calculate_end_time(start_time)\n+            ),\n+            __name__,\n+        )\n+    else:\n+        logger.error(translator.get(\"processing_video_failed\"), __name__)\n+        return 1\n+    return 0\ndiff --git a/facefusion/workflows/core.py b/facefusion/workflows/core.py\nindex e91501e..d4298bf 100644\n--- a/facefusion/workflows/core.py\n+++ b/facefusion/workflows/core.py\n@@ -7,55 +7,78 @@ from facefusion.filesystem import filter_audio_paths, is_video\n from facefusion.processors.core import get_processors_modules\n from facefusion.temp_helper import clear_temp_directory, create_temp_directory\n from facefusion.temp_helper import get_temp_file_path\n-from facefusion.vision import conditional_merge_vision_mask, extract_vision_mask, read_static_image, read_static_images, read_static_video_frame, write_image\n+from facefusion.vision import (\n+    conditional_merge_vision_mask,\n+    extract_vision_mask,\n+    read_static_image,\n+    read_static_images,\n+    read_static_video_frame,\n+    write_image,\n+)\n \n \n def is_process_stopping() -> bool:\n-\tif process_manager.is_stopping():\n-\t\tprocess_manager.end()\n-\t\tlogger.info(translator.get('processing_stopped'), __name__)\n-\treturn process_manager.is_pending()\n-\n-\n-def conditional_process_temp_frame(temp_frame_path : str, frame_number : int, output_video_fps : float) -> bool:\n-\tsource_vision_frames = read_static_images(state_manager.get_item('source_paths'))\n-\tsource_audio_path = get_first(filter_audio_paths(state_manager.get_item('source_paths')))\n-\ttarget_vision_frame = read_static_image(temp_frame_path, 'rgba')\n-\ttemp_vision_frame = target_vision_frame.copy()\n-\ttemp_vision_mask = extract_vision_mask(temp_vision_frame)\n-\n-\tif is_video(state_manager.get_item('target_path')):\n-\t\treference_vision_frame = read_static_video_frame(state_manager.get_item('target_path'), state_manager.get_item('reference_frame_number'))\n-\telse:\n-\t\ttemp_image_path = get_temp_file_path(state_manager.get_item('target_path'))\n-\t\treference_vision_frame = read_static_image(temp_image_path)\n-\n-\tsource_audio_frame = get_audio_frame(source_audio_path, output_video_fps, frame_number)\n-\tsource_voice_frame = get_voice_frame(source_audio_path, output_video_fps, frame_number)\n-\n-\tif not numpy.any(source_audio_frame):\n-\t\tsource_audio_frame = create_empty_audio_frame()\n-\tif not numpy.any(source_voice_frame):\n-\t\tsource_voice_frame = create_empty_audio_frame()\n-\n-\tfor processor_module in get_processors_modules(state_manager.get_item('processors')):\n-\t\ttemp_vision_frame, temp_vision_mask = processor_module.process_frame(\n-\t\t{\n-\t\t\t'reference_vision_frame': reference_vision_frame,\n-\t\t\t'source_vision_frames': source_vision_frames,\n-\t\t\t'source_audio_frame': source_audio_frame,\n-\t\t\t'source_voice_frame': source_voice_frame,\n-\t\t\t'target_vision_frame': target_vision_frame[:, :, :3],\n-\t\t\t'temp_vision_frame': temp_vision_frame[:, :, :3],\n-\t\t\t'temp_vision_mask': temp_vision_mask\n-\t\t})\n-\n-\ttemp_vision_frame = conditional_merge_vision_mask(temp_vision_frame, temp_vision_mask)\n-\treturn write_image(temp_frame_path, temp_vision_frame)\n+    if process_manager.is_stopping():\n+        process_manager.end()\n+        logger.info(translator.get(\"processing_stopped\"), __name__)\n+    return process_manager.is_pending()\n+\n+\n+def conditional_process_temp_frame(\n+    temp_frame_path: str, frame_number: int, output_video_fps: float\n+) -> bool:\n+    source_vision_frames = read_static_images(state_manager.get_item(\"source_paths\"))\n+    source_audio_path = get_first(\n+        filter_audio_paths(state_manager.get_item(\"source_paths\"))\n+    )\n+    target_vision_frame = read_static_image(temp_frame_path, \"rgba\")\n+    temp_vision_frame = target_vision_frame.copy()\n+    temp_vision_mask = extract_vision_mask(temp_vision_frame)\n+\n+    if is_video(state_manager.get_item(\"target_path\")):\n+        reference_vision_frame = read_static_video_frame(\n+            state_manager.get_item(\"target_path\"),\n+            state_manager.get_item(\"reference_frame_number\"),\n+        )\n+    else:\n+        temp_image_path = get_temp_file_path(state_manager.get_item(\"target_path\"))\n+        reference_vision_frame = read_static_image(temp_image_path)\n+\n+    source_audio_frame = get_audio_frame(\n+        source_audio_path, output_video_fps, frame_number\n+    )\n+    source_voice_frame = get_voice_frame(\n+        source_audio_path, output_video_fps, frame_number\n+    )\n+\n+    if not numpy.any(source_audio_frame):\n+        source_audio_frame = create_empty_audio_frame()\n+    if not numpy.any(source_voice_frame):\n+        source_voice_frame = create_empty_audio_frame()\n+\n+    for processor_module in get_processors_modules(\n+        state_manager.get_item(\"processors\")\n+    ):\n+        temp_vision_frame, temp_vision_mask = processor_module.process_frame(\n+            {\n+                \"reference_vision_frame\": reference_vision_frame,\n+                \"source_vision_frames\": source_vision_frames,\n+                \"source_audio_frame\": source_audio_frame,\n+                \"source_voice_frame\": source_voice_frame,\n+                \"target_vision_frame\": target_vision_frame[:, :, :3],\n+                \"temp_vision_frame\": temp_vision_frame[:, :, :3],\n+                \"temp_vision_mask\": temp_vision_mask,\n+            }\n+        )\n+\n+    temp_vision_frame = conditional_merge_vision_mask(\n+        temp_vision_frame, temp_vision_mask\n+    )\n+    return write_image(temp_frame_path, temp_vision_frame)\n \n \n def prepare_temp() -> None:\n-\tlogger.debug(translator.get('clearing_temp'), __name__)\n-\tclear_temp_directory(state_manager.get_item('target_path'))\n-\tlogger.debug(translator.get('creating_temp'), __name__)\n-\tcreate_temp_directory(state_manager.get_item('target_path'))\n+    logger.debug(translator.get(\"clearing_temp\"), __name__)\n+    clear_temp_directory(state_manager.get_item(\"target_path\"))\n+    logger.debug(translator.get(\"creating_temp\"), __name__)\n+    create_temp_directory(state_manager.get_item(\"target_path\"))\ndiff --git a/tests/test_cli_lip_syncer.py b/tests/test_cli_lip_syncer.py\nindex b61a255..f4a7bdc 100644\n--- a/tests/test_cli_lip_syncer.py\n+++ b/tests/test_cli_lip_syncer.py\n@@ -5,36 +5,84 @@ import pytest\n \n from facefusion.download import conditional_download\n from facefusion.jobs.job_manager import clear_jobs, init_jobs\n-from .helper import get_test_example_file, get_test_examples_directory, get_test_jobs_directory, get_test_output_file, is_test_output_file, prepare_test_output_directory\n+from .helper import (\n+    get_test_example_file,\n+    get_test_examples_directory,\n+    get_test_jobs_directory,\n+    get_test_output_file,\n+    is_test_output_file,\n+    prepare_test_output_directory,\n+)\n \n \n-@pytest.fixture(scope = 'module', autouse = True)\n+@pytest.fixture(scope=\"module\", autouse=True)\n def before_all() -> None:\n-\tconditional_download(get_test_examples_directory(),\n-\t[\n-\t\t'https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.jpg',\n-\t\t'https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.mp3',\n-\t\t'https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/target-240p.mp4'\n-\t])\n-\tsubprocess.run([ 'ffmpeg', '-i', get_test_example_file('target-240p.mp4'), '-vframes', '1', get_test_example_file('target-240p.jpg') ])\n+    conditional_download(\n+        get_test_examples_directory(),\n+        [\n+            \"https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.jpg\",\n+            \"https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.mp3\",\n+            \"https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/target-240p.mp4\",\n+        ],\n+    )\n+    subprocess.run(\n+        [\n+            \"ffmpeg\",\n+            \"-i\",\n+            get_test_example_file(\"target-240p.mp4\"),\n+            \"-vframes\",\n+            \"1\",\n+            get_test_example_file(\"target-240p.jpg\"),\n+        ]\n+    )\n \n \n-@pytest.fixture(scope = 'function', autouse = True)\n+@pytest.fixture(scope=\"function\", autouse=True)\n def before_each() -> None:\n-\tclear_jobs(get_test_jobs_directory())\n-\tinit_jobs(get_test_jobs_directory())\n-\tprepare_test_output_directory()\n+    clear_jobs(get_test_jobs_directory())\n+    init_jobs(get_test_jobs_directory())\n+    prepare_test_output_directory()\n \n \n def test_sync_lip_to_image() -> None:\n-\tcommands = [ sys.executable, 'facefusion.py', 'run', '--jobs-path', get_test_jobs_directory(), '--processors', 'lip_syncer', '-s', get_test_example_file('source.mp3'), '-t', get_test_example_file('target-240p.jpg'), '-o', get_test_output_file('test_sync_lip_to_image.jpg') ]\n+    commands = [\n+        sys.executable,\n+        \"facefusion.py\",\n+        \"run\",\n+        \"--jobs-path\",\n+        get_test_jobs_directory(),\n+        \"--processors\",\n+        \"lip_syncer\",\n+        \"-s\",\n+        get_test_example_file(\"source.mp3\"),\n+        \"-t\",\n+        get_test_example_file(\"target-240p.jpg\"),\n+        \"-o\",\n+        get_test_output_file(\"test_sync_lip_to_image.jpg\"),\n+    ]\n \n-\tassert subprocess.run(commands).returncode == 0\n-\tassert is_test_output_file('test_sync_lip_to_image.jpg') is True\n+    assert subprocess.run(commands).returncode == 0\n+    assert is_test_output_file(\"test_sync_lip_to_image.jpg\") is True\n \n \n def test_sync_lip_to_video() -> None:\n-\tcommands = [ sys.executable, 'facefusion.py', 'run', '--jobs-path', get_test_jobs_directory(), '--processors', 'lip_syncer', '-s', get_test_example_file('source.mp3'), '-t', get_test_example_file('target-240p.mp4'), '-o', get_test_output_file('test_sync_lip_to_video.mp4'), '--trim-frame-end', '1' ]\n+    commands = [\n+        sys.executable,\n+        \"facefusion.py\",\n+        \"run\",\n+        \"--jobs-path\",\n+        get_test_jobs_directory(),\n+        \"--processors\",\n+        \"lip_syncer\",\n+        \"-s\",\n+        get_test_example_file(\"source.mp3\"),\n+        \"-t\",\n+        get_test_example_file(\"target-240p.mp4\"),\n+        \"-o\",\n+        get_test_output_file(\"test_sync_lip_to_video.mp4\"),\n+        \"--trim-frame-end\",\n+        \"1\",\n+    ]\n \n-\tassert subprocess.run(commands).returncode == 0\n-\tassert is_test_output_file('test_sync_lip_to_video.mp4') is True\n+    assert subprocess.run(commands).returncode == 0\n+    assert is_test_output_file(\"test_sync_lip_to_video.mp4\") is True\n"
    },
    {
        "id": 309,
        "sha_fail": "064c3b62f835ae5315b5893b3b4bd98b54e34ffa",
        "diff": "diff --git a/cli/cli_app.py b/cli/cli_app.py\nindex 3aab095..0b0627f 100644\n--- a/cli/cli_app.py\n+++ b/cli/cli_app.py\n@@ -48,7 +48,6 @@ class CLIApp:\n         \"\"\"\u6e05\u7406MCP\u5e94\u7528 - \u4f7f\u7528\u5de5\u4f5c\u6d41\u9002\u914d\u5668\"\"\"\n         await self.workflow_adapter.cleanup_mcp_app()\n \n-\n     async def process_input(self, input_source: str, input_type: str):\n         \"\"\"\u5904\u7406\u8f93\u5165\u6e90\uff08URL\u6216\u6587\u4ef6\uff09- \u4f7f\u7528\u5347\u7ea7\u7248\u667a\u80fd\u4f53\u7f16\u6392\u5f15\u64ce\"\"\"\n         try:\ndiff --git a/cli/cli_interface.py b/cli/cli_interface.py\nindex e53773a..df5b60d 100644\n--- a/cli/cli_interface.py\n+++ b/cli/cli_interface.py\n@@ -40,10 +40,10 @@ class CLIInterface:\n         self.is_running = True\n         self.processing_history = []\n         self.enable_indexing = True  # Default configuration\n-        \n+\n         # Load segmentation config from the same source as UI\n         self._load_segmentation_config()\n-        \n+\n         # Initialize tkinter availability\n         self._init_tkinter()\n \n@@ -51,6 +51,7 @@ class CLIInterface:\n         \"\"\"Load segmentation configuration from mcp_agent.config.yaml\"\"\"\n         try:\n             from utils.llm_utils import get_document_segmentation_config\n+\n             seg_config = get_document_segmentation_config()\n             self.segmentation_enabled = seg_config.get(\"enabled\", True)\n             self.segmentation_threshold = seg_config.get(\"size_threshold_chars\", 50000)\n@@ -64,7 +65,7 @@ class CLIInterface:\n         \"\"\"Save segmentation configuration to mcp_agent.config.yaml\"\"\"\n         import yaml\n         import os\n-        \n+\n         # Get the project root directory (where mcp_agent.config.yaml is located)\n         current_file = os.path.abspath(__file__)\n         cli_dir = os.path.dirname(current_file)  # cli directory\n@@ -81,16 +82,22 @@ class CLIInterface:\n                 config[\"document_segmentation\"] = {}\n \n             config[\"document_segmentation\"][\"enabled\"] = self.segmentation_enabled\n-            config[\"document_segmentation\"][\"size_threshold_chars\"] = self.segmentation_threshold\n+            config[\"document_segmentation\"][\"size_threshold_chars\"] = (\n+                self.segmentation_threshold\n+            )\n \n             # Write updated config\n             with open(config_path, \"w\", encoding=\"utf-8\") as f:\n                 yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n \n-            print(f\"{Colors.OKGREEN}\u2705 Document segmentation configuration updated{Colors.ENDC}\")\n+            print(\n+                f\"{Colors.OKGREEN}\u2705 Document segmentation configuration updated{Colors.ENDC}\"\n+            )\n \n         except Exception as e:\n-            print(f\"{Colors.WARNING}\u26a0\ufe0f Failed to update segmentation config: {str(e)}{Colors.ENDC}\")\n+            print(\n+                f\"{Colors.WARNING}\u26a0\ufe0f Failed to update segmentation config: {str(e)}{Colors.ENDC}\"\n+            )\n \n     def _init_tkinter(self):\n         \"\"\"Initialize tkinter availability check\"\"\"\n@@ -790,8 +797,8 @@ class CLIInterface:\n \u2551         \u2717 Smart segmentation (Disabled)                                      \u2551\n \u2551                                                                               \u2551\n \u2551  {Colors.YELLOW}Current Settings:{Colors.CYAN}                                                         \u2551\n-\u2551    Pipeline: {'\ud83e\udde0 Comprehensive Mode' if self.enable_indexing else '\u26a1 Optimized Mode'}                                          \u2551\n-\u2551    Document: {'\ud83d\udcc4 Smart Segmentation' if segmentation_enabled else '\ud83d\udccb Traditional Processing'}                                \u2551\n+\u2551    Pipeline: {\"\ud83e\udde0 Comprehensive Mode\" if self.enable_indexing else \"\u26a1 Optimized Mode\"}                                          \u2551\n+\u2551    Document: {\"\ud83d\udcc4 Smart Segmentation\" if segmentation_enabled else \"\ud83d\udccb Traditional Processing\"}                                \u2551\n \u2551    Threshold: {segmentation_threshold} characters                                    \u2551\n \u2551                                                                               \u2551\n \u2551  {Colors.OKGREEN}[T] Toggle Pipeline    {Colors.BLUE}[S] Toggle Segmentation    {Colors.FAIL}[B] Back{Colors.CYAN}     \u2551\ndiff --git a/workflows/agent_orchestration_engine.py b/workflows/agent_orchestration_engine.py\nindex b766b64..f225a8d 100644\n--- a/workflows/agent_orchestration_engine.py\n+++ b/workflows/agent_orchestration_engine.py\n@@ -65,76 +65,98 @@ os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"  # Prevent .pyc file generation\n def _assess_output_completeness(text: str) -> float:\n     \"\"\"\n     \u667a\u80fd\u8bc4\u4f30\u8f93\u51fa\u5b8c\u6574\u6027\u7684\u9ad8\u7ea7\u7b97\u6cd5\n-    \n+\n     \u4f7f\u7528\u591a\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u68c0\u6d4b\u8f93\u51fa\u662f\u5426\u88ab\u622a\u65ad\uff1a\n     1. \u7ed3\u6784\u5316\u6807\u8bb0\u5b8c\u6574\u6027\u68c0\u67e5\n-    2. \u53e5\u5b50\u5b8c\u6574\u6027\u5206\u6790 \n+    2. \u53e5\u5b50\u5b8c\u6574\u6027\u5206\u6790\n     3. \u4ee3\u7801\u5757\u5b8c\u6574\u6027\u9a8c\u8bc1\n     4. \u9884\u671f\u5185\u5bb9\u5143\u7d20\u68c0\u67e5\n-    \n+\n     Returns:\n         float: \u5b8c\u6574\u6027\u5206\u6570 (0.0-1.0)\uff0c\u8d8a\u9ad8\u8868\u793a\u8d8a\u5b8c\u6574\n     \"\"\"\n     if not text or len(text.strip()) < 100:\n         return 0.0\n-    \n+\n     score = 0.0\n     factors = 0\n-    \n+\n     # 1. \u57fa\u672c\u957f\u5ea6\u68c0\u67e5 (\u6743\u91cd: 0.2)\n     if len(text) > 5000:  # \u671f\u671b\u7684\u6700\u5c0f\u8f93\u51fa\u957f\u5ea6\n         score += 0.2\n     elif len(text) > 2000:\n         score += 0.1\n     factors += 1\n-    \n+\n     # 2. \u7ed3\u6784\u5b8c\u6574\u6027\u68c0\u67e5 (\u6743\u91cd: 0.3)\n     structure_indicators = [\n-        \"## 1.\", \"## 2.\", \"## 3.\",  # \u7ae0\u8282\u6807\u9898\n-        \"```\", \"file_structure\", \"implementation\",\n-        \"algorithm\", \"method\", \"function\"\n+        \"## 1.\",\n+        \"## 2.\",\n+        \"## 3.\",  # \u7ae0\u8282\u6807\u9898\n+        \"```\",\n+        \"file_structure\",\n+        \"implementation\",\n+        \"algorithm\",\n+        \"method\",\n+        \"function\",\n     ]\n-    structure_count = sum(1 for indicator in structure_indicators if indicator.lower() in text.lower())\n+    structure_count = sum(\n+        1 for indicator in structure_indicators if indicator.lower() in text.lower()\n+    )\n     if structure_count >= 6:\n         score += 0.3\n     elif structure_count >= 3:\n         score += 0.15\n     factors += 1\n-    \n+\n     # 3. \u53e5\u5b50\u5b8c\u6574\u6027\u68c0\u67e5 (\u6743\u91cd: 0.2)\n-    lines = text.strip().split('\\n')\n+    lines = text.strip().split(\"\\n\")\n     if lines:\n         last_line = lines[-1].strip()\n         # \u68c0\u67e5\u6700\u540e\u4e00\u884c\u662f\u5426\u662f\u5b8c\u6574\u7684\u53e5\u5b50\u6216\u7ed3\u6784\u5316\u5185\u5bb9\n-        if (last_line.endswith(('.', ':', '```', '!', '?')) or \n-            last_line.startswith(('##', '-', '*', '`')) or\n-            len(last_line) < 10):  # \u5f88\u77ed\u7684\u884c\u53ef\u80fd\u662f\u5217\u8868\u9879\n+        if (\n+            last_line.endswith((\".\", \":\", \"```\", \"!\", \"?\"))\n+            or last_line.startswith((\"##\", \"-\", \"*\", \"`\"))\n+            or len(last_line) < 10\n+        ):  # \u5f88\u77ed\u7684\u884c\u53ef\u80fd\u662f\u5217\u8868\u9879\n             score += 0.2\n-        elif len(last_line) > 50 and not last_line.endswith(('.', ':', '```', '!', '?')):\n+        elif len(last_line) > 50 and not last_line.endswith(\n+            (\".\", \":\", \"```\", \"!\", \"?\")\n+        ):\n             # \u957f\u884c\u4f46\u6ca1\u6709\u9002\u5f53\u7ed3\u5c3e\uff0c\u53ef\u80fd\u88ab\u622a\u65ad\n             score += 0.05\n     factors += 1\n-    \n+\n     # 4. \u4ee3\u7801\u5b9e\u73b0\u8ba1\u5212\u5b8c\u6574\u6027 (\u6743\u91cd: 0.3)\n     implementation_keywords = [\n-        \"file structure\", \"architecture\", \"implementation\", \n-        \"requirements\", \"dependencies\", \"setup\", \"main\",\n-        \"class\", \"function\", \"method\", \"algorithm\"\n+        \"file structure\",\n+        \"architecture\",\n+        \"implementation\",\n+        \"requirements\",\n+        \"dependencies\",\n+        \"setup\",\n+        \"main\",\n+        \"class\",\n+        \"function\",\n+        \"method\",\n+        \"algorithm\",\n     ]\n-    impl_count = sum(1 for keyword in implementation_keywords if keyword.lower() in text.lower())\n+    impl_count = sum(\n+        1 for keyword in implementation_keywords if keyword.lower() in text.lower()\n+    )\n     if impl_count >= 8:\n         score += 0.3\n     elif impl_count >= 4:\n         score += 0.15\n     factors += 1\n-    \n+\n     return min(score, 1.0)  # \u786e\u4fdd\u4e0d\u8d85\u8fc71.0\n \n \n def _adjust_params_for_retry(params: RequestParams, retry_count: int) -> RequestParams:\n     \"\"\"\n     \u52a8\u6001\u8c03\u6574\u8bf7\u6c42\u53c2\u6570\u4ee5\u63d0\u9ad8\u6210\u529f\u7387\n-    \n+\n     \u57fa\u4e8e\u91cd\u8bd5\u6b21\u6570\u667a\u80fd\u8c03\u6574\u53c2\u6570\uff1a\n     - \u589e\u52a0token\u9650\u5236\n     - \u8c03\u6574temperature\n@@ -142,15 +164,17 @@ def _adjust_params_for_retry(params: RequestParams, retry_count: int) -> Request\n     \"\"\"\n     # \u57fa\u7840token\u589e\u91cf\uff1a\u6bcf\u6b21\u91cd\u8bd5\u589e\u52a0\u66f4\u591atokens\n     token_increment = 4096 * (retry_count + 1)\n-    new_max_tokens = min(params.max_tokens + token_increment, 32768)  # \u4e0d\u8d85\u8fc732K\u7684\u5408\u7406\u9650\u5236\n-    \n+    new_max_tokens = min(\n+        params.max_tokens + token_increment, 32768\n+    )  # \u4e0d\u8d85\u8fc732K\u7684\u5408\u7406\u9650\u5236\n+\n     # \u968f\u7740\u91cd\u8bd5\u6b21\u6570\u589e\u52a0\uff0c\u964d\u4f4etemperature\u4ee5\u83b7\u5f97\u66f4\u4e00\u81f4\u7684\u8f93\u51fa\n     new_temperature = max(params.temperature - (retry_count * 0.1), 0.1)\n-    \n+\n     print(f\"\ud83d\udd27 Adjusting parameters for retry {retry_count + 1}:\")\n     print(f\"   Token limit: {params.max_tokens} \u2192 {new_max_tokens}\")\n     print(f\"   Temperature: {params.temperature} \u2192 {new_temperature}\")\n-    \n+\n     return RequestParams(\n         max_tokens=new_max_tokens,\n         temperature=new_temperature,\n@@ -483,13 +507,15 @@ async def run_code_analyzer(\n         # \u5206\u6bb5\u6a21\u5f0f\uff1a\u53ef\u4ee5\u4f7f\u7528\u66f4\u9ad8\u7684token\u9650\u5236\uff0c\u56e0\u4e3a\u8f93\u5165\u5df2\u7ecf\u88ab\u4f18\u5316\n         max_tokens_limit = 16384  # \u4f7f\u7528\u66f4\u9ad8\u9650\u5236\uff0c\u56e0\u4e3a\u5206\u6bb5\u51cf\u5c11\u4e86\u8f93\u5165\u590d\u6742\u6027\n         temperature = 0.2  # \u7a0d\u5fae\u964d\u4f4etemperature\u4ee5\u63d0\u9ad8\u4e00\u81f4\u6027\n-        print(\"\ud83e\udde0 Using SEGMENTED mode: Higher token limit (16384) with optimized inputs\")\n+        print(\n+            \"\ud83e\udde0 Using SEGMENTED mode: Higher token limit (16384) with optimized inputs\"\n+        )\n     else:\n         # \u4f20\u7edf\u6a21\u5f0f\uff1a\u4f7f\u7528\u4fdd\u5b88\u7684token\u9650\u5236\u5e76\u542f\u7528\u589e\u91cf\u751f\u6210\n         max_tokens_limit = 12288  # \u4e2d\u7b49\u9650\u5236\uff0c\u4e3a\u805a\u5408\u8f93\u51fa\u7559\u51fa\u7a7a\u95f4\n         temperature = 0.3\n         print(\"\ud83e\udde0 Using TRADITIONAL mode: Moderate token limit (12288)\")\n-    \n+\n     enhanced_params = RequestParams(\n         max_tokens=max_tokens_limit,\n         temperature=temperature,\n@@ -509,33 +535,39 @@ The goal is to create a reproduction plan detailed enough for independent implem\n     # \u667a\u80fd\u8f93\u51fa\u5b8c\u6574\u6027\u68c0\u67e5\u548c\u91cd\u8bd5\u673a\u5236\n     max_retries = 3\n     retry_count = 0\n-    \n+\n     while retry_count < max_retries:\n         try:\n-            print(f\"\ud83d\ude80 Attempting code analysis (attempt {retry_count + 1}/{max_retries})\")\n+            print(\n+                f\"\ud83d\ude80 Attempting code analysis (attempt {retry_count + 1}/{max_retries})\"\n+            )\n             result = await code_aggregator_agent.generate_str(\n                 message=message, request_params=enhanced_params\n             )\n-            \n+\n             # \u68c0\u67e5\u8f93\u51fa\u5b8c\u6574\u6027\u7684\u9ad8\u7ea7\u6307\u6807\n             completeness_score = _assess_output_completeness(result)\n             print(f\"\ud83d\udcca Output completeness score: {completeness_score:.2f}/1.0\")\n-            \n+\n             if completeness_score >= 0.8:  # \u8f93\u51fa\u88ab\u8ba4\u4e3a\u662f\u5b8c\u6574\u7684\n-                print(f\"\u2705 Code analysis completed successfully (length: {len(result)} chars)\")\n+                print(\n+                    f\"\u2705 Code analysis completed successfully (length: {len(result)} chars)\"\n+                )\n                 return result\n             else:\n-                print(f\"\u26a0\ufe0f Output appears truncated (score: {completeness_score:.2f}), retrying with enhanced parameters...\")\n+                print(\n+                    f\"\u26a0\ufe0f Output appears truncated (score: {completeness_score:.2f}), retrying with enhanced parameters...\"\n+                )\n                 # \u52a8\u6001\u8c03\u6574\u53c2\u6570\u8fdb\u884c\u91cd\u8bd5\n                 enhanced_params = _adjust_params_for_retry(enhanced_params, retry_count)\n                 retry_count += 1\n-                \n+\n         except Exception as e:\n             print(f\"\u274c Error in code analysis attempt {retry_count + 1}: {e}\")\n             retry_count += 1\n             if retry_count >= max_retries:\n                 raise\n-    \n+\n     # \u5982\u679c\u6240\u6709\u91cd\u8bd5\u90fd\u5931\u8d25\uff0c\u8fd4\u56de\u6700\u540e\u4e00\u6b21\u7684\u7ed3\u679c\n     print(f\"\u26a0\ufe0f Returning potentially incomplete result after {max_retries} attempts\")\n     return result\n"
    },
    {
        "id": 310,
        "sha_fail": "c9c582cbef0e49bb73fb65cb83042e2e5eb849b1",
        "diff": "diff --git a/setup/unix-ci.py b/setup/unix-ci.py\nindex 8090e03..0ed1ee4 100644\n--- a/setup/unix-ci.py\n+++ b/setup/unix-ci.py\n@@ -23,17 +23,13 @@ def setenv(key, val):\n \n \n if ismacos:\n-\n     SWBASE = '/Users/Shared/calibre-build/sw'\n     SW = SWBASE + '/sw'\n \n     def install_env():\n         setenv('SWBASE', SWBASE)\n         setenv('SW', SW)\n-        setenv(\n-            'PATH',\n-            '$SW/bin:$SW/qt/bin:$SW/python/Python.framework/Versions/2.7/bin:$PWD/node_modules/.bin:$PATH'\n-        )\n+        setenv('PATH', '$SW/bin:$SW/qt/bin:$SW/python/Python.framework/Versions/2.7/bin:$PWD/node_modules/.bin:$PATH')\n         setenv('CFLAGS', '-I$SW/include')\n         setenv('LDFLAGS', '-L$SW/lib')\n         setenv('QMAKE', '$SW/qt/bin/qmake')\n@@ -44,8 +40,8 @@ def install_env():\n             old += ':'\n         setenv('DYLD_FALLBACK_LIBRARY_PATH', old + '$SW/lib')\n         setenv('CALIBRE_ESPEAK_DATA_DIR', '$SW/share/espeak-ng-data')\n-else:\n \n+else:\n     SWBASE = '/sw'\n     SW = SWBASE + '/sw'\n \n@@ -116,8 +112,21 @@ def run_python(*args):\n def install_linux_deps():\n     run('sudo', 'apt-get', 'update', '-y')\n     # run('sudo', 'apt-get', 'upgrade', '-y')\n-    run('sudo', 'apt-get', 'install', '-y',\n-        'gettext', 'libgl1-mesa-dev', 'libxkbcommon-dev', 'libxkbcommon-x11-dev', 'libfreetype-dev', 'pulseaudio', 'libasound2t64', 'libflite1', 'libspeechd2')\n+    run(\n+        'sudo',\n+        'apt-get',\n+        'install',\n+        '-y',\n+        'gettext',\n+        'libgl1-mesa-dev',\n+        'libxkbcommon-dev',\n+        'libxkbcommon-x11-dev',\n+        'libfreetype-dev',\n+        'pulseaudio',\n+        'libasound2',\n+        'libflite1',\n+        'libspeechd2',\n+    )\n \n \n def get_tx():\n@@ -132,6 +141,7 @@ def get_tx():\n def main():\n     if iswindows:\n         import runpy\n+\n         m = runpy.run_path('setup/win-ci.py')\n         return m['main']()\n     action = sys.argv[1]\n@@ -140,9 +150,7 @@ def main():\n         run('sudo', 'chown', '-R', os.environ['USER'], SWBASE)\n \n         tball = 'macos-64' if ismacos else 'linux-64'\n-        download_and_decompress(\n-            f'https://download.calibre-ebook.com/ci/calibre7/{tball}.tar.xz', SW\n-        )\n+        download_and_decompress(f'https://download.calibre-ebook.com/ci/calibre7/{tball}.tar.xz', SW)\n         if not ismacos:\n             install_linux_deps()\n \n@@ -151,7 +159,7 @@ def main():\n         run_python('setup.py bootstrap --ephemeral')\n \n     elif action == 'pot':\n-        transifexrc = '''\\\n+        transifexrc = \"\"\"\\\n [https://www.transifex.com]\n api_hostname  = https://api.transifex.com\n rest_hostname = https://rest.api.transifex.com\n@@ -159,7 +167,7 @@ def main():\n password = PASSWORD\n token = PASSWORD\n username = api\n-'''.replace('PASSWORD', os.environ['tx'])\n+\"\"\".replace('PASSWORD', os.environ['tx'])\n         with open(os.path.expanduser('~/.transifexrc'), 'w') as f:\n             f.write(transifexrc)\n         install_qt_source_code()\n@@ -172,8 +180,7 @@ def main():\n         os.environ['OPENSSL_MODULES'] = os.path.join(SW, 'lib', 'ossl-modules')\n         os.environ['PIPER_TTS_DIR'] = os.path.join(SW, 'piper')\n         if ismacos:\n-            os.environ['SSL_CERT_FILE'] = os.path.abspath(\n-                'resources/mozilla-ca-certs.pem')\n+            os.environ['SSL_CERT_FILE'] = os.path.abspath('resources/mozilla-ca-certs.pem')\n             # needed to ensure correct libxml2 is loaded\n             os.environ['DYLD_INSERT_LIBRARIES'] = ':'.join(os.path.join(SW, 'lib', x) for x in 'libxml2.dylib libxslt.dylib libexslt.dylib'.split())\n             os.environ['OPENSSL_ENGINES'] = os.path.join(SW, 'lib', 'engines-3')\n"
    },
    {
        "id": 311,
        "sha_fail": "496bc88f08677236887552428dea181ff16e7e46",
        "diff": "diff --git a/src/calibre/ebooks/oeb/polish/tests/kepubify.py b/src/calibre/ebooks/oeb/polish/tests/kepubify.py\nindex 05008d8..fa8e85f 100644\n--- a/src/calibre/ebooks/oeb/polish/tests/kepubify.py\n+++ b/src/calibre/ebooks/oeb/polish/tests/kepubify.py\n@@ -21,7 +21,6 @@\n \n \n class KepubifyTests(BaseTest):\n-\n     def test_kepubify(self):\n         def b(has_cover=True, epub_version='3'):\n             path = get_book_for_kepubify(has_cover=has_cover, epub_version=epub_version)\n@@ -40,8 +39,7 @@ def b(has_cover=True, epub_version='3'):\n             outpath = unkepubify_path(outpath)\n             expected = get_container(path, tweak_mode=True)\n             actual = get_container(outpath, tweak_mode=True)\n-            self.assertEqual(\n-                tuple(expected.manifest_items_with_property('cover-image')), tuple(actual.manifest_items_with_property('cover-image')))\n+            self.assertEqual(tuple(expected.manifest_items_with_property('cover-image')), tuple(actual.manifest_items_with_property('cover-image')))\n             self.assertEqual(expected.mime_map, actual.mime_map)\n             for name, mt in expected.mime_map.items():\n                 if mt in OEB_DOCS or mt in OEB_STYLES or name.endswith('.opf'):\n@@ -56,13 +54,13 @@ def test_kepubify_html(self):\n <html xmlns=\"http://www.w3.org/1999/xhtml\"><head><style type=\"text/css\" id=\"kobostylehacks\">\\\n div#book-inner {{ margin-top: 0; margin-bottom: 0; }}</style><script type=\"text/javascript\" src=\"{KOBO_JS_NAME}\"/></head>\\\n <body><div id=\"book-columns\"><div id=\"book-inner\">'''\n-        suffix =  '</div></div></body></html>'\n+        suffix = '</div></div></body></html>'\n \n         def perform(src, expected, prefer_justification):\n             opts = Options(remove_widows_and_orphans=True, remove_at_page_rules=True, prefer_justification=prefer_justification)\n             root = kepubify_html_data(src, KOBO_JS_NAME, opts)\n             actual = serialize_html(root).decode('utf-8')\n-            actual = actual[len(prefix):-len(suffix)]\n+            actual = actual[len(prefix) : -len(suffix)]\n             self.assertEqual(expected, actual, f'\\n\\nText:\\n{src}\\n\\nExpected:\\n{expected}\\n\\nActual:\\n{actual}')\n             expected = serialize_html(parse(src)).decode('utf-8')\n             opts = opts._replace(for_removal=True)\n@@ -72,77 +70,63 @@ def perform(src, expected, prefer_justification):\n \n         for src, expected in {\n             # basics\n-            '<p>one</p>  <p>\\xa0</p><p>\\xa0<i>a</i></p>':\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">one</span></p>  <p><span class=\"koboSpan\" id=\"kobo.2.1\">&#160;</span></p>'\n+            '<p>one</p>  <p>\\xa0</p><p>\\xa0<i>a</i></p>': '<p><span class=\"koboSpan\" id=\"kobo.1.1\">one</span></p>  <p><span class=\"koboSpan\" id=\"kobo.2.1\">&#160;</span></p>'\n             '<p>&#160;<i><span class=\"koboSpan\" id=\"kobo.3.1\">a</span></i></p>',\n-\n-            '<p>Simple sentences. In a single paragraph.'\n-            '<p>A sentence <i>with <b>nested</b>, tailed</i> formatting. Another.': (\n-\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">Simple sentences. </span><span class=\"koboSpan\" id=\"kobo.1.2\">In a single paragraph.</span></p>'\n-            '<p><span class=\"koboSpan\" id=\"kobo.2.1\">A sentence </span><i><span class=\"koboSpan\" id=\"kobo.2.2\">with </span>'\n-            '<b><span class=\"koboSpan\" id=\"kobo.2.3\">nested</span></b><span class=\"koboSpan\" id=\"kobo.2.4\">, tailed</span></i>'\n-            '<span class=\"koboSpan\" id=\"kobo.2.5\"> formatting. </span>'\n-            '<span class=\"koboSpan\" id=\"kobo.2.6\">Another.</span></p>',\n-\n-            # with prefer_justification\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">Simple sentences.</span> <span class=\"koboSpan\" id=\"kobo.1.2\">In a single paragraph.</span></p>'\n-            '<p><span class=\"koboSpan\" id=\"kobo.2.1\">A sentence</span> <i><span class=\"koboSpan\" id=\"kobo.2.2\">with</span>'\n-            ' <b><span class=\"koboSpan\" id=\"kobo.2.3\">nested</span></b><span class=\"koboSpan\" id=\"kobo.2.4\">, tailed</span></i> '\n-            '<span class=\"koboSpan\" id=\"kobo.2.5\">formatting.</span> '\n-            '<span class=\"koboSpan\" id=\"kobo.2.6\">Another.</span></p>',\n+            '<p>Simple sentences. In a single paragraph.<p>A sentence <i>with <b>nested</b>, tailed</i> formatting. Another.': (\n+                '<p><span class=\"koboSpan\" id=\"kobo.1.1\">Simple sentences. </span><span class=\"koboSpan\" id=\"kobo.1.2\">In a single paragraph.</span></p>'\n+                '<p><span class=\"koboSpan\" id=\"kobo.2.1\">A sentence </span><i><span class=\"koboSpan\" id=\"kobo.2.2\">with </span>'\n+                '<b><span class=\"koboSpan\" id=\"kobo.2.3\">nested</span></b><span class=\"koboSpan\" id=\"kobo.2.4\">, tailed</span></i>'\n+                '<span class=\"koboSpan\" id=\"kobo.2.5\"> formatting. </span>'\n+                '<span class=\"koboSpan\" id=\"kobo.2.6\">Another.</span></p>',\n+                # with prefer_justification\n+                '<p><span class=\"koboSpan\" id=\"kobo.1.1\">Simple sentences.</span> <span class=\"koboSpan\" id=\"kobo.1.2\">In a single paragraph.</span></p>'\n+                '<p><span class=\"koboSpan\" id=\"kobo.2.1\">A sentence</span> <i><span class=\"koboSpan\" id=\"kobo.2.2\">with</span>'\n+                ' <b><span class=\"koboSpan\" id=\"kobo.2.3\">nested</span></b><span class=\"koboSpan\" id=\"kobo.2.4\">, tailed</span></i> '\n+                '<span class=\"koboSpan\" id=\"kobo.2.5\">formatting.</span> '\n+                '<span class=\"koboSpan\" id=\"kobo.2.6\">Another.</span></p>',\n             ),\n-\n             # img tags\n-            '<p>An image<img src=\"x\">with tail<img src=\"b\"><i>without':\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">An image</span><span class=\"koboSpan\" id=\"kobo.2.1\"><img src=\"x\"/></span>'\n+            '<p>An image<img src=\"x\">with tail<img src=\"b\"><i>without': '<p><span class=\"koboSpan\" id=\"kobo.1.1\">An image</span><span class=\"koboSpan\" id=\"kobo.2.1\"><img src=\"x\"/></span>'\n             '<span class=\"koboSpan\" id=\"kobo.2.2\">with tail</span>'\n             '<span class=\"koboSpan\" id=\"kobo.3.1\"><img src=\"b\"/></span><i><span class=\"koboSpan\" id=\"kobo.3.2\">without</span></i></p>',\n-\n             # comments\n             '<p>A comment<!-- xx -->with tail'\n-            '<p>A comment<!-- xx --><i>without tail':\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">A comment</span><!-- xx --><span class=\"koboSpan\" id=\"kobo.1.2\">with tail</span></p>'\n+            '<p>A comment<!-- xx --><i>without tail': '<p><span class=\"koboSpan\" id=\"kobo.1.1\">A comment</span><!-- xx --><span class=\"koboSpan\" id=\"kobo.1.2\">with tail</span></p>'\n             '<p><span class=\"koboSpan\" id=\"kobo.2.1\">A comment</span><!-- xx --><i><span class=\"koboSpan\" id=\"kobo.2.2\">without tail</span></i></p>',\n-\n             # nested block tags\n             '<div>A div<div> nested.<ul><li>A list<p> with nested block</p> tail1</li> tail2</ul> tail3': (\n-            '<div><span class=\"koboSpan\" id=\"kobo.1.1\">A div</span><div><span class=\"koboSpan\" id=\"kobo.1.2\"> nested.</span>'\n-            '<ul><li><span class=\"koboSpan\" id=\"kobo.2.1\">A list</span><p><span class=\"koboSpan\" id=\"kobo.3.1\"> with nested block</span></p>'\n-            '<span class=\"koboSpan\" id=\"kobo.3.2\"> tail1</span></li><span class=\"koboSpan\" id=\"kobo.3.3\"> tail2</span></ul>'\n-            '<span class=\"koboSpan\" id=\"kobo.3.4\"> tail3</span></div></div>',\n-            # with prefer_justification\n-            '<div><span class=\"koboSpan\" id=\"kobo.1.1\">A div</span><div> <span class=\"koboSpan\" id=\"kobo.1.2\">nested.</span>'\n-            '<ul><li><span class=\"koboSpan\" id=\"kobo.2.1\">A list</span><p> <span class=\"koboSpan\" id=\"kobo.3.1\">with nested block</span></p>'\n-            ' <span class=\"koboSpan\" id=\"kobo.3.2\">tail1</span></li> <span class=\"koboSpan\" id=\"kobo.3.3\">tail2</span></ul>'\n-            ' <span class=\"koboSpan\" id=\"kobo.3.4\">tail3</span></div></div>',\n+                '<div><span class=\"koboSpan\" id=\"kobo.1.1\">A div</span><div><span class=\"koboSpan\" id=\"kobo.1.2\"> nested.</span>'\n+                '<ul><li><span class=\"koboSpan\" id=\"kobo.2.1\">A list</span><p><span class=\"koboSpan\" id=\"kobo.3.1\"> with nested block</span></p>'\n+                '<span class=\"koboSpan\" id=\"kobo.3.2\"> tail1</span></li><span class=\"koboSpan\" id=\"kobo.3.3\"> tail2</span></ul>'\n+                '<span class=\"koboSpan\" id=\"kobo.3.4\"> tail3</span></div></div>',\n+                # with prefer_justification\n+                '<div><span class=\"koboSpan\" id=\"kobo.1.1\">A div</span><div> <span class=\"koboSpan\" id=\"kobo.1.2\">nested.</span>'\n+                '<ul><li><span class=\"koboSpan\" id=\"kobo.2.1\">A list</span><p> <span class=\"koboSpan\" id=\"kobo.3.1\">with nested block</span></p>'\n+                ' <span class=\"koboSpan\" id=\"kobo.3.2\">tail1</span></li> <span class=\"koboSpan\" id=\"kobo.3.3\">tail2</span></ul>'\n+                ' <span class=\"koboSpan\" id=\"kobo.3.4\">tail3</span></div></div>',\n             ),\n-\n             # skipped tags\n             '<div>Script: <script>a = 1</script> with tail': (\n-            '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Script: </span><script>a = 1</script><span class=\"koboSpan\" id=\"kobo.1.2\"> with tail</span></div>',\n-            '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Script:</span> <script>a = 1</script> <span class=\"koboSpan\" id=\"kobo.1.2\">with tail</span></div>',\n+                '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Script: </span><script>a = 1</script><span class=\"koboSpan\" id=\"kobo.1.2\"> with tail</span></div>',\n+                '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Script:</span> <script>a = 1</script> <span class=\"koboSpan\" id=\"kobo.1.2\">with tail</span></div>',\n             ),\n             '<div>Svg: <svg>mouse</svg><i> no tail': (\n-            '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Svg: </span><svg xmlns=\"http://www.w3.org/2000/svg\">mouse</svg>'\n-            '<i><span class=\"koboSpan\" id=\"kobo.1.2\"> no tail</span></i></div>',\n-            '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Svg:</span> <svg xmlns=\"http://www.w3.org/2000/svg\">mouse</svg>'\n-            '<i> <span class=\"koboSpan\" id=\"kobo.1.2\">no tail</span></i></div>',\n+                '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Svg: </span><svg xmlns=\"http://www.w3.org/2000/svg\">mouse</svg>'\n+                '<i><span class=\"koboSpan\" id=\"kobo.1.2\"> no tail</span></i></div>',\n+                '<div><span class=\"koboSpan\" id=\"kobo.1.1\">Svg:</span> <svg xmlns=\"http://www.w3.org/2000/svg\">mouse</svg>'\n+                '<i> <span class=\"koboSpan\" id=\"kobo.1.2\">no tail</span></i></div>',\n             ),\n-\n             # encoding quirks\n             '<p>A\\xa0nbsp;&nbsp;': (\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">A&#160;nbsp;&#160;</span></p>',\n-            '<p><span class=\"koboSpan\" id=\"kobo.1.1\">A&#160;nbsp;</span>&#160;</p>',\n+                '<p><span class=\"koboSpan\" id=\"kobo.1.1\">A&#160;nbsp;&#160;</span></p>',\n+                '<p><span class=\"koboSpan\" id=\"kobo.1.1\">A&#160;nbsp;</span>&#160;</p>',\n             ),\n             '<div><script>1 < 2 & 3</script>':  # escaping with cdata note that kepubify doesn't do this\n             '<div><script><![CDATA[1 < 2 & 3]]></script></div>',\n-\n             # CSS filtering\n-            '<div><style>@page {\\n  margin: 13px;\\n}\\ndiv {\\n  widows: 12;\\n  color: red;\\n}</style>Some</div>':\n-            f'<div><style>/* {CSS_COMMENT_COOKIE}: @page {{\\n  margin: 13px;\\n}} */\\n'\n+            '<div><style>@page {\\n  margin: 13px;\\n}\\ndiv {\\n  widows: 12;\\n  color: red;\\n}</style>Some</div>': f'<div><style>/* {CSS_COMMENT_COOKIE}: @page {{\\n  margin: 13px;\\n}} */\\n'\n             f'div {{\\n  -{CSS_COMMENT_COOKIE}-widows: 12;\\n  color: red;\\n}}</style>'\n-            '<span class=\"koboSpan\" id=\"kobo.1.1\">Some</span></div>'\n+            '<span class=\"koboSpan\" id=\"kobo.1.1\">Some</span></div>',\n         }.items():\n             if isinstance(expected, str):\n                 expected = expected, expected\ndiff --git a/src/calibre/library/database2.py b/src/calibre/library/database2.py\nindex fe8f367..6f4d7d3 100644\n--- a/src/calibre/library/database2.py\n+++ b/src/calibre/library/database2.py\n@@ -1,10 +1,10 @@\n-__license__   = 'GPL v3'\n+__license__ = 'GPL v3'\n __copyright__ = '2008, Kovid Goyal kovid@kovidgoyal.net'\n __docformat__ = 'restructuredtext en'\n \n-'''\n+\"\"\"\n The database used to store ebook metadata\n-'''\n+\"\"\"\n \n import copy\n import functools\n@@ -58,13 +58,12 @@\n from polyglot.builtins import iteritems, string_or_bytes\n \n copyfile = os.link if hasattr(os, 'link') else shutil.copyfile\n-SPOOL_SIZE = 30*1024*1024\n+SPOOL_SIZE = 30 * 1024 * 1024\n \n ProxyMetadata = namedtuple('ProxyMetadata', 'book_size ondevice_col db_approx_formats')\n \n \n class DBPrefsWrapper:\n-\n     def __init__(self, db):\n         self.db = db\n         self.new_api = self\n@@ -77,15 +76,16 @@ def set_pref(self, name, val):\n \n \n class LibraryDatabase2(LibraryDatabase, SchemaUpgrade, CustomColumns):\n-    '''\n+    \"\"\"\n     An ebook metadata database that stores references to ebook files on disk.\n-    '''\n+    \"\"\"\n+\n     PATH_LIMIT = 40 if 'win32' in sys.platform else 100\n     WINDOWS_LIBRARY_PATH_LIMIT = 75\n \n     @property\n     def user_version(self):\n-        'The user version of this database'\n+        \"The user version of this database\"\n         return self.conn.get('pragma user_version;', all=False)\n \n     @user_version.setter\n@@ -95,7 +95,7 @@ def user_version(self, val):\n \n     @property\n     def library_id(self):\n-        '''The UUID for this library. As long as the user only operates on libraries with calibre, it will be unique'''\n+        \"\"\"The UUID for this library. As long as the user only operates on libraries with calibre, it will be unique\"\"\"\n         if self._library_id_ is None:\n             ans = self.conn.get('SELECT uuid FROM library_id', all=False)\n             if ans is None:\n@@ -115,19 +115,14 @@ def library_id(self, val):\n         self.conn.commit()\n \n     def connect(self):\n-        if iswindows and len(self.library_path) + 4*self.PATH_LIMIT + 10 > 259:\n-            raise ValueError(_(\n-                'Path to library too long. Must be less than'\n-                ' %d characters.')%(259-4*self.PATH_LIMIT-10))\n+        if iswindows and len(self.library_path) + 4 * self.PATH_LIMIT + 10 > 259:\n+            raise ValueError(_('Path to library too long. Must be less than %d characters.') % (259 - 4 * self.PATH_LIMIT - 10))\n         exists = os.path.exists(self.dbpath)\n         if not exists:\n             # Be more strict when creating new libraries as the old calculation\n             # allowed for max path lengths of 265 chars.\n-            if (iswindows and len(self.library_path) >\n-                    self.WINDOWS_LIBRARY_PATH_LIMIT):\n-                raise ValueError(_(\n-                    'Path to library too long. Must be less than'\n-                    ' %d characters.')%self.WINDOWS_LIBRARY_PATH_LIMIT)\n+            if iswindows and len(self.library_path) > self.WINDOWS_LIBRARY_PATH_LIMIT:\n+                raise ValueError(_('Path to library too long. Must be less than %d characters.') % self.WINDOWS_LIBRARY_PATH_LIMIT)\n \n         self.conn = connect(self.dbpath, self.row_factory)\n         if exists and self.user_version == 0:\n@@ -150,12 +145,19 @@ def exists_at(cls, path):\n \n     get_data_as_dict = get_data_as_dict\n \n-    def __init__(self, library_path, row_factory=False, default_prefs=None,\n-            read_only=False, is_second_db=False, progress_callback=None,\n-            restore_all_prefs=False, temp_db_path=None):\n+    def __init__(\n+        self,\n+        library_path,\n+        row_factory=False,\n+        default_prefs=None,\n+        read_only=False,\n+        is_second_db=False,\n+        progress_callback=None,\n+        restore_all_prefs=False,\n+        temp_db_path=None,\n+    ):\n         if temp_db_path is not None:\n-            raise ValueError(\"temp_db_path isn't supported in by the obsolete \"\n-                             \"library.database2. Use db.legacy.LibraryDatabase instead\")\n+            raise ValueError(\"temp_db_path isn't supported in by the obsolete library.database2. Use db.legacy.LibraryDatabase instead\")\n         self.is_second_db = is_second_db\n         try:\n             if isbytestring(library_path):\n@@ -174,8 +176,7 @@ def __init__(self, library_path, row_factory=False, default_prefs=None,\n         self.library_path = os.path.abspath(library_path)\n         self.row_factory = row_factory\n         self.dbpath = os.path.join(library_path, 'metadata.db')\n-        self.dbpath = os.environ.get('CALIBRE_OVERRIDE_DATABASE_PATH',\n-                self.dbpath)\n+        self.dbpath = os.environ.get('CALIBRE_OVERRIDE_DATABASE_PATH', self.dbpath)\n \n         if read_only and os.path.exists(self.dbpath):\n             # Work on only a copy of metadata.db to ensure that\n@@ -188,9 +189,7 @@ def __init__(self, library_path, row_factory=False, default_prefs=None,\n         apply_default_prefs = not os.path.exists(self.dbpath)\n         self.connect()\n \n-        self.is_case_sensitive = (not iswindows and\n-            not os.path.exists(self.dbpath.replace('metadata.db',\n-                'MeTAdAtA.dB')))\n+        self.is_case_sensitive = not iswindows and not os.path.exists(self.dbpath.replace('metadata.db', 'MeTAdAtA.dB'))\n         SchemaUpgrade.__init__(self)\n         # Guarantee that the library_id is set\n         self.library_id\n@@ -199,8 +198,10 @@ def __init__(self, library_path, row_factory=False, default_prefs=None,\n         # we need to do it before we call initialize_dynamic\n         if apply_default_prefs and default_prefs is not None:\n             if progress_callback is None:\n+\n                 def progress_callback(x, y):\n                     return True\n+\n             dbprefs = DBPrefs(self)\n             progress_callback(None, len(default_prefs))\n             for i, key in enumerate(default_prefs):\n@@ -208,15 +209,15 @@ def progress_callback(x, y):\n                 if not restore_all_prefs and key in frozenset(['news_to_be_synced']):\n                     continue\n                 dbprefs[key] = default_prefs[key]\n-                progress_callback(_('restored preference ') + key, i+1)\n+                progress_callback(_('restored preference ') + key, i + 1)\n             if 'field_metadata' in default_prefs:\n                 fmvals = [f for f in default_prefs['field_metadata'].values() if f['is_custom']]\n                 progress_callback(None, len(fmvals))\n                 for i, f in enumerate(fmvals):\n                     progress_callback(_('creating custom column ') + f['label'], i)\n-                    self.create_custom_column(f['label'], f['name'], f['datatype'],\n-                            f['is_multiple'] is not None and len(f['is_multiple']) > 0,\n-                            f['is_editable'], f['display'])\n+                    self.create_custom_column(\n+                        f['label'], f['name'], f['datatype'], f['is_multiple'] is not None and len(f['is_multiple']) > 0, f['is_editable'], f['display']\n+                    )\n         self.initialize_template_cache()\n         self.initialize_dynamic()\n \n@@ -246,33 +247,46 @@ def initialize_dynamic(self):\n         defs['similar_series_search_key'] = 'series'\n         defs['similar_series_match_kind'] = 'match_any'\n         defs['book_display_fields'] = [\n-        ('title', False), ('authors', True), ('formats', True),\n-        ('series', True), ('identifiers', True), ('tags', True),\n-        ('path', True), ('publisher', False), ('rating', False),\n-        ('author_sort', False), ('sort', False), ('timestamp', False),\n-        ('uuid', False), ('comments', True), ('id', False), ('pubdate', False),\n-        ('last_modified', False), ('size', False), ('languages', False),\n+            ('title', False),\n+            ('authors', True),\n+            ('formats', True),\n+            ('series', True),\n+            ('identifiers', True),\n+            ('tags', True),\n+            ('path', True),\n+            ('publisher', False),\n+            ('rating', False),\n+            ('author_sort', False),\n+            ('sort', False),\n+            ('timestamp', False),\n+            ('uuid', False),\n+            ('comments', True),\n+            ('id', False),\n+            ('pubdate', False),\n+            ('last_modified', False),\n+            ('size', False),\n+            ('languages', False),\n         ]\n         defs['virtual_libraries'] = {}\n         defs['virtual_lib_on_startup'] = defs['cs_virtual_lib_on_startup'] = ''\n         defs['virt_libs_hidden'] = defs['virt_libs_order'] = ()\n \n         # Migrate the bool tristate tweak\n-        defs['bools_are_tristate'] = \\\n-                tweaks.get('bool_custom_columns_are_tristate', 'yes') == 'yes'\n+        defs['bools_are_tristate'] = tweaks.get('bool_custom_columns_are_tristate', 'yes') == 'yes'\n         if self.prefs.get('bools_are_tristate') is None:\n             self.prefs.set('bools_are_tristate', defs['bools_are_tristate'])\n \n         # Migrate column coloring rules\n         if self.prefs.get('column_color_name_1', None) is not None:\n             from calibre.library.coloring import migrate_old_rule\n+\n             old_rules = []\n             for i in range(1, 6):\n-                col = self.prefs.get('column_color_name_'+str(i), None)\n-                templ = self.prefs.get('column_color_template_'+str(i), None)\n+                col = self.prefs.get('column_color_name_' + str(i), None)\n+                templ = self.prefs.get('column_color_template_' + str(i), None)\n                 if col and templ:\n                     try:\n-                        del self.prefs['column_color_name_'+str(i)]\n+                        del self.prefs['column_color_name_' + str(i)]\n                         rules = migrate_old_rule(self.field_metadata, templ)\n                         for templ in rules:\n                             old_rules.append((col, templ))\n@@ -340,7 +354,7 @@ def migrate_preference(key, default):\n                 suffix = 1\n                 while icu_lower(cat + str(suffix)) in catmap:\n                     suffix += 1\n-                prints(f'Renaming user category {cat} to {cat+str(suffix)}')\n+                prints(f'Renaming user category {cat} to {cat + str(suffix)}')\n                 user_cats[cat + str(suffix)] = user_cats[cat]\n                 del user_cats[cat]\n                 cats_changed = True\n@@ -348,13 +362,12 @@ def migrate_preference(key, default):\n             self.prefs.set('user_categories', user_cats)\n \n         if not self.is_second_db:\n-            load_user_template_functions(self.library_id,\n-                                         self.prefs.get('user_template_functions', []))\n+            load_user_template_functions(self.library_id, self.prefs.get('user_template_functions', []))\n \n         # Load the format filename cache\n         self.refresh_format_cache()\n \n-        self.conn.executescript('''\n+        self.conn.executescript(\"\"\"\n         DROP TRIGGER IF EXISTS author_insert_trg;\n         CREATE TEMP TRIGGER author_insert_trg\n             AFTER INSERT ON authors\n@@ -368,10 +381,10 @@ def migrate_preference(key, default):\n             UPDATE authors SET sort=author_to_author_sort(NEW.name)\n             WHERE id=NEW.id AND name <> NEW.name;\n         END;\n-        ''')\n-        self.conn.execute(\n-            'UPDATE authors SET sort=author_to_author_sort(name) WHERE sort IS NULL')\n-        self.conn.executescript('''\n+        \"\"\")\n+        self.conn.execute('UPDATE authors SET sort=author_to_author_sort(name) WHERE sort IS NULL')\n+        self.conn.executescript(\n+            \"\"\"\n             CREATE TEMP VIEW IF NOT EXISTS tag_browser_news AS SELECT DISTINCT\n                 id,\n                 name,\n@@ -382,9 +395,11 @@ def migrate_preference(key, default):\n                 (SELECT DISTINCT tag FROM books_tags_link WHERE book IN\n                     (SELECT DISTINCT book FROM books_tags_link WHERE tag IN\n                         (SELECT id FROM tags WHERE name=\"{0}\")));\n-            '''.format(_('News')))\n+            \"\"\".format(_('News'))\n+        )\n \n-        self.conn.executescript('''\n+        self.conn.executescript(\n+            \"\"\"\n             CREATE TEMP VIEW IF NOT EXISTS tag_browser_filtered_news AS SELECT DISTINCT\n                 id,\n                 name,\n@@ -395,20 +410,23 @@ def migrate_preference(key, default):\n                 (SELECT DISTINCT tag FROM books_tags_link WHERE book IN\n                     (SELECT DISTINCT book FROM books_tags_link WHERE tag IN\n                         (SELECT id FROM tags WHERE name=\"{0}\")));\n-            '''.format(_('News')))\n+            \"\"\".format(_('News'))\n+        )\n         self.conn.commit()\n \n         CustomColumns.__init__(self)\n-        template = '''\\\n+        template = \"\"\"\\\n                 (SELECT {query} FROM books_{table}_link AS link INNER JOIN\n                     {table} ON(link.{link_col}={table}.id) WHERE link.book=books.id)\n                     {col}\n-                '''\n-        columns = ['id', 'title',\n+                \"\"\"\n+        columns = [\n+            'id',\n+            'title',\n             # col         table     link_col          query\n             ('authors', 'authors', 'author', 'sortconcat(link.id, name)'),\n-             'timestamp',\n-             '(SELECT MAX(uncompressed_size) FROM data WHERE book=books.id) size',\n+            'timestamp',\n+            '(SELECT MAX(uncompressed_size) FROM data WHERE book=books.id) size',\n             ('rating', 'ratings', 'rating', 'ratings.rating'),\n             ('tags', 'tags', 'tag', 'group_concat(name)'),\n             '(SELECT text FROM comments WHERE book=books.id) comments',\n@@ -422,19 +440,16 @@ def migrate_preference(key, default):\n             'pubdate',\n             'uuid',\n             'has_cover',\n-            ('au_map', 'authors', 'author',\n-                'aum_sortconcat(link.id, authors.name, authors.sort, authors.link)'),\n+            ('au_map', 'authors', 'author', 'aum_sortconcat(link.id, authors.name, authors.sort, authors.link)'),\n             'last_modified',\n             '(SELECT identifiers_concat(type, val) FROM identifiers WHERE identifiers.book=books.id) identifiers',\n-            ('languages', 'languages', 'lang_code',\n-                'sortconcat(link.id, languages.lang_code)'),\n-            ]\n+            ('languages', 'languages', 'lang_code', 'sortconcat(link.id, languages.lang_code)'),\n+        ]\n         lines = []\n         for col in columns:\n             line = col\n             if isinstance(col, tuple):\n-                line = template.format(col=col[0], table=col[1],\n-                        link_col=col[2], query=col[3])\n+                line = template.format(col=col[0], table=col[1], link_col=col[2], query=col[3])\n             lines.append(line)\n \n         custom_map = self.custom_columns_in_meta()\n@@ -442,48 +457,61 @@ def migrate_preference(key, default):\n         custom_cols = sorted(custom_map.keys())\n         lines.extend([custom_map[x] for x in custom_cols])\n \n-        self.FIELD_MAP = {'id':0, 'title':1, 'authors':2, 'timestamp':3,\n-            'size':4, 'rating':5, 'tags':6, 'comments':7, 'series':8,\n-            'publisher':9, 'series_index':10, 'sort':11, 'author_sort':12,\n-            'formats':13, 'path':14, 'pubdate':15, 'uuid':16, 'cover':17,\n-            'au_map':18, 'last_modified':19, 'identifiers':20, 'languages':21}\n-\n-        for k,v in iteritems(self.FIELD_MAP):\n+        self.FIELD_MAP = {\n+            'id': 0,\n+            'title': 1,\n+            'authors': 2,\n+            'timestamp': 3,\n+            'size': 4,\n+            'rating': 5,\n+            'tags': 6,\n+            'comments': 7,\n+            'series': 8,\n+            'publisher': 9,\n+            'series_index': 10,\n+            'sort': 11,\n+            'author_sort': 12,\n+            'formats': 13,\n+            'path': 14,\n+            'pubdate': 15,\n+            'uuid': 16,\n+            'cover': 17,\n+            'au_map': 18,\n+            'last_modified': 19,\n+            'identifiers': 20,\n+            'languages': 21,\n+        }\n+\n+        for k, v in iteritems(self.FIELD_MAP):\n             self.field_metadata.set_field_record_index(k, v, prefer_custom=False)\n \n         base = max(self.FIELD_MAP.values())\n         for col in custom_cols:\n-            self.FIELD_MAP[col] = base = base+1\n-            self.field_metadata.set_field_record_index(\n-                                        self.custom_column_num_map[col]['label'],\n-                                        base,\n-                                        prefer_custom=True)\n+            self.FIELD_MAP[col] = base = base + 1\n+            self.field_metadata.set_field_record_index(self.custom_column_num_map[col]['label'], base, prefer_custom=True)\n             if self.custom_column_num_map[col]['datatype'] == 'series':\n                 # account for the series index column. Field_metadata knows that\n                 # the series index is one larger than the series. If you change\n                 # it here, be sure to change it there as well.\n-                self.FIELD_MAP[str(col)+'_index'] = base = base+1\n-                self.field_metadata.set_field_record_index(\n-                            self.custom_column_num_map[col]['label']+'_index',\n-                            base,\n-                            prefer_custom=True)\n+                self.FIELD_MAP[str(col) + '_index'] = base = base + 1\n+                self.field_metadata.set_field_record_index(self.custom_column_num_map[col]['label'] + '_index', base, prefer_custom=True)\n \n-        self.FIELD_MAP['ondevice'] = base = base+1\n+        self.FIELD_MAP['ondevice'] = base = base + 1\n         self.field_metadata.set_field_record_index('ondevice', base, prefer_custom=False)\n-        self.FIELD_MAP['marked'] = base = base+1\n+        self.FIELD_MAP['marked'] = base = base + 1\n         self.field_metadata.set_field_record_index('marked', base, prefer_custom=False)\n-        self.FIELD_MAP['series_sort'] = base = base+1\n+        self.FIELD_MAP['series_sort'] = base = base + 1\n         self.field_metadata.set_field_record_index('series_sort', base, prefer_custom=False)\n-        self.FIELD_MAP['in_tag_browser'] = base = base+1\n+        self.FIELD_MAP['in_tag_browser'] = base = base + 1\n         self.field_metadata.set_field_record_index('in_tag_browser', base, prefer_custom=False)\n \n-        script = '''\n+        script = \"\"\"\n         DROP VIEW IF EXISTS meta2;\n         CREATE TEMP VIEW meta2 AS\n         SELECT\n         {}\n         FROM books;\n-        '''.format(', \\n'.join(lines))\n+        \"\"\".format(', \\n'.join(lines))\n         self.conn.executescript(script)\n         self.conn.commit()\n \n@@ -508,35 +536,43 @@ def migrate_preference(key, default):\n         if len(saved_searches().names()):\n             self.field_metadata.add_search_category(label='search', name=_('Saved searches'))\n \n-        self.field_metadata.add_grouped_search_terms(\n-                                    self.prefs.get('grouped_search_terms', {}))\n+        self.field_metadata.add_grouped_search_terms(self.prefs.get('grouped_search_terms', {}))\n \n         self.book_on_device_func = None\n-        self.data    = ResultCache(self.FIELD_MAP, self.field_metadata, db_prefs=self.prefs)\n-        self.search  = self.data.search\n-        self.search_getting_ids  = self.data.search_getting_ids\n+        self.data = ResultCache(self.FIELD_MAP, self.field_metadata, db_prefs=self.prefs)\n+        self.search = self.data.search\n+        self.search_getting_ids = self.data.search_getting_ids\n         self.refresh = functools.partial(self.data.refresh, self)\n-        self.sort    = self.data.sort\n+        self.sort = self.data.sort\n         self.multisort = self.data.multisort\n-        self.index   = self.data.index\n+        self.index = self.data.index\n         self.refresh_ids = functools.partial(self.data.refresh_ids, self)\n-        self.row     = self.data.row\n-        self.has_id  = self.data.has_id\n-        self.count   = self.data.count\n+        self.row = self.data.row\n+        self.has_id = self.data.has_id\n+        self.count = self.data.count\n         self.set_marked_ids = self.data.set_marked_ids\n \n         for prop in (\n-                'author_sort', 'authors', 'comment', 'comments',\n-                'publisher', 'rating', 'series', 'series_index', 'tags',\n-                'title', 'timestamp', 'uuid', 'pubdate', 'ondevice',\n-                'metadata_last_modified', 'languages',\n-                ):\n-            fm = {'comment':'comments', 'metadata_last_modified':\n-                    'last_modified'}.get(prop, prop)\n-            setattr(self, prop, functools.partial(self.get_property,\n-                    loc=self.FIELD_MAP[fm]))\n-        setattr(self, 'title_sort', functools.partial(self.get_property,\n-                loc=self.FIELD_MAP['sort']))\n+            'author_sort',\n+            'authors',\n+            'comment',\n+            'comments',\n+            'publisher',\n+            'rating',\n+            'series',\n+            'series_index',\n+            'tags',\n+            'title',\n+            'timestamp',\n+            'uuid',\n+            'pubdate',\n+            'ondevice',\n+            'metadata_last_modified',\n+            'languages',\n+        ):\n+            fm = {'comment': 'comments', 'metadata_last_modified': 'last_modified'}.get(prop, prop)\n+            setattr(self, prop, functools.partial(self.get_property, loc=self.FIELD_MAP[fm]))\n+        setattr(self, 'title_sort', functools.partial(self.get_property, loc=self.FIELD_MAP['sort']))\n \n         d = self.conn.get('SELECT book FROM metadata_dirtied', all=True)\n         with self.dirtied_lock:\n@@ -552,12 +588,10 @@ def migrate_preference(key, default):\n \n     def break_cycles(self):\n         self.data.break_cycles()\n-        self.data = self.field_metadata = self.prefs = self.listeners = \\\n-            self.refresh_ondevice = None\n+        self.data = self.field_metadata = self.prefs = self.listeners = self.refresh_ondevice = None\n \n     def initialize_database(self):\n-        metadata_sqlite = P('metadata_sqlite.sql', data=True,\n-                allow_user_override=False).decode('utf-8')\n+        metadata_sqlite = P('metadata_sqlite.sql', data=True, allow_user_override=False).decode('utf-8')\n         self.conn.executescript(metadata_sqlite)\n         self.conn.commit()\n         if self.user_version == 0:\n@@ -582,13 +616,12 @@ def saved_search_set_all(self, smap):\n         saved_searches().set_all(smap)\n \n     def last_modified(self):\n-        ''' Return last modified time as a UTC datetime object'''\n+        \"\"\"Return last modified time as a UTC datetime object\"\"\"\n         return utcfromtimestamp(os.stat(self.dbpath).st_mtime)\n \n     def refresh_format_cache(self):\n         self.format_filename_cache = defaultdict(dict)\n-        for book_id, fmt, name in self.conn.get(\n-                'SELECT book,format,name FROM data'):\n+        for book_id, fmt, name in self.conn.get('SELECT book,format,name FROM data'):\n             self.format_filename_cache[book_id][fmt.upper() if fmt else ''] = name\n         self.format_metadata_cache = defaultdict(dict)\n \n@@ -599,28 +632,26 @@ def check_if_modified(self):\n         self.last_update_check = utcnow()\n \n     def path(self, index, index_is_id=False):\n-        'Return the relative path to the directory containing this books files as a unicode string.'\n+        \"Return the relative path to the directory containing this books files as a unicode string.\"\n         row = self.data._data[index] if index_is_id else self.data[index]\n         return row[self.FIELD_MAP['path']].replace('/', os.sep)\n \n     def abspath(self, index, index_is_id=False, create_dirs=True):\n-        'Return the absolute path to the directory containing this books files as a unicode string.'\n+        \"Return the absolute path to the directory containing this books files as a unicode string.\"\n         path = os.path.join(self.library_path, self.path(index, index_is_id=index_is_id))\n         if create_dirs and not os.path.exists(path):\n             os.makedirs(path)\n         return path\n \n     def construct_path_name(self, id):\n-        '''\n+        \"\"\"\n         Construct the directory name for this book based on its metadata.\n-        '''\n+        \"\"\"\n         authors = self.authors(id, index_is_id=True)\n         if not authors:\n             authors = _('Unknown')\n-        author = ascii_filename(authors.split(',')[0].replace('|', ',')\n-                    )[:self.PATH_LIMIT]\n-        title  = ascii_filename(self.title(id, index_is_id=True)\n-                    )[:self.PATH_LIMIT]\n+        author = ascii_filename(authors.split(',')[0].replace('|', ','))[: self.PATH_LIMIT]\n+        title = ascii_filename(self.title(id, index_is_id=True))[: self.PATH_LIMIT]\n         while author[-1] in (' ', '.'):\n             author = author[:-1]\n         if not author:\n@@ -629,17 +660,15 @@ def construct_path_name(self, id):\n         return path\n \n     def construct_file_name(self, id):\n-        '''\n+        \"\"\"\n         Construct the file name for this book based on its metadata.\n-        '''\n+        \"\"\"\n         authors = self.authors(id, index_is_id=True)\n         if not authors:\n             authors = _('Unknown')\n-        author = ascii_filename(authors.split(',')[0].replace('|', ',')\n-                    )[:self.PATH_LIMIT]\n-        title  = ascii_filename(self.title(id, index_is_id=True)\n-                    )[:self.PATH_LIMIT]\n-        name   = title + ' - ' + author\n+        author = ascii_filename(authors.split(',')[0].replace('|', ','))[: self.PATH_LIMIT]\n+        title = ascii_filename(self.title(id, index_is_id=True))[: self.PATH_LIMIT]\n+        name = title + ' - ' + author\n         while name.endswith('.'):\n             name = name[:-1]\n         return name\n@@ -655,11 +684,11 @@ def normpath(self, path):\n         return path\n \n     def set_path(self, index, index_is_id=False):\n-        '''\n+        \"\"\"\n         Set the path to the directory containing this books files based on its\n         current title and author. If there was a previous directory, its contents\n         are copied and it is deleted.\n-        '''\n+        \"\"\"\n         id = index if index_is_id else self.id(index)\n         path = self.construct_path_name(id)\n         current_path = self.path(id, index_is_id=True).replace(os.sep, '/')\n@@ -685,16 +714,11 @@ def set_path(self, index, index_is_id=False):\n                 os.makedirs(tpath)\n \n             if source_ok:  # Migrate existing files\n-                self.copy_cover_to(id, os.path.join(tpath, 'cover.jpg'),\n-                        index_is_id=True, windows_atomic_move=wam,\n-                        use_hardlink=True)\n+                self.copy_cover_to(id, os.path.join(tpath, 'cover.jpg'), index_is_id=True, windows_atomic_move=wam, use_hardlink=True)\n                 for format in formats:\n-                    copy_function = functools.partial(self.copy_format_to, id,\n-                            format, index_is_id=True, windows_atomic_move=wam,\n-                            use_hardlink=True)\n+                    copy_function = functools.partial(self.copy_format_to, id, format, index_is_id=True, windows_atomic_move=wam, use_hardlink=True)\n                     try:\n-                        self.add_format(id, format, None, index_is_id=True,\n-                            path=tpath, notify=False, copy_function=copy_function)\n+                        self.add_format(id, format, None, index_is_id=True, path=tpath, notify=False, copy_function=copy_function)\n                     except NoSuchFormat:\n                         continue\n             self.conn.execute('UPDATE books SET path=? WHERE id=?', (path, id))\n@@ -729,21 +753,20 @@ def set_path(self, index, index_is_id=False):\n             for oldseg, newseg in zip(c1, c2):\n                 if oldseg.lower() == newseg.lower() and oldseg != newseg:\n                     try:\n-                        os.rename(os.path.join(curpath, oldseg),\n-                                os.path.join(curpath, newseg))\n+                        os.rename(os.path.join(curpath, oldseg), os.path.join(curpath, newseg))\n                     except Exception:\n                         break  # Fail silently since nothing catastrophic has happened\n                 curpath = os.path.join(curpath, newseg)\n \n     def add_listener(self, listener):\n-        '''\n+        \"\"\"\n         Add a listener. Will be called on change events with two arguments.\n         Event name and list of affected ids.\n-        '''\n+        \"\"\"\n         self.listeners.add(listener)\n \n     def notify(self, event, ids=[]):\n-        'Notify all listeners'\n+        \"Notify all listeners\"\n         for listener in self.listeners:\n             try:\n                 listener(event, ids)\n@@ -751,9 +774,8 @@ def notify(self, event, ids=[]):\n                 traceback.print_exc()\n                 continue\n \n-    def cover(self, index, index_is_id=False, as_file=False, as_image=False,\n-            as_path=False):\n-        '''\n+    def cover(self, index, index_is_id=False, as_file=False, as_image=False, as_path=False):\n+        \"\"\"\n         Return the cover image as a bytestring (in JPEG format) or None.\n \n         WARNING: Using as_path will copy the cover to a temp file and return\n@@ -762,7 +784,7 @@ def cover(self, index, index_is_id=False, as_file=False, as_image=False,\n \n         :param as_file: If True return the image as an open file object (a SpooledTemporaryFile)\n         :param as_image: If True return the image as a QImage object\n-        '''\n+        \"\"\"\n         id = index if index_is_id else self.id(index)\n         path = os.path.join(self.library_path, self.path(id, index_is_id=True), 'cover.jpg')\n         if os.access(path, os.R_OK):\n@@ -785,6 +807,7 @@ def cover(self, index, index_is_id=False, as_file=False, as_image=False,\n                     ret = f.read()\n                     if as_image:\n                         from qt.core import QImage\n+\n                         i = QImage()\n                         i.loadFromData(ret)\n                         ret = i\n@@ -803,8 +826,7 @@ def cover_last_modified(self, index, index_is_id=False):\n     # The field-style interface. These use field keys.\n \n     def get_field(self, idx, key, default=None, index_is_id=False):\n-        mi = self.get_metadata(idx, index_is_id=index_is_id,\n-                               get_cover=key == 'cover')\n+        mi = self.get_metadata(idx, index_is_id=index_is_id, get_cover=key == 'cover')\n         return mi.get(key, default)\n \n     def standard_field_keys(self):\n@@ -835,18 +857,17 @@ def metadata_for_field(self, key):\n         return self.field_metadata[key]\n \n     def clear_dirtied(self, book_id, sequence):\n-        '''\n+        \"\"\"\n         Clear the dirtied indicator for the books. This is used when fetching\n         metadata, creating an OPF, and writing a file are separated into steps.\n         The last step is clearing the indicator\n-        '''\n+        \"\"\"\n         with self.dirtied_lock:\n             dc_sequence = self.dirtied_cache.get(book_id, None)\n             # print('clear_dirty: check book', book_id, dc_sequence)\n             if dc_sequence is None or sequence is None or dc_sequence == sequence:\n                 # print('needs to be cleaned')\n-                self.conn.execute('DELETE FROM metadata_dirtied WHERE book=?',\n-                        (book_id,))\n+                self.conn.execute('DELETE FROM metadata_dirtied WHERE book=?', (book_id,))\n                 self.conn.commit()\n                 try:\n                     del self.dirtied_cache[book_id]\n@@ -856,17 +877,15 @@ def clear_dirtied(self, book_id, sequence):\n                 # print('book needs to be done again')\n                 pass\n \n-    def dump_metadata(self, book_ids=None, remove_from_dirtied=True,\n-            commit=True, callback=None):\n-        '''\n+    def dump_metadata(self, book_ids=None, remove_from_dirtied=True, commit=True, callback=None):\n+        \"\"\"\n         Write metadata for each record to an individual OPF file. If callback\n         is not None, it is called once at the start with the number of book_ids\n         being processed. And once for every book_id, with arguments (book_id,\n         mi, ok).\n-        '''\n+        \"\"\"\n         if book_ids is None:\n-            book_ids = [x[0] for x in self.conn.get(\n-                'SELECT book FROM metadata_dirtied', all=True)]\n+            book_ids = [x[0] for x in self.conn.get('SELECT book FROM metadata_dirtied', all=True)]\n \n         if callback is not None:\n             book_ids = tuple(book_ids)\n@@ -899,9 +918,7 @@ def update_last_modified(self, book_ids, commit=False, now=None):\n         if now is None:\n             now = nowf()\n         if book_ids:\n-            self.conn.executemany(\n-                'UPDATE books SET last_modified=? WHERE id=?',\n-                [(now, book) for book in book_ids])\n+            self.conn.executemany('UPDATE books SET last_modified=? WHERE id=?', [(now, book) for book in book_ids])\n             for book_id in book_ids:\n                 self.data.set(book_id, self.FIELD_MAP['last_modified'], now, row_is_id=True)\n             if commit:\n@@ -918,9 +935,7 @@ def dirtied(self, book_ids, commit=True):\n                     continue\n                 # print('book not already dirty')\n \n-                self.conn.execute(\n-                    'INSERT OR IGNORE INTO metadata_dirtied (book) VALUES (?)',\n-                    (book,))\n+                self.conn.execute('INSERT OR IGNORE INTO metadata_dirtied (book) VALUES (?)', (book,))\n                 self.dirtied_cache[book] = self.dirtied_sequence\n                 self.dirtied_sequence += 1\n \n@@ -938,7 +953,7 @@ def get_a_dirtied_book(self):\n                 # The random stuff is here to prevent a single book from\n                 # blocking progress if its metadata cannot be written for some\n                 # reason.\n-                id_ = list(self.dirtied_cache.keys())[random.randint(0, l-1)]\n+                id_ = list(self.dirtied_cache.keys())[random.randint(0, l - 1)]\n                 sequence = self.dirtied_cache[id_]\n                 return id_, sequence\n             return None, None\n@@ -947,14 +962,14 @@ def dirty_queue_length(self):\n         return len(self.dirtied_cache)\n \n     def commit_dirty_cache(self):\n-        '''\n+        \"\"\"\n         Set the dirty indication for every book in the cache. The vast majority\n         of the time, the indication will already be set. However, sometimes\n         exceptions may have prevented a commit, which may remove some dirty\n         indications from the DB. This call will put them back. Note that there\n         is no problem with setting a dirty indication for a book that isn't in\n         fact dirty. Just wastes a few cycles.\n-        '''\n+        \"\"\"\n         with self.dirtied_lock:\n             book_ids = list(self.dirtied_cache.keys())\n             self.dirtied_cache = {}\n@@ -984,12 +999,11 @@ def get_metadata_for_dump(self, idx):\n             pass\n         return path, mi, sequence\n \n-    def get_metadata(self, idx, index_is_id=False, get_cover=False,\n-                     get_user_categories=True, cover_as_data=False):\n-        '''\n+    def get_metadata(self, idx, index_is_id=False, get_cover=False, get_user_categories=True, cover_as_data=False):\n+        \"\"\"\n         Convenience method to return metadata as a :class:`Metadata` object.\n         Note that the list of formats is not verified.\n-        '''\n+        \"\"\"\n         idx = idx if index_is_id else self.id(idx)\n         try:\n             row = self.data._data[idx]\n@@ -1011,7 +1025,7 @@ def get_metadata(self, idx, index_is_id=False, get_cover=False,\n         aus = {}\n         aul = {}\n         try:\n-            for (author, author_sort, link) in aut_list:\n+            for author, author_sort, link in aut_list:\n                 aut = author.replace('|', ',')\n                 aum.append(aut)\n                 aus[aut] = author_sort.replace('|', ',')\n@@ -1022,17 +1036,17 @@ def get_metadata(self, idx, index_is_id=False, get_cover=False,\n                 aum.append(x.replace('|', ','))\n                 aul[aum[-1]] = ''\n                 aus[aum[-1]] = aum[-1]\n-        mi.title       = row[fm['title']]\n-        mi.authors     = aum\n+        mi.title = row[fm['title']]\n+        mi.authors = aum\n         mi.author_sort = row[fm['author_sort']]\n         mi.author_sort_map = aus\n         mi.author_link_map = aul\n-        mi.comments    = row[fm['comments']]\n-        mi.publisher   = row[fm['publisher']]\n-        mi.timestamp   = row[fm['timestamp']]\n-        mi.pubdate     = row[fm['pubdate']]\n-        mi.uuid        = row[fm['uuid']]\n-        mi.title_sort  = row[fm['sort']]\n+        mi.comments = row[fm['comments']]\n+        mi.publisher = row[fm['publisher']]\n+        mi.timestamp = row[fm['timestamp']]\n+        mi.pubdate = row[fm['pubdate']]\n+        mi.uuid = row[fm['uuid']]\n+        mi.title_sort = row[fm['sort']]\n         mi.last_modified = row[fm['last_modified']]\n         formats = row[fm['formats']]\n         mi.format_metadata = {}\n@@ -1045,8 +1059,8 @@ def get_metadata(self, idx, index_is_id=False, get_cover=False,\n         mi.formats = good_formats\n         mi.db_approx_formats = formats\n         mi._proxy_metadata = p = ProxyMetadata(row[fm['size']], row[fm['ondevice']], formats)\n-        mi.book_size   = p.book_size\n-        mi.ondevice_col= p.ondevice_col\n+        mi.book_size = p.book_size\n+        mi.ondevice_col = p.ondevice_col\n         tags = row[fm['tags']]\n         if tags:\n             mi.tags = [i.strip() for i in tags.split(',')]\n@@ -1066,8 +1080,7 @@ def get_metadata(self, idx, index_is_id=False, get_cover=False,\n             if meta['datatype'] == 'composite':\n                 mi.set(key, val=row[meta['rec_index']])\n             else:\n-                val, extra = self.get_custom_and_extra(idx, label=meta['label'],\n-                                                       index_is_id=True)\n+                val, extra = self.get_custom_and_extra(idx, label=meta['label'], index_is_id=True)\n                 mi.set(key, val=val, extra=extra)\n \n         user_cats = self.prefs['user_categories']\n@@ -1075,13 +1088,13 @@ def get_metadata(self, idx, index_is_id=False, get_cover=False,\n         if get_user_categories:\n             for ucat in user_cats:\n                 res = []\n-                for name,cat,ign in user_cats[ucat]:\n+                for name, cat, ign in user_cats[ucat]:\n                     v = mi.get(cat, None)\n                     if isinstance(v, list):\n                         if name in v:\n-                            res.append([name,cat])\n+                            res.append([name, cat])\n                     elif name == v:\n-                        res.append([name,cat])\n+                        res.append([name, cat])\n                 user_cat_vals[ucat] = res\n         mi.user_categories = user_cat_vals\n \n@@ -1120,14 +1133,9 @@ def books_with_same_title(self, mi, all_matches=True):\n         return ans\n \n     def find_identical_books(self, mi):\n-        fuzzy_title_patterns = [(re.compile(pat, re.IGNORECASE) if\n-            isinstance(pat, string_or_bytes) else pat, repl) for pat, repl in\n-                [\n-                    (r'[\\[\\](){}<>\\'\";,:#]', ''),\n-                    (get_title_sort_pat(), ''),\n-                    (r'[-._]', ' '),\n-                    (r'\\s+', ' ')\n-                ]\n+        fuzzy_title_patterns = [\n+            (re.compile(pat, re.IGNORECASE) if isinstance(pat, string_or_bytes) else pat, repl)\n+            for pat, repl in [(r'[\\[\\](){}<>\\'\";,:#]', ''), (get_title_sort_pat(), ''), (r'[-._]', ' '), (r'\\s+', ' ')]\n         ]\n \n         def fuzzy_title(title):\n@@ -1141,8 +1149,7 @@ def fuzzy_title(title):\n             try:\n                 quathors = mi.authors[:10]  # Too many authors causes parsing of\n                 # the search expression to fail\n-                query = ' and '.join(['author:\"={}\"'.format(a.replace('\"', '')) for a in\n-                    quathors])\n+                query = ' and '.join(['author:\"={}\"'.format(a.replace('\"', '')) for a in quathors])\n                 qauthors = mi.authors[10:]\n             except ValueError:\n                 return identical_book_ids\n@@ -1157,8 +1164,7 @@ def fuzzy_title(title):\n                 for book_id in book_ids:\n                     aut = self.authors(book_id, index_is_id=True)\n                     if aut:\n-                        aut = {lower(x.replace('|', ',')) for x in\n-                                aut.split(',')}\n+                        aut = {lower(x.replace('|', ',')) for x in aut.split(',')}\n                         if aut.issuperset(qauthors):\n                             matches.add(book_id)\n                 book_ids = matches\n@@ -1187,17 +1193,15 @@ def remove_cover(self, id, notify=True, commit=True):\n             self.notify('cover', [id])\n \n     def set_cover(self, id, data, notify=True, commit=True):\n-        '''\n+        \"\"\"\n         Set the cover for this book.\n \n         `data`: Can be either a QImage, QPixmap, file object or bytestring\n-        '''\n-        base_path = os.path.join(self.library_path, self.path(id,\n-            index_is_id=True))\n+        \"\"\"\n+        base_path = os.path.join(self.library_path, self.path(id, index_is_id=True))\n         if not os.path.exists(base_path):\n             self.set_path(id, index_is_id=True)\n-            base_path = os.path.join(self.library_path, self.path(id,\n-                index_is_id=True))\n+            base_path = os.path.join(self.library_path, self.path(id, index_is_id=True))\n             self.dirtied([id])\n             if not os.path.exists(base_path):\n                 os.makedirs(base_path)\n@@ -1215,8 +1219,7 @@ def set_cover(self, id, data, notify=True, commit=True):\n                 time.sleep(0.2)\n                 save_cover_data_to(data, path)\n         now = nowf()\n-        self.conn.execute(\n-            'UPDATE books SET has_cover=1,last_modified=? WHERE id=?', (now, id))\n+        self.conn.execute('UPDATE books SET has_cover=1,last_modified=? WHERE id=?', (now, id))\n         if commit:\n             self.conn.commit()\n         self.data.set(id, self.FIELD_MAP['cover'], True, row_is_id=True)\n@@ -1230,9 +1233,7 @@ def has_cover(self, id):\n     def set_has_cover(self, id, val):\n         dval = 1 if val else 0\n         now = nowf()\n-        self.conn.execute(\n-                'UPDATE books SET has_cover=?,last_modified=? WHERE id=?',\n-                (dval, now, id))\n+        self.conn.execute('UPDATE books SET has_cover=?,last_modified=? WHERE id=?', (dval, now, id))\n         self.data.set(id, self.FIELD_MAP['cover'], val, row_is_id=True)\n         self.data.set(id, self.FIELD_MAP['last_modified'], now, row_is_id=True)\n \n@@ -1253,7 +1254,7 @@ def book_on_device_string(self, id):\n                 loc.append(_('Card A'))\n             if b is not None:\n                 loc.append(_('Card B'))\n-        return ', '.join(loc) + ((_(' (%s books)')%count) if count > 1 else '')\n+        return ', '.join(loc) + ((_(' (%s books)') % count) if count > 1 else '')\n \n     def set_book_on_device_func(self, func):\n         self.book_on_device_func = func\n@@ -1269,7 +1270,7 @@ def format_files(self, index, index_is_id=False):\n         return [(v, k) for k, v in iteritems(self.format_filename_cache[id])]\n \n     def formats(self, index, index_is_id=False, verify_formats=True):\n-        ''' Return available formats as a comma separated list or None if there are no available formats '''\n+        \"\"\"Return available formats as a comma separated list or None if there are no available formats\"\"\"\n         id_ = index if index_is_id else self.id(index)\n         formats = self.data.get(id_, self.FIELD_MAP['formats'], row_is_id=True)\n         if not formats:\n@@ -1293,8 +1294,7 @@ def format_last_modified(self, id_, fmt):\n         if m:\n             return m['mtime']\n \n-    def format_metadata(self, id_, fmt, allow_cache=True, update_db=False,\n-            commit=False):\n+    def format_metadata(self, id_, fmt, allow_cache=True, update_db=False, commit=False):\n         if not fmt:\n             return {}\n         fmt = fmt.upper()\n@@ -1311,9 +1311,7 @@ def format_metadata(self, id_, fmt, allow_cache=True, update_db=False,\n             ans['mtime'] = utcfromtimestamp(stat.st_mtime)\n             self.format_metadata_cache[id_][fmt] = ans\n             if update_db:\n-                self.conn.execute(\n-                    'UPDATE data SET uncompressed_size=? WHERE format=? AND'\n-                    ' book=?', (stat.st_size, fmt, id_))\n+                self.conn.execute('UPDATE data SET uncompressed_size=? WHERE format=? AND book=?', (stat.st_size, fmt, id_))\n                 if commit:\n                     self.conn.commit()\n         return ans\n@@ -1332,13 +1330,13 @@ def format_hash(self, id_, fmt):\n         return sha.hexdigest()\n \n     def format_path(self, index, fmt, index_is_id=False):\n-        '''\n+        \"\"\"\n         This method is intended to be used only in those rare situations, like\n         Drag'n Drop, when you absolutely need the path to the original file.\n         Otherwise, use format(..., as_path=True).\n \n         Note that a networked backend will always return None.\n-        '''\n+        \"\"\"\n         path = self.format_abspath(index, fmt, index_is_id=index_is_id)\n         if path is None:\n             id_ = index if index_is_id else self.id(index)\n@@ -1346,7 +1344,7 @@ def format_path(self, index, fmt, index_is_id=False):\n         return path\n \n     def format_abspath(self, index, format, index_is_id=False):\n-        '''\n+        \"\"\"\n         Return absolute path to the ebook file of format `format`\n \n         WARNING: This method will return a dummy path for a network backend DB,\n@@ -1357,7 +1355,7 @@ def format_abspath(self, index, format, index_is_id=False):\n \n         Apart from the viewer, I don't believe any of the others do any file\n         I/O with the results of this call.\n-        '''\n+        \"\"\"\n         id = index if index_is_id else self.id(index)\n         try:\n             name = self.format_filename_cache[id][format.upper()]\n@@ -1366,11 +1364,11 @@ def format_abspath(self, index, format, index_is_id=False):\n         if name:\n             path = os.path.join(self.library_path, self.path(id, index_is_id=True))\n             format = ('.' + format.lower()) if format else ''\n-            fmt_path = os.path.join(path, name+format)\n+            fmt_path = os.path.join(path, name + format)\n             if os.path.exists(fmt_path):\n                 return fmt_path\n             try:\n-                candidates = glob.glob(os.path.join(path, '*'+format))\n+                candidates = glob.glob(os.path.join(path, '*' + format))\n             except Exception:  # If path contains strange characters this throws an exc\n                 candidates = []\n             if format and candidates and os.path.exists(candidates[0]):\n@@ -1383,9 +1381,8 @@ def format_abspath(self, index, format, index_is_id=False):\n                     return None\n                 return fmt_path\n \n-    def copy_format_to(self, index, fmt, dest, index_is_id=False,\n-            windows_atomic_move=None, use_hardlink=False):\n-        '''\n+    def copy_format_to(self, index, fmt, dest, index_is_id=False, windows_atomic_move=None, use_hardlink=False):\n+        \"\"\"\n         Copy the format ``fmt`` to the file like object ``dest``. If the\n         specified format does not exist, raises :class:`NoSuchFormat` error.\n         dest can also be a path, in which case the format is copied to it, iff\n@@ -1398,15 +1395,14 @@ def copy_format_to(self, index, fmt, dest, index_is_id=False,\n \n         windows_atomic_move is an internally used parameter. You should not use\n         it in any code outside this module.\n-        '''\n+        \"\"\"\n         path = self.format_abspath(index, fmt, index_is_id=index_is_id)\n         if path is None:\n             id_ = index if index_is_id else self.id(index)\n             raise NoSuchFormat(f'Record {id_} has no {fmt} file')\n         if windows_atomic_move is not None:\n             if not isinstance(dest, string_or_bytes):\n-                raise Exception('Error, you must pass the dest as a path when'\n-                        ' using windows_atomic_move')\n+                raise Exception('Error, you must pass the dest as a path when using windows_atomic_move')\n             if dest:\n                 if samefile(path, dest):\n                     # Ensure that the file has the same case as dest\n@@ -1441,9 +1437,8 @@ def copy_format_to(self, index, fmt, dest, index_is_id=False,\n                     with open(path, 'rb') as f, open(dest, 'wb') as d:\n                         shutil.copyfileobj(f, d)\n \n-    def copy_cover_to(self, index, dest, index_is_id=False,\n-            windows_atomic_move=None, use_hardlink=False):\n-        '''\n+    def copy_cover_to(self, index, dest, index_is_id=False, windows_atomic_move=None, use_hardlink=False):\n+        \"\"\"\n         Copy the cover to the file like object ``dest``. Returns False\n         if no cover exists or dest is the same file as the current cover.\n         dest can also be a path in which case the cover is\n@@ -1456,13 +1451,12 @@ def copy_cover_to(self, index, dest, index_is_id=False,\n \n         windows_atomic_move is an internally used parameter. You should not use\n         it in any code outside this module.\n-        '''\n+        \"\"\"\n         id = index if index_is_id else self.id(index)\n         path = os.path.join(self.library_path, self.path(id, index_is_id=True), 'cover.jpg')\n         if windows_atomic_move is not None:\n             if not isinstance(dest, string_or_bytes):\n-                raise Exception('Error, you must pass the dest as a path when'\n-                        ' using windows_atomic_move')\n+                raise Exception('Error, you must pass the dest as a path when using windows_atomic_move')\n             if os.access(path, os.R_OK) and dest and not samefile(dest, path):\n                 windows_atomic_move.copy_path_to(path, dest)\n                 return True\n@@ -1491,9 +1485,8 @@ def copy_cover_to(self, index, dest, index_is_id=False,\n                         return True\n         return False\n \n-    def format(self, index, format, index_is_id=False, as_file=False,\n-            mode='r+b', as_path=False, preserve_filename=False):\n-        '''\n+    def format(self, index, format, index_is_id=False, as_file=False, mode='r+b', as_path=False, preserve_filename=False):\n+        \"\"\"\n         Return the ebook format as a bytestring or `None` if the format doesn't exist,\n         or we don't have permission to write to the ebook file.\n \n@@ -1508,7 +1501,7 @@ def format(self, index, format, index_is_id=False, as_file=False,\n                                   this means that repeated calls yield the same\n                                   temp file (which is re-created each time)\n         :param mode: This is ignored (present for legacy compatibility)\n-        '''\n+        \"\"\"\n         path = self.format_abspath(index, format, index_is_id=index_is_id)\n         if path is not None:\n             with open(path, mode) as f:\n@@ -1525,7 +1518,7 @@ def format(self, index, format, index_is_id=False, as_file=False,\n                         with open(ret, 'wb') as f2:\n                             shutil.copyfileobj(f, f2)\n                     else:\n-                        with PersistentTemporaryFile('.'+format.lower()) as pt:\n+                        with PersistentTemporaryFile('.' + format.lower()) as pt:\n                             shutil.copyfileobj(f, pt)\n                             ret = pt.name\n                 elif as_file:\n@@ -1539,20 +1532,17 @@ def format(self, index, format, index_is_id=False, as_file=False,\n                     ret = f.read()\n             return ret\n \n-    def add_format_with_hooks(self, index, format, fpath, index_is_id=False,\n-                              path=None, notify=True, replace=True):\n+    def add_format_with_hooks(self, index, format, fpath, index_is_id=False, path=None, notify=True, replace=True):\n         npath = self.run_import_plugins(fpath, format)\n         format = os.path.splitext(npath)[-1].lower().replace('.', '').upper()\n         stream = open(npath, 'rb')\n         format = check_ebook_format(stream, format)\n         id = index if index_is_id else self.id(index)\n-        retval = self.add_format(id, format, stream, replace=replace,\n-                               index_is_id=True, path=path, notify=notify)\n+        retval = self.add_format(id, format, stream, replace=replace, index_is_id=True, path=path, notify=notify)\n         run_plugins_on_postimport(self, id, format)\n         return retval\n \n-    def add_format(self, index, format, stream, index_is_id=False, path=None,\n-            notify=True, replace=True, copy_function=None):\n+    def add_format(self, index, format, stream, index_is_id=False, path=None, notify=True, replace=True, copy_function=None):\n         id = index if index_is_id else self.id(index)\n         if not format:\n             format = ''\n@@ -1564,7 +1554,7 @@ def add_format(self, index, format, stream, index_is_id=False, path=None,\n             return False\n         name = self.construct_file_name(id)\n         ext = ('.' + format.lower()) if format else ''\n-        dest = os.path.join(path, name+ext)\n+        dest = os.path.join(path, name + ext)\n         pdir = os.path.dirname(dest)\n         if not os.path.exists(pdir):\n             os.makedirs(pdir)\n@@ -1573,15 +1563,13 @@ def add_format(self, index, format, stream, index_is_id=False, path=None,\n             copy_function(dest)\n             size = os.path.getsize(dest)\n         else:\n-            if (not getattr(stream, 'name', False) or not samefile(dest,\n-                stream.name)):\n+            if not getattr(stream, 'name', False) or not samefile(dest, stream.name):\n                 with open(dest, 'wb') as f:\n                     shutil.copyfileobj(stream, f)\n                     size = f.tell()\n             elif os.path.exists(dest):\n                 size = os.path.getsize(dest)\n-        self.conn.execute('INSERT OR REPLACE INTO data (book,format,uncompressed_size,name) VALUES (?,?,?,?)',\n-                          (id, format.upper(), size, name))\n+        self.conn.execute('INSERT OR REPLACE INTO data (book,format,uncompressed_size,name) VALUES (?,?,?,?)', (id, format.upper(), size, name))\n         self.update_last_modified([id], commit=False)\n         self.conn.commit()\n         self.format_filename_cache[id][format.upper()] = name\n@@ -1597,7 +1585,7 @@ def save_original_format(self, book_id, fmt, notify=True):\n         opath = self.format_abspath(book_id, fmt, index_is_id=True)\n         if opath is None:\n             return False\n-        nfmt = 'ORIGINAL_'+fmt\n+        nfmt = 'ORIGINAL_' + fmt\n         with open(opath, 'rb') as f:\n             return self.add_format(book_id, nfmt, f, index_is_id=True, notify=notify)\n \n@@ -1617,12 +1605,11 @@ def restore_original_format(self, book_id, original_fmt, notify=True):\n             return True\n         return False\n \n-    def delete_book(self, id, notify=True, commit=True, permanent=False,\n-            do_clean=True):\n-        '''\n+    def delete_book(self, id, notify=True, commit=True, permanent=False, do_clean=True):\n+        \"\"\"\n         Removes book from the result cache and the underlying database.\n         If you set commit to False, you must call clean() manually afterwards\n-        '''\n+        \"\"\"\n         try:\n             path = os.path.join(self.library_path, self.path(id, index_is_id=True))\n         except Exception:\n@@ -1641,8 +1628,7 @@ def delete_book(self, id, notify=True, commit=True, permanent=False,\n         if notify:\n             self.notify('delete', [id])\n \n-    def remove_format(self, index, format, index_is_id=False, notify=True,\n-                      commit=True, db_only=False):\n+    def remove_format(self, index, format, index_is_id=False, notify=True, commit=True, db_only=False):\n         id = index if index_is_id else self.id(index)\n         if not format:\n             format = ''\n@@ -1668,13 +1654,11 @@ def clean_standard_field(self, field, commit=False):\n         # Don't bother with validity checking. Let the exception fly out so\n         # we can see what happened\n         def doit(table, ltable_col):\n-            st = (f'DELETE FROM books_{table}_link WHERE (SELECT COUNT(id) '\n-                    'FROM books WHERE id=book) < 1;')\n+            st = f'DELETE FROM books_{table}_link WHERE (SELECT COUNT(id) FROM books WHERE id=book) < 1;'\n             self.conn.execute(st)\n-            st = ('DELETE FROM {table} WHERE (SELECT COUNT(id) '\n-                    'FROM books_{table}_link WHERE '\n-                    '{ltable_col}={table}.id) < 1;').format(**dict(\n-                            table=table, ltable_col=ltable_col))\n+            st = ('DELETE FROM {table} WHERE (SELECT COUNT(id) FROM books_{table}_link WHERE {ltable_col}={table}.id) < 1;').format(\n+                **dict(table=table, ltable_col=ltable_col)\n+            )\n             self.conn.execute(st)\n \n         fm = self.field_metadata[field]\n@@ -1683,32 +1667,30 @@ def doit(table, ltable_col):\n             self.conn.commit()\n \n     def clean(self):\n-        '''\n+        \"\"\"\n         Remove orphaned entries.\n-        '''\n+        \"\"\"\n+\n         def doit(ltable, table, ltable_col):\n-            st = (f'DELETE FROM books_{ltable}_link WHERE (SELECT COUNT(id) '\n-                    'FROM books WHERE id=book) < 1;')\n+            st = f'DELETE FROM books_{ltable}_link WHERE (SELECT COUNT(id) FROM books WHERE id=book) < 1;'\n             self.conn.execute(st)\n-            st = ('DELETE FROM {table} WHERE (SELECT COUNT(id) '\n-                    'FROM books_{ltable}_link WHERE '\n-                    '{ltable_col}={table}.id) < 1;').format(**dict(\n-                            ltable=ltable, table=table, ltable_col=ltable_col))\n+            st = ('DELETE FROM {table} WHERE (SELECT COUNT(id) FROM books_{ltable}_link WHERE {ltable_col}={table}.id) < 1;').format(\n+                **dict(ltable=ltable, table=table, ltable_col=ltable_col)\n+            )\n             self.conn.execute(st)\n \n         for ltable, table, ltable_col in [\n-                ('authors', 'authors', 'author'),\n-                ('publishers', 'publishers', 'publisher'),\n-                ('tags', 'tags', 'tag'),\n-                ('series', 'series', 'series'),\n-                ('languages', 'languages', 'lang_code'),\n-                ]:\n+            ('authors', 'authors', 'author'),\n+            ('publishers', 'publishers', 'publisher'),\n+            ('tags', 'tags', 'tag'),\n+            ('series', 'series', 'series'),\n+            ('languages', 'languages', 'lang_code'),\n+        ]:\n             doit(ltable, table, ltable_col)\n \n         for id_, tag in self.conn.get('SELECT id, name FROM tags', all=True):\n             if not tag.strip():\n-                self.conn.execute('DELETE FROM books_tags_link WHERE tag=?',\n-                        (id_,))\n+                self.conn.execute('DELETE FROM books_tags_link WHERE tag=?', (id_,))\n                 self.conn.execute('DELETE FROM tags WHERE id=?', (id_,))\n         self.clean_custom()\n         self.conn.commit()\n@@ -1724,38 +1706,33 @@ def get_books_for_category(self, category, id_):\n             dex = field['rec_index']\n             for book in self.data.iterall():\n                 if field['is_multiple']:\n-                    vals = [v.strip() for v in\n-                            book[dex].split(field['is_multiple']['cache_to_list'])\n-                            if v.strip()]\n+                    vals = [v.strip() for v in book[dex].split(field['is_multiple']['cache_to_list']) if v.strip()]\n                     if id_ in vals:\n                         ans.add(book[0])\n                 elif book[dex] == id_:\n                     ans.add(book[0])\n             return ans\n \n-        ans = self.conn.get(\n-                'SELECT book FROM books_{tn}_link WHERE {col}=?'.format(\n-                    tn=field['table'], col=field['link_column']), (id_,))\n+        ans = self.conn.get('SELECT book FROM books_{tn}_link WHERE {col}=?'.format(tn=field['table'], col=field['link_column']), (id_,))\n         return {x[0] for x in ans}\n \n-# data structures for get_categories\n+    # data structures for get_categories\n \n     CATEGORY_SORTS = CATEGORY_SORTS\n     MATCH_TYPE = ('any', 'all')\n \n     class TCat_Tag:\n-\n         def __init__(self, name, sort):\n             self.n = name\n             self.s = sort\n-            self.c  = 0\n+            self.c = 0\n             self.id_set = set()\n             self.rt = 0\n             self.rc = 0\n             self.id = None\n \n         def set_all(self, c, rt, rc, id):\n-            self.c  = c\n+            self.c = c\n             self.rt = rt\n             self.rc = rc\n             self.id = id\n@@ -1803,20 +1780,18 @@ def get_categories(self, sort='name', ids=None):\n         # item -> (item_id, sort_val) map to use in the books loop\n         for category in tb_cats:\n             cat = tb_cats[category]\n-            if not cat['is_category'] or cat['kind'] in ['user', 'search'] \\\n-                    or category in ['news', 'formats'] or cat.get('is_csp',\n-                            False):\n+            if not cat['is_category'] or cat['kind'] in ['user', 'search'] or category in ['news', 'formats'] or cat.get('is_csp', False):\n                 continue\n             # Get the ids for the item values\n             if not cat['is_custom']:\n                 funcs = {\n-                        'authors': self.get_authors_with_ids,\n-                        'series': self.get_series_with_ids,\n-                        'publisher': self.get_publishers_with_ids,\n-                        'tags': self.get_tags_with_ids,\n-                        'languages': self.get_languages_with_ids,\n-                        'rating': self.get_ratings_with_ids,\n-                    }\n+                    'authors': self.get_authors_with_ids,\n+                    'series': self.get_series_with_ids,\n+                    'publisher': self.get_publishers_with_ids,\n+                    'tags': self.get_tags_with_ids,\n+                    'languages': self.get_languages_with_ids,\n+                    'rating': self.get_ratings_with_ids,\n+                }\n                 func = funcs.get(category, None)\n                 if func:\n                     list = func()\n@@ -1841,8 +1816,7 @@ def get_categories(self, sort='name', ids=None):\n                 for l in list:\n                     (id, val) = (l[0], l[1])\n                     tids[category][val] = (id, f'{val:05.2f}')\n-            elif cat['datatype'] == 'text' and cat['is_multiple'] and \\\n-                            cat['display'].get('is_names', False):\n+            elif cat['datatype'] == 'text' and cat['is_multiple'] and cat['display'].get('is_names', False):\n                 for l in list:\n                     (id, val) = (l[0], l[1])\n                     tids[category][val] = (id, author_to_author_sort(val))\n@@ -1854,18 +1828,14 @@ def get_categories(self, sort='name', ids=None):\n             tcategories[category] = {}\n             # create a list of category/field_index for the books scan to use.\n             # This saves iterating through field_metadata for each book\n-            md.append((category, cat['rec_index'],\n-                       cat['is_multiple'].get('cache_to_list', None), False))\n+            md.append((category, cat['rec_index'], cat['is_multiple'].get('cache_to_list', None), False))\n \n         for category in tb_cats:\n             cat = tb_cats[category]\n-            if cat['datatype'] == 'composite' and \\\n-                                cat['display'].get('make_category', False):\n+            if cat['datatype'] == 'composite' and cat['display'].get('make_category', False):\n                 tids[category] = {}\n                 tcategories[category] = {}\n-                md.append((category, cat['rec_index'],\n-                           cat['is_multiple'].get('cache_to_list', None),\n-                           cat['datatype'] == 'composite'))\n+                md.append((category, cat['rec_index'], cat['is_multiple'].get('cache_to_list', None), cat['datatype'] == 'composite'))\n         # print('end phase \"collection\":', time.clock() - last, 'seconds')\n         # last = time.clock()\n \n@@ -1879,7 +1849,7 @@ def get_categories(self, sort='name', ids=None):\n                 continue\n             rating = book[rating_dex]\n             # We kept track of all possible category field_map positions above\n-            for (cat, dex, mult, is_comp) in md:\n+            for cat, dex, mult, is_comp in md:\n                 if not book[dex]:\n                     continue\n                 tid_cat = tids[cat]\n@@ -1943,16 +1913,16 @@ def get_categories(self, sort='name', ids=None):\n         tn = cat['table']\n         cn = cat['column']\n         if ids is None:\n-            query = f'''SELECT id, {cn}, count, avg_rating, sort\n-                       FROM tag_browser_{tn}'''\n+            query = f\"\"\"SELECT id, {cn}, count, avg_rating, sort\n+                       FROM tag_browser_{tn}\"\"\"\n         else:\n-            query = f'''SELECT id, {cn}, count, avg_rating, sort\n-                       FROM tag_browser_filtered_{tn}'''\n+            query = f\"\"\"SELECT id, {cn}, count, avg_rating, sort\n+                       FROM tag_browser_filtered_{tn}\"\"\"\n         # results will be sorted later\n         data = self.conn.get(query)\n         for r in data:\n             item = LibraryDatabase2.TCat_Tag(r[1], r[1])\n-            item.set_all(c=r[2], rt=r[2]*r[3], rc=r[2], id=r[0])\n+            item.set_all(c=r[2], rt=r[2] * r[3], rc=r[2], id=r[0])\n             tcategories['news'][r[1]] = item\n \n         # print('end phase \"news\":', time.clock() - last, 'seconds')\n@@ -1985,60 +1955,76 @@ def get_categories(self, sort='name', ids=None):\n             datatype = cat['datatype']\n \n             def avgr(x):\n-                return (0.0 if x.rc == 0 else x.rt / x.rc)\n+                return 0.0 if x.rc == 0 else x.rt / x.rc\n+\n             # Duplicate the build of items below to avoid using a lambda func\n             # in the main Tag loop. Saves a few %\n             if datatype == 'rating':\n+\n                 def formatter(x):\n-                    return ('\u2605' * int(x // 2))\n+                    return '\u2605' * int(x // 2)\n+\n                 def avgr(x):\n                     return x.n\n+\n                 # eliminate the zero ratings line as well as count == 0\n                 items = [v for v in tcategories[category].values() if v.c > 0 and v.n != 0]\n             elif category == 'authors':\n                 # Clean up the authors strings to human-readable form\n                 def formatter(x):\n                     return x.replace('|', ',')\n+\n                 items = [v for v in tcategories[category].values() if v.c > 0]\n             elif category == 'languages':\n                 # Use a human readable language string\n                 formatter = calibre_langcode_to_name\n                 items = [v for v in tcategories[category].values() if v.c > 0]\n             else:\n+\n                 def formatter(x):\n                     return str(x)\n+\n                 items = [v for v in tcategories[category].values() if v.c > 0]\n \n             # sort the list\n             if sort == 'name':\n+\n                 def kf(x):\n                     return sort_key(x.s)\n-                reverse=False\n+\n+                reverse = False\n             elif sort == 'popularity':\n+\n                 def kf(x):\n                     return x.c\n-                reverse=True\n+\n+                reverse = True\n             else:\n                 kf = avgr\n-                reverse=True\n+                reverse = True\n             items.sort(key=kf, reverse=reverse)\n \n-            if tweaks['categories_use_field_for_author_name'] == 'author_sort' and\\\n-                    (category == 'authors' or\n-                     (cat['display'].get('is_names', False) and\n-                      cat['is_custom'] and cat['is_multiple'] and\n-                      cat['datatype'] == 'text')):\n+            if tweaks['categories_use_field_for_author_name'] == 'author_sort' and (\n+                category == 'authors' or (cat['display'].get('is_names', False) and cat['is_custom'] and cat['is_multiple'] and cat['datatype'] == 'text')\n+            ):\n                 use_sort_as_name = True\n             else:\n                 use_sort_as_name = False\n-            is_editable = (category not in ['news', 'rating', 'languages'] and\n-                                datatype != 'composite')\n-            categories[category] = [tag_class(formatter(r.n), count=r.c, id=r.id,\n-                                        avg=avgr(r), sort=r.s,\n-                                        category=category,\n-                                        id_set=r.id_set, is_editable=is_editable,\n-                                        use_sort_as_name=use_sort_as_name)\n-                                    for r in items]\n+            is_editable = category not in ['news', 'rating', 'languages'] and datatype != 'composite'\n+            categories[category] = [\n+                tag_class(\n+                    formatter(r.n),\n+                    count=r.c,\n+                    id=r.id,\n+                    avg=avgr(r),\n+                    sort=r.s,\n+                    category=category,\n+                    id_set=r.id_set,\n+                    is_editable=is_editable,\n+                    use_sort_as_name=use_sort_as_name,\n+                )\n+                for r in items\n+            ]\n \n         # print('end phase \"tags list\":', time.clock() - last, 'seconds')\n         # last = time.clock()\n@@ -2062,25 +2048,30 @@ def kf(x):\n         for fmt in self.conn.get('SELECT DISTINCT format FROM data'):\n             fmt = fmt[0]\n             if ids is not None:\n-                count = self.conn.get('''SELECT COUNT(id)\n+                count = self.conn.get(\n+                    \"\"\"SELECT COUNT(id)\n                                        FROM data\n                                        WHERE format=? AND\n-                                       books_list_filter(book)''', (fmt,),\n-                                       all=False)\n+                                       books_list_filter(book)\"\"\",\n+                    (fmt,),\n+                    all=False,\n+                )\n             else:\n-                count = self.conn.get('''SELECT COUNT(id)\n+                count = self.conn.get(\n+                    \"\"\"SELECT COUNT(id)\n                                        FROM data\n-                                       WHERE format=?''', (fmt,),\n-                                       all=False)\n+                                       WHERE format=?\"\"\",\n+                    (fmt,),\n+                    all=False,\n+                )\n             if count > 0:\n-                categories['formats'].append(Tag(fmt, count=count,\n-                                                 category='formats', is_editable=False))\n+                categories['formats'].append(Tag(fmt, count=count, category='formats', is_editable=False))\n \n         if sort == 'popularity':\n             categories['formats'].sort(key=lambda x: x.count, reverse=True)\n         else:  # no ratings exist to sort on\n             # No need for ICU here.\n-            categories['formats'].sort(key=lambda x:x.name)\n+            categories['formats'].sort(key=lambda x: x.name)\n \n         # Now do identifiers. This works like formats\n         categories['identifiers'] = []\n@@ -2090,26 +2081,30 @@ def kf(x):\n         for ident in self.conn.get('SELECT DISTINCT type FROM identifiers'):\n             ident = ident[0]\n             if ids is not None:\n-                count = self.conn.get('''SELECT COUNT(book)\n+                count = self.conn.get(\n+                    \"\"\"SELECT COUNT(book)\n                                        FROM identifiers\n                                        WHERE type=? AND\n-                                       books_list_filter(book)''', (ident,),\n-                                       all=False)\n+                                       books_list_filter(book)\"\"\",\n+                    (ident,),\n+                    all=False,\n+                )\n             else:\n-                count = self.conn.get('''SELECT COUNT(id)\n+                count = self.conn.get(\n+                    \"\"\"SELECT COUNT(id)\n                                        FROM identifiers\n-                                       WHERE type=?''', (ident,),\n-                                       all=False)\n+                                       WHERE type=?\"\"\",\n+                    (ident,),\n+                    all=False,\n+                )\n             if count > 0:\n-                categories['identifiers'].append(Tag(ident, count=count,\n-                                                 category='identifiers',\n-                                                 is_editable=False))\n+                categories['identifiers'].append(Tag(ident, count=count, category='identifiers', is_editable=False))\n \n         if sort == 'popularity':\n             categories['identifiers'].sort(key=lambda x: x.count, reverse=True)\n         else:  # no ratings exist to sort on\n             # No need for ICU here.\n-            categories['identifiers'].sort(key=lambda x:x.name)\n+            categories['identifiers'].sort(key=lambda x: x.name)\n \n         # ### Now do the user-defined categories. ####\n         user_categories = dict.copy(self.clean_user_categories())\n@@ -2138,7 +2133,7 @@ def kf(x):\n         for user_cat in sorted(user_categories.keys(), key=sort_key):\n             items = []\n             names_seen = {}\n-            for (name,label,ign) in user_categories[user_cat]:\n+            for name, label, ign in user_categories[user_cat]:\n                 n = icu_lower(name)\n                 if label in taglist and n in taglist[label]:\n                     if user_cat in gst:\n@@ -2160,14 +2155,11 @@ def kf(x):\n             if icon_map is not None:\n                 icon_map[cat_name] = icon_map['user:']\n             if sort == 'popularity':\n-                categories[cat_name] = \\\n-                    sorted(items, key=lambda x: x.count, reverse=True)\n+                categories[cat_name] = sorted(items, key=lambda x: x.count, reverse=True)\n             elif sort == 'name':\n-                categories[cat_name] = \\\n-                    sorted(items, key=lambda x: sort_key(x.sort))\n+                categories[cat_name] = sorted(items, key=lambda x: sort_key(x.sort))\n             else:\n-                categories[cat_name] = \\\n-                    sorted(items, key=lambda x: x.avg_rating, reverse=True)\n+                categories[cat_name] = sorted(items, key=lambda x: x.avg_rating, reverse=True)\n \n         # ### Finally, the saved searches category ####\n         items = []\n@@ -2175,9 +2167,7 @@ def kf(x):\n         if icon_map and 'search' in icon_map:\n             icon = icon_map['search']\n         for srch in saved_searches().names():\n-            items.append(Tag(srch,\n-                             sort=srch, category='search',\n-                             is_editable=False))\n+            items.append(Tag(srch, sort=srch, category='search', is_editable=False))\n         if len(items):\n             if icon_map is not None:\n                 icon_map['search'] = icon_map['search']\n@@ -2190,9 +2180,8 @@ def kf(x):\n \n     # End get_categories\n \n-    def tags_older_than(self, tag, delta, must_have_tag=None,\n-            must_have_authors=None):\n-        '''\n+    def tags_older_than(self, tag, delta, must_have_tag=None, must_have_authors=None):\n+        \"\"\"\n         Return the ids of all books having the tag ``tag`` that are older than\n         than the specified time. tag comparison is case insensitive.\n \n@@ -2203,7 +2192,7 @@ def tags_older_than(self, tag, delta, must_have_tag=None,\n         :param must_have_authors: A list of authors. If not None the list of\n         matches will be restricted to books that have these authors (case\n         insensitive).\n-        '''\n+        \"\"\"\n         tag = tag.lower().strip()\n         mht = must_have_tag.lower().strip() if must_have_tag else None\n         now = nowf()\n@@ -2231,27 +2220,24 @@ def tags_older_than(self, tag, delta, must_have_tag=None,\n     def get_next_series_num_for(self, series):\n         series_id = None\n         if series:\n-            series_id = self.conn.get('SELECT id from series WHERE name=?',\n-                (series,), all=False)\n+            series_id = self.conn.get('SELECT id from series WHERE name=?', (series,), all=False)\n         if series_id is None:\n             if isinstance(tweaks['series_index_auto_increment'], numbers.Number):\n                 return float(tweaks['series_index_auto_increment'])\n             return 1.0\n         series_indices = self.conn.get(\n-            ('SELECT series_index FROM books WHERE id IN '\n-            '(SELECT book FROM books_series_link where series=?) '\n-            'ORDER BY series_index'),\n-            (series_id,))\n+            ('SELECT series_index FROM books WHERE id IN (SELECT book FROM books_series_link where series=?) ORDER BY series_index'), (series_id,)\n+        )\n         return self._get_next_series_num_for_list(series_indices)\n \n     def _get_next_series_num_for_list(self, series_indices):\n         return _get_next_series_num_for_list(series_indices)\n \n     def set(self, row, column, val, allow_case_change=False):\n-        '''\n+        \"\"\"\n         Convenience method for setting the title, authors, publisher, tags or\n         rating\n-        '''\n+        \"\"\"\n         id = self.data[row][0]\n         col = self.FIELD_MAP[column]\n \n@@ -2259,29 +2245,25 @@ def set(self, row, column, val, allow_case_change=False):\n         set_args = (row, col, val)\n         if column == 'authors':\n             val = string_to_authors(val)\n-            books_to_refresh |= self.set_authors(id, val, notify=False,\n-                                                 allow_case_change=allow_case_change)\n+            books_to_refresh |= self.set_authors(id, val, notify=False, allow_case_change=allow_case_change)\n         elif column == 'title':\n             self.set_title(id, val, notify=False)\n         elif column == 'publisher':\n-            books_to_refresh |= self.set_publisher(id, val, notify=False,\n-                                                   allow_case_change=allow_case_change)\n+            books_to_refresh |= self.set_publisher(id, val, notify=False, allow_case_change=allow_case_change)\n         elif column == 'rating':\n             self.set_rating(id, val, notify=False)\n         elif column == 'tags':\n-            books_to_refresh |= \\\n-                self.set_tags(id, [x.strip() for x in val.split(',') if x.strip()],\n-                    append=False, notify=False, allow_case_change=allow_case_change)\n+            books_to_refresh |= self.set_tags(\n+                id, [x.strip() for x in val.split(',') if x.strip()], append=False, notify=False, allow_case_change=allow_case_change\n+            )\n         self.data.set(*set_args)\n         self.data.refresh_ids(self, [id])\n         self.set_path(id, True)\n         self.notify('metadata', [id])\n         return books_to_refresh\n \n-    def set_metadata(self, id, mi, ignore_errors=False, set_title=True,\n-                     set_authors=True, commit=True, force_changes=False,\n-                     notify=True):\n-        '''\n+    def set_metadata(self, id, mi, ignore_errors=False, set_title=True, set_authors=True, commit=True, force_changes=False, notify=True):\n+        \"\"\"\n         Set metadata for the book `id` from the `Metadata` object `mi`\n \n         Setting force_changes=True will force set_metadata to update fields even\n@@ -2293,7 +2275,7 @@ def set_metadata(self, id, mi, ignore_errors=False, set_title=True,\n         want the book to have. Covers are always changed if a new cover is\n         provided, but are never deleted. Also note that force_changes has no\n         effect on setting title or authors.\n-        '''\n+        \"\"\"\n         if callable(getattr(mi, 'to_book_metadata', None)):\n             # Handle code passing in a OPF object instead of a Metadata object\n             mi = mi.to_book_metadata()\n@@ -2308,8 +2290,7 @@ def doit(func, *args, **kwargs):\n                     raise\n \n         def should_replace_field(attr):\n-            return (force_changes and (mi.get(attr, None) is not None)) or \\\n-                    not mi.is_null(attr)\n+            return (force_changes and (mi.get(attr, None) is not None)) or not mi.is_null(attr)\n \n         path_changed = False\n         if set_title and mi.title:\n@@ -2329,11 +2310,9 @@ def should_replace_field(attr):\n         if should_replace_field('title_sort'):\n             self.set_title_sort(id, mi.title_sort, notify=False, commit=False)\n         if should_replace_field('author_sort'):\n-            doit(self.set_author_sort, id, mi.author_sort, notify=False,\n-                    commit=False)\n+            doit(self.set_author_sort, id, mi.author_sort, notify=False, commit=False)\n         if should_replace_field('publisher'):\n-            doit(self.set_publisher, id, mi.publisher, notify=False,\n-                    commit=False)\n+            doit(self.set_publisher, id, mi.publisher, notify=False, commit=False)\n \n         # Setting rating to zero is acceptable.\n         if mi.rating is not None:\n@@ -2364,13 +2343,11 @@ def should_replace_field(attr):\n \n         # Setting series_index to zero is acceptable\n         if mi.series_index is not None:\n-            doit(self.set_series_index, id, mi.series_index, notify=False,\n-                    commit=False)\n+            doit(self.set_series_index, id, mi.series_index, notify=False, commit=False)\n         if should_replace_field('pubdate'):\n             doit(self.set_pubdate, id, mi.pubdate, notify=False, commit=False)\n         if getattr(mi, 'timestamp', None) is not None:\n-            doit(self.set_timestamp, id, mi.timestamp, notify=False,\n-                    commit=False)\n+            doit(self.set_timestamp, id, mi.timestamp, notify=False, commit=False)\n \n         # identifiers will always be replaced if force_changes is True\n         mi_idents = mi.get_identifiers()\n@@ -2385,30 +2362,33 @@ def should_replace_field(attr):\n \n         user_mi = mi.get_all_user_metadata(make_copy=False)\n         for key in user_mi:\n-            if key in self.field_metadata and \\\n-                    user_mi[key]['datatype'] == self.field_metadata[key]['datatype'] and \\\n-                    (user_mi[key]['datatype'] != 'text' or\n-                     user_mi[key]['is_multiple'] == self.field_metadata[key]['is_multiple']):\n+            if (\n+                key in self.field_metadata\n+                and user_mi[key]['datatype'] == self.field_metadata[key]['datatype']\n+                and (user_mi[key]['datatype'] != 'text' or user_mi[key]['is_multiple'] == self.field_metadata[key]['is_multiple'])\n+            ):\n                 val = mi.get(key, None)\n                 if force_changes or val is not None:\n-                    doit(self.set_custom, id, val=val, extra=mi.get_extra(key),\n-                         label=user_mi[key]['label'], commit=False, notify=False)\n+                    doit(self.set_custom, id, val=val, extra=mi.get_extra(key), label=user_mi[key]['label'], commit=False, notify=False)\n         if commit:\n             self.conn.commit()\n         if notify:\n             self.notify('metadata', [id])\n \n     def authors_sort_strings(self, id, index_is_id=False):\n-        '''\n+        \"\"\"\n         Given a book, return the list of author sort strings\n         for the book's authors\n-        '''\n+        \"\"\"\n         id = id if index_is_id else self.id(id)\n-        aut_strings = self.conn.get('''\n+        aut_strings = self.conn.get(\n+            \"\"\"\n                         SELECT sort\n                         FROM authors, books_authors_link as bl\n                         WHERE bl.book=? and authors.id=bl.author\n-                        ORDER BY bl.id''', (id,))\n+                        ORDER BY bl.id\"\"\",\n+            (id,),\n+        )\n         result = []\n         for (sort,) in aut_strings:\n             result.append(sort)\n@@ -2417,13 +2397,16 @@ def authors_sort_strings(self, id, index_is_id=False):\n     # Given a book, return the map of author sort strings for the book's authors\n     def authors_with_sort_strings(self, id, index_is_id=False):\n         id = id if index_is_id else self.id(id)\n-        aut_strings = self.conn.get('''\n+        aut_strings = self.conn.get(\n+            \"\"\"\n                         SELECT authors.id, authors.name, authors.sort, authors.link\n                         FROM authors, books_authors_link as bl\n                         WHERE bl.book=? and authors.id=bl.author\n-                        ORDER BY bl.id''', (id,))\n+                        ORDER BY bl.id\"\"\",\n+            (id,),\n+        )\n         result = []\n-        for (id_, author, sort, link) in aut_strings:\n+        for id_, author, sort, link in aut_strings:\n             result.append((id_, author.replace('|', ','), sort, link))\n         return result\n \n@@ -2435,10 +2418,13 @@ def author_sort_from_book(self, id, index_is_id=False):\n     # Given an author, return a list of books with that author\n     def books_for_author(self, id_, index_is_id=False):\n         id_ = id_ if index_is_id else self.id(id_)\n-        books = self.conn.get('''\n+        books = self.conn.get(\n+            \"\"\"\n                         SELECT bl.book\n                         FROM books_authors_link as bl\n-                        WHERE bl.author=?''', (id_,))\n+                        WHERE bl.author=?\"\"\",\n+            (id_,),\n+        )\n         return [b[0] for b in books]\n \n     # Given a list of authors, return the author_sort string for the authors,\n@@ -2447,8 +2433,7 @@ def books_for_author(self, id_, index_is_id=False):\n     def author_sort_from_authors(self, authors):\n         result = []\n         for aut in authors:\n-            r = self.conn.get('SELECT sort FROM authors WHERE name=?',\n-                              (aut.replace(',', '|'),), all=False)\n+            r = self.conn.get('SELECT sort FROM authors WHERE name=?', (aut.replace(',', '|'),), all=False)\n             if r is None:\n                 result.append(author_to_author_sort(aut))\n             else:\n@@ -2457,21 +2442,16 @@ def author_sort_from_authors(self, authors):\n \n     def _update_author_in_cache(self, id_, ss, final_authors):\n         self.conn.execute('UPDATE books SET author_sort=? WHERE id=?', (ss, id_))\n-        self.data.set(id_, self.FIELD_MAP['authors'],\n-                      ','.join([a.replace(',', '|') for a in final_authors]),\n-                      row_is_id=True)\n+        self.data.set(id_, self.FIELD_MAP['authors'], ','.join([a.replace(',', '|') for a in final_authors]), row_is_id=True)\n         self.data.set(id_, self.FIELD_MAP['author_sort'], ss, row_is_id=True)\n \n         aum = self.authors_with_sort_strings(id_, index_is_id=True)\n-        self.data.set(id_, self.FIELD_MAP['au_map'],\n-            ':#:'.join([':::'.join((au.replace(',', '|'), aus, aul))\n-                                            for (_, au, aus, aul) in aum]),\n-            row_is_id=True)\n+        self.data.set(id_, self.FIELD_MAP['au_map'], ':#:'.join([':::'.join((au.replace(',', '|'), aus, aul)) for (_, au, aus, aul) in aum]), row_is_id=True)\n \n     def _set_authors(self, id, authors, allow_case_change=False):\n         if not authors:\n             authors = [_('Unknown')]\n-        self.conn.execute('DELETE FROM books_authors_link WHERE book=?',(id,))\n+        self.conn.execute('DELETE FROM books_authors_link WHERE book=?', (id,))\n         books_to_refresh = {id}\n         final_authors = []\n         for a in authors:\n@@ -2490,23 +2470,35 @@ def _set_authors(self, id, authors, allow_case_change=False):\n                         ns = author_to_author_sort(a.replace('|', ','))\n                         if strcmp(sort, ns) == 0:\n                             sort = ns\n-                        self.conn.execute('''UPDATE authors SET name=?, sort=?\n-                                             WHERE id=?''', (a, sort, aid))\n+                        self.conn.execute(\n+                            \"\"\"UPDATE authors SET name=?, sort=?\n+                                             WHERE id=?\"\"\",\n+                            (a, sort, aid),\n+                        )\n                         case_change = True\n                     else:\n                         a = name\n             else:\n-                aid = self.conn.execute('''INSERT INTO authors(name)\n-                                           VALUES (?)''', (a,)).lastrowid\n+                aid = self.conn.execute(\n+                    \"\"\"INSERT INTO authors(name)\n+                                           VALUES (?)\"\"\",\n+                    (a,),\n+                ).lastrowid\n             final_authors.append(a.replace('|', ','))\n             try:\n-                self.conn.execute('''INSERT INTO books_authors_link(book, author)\n-                                     VALUES (?,?)''', (id, aid))\n+                self.conn.execute(\n+                    \"\"\"INSERT INTO books_authors_link(book, author)\n+                                     VALUES (?,?)\"\"\",\n+                    (id, aid),\n+                )\n             except IntegrityError:  # Sometimes books specify the same author twice in their metadata\n                 pass\n             if case_change:\n-                bks = self.conn.get('''SELECT book FROM books_authors_link\n-                                       WHERE author=?''', (aid,))\n+                bks = self.conn.get(\n+                    \"\"\"SELECT book FROM books_authors_link\n+                                       WHERE author=?\"\"\",\n+                    (aid,),\n+                )\n                 books_to_refresh |= {bk[0] for bk in bks}\n                 for bk in books_to_refresh:\n                     ss = self.author_sort_from_book(id, index_is_id=True)\n@@ -2520,10 +2512,10 @@ def _set_authors(self, id, authors, allow_case_change=False):\n         return books_to_refresh\n \n     def windows_check_if_files_in_use(self, book_id):\n-        '''\n+        \"\"\"\n         Raises an EACCES IOError if any of the files in the folder of book_id\n         are opened in another program on windows.\n-        '''\n+        \"\"\"\n         if iswindows:\n             path = self.path(book_id, index_is_id=True)\n             if path:\n@@ -2536,18 +2528,16 @@ def windows_check_if_files_in_use(self, book_id):\n                         if wam is not None:\n                             wam.close_handles()\n \n-    def set_authors(self, id, authors, notify=True, commit=True,\n-                    allow_case_change=False):\n-        '''\n+    def set_authors(self, id, authors, notify=True, commit=True, allow_case_change=False):\n+        \"\"\"\n         Note that even if commit is False, the db will still be committed to\n         because this causes the location of files to change\n \n         :param authors: A list of authors.\n-        '''\n+        \"\"\"\n         self.windows_check_if_files_in_use(id)\n-        books_to_refresh = self._set_authors(id, authors,\n-                                             allow_case_change=allow_case_change)\n-        self.dirtied({id}|books_to_refresh, commit=False)\n+        books_to_refresh = self._set_authors(id, authors, allow_case_change=allow_case_change)\n+        self.dirtied({id} | books_to_refresh, commit=False)\n         if commit:\n             self.conn.commit()\n         self.set_path(id, index_is_id=True)\n@@ -2582,19 +2572,17 @@ def _set_title(self, id, title):\n         self.data.set(id, self.FIELD_MAP['title'], title, row_is_id=True)\n         if only_case_change:\n             # SQLite update trigger will not update sort on a case change\n-            self.conn.execute('UPDATE books SET sort=? WHERE id=?',\n-                    (title_sort(title), id))\n-        ts = self.conn.get('SELECT sort FROM books WHERE id=?', (id,),\n-                all=False)\n+            self.conn.execute('UPDATE books SET sort=? WHERE id=?', (title_sort(title), id))\n+        ts = self.conn.get('SELECT sort FROM books WHERE id=?', (id,), all=False)\n         if ts:\n             self.data.set(id, self.FIELD_MAP['sort'], ts, row_is_id=True)\n         return True\n \n     def set_title(self, id, title, notify=True, commit=True):\n-        '''\n+        \"\"\"\n         Note that even if commit is False, the db will still be committed to\n         because this causes the location of files to change\n-        '''\n+        \"\"\"\n         self.windows_check_if_files_in_use(id)\n         if not self._set_title(id, title):\n             return\n@@ -2606,32 +2594,34 @@ def set_title(self, id, title, notify=True, commit=True):\n             self.notify('metadata', [id])\n \n     def set_languages(self, book_id, languages, notify=True, commit=True):\n-        self.conn.execute(\n-            'DELETE FROM books_languages_link WHERE book=?', (book_id,))\n-        self.conn.execute('''DELETE FROM languages WHERE (SELECT COUNT(id)\n+        self.conn.execute('DELETE FROM books_languages_link WHERE book=?', (book_id,))\n+        self.conn.execute(\"\"\"DELETE FROM languages WHERE (SELECT COUNT(id)\n                                  FROM books_languages_link WHERE\n-                                 books_languages_link.lang_code=languages.id) < 1''')\n+                                 books_languages_link.lang_code=languages.id) < 1\"\"\")\n \n         books_to_refresh = {book_id}\n         final_languages = []\n         for l in languages:\n             lc = canonicalize_lang(l)\n-            if not lc or lc in final_languages or lc in ('und', 'zxx', 'mis',\n-                    'mul'):\n+            if not lc or lc in final_languages or lc in ('und', 'zxx', 'mis', 'mul'):\n                 continue\n             final_languages.append(lc)\n-            lc_id = self.conn.get('SELECT id FROM languages WHERE lang_code=?',\n-                    (lc,), all=False)\n+            lc_id = self.conn.get('SELECT id FROM languages WHERE lang_code=?', (lc,), all=False)\n             if lc_id is None:\n-                lc_id = self.conn.execute('''INSERT INTO languages(lang_code)\n-                                           VALUES (?)''', (lc,)).lastrowid\n-            self.conn.execute('''INSERT INTO books_languages_link(book, lang_code)\n-                                     VALUES (?,?)''', (book_id, lc_id))\n+                lc_id = self.conn.execute(\n+                    \"\"\"INSERT INTO languages(lang_code)\n+                                           VALUES (?)\"\"\",\n+                    (lc,),\n+                ).lastrowid\n+            self.conn.execute(\n+                \"\"\"INSERT INTO books_languages_link(book, lang_code)\n+                                     VALUES (?,?)\"\"\",\n+                (book_id, lc_id),\n+            )\n         self.dirtied(books_to_refresh, commit=False)\n         if commit:\n             self.conn.commit()\n-        self.data.set(book_id, self.FIELD_MAP['languages'],\n-                ','.join(final_languages), row_is_id=True)\n+        self.data.set(book_id, self.FIELD_MAP['languages'], ','.join(final_languages), row_is_id=True)\n         if notify:\n             self.notify('metadata', [book_id])\n         return books_to_refresh\n@@ -2661,40 +2651,54 @@ def set_pubdate(self, id, dt, notify=True, commit=True):\n         if notify:\n             self.notify('metadata', [id])\n \n-    def set_publisher(self, id, publisher, notify=True, commit=True,\n-                      allow_case_change=False):\n-        self.conn.execute('DELETE FROM books_publishers_link WHERE book=?',(id,))\n+    def set_publisher(self, id, publisher, notify=True, commit=True, allow_case_change=False):\n+        self.conn.execute('DELETE FROM books_publishers_link WHERE book=?', (id,))\n         books_to_refresh = {id}\n         if publisher:\n             case_change = False\n             if not isinstance(publisher, str):\n                 publisher = publisher.decode(preferred_encoding, 'replace')\n-            pubx = self.conn.get('''SELECT id,name from publishers\n-                                    WHERE name=?''', (publisher,))\n+            pubx = self.conn.get(\n+                \"\"\"SELECT id,name from publishers\n+                                    WHERE name=?\"\"\",\n+                (publisher,),\n+            )\n             if pubx:\n                 aid, cur_name = pubx[0]\n                 if publisher != cur_name:\n                     if allow_case_change:\n-                        self.conn.execute('''UPDATE publishers SET name=?\n-                                         WHERE id=?''', (publisher, aid))\n+                        self.conn.execute(\n+                            \"\"\"UPDATE publishers SET name=?\n+                                         WHERE id=?\"\"\",\n+                            (publisher, aid),\n+                        )\n                         case_change = True\n                     else:\n                         publisher = cur_name\n                         books_to_refresh = set()\n             else:\n-                aid = self.conn.execute('''INSERT INTO publishers(name)\n-                                           VALUES (?)''', (publisher,)).lastrowid\n-            self.conn.execute('''INSERT INTO books_publishers_link(book, publisher)\n-                                 VALUES (?,?)''', (id, aid))\n+                aid = self.conn.execute(\n+                    \"\"\"INSERT INTO publishers(name)\n+                                           VALUES (?)\"\"\",\n+                    (publisher,),\n+                ).lastrowid\n+            self.conn.execute(\n+                \"\"\"INSERT INTO books_publishers_link(book, publisher)\n+                                 VALUES (?,?)\"\"\",\n+                (id, aid),\n+            )\n             if case_change:\n-                bks = self.conn.get('''SELECT book FROM books_publishers_link\n-                                       WHERE publisher=?''', (aid,))\n+                bks = self.conn.get(\n+                    \"\"\"SELECT book FROM books_publishers_link\n+                                       WHERE publisher=?\"\"\",\n+                    (aid,),\n+                )\n                 books_to_refresh |= {bk[0] for bk in bks}\n-        self.conn.execute('''DELETE FROM publishers WHERE (SELECT COUNT(id)\n+        self.conn.execute(\"\"\"DELETE FROM publishers WHERE (SELECT COUNT(id)\n                              FROM books_publishers_link\n-                             WHERE publisher=publishers.id) < 1''')\n+                             WHERE publisher=publishers.id) < 1\"\"\")\n \n-        self.dirtied({id}|books_to_refresh, commit=False)\n+        self.dirtied({id} | books_to_refresh, commit=False)\n         if commit:\n             self.conn.commit()\n         self.data.set(id, self.FIELD_MAP['publisher'], publisher, row_is_id=True)\n@@ -2714,9 +2718,7 @@ def set_uuid(self, id, uuid, notify=True, commit=True):\n \n     def get_id_from_uuid(self, uuid):\n         if uuid:\n-            return (self.data._uuid_map.get(uuid, None) or\n-                    self.conn.get('SELECT id FROM books WHERE uuid=?', (uuid,),\n-                                 all=False))\n+            return self.data._uuid_map.get(uuid, None) or self.conn.get('SELECT id FROM books WHERE uuid=?', (uuid,), all=False)\n \n     # Convenience methods for tags_list_editor\n     # Note: we generally do not need to refresh_ids because library_view will\n@@ -2732,9 +2734,7 @@ def dirty_books_referencing(self, field, id, commit=True):\n         # Get the list of books to dirty -- all books that reference the item\n         table = self.field_metadata[field]['table']\n         link = self.field_metadata[field]['link_column']\n-        bks = self.conn.get(\n-            f'SELECT book from books_{table}_link WHERE {link}=?',\n-            (id,))\n+        bks = self.conn.get(f'SELECT book from books_{table}_link WHERE {link}=?', (id,))\n         books = []\n         for (book_id,) in bks:\n             books.append(book_id)\n@@ -2761,18 +2761,27 @@ def rename_tag(self, old_id, new_name):\n         new_names = new_names[1:]\n \n         # get the list of books that reference the tag being changed\n-        books = self.conn.get('''SELECT book from books_tags_link\n-                                 WHERE tag=?''', (old_id,))\n+        books = self.conn.get(\n+            \"\"\"SELECT book from books_tags_link\n+                                 WHERE tag=?\"\"\",\n+            (old_id,),\n+        )\n         books = [b[0] for b in books]\n \n         new_id = self.conn.get(\n-                    '''SELECT id from tags\n-                       WHERE name=?''', (new_name,), all=False)\n+            \"\"\"SELECT id from tags\n+                       WHERE name=?\"\"\",\n+            (new_name,),\n+            all=False,\n+        )\n         if new_id is None or old_id == new_id:\n             # easy cases. Simply rename the tag. Do it even if equal, in case\n             # there is a change of case\n-            self.conn.execute('''UPDATE tags SET name=?\n-                                 WHERE id=?''', (new_name, old_id))\n+            self.conn.execute(\n+                \"\"\"UPDATE tags SET name=?\n+                                 WHERE id=?\"\"\",\n+                (new_name, old_id),\n+            )\n             new_id = old_id\n         else:\n             # It is possible that by renaming a tag, the tag will appear\n@@ -2781,20 +2790,28 @@ def rename_tag(self, old_id, new_name):\n             # to the new_id from books referencing the old_id, so that\n             # renaming old_id to new_id will be unique on the book\n             for book_id in books:\n-                self.conn.execute('''DELETE FROM books_tags_link\n-                                     WHERE book=? and tag=?''', (book_id, new_id))\n+                self.conn.execute(\n+                    \"\"\"DELETE FROM books_tags_link\n+                                     WHERE book=? and tag=?\"\"\",\n+                    (book_id, new_id),\n+                )\n \n             # Change the link table to point at the new tag\n-            self.conn.execute('''UPDATE books_tags_link SET tag=?\n-                                 WHERE tag=?''',(new_id, old_id,))\n+            self.conn.execute(\n+                \"\"\"UPDATE books_tags_link SET tag=?\n+                                 WHERE tag=?\"\"\",\n+                (\n+                    new_id,\n+                    old_id,\n+                ),\n+            )\n             # Get rid of the no-longer used publisher\n             self.conn.execute('DELETE FROM tags WHERE id=?', (old_id,))\n \n         if new_names:\n             # have some left-over names to process. Add them to the book.\n             for book_id in books:\n-                self.set_tags(book_id, new_names, append=True, notify=False,\n-                              commit=False)\n+                self.set_tags(book_id, new_names, append=True, notify=False, commit=False)\n         self.dirtied(books, commit=False)\n         self.clean_standard_field('tags', commit=False)\n         self.conn.commit()\n@@ -2814,34 +2831,51 @@ def get_series_with_ids(self):\n     def rename_series(self, old_id, new_name, change_index=True):\n         new_name = new_name.strip()\n         new_id = self.conn.get(\n-                    '''SELECT id from series\n-                       WHERE name=?''', (new_name,), all=False)\n+            \"\"\"SELECT id from series\n+                       WHERE name=?\"\"\",\n+            (new_name,),\n+            all=False,\n+        )\n         if new_id is None or old_id == new_id:\n             new_id = old_id\n-            self.conn.execute('UPDATE series SET name=? WHERE id=?',\n-                              (new_name, old_id))\n+            self.conn.execute('UPDATE series SET name=? WHERE id=?', (new_name, old_id))\n         else:\n             # New series exists. Must update the link, then assign a\n             # new series index to each of the books.\n \n             if change_index:\n                 # Get the list of books where we must update the series index\n-                books = self.conn.get('''SELECT books.id\n+                books = self.conn.get(\n+                    \"\"\"SELECT books.id\n                                          FROM books, books_series_link as lt\n                                          WHERE books.id = lt.book AND lt.series=?\n-                                         ORDER BY books.series_index''', (old_id,))\n+                                         ORDER BY books.series_index\"\"\",\n+                    (old_id,),\n+                )\n             # Now update the link table\n-            self.conn.execute('''UPDATE books_series_link\n+            self.conn.execute(\n+                \"\"\"UPDATE books_series_link\n                                  SET series=?\n-                                 WHERE series=?''',(new_id, old_id,))\n+                                 WHERE series=?\"\"\",\n+                (\n+                    new_id,\n+                    old_id,\n+                ),\n+            )\n             if change_index and tweaks['series_index_auto_increment'] != 'no_change':\n                 # Now set the indices\n                 for (book_id,) in books:\n                     # Get the next series index\n                     index = self.get_next_series_num_for(new_name)\n-                    self.conn.execute('''UPDATE books\n+                    self.conn.execute(\n+                        \"\"\"UPDATE books\n                                          SET series_index=?\n-                                         WHERE id=?''',(index, book_id,))\n+                                         WHERE id=?\"\"\",\n+                        (\n+                            index,\n+                            book_id,\n+                        ),\n+                    )\n         self.dirty_books_referencing('series', new_id, commit=False)\n         self.clean_standard_field('series', commit=False)\n         self.conn.commit()\n@@ -2864,18 +2898,26 @@ def get_publishers_with_ids(self):\n     def rename_publisher(self, old_id, new_name):\n         new_name = new_name.strip()\n         new_id = self.conn.get(\n-                    '''SELECT id from publishers\n-                       WHERE name=?''', (new_name,), all=False)\n+            \"\"\"SELECT id from publishers\n+                       WHERE name=?\"\"\",\n+            (new_name,),\n+            all=False,\n+        )\n         if new_id is None or old_id == new_id:\n             new_id = old_id\n             # New name doesn't exist. Simply change the old name\n-            self.conn.execute('UPDATE publishers SET name=? WHERE id=?',\n-                              (new_name, old_id))\n+            self.conn.execute('UPDATE publishers SET name=? WHERE id=?', (new_name, old_id))\n         else:\n             # Change the link table to point at the new one\n-            self.conn.execute('''UPDATE books_publishers_link\n+            self.conn.execute(\n+                \"\"\"UPDATE books_publishers_link\n                                  SET publisher=?\n-                                 WHERE publisher=?''',(new_id, old_id,))\n+                                 WHERE publisher=?\"\"\",\n+                (\n+                    new_id,\n+                    old_id,\n+                ),\n+            )\n             # Get rid of the no-longer used publisher\n             self.conn.execute('DELETE FROM publishers WHERE id=?', (old_id,))\n         self.dirty_books_referencing('publisher', new_id, commit=False)\n@@ -2884,8 +2926,11 @@ def rename_publisher(self, old_id, new_name):\n \n     def delete_publisher_using_id(self, old_id):\n         self.dirty_books_referencing('publisher', old_id, commit=False)\n-        self.conn.execute('''DELETE FROM books_publishers_link\n-                             WHERE publisher=?''', (old_id,))\n+        self.conn.execute(\n+            \"\"\"DELETE FROM books_publishers_link\n+                             WHERE publisher=?\"\"\",\n+            (old_id,),\n+        )\n         self.conn.execute('DELETE FROM publishers WHERE id=?', (old_id,))\n         self.conn.commit()\n \n@@ -2897,8 +2942,7 @@ def get_authors_with_ids(self):\n \n     def get_author_id(self, author):\n         author = author.replace(',', '|')\n-        result = self.conn.get('SELECT id FROM authors WHERE name=?',\n-                               (author,), all=False)\n+        result = self.conn.get('SELECT id FROM authors WHERE name=?', (author,), all=False)\n         return result\n \n     def set_link_field_for_author(self, aid, link, commit=True, notify=False):\n@@ -2909,8 +2953,7 @@ def set_link_field_for_author(self, aid, link, commit=True, notify=False):\n             self.conn.commit()\n \n     def set_sort_field_for_author(self, old_id, new_sort, commit=True, notify=False):\n-        self.conn.execute('UPDATE authors SET sort=? WHERE id=?',\n-                              (new_sort.strip(), old_id))\n+        self.conn.execute('UPDATE authors SET sort=? WHERE id=?', (new_sort.strip(), old_id))\n         if commit:\n             self.conn.commit()\n         # Now change all the author_sort fields in books by this author\n@@ -2933,18 +2976,15 @@ def rename_author(self, old_id, new_name):\n             books.append(book_id)\n \n         # check if the new author already exists\n-        new_id = self.conn.get('SELECT id from authors WHERE name=?',\n-                                (new_name,), all=False)\n+        new_id = self.conn.get('SELECT id from authors WHERE name=?', (new_name,), all=False)\n         if new_id is None or old_id == new_id:\n             # No name clash. Go ahead and update the author's name\n-            self.conn.execute('UPDATE authors SET name=? WHERE id=?',\n-                              (new_name, old_id))\n+            self.conn.execute('UPDATE authors SET name=? WHERE id=?', (new_name, old_id))\n         else:\n             # First check for the degenerate case -- changing a value to itself.\n             # Update it in case there is a change of case, but do nothing else\n             if old_id == new_id:\n-                self.conn.execute('UPDATE authors SET name=? WHERE id=?',\n-                              (new_name, old_id))\n+                self.conn.execute('UPDATE authors SET name=? WHERE id=?', (new_name, old_id))\n                 self.conn.commit()\n                 return new_id\n             # Author exists. To fix this, we must replace all the authors\n@@ -2955,24 +2995,33 @@ def rename_author(self, old_id, new_name):\n \n             for book_id in books:\n                 # Get the existing list of authors\n-                authors = self.conn.get('''\n+                authors = self.conn.get(\n+                    \"\"\"\n                     SELECT author from books_authors_link\n                     WHERE book=?\n-                    ORDER BY id''',(book_id,))\n+                    ORDER BY id\"\"\",\n+                    (book_id,),\n+                )\n \n                 # unpack the double-list structure, replacing the old author\n                 # with the new one while we are at it\n-                for i,aut in enumerate(authors):\n+                for i, aut in enumerate(authors):\n                     authors[i] = aut[0] if aut[0] != old_id else new_id\n                 # Delete the existing authors list\n-                self.conn.execute('''DELETE FROM books_authors_link\n-                                     WHERE book=?''',(book_id,))\n+                self.conn.execute(\n+                    \"\"\"DELETE FROM books_authors_link\n+                                     WHERE book=?\"\"\",\n+                    (book_id,),\n+                )\n                 # Change the authors to the new list\n                 for aid in authors:\n                     try:\n-                        self.conn.execute('''\n+                        self.conn.execute(\n+                            \"\"\"\n                             INSERT INTO books_authors_link(book, author)\n-                            VALUES (?,?)''', (book_id, aid))\n+                            VALUES (?,?)\"\"\",\n+                            (book_id, aid),\n+                        )\n                     except IntegrityError:\n                         # Sometimes books specify the same author twice in their\n                         # metadata. Ignore it.\n@@ -2998,9 +3047,7 @@ def rename_author(self, old_id, new_name):\n     # end convenience methods\n \n     def get_tags(self, id):\n-        result = self.conn.get(\n-        'SELECT name FROM tags WHERE id IN (SELECT tag FROM books_tags_link WHERE book=?)',\n-        (id,), all=True)\n+        result = self.conn.get('SELECT name FROM tags WHERE id IN (SELECT tag FROM books_tags_link WHERE book=?)', (id,), all=True)\n         if not result:\n             return set()\n         return {r[0] for r in result}\n@@ -3008,8 +3055,7 @@ def get_tags(self, id):\n     @classmethod\n     def cleanup_tags(cls, tags):\n         tags = [x.strip().replace(',', ';') for x in tags if x.strip()]\n-        tags = [x.decode(preferred_encoding, 'replace')\n-                    if isbytestring(x) else x for x in tags]\n+        tags = [x.decode(preferred_encoding, 'replace') if isbytestring(x) else x for x in tags]\n         tags = [' '.join(x.split()) for x in tags]\n         ans, seen = [], set()\n         for tag in tags:\n@@ -3019,8 +3065,7 @@ def cleanup_tags(cls, tags):\n         return ans\n \n     def remove_all_tags(self, ids, notify=False, commit=True):\n-        self.conn.executemany(\n-            'DELETE FROM books_tags_link WHERE book=?', [(x,) for x in ids])\n+        self.conn.executemany('DELETE FROM books_tags_link WHERE book=?', [(x,) for x in ids])\n         self.dirtied(ids, commit=False)\n         if commit:\n             self.conn.commit()\n@@ -3042,43 +3087,39 @@ def bulk_modify_tags(self, ids, add=[], remove=[], notify=False):\n         lt = [t.lower() for t in all_tags]\n         new_tags = [t for t in add if t.lower() not in lt]\n         if new_tags:\n-            self.conn.executemany('INSERT INTO tags(name) VALUES (?)', [(x,) for x in\n-                new_tags])\n+            self.conn.executemany('INSERT INTO tags(name) VALUES (?)', [(x,) for x in new_tags])\n \n         # Create the temporary tables to store the ids for books and tags\n         # to be operated on\n-        tables = ('temp_bulk_tag_edit_books', 'temp_bulk_tag_edit_add',\n-                    'temp_bulk_tag_edit_remove')\n+        tables = ('temp_bulk_tag_edit_books', 'temp_bulk_tag_edit_add', 'temp_bulk_tag_edit_remove')\n         drops = '\\n'.join([f'DROP TABLE IF EXISTS {t};' for t in tables])\n-        creates = '\\n'.join([f'CREATE TEMP TABLE {t}(id INTEGER PRIMARY KEY);'\n-                for t in tables])\n+        creates = '\\n'.join([f'CREATE TEMP TABLE {t}(id INTEGER PRIMARY KEY);' for t in tables])\n         self.conn.executescript(drops + creates)\n \n         # Populate the books temp table\n-        self.conn.executemany(\n-            'INSERT INTO temp_bulk_tag_edit_books VALUES (?)', [(x,) for x in ids])\n+        self.conn.executemany('INSERT INTO temp_bulk_tag_edit_books VALUES (?)', [(x,) for x in ids])\n \n         # Populate the add/remove tags temp tables\n         for table, tags in enumerate([add, remove]):\n             if not tags:\n                 continue\n-            table = tables[table+1]\n-            insert = ('INSERT INTO %s(id) SELECT tags.id FROM tags WHERE name=?'\n-                     ' COLLATE PYNOCASE LIMIT 1')\n-            self.conn.executemany(insert%table, [(x,) for x in tags])\n+            table = tables[table + 1]\n+            insert = 'INSERT INTO %s(id) SELECT tags.id FROM tags WHERE name=? COLLATE PYNOCASE LIMIT 1'\n+            self.conn.executemany(insert % table, [(x,) for x in tags])\n \n         if remove:\n             self.conn.execute(\n-              f'''DELETE FROM books_tags_link WHERE\n+                f\"\"\"DELETE FROM books_tags_link WHERE\n                     book IN (SELECT id FROM {tables[0]}) AND\n-                    tag IN (SELECT id FROM {tables[2]})''')\n+                    tag IN (SELECT id FROM {tables[2]})\"\"\"\n+            )\n \n         if add:\n             self.conn.execute(\n-            f'''\n+                f\"\"\"\n             INSERT OR REPLACE INTO books_tags_link(book, tag) SELECT {tables[0]}.id, {tables[1]}.id FROM\n             {tables[0]}, {tables[1]}\n-            '''\n+            \"\"\"\n             )\n         self.conn.executescript(drops)\n         self.dirtied(ids, commit=False)\n@@ -3093,12 +3134,11 @@ def bulk_modify_tags(self, ids, add=[], remove=[], notify=False):\n     def commit(self):\n         self.conn.commit()\n \n-    def set_tags(self, id, tags, append=False, notify=True, commit=True,\n-                 allow_case_change=False):\n-        '''\n+    def set_tags(self, id, tags, append=False, notify=True, commit=True, allow_case_change=False):\n+        \"\"\"\n         @param tags: list of strings\n         @param append: If True existing tags are not removed\n-        '''\n+        \"\"\"\n         if not tags:\n             tags = []\n         if not append:\n@@ -3106,7 +3146,7 @@ def set_tags(self, id, tags, append=False, notify=True, commit=True,\n         otags = self.get_tags(id)\n         tags = self.cleanup_tags(tags)\n         books_to_refresh = {id}\n-        for tag in (set(tags)-otags):\n+        for tag in set(tags) - otags:\n             case_changed = False\n             tag = tag.strip()\n             if not tag:\n@@ -3128,17 +3168,23 @@ def set_tags(self, id, tags, append=False, notify=True, commit=True,\n             else:\n                 tid = self.conn.execute('INSERT INTO tags(name) VALUES(?)', (tag,)).lastrowid\n \n-            if not self.conn.get('''SELECT book FROM books_tags_link\n-                                    WHERE book=? AND tag=?''', (id, tid), all=False):\n-                self.conn.execute('''INSERT INTO books_tags_link(book, tag)\n-                                     VALUES (?,?)''', (id, tid))\n+            if not self.conn.get(\n+                \"\"\"SELECT book FROM books_tags_link\n+                                    WHERE book=? AND tag=?\"\"\",\n+                (id, tid),\n+                all=False,\n+            ):\n+                self.conn.execute(\n+                    \"\"\"INSERT INTO books_tags_link(book, tag)\n+                                     VALUES (?,?)\"\"\",\n+                    (id, tid),\n+                )\n             if case_changed:\n-                bks = self.conn.get('SELECT book FROM books_tags_link WHERE tag=?',\n-                                        (tid,))\n+                bks = self.conn.get('SELECT book FROM books_tags_link WHERE tag=?', (tid,))\n                 books_to_refresh |= {bk[0] for bk in bks}\n-        self.conn.execute('''DELETE FROM tags WHERE (SELECT COUNT(id)\n-                                FROM books_tags_link WHERE tag=tags.id) < 1''')\n-        self.dirtied({id}|books_to_refresh, commit=False)\n+        self.conn.execute(\"\"\"DELETE FROM tags WHERE (SELECT COUNT(id)\n+                                FROM books_tags_link WHERE tag=tags.id) < 1\"\"\")\n+        self.dirtied({id} | books_to_refresh, commit=False)\n         if commit:\n             self.conn.commit()\n         tags = ','.join(self.get_tags(id))\n@@ -3151,8 +3197,11 @@ def unapply_tags(self, book_id, tags, notify=True):\n         for tag in tags:\n             id = self.conn.get('SELECT id FROM tags WHERE name=?', (tag,), all=False)\n             if id:\n-                self.conn.execute('''DELETE FROM books_tags_link\n-                                     WHERE tag=? AND book=?''', (id, book_id))\n+                self.conn.execute(\n+                    \"\"\"DELETE FROM books_tags_link\n+                                     WHERE tag=? AND book=?\"\"\",\n+                    (id, book_id),\n+                )\n         self.conn.commit()\n         self.data.refresh_ids(self, [book_id])\n         if notify:\n@@ -3187,7 +3236,7 @@ def _get_series_values(self, val):\n         return _get_series_values(val)\n \n     def set_series(self, id, series, notify=True, commit=True, allow_case_change=True):\n-        self.conn.execute('DELETE FROM books_series_link WHERE book=?',(id,))\n+        self.conn.execute('DELETE FROM books_series_link WHERE book=?', (id,))\n         series, idx = self._get_series_values(series)\n         books_to_refresh = {id}\n         if series:\n@@ -3212,12 +3261,11 @@ def set_series(self, id, series, notify=True, commit=True, allow_case_change=Tru\n             if idx:\n                 self.set_series_index(id, idx, notify=notify, commit=commit)\n             if case_change:\n-                bks = self.conn.get('SELECT book FROM books_series_link WHERE series=?',\n-                                        (aid,))\n+                bks = self.conn.get('SELECT book FROM books_series_link WHERE series=?', (aid,))\n                 books_to_refresh |= {bk[0] for bk in bks}\n-        self.conn.execute('''DELETE FROM series\n+        self.conn.execute(\"\"\"DELETE FROM series\n                              WHERE (SELECT COUNT(id) FROM books_series_link\n-                                    WHERE series=series.id) < 1''')\n+                                    WHERE series=series.id) < 1\"\"\")\n         self.dirtied([id], commit=False)\n         if commit:\n             self.conn.commit()\n@@ -3245,7 +3293,7 @@ def set_rating(self, id, rating, notify=True, commit=True):\n         if not rating:\n             rating = 0\n         rating = int(rating)\n-        self.conn.execute('DELETE FROM books_ratings_link WHERE book=?',(id,))\n+        self.conn.execute('DELETE FROM books_ratings_link WHERE book=?', (id,))\n         rat = self.conn.get('SELECT id FROM ratings WHERE rating=?', (rating,), all=False)\n         rat = rat if rat is not None else self.conn.execute('INSERT INTO ratings(rating) VALUES (?)', (rating,)).lastrowid\n         self.conn.execute('INSERT INTO books_ratings_link(book, rating) VALUES (?,?)', (id, rat))\n@@ -3313,7 +3361,7 @@ def _clean_identifier(self, typ, val):\n         return typ, val\n \n     def set_identifier(self, id_, typ, val, notify=True, commit=True):\n-        'If val is empty, deletes identifier of type typ'\n+        \"If val is empty, deletes identifier of type typ\"\n         typ, val = self._clean_identifier(typ, val)\n         identifiers = self.get_identifiers(id_, index_is_id=True)\n         if not typ:\n@@ -3322,18 +3370,14 @@ def set_identifier(self, id_, typ, val, notify=True, commit=True):\n         if not val and typ in identifiers:\n             identifiers.pop(typ)\n             changed = True\n-            self.conn.execute(\n-                'DELETE from identifiers WHERE book=? AND type=?', (id_, typ))\n+            self.conn.execute('DELETE from identifiers WHERE book=? AND type=?', (id_, typ))\n         if val and identifiers.get(typ, None) != val:\n             changed = True\n             identifiers[typ] = val\n-            self.conn.execute(\n-                'INSERT OR REPLACE INTO identifiers (book, type, val) VALUES (?, ?, ?)', (id_, typ, val))\n+            self.conn.execute('INSERT OR REPLACE INTO identifiers (book, type, val) VALUES (?, ?, ?)', (id_, typ, val))\n         if changed:\n-            raw = ','.join([f'{k}:{v}' for k, v in\n-                iteritems(identifiers)])\n-            self.data.set(id_, self.FIELD_MAP['identifiers'], raw,\n-                    row_is_id=True)\n+            raw = ','.join([f'{k}:{v}' for k, v in iteritems(identifiers)])\n+            self.data.set(id_, self.FIELD_MAP['identifiers'], raw, row_is_id=True)\n             if commit:\n                 self.conn.commit()\n             if notify:\n@@ -3348,13 +3392,9 @@ def set_identifiers(self, id_, identifiers, notify=True, commit=True):\n             if val:\n                 cleaned[typ] = val\n         self.conn.execute('DELETE FROM identifiers WHERE book=?', (id_,))\n-        self.conn.executemany(\n-            'INSERT INTO identifiers (book, type, val) VALUES (?, ?, ?)',\n-            [(id_, k, v) for k, v in iteritems(cleaned)])\n-        raw = ','.join([f'{k}:{v}' for k, v in\n-                iteritems(cleaned)])\n-        self.data.set(id_, self.FIELD_MAP['identifiers'], raw,\n-                    row_is_id=True)\n+        self.conn.executemany('INSERT INTO identifiers (book, type, val) VALUES (?, ?, ?)', [(id_, k, v) for k, v in iteritems(cleaned)])\n+        raw = ','.join([f'{k}:{v}' for k, v in iteritems(cleaned)])\n+        self.data.set(id_, self.FIELD_MAP['identifiers'], raw, row_is_id=True)\n         if commit:\n             self.conn.commit()\n         if notify:\n@@ -3368,16 +3408,15 @@ def add_catalog(self, path, title):\n \n         format = os.path.splitext(path)[1][1:].lower()\n         with open(path, 'rb') as stream:\n-            matches = self.data.get_matches('title', '='+title)\n+            matches = self.data.get_matches('title', '=' + title)\n             if matches:\n-                tag_matches = self.data.get_matches('tags', '='+_('Catalog'))\n+                tag_matches = self.data.get_matches('tags', '=' + _('Catalog'))\n                 matches = matches.intersection(tag_matches)\n             db_id = None\n             if matches:\n                 db_id = list(matches)[0]\n             if db_id is None:\n-                obj = self.conn.execute('INSERT INTO books(title, author_sort) VALUES (?, ?)',\n-                                    (title, 'calibre'))\n+                obj = self.conn.execute('INSERT INTO books(title, author_sort) VALUES (?, ?)', (title, 'calibre'))\n                 db_id = obj.lastrowid\n                 self.data.books_added([db_id], self)\n                 self.set_path(db_id, index_is_id=True)\n@@ -3405,8 +3444,7 @@ def add_news(self, path, arg):\n         format = os.path.splitext(path)[1][1:].lower()\n         stream = path if hasattr(path, 'read') else open(path, 'rb')\n         stream.seek(0)\n-        mi = get_metadata(stream, format, use_libprs_metadata=False,\n-                force_read_metadata=True)\n+        mi = get_metadata(stream, format, use_libprs_metadata=False, force_read_metadata=True)\n         # Force the author to calibre as the auto delete of old news checks for\n         # both the author==calibre and the tag News\n         mi.authors = ['calibre']\n@@ -3418,8 +3456,7 @@ def add_news(self, path, arg):\n             mi.tags += [arg['title']]\n         if arg['custom_tags']:\n             mi.tags += arg['custom_tags']\n-        obj = self.conn.execute('INSERT INTO books(title, author_sort) VALUES (?, ?)',\n-                              (mi.title, mi.authors[0]))\n+        obj = self.conn.execute('INSERT INTO books(title, author_sort) VALUES (?, ?)', (mi.title, mi.authors[0]))\n         id = obj.lastrowid\n         self.data.books_added([id], self)\n         self.set_path(id, index_is_id=True)\n@@ -3441,7 +3478,7 @@ def run_import_plugins(self, path_or_stream, format):\n         format = format.lower()\n         if hasattr(path_or_stream, 'seek'):\n             path_or_stream.seek(0)\n-            pt = PersistentTemporaryFile('_import_plugin.'+format)\n+            pt = PersistentTemporaryFile('_import_plugin.' + format)\n             shutil.copyfileobj(path_or_stream, pt, 1024**2)\n             pt.close()\n             path = pt.name\n@@ -3459,15 +3496,13 @@ def _add_newbook_tag(self, mi):\n                     else:\n                         mi.tags.append(tag)\n \n-    def create_book_entry(self, mi, cover=None, add_duplicates=True,\n-            force_id=None):\n+    def create_book_entry(self, mi, cover=None, add_duplicates=True, force_id=None):\n         if mi.tags:\n             mi.tags = list(mi.tags)\n         self._add_newbook_tag(mi)\n         if not add_duplicates and self.has_book(mi):\n             return None\n-        series_index = self.get_next_series_num_for(mi.series) \\\n-                    if mi.series_index is None else mi.series_index\n+        series_index = self.get_next_series_num_for(mi.series) if mi.series_index is None else mi.series_index\n         aus = mi.author_sort if mi.author_sort else self.author_sort_from_authors(mi.authors)\n         title = mi.title\n         if isbytestring(aus):\n@@ -3475,14 +3510,11 @@ def create_book_entry(self, mi, cover=None, add_duplicates=True,\n         if isbytestring(title):\n             title = title.decode(preferred_encoding, 'replace')\n         if force_id is None:\n-            obj = self.conn.execute('INSERT INTO books(title, series_index, author_sort) VALUES (?, ?, ?)',\n-                                (title, series_index, aus))\n+            obj = self.conn.execute('INSERT INTO books(title, series_index, author_sort) VALUES (?, ?, ?)', (title, series_index, aus))\n             id = obj.lastrowid\n         else:\n             id = force_id\n-            obj = self.conn.execute(\n-                    'INSERT INTO books(id, title, series_index, '\n-                    'author_sort) VALUES (?, ?, ?, ?)', (id, title, series_index, aus))\n+            obj = self.conn.execute('INSERT INTO books(id, title, series_index, author_sort) VALUES (?, ?, ?, ?)', (id, title, series_index, aus))\n \n         self.data.books_added([id], self)\n         if mi.timestamp is None:\n@@ -3497,12 +3529,11 @@ def create_book_entry(self, mi, cover=None, add_duplicates=True,\n                 traceback.print_exc()\n         return id\n \n-    def add_books(self, paths, formats, metadata, add_duplicates=True,\n-            return_ids=False):\n-        '''\n+    def add_books(self, paths, formats, metadata, add_duplicates=True, return_ids=False):\n+        \"\"\"\n         Add a book to the database. The result cache is not updated.\n         :param:`paths` List of paths to book files or file-like objects\n-        '''\n+        \"\"\"\n         formats, metadata = iter(formats), iter(metadata)\n         duplicates = []\n         ids = []\n@@ -3514,16 +3545,14 @@ def add_books(self, paths, formats, metadata, add_duplicates=True,\n             if not add_duplicates and self.has_book(mi):\n                 duplicates.append((path, format, mi))\n                 continue\n-            series_index = self.get_next_series_num_for(mi.series) \\\n-                            if mi.series_index is None else mi.series_index\n+            series_index = self.get_next_series_num_for(mi.series) if mi.series_index is None else mi.series_index\n             aus = mi.author_sort if mi.author_sort else self.author_sort_from_authors(mi.authors)\n             title = mi.title\n             if isinstance(aus, bytes):\n                 aus = aus.decode(preferred_encoding, 'replace')\n             if isinstance(title, bytes):\n                 title = title.decode(preferred_encoding)\n-            obj = self.conn.execute('INSERT INTO books(title, series_index, author_sort) VALUES (?, ?, ?)',\n-                              (title, series_index, aus))\n+            obj = self.conn.execute('INSERT INTO books(title, series_index, author_sort) VALUES (?, ?, ?)', (title, series_index, aus))\n             id = obj.lastrowid\n             self.data.books_added([id], self)\n             ids.append(id)\n@@ -3543,16 +3572,14 @@ def add_books(self, paths, formats, metadata, add_duplicates=True,\n         for book_id, fmt in postimport:\n             run_plugins_on_postimport(self, book_id, fmt)\n         if duplicates:\n-            paths    = [duplicate[0] for duplicate in duplicates]\n-            formats  = [duplicate[1] for duplicate in duplicates]\n+            paths = [duplicate[0] for duplicate in duplicates]\n+            formats = [duplicate[1] for duplicate in duplicates]\n             metadata = [duplicate[2] for duplicate in duplicates]\n             return (paths, formats, metadata), (ids if return_ids else len(ids))\n         return None, (ids if return_ids else len(ids))\n \n-    def import_book(self, mi, formats, notify=True, import_hooks=True,\n-            apply_import_tags=True, preserve_uuid=False):\n-        series_index = self.get_next_series_num_for(mi.series) \\\n-                        if mi.series_index is None else mi.series_index\n+    def import_book(self, mi, formats, notify=True, import_hooks=True, apply_import_tags=True, preserve_uuid=False):\n+        series_index = self.get_next_series_num_for(mi.series) if mi.series_index is None else mi.series_index\n         if apply_import_tags:\n             self._add_newbook_tag(mi)\n         if not mi.title:\n@@ -3562,10 +3589,8 @@ def import_book(self, mi, formats, notify=True, import_hooks=True,\n         aus = mi.author_sort if mi.author_sort else self.author_sort_from_authors(mi.authors)\n         if isinstance(aus, bytes):\n             aus = aus.decode(preferred_encoding, 'replace')\n-        title = mi.title if isinstance(mi.title, str) else \\\n-                mi.title.decode(preferred_encoding, 'replace')\n-        obj = self.conn.execute('INSERT INTO books(title, series_index, author_sort) VALUES (?, ?, ?)',\n-                          (title, series_index, aus))\n+        title = mi.title if isinstance(mi.title, str) else mi.title.decode(preferred_encoding, 'replace')\n+        obj = self.conn.execute('INSERT INTO books(title, series_index, author_sort) VALUES (?, ?, ?)', (title, series_index, aus))\n         id = obj.lastrowid\n         self.data.books_added([id], self)\n         if mi.timestamp is None:\n@@ -3614,8 +3639,10 @@ def get_top_level_move_items(self):\n \n     def move_library_to(self, newloc, progress=None):\n         if progress is None:\n+\n             def progress(x):\n                 return x\n+\n         if not os.path.exists(newloc):\n             os.makedirs(newloc)\n         old_dirs = set()\n@@ -3665,15 +3692,13 @@ def all_ids(self):\n     def find_books_in_directory(self, dirpath, single_book_per_directory):\n         return find_books_in_directory(dirpath, single_book_per_directory)\n \n-    def import_book_directory_multiple(self, dirpath, callback=None,\n-            added_ids=None):\n+    def import_book_directory_multiple(self, dirpath, callback=None, added_ids=None):\n         return import_book_directory_multiple(self, dirpath, callback=callback, added_ids=added_ids)\n \n     def import_book_directory(self, dirpath, callback=None, added_ids=None):\n         return import_book_directory(self, dirpath, callback=callback, added_ids=added_ids)\n \n-    def recursive_import(self, root, single_book_per_directory=True,\n-            callback=None, added_ids=None):\n+    def recursive_import(self, root, single_book_per_directory=True, callback=None, added_ids=None):\n         return recursive_import(self, root, single_book_per_directory=single_book_per_directory, callback=callback, added_ids=added_ids)\n \n     def add_custom_book_data(self, book_id, name, val):\n@@ -3682,23 +3707,30 @@ def add_custom_book_data(self, book_id, name, val):\n             raise ValueError(f'add_custom_book_data: no such book_id {book_id}')\n         # Do the json encode first, in case it throws an exception\n         s = json.dumps(val, default=to_json)\n-        self.conn.execute('''INSERT OR REPLACE INTO books_plugin_data(book, name, val)\n-                             VALUES(?, ?, ?)''', (book_id, name, s))\n+        self.conn.execute(\n+            \"\"\"INSERT OR REPLACE INTO books_plugin_data(book, name, val)\n+                             VALUES(?, ?, ?)\"\"\",\n+            (book_id, name, s),\n+        )\n         self.commit()\n \n     def add_multiple_custom_book_data(self, name, vals, delete_first=False):\n         if delete_first:\n-            self.conn.execute('DELETE FROM books_plugin_data WHERE name=?', (name, ))\n+            self.conn.execute('DELETE FROM books_plugin_data WHERE name=?', (name,))\n         self.conn.executemany(\n             'INSERT OR REPLACE INTO books_plugin_data (book, name, val) VALUES (?, ?, ?)',\n-            [(book_id, name, json.dumps(val, default=to_json))\n-                    for book_id, val in iteritems(vals)])\n+            [(book_id, name, json.dumps(val, default=to_json)) for book_id, val in iteritems(vals)],\n+        )\n         self.commit()\n \n     def get_custom_book_data(self, book_id, name, default=None):\n         try:\n-            s = self.conn.get('''select val FROM books_plugin_data\n-                    WHERE book=? AND name=?''', (book_id, name), all=False)\n+            s = self.conn.get(\n+                \"\"\"select val FROM books_plugin_data\n+                    WHERE book=? AND name=?\"\"\",\n+                (book_id, name),\n+                all=False,\n+            )\n             if s is None:\n                 return default\n             return json.loads(s, object_hook=from_json)\n@@ -3708,8 +3740,11 @@ def get_custom_book_data(self, book_id, name, default=None):\n \n     def get_all_custom_book_data(self, name, default=None):\n         try:\n-            s = self.conn.get('''select book, val FROM books_plugin_data\n-                    WHERE name=?''', (name,))\n+            s = self.conn.get(\n+                \"\"\"select book, val FROM books_plugin_data\n+                    WHERE name=?\"\"\",\n+                (name,),\n+            )\n             if s is None:\n                 return default\n             res = {}\n@@ -3721,25 +3756,22 @@ def get_all_custom_book_data(self, name, default=None):\n         return default\n \n     def delete_custom_book_data(self, book_id, name):\n-        self.conn.execute('DELETE FROM books_plugin_data WHERE book=? AND name=?',\n-                          (book_id, name))\n+        self.conn.execute('DELETE FROM books_plugin_data WHERE book=? AND name=?', (book_id, name))\n         self.commit()\n \n     def delete_all_custom_book_data(self, name):\n-        self.conn.execute('DELETE FROM books_plugin_data WHERE name=?', (name, ))\n+        self.conn.execute('DELETE FROM books_plugin_data WHERE name=?', (name,))\n         self.commit()\n \n     def get_ids_for_custom_book_data(self, name):\n-        s = self.conn.get('''SELECT book FROM books_plugin_data WHERE name=?''', (name,))\n+        s = self.conn.get(\"\"\"SELECT book FROM books_plugin_data WHERE name=?\"\"\", (name,))\n         return [x[0] for x in s]\n \n     def get_usage_count_by_id(self, field):\n         fm = self.field_metadata[field]\n         if not fm.get('link_column', None):\n             raise ValueError('%s is not an is_multiple field')\n-        return self.conn.get(\n-            'SELECT {0}, count(*) FROM books_{1}_link GROUP BY {0}'.format(\n-                fm['link_column'], fm['table']))\n+        return self.conn.get('SELECT {0}, count(*) FROM books_{1}_link GROUP BY {0}'.format(fm['link_column'], fm['table']))\n \n     def all_author_names(self):\n         ai = self.FIELD_MAP['authors']\ndiff --git a/src/calibre/scraper/test_fetch_backend.py b/src/calibre/scraper/test_fetch_backend.py\nindex 0bdd09d..47f5cff 100644\n--- a/src/calibre/scraper/test_fetch_backend.py\n+++ b/src/calibre/scraper/test_fetch_backend.py\n@@ -21,7 +21,6 @@\n \n \n class Handler(http.server.BaseHTTPRequestHandler):\n-\n     def __init__(self, test_obj, *a):\n         self.test_obj = test_obj\n         super().__init__(*a)\n@@ -75,7 +74,6 @@ def log_request(self, code='-', size='-'):\n \n @unittest.skipIf(skip, skip)\n class TestFetchBackend(unittest.TestCase):\n-\n     ae = unittest.TestCase.assertEqual\n \n     def setUp(self):\ndiff --git a/src/calibre/srv/pool.py b/src/calibre/srv/pool.py\nindex b950ae6..065effc 100644\n--- a/src/calibre/srv/pool.py\n+++ b/src/calibre/srv/pool.py\n@@ -12,7 +12,6 @@\n \n \n class Worker(Thread):\n-\n     daemon = True\n \n     def __init__(self, log, notify_server, num, request_queue, result_queue):\n@@ -47,7 +46,6 @@ def handle_error(self, job_id):\n \n \n class ThreadPool:\n-\n     def __init__(self, log, notify_server, count=10, queue_size=1000):\n         self.request_queue, self.result_queue = Queue(queue_size), Queue(queue_size)\n         self.workers = [Worker(log, notify_server, i, self.request_queue, self.result_queue) for i in range(count)]\n@@ -85,7 +83,6 @@ def idle(self):\n \n \n class PluginPool:\n-\n     def __init__(self, loop, plugins):\n         self.workers = []\n         self.loop = loop\ndiff --git a/src/calibre/srv/tests/http.py b/src/calibre/srv/tests/http.py\nindex ecdd024..1b221a8 100644\n--- a/src/calibre/srv/tests/http.py\n+++ b/src/calibre/srv/tests/http.py\n@@ -24,9 +24,8 @@\n \n \n class TestHTTP(BaseTest):\n-\n     def test_header_parsing(self):  # {{{\n-        'Test parsing of HTTP headers'\n+        \"Test parsing of HTTP headers\"\n         from calibre.srv.http_request import HTTPHeaderParser\n \n         def test(name, *lines, **kwargs):\n@@ -35,21 +34,11 @@ def test(name, *lines, **kwargs):\n             self.assertTrue(p.finished)\n             self.assertSetEqual(set(p.hdict.items()), {(k.replace('_', '-').title(), v) for k, v in iteritems(kwargs)}, name + ' failed')\n \n-        test('Continuation line parsing',\n-             b'a: one',\n-             b'b: two',\n-             b' 2',\n-             b'\\t3',\n-             b'c:three',\n-             b'\\r\\n', a='one', b='two 2 3', c='three')\n+        test('Continuation line parsing', b'a: one', b'b: two', b' 2', b'\\t3', b'c:three', b'\\r\\n', a='one', b='two 2 3', c='three')\n \n-        test('Non-ascii headers parsing',\n-             'a:m\u016bs\\r'.encode(), b'\\r\\n', a='m\u016bs')\n+        test('Non-ascii headers parsing', 'a:m\u016bs\\r'.encode(), b'\\r\\n', a='m\u016bs')\n \n-        test('Comma-separated parsing',\n-             b'Accept-Encoding: one',\n-             b'accept-Encoding: two',\n-             b'\\r\\n', accept_encoding='one, two')\n+        test('Comma-separated parsing', b'Accept-Encoding: one', b'accept-Encoding: two', b'\\r\\n', accept_encoding='one, two')\n \n         def parse(*lines):\n             lines = list(lines)\n@@ -61,27 +50,31 @@ def parse(*lines):\n         parse(b'Connection:a\\r\\n', b'\\r\\n')\n         parse(b' Connection:a\\n')\n         parse(b':a\\n')\n+\n     # }}}\n \n     def test_accept_encoding(self):  # {{{\n-        'Test parsing of Accept-Encoding'\n+        \"Test parsing of Accept-Encoding\"\n         from calibre.srv.http_response import acceptable_encoding\n \n         def test(name, val, ans, allowed={'gzip'}):\n             self.ae(acceptable_encoding(val, allowed), ans, name + ' failed')\n+\n         test('Empty field', '', None)\n         test('Simple', 'gzip', 'gzip')\n         test('Case insensitive', 'GZIp', 'gzip')\n         test('Multiple', 'gzip, identity', 'gzip')\n         test('Priority', '1;q=0.5, 2;q=0.75, 3;q=1.0', '3', {'1', '2', '3'})\n+\n     # }}}\n \n     def test_accept_language(self):  # {{{\n-        'Test parsing of Accept-Language'\n+        \"Test parsing of Accept-Language\"\n         from calibre.srv.http_response import preferred_lang\n \n         def test(name, val, ans):\n-            self.ae(preferred_lang(val, lambda x:(True, x, None)), ans, name + ' failed')\n+            self.ae(preferred_lang(val, lambda x: (True, x, None)), ans, name + ' failed')\n+\n         test('Empty field', '', 'en')\n         test('Simple', 'de', 'de')\n         test('Case insensitive', 'Es', 'es')\n@@ -117,7 +110,7 @@ def test(al, q):\n     # }}}\n \n     def test_range_parsing(self):  # {{{\n-        'Test parsing of Range header'\n+        \"Test parsing of Range header\"\n         from calibre.srv.http_response import get_ranges\n \n         def test(val, *args):\n@@ -126,6 +119,7 @@ def test(val, *args):\n                 self.assertIsNone(pval, val)\n             else:\n                 self.assertListEqual([tuple(x) for x in pval], list(args), val)\n+\n         test('crap', None)\n         test('crap=', None)\n         test('crap=1', None)\n@@ -136,10 +130,11 @@ def test(val, *args):\n         test('bytes=-5', (95, 99, 5))\n         test('bytes=95-', (95, 99, 5))\n         test('bytes=-200', (0, 99, 100))\n+\n     # }}}\n \n     def test_http_basic(self):  # {{{\n-        'Test basic HTTP protocol conformance'\n+        \"Test basic HTTP protocol conformance\"\n         try:\n             self.do_http_basic()\n         except Exception:\n@@ -149,6 +144,7 @@ def test_http_basic(self):  # {{{\n \n     def do_http_basic(self):\n         from calibre.srv.errors import HTTPNotFound, HTTPRedirect\n+\n         body = 'Requested resource not found'\n \n         def handler(data):\n@@ -161,7 +157,7 @@ def raw_send(conn, raw):\n \n         base_timeout = 0.5 if is_ci else 0.1\n \n-        with TestServer(handler, timeout=base_timeout, max_header_line_size=100./1024, max_request_body_size=100./(1024*1024)) as server:\n+        with TestServer(handler, timeout=base_timeout, max_header_line_size=100.0 / 1024, max_request_body_size=100.0 / (1024 * 1024)) as server:\n             conn = server.connect()\n             r = raw_send(conn, b'hello\\n')\n             self.ae(r.status, http_client.BAD_REQUEST)\n@@ -200,7 +196,7 @@ def raw_send(conn, raw):\n             self.ae(r.read(), b'Requested resource not found')\n \n             # Test 500\n-            server.change_handler(lambda data:1/0)\n+            server.change_handler(lambda data: 1 / 0)\n             conn = server.connect()\n             conn.request('GET', '/test/')\n             r = conn.getresponse()\n@@ -209,6 +205,7 @@ def raw_send(conn, raw):\n             # Test 301\n             def handler(data):\n                 raise HTTPRedirect('/somewhere-else')\n+\n             server.change_handler(handler)\n             conn = server.connect()\n             conn.request('GET', '/')\n@@ -265,7 +262,7 @@ def handler(data):\n             conn.send(b'4\\r\\nbody\\r\\n200\\r\\n\\r\\n')\n             r = conn.getresponse()\n             self.ae(r.status, http_client.REQUEST_ENTITY_TOO_LARGE)\n-            conn.request('POST', '/test', body='a'*200)\n+            conn.request('POST', '/test', body='a' * 200)\n             r = conn.getresponse()\n             self.ae(r.status, http_client.REQUEST_ENTITY_TOO_LARGE)\n \n@@ -286,7 +283,7 @@ def handler(data):\n \n             # Test closing\n             server.loop.opts.timeout = 10  # ensure socket is not closed because of timeout\n-            conn.request('GET', '/close', headers={'Connection':'close'})\n+            conn.request('GET', '/close', headers={'Connection': 'close'})\n             r = conn.getresponse()\n             self.ae(r.status, 200), self.ae(r.read(), b'close')\n             server.loop.wakeup()\n@@ -304,16 +301,21 @@ def handler(data):\n             r = conn.getresponse()\n             self.ae(r.status, 200), self.ae(r.read(), b'something')\n             self.assertIn(b'Request Timeout', eintr_retry_call(conn.sock.recv, 500))\n+\n     # }}}\n \n     def test_http_response(self):  # {{{\n-        'Test HTTP protocol responses'\n+        \"Test HTTP protocol responses\"\n         from calibre.srv.http_response import parse_multipart_byterange\n \n         def handler(conn):\n             return conn.generate_static_output('test', lambda: ''.join(conn.path))\n-        with NamedTemporaryFile(suffix='test.epub') as f, open(P('localization/locales.zip'), 'rb') as lf, \\\n-                TestServer(handler, timeout=1, compress_min_size=0) as server:\n+\n+        with (\n+            NamedTemporaryFile(suffix='test.epub') as f,\n+            open(P('localization/locales.zip'), 'rb') as lf,\n+            TestServer(handler, timeout=1, compress_min_size=0) as server,\n+        ):\n             fdata = (string.ascii_letters * 100).encode('ascii')\n             f.write(fdata), f.seek(0)\n \n@@ -324,19 +326,19 @@ def handler(conn):\n             self.ae(r.status, http_client.OK), self.ae(r.read(), b'an_etagged_path')\n             etag = r.getheader('ETag')\n             self.ae(etag, '\"{}\"'.format(hashlib.sha1(b'an_etagged_path').hexdigest()))\n-            conn.request('GET', '/an_etagged_path', headers={'If-None-Match':etag})\n+            conn.request('GET', '/an_etagged_path', headers={'If-None-Match': etag})\n             r = conn.getresponse()\n             self.ae(r.status, http_client.NOT_MODIFIED)\n             self.ae(r.read(), b'')\n \n             # Test gzip\n-            raw = b'a'*20000\n+            raw = b'a' * 20000\n             server.change_handler(lambda conn: raw)\n             conn = server.connect()\n-            conn.request('GET', '/an_etagged_path', headers={'Accept-Encoding':'gzip'})\n+            conn.request('GET', '/an_etagged_path', headers={'Accept-Encoding': 'gzip'})\n             r = conn.getresponse()\n             self.ae(str(len(raw)), r.getheader('Calibre-Uncompressed-Length'))\n-            self.ae(r.status, http_client.OK), self.ae(zlib.decompress(r.read(), 16+zlib.MAX_WBITS), raw)\n+            self.ae(r.status, http_client.OK), self.ae(zlib.decompress(r.read(), 16 + zlib.MAX_WBITS), raw)\n \n             # Test dynamic etagged content\n             num_calls = [0]\n@@ -344,6 +346,7 @@ def handler(conn):\n             def edfunc():\n                 num_calls[0] += 1\n                 return b'data'\n+\n             server.change_handler(lambda conn: conn.etagged_dynamic_response('xxx', edfunc))\n             conn = server.connect()\n             conn.request('GET', '/an_etagged_path')\n@@ -352,7 +355,7 @@ def edfunc():\n             etag = r.getheader('ETag')\n             self.ae(etag, '\"xxx\"')\n             self.ae(r.getheader('Content-Length'), '4')\n-            conn.request('GET', '/an_etagged_path', headers={'If-None-Match':etag})\n+            conn.request('GET', '/an_etagged_path', headers={'If-None-Match': etag})\n             r = conn.getresponse()\n             self.ae(r.status, http_client.NOT_MODIFIED)\n             self.ae(r.read(), b'')\n@@ -372,7 +375,7 @@ def edfunc():\n                 self.ae(int(r.getheader('Content-Length')), len(fdata))\n                 self.ae(r.status, http_client.OK), self.ae(r.read(), fdata)\n \n-                conn.request('GET', '/test', headers={'Range':'bytes=2-25'})\n+                conn.request('GET', '/test', headers={'Range': 'bytes=2-25'})\n                 r = conn.getresponse()\n                 self.ae(r.status, http_client.PARTIAL_CONTENT)\n                 self.ae(str(r.getheader('Accept-Ranges')), 'bytes')\n@@ -380,27 +383,27 @@ def edfunc():\n                 self.ae(int(r.getheader('Content-Length')), 24)\n                 self.ae(r.read(), fdata[2:26])\n \n-                conn.request('GET', '/test', headers={'Range':'bytes=100000-'})\n+                conn.request('GET', '/test', headers={'Range': 'bytes=100000-'})\n                 r = conn.getresponse()\n                 self.ae(r.status, http_client.REQUESTED_RANGE_NOT_SATISFIABLE)\n                 self.ae(str(r.getheader('Content-Range')), f'bytes */{len(fdata)}')\n \n-                conn.request('GET', '/test', headers={'Range':'bytes=25-50', 'If-Range':etag})\n+                conn.request('GET', '/test', headers={'Range': 'bytes=25-50', 'If-Range': etag})\n                 r = conn.getresponse()\n                 self.ae(r.status, http_client.PARTIAL_CONTENT), self.ae(r.read(), fdata[25:51])\n                 self.ae(int(r.getheader('Content-Length')), 26)\n \n-                conn.request('GET', '/test', headers={'Range':'bytes=0-1000000'})\n+                conn.request('GET', '/test', headers={'Range': 'bytes=0-1000000'})\n                 r = conn.getresponse()\n                 self.ae(r.status, http_client.PARTIAL_CONTENT), self.ae(r.read(), fdata)\n \n-                conn.request('GET', '/test', headers={'Range':'bytes=25-50', 'If-Range':'\"nomatch\"'})\n+                conn.request('GET', '/test', headers={'Range': 'bytes=25-50', 'If-Range': '\"nomatch\"'})\n                 r = conn.getresponse()\n                 self.ae(r.status, http_client.OK), self.ae(r.read(), fdata)\n                 self.assertFalse(r.getheader('Content-Range'))\n                 self.ae(int(r.getheader('Content-Length')), len(fdata))\n \n-                conn.request('GET', '/test', headers={'Range':'bytes=0-25,26-50'})\n+                conn.request('GET', '/test', headers={'Range': 'bytes=0-25,26-50'})\n                 r = conn.getresponse()\n                 self.ae(r.status, http_client.PARTIAL_CONTENT)\n                 clen = int(r.getheader('Content-Length'))\n@@ -428,11 +431,12 @@ def edfunc():\n     # }}}\n \n     def test_static_generation(self):  # {{{\n-        'Test static generation'\n+        \"Test static generation\"\n         nums = list(map(str, range(10)))\n \n         def handler(conn):\n             return conn.generate_static_output('test', nums.pop)\n+\n         with TestServer(handler) as server:\n             conn = server.connect()\n             conn.request('GET', '/an_etagged_path')\n@@ -442,4 +446,5 @@ def handler(conn):\n                 conn.request('GET', '/an_etagged_path')\n                 r = conn.getresponse()\n                 self.assertEqual(data, r.read())\n+\n     # }}}\n"
    },
    {
        "id": 312,
        "sha_fail": "5fd445c656b537125771608dac8823bded23f826",
        "diff": "diff --git a/setup/unix-ci.py b/setup/unix-ci.py\nindex 8090e03..0ed1ee4 100644\n--- a/setup/unix-ci.py\n+++ b/setup/unix-ci.py\n@@ -23,17 +23,13 @@ def setenv(key, val):\n \n \n if ismacos:\n-\n     SWBASE = '/Users/Shared/calibre-build/sw'\n     SW = SWBASE + '/sw'\n \n     def install_env():\n         setenv('SWBASE', SWBASE)\n         setenv('SW', SW)\n-        setenv(\n-            'PATH',\n-            '$SW/bin:$SW/qt/bin:$SW/python/Python.framework/Versions/2.7/bin:$PWD/node_modules/.bin:$PATH'\n-        )\n+        setenv('PATH', '$SW/bin:$SW/qt/bin:$SW/python/Python.framework/Versions/2.7/bin:$PWD/node_modules/.bin:$PATH')\n         setenv('CFLAGS', '-I$SW/include')\n         setenv('LDFLAGS', '-L$SW/lib')\n         setenv('QMAKE', '$SW/qt/bin/qmake')\n@@ -44,8 +40,8 @@ def install_env():\n             old += ':'\n         setenv('DYLD_FALLBACK_LIBRARY_PATH', old + '$SW/lib')\n         setenv('CALIBRE_ESPEAK_DATA_DIR', '$SW/share/espeak-ng-data')\n-else:\n \n+else:\n     SWBASE = '/sw'\n     SW = SWBASE + '/sw'\n \n@@ -116,8 +112,21 @@ def run_python(*args):\n def install_linux_deps():\n     run('sudo', 'apt-get', 'update', '-y')\n     # run('sudo', 'apt-get', 'upgrade', '-y')\n-    run('sudo', 'apt-get', 'install', '-y',\n-        'gettext', 'libgl1-mesa-dev', 'libxkbcommon-dev', 'libxkbcommon-x11-dev', 'libfreetype-dev', 'pulseaudio', 'libasound2t64', 'libflite1', 'libspeechd2')\n+    run(\n+        'sudo',\n+        'apt-get',\n+        'install',\n+        '-y',\n+        'gettext',\n+        'libgl1-mesa-dev',\n+        'libxkbcommon-dev',\n+        'libxkbcommon-x11-dev',\n+        'libfreetype-dev',\n+        'pulseaudio',\n+        'libasound2',\n+        'libflite1',\n+        'libspeechd2',\n+    )\n \n \n def get_tx():\n@@ -132,6 +141,7 @@ def get_tx():\n def main():\n     if iswindows:\n         import runpy\n+\n         m = runpy.run_path('setup/win-ci.py')\n         return m['main']()\n     action = sys.argv[1]\n@@ -140,9 +150,7 @@ def main():\n         run('sudo', 'chown', '-R', os.environ['USER'], SWBASE)\n \n         tball = 'macos-64' if ismacos else 'linux-64'\n-        download_and_decompress(\n-            f'https://download.calibre-ebook.com/ci/calibre7/{tball}.tar.xz', SW\n-        )\n+        download_and_decompress(f'https://download.calibre-ebook.com/ci/calibre7/{tball}.tar.xz', SW)\n         if not ismacos:\n             install_linux_deps()\n \n@@ -151,7 +159,7 @@ def main():\n         run_python('setup.py bootstrap --ephemeral')\n \n     elif action == 'pot':\n-        transifexrc = '''\\\n+        transifexrc = \"\"\"\\\n [https://www.transifex.com]\n api_hostname  = https://api.transifex.com\n rest_hostname = https://rest.api.transifex.com\n@@ -159,7 +167,7 @@ def main():\n password = PASSWORD\n token = PASSWORD\n username = api\n-'''.replace('PASSWORD', os.environ['tx'])\n+\"\"\".replace('PASSWORD', os.environ['tx'])\n         with open(os.path.expanduser('~/.transifexrc'), 'w') as f:\n             f.write(transifexrc)\n         install_qt_source_code()\n@@ -172,8 +180,7 @@ def main():\n         os.environ['OPENSSL_MODULES'] = os.path.join(SW, 'lib', 'ossl-modules')\n         os.environ['PIPER_TTS_DIR'] = os.path.join(SW, 'piper')\n         if ismacos:\n-            os.environ['SSL_CERT_FILE'] = os.path.abspath(\n-                'resources/mozilla-ca-certs.pem')\n+            os.environ['SSL_CERT_FILE'] = os.path.abspath('resources/mozilla-ca-certs.pem')\n             # needed to ensure correct libxml2 is loaded\n             os.environ['DYLD_INSERT_LIBRARIES'] = ':'.join(os.path.join(SW, 'lib', x) for x in 'libxml2.dylib libxslt.dylib libexslt.dylib'.split())\n             os.environ['OPENSSL_ENGINES'] = os.path.join(SW, 'lib', 'engines-3')\n"
    },
    {
        "id": 313,
        "sha_fail": "4ffacbbe801b90aa147af206809b8c610ccbcefb",
        "diff": "diff --git a/.github/workflows/ci.py b/.github/workflows/ci.py\nindex c83f522..77a204a 100644\n--- a/.github/workflows/ci.py\n+++ b/.github/workflows/ci.py\n@@ -52,6 +52,7 @@ def run(*a: str, print_crash_reports: bool = False) -> None:\n     if ret != 0:\n         if ret < 0:\n             import signal\n+\n             try:\n                 sig = signal.Signals(-ret)\n             except ValueError:\n@@ -65,6 +66,7 @@ def run(*a: str, print_crash_reports: bool = False) -> None:\n \n def download_with_retry(url: str | Request, count: int = 5) -> bytes:\n     from urllib.request import urlopen\n+\n     for i in range(count):\n         try:\n             print('Downloading', url, flush=True)\n@@ -105,15 +107,18 @@ def install_deps() -> None:\n             openssl = 'openssl'\n             items.remove('go')  # already installed by ci.yml\n             import ssl\n+\n             if ssl.OPENSSL_VERSION_INFO[0] == 1:\n                 openssl += '@1.1'\n             run('brew', 'install', 'fish', openssl, *items)\n     else:\n         run('sudo apt-get update')\n-        run('sudo apt-get install -y libgl1-mesa-dev libxi-dev libxrandr-dev libxinerama-dev ca-certificates'\n+        run(\n+            'sudo apt-get install -y libgl1-mesa-dev libxi-dev libxrandr-dev libxinerama-dev ca-certificates'\n             ' libxcursor-dev libxcb-xkb-dev libdbus-1-dev libxkbcommon-dev libharfbuzz-dev libx11-xcb-dev zsh'\n             ' libpng-dev liblcms2-dev libfontconfig-dev libxkbcommon-x11-dev libcanberra-dev libxxhash-dev uuid-dev'\n-            ' libsimde-dev libsystemd-dev libcairo2-dev zsh bash dash systemd-coredump gdb')\n+            ' libsimde-dev libsystemd-dev libcairo2-dev zsh bash dash systemd-coredump gdb'\n+        )\n         # for some reason these directories are world writable which causes zsh\n         # compinit to break\n         run('sudo chmod -R og-w /usr/share/zsh')\n@@ -197,9 +202,12 @@ def install_bundle(dest: str = '', which: str = '') -> None:\n \n def install_grype() -> str:\n     dest = os.path.join(SW, 'bin')\n-    rq = Request('https://api.github.com/repos/anchore/grype/releases/latest', headers={\n-        'Accept': 'application/vnd.github.v3+json',\n-    })\n+    rq = Request(\n+        'https://api.github.com/repos/anchore/grype/releases/latest',\n+        headers={\n+            'Accept': 'application/vnd.github.v3+json',\n+        },\n+    )\n     m = json.loads(download_with_retry(rq))\n     for asset in m['assets']:\n         if asset['name'].endswith('_linux_amd64.tar.gz'):\n@@ -216,10 +224,10 @@ def install_grype() -> str:\n \n IGNORED_DEPENDENCY_CVES = [\n     # Python stdlib\n-    'CVE-2025-8194', # DoS in tarfile\n-    'CVE-2025-6069', # DoS in HTMLParser\n+    'CVE-2025-8194',  # DoS in tarfile\n+    'CVE-2025-6069',  # DoS in HTMLParser\n     # glib\n-    'CVE-2025-4056', # Only affects Windows, on which we dont run\n+    'CVE-2025-4056',  # Only affects Windows, on which we dont run\n ]\n \n \n@@ -240,6 +248,7 @@ def check_dependencies() -> None:\n         raise SystemExit(cp.returncode)\n     # Now test against the SBOM\n     import runpy\n+\n     orig = sys.argv, sys.stdout\n     sys.argv = ['bypy', 'sbom', 'myproject', '1.0.0']\n     buf = io.StringIO()\n"
    }
]