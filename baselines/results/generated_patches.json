[
    {
        "id": 1,
        "sha_fail": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
        "diff": "diff --git a/examples/community/ip_adapter_face_id.py b/examples/community/ip_adapter_face_id.py\nindex e3c5a2c..d932574 100644\n--- a/examples/community/ip_adapter_face_id.py\n+++ b/examples/community/ip_adapter_face_id.py\n@@ -14,12 +14,12 @@\n \n import inspect\n from typing import Any, Callable, Dict, List, Optional, Union\n-from safetensors import safe_open\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from packaging import version\n+from safetensors import safe_open\n from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n \n from diffusers.configuration_utils import FrozenDict\n@@ -27,20 +27,20 @@ from diffusers.image_processor import VaeImageProcessor\n from diffusers.loaders import FromSingleFileMixin, IPAdapterMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n from diffusers.models import AutoencoderKL, UNet2DConditionModel\n from diffusers.models.attention_processor import FusedAttnProcessor2_0\n-from diffusers.models.lora import adjust_lora_scale_text_encoder, LoRALinearLayer\n+from diffusers.models.lora import LoRALinearLayer, adjust_lora_scale_text_encoder\n+from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n+from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n+from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n from diffusers.schedulers import KarrasDiffusionSchedulers\n from diffusers.utils import (\n-    _get_model_file,\n     USE_PEFT_BACKEND,\n+    _get_model_file,\n     deprecate,\n     logging,\n     scale_lora_layers,\n     unscale_lora_layers,\n )\n from diffusers.utils.torch_utils import randn_tensor\n-from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n-from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n-from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -555,7 +555,7 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             revision=revision,\n             subfolder=subfolder,\n             user_agent=user_agent,\n-            )\n+        )\n         if weight_name.endswith(\".safetensors\"):\n             state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n             with safe_open(model_file, framework=\"pt\", device=\"cpu\") as f:\n@@ -1438,7 +1438,7 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n \n         # 6.1 Add image embeds for IP-Adapter\n-        added_cond_kwargs ={\"image_embeds\": image_embeds} if image_embeds is not None else None\n+        added_cond_kwargs = {\"image_embeds\": image_embeds} if image_embeds is not None else None\n \n         # 6.2 Optionally get Guidance Scale Embedding\n         timestep_cond = None\n"
    },
    {
        "id": 2,
        "sha_fail": "db6550a228941b538f340fb5b65ed16c43a21b88",
        "diff": "diff --git a/src/diffusers/loaders/ip_adapter.py b/src/diffusers/loaders/ip_adapter.py\nindex df9caa9..0c31001 100644\n--- a/src/diffusers/loaders/ip_adapter.py\n+++ b/src/diffusers/loaders/ip_adapter.py\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import os\n-from typing import Dict, Optional, Union\n+from typing import Dict, Union\n \n import torch\n from huggingface_hub.utils import validate_hf_hub_args\n"
    },
    {
        "id": 3,
        "sha_fail": "102f918deb2532bb7b825f00258f2c1414cf94da",
        "diff": "diff --git a/requirements/requirements-dev.txt b/requirements/requirements-dev.txt\nindex c009755..7769129 100644\n--- a/requirements/requirements-dev.txt\n+++ b/requirements/requirements-dev.txt\n@@ -5,5 +5,5 @@ pre-commit>=2.16.0\n watchfiles==0.19.0\n setuptools\n wheel\n-deptry==0.12.0\n+# deptry==0.12.0  # Commented out unused dependency\n twine\ndiff --git a/requirements/requirements.txt b/requirements/requirements.txt\nindex 268a4b0..cbae10b 100644\n--- a/requirements/requirements.txt\n+++ b/requirements/requirements.txt\n@@ -24,7 +24,7 @@ duckdb == 0.9.1\n requests >= 2.30.0\n pydateinfer==0.3.0\n dataprep_ml==0.0.21\n-type_infer==0.0.17\n+# type_infer==0.0.17\n dill == 0.3.6\n numpy\n pytz\n"
    },
    {
        "id": 5,
        "sha_fail": "d1b0280fb92d0d8590cf403ca46af3550507d4d2",
        "diff": "diff --git a/tornado/ioloop.py b/tornado/ioloop.py\nindex 3fb1359..13d0569 100644\n--- a/tornado/ioloop.py\n+++ b/tornado/ioloop.py\n@@ -557,7 +557,7 @@ class IOLoop(Configurable):\n         deadline: Union[float, datetime.timedelta],\n         callback: Callable,\n         *args: Any,\n-        **kwargs: Any\n+        **kwargs: Any,\n     ) -> object:\n         \"\"\"Runs the ``callback`` at the time ``deadline`` from the I/O loop.\n \n@@ -708,7 +708,7 @@ class IOLoop(Configurable):\n         self,\n         executor: Optional[concurrent.futures.Executor],\n         func: Callable[..., _T],\n-        *args: Any\n+        *args: Any,\n     ) -> \"Future[_T]\":\n         \"\"\"Runs a function in a ``concurrent.futures.Executor``. If\n         ``executor`` is ``None``, the IO loop's default executor will be used.\ndiff --git a/tornado/test/iostream_test.py b/tornado/test/iostream_test.py\nindex 02318db..de58a9c 100644\n--- a/tornado/test/iostream_test.py\n+++ b/tornado/test/iostream_test.py\n@@ -952,9 +952,7 @@ class TestIOStreamStartTLS(AsyncTestCase):\n             self.server_stream = None\n             self.server_accepted = Future()  # type: Future[None]\n             netutil.add_accept_handler(self.listener, self.accept)\n-            self.client_stream = IOStream(\n-                socket.socket()\n-            )  # type: typing.Optional[IOStream]\n+            self.client_stream = IOStream(socket.socket())  # type: typing.Optional[IOStream]\n             self.io_loop.add_future(\n                 self.client_stream.connect((\"127.0.0.1\", self.port)), self.stop\n             )\n"
    },
    {
        "id": 6,
        "sha_fail": "f18f82de3e0270f6dfddf22f1f487104b2428e35",
        "diff": "diff --git a/conftest.py b/conftest.py\nindex ca4743e..5edec3b 100644\n--- a/conftest.py\n+++ b/conftest.py\n@@ -7,6 +7,7 @@ Any imports that are performed at the top-level here must be installed wherever\n any of these tests run: that is to say, they must be listed in\n ``integration-requirements.txt`` and in ``test-requirements.txt``.\n \"\"\"\n+\n # If we don't import this early, lru_cache may get applied before we have the\n # chance to patch. This is also too early for the pytest-antilru plugin\n # to work.\ndiff --git a/tests/unittests/sources/test_wsl.py b/tests/unittests/sources/test_wsl.py\nindex 9653cfa..03b3a1f 100644\n--- a/tests/unittests/sources/test_wsl.py\n+++ b/tests/unittests/sources/test_wsl.py\n@@ -7,7 +7,7 @@\n import os\n from copy import deepcopy\n from email.mime.multipart import MIMEMultipart\n-from typing import Optional, cast\n+from typing import cast\n \n from cloudinit import helpers, util\n from cloudinit.sources import DataSourceWSL as wsl\n"
    },
    {
        "id": 7,
        "sha_fail": "55d2e8d4abb024997be878797d5625effad65d43",
        "diff": "diff --git a/conftest.py b/conftest.py\nindex bcf5a67..b4b6be0 100644\n--- a/conftest.py\n+++ b/conftest.py\n@@ -7,6 +7,7 @@ Any imports that are performed at the top-level here must be installed wherever\n any of these tests run: that is to say, they must be listed in\n ``integration-requirements.txt`` and in ``test-requirements.txt``.\n \"\"\"\n+\n # If we don't import this early, lru_cache may get applied before we have the\n # chance to patch. This is also too early for the pytest-antilru plugin\n # to work.\n"
    },
    {
        "id": 8,
        "sha_fail": "385c14d0ae500918cff5565ea836884bfaa2bfa5",
        "diff": "diff --git a/conftest.py b/conftest.py\nindex ca4743e..5edec3b 100644\n--- a/conftest.py\n+++ b/conftest.py\n@@ -7,6 +7,7 @@ Any imports that are performed at the top-level here must be installed wherever\n any of these tests run: that is to say, they must be listed in\n ``integration-requirements.txt`` and in ``test-requirements.txt``.\n \"\"\"\n+\n # If we don't import this early, lru_cache may get applied before we have the\n # chance to patch. This is also too early for the pytest-antilru plugin\n # to work.\ndiff --git a/tests/unittests/net/test_dhcp.py b/tests/unittests/net/test_dhcp.py\nindex 2841a72..8ec96ee 100644\n--- a/tests/unittests/net/test_dhcp.py\n+++ b/tests/unittests/net/test_dhcp.py\n@@ -447,8 +447,8 @@ class TestDHCPDiscoveryClean(CiTestCase):\n             self.logs.getvalue(),\n         )\n         self.assertIn(\n-           \"DHCP client not found: dhcpcd\",\n-           self.logs.getvalue(),\n+            \"DHCP client not found: dhcpcd\",\n+            self.logs.getvalue(),\n         )\n \n     @mock.patch(\"cloudinit.net.dhcp.find_fallback_nic\", return_value=None)\n"
    },
    {
        "id": 9,
        "sha_fail": "4d5898b8a73c93e1ed4434744c2fa7c3f7fbd501",
        "diff": "diff --git a/conftest.py b/conftest.py\nindex ca4743e..5edec3b 100644\n--- a/conftest.py\n+++ b/conftest.py\n@@ -7,6 +7,7 @@ Any imports that are performed at the top-level here must be installed wherever\n any of these tests run: that is to say, they must be listed in\n ``integration-requirements.txt`` and in ``test-requirements.txt``.\n \"\"\"\n+\n # If we don't import this early, lru_cache may get applied before we have the\n # chance to patch. This is also too early for the pytest-antilru plugin\n # to work.\n"
    },
    {
        "id": 10,
        "sha_fail": "ecb486addc70aecc9b28f2b30a77eaf2fd587091",
        "diff": "diff --git a/cloudinit/distros/__init__.py b/cloudinit/distros/__init__.py\nindex 9a07374..5233c31 100644\n--- a/cloudinit/distros/__init__.py\n+++ b/cloudinit/distros/__init__.py\n@@ -190,8 +190,7 @@ class Distro(persistence.CloudInitPickleMixin, metaclass=abc.ABCMeta):\n             if len(entry) == 2:\n                 return tuple(entry)\n         raise ValueError(\n-            \"Invalid 'packages' yaml specification. \"\n-            \"Check schema definition.\"\n+            \"Invalid 'packages' yaml specification. Check schema definition.\"\n         )\n \n     def _extract_package_by_manager(\n"
    },
    {
        "id": 11,
        "sha_fail": "cced5b5d68a3fe1a02d8ac1186e9d12b6c75dc8d",
        "diff": "diff --git a/sky/resources.py b/sky/resources.py\nindex ef8e888..a7b1c7f 100644\n--- a/sky/resources.py\n+++ b/sky/resources.py\n@@ -1,4 +1,5 @@\n \"\"\"Resources: compute requirements of Tasks.\"\"\"\n+\n import functools\n import textwrap\n from typing import Dict, List, Optional, Set, Tuple, Union\n@@ -535,7 +536,7 @@ class Resources:\n                         accelerator_args[\"runtime_version\"] = \"2.12.0\"\n                     logger.info(\n                         \"Missing runtime_version in accelerator_args, using\"\n-                        f' default ({accelerator_args[\"runtime_version\"]})'\n+                        f\" default ({accelerator_args['runtime_version']})\"\n                     )\n \n         self._accelerators = accelerators\n@@ -691,8 +692,7 @@ class Resources:\n                     cloud_str = f\"for any cloud among {enabled_clouds}\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f\"Invalid instance type {self._instance_type!r} \"\n-                        f\"{cloud_str}.\"\n+                        f\"Invalid instance type {self._instance_type!r} {cloud_str}.\"\n                     )\n             if len(valid_clouds) > 1:\n                 with ux_utils.print_exception_no_traceback():\n@@ -770,7 +770,7 @@ class Resources:\n             if self._use_spot:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        \"Local/On-prem mode does not support spot \" \"instances.\"\n+                        \"Local/On-prem mode does not support spot instances.\"\n                     )\n             local_instance = clouds.Local.get_default_instance_type()\n             if (\n@@ -785,7 +785,7 @@ class Resources:\n             if self._image_id is not None:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        \"Local/On-prem mode does not support custom \" \"images.\"\n+                        \"Local/On-prem mode does not support custom images.\"\n                     )\n \n     def extract_docker_image(self) -> Optional[str]:\n@@ -902,7 +902,7 @@ class Resources:\n         if skypilot_config.get_nested((\"aws\", \"security_group_name\"), None) is not None:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    \"Cannot specify ports when AWS security group name is \" \"specified.\"\n+                    \"Cannot specify ports when AWS security group name is specified.\"\n                 )\n         if self.cloud is not None:\n             self.cloud.check_features_are_supported(\n"
    },
    {
        "id": 12,
        "sha_fail": "edaf59b69f96acdf155c4514061ea648ea6df122",
        "diff": "diff --git a/sky/data/storage.py b/sky/data/storage.py\nindex dea9b90..94ae7ac 100644\n--- a/sky/data/storage.py\n+++ b/sky/data/storage.py\n@@ -1,4 +1,5 @@\n \"\"\"Storage and Store Classes for Sky Data.\"\"\"\n+\n import enum\n import os\n import re\n@@ -50,48 +51,51 @@ SourceType = Union[Path, List[Path]]\n STORE_ENABLED_CLOUDS: List[str] = [\n     str(clouds.AWS()),\n     str(clouds.GCP()),\n-    str(clouds.IBM()), cloudflare.NAME\n+    str(clouds.IBM()),\n+    cloudflare.NAME,\n ]\n \n # Maximum number of concurrent rsync upload processes\n _MAX_CONCURRENT_UPLOADS = 32\n \n _BUCKET_FAIL_TO_CONNECT_MESSAGE = (\n-    'Failed to access existing bucket {name!r}. '\n-    'This is likely because it is a private bucket you do not have access to.\\n'\n-    'To fix: \\n'\n-    '  1. If you are trying to create a new bucket: use a different name.\\n'\n-    '  2. If you are trying to connect to an existing bucket: make sure '\n-    'your cloud credentials have access to it.')\n+    \"Failed to access existing bucket {name!r}. \"\n+    \"This is likely because it is a private bucket you do not have access to.\\n\"\n+    \"To fix: \\n\"\n+    \"  1. If you are trying to create a new bucket: use a different name.\\n\"\n+    \"  2. If you are trying to connect to an existing bucket: make sure \"\n+    \"your cloud credentials have access to it.\"\n+)\n \n _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE = (\n-    'Bucket {bucket_name!r} does not exist. '\n-    'It may have been deleted externally.')\n+    \"Bucket {bucket_name!r} does not exist. It may have been deleted externally.\"\n+)\n \n \n-def _is_storage_cloud_enabled(cloud_name: str,\n-                              try_fix_with_sky_check: bool = True) -> bool:\n+def _is_storage_cloud_enabled(\n+    cloud_name: str, try_fix_with_sky_check: bool = True\n+) -> bool:\n     enabled_storage_clouds = global_user_state.get_enabled_storage_clouds()\n     if cloud_name in enabled_storage_clouds:\n         return True\n     if try_fix_with_sky_check:\n         # TODO(zhwu): Only check the specified cloud to speed up.\n         check.check(quiet=True)\n-        return _is_storage_cloud_enabled(cloud_name,\n-                                         try_fix_with_sky_check=False)\n+        return _is_storage_cloud_enabled(cloud_name, try_fix_with_sky_check=False)\n     return False\n \n \n class StoreType(enum.Enum):\n     \"\"\"Enum for the different types of stores.\"\"\"\n-    S3 = 'S3'\n-    GCS = 'GCS'\n-    AZURE = 'AZURE'\n-    R2 = 'R2'\n-    IBM = 'IBM'\n+\n+    S3 = \"S3\"\n+    GCS = \"GCS\"\n+    AZURE = \"AZURE\"\n+    R2 = \"R2\"\n+    IBM = \"IBM\"\n \n     @classmethod\n-    def from_cloud(cls, cloud: clouds.Cloud) -> 'StoreType':\n+    def from_cloud(cls, cloud: clouds.Cloud) -> \"StoreType\":\n         if isinstance(cloud, clouds.AWS):\n             return StoreType.S3\n         elif isinstance(cloud, clouds.GCP):\n@@ -101,10 +105,10 @@ class StoreType(enum.Enum):\n         elif isinstance(cloud, clouds.IBM):\n             return StoreType.IBM\n \n-        raise ValueError(f'Unsupported cloud for StoreType: {cloud}')\n+        raise ValueError(f\"Unsupported cloud for StoreType: {cloud}\")\n \n     @classmethod\n-    def from_store(cls, store: 'AbstractStore') -> 'StoreType':\n+    def from_store(cls, store: \"AbstractStore\") -> \"StoreType\":\n         if isinstance(store, S3Store):\n             return StoreType.S3\n         elif isinstance(store, GcsStore):\n@@ -115,7 +119,7 @@ class StoreType(enum.Enum):\n             return StoreType.IBM\n         else:\n             with ux_utils.print_exception_no_traceback():\n-                raise ValueError(f'Unknown store type: {store}')\n+                raise ValueError(f\"Unknown store type: {store}\")\n \n \n def get_storetype_from_cloud(cloud: clouds.Cloud) -> StoreType:\n@@ -127,34 +131,34 @@ def get_storetype_from_cloud(cloud: clouds.Cloud) -> StoreType:\n         return StoreType.IBM\n     elif isinstance(cloud, clouds.Azure):\n         with ux_utils.print_exception_no_traceback():\n-            raise ValueError('Azure Blob Storage is not supported yet.')\n+            raise ValueError(\"Azure Blob Storage is not supported yet.\")\n     elif isinstance(cloud, clouds.Lambda):\n         with ux_utils.print_exception_no_traceback():\n-            raise ValueError('Lambda Cloud does not provide cloud storage.')\n+            raise ValueError(\"Lambda Cloud does not provide cloud storage.\")\n     elif isinstance(cloud, clouds.SCP):\n         with ux_utils.print_exception_no_traceback():\n-            raise ValueError('SCP does not provide cloud storage.')\n+            raise ValueError(\"SCP does not provide cloud storage.\")\n     else:\n         with ux_utils.print_exception_no_traceback():\n-            raise ValueError(f'Unknown cloud type: {cloud}')\n+            raise ValueError(f\"Unknown cloud type: {cloud}\")\n \n \n def get_store_prefix(storetype: StoreType) -> str:\n     if storetype == StoreType.S3:\n-        return 's3://'\n+        return \"s3://\"\n     elif storetype == StoreType.GCS:\n-        return 'gs://'\n+        return \"gs://\"\n     # R2 storages use 's3://' as a prefix for various aws cli commands\n     elif storetype == StoreType.R2:\n-        return 's3://'\n+        return \"s3://\"\n     elif storetype == StoreType.IBM:\n-        return 'cos://'\n+        return \"cos://\"\n     elif storetype == StoreType.AZURE:\n         with ux_utils.print_exception_no_traceback():\n-            raise ValueError('Azure Blob Storage is not supported yet.')\n+            raise ValueError(\"Azure Blob Storage is not supported yet.\")\n     else:\n         with ux_utils.print_exception_no_traceback():\n-            raise ValueError(f'Unknown store type: {storetype}')\n+            raise ValueError(f\"Unknown store type: {storetype}\")\n \n \n class AbstractStore:\n@@ -165,9 +169,9 @@ class AbstractStore:\n     present in a cloud.\n     \"\"\"\n \n-    _STAT_CACHE_TTL = '5s'\n+    _STAT_CACHE_TTL = \"5s\"\n     _STAT_CACHE_CAPACITY = 4096\n-    _TYPE_CACHE_TTL = '5s'\n+    _TYPE_CACHE_TTL = \"5s\"\n     _RENAME_DIR_LIMIT = 10000\n \n     class StoreMetadata:\n@@ -177,30 +181,36 @@ class AbstractStore:\n         global_user_state.\n         \"\"\"\n \n-        def __init__(self,\n-                     *,\n-                     name: str,\n-                     source: Optional[SourceType],\n-                     region: Optional[str] = None,\n-                     is_sky_managed: Optional[bool] = None):\n+        def __init__(\n+            self,\n+            *,\n+            name: str,\n+            source: Optional[SourceType],\n+            region: Optional[str] = None,\n+            is_sky_managed: Optional[bool] = None,\n+        ):\n             self.name = name\n             self.source = source\n             self.region = region\n             self.is_sky_managed = is_sky_managed\n \n         def __repr__(self):\n-            return (f'StoreMetadata('\n-                    f'\\n\\tname={self.name},'\n-                    f'\\n\\tsource={self.source},'\n-                    f'\\n\\tregion={self.region},'\n-                    f'\\n\\tis_sky_managed={self.is_sky_managed})')\n-\n-    def __init__(self,\n-                 name: str,\n-                 source: Optional[SourceType],\n-                 region: Optional[str] = None,\n-                 is_sky_managed: Optional[bool] = None,\n-                 sync_on_reconstruction: Optional[bool] = True):\n+            return (\n+                f\"StoreMetadata(\"\n+                f\"\\n\\tname={self.name},\"\n+                f\"\\n\\tsource={self.source},\"\n+                f\"\\n\\tregion={self.region},\"\n+                f\"\\n\\tis_sky_managed={self.is_sky_managed})\"\n+            )\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        source: Optional[SourceType],\n+        region: Optional[str] = None,\n+        is_sky_managed: Optional[bool] = None,\n+        sync_on_reconstruction: Optional[bool] = True,\n+    ):\n         \"\"\"Initialize AbstractStore\n \n         Args:\n@@ -237,19 +247,21 @@ class AbstractStore:\n         Used when reconstructing Storage and Store objects from\n         global_user_state.\n         \"\"\"\n-        return cls(name=override_args.get('name', metadata.name),\n-                   source=override_args.get('source', metadata.source),\n-                   region=override_args.get('region', metadata.region),\n-                   is_sky_managed=override_args.get('is_sky_managed',\n-                                                    metadata.is_sky_managed),\n-                   sync_on_reconstruction=override_args.get(\n-                       'sync_on_reconstruction', True))\n+        return cls(\n+            name=override_args.get(\"name\", metadata.name),\n+            source=override_args.get(\"source\", metadata.source),\n+            region=override_args.get(\"region\", metadata.region),\n+            is_sky_managed=override_args.get(\"is_sky_managed\", metadata.is_sky_managed),\n+            sync_on_reconstruction=override_args.get(\"sync_on_reconstruction\", True),\n+        )\n \n     def get_metadata(self) -> StoreMetadata:\n-        return self.StoreMetadata(name=self.name,\n-                                  source=self.source,\n-                                  region=self.region,\n-                                  is_sky_managed=self.is_sky_managed)\n+        return self.StoreMetadata(\n+            name=self.name,\n+            source=self.source,\n+            region=self.region,\n+            is_sky_managed=self.is_sky_managed,\n+        )\n \n     def initialize(self):\n         \"\"\"Initializes the Store object on the cloud.\n@@ -314,9 +326,9 @@ class AbstractStore:\n         \"\"\"\n         raise NotImplementedError\n \n-    def csync_command(self,\n-                      csync_path: str,\n-                      interval_seconds: Optional[int] = None) -> str:\n+    def csync_command(\n+        self, csync_path: str, interval_seconds: Optional[int] = None\n+    ) -> str:\n         \"\"\"Returns command to mount CSYNC with Storage bucket on CSYNC_PATH.\n \n         Args:\n@@ -336,34 +348,38 @@ class AbstractStore:\n             if self.source is not None:\n                 if isinstance(self.source, (str, Path)):\n                     if store_type == StoreType.GCS:\n-                        destination = str(self.source).replace('gs://', '')\n+                        destination = str(self.source).replace(\"gs://\", \"\")\n                     else:\n                         destination = str(self.source).replace(\n-                            f'{store_type_str}://', '')\n+                            f\"{store_type_str}://\", \"\"\n+                        )\n                 elif isinstance(self.source, list):\n                     raise TypeError(\n-                        'CSYNC mode does not supprot multiple sources '\n-                        'for a single storage.')\n+                        \"CSYNC mode does not supprot multiple sources \"\n+                        \"for a single storage.\"\n+                    )\n         else:\n             assert self.bucket is not None, (\n-                'Bucket should be initialized before calling csync_command')\n+                \"Bucket should be initialized before calling csync_command\"\n+            )\n             destination = self.bucket.name\n \n         # Parse destination to get sync_point for log file naming\n         # The exact mounting point is either the name of bucket\n         # or the subdirectory in it\n         result = urllib.parse.urlsplit(destination)\n-        sync_point = result.path.split('/')[-1]\n-        log_file_name = f'csync_{store_type_str}_{sync_point}.log'\n-        log_path = f'~/.sky/{log_file_name}'\n-\n-        csync_cmd = (f'python -m sky.data.sky_csync csync {csync_path} '\n-                     f'{store_type_str} {destination} --interval-seconds '\n-                     f'{interval_seconds} --delete --no-follow-symlinks')\n-        return mounting_utils.get_mounting_command(StorageMode.CSYNC,\n-                                                   csync_path,\n-                                                   csync_cmd,\n-                                                   csync_log_path=log_path)\n+        sync_point = result.path.split(\"/\")[-1]\n+        log_file_name = f\"csync_{store_type_str}_{sync_point}.log\"\n+        log_path = f\"~/.sky/{log_file_name}\"\n+\n+        csync_cmd = (\n+            f\"python -m sky.data.sky_csync csync {csync_path} \"\n+            f\"{store_type_str} {destination} --interval-seconds \"\n+            f\"{interval_seconds} --delete --no-follow-symlinks\"\n+        )\n+        return mounting_utils.get_mounting_command(\n+            StorageMode.CSYNC, csync_path, csync_cmd, csync_log_path=log_path\n+        )\n \n     def __deepcopy__(self, memo):\n         # S3 Client and GCS Client cannot be deep copied, hence the\n@@ -418,8 +434,7 @@ class Storage(object):\n             source: Optional[SourceType],\n             interval_seconds: Optional[int],\n             mode: Optional[StorageMode] = None,\n-            sky_stores: Optional[Dict[StoreType,\n-                                      AbstractStore.StoreMetadata]] = None,\n+            sky_stores: Optional[Dict[StoreType, AbstractStore.StoreMetadata]] = None,\n         ):\n             assert storage_name is not None or source is not None\n             self.storage_name = storage_name\n@@ -431,12 +446,14 @@ class Storage(object):\n             self.sky_stores = {} if sky_stores is None else sky_stores\n \n         def __repr__(self):\n-            return (f'StorageMetadata('\n-                    f'\\n\\tstorage_name={self.storage_name},'\n-                    f'\\n\\tsource={self.source},'\n-                    f'\\n\\tinterval_seconds={self.interval_seconds},'\n-                    f'\\n\\tmode={self.mode},'\n-                    f'\\n\\tstores={self.sky_stores})')\n+            return (\n+                f\"StorageMetadata(\"\n+                f\"\\n\\tstorage_name={self.storage_name},\"\n+                f\"\\n\\tsource={self.source},\"\n+                f\"\\n\\tinterval_seconds={self.interval_seconds},\"\n+                f\"\\n\\tmode={self.mode},\"\n+                f\"\\n\\tstores={self.sky_stores})\"\n+            )\n \n         def add_store(self, store: AbstractStore) -> None:\n             storetype = StoreType.from_store(store)\n@@ -447,14 +464,16 @@ class Storage(object):\n             if storetype in self.sky_stores:\n                 del self.sky_stores[storetype]\n \n-    def __init__(self,\n-                 name: Optional[str] = None,\n-                 source: Optional[SourceType] = None,\n-                 stores: Optional[Dict[StoreType, AbstractStore]] = None,\n-                 persistent: Optional[bool] = True,\n-                 mode: StorageMode = StorageMode.MOUNT,\n-                 interval_seconds: Optional[int] = None,\n-                 sync_on_reconstruction: bool = True) -> None:\n+    def __init__(\n+        self,\n+        name: Optional[str] = None,\n+        source: Optional[SourceType] = None,\n+        stores: Optional[Dict[StoreType, AbstractStore]] = None,\n+        persistent: Optional[bool] = True,\n+        mode: StorageMode = StorageMode.MOUNT,\n+        interval_seconds: Optional[int] = None,\n+        sync_on_reconstruction: bool = True,\n+    ) -> None:\n         \"\"\"Initializes a Storage object.\n \n         Three fields are required: the name of the storage, the source\n@@ -521,52 +540,53 @@ class Storage(object):\n         if handle is not None:\n             self.handle = handle\n             # Reconstruct the Storage object from the global_user_state\n-            logger.debug('Detected existing storage object, '\n-                         f'loading Storage: {self.name}')\n+            logger.debug(\n+                f\"Detected existing storage object, loading Storage: {self.name}\"\n+            )\n             self._add_store_from_metadata(self.handle.sky_stores)\n \n             # TODO(romilb): This logic should likely be in add_store to move\n             # syncing to file_mount stage..\n             if self.sync_on_reconstruction:\n-                msg = ''\n-                if (self.source and\n-                    (isinstance(self.source, list) or\n-                     not data_utils.is_cloud_store_url(self.source))):\n-                    msg = ' and uploading from source'\n-                logger.info(f'Verifying bucket{msg} for storage {self.name}')\n+                msg = \"\"\n+                if self.source and (\n+                    isinstance(self.source, list)\n+                    or not data_utils.is_cloud_store_url(self.source)\n+                ):\n+                    msg = \" and uploading from source\"\n+                logger.info(f\"Verifying bucket{msg} for storage {self.name}\")\n                 self.sync_all_stores()\n \n         else:\n             # Storage does not exist in global_user_state, create new stores\n             sky_managed_stores = {\n-                t: s.get_metadata()\n-                for t, s in self.stores.items()\n-                if s.is_sky_managed\n+                t: s.get_metadata() for t, s in self.stores.items() if s.is_sky_managed\n             }\n             self.handle = self.StorageMetadata(\n                 storage_name=self.name,\n                 source=self.source,\n                 interval_seconds=self.interval_seconds,\n                 mode=self.mode,\n-                sky_stores=sky_managed_stores)\n+                sky_stores=sky_managed_stores,\n+            )\n \n             if self.source is not None:\n                 # If source is a pre-existing bucket, connect to the bucket\n                 # If the bucket does not exist, this will error out\n                 if isinstance(self.source, str):\n-                    if self.source.startswith('s3://'):\n+                    if self.source.startswith(\"s3://\"):\n                         self.add_store(StoreType.S3)\n-                    elif self.source.startswith('gs://'):\n+                    elif self.source.startswith(\"gs://\"):\n                         self.add_store(StoreType.GCS)\n-                    elif self.source.startswith('r2://'):\n+                    elif self.source.startswith(\"r2://\"):\n                         self.add_store(StoreType.R2)\n-                    elif self.source.startswith('cos://'):\n+                    elif self.source.startswith(\"cos://\"):\n                         self.add_store(StoreType.IBM)\n \n     @staticmethod\n     def _validate_source(\n-            source: SourceType, mode: StorageMode,\n-            sync_on_reconstruction: bool) -> Tuple[SourceType, bool]:\n+        source: SourceType, mode: StorageMode, sync_on_reconstruction: bool\n+    ) -> Tuple[SourceType, bool]:\n         \"\"\"Validates the source path.\n \n         Args:\n@@ -589,30 +609,34 @@ class Storage(object):\n             if conflicts:\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageSourceError(\n-                        'Cannot have multiple files or directories with the '\n-                        'same name in source. Conflicts found for: '\n-                        f'{\", \".join(conflicts)}')\n+                        \"Cannot have multiple files or directories with the \"\n+                        \"same name in source. Conflicts found for: \"\n+                        f\"{', '.join(conflicts)}\"\n+                    )\n \n         def _validate_local_source(local_source):\n-            if local_source.endswith('/'):\n+            if local_source.endswith(\"/\"):\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageSourceError(\n-                        'Storage source paths cannot end with a slash '\n+                        \"Storage source paths cannot end with a slash \"\n                         '(try \"/mydir: /mydir\" or \"/myfile: /myfile\"). '\n-                        f'Found source={local_source}')\n+                        f\"Found source={local_source}\"\n+                    )\n             # Local path, check if it exists\n             full_src = os.path.abspath(os.path.expanduser(local_source))\n             # Only check if local source exists if it is synced to the bucket\n             if not os.path.exists(full_src) and sync_on_reconstruction:\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageSourceError(\n-                        'Local source path does not'\n-                        f' exist: {local_source}')\n+                        f\"Local source path does not exist: {local_source}\"\n+                    )\n             # Raise warning if user's path is a symlink\n             elif os.path.islink(full_src):\n-                logger.warning(f'Source path {source} is a symlink. '\n-                               'Referenced contents are uploaded, matching '\n-                               'the default behavior for S3 and GCS syncing.')\n+                logger.warning(\n+                    f\"Source path {source} is a symlink. \"\n+                    \"Referenced contents are uploaded, matching \"\n+                    \"the default behavior for S3 and GCS syncing.\"\n+                )\n \n         # Check if source is a list of paths\n         if isinstance(source, list):\n@@ -625,79 +649,87 @@ class Storage(object):\n         else:\n             # Check if str source is a valid local/remote URL\n             split_path = urllib.parse.urlsplit(source)\n-            if split_path.scheme == '':\n+            if split_path.scheme == \"\":\n                 _validate_local_source(source)\n                 # Check if source is a file - throw error if it is\n                 full_src = os.path.abspath(os.path.expanduser(source))\n                 if os.path.isfile(full_src):\n                     with ux_utils.print_exception_no_traceback():\n                         raise exceptions.StorageSourceError(\n-                            'Storage source path cannot be a file - only'\n-                            ' directories are supported as a source. '\n-                            'To upload a single file, specify it in a list '\n-                            f'by writing source: [{source}]. Note '\n-                            'that the file will be uploaded to the root of the '\n-                            'bucket and will appear at <destination_path>/'\n-                            f'{os.path.basename(source)}. Alternatively, you '\n-                            'can directly upload the file to the VM without '\n-                            'using a bucket by writing <destination_path>: '\n-                            f'{source} in the file_mounts section of your YAML')\n+                            \"Storage source path cannot be a file - only\"\n+                            \" directories are supported as a source. \"\n+                            \"To upload a single file, specify it in a list \"\n+                            f\"by writing source: [{source}]. Note \"\n+                            \"that the file will be uploaded to the root of the \"\n+                            \"bucket and will appear at <destination_path>/\"\n+                            f\"{os.path.basename(source)}. Alternatively, you \"\n+                            \"can directly upload the file to the VM without \"\n+                            \"using a bucket by writing <destination_path>: \"\n+                            f\"{source} in the file_mounts section of your YAML\"\n+                        )\n                 is_local_source = True\n-            elif split_path.scheme in ['s3', 'gs', 'r2', 'cos']:\n+            elif split_path.scheme in [\"s3\", \"gs\", \"r2\", \"cos\"]:\n                 is_local_source = False\n                 # Storage mounting does not support mounting specific files from\n                 # cloud store - ensure path points to only a directory\n                 if mode == StorageMode.MOUNT:\n-                    if ((not split_path.scheme == 'cos' and\n-                         split_path.path.strip('/') != '') or\n-                        (split_path.scheme == 'cos' and\n-                         not re.match(r'^/[-\\w]+(/\\s*)?$', split_path.path))):\n+                    if (\n+                        not split_path.scheme == \"cos\"\n+                        and split_path.path.strip(\"/\") != \"\"\n+                    ) or (\n+                        split_path.scheme == \"cos\"\n+                        and not re.match(r\"^/[-\\w]+(/\\s*)?$\", split_path.path)\n+                    ):\n                         # regex allows split_path.path to include /bucket\n                         # or /bucket/optional_whitespaces while considering\n                         # cos URI's regions (cos://region/bucket_name)\n                         with ux_utils.print_exception_no_traceback():\n                             raise exceptions.StorageModeError(\n-                                'MOUNT mode does not support'\n-                                ' mounting specific files from cloud'\n-                                ' storage. Please use COPY mode or'\n-                                ' specify only the bucket name as'\n-                                ' the source.')\n+                                \"MOUNT mode does not support\"\n+                                \" mounting specific files from cloud\"\n+                                \" storage. Please use COPY mode or\"\n+                                \" specify only the bucket name as\"\n+                                \" the source.\"\n+                            )\n             else:\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageSourceError(\n-                        f'Supported paths: local, s3://, gs://, '\n-                        f'r2://, cos://. Got: {source}')\n+                        f\"Supported paths: local, s3://, gs://, \"\n+                        f\"r2://, cos://. Got: {source}\"\n+                    )\n         return source, is_local_source\n \n     def _validate_storage_spec(self, name: Optional[str]) -> None:\n         \"\"\"Validates the storage spec and updates local fields if necessary.\"\"\"\n \n         def validate_name(name):\n-            \"\"\" Checks for validating the storage name.\n+            \"\"\"Checks for validating the storage name.\n \n             Checks if the name starts the s3, gcs or r2 prefix and raise error\n             if it does. Store specific validation checks (e.g., S3 specific\n             rules) happen in the corresponding store class.\n             \"\"\"\n-            prefix = name.split('://')[0]\n+            prefix = name.split(\"://\")[0]\n             prefix = prefix.lower()\n-            if prefix in ['s3', 'gs', 'r2', 'cos']:\n+            if prefix in [\"s3\", \"gs\", \"r2\", \"cos\"]:\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageNameError(\n-                        'Prefix detected: `name` cannot start with '\n-                        f'{prefix}://. If you are trying to use an existing '\n-                        'bucket created outside of SkyPilot, please specify it '\n-                        'using the `source` field (e.g. '\n-                        '`source: s3://mybucket/`). If you are trying to '\n-                        'create a new bucket, please use the `store` field to '\n-                        'specify the store type (e.g. `store: s3`).')\n+                        \"Prefix detected: `name` cannot start with \"\n+                        f\"{prefix}://. If you are trying to use an existing \"\n+                        \"bucket created outside of SkyPilot, please specify it \"\n+                        \"using the `source` field (e.g. \"\n+                        \"`source: s3://mybucket/`). If you are trying to \"\n+                        \"create a new bucket, please use the `store` field to \"\n+                        \"specify the store type (e.g. `store: s3`).\"\n+                    )\n \n         # interval_seconds is a field only used with CSYNC mode\n         if self.mode != StorageMode.CSYNC and self.interval_seconds is not None:\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageSourceError(\n                     'The field \"interval_seconds\" can be specified only '\n-                    'with CSYNC mdoe.')\n+                    \"with CSYNC mdoe.\"\n+                )\n \n         if self.source is None:\n             # If the mode is COPY, the source must be specified\n@@ -710,8 +742,9 @@ class Storage(object):\n                 if handle is None:\n                     with ux_utils.print_exception_no_traceback():\n                         raise exceptions.StorageSourceError(\n-                            'New storage object: source must be specified when '\n-                            'using COPY mode.')\n+                            \"New storage object: source must be specified when \"\n+                            \"using COPY mode.\"\n+                        )\n             else:\n                 # If source is not specified in COPY mode, the intent is to\n                 # create a bucket and use it as scratch disk. Name must be\n@@ -719,24 +752,26 @@ class Storage(object):\n                 if not name:\n                     with ux_utils.print_exception_no_traceback():\n                         raise exceptions.StorageSpecError(\n-                            'Storage source or storage name must be specified.')\n+                            \"Storage source or storage name must be specified.\"\n+                        )\n             assert name is not None, handle\n             validate_name(name)\n             self.name = name\n             return\n         elif self.source is not None:\n             source, is_local_source = Storage._validate_source(\n-                self.source, self.mode, self.sync_on_reconstruction)\n+                self.source, self.mode, self.sync_on_reconstruction\n+            )\n             if not name:\n                 if is_local_source:\n                     with ux_utils.print_exception_no_traceback():\n                         raise exceptions.StorageNameError(\n-                            'Storage name must be specified if the source is '\n-                            'local.')\n+                            \"Storage name must be specified if the source is local.\"\n+                        )\n                 else:\n                     assert isinstance(source, str)\n                     # Set name to source bucket name and continue\n-                    if source.startswith('cos://'):\n+                    if source.startswith(\"cos://\"):\n                         # cos url requires custom parsing\n                         name = data_utils.split_cos_path(source)[0]\n                     else:\n@@ -756,15 +791,17 @@ class Storage(object):\n                     # is a URI. Name will be inferred from the URI.\n                     with ux_utils.print_exception_no_traceback():\n                         raise exceptions.StorageSpecError(\n-                            'Storage name should not be specified if the '\n-                            'source is a remote URI.')\n+                            \"Storage name should not be specified if the \"\n+                            \"source is a remote URI.\"\n+                        )\n         raise exceptions.StorageSpecError(\n-            f'Validation failed for storage source {self.source}, name '\n-            f'{self.name} and mode {self.mode}. Please check the arguments.')\n+            f\"Validation failed for storage source {self.source}, name \"\n+            f\"{self.name} and mode {self.mode}. Please check the arguments.\"\n+        )\n \n     def _add_store_from_metadata(\n-            self, sky_stores: Dict[StoreType,\n-                                   AbstractStore.StoreMetadata]) -> None:\n+        self, sky_stores: Dict[StoreType, AbstractStore.StoreMetadata]\n+    ) -> None:\n         \"\"\"Reconstructs Storage.stores from sky_stores.\n \n         Reconstruct AbstractStore objects from sky_store's metadata and\n@@ -778,65 +815,71 @@ class Storage(object):\n                     store = S3Store.from_metadata(\n                         s_metadata,\n                         source=self.source,\n-                        sync_on_reconstruction=self.sync_on_reconstruction)\n+                        sync_on_reconstruction=self.sync_on_reconstruction,\n+                    )\n                 elif s_type == StoreType.GCS:\n                     store = GcsStore.from_metadata(\n                         s_metadata,\n                         source=self.source,\n-                        sync_on_reconstruction=self.sync_on_reconstruction)\n+                        sync_on_reconstruction=self.sync_on_reconstruction,\n+                    )\n                 elif s_type == StoreType.R2:\n                     store = R2Store.from_metadata(\n                         s_metadata,\n                         source=self.source,\n-                        sync_on_reconstruction=self.sync_on_reconstruction)\n+                        sync_on_reconstruction=self.sync_on_reconstruction,\n+                    )\n                 elif s_type == StoreType.IBM:\n                     store = IBMCosStore.from_metadata(\n                         s_metadata,\n                         source=self.source,\n-                        sync_on_reconstruction=self.sync_on_reconstruction)\n+                        sync_on_reconstruction=self.sync_on_reconstruction,\n+                    )\n                 else:\n                     with ux_utils.print_exception_no_traceback():\n-                        raise ValueError(f'Unknown store type: {s_type}')\n+                        raise ValueError(f\"Unknown store type: {s_type}\")\n             # Following error is raised from _get_bucket and caught only when\n             # an externally removed storage is attempted to be fetched.\n             except exceptions.StorageExternalDeletionError:\n-                logger.debug(f'Storage object, {self.name}, was attempted to '\n-                             'be reconstructed while the corresponding bucket'\n-                             ' was externally deleted.')\n+                logger.debug(\n+                    f\"Storage object, {self.name}, was attempted to \"\n+                    \"be reconstructed while the corresponding bucket\"\n+                    \" was externally deleted.\"\n+                )\n                 continue\n \n             self._add_store(store, is_reconstructed=True)\n \n     @classmethod\n-    def from_metadata(cls, metadata: StorageMetadata,\n-                      **override_args) -> 'Storage':\n+    def from_metadata(cls, metadata: StorageMetadata, **override_args) -> \"Storage\":\n         \"\"\"Create Storage from StorageMetadata object.\n \n         Used when reconstructing Storage object and AbstractStore objects from\n         global_user_state.\n         \"\"\"\n         # Name should not be specified if the source is a cloud store URL.\n-        source = override_args.get('source', metadata.source)\n-        name = override_args.get('name', metadata.storage_name)\n+        source = override_args.get(\"source\", metadata.source)\n+        name = override_args.get(\"name\", metadata.storage_name)\n         # If the source is a list, it consists of local paths\n-        if not isinstance(source, list): \n+        if not isinstance(source, list):\n             if data_utils.is_cloud_store_url(source):\n                 name = None\n \n-        storage_obj = cls(name=name,\n-                          source=source,\n-                          sync_on_reconstruction=override_args.get(\n-                              'sync_on_reconstruction', True))\n+        storage_obj = cls(\n+            name=name,\n+            source=source,\n+            sync_on_reconstruction=override_args.get(\"sync_on_reconstruction\", True),\n+        )\n \n         # For backward compatibility\n         # TODO(Doyoung): Implement __setstate__ to resolve backwards\n         # compatibility issue\n-        if hasattr(metadata, 'interval_seconds'):\n+        if hasattr(metadata, \"interval_seconds\"):\n             storage_obj.interval_seconds = metadata.interval_seconds\n \n-        if hasattr(metadata, 'mode'):\n+        if hasattr(metadata, \"mode\"):\n             if metadata.mode:\n-                storage_obj.mode = override_args.get('mode', metadata.mode)\n+                storage_obj.mode = override_args.get(\"mode\", metadata.mode)\n \n         return storage_obj\n \n@@ -853,7 +896,7 @@ class Storage(object):\n             store_type = StoreType(store_type)\n \n         if store_type in self.stores:\n-            logger.info(f'Storage type {store_type} already exists.')\n+            logger.info(f\"Storage type {store_type} already exists.\")\n             return self.stores[store_type]\n \n         store_cls: Type[AbstractStore]\n@@ -868,37 +911,38 @@ class Storage(object):\n         else:\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageSpecError(\n-                    f'{store_type} not supported as a Store.')\n+                    f\"{store_type} not supported as a Store.\"\n+                )\n \n         if self.mode == StorageMode.CSYNC:\n             if store_type in (StoreType.R2, StoreType.IBM):\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageSpecError(\n-                        f'Currently, {store_type} does not support '\n-                        'CSYNC mode.')\n+                        f\"Currently, {store_type} does not support CSYNC mode.\"\n+                    )\n \n         # Initialize store object and get/create bucket\n         try:\n             store = store_cls(\n                 name=self.name,\n                 source=self.source,\n-                sync_on_reconstruction=self.sync_on_reconstruction)\n+                sync_on_reconstruction=self.sync_on_reconstruction,\n+            )\n         except exceptions.StorageBucketCreateError:\n             # Creation failed, so this must be sky managed store. Add failure\n             # to state.\n-            logger.error(f'Could not create {store_type} store '\n-                         f'with name {self.name}.')\n-            global_user_state.set_storage_status(self.name,\n-                                                 StorageStatus.INIT_FAILED)\n+            logger.error(f\"Could not create {store_type} store with name {self.name}.\")\n+            global_user_state.set_storage_status(self.name, StorageStatus.INIT_FAILED)\n             raise\n         except exceptions.StorageBucketGetError:\n             # Bucket get failed, so this is not sky managed. Do not update state\n-            logger.error(f'Could not get {store_type} store '\n-                         f'with name {self.name}.')\n+            logger.error(f\"Could not get {store_type} store with name {self.name}.\")\n             raise\n         except exceptions.StorageInitError:\n-            logger.error(f'Could not initialize {store_type} store with '\n-                         f'name {self.name}. General initialization error.')\n+            logger.error(\n+                f\"Could not initialize {store_type} store with \"\n+                f\"name {self.name}. General initialization error.\"\n+            )\n             raise\n \n         # Add store to storage\n@@ -917,8 +961,9 @@ class Storage(object):\n         if store.is_sky_managed:\n             self.handle.add_store(store)\n             if not is_reconstructed:\n-                global_user_state.add_or_update_storage(self.name, self.handle,\n-                                                        StorageStatus.INIT)\n+                global_user_state.add_or_update_storage(\n+                    self.name, self.handle, StorageStatus.INIT\n+                )\n \n     def delete(self, store_type: Optional[StoreType] = None) -> None:\n         \"\"\"Deletes data for all sky-managed storage objects.\n@@ -931,7 +976,7 @@ class Storage(object):\n               of backing stores.\n         \"\"\"\n         if not self.stores:\n-            logger.info('No backing stores found. Deleting storage.')\n+            logger.info(\"No backing stores found. Deleting storage.\")\n             global_user_state.remove_storage(self.name)\n         if store_type:\n             store = self.stores[store_type]\n@@ -943,8 +988,7 @@ class Storage(object):\n                 store.delete()\n                 # Check remaining stores - if none is sky managed, remove\n                 # the storage from global_user_state.\n-                delete = all(\n-                    s.is_sky_managed is False for s in self.stores.values())\n+                delete = all(s.is_sky_managed is False for s in self.stores.values())\n                 if delete:\n                     global_user_state.remove_storage(self.name)\n                 else:\n@@ -973,9 +1017,10 @@ class Storage(object):\n         \"\"\"Runs the upload routine for the store and handles failures\"\"\"\n \n         def warn_for_git_dir(source: str):\n-            if os.path.isdir(os.path.join(source, '.git')):\n-                logger.warning(f'\\'.git\\' directory under \\'{self.source}\\' '\n-                               'is excluded during sync.')\n+            if os.path.isdir(os.path.join(source, \".git\")):\n+                logger.warning(\n+                    f\"'.git' directory under '{self.source}' is excluded during sync.\"\n+                )\n \n         try:\n             if self.source is not None:\n@@ -986,11 +1031,13 @@ class Storage(object):\n                         warn_for_git_dir(source)\n             store.upload()\n         except exceptions.StorageUploadError:\n-            logger.error(f'Could not upload {self.source!r} to store '\n-                         f'name {store.name!r}.')\n+            logger.error(\n+                f\"Could not upload {self.source!r} to store name {store.name!r}.\"\n+            )\n             if store.is_sky_managed:\n                 global_user_state.set_storage_status(\n-                    self.name, StorageStatus.UPLOAD_FAILED)\n+                    self.name, StorageStatus.UPLOAD_FAILED\n+                )\n             raise\n \n         # Upload succeeded - update state\n@@ -998,16 +1045,17 @@ class Storage(object):\n             global_user_state.set_storage_status(self.name, StorageStatus.READY)\n \n     @classmethod\n-    def from_yaml_config(cls, config: Dict[str, Any]) -> 'Storage':\n-        common_utils.validate_schema(config, schemas.get_storage_schema(),\n-                                     'Invalid storage YAML: ')\n-\n-        name = config.pop('name', None)\n-        source = config.pop('source', None)\n-        store = config.pop('store', None)\n-        mode_str = config.pop('mode', None)\n-        interval_seconds = config.pop('interval_seconds', None)\n-        force_delete = config.pop('_force_delete', None)\n+    def from_yaml_config(cls, config: Dict[str, Any]) -> \"Storage\":\n+        common_utils.validate_schema(\n+            config, schemas.get_storage_schema(), \"Invalid storage YAML: \"\n+        )\n+\n+        name = config.pop(\"name\", None)\n+        source = config.pop(\"source\", None)\n+        store = config.pop(\"store\", None)\n+        mode_str = config.pop(\"mode\", None)\n+        interval_seconds = config.pop(\"interval_seconds\", None)\n+        force_delete = config.pop(\"_force_delete\", None)\n         if force_delete is None:\n             force_delete = False\n \n@@ -1017,18 +1065,20 @@ class Storage(object):\n         else:\n             # Make sure this keeps the same as the default mode in __init__\n             mode = StorageMode.MOUNT\n-        persistent = config.pop('persistent', None)\n+        persistent = config.pop(\"persistent\", None)\n         if persistent is None:\n             persistent = True\n \n-        assert not config, f'Invalid storage args: {config.keys()}'\n+        assert not config, f\"Invalid storage args: {config.keys()}\"\n \n         # Validation of the config object happens on instantiation.\n-        storage_obj = cls(name=name,\n-                          source=source,\n-                          persistent=persistent,\n-                          mode=mode,\n-                          interval_seconds=interval_seconds)\n+        storage_obj = cls(\n+            name=name,\n+            source=source,\n+            persistent=persistent,\n+            mode=mode,\n+            interval_seconds=interval_seconds,\n+        )\n         if store is not None:\n             storage_obj.add_store(StoreType(store.upper()))\n \n@@ -1044,22 +1094,25 @@ class Storage(object):\n                 config[key] = value\n \n         name = None\n-        if (self.source is None or not isinstance(self.source, str) or\n-                not data_utils.is_cloud_store_url(self.source)):\n+        if (\n+            self.source is None\n+            or not isinstance(self.source, str)\n+            or not data_utils.is_cloud_store_url(self.source)\n+        ):\n             # Remove name if source is a cloud store URL\n             name = self.name\n-        add_if_not_none('name', name)\n-        add_if_not_none('source', self.source)\n+        add_if_not_none(\"name\", name)\n+        add_if_not_none(\"source\", self.source)\n \n         stores = None\n         if len(self.stores) > 0:\n-            stores = ','.join([store.value for store in self.stores])\n-        add_if_not_none('store', stores)\n-        add_if_not_none('persistent', self.persistent)\n-        add_if_not_none('mode', self.mode.value)\n-        add_if_not_none('interval_seconds', self.interval_seconds)\n+            stores = \",\".join([store.value for store in self.stores])\n+        add_if_not_none(\"store\", stores)\n+        add_if_not_none(\"persistent\", self.persistent)\n+        add_if_not_none(\"mode\", self.mode.value)\n+        add_if_not_none(\"interval_seconds\", self.interval_seconds)\n         if self.force_delete:\n-            config['_force_delete'] = True\n+            config[\"_force_delete\"] = True\n         return config\n \n     def get_storage_name(self):\n@@ -1071,46 +1124,54 @@ class S3Store(AbstractStore):\n     for S3 buckets.\n     \"\"\"\n \n-    _ACCESS_DENIED_MESSAGE = 'Access Denied'\n+    _ACCESS_DENIED_MESSAGE = \"Access Denied\"\n \n-    def __init__(self,\n-                 name: str,\n-                 source: str,\n-                 region: Optional[str] = 'us-east-2',\n-                 is_sky_managed: Optional[bool] = None,\n-                 sync_on_reconstruction: bool = True):\n-        self.client: 'boto3.client.Client'\n-        self.bucket: 'StorageHandle'\n-        super().__init__(name, source, region, is_sky_managed,\n-                         sync_on_reconstruction)\n+    def __init__(\n+        self,\n+        name: str,\n+        source: str,\n+        region: Optional[str] = \"us-east-2\",\n+        is_sky_managed: Optional[bool] = None,\n+        sync_on_reconstruction: bool = True,\n+    ):\n+        self.client: \"boto3.client.Client\"\n+        self.bucket: \"StorageHandle\"\n+        super().__init__(name, source, region, is_sky_managed, sync_on_reconstruction)\n \n     def _validate(self):\n         if self.source is not None and isinstance(self.source, str):\n-            if self.source.startswith('s3://'):\n+            if self.source.startswith(\"s3://\"):\n                 assert self.name == data_utils.split_s3_path(self.source)[0], (\n-                    'S3 Bucket is specified as path, the name should be the'\n-                    ' same as S3 bucket.')\n-            elif self.source.startswith('gs://'):\n+                    \"S3 Bucket is specified as path, the name should be the\"\n+                    \" same as S3 bucket.\"\n+                )\n+            elif self.source.startswith(\"gs://\"):\n                 assert self.name == data_utils.split_gcs_path(self.source)[0], (\n-                    'GCS Bucket is specified as path, the name should be '\n-                    'the same as GCS bucket.')\n+                    \"GCS Bucket is specified as path, the name should be \"\n+                    \"the same as GCS bucket.\"\n+                )\n                 assert data_utils.verify_gcs_bucket(self.name), (\n-                    f'Source specified as {self.source}, a GCS bucket. ',\n-                    'GCS Bucket should exist.')\n-            elif self.source.startswith('r2://'):\n+                    f\"Source specified as {self.source}, a GCS bucket. \",\n+                    \"GCS Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"r2://\"):\n                 assert self.name == data_utils.split_r2_path(self.source)[0], (\n-                    'R2 Bucket is specified as path, the name should be '\n-                    'the same as R2 bucket.')\n+                    \"R2 Bucket is specified as path, the name should be \"\n+                    \"the same as R2 bucket.\"\n+                )\n                 assert data_utils.verify_r2_bucket(self.name), (\n-                    f'Source specified as {self.source}, a R2 bucket. ',\n-                    'R2 Bucket should exist.')\n-            elif self.source.startswith('cos://'):\n+                    f\"Source specified as {self.source}, a R2 bucket. \",\n+                    \"R2 Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"cos://\"):\n                 assert self.name == data_utils.split_cos_path(self.source)[0], (\n-                    'COS Bucket is specified as path, the name should be '\n-                    'the same as COS bucket.')\n+                    \"COS Bucket is specified as path, the name should be \"\n+                    \"the same as COS bucket.\"\n+                )\n                 assert data_utils.verify_ibm_cos_bucket(self.name), (\n-                    f'Source specified as {self.source}, a COS bucket. ',\n-                    'COS Bucket should exist.')\n+                    f\"Source specified as {self.source}, a COS bucket. \",\n+                    \"COS Bucket should exist.\",\n+                )\n         # Validate name\n         self.name = self.validate_name(self.name)\n \n@@ -1118,11 +1179,11 @@ class S3Store(AbstractStore):\n         if not _is_storage_cloud_enabled(str(clouds.AWS())):\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.ResourcesUnavailableError(\n-                    'Storage \\'store: s3\\' specified, but ' \\\n-                    'AWS access is disabled. To fix, enable '\\\n-                    'AWS by running `sky check`. More info: '\\\n-                    'https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.' # pylint: disable=line-too-long\n-                    )\n+                    \"Storage 'store: s3' specified, but \"\n+                    \"AWS access is disabled. To fix, enable \"\n+                    \"AWS by running `sky check`. More info: \"\n+                    \"https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.\"  # pylint: disable=line-too-long\n+                )\n \n     @classmethod\n     def validate_name(cls, name) -> str:\n@@ -1138,49 +1199,56 @@ class S3Store(AbstractStore):\n         if name is not None and isinstance(name, str):\n             if not 3 <= len(name) <= 63:\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must be between 3 (min) '\n-                    'and 63 (max) characters long.')\n+                    f\"Invalid store name: name {name} must be between 3 (min) \"\n+                    \"and 63 (max) characters long.\"\n+                )\n \n             # Check for valid characters and start/end with a letter or number\n-            pattern = r'^[a-z0-9][-a-z0-9.]*[a-z0-9]$'\n+            pattern = r\"^[a-z0-9][-a-z0-9.]*[a-z0-9]$\"\n             if not re.match(pattern, name):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} can consist only of '\n-                    'lowercase letters, numbers, dots (.), and hyphens (-). '\n-                    'It must begin and end with a letter or number.')\n+                    f\"Invalid store name: name {name} can consist only of \"\n+                    \"lowercase letters, numbers, dots (.), and hyphens (-). \"\n+                    \"It must begin and end with a letter or number.\"\n+                )\n \n             # Check for two adjacent periods\n-            if '..' in name:\n+            if \"..\" in name:\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must not contain '\n-                    'two adjacent periods.')\n+                    f\"Invalid store name: name {name} must not contain \"\n+                    \"two adjacent periods.\"\n+                )\n \n             # Check for IP address format\n-            ip_pattern = r'^(?:\\d{1,3}\\.){3}\\d{1,3}$'\n+            ip_pattern = r\"^(?:\\d{1,3}\\.){3}\\d{1,3}$\"\n             if re.match(ip_pattern, name):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must not be formatted as '\n-                    'an IP address (for example, 192.168.5.4).')\n+                    f\"Invalid store name: name {name} must not be formatted as \"\n+                    \"an IP address (for example, 192.168.5.4).\"\n+                )\n \n             # Check for 'xn--' prefix\n-            if name.startswith('xn--'):\n+            if name.startswith(\"xn--\"):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must not start with the '\n-                    'prefix \"xn--\".')\n+                    f\"Invalid store name: name {name} must not start with the \"\n+                    'prefix \"xn--\".'\n+                )\n \n             # Check for '-s3alias' suffix\n-            if name.endswith('-s3alias'):\n+            if name.endswith(\"-s3alias\"):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must not end with the '\n-                    'suffix \"-s3alias\".')\n+                    f\"Invalid store name: name {name} must not end with the \"\n+                    'suffix \"-s3alias\".'\n+                )\n \n             # Check for '--ol-s3' suffix\n-            if name.endswith('--ol-s3'):\n+            if name.endswith(\"--ol-s3\"):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must not end with the '\n-                    'suffix \"--ol-s3\".')\n+                    f\"Invalid store name: name {name} must not end with the \"\n+                    'suffix \"--ol-s3\".'\n+                )\n         else:\n-            _raise_no_traceback_name_error('Store name must be specified.')\n+            _raise_no_traceback_name_error(\"Store name must be specified.\")\n         return name\n \n     def initialize(self):\n@@ -1216,11 +1284,11 @@ class S3Store(AbstractStore):\n             if isinstance(self.source, list):\n                 self.batch_aws_rsync(self.source, create_dirs=True)\n             elif self.source is not None:\n-                if self.source.startswith('s3://'):\n+                if self.source.startswith(\"s3://\"):\n                     pass\n-                elif self.source.startswith('gs://'):\n+                elif self.source.startswith(\"gs://\"):\n                     self._transfer_to_s3()\n-                elif self.source.startswith('r2://'):\n+                elif self.source.startswith(\"r2://\"):\n                     self._transfer_to_s3()\n                 else:\n                     self.batch_aws_rsync([self.source])\n@@ -1228,24 +1296,26 @@ class S3Store(AbstractStore):\n             raise\n         except Exception as e:\n             raise exceptions.StorageUploadError(\n-                f'Upload failed for store {self.name}') from e\n+                f\"Upload failed for store {self.name}\"\n+            ) from e\n \n     def delete(self) -> None:\n         deleted_by_skypilot = self._delete_s3_bucket(self.name)\n         if deleted_by_skypilot:\n-            msg_str = f'Deleted S3 bucket {self.name}.'\n+            msg_str = f\"Deleted S3 bucket {self.name}.\"\n         else:\n-            msg_str = f'S3 bucket {self.name} may have been deleted ' \\\n-                      f'externally. Removing from local state.'\n-        logger.info(f'{colorama.Fore.GREEN}{msg_str}'\n-                    f'{colorama.Style.RESET_ALL}')\n+            msg_str = (\n+                f\"S3 bucket {self.name} may have been deleted \"\n+                f\"externally. Removing from local state.\"\n+            )\n+        logger.info(f\"{colorama.Fore.GREEN}{msg_str}{colorama.Style.RESET_ALL}\")\n \n     def get_handle(self) -> StorageHandle:\n-        return aws.resource('s3').Bucket(self.name)\n+        return aws.resource(\"s3\").Bucket(self.name)\n \n-    def batch_aws_rsync(self,\n-                        source_path_list: List[Path],\n-                        create_dirs: bool = False) -> None:\n+    def batch_aws_rsync(\n+        self, source_path_list: List[Path], create_dirs: bool = False\n+    ) -> None:\n         \"\"\"Invokes aws s3 sync to batch upload a list of local paths to S3\n \n         AWS Sync by default uses 10 threads to upload files to the bucket.  To\n@@ -1265,34 +1335,42 @@ class S3Store(AbstractStore):\n         \"\"\"\n \n         def get_file_sync_command(base_dir_path, file_names):\n-            includes = ' '.join(\n-                [f'--include \"{file_name}\"' for file_name in file_names])\n-            sync_command = ('aws s3 sync --no-follow-symlinks --exclude=\"*\" '\n-                            f'{includes} {base_dir_path} '\n-                            f's3://{self.name}')\n+            includes = \" \".join(\n+                [f'--include \"{file_name}\"' for file_name in file_names]\n+            )\n+            sync_command = (\n+                'aws s3 sync --no-follow-symlinks --exclude=\"*\" '\n+                f\"{includes} {base_dir_path} \"\n+                f\"s3://{self.name}\"\n+            )\n             return sync_command\n \n         def get_dir_sync_command(src_dir_path, dest_dir_name):\n             # we exclude .git directory from the sync\n             excluded_list = storage_utils.get_excluded_files_from_gitignore(\n-                src_dir_path)\n-            excluded_list.append('.git/*')\n-            excludes = ' '.join(\n-                [f'--exclude \"{file_name}\"' for file_name in excluded_list])\n-            sync_command = (f'aws s3 sync --no-follow-symlinks {excludes} '\n-                            f'{src_dir_path} '\n-                            f's3://{self.name}/{dest_dir_name}')\n+                src_dir_path\n+            )\n+            excluded_list.append(\".git/*\")\n+            excludes = \" \".join(\n+                [f'--exclude \"{file_name}\"' for file_name in excluded_list]\n+            )\n+            sync_command = (\n+                f\"aws s3 sync --no-follow-symlinks {excludes} \"\n+                f\"{src_dir_path} \"\n+                f\"s3://{self.name}/{dest_dir_name}\"\n+            )\n             return sync_command\n \n         # Generate message for upload\n         if len(source_path_list) > 1:\n-            source_message = f'{len(source_path_list)} paths'\n+            source_message = f\"{len(source_path_list)} paths\"\n         else:\n             source_message = source_path_list[0]\n \n         with rich_utils.safe_status(\n-                f'[bold cyan]Syncing '\n-                f'[green]{source_message}[/] to [green]s3://{self.name}/[/]'):\n+            f\"[bold cyan]Syncing \"\n+            f\"[green]{source_message}[/] to [green]s3://{self.name}/[/]\"\n+        ):\n             data_utils.parallel_upload(\n                 source_path_list,\n                 get_file_sync_command,\n@@ -1300,13 +1378,14 @@ class S3Store(AbstractStore):\n                 self.name,\n                 self._ACCESS_DENIED_MESSAGE,\n                 create_dirs=create_dirs,\n-                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)\n+                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS,\n+            )\n \n     def _transfer_to_s3(self) -> None:\n         assert isinstance(self.source, str), self.source\n-        if self.source.startswith('gs://'):\n+        if self.source.startswith(\"gs://\"):\n             data_transfer.gcs_to_s3(self.name, self.name)\n-        elif self.source.startswith('r2://'):\n+        elif self.source.startswith(\"r2://\"):\n             data_transfer.r2_to_s3(self.name, self.name)\n \n     def _get_bucket(self) -> Tuple[StorageHandle, bool]:\n@@ -1326,7 +1405,7 @@ class S3Store(AbstractStore):\n                 attempted to be fetched while reconstructing the storage for\n                 'sky storage delete' or 'sky start'\n         \"\"\"\n-        s3 = aws.resource('s3')\n+        s3 = aws.resource(\"s3\")\n         bucket = s3.Bucket(self.name)\n \n         try:\n@@ -1337,22 +1416,24 @@ class S3Store(AbstractStore):\n             self.client.head_bucket(Bucket=self.name)\n             return bucket, False\n         except aws.botocore_exceptions().ClientError as e:\n-            error_code = e.response['Error']['Code']\n+            error_code = e.response[\"Error\"][\"Code\"]\n             # AccessDenied error for buckets that are private and not owned by\n             # user.\n-            if error_code == '403':\n-                command = f'aws s3 ls {self.name}'\n+            if error_code == \"403\":\n+                command = f\"aws s3 ls {self.name}\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketGetError(\n-                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +\n-                        f' To debug, consider running `{command}`.') from e\n+                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name)\n+                        + f\" To debug, consider running `{command}`.\"\n+                    ) from e\n \n-        if isinstance(self.source, str) and self.source.startswith('s3://'):\n+        if isinstance(self.source, str) and self.source.startswith(\"s3://\"):\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketGetError(\n-                    'Attempted to use a non-existent bucket as a source: '\n-                    f'{self.source}. Consider using `aws s3 ls '\n-                    f'{self.source}` to debug.')\n+                    \"Attempted to use a non-existent bucket as a source: \"\n+                    f\"{self.source}. Consider using `aws s3 ls \"\n+                    f\"{self.source}` to debug.\"\n+                )\n \n         # If bucket cannot be found in both private and public settings,\n         # the bucket is to be created by Sky. However, creation is skipped if\n@@ -1366,8 +1447,8 @@ class S3Store(AbstractStore):\n             # delete or to re-mount Storages with sky start but the storage\n             # is already removed externally.\n             raise exceptions.StorageExternalDeletionError(\n-                'Attempted to fetch a non-existent bucket: '\n-                f'{self.name}')\n+                f\"Attempted to fetch a non-existent bucket: {self.name}\"\n+            )\n \n     def _download_file(self, remote_path: str, local_path: str) -> None:\n         \"\"\"Downloads file from remote to local on s3 bucket\n@@ -1387,21 +1468,23 @@ class S3Store(AbstractStore):\n         Args:\n           mount_path: str; Path to mount the bucket to.\n         \"\"\"\n-        install_cmd = ('sudo wget -nc https://github.com/romilbhardwaj/goofys/'\n-                       'releases/download/0.24.0-romilb-upstream/goofys '\n-                       '-O /usr/local/bin/goofys && '\n-                       'sudo chmod +x /usr/local/bin/goofys')\n-        mount_cmd = ('goofys -o allow_other '\n-                     f'--stat-cache-ttl {self._STAT_CACHE_TTL} '\n-                     f'--type-cache-ttl {self._TYPE_CACHE_TTL} '\n-                     f'{self.bucket.name} {mount_path}')\n-        return mounting_utils.get_mounting_command(StorageMode.MOUNT,\n-                                                   mount_path, mount_cmd,\n-                                                   install_cmd)\n-\n-    def _create_s3_bucket(self,\n-                          bucket_name: str,\n-                          region='us-east-2') -> StorageHandle:\n+        install_cmd = (\n+            \"sudo wget -nc https://github.com/romilbhardwaj/goofys/\"\n+            \"releases/download/0.24.0-romilb-upstream/goofys \"\n+            \"-O /usr/local/bin/goofys && \"\n+            \"sudo chmod +x /usr/local/bin/goofys\"\n+        )\n+        mount_cmd = (\n+            \"goofys -o allow_other \"\n+            f\"--stat-cache-ttl {self._STAT_CACHE_TTL} \"\n+            f\"--type-cache-ttl {self._TYPE_CACHE_TTL} \"\n+            f\"{self.bucket.name} {mount_path}\"\n+        )\n+        return mounting_utils.get_mounting_command(\n+            StorageMode.MOUNT, mount_path, mount_cmd, install_cmd\n+        )\n+\n+    def _create_s3_bucket(self, bucket_name: str, region=\"us-east-2\") -> StorageHandle:\n         \"\"\"Creates S3 bucket with specific name in specific region\n \n         Args:\n@@ -1415,16 +1498,17 @@ class S3Store(AbstractStore):\n             if region is None:\n                 s3_client.create_bucket(Bucket=bucket_name)\n             else:\n-                location = {'LocationConstraint': region}\n-                s3_client.create_bucket(Bucket=bucket_name,\n-                                        CreateBucketConfiguration=location)\n-                logger.info(f'Created S3 bucket {bucket_name} in {region}')\n+                location = {\"LocationConstraint\": region}\n+                s3_client.create_bucket(\n+                    Bucket=bucket_name, CreateBucketConfiguration=location\n+                )\n+                logger.info(f\"Created S3 bucket {bucket_name} in {region}\")\n         except aws.botocore_exceptions().ClientError as e:\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketCreateError(\n-                    f'Attempted to create a bucket '\n-                    f'{self.name} but failed.') from e\n-        return aws.resource('s3').Bucket(bucket_name)\n+                    f\"Attempted to create a bucket {self.name} but failed.\"\n+                ) from e\n+        return aws.resource(\"s3\").Bucket(bucket_name)\n \n     def _delete_s3_bucket(self, bucket_name: str) -> bool:\n         \"\"\"Deletes S3 bucket, including all objects in bucket\n@@ -1442,23 +1526,28 @@ class S3Store(AbstractStore):\n         # https://stackoverflow.com/questions/49239351/why-is-it-so-much-slower-to-delete-objects-in-aws-s3-than-it-is-to-create-them\n         # The fastest way to delete is to run `aws s3 rb --force`,\n         # which removes the bucket by force.\n-        remove_command = f'aws s3 rb s3://{bucket_name} --force'\n+        remove_command = f\"aws s3 rb s3://{bucket_name} --force\"\n         try:\n             with rich_utils.safe_status(\n-                    f'[bold cyan]Deleting S3 bucket {bucket_name}[/]'):\n-                subprocess.check_output(remove_command.split(' '),\n-                                        stderr=subprocess.STDOUT)\n+                f\"[bold cyan]Deleting S3 bucket {bucket_name}[/]\"\n+            ):\n+                subprocess.check_output(\n+                    remove_command.split(\" \"), stderr=subprocess.STDOUT\n+                )\n         except subprocess.CalledProcessError as e:\n-            if 'NoSuchBucket' in e.output.decode('utf-8'):\n+            if \"NoSuchBucket\" in e.output.decode(\"utf-8\"):\n                 logger.debug(\n                     _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE.format(\n-                        bucket_name=bucket_name))\n+                        bucket_name=bucket_name\n+                    )\n+                )\n                 return False\n             else:\n                 logger.error(e.output)\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketDeleteError(\n-                        f'Failed to delete S3 bucket {bucket_name}.')\n+                        f\"Failed to delete S3 bucket {bucket_name}.\"\n+                    )\n \n         # Wait until bucket deletion propagates on AWS servers\n         while data_utils.verify_s3_bucket(bucket_name):\n@@ -1471,65 +1560,67 @@ class GcsStore(AbstractStore):\n     for GCS buckets.\n     \"\"\"\n \n-    _ACCESS_DENIED_MESSAGE = 'AccessDeniedException'\n-    GCSFUSE_VERSION = '1.0.1'\n-\n-    def __init__(self,\n-                 name: str,\n-                 source: str,\n-                 region: Optional[str] = 'us-central1',\n-                 is_sky_managed: Optional[bool] = None,\n-                 sync_on_reconstruction: Optional[bool] = True):\n-        self.client: 'storage.Client'\n+    _ACCESS_DENIED_MESSAGE = \"AccessDeniedException\"\n+    GCSFUSE_VERSION = \"1.0.1\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        source: str,\n+        region: Optional[str] = \"us-central1\",\n+        is_sky_managed: Optional[bool] = None,\n+        sync_on_reconstruction: Optional[bool] = True,\n+    ):\n+        self.client: \"storage.Client\"\n         self.bucket: StorageHandle\n-        super().__init__(name, source, region, is_sky_managed,\n-                         sync_on_reconstruction)\n+        super().__init__(name, source, region, is_sky_managed, sync_on_reconstruction)\n \n     def _validate(self):\n         if self.source is not None:\n             if isinstance(self.source, str):\n-                if self.source.startswith('s3://'):\n-                    assert self.name == data_utils.split_s3_path(\n-                        self.source\n-                    )[0], (\n-                        'S3 Bucket is specified as path, the name should be the'\n-                        ' same as S3 bucket.')\n+                if self.source.startswith(\"s3://\"):\n+                    assert self.name == data_utils.split_s3_path(self.source)[0], (\n+                        \"S3 Bucket is specified as path, the name should be the\"\n+                        \" same as S3 bucket.\"\n+                    )\n                     assert data_utils.verify_s3_bucket(self.name), (\n-                        f'Source specified as {self.source}, an S3 bucket. ',\n-                        'S3 Bucket should exist.')\n-                elif self.source.startswith('gs://'):\n-                    assert self.name == data_utils.split_gcs_path(\n-                        self.source\n-                    )[0], (\n-                        'GCS Bucket is specified as path, the name should be '\n-                        'the same as GCS bucket.')\n-                elif self.source.startswith('r2://'):\n-                    assert self.name == data_utils.split_r2_path(\n-                        self.source\n-                    )[0], ('R2 Bucket is specified as path, the name should be '\n-                           'the same as R2 bucket.')\n+                        f\"Source specified as {self.source}, an S3 bucket. \",\n+                        \"S3 Bucket should exist.\",\n+                    )\n+                elif self.source.startswith(\"gs://\"):\n+                    assert self.name == data_utils.split_gcs_path(self.source)[0], (\n+                        \"GCS Bucket is specified as path, the name should be \"\n+                        \"the same as GCS bucket.\"\n+                    )\n+                elif self.source.startswith(\"r2://\"):\n+                    assert self.name == data_utils.split_r2_path(self.source)[0], (\n+                        \"R2 Bucket is specified as path, the name should be \"\n+                        \"the same as R2 bucket.\"\n+                    )\n                     assert data_utils.verify_r2_bucket(self.name), (\n-                        f'Source specified as {self.source}, a R2 bucket. ',\n-                        'R2 Bucket should exist.')\n-                elif self.source.startswith('cos://'):\n-                    assert self.name == data_utils.split_cos_path(\n-                        self.source\n-                    )[0], (\n-                        'COS Bucket is specified as path, the name should be '\n-                        'the same as COS bucket.')\n+                        f\"Source specified as {self.source}, a R2 bucket. \",\n+                        \"R2 Bucket should exist.\",\n+                    )\n+                elif self.source.startswith(\"cos://\"):\n+                    assert self.name == data_utils.split_cos_path(self.source)[0], (\n+                        \"COS Bucket is specified as path, the name should be \"\n+                        \"the same as COS bucket.\"\n+                    )\n                     assert data_utils.verify_ibm_cos_bucket(self.name), (\n-                        f'Source specified as {self.source}, a COS bucket. ',\n-                        'COS Bucket should exist.')\n+                        f\"Source specified as {self.source}, a COS bucket. \",\n+                        \"COS Bucket should exist.\",\n+                    )\n         # Validate name\n         self.name = self.validate_name(self.name)\n         # Check if the storage is enabled\n         if not _is_storage_cloud_enabled(str(clouds.GCP())):\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.ResourcesUnavailableError(\n-                    'Storage \\'store: gcs\\' specified, but '\n-                    'GCP access is disabled. To fix, enable '\n-                    'GCP by running `sky check`. '\n-                    'More info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.')  # pylint: disable=line-too-long\n+                    \"Storage 'store: gcs' specified, but \"\n+                    \"GCP access is disabled. To fix, enable \"\n+                    \"GCP by running `sky check`. \"\n+                    \"More info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.\"\n+                )  # pylint: disable=line-too-long\n \n     @classmethod\n     def validate_name(cls, name) -> str:\n@@ -1546,47 +1637,52 @@ class GcsStore(AbstractStore):\n             # Check for overall length\n             if not 3 <= len(name) <= 222:\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must contain 3-222 '\n-                    'characters.')\n+                    f\"Invalid store name: name {name} must contain 3-222 characters.\"\n+                )\n \n             # Check for valid characters and start/end with a number or letter\n-            pattern = r'^[a-z0-9][-a-z0-9._]*[a-z0-9]$'\n+            pattern = r\"^[a-z0-9][-a-z0-9._]*[a-z0-9]$\"\n             if not re.match(pattern, name):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} can only contain '\n-                    'lowercase letters, numeric characters, dashes (-), '\n-                    'underscores (_), and dots (.). Spaces are not allowed. '\n-                    'Names must start and end with a number or letter.')\n+                    f\"Invalid store name: name {name} can only contain \"\n+                    \"lowercase letters, numeric characters, dashes (-), \"\n+                    \"underscores (_), and dots (.). Spaces are not allowed. \"\n+                    \"Names must start and end with a number or letter.\"\n+                )\n \n             # Check for 'goog' prefix and 'google' in the name\n-            if name.startswith('goog') or any(\n-                    s in name\n-                    for s in ['google', 'g00gle', 'go0gle', 'g0ogle']):\n+            if name.startswith(\"goog\") or any(\n+                s in name for s in [\"google\", \"g00gle\", \"go0gle\", \"g0ogle\"]\n+            ):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} cannot begin with the '\n-                    '\"goog\" prefix or contain \"google\" in various forms.')\n+                    f\"Invalid store name: name {name} cannot begin with the \"\n+                    '\"goog\" prefix or contain \"google\" in various forms.'\n+                )\n \n             # Check for dot-separated components length\n-            components = name.split('.')\n+            components = name.split(\".\")\n             if any(len(component) > 63 for component in components):\n                 _raise_no_traceback_name_error(\n-                    'Invalid store name: Dot-separated components in name '\n-                    f'{name} can be no longer than 63 characters.')\n+                    \"Invalid store name: Dot-separated components in name \"\n+                    f\"{name} can be no longer than 63 characters.\"\n+                )\n \n-            if '..' in name or '.-' in name or '-.' in name:\n+            if \"..\" in name or \".-\" in name or \"-.\" in name:\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} must not contain two '\n-                    'adjacent periods or a dot next to a hyphen.')\n+                    f\"Invalid store name: name {name} must not contain two \"\n+                    \"adjacent periods or a dot next to a hyphen.\"\n+                )\n \n             # Check for IP address format\n-            ip_pattern = r'^(?:\\d{1,3}\\.){3}\\d{1,3}$'\n+            ip_pattern = r\"^(?:\\d{1,3}\\.){3}\\d{1,3}$\"\n             if re.match(ip_pattern, name):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: name {name} cannot be represented as '\n-                    'an IP address in dotted-decimal notation '\n-                    '(for example, 192.168.5.4).')\n+                    f\"Invalid store name: name {name} cannot be represented as \"\n+                    \"an IP address in dotted-decimal notation \"\n+                    \"(for example, 192.168.5.4).\"\n+                )\n         else:\n-            _raise_no_traceback_name_error('Store name must be specified.')\n+            _raise_no_traceback_name_error(\"Store name must be specified.\")\n         return name\n \n     def initialize(self):\n@@ -1622,11 +1718,11 @@ class GcsStore(AbstractStore):\n             if isinstance(self.source, list):\n                 self.batch_gsutil_rsync(self.source, create_dirs=True)\n             elif self.source is not None:\n-                if self.source.startswith('gs://'):\n+                if self.source.startswith(\"gs://\"):\n                     pass\n-                elif self.source.startswith('s3://'):\n+                elif self.source.startswith(\"s3://\"):\n                     self._transfer_to_gcs()\n-                elif self.source.startswith('r2://'):\n+                elif self.source.startswith(\"r2://\"):\n                     self._transfer_to_gcs()\n                 else:\n                     # If a single directory is specified in source, upload\n@@ -1636,24 +1732,26 @@ class GcsStore(AbstractStore):\n             raise\n         except Exception as e:\n             raise exceptions.StorageUploadError(\n-                f'Upload failed for store {self.name}') from e\n+                f\"Upload failed for store {self.name}\"\n+            ) from e\n \n     def delete(self) -> None:\n         deleted_by_skypilot = self._delete_gcs_bucket(self.name)\n         if deleted_by_skypilot:\n-            msg_str = f'Deleted GCS bucket {self.name}.'\n+            msg_str = f\"Deleted GCS bucket {self.name}.\"\n         else:\n-            msg_str = f'GCS bucket {self.name} may have been deleted ' \\\n-                      f'externally. Removing from local state.'\n-        logger.info(f'{colorama.Fore.GREEN}{msg_str}'\n-                    f'{colorama.Style.RESET_ALL}')\n+            msg_str = (\n+                f\"GCS bucket {self.name} may have been deleted \"\n+                f\"externally. Removing from local state.\"\n+            )\n+        logger.info(f\"{colorama.Fore.GREEN}{msg_str}{colorama.Style.RESET_ALL}\")\n \n     def get_handle(self) -> StorageHandle:\n         return self.client.get_bucket(self.name)\n \n-    def batch_gsutil_cp(self,\n-                        source_path_list: List[Path],\n-                        create_dirs: bool = False) -> None:\n+    def batch_gsutil_cp(\n+        self, source_path_list: List[Path], create_dirs: bool = False\n+    ) -> None:\n         \"\"\"Invokes gsutil cp -n to batch upload a list of local paths\n \n         -n flag to gsutil cp checks the existence of an object before uploading,\n@@ -1664,7 +1762,7 @@ class GcsStore(AbstractStore):\n         \"\"\"\n         # Generate message for upload\n         if len(source_path_list) > 1:\n-            source_message = f'{len(source_path_list)} paths'\n+            source_message = f\"{len(source_path_list)} paths\"\n         else:\n             source_message = source_path_list[0]\n \n@@ -1673,26 +1771,29 @@ class GcsStore(AbstractStore):\n         # contents of directory to the root, add /* to the directory path\n         # e.g., /mydir/*\n         source_path_list = [\n-            str(path) + '/*' if\n-            (os.path.isdir(path) and not create_dirs) else str(path)\n+            str(path) + \"/*\" if (os.path.isdir(path) and not create_dirs) else str(path)\n             for path in source_path_list\n         ]\n-        copy_list = '\\n'.join(\n-            os.path.abspath(os.path.expanduser(p)) for p in source_path_list)\n+        copy_list = \"\\n\".join(\n+            os.path.abspath(os.path.expanduser(p)) for p in source_path_list\n+        )\n         gsutil_alias, alias_gen = data_utils.get_gsutil_command()\n-        sync_command = (f'{alias_gen}; echo \"{copy_list}\" | {gsutil_alias} '\n-                        f'cp -e -n -r -I gs://{self.name}')\n+        sync_command = (\n+            f'{alias_gen}; echo \"{copy_list}\" | {gsutil_alias} '\n+            f\"cp -e -n -r -I gs://{self.name}\"\n+        )\n \n         with rich_utils.safe_status(\n-                f'[bold cyan]Syncing '\n-                f'[green]{source_message}[/] to [green]gs://{self.name}/[/]'):\n-            data_utils.run_upload_cli(sync_command,\n-                                      self._ACCESS_DENIED_MESSAGE,\n-                                      bucket_name=self.name)\n-\n-    def batch_gsutil_rsync(self,\n-                           source_path_list: List[Path],\n-                           create_dirs: bool = False) -> None:\n+            f\"[bold cyan]Syncing \"\n+            f\"[green]{source_message}[/] to [green]gs://{self.name}/[/]\"\n+        ):\n+            data_utils.run_upload_cli(\n+                sync_command, self._ACCESS_DENIED_MESSAGE, bucket_name=self.name\n+            )\n+\n+    def batch_gsutil_rsync(\n+        self, source_path_list: List[Path], create_dirs: bool = False\n+    ) -> None:\n         \"\"\"Invokes gsutil rsync to batch upload a list of local paths\n \n         Since gsutil rsync does not support include commands, We use negative\n@@ -1712,34 +1813,40 @@ class GcsStore(AbstractStore):\n         \"\"\"\n \n         def get_file_sync_command(base_dir_path, file_names):\n-            sync_format = '|'.join(file_names)\n+            sync_format = \"|\".join(file_names)\n             gsutil_alias, alias_gen = data_utils.get_gsutil_command()\n-            sync_command = (f'{alias_gen}; {gsutil_alias} '\n-                            f'rsync -e -x \\'^(?!{sync_format}$).*\\' '\n-                            f'{base_dir_path} gs://{self.name}')\n+            sync_command = (\n+                f\"{alias_gen}; {gsutil_alias} \"\n+                f\"rsync -e -x '^(?!{sync_format}$).*' \"\n+                f\"{base_dir_path} gs://{self.name}\"\n+            )\n             return sync_command\n \n         def get_dir_sync_command(src_dir_path, dest_dir_name):\n             excluded_list = storage_utils.get_excluded_files_from_gitignore(\n-                src_dir_path)\n+                src_dir_path\n+            )\n             # we exclude .git directory from the sync\n-            excluded_list.append(r'^\\.git/.*$')\n-            excludes = '|'.join(excluded_list)\n+            excluded_list.append(r\"^\\.git/.*$\")\n+            excludes = \"|\".join(excluded_list)\n             gsutil_alias, alias_gen = data_utils.get_gsutil_command()\n-            sync_command = (f'{alias_gen}; {gsutil_alias} '\n-                            f'rsync -e -r -x \\'({excludes})\\' {src_dir_path} '\n-                            f'gs://{self.name}/{dest_dir_name}')\n+            sync_command = (\n+                f\"{alias_gen}; {gsutil_alias} \"\n+                f\"rsync -e -r -x '({excludes})' {src_dir_path} \"\n+                f\"gs://{self.name}/{dest_dir_name}\"\n+            )\n             return sync_command\n \n         # Generate message for upload\n         if len(source_path_list) > 1:\n-            source_message = f'{len(source_path_list)} paths'\n+            source_message = f\"{len(source_path_list)} paths\"\n         else:\n             source_message = source_path_list[0]\n \n         with rich_utils.safe_status(\n-                f'[bold cyan]Syncing '\n-                f'[green]{source_message}[/] to [green]gs://{self.name}/[/]'):\n+            f\"[bold cyan]Syncing \"\n+            f\"[green]{source_message}[/] to [green]gs://{self.name}/[/]\"\n+        ):\n             data_utils.parallel_upload(\n                 source_path_list,\n                 get_file_sync_command,\n@@ -1747,12 +1854,13 @@ class GcsStore(AbstractStore):\n                 self.name,\n                 self._ACCESS_DENIED_MESSAGE,\n                 create_dirs=create_dirs,\n-                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)\n+                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS,\n+            )\n \n     def _transfer_to_gcs(self) -> None:\n-        if isinstance(self.source, str) and self.source.startswith('s3://'):\n+        if isinstance(self.source, str) and self.source.startswith(\"s3://\"):\n             data_transfer.s3_to_gcs(self.name, self.name)\n-        elif isinstance(self.source, str) and self.source.startswith('r2://'):\n+        elif isinstance(self.source, str) and self.source.startswith(\"r2://\"):\n             data_transfer.r2_to_gcs(self.name, self.name)\n \n     def _get_bucket(self) -> Tuple[StorageHandle, bool]:\n@@ -1776,11 +1884,12 @@ class GcsStore(AbstractStore):\n             bucket = self.client.get_bucket(self.name)\n             return bucket, False\n         except gcp.not_found_exception() as e:\n-            if isinstance(self.source, str) and self.source.startswith('gs://'):\n+            if isinstance(self.source, str) and self.source.startswith(\"gs://\"):\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketGetError(\n-                        'Attempted to use a non-existent bucket as a source: '\n-                        f'{self.source}') from e\n+                        \"Attempted to use a non-existent bucket as a source: \"\n+                        f\"{self.source}\"\n+                    ) from e\n             else:\n                 # If bucket cannot be found (i.e., does not exist), it is to be\n                 # created by Sky. However, creation is skipped if Store object\n@@ -1794,12 +1903,11 @@ class GcsStore(AbstractStore):\n                     # sky storage delete or to re-mount Storages with sky start\n                     # but the storage is already removed externally.\n                     raise exceptions.StorageExternalDeletionError(\n-                        'Attempted to fetch a non-existent bucket: '\n-                        f'{self.name}') from e\n+                        f\"Attempted to fetch a non-existent bucket: {self.name}\"\n+                    ) from e\n         except gcp.forbidden_exception():\n             # Try public bucket to see if bucket exists\n-            logger.info(\n-                'External Bucket detected; Connecting to external bucket...')\n+            logger.info(\"External Bucket detected; Connecting to external bucket...\")\n             try:\n                 a_client = gcp.anonymous_storage_client()\n                 bucket = a_client.bucket(self.name)\n@@ -1807,11 +1915,12 @@ class GcsStore(AbstractStore):\n                 next(bucket.list_blobs())\n                 return bucket, False\n             except (gcp.not_found_exception(), ValueError) as e:\n-                command = f'gsutil ls gs://{self.name}'\n+                command = f\"gsutil ls gs://{self.name}\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketGetError(\n-                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +\n-                        f' To debug, consider running `{command}`.') from e\n+                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name)\n+                        + f\" To debug, consider running `{command}`.\"\n+                    ) from e\n \n     def mount_command(self, mount_path: str) -> str:\n         \"\"\"Returns the command to mount the bucket to the mount_path.\n@@ -1821,24 +1930,26 @@ class GcsStore(AbstractStore):\n         Args:\n           mount_path: str; Path to mount the bucket to.\n         \"\"\"\n-        install_cmd = ('wget -nc https://github.com/GoogleCloudPlatform/gcsfuse'\n-                       f'/releases/download/v{self.GCSFUSE_VERSION}/'\n-                       f'gcsfuse_{self.GCSFUSE_VERSION}_amd64.deb '\n-                       '-O /tmp/gcsfuse.deb && '\n-                       'sudo dpkg --install /tmp/gcsfuse.deb')\n-        mount_cmd = ('gcsfuse -o allow_other '\n-                     '--implicit-dirs '\n-                     f'--stat-cache-capacity {self._STAT_CACHE_CAPACITY} '\n-                     f'--stat-cache-ttl {self._STAT_CACHE_TTL} '\n-                     f'--type-cache-ttl {self._TYPE_CACHE_TTL} '\n-                     f'--rename-dir-limit {self._RENAME_DIR_LIMIT} '\n-                     f'{self.bucket.name} {mount_path}')\n-        version_check_cmd = (\n-            f'gcsfuse --version | grep -q {self.GCSFUSE_VERSION}')\n-        return mounting_utils.get_mounting_command(StorageMode.MOUNT,\n-                                                   mount_path, mount_cmd,\n-                                                   install_cmd,\n-                                                   version_check_cmd)\n+        install_cmd = (\n+            \"wget -nc https://github.com/GoogleCloudPlatform/gcsfuse\"\n+            f\"/releases/download/v{self.GCSFUSE_VERSION}/\"\n+            f\"gcsfuse_{self.GCSFUSE_VERSION}_amd64.deb \"\n+            \"-O /tmp/gcsfuse.deb && \"\n+            \"sudo dpkg --install /tmp/gcsfuse.deb\"\n+        )\n+        mount_cmd = (\n+            \"gcsfuse -o allow_other \"\n+            \"--implicit-dirs \"\n+            f\"--stat-cache-capacity {self._STAT_CACHE_CAPACITY} \"\n+            f\"--stat-cache-ttl {self._STAT_CACHE_TTL} \"\n+            f\"--type-cache-ttl {self._TYPE_CACHE_TTL} \"\n+            f\"--rename-dir-limit {self._RENAME_DIR_LIMIT} \"\n+            f\"{self.bucket.name} {mount_path}\"\n+        )\n+        version_check_cmd = f\"gcsfuse --version | grep -q {self.GCSFUSE_VERSION}\"\n+        return mounting_utils.get_mounting_command(\n+            StorageMode.MOUNT, mount_path, mount_cmd, install_cmd, version_check_cmd\n+        )\n \n     def _download_file(self, remote_path: str, local_path: str) -> None:\n         \"\"\"Downloads file from remote to local on GS bucket\n@@ -1850,9 +1961,9 @@ class GcsStore(AbstractStore):\n         blob = self.bucket.blob(remote_path)\n         blob.download_to_filename(local_path, timeout=None)\n \n-    def _create_gcs_bucket(self,\n-                           bucket_name: str,\n-                           region='us-central1') -> StorageHandle:\n+    def _create_gcs_bucket(\n+        self, bucket_name: str, region=\"us-central1\"\n+    ) -> StorageHandle:\n         \"\"\"Creates GCS bucket with specific name in specific region\n \n         Args:\n@@ -1861,16 +1972,17 @@ class GcsStore(AbstractStore):\n         \"\"\"\n         try:\n             bucket = self.client.bucket(bucket_name)\n-            bucket.storage_class = 'STANDARD'\n+            bucket.storage_class = \"STANDARD\"\n             new_bucket = self.client.create_bucket(bucket, location=region)\n         except Exception as e:  # pylint: disable=broad-except\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketCreateError(\n-                    f'Attempted to create a bucket {self.name} but failed.'\n+                    f\"Attempted to create a bucket {self.name} but failed.\"\n                 ) from e\n         logger.info(\n-            f'Created GCS bucket {new_bucket.name} in {new_bucket.location} '\n-            f'with storage class {new_bucket.storage_class}')\n+            f\"Created GCS bucket {new_bucket.name} in {new_bucket.location} \"\n+            f\"with storage class {new_bucket.storage_class}\"\n+        )\n         return new_bucket\n \n     def _delete_gcs_bucket(self, bucket_name: str) -> bool:\n@@ -1883,37 +1995,43 @@ class GcsStore(AbstractStore):\n          bool; True if bucket was deleted, False if it was deleted externally.\n         \"\"\"\n \n-        with rich_utils.safe_status(\n-                f'[bold cyan]Deleting GCS bucket {bucket_name}[/]'):\n+        with rich_utils.safe_status(f\"[bold cyan]Deleting GCS bucket {bucket_name}[/]\"):\n             try:\n                 self.client.get_bucket(bucket_name)\n             except gcp.forbidden_exception() as e:\n                 # Try public bucket to see if bucket exists\n                 with ux_utils.print_exception_no_traceback():\n                     raise PermissionError(\n-                        'External Bucket detected. User not allowed to delete '\n-                        'external bucket.') from e\n+                        \"External Bucket detected. User not allowed to delete \"\n+                        \"external bucket.\"\n+                    ) from e\n             except gcp.not_found_exception():\n                 # If bucket does not exist, it may have been deleted externally.\n                 # Do a no-op in that case.\n                 logger.debug(\n                     _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE.format(\n-                        bucket_name=bucket_name))\n+                        bucket_name=bucket_name\n+                    )\n+                )\n                 return False\n             try:\n                 gsutil_alias, alias_gen = data_utils.get_gsutil_command()\n-                remove_obj_command = (f'{alias_gen};{gsutil_alias} '\n-                                      f'rm -r gs://{bucket_name}')\n-                subprocess.check_output(remove_obj_command,\n-                                        stderr=subprocess.STDOUT,\n-                                        shell=True,\n-                                        executable='/bin/bash')\n+                remove_obj_command = (\n+                    f\"{alias_gen};{gsutil_alias} rm -r gs://{bucket_name}\"\n+                )\n+                subprocess.check_output(\n+                    remove_obj_command,\n+                    stderr=subprocess.STDOUT,\n+                    shell=True,\n+                    executable=\"/bin/bash\",\n+                )\n                 return True\n             except subprocess.CalledProcessError as e:\n                 logger.error(e.output)\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketDeleteError(\n-                        f'Failed to delete GCS bucket {bucket_name}.')\n+                        f\"Failed to delete GCS bucket {bucket_name}.\"\n+                    )\n \n \n class R2Store(AbstractStore):\n@@ -1921,57 +2039,65 @@ class R2Store(AbstractStore):\n     for R2 buckets.\n     \"\"\"\n \n-    _ACCESS_DENIED_MESSAGE = 'Access Denied'\n+    _ACCESS_DENIED_MESSAGE = \"Access Denied\"\n \n-    def __init__(self,\n-                 name: str,\n-                 source: str,\n-                 region: Optional[str] = 'auto',\n-                 is_sky_managed: Optional[bool] = None,\n-                 sync_on_reconstruction: Optional[bool] = True):\n-        self.client: 'boto3.client.Client'\n-        self.bucket: 'StorageHandle'\n-        super().__init__(name, source, region, is_sky_managed,\n-                         sync_on_reconstruction)\n+    def __init__(\n+        self,\n+        name: str,\n+        source: str,\n+        region: Optional[str] = \"auto\",\n+        is_sky_managed: Optional[bool] = None,\n+        sync_on_reconstruction: Optional[bool] = True,\n+    ):\n+        self.client: \"boto3.client.Client\"\n+        self.bucket: \"StorageHandle\"\n+        super().__init__(name, source, region, is_sky_managed, sync_on_reconstruction)\n \n     def _validate(self):\n         if self.source is not None and isinstance(self.source, str):\n-            if self.source.startswith('s3://'):\n+            if self.source.startswith(\"s3://\"):\n                 assert self.name == data_utils.split_s3_path(self.source)[0], (\n-                    'S3 Bucket is specified as path, the name should be the'\n-                    ' same as S3 bucket.')\n+                    \"S3 Bucket is specified as path, the name should be the\"\n+                    \" same as S3 bucket.\"\n+                )\n                 assert data_utils.verify_s3_bucket(self.name), (\n-                    f'Source specified as {self.source}, a S3 bucket. ',\n-                    'S3 Bucket should exist.')\n-            elif self.source.startswith('gs://'):\n+                    f\"Source specified as {self.source}, a S3 bucket. \",\n+                    \"S3 Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"gs://\"):\n                 assert self.name == data_utils.split_gcs_path(self.source)[0], (\n-                    'GCS Bucket is specified as path, the name should be '\n-                    'the same as GCS bucket.')\n+                    \"GCS Bucket is specified as path, the name should be \"\n+                    \"the same as GCS bucket.\"\n+                )\n                 assert data_utils.verify_gcs_bucket(self.name), (\n-                    f'Source specified as {self.source}, a GCS bucket. ',\n-                    'GCS Bucket should exist.')\n-            elif self.source.startswith('r2://'):\n+                    f\"Source specified as {self.source}, a GCS bucket. \",\n+                    \"GCS Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"r2://\"):\n                 assert self.name == data_utils.split_r2_path(self.source)[0], (\n-                    'R2 Bucket is specified as path, the name should be '\n-                    'the same as R2 bucket.')\n-            elif self.source.startswith('cos://'):\n+                    \"R2 Bucket is specified as path, the name should be \"\n+                    \"the same as R2 bucket.\"\n+                )\n+            elif self.source.startswith(\"cos://\"):\n                 assert self.name == data_utils.split_cos_path(self.source)[0], (\n-                    'IBM COS Bucket is specified as path, the name should be '\n-                    'the same as COS bucket.')\n+                    \"IBM COS Bucket is specified as path, the name should be \"\n+                    \"the same as COS bucket.\"\n+                )\n                 assert data_utils.verify_ibm_cos_bucket(self.name), (\n-                    f'Source specified as {self.source}, a COS bucket. ',\n-                    'COS Bucket should exist.')\n+                    f\"Source specified as {self.source}, a COS bucket. \",\n+                    \"COS Bucket should exist.\",\n+                )\n         # Validate name\n         self.name = S3Store.validate_name(self.name)\n         # Check if the storage is enabled\n         if not _is_storage_cloud_enabled(cloudflare.NAME):\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.ResourcesUnavailableError(\n-                    'Storage \\'store: r2\\' specified, but ' \\\n-                    'Cloudflare R2 access is disabled. To fix, '\\\n-                    'enable Cloudflare R2 by running `sky check`. '\\\n-                    'More info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.'  # pylint: disable=line-too-long\n-                    )\n+                    \"Storage 'store: r2' specified, but \"\n+                    \"Cloudflare R2 access is disabled. To fix, \"\n+                    \"enable Cloudflare R2 by running `sky check`. \"\n+                    \"More info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.\"  # pylint: disable=line-too-long\n+                )\n \n     def initialize(self):\n         \"\"\"Initializes the R2 store object on the cloud.\n@@ -2006,11 +2132,11 @@ class R2Store(AbstractStore):\n             if isinstance(self.source, list):\n                 self.batch_aws_rsync(self.source, create_dirs=True)\n             elif self.source is not None:\n-                if self.source.startswith('s3://'):\n+                if self.source.startswith(\"s3://\"):\n                     self._transfer_to_r2()\n-                elif self.source.startswith('gs://'):\n+                elif self.source.startswith(\"gs://\"):\n                     self._transfer_to_r2()\n-                elif self.source.startswith('r2://'):\n+                elif self.source.startswith(\"r2://\"):\n                     pass\n                 else:\n                     self.batch_aws_rsync([self.source])\n@@ -2018,24 +2144,26 @@ class R2Store(AbstractStore):\n             raise\n         except Exception as e:\n             raise exceptions.StorageUploadError(\n-                f'Upload failed for store {self.name}') from e\n+                f\"Upload failed for store {self.name}\"\n+            ) from e\n \n     def delete(self) -> None:\n         deleted_by_skypilot = self._delete_r2_bucket(self.name)\n         if deleted_by_skypilot:\n-            msg_str = f'Deleted R2 bucket {self.name}.'\n+            msg_str = f\"Deleted R2 bucket {self.name}.\"\n         else:\n-            msg_str = f'R2 bucket {self.name} may have been deleted ' \\\n-                      f'externally. Removing from local state.'\n-        logger.info(f'{colorama.Fore.GREEN}{msg_str}'\n-                    f'{colorama.Style.RESET_ALL}')\n+            msg_str = (\n+                f\"R2 bucket {self.name} may have been deleted \"\n+                f\"externally. Removing from local state.\"\n+            )\n+        logger.info(f\"{colorama.Fore.GREEN}{msg_str}{colorama.Style.RESET_ALL}\")\n \n     def get_handle(self) -> StorageHandle:\n-        return cloudflare.resource('s3').Bucket(self.name)\n+        return cloudflare.resource(\"s3\").Bucket(self.name)\n \n-    def batch_aws_rsync(self,\n-                        source_path_list: List[Path],\n-                        create_dirs: bool = False) -> None:\n+    def batch_aws_rsync(\n+        self, source_path_list: List[Path], create_dirs: bool = False\n+    ) -> None:\n         \"\"\"Invokes aws s3 sync to batch upload a list of local paths to R2\n \n         AWS Sync by default uses 10 threads to upload files to the bucket.  To\n@@ -2055,45 +2183,53 @@ class R2Store(AbstractStore):\n         \"\"\"\n \n         def get_file_sync_command(base_dir_path, file_names):\n-            includes = ' '.join(\n-                [f'--include \"{file_name}\"' for file_name in file_names])\n+            includes = \" \".join(\n+                [f'--include \"{file_name}\"' for file_name in file_names]\n+            )\n             endpoint_url = cloudflare.create_endpoint()\n-            sync_command = ('AWS_SHARED_CREDENTIALS_FILE='\n-                            f'{cloudflare.R2_CREDENTIALS_PATH} '\n-                            'aws s3 sync --no-follow-symlinks --exclude=\"*\" '\n-                            f'{includes} {base_dir_path} '\n-                            f's3://{self.name} '\n-                            f'--endpoint {endpoint_url} '\n-                            f'--profile={cloudflare.R2_PROFILE_NAME}')\n+            sync_command = (\n+                \"AWS_SHARED_CREDENTIALS_FILE=\"\n+                f\"{cloudflare.R2_CREDENTIALS_PATH} \"\n+                'aws s3 sync --no-follow-symlinks --exclude=\"*\" '\n+                f\"{includes} {base_dir_path} \"\n+                f\"s3://{self.name} \"\n+                f\"--endpoint {endpoint_url} \"\n+                f\"--profile={cloudflare.R2_PROFILE_NAME}\"\n+            )\n             return sync_command\n \n         def get_dir_sync_command(src_dir_path, dest_dir_name):\n             # we exclude .git directory from the sync\n             excluded_list = storage_utils.get_excluded_files_from_gitignore(\n-                src_dir_path)\n-            excluded_list.append('.git/*')\n-            excludes = ' '.join(\n-                [f'--exclude \"{file_name}\"' for file_name in excluded_list])\n+                src_dir_path\n+            )\n+            excluded_list.append(\".git/*\")\n+            excludes = \" \".join(\n+                [f'--exclude \"{file_name}\"' for file_name in excluded_list]\n+            )\n             endpoint_url = cloudflare.create_endpoint()\n \n-            sync_command = ('AWS_SHARED_CREDENTIALS_FILE='\n-                            f'{cloudflare.R2_CREDENTIALS_PATH} '\n-                            f'aws s3 sync --no-follow-symlinks {excludes} '\n-                            f'{src_dir_path} '\n-                            f's3://{self.name}/{dest_dir_name} '\n-                            f'--endpoint {endpoint_url} '\n-                            f'--profile={cloudflare.R2_PROFILE_NAME}')\n+            sync_command = (\n+                \"AWS_SHARED_CREDENTIALS_FILE=\"\n+                f\"{cloudflare.R2_CREDENTIALS_PATH} \"\n+                f\"aws s3 sync --no-follow-symlinks {excludes} \"\n+                f\"{src_dir_path} \"\n+                f\"s3://{self.name}/{dest_dir_name} \"\n+                f\"--endpoint {endpoint_url} \"\n+                f\"--profile={cloudflare.R2_PROFILE_NAME}\"\n+            )\n             return sync_command\n \n         # Generate message for upload\n         if len(source_path_list) > 1:\n-            source_message = f'{len(source_path_list)} paths'\n+            source_message = f\"{len(source_path_list)} paths\"\n         else:\n             source_message = source_path_list[0]\n \n         with rich_utils.safe_status(\n-                f'[bold cyan]Syncing '\n-                f'[green]{source_message}[/] to [green]r2://{self.name}/[/]'):\n+            f\"[bold cyan]Syncing \"\n+            f\"[green]{source_message}[/] to [green]r2://{self.name}/[/]\"\n+        ):\n             data_utils.parallel_upload(\n                 source_path_list,\n                 get_file_sync_command,\n@@ -2101,13 +2237,14 @@ class R2Store(AbstractStore):\n                 self.name,\n                 self._ACCESS_DENIED_MESSAGE,\n                 create_dirs=create_dirs,\n-                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)\n+                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS,\n+            )\n \n     def _transfer_to_r2(self) -> None:\n         assert isinstance(self.source, str), self.source\n-        if self.source.startswith('gs://'):\n+        if self.source.startswith(\"gs://\"):\n             data_transfer.gcs_to_r2(self.name, self.name)\n-        elif self.source.startswith('s3://'):\n+        elif self.source.startswith(\"s3://\"):\n             data_transfer.s3_to_r2(self.name, self.name)\n \n     def _get_bucket(self) -> Tuple[StorageHandle, bool]:\n@@ -2127,7 +2264,7 @@ class R2Store(AbstractStore):\n                 attempted to be fetched while reconstructing the storage for\n                 'sky storage delete' or 'sky start'\n         \"\"\"\n-        r2 = cloudflare.resource('s3')\n+        r2 = cloudflare.resource(\"s3\")\n         bucket = r2.Bucket(self.name)\n         endpoint_url = cloudflare.create_endpoint()\n         try:\n@@ -2138,31 +2275,35 @@ class R2Store(AbstractStore):\n             self.client.head_bucket(Bucket=self.name)\n             return bucket, False\n         except aws.botocore_exceptions().ClientError as e:\n-            error_code = e.response['Error']['Code']\n+            error_code = e.response[\"Error\"][\"Code\"]\n             # AccessDenied error for buckets that are private and not owned by\n             # user.\n-            if error_code == '403':\n-                command = ('AWS_SHARED_CREDENTIALS_FILE='\n-                           f'{cloudflare.R2_CREDENTIALS_PATH} '\n-                           f'aws s3 ls s3://{self.name} '\n-                           f'--endpoint {endpoint_url} '\n-                           f'--profile={cloudflare.R2_PROFILE_NAME}')\n+            if error_code == \"403\":\n+                command = (\n+                    \"AWS_SHARED_CREDENTIALS_FILE=\"\n+                    f\"{cloudflare.R2_CREDENTIALS_PATH} \"\n+                    f\"aws s3 ls s3://{self.name} \"\n+                    f\"--endpoint {endpoint_url} \"\n+                    f\"--profile={cloudflare.R2_PROFILE_NAME}\"\n+                )\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketGetError(\n-                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +\n-                        f' To debug, consider running `{command}`.') from e\n+                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name)\n+                        + f\" To debug, consider running `{command}`.\"\n+                    ) from e\n \n-        if isinstance(self.source, str) and self.source.startswith('r2://'):\n+        if isinstance(self.source, str) and self.source.startswith(\"r2://\"):\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketGetError(\n-                    'Attempted to use a non-existent bucket as a source: '\n-                    f'{self.source}. Consider using '\n-                    '`AWS_SHARED_CREDENTIALS_FILE='\n-                    f'{cloudflare.R2_CREDENTIALS_PATH} aws s3 ls '\n-                    f's3://{self.name} '\n-                    f'--endpoint {endpoint_url} '\n-                    f'--profile={cloudflare.R2_PROFILE_NAME}\\' '\n-                    'to debug.')\n+                    \"Attempted to use a non-existent bucket as a source: \"\n+                    f\"{self.source}. Consider using \"\n+                    \"`AWS_SHARED_CREDENTIALS_FILE=\"\n+                    f\"{cloudflare.R2_CREDENTIALS_PATH} aws s3 ls \"\n+                    f\"s3://{self.name} \"\n+                    f\"--endpoint {endpoint_url} \"\n+                    f\"--profile={cloudflare.R2_PROFILE_NAME}' \"\n+                    \"to debug.\"\n+                )\n \n         # If bucket cannot be found in both private and public settings,\n         # the bucket is to be created by Sky. However, creation is skipped if\n@@ -2176,8 +2317,8 @@ class R2Store(AbstractStore):\n             # delete or to re-mount Storages with sky start but the storage\n             # is already removed externally.\n             raise exceptions.StorageExternalDeletionError(\n-                'Attempted to fetch a non-existent bucket: '\n-                f'{self.name}')\n+                f\"Attempted to fetch a non-existent bucket: {self.name}\"\n+            )\n \n     def _download_file(self, remote_path: str, local_path: str) -> None:\n         \"\"\"Downloads file from remote to local on r2 bucket\n@@ -2197,25 +2338,28 @@ class R2Store(AbstractStore):\n         Args:\n           mount_path: str; Path to mount the bucket to.\n         \"\"\"\n-        install_cmd = ('sudo wget -nc https://github.com/romilbhardwaj/goofys/'\n-                       'releases/download/0.24.0-romilb-upstream/goofys '\n-                       '-O /usr/local/bin/goofys && '\n-                       'sudo chmod +x /usr/local/bin/goofys')\n+        install_cmd = (\n+            \"sudo wget -nc https://github.com/romilbhardwaj/goofys/\"\n+            \"releases/download/0.24.0-romilb-upstream/goofys \"\n+            \"-O /usr/local/bin/goofys && \"\n+            \"sudo chmod +x /usr/local/bin/goofys\"\n+        )\n         endpoint_url = cloudflare.create_endpoint()\n         mount_cmd = (\n-            f'AWS_SHARED_CREDENTIALS_FILE={cloudflare.R2_CREDENTIALS_PATH} '\n-            f'AWS_PROFILE={cloudflare.R2_PROFILE_NAME} goofys -o allow_other '\n-            f'--stat-cache-ttl {self._STAT_CACHE_TTL} '\n-            f'--type-cache-ttl {self._TYPE_CACHE_TTL} '\n-            f'--endpoint {endpoint_url} '\n-            f'{self.bucket.name} {mount_path}')\n-        return mounting_utils.get_mounting_command(StorageMode.MOUNT,\n-                                                   mount_path, mount_cmd,\n-                                                   install_cmd)\n-\n-    def csync_command(self,\n-                      csync_path: str,\n-                      interval_seconds: Optional[int] = None) -> str:\n+            f\"AWS_SHARED_CREDENTIALS_FILE={cloudflare.R2_CREDENTIALS_PATH} \"\n+            f\"AWS_PROFILE={cloudflare.R2_PROFILE_NAME} goofys -o allow_other \"\n+            f\"--stat-cache-ttl {self._STAT_CACHE_TTL} \"\n+            f\"--type-cache-ttl {self._TYPE_CACHE_TTL} \"\n+            f\"--endpoint {endpoint_url} \"\n+            f\"{self.bucket.name} {mount_path}\"\n+        )\n+        return mounting_utils.get_mounting_command(\n+            StorageMode.MOUNT, mount_path, mount_cmd, install_cmd\n+        )\n+\n+    def csync_command(\n+        self, csync_path: str, interval_seconds: Optional[int] = None\n+    ) -> str:\n         \"\"\"Returns command to mount CSYNC with Storage bucket on CSYNC_PATH.\n \n         Args:\n@@ -2224,9 +2368,7 @@ class R2Store(AbstractStore):\n         \"\"\"\n         raise NotImplementedError\n \n-    def _create_r2_bucket(self,\n-                          bucket_name: str,\n-                          region='auto') -> StorageHandle:\n+    def _create_r2_bucket(self, bucket_name: str, region=\"auto\") -> StorageHandle:\n         \"\"\"Creates R2 bucket with specific name in specific region\n \n         Args:\n@@ -2240,16 +2382,17 @@ class R2Store(AbstractStore):\n             if region is None:\n                 r2_client.create_bucket(Bucket=bucket_name)\n             else:\n-                location = {'LocationConstraint': region}\n-                r2_client.create_bucket(Bucket=bucket_name,\n-                                        CreateBucketConfiguration=location)\n-                logger.info(f'Created R2 bucket {bucket_name} in {region}')\n+                location = {\"LocationConstraint\": region}\n+                r2_client.create_bucket(\n+                    Bucket=bucket_name, CreateBucketConfiguration=location\n+                )\n+                logger.info(f\"Created R2 bucket {bucket_name} in {region}\")\n         except aws.botocore_exceptions().ClientError as e:\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketCreateError(\n-                    f'Attempted to create a bucket '\n-                    f'{self.name} but failed.') from e\n-        return cloudflare.resource('s3').Bucket(bucket_name)\n+                    f\"Attempted to create a bucket {self.name} but failed.\"\n+                ) from e\n+        return cloudflare.resource(\"s3\").Bucket(bucket_name)\n \n     def _delete_r2_bucket(self, bucket_name: str) -> bool:\n         \"\"\"Deletes R2 bucket, including all objects in bucket\n@@ -2269,27 +2412,32 @@ class R2Store(AbstractStore):\n         # which removes the bucket by force.\n         endpoint_url = cloudflare.create_endpoint()\n         remove_command = (\n-            f'AWS_SHARED_CREDENTIALS_FILE={cloudflare.R2_CREDENTIALS_PATH} '\n-            f'aws s3 rb s3://{bucket_name} --force '\n-            f'--endpoint {endpoint_url} '\n-            f'--profile={cloudflare.R2_PROFILE_NAME}')\n+            f\"AWS_SHARED_CREDENTIALS_FILE={cloudflare.R2_CREDENTIALS_PATH} \"\n+            f\"aws s3 rb s3://{bucket_name} --force \"\n+            f\"--endpoint {endpoint_url} \"\n+            f\"--profile={cloudflare.R2_PROFILE_NAME}\"\n+        )\n         try:\n             with rich_utils.safe_status(\n-                    f'[bold cyan]Deleting R2 bucket {bucket_name}[/]'):\n-                subprocess.check_output(remove_command,\n-                                        stderr=subprocess.STDOUT,\n-                                        shell=True)\n+                f\"[bold cyan]Deleting R2 bucket {bucket_name}[/]\"\n+            ):\n+                subprocess.check_output(\n+                    remove_command, stderr=subprocess.STDOUT, shell=True\n+                )\n         except subprocess.CalledProcessError as e:\n-            if 'NoSuchBucket' in e.output.decode('utf-8'):\n+            if \"NoSuchBucket\" in e.output.decode(\"utf-8\"):\n                 logger.debug(\n                     _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE.format(\n-                        bucket_name=bucket_name))\n+                        bucket_name=bucket_name\n+                    )\n+                )\n                 return False\n             else:\n                 logger.error(e.output)\n                 with ux_utils.print_exception_no_traceback():\n                     raise exceptions.StorageBucketDeleteError(\n-                        f'Failed to delete R2 bucket {bucket_name}.')\n+                        f\"Failed to delete R2 bucket {bucket_name}.\"\n+                    )\n \n         # Wait until bucket deletion propagates on AWS servers\n         while data_utils.verify_r2_bucket(bucket_name):\n@@ -2301,49 +2449,58 @@ class IBMCosStore(AbstractStore):\n     \"\"\"IBMCosStore inherits from Storage Object and represents the backend\n     for COS buckets.\n     \"\"\"\n-    _ACCESS_DENIED_MESSAGE = 'Access Denied'\n-\n-    def __init__(self,\n-                 name: str,\n-                 source: str,\n-                 region: Optional[str] = 'us-east',\n-                 is_sky_managed: Optional[bool] = None,\n-                 sync_on_reconstruction: bool = True):\n-        self.client: 'storage.Client'\n-        self.bucket: 'StorageHandle'\n-        super().__init__(name, source, region, is_sky_managed,\n-                         sync_on_reconstruction)\n-        self.bucket_rclone_profile = \\\n-          Rclone.generate_rclone_bucket_profile_name(\n-            self.name, Rclone.RcloneClouds.IBM)\n+\n+    _ACCESS_DENIED_MESSAGE = \"Access Denied\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        source: str,\n+        region: Optional[str] = \"us-east\",\n+        is_sky_managed: Optional[bool] = None,\n+        sync_on_reconstruction: bool = True,\n+    ):\n+        self.client: \"storage.Client\"\n+        self.bucket: \"StorageHandle\"\n+        super().__init__(name, source, region, is_sky_managed, sync_on_reconstruction)\n+        self.bucket_rclone_profile = Rclone.generate_rclone_bucket_profile_name(\n+            self.name, Rclone.RcloneClouds.IBM\n+        )\n \n     def _validate(self):\n         if self.source is not None and isinstance(self.source, str):\n-            if self.source.startswith('s3://'):\n+            if self.source.startswith(\"s3://\"):\n                 assert self.name == data_utils.split_s3_path(self.source)[0], (\n-                    'S3 Bucket is specified as path, the name should be the'\n-                    ' same as S3 bucket.')\n+                    \"S3 Bucket is specified as path, the name should be the\"\n+                    \" same as S3 bucket.\"\n+                )\n                 assert data_utils.verify_s3_bucket(self.name), (\n-                    f'Source specified as {self.source}, a S3 bucket. ',\n-                    'S3 Bucket should exist.')\n-            elif self.source.startswith('gs://'):\n+                    f\"Source specified as {self.source}, a S3 bucket. \",\n+                    \"S3 Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"gs://\"):\n                 assert self.name == data_utils.split_gcs_path(self.source)[0], (\n-                    'GCS Bucket is specified as path, the name should be '\n-                    'the same as GCS bucket.')\n+                    \"GCS Bucket is specified as path, the name should be \"\n+                    \"the same as GCS bucket.\"\n+                )\n                 assert data_utils.verify_gcs_bucket(self.name), (\n-                    f'Source specified as {self.source}, a GCS bucket. ',\n-                    'GCS Bucket should exist.')\n-            elif self.source.startswith('r2://'):\n+                    f\"Source specified as {self.source}, a GCS bucket. \",\n+                    \"GCS Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"r2://\"):\n                 assert self.name == data_utils.split_r2_path(self.source)[0], (\n-                    'R2 Bucket is specified as path, the name should be '\n-                    'the same as R2 bucket.')\n+                    \"R2 Bucket is specified as path, the name should be \"\n+                    \"the same as R2 bucket.\"\n+                )\n                 assert data_utils.verify_r2_bucket(self.name), (\n-                    f'Source specified as {self.source}, a R2 bucket. ',\n-                    'R2 Bucket should exist.')\n-            elif self.source.startswith('cos://'):\n+                    f\"Source specified as {self.source}, a R2 bucket. \",\n+                    \"R2 Bucket should exist.\",\n+                )\n+            elif self.source.startswith(\"cos://\"):\n                 assert self.name == data_utils.split_cos_path(self.source)[0], (\n-                    'COS Bucket is specified as path, the name should be '\n-                    'the same as COS bucket.')\n+                    \"COS Bucket is specified as path, the name should be \"\n+                    \"the same as COS bucket.\"\n+                )\n         # Validate name\n         self.name = IBMCosStore.validate_name(self.name)\n \n@@ -2361,36 +2518,41 @@ class IBMCosStore(AbstractStore):\n         if name is not None and isinstance(name, str):\n             if not 3 <= len(name) <= 63:\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: {name} must be between 3 (min) '\n-                    'and 63 (max) characters long.')\n+                    f\"Invalid store name: {name} must be between 3 (min) \"\n+                    \"and 63 (max) characters long.\"\n+                )\n \n             # Check for valid characters and start/end with a letter or number\n-            pattern = r'^[a-z0-9][-a-z0-9.]*[a-z0-9]$'\n+            pattern = r\"^[a-z0-9][-a-z0-9.]*[a-z0-9]$\"\n             if not re.match(pattern, name):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: {name} can consist only of '\n-                    'lowercase letters, numbers, dots (.), and dashes (-). '\n-                    'It must begin and end with a letter or number.')\n+                    f\"Invalid store name: {name} can consist only of \"\n+                    \"lowercase letters, numbers, dots (.), and dashes (-). \"\n+                    \"It must begin and end with a letter or number.\"\n+                )\n \n             # Check for two adjacent periods or dashes\n-            if any(substring in name for substring in ['..', '--']):\n+            if any(substring in name for substring in [\"..\", \"--\"]):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: {name} must not contain '\n-                    'two adjacent periods/dashes')\n+                    f\"Invalid store name: {name} must not contain \"\n+                    \"two adjacent periods/dashes\"\n+                )\n \n             # Check for IP address format\n-            ip_pattern = r'^(?:\\d{1,3}\\.){3}\\d{1,3}$'\n+            ip_pattern = r\"^(?:\\d{1,3}\\.){3}\\d{1,3}$\"\n             if re.match(ip_pattern, name):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: {name} must not be formatted as '\n-                    'an IP address (for example, 192.168.5.4).')\n+                    f\"Invalid store name: {name} must not be formatted as \"\n+                    \"an IP address (for example, 192.168.5.4).\"\n+                )\n \n-            if any(substring in name for substring in ['.-', '-.']):\n+            if any(substring in name for substring in [\".-\", \"-.\"]):\n                 _raise_no_traceback_name_error(\n-                    f'Invalid store name: {name} must '\n-                    'not allow substrings: \".-\", \"-.\" .')\n+                    f\"Invalid store name: {name} must \"\n+                    'not allow substrings: \".-\", \"-.\" .'\n+                )\n         else:\n-            _raise_no_traceback_name_error('Store name must be specified.')\n+            _raise_no_traceback_name_error(\"Store name must be specified.\")\n         return name\n \n     def initialize(self):\n@@ -2427,36 +2589,45 @@ class IBMCosStore(AbstractStore):\n             if isinstance(self.source, list):\n                 self.batch_ibm_rsync(self.source, create_dirs=True)\n             elif self.source is not None:\n-                if self.source.startswith('cos://'):\n+                if self.source.startswith(\"cos://\"):\n                     # cos bucket used as a dest, can't be used as source.\n                     pass\n-                elif self.source.startswith('s3://'):\n-                    raise Exception('IBM COS currently not supporting'\n-                                    'data transfers between COS and S3')\n-                elif self.source.startswith('gs://'):\n-                    raise Exception('IBM COS currently not supporting'\n-                                    'data transfers between COS and GS')\n-                elif self.source.startswith('r2://'):\n-                    raise Exception('IBM COS currently not supporting'\n-                                    'data transfers between COS and r2')\n+                elif self.source.startswith(\"s3://\"):\n+                    raise Exception(\n+                        \"IBM COS currently not supporting\"\n+                        \"data transfers between COS and S3\"\n+                    )\n+                elif self.source.startswith(\"gs://\"):\n+                    raise Exception(\n+                        \"IBM COS currently not supporting\"\n+                        \"data transfers between COS and GS\"\n+                    )\n+                elif self.source.startswith(\"r2://\"):\n+                    raise Exception(\n+                        \"IBM COS currently not supporting\"\n+                        \"data transfers between COS and r2\"\n+                    )\n                 else:\n                     self.batch_ibm_rsync([self.source])\n \n         except Exception as e:\n             raise exceptions.StorageUploadError(\n-                f'Upload failed for store {self.name}') from e\n+                f\"Upload failed for store {self.name}\"\n+            ) from e\n \n     def delete(self) -> None:\n         self._delete_cos_bucket()\n-        logger.info(f'{colorama.Fore.GREEN}Deleted COS bucket {self.name}.'\n-                    f'{colorama.Style.RESET_ALL}')\n+        logger.info(\n+            f\"{colorama.Fore.GREEN}Deleted COS bucket {self.name}.\"\n+            f\"{colorama.Style.RESET_ALL}\"\n+        )\n \n     def get_handle(self) -> StorageHandle:\n         return self.s3_resource.Bucket(self.name)\n \n-    def batch_ibm_rsync(self,\n-                        source_path_list: List[Path],\n-                        create_dirs: bool = False) -> None:\n+    def batch_ibm_rsync(\n+        self, source_path_list: List[Path], create_dirs: bool = False\n+    ) -> None:\n         \"\"\"Invokes rclone copy to batch upload a list of local paths to cos\n \n         Since rclone does not support batch operations, we construct\n@@ -2493,7 +2664,8 @@ class IBMCosStore(AbstractStore):\n             sync_command = (\n                 'rclone copy --exclude \".git/*\" '\n                 f'\"{src_dir_path}\" '\n-                f'{self.bucket_rclone_profile}:{self.name}/{dest_dir_name}')\n+                f\"{self.bucket_rclone_profile}:{self.name}/{dest_dir_name}\"\n+            )\n             return sync_command\n \n         def get_file_sync_command(base_dir_path, file_names) -> str:\n@@ -2514,23 +2686,27 @@ class IBMCosStore(AbstractStore):\n             \"\"\"\n \n             # wrapping file_name with \"\" to support spaces\n-            includes = ' '.join(\n-                [f'--include \"{file_name}\"' for file_name in file_names])\n-            sync_command = ('rclone copy '\n-                            f'{includes} \"{base_dir_path}\" '\n-                            f'{self.bucket_rclone_profile}:{self.name}')\n+            includes = \" \".join(\n+                [f'--include \"{file_name}\"' for file_name in file_names]\n+            )\n+            sync_command = (\n+                \"rclone copy \"\n+                f'{includes} \"{base_dir_path}\" '\n+                f\"{self.bucket_rclone_profile}:{self.name}\"\n+            )\n             return sync_command\n \n         # Generate message for upload\n         if len(source_path_list) > 1:\n-            source_message = f'{len(source_path_list)} paths'\n+            source_message = f\"{len(source_path_list)} paths\"\n         else:\n             source_message = source_path_list[0]\n \n         with rich_utils.safe_status(\n-                f'[bold cyan]Syncing '\n-                f'[green]{source_message}[/] to '\n-                f'[green]cos://{self.region}/{self.name}/[/]'):\n+            f\"[bold cyan]Syncing \"\n+            f\"[green]{source_message}[/] to \"\n+            f\"[green]cos://{self.region}/{self.name}/[/]\"\n+        ):\n             data_utils.parallel_upload(\n                 source_path_list,\n                 get_file_sync_command,\n@@ -2538,7 +2714,8 @@ class IBMCosStore(AbstractStore):\n                 self.name,\n                 self._ACCESS_DENIED_MESSAGE,\n                 create_dirs=create_dirs,\n-                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)\n+                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS,\n+            )\n \n     def _get_bucket(self) -> Tuple[StorageHandle, bool]:\n         \"\"\"returns IBM COS bucket object if exists, otherwise creates it.\n@@ -2560,35 +2737,41 @@ class IBMCosStore(AbstractStore):\n             bucket_region = data_utils.get_ibm_cos_bucket_region(self.name)\n         except exceptions.StorageBucketGetError as e:\n             with ux_utils.print_exception_no_traceback():\n-                command = f'rclone lsd {bucket_profile_name}: '\n+                command = f\"rclone lsd {bucket_profile_name}: \"\n                 raise exceptions.StorageBucketGetError(\n-                    _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +\n-                    f' To debug, consider running `{command}`.') from e\n+                    _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name)\n+                    + f\" To debug, consider running `{command}`.\"\n+                ) from e\n \n         try:\n-            uri_region = data_utils.split_cos_path(\n-                self.source)[2]  # type: ignore\n+            uri_region = data_utils.split_cos_path(self.source)[2]  # type: ignore\n         except ValueError:\n             # source isn't a cos uri\n-            uri_region = ''\n+            uri_region = \"\"\n \n         # bucket's region doesn't match specified region in URI\n-        if bucket_region and uri_region and uri_region != bucket_region\\\n-              and self.sync_on_reconstruction:\n+        if (\n+            bucket_region\n+            and uri_region\n+            and uri_region != bucket_region\n+            and self.sync_on_reconstruction\n+        ):\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketGetError(\n-                    f'Bucket {self.name} exists in '\n-                    f'region {bucket_region}, '\n-                    f'but URI specified region {uri_region}.')\n+                    f\"Bucket {self.name} exists in \"\n+                    f\"region {bucket_region}, \"\n+                    f\"but URI specified region {uri_region}.\"\n+                )\n \n         if not bucket_region and uri_region:\n             # bucket doesn't exist but source is a bucket URI\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketGetError(\n-                    'Attempted to use a non-existent bucket as a source: '\n-                    f'{self.name} by providing URI. Consider using '\n-                    '`rclone lsd <remote>` on relevant remotes returned '\n-                    'via `rclone listremotes` to debug.')\n+                    \"Attempted to use a non-existent bucket as a source: \"\n+                    f\"{self.name} by providing URI. Consider using \"\n+                    \"`rclone lsd <remote>` on relevant remotes returned \"\n+                    \"via `rclone listremotes` to debug.\"\n+                )\n \n         Rclone.store_rclone_config(\n             self.name,\n@@ -2603,8 +2786,8 @@ class IBMCosStore(AbstractStore):\n             # delete or to re-mount Storages with sky start but the storage\n             # is already removed externally.\n             raise exceptions.StorageExternalDeletionError(\n-                'Attempted to fetch a non-existent bucket: '\n-                f'{self.name}')\n+                f\"Attempted to fetch a non-existent bucket: {self.name}\"\n+            )\n         else:\n             # bucket exists\n             return self.s3_resource.Bucket(self.name), False\n@@ -2635,22 +2818,20 @@ class IBMCosStore(AbstractStore):\n         )\n         # pylint: disable=line-too-long\n         # creates a fusermount soft link on older (<22) Ubuntu systems for rclone's mount utility.\n-        create_fuser3_soft_link = '[ ! -f /bin/fusermount3 ] && sudo ln -s /bin/fusermount /bin/fusermount3 || true'\n+        create_fuser3_soft_link = \"[ ! -f /bin/fusermount3 ] && sudo ln -s /bin/fusermount /bin/fusermount3 || true\"\n         # stores bucket profile in rclone config file at the cluster's nodes.\n-        configure_rclone_profile = (\n-            f'{create_fuser3_soft_link}; mkdir -p ~/.config/rclone/ && echo \"{rclone_config_data}\">> {Rclone.RCLONE_CONFIG_PATH}'\n-        )\n+        configure_rclone_profile = f'{create_fuser3_soft_link}; mkdir -p ~/.config/rclone/ && echo \"{rclone_config_data}\">> {Rclone.RCLONE_CONFIG_PATH}'\n         # install rclone if not installed.\n-        install_cmd = 'rclone version >/dev/null 2>&1 || (curl https://rclone.org/install.sh | sudo bash)'\n+        install_cmd = \"rclone version >/dev/null 2>&1 || (curl https://rclone.org/install.sh | sudo bash)\"\n         # --daemon will keep the mounting process running in the background.\n-        mount_cmd = f'{configure_rclone_profile} && rclone mount {self.bucket_rclone_profile}:{self.bucket.name} {mount_path} --daemon'\n-        return mounting_utils.get_mounting_command(StorageMode.MOUNT,\n-                                                   mount_path, mount_cmd,\n-                                                   install_cmd)\n-\n-    def csync_command(self,\n-                      csync_path: str,\n-                      interval_seconds: Optional[int] = None) -> str:\n+        mount_cmd = f\"{configure_rclone_profile} && rclone mount {self.bucket_rclone_profile}:{self.bucket.name} {mount_path} --daemon\"\n+        return mounting_utils.get_mounting_command(\n+            StorageMode.MOUNT, mount_path, mount_cmd, install_cmd\n+        )\n+\n+    def csync_command(\n+        self, csync_path: str, interval_seconds: Optional[int] = None\n+    ) -> str:\n         \"\"\"Returns command to mount CSYNC with Storage bucket on CSYNC_PATH.\n \n         Args:\n@@ -2659,9 +2840,7 @@ class IBMCosStore(AbstractStore):\n         \"\"\"\n         raise NotImplementedError\n \n-    def _create_cos_bucket(self,\n-                           bucket_name: str,\n-                           region='us-east') -> StorageHandle:\n+    def _create_cos_bucket(self, bucket_name: str, region=\"us-east\") -> StorageHandle:\n         \"\"\"Creates IBM COS bucket with specific name in specific region\n \n         Args:\n@@ -2673,20 +2852,21 @@ class IBMCosStore(AbstractStore):\n         try:\n             self.client.create_bucket(\n                 Bucket=bucket_name,\n-                CreateBucketConfiguration={\n-                    'LocationConstraint': f'{region}-smart'\n-                })\n-            logger.info(f'Created IBM COS bucket {bucket_name} in {region} '\n-                        f'with storage class smart tier')\n+                CreateBucketConfiguration={\"LocationConstraint\": f\"{region}-smart\"},\n+            )\n+            logger.info(\n+                f\"Created IBM COS bucket {bucket_name} in {region} \"\n+                f\"with storage class smart tier\"\n+            )\n             self.bucket = self.s3_resource.Bucket(bucket_name)\n \n         except ibm.ibm_botocore.exceptions.ClientError as e:  # type: ignore[union-attr]  # pylint: disable=line-too-long\n             with ux_utils.print_exception_no_traceback():\n                 raise exceptions.StorageBucketCreateError(\n-                    f'Failed to create bucket: '\n-                    f'{bucket_name}') from e\n+                    f\"Failed to create bucket: {bucket_name}\"\n+                ) from e\n \n-        s3_bucket_exists_waiter = self.client.get_waiter('bucket_exists')\n+        s3_bucket_exists_waiter = self.client.get_waiter(\"bucket_exists\")\n         s3_bucket_exists_waiter.wait(Bucket=bucket_name)\n \n         return self.bucket\n@@ -2695,14 +2875,14 @@ class IBMCosStore(AbstractStore):\n         bucket = self.s3_resource.Bucket(self.name)\n         try:\n             bucket_versioning = self.s3_resource.BucketVersioning(self.name)\n-            if bucket_versioning.status == 'Enabled':\n+            if bucket_versioning.status == \"Enabled\":\n                 res = list(bucket.object_versions.delete())\n             else:\n                 res = list(bucket.objects.delete())\n-            logger.debug(f'Deleted bucket\\'s content:\\n{res}')\n+            logger.debug(f\"Deleted bucket's content:\\n{res}\")\n             bucket.delete()\n             bucket.wait_until_not_exists()\n         except ibm.ibm_botocore.exceptions.ClientError as e:\n-            if e.__class__.__name__ == 'NoSuchBucket':\n-                logger.debug('bucket already removed')\n+            if e.__class__.__name__ == \"NoSuchBucket\":\n+                logger.debug(\"bucket already removed\")\n         Rclone.delete_rclone_bucket_profile(self.name, Rclone.RcloneClouds.IBM)\n"
    },
    {
        "id": 13,
        "sha_fail": "b639adb71066410b3b12d97a674ee7fcb51e9980",
        "diff": "diff --git a/sky/resources.py b/sky/resources.py\nindex 2f7aebc..2669eee 100644\n--- a/sky/resources.py\n+++ b/sky/resources.py\n@@ -1,4 +1,5 @@\n \"\"\"Resources: compute requirements of Tasks.\"\"\"\n+\n import functools\n import textwrap\n from typing import Dict, List, Optional, Set, Tuple, Union\n@@ -42,6 +43,7 @@ class Resources:\n     * for provisioning on a cloud\n \n     \"\"\"\n+\n     # If any fields changed, increment the version. For backward compatibility,\n     # modify the __setstate__ method to handle the old version.\n     _VERSION = 13\n@@ -60,7 +62,7 @@ class Resources:\n         zone: Optional[str] = None,\n         image_id: Union[Dict[str, str], str, None] = None,\n         disk_size: Optional[int] = None,\n-        disk_tier: Optional[Literal['high', 'medium', 'low']] = None,\n+        disk_tier: Optional[Literal[\"high\", \"medium\", \"low\"]] = None,\n         ports: Optional[Union[int, str, List[str], Tuple[str]]] = None,\n         # Internal use only.\n         _docker_login_config: Optional[docker_utils.DockerLoginConfig] = None,\n@@ -144,14 +146,15 @@ class Resources:\n         self._use_spot = use_spot if use_spot is not None else False\n         self._spot_recovery = None\n         if spot_recovery is not None:\n-            if spot_recovery.strip().lower() != 'none':\n+            if spot_recovery.strip().lower() != \"none\":\n                 self._spot_recovery = spot_recovery.upper()\n \n         if disk_size is not None:\n             if round(disk_size) != disk_size:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'OS disk size must be an integer. Got: {disk_size}.')\n+                        f\"OS disk size must be an integer. Got: {disk_size}.\"\n+                    )\n             self._disk_size = int(disk_size)\n         else:\n             self._disk_size = _DEFAULT_DISK_SIZE_GB\n@@ -165,9 +168,7 @@ class Resources:\n             if None in image_id:\n                 self._image_id = {self._region: image_id[None].strip()}\n             else:\n-                self._image_id = {\n-                    k.strip(): v.strip() for k, v in image_id.items()\n-                }\n+                self._image_id = {k.strip(): v.strip() for k, v in image_id.items()}\n         self._is_image_managed = _is_image_managed\n \n         self._disk_tier = disk_tier\n@@ -177,8 +178,7 @@ class Resources:\n                 ports = list(ports)\n             if not isinstance(ports, list):\n                 ports = [ports]\n-            ports = resources_utils.simplify_ports(\n-                [str(port) for port in ports])\n+            ports = resources_utils.simplify_ports([str(port) for port in ports])\n             if not ports:\n                 # Set to None if empty. This is mainly for resources from\n                 # cli, which will comes in as an empty tuple.\n@@ -229,83 +229,84 @@ class Resources:\n             >>> sky.Resources(disk_size=100)\n             <Cloud>(disk_size=100)\n         \"\"\"\n-        accelerators = ''\n-        accelerator_args = ''\n+        accelerators = \"\"\n+        accelerator_args = \"\"\n         if self.accelerators is not None:\n-            accelerators = f', {self.accelerators}'\n+            accelerators = f\", {self.accelerators}\"\n             if self.accelerator_args is not None:\n-                accelerator_args = f', accelerator_args={self.accelerator_args}'\n+                accelerator_args = f\", accelerator_args={self.accelerator_args}\"\n \n-        cpus = ''\n+        cpus = \"\"\n         if self.cpus is not None:\n-            cpus = f', cpus={self.cpus}'\n+            cpus = f\", cpus={self.cpus}\"\n \n-        memory = ''\n+        memory = \"\"\n         if self.memory is not None:\n-            memory = f', mem={self.memory}'\n+            memory = f\", mem={self.memory}\"\n \n         if isinstance(self.cloud, clouds.Local):\n-            return f'{self.cloud}({self.accelerators})'\n+            return f\"{self.cloud}({self.accelerators})\"\n \n-        use_spot = ''\n+        use_spot = \"\"\n         if self.use_spot:\n-            use_spot = '[Spot]'\n+            use_spot = \"[Spot]\"\n \n-        image_id = ''\n+        image_id = \"\"\n         if self.image_id is not None:\n             if None in self.image_id:\n-                image_id = f', image_id={self.image_id[None]}'\n+                image_id = f\", image_id={self.image_id[None]}\"\n             else:\n-                image_id = f', image_id={self.image_id}'\n+                image_id = f\", image_id={self.image_id}\"\n \n-        disk_tier = ''\n+        disk_tier = \"\"\n         if self.disk_tier is not None:\n-            disk_tier = f', disk_tier={self.disk_tier}'\n+            disk_tier = f\", disk_tier={self.disk_tier}\"\n \n-        disk_size = ''\n+        disk_size = \"\"\n         if self.disk_size != _DEFAULT_DISK_SIZE_GB:\n-            disk_size = f', disk_size={self.disk_size}'\n+            disk_size = f\", disk_size={self.disk_size}\"\n \n-        ports = ''\n+        ports = \"\"\n         if self.ports is not None:\n-            ports = f', ports={self.ports}'\n+            ports = f\", ports={self.ports}\"\n \n         if self._instance_type is not None:\n-            instance_type = f'{self._instance_type}'\n+            instance_type = f\"{self._instance_type}\"\n         else:\n-            instance_type = ''\n+            instance_type = \"\"\n \n         # Do not show region/zone here as `sky status -a` would show them as\n         # separate columns. Also, Resources repr will be printed during\n         # failover, and the region may be dynamically determined.\n         hardware_str = (\n-            f'{instance_type}{use_spot}'\n-            f'{cpus}{memory}{accelerators}{accelerator_args}{image_id}'\n-            f'{disk_tier}{disk_size}{ports}')\n+            f\"{instance_type}{use_spot}\"\n+            f\"{cpus}{memory}{accelerators}{accelerator_args}{image_id}\"\n+            f\"{disk_tier}{disk_size}{ports}\"\n+        )\n         # It may have leading ',' (for example, instance_type not set) or empty\n         # spaces.  Remove them.\n-        while hardware_str and hardware_str[0] in (',', ' '):\n+        while hardware_str and hardware_str[0] in (\",\", \" \"):\n             hardware_str = hardware_str[1:]\n \n-        cloud_str = '<Cloud>'\n+        cloud_str = \"<Cloud>\"\n         if self.cloud is not None:\n-            cloud_str = f'{self.cloud}'\n+            cloud_str = f\"{self.cloud}\"\n \n-        return f'{cloud_str}({hardware_str})'\n+        return f\"{cloud_str}({hardware_str})\"\n \n     @property\n     def repr_with_region_zone(self) -> str:\n-        region_str = ''\n+        region_str = \"\"\n         if self.region is not None:\n-            region_str = f', region={self.region}'\n-        zone_str = ''\n+            region_str = f\", region={self.region}\"\n+        zone_str = \"\"\n         if self.zone is not None:\n-            zone_str = f', zone={self.zone}'\n+            zone_str = f\", zone={self.zone}\"\n         repr_str = str(self)\n-        if repr_str.endswith(')'):\n-            repr_str = repr_str[:-1] + f'{region_str}{zone_str})'\n+        if repr_str.endswith(\")\"):\n+            repr_str = repr_str[:-1] + f\"{region_str}{zone_str})\"\n         else:\n-            repr_str += f'{region_str}{zone_str}'\n+            repr_str += f\"{region_str}{zone_str}\"\n         return repr_str\n \n     @property\n@@ -363,8 +364,7 @@ class Resources:\n         if self._accelerators is not None:\n             return self._accelerators\n         if self.cloud is not None and self._instance_type is not None:\n-            return self.cloud.get_accelerators_from_instance_type(\n-                self._instance_type)\n+            return self.cloud.get_accelerators_from_instance_type(self._instance_type)\n         return None\n \n     @property\n@@ -413,7 +413,7 @@ class Resources:\n \n         self._cpus = str(cpus)\n         if isinstance(cpus, str):\n-            if cpus.endswith('+'):\n+            if cpus.endswith(\"+\"):\n                 num_cpus_str = cpus[:-1]\n             else:\n                 num_cpus_str = cpus\n@@ -424,14 +424,16 @@ class Resources:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n                         f'The \"cpus\" field should be either a number or '\n-                        f'a string \"<number>+\". Found: {cpus!r}') from None\n+                        f'a string \"<number>+\". Found: {cpus!r}'\n+                    ) from None\n         else:\n             num_cpus = float(cpus)\n \n         if num_cpus <= 0:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'The \"cpus\" field should be positive. Found: {cpus!r}')\n+                    f'The \"cpus\" field should be positive. Found: {cpus!r}'\n+                )\n \n     def _set_memory(\n         self,\n@@ -443,7 +445,7 @@ class Resources:\n \n         self._memory = str(memory)\n         if isinstance(memory, str):\n-            if memory.endswith('+'):\n+            if memory.endswith(\"+\"):\n                 num_memory_gb = memory[:-1]\n             else:\n                 num_memory_gb = memory\n@@ -454,14 +456,16 @@ class Resources:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n                         f'The \"memory\" field should be either a number or '\n-                        f'a string \"<number>+\". Found: {memory!r}') from None\n+                        f'a string \"<number>+\". Found: {memory!r}'\n+                    ) from None\n         else:\n             memory_gb = float(memory)\n \n         if memory_gb <= 0:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'The \"cpus\" field should be positive. Found: {memory!r}')\n+                    f'The \"cpus\" field should be positive. Found: {memory!r}'\n+                )\n \n     def _set_accelerators(\n         self,\n@@ -476,13 +480,15 @@ class Resources:\n         \"\"\"\n         if accelerators is not None:\n             if isinstance(accelerators, str):  # Convert to Dict[str, int].\n-                if ':' not in accelerators:\n+                if \":\" not in accelerators:\n                     accelerators = {accelerators: 1}\n                 else:\n-                    splits = accelerators.split(':')\n-                    parse_error = ('The \"accelerators\" field as a str '\n-                                   'should be <name> or <name>:<cnt>. '\n-                                   f'Found: {accelerators!r}')\n+                    splits = accelerators.split(\":\")\n+                    parse_error = (\n+                        'The \"accelerators\" field as a str '\n+                        \"should be <name> or <name>:<cnt>. \"\n+                        f\"Found: {accelerators!r}\"\n+                    )\n                     if len(splits) != 2:\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(parse_error)\n@@ -502,35 +508,36 @@ class Resources:\n \n             # Canonicalize the accelerator names.\n             accelerators = {\n-                accelerator_registry.canonicalize_accelerator_name(acc):\n-                acc_count for acc, acc_count in accelerators.items()\n+                accelerator_registry.canonicalize_accelerator_name(acc): acc_count\n+                for acc, acc_count in accelerators.items()\n             }\n \n             acc, _ = list(accelerators.items())[0]\n-            if 'tpu' in acc.lower():\n+            if \"tpu\" in acc.lower():\n                 if self.cloud is None:\n                     self._cloud = clouds.GCP()\n-                assert self.cloud.is_same_cloud(\n-                    clouds.GCP()), 'Cloud must be GCP.'\n+                assert self.cloud.is_same_cloud(clouds.GCP()), \"Cloud must be GCP.\"\n                 if accelerator_args is None:\n                     accelerator_args = {}\n-                use_tpu_vm = accelerator_args.get('tpu_vm', False)\n+                use_tpu_vm = accelerator_args.get(\"tpu_vm\", False)\n                 if use_tpu_vm:\n                     tpu_utils.check_gcp_cli_include_tpu_vm()\n                 if self.instance_type is not None and use_tpu_vm:\n-                    if self.instance_type != 'TPU-VM':\n+                    if self.instance_type != \"TPU-VM\":\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                'Cannot specify instance type'\n-                                f' (got \"{self.instance_type}\") for TPU VM.')\n-                if 'runtime_version' not in accelerator_args:\n+                                \"Cannot specify instance type\"\n+                                f' (got \"{self.instance_type}\") for TPU VM.'\n+                            )\n+                if \"runtime_version\" not in accelerator_args:\n                     if use_tpu_vm:\n-                        accelerator_args['runtime_version'] = 'tpu-vm-base'\n+                        accelerator_args[\"runtime_version\"] = \"tpu-vm-base\"\n                     else:\n-                        accelerator_args['runtime_version'] = '2.12.0'\n+                        accelerator_args[\"runtime_version\"] = \"2.12.0\"\n                     logger.info(\n-                        'Missing runtime_version in accelerator_args, using'\n-                        f' default ({accelerator_args[\"runtime_version\"]})')\n+                        \"Missing runtime_version in accelerator_args, using\"\n+                        f\" default ({accelerator_args['runtime_version']})\"\n+                    )\n \n         self._accelerators = accelerators\n         self._accelerator_args = accelerator_args\n@@ -543,8 +550,9 @@ class Resources:\n         assert self.is_launchable(), self\n         return self.cloud.need_cleanup_after_preemption(self)\n \n-    def _validate_and_set_region_zone(self, region: Optional[str],\n-                                      zone: Optional[str]) -> None:\n+    def _validate_and_set_region_zone(\n+        self, region: Optional[str], zone: Optional[str]\n+    ) -> None:\n         if region is None and zone is None:\n             return\n \n@@ -564,39 +572,41 @@ class Resources:\n \n             if len(valid_clouds) == 0:\n                 if len(enabled_clouds) == 1:\n-                    cloud_str = f'for cloud {enabled_clouds[0]}'\n+                    cloud_str = f\"for cloud {enabled_clouds[0]}\"\n                 else:\n-                    cloud_str = f'for any cloud among {enabled_clouds}'\n+                    cloud_str = f\"for any cloud among {enabled_clouds}\"\n                 with ux_utils.print_exception_no_traceback():\n                     if len(cloud_to_errors) == 1:\n                         # UX: if 1 cloud, don't print a table.\n                         hint = list(cloud_to_errors.items())[0][-1]\n                     else:\n-                        table = log_utils.create_table(['Cloud', 'Hint'])\n-                        table.add_row(['-----', '----'])\n+                        table = log_utils.create_table([\"Cloud\", \"Hint\"])\n+                        table.add_row([\"-----\", \"----\"])\n                         for cloud, error in cloud_to_errors.items():\n-                            reason_str = '\\n'.join(textwrap.wrap(\n-                                str(error), 80))\n+                            reason_str = \"\\n\".join(textwrap.wrap(str(error), 80))\n                             table.add_row([str(cloud), reason_str])\n                         hint = table.get_string()\n                     raise ValueError(\n-                        f'Invalid (region {region!r}, zone {zone!r}) '\n-                        f'{cloud_str}. Details:\\n{hint}')\n+                        f\"Invalid (region {region!r}, zone {zone!r}) \"\n+                        f\"{cloud_str}. Details:\\n{hint}\"\n+                    )\n             elif len(valid_clouds) > 1:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Cannot infer cloud from (region {region!r}, zone '\n-                        f'{zone!r}). Multiple enabled clouds have region/zone '\n-                        f'of the same names: {valid_clouds}. '\n-                        f'To fix: explicitly specify `cloud`.')\n-            logger.debug(f'Cloud is not specified, using {valid_clouds[0]} '\n-                         f'inferred from region {region!r} and zone {zone!r}')\n+                        f\"Cannot infer cloud from (region {region!r}, zone \"\n+                        f\"{zone!r}). Multiple enabled clouds have region/zone \"\n+                        f\"of the same names: {valid_clouds}. \"\n+                        f\"To fix: explicitly specify `cloud`.\"\n+                    )\n+            logger.debug(\n+                f\"Cloud is not specified, using {valid_clouds[0]} \"\n+                f\"inferred from region {region!r} and zone {zone!r}\"\n+            )\n             self._cloud = valid_clouds[0]\n \n         # Validate if region and zone exist in the catalog, and set the region\n         # if zone is specified.\n-        self._region, self._zone = self._cloud.validate_region_zone(\n-            region, zone)\n+        self._region, self._zone = self._cloud.validate_region_zone(region, zone)\n \n     def get_valid_regions_for_launchable(self) -> List[clouds.Region]:\n         \"\"\"Returns a set of `Region`s that can provision this Resources.\n@@ -609,18 +619,24 @@ class Resources:\n         \"\"\"\n         assert self.is_launchable(), self\n \n-        regions = self._cloud.regions_with_offering(self._instance_type,\n-                                                    self.accelerators,\n-                                                    self._use_spot,\n-                                                    self._region, self._zone)\n+        regions = self._cloud.regions_with_offering(\n+            self._instance_type,\n+            self.accelerators,\n+            self._use_spot,\n+            self._region,\n+            self._zone,\n+        )\n         if self._image_id is not None and None not in self._image_id:\n             regions = [r for r in regions if r.name in self._image_id]\n \n         # Filter the regions by the skypilot_config\n         ssh_proxy_command_config = skypilot_config.get_nested(\n-            (str(self._cloud).lower(), 'ssh_proxy_command'), None)\n-        if (isinstance(ssh_proxy_command_config, str) or\n-                ssh_proxy_command_config is None):\n+            (str(self._cloud).lower(), \"ssh_proxy_command\"), None\n+        )\n+        if (\n+            isinstance(ssh_proxy_command_config, str)\n+            or ssh_proxy_command_config is None\n+        ):\n             # All regions are valid as the regions are not specified for the\n             # ssh_proxy_command config.\n             return regions\n@@ -642,9 +658,10 @@ class Resources:\n             yellow = colorama.Fore.YELLOW\n             reset = colorama.Style.RESET_ALL\n             logger.warning(\n-                f'{yellow}Request {self} cannot be satisfied by any feasible '\n-                'region. To fix, check that ssh_proxy_command\\'s region keys '\n-                f'include the regions to use.{reset}')\n+                f\"{yellow}Request {self} cannot be satisfied by any feasible \"\n+                \"region. To fix, check that ssh_proxy_command's region keys \"\n+                f\"include the regions to use.{reset}\"\n+            )\n \n         return filtered_regions\n \n@@ -658,8 +675,9 @@ class Resources:\n             if not valid:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Invalid instance type {self._instance_type!r} '\n-                        f'for cloud {self.cloud}.')\n+                        f\"Invalid instance type {self._instance_type!r} \"\n+                        f\"for cloud {self.cloud}.\"\n+                    )\n         else:\n             # If cloud not specified\n             valid_clouds = []\n@@ -669,22 +687,23 @@ class Resources:\n                     valid_clouds.append(cloud)\n             if len(valid_clouds) == 0:\n                 if len(enabled_clouds) == 1:\n-                    cloud_str = f'for cloud {enabled_clouds[0]}'\n+                    cloud_str = f\"for cloud {enabled_clouds[0]}\"\n                 else:\n-                    cloud_str = f'for any cloud among {enabled_clouds}'\n+                    cloud_str = f\"for any cloud among {enabled_clouds}\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Invalid instance type {self._instance_type!r} '\n-                        f'{cloud_str}.')\n+                        f\"Invalid instance type {self._instance_type!r} {cloud_str}.\"\n+                    )\n             if len(valid_clouds) > 1:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Ambiguous instance type {self._instance_type!r}. '\n-                        f'Please specify cloud explicitly among {valid_clouds}.'\n+                        f\"Ambiguous instance type {self._instance_type!r}. \"\n+                        f\"Please specify cloud explicitly among {valid_clouds}.\"\n                     )\n             logger.debug(\n-                f'Cloud is not specified, using {valid_clouds[0]} '\n-                f'inferred from the instance_type {self.instance_type!r}.')\n+                f\"Cloud is not specified, using {valid_clouds[0]} \"\n+                f\"inferred from the instance_type {self.instance_type!r}.\"\n+            )\n             self._cloud = valid_clouds[0]\n \n     def _try_validate_cpus_mem(self) -> None:\n@@ -696,36 +715,39 @@ class Resources:\n             # The _try_validate_instance_type() method infers and sets\n             # self.cloud if self.instance_type is not None.\n             assert self.cloud is not None\n-            cpus, mem = self.cloud.get_vcpus_mem_from_instance_type(\n-                self.instance_type)\n+            cpus, mem = self.cloud.get_vcpus_mem_from_instance_type(self.instance_type)\n             if self.cpus is not None:\n-                if self.cpus.endswith('+'):\n+                if self.cpus.endswith(\"+\"):\n                     if cpus < float(self.cpus[:-1]):\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                f'{self.instance_type} does not have enough '\n-                                f'vCPUs. {self.instance_type} has {cpus} '\n-                                f'vCPUs, but {self.cpus} is requested.')\n+                                f\"{self.instance_type} does not have enough \"\n+                                f\"vCPUs. {self.instance_type} has {cpus} \"\n+                                f\"vCPUs, but {self.cpus} is requested.\"\n+                            )\n                 elif cpus != float(self.cpus):\n                     with ux_utils.print_exception_no_traceback():\n                         raise ValueError(\n-                            f'{self.instance_type} does not have the requested '\n-                            f'number of vCPUs. {self.instance_type} has {cpus} '\n-                            f'vCPUs, but {self.cpus} is requested.')\n+                            f\"{self.instance_type} does not have the requested \"\n+                            f\"number of vCPUs. {self.instance_type} has {cpus} \"\n+                            f\"vCPUs, but {self.cpus} is requested.\"\n+                        )\n             if self.memory is not None:\n-                if self.memory.endswith('+'):\n+                if self.memory.endswith(\"+\"):\n                     if mem < float(self.memory[:-1]):\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                f'{self.instance_type} does not have enough '\n-                                f'memory. {self.instance_type} has {mem} GB '\n-                                f'memory, but {self.memory} is requested.')\n+                                f\"{self.instance_type} does not have enough \"\n+                                f\"memory. {self.instance_type} has {mem} GB \"\n+                                f\"memory, but {self.memory} is requested.\"\n+                            )\n                 elif mem != float(self.memory):\n                     with ux_utils.print_exception_no_traceback():\n                         raise ValueError(\n-                            f'{self.instance_type} does not have the requested '\n-                            f'memory. {self.instance_type} has {mem} GB '\n-                            f'memory, but {self.memory} is requested.')\n+                            f\"{self.instance_type} does not have the requested \"\n+                            f\"memory. {self.instance_type} has {mem} GB \"\n+                            f\"memory, but {self.memory} is requested.\"\n+                        )\n \n     def _try_validate_spot(self) -> None:\n         if self._spot_recovery is None:\n@@ -733,41 +755,46 @@ class Resources:\n         if not self._use_spot:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    'Cannot specify spot_recovery without use_spot set to True.'\n+                    \"Cannot specify spot_recovery without use_spot set to True.\"\n                 )\n         if self._spot_recovery not in spot.SPOT_STRATEGIES:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'Spot recovery strategy {self._spot_recovery} '\n-                    'is not supported. The strategy should be among '\n-                    f'{list(spot.SPOT_STRATEGIES.keys())}')\n+                    f\"Spot recovery strategy {self._spot_recovery} \"\n+                    \"is not supported. The strategy should be among \"\n+                    f\"{list(spot.SPOT_STRATEGIES.keys())}\"\n+                )\n \n     def _try_validate_local(self) -> None:\n         if isinstance(self._cloud, clouds.Local):\n             if self._use_spot:\n                 with ux_utils.print_exception_no_traceback():\n-                    raise ValueError('Local/On-prem mode does not support spot '\n-                                     'instances.')\n+                    raise ValueError(\n+                        \"Local/On-prem mode does not support spot instances.\"\n+                    )\n             local_instance = clouds.Local.get_default_instance_type()\n-            if (self._instance_type is not None and\n-                    self._instance_type != local_instance):\n+            if (\n+                self._instance_type is not None\n+                and self._instance_type != local_instance\n+            ):\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        'Local/On-prem mode does not support instance type:'\n-                        f' {self._instance_type}.')\n+                        \"Local/On-prem mode does not support instance type:\"\n+                        f\" {self._instance_type}.\"\n+                    )\n             if self._image_id is not None:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        'Local/On-prem mode does not support custom '\n-                        'images.')\n+                        \"Local/On-prem mode does not support custom images.\"\n+                    )\n \n     def extract_docker_image(self) -> Optional[str]:\n         if self.image_id is None:\n             return None\n         if len(self.image_id) == 1 and self.region in self.image_id:\n             image_id = self.image_id[self.region]\n-            if image_id.startswith('docker:'):\n-                return image_id[len('docker:'):]\n+            if image_id.startswith(\"docker:\"):\n+                return image_id[len(\"docker:\") :]\n         return None\n \n     def _try_validate_image_id(self) -> None:\n@@ -778,56 +805,66 @@ class Resources:\n             # TODO(tian): validate the docker image exists / of reasonable size\n             if self.accelerators is not None:\n                 for acc in self.accelerators.keys():\n-                    if acc.lower().startswith('tpu'):\n+                    if acc.lower().startswith(\"tpu\"):\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                'Docker image is not supported for TPU VM.')\n+                                \"Docker image is not supported for TPU VM.\"\n+                            )\n             if self.cloud is not None:\n                 self.cloud.check_features_are_supported(\n-                    {clouds.CloudImplementationFeatures.DOCKER_IMAGE})\n+                    {clouds.CloudImplementationFeatures.DOCKER_IMAGE}\n+                )\n             return\n \n         if self.cloud is None:\n             with ux_utils.print_exception_no_traceback():\n-                raise ValueError(\n-                    'Cloud must be specified when image_id is provided.')\n+                raise ValueError(\"Cloud must be specified when image_id is provided.\")\n \n         # Apr, 2023 by Hysun(hysun.he@oracle.com): Added support for OCI\n-        if not self._cloud.is_same_cloud(\n-                clouds.AWS()) and not self._cloud.is_same_cloud(\n-                    clouds.GCP()) and not self._cloud.is_same_cloud(\n-                        clouds.IBM()) and not self._cloud.is_same_cloud(\n-                            clouds.OCI()):\n+        if (\n+            not self._cloud.is_same_cloud(clouds.AWS())\n+            and not self._cloud.is_same_cloud(clouds.GCP())\n+            and not self._cloud.is_same_cloud(clouds.IBM())\n+            and not self._cloud.is_same_cloud(clouds.OCI())\n+        ):\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    'image_id is only supported for AWS/GCP/IBM/OCI, please '\n-                    'explicitly specify the cloud.')\n+                    \"image_id is only supported for AWS/GCP/IBM/OCI, please \"\n+                    \"explicitly specify the cloud.\"\n+                )\n \n         if self._region is not None:\n             if self._region not in self._image_id:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'image_id {self._image_id} should contain the image '\n-                        f'for the specified region {self._region}.')\n+                        f\"image_id {self._image_id} should contain the image \"\n+                        f\"for the specified region {self._region}.\"\n+                    )\n             # Narrow down the image_id to the specified region.\n             self._image_id = {self._region: self._image_id[self._region]}\n \n         # Check the image_id's are valid.\n         for region, image_id in self._image_id.items():\n-            if (image_id.startswith('skypilot:') and\n-                    not self._cloud.is_image_tag_valid(image_id, region)):\n-                region_str = f' ({region})' if region else ''\n+            if image_id.startswith(\"skypilot:\") and not self._cloud.is_image_tag_valid(\n+                image_id, region\n+            ):\n+                region_str = f\" ({region})\" if region else \"\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Image tag {image_id!r} is not valid, please make sure'\n-                        f' the tag exists in {self._cloud}{region_str}.')\n+                        f\"Image tag {image_id!r} is not valid, please make sure\"\n+                        f\" the tag exists in {self._cloud}{region_str}.\"\n+                    )\n \n-            if (self._cloud.is_same_cloud(clouds.AWS()) and\n-                    not image_id.startswith('skypilot:') and region is None):\n+            if (\n+                self._cloud.is_same_cloud(clouds.AWS())\n+                and not image_id.startswith(\"skypilot:\")\n+                and region is None\n+            ):\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        'image_id is only supported for AWS in a specific '\n-                        'region, please explicitly specify the region.')\n+                        \"image_id is only supported for AWS in a specific \"\n+                        \"region, please explicitly specify the region.\"\n+                    )\n \n         # Validate the image exists and the size is smaller than the disk size.\n         for region, image_id in self._image_id.items():\n@@ -836,40 +873,42 @@ class Resources:\n             image_size = self.cloud.get_image_size(image_id, region)\n             if image_size >= self.disk_size:\n                 with ux_utils.print_exception_no_traceback():\n-                    size_compare = 'larger than' if image_size > self.disk_size \\\n-                        else 'equal to'\n+                    size_compare = (\n+                        \"larger than\" if image_size > self.disk_size else \"equal to\"\n+                    )\n                     raise ValueError(\n-                        f'Image {image_id!r} is {image_size}GB, which is '\n-                        f'{size_compare} the specified disk_size: '\n-                        f'{self.disk_size} GB. Please specify a larger '\n-                        'disk_size to use this image.')\n+                        f\"Image {image_id!r} is {image_size}GB, which is \"\n+                        f\"{size_compare} the specified disk_size: \"\n+                        f\"{self.disk_size} GB. Please specify a larger \"\n+                        \"disk_size to use this image.\"\n+                    )\n \n     def _try_validate_disk_tier(self) -> None:\n         if self.disk_tier is None:\n             return\n-        if self.disk_tier not in ['high', 'medium', 'low']:\n+        if self.disk_tier not in [\"high\", \"medium\", \"low\"]:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'Invalid disk_tier {self.disk_tier}. '\n-                    'Please use one of \"high\", \"medium\", or \"low\".')\n+                    f\"Invalid disk_tier {self.disk_tier}. \"\n+                    'Please use one of \"high\", \"medium\", or \"low\".'\n+                )\n         if self.instance_type is None:\n             return\n         if self.cloud is not None:\n-            self.cloud.check_disk_tier_enabled(self.instance_type,\n-                                               self.disk_tier)\n+            self.cloud.check_disk_tier_enabled(self.instance_type, self.disk_tier)\n \n     def _try_validate_ports(self) -> None:\n         if self.ports is None:\n             return\n-        if skypilot_config.get_nested(('aws', 'security_group_name'),\n-                                      None) is not None:\n+        if skypilot_config.get_nested((\"aws\", \"security_group_name\"), None) is not None:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    'Cannot specify ports when AWS security group name is '\n-                    'specified.')\n+                    \"Cannot specify ports when AWS security group name is specified.\"\n+                )\n         if self.cloud is not None:\n             self.cloud.check_features_are_supported(\n-                {clouds.CloudImplementationFeatures.OPEN_PORTS})\n+                {clouds.CloudImplementationFeatures.OPEN_PORTS}\n+            )\n         # We don't need to check the ports format since we already done it\n         # in resources_utils.simplify_ports\n \n@@ -878,16 +917,21 @@ class Resources:\n         hours = seconds / 3600\n         # Instance.\n         hourly_cost = self.cloud.instance_type_to_hourly_cost(\n-            self._instance_type, self.use_spot, self._region, self._zone)\n+            self._instance_type, self.use_spot, self._region, self._zone\n+        )\n         # Accelerators (if any).\n         if self.accelerators is not None:\n             hourly_cost += self.cloud.accelerators_to_hourly_cost(\n-                self.accelerators, self.use_spot, self._region, self._zone)\n+                self.accelerators, self.use_spot, self._region, self._zone\n+            )\n         return hourly_cost * hours\n \n     def make_deploy_variables(\n-            self, cluster_name_on_cloud: str, region: clouds.Region,\n-            zones: Optional[List[clouds.Zone]]) -> Dict[str, Optional[str]]:\n+        self,\n+        cluster_name_on_cloud: str,\n+        region: clouds.Region,\n+        zones: Optional[List[clouds.Zone]],\n+    ) -> Dict[str, Optional[str]]:\n         \"\"\"Converts planned sky.Resources to resource variables.\n \n         These variables are divided into two categories: cloud-specific and\n@@ -896,7 +940,8 @@ class Resources:\n         variables are generated by this method.\n         \"\"\"\n         cloud_specific_variables = self.cloud.make_deploy_resources_variables(\n-            self, cluster_name_on_cloud, region, zones)\n+            self, cluster_name_on_cloud, region, zones\n+        )\n         docker_image = self.extract_docker_image()\n         return dict(\n             cloud_specific_variables,\n@@ -904,18 +949,19 @@ class Resources:\n                 # Docker config\n                 # Docker image. The image name used to pull the image, e.g.\n                 # ubuntu:latest.\n-                'docker_image': docker_image,\n+                \"docker_image\": docker_image,\n                 # Docker container name. The name of the container. Default to\n                 # `sky_container`.\n-                'docker_container_name':\n-                    constants.DEFAULT_DOCKER_CONTAINER_NAME,\n+                \"docker_container_name\": constants.DEFAULT_DOCKER_CONTAINER_NAME,\n                 # Docker login config (if any). This helps pull the image from\n                 # private registries.\n-                'docker_login_config': self._docker_login_config\n-            })\n+                \"docker_login_config\": self._docker_login_config,\n+            },\n+        )\n \n     def get_reservations_available_resources(\n-            self, specific_reservations: Set[str]) -> Dict[str, int]:\n+        self, specific_reservations: Set[str]\n+    ) -> Dict[str, int]:\n         \"\"\"Returns the number of available reservation resources.\"\"\"\n         if self.use_spot:\n             # GCP's & AWS's reservations do not support spot instances. We\n@@ -923,12 +969,12 @@ class Resources:\n             # to each cloud if any cloud supports reservations for spot.\n             return {}\n         return self.cloud.get_reservations_available_resources(\n-            self._instance_type, self._region, self._zone,\n-            specific_reservations)\n+            self._instance_type, self._region, self._zone, specific_reservations\n+        )\n \n     def less_demanding_than(\n         self,\n-        other: Union[List['Resources'], 'Resources'],\n+        other: Union[List[\"Resources\"], \"Resources\"],\n         requested_num_nodes: int = 1,\n         check_ports: bool = False,\n     ) -> bool:\n@@ -964,15 +1010,17 @@ class Resources:\n                 if not self.image_id.items() <= other.image_id.items():\n                     return False\n             else:\n-                this_image = (self.image_id.get(other.region) or\n-                              self.image_id.get(None))\n-                other_image = (other.image_id.get(other.region) or\n-                               other.image_id.get(None))\n+                this_image = self.image_id.get(other.region) or self.image_id.get(None)\n+                other_image = other.image_id.get(other.region) or other.image_id.get(\n+                    None\n+                )\n                 if this_image != other_image:\n                     return False\n \n-        if (self._instance_type is not None and\n-                self._instance_type != other.instance_type):\n+        if (\n+            self._instance_type is not None\n+            and self._instance_type != other.instance_type\n+        ):\n             return False\n         # self._instance_type <= other.instance_type\n \n@@ -987,8 +1035,10 @@ class Resources:\n                     return False\n         # self.accelerators <= other.accelerators\n \n-        if (self.accelerator_args is not None and\n-                self.accelerator_args != other.accelerator_args):\n+        if (\n+            self.accelerator_args is not None\n+            and self.accelerator_args != other.accelerator_args\n+        ):\n             return False\n         # self.accelerator_args == other.accelerator_args\n \n@@ -999,9 +1049,8 @@ class Resources:\n             if other.disk_tier is None:\n                 return False\n             if self.disk_tier != other.disk_tier:\n-                types = ['low', 'medium', 'high']\n-                return types.index(self.disk_tier) < types.index(\n-                    other.disk_tier)\n+                types = [\"low\", \"medium\", \"high\"]\n+                return types.index(self.disk_tier) < types.index(other.disk_tier)\n \n         if check_ports:\n             if self.ports is not None:\n@@ -1015,68 +1064,72 @@ class Resources:\n         # self <= other\n         return True\n \n-    def should_be_blocked_by(self, blocked: 'Resources') -> bool:\n+    def should_be_blocked_by(self, blocked: \"Resources\") -> bool:\n         \"\"\"Whether this Resources matches the blocked Resources.\n \n         If a field in `blocked` is None, it should be considered as a wildcard\n         for that field.\n         \"\"\"\n         is_matched = True\n-        if (blocked.cloud is not None and\n-                not self.cloud.is_same_cloud(blocked.cloud)):\n+        if blocked.cloud is not None and not self.cloud.is_same_cloud(blocked.cloud):\n             is_matched = False\n-        if (blocked.instance_type is not None and\n-                self.instance_type != blocked.instance_type):\n+        if (\n+            blocked.instance_type is not None\n+            and self.instance_type != blocked.instance_type\n+        ):\n             is_matched = False\n         if blocked.region is not None and self._region != blocked.region:\n             is_matched = False\n         if blocked.zone is not None and self._zone != blocked.zone:\n             is_matched = False\n-        if (blocked.accelerators is not None and\n-                self.accelerators != blocked.accelerators):\n+        if (\n+            blocked.accelerators is not None\n+            and self.accelerators != blocked.accelerators\n+        ):\n             is_matched = False\n         return is_matched\n \n     def is_empty(self) -> bool:\n         \"\"\"Is this Resources an empty request (all fields None)?\"\"\"\n-        return all([\n-            self.cloud is None,\n-            self._instance_type is None,\n-            self.cpus is None,\n-            self.memory is None,\n-            self.accelerators is None,\n-            self.accelerator_args is None,\n-            not self._use_spot_specified,\n-            self.disk_size == _DEFAULT_DISK_SIZE_GB,\n-            self.disk_tier is None,\n-            self._image_id is None,\n-            self.ports is None,\n-            self._docker_login_config is None,\n-        ])\n-\n-    def copy(self, **override) -> 'Resources':\n+        return all(\n+            [\n+                self.cloud is None,\n+                self._instance_type is None,\n+                self.cpus is None,\n+                self.memory is None,\n+                self.accelerators is None,\n+                self.accelerator_args is None,\n+                not self._use_spot_specified,\n+                self.disk_size == _DEFAULT_DISK_SIZE_GB,\n+                self.disk_tier is None,\n+                self._image_id is None,\n+                self.ports is None,\n+                self._docker_login_config is None,\n+            ]\n+        )\n+\n+    def copy(self, **override) -> \"Resources\":\n         \"\"\"Returns a copy of the given Resources.\"\"\"\n         use_spot = self.use_spot if self._use_spot_specified else None\n         resources = Resources(\n-            cloud=override.pop('cloud', self.cloud),\n-            instance_type=override.pop('instance_type', self.instance_type),\n-            cpus=override.pop('cpus', self.cpus),\n-            memory=override.pop('memory', self.memory),\n-            accelerators=override.pop('accelerators', self.accelerators),\n-            accelerator_args=override.pop('accelerator_args',\n-                                          self.accelerator_args),\n-            use_spot=override.pop('use_spot', use_spot),\n-            spot_recovery=override.pop('spot_recovery', self.spot_recovery),\n-            disk_size=override.pop('disk_size', self.disk_size),\n-            region=override.pop('region', self.region),\n-            zone=override.pop('zone', self.zone),\n-            image_id=override.pop('image_id', self.image_id),\n-            disk_tier=override.pop('disk_tier', self.disk_tier),\n-            ports=override.pop('ports', self.ports),\n-            _docker_login_config=override.pop('_docker_login_config',\n-                                              self._docker_login_config),\n-            _is_image_managed=override.pop('_is_image_managed',\n-                                           self._is_image_managed),\n+            cloud=override.pop(\"cloud\", self.cloud),\n+            instance_type=override.pop(\"instance_type\", self.instance_type),\n+            cpus=override.pop(\"cpus\", self.cpus),\n+            memory=override.pop(\"memory\", self.memory),\n+            accelerators=override.pop(\"accelerators\", self.accelerators),\n+            accelerator_args=override.pop(\"accelerator_args\", self.accelerator_args),\n+            use_spot=override.pop(\"use_spot\", use_spot),\n+            spot_recovery=override.pop(\"spot_recovery\", self.spot_recovery),\n+            disk_size=override.pop(\"disk_size\", self.disk_size),\n+            region=override.pop(\"region\", self.region),\n+            zone=override.pop(\"zone\", self.zone),\n+            image_id=override.pop(\"image_id\", self.image_id),\n+            disk_tier=override.pop(\"disk_tier\", self.disk_tier),\n+            ports=override.pop(\"ports\", self.ports),\n+            _docker_login_config=override.pop(\n+                \"_docker_login_config\", self._docker_login_config\n+            ),\n+            _is_image_managed=override.pop(\"_is_image_managed\", self._is_image_managed),\n         )\n         assert len(override) == 0\n         return resources\n@@ -1092,8 +1145,7 @@ class Resources:\n                 return False\n         return True\n \n-    def get_required_cloud_features(\n-            self) -> Set[clouds.CloudImplementationFeatures]:\n+    def get_required_cloud_features(self) -> Set[clouds.CloudImplementationFeatures]:\n         \"\"\"Returns the set of cloud features required by this Resources.\"\"\"\n         features = set()\n         if self.use_spot:\n@@ -1107,46 +1159,48 @@ class Resources:\n         return features\n \n     @classmethod\n-    def from_yaml_config(cls, config: Optional[Dict[str, str]]) -> 'Resources':\n+    def from_yaml_config(cls, config: Optional[Dict[str, str]]) -> \"Resources\":\n         if config is None:\n             return Resources()\n \n-        common_utils.validate_schema(config, schemas.get_resources_schema(),\n-                                     'Invalid resources YAML: ')\n+        common_utils.validate_schema(\n+            config, schemas.get_resources_schema(), \"Invalid resources YAML: \"\n+        )\n \n         resources_fields = {}\n-        resources_fields['cloud'] = clouds.CLOUD_REGISTRY.from_str(\n-            config.pop('cloud', None))\n-        resources_fields['instance_type'] = config.pop('instance_type', None)\n-        resources_fields['cpus'] = config.pop('cpus', None)\n-        resources_fields['memory'] = config.pop('memory', None)\n-        resources_fields['accelerators'] = config.pop('accelerators', None)\n-        resources_fields['accelerator_args'] = config.pop(\n-            'accelerator_args', None)\n-        resources_fields['use_spot'] = config.pop('use_spot', None)\n-        resources_fields['spot_recovery'] = config.pop('spot_recovery', None)\n-        resources_fields['disk_size'] = config.pop('disk_size', None)\n-        resources_fields['region'] = config.pop('region', None)\n-        resources_fields['zone'] = config.pop('zone', None)\n-        resources_fields['image_id'] = config.pop('image_id', None)\n-        resources_fields['disk_tier'] = config.pop('disk_tier', None)\n-        resources_fields['ports'] = config.pop('ports', None)\n-        resources_fields['_docker_login_config'] = config.pop(\n-            '_docker_login_config', None)\n-        resources_fields['_is_image_managed'] = config.pop(\n-            '_is_image_managed', None)\n-\n-        if resources_fields['cpus'] is not None:\n-            resources_fields['cpus'] = str(resources_fields['cpus'])\n-        if resources_fields['memory'] is not None:\n-            resources_fields['memory'] = str(resources_fields['memory'])\n-        if resources_fields['accelerator_args'] is not None:\n-            resources_fields['accelerator_args'] = dict(\n-                resources_fields['accelerator_args'])\n-        if resources_fields['disk_size'] is not None:\n-            resources_fields['disk_size'] = int(resources_fields['disk_size'])\n-\n-        assert not config, f'Invalid resource args: {config.keys()}'\n+        resources_fields[\"cloud\"] = clouds.CLOUD_REGISTRY.from_str(\n+            config.pop(\"cloud\", None)\n+        )\n+        resources_fields[\"instance_type\"] = config.pop(\"instance_type\", None)\n+        resources_fields[\"cpus\"] = config.pop(\"cpus\", None)\n+        resources_fields[\"memory\"] = config.pop(\"memory\", None)\n+        resources_fields[\"accelerators\"] = config.pop(\"accelerators\", None)\n+        resources_fields[\"accelerator_args\"] = config.pop(\"accelerator_args\", None)\n+        resources_fields[\"use_spot\"] = config.pop(\"use_spot\", None)\n+        resources_fields[\"spot_recovery\"] = config.pop(\"spot_recovery\", None)\n+        resources_fields[\"disk_size\"] = config.pop(\"disk_size\", None)\n+        resources_fields[\"region\"] = config.pop(\"region\", None)\n+        resources_fields[\"zone\"] = config.pop(\"zone\", None)\n+        resources_fields[\"image_id\"] = config.pop(\"image_id\", None)\n+        resources_fields[\"disk_tier\"] = config.pop(\"disk_tier\", None)\n+        resources_fields[\"ports\"] = config.pop(\"ports\", None)\n+        resources_fields[\"_docker_login_config\"] = config.pop(\n+            \"_docker_login_config\", None\n+        )\n+        resources_fields[\"_is_image_managed\"] = config.pop(\"_is_image_managed\", None)\n+\n+        if resources_fields[\"cpus\"] is not None:\n+            resources_fields[\"cpus\"] = str(resources_fields[\"cpus\"])\n+        if resources_fields[\"memory\"] is not None:\n+            resources_fields[\"memory\"] = str(resources_fields[\"memory\"])\n+        if resources_fields[\"accelerator_args\"] is not None:\n+            resources_fields[\"accelerator_args\"] = dict(\n+                resources_fields[\"accelerator_args\"]\n+            )\n+        if resources_fields[\"disk_size\"] is not None:\n+            resources_fields[\"disk_size\"] = int(resources_fields[\"disk_size\"])\n+\n+        assert not config, f\"Invalid resource args: {config.keys()}\"\n         return Resources(**resources_fields)\n \n     def to_yaml_config(self) -> Dict[str, Union[str, int]]:\n@@ -1154,28 +1208,28 @@ class Resources:\n         config = {}\n \n         def add_if_not_none(key, value):\n-            if value is not None and value != 'None':\n+            if value is not None and value != \"None\":\n                 config[key] = value\n \n-        add_if_not_none('cloud', str(self.cloud))\n-        add_if_not_none('instance_type', self.instance_type)\n-        add_if_not_none('cpus', self.cpus)\n-        add_if_not_none('memory', self.memory)\n-        add_if_not_none('accelerators', self.accelerators)\n-        add_if_not_none('accelerator_args', self.accelerator_args)\n+        add_if_not_none(\"cloud\", str(self.cloud))\n+        add_if_not_none(\"instance_type\", self.instance_type)\n+        add_if_not_none(\"cpus\", self.cpus)\n+        add_if_not_none(\"memory\", self.memory)\n+        add_if_not_none(\"accelerators\", self.accelerators)\n+        add_if_not_none(\"accelerator_args\", self.accelerator_args)\n \n         if self._use_spot_specified:\n-            add_if_not_none('use_spot', self.use_spot)\n-        config['spot_recovery'] = self.spot_recovery\n-        config['disk_size'] = self.disk_size\n-        add_if_not_none('region', self.region)\n-        add_if_not_none('zone', self.zone)\n-        add_if_not_none('image_id', self.image_id)\n-        add_if_not_none('disk_tier', self.disk_tier)\n-        add_if_not_none('ports', self.ports)\n-        add_if_not_none('_docker_login_config', self._docker_login_config)\n+            add_if_not_none(\"use_spot\", self.use_spot)\n+        config[\"spot_recovery\"] = self.spot_recovery\n+        config[\"disk_size\"] = self.disk_size\n+        add_if_not_none(\"region\", self.region)\n+        add_if_not_none(\"zone\", self.zone)\n+        add_if_not_none(\"image_id\", self.image_id)\n+        add_if_not_none(\"disk_tier\", self.disk_tier)\n+        add_if_not_none(\"ports\", self.ports)\n+        add_if_not_none(\"_docker_login_config\", self._docker_login_config)\n         if self._is_image_managed is not None:\n-            config['_is_image_managed'] = self._is_image_managed\n+            config[\"_is_image_managed\"] = self._is_image_managed\n         return config\n \n     def __setstate__(self, state):\n@@ -1184,25 +1238,25 @@ class Resources:\n \n         # TODO (zhwu): Design our persistent state format with `__getstate__`,\n         # so that to get rid of the version tracking.\n-        version = state.pop('_version', None)\n+        version = state.pop(\"_version\", None)\n         # Handle old version(s) here.\n         if version is None:\n             version = -1\n         if version < 0:\n-            cloud = state.pop('cloud', None)\n-            state['_cloud'] = cloud\n+            cloud = state.pop(\"cloud\", None)\n+            state[\"_cloud\"] = cloud\n \n-            instance_type = state.pop('instance_type', None)\n-            state['_instance_type'] = instance_type\n+            instance_type = state.pop(\"instance_type\", None)\n+            state[\"_instance_type\"] = instance_type\n \n-            use_spot = state.pop('use_spot', False)\n-            state['_use_spot'] = use_spot\n+            use_spot = state.pop(\"use_spot\", False)\n+            state[\"_use_spot\"] = use_spot\n \n-            accelerator_args = state.pop('accelerator_args', None)\n-            state['_accelerator_args'] = accelerator_args\n+            accelerator_args = state.pop(\"accelerator_args\", None)\n+            state[\"_accelerator_args\"] = accelerator_args\n \n-            disk_size = state.pop('disk_size', _DEFAULT_DISK_SIZE_GB)\n-            state['_disk_size'] = disk_size\n+            disk_size = state.pop(\"disk_size\", _DEFAULT_DISK_SIZE_GB)\n+            state[\"_disk_size\"] = disk_size\n \n         if version < 2:\n             self._region = None\n@@ -1217,13 +1271,13 @@ class Resources:\n             self._zone = None\n \n         if version < 6:\n-            accelerators = state.pop('_accelerators', None)\n+            accelerators = state.pop(\"_accelerators\", None)\n             if accelerators is not None:\n                 accelerators = {\n-                    accelerator_registry.canonicalize_accelerator_name(acc):\n-                    acc_count for acc, acc_count in accelerators.items()\n+                    accelerator_registry.canonicalize_accelerator_name(acc): acc_count\n+                    for acc, acc_count in accelerators.items()\n                 }\n-            state['_accelerators'] = accelerators\n+            state[\"_accelerators\"] = accelerators\n \n         if version < 7:\n             self._cpus = None\n@@ -1231,9 +1285,9 @@ class Resources:\n         if version < 8:\n             self._memory = None\n \n-        image_id = state.get('_image_id', None)\n+        image_id = state.get(\"_image_id\", None)\n         if isinstance(image_id, str):\n-            state['_image_id'] = {state.get('_region', None): image_id}\n+            state[\"_image_id\"] = {state.get(\"_region\", None): image_id}\n \n         if version < 9:\n             self._disk_tier = None\n@@ -1248,9 +1302,10 @@ class Resources:\n             self._docker_login_config = None\n \n         if version < 13:\n-            original_ports = state.get('_ports', None)\n+            original_ports = state.get(\"_ports\", None)\n             if original_ports is not None:\n-                state['_ports'] = resources_utils.simplify_ports(\n-                    [str(port) for port in original_ports])\n+                state[\"_ports\"] = resources_utils.simplify_ports(\n+                    [str(port) for port in original_ports]\n+                )\n \n         self.__dict__.update(state)\n"
    },
    {
        "id": 14,
        "sha_fail": "d2f64daf7608d00cb7a8659cfd1dee42c54bb12c",
        "diff": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 204ed7c..34ba7f8 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -3,15 +3,15 @@\n import os\n import sys\n \n-sys.path.insert(0, os.path.abspath('.'))\n-sys.path.insert(0, os.path.abspath('../'))\n-sys.path.insert(0, os.path.abspath('../../'))\n+sys.path.insert(0, os.path.abspath(\".\"))\n+sys.path.insert(0, os.path.abspath(\"../\"))\n+sys.path.insert(0, os.path.abspath(\"../../\"))\n \n # -- Project information\n \n-project = 'SkyPilot'\n-copyright = '2024, SkyPilot Team'\n-author = 'the SkyPilot authors'\n+project = \"SkyPilot\"\n+copyright = \"2024, SkyPilot Team\"\n+author = \"the SkyPilot authors\"\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -24,31 +24,31 @@ release = version\n # -- General configuration\n \n extensions = [\n-    'sphinxemoji.sphinxemoji',\n-    'sphinx.ext.autodoc',\n-    'sphinx.ext.autosummary',\n-    'sphinx.ext.duration',\n-    'sphinx.ext.doctest',\n-    'sphinx.ext.intersphinx',\n-    'sphinx.ext.napoleon',\n-    'sphinx.ext.viewcode',\n-    'sphinx_autodoc_typehints',\n-    'sphinx_click',\n-    'sphinx_copybutton',\n-    'sphinx_design',\n-    'myst_parser',\n+    \"sphinxemoji.sphinxemoji\",\n+    \"sphinx.ext.autodoc\",\n+    \"sphinx.ext.autosummary\",\n+    \"sphinx.ext.duration\",\n+    \"sphinx.ext.doctest\",\n+    \"sphinx.ext.intersphinx\",\n+    \"sphinx.ext.napoleon\",\n+    \"sphinx.ext.viewcode\",\n+    \"sphinx_autodoc_typehints\",\n+    \"sphinx_click\",\n+    \"sphinx_copybutton\",\n+    \"sphinx_design\",\n+    \"myst_parser\",\n ]\n \n intersphinx_mapping = {\n-    'python': ('https://docs.python.org/3/', None),\n-    'sphinx': ('https://www.sphinx-doc.org/en/master/', None),\n+    \"python\": (\"https://docs.python.org/3/\", None),\n+    \"sphinx\": (\"https://www.sphinx-doc.org/en/master/\", None),\n }\n-intersphinx_disabled_domains = ['std']\n+intersphinx_disabled_domains = [\"std\"]\n \n-templates_path = ['_templates']\n+templates_path = [\"_templates\"]\n \n # The main toctree document.\n-main_doc = 'index'\n+main_doc = \"index\"\n \n autosummary_generate = True\n napolean_use_rtype = False\n@@ -56,52 +56,52 @@ napolean_use_rtype = False\n # -- Options for autodoc\n \n # Python methods should be presented in source code order\n-autodoc_member_order = 'bysource'\n+autodoc_member_order = \"bysource\"\n \n # -- Options for HTML output\n \n-html_theme = 'sphinx_book_theme'\n+html_theme = \"sphinx_book_theme\"\n html_theme_options = {\n     # 'show_toc_level': 2,\n-    'logo': {\n-        'image_dark': '_static/SkyPilot_wide_dark.svg',\n+    \"logo\": {\n+        \"image_dark\": \"_static/SkyPilot_wide_dark.svg\",\n     },\n-    'repository_url': 'https://github.com/skypilot-org/skypilot',\n-    'use_repository_button': True,\n-    'use_issues_button': True,\n-    'use_edit_page_button': True,\n-    'repository_branch': 'master',\n-    'path_to_docs': 'docs/source',\n-    'pygment_light_style': 'tango',\n-    'pygment_dark_style': 'monokai',\n-    'primary_sidebar_end': [],\n+    \"repository_url\": \"https://github.com/skypilot-org/skypilot\",\n+    \"use_repository_button\": True,\n+    \"use_issues_button\": True,\n+    \"use_edit_page_button\": True,\n+    \"repository_branch\": \"master\",\n+    \"path_to_docs\": \"docs/source\",\n+    \"pygment_light_style\": \"tango\",\n+    \"pygment_dark_style\": \"monokai\",\n+    \"primary_sidebar_end\": [],\n }\n \n # The name for this set of Sphinx documents.  If None, it defaults to\n # \"<project> v<release> documentation\".\n-html_title = 'SkyPilot documentation'\n+html_title = \"SkyPilot documentation\"\n \n # -- Options for EPUB output\n-epub_show_urls = 'footnote'\n+epub_show_urls = \"footnote\"\n \n # -- Options for sphinx-copybutton\n-copybutton_prompt_text = r'\\$ '\n+copybutton_prompt_text = r\"\\$ \"\n copybutton_prompt_is_regexp = True\n \n html_show_sourcelink = False\n \n # The name of an image file (relative to this directory) to place at the top\n # of the sidebar.\n-html_logo = '_static/SkyPilot_wide_light.svg'\n+html_logo = \"_static/SkyPilot_wide_light.svg\"\n \n \n # The name of an image file (within the static path) to use as favicon of the\n # docs. This file should be a Windows icon file (.ico), 16x16 or 32x32 pixels.\n-html_favicon = '_static/favicon.ico'\n+html_favicon = \"_static/favicon.ico\"\n \n # Add any paths that contain custom static files (such as style sheets) here,\n # relative to this directory. They are copied after the builtin static files,\n # so a file named 'default.css' will overwrite the builtin 'default.css'.\n-html_static_path = ['_static']\n-html_js_files = ['custom.js']\n-html_css_files = ['custom.css']\n+html_static_path = [\"_static\"]\n+html_js_files = [\"custom.js\"]\n+html_css_files = [\"custom.css\"]\n"
    },
    {
        "id": 15,
        "sha_fail": "c3f4fe9eefdb297183b6d51bfc305e40feeec358",
        "diff": "diff --git a/sky/resources.py b/sky/resources.py\nindex b965401..10f889b 100644\n--- a/sky/resources.py\n+++ b/sky/resources.py\n@@ -1,4 +1,5 @@\n \"\"\"Resources: compute requirements of Tasks.\"\"\"\n+\n import functools\n import textwrap\n from typing import Dict, List, Optional, Set, Tuple, Union\n@@ -42,6 +43,7 @@ class Resources:\n     * for provisioning on a cloud\n \n     \"\"\"\n+\n     # If any fields changed, increment the version. For backward compatibility,\n     # modify the __setstate__ method to handle the old version.\n     _VERSION = 13\n@@ -60,7 +62,7 @@ class Resources:\n         zone: Optional[str] = None,\n         image_id: Union[Dict[str, str], str, None] = None,\n         disk_size: Optional[int] = None,\n-        disk_tier: Optional[Literal['high', 'medium', 'low']] = None,\n+        disk_tier: Optional[Literal[\"high\", \"medium\", \"low\"]] = None,\n         ports: Optional[Union[int, str, List[str], Tuple[str]]] = None,\n         # Internal use only.\n         _docker_login_config: Optional[docker_utils.DockerLoginConfig] = None,\n@@ -144,14 +146,15 @@ class Resources:\n         self._use_spot = use_spot if use_spot is not None else False\n         self._spot_recovery = None\n         if spot_recovery is not None:\n-            if spot_recovery.strip().lower() != 'none':\n+            if spot_recovery.strip().lower() != \"none\":\n                 self._spot_recovery = spot_recovery.upper()\n \n         if disk_size is not None:\n             if round(disk_size) != disk_size:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'OS disk size must be an integer. Got: {disk_size}.')\n+                        f\"OS disk size must be an integer. Got: {disk_size}.\"\n+                    )\n             self._disk_size = int(disk_size)\n         else:\n             self._disk_size = _DEFAULT_DISK_SIZE_GB\n@@ -165,9 +168,7 @@ class Resources:\n             if None in image_id:\n                 self._image_id = {self._region: image_id[None].strip()}\n             else:\n-                self._image_id = {\n-                    k.strip(): v.strip() for k, v in image_id.items()\n-                }\n+                self._image_id = {k.strip(): v.strip() for k, v in image_id.items()}\n         self._is_image_managed = _is_image_managed\n \n         self._disk_tier = disk_tier\n@@ -177,8 +178,7 @@ class Resources:\n                 ports = list(ports)\n             if not isinstance(ports, list):\n                 ports = [ports]\n-            ports = resources_utils.simplify_ports(\n-                [str(port) for port in ports])\n+            ports = resources_utils.simplify_ports([str(port) for port in ports])\n             if not ports:\n                 # Set to None if empty. This is mainly for resources from\n                 # cli, which will comes in as an empty tuple.\n@@ -229,83 +229,84 @@ class Resources:\n             >>> sky.Resources(disk_size=100)\n             <Cloud>(disk_size=100)\n         \"\"\"\n-        accelerators = ''\n-        accelerator_args = ''\n+        accelerators = \"\"\n+        accelerator_args = \"\"\n         if self.accelerators is not None:\n-            accelerators = f', {self.accelerators}'\n+            accelerators = f\", {self.accelerators}\"\n             if self.accelerator_args is not None:\n-                accelerator_args = f', accelerator_args={self.accelerator_args}'\n+                accelerator_args = f\", accelerator_args={self.accelerator_args}\"\n \n-        cpus = ''\n+        cpus = \"\"\n         if self.cpus is not None:\n-            cpus = f', cpus={self.cpus}'\n+            cpus = f\", cpus={self.cpus}\"\n \n-        memory = ''\n+        memory = \"\"\n         if self.memory is not None:\n-            memory = f', mem={self.memory}'\n+            memory = f\", mem={self.memory}\"\n \n         if isinstance(self.cloud, clouds.Local):\n-            return f'{self.cloud}({self.accelerators})'\n+            return f\"{self.cloud}({self.accelerators})\"\n \n-        use_spot = ''\n+        use_spot = \"\"\n         if self.use_spot:\n-            use_spot = '[Spot]'\n+            use_spot = \"[Spot]\"\n \n-        image_id = ''\n+        image_id = \"\"\n         if self.image_id is not None:\n             if None in self.image_id:\n-                image_id = f', image_id={self.image_id[None]}'\n+                image_id = f\", image_id={self.image_id[None]}\"\n             else:\n-                image_id = f', image_id={self.image_id}'\n+                image_id = f\", image_id={self.image_id}\"\n \n-        disk_tier = ''\n+        disk_tier = \"\"\n         if self.disk_tier is not None:\n-            disk_tier = f', disk_tier={self.disk_tier}'\n+            disk_tier = f\", disk_tier={self.disk_tier}\"\n \n-        disk_size = ''\n+        disk_size = \"\"\n         if self.disk_size != _DEFAULT_DISK_SIZE_GB:\n-            disk_size = f', disk_size={self.disk_size}'\n+            disk_size = f\", disk_size={self.disk_size}\"\n \n-        ports = ''\n+        ports = \"\"\n         if self.ports is not None:\n-            ports = f', ports={self.ports}'\n+            ports = f\", ports={self.ports}\"\n \n         if self._instance_type is not None:\n-            instance_type = f'{self._instance_type}'\n+            instance_type = f\"{self._instance_type}\"\n         else:\n-            instance_type = ''\n+            instance_type = \"\"\n \n         # Do not show region/zone here as `sky status -a` would show them as\n         # separate columns. Also, Resources repr will be printed during\n         # failover, and the region may be dynamically determined.\n         hardware_str = (\n-            f'{instance_type}{use_spot}'\n-            f'{cpus}{memory}{accelerators}{accelerator_args}{image_id}'\n-            f'{disk_tier}{disk_size}{ports}')\n+            f\"{instance_type}{use_spot}\"\n+            f\"{cpus}{memory}{accelerators}{accelerator_args}{image_id}\"\n+            f\"{disk_tier}{disk_size}{ports}\"\n+        )\n         # It may have leading ',' (for example, instance_type not set) or empty\n         # spaces.  Remove them.\n-        while hardware_str and hardware_str[0] in (',', ' '):\n+        while hardware_str and hardware_str[0] in (\",\", \" \"):\n             hardware_str = hardware_str[1:]\n \n-        cloud_str = '<Cloud>'\n+        cloud_str = \"<Cloud>\"\n         if self.cloud is not None:\n-            cloud_str = f'{self.cloud}'\n+            cloud_str = f\"{self.cloud}\"\n \n-        return f'{cloud_str}({hardware_str})'\n+        return f\"{cloud_str}({hardware_str})\"\n \n     @property\n     def repr_with_region_zone(self) -> str:\n-        region_str = ''\n+        region_str = \"\"\n         if self.region is not None:\n-            region_str = f', region={self.region}'\n-        zone_str = ''\n+            region_str = f\", region={self.region}\"\n+        zone_str = \"\"\n         if self.zone is not None:\n-            zone_str = f', zone={self.zone}'\n+            zone_str = f\", zone={self.zone}\"\n         repr_str = str(self)\n-        if repr_str.endswith(')'):\n-            repr_str = repr_str[:-1] + f'{region_str}{zone_str})'\n+        if repr_str.endswith(\")\"):\n+            repr_str = repr_str[:-1] + f\"{region_str}{zone_str})\"\n         else:\n-            repr_str += f'{region_str}{zone_str}'\n+            repr_str += f\"{region_str}{zone_str}\"\n         return repr_str\n \n     @property\n@@ -363,8 +364,7 @@ class Resources:\n         if self._accelerators is not None:\n             return self._accelerators\n         if self.cloud is not None and self._instance_type is not None:\n-            return self.cloud.get_accelerators_from_instance_type(\n-                self._instance_type)\n+            return self.cloud.get_accelerators_from_instance_type(self._instance_type)\n         return None\n \n     @property\n@@ -413,7 +413,7 @@ class Resources:\n \n         self._cpus = str(cpus)\n         if isinstance(cpus, str):\n-            if cpus.endswith('+'):\n+            if cpus.endswith(\"+\"):\n                 num_cpus_str = cpus[:-1]\n             else:\n                 num_cpus_str = cpus\n@@ -424,14 +424,16 @@ class Resources:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n                         f'The \"cpus\" field should be either a number or '\n-                        f'a string \"<number>+\". Found: {cpus!r}') from None\n+                        f'a string \"<number>+\". Found: {cpus!r}'\n+                    ) from None\n         else:\n             num_cpus = float(cpus)\n \n         if num_cpus <= 0:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'The \"cpus\" field should be positive. Found: {cpus!r}')\n+                    f'The \"cpus\" field should be positive. Found: {cpus!r}'\n+                )\n \n     def _set_memory(\n         self,\n@@ -443,7 +445,7 @@ class Resources:\n \n         self._memory = str(memory)\n         if isinstance(memory, str):\n-            if memory.endswith('+'):\n+            if memory.endswith(\"+\"):\n                 num_memory_gb = memory[:-1]\n             else:\n                 num_memory_gb = memory\n@@ -454,14 +456,16 @@ class Resources:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n                         f'The \"memory\" field should be either a number or '\n-                        f'a string \"<number>+\". Found: {memory!r}') from None\n+                        f'a string \"<number>+\". Found: {memory!r}'\n+                    ) from None\n         else:\n             memory_gb = float(memory)\n \n         if memory_gb <= 0:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'The \"cpus\" field should be positive. Found: {memory!r}')\n+                    f'The \"cpus\" field should be positive. Found: {memory!r}'\n+                )\n \n     def _set_accelerators(\n         self,\n@@ -476,13 +480,15 @@ class Resources:\n         \"\"\"\n         if accelerators is not None:\n             if isinstance(accelerators, str):  # Convert to Dict[str, int].\n-                if ':' not in accelerators:\n+                if \":\" not in accelerators:\n                     accelerators = {accelerators: 1}\n                 else:\n-                    splits = accelerators.split(':')\n-                    parse_error = ('The \"accelerators\" field as a str '\n-                                   'should be <name> or <name>:<cnt>. '\n-                                   f'Found: {accelerators!r}')\n+                    splits = accelerators.split(\":\")\n+                    parse_error = (\n+                        'The \"accelerators\" field as a str '\n+                        \"should be <name> or <name>:<cnt>. \"\n+                        f\"Found: {accelerators!r}\"\n+                    )\n                     if len(splits) != 2:\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(parse_error)\n@@ -502,35 +508,36 @@ class Resources:\n \n             # Canonicalize the accelerator names.\n             accelerators = {\n-                accelerator_registry.canonicalize_accelerator_name(acc):\n-                acc_count for acc, acc_count in accelerators.items()\n+                accelerator_registry.canonicalize_accelerator_name(acc): acc_count\n+                for acc, acc_count in accelerators.items()\n             }\n \n             acc, _ = list(accelerators.items())[0]\n-            if 'tpu' in acc.lower():\n+            if \"tpu\" in acc.lower():\n                 if self.cloud is None:\n                     self._cloud = clouds.GCP()\n-                assert self.cloud.is_same_cloud(\n-                    clouds.GCP()), 'Cloud must be GCP.'\n+                assert self.cloud.is_same_cloud(clouds.GCP()), \"Cloud must be GCP.\"\n                 if accelerator_args is None:\n                     accelerator_args = {}\n-                use_tpu_vm = accelerator_args.get('tpu_vm', False)\n+                use_tpu_vm = accelerator_args.get(\"tpu_vm\", False)\n                 if use_tpu_vm:\n                     tpu_utils.check_gcp_cli_include_tpu_vm()\n                 if self.instance_type is not None and use_tpu_vm:\n-                    if self.instance_type != 'TPU-VM':\n+                    if self.instance_type != \"TPU-VM\":\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                'Cannot specify instance type'\n-                                f' (got \"{self.instance_type}\") for TPU VM.')\n-                if 'runtime_version' not in accelerator_args:\n+                                \"Cannot specify instance type\"\n+                                f' (got \"{self.instance_type}\") for TPU VM.'\n+                            )\n+                if \"runtime_version\" not in accelerator_args:\n                     if use_tpu_vm:\n-                        accelerator_args['runtime_version'] = 'tpu-vm-base'\n+                        accelerator_args[\"runtime_version\"] = \"tpu-vm-base\"\n                     else:\n-                        accelerator_args['runtime_version'] = '2.12.0'\n+                        accelerator_args[\"runtime_version\"] = \"2.12.0\"\n                     logger.info(\n-                        'Missing runtime_version in accelerator_args, using'\n-                        f' default ({accelerator_args[\"runtime_version\"]})')\n+                        \"Missing runtime_version in accelerator_args, using\"\n+                        f\" default ({accelerator_args['runtime_version']})\"\n+                    )\n \n         self._accelerators = accelerators\n         self._accelerator_args = accelerator_args\n@@ -543,8 +550,9 @@ class Resources:\n         assert self.is_launchable(), self\n         return self.cloud.need_cleanup_after_preemption(self)\n \n-    def _validate_and_set_region_zone(self, region: Optional[str],\n-                                      zone: Optional[str]) -> None:\n+    def _validate_and_set_region_zone(\n+        self, region: Optional[str], zone: Optional[str]\n+    ) -> None:\n         if region is None and zone is None:\n             return\n \n@@ -564,39 +572,41 @@ class Resources:\n \n             if len(valid_clouds) == 0:\n                 if len(enabled_clouds) == 1:\n-                    cloud_str = f'for cloud {enabled_clouds[0]}'\n+                    cloud_str = f\"for cloud {enabled_clouds[0]}\"\n                 else:\n-                    cloud_str = f'for any cloud among {enabled_clouds}'\n+                    cloud_str = f\"for any cloud among {enabled_clouds}\"\n                 with ux_utils.print_exception_no_traceback():\n                     if len(cloud_to_errors) == 1:\n                         # UX: if 1 cloud, don't print a table.\n                         hint = list(cloud_to_errors.items())[0][-1]\n                     else:\n-                        table = log_utils.create_table(['Cloud', 'Hint'])\n-                        table.add_row(['-----', '----'])\n+                        table = log_utils.create_table([\"Cloud\", \"Hint\"])\n+                        table.add_row([\"-----\", \"----\"])\n                         for cloud, error in cloud_to_errors.items():\n-                            reason_str = '\\n'.join(textwrap.wrap(\n-                                str(error), 80))\n+                            reason_str = \"\\n\".join(textwrap.wrap(str(error), 80))\n                             table.add_row([str(cloud), reason_str])\n                         hint = table.get_string()\n                     raise ValueError(\n-                        f'Invalid (region {region!r}, zone {zone!r}) '\n-                        f'{cloud_str}. Details:\\n{hint}')\n+                        f\"Invalid (region {region!r}, zone {zone!r}) \"\n+                        f\"{cloud_str}. Details:\\n{hint}\"\n+                    )\n             elif len(valid_clouds) > 1:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Cannot infer cloud from (region {region!r}, zone '\n-                        f'{zone!r}). Multiple enabled clouds have region/zone '\n-                        f'of the same names: {valid_clouds}. '\n-                        f'To fix: explicitly specify `cloud`.')\n-            logger.debug(f'Cloud is not specified, using {valid_clouds[0]} '\n-                         f'inferred from region {region!r} and zone {zone!r}')\n+                        f\"Cannot infer cloud from (region {region!r}, zone \"\n+                        f\"{zone!r}). Multiple enabled clouds have region/zone \"\n+                        f\"of the same names: {valid_clouds}. \"\n+                        f\"To fix: explicitly specify `cloud`.\"\n+                    )\n+            logger.debug(\n+                f\"Cloud is not specified, using {valid_clouds[0]} \"\n+                f\"inferred from region {region!r} and zone {zone!r}\"\n+            )\n             self._cloud = valid_clouds[0]\n \n         # Validate if region and zone exist in the catalog, and set the region\n         # if zone is specified.\n-        self._region, self._zone = self._cloud.validate_region_zone(\n-            region, zone)\n+        self._region, self._zone = self._cloud.validate_region_zone(region, zone)\n \n     def get_valid_regions_for_launchable(self) -> List[clouds.Region]:\n         \"\"\"Returns a set of `Region`s that can provision this Resources.\n@@ -609,18 +619,24 @@ class Resources:\n         \"\"\"\n         assert self.is_launchable(), self\n \n-        regions = self._cloud.regions_with_offering(self._instance_type,\n-                                                    self.accelerators,\n-                                                    self._use_spot,\n-                                                    self._region, self._zone)\n+        regions = self._cloud.regions_with_offering(\n+            self._instance_type,\n+            self.accelerators,\n+            self._use_spot,\n+            self._region,\n+            self._zone,\n+        )\n         if self._image_id is not None and None not in self._image_id:\n             regions = [r for r in regions if r.name in self._image_id]\n \n         # Filter the regions by the skypilot_config\n         ssh_proxy_command_config = skypilot_config.get_nested(\n-            (str(self._cloud).lower(), 'ssh_proxy_command'), None)\n-        if (isinstance(ssh_proxy_command_config, str) or\n-                ssh_proxy_command_config is None):\n+            (str(self._cloud).lower(), \"ssh_proxy_command\"), None\n+        )\n+        if (\n+            isinstance(ssh_proxy_command_config, str)\n+            or ssh_proxy_command_config is None\n+        ):\n             # All regions are valid as the regions are not specified for the\n             # ssh_proxy_command config.\n             return regions\n@@ -642,9 +658,10 @@ class Resources:\n             yellow = colorama.Fore.YELLOW\n             reset = colorama.Style.RESET_ALL\n             logger.warning(\n-                f'{yellow}Request {self} cannot be satisfied by any feasible '\n-                'region. To fix, check that ssh_proxy_command\\'s region keys '\n-                f'include the regions to use.{reset}')\n+                f\"{yellow}Request {self} cannot be satisfied by any feasible \"\n+                \"region. To fix, check that ssh_proxy_command's region keys \"\n+                f\"include the regions to use.{reset}\"\n+            )\n \n         return filtered_regions\n \n@@ -658,8 +675,9 @@ class Resources:\n             if not valid:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Invalid instance type {self._instance_type!r} '\n-                        f'for cloud {self.cloud}.')\n+                        f\"Invalid instance type {self._instance_type!r} \"\n+                        f\"for cloud {self.cloud}.\"\n+                    )\n         else:\n             # If cloud not specified\n             valid_clouds = []\n@@ -669,22 +687,23 @@ class Resources:\n                     valid_clouds.append(cloud)\n             if len(valid_clouds) == 0:\n                 if len(enabled_clouds) == 1:\n-                    cloud_str = f'for cloud {enabled_clouds[0]}'\n+                    cloud_str = f\"for cloud {enabled_clouds[0]}\"\n                 else:\n-                    cloud_str = f'for any cloud among {enabled_clouds}'\n+                    cloud_str = f\"for any cloud among {enabled_clouds}\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Invalid instance type {self._instance_type!r} '\n-                        f'{cloud_str}.')\n+                        f\"Invalid instance type {self._instance_type!r} {cloud_str}.\"\n+                    )\n             if len(valid_clouds) > 1:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Ambiguous instance type {self._instance_type!r}. '\n-                        f'Please specify cloud explicitly among {valid_clouds}.'\n+                        f\"Ambiguous instance type {self._instance_type!r}. \"\n+                        f\"Please specify cloud explicitly among {valid_clouds}.\"\n                     )\n             logger.debug(\n-                f'Cloud is not specified, using {valid_clouds[0]} '\n-                f'inferred from the instance_type {self.instance_type!r}.')\n+                f\"Cloud is not specified, using {valid_clouds[0]} \"\n+                f\"inferred from the instance_type {self.instance_type!r}.\"\n+            )\n             self._cloud = valid_clouds[0]\n \n     def _try_validate_cpus_mem(self) -> None:\n@@ -696,36 +715,39 @@ class Resources:\n             # The _try_validate_instance_type() method infers and sets\n             # self.cloud if self.instance_type is not None.\n             assert self.cloud is not None\n-            cpus, mem = self.cloud.get_vcpus_mem_from_instance_type(\n-                self.instance_type)\n+            cpus, mem = self.cloud.get_vcpus_mem_from_instance_type(self.instance_type)\n             if self.cpus is not None:\n-                if self.cpus.endswith('+'):\n+                if self.cpus.endswith(\"+\"):\n                     if cpus < float(self.cpus[:-1]):\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                f'{self.instance_type} does not have enough '\n-                                f'vCPUs. {self.instance_type} has {cpus} '\n-                                f'vCPUs, but {self.cpus} is requested.')\n+                                f\"{self.instance_type} does not have enough \"\n+                                f\"vCPUs. {self.instance_type} has {cpus} \"\n+                                f\"vCPUs, but {self.cpus} is requested.\"\n+                            )\n                 elif cpus != float(self.cpus):\n                     with ux_utils.print_exception_no_traceback():\n                         raise ValueError(\n-                            f'{self.instance_type} does not have the requested '\n-                            f'number of vCPUs. {self.instance_type} has {cpus} '\n-                            f'vCPUs, but {self.cpus} is requested.')\n+                            f\"{self.instance_type} does not have the requested \"\n+                            f\"number of vCPUs. {self.instance_type} has {cpus} \"\n+                            f\"vCPUs, but {self.cpus} is requested.\"\n+                        )\n             if self.memory is not None:\n-                if self.memory.endswith('+'):\n+                if self.memory.endswith(\"+\"):\n                     if mem < float(self.memory[:-1]):\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                f'{self.instance_type} does not have enough '\n-                                f'memory. {self.instance_type} has {mem} GB '\n-                                f'memory, but {self.memory} is requested.')\n+                                f\"{self.instance_type} does not have enough \"\n+                                f\"memory. {self.instance_type} has {mem} GB \"\n+                                f\"memory, but {self.memory} is requested.\"\n+                            )\n                 elif mem != float(self.memory):\n                     with ux_utils.print_exception_no_traceback():\n                         raise ValueError(\n-                            f'{self.instance_type} does not have the requested '\n-                            f'memory. {self.instance_type} has {mem} GB '\n-                            f'memory, but {self.memory} is requested.')\n+                            f\"{self.instance_type} does not have the requested \"\n+                            f\"memory. {self.instance_type} has {mem} GB \"\n+                            f\"memory, but {self.memory} is requested.\"\n+                        )\n \n     def _try_validate_spot(self) -> None:\n         if self._spot_recovery is None:\n@@ -733,41 +755,46 @@ class Resources:\n         if not self._use_spot:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    'Cannot specify spot_recovery without use_spot set to True.'\n+                    \"Cannot specify spot_recovery without use_spot set to True.\"\n                 )\n         if self._spot_recovery not in spot.SPOT_STRATEGIES:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'Spot recovery strategy {self._spot_recovery} '\n-                    'is not supported. The strategy should be among '\n-                    f'{list(spot.SPOT_STRATEGIES.keys())}')\n+                    f\"Spot recovery strategy {self._spot_recovery} \"\n+                    \"is not supported. The strategy should be among \"\n+                    f\"{list(spot.SPOT_STRATEGIES.keys())}\"\n+                )\n \n     def _try_validate_local(self) -> None:\n         if isinstance(self._cloud, clouds.Local):\n             if self._use_spot:\n                 with ux_utils.print_exception_no_traceback():\n-                    raise ValueError('Local/On-prem mode does not support spot '\n-                                     'instances.')\n+                    raise ValueError(\n+                        \"Local/On-prem mode does not support spot instances.\"\n+                    )\n             local_instance = clouds.Local.get_default_instance_type()\n-            if (self._instance_type is not None and\n-                    self._instance_type != local_instance):\n+            if (\n+                self._instance_type is not None\n+                and self._instance_type != local_instance\n+            ):\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        'Local/On-prem mode does not support instance type:'\n-                        f' {self._instance_type}.')\n+                        \"Local/On-prem mode does not support instance type:\"\n+                        f\" {self._instance_type}.\"\n+                    )\n             if self._image_id is not None:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        'Local/On-prem mode does not support custom '\n-                        'images.')\n+                        \"Local/On-prem mode does not support custom images.\"\n+                    )\n \n     def extract_docker_image(self) -> Optional[str]:\n         if self.image_id is None:\n             return None\n         if len(self.image_id) == 1 and self.region in self.image_id:\n             image_id = self.image_id[self.region]\n-            if image_id.startswith('docker:'):\n-                return image_id[len('docker:'):]\n+            if image_id.startswith(\"docker:\"):\n+                return image_id[len(\"docker:\") :]\n         return None\n \n     def _try_validate_image_id(self) -> None:\n@@ -778,56 +805,66 @@ class Resources:\n             # TODO(tian): validate the docker image exists / of reasonable size\n             if self.accelerators is not None:\n                 for acc in self.accelerators.keys():\n-                    if acc.lower().startswith('tpu'):\n+                    if acc.lower().startswith(\"tpu\"):\n                         with ux_utils.print_exception_no_traceback():\n                             raise ValueError(\n-                                'Docker image is not supported for TPU VM.')\n+                                \"Docker image is not supported for TPU VM.\"\n+                            )\n             if self.cloud is not None:\n                 self.cloud.check_features_are_supported(\n-                    {clouds.CloudImplementationFeatures.DOCKER_IMAGE})\n+                    {clouds.CloudImplementationFeatures.DOCKER_IMAGE}\n+                )\n             return\n \n         if self.cloud is None:\n             with ux_utils.print_exception_no_traceback():\n-                raise ValueError(\n-                    'Cloud must be specified when image_id is provided.')\n+                raise ValueError(\"Cloud must be specified when image_id is provided.\")\n \n         # Apr, 2023 by Hysun(hysun.he@oracle.com): Added support for OCI\n-        if not self._cloud.is_same_cloud(\n-                clouds.AWS()) and not self._cloud.is_same_cloud(\n-                    clouds.GCP()) and not self._cloud.is_same_cloud(\n-                        clouds.IBM()) and not self._cloud.is_same_cloud(\n-                            clouds.OCI()):\n+        if (\n+            not self._cloud.is_same_cloud(clouds.AWS())\n+            and not self._cloud.is_same_cloud(clouds.GCP())\n+            and not self._cloud.is_same_cloud(clouds.IBM())\n+            and not self._cloud.is_same_cloud(clouds.OCI())\n+        ):\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    'image_id is only supported for AWS/GCP/IBM/OCI, please '\n-                    'explicitly specify the cloud.')\n+                    \"image_id is only supported for AWS/GCP/IBM/OCI, please \"\n+                    \"explicitly specify the cloud.\"\n+                )\n \n         if self._region is not None:\n             if self._region not in self._image_id:\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'image_id {self._image_id} should contain the image '\n-                        f'for the specified region {self._region}.')\n+                        f\"image_id {self._image_id} should contain the image \"\n+                        f\"for the specified region {self._region}.\"\n+                    )\n             # Narrow down the image_id to the specified region.\n             self._image_id = {self._region: self._image_id[self._region]}\n \n         # Check the image_id's are valid.\n         for region, image_id in self._image_id.items():\n-            if (image_id.startswith('skypilot:') and\n-                    not self._cloud.is_image_tag_valid(image_id, region)):\n-                region_str = f' ({region})' if region else ''\n+            if image_id.startswith(\"skypilot:\") and not self._cloud.is_image_tag_valid(\n+                image_id, region\n+            ):\n+                region_str = f\" ({region})\" if region else \"\"\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        f'Image tag {image_id!r} is not valid, please make sure'\n-                        f' the tag exists in {self._cloud}{region_str}.')\n+                        f\"Image tag {image_id!r} is not valid, please make sure\"\n+                        f\" the tag exists in {self._cloud}{region_str}.\"\n+                    )\n \n-            if (self._cloud.is_same_cloud(clouds.AWS()) and\n-                    not image_id.startswith('skypilot:') and region is None):\n+            if (\n+                self._cloud.is_same_cloud(clouds.AWS())\n+                and not image_id.startswith(\"skypilot:\")\n+                and region is None\n+            ):\n                 with ux_utils.print_exception_no_traceback():\n                     raise ValueError(\n-                        'image_id is only supported for AWS in a specific '\n-                        'region, please explicitly specify the region.')\n+                        \"image_id is only supported for AWS in a specific \"\n+                        \"region, please explicitly specify the region.\"\n+                    )\n \n         # Validate the image exists and the size is smaller than the disk size.\n         for region, image_id in self._image_id.items():\n@@ -836,40 +873,42 @@ class Resources:\n             image_size = self.cloud.get_image_size(image_id, region)\n             if image_size >= self.disk_size:\n                 with ux_utils.print_exception_no_traceback():\n-                    size_comp = ('larger than' if image_size > self.disk_size\n-                                 else 'equal to')\n+                    size_comp = (\n+                        \"larger than\" if image_size > self.disk_size else \"equal to\"\n+                    )\n                     raise ValueError(\n-                        f'Image {image_id!r} is {image_size}GB, which is '\n-                        f'{size_comp} the specified disk_size: '\n-                        f'{self.disk_size} GB. Please specify a larger '\n-                        'disk_size to use this image.')\n+                        f\"Image {image_id!r} is {image_size}GB, which is \"\n+                        f\"{size_comp} the specified disk_size: \"\n+                        f\"{self.disk_size} GB. Please specify a larger \"\n+                        \"disk_size to use this image.\"\n+                    )\n \n     def _try_validate_disk_tier(self) -> None:\n         if self.disk_tier is None:\n             return\n-        if self.disk_tier not in ['high', 'medium', 'low']:\n+        if self.disk_tier not in [\"high\", \"medium\", \"low\"]:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    f'Invalid disk_tier {self.disk_tier}. '\n-                    'Please use one of \"high\", \"medium\", or \"low\".')\n+                    f\"Invalid disk_tier {self.disk_tier}. \"\n+                    'Please use one of \"high\", \"medium\", or \"low\".'\n+                )\n         if self.instance_type is None:\n             return\n         if self.cloud is not None:\n-            self.cloud.check_disk_tier_enabled(self.instance_type,\n-                                               self.disk_tier)\n+            self.cloud.check_disk_tier_enabled(self.instance_type, self.disk_tier)\n \n     def _try_validate_ports(self) -> None:\n         if self.ports is None:\n             return\n-        if skypilot_config.get_nested(('aws', 'security_group_name'),\n-                                      None) is not None:\n+        if skypilot_config.get_nested((\"aws\", \"security_group_name\"), None) is not None:\n             with ux_utils.print_exception_no_traceback():\n                 raise ValueError(\n-                    'Cannot specify ports when AWS security group name is '\n-                    'specified.')\n+                    \"Cannot specify ports when AWS security group name is specified.\"\n+                )\n         if self.cloud is not None:\n             self.cloud.check_features_are_supported(\n-                {clouds.CloudImplementationFeatures.OPEN_PORTS})\n+                {clouds.CloudImplementationFeatures.OPEN_PORTS}\n+            )\n         # We don't need to check the ports format since we already done it\n         # in resources_utils.simplify_ports\n \n@@ -878,16 +917,21 @@ class Resources:\n         hours = seconds / 3600\n         # Instance.\n         hourly_cost = self.cloud.instance_type_to_hourly_cost(\n-            self._instance_type, self.use_spot, self._region, self._zone)\n+            self._instance_type, self.use_spot, self._region, self._zone\n+        )\n         # Accelerators (if any).\n         if self.accelerators is not None:\n             hourly_cost += self.cloud.accelerators_to_hourly_cost(\n-                self.accelerators, self.use_spot, self._region, self._zone)\n+                self.accelerators, self.use_spot, self._region, self._zone\n+            )\n         return hourly_cost * hours\n \n     def make_deploy_variables(\n-            self, cluster_name_on_cloud: str, region: clouds.Region,\n-            zones: Optional[List[clouds.Zone]]) -> Dict[str, Optional[str]]:\n+        self,\n+        cluster_name_on_cloud: str,\n+        region: clouds.Region,\n+        zones: Optional[List[clouds.Zone]],\n+    ) -> Dict[str, Optional[str]]:\n         \"\"\"Converts planned sky.Resources to resource variables.\n \n         These variables are divided into two categories: cloud-specific and\n@@ -896,7 +940,8 @@ class Resources:\n         variables are generated by this method.\n         \"\"\"\n         cloud_specific_variables = self.cloud.make_deploy_resources_variables(\n-            self, cluster_name_on_cloud, region, zones)\n+            self, cluster_name_on_cloud, region, zones\n+        )\n         docker_image = self.extract_docker_image()\n         return dict(\n             cloud_specific_variables,\n@@ -904,18 +949,19 @@ class Resources:\n                 # Docker config\n                 # Docker image. The image name used to pull the image, e.g.\n                 # ubuntu:latest.\n-                'docker_image': docker_image,\n+                \"docker_image\": docker_image,\n                 # Docker container name. The name of the container. Default to\n                 # `sky_container`.\n-                'docker_container_name':\n-                    constants.DEFAULT_DOCKER_CONTAINER_NAME,\n+                \"docker_container_name\": constants.DEFAULT_DOCKER_CONTAINER_NAME,\n                 # Docker login config (if any). This helps pull the image from\n                 # private registries.\n-                'docker_login_config': self._docker_login_config\n-            })\n+                \"docker_login_config\": self._docker_login_config,\n+            },\n+        )\n \n     def get_reservations_available_resources(\n-            self, specific_reservations: Set[str]) -> Dict[str, int]:\n+        self, specific_reservations: Set[str]\n+    ) -> Dict[str, int]:\n         \"\"\"Returns the number of available reservation resources.\"\"\"\n         if self.use_spot:\n             # GCP's & AWS's reservations do not support spot instances. We\n@@ -923,12 +969,12 @@ class Resources:\n             # to each cloud if any cloud supports reservations for spot.\n             return {}\n         return self.cloud.get_reservations_available_resources(\n-            self._instance_type, self._region, self._zone,\n-            specific_reservations)\n+            self._instance_type, self._region, self._zone, specific_reservations\n+        )\n \n     def less_demanding_than(\n         self,\n-        other: Union[List['Resources'], 'Resources'],\n+        other: Union[List[\"Resources\"], \"Resources\"],\n         requested_num_nodes: int = 1,\n         check_ports: bool = False,\n     ) -> bool:\n@@ -964,15 +1010,17 @@ class Resources:\n                 if not self.image_id.items() <= other.image_id.items():\n                     return False\n             else:\n-                this_image = (self.image_id.get(other.region) or\n-                              self.image_id.get(None))\n-                other_image = (other.image_id.get(other.region) or\n-                               other.image_id.get(None))\n+                this_image = self.image_id.get(other.region) or self.image_id.get(None)\n+                other_image = other.image_id.get(other.region) or other.image_id.get(\n+                    None\n+                )\n                 if this_image != other_image:\n                     return False\n \n-        if (self._instance_type is not None and\n-                self._instance_type != other.instance_type):\n+        if (\n+            self._instance_type is not None\n+            and self._instance_type != other.instance_type\n+        ):\n             return False\n         # self._instance_type <= other.instance_type\n \n@@ -987,8 +1035,10 @@ class Resources:\n                     return False\n         # self.accelerators <= other.accelerators\n \n-        if (self.accelerator_args is not None and\n-                self.accelerator_args != other.accelerator_args):\n+        if (\n+            self.accelerator_args is not None\n+            and self.accelerator_args != other.accelerator_args\n+        ):\n             return False\n         # self.accelerator_args == other.accelerator_args\n \n@@ -999,9 +1049,8 @@ class Resources:\n             if other.disk_tier is None:\n                 return False\n             if self.disk_tier != other.disk_tier:\n-                types = ['low', 'medium', 'high']\n-                return types.index(self.disk_tier) < types.index(\n-                    other.disk_tier)\n+                types = [\"low\", \"medium\", \"high\"]\n+                return types.index(self.disk_tier) < types.index(other.disk_tier)\n \n         if check_ports:\n             if self.ports is not None:\n@@ -1015,68 +1064,72 @@ class Resources:\n         # self <= other\n         return True\n \n-    def should_be_blocked_by(self, blocked: 'Resources') -> bool:\n+    def should_be_blocked_by(self, blocked: \"Resources\") -> bool:\n         \"\"\"Whether this Resources matches the blocked Resources.\n \n         If a field in `blocked` is None, it should be considered as a wildcard\n         for that field.\n         \"\"\"\n         is_matched = True\n-        if (blocked.cloud is not None and\n-                not self.cloud.is_same_cloud(blocked.cloud)):\n+        if blocked.cloud is not None and not self.cloud.is_same_cloud(blocked.cloud):\n             is_matched = False\n-        if (blocked.instance_type is not None and\n-                self.instance_type != blocked.instance_type):\n+        if (\n+            blocked.instance_type is not None\n+            and self.instance_type != blocked.instance_type\n+        ):\n             is_matched = False\n         if blocked.region is not None and self._region != blocked.region:\n             is_matched = False\n         if blocked.zone is not None and self._zone != blocked.zone:\n             is_matched = False\n-        if (blocked.accelerators is not None and\n-                self.accelerators != blocked.accelerators):\n+        if (\n+            blocked.accelerators is not None\n+            and self.accelerators != blocked.accelerators\n+        ):\n             is_matched = False\n         return is_matched\n \n     def is_empty(self) -> bool:\n         \"\"\"Is this Resources an empty request (all fields None)?\"\"\"\n-        return all([\n-            self.cloud is None,\n-            self._instance_type is None,\n-            self.cpus is None,\n-            self.memory is None,\n-            self.accelerators is None,\n-            self.accelerator_args is None,\n-            not self._use_spot_specified,\n-            self.disk_size == _DEFAULT_DISK_SIZE_GB,\n-            self.disk_tier is None,\n-            self._image_id is None,\n-            self.ports is None,\n-            self._docker_login_config is None,\n-        ])\n-\n-    def copy(self, **override) -> 'Resources':\n+        return all(\n+            [\n+                self.cloud is None,\n+                self._instance_type is None,\n+                self.cpus is None,\n+                self.memory is None,\n+                self.accelerators is None,\n+                self.accelerator_args is None,\n+                not self._use_spot_specified,\n+                self.disk_size == _DEFAULT_DISK_SIZE_GB,\n+                self.disk_tier is None,\n+                self._image_id is None,\n+                self.ports is None,\n+                self._docker_login_config is None,\n+            ]\n+        )\n+\n+    def copy(self, **override) -> \"Resources\":\n         \"\"\"Returns a copy of the given Resources.\"\"\"\n         use_spot = self.use_spot if self._use_spot_specified else None\n         resources = Resources(\n-            cloud=override.pop('cloud', self.cloud),\n-            instance_type=override.pop('instance_type', self.instance_type),\n-            cpus=override.pop('cpus', self.cpus),\n-            memory=override.pop('memory', self.memory),\n-            accelerators=override.pop('accelerators', self.accelerators),\n-            accelerator_args=override.pop('accelerator_args',\n-                                          self.accelerator_args),\n-            use_spot=override.pop('use_spot', use_spot),\n-            spot_recovery=override.pop('spot_recovery', self.spot_recovery),\n-            disk_size=override.pop('disk_size', self.disk_size),\n-            region=override.pop('region', self.region),\n-            zone=override.pop('zone', self.zone),\n-            image_id=override.pop('image_id', self.image_id),\n-            disk_tier=override.pop('disk_tier', self.disk_tier),\n-            ports=override.pop('ports', self.ports),\n-            _docker_login_config=override.pop('_docker_login_config',\n-                                              self._docker_login_config),\n-            _is_image_managed=override.pop('_is_image_managed',\n-                                           self._is_image_managed),\n+            cloud=override.pop(\"cloud\", self.cloud),\n+            instance_type=override.pop(\"instance_type\", self.instance_type),\n+            cpus=override.pop(\"cpus\", self.cpus),\n+            memory=override.pop(\"memory\", self.memory),\n+            accelerators=override.pop(\"accelerators\", self.accelerators),\n+            accelerator_args=override.pop(\"accelerator_args\", self.accelerator_args),\n+            use_spot=override.pop(\"use_spot\", use_spot),\n+            spot_recovery=override.pop(\"spot_recovery\", self.spot_recovery),\n+            disk_size=override.pop(\"disk_size\", self.disk_size),\n+            region=override.pop(\"region\", self.region),\n+            zone=override.pop(\"zone\", self.zone),\n+            image_id=override.pop(\"image_id\", self.image_id),\n+            disk_tier=override.pop(\"disk_tier\", self.disk_tier),\n+            ports=override.pop(\"ports\", self.ports),\n+            _docker_login_config=override.pop(\n+                \"_docker_login_config\", self._docker_login_config\n+            ),\n+            _is_image_managed=override.pop(\"_is_image_managed\", self._is_image_managed),\n         )\n         assert len(override) == 0\n         return resources\n@@ -1092,8 +1145,7 @@ class Resources:\n                 return False\n         return True\n \n-    def get_required_cloud_features(\n-            self) -> Set[clouds.CloudImplementationFeatures]:\n+    def get_required_cloud_features(self) -> Set[clouds.CloudImplementationFeatures]:\n         \"\"\"Returns the set of cloud features required by this Resources.\"\"\"\n         features = set()\n         if self.use_spot:\n@@ -1107,46 +1159,48 @@ class Resources:\n         return features\n \n     @classmethod\n-    def from_yaml_config(cls, config: Optional[Dict[str, str]]) -> 'Resources':\n+    def from_yaml_config(cls, config: Optional[Dict[str, str]]) -> \"Resources\":\n         if config is None:\n             return Resources()\n \n-        common_utils.validate_schema(config, schemas.get_resources_schema(),\n-                                     'Invalid resources YAML: ')\n+        common_utils.validate_schema(\n+            config, schemas.get_resources_schema(), \"Invalid resources YAML: \"\n+        )\n \n         resources_fields = {}\n-        resources_fields['cloud'] = clouds.CLOUD_REGISTRY.from_str(\n-            config.pop('cloud', None))\n-        resources_fields['instance_type'] = config.pop('instance_type', None)\n-        resources_fields['cpus'] = config.pop('cpus', None)\n-        resources_fields['memory'] = config.pop('memory', None)\n-        resources_fields['accelerators'] = config.pop('accelerators', None)\n-        resources_fields['accelerator_args'] = config.pop(\n-            'accelerator_args', None)\n-        resources_fields['use_spot'] = config.pop('use_spot', None)\n-        resources_fields['spot_recovery'] = config.pop('spot_recovery', None)\n-        resources_fields['disk_size'] = config.pop('disk_size', None)\n-        resources_fields['region'] = config.pop('region', None)\n-        resources_fields['zone'] = config.pop('zone', None)\n-        resources_fields['image_id'] = config.pop('image_id', None)\n-        resources_fields['disk_tier'] = config.pop('disk_tier', None)\n-        resources_fields['ports'] = config.pop('ports', None)\n-        resources_fields['_docker_login_config'] = config.pop(\n-            '_docker_login_config', None)\n-        resources_fields['_is_image_managed'] = config.pop(\n-            '_is_image_managed', None)\n-\n-        if resources_fields['cpus'] is not None:\n-            resources_fields['cpus'] = str(resources_fields['cpus'])\n-        if resources_fields['memory'] is not None:\n-            resources_fields['memory'] = str(resources_fields['memory'])\n-        if resources_fields['accelerator_args'] is not None:\n-            resources_fields['accelerator_args'] = dict(\n-                resources_fields['accelerator_args'])\n-        if resources_fields['disk_size'] is not None:\n-            resources_fields['disk_size'] = int(resources_fields['disk_size'])\n-\n-        assert not config, f'Invalid resource args: {config.keys()}'\n+        resources_fields[\"cloud\"] = clouds.CLOUD_REGISTRY.from_str(\n+            config.pop(\"cloud\", None)\n+        )\n+        resources_fields[\"instance_type\"] = config.pop(\"instance_type\", None)\n+        resources_fields[\"cpus\"] = config.pop(\"cpus\", None)\n+        resources_fields[\"memory\"] = config.pop(\"memory\", None)\n+        resources_fields[\"accelerators\"] = config.pop(\"accelerators\", None)\n+        resources_fields[\"accelerator_args\"] = config.pop(\"accelerator_args\", None)\n+        resources_fields[\"use_spot\"] = config.pop(\"use_spot\", None)\n+        resources_fields[\"spot_recovery\"] = config.pop(\"spot_recovery\", None)\n+        resources_fields[\"disk_size\"] = config.pop(\"disk_size\", None)\n+        resources_fields[\"region\"] = config.pop(\"region\", None)\n+        resources_fields[\"zone\"] = config.pop(\"zone\", None)\n+        resources_fields[\"image_id\"] = config.pop(\"image_id\", None)\n+        resources_fields[\"disk_tier\"] = config.pop(\"disk_tier\", None)\n+        resources_fields[\"ports\"] = config.pop(\"ports\", None)\n+        resources_fields[\"_docker_login_config\"] = config.pop(\n+            \"_docker_login_config\", None\n+        )\n+        resources_fields[\"_is_image_managed\"] = config.pop(\"_is_image_managed\", None)\n+\n+        if resources_fields[\"cpus\"] is not None:\n+            resources_fields[\"cpus\"] = str(resources_fields[\"cpus\"])\n+        if resources_fields[\"memory\"] is not None:\n+            resources_fields[\"memory\"] = str(resources_fields[\"memory\"])\n+        if resources_fields[\"accelerator_args\"] is not None:\n+            resources_fields[\"accelerator_args\"] = dict(\n+                resources_fields[\"accelerator_args\"]\n+            )\n+        if resources_fields[\"disk_size\"] is not None:\n+            resources_fields[\"disk_size\"] = int(resources_fields[\"disk_size\"])\n+\n+        assert not config, f\"Invalid resource args: {config.keys()}\"\n         return Resources(**resources_fields)\n \n     def to_yaml_config(self) -> Dict[str, Union[str, int]]:\n@@ -1154,28 +1208,28 @@ class Resources:\n         config = {}\n \n         def add_if_not_none(key, value):\n-            if value is not None and value != 'None':\n+            if value is not None and value != \"None\":\n                 config[key] = value\n \n-        add_if_not_none('cloud', str(self.cloud))\n-        add_if_not_none('instance_type', self.instance_type)\n-        add_if_not_none('cpus', self.cpus)\n-        add_if_not_none('memory', self.memory)\n-        add_if_not_none('accelerators', self.accelerators)\n-        add_if_not_none('accelerator_args', self.accelerator_args)\n+        add_if_not_none(\"cloud\", str(self.cloud))\n+        add_if_not_none(\"instance_type\", self.instance_type)\n+        add_if_not_none(\"cpus\", self.cpus)\n+        add_if_not_none(\"memory\", self.memory)\n+        add_if_not_none(\"accelerators\", self.accelerators)\n+        add_if_not_none(\"accelerator_args\", self.accelerator_args)\n \n         if self._use_spot_specified:\n-            add_if_not_none('use_spot', self.use_spot)\n-        config['spot_recovery'] = self.spot_recovery\n-        config['disk_size'] = self.disk_size\n-        add_if_not_none('region', self.region)\n-        add_if_not_none('zone', self.zone)\n-        add_if_not_none('image_id', self.image_id)\n-        add_if_not_none('disk_tier', self.disk_tier)\n-        add_if_not_none('ports', self.ports)\n-        add_if_not_none('_docker_login_config', self._docker_login_config)\n+            add_if_not_none(\"use_spot\", self.use_spot)\n+        config[\"spot_recovery\"] = self.spot_recovery\n+        config[\"disk_size\"] = self.disk_size\n+        add_if_not_none(\"region\", self.region)\n+        add_if_not_none(\"zone\", self.zone)\n+        add_if_not_none(\"image_id\", self.image_id)\n+        add_if_not_none(\"disk_tier\", self.disk_tier)\n+        add_if_not_none(\"ports\", self.ports)\n+        add_if_not_none(\"_docker_login_config\", self._docker_login_config)\n         if self._is_image_managed is not None:\n-            config['_is_image_managed'] = self._is_image_managed\n+            config[\"_is_image_managed\"] = self._is_image_managed\n         return config\n \n     def __setstate__(self, state):\n@@ -1184,25 +1238,25 @@ class Resources:\n \n         # TODO (zhwu): Design our persistent state format with `__getstate__`,\n         # so that to get rid of the version tracking.\n-        version = state.pop('_version', None)\n+        version = state.pop(\"_version\", None)\n         # Handle old version(s) here.\n         if version is None:\n             version = -1\n         if version < 0:\n-            cloud = state.pop('cloud', None)\n-            state['_cloud'] = cloud\n+            cloud = state.pop(\"cloud\", None)\n+            state[\"_cloud\"] = cloud\n \n-            instance_type = state.pop('instance_type', None)\n-            state['_instance_type'] = instance_type\n+            instance_type = state.pop(\"instance_type\", None)\n+            state[\"_instance_type\"] = instance_type\n \n-            use_spot = state.pop('use_spot', False)\n-            state['_use_spot'] = use_spot\n+            use_spot = state.pop(\"use_spot\", False)\n+            state[\"_use_spot\"] = use_spot\n \n-            accelerator_args = state.pop('accelerator_args', None)\n-            state['_accelerator_args'] = accelerator_args\n+            accelerator_args = state.pop(\"accelerator_args\", None)\n+            state[\"_accelerator_args\"] = accelerator_args\n \n-            disk_size = state.pop('disk_size', _DEFAULT_DISK_SIZE_GB)\n-            state['_disk_size'] = disk_size\n+            disk_size = state.pop(\"disk_size\", _DEFAULT_DISK_SIZE_GB)\n+            state[\"_disk_size\"] = disk_size\n \n         if version < 2:\n             self._region = None\n@@ -1217,13 +1271,13 @@ class Resources:\n             self._zone = None\n \n         if version < 6:\n-            accelerators = state.pop('_accelerators', None)\n+            accelerators = state.pop(\"_accelerators\", None)\n             if accelerators is not None:\n                 accelerators = {\n-                    accelerator_registry.canonicalize_accelerator_name(acc):\n-                    acc_count for acc, acc_count in accelerators.items()\n+                    accelerator_registry.canonicalize_accelerator_name(acc): acc_count\n+                    for acc, acc_count in accelerators.items()\n                 }\n-            state['_accelerators'] = accelerators\n+            state[\"_accelerators\"] = accelerators\n \n         if version < 7:\n             self._cpus = None\n@@ -1231,9 +1285,9 @@ class Resources:\n         if version < 8:\n             self._memory = None\n \n-        image_id = state.get('_image_id', None)\n+        image_id = state.get(\"_image_id\", None)\n         if isinstance(image_id, str):\n-            state['_image_id'] = {state.get('_region', None): image_id}\n+            state[\"_image_id\"] = {state.get(\"_region\", None): image_id}\n \n         if version < 9:\n             self._disk_tier = None\n@@ -1248,9 +1302,10 @@ class Resources:\n             self._docker_login_config = None\n \n         if version < 13:\n-            original_ports = state.get('_ports', None)\n+            original_ports = state.get(\"_ports\", None)\n             if original_ports is not None:\n-                state['_ports'] = resources_utils.simplify_ports(\n-                    [str(port) for port in original_ports])\n+                state[\"_ports\"] = resources_utils.simplify_ports(\n+                    [str(port) for port in original_ports]\n+                )\n \n         self.__dict__.update(state)\n"
    },
    {
        "id": 16,
        "sha_fail": "0d26cc1482ff5080ec579b17b29f22657a20c562",
        "diff": "diff --git a/beetsplug/edit.py b/beetsplug/edit.py\nindex 8a07fcc..a7a1422 100644\n--- a/beetsplug/edit.py\n+++ b/beetsplug/edit.py\n@@ -12,8 +12,7 @@\n # The above copyright notice and this permission notice shall be\n # included in all copies or substantial portions of the Software.\n \n-\"\"\"Open metadata information in a text editor to let the user edit it.\n-\"\"\"\n+\"\"\"Open metadata information in a text editor to let the user edit it.\"\"\"\n \n import codecs\n import os\n@@ -47,9 +46,7 @@ def edit(filename, log):\n     try:\n         subprocess.call(cmd)\n     except OSError as exc:\n-        raise ui.UserError(\n-            \"could not run editor command {!r}: {}\".format(cmd[0], exc)\n-        )\n+        raise ui.UserError(\"could not run editor command {!r}: {}\".format(cmd[0], exc))\n \n \n def dump(arg):\n@@ -72,9 +69,7 @@ def load(s):\n         for d in yaml.safe_load_all(s):\n             if not isinstance(d, dict):\n                 raise ParseError(\n-                    \"each entry must be a dictionary; found {}\".format(\n-                        type(d).__name__\n-                    )\n+                    \"each entry must be a dictionary; found {}\".format(type(d).__name__)\n                 )\n \n             # Convert all keys to strings. They started out as strings,\n@@ -246,7 +241,7 @@ class EditPlugin(plugins.BeetsPlugin):\n                 old_str += f\"# - {key}\\n\"\n             for obj in old_data:\n                 # those values will be enforced later anyway\n-                obj.update({k:v.get() for k,v in set_fields.items()})\n+                obj.update({k: v.get() for k, v in set_fields.items()})\n         else:\n             old_str = \"\"\n \n@@ -296,9 +291,7 @@ class EditPlugin(plugins.BeetsPlugin):\n                     return False\n \n                 # Confirm the changes.\n-                choice = ui.input_options(\n-                    (\"continue Editing\", \"apply\", \"cancel\")\n-                )\n+                choice = ui.input_options((\"continue Editing\", \"apply\", \"cancel\"))\n                 if choice == \"a\":  # Apply.\n                     return True\n                 elif choice == \"c\":  # Cancel.\n@@ -306,9 +299,7 @@ class EditPlugin(plugins.BeetsPlugin):\n                 elif choice == \"e\":  # Keep editing.\n                     # Reset the temporary changes to the objects. I we have a\n                     # copy from above, use that, else reload from the database.\n-                    objs = [\n-                        (old_obj or obj) for old_obj, obj in zip(objs_old, objs)\n-                    ]\n+                    objs = [(old_obj or obj) for old_obj, obj in zip(objs_old, objs)]\n                     for obj in objs:\n                         if not obj.id < 0:\n                             obj.load()\n@@ -366,9 +357,7 @@ class EditPlugin(plugins.BeetsPlugin):\n         choices = [PromptChoice(\"d\", \"eDit\", self.importer_edit)]\n         if task.candidates:\n             choices.append(\n-                PromptChoice(\n-                    \"c\", \"edit Candidates\", self.importer_edit_candidate\n-                )\n+                PromptChoice(\"c\", \"edit Candidates\", self.importer_edit_candidate)\n             )\n \n         return choices\n"
    },
    {
        "id": 17,
        "sha_fail": "7440ca51fb0ff3fb94a725fcd278f7fd5ea77c04",
        "diff": "diff --git a/beetsplug/listenbrainz.py b/beetsplug/listenbrainz.py\nindex d7ef0d4..02a5d81 100644\n--- a/beetsplug/listenbrainz.py\n+++ b/beetsplug/listenbrainz.py\n@@ -116,13 +116,9 @@ class ListenBrainzPlugin(BeetsPlugin):\n                 mbid = self.get_mb_recording_id(track)\n             tracks.append(\n                 {\n-                    \"album\": {\n-                        \"name\": track[\"track_metadata\"].get(\"release_name\")\n-                    },\n+                    \"album\": {\"name\": track[\"track_metadata\"].get(\"release_name\")},\n                     \"name\": track[\"track_metadata\"].get(\"track_name\"),\n-                    \"artist\": {\n-                        \"name\": track[\"track_metadata\"].get(\"artist_name\")\n-                    },\n+                    \"artist\": {\"name\": track[\"track_metadata\"].get(\"artist_name\")},\n                     \"mbid\": mbid,\n                     \"release_mbid\": mbid_mapping.get(\"release_mbid\"),\n                     \"listened_at\": track.get(\"listened_at\"),\n@@ -157,9 +153,7 @@ class ListenBrainzPlugin(BeetsPlugin):\n             playlist_info = playlist.get(\"playlist\")\n             if playlist_info.get(\"creator\") == \"listenbrainz\":\n                 title = playlist_info.get(\"title\")\n-                playlist_type = (\n-                    \"Exploration\" if \"Exploration\" in title else \"Jams\"\n-                )\n+                playlist_type = \"Exploration\" if \"Exploration\" in title else \"Jams\"\n                 if \"week of \" in title:\n                     date_str = title.split(\"week of \")[1].split(\" \")[0]\n                     date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n"
    },
    {
        "id": 18,
        "sha_fail": "b94b7e71f74eb6e8c4ef7f299c24a20f5cded2f8",
        "diff": "diff --git a/test/plugins/test_smartplaylist.py b/test/plugins/test_smartplaylist.py\nindex eecf408..c3a57d2 100644\n--- a/test/plugins/test_smartplaylist.py\n+++ b/test/plugins/test_smartplaylist.py\n@@ -241,7 +241,6 @@ class SmartPlaylistTest(_common.TestCase):\n             + b\"http://beets:8337/files/tagada.mp3\\n\",\n         )\n \n-\n     def test_playlist_update_uri_template(self):\n         spl = SmartPlaylistPlugin()\n \n"
    },
    {
        "id": 19,
        "sha_fail": "454164496177fd8b9d6aad4f106e68e816becb6c",
        "diff": "diff --git a/beetsplug/listenbrainz.py b/beetsplug/listenbrainz.py\nindex bb50209..b522c9c 100644\n--- a/beetsplug/listenbrainz.py\n+++ b/beetsplug/listenbrainz.py\n@@ -1,4 +1,4 @@\n-\"\"\" Adds Listenbrainz support to Beets. \"\"\"\n+\"\"\"Adds Listenbrainz support to Beets.\"\"\"\n \n import datetime\n \n@@ -10,7 +10,8 @@ from beetsplug.lastimport import process_tracks\n \n \n class ListenBrainzPlugin(BeetsPlugin):\n-    \"\"\" A Beets plugin for interacting with ListenBrainz.\"\"\"\n+    \"\"\"A Beets plugin for interacting with ListenBrainz.\"\"\"\n+\n     data_source = \"ListenBrainz\"\n     ROOT = \"http://api.listenbrainz.org/1/\"\n \n@@ -101,7 +102,6 @@ class ListenBrainzPlugin(BeetsPlugin):\n         else:\n             return None\n \n-\n     def get_tracks_from_listens(self, listens):\n         \"\"\"Returns a list of tracks from a list of listens.\"\"\"\n         tracks = []\n@@ -115,13 +115,9 @@ class ListenBrainzPlugin(BeetsPlugin):\n                 mbid = self.get_mb_recording_id(track)\n             tracks.append(\n                 {\n-                    \"album\": {\n-                        \"name\": track[\"track_metadata\"].get(\"release_name\")\n-                    },\n+                    \"album\": {\"name\": track[\"track_metadata\"].get(\"release_name\")},\n                     \"name\": track[\"track_metadata\"].get(\"track_name\"),\n-                    \"artist\": {\n-                        \"name\": track[\"track_metadata\"].get(\"artist_name\")\n-                    },\n+                    \"artist\": {\"name\": track[\"track_metadata\"].get(\"artist_name\")},\n                     \"mbid\": mbid,\n                     \"release_mbid\": mbid_mapping.get(\"release_mbid\"),\n                     \"listened_at\": track.get(\"listened_at\"),\n@@ -156,9 +152,7 @@ class ListenBrainzPlugin(BeetsPlugin):\n             playlist_info = playlist.get(\"playlist\")\n             if playlist_info.get(\"creator\") == \"listenbrainz\":\n                 title = playlist_info.get(\"title\")\n-                playlist_type = (\n-                    \"Exploration\" if \"Exploration\" in title else \"Jams\"\n-                )\n+                playlist_type = \"Exploration\" if \"Exploration\" in title else \"Jams\"\n                 date_str = title.split(\"week of \")[1].split(\" \")[0]\n                 date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n                 identifier = playlist_info.get(\"identifier\")\n"
    },
    {
        "id": 20,
        "sha_fail": "537b57d99d10ecbcf8a9835bda18a73ee284d88f",
        "diff": "diff --git a/beetsplug/listenbrainz.py b/beetsplug/listenbrainz.py\nindex 11ef715..4fbf4e8 100644\n--- a/beetsplug/listenbrainz.py\n+++ b/beetsplug/listenbrainz.py\n@@ -116,13 +116,9 @@ class ListenBrainzPlugin(BeetsPlugin):\n                 mbid = self.get_mb_recording_id(track)\n             tracks.append(\n                 {\n-                    \"album\": {\n-                        \"name\": track[\"track_metadata\"].get(\"release_name\")\n-                    },\n+                    \"album\": {\"name\": track[\"track_metadata\"].get(\"release_name\")},\n                     \"name\": track[\"track_metadata\"].get(\"track_name\"),\n-                    \"artist\": {\n-                        \"name\": track[\"track_metadata\"].get(\"artist_name\")\n-                    },\n+                    \"artist\": {\"name\": track[\"track_metadata\"].get(\"artist_name\")},\n                     \"mbid\": mbid,\n                     \"release_mbid\": mbid_mapping.get(\"release_mbid\"),\n                     \"listened_at\": track.get(\"listened_at\"),\n@@ -150,6 +146,7 @@ class ListenBrainzPlugin(BeetsPlugin):\n     def get_listenbrainz_playlists(self):\n         \"\"\"Returns a list of playlists created by ListenBrainz.\"\"\"\n         import re\n+\n         resp = self.get_playlists_createdfor(self.username)\n         playlists = resp.get(\"playlists\")\n         listenbrainz_playlists = []\n@@ -158,7 +155,9 @@ class ListenBrainzPlugin(BeetsPlugin):\n             playlist_info = playlist.get(\"playlist\")\n             if playlist_info.get(\"creator\") == \"listenbrainz\":\n                 title = playlist_info.get(\"title\")\n-                match = re.search(r\"(Missed Recordings of \\d{4}|Discoveries of \\d{4})\", title)\n+                match = re.search(\n+                    r\"(Missed Recordings of \\d{4}|Discoveries of \\d{4})\", title\n+                )\n                 if \"Exploration\" in title:\n                     playlist_type = \"Exploration\"\n                 elif \"Jams\" in title:\n@@ -169,16 +168,14 @@ class ListenBrainzPlugin(BeetsPlugin):\n                     playlist_type = None\n                 if \"week of \" in title:\n                     date_str = title.split(\"week of \")[1].split(\" \")[0]\n-                    date = datetime.datetime.strptime(\n-                        date_str, \"%Y-%m-%d\"\n-                    ).date()\n+                    date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n                 else:\n                     date = None\n                 identifier = playlist_info.get(\"identifier\")\n                 id = identifier.split(\"/\")[-1]\n                 if playlist_type in [\"Jams\", \"Exploration\"]:\n                     listenbrainz_playlists.append(\n-                    {\"type\": playlist_type, \"date\": date, \"identifier\": id}\n+                        {\"type\": playlist_type, \"date\": date, \"identifier\": id}\n                     )\n         return listenbrainz_playlists\n \n"
    },
    {
        "id": 21,
        "sha_fail": "68ddb2559e616656301858d441a523ebd64a710f",
        "diff": "diff --git a/src/diffusers/loaders/single_file_utils.py b/src/diffusers/loaders/single_file_utils.py\nindex 55b438b..8f62131 100644\n--- a/src/diffusers/loaders/single_file_utils.py\n+++ b/src/diffusers/loaders/single_file_utils.py\n@@ -12,7 +12,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\" Conversion script for the Stable Diffusion checkpoints.\"\"\"\n+\"\"\"Conversion script for the Stable Diffusion checkpoints.\"\"\"\n \n import os\n import re\n@@ -20,9 +20,7 @@ from contextlib import nullcontext\n from io import BytesIO\n \n import requests\n-import torch\n import yaml\n-from safetensors.torch import load_file as safe_load\n from transformers import (\n     CLIPTextConfig,\n     CLIPTextModel,\n@@ -1117,7 +1115,9 @@ def create_text_encoders_and_tokenizers_from_ldm(\n     elif model_type == \"FrozenCLIPEmbedder\":\n         try:\n             config_name = \"openai/clip-vit-large-patch14\"\n-            text_encoder = create_text_encoder_from_ldm_clip_checkpoint(config_name, checkpoint, local_files_only=local_files_only)\n+            text_encoder = create_text_encoder_from_ldm_clip_checkpoint(\n+                config_name, checkpoint, local_files_only=local_files_only\n+            )\n             tokenizer = CLIPTokenizer.from_pretrained(config_name, local_files_only=local_files_only)\n \n         except Exception:\n@@ -1159,7 +1159,9 @@ def create_text_encoders_and_tokenizers_from_ldm(\n         try:\n             config_name = \"openai/clip-vit-large-patch14\"\n             tokenizer = CLIPTokenizer.from_pretrained(config_name, local_files_only=local_files_only)\n-            text_encoder = create_text_encoder_from_ldm_clip_checkpoint(config_name, checkpoint, local_files_only=local_files_only)\n+            text_encoder = create_text_encoder_from_ldm_clip_checkpoint(\n+                config_name, checkpoint, local_files_only=local_files_only\n+            )\n \n         except Exception:\n             raise ValueError(\n"
    },
    {
        "id": 23,
        "sha_fail": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
        "diff": "diff --git a/devscripts/make_lazy_extractors.py b/devscripts/make_lazy_extractors.py\nindex d74ea20..1b1a1c3 100644\n--- a/devscripts/make_lazy_extractors.py\n+++ b/devscripts/make_lazy_extractors.py\n@@ -14,27 +14,39 @@\n \n NO_ATTR = object()\n STATIC_CLASS_PROPERTIES = [\n-    'IE_NAME', '_ENABLED', '_VALID_URL',  # Used for URL matching\n-    '_WORKING', 'IE_DESC', '_NETRC_MACHINE', 'SEARCH_KEY',  # Used for --extractor-descriptions\n-    'age_limit',  # Used for --age-limit (evaluated)\n-    '_RETURN_TYPE',  # Accessed in CLI only with instance (evaluated)\n+    \"IE_NAME\",\n+    \"_ENABLED\",\n+    \"_VALID_URL\",  # Used for URL matching\n+    \"_WORKING\",\n+    \"IE_DESC\",\n+    \"_NETRC_MACHINE\",\n+    \"SEARCH_KEY\",  # Used for --extractor-descriptions\n+    \"age_limit\",  # Used for --age-limit (evaluated)\n+    \"_RETURN_TYPE\",  # Accessed in CLI only with instance (evaluated)\n ]\n CLASS_METHODS = [\n-    'ie_key', 'suitable', '_match_valid_url',  # Used for URL matching\n-    'working', 'get_temp_id', '_match_id',  # Accessed just before instance creation\n-    'description',  # Used for --extractor-descriptions\n-    'is_suitable',  # Used for --age-limit\n-    'supports_login', 'is_single_video',  # Accessed in CLI only with instance\n+    \"ie_key\",\n+    \"suitable\",\n+    \"_match_valid_url\",  # Used for URL matching\n+    \"working\",\n+    \"get_temp_id\",\n+    \"_match_id\",  # Accessed just before instance creation\n+    \"description\",  # Used for --extractor-descriptions\n+    \"is_suitable\",  # Used for --age-limit\n+    \"supports_login\",\n+    \"is_single_video\",  # Accessed in CLI only with instance\n ]\n-IE_TEMPLATE = '''\n+IE_TEMPLATE = \"\"\"\n class {name}({bases}):\n     _module = {module!r}\n-'''\n-MODULE_TEMPLATE = read_file('devscripts/lazy_load_template.py')\n+\"\"\"\n+MODULE_TEMPLATE = read_file(\"devscripts/lazy_load_template.py\")\n \n \n def main():\n-    lazy_extractors_filename = get_filename_args(default_outfile='yt_dlp/extractor/lazy_extractors.py')\n+    lazy_extractors_filename = get_filename_args(\n+        default_outfile=\"yt_dlp/extractor/lazy_extractors.py\"\n+    )\n     if os.path.exists(lazy_extractors_filename):\n         os.remove(lazy_extractors_filename)\n \n@@ -44,23 +56,31 @@ def main():\n     from yt_dlp.extractor.common import InfoExtractor, SearchInfoExtractor\n \n     # Filter out plugins\n-    _ALL_CLASSES = [cls for cls in _ALL_CLASSES if not cls.__module__.startswith(f'{yt_dlp.plugins.PACKAGE_NAME}.')]\n-\n-    DummyInfoExtractor = type('InfoExtractor', (InfoExtractor,), {'IE_NAME': NO_ATTR})\n-    module_src = '\\n'.join((\n-        MODULE_TEMPLATE,\n-        '    _module = None',\n-        *extra_ie_code(DummyInfoExtractor),\n-        '\\nclass LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n',\n-        *build_ies(_ALL_CLASSES, (InfoExtractor, SearchInfoExtractor), DummyInfoExtractor),\n-    ))\n-\n-    write_file(lazy_extractors_filename, f'{module_src}\\n')\n+    _ALL_CLASSES = [\n+        cls\n+        for cls in _ALL_CLASSES\n+        if not cls.__module__.startswith(f\"{yt_dlp.plugins.PACKAGE_NAME}.\")\n+    ]\n+\n+    DummyInfoExtractor = type(\"InfoExtractor\", (InfoExtractor,), {\"IE_NAME\": NO_ATTR})\n+    module_src = \"\\n\".join(\n+        (\n+            MODULE_TEMPLATE,\n+            \"    _module = None\",\n+            *extra_ie_code(DummyInfoExtractor),\n+            \"\\nclass LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n\",\n+            *build_ies(\n+                _ALL_CLASSES, (InfoExtractor, SearchInfoExtractor), DummyInfoExtractor\n+            ),\n+        )\n+    )\n+\n+    write_file(lazy_extractors_filename, f\"{module_src}\\n\")\n \n \n def get_all_ies():\n-    PLUGINS_DIRNAME = 'ytdlp_plugins'\n-    BLOCKED_DIRNAME = f'{PLUGINS_DIRNAME}_blocked'\n+    PLUGINS_DIRNAME = \"ytdlp_plugins\"\n+    BLOCKED_DIRNAME = f\"{PLUGINS_DIRNAME}_blocked\"\n     if os.path.exists(PLUGINS_DIRNAME):\n         # os.rename cannot be used, e.g. in Docker. See https://github.com/yt-dlp/yt-dlp/pull/4958\n         shutil.move(PLUGINS_DIRNAME, BLOCKED_DIRNAME)\n@@ -76,8 +96,8 @@ def extra_ie_code(ie, base=None):\n     for var in STATIC_CLASS_PROPERTIES:\n         val = getattr(ie, var)\n         if val != (getattr(base, var) if base else NO_ATTR):\n-            yield f'    {var} = {val!r}'\n-    yield ''\n+            yield f\"    {var} = {val!r}\"\n+    yield \"\"\n \n     for name in CLASS_METHODS:\n         f = getattr(ie, name)\n@@ -92,20 +112,20 @@ def build_ies(ies, bases, attr_base):\n         if ie in ies:\n             names.append(ie.__name__)\n \n-    yield f'\\n_ALL_CLASSES = [{\", \".join(names)}]'\n+    yield f\"\\n_ALL_CLASSES = [{', '.join(names)}]\"\n \n \n def sort_ies(ies, ignored_bases):\n     \"\"\"find the correct sorting and add the required base classes so that subclasses can be correctly created\"\"\"\n     classes, returned_classes = ies[:-1], set()\n-    assert ies[-1].__name__ == 'GenericIE', 'Last IE must be GenericIE'\n+    assert ies[-1].__name__ == \"GenericIE\", \"Last IE must be GenericIE\"\n     while classes:\n         for c in classes[:]:\n             bases = set(c.__bases__) - {object, *ignored_bases}\n             restart = False\n             for b in sorted(bases, key=lambda x: x.__name__):\n                 if b not in classes and b not in returned_classes:\n-                    assert b.__name__ != 'GenericIE', 'Cannot inherit from GenericIE'\n+                    assert b.__name__ != \"GenericIE\", \"Cannot inherit from GenericIE\"\n                     classes.insert(0, b)\n                     restart = True\n             if restart:\n@@ -119,14 +139,17 @@ def sort_ies(ies, ignored_bases):\n \n \n def build_lazy_ie(ie, name, attr_base):\n-    bases = ', '.join({\n-        'InfoExtractor': 'LazyLoadExtractor',\n-        'SearchInfoExtractor': 'LazyLoadSearchExtractor',\n-    }.get(base.__name__, base.__name__) for base in ie.__bases__)\n+    bases = \", \".join(\n+        {\n+            \"InfoExtractor\": \"LazyLoadExtractor\",\n+            \"SearchInfoExtractor\": \"LazyLoadSearchExtractor\",\n+        }.get(base.__name__, base.__name__)\n+        for base in ie.__bases__\n+    )\n \n     s = IE_TEMPLATE.format(name=name, module=ie.__module__, bases=bases)\n-    return s + '\\n'.join(extra_ie_code(ie, attr_base))\n+    return s + \"\\n\".join(extra_ie_code(ie, attr_base))\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\ndiff --git a/test/test_InfoExtractor.py b/test/test_InfoExtractor.py\nindex b7dee49..bd59d7d 100644\n--- a/test/test_InfoExtractor.py\n+++ b/test/test_InfoExtractor.py\n@@ -31,9 +31,9 @@ def log_message(self, format, *args):\n         pass\n \n     def do_GET(self):\n-        if self.path == '/teapot':\n+        if self.path == \"/teapot\":\n             self.send_response(TEAPOT_RESPONSE_STATUS)\n-            self.send_header('Content-Type', 'text/html; charset=utf-8')\n+            self.send_header(\"Content-Type\", \"text/html; charset=utf-8\")\n             self.end_headers()\n             self.wfile.write(TEAPOT_RESPONSE_BODY.encode())\n         else:\n@@ -43,7 +43,8 @@ def do_GET(self):\n class DummyIE(InfoExtractor):\n     def _sort_formats(self, formats, field_preference=[]):\n         self._downloader.sort_formats(\n-            {'formats': formats, '_format_sort_fields': field_preference})\n+            {\"formats\": formats, \"_format_sort_fields\": field_preference}\n+        )\n \n \n class TestInfoExtractor(unittest.TestCase):\n@@ -56,11 +57,11 @@ def test_ie_key(self):\n     def test_html_search_regex(self):\n         html = '<p id=\"foo\">Watch this <a href=\"http://www.youtube.com/watch?v=BaW_jenozKc\">video</a></p>'\n         search = lambda re, *args: self.ie._html_search_regex(re, html, *args)\n-        self.assertEqual(search(r'<p id=\"foo\">(.+?)</p>', 'foo'), 'Watch this video')\n+        self.assertEqual(search(r'<p id=\"foo\">(.+?)</p>', \"foo\"), \"Watch this video\")\n \n     def test_opengraph(self):\n         ie = self.ie\n-        html = '''\n+        html = \"\"\"\n             <meta name=\"og:title\" content='Foo'/>\n             <meta content=\"Some video's description \" name=\"og:description\"/>\n             <meta property='og:image' content='http://domain.com/pic.jpg?key1=val1&amp;key2=val2'/>\n@@ -70,48 +71,68 @@ def test_opengraph(self):\n             <meta name=\"og:test2\" content=\"foo >//< bar\"/>\n             <meta property=og-test3 content='Ill-formatted opengraph'/>\n             <meta property=og:test4 content=unquoted-value/>\n-            '''\n-        self.assertEqual(ie._og_search_title(html), 'Foo')\n-        self.assertEqual(ie._og_search_description(html), 'Some video\\'s description ')\n-        self.assertEqual(ie._og_search_thumbnail(html), 'http://domain.com/pic.jpg?key1=val1&key2=val2')\n+            \"\"\"\n+        self.assertEqual(ie._og_search_title(html), \"Foo\")\n+        self.assertEqual(ie._og_search_description(html), \"Some video's description \")\n+        self.assertEqual(\n+            ie._og_search_thumbnail(html),\n+            \"http://domain.com/pic.jpg?key1=val1&key2=val2\",\n+        )\n         self.assertEqual(ie._og_search_video_url(html, default=None), None)\n-        self.assertEqual(ie._og_search_property('foobar', html), 'Foo')\n-        self.assertEqual(ie._og_search_property('test1', html), 'foo > < bar')\n-        self.assertEqual(ie._og_search_property('test2', html), 'foo >//< bar')\n-        self.assertEqual(ie._og_search_property('test3', html), 'Ill-formatted opengraph')\n-        self.assertEqual(ie._og_search_property(('test0', 'test1'), html), 'foo > < bar')\n-        self.assertRaises(RegexNotFoundError, ie._og_search_property, 'test0', html, None, fatal=True)\n-        self.assertRaises(RegexNotFoundError, ie._og_search_property, ('test0', 'test00'), html, None, fatal=True)\n-        self.assertEqual(ie._og_search_property('test4', html), 'unquoted-value')\n+        self.assertEqual(ie._og_search_property(\"foobar\", html), \"Foo\")\n+        self.assertEqual(ie._og_search_property(\"test1\", html), \"foo > < bar\")\n+        self.assertEqual(ie._og_search_property(\"test2\", html), \"foo >//< bar\")\n+        self.assertEqual(\n+            ie._og_search_property(\"test3\", html), \"Ill-formatted opengraph\"\n+        )\n+        self.assertEqual(\n+            ie._og_search_property((\"test0\", \"test1\"), html), \"foo > < bar\"\n+        )\n+        self.assertRaises(\n+            RegexNotFoundError, ie._og_search_property, \"test0\", html, None, fatal=True\n+        )\n+        self.assertRaises(\n+            RegexNotFoundError,\n+            ie._og_search_property,\n+            (\"test0\", \"test00\"),\n+            html,\n+            None,\n+            fatal=True,\n+        )\n+        self.assertEqual(ie._og_search_property(\"test4\", html), \"unquoted-value\")\n \n     def test_html_search_meta(self):\n         ie = self.ie\n-        html = '''\n+        html = \"\"\"\n             <meta name=\"a\" content=\"1\" />\n             <meta name='b' content='2'>\n             <meta name=\"c\" content='3'>\n             <meta name=d content='4'>\n             <meta property=\"e\" content='5' >\n             <meta content=\"6\" name=\"f\">\n-        '''\n+        \"\"\"\n \n-        self.assertEqual(ie._html_search_meta('a', html), '1')\n-        self.assertEqual(ie._html_search_meta('b', html), '2')\n-        self.assertEqual(ie._html_search_meta('c', html), '3')\n-        self.assertEqual(ie._html_search_meta('d', html), '4')\n-        self.assertEqual(ie._html_search_meta('e', html), '5')\n-        self.assertEqual(ie._html_search_meta('f', html), '6')\n-        self.assertEqual(ie._html_search_meta(('a', 'b', 'c'), html), '1')\n-        self.assertEqual(ie._html_search_meta(('c', 'b', 'a'), html), '3')\n-        self.assertEqual(ie._html_search_meta(('z', 'x', 'c'), html), '3')\n-        self.assertRaises(RegexNotFoundError, ie._html_search_meta, 'z', html, None, fatal=True)\n-        self.assertRaises(RegexNotFoundError, ie._html_search_meta, ('z', 'x'), html, None, fatal=True)\n+        self.assertEqual(ie._html_search_meta(\"a\", html), \"1\")\n+        self.assertEqual(ie._html_search_meta(\"b\", html), \"2\")\n+        self.assertEqual(ie._html_search_meta(\"c\", html), \"3\")\n+        self.assertEqual(ie._html_search_meta(\"d\", html), \"4\")\n+        self.assertEqual(ie._html_search_meta(\"e\", html), \"5\")\n+        self.assertEqual(ie._html_search_meta(\"f\", html), \"6\")\n+        self.assertEqual(ie._html_search_meta((\"a\", \"b\", \"c\"), html), \"1\")\n+        self.assertEqual(ie._html_search_meta((\"c\", \"b\", \"a\"), html), \"3\")\n+        self.assertEqual(ie._html_search_meta((\"z\", \"x\", \"c\"), html), \"3\")\n+        self.assertRaises(\n+            RegexNotFoundError, ie._html_search_meta, \"z\", html, None, fatal=True\n+        )\n+        self.assertRaises(\n+            RegexNotFoundError, ie._html_search_meta, (\"z\", \"x\"), html, None, fatal=True\n+        )\n \n     def test_search_json_ld_realworld(self):\n         _TESTS = [\n             # https://github.com/ytdl-org/youtube-dl/issues/23306\n             (\n-                r'''<script type=\"application/ld+json\">\n+                r\"\"\"<script type=\"application/ld+json\">\n {\n \"@context\": \"http://schema.org/\",\n \"@type\": \"VideoObject\",\n@@ -144,21 +165,21 @@ def test_search_json_ld_realworld(self):\n \"name\": \"Kleio Valentien\",\n \"url\": \"https://www.eporner.com/pornstar/kleio-valentien/\"\n }]}\n-                </script>''',\n+                </script>\"\"\",\n                 {\n-                    'title': '1 On 1 With Kleio',\n-                    'description': 'Kleio Valentien',\n-                    'url': 'https://gvideo.eporner.com/xN49A1cT3eB/xN49A1cT3eB.mp4',\n-                    'timestamp': 1449347075,\n-                    'duration': 743.0,\n-                    'view_count': 1120958,\n-                    'width': 1920,\n-                    'height': 1080,\n+                    \"title\": \"1 On 1 With Kleio\",\n+                    \"description\": \"Kleio Valentien\",\n+                    \"url\": \"https://gvideo.eporner.com/xN49A1cT3eB/xN49A1cT3eB.mp4\",\n+                    \"timestamp\": 1449347075,\n+                    \"duration\": 743.0,\n+                    \"view_count\": 1120958,\n+                    \"width\": 1920,\n+                    \"height\": 1080,\n                 },\n                 {},\n             ),\n             (\n-                r'''<script type=\"application/ld+json\">\n+                r\"\"\"<script type=\"application/ld+json\">\n       {\n       \"@context\": \"https://schema.org\",\n       \"@graph\": [\n@@ -210,15 +231,15 @@ def test_search_json_ld_realworld(self):\n       }\n       ]\n       }\n-                </script>''',\n+                </script>\"\"\",\n                 {\n-                    'timestamp': 1636523400,\n-                    'title': 'md5:91fe569e952e4d146485740ae927662b',\n+                    \"timestamp\": 1636523400,\n+                    \"title\": \"md5:91fe569e952e4d146485740ae927662b\",\n                 },\n-                {'expected_type': 'NewsArticle'},\n+                {\"expected_type\": \"NewsArticle\"},\n             ),\n             (\n-                r'''<script type=\"application/ld+json\">\n+                r\"\"\"<script type=\"application/ld+json\">\n                 {\"url\":\"/vrtnu/a-z/het-journaal/2021/het-journaal-het-journaal-19u-20211231/\",\n                 \"name\":\"Het journaal 19u\",\n                 \"description\":\"Het journaal 19u van vrijdag 31 december 2021.\",\n@@ -259,63 +280,103 @@ def test_search_json_ld_realworld(self):\n                 \"partOfSeries\":{\"name\":\"Het journaal\",\"@id\":\"222831405527\",\"@type\":\"TVSeries\"},\n                 \"partOfSeason\":{\"name\":\"Seizoen 2021\",\"@id\":\"961809365527\",\"@type\":\"TVSeason\"},\n                 \"@context\":\"https://schema.org\",\"@id\":\"961685295527\",\"@type\":\"TVEpisode\"}</script>\n-                ''',\n+                \"\"\",\n                 {\n-                    'chapters': [\n-                        {\"title\": \"Explosie Turnhout\", \"start_time\": 70, \"end_time\": 440},\n+                    \"chapters\": [\n+                        {\n+                            \"title\": \"Explosie Turnhout\",\n+                            \"start_time\": 70,\n+                            \"end_time\": 440,\n+                        },\n                         {\"title\": \"Jaarwisseling\", \"start_time\": 440, \"end_time\": 1179},\n-                        {\"title\": \"Natuurbranden Colorado\", \"start_time\": 1179, \"end_time\": 1263},\n-                        {\"title\": \"Klimaatverandering\", \"start_time\": 1263, \"end_time\": 1367},\n+                        {\n+                            \"title\": \"Natuurbranden Colorado\",\n+                            \"start_time\": 1179,\n+                            \"end_time\": 1263,\n+                        },\n+                        {\n+                            \"title\": \"Klimaatverandering\",\n+                            \"start_time\": 1263,\n+                            \"end_time\": 1367,\n+                        },\n                         {\"title\": \"Zacht weer\", \"start_time\": 1367, \"end_time\": 1383},\n-                        {\"title\": \"Financi\u00eble balans\", \"start_time\": 1383, \"end_time\": 1484},\n+                        {\n+                            \"title\": \"Financi\u00eble balans\",\n+                            \"start_time\": 1383,\n+                            \"end_time\": 1484,\n+                        },\n                         {\"title\": \"Club Brugge\", \"start_time\": 1484, \"end_time\": 1575},\n-                        {\"title\": \"Mentale gezondheid bij topsporters\", \"start_time\": 1575, \"end_time\": 1728},\n-                        {\"title\": \"Olympische Winterspelen\", \"start_time\": 1728, \"end_time\": 1873},\n-                        {\"title\": \"Sober oudjaar in Nederland\", \"start_time\": 1873, \"end_time\": 2079.23}\n+                        {\n+                            \"title\": \"Mentale gezondheid bij topsporters\",\n+                            \"start_time\": 1575,\n+                            \"end_time\": 1728,\n+                        },\n+                        {\n+                            \"title\": \"Olympische Winterspelen\",\n+                            \"start_time\": 1728,\n+                            \"end_time\": 1873,\n+                        },\n+                        {\n+                            \"title\": \"Sober oudjaar in Nederland\",\n+                            \"start_time\": 1873,\n+                            \"end_time\": 2079.23,\n+                        },\n                     ],\n-                    'title': 'Het journaal - Aflevering 365 (Seizoen 2021)'\n-                }, {}\n+                    \"title\": \"Het journaal - Aflevering 365 (Seizoen 2021)\",\n+                },\n+                {},\n             ),\n             (\n                 # test multiple thumbnails in a list\n-                r'''\n+                r\"\"\"\n <script type=\"application/ld+json\">\n {\"@context\":\"https://schema.org\",\n \"@type\":\"VideoObject\",\n \"thumbnailUrl\":[\"https://www.rainews.it/cropgd/640x360/dl/img/2021/12/30/1640886376927_GettyImages.jpg\"]}\n-</script>''',\n+</script>\"\"\",\n                 {\n-                    'thumbnails': [{'url': 'https://www.rainews.it/cropgd/640x360/dl/img/2021/12/30/1640886376927_GettyImages.jpg'}],\n+                    \"thumbnails\": [\n+                        {\n+                            \"url\": \"https://www.rainews.it/cropgd/640x360/dl/img/2021/12/30/1640886376927_GettyImages.jpg\"\n+                        }\n+                    ],\n                 },\n                 {},\n             ),\n             (\n                 # test single thumbnail\n-                r'''\n+                r\"\"\"\n <script type=\"application/ld+json\">\n {\"@context\":\"https://schema.org\",\n \"@type\":\"VideoObject\",\n \"thumbnailUrl\":\"https://www.rainews.it/cropgd/640x360/dl/img/2021/12/30/1640886376927_GettyImages.jpg\"}\n-</script>''',\n+</script>\"\"\",\n                 {\n-                    'thumbnails': [{'url': 'https://www.rainews.it/cropgd/640x360/dl/img/2021/12/30/1640886376927_GettyImages.jpg'}],\n+                    \"thumbnails\": [\n+                        {\n+                            \"url\": \"https://www.rainews.it/cropgd/640x360/dl/img/2021/12/30/1640886376927_GettyImages.jpg\"\n+                        }\n+                    ],\n                 },\n                 {},\n-            )\n+            ),\n         ]\n         for html, expected_dict, search_json_ld_kwargs in _TESTS:\n             expect_dict(\n                 self,\n                 self.ie._search_json_ld(html, None, **search_json_ld_kwargs),\n-                expected_dict\n+                expected_dict,\n             )\n \n     def test_download_json(self):\n-        uri = encode_data_uri(b'{\"foo\": \"blah\"}', 'application/json')\n-        self.assertEqual(self.ie._download_json(uri, None), {'foo': 'blah'})\n-        uri = encode_data_uri(b'callback({\"foo\": \"blah\"})', 'application/javascript')\n-        self.assertEqual(self.ie._download_json(uri, None, transform_source=strip_jsonp), {'foo': 'blah'})\n-        uri = encode_data_uri(b'{\"foo\": invalid}', 'application/json')\n+        uri = encode_data_uri(b'{\"foo\": \"blah\"}', \"application/json\")\n+        self.assertEqual(self.ie._download_json(uri, None), {\"foo\": \"blah\"})\n+        uri = encode_data_uri(b'callback({\"foo\": \"blah\"})', \"application/javascript\")\n+        self.assertEqual(\n+            self.ie._download_json(uri, None, transform_source=strip_jsonp),\n+            {\"foo\": \"blah\"},\n+        )\n+        uri = encode_data_uri(b'{\"foo\": invalid}', \"application/json\")\n         self.assertRaises(ExtractorError, self.ie._download_json, uri, None)\n         self.assertEqual(self.ie._download_json(uri, None, fatal=False), None)\n \n@@ -324,58 +385,70 @@ def test_parse_html5_media_entries(self):\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://127.0.0.1/video.html',\n-                r'<html><video src=\"/vid.mp4\" /></html>', None)[0],\n+                \"https://127.0.0.1/video.html\",\n+                r'<html><video src=\"/vid.mp4\" /></html>',\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://127.0.0.1/vid.mp4',\n-                }],\n-            })\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://127.0.0.1/vid.mp4\",\n+                    }\n+                ],\n+            },\n+        )\n \n         # from https://www.r18.com/\n         # with kpbs in label\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://www.r18.com/',\n-                r'''\n+                \"https://www.r18.com/\",\n+                r\"\"\"\n                 <video id=\"samplevideo_amateur\" class=\"js-samplevideo video-js vjs-default-skin vjs-big-play-centered\" controls preload=\"auto\" width=\"400\" height=\"225\" poster=\"//pics.r18.com/digital/amateur/mgmr105/mgmr105jp.jpg\">\n                     <source id=\"video_source\" src=\"https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_sm_w.mp4\" type=\"video/mp4\"  res=\"240\" label=\"300kbps\">\n                     <source id=\"video_source\" src=\"https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_dm_w.mp4\" type=\"video/mp4\"  res=\"480\" label=\"1000kbps\">\n                     <source id=\"video_source\" src=\"https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_dmb_w.mp4\" type=\"video/mp4\"  res=\"740\" label=\"1500kbps\">\n                     <p>Your browser does not support the video tag.</p>\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_sm_w.mp4',\n-                    'ext': 'mp4',\n-                    'format_id': '300kbps',\n-                    'height': 240,\n-                    'tbr': 300,\n-                }, {\n-                    'url': 'https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_dm_w.mp4',\n-                    'ext': 'mp4',\n-                    'format_id': '1000kbps',\n-                    'height': 480,\n-                    'tbr': 1000,\n-                }, {\n-                    'url': 'https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_dmb_w.mp4',\n-                    'ext': 'mp4',\n-                    'format_id': '1500kbps',\n-                    'height': 740,\n-                    'tbr': 1500,\n-                }],\n-                'thumbnail': '//pics.r18.com/digital/amateur/mgmr105/mgmr105jp.jpg'\n-            })\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_sm_w.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"300kbps\",\n+                        \"height\": 240,\n+                        \"tbr\": 300,\n+                    },\n+                    {\n+                        \"url\": \"https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_dm_w.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"1000kbps\",\n+                        \"height\": 480,\n+                        \"tbr\": 1000,\n+                    },\n+                    {\n+                        \"url\": \"https://awscc3001.r18.com/litevideo/freepv/m/mgm/mgmr105/mgmr105_dmb_w.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"1500kbps\",\n+                        \"height\": 740,\n+                        \"tbr\": 1500,\n+                    },\n+                ],\n+                \"thumbnail\": \"//pics.r18.com/digital/amateur/mgmr105/mgmr105jp.jpg\",\n+            },\n+        )\n \n         # from https://www.csfd.cz/\n         # with width and height\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://www.csfd.cz/',\n-                r'''\n+                \"https://www.csfd.cz/\",\n+                r\"\"\"\n                 <video width=\"770\" height=\"328\" preload=\"none\" controls  poster=\"https://img.csfd.cz/files/images/film/video/preview/163/344/163344118_748d20.png?h360\" >\n                     <source src=\"https://video.csfd.cz/files/videos/157/750/157750813/163327358_eac647.mp4\" type=\"video/mp4\" width=\"640\" height=\"360\">\n                     <source src=\"https://video.csfd.cz/files/videos/157/750/157750813/163327360_3d2646.mp4\" type=\"video/mp4\" width=\"1280\" height=\"720\">\n@@ -385,52 +458,66 @@ def test_parse_html5_media_entries(self):\n                     <source src=\"https://video.csfd.cz/files/videos/157/750/157750813/163327357_8ab472.webm\" type=\"video/webm\" width=\"1920\" height=\"1080\">\n                     <track src=\"https://video.csfd.cz/files/subtitles/163/344/163344115_4c388b.srt\" type=\"text/x-srt\" kind=\"subtitles\" srclang=\"cs\" label=\"cs\">\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://video.csfd.cz/files/videos/157/750/157750813/163327358_eac647.mp4',\n-                    'ext': 'mp4',\n-                    'width': 640,\n-                    'height': 360,\n-                }, {\n-                    'url': 'https://video.csfd.cz/files/videos/157/750/157750813/163327360_3d2646.mp4',\n-                    'ext': 'mp4',\n-                    'width': 1280,\n-                    'height': 720,\n-                }, {\n-                    'url': 'https://video.csfd.cz/files/videos/157/750/157750813/163327356_91f258.mp4',\n-                    'ext': 'mp4',\n-                    'width': 1920,\n-                    'height': 1080,\n-                }, {\n-                    'url': 'https://video.csfd.cz/files/videos/157/750/157750813/163327359_962b4a.webm',\n-                    'ext': 'webm',\n-                    'width': 640,\n-                    'height': 360,\n-                }, {\n-                    'url': 'https://video.csfd.cz/files/videos/157/750/157750813/163327361_6feee0.webm',\n-                    'ext': 'webm',\n-                    'width': 1280,\n-                    'height': 720,\n-                }, {\n-                    'url': 'https://video.csfd.cz/files/videos/157/750/157750813/163327357_8ab472.webm',\n-                    'ext': 'webm',\n-                    'width': 1920,\n-                    'height': 1080,\n-                }],\n-                'subtitles': {\n-                    'cs': [{'url': 'https://video.csfd.cz/files/subtitles/163/344/163344115_4c388b.srt'}]\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://video.csfd.cz/files/videos/157/750/157750813/163327358_eac647.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                    },\n+                    {\n+                        \"url\": \"https://video.csfd.cz/files/videos/157/750/157750813/163327360_3d2646.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                    },\n+                    {\n+                        \"url\": \"https://video.csfd.cz/files/videos/157/750/157750813/163327356_91f258.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                    },\n+                    {\n+                        \"url\": \"https://video.csfd.cz/files/videos/157/750/157750813/163327359_962b4a.webm\",\n+                        \"ext\": \"webm\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                    },\n+                    {\n+                        \"url\": \"https://video.csfd.cz/files/videos/157/750/157750813/163327361_6feee0.webm\",\n+                        \"ext\": \"webm\",\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                    },\n+                    {\n+                        \"url\": \"https://video.csfd.cz/files/videos/157/750/157750813/163327357_8ab472.webm\",\n+                        \"ext\": \"webm\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                    },\n+                ],\n+                \"subtitles\": {\n+                    \"cs\": [\n+                        {\n+                            \"url\": \"https://video.csfd.cz/files/subtitles/163/344/163344115_4c388b.srt\"\n+                        }\n+                    ]\n                 },\n-                'thumbnail': 'https://img.csfd.cz/files/images/film/video/preview/163/344/163344118_748d20.png?h360'\n-            })\n+                \"thumbnail\": \"https://img.csfd.cz/files/images/film/video/preview/163/344/163344118_748d20.png?h360\",\n+            },\n+        )\n \n         # from https://tamasha.com/v/Kkdjw\n         # with height in label\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://tamasha.com/v/Kkdjw',\n-                r'''\n+                \"https://tamasha.com/v/Kkdjw\",\n+                r\"\"\"\n                 <video crossorigin=\"anonymous\">\n                         <source src=\"https://s-v2.tamasha.com/statics/videos_file/19/8f/Kkdjw_198feff8577d0057536e905cce1fb61438dd64e0_n_240.mp4\" type=\"video/mp4\" label=\"AUTO\" res=\"0\"/>\n                                 <source src=\"https://s-v2.tamasha.com/statics/videos_file/19/8f/Kkdjw_198feff8577d0057536e905cce1fb61438dd64e0_n_240.mp4\" type=\"video/mp4\"\n@@ -438,100 +525,128 @@ def test_parse_html5_media_entries(self):\n                                 <source src=\"https://s-v2.tamasha.com/statics/videos_file/20/00/Kkdjw_200041c66f657fc967db464d156eafbc1ed9fe6f_n_144.mp4\" type=\"video/mp4\"\n                                         label=\"144p\" res=\"144\"/>\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://s-v2.tamasha.com/statics/videos_file/19/8f/Kkdjw_198feff8577d0057536e905cce1fb61438dd64e0_n_240.mp4',\n-                }, {\n-                    'url': 'https://s-v2.tamasha.com/statics/videos_file/19/8f/Kkdjw_198feff8577d0057536e905cce1fb61438dd64e0_n_240.mp4',\n-                    'ext': 'mp4',\n-                    'format_id': '240p',\n-                    'height': 240,\n-                }, {\n-                    'url': 'https://s-v2.tamasha.com/statics/videos_file/20/00/Kkdjw_200041c66f657fc967db464d156eafbc1ed9fe6f_n_144.mp4',\n-                    'ext': 'mp4',\n-                    'format_id': '144p',\n-                    'height': 144,\n-                }]\n-            })\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://s-v2.tamasha.com/statics/videos_file/19/8f/Kkdjw_198feff8577d0057536e905cce1fb61438dd64e0_n_240.mp4\",\n+                    },\n+                    {\n+                        \"url\": \"https://s-v2.tamasha.com/statics/videos_file/19/8f/Kkdjw_198feff8577d0057536e905cce1fb61438dd64e0_n_240.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"240p\",\n+                        \"height\": 240,\n+                    },\n+                    {\n+                        \"url\": \"https://s-v2.tamasha.com/statics/videos_file/20/00/Kkdjw_200041c66f657fc967db464d156eafbc1ed9fe6f_n_144.mp4\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"144p\",\n+                        \"height\": 144,\n+                    },\n+                ]\n+            },\n+        )\n \n         # from https://www.directvnow.com\n         # with data-src\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://www.directvnow.com',\n-                r'''\n+                \"https://www.directvnow.com\",\n+                r\"\"\"\n                 <video id=\"vid1\" class=\"header--video-masked active\" muted playsinline>\n                     <source data-src=\"https://cdn.directv.com/content/dam/dtv/prod/website_directvnow-international/videos/DTVN_hdr_HBO_v3.mp4\" type=\"video/mp4\" />\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'ext': 'mp4',\n-                    'url': 'https://cdn.directv.com/content/dam/dtv/prod/website_directvnow-international/videos/DTVN_hdr_HBO_v3.mp4',\n-                }]\n-            })\n+                \"formats\": [\n+                    {\n+                        \"ext\": \"mp4\",\n+                        \"url\": \"https://cdn.directv.com/content/dam/dtv/prod/website_directvnow-international/videos/DTVN_hdr_HBO_v3.mp4\",\n+                    }\n+                ]\n+            },\n+        )\n \n         # from https://www.directvnow.com\n         # with data-src\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://www.directvnow.com',\n-                r'''\n+                \"https://www.directvnow.com\",\n+                r\"\"\"\n                 <video id=\"vid1\" class=\"header--video-masked active\" muted playsinline>\n                     <source data-src=\"https://cdn.directv.com/content/dam/dtv/prod/website_directvnow-international/videos/DTVN_hdr_HBO_v3.mp4\" type=\"video/mp4\" />\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://cdn.directv.com/content/dam/dtv/prod/website_directvnow-international/videos/DTVN_hdr_HBO_v3.mp4',\n-                    'ext': 'mp4',\n-                }]\n-            })\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://cdn.directv.com/content/dam/dtv/prod/website_directvnow-international/videos/DTVN_hdr_HBO_v3.mp4\",\n+                        \"ext\": \"mp4\",\n+                    }\n+                ]\n+            },\n+        )\n \n         # from https://www.klarna.com/uk/\n         # with data-video-src\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://www.directvnow.com',\n-                r'''\n+                \"https://www.directvnow.com\",\n+                r\"\"\"\n                 <video loop autoplay muted class=\"responsive-video block-kl__video video-on-medium\">\n                     <source src=\"\" data-video-desktop data-video-src=\"https://www.klarna.com/uk/wp-content/uploads/sites/11/2019/01/KL062_Smooth3_0_DogWalking_5s_920x080_.mp4\" type=\"video/mp4\" />\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://www.klarna.com/uk/wp-content/uploads/sites/11/2019/01/KL062_Smooth3_0_DogWalking_5s_920x080_.mp4',\n-                    'ext': 'mp4',\n-                }],\n-            })\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://www.klarna.com/uk/wp-content/uploads/sites/11/2019/01/KL062_Smooth3_0_DogWalking_5s_920x080_.mp4\",\n+                        \"ext\": \"mp4\",\n+                    }\n+                ],\n+            },\n+        )\n \n         # from https://0000.studio/\n         # with type attribute but without extension in URL\n         expect_dict(\n             self,\n             self.ie._parse_html5_media_entries(\n-                'https://0000.studio',\n-                r'''\n+                \"https://0000.studio\",\n+                r\"\"\"\n                 <video src=\"https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92\"\n                     controls=\"controls\" type=\"video/mp4\" preload=\"metadata\" autoplay=\"autoplay\" playsinline class=\"object-contain\">\n                 </video>\n-                ''', None)[0],\n+                \"\"\",\n+                None,\n+            )[0],\n             {\n-                'formats': [{\n-                    'url': 'https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92',\n-                    'ext': 'mp4',\n-                }],\n-            })\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92\",\n+                        \"ext\": \"mp4\",\n+                    }\n+                ],\n+            },\n+        )\n \n     def test_extract_jwplayer_data_realworld(self):\n         # from http://www.suffolk.edu/sjc/\n         expect_dict(\n             self,\n-            self.ie._extract_jwplayer_data(r'''\n+            self.ie._extract_jwplayer_data(\n+                r\"\"\"\n                 <script type='text/javascript'>\n                     jwplayer('my-video').setup({\n                         file: 'rtmp://192.138.214.154/live/sjclive',\n@@ -542,19 +657,23 @@ def test_extract_jwplayer_data_realworld(self):\n                       mediaid:'XEgvuql4'\n                     });\n                 </script>\n-                ''', None, require_title=False),\n+                \"\"\",\n+                None,\n+                require_title=False,\n+            ),\n             {\n-                'id': 'XEgvuql4',\n-                'formats': [{\n-                    'url': 'rtmp://192.138.214.154/live/sjclive',\n-                    'ext': 'flv'\n-                }]\n-            })\n+                \"id\": \"XEgvuql4\",\n+                \"formats\": [\n+                    {\"url\": \"rtmp://192.138.214.154/live/sjclive\", \"ext\": \"flv\"}\n+                ],\n+            },\n+        )\n \n         # from https://www.pornoxo.com/videos/7564/striptease-from-sexy-secretary/\n         expect_dict(\n             self,\n-            self.ie._extract_jwplayer_data(r'''\n+            self.ie._extract_jwplayer_data(\n+                r\"\"\"\n <script type=\"text/javascript\">\n     jwplayer(\"mediaplayer\").setup({\n         'videoid': \"7564\",\n@@ -583,19 +702,26 @@ def test_extract_jwplayer_data_realworld(self):\n         autostart: false\n     });\n </script>\n-            ''', 'dummy', require_title=False),\n+            \"\"\",\n+                \"dummy\",\n+                require_title=False,\n+            ),\n             {\n-                'thumbnail': 'https://t03.vipstreamservice.com/thumbs/pxo-full/2009-12/14/a4b2157147afe5efa93ce1978e0265289c193874e02597.flv-full-13.jpg',\n-                'formats': [{\n-                    'url': 'https://cdn.pornoxo.com/key=MF+oEbaxqTKb50P-w9G3nA,end=1489689259,ip=104.199.146.27/ip=104.199.146.27/speed=6573765/buffer=3.0/2009-12/4b2157147afe5efa93ce1978e0265289c193874e02597.flv',\n-                    'ext': 'flv'\n-                }]\n-            })\n+                \"thumbnail\": \"https://t03.vipstreamservice.com/thumbs/pxo-full/2009-12/14/a4b2157147afe5efa93ce1978e0265289c193874e02597.flv-full-13.jpg\",\n+                \"formats\": [\n+                    {\n+                        \"url\": \"https://cdn.pornoxo.com/key=MF+oEbaxqTKb50P-w9G3nA,end=1489689259,ip=104.199.146.27/ip=104.199.146.27/speed=6573765/buffer=3.0/2009-12/4b2157147afe5efa93ce1978e0265289c193874e02597.flv\",\n+                        \"ext\": \"flv\",\n+                    }\n+                ],\n+            },\n+        )\n \n         # from http://www.indiedb.com/games/king-machine/videos\n         expect_dict(\n             self,\n-            self.ie._extract_jwplayer_data(r'''\n+            self.ie._extract_jwplayer_data(\n+                r\"\"\"\n <script>\n jwplayer(\"mediaplayer\").setup({\"abouttext\":\"Visit Indie DB\",\"aboutlink\":\"http:\\/\\/www.indiedb.com\\/\",\"displaytitle\":false,\"autostart\":false,\"repeat\":false,\"title\":\"king machine trailer 1\",\"sharing\":{\"link\":\"http:\\/\\/www.indiedb.com\\/games\\/king-machine\\/videos\\/king-machine-trailer-1\",\"code\":\"<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"http:\\/\\/www.indiedb.com\\/media\\/iframe\\/1522983\\\" frameborder=\\\"0\\\" allowfullscreen><\\/iframe><br><a href=\\\"http:\\/\\/www.indiedb.com\\/games\\/king-machine\\/videos\\/king-machine-trailer-1\\\">king machine trailer 1 - Indie DB<\\/a>\"},\"related\":{\"file\":\"http:\\/\\/rss.indiedb.com\\/media\\/recommended\\/1522983\\/feed\\/rss.xml\",\"dimensions\":\"160x120\",\"onclick\":\"link\"},\"sources\":[{\"file\":\"http:\\/\\/cdn.dbolical.com\\/cache\\/videos\\/games\\/1\\/50\\/49678\\/encode_mp4\\/king-machine-trailer.mp4\",\"label\":\"360p SD\",\"default\":\"true\"},{\"file\":\"http:\\/\\/cdn.dbolical.com\\/cache\\/videos\\/games\\/1\\/50\\/49678\\/encode720p_mp4\\/king-machine-trailer.mp4\",\"label\":\"720p HD\"}],\"image\":\"http:\\/\\/media.indiedb.com\\/cache\\/images\\/games\\/1\\/50\\/49678\\/thumb_620x2000\\/king-machine-trailer.mp4.jpg\",\"advertising\":{\"client\":\"vast\",\"tag\":\"http:\\/\\/ads.intergi.com\\/adrawdata\\/3.0\\/5205\\/4251742\\/0\\/1013\\/ADTECH;cors=yes;width=560;height=315;referring_url=http:\\/\\/www.indiedb.com\\/games\\/king-machine\\/videos\\/king-machine-trailer-1;content_url=http:\\/\\/www.indiedb.com\\/games\\/king-machine\\/videos\\/king-machine-trailer-1;media_id=1522983;title=king+machine+trailer+1;device=__DEVICE__;model=__MODEL__;os=Windows+OS;osversion=__OSVERSION__;ua=__UA__;ip=109.171.17.81;uniqueid=1522983;tags=__TAGS__;number=58cac25928151;time=1489683033\"},\"width\":620,\"height\":349}).once(\"play\", function(event) {\n             videoAnalytics(\"play\");\n@@ -603,434 +729,491 @@ def test_extract_jwplayer_data_realworld(self):\n     videoAnalytics(\"completed\");\n });\n </script>\n-                ''', 'dummy'),\n+                \"\"\",\n+                \"dummy\",\n+            ),\n             {\n-                'title': 'king machine trailer 1',\n-                'thumbnail': 'http://media.indiedb.com/cache/images/games/1/50/49678/thumb_620x2000/king-machine-trailer.mp4.jpg',\n-                'formats': [{\n-                    'url': 'http://cdn.dbolical.com/cache/videos/games/1/50/49678/encode_mp4/king-machine-trailer.mp4',\n-                    'height': 360,\n-                    'ext': 'mp4'\n-                }, {\n-                    'url': 'http://cdn.dbolical.com/cache/videos/games/1/50/49678/encode720p_mp4/king-machine-trailer.mp4',\n-                    'height': 720,\n-                    'ext': 'mp4'\n-                }]\n-            })\n+                \"title\": \"king machine trailer 1\",\n+                \"thumbnail\": \"http://media.indiedb.com/cache/images/games/1/50/49678/thumb_620x2000/king-machine-trailer.mp4.jpg\",\n+                \"formats\": [\n+                    {\n+                        \"url\": \"http://cdn.dbolical.com/cache/videos/games/1/50/49678/encode_mp4/king-machine-trailer.mp4\",\n+                        \"height\": 360,\n+                        \"ext\": \"mp4\",\n+                    },\n+                    {\n+                        \"url\": \"http://cdn.dbolical.com/cache/videos/games/1/50/49678/encode720p_mp4/king-machine-trailer.mp4\",\n+                        \"height\": 720,\n+                        \"ext\": \"mp4\",\n+                    },\n+                ],\n+            },\n+        )\n \n     def test_parse_m3u8_formats(self):\n         _TEST_CASES = [\n             (\n                 # https://github.com/ytdl-org/youtube-dl/issues/11995\n                 # http://teamcoco.com/video/clueless-gamer-super-bowl-for-honor\n-                'img_bipbop_adv_example_fmp4',\n-                'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                [{\n-                    'format_id': 'aud1-English',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/a1/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'language': 'en',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'audio_ext': 'mp4',\n-                }, {\n-                    'format_id': 'aud2-English',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/a2/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'language': 'en',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'audio_ext': 'mp4',\n-                }, {\n-                    'format_id': 'aud3-English',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/a3/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'language': 'en',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'audio_ext': 'mp4',\n-                }, {\n-                    'format_id': '530',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v2/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 480,\n-                    'height': 270,\n-                    'vcodec': 'avc1.640015',\n-                }, {\n-                    'format_id': '561',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v2/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 480,\n-                    'height': 270,\n-                    'vcodec': 'avc1.640015',\n-                }, {\n-                    'format_id': '753',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v2/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 480,\n-                    'height': 270,\n-                    'vcodec': 'avc1.640015',\n-                }, {\n-                    'format_id': '895',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v3/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 640,\n-                    'height': 360,\n-                    'vcodec': 'avc1.64001e',\n-                }, {\n-                    'format_id': '926',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v3/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 640,\n-                    'height': 360,\n-                    'vcodec': 'avc1.64001e',\n-                }, {\n-                    'format_id': '1118',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v3/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 640,\n-                    'height': 360,\n-                    'vcodec': 'avc1.64001e',\n-                }, {\n-                    'format_id': '1265',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v4/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 768,\n-                    'height': 432,\n-                    'vcodec': 'avc1.64001e',\n-                }, {\n-                    'format_id': '1295',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v4/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 768,\n-                    'height': 432,\n-                    'vcodec': 'avc1.64001e',\n-                }, {\n-                    'format_id': '1487',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v4/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 768,\n-                    'height': 432,\n-                    'vcodec': 'avc1.64001e',\n-                }, {\n-                    'format_id': '2168',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v5/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 960,\n-                    'height': 540,\n-                    'vcodec': 'avc1.640020',\n-                }, {\n-                    'format_id': '2198',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v5/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 960,\n-                    'height': 540,\n-                    'vcodec': 'avc1.640020',\n-                }, {\n-                    'format_id': '2390',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v5/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 960,\n-                    'height': 540,\n-                    'vcodec': 'avc1.640020',\n-                }, {\n-                    'format_id': '3168',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v6/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1280,\n-                    'height': 720,\n-                    'vcodec': 'avc1.640020',\n-                }, {\n-                    'format_id': '3199',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v6/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1280,\n-                    'height': 720,\n-                    'vcodec': 'avc1.640020',\n-                }, {\n-                    'format_id': '3391',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v6/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1280,\n-                    'height': 720,\n-                    'vcodec': 'avc1.640020',\n-                }, {\n-                    'format_id': '4670',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v7/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '4701',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v7/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '4893',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v7/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '6170',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v8/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '6200',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v8/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '6392',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v8/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '7968',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v9/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '7998',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v9/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }, {\n-                    'format_id': '8190',\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v9/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.64002a',\n-                }],\n-                {}\n+                \"img_bipbop_adv_example_fmp4\",\n+                \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                [\n+                    {\n+                        \"format_id\": \"aud1-English\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/a1/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"language\": \"en\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"audio_ext\": \"mp4\",\n+                    },\n+                    {\n+                        \"format_id\": \"aud2-English\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/a2/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"language\": \"en\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"audio_ext\": \"mp4\",\n+                    },\n+                    {\n+                        \"format_id\": \"aud3-English\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/a3/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"language\": \"en\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"audio_ext\": \"mp4\",\n+                    },\n+                    {\n+                        \"format_id\": \"530\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v2/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 480,\n+                        \"height\": 270,\n+                        \"vcodec\": \"avc1.640015\",\n+                    },\n+                    {\n+                        \"format_id\": \"561\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v2/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 480,\n+                        \"height\": 270,\n+                        \"vcodec\": \"avc1.640015\",\n+                    },\n+                    {\n+                        \"format_id\": \"753\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v2/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 480,\n+                        \"height\": 270,\n+                        \"vcodec\": \"avc1.640015\",\n+                    },\n+                    {\n+                        \"format_id\": \"895\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v3/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                        \"vcodec\": \"avc1.64001e\",\n+                    },\n+                    {\n+                        \"format_id\": \"926\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v3/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                        \"vcodec\": \"avc1.64001e\",\n+                    },\n+                    {\n+                        \"format_id\": \"1118\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v3/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                        \"vcodec\": \"avc1.64001e\",\n+                    },\n+                    {\n+                        \"format_id\": \"1265\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v4/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 768,\n+                        \"height\": 432,\n+                        \"vcodec\": \"avc1.64001e\",\n+                    },\n+                    {\n+                        \"format_id\": \"1295\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v4/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 768,\n+                        \"height\": 432,\n+                        \"vcodec\": \"avc1.64001e\",\n+                    },\n+                    {\n+                        \"format_id\": \"1487\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v4/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 768,\n+                        \"height\": 432,\n+                        \"vcodec\": \"avc1.64001e\",\n+                    },\n+                    {\n+                        \"format_id\": \"2168\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v5/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 960,\n+                        \"height\": 540,\n+                        \"vcodec\": \"avc1.640020\",\n+                    },\n+                    {\n+                        \"format_id\": \"2198\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v5/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 960,\n+                        \"height\": 540,\n+                        \"vcodec\": \"avc1.640020\",\n+                    },\n+                    {\n+                        \"format_id\": \"2390\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v5/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 960,\n+                        \"height\": 540,\n+                        \"vcodec\": \"avc1.640020\",\n+                    },\n+                    {\n+                        \"format_id\": \"3168\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v6/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                        \"vcodec\": \"avc1.640020\",\n+                    },\n+                    {\n+                        \"format_id\": \"3199\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v6/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                        \"vcodec\": \"avc1.640020\",\n+                    },\n+                    {\n+                        \"format_id\": \"3391\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v6/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                        \"vcodec\": \"avc1.640020\",\n+                    },\n+                    {\n+                        \"format_id\": \"4670\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v7/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"4701\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v7/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"4893\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v7/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"6170\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v8/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"6200\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v8/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"6392\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v8/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"7968\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v9/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"7998\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v9/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                    {\n+                        \"format_id\": \"8190\",\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/v9/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/img_bipbop_adv_example_fmp4/master.m3u8\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.64002a\",\n+                    },\n+                ],\n+                {},\n             ),\n             (\n-                'bipbop_16x9',\n-                'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                [{\n-                    'format_id': 'bipbop_audio-BipBop Audio 2',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/alternate_audio_aac/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'language': 'eng',\n-                    'ext': 'mp4',\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'vcodec': 'none',\n-                    'audio_ext': 'mp4',\n-                    'video_ext': 'none',\n-                }, {\n-                    'format_id': '41',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear0/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'tbr': 41.457,\n-                    'ext': 'mp4',\n-                    'fps': None,\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'vcodec': 'none',\n-                    'acodec': 'mp4a.40.2',\n-                    'audio_ext': 'mp4',\n-                    'video_ext': 'none',\n-                    'abr': 41.457,\n-                }, {\n-                    'format_id': '263',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear1/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'tbr': 263.851,\n-                    'ext': 'mp4',\n-                    'fps': None,\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'width': 416,\n-                    'height': 234,\n-                    'vcodec': 'avc1.4d400d',\n-                    'acodec': 'mp4a.40.2',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                }, {\n-                    'format_id': '577',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear2/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'tbr': 577.61,\n-                    'ext': 'mp4',\n-                    'fps': None,\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'width': 640,\n-                    'height': 360,\n-                    'vcodec': 'avc1.4d401e',\n-                    'acodec': 'mp4a.40.2',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                }, {\n-                    'format_id': '915',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear3/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'tbr': 915.905,\n-                    'ext': 'mp4',\n-                    'fps': None,\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'width': 960,\n-                    'height': 540,\n-                    'vcodec': 'avc1.4d401f',\n-                    'acodec': 'mp4a.40.2',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                }, {\n-                    'format_id': '1030',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear4/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'tbr': 1030.138,\n-                    'ext': 'mp4',\n-                    'fps': None,\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'width': 1280,\n-                    'height': 720,\n-                    'vcodec': 'avc1.4d401f',\n-                    'acodec': 'mp4a.40.2',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                }, {\n-                    'format_id': '1924',\n-                    'format_index': None,\n-                    'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear5/prog_index.m3u8',\n-                    'manifest_url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8',\n-                    'tbr': 1924.009,\n-                    'ext': 'mp4',\n-                    'fps': None,\n-                    'protocol': 'm3u8_native',\n-                    'preference': None,\n-                    'quality': None,\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'vcodec': 'avc1.4d401f',\n-                    'acodec': 'mp4a.40.2',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                }],\n+                \"bipbop_16x9\",\n+                \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                [\n+                    {\n+                        \"format_id\": \"bipbop_audio-BipBop Audio 2\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/alternate_audio_aac/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"language\": \"eng\",\n+                        \"ext\": \"mp4\",\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"vcodec\": \"none\",\n+                        \"audio_ext\": \"mp4\",\n+                        \"video_ext\": \"none\",\n+                    },\n+                    {\n+                        \"format_id\": \"41\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear0/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"tbr\": 41.457,\n+                        \"ext\": \"mp4\",\n+                        \"fps\": None,\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"vcodec\": \"none\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"audio_ext\": \"mp4\",\n+                        \"video_ext\": \"none\",\n+                        \"abr\": 41.457,\n+                    },\n+                    {\n+                        \"format_id\": \"263\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear1/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"tbr\": 263.851,\n+                        \"ext\": \"mp4\",\n+                        \"fps\": None,\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"width\": 416,\n+                        \"height\": 234,\n+                        \"vcodec\": \"avc1.4d400d\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                    },\n+                    {\n+                        \"format_id\": \"577\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear2/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"tbr\": 577.61,\n+                        \"ext\": \"mp4\",\n+                        \"fps\": None,\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                        \"vcodec\": \"avc1.4d401e\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                    },\n+                    {\n+                        \"format_id\": \"915\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear3/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"tbr\": 915.905,\n+                        \"ext\": \"mp4\",\n+                        \"fps\": None,\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"width\": 960,\n+                        \"height\": 540,\n+                        \"vcodec\": \"avc1.4d401f\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                    },\n+                    {\n+                        \"format_id\": \"1030\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear4/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"tbr\": 1030.138,\n+                        \"ext\": \"mp4\",\n+                        \"fps\": None,\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                        \"vcodec\": \"avc1.4d401f\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                    },\n+                    {\n+                        \"format_id\": \"1924\",\n+                        \"format_index\": None,\n+                        \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/gear5/prog_index.m3u8\",\n+                        \"manifest_url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/bipbop_16x9_variant.m3u8\",\n+                        \"tbr\": 1924.009,\n+                        \"ext\": \"mp4\",\n+                        \"fps\": None,\n+                        \"protocol\": \"m3u8_native\",\n+                        \"preference\": None,\n+                        \"quality\": None,\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"vcodec\": \"avc1.4d401f\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                    },\n+                ],\n                 {\n-                    'en': [{\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/eng/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }, {\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/eng_forced/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }],\n-                    'fr': [{\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/fra/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }, {\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/fra_forced/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }],\n-                    'es': [{\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/spa/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }, {\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/spa_forced/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }],\n-                    'ja': [{\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/jpn/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }, {\n-                        'url': 'https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/jpn_forced/prog_index.m3u8',\n-                        'ext': 'vtt',\n-                        'protocol': 'm3u8_native'\n-                    }],\n-                }\n+                    \"en\": [\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/eng/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/eng_forced/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                    ],\n+                    \"fr\": [\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/fra/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/fra_forced/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                    ],\n+                    \"es\": [\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/spa/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/spa_forced/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                    ],\n+                    \"ja\": [\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/jpn/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                        {\n+                            \"url\": \"https://devstreaming-cdn.apple.com/videos/streaming/examples/bipbop_16x9/subtitles/jpn_forced/prog_index.m3u8\",\n+                            \"ext\": \"vtt\",\n+                            \"protocol\": \"m3u8_native\",\n+                        },\n+                    ],\n+                },\n             ),\n         ]\n \n         for m3u8_file, m3u8_url, expected_formats, expected_subs in _TEST_CASES:\n-            with open('./test/testdata/m3u8/%s.m3u8' % m3u8_file, encoding='utf-8') as f:\n+            with open(\n+                \"./test/testdata/m3u8/%s.m3u8\" % m3u8_file, encoding=\"utf-8\"\n+            ) as f:\n                 formats, subs = self.ie._parse_m3u8_formats_and_subtitles(\n-                    f.read(), m3u8_url, ext='mp4')\n+                    f.read(), m3u8_url, ext=\"mp4\"\n+                )\n                 self.ie._sort_formats(formats)\n                 expect_value(self, formats, expected_formats, None)\n                 expect_value(self, subs, expected_subs, None)\n@@ -1041,342 +1224,379 @@ def test_parse_mpd_formats(self):\n                 # https://github.com/ytdl-org/youtube-dl/issues/13919\n                 # Also tests duplicate representation ids, see\n                 # https://github.com/ytdl-org/youtube-dl/issues/15111\n-                'float_duration',\n-                'http://unknown/manifest.mpd',  # mpd_url\n+                \"float_duration\",\n+                \"http://unknown/manifest.mpd\",  # mpd_url\n                 None,  # mpd_base_url\n-                [{\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'm4a',\n-                    'format_id': '318597',\n-                    'format_note': 'DASH audio',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'none',\n-                    'tbr': 61.587,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': '318597',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.42001f',\n-                    'tbr': 318.597,\n-                    'width': 340,\n-                    'height': 192,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': '638590',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.42001f',\n-                    'tbr': 638.59,\n-                    'width': 512,\n-                    'height': 288,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': '1022565',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.4d001f',\n-                    'tbr': 1022.565,\n-                    'width': 688,\n-                    'height': 384,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': '2046506',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.4d001f',\n-                    'tbr': 2046.506,\n-                    'width': 1024,\n-                    'height': 576,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': '3998017',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.640029',\n-                    'tbr': 3998.017,\n-                    'width': 1280,\n-                    'height': 720,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': '5997485',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.640032',\n-                    'tbr': 5997.485,\n-                    'width': 1920,\n-                    'height': 1080,\n-                }],\n+                [\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"m4a\",\n+                        \"format_id\": \"318597\",\n+                        \"format_note\": \"DASH audio\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"none\",\n+                        \"tbr\": 61.587,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"318597\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.42001f\",\n+                        \"tbr\": 318.597,\n+                        \"width\": 340,\n+                        \"height\": 192,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"638590\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.42001f\",\n+                        \"tbr\": 638.59,\n+                        \"width\": 512,\n+                        \"height\": 288,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"1022565\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.4d001f\",\n+                        \"tbr\": 1022.565,\n+                        \"width\": 688,\n+                        \"height\": 384,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"2046506\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.4d001f\",\n+                        \"tbr\": 2046.506,\n+                        \"width\": 1024,\n+                        \"height\": 576,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"3998017\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.640029\",\n+                        \"tbr\": 3998.017,\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"5997485\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.640032\",\n+                        \"tbr\": 5997.485,\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                    },\n+                ],\n                 {},\n-            ), (\n+            ),\n+            (\n                 # https://github.com/ytdl-org/youtube-dl/pull/14844\n-                'urls_only',\n-                'http://unknown/manifest.mpd',  # mpd_url\n+                \"urls_only\",\n+                \"http://unknown/manifest.mpd\",  # mpd_url\n                 None,  # mpd_base_url\n-                [{\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_144p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 200,\n-                    'width': 256,\n-                    'height': 144,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_240p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 400,\n-                    'width': 424,\n-                    'height': 240,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_360p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 800,\n-                    'width': 640,\n-                    'height': 360,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_480p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 1200,\n-                    'width': 856,\n-                    'height': 480,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_576p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 1600,\n-                    'width': 1024,\n-                    'height': 576,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_720p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 2400,\n-                    'width': 1280,\n-                    'height': 720,\n-                }, {\n-                    'manifest_url': 'http://unknown/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'h264_aac_1080p_m4s',\n-                    'format_note': 'DASH video',\n-                    'protocol': 'http_dash_segments',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'avc3.42c01e',\n-                    'tbr': 4400,\n-                    'width': 1920,\n-                    'height': 1080,\n-                }],\n+                [\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_144p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 200,\n+                        \"width\": 256,\n+                        \"height\": 144,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_240p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 400,\n+                        \"width\": 424,\n+                        \"height\": 240,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_360p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 800,\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_480p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 1200,\n+                        \"width\": 856,\n+                        \"height\": 480,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_576p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 1600,\n+                        \"width\": 1024,\n+                        \"height\": 576,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_720p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 2400,\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                    },\n+                    {\n+                        \"manifest_url\": \"http://unknown/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"h264_aac_1080p_m4s\",\n+                        \"format_note\": \"DASH video\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"avc3.42c01e\",\n+                        \"tbr\": 4400,\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                    },\n+                ],\n                 {},\n-            ), (\n+            ),\n+            (\n                 # https://github.com/ytdl-org/youtube-dl/issues/20346\n                 # Media considered unfragmented even though it contains\n                 # Initialization tag\n-                'unfragmented',\n-                'https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd',  # mpd_url\n-                'https://v.redd.it/hw1x7rcg7zl21',  # mpd_base_url\n-                [{\n-                    'url': 'https://v.redd.it/hw1x7rcg7zl21/audio',\n-                    'manifest_url': 'https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd',\n-                    'ext': 'm4a',\n-                    'format_id': 'AUDIO-1',\n-                    'format_note': 'DASH audio',\n-                    'container': 'm4a_dash',\n-                    'acodec': 'mp4a.40.2',\n-                    'vcodec': 'none',\n-                    'tbr': 129.87,\n-                    'asr': 48000,\n-\n-                }, {\n-                    'url': 'https://v.redd.it/hw1x7rcg7zl21/DASH_240',\n-                    'manifest_url': 'https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'VIDEO-2',\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.4d401e',\n-                    'tbr': 608.0,\n-                    'width': 240,\n-                    'height': 240,\n-                    'fps': 30,\n-                }, {\n-                    'url': 'https://v.redd.it/hw1x7rcg7zl21/DASH_360',\n-                    'manifest_url': 'https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd',\n-                    'ext': 'mp4',\n-                    'format_id': 'VIDEO-1',\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'acodec': 'none',\n-                    'vcodec': 'avc1.4d401e',\n-                    'tbr': 804.261,\n-                    'width': 360,\n-                    'height': 360,\n-                    'fps': 30,\n-                }],\n+                \"unfragmented\",\n+                \"https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd\",  # mpd_url\n+                \"https://v.redd.it/hw1x7rcg7zl21\",  # mpd_base_url\n+                [\n+                    {\n+                        \"url\": \"https://v.redd.it/hw1x7rcg7zl21/audio\",\n+                        \"manifest_url\": \"https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd\",\n+                        \"ext\": \"m4a\",\n+                        \"format_id\": \"AUDIO-1\",\n+                        \"format_note\": \"DASH audio\",\n+                        \"container\": \"m4a_dash\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"vcodec\": \"none\",\n+                        \"tbr\": 129.87,\n+                        \"asr\": 48000,\n+                    },\n+                    {\n+                        \"url\": \"https://v.redd.it/hw1x7rcg7zl21/DASH_240\",\n+                        \"manifest_url\": \"https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"VIDEO-2\",\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.4d401e\",\n+                        \"tbr\": 608.0,\n+                        \"width\": 240,\n+                        \"height\": 240,\n+                        \"fps\": 30,\n+                    },\n+                    {\n+                        \"url\": \"https://v.redd.it/hw1x7rcg7zl21/DASH_360\",\n+                        \"manifest_url\": \"https://v.redd.it/hw1x7rcg7zl21/DASHPlaylist.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"format_id\": \"VIDEO-1\",\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"acodec\": \"none\",\n+                        \"vcodec\": \"avc1.4d401e\",\n+                        \"tbr\": 804.261,\n+                        \"width\": 360,\n+                        \"height\": 360,\n+                        \"fps\": 30,\n+                    },\n+                ],\n                 {},\n-            ), (\n-                'subtitles',\n-                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/',\n-                [{\n-                    'format_id': 'audio=128001',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'ext': 'm4a',\n-                    'tbr': 128.001,\n-                    'asr': 48000,\n-                    'format_note': 'DASH audio',\n-                    'container': 'm4a_dash',\n-                    'vcodec': 'none',\n-                    'acodec': 'mp4a.40.2',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                    'protocol': 'http_dash_segments',\n-                    'audio_ext': 'm4a',\n-                    'video_ext': 'none',\n-                    'abr': 128.001,\n-                }, {\n-                    'format_id': 'video=100000',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'width': 336,\n-                    'height': 144,\n-                    'tbr': 100,\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'vcodec': 'avc1.4D401F',\n-                    'acodec': 'none',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                    'protocol': 'http_dash_segments',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                    'vbr': 100,\n-                }, {\n-                    'format_id': 'video=326000',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'width': 562,\n-                    'height': 240,\n-                    'tbr': 326,\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'vcodec': 'avc1.4D401F',\n-                    'acodec': 'none',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                    'protocol': 'http_dash_segments',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                    'vbr': 326,\n-                }, {\n-                    'format_id': 'video=698000',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'width': 844,\n-                    'height': 360,\n-                    'tbr': 698,\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'vcodec': 'avc1.4D401F',\n-                    'acodec': 'none',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                    'protocol': 'http_dash_segments',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                    'vbr': 698,\n-                }, {\n-                    'format_id': 'video=1493000',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'width': 1126,\n-                    'height': 480,\n-                    'tbr': 1493,\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'vcodec': 'avc1.4D401F',\n-                    'acodec': 'none',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                    'protocol': 'http_dash_segments',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                    'vbr': 1493,\n-                }, {\n-                    'format_id': 'video=4482000',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'ext': 'mp4',\n-                    'width': 1688,\n-                    'height': 720,\n-                    'tbr': 4482,\n-                    'format_note': 'DASH video',\n-                    'container': 'mp4_dash',\n-                    'vcodec': 'avc1.4D401F',\n-                    'acodec': 'none',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                    'protocol': 'http_dash_segments',\n-                    'video_ext': 'mp4',\n-                    'audio_ext': 'none',\n-                    'vbr': 4482,\n-                }],\n+            ),\n+            (\n+                \"subtitles\",\n+                \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/\",\n+                [\n+                    {\n+                        \"format_id\": \"audio=128001\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"ext\": \"m4a\",\n+                        \"tbr\": 128.001,\n+                        \"asr\": 48000,\n+                        \"format_note\": \"DASH audio\",\n+                        \"container\": \"m4a_dash\",\n+                        \"vcodec\": \"none\",\n+                        \"acodec\": \"mp4a.40.2\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"audio_ext\": \"m4a\",\n+                        \"video_ext\": \"none\",\n+                        \"abr\": 128.001,\n+                    },\n+                    {\n+                        \"format_id\": \"video=100000\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 336,\n+                        \"height\": 144,\n+                        \"tbr\": 100,\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"vcodec\": \"avc1.4D401F\",\n+                        \"acodec\": \"none\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                        \"vbr\": 100,\n+                    },\n+                    {\n+                        \"format_id\": \"video=326000\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 562,\n+                        \"height\": 240,\n+                        \"tbr\": 326,\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"vcodec\": \"avc1.4D401F\",\n+                        \"acodec\": \"none\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                        \"vbr\": 326,\n+                    },\n+                    {\n+                        \"format_id\": \"video=698000\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 844,\n+                        \"height\": 360,\n+                        \"tbr\": 698,\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"vcodec\": \"avc1.4D401F\",\n+                        \"acodec\": \"none\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                        \"vbr\": 698,\n+                    },\n+                    {\n+                        \"format_id\": \"video=1493000\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 1126,\n+                        \"height\": 480,\n+                        \"tbr\": 1493,\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"vcodec\": \"avc1.4D401F\",\n+                        \"acodec\": \"none\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                        \"vbr\": 1493,\n+                    },\n+                    {\n+                        \"format_id\": \"video=4482000\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"ext\": \"mp4\",\n+                        \"width\": 1688,\n+                        \"height\": 720,\n+                        \"tbr\": 4482,\n+                        \"format_note\": \"DASH video\",\n+                        \"container\": \"mp4_dash\",\n+                        \"vcodec\": \"avc1.4D401F\",\n+                        \"acodec\": \"none\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                        \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                        \"protocol\": \"http_dash_segments\",\n+                        \"video_ext\": \"mp4\",\n+                        \"audio_ext\": \"none\",\n+                        \"vbr\": 4482,\n+                    },\n+                ],\n                 {\n-                    'en': [\n+                    \"en\": [\n                         {\n-                            'ext': 'mp4',\n-                            'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                            'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n-                            'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n-                            'protocol': 'http_dash_segments',\n+                            \"ext\": \"mp4\",\n+                            \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                            \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd\",\n+                            \"fragment_base_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/\",\n+                            \"protocol\": \"http_dash_segments\",\n                         }\n                     ]\n                 },\n-            )\n+            ),\n         ]\n \n-        for mpd_file, mpd_url, mpd_base_url, expected_formats, expected_subtitles in _TEST_CASES:\n-            with open('./test/testdata/mpd/%s.mpd' % mpd_file, encoding='utf-8') as f:\n+        for (\n+            mpd_file,\n+            mpd_url,\n+            mpd_base_url,\n+            expected_formats,\n+            expected_subtitles,\n+        ) in _TEST_CASES:\n+            with open(\"./test/testdata/mpd/%s.mpd\" % mpd_file, encoding=\"utf-8\") as f:\n                 formats, subtitles = self.ie._parse_mpd_formats_and_subtitles(\n                     compat_etree_fromstring(f.read().encode()),\n-                    mpd_base_url=mpd_base_url, mpd_url=mpd_url)\n+                    mpd_base_url=mpd_base_url,\n+                    mpd_url=mpd_url,\n+                )\n                 self.ie._sort_formats(formats)\n                 expect_value(self, formats, expected_formats, None)\n                 expect_value(self, subtitles, expected_subtitles, None)\n@@ -1384,431 +1604,452 @@ def test_parse_mpd_formats(self):\n     def test_parse_ism_formats(self):\n         _TEST_CASES = [\n             (\n-                'sintel',\n-                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                [{\n-                    'format_id': 'audio-128',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'ext': 'isma',\n-                    'tbr': 128,\n-                    'asr': 48000,\n-                    'vcodec': 'none',\n-                    'acodec': 'AACL',\n-                    'protocol': 'ism',\n-                    'audio_channels': 2,\n-                    '_download_params': {\n-                        'stream_type': 'audio',\n-                        'duration': 8880746666,\n-                        'timescale': 10000000,\n-                        'width': 0,\n-                        'height': 0,\n-                        'fourcc': 'AACL',\n-                        'codec_private_data': '1190',\n-                        'sampling_rate': 48000,\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video-100',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 336,\n-                    'height': 144,\n-                    'tbr': 100,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 8880746666,\n-                        'timescale': 10000000,\n-                        'width': 336,\n-                        'height': 144,\n-                        'fourcc': 'AVC1',\n-                        'codec_private_data': '00000001674D401FDA0544EFFC2D002CBC40000003004000000C03C60CA80000000168EF32C8',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video-326',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 562,\n-                    'height': 240,\n-                    'tbr': 326,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 8880746666,\n-                        'timescale': 10000000,\n-                        'width': 562,\n-                        'height': 240,\n-                        'fourcc': 'AVC1',\n-                        'codec_private_data': '00000001674D401FDA0241FE23FFC3BC83BA44000003000400000300C03C60CA800000000168EF32C8',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video-698',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 844,\n-                    'height': 360,\n-                    'tbr': 698,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 8880746666,\n-                        'timescale': 10000000,\n-                        'width': 844,\n-                        'height': 360,\n-                        'fourcc': 'AVC1',\n-                        'codec_private_data': '00000001674D401FDA0350BFB97FF06AF06AD1000003000100000300300F1832A00000000168EF32C8',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video-1493',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 1126,\n-                    'height': 480,\n-                    'tbr': 1493,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 8880746666,\n-                        'timescale': 10000000,\n-                        'width': 1126,\n-                        'height': 480,\n-                        'fourcc': 'AVC1',\n-                        'codec_private_data': '00000001674D401FDA011C3DE6FFF0D890D871000003000100000300300F1832A00000000168EF32C8',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video-4482',\n-                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 1688,\n-                    'height': 720,\n-                    'tbr': 4482,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 8880746666,\n-                        'timescale': 10000000,\n-                        'width': 1688,\n-                        'height': 720,\n-                        'fourcc': 'AVC1',\n-                        'codec_private_data': '00000001674D401FDA01A816F97FFC1ABC1AB440000003004000000C03C60CA80000000168EF32C8',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }],\n+                \"sintel\",\n+                \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                [\n+                    {\n+                        \"format_id\": \"audio-128\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"ext\": \"isma\",\n+                        \"tbr\": 128,\n+                        \"asr\": 48000,\n+                        \"vcodec\": \"none\",\n+                        \"acodec\": \"AACL\",\n+                        \"protocol\": \"ism\",\n+                        \"audio_channels\": 2,\n+                        \"_download_params\": {\n+                            \"stream_type\": \"audio\",\n+                            \"duration\": 8880746666,\n+                            \"timescale\": 10000000,\n+                            \"width\": 0,\n+                            \"height\": 0,\n+                            \"fourcc\": \"AACL\",\n+                            \"codec_private_data\": \"1190\",\n+                            \"sampling_rate\": 48000,\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video-100\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 336,\n+                        \"height\": 144,\n+                        \"tbr\": 100,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 8880746666,\n+                            \"timescale\": 10000000,\n+                            \"width\": 336,\n+                            \"height\": 144,\n+                            \"fourcc\": \"AVC1\",\n+                            \"codec_private_data\": \"00000001674D401FDA0544EFFC2D002CBC40000003004000000C03C60CA80000000168EF32C8\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video-326\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 562,\n+                        \"height\": 240,\n+                        \"tbr\": 326,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 8880746666,\n+                            \"timescale\": 10000000,\n+                            \"width\": 562,\n+                            \"height\": 240,\n+                            \"fourcc\": \"AVC1\",\n+                            \"codec_private_data\": \"00000001674D401FDA0241FE23FFC3BC83BA44000003000400000300C03C60CA800000000168EF32C8\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video-698\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 844,\n+                        \"height\": 360,\n+                        \"tbr\": 698,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 8880746666,\n+                            \"timescale\": 10000000,\n+                            \"width\": 844,\n+                            \"height\": 360,\n+                            \"fourcc\": \"AVC1\",\n+                            \"codec_private_data\": \"00000001674D401FDA0350BFB97FF06AF06AD1000003000100000300300F1832A00000000168EF32C8\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video-1493\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 1126,\n+                        \"height\": 480,\n+                        \"tbr\": 1493,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 8880746666,\n+                            \"timescale\": 10000000,\n+                            \"width\": 1126,\n+                            \"height\": 480,\n+                            \"fourcc\": \"AVC1\",\n+                            \"codec_private_data\": \"00000001674D401FDA011C3DE6FFF0D890D871000003000100000300300F1832A00000000168EF32C8\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video-4482\",\n+                        \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 1688,\n+                        \"height\": 720,\n+                        \"tbr\": 4482,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 8880746666,\n+                            \"timescale\": 10000000,\n+                            \"width\": 1688,\n+                            \"height\": 720,\n+                            \"fourcc\": \"AVC1\",\n+                            \"codec_private_data\": \"00000001674D401FDA01A816F97FFC1ABC1AB440000003004000000C03C60CA80000000168EF32C8\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                ],\n                 {\n-                    'eng': [\n+                    \"eng\": [\n                         {\n-                            'ext': 'ismt',\n-                            'protocol': 'ism',\n-                            'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                            'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest',\n-                            '_download_params': {\n-                                'stream_type': 'text',\n-                                'duration': 8880746666,\n-                                'timescale': 10000000,\n-                                'fourcc': 'TTML',\n-                                'codec_private_data': ''\n-                            }\n+                            \"ext\": \"ismt\",\n+                            \"protocol\": \"ism\",\n+                            \"url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                            \"manifest_url\": \"https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/Manifest\",\n+                            \"_download_params\": {\n+                                \"stream_type\": \"text\",\n+                                \"duration\": 8880746666,\n+                                \"timescale\": 10000000,\n+                                \"fourcc\": \"TTML\",\n+                                \"codec_private_data\": \"\",\n+                            },\n                         }\n                     ]\n                 },\n             ),\n             (\n-                'ec-3_test',\n-                'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                [{\n-                    'format_id': 'audio_deu-127',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'isma',\n-                    'tbr': 127,\n-                    'asr': 48000,\n-                    'vcodec': 'none',\n-                    'acodec': 'AACL',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    'audio_channels': 2,\n-                    '_download_params': {\n-                        'stream_type': 'audio',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 0,\n-                        'height': 0,\n-                        'fourcc': 'AACL',\n-                        'language': 'deu',\n-                        'codec_private_data': '1190',\n-                        'sampling_rate': 48000,\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'audio_deu_1-224',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'isma',\n-                    'tbr': 224,\n-                    'asr': 48000,\n-                    'vcodec': 'none',\n-                    'acodec': 'EC-3',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    'audio_channels': 6,\n-                    '_download_params': {\n-                        'stream_type': 'audio',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 0,\n-                        'height': 0,\n-                        'fourcc': 'EC-3',\n-                        'language': 'deu',\n-                        'codec_private_data': '00063F000000AF87FBA7022DFB42A4D405CD93843BDD0700200F00',\n-                        'sampling_rate': 48000,\n-                        'channels': 6,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-23',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 384,\n-                    'height': 216,\n-                    'tbr': 23,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 384,\n-                        'height': 216,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '000000016742C00CDB06077E5C05A808080A00000300020000030009C0C02EE0177CC6300F142AE00000000168CA8DC8',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-403',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 400,\n-                    'height': 224,\n-                    'tbr': 403,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 400,\n-                        'height': 224,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D4014E98323B602D4040405000003000100000300320F1429380000000168EAECF2',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-680',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 640,\n-                    'height': 360,\n-                    'tbr': 680,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 640,\n-                        'height': 360,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D401EE981405FF2E02D4040405000000300100000030320F162D3800000000168EAECF2',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-1253',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 640,\n-                    'height': 360,\n-                    'tbr': 1253,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'vbr': 1253,\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 640,\n-                        'height': 360,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D401EE981405FF2E02D4040405000000300100000030320F162D3800000000168EAECF2',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-2121',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 768,\n-                    'height': 432,\n-                    'tbr': 2121,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 768,\n-                        'height': 432,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D401EECA0601BD80B50101014000003000400000300C83C58B6580000000168E93B3C80',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-3275',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 1280,\n-                    'height': 720,\n-                    'tbr': 3275,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 1280,\n-                        'height': 720,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D4020ECA02802DD80B501010140000003004000000C83C60C65800000000168E93B3C80',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-5300',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'tbr': 5300,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 1920,\n-                        'height': 1080,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D4028ECA03C0113F2E02D4040405000000300100000030320F18319600000000168E93B3C80',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }, {\n-                    'format_id': 'video_deu-8079',\n-                    'url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'manifest_url': 'https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest',\n-                    'ext': 'ismv',\n-                    'width': 1920,\n-                    'height': 1080,\n-                    'tbr': 8079,\n-                    'vcodec': 'AVC1',\n-                    'acodec': 'none',\n-                    'protocol': 'ism',\n-                    'language': 'deu',\n-                    '_download_params': {\n-                        'stream_type': 'video',\n-                        'duration': 370000000,\n-                        'timescale': 10000000,\n-                        'width': 1920,\n-                        'height': 1080,\n-                        'fourcc': 'AVC1',\n-                        'language': 'deu',\n-                        'codec_private_data': '00000001674D4028ECA03C0113F2E02D4040405000000300100000030320F18319600000000168E93B3C80',\n-                        'channels': 2,\n-                        'bits_per_sample': 16,\n-                        'nal_unit_length_field': 4\n-                    },\n-                }],\n+                \"ec-3_test\",\n+                \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                [\n+                    {\n+                        \"format_id\": \"audio_deu-127\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"isma\",\n+                        \"tbr\": 127,\n+                        \"asr\": 48000,\n+                        \"vcodec\": \"none\",\n+                        \"acodec\": \"AACL\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"audio_channels\": 2,\n+                        \"_download_params\": {\n+                            \"stream_type\": \"audio\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 0,\n+                            \"height\": 0,\n+                            \"fourcc\": \"AACL\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"1190\",\n+                            \"sampling_rate\": 48000,\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"audio_deu_1-224\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"isma\",\n+                        \"tbr\": 224,\n+                        \"asr\": 48000,\n+                        \"vcodec\": \"none\",\n+                        \"acodec\": \"EC-3\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"audio_channels\": 6,\n+                        \"_download_params\": {\n+                            \"stream_type\": \"audio\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 0,\n+                            \"height\": 0,\n+                            \"fourcc\": \"EC-3\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00063F000000AF87FBA7022DFB42A4D405CD93843BDD0700200F00\",\n+                            \"sampling_rate\": 48000,\n+                            \"channels\": 6,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-23\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 384,\n+                        \"height\": 216,\n+                        \"tbr\": 23,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 384,\n+                            \"height\": 216,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"000000016742C00CDB06077E5C05A808080A00000300020000030009C0C02EE0177CC6300F142AE00000000168CA8DC8\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-403\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 400,\n+                        \"height\": 224,\n+                        \"tbr\": 403,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 400,\n+                            \"height\": 224,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D4014E98323B602D4040405000003000100000300320F1429380000000168EAECF2\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-680\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                        \"tbr\": 680,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 640,\n+                            \"height\": 360,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D401EE981405FF2E02D4040405000000300100000030320F162D3800000000168EAECF2\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-1253\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 640,\n+                        \"height\": 360,\n+                        \"tbr\": 1253,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"vbr\": 1253,\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 640,\n+                            \"height\": 360,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D401EE981405FF2E02D4040405000000300100000030320F162D3800000000168EAECF2\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-2121\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 768,\n+                        \"height\": 432,\n+                        \"tbr\": 2121,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 768,\n+                            \"height\": 432,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D401EECA0601BD80B50101014000003000400000300C83C58B6580000000168E93B3C80\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-3275\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                        \"tbr\": 3275,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 1280,\n+                            \"height\": 720,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D4020ECA02802DD80B501010140000003004000000C83C60C65800000000168E93B3C80\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-5300\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"tbr\": 5300,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 1920,\n+                            \"height\": 1080,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D4028ECA03C0113F2E02D4040405000000300100000030320F18319600000000168E93B3C80\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                    {\n+                        \"format_id\": \"video_deu-8079\",\n+                        \"url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"manifest_url\": \"https://smstr01.dmm.t-online.de/smooth24/smoothstream_m1/streaming/sony/9221438342941275747/636887760842957027/25_km_h-Trailer-9221571562372022953_deu_20_1300k_HD_H_264_ISMV.ism/Manifest\",\n+                        \"ext\": \"ismv\",\n+                        \"width\": 1920,\n+                        \"height\": 1080,\n+                        \"tbr\": 8079,\n+                        \"vcodec\": \"AVC1\",\n+                        \"acodec\": \"none\",\n+                        \"protocol\": \"ism\",\n+                        \"language\": \"deu\",\n+                        \"_download_params\": {\n+                            \"stream_type\": \"video\",\n+                            \"duration\": 370000000,\n+                            \"timescale\": 10000000,\n+                            \"width\": 1920,\n+                            \"height\": 1080,\n+                            \"fourcc\": \"AVC1\",\n+                            \"language\": \"deu\",\n+                            \"codec_private_data\": \"00000001674D4028ECA03C0113F2E02D4040405000000300100000030320F18319600000000168E93B3C80\",\n+                            \"channels\": 2,\n+                            \"bits_per_sample\": 16,\n+                            \"nal_unit_length_field\": 4,\n+                        },\n+                    },\n+                ],\n                 {},\n             ),\n         ]\n \n         for ism_file, ism_url, expected_formats, expected_subtitles in _TEST_CASES:\n-            with open('./test/testdata/ism/%s.Manifest' % ism_file, encoding='utf-8') as f:\n+            with open(\n+                \"./test/testdata/ism/%s.Manifest\" % ism_file, encoding=\"utf-8\"\n+            ) as f:\n                 formats, subtitles = self.ie._parse_ism_formats_and_subtitles(\n-                    compat_etree_fromstring(f.read().encode()), ism_url=ism_url)\n+                    compat_etree_fromstring(f.read().encode()), ism_url=ism_url\n+                )\n                 self.ie._sort_formats(formats)\n                 expect_value(self, formats, expected_formats, None)\n                 expect_value(self, subtitles, expected_subtitles, None)\n@@ -1817,72 +2058,90 @@ def test_parse_f4m_formats(self):\n         _TEST_CASES = [\n             (\n                 # https://github.com/ytdl-org/youtube-dl/issues/14660\n-                'custom_base_url',\n-                'http://api.new.livestream.com/accounts/6115179/events/6764928/videos/144884262.f4m',\n-                [{\n-                    'manifest_url': 'http://api.new.livestream.com/accounts/6115179/events/6764928/videos/144884262.f4m',\n-                    'ext': 'flv',\n-                    'format_id': '2148',\n-                    'protocol': 'f4m',\n-                    'tbr': 2148,\n-                    'width': 1280,\n-                    'height': 720,\n-                }]\n+                \"custom_base_url\",\n+                \"http://api.new.livestream.com/accounts/6115179/events/6764928/videos/144884262.f4m\",\n+                [\n+                    {\n+                        \"manifest_url\": \"http://api.new.livestream.com/accounts/6115179/events/6764928/videos/144884262.f4m\",\n+                        \"ext\": \"flv\",\n+                        \"format_id\": \"2148\",\n+                        \"protocol\": \"f4m\",\n+                        \"tbr\": 2148,\n+                        \"width\": 1280,\n+                        \"height\": 720,\n+                    }\n+                ],\n             ),\n         ]\n \n         for f4m_file, f4m_url, expected_formats in _TEST_CASES:\n-            with open('./test/testdata/f4m/%s.f4m' % f4m_file, encoding='utf-8') as f:\n+            with open(\"./test/testdata/f4m/%s.f4m\" % f4m_file, encoding=\"utf-8\") as f:\n                 formats = self.ie._parse_f4m_formats(\n-                    compat_etree_fromstring(f.read().encode()),\n-                    f4m_url, None)\n+                    compat_etree_fromstring(f.read().encode()), f4m_url, None\n+                )\n                 self.ie._sort_formats(formats)\n                 expect_value(self, formats, expected_formats, None)\n \n     def test_parse_xspf(self):\n         _TEST_CASES = [\n             (\n-                'foo_xspf',\n-                'https://example.org/src/foo_xspf.xspf',\n-                [{\n-                    'id': 'foo_xspf',\n-                    'title': 'Pandemonium',\n-                    'description': 'Visit http://bigbrother404.bandcamp.com',\n-                    'duration': 202.416,\n-                    'formats': [{\n-                        'manifest_url': 'https://example.org/src/foo_xspf.xspf',\n-                        'url': 'https://example.org/src/cd1/track%201.mp3',\n-                    }],\n-                }, {\n-                    'id': 'foo_xspf',\n-                    'title': 'Final Cartridge (Nichico Twelve Remix)',\n-                    'description': 'Visit http://bigbrother404.bandcamp.com',\n-                    'duration': 255.857,\n-                    'formats': [{\n-                        'manifest_url': 'https://example.org/src/foo_xspf.xspf',\n-                        'url': 'https://example.org/%E3%83%88%E3%83%A9%E3%83%83%E3%82%AF%E3%80%80%EF%BC%92.mp3',\n-                    }],\n-                }, {\n-                    'id': 'foo_xspf',\n-                    'title': 'Rebuilding Nightingale',\n-                    'description': 'Visit http://bigbrother404.bandcamp.com',\n-                    'duration': 287.915,\n-                    'formats': [{\n-                        'manifest_url': 'https://example.org/src/foo_xspf.xspf',\n-                        'url': 'https://example.org/src/track3.mp3',\n-                    }, {\n-                        'manifest_url': 'https://example.org/src/foo_xspf.xspf',\n-                        'url': 'https://example.com/track3.mp3',\n-                    }]\n-                }]\n+                \"foo_xspf\",\n+                \"https://example.org/src/foo_xspf.xspf\",\n+                [\n+                    {\n+                        \"id\": \"foo_xspf\",\n+                        \"title\": \"Pandemonium\",\n+                        \"description\": \"Visit http://bigbrother404.bandcamp.com\",\n+                        \"duration\": 202.416,\n+                        \"formats\": [\n+                            {\n+                                \"manifest_url\": \"https://example.org/src/foo_xspf.xspf\",\n+                                \"url\": \"https://example.org/src/cd1/track%201.mp3\",\n+                            }\n+                        ],\n+                    },\n+                    {\n+                        \"id\": \"foo_xspf\",\n+                        \"title\": \"Final Cartridge (Nichico Twelve Remix)\",\n+                        \"description\": \"Visit http://bigbrother404.bandcamp.com\",\n+                        \"duration\": 255.857,\n+                        \"formats\": [\n+                            {\n+                                \"manifest_url\": \"https://example.org/src/foo_xspf.xspf\",\n+                                \"url\": \"https://example.org/%E3%83%88%E3%83%A9%E3%83%83%E3%82%AF%E3%80%80%EF%BC%92.mp3\",\n+                            }\n+                        ],\n+                    },\n+                    {\n+                        \"id\": \"foo_xspf\",\n+                        \"title\": \"Rebuilding Nightingale\",\n+                        \"description\": \"Visit http://bigbrother404.bandcamp.com\",\n+                        \"duration\": 287.915,\n+                        \"formats\": [\n+                            {\n+                                \"manifest_url\": \"https://example.org/src/foo_xspf.xspf\",\n+                                \"url\": \"https://example.org/src/track3.mp3\",\n+                            },\n+                            {\n+                                \"manifest_url\": \"https://example.org/src/foo_xspf.xspf\",\n+                                \"url\": \"https://example.com/track3.mp3\",\n+                            },\n+                        ],\n+                    },\n+                ],\n             ),\n         ]\n \n         for xspf_file, xspf_url, expected_entries in _TEST_CASES:\n-            with open('./test/testdata/xspf/%s.xspf' % xspf_file, encoding='utf-8') as f:\n+            with open(\n+                \"./test/testdata/xspf/%s.xspf\" % xspf_file, encoding=\"utf-8\"\n+            ) as f:\n                 entries = self.ie._parse_xspf(\n                     compat_etree_fromstring(f.read().encode()),\n-                    xspf_file, xspf_url=xspf_url, xspf_base_url=xspf_url)\n+                    xspf_file,\n+                    xspf_url=xspf_url,\n+                    xspf_base_url=xspf_url,\n+                )\n                 expect_value(self, entries, expected_entries, None)\n                 for i in range(len(entries)):\n                     expect_dict(self, entries[i], expected_entries[i])\n@@ -1895,17 +2154,20 @@ def test_response_with_expected_status_returns_content(self):\n         # when a response matches `expected_status`.\n \n         httpd = http.server.HTTPServer(\n-            ('127.0.0.1', 0), InfoExtractorTestRequestHandler)\n+            (\"127.0.0.1\", 0), InfoExtractorTestRequestHandler\n+        )\n         port = http_server_port(httpd)\n         server_thread = threading.Thread(target=httpd.serve_forever)\n         server_thread.daemon = True\n         server_thread.start()\n \n         (content, urlh) = self.ie._download_webpage_handle(\n-            'http://127.0.0.1:%d/teapot' % port, None,\n-            expected_status=TEAPOT_RESPONSE_STATUS)\n+            \"http://127.0.0.1:%d/teapot\" % port,\n+            None,\n+            expected_status=TEAPOT_RESPONSE_STATUS,\n+        )\n         self.assertEqual(content, TEAPOT_RESPONSE_BODY)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_YoutubeDL.py b/test/test_YoutubeDL.py\nindex 0087cbc..c0b56d6 100644\n--- a/test/test_YoutubeDL.py\n+++ b/test/test_YoutubeDL.py\n@@ -26,7 +26,7 @@\n )\n from yt_dlp.utils.traversal import traverse_obj\n \n-TEST_URL = 'http://localhost/sample.mp4'\n+TEST_URL = \"http://localhost/sample.mp4\"\n \n \n class YDL(FakeYDL):\n@@ -42,17 +42,17 @@ def to_screen(self, msg, *args, **kwargs):\n         self.msgs.append(msg)\n \n     def dl(self, *args, **kwargs):\n-        assert False, 'Downloader must not be invoked for test_YoutubeDL'\n+        assert False, \"Downloader must not be invoked for test_YoutubeDL\"\n \n \n def _make_result(formats, **kwargs):\n     res = {\n-        'formats': formats,\n-        'id': 'testid',\n-        'title': 'testttitle',\n-        'extractor': 'testex',\n-        'extractor_key': 'TestEx',\n-        'webpage_url': 'http://example.com/watch?v=shenanigans',\n+        \"formats\": formats,\n+        \"id\": \"testid\",\n+        \"title\": \"testttitle\",\n+        \"extractor\": \"testex\",\n+        \"extractor_key\": \"TestEx\",\n+        \"webpage_url\": \"http://example.com/watch?v=shenanigans\",\n     }\n     res.update(**kwargs)\n     return res\n@@ -62,246 +62,320 @@ class TestFormatSelection(unittest.TestCase):\n     def test_prefer_free_formats(self):\n         # Same resolution => download webm\n         ydl = YDL()\n-        ydl.params['prefer_free_formats'] = True\n+        ydl.params[\"prefer_free_formats\"] = True\n         formats = [\n-            {'ext': 'webm', 'height': 460, 'url': TEST_URL},\n-            {'ext': 'mp4', 'height': 460, 'url': TEST_URL},\n+            {\"ext\": \"webm\", \"height\": 460, \"url\": TEST_URL},\n+            {\"ext\": \"mp4\", \"height\": 460, \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['ext'], 'webm')\n+        self.assertEqual(downloaded[\"ext\"], \"webm\")\n \n         # Different resolution => download best quality (mp4)\n         ydl = YDL()\n-        ydl.params['prefer_free_formats'] = True\n+        ydl.params[\"prefer_free_formats\"] = True\n         formats = [\n-            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n-            {'ext': 'mp4', 'height': 1080, 'url': TEST_URL},\n+            {\"ext\": \"webm\", \"height\": 720, \"url\": TEST_URL},\n+            {\"ext\": \"mp4\", \"height\": 1080, \"url\": TEST_URL},\n         ]\n-        info_dict['formats'] = formats\n+        info_dict[\"formats\"] = formats\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['ext'], 'mp4')\n+        self.assertEqual(downloaded[\"ext\"], \"mp4\")\n \n         # No prefer_free_formats => prefer mp4 and webm\n         ydl = YDL()\n-        ydl.params['prefer_free_formats'] = False\n+        ydl.params[\"prefer_free_formats\"] = False\n         formats = [\n-            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n-            {'ext': 'mp4', 'height': 720, 'url': TEST_URL},\n-            {'ext': 'flv', 'height': 720, 'url': TEST_URL},\n+            {\"ext\": \"webm\", \"height\": 720, \"url\": TEST_URL},\n+            {\"ext\": \"mp4\", \"height\": 720, \"url\": TEST_URL},\n+            {\"ext\": \"flv\", \"height\": 720, \"url\": TEST_URL},\n         ]\n-        info_dict['formats'] = formats\n+        info_dict[\"formats\"] = formats\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['ext'], 'mp4')\n+        self.assertEqual(downloaded[\"ext\"], \"mp4\")\n \n         ydl = YDL()\n-        ydl.params['prefer_free_formats'] = False\n+        ydl.params[\"prefer_free_formats\"] = False\n         formats = [\n-            {'ext': 'flv', 'height': 720, 'url': TEST_URL},\n-            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n+            {\"ext\": \"flv\", \"height\": 720, \"url\": TEST_URL},\n+            {\"ext\": \"webm\", \"height\": 720, \"url\": TEST_URL},\n         ]\n-        info_dict['formats'] = formats\n+        info_dict[\"formats\"] = formats\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['ext'], 'webm')\n+        self.assertEqual(downloaded[\"ext\"], \"webm\")\n \n     def test_format_selection(self):\n         formats = [\n-            {'format_id': '35', 'ext': 'mp4', 'preference': 0, 'url': TEST_URL},\n-            {'format_id': 'example-with-dashes', 'ext': 'webm', 'preference': 1, 'url': TEST_URL},\n-            {'format_id': '45', 'ext': 'webm', 'preference': 2, 'url': TEST_URL},\n-            {'format_id': '47', 'ext': 'webm', 'preference': 3, 'url': TEST_URL},\n-            {'format_id': '2', 'ext': 'flv', 'preference': 4, 'url': TEST_URL},\n+            {\"format_id\": \"35\", \"ext\": \"mp4\", \"preference\": 0, \"url\": TEST_URL},\n+            {\n+                \"format_id\": \"example-with-dashes\",\n+                \"ext\": \"webm\",\n+                \"preference\": 1,\n+                \"url\": TEST_URL,\n+            },\n+            {\"format_id\": \"45\", \"ext\": \"webm\", \"preference\": 2, \"url\": TEST_URL},\n+            {\"format_id\": \"47\", \"ext\": \"webm\", \"preference\": 3, \"url\": TEST_URL},\n+            {\"format_id\": \"2\", \"ext\": \"flv\", \"preference\": 4, \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n \n         def test(inp, *expected, multi=False):\n-            ydl = YDL({\n-                'format': inp,\n-                'allow_multiple_video_streams': multi,\n-                'allow_multiple_audio_streams': multi,\n-            })\n+            ydl = YDL(\n+                {\n+                    \"format\": inp,\n+                    \"allow_multiple_video_streams\": multi,\n+                    \"allow_multiple_audio_streams\": multi,\n+                }\n+            )\n             ydl.process_ie_result(info_dict.copy())\n-            downloaded = map(lambda x: x['format_id'], ydl.downloaded_info_dicts)\n+            downloaded = map(lambda x: x[\"format_id\"], ydl.downloaded_info_dicts)\n             self.assertEqual(list(downloaded), list(expected))\n \n-        test('20/47', '47')\n-        test('20/71/worst', '35')\n-        test(None, '2')\n-        test('webm/mp4', '47')\n-        test('3gp/40/mp4', '35')\n-        test('example-with-dashes', 'example-with-dashes')\n-        test('all', '2', '47', '45', 'example-with-dashes', '35')\n-        test('mergeall', '2+47+45+example-with-dashes+35', multi=True)\n+        test(\"20/47\", \"47\")\n+        test(\"20/71/worst\", \"35\")\n+        test(None, \"2\")\n+        test(\"webm/mp4\", \"47\")\n+        test(\"3gp/40/mp4\", \"35\")\n+        test(\"example-with-dashes\", \"example-with-dashes\")\n+        test(\"all\", \"2\", \"47\", \"45\", \"example-with-dashes\", \"35\")\n+        test(\"mergeall\", \"2+47+45+example-with-dashes+35\", multi=True)\n         # See: https://github.com/yt-dlp/yt-dlp/pulls/8797\n-        test('7_a/worst', '35')\n+        test(\"7_a/worst\", \"35\")\n \n     def test_format_selection_audio(self):\n         formats = [\n-            {'format_id': 'audio-low', 'ext': 'webm', 'preference': 1, 'vcodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'audio-mid', 'ext': 'webm', 'preference': 2, 'vcodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'audio-high', 'ext': 'flv', 'preference': 3, 'vcodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'vid', 'ext': 'mp4', 'preference': 4, 'url': TEST_URL},\n+            {\n+                \"format_id\": \"audio-low\",\n+                \"ext\": \"webm\",\n+                \"preference\": 1,\n+                \"vcodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n+            {\n+                \"format_id\": \"audio-mid\",\n+                \"ext\": \"webm\",\n+                \"preference\": 2,\n+                \"vcodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n+            {\n+                \"format_id\": \"audio-high\",\n+                \"ext\": \"flv\",\n+                \"preference\": 3,\n+                \"vcodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n+            {\"format_id\": \"vid\", \"ext\": \"mp4\", \"preference\": 4, \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n \n-        ydl = YDL({'format': 'bestaudio'})\n+        ydl = YDL({\"format\": \"bestaudio\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'audio-high')\n+        self.assertEqual(downloaded[\"format_id\"], \"audio-high\")\n \n-        ydl = YDL({'format': 'worstaudio'})\n+        ydl = YDL({\"format\": \"worstaudio\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'audio-low')\n+        self.assertEqual(downloaded[\"format_id\"], \"audio-low\")\n \n         formats = [\n-            {'format_id': 'vid-low', 'ext': 'mp4', 'preference': 1, 'url': TEST_URL},\n-            {'format_id': 'vid-high', 'ext': 'mp4', 'preference': 2, 'url': TEST_URL},\n+            {\"format_id\": \"vid-low\", \"ext\": \"mp4\", \"preference\": 1, \"url\": TEST_URL},\n+            {\"format_id\": \"vid-high\", \"ext\": \"mp4\", \"preference\": 2, \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n \n-        ydl = YDL({'format': 'bestaudio/worstaudio/best'})\n+        ydl = YDL({\"format\": \"bestaudio/worstaudio/best\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'vid-high')\n+        self.assertEqual(downloaded[\"format_id\"], \"vid-high\")\n \n     def test_format_selection_audio_exts(self):\n         formats = [\n-            {'format_id': 'mp3-64', 'ext': 'mp3', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n-            {'format_id': 'ogg-64', 'ext': 'ogg', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n-            {'format_id': 'aac-64', 'ext': 'aac', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n-            {'format_id': 'mp3-32', 'ext': 'mp3', 'abr': 32, 'url': 'http://_', 'vcodec': 'none'},\n-            {'format_id': 'aac-32', 'ext': 'aac', 'abr': 32, 'url': 'http://_', 'vcodec': 'none'},\n+            {\n+                \"format_id\": \"mp3-64\",\n+                \"ext\": \"mp3\",\n+                \"abr\": 64,\n+                \"url\": \"http://_\",\n+                \"vcodec\": \"none\",\n+            },\n+            {\n+                \"format_id\": \"ogg-64\",\n+                \"ext\": \"ogg\",\n+                \"abr\": 64,\n+                \"url\": \"http://_\",\n+                \"vcodec\": \"none\",\n+            },\n+            {\n+                \"format_id\": \"aac-64\",\n+                \"ext\": \"aac\",\n+                \"abr\": 64,\n+                \"url\": \"http://_\",\n+                \"vcodec\": \"none\",\n+            },\n+            {\n+                \"format_id\": \"mp3-32\",\n+                \"ext\": \"mp3\",\n+                \"abr\": 32,\n+                \"url\": \"http://_\",\n+                \"vcodec\": \"none\",\n+            },\n+            {\n+                \"format_id\": \"aac-32\",\n+                \"ext\": \"aac\",\n+                \"abr\": 32,\n+                \"url\": \"http://_\",\n+                \"vcodec\": \"none\",\n+            },\n         ]\n \n         info_dict = _make_result(formats)\n-        ydl = YDL({'format': 'best'})\n+        ydl = YDL({\"format\": \"best\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(copy.deepcopy(info_dict))\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'aac-64')\n+        self.assertEqual(downloaded[\"format_id\"], \"aac-64\")\n \n-        ydl = YDL({'format': 'mp3'})\n+        ydl = YDL({\"format\": \"mp3\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(copy.deepcopy(info_dict))\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'mp3-64')\n+        self.assertEqual(downloaded[\"format_id\"], \"mp3-64\")\n \n-        ydl = YDL({'prefer_free_formats': True})\n+        ydl = YDL({\"prefer_free_formats\": True})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(copy.deepcopy(info_dict))\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'ogg-64')\n+        self.assertEqual(downloaded[\"format_id\"], \"ogg-64\")\n \n     def test_format_selection_video(self):\n         formats = [\n-            {'format_id': 'dash-video-low', 'ext': 'mp4', 'preference': 1, 'acodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'dash-video-high', 'ext': 'mp4', 'preference': 2, 'acodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'vid', 'ext': 'mp4', 'preference': 3, 'url': TEST_URL},\n+            {\n+                \"format_id\": \"dash-video-low\",\n+                \"ext\": \"mp4\",\n+                \"preference\": 1,\n+                \"acodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n+            {\n+                \"format_id\": \"dash-video-high\",\n+                \"ext\": \"mp4\",\n+                \"preference\": 2,\n+                \"acodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n+            {\"format_id\": \"vid\", \"ext\": \"mp4\", \"preference\": 3, \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n \n-        ydl = YDL({'format': 'bestvideo'})\n+        ydl = YDL({\"format\": \"bestvideo\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'dash-video-high')\n+        self.assertEqual(downloaded[\"format_id\"], \"dash-video-high\")\n \n-        ydl = YDL({'format': 'worstvideo'})\n+        ydl = YDL({\"format\": \"worstvideo\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'dash-video-low')\n+        self.assertEqual(downloaded[\"format_id\"], \"dash-video-low\")\n \n-        ydl = YDL({'format': 'bestvideo[format_id^=dash][format_id$=low]'})\n+        ydl = YDL({\"format\": \"bestvideo[format_id^=dash][format_id$=low]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'dash-video-low')\n+        self.assertEqual(downloaded[\"format_id\"], \"dash-video-low\")\n \n         formats = [\n-            {'format_id': 'vid-vcodec-dot', 'ext': 'mp4', 'preference': 1, 'vcodec': 'avc1.123456', 'acodec': 'none', 'url': TEST_URL},\n+            {\n+                \"format_id\": \"vid-vcodec-dot\",\n+                \"ext\": \"mp4\",\n+                \"preference\": 1,\n+                \"vcodec\": \"avc1.123456\",\n+                \"acodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n         ]\n         info_dict = _make_result(formats)\n \n-        ydl = YDL({'format': 'bestvideo[vcodec=avc1.123456]'})\n+        ydl = YDL({\"format\": \"bestvideo[vcodec=avc1.123456]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'vid-vcodec-dot')\n+        self.assertEqual(downloaded[\"format_id\"], \"vid-vcodec-dot\")\n \n     def test_format_selection_string_ops(self):\n         formats = [\n-            {'format_id': 'abc-cba', 'ext': 'mp4', 'url': TEST_URL},\n-            {'format_id': 'zxc-cxz', 'ext': 'webm', 'url': TEST_URL},\n+            {\"format_id\": \"abc-cba\", \"ext\": \"mp4\", \"url\": TEST_URL},\n+            {\"format_id\": \"zxc-cxz\", \"ext\": \"webm\", \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n \n         # equals (=)\n-        ydl = YDL({'format': '[format_id=abc-cba]'})\n+        ydl = YDL({\"format\": \"[format_id=abc-cba]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'abc-cba')\n+        self.assertEqual(downloaded[\"format_id\"], \"abc-cba\")\n \n         # does not equal (!=)\n-        ydl = YDL({'format': '[format_id!=abc-cba]'})\n+        ydl = YDL({\"format\": \"[format_id!=abc-cba]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n+        self.assertEqual(downloaded[\"format_id\"], \"zxc-cxz\")\n \n-        ydl = YDL({'format': '[format_id!=abc-cba][format_id!=zxc-cxz]'})\n+        ydl = YDL({\"format\": \"[format_id!=abc-cba][format_id!=zxc-cxz]\"})\n         self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n \n         # starts with (^=)\n-        ydl = YDL({'format': '[format_id^=abc]'})\n+        ydl = YDL({\"format\": \"[format_id^=abc]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'abc-cba')\n+        self.assertEqual(downloaded[\"format_id\"], \"abc-cba\")\n \n         # does not start with (!^=)\n-        ydl = YDL({'format': '[format_id!^=abc]'})\n+        ydl = YDL({\"format\": \"[format_id!^=abc]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n+        self.assertEqual(downloaded[\"format_id\"], \"zxc-cxz\")\n \n-        ydl = YDL({'format': '[format_id!^=abc][format_id!^=zxc]'})\n+        ydl = YDL({\"format\": \"[format_id!^=abc][format_id!^=zxc]\"})\n         self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n \n         # ends with ($=)\n-        ydl = YDL({'format': '[format_id$=cba]'})\n+        ydl = YDL({\"format\": \"[format_id$=cba]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'abc-cba')\n+        self.assertEqual(downloaded[\"format_id\"], \"abc-cba\")\n \n         # does not end with (!$=)\n-        ydl = YDL({'format': '[format_id!$=cba]'})\n+        ydl = YDL({\"format\": \"[format_id!$=cba]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n+        self.assertEqual(downloaded[\"format_id\"], \"zxc-cxz\")\n \n-        ydl = YDL({'format': '[format_id!$=cba][format_id!$=cxz]'})\n+        ydl = YDL({\"format\": \"[format_id!$=cba][format_id!$=cxz]\"})\n         self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n \n         # contains (*=)\n-        ydl = YDL({'format': '[format_id*=bc-cb]'})\n+        ydl = YDL({\"format\": \"[format_id*=bc-cb]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'abc-cba')\n+        self.assertEqual(downloaded[\"format_id\"], \"abc-cba\")\n \n         # does not contain (!*=)\n-        ydl = YDL({'format': '[format_id!*=bc-cb]'})\n+        ydl = YDL({\"format\": \"[format_id!*=bc-cb]\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n+        self.assertEqual(downloaded[\"format_id\"], \"zxc-cxz\")\n \n-        ydl = YDL({'format': '[format_id!*=abc][format_id!*=zxc]'})\n+        ydl = YDL({\"format\": \"[format_id!*=abc][format_id!*=zxc]\"})\n         self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n \n-        ydl = YDL({'format': '[format_id!*=-]'})\n+        ydl = YDL({\"format\": \"[format_id!*=-]\"})\n         self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n \n     def test_youtube_format_selection(self):\n@@ -309,16 +383,57 @@ def test_youtube_format_selection(self):\n         return\n \n         order = [\n-            '38', '37', '46', '22', '45', '35', '44', '18', '34', '43', '6', '5', '17', '36', '13',\n+            \"38\",\n+            \"37\",\n+            \"46\",\n+            \"22\",\n+            \"45\",\n+            \"35\",\n+            \"44\",\n+            \"18\",\n+            \"34\",\n+            \"43\",\n+            \"6\",\n+            \"5\",\n+            \"17\",\n+            \"36\",\n+            \"13\",\n             # Apple HTTP Live Streaming\n-            '96', '95', '94', '93', '92', '132', '151',\n+            \"96\",\n+            \"95\",\n+            \"94\",\n+            \"93\",\n+            \"92\",\n+            \"132\",\n+            \"151\",\n             # 3D\n-            '85', '84', '102', '83', '101', '82', '100',\n+            \"85\",\n+            \"84\",\n+            \"102\",\n+            \"83\",\n+            \"101\",\n+            \"82\",\n+            \"100\",\n             # Dash video\n-            '137', '248', '136', '247', '135', '246',\n-            '245', '244', '134', '243', '133', '242', '160',\n+            \"137\",\n+            \"248\",\n+            \"136\",\n+            \"247\",\n+            \"135\",\n+            \"246\",\n+            \"245\",\n+            \"244\",\n+            \"134\",\n+            \"243\",\n+            \"133\",\n+            \"242\",\n+            \"160\",\n             # Dash audio\n-            '141', '172', '140', '171', '139',\n+            \"141\",\n+            \"172\",\n+            \"140\",\n+            \"171\",\n+            \"139\",\n         ]\n \n         def format_info(f_id):\n@@ -329,73 +444,78 @@ def format_info(f_id):\n             # commit a6c2c24479e5f4827ceb06f64d855329c0a6f593\n             # test_YoutubeDL.test_youtube_format_selection is broken without\n             # this fix\n-            if 'acodec' in info and 'vcodec' not in info:\n-                info['vcodec'] = 'none'\n-            elif 'vcodec' in info and 'acodec' not in info:\n-                info['acodec'] = 'none'\n+            if \"acodec\" in info and \"vcodec\" not in info:\n+                info[\"vcodec\"] = \"none\"\n+            elif \"vcodec\" in info and \"acodec\" not in info:\n+                info[\"acodec\"] = \"none\"\n \n-            info['format_id'] = f_id\n-            info['url'] = 'url:' + f_id\n+            info[\"format_id\"] = f_id\n+            info[\"url\"] = \"url:\" + f_id\n             return info\n+\n         formats_order = [format_info(f_id) for f_id in order]\n \n-        info_dict = _make_result(list(formats_order), extractor='youtube')\n-        ydl = YDL({'format': 'bestvideo+bestaudio'})\n+        info_dict = _make_result(list(formats_order), extractor=\"youtube\")\n+        ydl = YDL({\"format\": \"bestvideo+bestaudio\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], '248+172')\n-        self.assertEqual(downloaded['ext'], 'mp4')\n+        self.assertEqual(downloaded[\"format_id\"], \"248+172\")\n+        self.assertEqual(downloaded[\"ext\"], \"mp4\")\n \n-        info_dict = _make_result(list(formats_order), extractor='youtube')\n-        ydl = YDL({'format': 'bestvideo[height>=999999]+bestaudio/best'})\n+        info_dict = _make_result(list(formats_order), extractor=\"youtube\")\n+        ydl = YDL({\"format\": \"bestvideo[height>=999999]+bestaudio/best\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], '38')\n+        self.assertEqual(downloaded[\"format_id\"], \"38\")\n \n-        info_dict = _make_result(list(formats_order), extractor='youtube')\n-        ydl = YDL({'format': 'bestvideo/best,bestaudio'})\n+        info_dict = _make_result(list(formats_order), extractor=\"youtube\")\n+        ydl = YDL({\"format\": \"bestvideo/best,bestaudio\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n-        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n-        self.assertEqual(downloaded_ids, ['137', '141'])\n+        downloaded_ids = [info[\"format_id\"] for info in ydl.downloaded_info_dicts]\n+        self.assertEqual(downloaded_ids, [\"137\", \"141\"])\n \n-        info_dict = _make_result(list(formats_order), extractor='youtube')\n-        ydl = YDL({'format': '(bestvideo[ext=mp4],bestvideo[ext=webm])+bestaudio'})\n+        info_dict = _make_result(list(formats_order), extractor=\"youtube\")\n+        ydl = YDL({\"format\": \"(bestvideo[ext=mp4],bestvideo[ext=webm])+bestaudio\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n-        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n-        self.assertEqual(downloaded_ids, ['137+141', '248+141'])\n+        downloaded_ids = [info[\"format_id\"] for info in ydl.downloaded_info_dicts]\n+        self.assertEqual(downloaded_ids, [\"137+141\", \"248+141\"])\n \n-        info_dict = _make_result(list(formats_order), extractor='youtube')\n-        ydl = YDL({'format': '(bestvideo[ext=mp4],bestvideo[ext=webm])[height<=720]+bestaudio'})\n+        info_dict = _make_result(list(formats_order), extractor=\"youtube\")\n+        ydl = YDL(\n+            {\n+                \"format\": \"(bestvideo[ext=mp4],bestvideo[ext=webm])[height<=720]+bestaudio\"\n+            }\n+        )\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n-        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n-        self.assertEqual(downloaded_ids, ['136+141', '247+141'])\n+        downloaded_ids = [info[\"format_id\"] for info in ydl.downloaded_info_dicts]\n+        self.assertEqual(downloaded_ids, [\"136+141\", \"247+141\"])\n \n-        info_dict = _make_result(list(formats_order), extractor='youtube')\n-        ydl = YDL({'format': '(bestvideo[ext=none]/bestvideo[ext=webm])+bestaudio'})\n+        info_dict = _make_result(list(formats_order), extractor=\"youtube\")\n+        ydl = YDL({\"format\": \"(bestvideo[ext=none]/bestvideo[ext=webm])+bestaudio\"})\n         ydl.sort_formats(info_dict)\n         ydl.process_ie_result(info_dict)\n-        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n-        self.assertEqual(downloaded_ids, ['248+141'])\n+        downloaded_ids = [info[\"format_id\"] for info in ydl.downloaded_info_dicts]\n+        self.assertEqual(downloaded_ids, [\"248+141\"])\n \n         for f1, f2 in zip(formats_order, formats_order[1:]):\n-            info_dict = _make_result([f1, f2], extractor='youtube')\n-            ydl = YDL({'format': 'best/bestvideo'})\n+            info_dict = _make_result([f1, f2], extractor=\"youtube\")\n+            ydl = YDL({\"format\": \"best/bestvideo\"})\n             ydl.sort_formats(info_dict)\n             ydl.process_ie_result(info_dict)\n             downloaded = ydl.downloaded_info_dicts[0]\n-            self.assertEqual(downloaded['format_id'], f1['format_id'])\n+            self.assertEqual(downloaded[\"format_id\"], f1[\"format_id\"])\n \n-            info_dict = _make_result([f2, f1], extractor='youtube')\n-            ydl = YDL({'format': 'best/bestvideo'})\n+            info_dict = _make_result([f2, f1], extractor=\"youtube\")\n+            ydl = YDL({\"format\": \"best/bestvideo\"})\n             ydl.sort_formats(info_dict)\n             ydl.process_ie_result(info_dict)\n             downloaded = ydl.downloaded_info_dicts[0]\n-            self.assertEqual(downloaded['format_id'], f1['format_id'])\n+            self.assertEqual(downloaded[\"format_id\"], f1[\"format_id\"])\n \n     def test_audio_only_extractor_format_selection(self):\n         # For extractors with incomplete formats (all formats are audio-only or\n@@ -403,118 +523,136 @@ def test_audio_only_extractor_format_selection(self):\n         # video-only or audio-only formats (as per\n         # https://github.com/ytdl-org/youtube-dl/pull/5556)\n         formats = [\n-            {'format_id': 'low', 'ext': 'mp3', 'preference': 1, 'vcodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'high', 'ext': 'mp3', 'preference': 2, 'vcodec': 'none', 'url': TEST_URL},\n+            {\n+                \"format_id\": \"low\",\n+                \"ext\": \"mp3\",\n+                \"preference\": 1,\n+                \"vcodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n+            {\n+                \"format_id\": \"high\",\n+                \"ext\": \"mp3\",\n+                \"preference\": 2,\n+                \"vcodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n         ]\n         info_dict = _make_result(formats)\n \n-        ydl = YDL({'format': 'best'})\n+        ydl = YDL({\"format\": \"best\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'high')\n+        self.assertEqual(downloaded[\"format_id\"], \"high\")\n \n-        ydl = YDL({'format': 'worst'})\n+        ydl = YDL({\"format\": \"worst\"})\n         ydl.process_ie_result(info_dict.copy())\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'low')\n+        self.assertEqual(downloaded[\"format_id\"], \"low\")\n \n     def test_format_not_available(self):\n         formats = [\n-            {'format_id': 'regular', 'ext': 'mp4', 'height': 360, 'url': TEST_URL},\n-            {'format_id': 'video', 'ext': 'mp4', 'height': 720, 'acodec': 'none', 'url': TEST_URL},\n+            {\"format_id\": \"regular\", \"ext\": \"mp4\", \"height\": 360, \"url\": TEST_URL},\n+            {\n+                \"format_id\": \"video\",\n+                \"ext\": \"mp4\",\n+                \"height\": 720,\n+                \"acodec\": \"none\",\n+                \"url\": TEST_URL,\n+            },\n         ]\n         info_dict = _make_result(formats)\n \n         # This must fail since complete video-audio format does not match filter\n         # and extractor does not provide incomplete only formats (i.e. only\n         # video-only or audio-only).\n-        ydl = YDL({'format': 'best[height>360]'})\n+        ydl = YDL({\"format\": \"best[height>360]\"})\n         self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n \n     def test_format_selection_issue_10083(self):\n         # See https://github.com/ytdl-org/youtube-dl/issues/10083\n         formats = [\n-            {'format_id': 'regular', 'height': 360, 'url': TEST_URL},\n-            {'format_id': 'video', 'height': 720, 'acodec': 'none', 'url': TEST_URL},\n-            {'format_id': 'audio', 'vcodec': 'none', 'url': TEST_URL},\n+            {\"format_id\": \"regular\", \"height\": 360, \"url\": TEST_URL},\n+            {\"format_id\": \"video\", \"height\": 720, \"acodec\": \"none\", \"url\": TEST_URL},\n+            {\"format_id\": \"audio\", \"vcodec\": \"none\", \"url\": TEST_URL},\n         ]\n         info_dict = _make_result(formats)\n \n-        ydl = YDL({'format': 'best[height>360]/bestvideo[height>360]+bestaudio'})\n+        ydl = YDL({\"format\": \"best[height>360]/bestvideo[height>360]+bestaudio\"})\n         ydl.process_ie_result(info_dict.copy())\n-        self.assertEqual(ydl.downloaded_info_dicts[0]['format_id'], 'video+audio')\n+        self.assertEqual(ydl.downloaded_info_dicts[0][\"format_id\"], \"video+audio\")\n \n     def test_invalid_format_specs(self):\n         def assert_syntax_error(format_spec):\n-            self.assertRaises(SyntaxError, YDL, {'format': format_spec})\n+            self.assertRaises(SyntaxError, YDL, {\"format\": format_spec})\n \n-        assert_syntax_error('bestvideo,,best')\n-        assert_syntax_error('+bestaudio')\n-        assert_syntax_error('bestvideo+')\n-        assert_syntax_error('/')\n-        assert_syntax_error('[720<height]')\n+        assert_syntax_error(\"bestvideo,,best\")\n+        assert_syntax_error(\"+bestaudio\")\n+        assert_syntax_error(\"bestvideo+\")\n+        assert_syntax_error(\"/\")\n+        assert_syntax_error(\"[720<height]\")\n \n     def test_format_filtering(self):\n         formats = [\n-            {'format_id': 'A', 'filesize': 500, 'width': 1000},\n-            {'format_id': 'B', 'filesize': 1000, 'width': 500},\n-            {'format_id': 'C', 'filesize': 1000, 'width': 400},\n-            {'format_id': 'D', 'filesize': 2000, 'width': 600},\n-            {'format_id': 'E', 'filesize': 3000},\n-            {'format_id': 'F'},\n-            {'format_id': 'G', 'filesize': 1000000},\n+            {\"format_id\": \"A\", \"filesize\": 500, \"width\": 1000},\n+            {\"format_id\": \"B\", \"filesize\": 1000, \"width\": 500},\n+            {\"format_id\": \"C\", \"filesize\": 1000, \"width\": 400},\n+            {\"format_id\": \"D\", \"filesize\": 2000, \"width\": 600},\n+            {\"format_id\": \"E\", \"filesize\": 3000},\n+            {\"format_id\": \"F\"},\n+            {\"format_id\": \"G\", \"filesize\": 1000000},\n         ]\n         for f in formats:\n-            f['url'] = 'http://_/'\n-            f['ext'] = 'unknown'\n-        info_dict = _make_result(formats, _format_sort_fields=('id', ))\n+            f[\"url\"] = \"http://_/\"\n+            f[\"ext\"] = \"unknown\"\n+        info_dict = _make_result(formats, _format_sort_fields=(\"id\",))\n \n-        ydl = YDL({'format': 'best[filesize<3000]'})\n+        ydl = YDL({\"format\": \"best[filesize<3000]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'D')\n+        self.assertEqual(downloaded[\"format_id\"], \"D\")\n \n-        ydl = YDL({'format': 'best[filesize<=3000]'})\n+        ydl = YDL({\"format\": \"best[filesize<=3000]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'E')\n+        self.assertEqual(downloaded[\"format_id\"], \"E\")\n \n-        ydl = YDL({'format': 'best[filesize <= ? 3000]'})\n+        ydl = YDL({\"format\": \"best[filesize <= ? 3000]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'F')\n+        self.assertEqual(downloaded[\"format_id\"], \"F\")\n \n-        ydl = YDL({'format': 'best [filesize = 1000] [width>450]'})\n+        ydl = YDL({\"format\": \"best [filesize = 1000] [width>450]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'B')\n+        self.assertEqual(downloaded[\"format_id\"], \"B\")\n \n-        ydl = YDL({'format': 'best [filesize = 1000] [width!=450]'})\n+        ydl = YDL({\"format\": \"best [filesize = 1000] [width!=450]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'C')\n+        self.assertEqual(downloaded[\"format_id\"], \"C\")\n \n-        ydl = YDL({'format': '[filesize>?1]'})\n+        ydl = YDL({\"format\": \"[filesize>?1]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'G')\n+        self.assertEqual(downloaded[\"format_id\"], \"G\")\n \n-        ydl = YDL({'format': '[filesize<1M]'})\n+        ydl = YDL({\"format\": \"[filesize<1M]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'E')\n+        self.assertEqual(downloaded[\"format_id\"], \"E\")\n \n-        ydl = YDL({'format': '[filesize<1MiB]'})\n+        ydl = YDL({\"format\": \"[filesize<1MiB]\"})\n         ydl.process_ie_result(info_dict)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['format_id'], 'G')\n+        self.assertEqual(downloaded[\"format_id\"], \"G\")\n \n-        ydl = YDL({'format': 'all[width>=400][width<=600]'})\n+        ydl = YDL({\"format\": \"all[width>=400][width<=600]\"})\n         ydl.process_ie_result(info_dict)\n-        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n-        self.assertEqual(downloaded_ids, ['D', 'C', 'B'])\n+        downloaded_ids = [info[\"format_id\"] for info in ydl.downloaded_info_dicts]\n+        self.assertEqual(downloaded_ids, [\"D\", \"C\", \"B\"])\n \n-        ydl = YDL({'format': 'best[height<40]'})\n+        ydl = YDL({\"format\": \"best[height<40]\"})\n         try:\n             ydl.process_ie_result(info_dict)\n         except ExtractorError:\n@@ -522,143 +660,165 @@ def test_format_filtering(self):\n         self.assertEqual(ydl.downloaded_info_dicts, [])\n \n     def test_default_format_spec(self):\n-        ydl = YDL({'simulate': True})\n-        self.assertEqual(ydl._default_format_spec({}), 'bestvideo*+bestaudio/best')\n+        ydl = YDL({\"simulate\": True})\n+        self.assertEqual(ydl._default_format_spec({}), \"bestvideo*+bestaudio/best\")\n \n         ydl = YDL({})\n-        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'best/bestvideo+bestaudio')\n+        self.assertEqual(\n+            ydl._default_format_spec({\"is_live\": True}), \"best/bestvideo+bestaudio\"\n+        )\n \n-        ydl = YDL({'simulate': True})\n-        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'bestvideo*+bestaudio/best')\n+        ydl = YDL({\"simulate\": True})\n+        self.assertEqual(\n+            ydl._default_format_spec({\"is_live\": True}), \"bestvideo*+bestaudio/best\"\n+        )\n \n-        ydl = YDL({'outtmpl': '-'})\n-        self.assertEqual(ydl._default_format_spec({}), 'best/bestvideo+bestaudio')\n+        ydl = YDL({\"outtmpl\": \"-\"})\n+        self.assertEqual(ydl._default_format_spec({}), \"best/bestvideo+bestaudio\")\n \n         ydl = YDL({})\n-        self.assertEqual(ydl._default_format_spec({}, download=False), 'bestvideo*+bestaudio/best')\n-        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'best/bestvideo+bestaudio')\n+        self.assertEqual(\n+            ydl._default_format_spec({}, download=False), \"bestvideo*+bestaudio/best\"\n+        )\n+        self.assertEqual(\n+            ydl._default_format_spec({\"is_live\": True}), \"best/bestvideo+bestaudio\"\n+        )\n \n \n class TestYoutubeDL(unittest.TestCase):\n     def test_subtitles(self):\n         def s_formats(lang, autocaption=False):\n-            return [{\n-                'ext': ext,\n-                'url': f'http://localhost/video.{lang}.{ext}',\n-                '_auto': autocaption,\n-            } for ext in ['vtt', 'srt', 'ass']]\n-        subtitles = {l: s_formats(l) for l in ['en', 'fr', 'es']}\n-        auto_captions = {l: s_formats(l, True) for l in ['it', 'pt', 'es']}\n+            return [\n+                {\n+                    \"ext\": ext,\n+                    \"url\": f\"http://localhost/video.{lang}.{ext}\",\n+                    \"_auto\": autocaption,\n+                }\n+                for ext in [\"vtt\", \"srt\", \"ass\"]\n+            ]\n+\n+        subtitles = {l: s_formats(l) for l in [\"en\", \"fr\", \"es\"]}\n+        auto_captions = {l: s_formats(l, True) for l in [\"it\", \"pt\", \"es\"]}\n         info_dict = {\n-            'id': 'test',\n-            'title': 'Test',\n-            'url': 'http://localhost/video.mp4',\n-            'subtitles': subtitles,\n-            'automatic_captions': auto_captions,\n-            'extractor': 'TEST',\n-            'webpage_url': 'http://example.com/watch?v=shenanigans',\n+            \"id\": \"test\",\n+            \"title\": \"Test\",\n+            \"url\": \"http://localhost/video.mp4\",\n+            \"subtitles\": subtitles,\n+            \"automatic_captions\": auto_captions,\n+            \"extractor\": \"TEST\",\n+            \"webpage_url\": \"http://example.com/watch?v=shenanigans\",\n         }\n \n         def get_info(params={}):\n-            params.setdefault('simulate', True)\n+            params.setdefault(\"simulate\", True)\n             ydl = YDL(params)\n             ydl.report_warning = lambda *args, **kargs: None\n             return ydl.process_video_result(info_dict, download=False)\n \n         result = get_info()\n-        self.assertFalse(result.get('requested_subtitles'))\n-        self.assertEqual(result['subtitles'], subtitles)\n-        self.assertEqual(result['automatic_captions'], auto_captions)\n+        self.assertFalse(result.get(\"requested_subtitles\"))\n+        self.assertEqual(result[\"subtitles\"], subtitles)\n+        self.assertEqual(result[\"automatic_captions\"], auto_captions)\n \n-        result = get_info({'writesubtitles': True})\n-        subs = result['requested_subtitles']\n+        result = get_info({\"writesubtitles\": True})\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'en'})\n-        self.assertTrue(subs['en'].get('data') is None)\n-        self.assertEqual(subs['en']['ext'], 'ass')\n-\n-        result = get_info({'writesubtitles': True, 'subtitlesformat': 'foo/srt'})\n-        subs = result['requested_subtitles']\n-        self.assertEqual(subs['en']['ext'], 'srt')\n-\n-        result = get_info({'writesubtitles': True, 'subtitleslangs': ['es', 'fr', 'it']})\n-        subs = result['requested_subtitles']\n+        self.assertEqual(set(subs.keys()), {\"en\"})\n+        self.assertTrue(subs[\"en\"].get(\"data\") is None)\n+        self.assertEqual(subs[\"en\"][\"ext\"], \"ass\")\n+\n+        result = get_info({\"writesubtitles\": True, \"subtitlesformat\": \"foo/srt\"})\n+        subs = result[\"requested_subtitles\"]\n+        self.assertEqual(subs[\"en\"][\"ext\"], \"srt\")\n+\n+        result = get_info(\n+            {\"writesubtitles\": True, \"subtitleslangs\": [\"es\", \"fr\", \"it\"]}\n+        )\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'es', 'fr'})\n+        self.assertEqual(set(subs.keys()), {\"es\", \"fr\"})\n \n-        result = get_info({'writesubtitles': True, 'subtitleslangs': ['all', '-en']})\n-        subs = result['requested_subtitles']\n+        result = get_info({\"writesubtitles\": True, \"subtitleslangs\": [\"all\", \"-en\"]})\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'es', 'fr'})\n+        self.assertEqual(set(subs.keys()), {\"es\", \"fr\"})\n \n-        result = get_info({'writesubtitles': True, 'subtitleslangs': ['en', 'fr', '-en']})\n-        subs = result['requested_subtitles']\n+        result = get_info(\n+            {\"writesubtitles\": True, \"subtitleslangs\": [\"en\", \"fr\", \"-en\"]}\n+        )\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'fr'})\n+        self.assertEqual(set(subs.keys()), {\"fr\"})\n \n-        result = get_info({'writesubtitles': True, 'subtitleslangs': ['-en', 'en']})\n-        subs = result['requested_subtitles']\n+        result = get_info({\"writesubtitles\": True, \"subtitleslangs\": [\"-en\", \"en\"]})\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'en'})\n+        self.assertEqual(set(subs.keys()), {\"en\"})\n \n-        result = get_info({'writesubtitles': True, 'subtitleslangs': ['e.+']})\n-        subs = result['requested_subtitles']\n+        result = get_info({\"writesubtitles\": True, \"subtitleslangs\": [\"e.+\"]})\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'es', 'en'})\n+        self.assertEqual(set(subs.keys()), {\"es\", \"en\"})\n \n-        result = get_info({'writesubtitles': True, 'writeautomaticsub': True, 'subtitleslangs': ['es', 'pt']})\n-        subs = result['requested_subtitles']\n+        result = get_info(\n+            {\n+                \"writesubtitles\": True,\n+                \"writeautomaticsub\": True,\n+                \"subtitleslangs\": [\"es\", \"pt\"],\n+            }\n+        )\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'es', 'pt'})\n-        self.assertFalse(subs['es']['_auto'])\n-        self.assertTrue(subs['pt']['_auto'])\n+        self.assertEqual(set(subs.keys()), {\"es\", \"pt\"})\n+        self.assertFalse(subs[\"es\"][\"_auto\"])\n+        self.assertTrue(subs[\"pt\"][\"_auto\"])\n \n-        result = get_info({'writeautomaticsub': True, 'subtitleslangs': ['es', 'pt']})\n-        subs = result['requested_subtitles']\n+        result = get_info({\"writeautomaticsub\": True, \"subtitleslangs\": [\"es\", \"pt\"]})\n+        subs = result[\"requested_subtitles\"]\n         self.assertTrue(subs)\n-        self.assertEqual(set(subs.keys()), {'es', 'pt'})\n-        self.assertTrue(subs['es']['_auto'])\n-        self.assertTrue(subs['pt']['_auto'])\n+        self.assertEqual(set(subs.keys()), {\"es\", \"pt\"})\n+        self.assertTrue(subs[\"es\"][\"_auto\"])\n+        self.assertTrue(subs[\"pt\"][\"_auto\"])\n \n     def test_add_extra_info(self):\n         test_dict = {\n-            'extractor': 'Foo',\n+            \"extractor\": \"Foo\",\n         }\n         extra_info = {\n-            'extractor': 'Bar',\n-            'playlist': 'funny videos',\n+            \"extractor\": \"Bar\",\n+            \"playlist\": \"funny videos\",\n         }\n         YDL.add_extra_info(test_dict, extra_info)\n-        self.assertEqual(test_dict['extractor'], 'Foo')\n-        self.assertEqual(test_dict['playlist'], 'funny videos')\n+        self.assertEqual(test_dict[\"extractor\"], \"Foo\")\n+        self.assertEqual(test_dict[\"playlist\"], \"funny videos\")\n \n     outtmpl_info = {\n-        'id': '1234',\n-        'ext': 'mp4',\n-        'width': None,\n-        'height': 1080,\n-        'filesize': 1024,\n-        'title1': '$PATH',\n-        'title2': '%PATH%',\n-        'title3': 'foo/bar\\\\test',\n-        'title4': 'foo \"bar\" test',\n-        'title5': '\u00e1\u00e9\u00ed \ud835\udc00',\n-        'timestamp': 1618488000,\n-        'duration': 100000,\n-        'playlist_index': 1,\n-        'playlist_autonumber': 2,\n-        '__last_playlist_index': 100,\n-        'n_entries': 10,\n-        'formats': [\n-            {'id': 'id 1', 'height': 1080, 'width': 1920},\n-            {'id': 'id 2', 'height': 720},\n-            {'id': 'id 3'}\n-        ]\n+        \"id\": \"1234\",\n+        \"ext\": \"mp4\",\n+        \"width\": None,\n+        \"height\": 1080,\n+        \"filesize\": 1024,\n+        \"title1\": \"$PATH\",\n+        \"title2\": \"%PATH%\",\n+        \"title3\": \"foo/bar\\\\test\",\n+        \"title4\": 'foo \"bar\" test',\n+        \"title5\": \"\u00e1\u00e9\u00ed \ud835\udc00\",\n+        \"timestamp\": 1618488000,\n+        \"duration\": 100000,\n+        \"playlist_index\": 1,\n+        \"playlist_autonumber\": 2,\n+        \"__last_playlist_index\": 100,\n+        \"n_entries\": 10,\n+        \"formats\": [\n+            {\"id\": \"id 1\", \"height\": 1080, \"width\": 1920},\n+            {\"id\": \"id 2\", \"height\": 720},\n+            {\"id\": \"id 3\"},\n+        ],\n     }\n \n     def test_prepare_outtmpl_and_filename(self):\n         def test(tmpl, expected, *, info=None, **params):\n-            params['outtmpl'] = tmpl\n+            params[\"outtmpl\"] = tmpl\n             ydl = FakeYDL(params)\n             ydl._num_downloads = 1\n             self.assertEqual(ydl.validate_outtmpl(tmpl), None)\n@@ -668,60 +828,62 @@ def test(tmpl, expected, *, info=None, **params):\n \n             if not isinstance(expected, (list, tuple)):\n                 expected = (expected, expected)\n-            for (name, got), expect in zip((('outtmpl', out), ('filename', fname)), expected):\n+            for (name, got), expect in zip(\n+                ((\"outtmpl\", out), (\"filename\", fname)), expected\n+            ):\n                 if callable(expect):\n-                    self.assertTrue(expect(got), f'Wrong {name} from {tmpl}')\n+                    self.assertTrue(expect(got), f\"Wrong {name} from {tmpl}\")\n                 elif expect is not None:\n-                    self.assertEqual(got, expect, f'Wrong {name} from {tmpl}')\n+                    self.assertEqual(got, expect, f\"Wrong {name} from {tmpl}\")\n \n         # Side-effects\n         original_infodict = dict(self.outtmpl_info)\n-        test('foo.bar', 'foo.bar')\n-        original_infodict['epoch'] = self.outtmpl_info.get('epoch')\n-        self.assertTrue(isinstance(original_infodict['epoch'], int))\n-        test('%(epoch)d', int_or_none)\n+        test(\"foo.bar\", \"foo.bar\")\n+        original_infodict[\"epoch\"] = self.outtmpl_info.get(\"epoch\")\n+        self.assertTrue(isinstance(original_infodict[\"epoch\"], int))\n+        test(\"%(epoch)d\", int_or_none)\n         self.assertEqual(original_infodict, self.outtmpl_info)\n \n         # Auto-generated fields\n-        test('%(id)s.%(ext)s', '1234.mp4')\n-        test('%(duration_string)s', ('27:46:40', '27-46-40'))\n-        test('%(resolution)s', '1080p')\n-        test('%(playlist_index|)s', '001')\n-        test('%(playlist_index&{}!)s', '1!')\n-        test('%(playlist_autonumber)s', '02')\n-        test('%(autonumber)s', '00001')\n-        test('%(autonumber+2)03d', '005', autonumber_start=3)\n-        test('%(autonumber)s', '001', autonumber_size=3)\n+        test(\"%(id)s.%(ext)s\", \"1234.mp4\")\n+        test(\"%(duration_string)s\", (\"27:46:40\", \"27-46-40\"))\n+        test(\"%(resolution)s\", \"1080p\")\n+        test(\"%(playlist_index|)s\", \"001\")\n+        test(\"%(playlist_index&{}!)s\", \"1!\")\n+        test(\"%(playlist_autonumber)s\", \"02\")\n+        test(\"%(autonumber)s\", \"00001\")\n+        test(\"%(autonumber+2)03d\", \"005\", autonumber_start=3)\n+        test(\"%(autonumber)s\", \"001\", autonumber_size=3)\n \n         # Escaping %\n-        test('%', '%')\n-        test('%%', '%')\n-        test('%%%%', '%%')\n-        test('%s', '%s')\n-        test('%%%s', '%%s')\n-        test('%d', '%d')\n-        test('%abc%', '%abc%')\n-        test('%%(width)06d.%(ext)s', '%(width)06d.mp4')\n-        test('%%%(height)s', '%1080')\n-        test('%(width)06d.%(ext)s', 'NA.mp4')\n-        test('%(width)06d.%%(ext)s', 'NA.%(ext)s')\n-        test('%%(width)06d.%(ext)s', '%(width)06d.mp4')\n+        test(\"%\", \"%\")\n+        test(\"%%\", \"%\")\n+        test(\"%%%%\", \"%%\")\n+        test(\"%s\", \"%s\")\n+        test(\"%%%s\", \"%%s\")\n+        test(\"%d\", \"%d\")\n+        test(\"%abc%\", \"%abc%\")\n+        test(\"%%(width)06d.%(ext)s\", \"%(width)06d.mp4\")\n+        test(\"%%%(height)s\", \"%1080\")\n+        test(\"%(width)06d.%(ext)s\", \"NA.mp4\")\n+        test(\"%(width)06d.%%(ext)s\", \"NA.%(ext)s\")\n+        test(\"%%(width)06d.%(ext)s\", \"%(width)06d.mp4\")\n \n         # ID sanitization\n-        test('%(id)s', '_abcd', info={'id': '_abcd'})\n-        test('%(some_id)s', '_abcd', info={'some_id': '_abcd'})\n-        test('%(formats.0.id)s', '_abcd', info={'formats': [{'id': '_abcd'}]})\n-        test('%(id)s', '-abcd', info={'id': '-abcd'})\n-        test('%(id)s', '.abcd', info={'id': '.abcd'})\n-        test('%(id)s', 'ab__cd', info={'id': 'ab__cd'})\n-        test('%(id)s', ('ab:cd', 'ab\uff1acd'), info={'id': 'ab:cd'})\n-        test('%(id.0)s', '-', info={'id': '--'})\n+        test(\"%(id)s\", \"_abcd\", info={\"id\": \"_abcd\"})\n+        test(\"%(some_id)s\", \"_abcd\", info={\"some_id\": \"_abcd\"})\n+        test(\"%(formats.0.id)s\", \"_abcd\", info={\"formats\": [{\"id\": \"_abcd\"}]})\n+        test(\"%(id)s\", \"-abcd\", info={\"id\": \"-abcd\"})\n+        test(\"%(id)s\", \".abcd\", info={\"id\": \".abcd\"})\n+        test(\"%(id)s\", \"ab__cd\", info={\"id\": \"ab__cd\"})\n+        test(\"%(id)s\", (\"ab:cd\", \"ab\uff1acd\"), info={\"id\": \"ab:cd\"})\n+        test(\"%(id.0)s\", \"-\", info={\"id\": \"--\"})\n \n         # Invalid templates\n-        self.assertTrue(isinstance(YoutubeDL.validate_outtmpl('%(title)'), ValueError))\n-        test('%(invalid@tmpl|def)s', 'none', outtmpl_na_placeholder='none')\n-        test('%(..)s', 'NA')\n-        test('%(formats.{id)s', 'NA')\n+        self.assertTrue(isinstance(YoutubeDL.validate_outtmpl(\"%(title)\"), ValueError))\n+        test(\"%(invalid@tmpl|def)s\", \"none\", outtmpl_na_placeholder=\"none\")\n+        test(\"%(..)s\", \"NA\")\n+        test(\"%(formats.{id)s\", \"NA\")\n \n         # Entire info_dict\n         def expect_same_infodict(out):\n@@ -730,266 +892,301 @@ def expect_same_infodict(out):\n                 self.assertEqual(got_dict.get(info_field), expected, info_field)\n             return True\n \n-        test('%()j', (expect_same_infodict, None))\n+        test(\"%()j\", (expect_same_infodict, None))\n \n         # NA placeholder\n-        NA_TEST_OUTTMPL = '%(uploader_date)s-%(width)d-%(x|def)s-%(id)s.%(ext)s'\n-        test(NA_TEST_OUTTMPL, 'NA-NA-def-1234.mp4')\n-        test(NA_TEST_OUTTMPL, 'none-none-def-1234.mp4', outtmpl_na_placeholder='none')\n-        test(NA_TEST_OUTTMPL, '--def-1234.mp4', outtmpl_na_placeholder='')\n-        test('%(non_existent.0)s', 'NA')\n+        NA_TEST_OUTTMPL = \"%(uploader_date)s-%(width)d-%(x|def)s-%(id)s.%(ext)s\"\n+        test(NA_TEST_OUTTMPL, \"NA-NA-def-1234.mp4\")\n+        test(NA_TEST_OUTTMPL, \"none-none-def-1234.mp4\", outtmpl_na_placeholder=\"none\")\n+        test(NA_TEST_OUTTMPL, \"--def-1234.mp4\", outtmpl_na_placeholder=\"\")\n+        test(\"%(non_existent.0)s\", \"NA\")\n \n         # String formatting\n-        FMT_TEST_OUTTMPL = '%%(height)%s.%%(ext)s'\n-        test(FMT_TEST_OUTTMPL % 's', '1080.mp4')\n-        test(FMT_TEST_OUTTMPL % 'd', '1080.mp4')\n-        test(FMT_TEST_OUTTMPL % '6d', '  1080.mp4')\n-        test(FMT_TEST_OUTTMPL % '-6d', '1080  .mp4')\n-        test(FMT_TEST_OUTTMPL % '06d', '001080.mp4')\n-        test(FMT_TEST_OUTTMPL % ' 06d', ' 01080.mp4')\n-        test(FMT_TEST_OUTTMPL % '   06d', ' 01080.mp4')\n-        test(FMT_TEST_OUTTMPL % '0 6d', ' 01080.mp4')\n-        test(FMT_TEST_OUTTMPL % '0   6d', ' 01080.mp4')\n-        test(FMT_TEST_OUTTMPL % '   0   6d', ' 01080.mp4')\n+        FMT_TEST_OUTTMPL = \"%%(height)%s.%%(ext)s\"\n+        test(FMT_TEST_OUTTMPL % \"s\", \"1080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"d\", \"1080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"6d\", \"  1080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"-6d\", \"1080  .mp4\")\n+        test(FMT_TEST_OUTTMPL % \"06d\", \"001080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \" 06d\", \" 01080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"   06d\", \" 01080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"0 6d\", \" 01080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"0   6d\", \" 01080.mp4\")\n+        test(FMT_TEST_OUTTMPL % \"   0   6d\", \" 01080.mp4\")\n \n         # Type casting\n-        test('%(id)d', '1234')\n-        test('%(height)c', '1')\n-        test('%(ext)c', 'm')\n-        test('%(id)d %(id)r', \"1234 '1234'\")\n-        test('%(id)r %(height)r', \"'1234' 1080\")\n-        test('%(title5)a %(height)a', (R\"'\\xe1\\xe9\\xed \\U0001d400' 1080\", None))\n-        test('%(ext)s-%(ext|def)d', 'mp4-def')\n-        test('%(width|0)04d', '0')\n-        test('a%(width|b)d', 'ab', outtmpl_na_placeholder='none')\n-\n-        FORMATS = self.outtmpl_info['formats']\n+        test(\"%(id)d\", \"1234\")\n+        test(\"%(height)c\", \"1\")\n+        test(\"%(ext)c\", \"m\")\n+        test(\"%(id)d %(id)r\", \"1234 '1234'\")\n+        test(\"%(id)r %(height)r\", \"'1234' 1080\")\n+        test(\"%(title5)a %(height)a\", (R\"'\\xe1\\xe9\\xed \\U0001d400' 1080\", None))\n+        test(\"%(ext)s-%(ext|def)d\", \"mp4-def\")\n+        test(\"%(width|0)04d\", \"0\")\n+        test(\"a%(width|b)d\", \"ab\", outtmpl_na_placeholder=\"none\")\n+\n+        FORMATS = self.outtmpl_info[\"formats\"]\n \n         # Custom type casting\n-        test('%(formats.:.id)l', 'id 1, id 2, id 3')\n-        test('%(formats.:.id)#l', ('id 1\\nid 2\\nid 3', 'id 1 id 2 id 3'))\n-        test('%(ext)l', 'mp4')\n-        test('%(formats.:.id) 18l', '  id 1, id 2, id 3')\n-        test('%(formats)j', (json.dumps(FORMATS), None))\n-        test('%(formats)#j', (\n-            json.dumps(FORMATS, indent=4),\n-            json.dumps(FORMATS, indent=4).replace(':', '\uff1a').replace('\"', \"\uff02\").replace('\\n', ' ')\n-        ))\n-        test('%(title5).3B', '\u00e1')\n-        test('%(title5)U', '\u00e1\u00e9\u00ed \ud835\udc00')\n-        test('%(title5)#U', 'a\\u0301e\\u0301i\\u0301 \ud835\udc00')\n-        test('%(title5)+U', '\u00e1\u00e9\u00ed A')\n-        test('%(title5)+#U', 'a\\u0301e\\u0301i\\u0301 A')\n-        test('%(height)D', '1k')\n-        test('%(filesize)#D', '1Ki')\n-        test('%(height)5.2D', ' 1.08k')\n-        test('%(title4)#S', 'foo_bar_test')\n-        test('%(title4).10S', ('foo \uff02bar\uff02 ', 'foo \uff02bar\uff02' + ('#' if compat_os_name == 'nt' else ' ')))\n-        if compat_os_name == 'nt':\n-            test('%(title4)q', ('\"foo \"\"bar\"\" test\"', None))\n-            test('%(formats.:.id)#q', ('\"id 1\" \"id 2\" \"id 3\"', None))\n-            test('%(formats.0.id)#q', ('\"id 1\"', None))\n+        test(\"%(formats.:.id)l\", \"id 1, id 2, id 3\")\n+        test(\"%(formats.:.id)#l\", (\"id 1\\nid 2\\nid 3\", \"id 1 id 2 id 3\"))\n+        test(\"%(ext)l\", \"mp4\")\n+        test(\"%(formats.:.id) 18l\", \"  id 1, id 2, id 3\")\n+        test(\"%(formats)j\", (json.dumps(FORMATS), None))\n+        test(\n+            \"%(formats)#j\",\n+            (\n+                json.dumps(FORMATS, indent=4),\n+                json.dumps(FORMATS, indent=4)\n+                .replace(\":\", \"\uff1a\")\n+                .replace('\"', \"\uff02\")\n+                .replace(\"\\n\", \" \"),\n+            ),\n+        )\n+        test(\"%(title5).3B\", \"\u00e1\")\n+        test(\"%(title5)U\", \"\u00e1\u00e9\u00ed \ud835\udc00\")\n+        test(\"%(title5)#U\", \"a\\u0301e\\u0301i\\u0301 \ud835\udc00\")\n+        test(\"%(title5)+U\", \"\u00e1\u00e9\u00ed A\")\n+        test(\"%(title5)+#U\", \"a\\u0301e\\u0301i\\u0301 A\")\n+        test(\"%(height)D\", \"1k\")\n+        test(\"%(filesize)#D\", \"1Ki\")\n+        test(\"%(height)5.2D\", \" 1.08k\")\n+        test(\"%(title4)#S\", \"foo_bar_test\")\n+        test(\n+            \"%(title4).10S\",\n+            (\"foo \uff02bar\uff02 \", \"foo \uff02bar\uff02\" + (\"#\" if compat_os_name == \"nt\" else \" \")),\n+        )\n+        if compat_os_name == \"nt\":\n+            test(\"%(title4)q\", ('\"foo \"\"bar\"\" test\"', None))\n+            test(\"%(formats.:.id)#q\", ('\"id 1\" \"id 2\" \"id 3\"', None))\n+            test(\"%(formats.0.id)#q\", ('\"id 1\"', None))\n         else:\n-            test('%(title4)q', ('\\'foo \"bar\" test\\'', '\\'foo \uff02bar\uff02 test\\''))\n-            test('%(formats.:.id)#q', \"'id 1' 'id 2' 'id 3'\")\n-            test('%(formats.0.id)#q', \"'id 1'\")\n+            test(\"%(title4)q\", (\"'foo \\\"bar\\\" test'\", \"'foo \uff02bar\uff02 test'\"))\n+            test(\"%(formats.:.id)#q\", \"'id 1' 'id 2' 'id 3'\")\n+            test(\"%(formats.0.id)#q\", \"'id 1'\")\n \n         # Internal formatting\n-        test('%(timestamp-1000>%H-%M-%S)s', '11-43-20')\n-        test('%(title|%)s %(title|%%)s', '% %%')\n-        test('%(id+1-height+3)05d', '00158')\n-        test('%(width+100)05d', 'NA')\n-        test('%(filesize*8)d', '8192')\n-        test('%(formats.0) 15s', ('% 15s' % FORMATS[0], None))\n-        test('%(formats.0)r', (repr(FORMATS[0]), None))\n-        test('%(height.0)03d', '001')\n-        test('%(-height.0)04d', '-001')\n-        test('%(formats.-1.id)s', FORMATS[-1]['id'])\n-        test('%(formats.0.id.-1)d', FORMATS[0]['id'][-1])\n-        test('%(formats.3)s', 'NA')\n-        test('%(formats.:2:-1)r', repr(FORMATS[:2:-1]))\n-        test('%(formats.0.id.-1+id)f', '1235.000000')\n-        test('%(formats.0.id.-1+formats.1.id.-1)d', '3')\n-        out = json.dumps([{'id': f['id'], 'height.:2': str(f['height'])[:2]}\n-                          if 'height' in f else {'id': f['id']}\n-                          for f in FORMATS])\n-        test('%(formats.:.{id,height.:2})j', (out, None))\n-        test('%(formats.:.{id,height}.id)l', ', '.join(f['id'] for f in FORMATS))\n-        test('%(.{id,title})j', ('{\"id\": \"1234\"}', '{\uff02id\uff02\uff1a \uff021234\uff02}'))\n+        test(\"%(timestamp-1000>%H-%M-%S)s\", \"11-43-20\")\n+        test(\"%(title|%)s %(title|%%)s\", \"% %%\")\n+        test(\"%(id+1-height+3)05d\", \"00158\")\n+        test(\"%(width+100)05d\", \"NA\")\n+        test(\"%(filesize*8)d\", \"8192\")\n+        test(\"%(formats.0) 15s\", (\"% 15s\" % FORMATS[0], None))\n+        test(\"%(formats.0)r\", (repr(FORMATS[0]), None))\n+        test(\"%(height.0)03d\", \"001\")\n+        test(\"%(-height.0)04d\", \"-001\")\n+        test(\"%(formats.-1.id)s\", FORMATS[-1][\"id\"])\n+        test(\"%(formats.0.id.-1)d\", FORMATS[0][\"id\"][-1])\n+        test(\"%(formats.3)s\", \"NA\")\n+        test(\"%(formats.:2:-1)r\", repr(FORMATS[:2:-1]))\n+        test(\"%(formats.0.id.-1+id)f\", \"1235.000000\")\n+        test(\"%(formats.0.id.-1+formats.1.id.-1)d\", \"3\")\n+        out = json.dumps(\n+            [\n+                {\"id\": f[\"id\"], \"height.:2\": str(f[\"height\"])[:2]}\n+                if \"height\" in f\n+                else {\"id\": f[\"id\"]}\n+                for f in FORMATS\n+            ]\n+        )\n+        test(\"%(formats.:.{id,height.:2})j\", (out, None))\n+        test(\"%(formats.:.{id,height}.id)l\", \", \".join(f[\"id\"] for f in FORMATS))\n+        test(\"%(.{id,title})j\", ('{\"id\": \"1234\"}', \"{\uff02id\uff02\uff1a \uff021234\uff02}\"))\n \n         # Alternates\n-        test('%(title,id)s', '1234')\n-        test('%(width-100,height+20|def)d', '1100')\n-        test('%(width-100,height+width|def)s', 'def')\n-        test('%(timestamp-x>%H\\\\,%M\\\\,%S,timestamp>%H\\\\,%M\\\\,%S)s', '12,00,00')\n+        test(\"%(title,id)s\", \"1234\")\n+        test(\"%(width-100,height+20|def)d\", \"1100\")\n+        test(\"%(width-100,height+width|def)s\", \"def\")\n+        test(\"%(timestamp-x>%H\\\\,%M\\\\,%S,timestamp>%H\\\\,%M\\\\,%S)s\", \"12,00,00\")\n \n         # Replacement\n-        test('%(id&foo)s.bar', 'foo.bar')\n-        test('%(title&foo)s.bar', 'NA.bar')\n-        test('%(title&foo|baz)s.bar', 'baz.bar')\n-        test('%(x,id&foo|baz)s.bar', 'foo.bar')\n-        test('%(x,title&foo|baz)s.bar', 'baz.bar')\n-        test('%(id&a\\nb|)s', ('a\\nb', 'a b'))\n-        test('%(id&hi {:>10} {}|)s', 'hi       1234 1234')\n-        test(R'%(id&{0} {}|)s', 'NA')\n-        test(R'%(id&{0.1}|)s', 'NA')\n-        test('%(height&{:,d})S', '1,080')\n+        test(\"%(id&foo)s.bar\", \"foo.bar\")\n+        test(\"%(title&foo)s.bar\", \"NA.bar\")\n+        test(\"%(title&foo|baz)s.bar\", \"baz.bar\")\n+        test(\"%(x,id&foo|baz)s.bar\", \"foo.bar\")\n+        test(\"%(x,title&foo|baz)s.bar\", \"baz.bar\")\n+        test(\"%(id&a\\nb|)s\", (\"a\\nb\", \"a b\"))\n+        test(\"%(id&hi {:>10} {}|)s\", \"hi       1234 1234\")\n+        test(R\"%(id&{0} {}|)s\", \"NA\")\n+        test(R\"%(id&{0.1}|)s\", \"NA\")\n+        test(\"%(height&{:,d})S\", \"1,080\")\n \n         # Laziness\n         def gen():\n             yield from range(5)\n-            raise self.assertTrue(False, 'LazyList should not be evaluated till here')\n-        test('%(key.4)s', '4', info={'key': LazyList(gen())})\n+            raise self.assertTrue(False, \"LazyList should not be evaluated till here\")\n+\n+        test(\"%(key.4)s\", \"4\", info={\"key\": LazyList(gen())})\n \n         # Empty filename\n-        test('%(foo|)s-%(bar|)s.%(ext)s', '-.mp4')\n+        test(\"%(foo|)s-%(bar|)s.%(ext)s\", \"-.mp4\")\n         # test('%(foo|)s.%(ext)s', ('.mp4', '_.mp4'))  # fixme\n         # test('%(foo|)s', ('', '_'))  # fixme\n \n         # Environment variable expansion for prepare_filename\n-        os.environ['__yt_dlp_var'] = 'expanded'\n-        envvar = '%__yt_dlp_var%' if compat_os_name == 'nt' else '$__yt_dlp_var'\n-        test(envvar, (envvar, 'expanded'))\n-        if compat_os_name == 'nt':\n-            test('%s%', ('%s%', '%s%'))\n-            os.environ['s'] = 'expanded'\n-            test('%s%', ('%s%', 'expanded'))  # %s% should be expanded before escaping %s\n-            os.environ['(test)s'] = 'expanded'\n-            test('%(test)s%', ('NA%', 'expanded'))  # Environment should take priority over template\n+        os.environ[\"__yt_dlp_var\"] = \"expanded\"\n+        envvar = \"%__yt_dlp_var%\" if compat_os_name == \"nt\" else \"$__yt_dlp_var\"\n+        test(envvar, (envvar, \"expanded\"))\n+        if compat_os_name == \"nt\":\n+            test(\"%s%\", (\"%s%\", \"%s%\"))\n+            os.environ[\"s\"] = \"expanded\"\n+            test(\n+                \"%s%\", (\"%s%\", \"expanded\")\n+            )  # %s% should be expanded before escaping %s\n+            os.environ[\"(test)s\"] = \"expanded\"\n+            test(\n+                \"%(test)s%\", (\"NA%\", \"expanded\")\n+            )  # Environment should take priority over template\n \n         # Path expansion and escaping\n-        test('Hello %(title1)s', 'Hello $PATH')\n-        test('Hello %(title2)s', 'Hello %PATH%')\n-        test('%(title3)s', ('foo/bar\\\\test', 'foo\u29f8bar\u29f9test'))\n-        test('folder/%(title3)s', ('folder/foo/bar\\\\test', 'folder%sfoo\u29f8bar\u29f9test' % os.path.sep))\n+        test(\"Hello %(title1)s\", \"Hello $PATH\")\n+        test(\"Hello %(title2)s\", \"Hello %PATH%\")\n+        test(\"%(title3)s\", (\"foo/bar\\\\test\", \"foo\u29f8bar\u29f9test\"))\n+        test(\n+            \"folder/%(title3)s\",\n+            (\"folder/foo/bar\\\\test\", \"folder%sfoo\u29f8bar\u29f9test\" % os.path.sep),\n+        )\n \n     def test_format_note(self):\n         ydl = YoutubeDL()\n-        self.assertEqual(ydl._format_note({}), '')\n-        assertRegexpMatches(self, ydl._format_note({\n-            'vbr': 10,\n-        }), r'^\\s*10k$')\n-        assertRegexpMatches(self, ydl._format_note({\n-            'fps': 30,\n-        }), r'^30fps$')\n+        self.assertEqual(ydl._format_note({}), \"\")\n+        assertRegexpMatches(\n+            self,\n+            ydl._format_note(\n+                {\n+                    \"vbr\": 10,\n+                }\n+            ),\n+            r\"^\\s*10k$\",\n+        )\n+        assertRegexpMatches(\n+            self,\n+            ydl._format_note(\n+                {\n+                    \"fps\": 30,\n+                }\n+            ),\n+            r\"^30fps$\",\n+        )\n \n     def test_postprocessors(self):\n-        filename = 'post-processor-testfile.mp4'\n-        audiofile = filename + '.mp3'\n+        filename = \"post-processor-testfile.mp4\"\n+        audiofile = filename + \".mp3\"\n \n         class SimplePP(PostProcessor):\n             def run(self, info):\n-                with open(audiofile, 'w') as f:\n-                    f.write('EXAMPLE')\n-                return [info['filepath']], info\n+                with open(audiofile, \"w\") as f:\n+                    f.write(\"EXAMPLE\")\n+                return [info[\"filepath\"]], info\n \n         def run_pp(params, PP):\n-            with open(filename, 'w') as f:\n-                f.write('EXAMPLE')\n+            with open(filename, \"w\") as f:\n+                f.write(\"EXAMPLE\")\n             ydl = YoutubeDL(params)\n             ydl.add_post_processor(PP())\n-            ydl.post_process(filename, {'filepath': filename})\n+            ydl.post_process(filename, {\"filepath\": filename})\n \n-        run_pp({'keepvideo': True}, SimplePP)\n-        self.assertTrue(os.path.exists(filename), '%s doesn\\'t exist' % filename)\n-        self.assertTrue(os.path.exists(audiofile), '%s doesn\\'t exist' % audiofile)\n+        run_pp({\"keepvideo\": True}, SimplePP)\n+        self.assertTrue(os.path.exists(filename), \"%s doesn't exist\" % filename)\n+        self.assertTrue(os.path.exists(audiofile), \"%s doesn't exist\" % audiofile)\n         os.unlink(filename)\n         os.unlink(audiofile)\n \n-        run_pp({'keepvideo': False}, SimplePP)\n-        self.assertFalse(os.path.exists(filename), '%s exists' % filename)\n-        self.assertTrue(os.path.exists(audiofile), '%s doesn\\'t exist' % audiofile)\n+        run_pp({\"keepvideo\": False}, SimplePP)\n+        self.assertFalse(os.path.exists(filename), \"%s exists\" % filename)\n+        self.assertTrue(os.path.exists(audiofile), \"%s doesn't exist\" % audiofile)\n         os.unlink(audiofile)\n \n         class ModifierPP(PostProcessor):\n             def run(self, info):\n-                with open(info['filepath'], 'w') as f:\n-                    f.write('MODIFIED')\n+                with open(info[\"filepath\"], \"w\") as f:\n+                    f.write(\"MODIFIED\")\n                 return [], info\n \n-        run_pp({'keepvideo': False}, ModifierPP)\n-        self.assertTrue(os.path.exists(filename), '%s doesn\\'t exist' % filename)\n+        run_pp({\"keepvideo\": False}, ModifierPP)\n+        self.assertTrue(os.path.exists(filename), \"%s doesn't exist\" % filename)\n         os.unlink(filename)\n \n     def test_match_filter(self):\n         first = {\n-            'id': '1',\n-            'url': TEST_URL,\n-            'title': 'one',\n-            'extractor': 'TEST',\n-            'duration': 30,\n-            'filesize': 10 * 1024,\n-            'playlist_id': '42',\n-            'uploader': \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\",\n-            'creator': \"\u0442\u0435\u0441\u0442 ' 123 ' \u0442\u0435\u0441\u0442--\",\n-            'webpage_url': 'http://example.com/watch?v=shenanigans',\n+            \"id\": \"1\",\n+            \"url\": TEST_URL,\n+            \"title\": \"one\",\n+            \"extractor\": \"TEST\",\n+            \"duration\": 30,\n+            \"filesize\": 10 * 1024,\n+            \"playlist_id\": \"42\",\n+            \"uploader\": \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\",\n+            \"creator\": \"\u0442\u0435\u0441\u0442 ' 123 ' \u0442\u0435\u0441\u0442--\",\n+            \"webpage_url\": \"http://example.com/watch?v=shenanigans\",\n         }\n         second = {\n-            'id': '2',\n-            'url': TEST_URL,\n-            'title': 'two',\n-            'extractor': 'TEST',\n-            'duration': 10,\n-            'description': 'foo',\n-            'filesize': 5 * 1024,\n-            'playlist_id': '43',\n-            'uploader': \"\u0442\u0435\u0441\u0442 123\",\n-            'webpage_url': 'http://example.com/watch?v=SHENANIGANS',\n+            \"id\": \"2\",\n+            \"url\": TEST_URL,\n+            \"title\": \"two\",\n+            \"extractor\": \"TEST\",\n+            \"duration\": 10,\n+            \"description\": \"foo\",\n+            \"filesize\": 5 * 1024,\n+            \"playlist_id\": \"43\",\n+            \"uploader\": \"\u0442\u0435\u0441\u0442 123\",\n+            \"webpage_url\": \"http://example.com/watch?v=SHENANIGANS\",\n         }\n         videos = [first, second]\n \n         def get_videos(filter_=None):\n-            ydl = YDL({'match_filter': filter_, 'simulate': True})\n+            ydl = YDL({\"match_filter\": filter_, \"simulate\": True})\n             for v in videos:\n                 ydl.process_ie_result(v, download=True)\n-            return [v['id'] for v in ydl.downloaded_info_dicts]\n+            return [v[\"id\"] for v in ydl.downloaded_info_dicts]\n \n         res = get_videos()\n-        self.assertEqual(res, ['1', '2'])\n+        self.assertEqual(res, [\"1\", \"2\"])\n \n         def f(v, incomplete):\n-            if v['id'] == '1':\n+            if v[\"id\"] == \"1\":\n                 return None\n             else:\n-                return 'Video id is not 1'\n+                return \"Video id is not 1\"\n+\n         res = get_videos(f)\n-        self.assertEqual(res, ['1'])\n+        self.assertEqual(res, [\"1\"])\n \n-        f = match_filter_func('duration < 30')\n+        f = match_filter_func(\"duration < 30\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['2'])\n+        self.assertEqual(res, [\"2\"])\n \n-        f = match_filter_func('description = foo')\n+        f = match_filter_func(\"description = foo\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['2'])\n+        self.assertEqual(res, [\"2\"])\n \n-        f = match_filter_func('description =? foo')\n+        f = match_filter_func(\"description =? foo\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['1', '2'])\n+        self.assertEqual(res, [\"1\", \"2\"])\n \n-        f = match_filter_func('filesize > 5KiB')\n+        f = match_filter_func(\"filesize > 5KiB\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['1'])\n+        self.assertEqual(res, [\"1\"])\n \n-        f = match_filter_func('playlist_id = 42')\n+        f = match_filter_func(\"playlist_id = 42\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['1'])\n+        self.assertEqual(res, [\"1\"])\n \n         f = match_filter_func('uploader = \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\"')\n         res = get_videos(f)\n-        self.assertEqual(res, ['1'])\n+        self.assertEqual(res, [\"1\"])\n \n         f = match_filter_func('uploader != \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\"')\n         res = get_videos(f)\n-        self.assertEqual(res, ['2'])\n+        self.assertEqual(res, [\"2\"])\n \n-        f = match_filter_func('creator = \"\u0442\u0435\u0441\u0442 \\' 123 \\' \u0442\u0435\u0441\u0442--\"')\n+        f = match_filter_func(\"creator = \\\"\u0442\u0435\u0441\u0442 ' 123 ' \u0442\u0435\u0441\u0442--\\\"\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['1'])\n+        self.assertEqual(res, [\"1\"])\n \n         f = match_filter_func(\"creator = '\u0442\u0435\u0441\u0442 \\\\' 123 \\\\' \u0442\u0435\u0441\u0442--'\")\n         res = get_videos(f)\n-        self.assertEqual(res, ['1'])\n+        self.assertEqual(res, [\"1\"])\n \n         f = match_filter_func(r\"creator = '\u0442\u0435\u0441\u0442 \\' 123 \\' \u0442\u0435\u0441\u0442--' & duration > 30\")\n         res = get_videos(f)\n@@ -1001,16 +1198,17 @@ def test_playlist_items_selection(self):\n         def entry(i, evaluated):\n             evaluated.append(i)\n             return {\n-                'id': str(i),\n-                'title': str(i),\n-                'url': TEST_URL,\n+                \"id\": str(i),\n+                \"title\": str(i),\n+                \"url\": TEST_URL,\n             }\n \n         def pagedlist_entries(evaluated):\n             def page_func(n):\n                 start = PAGE_SIZE * n\n-                for i in INDICES[start: start + PAGE_SIZE]:\n+                for i in INDICES[start : start + PAGE_SIZE]:\n                     yield entry(i, evaluated)\n+\n             return OnDemandPagedList(page_func, PAGE_SIZE)\n \n         def page_num(i):\n@@ -1028,14 +1226,16 @@ def lazylist_entries(evaluated):\n \n         def get_downloaded_info_dicts(params, entries):\n             ydl = YDL(params)\n-            ydl.process_ie_result({\n-                '_type': 'playlist',\n-                'id': 'test',\n-                'extractor': 'test:playlist',\n-                'extractor_key': 'test:playlist',\n-                'webpage_url': 'http://example.com',\n-                'entries': entries,\n-            })\n+            ydl.process_ie_result(\n+                {\n+                    \"_type\": \"playlist\",\n+                    \"id\": \"test\",\n+                    \"extractor\": \"test:playlist\",\n+                    \"extractor_key\": \"test:playlist\",\n+                    \"webpage_url\": \"http://example.com\",\n+                    \"entries\": entries,\n+                }\n+            )\n             return ydl.downloaded_info_dicts\n \n         def test_selection(params, expected_ids, evaluate_all=False):\n@@ -1045,111 +1245,122 @@ def test_selection(params, expected_ids, evaluate_all=False):\n             elif not expected_ids:\n                 generator_eval = pagedlist_eval = []\n             else:\n-                generator_eval = INDICES[0: max(expected_ids)]\n-                pagedlist_eval = INDICES[PAGE_SIZE * page_num(min(expected_ids)) - PAGE_SIZE:\n-                                         PAGE_SIZE * page_num(max(expected_ids))]\n+                generator_eval = INDICES[0 : max(expected_ids)]\n+                pagedlist_eval = INDICES[\n+                    PAGE_SIZE * page_num(min(expected_ids)) - PAGE_SIZE : PAGE_SIZE\n+                    * page_num(max(expected_ids))\n+                ]\n \n             for name, func, expected_eval in (\n-                ('list', list_entries, INDICES),\n-                ('Generator', generator_entries, generator_eval),\n+                (\"list\", list_entries, INDICES),\n+                (\"Generator\", generator_entries, generator_eval),\n                 # ('LazyList', lazylist_entries, generator_eval),  # Generator and LazyList follow the exact same code path\n-                ('PagedList', pagedlist_entries, pagedlist_eval),\n+                (\"PagedList\", pagedlist_entries, pagedlist_eval),\n             ):\n                 evaluated = []\n                 entries = func(evaluated)\n-                results = [(v['playlist_autonumber'] - 1, (int(v['id']), v['playlist_index']))\n-                           for v in get_downloaded_info_dicts(params, entries)]\n-                self.assertEqual(results, list(enumerate(zip(expected_ids, expected_ids))), f'Entries of {name} for {params}')\n-                self.assertEqual(sorted(evaluated), expected_eval, f'Evaluation of {name} for {params}')\n+                results = [\n+                    (v[\"playlist_autonumber\"] - 1, (int(v[\"id\"]), v[\"playlist_index\"]))\n+                    for v in get_downloaded_info_dicts(params, entries)\n+                ]\n+                self.assertEqual(\n+                    results,\n+                    list(enumerate(zip(expected_ids, expected_ids))),\n+                    f\"Entries of {name} for {params}\",\n+                )\n+                self.assertEqual(\n+                    sorted(evaluated),\n+                    expected_eval,\n+                    f\"Evaluation of {name} for {params}\",\n+                )\n \n         test_selection({}, INDICES)\n-        test_selection({'playlistend': 20}, INDICES, True)\n-        test_selection({'playlistend': 2}, INDICES[:2])\n-        test_selection({'playliststart': 11}, [], True)\n-        test_selection({'playliststart': 2}, INDICES[1:])\n-        test_selection({'playlist_items': '2-4'}, INDICES[1:4])\n-        test_selection({'playlist_items': '2,4'}, [2, 4])\n-        test_selection({'playlist_items': '20'}, [], True)\n-        test_selection({'playlist_items': '0'}, [])\n+        test_selection({\"playlistend\": 20}, INDICES, True)\n+        test_selection({\"playlistend\": 2}, INDICES[:2])\n+        test_selection({\"playliststart\": 11}, [], True)\n+        test_selection({\"playliststart\": 2}, INDICES[1:])\n+        test_selection({\"playlist_items\": \"2-4\"}, INDICES[1:4])\n+        test_selection({\"playlist_items\": \"2,4\"}, [2, 4])\n+        test_selection({\"playlist_items\": \"20\"}, [], True)\n+        test_selection({\"playlist_items\": \"0\"}, [])\n \n         # Tests for https://github.com/ytdl-org/youtube-dl/issues/10591\n-        test_selection({'playlist_items': '2-4,3-4,3'}, [2, 3, 4])\n-        test_selection({'playlist_items': '4,2'}, [4, 2])\n+        test_selection({\"playlist_items\": \"2-4,3-4,3\"}, [2, 3, 4])\n+        test_selection({\"playlist_items\": \"4,2\"}, [4, 2])\n \n         # Tests for https://github.com/yt-dlp/yt-dlp/issues/720\n         # https://github.com/yt-dlp/yt-dlp/issues/302\n-        test_selection({'playlistreverse': True}, INDICES[::-1])\n-        test_selection({'playliststart': 2, 'playlistreverse': True}, INDICES[:0:-1])\n-        test_selection({'playlist_items': '2,4', 'playlistreverse': True}, [4, 2])\n-        test_selection({'playlist_items': '4,2'}, [4, 2])\n+        test_selection({\"playlistreverse\": True}, INDICES[::-1])\n+        test_selection({\"playliststart\": 2, \"playlistreverse\": True}, INDICES[:0:-1])\n+        test_selection({\"playlist_items\": \"2,4\", \"playlistreverse\": True}, [4, 2])\n+        test_selection({\"playlist_items\": \"4,2\"}, [4, 2])\n \n         # Tests for --playlist-items start:end:step\n-        test_selection({'playlist_items': ':'}, INDICES, True)\n-        test_selection({'playlist_items': '::1'}, INDICES, True)\n-        test_selection({'playlist_items': '::-1'}, INDICES[::-1], True)\n-        test_selection({'playlist_items': ':6'}, INDICES[:6])\n-        test_selection({'playlist_items': ':-6'}, INDICES[:-5], True)\n-        test_selection({'playlist_items': '-1:6:-2'}, INDICES[:4:-2], True)\n-        test_selection({'playlist_items': '9:-6:-2'}, INDICES[8:3:-2], True)\n-\n-        test_selection({'playlist_items': '1:inf:2'}, INDICES[::2], True)\n-        test_selection({'playlist_items': '-2:inf'}, INDICES[-2:], True)\n-        test_selection({'playlist_items': ':inf:-1'}, [], True)\n-        test_selection({'playlist_items': '0-2:2'}, [2])\n-        test_selection({'playlist_items': '1-:2'}, INDICES[::2], True)\n-        test_selection({'playlist_items': '0--2:2'}, INDICES[1:-1:2], True)\n-\n-        test_selection({'playlist_items': '10::3'}, [10], True)\n-        test_selection({'playlist_items': '-1::3'}, [10], True)\n-        test_selection({'playlist_items': '11::3'}, [], True)\n-        test_selection({'playlist_items': '-15::2'}, INDICES[1::2], True)\n-        test_selection({'playlist_items': '-15::15'}, [], True)\n+        test_selection({\"playlist_items\": \":\"}, INDICES, True)\n+        test_selection({\"playlist_items\": \"::1\"}, INDICES, True)\n+        test_selection({\"playlist_items\": \"::-1\"}, INDICES[::-1], True)\n+        test_selection({\"playlist_items\": \":6\"}, INDICES[:6])\n+        test_selection({\"playlist_items\": \":-6\"}, INDICES[:-5], True)\n+        test_selection({\"playlist_items\": \"-1:6:-2\"}, INDICES[:4:-2], True)\n+        test_selection({\"playlist_items\": \"9:-6:-2\"}, INDICES[8:3:-2], True)\n+\n+        test_selection({\"playlist_items\": \"1:inf:2\"}, INDICES[::2], True)\n+        test_selection({\"playlist_items\": \"-2:inf\"}, INDICES[-2:], True)\n+        test_selection({\"playlist_items\": \":inf:-1\"}, [], True)\n+        test_selection({\"playlist_items\": \"0-2:2\"}, [2])\n+        test_selection({\"playlist_items\": \"1-:2\"}, INDICES[::2], True)\n+        test_selection({\"playlist_items\": \"0--2:2\"}, INDICES[1:-1:2], True)\n+\n+        test_selection({\"playlist_items\": \"10::3\"}, [10], True)\n+        test_selection({\"playlist_items\": \"-1::3\"}, [10], True)\n+        test_selection({\"playlist_items\": \"11::3\"}, [], True)\n+        test_selection({\"playlist_items\": \"-15::2\"}, INDICES[1::2], True)\n+        test_selection({\"playlist_items\": \"-15::15\"}, [], True)\n \n     def test_do_not_override_ie_key_in_url_transparent(self):\n         ydl = YDL()\n \n         class Foo1IE(InfoExtractor):\n-            _VALID_URL = r'foo1:'\n+            _VALID_URL = r\"foo1:\"\n \n             def _real_extract(self, url):\n                 return {\n-                    '_type': 'url_transparent',\n-                    'url': 'foo2:',\n-                    'ie_key': 'Foo2',\n-                    'title': 'foo1 title',\n-                    'id': 'foo1_id',\n+                    \"_type\": \"url_transparent\",\n+                    \"url\": \"foo2:\",\n+                    \"ie_key\": \"Foo2\",\n+                    \"title\": \"foo1 title\",\n+                    \"id\": \"foo1_id\",\n                 }\n \n         class Foo2IE(InfoExtractor):\n-            _VALID_URL = r'foo2:'\n+            _VALID_URL = r\"foo2:\"\n \n             def _real_extract(self, url):\n                 return {\n-                    '_type': 'url',\n-                    'url': 'foo3:',\n-                    'ie_key': 'Foo3',\n+                    \"_type\": \"url\",\n+                    \"url\": \"foo3:\",\n+                    \"ie_key\": \"Foo3\",\n                 }\n \n         class Foo3IE(InfoExtractor):\n-            _VALID_URL = r'foo3:'\n+            _VALID_URL = r\"foo3:\"\n \n             def _real_extract(self, url):\n-                return _make_result([{'url': TEST_URL}], title='foo3 title')\n+                return _make_result([{\"url\": TEST_URL}], title=\"foo3 title\")\n \n         ydl.add_info_extractor(Foo1IE(ydl))\n         ydl.add_info_extractor(Foo2IE(ydl))\n         ydl.add_info_extractor(Foo3IE(ydl))\n-        ydl.extract_info('foo1:')\n+        ydl.extract_info(\"foo1:\")\n         downloaded = ydl.downloaded_info_dicts[0]\n-        self.assertEqual(downloaded['url'], TEST_URL)\n-        self.assertEqual(downloaded['title'], 'foo1 title')\n-        self.assertEqual(downloaded['id'], 'testid')\n-        self.assertEqual(downloaded['extractor'], 'testex')\n-        self.assertEqual(downloaded['extractor_key'], 'TestEx')\n+        self.assertEqual(downloaded[\"url\"], TEST_URL)\n+        self.assertEqual(downloaded[\"title\"], \"foo1 title\")\n+        self.assertEqual(downloaded[\"id\"], \"testid\")\n+        self.assertEqual(downloaded[\"extractor\"], \"testex\")\n+        self.assertEqual(downloaded[\"extractor_key\"], \"TestEx\")\n \n     # Test case for https://github.com/ytdl-org/youtube-dl/issues/27064\n     def test_ignoreerrors_for_playlist_with_url_transparent_iterable_entries(self):\n-\n         class _YDL(YDL):\n             def __init__(self, *args, **kwargs):\n                 super().__init__(*args, **kwargs)\n@@ -1157,45 +1368,51 @@ def __init__(self, *args, **kwargs):\n             def trouble(self, s, tb=None):\n                 pass\n \n-        ydl = _YDL({\n-            'format': 'extra',\n-            'ignoreerrors': True,\n-        })\n+        ydl = _YDL(\n+            {\n+                \"format\": \"extra\",\n+                \"ignoreerrors\": True,\n+            }\n+        )\n \n         class VideoIE(InfoExtractor):\n-            _VALID_URL = r'video:(?P<id>\\d+)'\n+            _VALID_URL = r\"video:(?P<id>\\d+)\"\n \n             def _real_extract(self, url):\n                 video_id = self._match_id(url)\n-                formats = [{\n-                    'format_id': 'default',\n-                    'url': 'url:',\n-                }]\n-                if video_id == '0':\n-                    raise ExtractorError('foo')\n-                if video_id == '2':\n-                    formats.append({\n-                        'format_id': 'extra',\n-                        'url': TEST_URL,\n-                    })\n+                formats = [\n+                    {\n+                        \"format_id\": \"default\",\n+                        \"url\": \"url:\",\n+                    }\n+                ]\n+                if video_id == \"0\":\n+                    raise ExtractorError(\"foo\")\n+                if video_id == \"2\":\n+                    formats.append(\n+                        {\n+                            \"format_id\": \"extra\",\n+                            \"url\": TEST_URL,\n+                        }\n+                    )\n                 return {\n-                    'id': video_id,\n-                    'title': 'Video %s' % video_id,\n-                    'formats': formats,\n+                    \"id\": video_id,\n+                    \"title\": \"Video %s\" % video_id,\n+                    \"formats\": formats,\n                 }\n \n         class PlaylistIE(InfoExtractor):\n-            _VALID_URL = r'playlist:'\n+            _VALID_URL = r\"playlist:\"\n \n             def _entries(self):\n                 for n in range(3):\n                     video_id = str(n)\n                     yield {\n-                        '_type': 'url_transparent',\n-                        'ie_key': VideoIE.ie_key(),\n-                        'id': video_id,\n-                        'url': 'video:%s' % video_id,\n-                        'title': 'Video Transparent %s' % video_id,\n+                        \"_type\": \"url_transparent\",\n+                        \"ie_key\": VideoIE.ie_key(),\n+                        \"id\": video_id,\n+                        \"url\": \"video:%s\" % video_id,\n+                        \"title\": \"Video Transparent %s\" % video_id,\n                     }\n \n             def _real_extract(self, url):\n@@ -1203,20 +1420,20 @@ def _real_extract(self, url):\n \n         ydl.add_info_extractor(VideoIE(ydl))\n         ydl.add_info_extractor(PlaylistIE(ydl))\n-        info = ydl.extract_info('playlist:')\n-        entries = info['entries']\n+        info = ydl.extract_info(\"playlist:\")\n+        entries = info[\"entries\"]\n         self.assertEqual(len(entries), 3)\n         self.assertTrue(entries[0] is None)\n         self.assertTrue(entries[1] is None)\n         self.assertEqual(len(ydl.downloaded_info_dicts), 1)\n         downloaded = ydl.downloaded_info_dicts[0]\n-        entries[2].pop('requested_downloads', None)\n+        entries[2].pop(\"requested_downloads\", None)\n         self.assertEqual(entries[2], downloaded)\n-        self.assertEqual(downloaded['url'], TEST_URL)\n-        self.assertEqual(downloaded['title'], 'Video Transparent 2')\n-        self.assertEqual(downloaded['id'], '2')\n-        self.assertEqual(downloaded['extractor'], 'Video')\n-        self.assertEqual(downloaded['extractor_key'], 'Video')\n+        self.assertEqual(downloaded[\"url\"], TEST_URL)\n+        self.assertEqual(downloaded[\"title\"], \"Video Transparent 2\")\n+        self.assertEqual(downloaded[\"id\"], \"2\")\n+        self.assertEqual(downloaded[\"extractor\"], \"Video\")\n+        self.assertEqual(downloaded[\"extractor_key\"], \"Video\")\n \n     def test_header_cookies(self):\n         from http.cookiejar import Cookie\n@@ -1224,30 +1441,52 @@ def test_header_cookies(self):\n         ydl = FakeYDL()\n         ydl.report_warning = lambda *_, **__: None\n \n-        def cookie(name, value, version=None, domain='', path='', secure=False, expires=None):\n+        def cookie(\n+            name, value, version=None, domain=\"\", path=\"\", secure=False, expires=None\n+        ):\n             return Cookie(\n-                version or 0, name, value, None, False,\n-                domain, bool(domain), bool(domain), path, bool(path),\n-                secure, expires, False, None, None, rest={})\n-\n-        _test_url = 'https://yt.dlp/test'\n-\n-        def test(encoded_cookies, cookies, *, headers=False, round_trip=None, error_re=None):\n+                version or 0,\n+                name,\n+                value,\n+                None,\n+                False,\n+                domain,\n+                bool(domain),\n+                bool(domain),\n+                path,\n+                bool(path),\n+                secure,\n+                expires,\n+                False,\n+                None,\n+                None,\n+                rest={},\n+            )\n+\n+        _test_url = \"https://yt.dlp/test\"\n+\n+        def test(\n+            encoded_cookies, cookies, *, headers=False, round_trip=None, error_re=None\n+        ):\n             def _test():\n                 ydl.cookiejar.clear()\n                 ydl._load_cookies(encoded_cookies, autoscope=headers)\n                 if headers:\n                     ydl._apply_header_cookies(_test_url)\n-                data = {'url': _test_url}\n+                data = {\"url\": _test_url}\n                 ydl._calc_headers(data)\n                 self.assertCountEqual(\n-                    map(vars, ydl.cookiejar), map(vars, cookies),\n-                    'Extracted cookiejar.Cookie is not the same')\n+                    map(vars, ydl.cookiejar),\n+                    map(vars, cookies),\n+                    \"Extracted cookiejar.Cookie is not the same\",\n+                )\n                 if not headers:\n                     self.assertEqual(\n-                        data.get('cookies'), round_trip or encoded_cookies,\n-                        'Cookie is not the same as round trip')\n-                ydl.__dict__['_YoutubeDL__header_cookies'] = []\n+                        data.get(\"cookies\"),\n+                        round_trip or encoded_cookies,\n+                        \"Cookie is not the same as round trip\",\n+                    )\n+                ydl.__dict__[\"_YoutubeDL__header_cookies\"] = []\n \n             with self.subTest(msg=encoded_cookies):\n                 if not error_re:\n@@ -1256,91 +1495,184 @@ def _test():\n                 with self.assertRaisesRegex(Exception, error_re):\n                     _test()\n \n-        test('test=value; Domain=.yt.dlp', [cookie('test', 'value', domain='.yt.dlp')])\n-        test('test=value', [cookie('test', 'value')], error_re=r'Unscoped cookies are not allowed')\n-        test('cookie1=value1; Domain=.yt.dlp; Path=/test; cookie2=value2; Domain=.yt.dlp; Path=/', [\n-            cookie('cookie1', 'value1', domain='.yt.dlp', path='/test'),\n-            cookie('cookie2', 'value2', domain='.yt.dlp', path='/')])\n-        test('test=value; Domain=.yt.dlp; Path=/test; Secure; Expires=9999999999', [\n-            cookie('test', 'value', domain='.yt.dlp', path='/test', secure=True, expires=9999999999)])\n-        test('test=\"value; \"; path=/test; domain=.yt.dlp', [\n-            cookie('test', 'value; ', domain='.yt.dlp', path='/test')],\n-            round_trip='test=\"value\\\\073 \"; Domain=.yt.dlp; Path=/test')\n-        test('name=; Domain=.yt.dlp', [cookie('name', '', domain='.yt.dlp')],\n-             round_trip='name=\"\"; Domain=.yt.dlp')\n-\n-        test('test=value', [cookie('test', 'value', domain='.yt.dlp')], headers=True)\n-        test('cookie1=value; Domain=.yt.dlp; cookie2=value', [], headers=True, error_re=r'Invalid syntax')\n+        test(\"test=value; Domain=.yt.dlp\", [cookie(\"test\", \"value\", domain=\".yt.dlp\")])\n+        test(\n+            \"test=value\",\n+            [cookie(\"test\", \"value\")],\n+            error_re=r\"Unscoped cookies are not allowed\",\n+        )\n+        test(\n+            \"cookie1=value1; Domain=.yt.dlp; Path=/test; cookie2=value2; Domain=.yt.dlp; Path=/\",\n+            [\n+                cookie(\"cookie1\", \"value1\", domain=\".yt.dlp\", path=\"/test\"),\n+                cookie(\"cookie2\", \"value2\", domain=\".yt.dlp\", path=\"/\"),\n+            ],\n+        )\n+        test(\n+            \"test=value; Domain=.yt.dlp; Path=/test; Secure; Expires=9999999999\",\n+            [\n+                cookie(\n+                    \"test\",\n+                    \"value\",\n+                    domain=\".yt.dlp\",\n+                    path=\"/test\",\n+                    secure=True,\n+                    expires=9999999999,\n+                )\n+            ],\n+        )\n+        test(\n+            'test=\"value; \"; path=/test; domain=.yt.dlp',\n+            [cookie(\"test\", \"value; \", domain=\".yt.dlp\", path=\"/test\")],\n+            round_trip='test=\"value\\\\073 \"; Domain=.yt.dlp; Path=/test',\n+        )\n+        test(\n+            \"name=; Domain=.yt.dlp\",\n+            [cookie(\"name\", \"\", domain=\".yt.dlp\")],\n+            round_trip='name=\"\"; Domain=.yt.dlp',\n+        )\n+\n+        test(\"test=value\", [cookie(\"test\", \"value\", domain=\".yt.dlp\")], headers=True)\n+        test(\n+            \"cookie1=value; Domain=.yt.dlp; cookie2=value\",\n+            [],\n+            headers=True,\n+            error_re=r\"Invalid syntax\",\n+        )\n         ydl.deprecated_feature = ydl.report_error\n-        test('test=value', [], headers=True, error_re=r'Passing cookies as a header is a potential security risk')\n+        test(\n+            \"test=value\",\n+            [],\n+            headers=True,\n+            error_re=r\"Passing cookies as a header is a potential security risk\",\n+        )\n \n     def test_infojson_cookies(self):\n-        TEST_FILE = 'test_infojson_cookies.info.json'\n-        TEST_URL = 'https://example.com/example.mp4'\n-        COOKIES = 'a=b; Domain=.example.com; c=d; Domain=.example.com'\n-        COOKIE_HEADER = {'Cookie': 'a=b; c=d'}\n+        TEST_FILE = \"test_infojson_cookies.info.json\"\n+        TEST_URL = \"https://example.com/example.mp4\"\n+        COOKIES = \"a=b; Domain=.example.com; c=d; Domain=.example.com\"\n+        COOKIE_HEADER = {\"Cookie\": \"a=b; c=d\"}\n \n         ydl = FakeYDL()\n-        ydl.process_info = lambda x: ydl._write_info_json('test', x, TEST_FILE)\n+        ydl.process_info = lambda x: ydl._write_info_json(\"test\", x, TEST_FILE)\n \n-        def make_info(info_header_cookies=False, fmts_header_cookies=False, cookies_field=False):\n-            fmt = {'url': TEST_URL}\n+        def make_info(\n+            info_header_cookies=False, fmts_header_cookies=False, cookies_field=False\n+        ):\n+            fmt = {\"url\": TEST_URL}\n             if fmts_header_cookies:\n-                fmt['http_headers'] = COOKIE_HEADER\n+                fmt[\"http_headers\"] = COOKIE_HEADER\n             if cookies_field:\n-                fmt['cookies'] = COOKIES\n-            return _make_result([fmt], http_headers=COOKIE_HEADER if info_header_cookies else None)\n+                fmt[\"cookies\"] = COOKIES\n+            return _make_result(\n+                [fmt], http_headers=COOKIE_HEADER if info_header_cookies else None\n+            )\n \n         def test(initial_info, note):\n             result = {}\n-            result['processed'] = ydl.process_ie_result(initial_info)\n-            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n-                            msg=f'No cookies set in cookiejar after initial process when {note}')\n+            result[\"processed\"] = ydl.process_ie_result(initial_info)\n+            self.assertTrue(\n+                ydl.cookiejar.get_cookies_for_url(TEST_URL),\n+                msg=f\"No cookies set in cookiejar after initial process when {note}\",\n+            )\n             ydl.cookiejar.clear()\n             with open(TEST_FILE) as infojson:\n-                result['loaded'] = ydl.sanitize_info(json.load(infojson), True)\n-            result['final'] = ydl.process_ie_result(result['loaded'].copy(), download=False)\n-            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n-                            msg=f'No cookies set in cookiejar after final process when {note}')\n+                result[\"loaded\"] = ydl.sanitize_info(json.load(infojson), True)\n+            result[\"final\"] = ydl.process_ie_result(\n+                result[\"loaded\"].copy(), download=False\n+            )\n+            self.assertTrue(\n+                ydl.cookiejar.get_cookies_for_url(TEST_URL),\n+                msg=f\"No cookies set in cookiejar after final process when {note}\",\n+            )\n             ydl.cookiejar.clear()\n-            for key in ('processed', 'loaded', 'final'):\n+            for key in (\"processed\", \"loaded\", \"final\"):\n                 info = result[key]\n                 self.assertIsNone(\n-                    traverse_obj(info, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False),\n-                    msg=f'Cookie header not removed in {key} result when {note}')\n+                    traverse_obj(\n+                        info,\n+                        ((None, (\"formats\", 0)), \"http_headers\", \"Cookie\"),\n+                        casesense=False,\n+                        get_all=False,\n+                    ),\n+                    msg=f\"Cookie header not removed in {key} result when {note}\",\n+                )\n                 self.assertEqual(\n-                    traverse_obj(info, ((None, ('formats', 0)), 'cookies'), get_all=False), COOKIES,\n-                    msg=f'No cookies field found in {key} result when {note}')\n-\n-        test({'url': TEST_URL, 'http_headers': COOKIE_HEADER, 'id': '1', 'title': 'x'}, 'no formats field')\n-        test(make_info(info_header_cookies=True), 'info_dict header cokies')\n-        test(make_info(fmts_header_cookies=True), 'format header cookies')\n-        test(make_info(info_header_cookies=True, fmts_header_cookies=True), 'info_dict and format header cookies')\n-        test(make_info(info_header_cookies=True, fmts_header_cookies=True, cookies_field=True), 'all cookies fields')\n-        test(make_info(cookies_field=True), 'cookies format field')\n-        test({'url': TEST_URL, 'cookies': COOKIES, 'id': '1', 'title': 'x'}, 'info_dict cookies field only')\n+                    traverse_obj(\n+                        info, ((None, (\"formats\", 0)), \"cookies\"), get_all=False\n+                    ),\n+                    COOKIES,\n+                    msg=f\"No cookies field found in {key} result when {note}\",\n+                )\n+\n+        test(\n+            {\"url\": TEST_URL, \"http_headers\": COOKIE_HEADER, \"id\": \"1\", \"title\": \"x\"},\n+            \"no formats field\",\n+        )\n+        test(make_info(info_header_cookies=True), \"info_dict header cokies\")\n+        test(make_info(fmts_header_cookies=True), \"format header cookies\")\n+        test(\n+            make_info(info_header_cookies=True, fmts_header_cookies=True),\n+            \"info_dict and format header cookies\",\n+        )\n+        test(\n+            make_info(\n+                info_header_cookies=True, fmts_header_cookies=True, cookies_field=True\n+            ),\n+            \"all cookies fields\",\n+        )\n+        test(make_info(cookies_field=True), \"cookies format field\")\n+        test(\n+            {\"url\": TEST_URL, \"cookies\": COOKIES, \"id\": \"1\", \"title\": \"x\"},\n+            \"info_dict cookies field only\",\n+        )\n \n         try_rm(TEST_FILE)\n \n     def test_add_headers_cookie(self):\n         def check_for_cookie_header(result):\n-            return traverse_obj(result, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False)\n-\n-        ydl = FakeYDL({'http_headers': {'Cookie': 'a=b'}})\n-        ydl._apply_header_cookies(_make_result([])['webpage_url'])  # Scope to input webpage URL: .example.com\n-\n-        fmt = {'url': 'https://example.com/video.mp4'}\n+            return traverse_obj(\n+                result,\n+                ((None, (\"formats\", 0)), \"http_headers\", \"Cookie\"),\n+                casesense=False,\n+                get_all=False,\n+            )\n+\n+        ydl = FakeYDL({\"http_headers\": {\"Cookie\": \"a=b\"}})\n+        ydl._apply_header_cookies(\n+            _make_result([])[\"webpage_url\"]\n+        )  # Scope to input webpage URL: .example.com\n+\n+        fmt = {\"url\": \"https://example.com/video.mp4\"}\n         result = ydl.process_ie_result(_make_result([fmt]), download=False)\n-        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies in result info_dict')\n-        self.assertEqual(result.get('cookies'), 'a=b; Domain=.example.com', msg='No cookies were set in cookies field')\n-        self.assertIn('a=b', ydl.cookiejar.get_cookie_header(fmt['url']), msg='No cookies were set in cookiejar')\n-\n-        fmt = {'url': 'https://wrong.com/video.mp4'}\n+        self.assertIsNone(\n+            check_for_cookie_header(result),\n+            msg=\"http_headers cookies in result info_dict\",\n+        )\n+        self.assertEqual(\n+            result.get(\"cookies\"),\n+            \"a=b; Domain=.example.com\",\n+            msg=\"No cookies were set in cookies field\",\n+        )\n+        self.assertIn(\n+            \"a=b\",\n+            ydl.cookiejar.get_cookie_header(fmt[\"url\"]),\n+            msg=\"No cookies were set in cookiejar\",\n+        )\n+\n+        fmt = {\"url\": \"https://wrong.com/video.mp4\"}\n         result = ydl.process_ie_result(_make_result([fmt]), download=False)\n-        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies for wrong domain')\n-        self.assertFalse(result.get('cookies'), msg='Cookies set in cookies field for wrong domain')\n-        self.assertFalse(ydl.cookiejar.get_cookie_header(fmt['url']), msg='Cookies set in cookiejar for wrong domain')\n-\n-\n-if __name__ == '__main__':\n+        self.assertIsNone(\n+            check_for_cookie_header(result), msg=\"http_headers cookies for wrong domain\"\n+        )\n+        self.assertFalse(\n+            result.get(\"cookies\"), msg=\"Cookies set in cookies field for wrong domain\"\n+        )\n+        self.assertFalse(\n+            ydl.cookiejar.get_cookie_header(fmt[\"url\"]),\n+            msg=\"Cookies set in cookiejar for wrong domain\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_all_urls.py b/test/test_all_urls.py\nindex 848c96f..00d4b06 100644\n--- a/test/test_all_urls.py\n+++ b/test/test_all_urls.py\n@@ -19,104 +19,149 @@ def setUp(self):\n         self.ies = gen_extractors()\n \n     def matching_ies(self, url):\n-        return [ie.IE_NAME for ie in self.ies if ie.suitable(url) and ie.IE_NAME != 'generic']\n+        return [\n+            ie.IE_NAME\n+            for ie in self.ies\n+            if ie.suitable(url) and ie.IE_NAME != \"generic\"\n+        ]\n \n     def assertMatch(self, url, ie_list):\n         self.assertEqual(self.matching_ies(url), ie_list)\n \n     def test_youtube_playlist_matching(self):\n-        assertPlaylist = lambda url: self.assertMatch(url, ['youtube:playlist'])\n-        assertTab = lambda url: self.assertMatch(url, ['youtube:tab'])\n-        assertPlaylist('ECUl4u3cNGP61MdtwGTqZA0MreSaDybji8')\n-        assertPlaylist('UUBABnxM4Ar9ten8Mdjj1j0Q')  # 585\n-        assertPlaylist('PL63F0C78739B09958')\n-        assertTab('https://www.youtube.com/AsapSCIENCE')\n-        assertTab('https://www.youtube.com/embedded')\n-        assertTab('https://www.youtube.com/playlist?list=UUBABnxM4Ar9ten8Mdjj1j0Q')\n-        assertTab('https://www.youtube.com/playlist?list=PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC')\n-        assertTab('https://www.youtube.com/watch?v=AV6J6_AeFEQ&playnext=1&list=PL4023E734DA416012')  # 668\n-        self.assertFalse('youtube:playlist' in self.matching_ies('PLtS2H6bU1M'))\n+        assertPlaylist = lambda url: self.assertMatch(url, [\"youtube:playlist\"])\n+        assertTab = lambda url: self.assertMatch(url, [\"youtube:tab\"])\n+        assertPlaylist(\"ECUl4u3cNGP61MdtwGTqZA0MreSaDybji8\")\n+        assertPlaylist(\"UUBABnxM4Ar9ten8Mdjj1j0Q\")  # 585\n+        assertPlaylist(\"PL63F0C78739B09958\")\n+        assertTab(\"https://www.youtube.com/AsapSCIENCE\")\n+        assertTab(\"https://www.youtube.com/embedded\")\n+        assertTab(\"https://www.youtube.com/playlist?list=UUBABnxM4Ar9ten8Mdjj1j0Q\")\n+        assertTab(\n+            \"https://www.youtube.com/playlist?list=PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC\"\n+        )\n+        assertTab(\n+            \"https://www.youtube.com/watch?v=AV6J6_AeFEQ&playnext=1&list=PL4023E734DA416012\"\n+        )  # 668\n+        self.assertFalse(\"youtube:playlist\" in self.matching_ies(\"PLtS2H6bU1M\"))\n         # Top tracks\n-        assertTab('https://www.youtube.com/playlist?list=MCUS.20142101')\n+        assertTab(\"https://www.youtube.com/playlist?list=MCUS.20142101\")\n \n     def test_youtube_matching(self):\n-        self.assertTrue(YoutubeIE.suitable('PLtS2H6bU1M'))\n-        self.assertFalse(YoutubeIE.suitable('https://www.youtube.com/watch?v=AV6J6_AeFEQ&playnext=1&list=PL4023E734DA416012'))  # 668\n-        self.assertMatch('http://youtu.be/BaW_jenozKc', ['youtube'])\n+        self.assertTrue(YoutubeIE.suitable(\"PLtS2H6bU1M\"))\n+        self.assertFalse(\n+            YoutubeIE.suitable(\n+                \"https://www.youtube.com/watch?v=AV6J6_AeFEQ&playnext=1&list=PL4023E734DA416012\"\n+            )\n+        )  # 668\n+        self.assertMatch(\"http://youtu.be/BaW_jenozKc\", [\"youtube\"])\n         # self.assertMatch('http://www.youtube.com/v/BaW_jenozKc', ['youtube'])  # /v/ is no longer valid\n-        self.assertMatch('https://youtube.googleapis.com/v/BaW_jenozKc', ['youtube'])\n-        self.assertMatch('http://www.cleanvideosearch.com/media/action/yt/watch?videoId=8v_4O44sfjM', ['youtube'])\n+        self.assertMatch(\"https://youtube.googleapis.com/v/BaW_jenozKc\", [\"youtube\"])\n+        self.assertMatch(\n+            \"http://www.cleanvideosearch.com/media/action/yt/watch?videoId=8v_4O44sfjM\",\n+            [\"youtube\"],\n+        )\n \n     def test_youtube_channel_matching(self):\n-        assertChannel = lambda url: self.assertMatch(url, ['youtube:tab'])\n-        assertChannel('https://www.youtube.com/channel/HCtnHdj3df7iM')\n-        assertChannel('https://www.youtube.com/channel/HCtnHdj3df7iM?feature=gb_ch_rec')\n-        assertChannel('https://www.youtube.com/channel/HCtnHdj3df7iM/videos')\n+        assertChannel = lambda url: self.assertMatch(url, [\"youtube:tab\"])\n+        assertChannel(\"https://www.youtube.com/channel/HCtnHdj3df7iM\")\n+        assertChannel(\"https://www.youtube.com/channel/HCtnHdj3df7iM?feature=gb_ch_rec\")\n+        assertChannel(\"https://www.youtube.com/channel/HCtnHdj3df7iM/videos\")\n \n     def test_youtube_user_matching(self):\n-        self.assertMatch('http://www.youtube.com/NASAgovVideo/videos', ['youtube:tab'])\n+        self.assertMatch(\"http://www.youtube.com/NASAgovVideo/videos\", [\"youtube:tab\"])\n \n     def test_youtube_feeds(self):\n-        self.assertMatch('https://www.youtube.com/feed/library', ['youtube:tab'])\n-        self.assertMatch('https://www.youtube.com/feed/history', ['youtube:tab'])\n-        self.assertMatch('https://www.youtube.com/feed/watch_later', ['youtube:tab'])\n-        self.assertMatch('https://www.youtube.com/feed/subscriptions', ['youtube:tab'])\n+        self.assertMatch(\"https://www.youtube.com/feed/library\", [\"youtube:tab\"])\n+        self.assertMatch(\"https://www.youtube.com/feed/history\", [\"youtube:tab\"])\n+        self.assertMatch(\"https://www.youtube.com/feed/watch_later\", [\"youtube:tab\"])\n+        self.assertMatch(\"https://www.youtube.com/feed/subscriptions\", [\"youtube:tab\"])\n \n     def test_youtube_search_matching(self):\n-        self.assertMatch('http://www.youtube.com/results?search_query=making+mustard', ['youtube:search_url'])\n-        self.assertMatch('https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video', ['youtube:search_url'])\n+        self.assertMatch(\n+            \"http://www.youtube.com/results?search_query=making+mustard\",\n+            [\"youtube:search_url\"],\n+        )\n+        self.assertMatch(\n+            \"https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video\",\n+            [\"youtube:search_url\"],\n+        )\n \n     def test_facebook_matching(self):\n-        self.assertTrue(FacebookIE.suitable('https://www.facebook.com/Shiniknoh#!/photo.php?v=10153317450565268'))\n-        self.assertTrue(FacebookIE.suitable('https://www.facebook.com/cindyweather?fref=ts#!/photo.php?v=10152183998945793'))\n+        self.assertTrue(\n+            FacebookIE.suitable(\n+                \"https://www.facebook.com/Shiniknoh#!/photo.php?v=10153317450565268\"\n+            )\n+        )\n+        self.assertTrue(\n+            FacebookIE.suitable(\n+                \"https://www.facebook.com/cindyweather?fref=ts#!/photo.php?v=10152183998945793\"\n+            )\n+        )\n \n     def test_no_duplicates(self):\n         ies = gen_extractors()\n         for tc in gettestcases(include_onlymatching=True):\n-            url = tc['url']\n+            url = tc[\"url\"]\n             for ie in ies:\n-                if type(ie).__name__ in ('GenericIE', tc['name'] + 'IE'):\n-                    self.assertTrue(ie.suitable(url), f'{type(ie).__name__} should match URL {url!r}')\n+                if type(ie).__name__ in (\"GenericIE\", tc[\"name\"] + \"IE\"):\n+                    self.assertTrue(\n+                        ie.suitable(url),\n+                        f\"{type(ie).__name__} should match URL {url!r}\",\n+                    )\n                 else:\n                     self.assertFalse(\n                         ie.suitable(url),\n-                        f'{type(ie).__name__} should not match URL {url!r} . That URL belongs to {tc[\"name\"]}.')\n+                        f\"{type(ie).__name__} should not match URL {url!r} . That URL belongs to {tc['name']}.\",\n+                    )\n \n     def test_keywords(self):\n-        self.assertMatch(':ytsubs', ['youtube:subscriptions'])\n-        self.assertMatch(':ytsubscriptions', ['youtube:subscriptions'])\n-        self.assertMatch(':ythistory', ['youtube:history'])\n+        self.assertMatch(\":ytsubs\", [\"youtube:subscriptions\"])\n+        self.assertMatch(\":ytsubscriptions\", [\"youtube:subscriptions\"])\n+        self.assertMatch(\":ythistory\", [\"youtube:history\"])\n \n     def test_vimeo_matching(self):\n-        self.assertMatch('https://vimeo.com/channels/tributes', ['vimeo:channel'])\n-        self.assertMatch('https://vimeo.com/channels/31259', ['vimeo:channel'])\n-        self.assertMatch('https://vimeo.com/channels/31259/53576664', ['vimeo'])\n-        self.assertMatch('https://vimeo.com/user7108434', ['vimeo:user'])\n-        self.assertMatch('https://vimeo.com/user7108434/videos', ['vimeo:user'])\n-        self.assertMatch('https://vimeo.com/user21297594/review/75524534/3c257a1b5d', ['vimeo:review'])\n+        self.assertMatch(\"https://vimeo.com/channels/tributes\", [\"vimeo:channel\"])\n+        self.assertMatch(\"https://vimeo.com/channels/31259\", [\"vimeo:channel\"])\n+        self.assertMatch(\"https://vimeo.com/channels/31259/53576664\", [\"vimeo\"])\n+        self.assertMatch(\"https://vimeo.com/user7108434\", [\"vimeo:user\"])\n+        self.assertMatch(\"https://vimeo.com/user7108434/videos\", [\"vimeo:user\"])\n+        self.assertMatch(\n+            \"https://vimeo.com/user21297594/review/75524534/3c257a1b5d\",\n+            [\"vimeo:review\"],\n+        )\n \n     # https://github.com/ytdl-org/youtube-dl/issues/1930\n     def test_soundcloud_not_matching_sets(self):\n-        self.assertMatch('http://soundcloud.com/floex/sets/gone-ep', ['soundcloud:set'])\n+        self.assertMatch(\"http://soundcloud.com/floex/sets/gone-ep\", [\"soundcloud:set\"])\n \n     def test_tumblr(self):\n-        self.assertMatch('http://tatianamaslanydaily.tumblr.com/post/54196191430/orphan-black-dvd-extra-behind-the-scenes', ['Tumblr'])\n-        self.assertMatch('http://tatianamaslanydaily.tumblr.com/post/54196191430', ['Tumblr'])\n+        self.assertMatch(\n+            \"http://tatianamaslanydaily.tumblr.com/post/54196191430/orphan-black-dvd-extra-behind-the-scenes\",\n+            [\"Tumblr\"],\n+        )\n+        self.assertMatch(\n+            \"http://tatianamaslanydaily.tumblr.com/post/54196191430\", [\"Tumblr\"]\n+        )\n \n     def test_pbs(self):\n         # https://github.com/ytdl-org/youtube-dl/issues/2350\n-        self.assertMatch('http://video.pbs.org/viralplayer/2365173446/', ['pbs'])\n-        self.assertMatch('http://video.pbs.org/widget/partnerplayer/980042464/', ['pbs'])\n+        self.assertMatch(\"http://video.pbs.org/viralplayer/2365173446/\", [\"pbs\"])\n+        self.assertMatch(\n+            \"http://video.pbs.org/widget/partnerplayer/980042464/\", [\"pbs\"]\n+        )\n \n     def test_no_duplicated_ie_names(self):\n         name_accu = collections.defaultdict(list)\n         for ie in self.ies:\n             name_accu[ie.IE_NAME.lower()].append(type(ie).__name__)\n-        for (ie_name, ie_list) in name_accu.items():\n+        for ie_name, ie_list in name_accu.items():\n             self.assertEqual(\n-                len(ie_list), 1,\n-                f'Multiple extractors with the same IE_NAME \"{ie_name}\" ({\", \".join(ie_list)})')\n+                len(ie_list),\n+                1,\n+                f'Multiple extractors with the same IE_NAME \"{ie_name}\" ({\", \".join(ie_list)})',\n+            )\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_download.py b/test/test_download.py\nindex 2530792..b6046a5 100755\n--- a/test/test_download.py\n+++ b/test/test_download.py\n@@ -55,7 +55,7 @@ def process_info(self, info_dict):\n \n \n def _file_md5(fn):\n-    with open(fn, 'rb') as f:\n+    with open(fn, \"rb\") as f:\n         return hashlib.md5(f.read()).hexdigest()\n \n \n@@ -77,73 +77,90 @@ class TestDownload(unittest.TestCase):\n     def __str__(self):\n         \"\"\"Identify each test with the `add_ie` attribute, if available.\"\"\"\n         cls, add_ie = type(self), getattr(self, self._testMethodName).add_ie\n-        return f'{self._testMethodName} ({cls.__module__}.{cls.__name__}){f\" [{add_ie}]\" if add_ie else \"\"}:'\n+        return f\"{self._testMethodName} ({cls.__module__}.{cls.__name__}){f' [{add_ie}]' if add_ie else ''}:\"\n \n \n # Dynamically generate tests\n \n+\n def generator(test_case, tname):\n     def test_template(self):\n         if self.COMPLETED_TESTS.get(tname):\n             return\n         self.COMPLETED_TESTS[tname] = True\n-        ie = yt_dlp.extractor.get_info_extractor(test_case['name'])()\n-        other_ies = [get_info_extractor(ie_key)() for ie_key in test_case.get('add_ie', [])]\n-        is_playlist = any(k.startswith('playlist') for k in test_case)\n-        test_cases = test_case.get(\n-            'playlist', [] if is_playlist else [test_case])\n+        ie = yt_dlp.extractor.get_info_extractor(test_case[\"name\"])()\n+        other_ies = [\n+            get_info_extractor(ie_key)() for ie_key in test_case.get(\"add_ie\", [])\n+        ]\n+        is_playlist = any(k.startswith(\"playlist\") for k in test_case)\n+        test_cases = test_case.get(\"playlist\", [] if is_playlist else [test_case])\n \n         def print_skipping(reason):\n-            print('Skipping %s: %s' % (test_case['name'], reason))\n+            print(\"Skipping %s: %s\" % (test_case[\"name\"], reason))\n             self.skipTest(reason)\n \n         if not ie.working():\n-            print_skipping('IE marked as not _WORKING')\n+            print_skipping(\"IE marked as not _WORKING\")\n \n         for tc in test_cases:\n-            if tc.get('expected_exception'):\n+            if tc.get(\"expected_exception\"):\n                 continue\n-            info_dict = tc.get('info_dict', {})\n-            params = tc.get('params', {})\n-            if not info_dict.get('id'):\n-                raise Exception(f'Test {tname} definition incorrect - \"id\" key is not present')\n-            elif not info_dict.get('ext') and info_dict.get('_type', 'video') == 'video':\n-                if params.get('skip_download') and params.get('ignore_no_formats_error'):\n+            info_dict = tc.get(\"info_dict\", {})\n+            params = tc.get(\"params\", {})\n+            if not info_dict.get(\"id\"):\n+                raise Exception(\n+                    f'Test {tname} definition incorrect - \"id\" key is not present'\n+                )\n+            elif (\n+                not info_dict.get(\"ext\") and info_dict.get(\"_type\", \"video\") == \"video\"\n+            ):\n+                if params.get(\"skip_download\") and params.get(\n+                    \"ignore_no_formats_error\"\n+                ):\n                     continue\n-                raise Exception(f'Test {tname} definition incorrect - \"ext\" key must be present to define the output file')\n+                raise Exception(\n+                    f'Test {tname} definition incorrect - \"ext\" key must be present to define the output file'\n+                )\n \n-        if 'skip' in test_case:\n-            print_skipping(test_case['skip'])\n+        if \"skip\" in test_case:\n+            print_skipping(test_case[\"skip\"])\n \n         for other_ie in other_ies:\n             if not other_ie.working():\n-                print_skipping('test depends on %sIE, marked as not WORKING' % other_ie.ie_key())\n-\n-        params = get_params(test_case.get('params', {}))\n-        params['outtmpl'] = tname + '_' + params['outtmpl']\n-        if is_playlist and 'playlist' not in test_case:\n-            params.setdefault('extract_flat', 'in_playlist')\n-            params.setdefault('playlistend', test_case.get(\n-                'playlist_mincount', test_case.get('playlist_count', -2) + 1))\n-            params.setdefault('skip_download', True)\n+                print_skipping(\n+                    \"test depends on %sIE, marked as not WORKING\" % other_ie.ie_key()\n+                )\n+\n+        params = get_params(test_case.get(\"params\", {}))\n+        params[\"outtmpl\"] = tname + \"_\" + params[\"outtmpl\"]\n+        if is_playlist and \"playlist\" not in test_case:\n+            params.setdefault(\"extract_flat\", \"in_playlist\")\n+            params.setdefault(\n+                \"playlistend\",\n+                test_case.get(\n+                    \"playlist_mincount\", test_case.get(\"playlist_count\", -2) + 1\n+                ),\n+            )\n+            params.setdefault(\"skip_download\", True)\n \n         ydl = YoutubeDL(params, auto_init=False)\n         ydl.add_default_info_extractors()\n         finished_hook_called = set()\n \n         def _hook(status):\n-            if status['status'] == 'finished':\n-                finished_hook_called.add(status['filename'])\n+            if status[\"status\"] == \"finished\":\n+                finished_hook_called.add(status[\"filename\"])\n+\n         ydl.add_progress_hook(_hook)\n-        expect_warnings(ydl, test_case.get('expected_warnings', []))\n+        expect_warnings(ydl, test_case.get(\"expected_warnings\", []))\n \n         def get_tc_filename(tc):\n-            return ydl.prepare_filename(dict(tc.get('info_dict', {})))\n+            return ydl.prepare_filename(dict(tc.get(\"info_dict\", {})))\n \n         res_dict = None\n \n         def match_exception(err):\n-            expected_exception = test_case.get('expected_exception')\n+            expected_exception = test_case.get(\"expected_exception\")\n             if not expected_exception:\n                 return False\n             if err.__class__.__name__ == expected_exception:\n@@ -159,8 +176,9 @@ def try_rm_tcs_files(tcs=None):\n             for tc in tcs:\n                 tc_filename = get_tc_filename(tc)\n                 try_rm(tc_filename)\n-                try_rm(tc_filename + '.part')\n-                try_rm(os.path.splitext(tc_filename)[0] + '.info.json')\n+                try_rm(tc_filename + \".part\")\n+                try_rm(os.path.splitext(tc_filename)[0] + \".info.json\")\n+\n         try_rm_tcs_files()\n         try:\n             try_num = 1\n@@ -170,21 +188,31 @@ def try_rm_tcs_files(tcs=None):\n                     # for outside error handling, and returns the exit code\n                     # instead of the result dict.\n                     res_dict = ydl.extract_info(\n-                        test_case['url'],\n-                        force_generic_extractor=params.get('force_generic_extractor', False))\n+                        test_case[\"url\"],\n+                        force_generic_extractor=params.get(\n+                            \"force_generic_extractor\", False\n+                        ),\n+                    )\n                 except (DownloadError, ExtractorError) as err:\n                     # Check if the exception is not a network related one\n-                    if not isinstance(err.exc_info[1], (TransportError, UnavailableVideoError)) or (isinstance(err.exc_info[1], HTTPError) and err.exc_info[1].status == 503):\n+                    if not isinstance(\n+                        err.exc_info[1], (TransportError, UnavailableVideoError)\n+                    ) or (\n+                        isinstance(err.exc_info[1], HTTPError)\n+                        and err.exc_info[1].status == 503\n+                    ):\n                         if match_exception(err):\n                             return\n-                        err.msg = f'{getattr(err, \"msg\", err)} ({tname})'\n+                        err.msg = f\"{getattr(err, 'msg', err)} ({tname})\"\n                         raise\n \n                     if try_num == RETRIES:\n-                        report_warning('%s failed due to network errors, skipping...' % tname)\n+                        report_warning(\n+                            \"%s failed due to network errors, skipping...\" % tname\n+                        )\n                         return\n \n-                    print(f'Retrying: {try_num} failed tries\\n\\n##########\\n\\n')\n+                    print(f\"Retrying: {try_num} failed tries\\n\\n##########\\n\\n\")\n \n                     try_num += 1\n                 except YoutubeDLError as err:\n@@ -195,120 +223,136 @@ def try_rm_tcs_files(tcs=None):\n                     break\n \n             if is_playlist:\n-                self.assertTrue(res_dict['_type'] in ['playlist', 'multi_video'])\n-                self.assertTrue('entries' in res_dict)\n-                expect_info_dict(self, res_dict, test_case.get('info_dict', {}))\n+                self.assertTrue(res_dict[\"_type\"] in [\"playlist\", \"multi_video\"])\n+                self.assertTrue(\"entries\" in res_dict)\n+                expect_info_dict(self, res_dict, test_case.get(\"info_dict\", {}))\n \n-            if 'playlist_mincount' in test_case:\n+            if \"playlist_mincount\" in test_case:\n                 assertGreaterEqual(\n                     self,\n-                    len(res_dict['entries']),\n-                    test_case['playlist_mincount'],\n-                    'Expected at least %d in playlist %s, but got only %d' % (\n-                        test_case['playlist_mincount'], test_case['url'],\n-                        len(res_dict['entries'])))\n-            if 'playlist_count' in test_case:\n-                self.assertEqual(\n-                    len(res_dict['entries']),\n-                    test_case['playlist_count'],\n-                    'Expected %d entries in playlist %s, but got %d.' % (\n-                        test_case['playlist_count'],\n-                        test_case['url'],\n-                        len(res_dict['entries']),\n-                    ))\n-            if 'playlist_duration_sum' in test_case:\n-                got_duration = sum(e['duration'] for e in res_dict['entries'])\n+                    len(res_dict[\"entries\"]),\n+                    test_case[\"playlist_mincount\"],\n+                    \"Expected at least %d in playlist %s, but got only %d\"\n+                    % (\n+                        test_case[\"playlist_mincount\"],\n+                        test_case[\"url\"],\n+                        len(res_dict[\"entries\"]),\n+                    ),\n+                )\n+            if \"playlist_count\" in test_case:\n                 self.assertEqual(\n-                    test_case['playlist_duration_sum'], got_duration)\n+                    len(res_dict[\"entries\"]),\n+                    test_case[\"playlist_count\"],\n+                    \"Expected %d entries in playlist %s, but got %d.\"\n+                    % (\n+                        test_case[\"playlist_count\"],\n+                        test_case[\"url\"],\n+                        len(res_dict[\"entries\"]),\n+                    ),\n+                )\n+            if \"playlist_duration_sum\" in test_case:\n+                got_duration = sum(e[\"duration\"] for e in res_dict[\"entries\"])\n+                self.assertEqual(test_case[\"playlist_duration_sum\"], got_duration)\n \n             # Generalize both playlists and single videos to unified format for\n             # simplicity\n-            if 'entries' not in res_dict:\n-                res_dict['entries'] = [res_dict]\n+            if \"entries\" not in res_dict:\n+                res_dict[\"entries\"] = [res_dict]\n \n             for tc_num, tc in enumerate(test_cases):\n-                tc_res_dict = res_dict['entries'][tc_num]\n+                tc_res_dict = res_dict[\"entries\"][tc_num]\n                 # First, check test cases' data against extracted data alone\n-                expect_info_dict(self, tc_res_dict, tc.get('info_dict', {}))\n-                if tc_res_dict.get('_type', 'video') != 'video':\n+                expect_info_dict(self, tc_res_dict, tc.get(\"info_dict\", {}))\n+                if tc_res_dict.get(\"_type\", \"video\") != \"video\":\n                     continue\n                 # Now, check downloaded file consistency\n                 tc_filename = get_tc_filename(tc)\n-                if not test_case.get('params', {}).get('skip_download', False):\n-                    self.assertTrue(os.path.exists(tc_filename), msg='Missing file ' + tc_filename)\n+                if not test_case.get(\"params\", {}).get(\"skip_download\", False):\n+                    self.assertTrue(\n+                        os.path.exists(tc_filename), msg=\"Missing file \" + tc_filename\n+                    )\n                     self.assertTrue(tc_filename in finished_hook_called)\n-                    expected_minsize = tc.get('file_minsize', 10000)\n+                    expected_minsize = tc.get(\"file_minsize\", 10000)\n                     if expected_minsize is not None:\n-                        if params.get('test'):\n+                        if params.get(\"test\"):\n                             expected_minsize = max(expected_minsize, 10000)\n                         got_fsize = os.path.getsize(tc_filename)\n                         assertGreaterEqual(\n-                            self, got_fsize, expected_minsize,\n-                            'Expected %s to be at least %s, but it\\'s only %s ' %\n-                            (tc_filename, format_bytes(expected_minsize),\n-                                format_bytes(got_fsize)))\n-                    if 'md5' in tc:\n+                            self,\n+                            got_fsize,\n+                            expected_minsize,\n+                            \"Expected %s to be at least %s, but it's only %s \"\n+                            % (\n+                                tc_filename,\n+                                format_bytes(expected_minsize),\n+                                format_bytes(got_fsize),\n+                            ),\n+                        )\n+                    if \"md5\" in tc:\n                         md5_for_file = _file_md5(tc_filename)\n-                        self.assertEqual(tc['md5'], md5_for_file)\n+                        self.assertEqual(tc[\"md5\"], md5_for_file)\n                 # Finally, check test cases' data again but this time against\n                 # extracted data from info JSON file written during processing\n-                info_json_fn = os.path.splitext(tc_filename)[0] + '.info.json'\n+                info_json_fn = os.path.splitext(tc_filename)[0] + \".info.json\"\n                 self.assertTrue(\n-                    os.path.exists(info_json_fn),\n-                    'Missing info file %s' % info_json_fn)\n-                with open(info_json_fn, encoding='utf-8') as infof:\n+                    os.path.exists(info_json_fn), \"Missing info file %s\" % info_json_fn\n+                )\n+                with open(info_json_fn, encoding=\"utf-8\") as infof:\n                     info_dict = json.load(infof)\n-                expect_info_dict(self, info_dict, tc.get('info_dict', {}))\n+                expect_info_dict(self, info_dict, tc.get(\"info_dict\", {}))\n         finally:\n             try_rm_tcs_files()\n-            if is_playlist and res_dict is not None and res_dict.get('entries'):\n+            if is_playlist and res_dict is not None and res_dict.get(\"entries\"):\n                 # Remove all other files that may have been extracted if the\n                 # extractor returns full results even with extract_flat\n-                res_tcs = [{'info_dict': e} for e in res_dict['entries']]\n+                res_tcs = [{\"info_dict\": e} for e in res_dict[\"entries\"]]\n                 try_rm_tcs_files(res_tcs)\n             ydl.close()\n+\n     return test_template\n \n \n # And add them to TestDownload\n-def inject_tests(test_cases, label=''):\n+def inject_tests(test_cases, label=\"\"):\n     for test_case in test_cases:\n-        name = test_case['name']\n-        tname = join_nonempty('test', name, label, tests_counter[name][label], delim='_')\n+        name = test_case[\"name\"]\n+        tname = join_nonempty(\n+            \"test\", name, label, tests_counter[name][label], delim=\"_\"\n+        )\n         tests_counter[name][label] += 1\n \n         test_method = generator(test_case, tname)\n         test_method.__name__ = tname\n-        test_method.add_ie = ','.join(test_case.get('add_ie', []))\n+        test_method.add_ie = \",\".join(test_case.get(\"add_ie\", []))\n         setattr(TestDownload, test_method.__name__, test_method)\n \n \n inject_tests(normal_test_cases)\n \n # TODO: disable redirection to the IE to ensure we are actually testing the webpage extraction\n-inject_tests(webpage_test_cases, 'webpage')\n+inject_tests(webpage_test_cases, \"webpage\")\n \n \n def batch_generator(name):\n     def test_template(self):\n         for label, num_tests in tests_counter[name].items():\n             for i in range(num_tests):\n-                test_name = join_nonempty('test', name, label, i, delim='_')\n+                test_name = join_nonempty(\"test\", name, label, i, delim=\"_\")\n                 try:\n                     getattr(self, test_name)()\n                 except unittest.SkipTest:\n-                    print(f'Skipped {test_name}')\n+                    print(f\"Skipped {test_name}\")\n \n     return test_template\n \n \n for name in tests_counter:\n     test_method = batch_generator(name)\n-    test_method.__name__ = f'test_{name}_all'\n-    test_method.add_ie = ''\n+    test_method.__name__ = f\"test_{name}_all\"\n+    test_method.add_ie = \"\"\n     setattr(TestDownload, test_method.__name__, test_method)\n del test_method\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_iqiyi_sdk_interpreter.py b/test/test_iqiyi_sdk_interpreter.py\nindex 47c632a..fa81c72 100644\n--- a/test/test_iqiyi_sdk_interpreter.py\n+++ b/test/test_iqiyi_sdk_interpreter.py\n@@ -29,16 +29,16 @@ def error(self, msg):\n @is_download_test\n class TestIqiyiSDKInterpreter(unittest.TestCase):\n     def test_iqiyi_sdk_interpreter(self):\n-        '''\n+        \"\"\"\n         Test the functionality of IqiyiSDKInterpreter by trying to log in\n \n         If `sign` is incorrect, /validate call throws an HTTP 556 error\n-        '''\n+        \"\"\"\n         logger = WarningLogger()\n-        ie = IqiyiIE(FakeYDL({'logger': logger}))\n-        ie._perform_login('foo', 'bar')\n-        self.assertTrue('unable to log in:' in logger.messages[0])\n+        ie = IqiyiIE(FakeYDL({\"logger\": logger}))\n+        ie._perform_login(\"foo\", \"bar\")\n+        self.assertTrue(\"unable to log in:\" in logger.messages[0])\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_subtitles.py b/test/test_subtitles.py\nindex 5736289..6273b3d 100644\n--- a/test/test_subtitles.py\n+++ b/test/test_subtitles.py\n@@ -40,8 +40,8 @@ def setUp(self):\n         self.ie = self.IE()\n         self.DL.add_info_extractor(self.ie)\n         if not self.IE.working():\n-            print('Skipping: %s marked as not _WORKING' % self.IE.ie_key())\n-            self.skipTest('IE marked as not _WORKING')\n+            print(\"Skipping: %s marked as not _WORKING\" % self.IE.ie_key())\n+            self.skipTest(\"IE marked as not _WORKING\")\n \n     def getInfoDict(self):\n         info_dict = self.DL.extract_info(self.url, download=False)\n@@ -49,14 +49,14 @@ def getInfoDict(self):\n \n     def getSubtitles(self):\n         info_dict = self.getInfoDict()\n-        subtitles = info_dict['requested_subtitles']\n+        subtitles = info_dict[\"requested_subtitles\"]\n         if not subtitles:\n             return subtitles\n         for sub_info in subtitles.values():\n-            if sub_info.get('data') is None:\n-                uf = self.DL.urlopen(sub_info['url'])\n-                sub_info['data'] = uf.read().decode()\n-        return {l: sub_info['data'] for l, sub_info in subtitles.items()}\n+            if sub_info.get(\"data\") is None:\n+                uf = self.DL.urlopen(sub_info[\"url\"])\n+                sub_info[\"data\"] = uf.read().decode()\n+        return {l: sub_info[\"data\"] for l, sub_info in subtitles.items()}\n \n \n @is_download_test\n@@ -76,38 +76,41 @@ class TestYoutubeSubtitles(BaseTestSubtitles):\n     # es-MX    vtt, ttml, srv3, srv2, srv1, json3\n     # ja       vtt, ttml, srv3, srv2, srv1, json3\n     # pl       vtt, ttml, srv3, srv2, srv1, json3\n-    url = 'QRS8MkLhQmM'\n+    url = \"QRS8MkLhQmM\"\n     IE = YoutubeIE\n \n     def test_youtube_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertEqual(len(subtitles.keys()), 13)\n-        self.assertEqual(md5(subtitles['en']), 'ae1bd34126571a77aabd4d276b28044d')\n-        self.assertEqual(md5(subtitles['it']), '0e0b667ba68411d88fd1c5f4f4eab2f9')\n-        for lang in ['fr', 'de']:\n-            self.assertTrue(subtitles.get(lang) is not None, 'Subtitles for \\'%s\\' not extracted' % lang)\n-\n-    def _test_subtitles_format(self, fmt, md5_hash, lang='en'):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = fmt\n+        self.assertEqual(md5(subtitles[\"en\"]), \"ae1bd34126571a77aabd4d276b28044d\")\n+        self.assertEqual(md5(subtitles[\"it\"]), \"0e0b667ba68411d88fd1c5f4f4eab2f9\")\n+        for lang in [\"fr\", \"de\"]:\n+            self.assertTrue(\n+                subtitles.get(lang) is not None,\n+                \"Subtitles for '%s' not extracted\" % lang,\n+            )\n+\n+    def _test_subtitles_format(self, fmt, md5_hash, lang=\"en\"):\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"subtitlesformat\"] = fmt\n         subtitles = self.getSubtitles()\n         self.assertEqual(md5(subtitles[lang]), md5_hash)\n \n     def test_youtube_subtitles_ttml_format(self):\n-        self._test_subtitles_format('ttml', 'c97ddf1217390906fa9fbd34901f3da2')\n+        self._test_subtitles_format(\"ttml\", \"c97ddf1217390906fa9fbd34901f3da2\")\n \n     def test_youtube_subtitles_vtt_format(self):\n-        self._test_subtitles_format('vtt', 'ae1bd34126571a77aabd4d276b28044d')\n+        self._test_subtitles_format(\"vtt\", \"ae1bd34126571a77aabd4d276b28044d\")\n \n     def test_youtube_subtitles_json3_format(self):\n-        self._test_subtitles_format('json3', '688dd1ce0981683867e7fe6fde2a224b')\n+        self._test_subtitles_format(\"json3\", \"688dd1ce0981683867e7fe6fde2a224b\")\n \n     def _test_automatic_captions(self, url, lang):\n         self.url = url\n-        self.DL.params['writeautomaticsub'] = True\n-        self.DL.params['subtitleslangs'] = [lang]\n+        self.DL.params[\"writeautomaticsub\"] = True\n+        self.DL.params[\"subtitleslangs\"] = [lang]\n         subtitles = self.getSubtitles()\n         self.assertTrue(subtitles[lang] is not None)\n \n@@ -125,190 +128,198 @@ def test_youtube_automatic_captions(self):\n         # ka, kk, sr, sq, ko, kn, km, st, sk, si, so, sn, sm, sl,\n         # ky, sd\n         # ...\n-        self._test_automatic_captions('8YoUxe5ncPo', 'it')\n+        self._test_automatic_captions(\"8YoUxe5ncPo\", \"it\")\n \n-    @unittest.skip('Video unavailable')\n+    @unittest.skip(\"Video unavailable\")\n     def test_youtube_translated_subtitles(self):\n         # This video has a subtitles track, which can be translated (#4555)\n-        self._test_automatic_captions('Ky9eprVWzlI', 'it')\n+        self._test_automatic_captions(\"Ky9eprVWzlI\", \"it\")\n \n     def test_youtube_nosubtitles(self):\n-        self.DL.expect_warning('video doesn\\'t have subtitles')\n+        self.DL.expect_warning(\"video doesn't have subtitles\")\n         # Available automatic captions for 8YoUxe5ncPo:\n         # ...\n         # 8YoUxe5ncPo has no subtitles\n-        self.url = '8YoUxe5ncPo'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.url = \"8YoUxe5ncPo\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertFalse(subtitles)\n \n \n @is_download_test\n class TestDailymotionSubtitles(BaseTestSubtitles):\n-    url = 'http://www.dailymotion.com/video/xczg00'\n+    url = \"http://www.dailymotion.com/video/xczg00\"\n     IE = DailymotionIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertTrue(len(subtitles.keys()) >= 6)\n-        self.assertEqual(md5(subtitles['en']), '976553874490cba125086bbfea3ff76f')\n-        self.assertEqual(md5(subtitles['fr']), '594564ec7d588942e384e920e5341792')\n-        for lang in ['es', 'fr', 'de']:\n-            self.assertTrue(subtitles.get(lang) is not None, 'Subtitles for \\'%s\\' not extracted' % lang)\n+        self.assertEqual(md5(subtitles[\"en\"]), \"976553874490cba125086bbfea3ff76f\")\n+        self.assertEqual(md5(subtitles[\"fr\"]), \"594564ec7d588942e384e920e5341792\")\n+        for lang in [\"es\", \"fr\", \"de\"]:\n+            self.assertTrue(\n+                subtitles.get(lang) is not None,\n+                \"Subtitles for '%s' not extracted\" % lang,\n+            )\n \n     def test_nosubtitles(self):\n-        self.DL.expect_warning('video doesn\\'t have subtitles')\n-        self.url = 'http://www.dailymotion.com/video/x12u166_le-zapping-tele-star-du-08-aout-2013_tv'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.expect_warning(\"video doesn't have subtitles\")\n+        self.url = \"http://www.dailymotion.com/video/x12u166_le-zapping-tele-star-du-08-aout-2013_tv\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertFalse(subtitles)\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestTedSubtitles(BaseTestSubtitles):\n-    url = 'http://www.ted.com/talks/dan_dennett_on_our_consciousness.html'\n+    url = \"http://www.ted.com/talks/dan_dennett_on_our_consciousness.html\"\n     IE = TedTalkIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertTrue(len(subtitles.keys()) >= 28)\n-        self.assertEqual(md5(subtitles['en']), '4262c1665ff928a2dada178f62cb8d14')\n-        self.assertEqual(md5(subtitles['fr']), '66a63f7f42c97a50f8c0e90bc7797bb5')\n-        for lang in ['es', 'fr', 'de']:\n-            self.assertTrue(subtitles.get(lang) is not None, 'Subtitles for \\'%s\\' not extracted' % lang)\n+        self.assertEqual(md5(subtitles[\"en\"]), \"4262c1665ff928a2dada178f62cb8d14\")\n+        self.assertEqual(md5(subtitles[\"fr\"]), \"66a63f7f42c97a50f8c0e90bc7797bb5\")\n+        for lang in [\"es\", \"fr\", \"de\"]:\n+            self.assertTrue(\n+                subtitles.get(lang) is not None,\n+                \"Subtitles for '%s' not extracted\" % lang,\n+            )\n \n \n @is_download_test\n class TestVimeoSubtitles(BaseTestSubtitles):\n-    url = 'http://vimeo.com/76979871'\n+    url = \"http://vimeo.com/76979871\"\n     IE = VimeoIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'de', 'en', 'es', 'fr'})\n-        self.assertEqual(md5(subtitles['en']), '386cbc9320b94e25cb364b97935e5dd1')\n-        self.assertEqual(md5(subtitles['fr']), 'c9b69eef35bc6641c0d4da8a04f9dfac')\n+        self.assertEqual(set(subtitles.keys()), {\"de\", \"en\", \"es\", \"fr\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"386cbc9320b94e25cb364b97935e5dd1\")\n+        self.assertEqual(md5(subtitles[\"fr\"]), \"c9b69eef35bc6641c0d4da8a04f9dfac\")\n \n     def test_nosubtitles(self):\n-        self.DL.expect_warning('video doesn\\'t have subtitles')\n-        self.url = 'http://vimeo.com/68093876'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.expect_warning(\"video doesn't have subtitles\")\n+        self.url = \"http://vimeo.com/68093876\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertFalse(subtitles)\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestWallaSubtitles(BaseTestSubtitles):\n-    url = 'http://vod.walla.co.il/movie/2705958/the-yes-men'\n+    url = \"http://vod.walla.co.il/movie/2705958/the-yes-men\"\n     IE = WallaIE\n \n     def test_allsubtitles(self):\n-        self.DL.expect_warning('Automatic Captions not supported by this server')\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.expect_warning(\"Automatic Captions not supported by this server\")\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'heb'})\n-        self.assertEqual(md5(subtitles['heb']), 'e758c5d7cb982f6bef14f377ec7a3920')\n+        self.assertEqual(set(subtitles.keys()), {\"heb\"})\n+        self.assertEqual(md5(subtitles[\"heb\"]), \"e758c5d7cb982f6bef14f377ec7a3920\")\n \n     def test_nosubtitles(self):\n-        self.DL.expect_warning('video doesn\\'t have subtitles')\n-        self.url = 'http://vod.walla.co.il/movie/2642630/one-direction-all-for-one'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.expect_warning(\"video doesn't have subtitles\")\n+        self.url = \"http://vod.walla.co.il/movie/2642630/one-direction-all-for-one\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertFalse(subtitles)\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestCeskaTelevizeSubtitles(BaseTestSubtitles):\n-    url = 'http://www.ceskatelevize.cz/ivysilani/10600540290-u6-uzasny-svet-techniky'\n+    url = \"http://www.ceskatelevize.cz/ivysilani/10600540290-u6-uzasny-svet-techniky\"\n     IE = CeskaTelevizeIE\n \n     def test_allsubtitles(self):\n-        self.DL.expect_warning('Automatic Captions not supported by this server')\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.expect_warning(\"Automatic Captions not supported by this server\")\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'cs'})\n-        self.assertTrue(len(subtitles['cs']) > 20000)\n+        self.assertEqual(set(subtitles.keys()), {\"cs\"})\n+        self.assertTrue(len(subtitles[\"cs\"]) > 20000)\n \n     def test_nosubtitles(self):\n-        self.DL.expect_warning('video doesn\\'t have subtitles')\n-        self.url = 'http://www.ceskatelevize.cz/ivysilani/ivysilani/10441294653-hyde-park-civilizace/214411058091220'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.expect_warning(\"video doesn't have subtitles\")\n+        self.url = \"http://www.ceskatelevize.cz/ivysilani/ivysilani/10441294653-hyde-park-civilizace/214411058091220\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n         self.assertFalse(subtitles)\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestLyndaSubtitles(BaseTestSubtitles):\n-    url = 'http://www.lynda.com/Bootstrap-tutorials/Using-exercise-files/110885/114408-4.html'\n+    url = \"http://www.lynda.com/Bootstrap-tutorials/Using-exercise-files/110885/114408-4.html\"\n     IE = LyndaIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), '09bbe67222259bed60deaa26997d73a7')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"09bbe67222259bed60deaa26997d73a7\")\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestNPOSubtitles(BaseTestSubtitles):\n-    url = 'http://www.npo.nl/nos-journaal/28-08-2014/POW_00722860'\n+    url = \"http://www.npo.nl/nos-journaal/28-08-2014/POW_00722860\"\n     IE = NPOIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'nl'})\n-        self.assertEqual(md5(subtitles['nl']), 'fc6435027572b63fb4ab143abd5ad3f4')\n+        self.assertEqual(set(subtitles.keys()), {\"nl\"})\n+        self.assertEqual(md5(subtitles[\"nl\"]), \"fc6435027572b63fb4ab143abd5ad3f4\")\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestMTVSubtitles(BaseTestSubtitles):\n-    url = 'http://www.cc.com/video-clips/p63lk0/adam-devine-s-house-party-chasing-white-swans'\n+    url = \"http://www.cc.com/video-clips/p63lk0/adam-devine-s-house-party-chasing-white-swans\"\n     IE = ComedyCentralIE\n \n     def getInfoDict(self):\n-        return super().getInfoDict()['entries'][0]\n+        return super().getInfoDict()[\"entries\"][0]\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), '78206b8d8a0cfa9da64dc026eea48961')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"78206b8d8a0cfa9da64dc026eea48961\")\n \n \n @is_download_test\n class TestNRKSubtitles(BaseTestSubtitles):\n-    url = 'http://tv.nrk.no/serie/ikke-gjoer-dette-hjemme/DMPV73000411/sesong-2/episode-1'\n+    url = (\n+        \"http://tv.nrk.no/serie/ikke-gjoer-dette-hjemme/DMPV73000411/sesong-2/episode-1\"\n+    )\n     IE = NRKTVIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'nb-ttv'})\n-        self.assertEqual(md5(subtitles['nb-ttv']), '67e06ff02d0deaf975e68f6cb8f6a149')\n+        self.assertEqual(set(subtitles.keys()), {\"nb-ttv\"})\n+        self.assertEqual(md5(subtitles[\"nb-ttv\"]), \"67e06ff02d0deaf975e68f6cb8f6a149\")\n \n \n @is_download_test\n@@ -316,137 +327,141 @@ class TestRaiPlaySubtitles(BaseTestSubtitles):\n     IE = RaiPlayIE\n \n     def test_subtitles_key(self):\n-        self.url = 'http://www.raiplay.it/video/2014/04/Report-del-07042014-cb27157f-9dd0-4aee-b788-b1f67643a391.html'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.url = \"http://www.raiplay.it/video/2014/04/Report-del-07042014-cb27157f-9dd0-4aee-b788-b1f67643a391.html\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'it'})\n-        self.assertEqual(md5(subtitles['it']), 'b1d90a98755126b61e667567a1f6680a')\n+        self.assertEqual(set(subtitles.keys()), {\"it\"})\n+        self.assertEqual(md5(subtitles[\"it\"]), \"b1d90a98755126b61e667567a1f6680a\")\n \n     def test_subtitles_array_key(self):\n-        self.url = 'https://www.raiplay.it/video/2020/12/Report---04-01-2021-2e90f1de-8eee-4de4-ac0e-78d21db5b600.html'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.url = \"https://www.raiplay.it/video/2020/12/Report---04-01-2021-2e90f1de-8eee-4de4-ac0e-78d21db5b600.html\"\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'it'})\n-        self.assertEqual(md5(subtitles['it']), '4b3264186fbb103508abe5311cfcb9cd')\n+        self.assertEqual(set(subtitles.keys()), {\"it\"})\n+        self.assertEqual(md5(subtitles[\"it\"]), \"4b3264186fbb103508abe5311cfcb9cd\")\n \n \n @is_download_test\n-@unittest.skip('IE broken - DRM only')\n+@unittest.skip(\"IE broken - DRM only\")\n class TestVikiSubtitles(BaseTestSubtitles):\n-    url = 'http://www.viki.com/videos/1060846v-punch-episode-18'\n+    url = \"http://www.viki.com/videos/1060846v-punch-episode-18\"\n     IE = VikiIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), '53cb083a5914b2d84ef1ab67b880d18a')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"53cb083a5914b2d84ef1ab67b880d18a\")\n \n \n @is_download_test\n class TestThePlatformSubtitles(BaseTestSubtitles):\n     # from http://www.3playmedia.com/services-features/tools/integrations/theplatform/\n     # (see http://theplatform.com/about/partners/type/subtitles-closed-captioning/)\n-    url = 'theplatform:JFUjUE1_ehvq'\n+    url = \"theplatform:JFUjUE1_ehvq\"\n     IE = ThePlatformIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), '97e7670cbae3c4d26ae8bcc7fdd78d4b')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"97e7670cbae3c4d26ae8bcc7fdd78d4b\")\n \n \n @is_download_test\n-@unittest.skip('IE broken')\n+@unittest.skip(\"IE broken\")\n class TestThePlatformFeedSubtitles(BaseTestSubtitles):\n-    url = 'http://feed.theplatform.com/f/7wvmTC/msnbc_video-p-test?form=json&pretty=true&range=-40&byGuid=n_hardball_5biden_140207'\n+    url = \"http://feed.theplatform.com/f/7wvmTC/msnbc_video-p-test?form=json&pretty=true&range=-40&byGuid=n_hardball_5biden_140207\"\n     IE = ThePlatformFeedIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), '48649a22e82b2da21c9a67a395eedade')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"48649a22e82b2da21c9a67a395eedade\")\n \n \n @is_download_test\n class TestRtveSubtitles(BaseTestSubtitles):\n-    url = 'http://www.rtve.es/alacarta/videos/los-misterios-de-laura/misterios-laura-capitulo-32-misterio-del-numero-17-2-parte/2428621/'\n+    url = \"http://www.rtve.es/alacarta/videos/los-misterios-de-laura/misterios-laura-capitulo-32-misterio-del-numero-17-2-parte/2428621/\"\n     IE = RTVEALaCartaIE\n \n     def test_allsubtitles(self):\n-        print('Skipping, only available from Spain')\n+        print(\"Skipping, only available from Spain\")\n         return\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'es'})\n-        self.assertEqual(md5(subtitles['es']), '69e70cae2d40574fb7316f31d6eb7fca')\n+        self.assertEqual(set(subtitles.keys()), {\"es\"})\n+        self.assertEqual(md5(subtitles[\"es\"]), \"69e70cae2d40574fb7316f31d6eb7fca\")\n \n \n @is_download_test\n class TestDemocracynowSubtitles(BaseTestSubtitles):\n-    url = 'http://www.democracynow.org/shows/2015/7/3'\n+    url = \"http://www.democracynow.org/shows/2015/7/3\"\n     IE = DemocracynowIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), 'a3cc4c0b5eadd74d9974f1c1f5101045')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"a3cc4c0b5eadd74d9974f1c1f5101045\")\n \n     def test_subtitles_in_page(self):\n-        self.url = 'http://www.democracynow.org/2015/7/3/this_flag_comes_down_today_bree'\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.url = (\n+            \"http://www.democracynow.org/2015/7/3/this_flag_comes_down_today_bree\"\n+        )\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n-        self.assertEqual(md5(subtitles['en']), 'a3cc4c0b5eadd74d9974f1c1f5101045')\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n+        self.assertEqual(md5(subtitles[\"en\"]), \"a3cc4c0b5eadd74d9974f1c1f5101045\")\n \n \n @is_download_test\n class TestPBSSubtitles(BaseTestSubtitles):\n-    url = 'https://www.pbs.org/video/how-fantasy-reflects-our-world-picecq/'\n+    url = \"https://www.pbs.org/video/how-fantasy-reflects-our-world-picecq/\"\n     IE = PBSIE\n \n     def test_allsubtitles(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['allsubtitles'] = True\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"allsubtitles\"] = True\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), {'en'})\n+        self.assertEqual(set(subtitles.keys()), {\"en\"})\n \n     def test_subtitles_dfxp_format(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = 'dfxp'\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"subtitlesformat\"] = \"dfxp\"\n         subtitles = self.getSubtitles()\n-        self.assertIn(md5(subtitles['en']), ['643b034254cdc3768ff1e750b6b5873b'])\n+        self.assertIn(md5(subtitles[\"en\"]), [\"643b034254cdc3768ff1e750b6b5873b\"])\n \n     def test_subtitles_vtt_format(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = 'vtt'\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"subtitlesformat\"] = \"vtt\"\n         subtitles = self.getSubtitles()\n         self.assertIn(\n-            md5(subtitles['en']), ['937a05711555b165d4c55a9667017045', 'f49ea998d6824d94959c8152a368ff73'])\n+            md5(subtitles[\"en\"]),\n+            [\"937a05711555b165d4c55a9667017045\", \"f49ea998d6824d94959c8152a368ff73\"],\n+        )\n \n     def test_subtitles_srt_format(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = 'srt'\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"subtitlesformat\"] = \"srt\"\n         subtitles = self.getSubtitles()\n-        self.assertIn(md5(subtitles['en']), ['2082c21b43759d9bf172931b2f2ca371'])\n+        self.assertIn(md5(subtitles[\"en\"]), [\"2082c21b43759d9bf172931b2f2ca371\"])\n \n     def test_subtitles_sami_format(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = 'sami'\n+        self.DL.params[\"writesubtitles\"] = True\n+        self.DL.params[\"subtitlesformat\"] = \"sami\"\n         subtitles = self.getSubtitles()\n-        self.assertIn(md5(subtitles['en']), ['4256b16ac7da6a6780fafd04294e85cd'])\n+        self.assertIn(md5(subtitles[\"en\"]), [\"4256b16ac7da6a6780fafd04294e85cd\"])\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_youtube_lists.py b/test/test_youtube_lists.py\nindex b3f323e..06125e5 100644\n--- a/test/test_youtube_lists.py\n+++ b/test/test_youtube_lists.py\n@@ -17,55 +17,61 @@\n class TestYoutubeLists(unittest.TestCase):\n     def assertIsPlaylist(self, info):\n         \"\"\"Make sure the info has '_type' set to 'playlist'\"\"\"\n-        self.assertEqual(info['_type'], 'playlist')\n+        self.assertEqual(info[\"_type\"], \"playlist\")\n \n     def test_youtube_playlist_noplaylist(self):\n         dl = FakeYDL()\n-        dl.params['noplaylist'] = True\n+        dl.params[\"noplaylist\"] = True\n         ie = YoutubeTabIE(dl)\n-        result = ie.extract('https://www.youtube.com/watch?v=OmJ-4B-mS-Y&list=PLydZ2Hrp_gPRJViZjLFKaBMgCQOYEEkyp&index=2')\n-        self.assertEqual(result['_type'], 'url')\n-        self.assertEqual(result['ie_key'], YoutubeIE.ie_key())\n-        self.assertEqual(YoutubeIE.extract_id(result['url']), 'OmJ-4B-mS-Y')\n+        result = ie.extract(\n+            \"https://www.youtube.com/watch?v=OmJ-4B-mS-Y&list=PLydZ2Hrp_gPRJViZjLFKaBMgCQOYEEkyp&index=2\"\n+        )\n+        self.assertEqual(result[\"_type\"], \"url\")\n+        self.assertEqual(result[\"ie_key\"], YoutubeIE.ie_key())\n+        self.assertEqual(YoutubeIE.extract_id(result[\"url\"]), \"OmJ-4B-mS-Y\")\n \n     def test_youtube_mix(self):\n         dl = FakeYDL()\n         ie = YoutubeTabIE(dl)\n-        result = ie.extract('https://www.youtube.com/watch?v=tyITL_exICo&list=RDCLAK5uy_kLWIr9gv1XLlPbaDS965-Db4TrBoUTxQ8')\n-        entries = list(result['entries'])\n+        result = ie.extract(\n+            \"https://www.youtube.com/watch?v=tyITL_exICo&list=RDCLAK5uy_kLWIr9gv1XLlPbaDS965-Db4TrBoUTxQ8\"\n+        )\n+        entries = list(result[\"entries\"])\n         self.assertTrue(len(entries) >= 50)\n         original_video = entries[0]\n-        self.assertEqual(original_video['id'], 'tyITL_exICo')\n+        self.assertEqual(original_video[\"id\"], \"tyITL_exICo\")\n \n     def test_youtube_flat_playlist_extraction(self):\n         dl = FakeYDL()\n-        dl.params['extract_flat'] = True\n+        dl.params[\"extract_flat\"] = True\n         ie = YoutubeTabIE(dl)\n-        result = ie.extract('https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc')\n+        result = ie.extract(\n+            \"https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc\"\n+        )\n         self.assertIsPlaylist(result)\n-        entries = list(result['entries'])\n+        entries = list(result[\"entries\"])\n         self.assertTrue(len(entries) == 1)\n         video = entries[0]\n-        self.assertEqual(video['_type'], 'url')\n-        self.assertEqual(video['ie_key'], 'Youtube')\n-        self.assertEqual(video['id'], 'BaW_jenozKc')\n-        self.assertEqual(video['url'], 'https://www.youtube.com/watch?v=BaW_jenozKc')\n-        self.assertEqual(video['title'], 'youtube-dl test video \"\\'/\\\\\u00e4\u21ad\ud835\udd50')\n-        self.assertEqual(video['duration'], 10)\n-        self.assertEqual(video['uploader'], 'Philipp Hagemeister')\n+        self.assertEqual(video[\"_type\"], \"url\")\n+        self.assertEqual(video[\"ie_key\"], \"Youtube\")\n+        self.assertEqual(video[\"id\"], \"BaW_jenozKc\")\n+        self.assertEqual(video[\"url\"], \"https://www.youtube.com/watch?v=BaW_jenozKc\")\n+        self.assertEqual(video[\"title\"], \"youtube-dl test video \\\"'/\\\\\u00e4\u21ad\ud835\udd50\")\n+        self.assertEqual(video[\"duration\"], 10)\n+        self.assertEqual(video[\"uploader\"], \"Philipp Hagemeister\")\n \n     def test_youtube_channel_no_uploads(self):\n         dl = FakeYDL()\n-        dl.params['extract_flat'] = True\n+        dl.params[\"extract_flat\"] = True\n         ie = YoutubeTabIE(dl)\n         # no uploads\n-        with self.assertRaisesRegex(ExtractorError, r'no uploads'):\n-            ie.extract('https://www.youtube.com/channel/UC2yXPzFejc422buOIzn_0CA')\n+        with self.assertRaisesRegex(ExtractorError, r\"no uploads\"):\n+            ie.extract(\"https://www.youtube.com/channel/UC2yXPzFejc422buOIzn_0CA\")\n \n         # no uploads and no UCID given\n-        with self.assertRaisesRegex(ExtractorError, r'no uploads'):\n-            ie.extract('https://www.youtube.com/news')\n+        with self.assertRaisesRegex(ExtractorError, r\"no uploads\"):\n+            ie.extract(\"https://www.youtube.com/news\")\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_youtube_misc.py b/test/test_youtube_misc.py\nindex 81be5d3..86b590c 100644\n--- a/test/test_youtube_misc.py\n+++ b/test/test_youtube_misc.py\n@@ -13,14 +13,24 @@\n \n class TestYoutubeMisc(unittest.TestCase):\n     def test_youtube_extract(self):\n-        assertExtractId = lambda url, id: self.assertEqual(YoutubeIE.extract_id(url), id)\n-        assertExtractId('http://www.youtube.com/watch?&v=BaW_jenozKc', 'BaW_jenozKc')\n-        assertExtractId('https://www.youtube.com/watch?&v=BaW_jenozKc', 'BaW_jenozKc')\n-        assertExtractId('https://www.youtube.com/watch?feature=player_embedded&v=BaW_jenozKc', 'BaW_jenozKc')\n-        assertExtractId('https://www.youtube.com/watch_popup?v=BaW_jenozKc', 'BaW_jenozKc')\n-        assertExtractId('http://www.youtube.com/watch?v=BaW_jenozKcsharePLED17F32AD9753930', 'BaW_jenozKc')\n-        assertExtractId('BaW_jenozKc', 'BaW_jenozKc')\n-\n-\n-if __name__ == '__main__':\n+        assertExtractId = lambda url, id: self.assertEqual(\n+            YoutubeIE.extract_id(url), id\n+        )\n+        assertExtractId(\"http://www.youtube.com/watch?&v=BaW_jenozKc\", \"BaW_jenozKc\")\n+        assertExtractId(\"https://www.youtube.com/watch?&v=BaW_jenozKc\", \"BaW_jenozKc\")\n+        assertExtractId(\n+            \"https://www.youtube.com/watch?feature=player_embedded&v=BaW_jenozKc\",\n+            \"BaW_jenozKc\",\n+        )\n+        assertExtractId(\n+            \"https://www.youtube.com/watch_popup?v=BaW_jenozKc\", \"BaW_jenozKc\"\n+        )\n+        assertExtractId(\n+            \"http://www.youtube.com/watch?v=BaW_jenozKcsharePLED17F32AD9753930\",\n+            \"BaW_jenozKc\",\n+        )\n+        assertExtractId(\"BaW_jenozKc\", \"BaW_jenozKc\")\n+\n+\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex c559284..a157862 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -19,149 +19,172 @@\n \n _SIG_TESTS = [\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-vflHOr_nV.js',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-vflHOr_nV.js\",\n         86,\n-        '>=<;:/.-[+*)(\\'&%$#\"!ZYX0VUTSRQPONMLKJIHGFEDCBA\\\\yxwvutsrqponmlkjihgfedcba987654321',\n+        \">=<;:/.-[+*)('&%$#\\\"!ZYX0VUTSRQPONMLKJIHGFEDCBA\\\\yxwvutsrqponmlkjihgfedcba987654321\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-vfldJ8xgI.js',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-vfldJ8xgI.js\",\n         85,\n-        '3456789a0cdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRS[UVWXYZ!\"#$%&\\'()*+,-./:;<=>?@',\n+        \"3456789a0cdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRS[UVWXYZ!\\\"#$%&'()*+,-./:;<=>?@\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-vfle-mVwz.js',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-vfle-mVwz.js\",\n         90,\n-        ']\\\\[@?>=<;:/.-,+*)(\\'&%$#\"hZYXWVUTSRQPONMLKJIHGFEDCBAzyxwvutsrqponmlkjiagfedcb39876',\n+        \"]\\\\[@?>=<;:/.-,+*)('&%$#\\\"hZYXWVUTSRQPONMLKJIHGFEDCBAzyxwvutsrqponmlkjiagfedcb39876\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-en_US-vfl0Cbn9e.js',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vfl0Cbn9e.js\",\n         84,\n-        'O1I3456789abcde0ghijklmnopqrstuvwxyzABCDEFGHfJKLMN2PQRSTUVW@YZ!\"#$%&\\'()*+,-./:;<=',\n+        \"O1I3456789abcde0ghijklmnopqrstuvwxyzABCDEFGHfJKLMN2PQRSTUVW@YZ!\\\"#$%&'()*+,-./:;<=\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflXGBaUN.js',\n-        '2ACFC7A61CA478CD21425E5A57EBD73DDC78E22A.2094302436B2D377D14A3BBA23022D023B8BC25AA',\n-        'A52CB8B320D22032ABB3A41D773D2B6342034902.A22E87CDD37DBE75A5E52412DC874AC16A7CFCA2',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vflXGBaUN.js\",\n+        \"2ACFC7A61CA478CD21425E5A57EBD73DDC78E22A.2094302436B2D377D14A3BBA23022D023B8BC25AA\",\n+        \"A52CB8B320D22032ABB3A41D773D2B6342034902.A22E87CDD37DBE75A5E52412DC874AC16A7CFCA2\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflBb0OQx.js',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vflBb0OQx.js\",\n         84,\n-        '123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ0STUVWXYZ!\"#$%&\\'()*+,@./:;<=>'\n+        \"123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ0STUVWXYZ!\\\"#$%&'()*+,@./:;<=>\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-en_US-vfl9FYC6l.js',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vfl9FYC6l.js\",\n         83,\n-        '123456789abcdefghijklmnopqr0tuvwxyzABCDETGHIJKLMNOPQRS>UVWXYZ!\"#$%&\\'()*+,-./:;<=F'\n+        \"123456789abcdefghijklmnopqr0tuvwxyzABCDETGHIJKLMNOPQRS>UVWXYZ!\\\"#$%&'()*+,-./:;<=F\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflCGk6yw/html5player.js',\n-        '4646B5181C6C3020DF1D9C7FCFEA.AD80ABF70C39BD369CCCAE780AFBB98FA6B6CB42766249D9488C288',\n-        '82C8849D94266724DC6B6AF89BBFA087EACCD963.B93C07FBA084ACAEFCF7C9D1FD0203C6C1815B6B'\n+        \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vflCGk6yw/html5player.js\",\n+        \"4646B5181C6C3020DF1D9C7FCFEA.AD80ABF70C39BD369CCCAE780AFBB98FA6B6CB42766249D9488C288\",\n+        \"82C8849D94266724DC6B6AF89BBFA087EACCD963.B93C07FBA084ACAEFCF7C9D1FD0203C6C1815B6B\",\n     ),\n     (\n-        'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflKjOTVq/html5player.js',\n-        '312AA52209E3623129A412D56A40F11CB0AF14AE.3EE09501CB14E3BCDC3B2AE808BF3F1D14E7FBF12',\n-        '112AA5220913623229A412D56A40F11CB0AF14AE.3EE0950FCB14EEBCDC3B2AE808BF331D14E7FBF3',\n+        \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vflKjOTVq/html5player.js\",\n+        \"312AA52209E3623129A412D56A40F11CB0AF14AE.3EE09501CB14E3BCDC3B2AE808BF3F1D14E7FBF12\",\n+        \"112AA5220913623229A412D56A40F11CB0AF14AE.3EE0950FCB14EEBCDC3B2AE808BF331D14E7FBF3\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/6ed0d907/player_ias.vflset/en_US/base.js',\n-        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n-        'AOq0QJ8wRAIgXmPlOPSBkkUs1bYFYlJCfe29xx8j7v1pDL2QwbdV96sCIEzpWqMGkFR20CFOg51Tp-7vj_EMu-m37KtXJoOySqa0',\n+        \"https://www.youtube.com/s/player/6ed0d907/player_ias.vflset/en_US/base.js\",\n+        \"2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA\",\n+        \"AOq0QJ8wRAIgXmPlOPSBkkUs1bYFYlJCfe29xx8j7v1pDL2QwbdV96sCIEzpWqMGkFR20CFOg51Tp-7vj_EMu-m37KtXJoOySqa0\",\n     ),\n ]\n \n _NSIG_TESTS = [\n     (\n-        'https://www.youtube.com/s/player/7862ca1f/player_ias.vflset/en_US/base.js',\n-        'X_LCxVDjAavgE5t', 'yxJ1dM6iz5ogUg',\n+        \"https://www.youtube.com/s/player/7862ca1f/player_ias.vflset/en_US/base.js\",\n+        \"X_LCxVDjAavgE5t\",\n+        \"yxJ1dM6iz5ogUg\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/9216d1f7/player_ias.vflset/en_US/base.js',\n-        'SLp9F5bwjAdhE9F-', 'gWnb9IK2DJ8Q1w',\n+        \"https://www.youtube.com/s/player/9216d1f7/player_ias.vflset/en_US/base.js\",\n+        \"SLp9F5bwjAdhE9F-\",\n+        \"gWnb9IK2DJ8Q1w\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/f8cb7a3b/player_ias.vflset/en_US/base.js',\n-        'oBo2h5euWy6osrUt', 'ivXHpm7qJjJN',\n+        \"https://www.youtube.com/s/player/f8cb7a3b/player_ias.vflset/en_US/base.js\",\n+        \"oBo2h5euWy6osrUt\",\n+        \"ivXHpm7qJjJN\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/2dfe380c/player_ias.vflset/en_US/base.js',\n-        'oBo2h5euWy6osrUt', '3DIBbn3qdQ',\n+        \"https://www.youtube.com/s/player/2dfe380c/player_ias.vflset/en_US/base.js\",\n+        \"oBo2h5euWy6osrUt\",\n+        \"3DIBbn3qdQ\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/f1ca6900/player_ias.vflset/en_US/base.js',\n-        'cu3wyu6LQn2hse', 'jvxetvmlI9AN9Q',\n+        \"https://www.youtube.com/s/player/f1ca6900/player_ias.vflset/en_US/base.js\",\n+        \"cu3wyu6LQn2hse\",\n+        \"jvxetvmlI9AN9Q\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/8040e515/player_ias.vflset/en_US/base.js',\n-        'wvOFaY-yjgDuIEg5', 'HkfBFDHmgw4rsw',\n+        \"https://www.youtube.com/s/player/8040e515/player_ias.vflset/en_US/base.js\",\n+        \"wvOFaY-yjgDuIEg5\",\n+        \"HkfBFDHmgw4rsw\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/e06dea74/player_ias.vflset/en_US/base.js',\n-        'AiuodmaDDYw8d3y4bf', 'ankd8eza2T6Qmw',\n+        \"https://www.youtube.com/s/player/e06dea74/player_ias.vflset/en_US/base.js\",\n+        \"AiuodmaDDYw8d3y4bf\",\n+        \"ankd8eza2T6Qmw\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/5dd88d1d/player-plasma-ias-phone-en_US.vflset/base.js',\n-        'kSxKFLeqzv_ZyHSAt', 'n8gS8oRlHOxPFA',\n+        \"https://www.youtube.com/s/player/5dd88d1d/player-plasma-ias-phone-en_US.vflset/base.js\",\n+        \"kSxKFLeqzv_ZyHSAt\",\n+        \"n8gS8oRlHOxPFA\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/324f67b9/player_ias.vflset/en_US/base.js',\n-        'xdftNy7dh9QGnhW', '22qLGxrmX8F1rA',\n+        \"https://www.youtube.com/s/player/324f67b9/player_ias.vflset/en_US/base.js\",\n+        \"xdftNy7dh9QGnhW\",\n+        \"22qLGxrmX8F1rA\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js',\n-        'TDCstCG66tEAO5pR9o', 'dbxNtZ14c-yWyw',\n+        \"https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js\",\n+        \"TDCstCG66tEAO5pR9o\",\n+        \"dbxNtZ14c-yWyw\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/c81bbb4a/player_ias.vflset/en_US/base.js',\n-        'gre3EcLurNY2vqp94', 'Z9DfGxWP115WTg',\n+        \"https://www.youtube.com/s/player/c81bbb4a/player_ias.vflset/en_US/base.js\",\n+        \"gre3EcLurNY2vqp94\",\n+        \"Z9DfGxWP115WTg\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/1f7d5369/player_ias.vflset/en_US/base.js',\n-        'batNX7sYqIJdkJ', 'IhOkL_zxbkOZBw',\n+        \"https://www.youtube.com/s/player/1f7d5369/player_ias.vflset/en_US/base.js\",\n+        \"batNX7sYqIJdkJ\",\n+        \"IhOkL_zxbkOZBw\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/009f1d77/player_ias.vflset/en_US/base.js',\n-        '5dwFHw8aFWQUQtffRq', 'audescmLUzI3jw',\n+        \"https://www.youtube.com/s/player/009f1d77/player_ias.vflset/en_US/base.js\",\n+        \"5dwFHw8aFWQUQtffRq\",\n+        \"audescmLUzI3jw\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/dc0c6770/player_ias.vflset/en_US/base.js',\n-        '5EHDMgYLV6HPGk_Mu-kk', 'n9lUJLHbxUI0GQ',\n+        \"https://www.youtube.com/s/player/dc0c6770/player_ias.vflset/en_US/base.js\",\n+        \"5EHDMgYLV6HPGk_Mu-kk\",\n+        \"n9lUJLHbxUI0GQ\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/113ca41c/player_ias.vflset/en_US/base.js',\n-        'cgYl-tlYkhjT7A', 'hI7BBr2zUgcmMg',\n+        \"https://www.youtube.com/s/player/113ca41c/player_ias.vflset/en_US/base.js\",\n+        \"cgYl-tlYkhjT7A\",\n+        \"hI7BBr2zUgcmMg\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/c57c113c/player_ias.vflset/en_US/base.js',\n-        'M92UUMHa8PdvPd3wyM', '3hPqLJsiNZx7yA',\n+        \"https://www.youtube.com/s/player/c57c113c/player_ias.vflset/en_US/base.js\",\n+        \"M92UUMHa8PdvPd3wyM\",\n+        \"3hPqLJsiNZx7yA\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/5a3b6271/player_ias.vflset/en_US/base.js',\n-        'B2j7f_UPT4rfje85Lu_e', 'm5DmNymaGQ5RdQ',\n+        \"https://www.youtube.com/s/player/5a3b6271/player_ias.vflset/en_US/base.js\",\n+        \"B2j7f_UPT4rfje85Lu_e\",\n+        \"m5DmNymaGQ5RdQ\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/7a062b77/player_ias.vflset/en_US/base.js',\n-        'NRcE3y3mVtm_cV-W', 'VbsCYUATvqlt5w',\n+        \"https://www.youtube.com/s/player/7a062b77/player_ias.vflset/en_US/base.js\",\n+        \"NRcE3y3mVtm_cV-W\",\n+        \"VbsCYUATvqlt5w\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/dac945fd/player_ias.vflset/en_US/base.js',\n-        'o8BkRxXhuYsBCWi6RplPdP', '3Lx32v_hmzTm6A',\n+        \"https://www.youtube.com/s/player/dac945fd/player_ias.vflset/en_US/base.js\",\n+        \"o8BkRxXhuYsBCWi6RplPdP\",\n+        \"3Lx32v_hmzTm6A\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/6f20102c/player_ias.vflset/en_US/base.js',\n-        'lE8DhoDmKqnmJJ', 'pJTTX6XyJP2BYw',\n+        \"https://www.youtube.com/s/player/6f20102c/player_ias.vflset/en_US/base.js\",\n+        \"lE8DhoDmKqnmJJ\",\n+        \"pJTTX6XyJP2BYw\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/cfa9e7cb/player_ias.vflset/en_US/base.js',\n-        'aCi3iElgd2kq0bxVbQ', 'QX1y8jGb2IbZ0w',\n+        \"https://www.youtube.com/s/player/cfa9e7cb/player_ias.vflset/en_US/base.js\",\n+        \"aCi3iElgd2kq0bxVbQ\",\n+        \"QX1y8jGb2IbZ0w\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/8c7583ff/player_ias.vflset/en_US/base.js',\n-        '1wWCVpRR96eAmMI87L', 'KSkWAVv1ZQxC3A',\n+        \"https://www.youtube.com/s/player/8c7583ff/player_ias.vflset/en_US/base.js\",\n+        \"1wWCVpRR96eAmMI87L\",\n+        \"KSkWAVv1ZQxC3A\",\n     ),\n     (\n-        'https://www.youtube.com/s/player/b7910ca8/player_ias.vflset/en_US/base.js',\n-        '_hXMCwMt9qE310D', 'LoZMgkkofRMCZQ',\n+        \"https://www.youtube.com/s/player/b7910ca8/player_ias.vflset/en_US/base.js\",\n+        \"_hXMCwMt9qE310D\",\n+        \"LoZMgkkofRMCZQ\",\n     ),\n ]\n \n@@ -170,20 +193,59 @@\n class TestPlayerInfo(unittest.TestCase):\n     def test_youtube_extract_player_info(self):\n         PLAYER_URLS = (\n-            ('https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js', '4c3f79c5'),\n-            ('https://www.youtube.com/s/player/64dddad9/player_ias.vflset/en_US/base.js', '64dddad9'),\n-            ('https://www.youtube.com/s/player/64dddad9/player_ias.vflset/fr_FR/base.js', '64dddad9'),\n-            ('https://www.youtube.com/s/player/64dddad9/player-plasma-ias-phone-en_US.vflset/base.js', '64dddad9'),\n-            ('https://www.youtube.com/s/player/64dddad9/player-plasma-ias-phone-de_DE.vflset/base.js', '64dddad9'),\n-            ('https://www.youtube.com/s/player/64dddad9/player-plasma-ias-tablet-en_US.vflset/base.js', '64dddad9'),\n+            (\n+                \"https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js\",\n+                \"4c3f79c5\",\n+            ),\n+            (\n+                \"https://www.youtube.com/s/player/64dddad9/player_ias.vflset/en_US/base.js\",\n+                \"64dddad9\",\n+            ),\n+            (\n+                \"https://www.youtube.com/s/player/64dddad9/player_ias.vflset/fr_FR/base.js\",\n+                \"64dddad9\",\n+            ),\n+            (\n+                \"https://www.youtube.com/s/player/64dddad9/player-plasma-ias-phone-en_US.vflset/base.js\",\n+                \"64dddad9\",\n+            ),\n+            (\n+                \"https://www.youtube.com/s/player/64dddad9/player-plasma-ias-phone-de_DE.vflset/base.js\",\n+                \"64dddad9\",\n+            ),\n+            (\n+                \"https://www.youtube.com/s/player/64dddad9/player-plasma-ias-tablet-en_US.vflset/base.js\",\n+                \"64dddad9\",\n+            ),\n             # obsolete\n-            ('https://www.youtube.com/yts/jsbin/player_ias-vfle4-e03/en_US/base.js', 'vfle4-e03'),\n-            ('https://www.youtube.com/yts/jsbin/player_ias-vfl49f_g4/en_US/base.js', 'vfl49f_g4'),\n-            ('https://www.youtube.com/yts/jsbin/player_ias-vflCPQUIL/en_US/base.js', 'vflCPQUIL'),\n-            ('https://www.youtube.com/yts/jsbin/player-vflzQZbt7/en_US/base.js', 'vflzQZbt7'),\n-            ('https://www.youtube.com/yts/jsbin/player-en_US-vflaxXRn1/base.js', 'vflaxXRn1'),\n-            ('https://s.ytimg.com/yts/jsbin/html5player-en_US-vflXGBaUN.js', 'vflXGBaUN'),\n-            ('https://s.ytimg.com/yts/jsbin/html5player-en_US-vflKjOTVq/html5player.js', 'vflKjOTVq'),\n+            (\n+                \"https://www.youtube.com/yts/jsbin/player_ias-vfle4-e03/en_US/base.js\",\n+                \"vfle4-e03\",\n+            ),\n+            (\n+                \"https://www.youtube.com/yts/jsbin/player_ias-vfl49f_g4/en_US/base.js\",\n+                \"vfl49f_g4\",\n+            ),\n+            (\n+                \"https://www.youtube.com/yts/jsbin/player_ias-vflCPQUIL/en_US/base.js\",\n+                \"vflCPQUIL\",\n+            ),\n+            (\n+                \"https://www.youtube.com/yts/jsbin/player-vflzQZbt7/en_US/base.js\",\n+                \"vflzQZbt7\",\n+            ),\n+            (\n+                \"https://www.youtube.com/yts/jsbin/player-en_US-vflaxXRn1/base.js\",\n+                \"vflaxXRn1\",\n+            ),\n+            (\n+                \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vflXGBaUN.js\",\n+                \"vflXGBaUN\",\n+            ),\n+            (\n+                \"https://s.ytimg.com/yts/jsbin/html5player-en_US-vflKjOTVq/html5player.js\",\n+                \"vflKjOTVq\",\n+            ),\n         )\n         for player_url, expected_player_id in PLAYER_URLS:\n             player_id = YoutubeIE._extract_player_info(player_url)\n@@ -194,7 +256,7 @@ def test_youtube_extract_player_info(self):\n class TestSignature(unittest.TestCase):\n     def setUp(self):\n         TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n-        self.TESTDATA_DIR = os.path.join(TEST_DIR, 'testdata/sigs')\n+        self.TESTDATA_DIR = os.path.join(TEST_DIR, \"testdata/sigs\")\n         if not os.path.exists(self.TESTDATA_DIR):\n             os.mkdir(self.TESTDATA_DIR)\n \n@@ -207,29 +269,30 @@ def tearDown(self):\n def t_factory(name, sig_func, url_pattern):\n     def make_tfunc(url, sig_input, expected_sig):\n         m = url_pattern.match(url)\n-        assert m, '%r should follow URL format' % url\n-        test_id = m.group('id')\n+        assert m, \"%r should follow URL format\" % url\n+        test_id = m.group(\"id\")\n \n         def test_func(self):\n-            basename = f'player-{name}-{test_id}.js'\n+            basename = f\"player-{name}-{test_id}.js\"\n             fn = os.path.join(self.TESTDATA_DIR, basename)\n \n             if not os.path.exists(fn):\n                 urllib.request.urlretrieve(url, fn)\n-            with open(fn, encoding='utf-8') as testf:\n+            with open(fn, encoding=\"utf-8\") as testf:\n                 jscode = testf.read()\n             self.assertEqual(sig_func(jscode, sig_input), expected_sig)\n \n-        test_func.__name__ = f'test_{name}_js_{test_id}'\n+        test_func.__name__ = f\"test_{name}_js_{test_id}\"\n         setattr(TestSignature, test_func.__name__, test_func)\n+\n     return make_tfunc\n \n \n def signature(jscode, sig_input):\n     func = YoutubeIE(FakeYDL())._parse_sig_js(jscode)\n     src_sig = (\n-        str(string.printable[:sig_input])\n-        if isinstance(sig_input, int) else sig_input)\n+        str(string.printable[:sig_input]) if isinstance(sig_input, int) else sig_input\n+    )\n     return func(src_sig)\n \n \n@@ -239,15 +302,21 @@ def n_sig(jscode, sig_input):\n \n \n make_sig_test = t_factory(\n-    'signature', signature, re.compile(r'.*(?:-|/player/)(?P<id>[a-zA-Z0-9_-]+)(?:/.+\\.js|(?:/watch_as3|/html5player)?\\.[a-z]+)$'))\n+    \"signature\",\n+    signature,\n+    re.compile(\n+        r\".*(?:-|/player/)(?P<id>[a-zA-Z0-9_-]+)(?:/.+\\.js|(?:/watch_as3|/html5player)?\\.[a-z]+)$\"\n+    ),\n+)\n for test_spec in _SIG_TESTS:\n     make_sig_test(*test_spec)\n \n make_nsig_test = t_factory(\n-    'nsig', n_sig, re.compile(r'.+/player/(?P<id>[a-zA-Z0-9_-]+)/.+.js$'))\n+    \"nsig\", n_sig, re.compile(r\".+/player/(?P<id>[a-zA-Z0-9_-]+)/.+.js$\")\n+)\n for test_spec in _NSIG_TESTS:\n     make_nsig_test(*test_spec)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\ndiff --git a/yt_dlp/YoutubeDL.py b/yt_dlp/YoutubeDL.py\nindex 8d96498..7680efc 100644\n--- a/yt_dlp/YoutubeDL.py\n+++ b/yt_dlp/YoutubeDL.py\n@@ -166,7 +166,7 @@\n )\n from .version import CHANNEL, ORIGIN, RELEASE_GIT_HEAD, VARIANT, __version__\n \n-if compat_os_name == 'nt':\n+if compat_os_name == \"nt\":\n     import ctypes\n \n \n@@ -561,30 +561,95 @@ class YoutubeDL:\n     \"\"\"\n \n     _NUMERIC_FIELDS = {\n-        'width', 'height', 'asr', 'audio_channels', 'fps',\n-        'tbr', 'abr', 'vbr', 'filesize', 'filesize_approx',\n-        'timestamp', 'release_timestamp',\n-        'duration', 'view_count', 'like_count', 'dislike_count', 'repost_count',\n-        'average_rating', 'comment_count', 'age_limit',\n-        'start_time', 'end_time',\n-        'chapter_number', 'season_number', 'episode_number',\n-        'track_number', 'disc_number', 'release_year',\n+        \"width\",\n+        \"height\",\n+        \"asr\",\n+        \"audio_channels\",\n+        \"fps\",\n+        \"tbr\",\n+        \"abr\",\n+        \"vbr\",\n+        \"filesize\",\n+        \"filesize_approx\",\n+        \"timestamp\",\n+        \"release_timestamp\",\n+        \"duration\",\n+        \"view_count\",\n+        \"like_count\",\n+        \"dislike_count\",\n+        \"repost_count\",\n+        \"average_rating\",\n+        \"comment_count\",\n+        \"age_limit\",\n+        \"start_time\",\n+        \"end_time\",\n+        \"chapter_number\",\n+        \"season_number\",\n+        \"episode_number\",\n+        \"track_number\",\n+        \"disc_number\",\n+        \"release_year\",\n     }\n \n     _format_fields = {\n         # NB: Keep in sync with the docstring of extractor/common.py\n-        'url', 'manifest_url', 'manifest_stream_number', 'ext', 'format', 'format_id', 'format_note',\n-        'width', 'height', 'aspect_ratio', 'resolution', 'dynamic_range', 'tbr', 'abr', 'acodec', 'asr', 'audio_channels',\n-        'vbr', 'fps', 'vcodec', 'container', 'filesize', 'filesize_approx', 'rows', 'columns',\n-        'player_url', 'protocol', 'fragment_base_url', 'fragments', 'is_from_start',\n-        'preference', 'language', 'language_preference', 'quality', 'source_preference', 'cookies',\n-        'http_headers', 'stretched_ratio', 'no_resume', 'has_drm', 'extra_param_to_segment_url', 'hls_aes', 'downloader_options',\n-        'page_url', 'app', 'play_path', 'tc_url', 'flash_version', 'rtmp_live', 'rtmp_conn', 'rtmp_protocol', 'rtmp_real_time'\n+        \"url\",\n+        \"manifest_url\",\n+        \"manifest_stream_number\",\n+        \"ext\",\n+        \"format\",\n+        \"format_id\",\n+        \"format_note\",\n+        \"width\",\n+        \"height\",\n+        \"aspect_ratio\",\n+        \"resolution\",\n+        \"dynamic_range\",\n+        \"tbr\",\n+        \"abr\",\n+        \"acodec\",\n+        \"asr\",\n+        \"audio_channels\",\n+        \"vbr\",\n+        \"fps\",\n+        \"vcodec\",\n+        \"container\",\n+        \"filesize\",\n+        \"filesize_approx\",\n+        \"rows\",\n+        \"columns\",\n+        \"player_url\",\n+        \"protocol\",\n+        \"fragment_base_url\",\n+        \"fragments\",\n+        \"is_from_start\",\n+        \"preference\",\n+        \"language\",\n+        \"language_preference\",\n+        \"quality\",\n+        \"source_preference\",\n+        \"cookies\",\n+        \"http_headers\",\n+        \"stretched_ratio\",\n+        \"no_resume\",\n+        \"has_drm\",\n+        \"extra_param_to_segment_url\",\n+        \"hls_aes\",\n+        \"downloader_options\",\n+        \"page_url\",\n+        \"app\",\n+        \"play_path\",\n+        \"tc_url\",\n+        \"flash_version\",\n+        \"rtmp_live\",\n+        \"rtmp_conn\",\n+        \"rtmp_protocol\",\n+        \"rtmp_real_time\",\n     }\n     _format_selection_exts = {\n-        'audio': set(MEDIA_EXTENSIONS.common_audio),\n-        'video': set(MEDIA_EXTENSIONS.common_video + ('3gp', )),\n-        'storyboards': set(MEDIA_EXTENSIONS.storyboards),\n+        \"audio\": set(MEDIA_EXTENSIONS.common_audio),\n+        \"video\": set(MEDIA_EXTENSIONS.common_video + (\"3gp\",)),\n+        \"storyboards\": set(MEDIA_EXTENSIONS.storyboards),\n     }\n \n     def __init__(self, params=None, auto_init=True):\n@@ -611,166 +676,209 @@ def __init__(self, params=None, auto_init=True):\n         self.cache = Cache(self)\n         self.__header_cookies = []\n \n-        stdout = sys.stderr if self.params.get('logtostderr') else sys.stdout\n+        stdout = sys.stderr if self.params.get(\"logtostderr\") else sys.stdout\n         self._out_files = Namespace(\n             out=stdout,\n             error=sys.stderr,\n-            screen=sys.stderr if self.params.get('quiet') else stdout,\n-            console=None if compat_os_name == 'nt' else next(\n-                filter(supports_terminal_sequences, (sys.stderr, sys.stdout)), None)\n+            screen=sys.stderr if self.params.get(\"quiet\") else stdout,\n+            console=None\n+            if compat_os_name == \"nt\"\n+            else next(\n+                filter(supports_terminal_sequences, (sys.stderr, sys.stdout)), None\n+            ),\n         )\n \n         try:\n             windows_enable_vt_mode()\n         except Exception as e:\n-            self.write_debug(f'Failed to enable VT mode: {e}')\n+            self.write_debug(f\"Failed to enable VT mode: {e}\")\n \n-        if self.params.get('no_color'):\n-            if self.params.get('color') is not None:\n-                self.params.setdefault('_warnings', []).append(\n-                    'Overwriting params from \"color\" with \"no_color\"')\n-            self.params['color'] = 'no_color'\n+        if self.params.get(\"no_color\"):\n+            if self.params.get(\"color\") is not None:\n+                self.params.setdefault(\"_warnings\", []).append(\n+                    'Overwriting params from \"color\" with \"no_color\"'\n+                )\n+            self.params[\"color\"] = \"no_color\"\n \n-        term_allow_color = os.getenv('TERM', '').lower() != 'dumb'\n-        no_color = bool(os.getenv('NO_COLOR'))\n+        term_allow_color = os.getenv(\"TERM\", \"\").lower() != \"dumb\"\n+        no_color = bool(os.getenv(\"NO_COLOR\"))\n \n         def process_color_policy(stream):\n-            stream_name = {sys.stdout: 'stdout', sys.stderr: 'stderr'}[stream]\n-            policy = traverse_obj(self.params, ('color', (stream_name, None), {str}), get_all=False)\n-            if policy in ('auto', None):\n+            stream_name = {sys.stdout: \"stdout\", sys.stderr: \"stderr\"}[stream]\n+            policy = traverse_obj(\n+                self.params, (\"color\", (stream_name, None), {str}), get_all=False\n+            )\n+            if policy in (\"auto\", None):\n                 if term_allow_color and supports_terminal_sequences(stream):\n-                    return 'no_color' if no_color else True\n+                    return \"no_color\" if no_color else True\n                 return False\n-            assert policy in ('always', 'never', 'no_color'), policy\n-            return {'always': True, 'never': False}.get(policy, policy)\n-\n-        self._allow_colors = Namespace(**{\n-            name: process_color_policy(stream)\n-            for name, stream in self._out_files.items_ if name != 'console'\n-        })\n+            assert policy in (\"always\", \"never\", \"no_color\"), policy\n+            return {\"always\": True, \"never\": False}.get(policy, policy)\n+\n+        self._allow_colors = Namespace(\n+            **{\n+                name: process_color_policy(stream)\n+                for name, stream in self._out_files.items_\n+                if name != \"console\"\n+            }\n+        )\n \n         system_deprecation = _get_system_deprecation()\n         if system_deprecation:\n-            self.deprecated_feature(system_deprecation.replace('\\n', '\\n                    '))\n+            self.deprecated_feature(\n+                system_deprecation.replace(\"\\n\", \"\\n                    \")\n+            )\n \n-        if self.params.get('allow_unplayable_formats'):\n+        if self.params.get(\"allow_unplayable_formats\"):\n             self.report_warning(\n-                f'You have asked for {self._format_err(\"UNPLAYABLE\", self.Styles.EMPHASIS)} formats to be listed/downloaded. '\n-                'This is a developer option intended for debugging. \\n'\n-                '         If you experience any issues while using this option, '\n-                f'{self._format_err(\"DO NOT\", self.Styles.ERROR)} open a bug report')\n+                f\"You have asked for {self._format_err('UNPLAYABLE', self.Styles.EMPHASIS)} formats to be listed/downloaded. \"\n+                \"This is a developer option intended for debugging. \\n\"\n+                \"         If you experience any issues while using this option, \"\n+                f\"{self._format_err('DO NOT', self.Styles.ERROR)} open a bug report\"\n+            )\n \n-        if self.params.get('bidi_workaround', False):\n+        if self.params.get(\"bidi_workaround\", False):\n             try:\n                 import pty\n+\n                 master, slave = pty.openpty()\n                 width = shutil.get_terminal_size().columns\n-                width_args = [] if width is None else ['-w', str(width)]\n-                sp_kwargs = {'stdin': subprocess.PIPE, 'stdout': slave, 'stderr': self._out_files.error}\n+                width_args = [] if width is None else [\"-w\", str(width)]\n+                sp_kwargs = {\n+                    \"stdin\": subprocess.PIPE,\n+                    \"stdout\": slave,\n+                    \"stderr\": self._out_files.error,\n+                }\n                 try:\n-                    self._output_process = Popen(['bidiv'] + width_args, **sp_kwargs)\n+                    self._output_process = Popen([\"bidiv\"] + width_args, **sp_kwargs)\n                 except OSError:\n-                    self._output_process = Popen(['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)\n-                self._output_channel = os.fdopen(master, 'rb')\n+                    self._output_process = Popen(\n+                        [\"fribidi\", \"-c\", \"UTF-8\"] + width_args, **sp_kwargs\n+                    )\n+                self._output_channel = os.fdopen(master, \"rb\")\n             except OSError as ose:\n                 if ose.errno == errno.ENOENT:\n                     self.report_warning(\n-                        'Could not find fribidi executable, ignoring --bidi-workaround. '\n-                        'Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')\n+                        \"Could not find fribidi executable, ignoring --bidi-workaround. \"\n+                        \"Make sure that  fribidi  is an executable file in one of the directories in your $PATH.\"\n+                    )\n                 else:\n                     raise\n \n-        self.params['compat_opts'] = set(self.params.get('compat_opts', ()))\n-        self.params['http_headers'] = HTTPHeaderDict(std_headers, self.params.get('http_headers'))\n-        self._load_cookies(self.params['http_headers'].get('Cookie'))  # compat\n-        self.params['http_headers'].pop('Cookie', None)\n-        self._request_director = self.build_request_director(_REQUEST_HANDLERS.values(), _RH_PREFERENCES)\n+        self.params[\"compat_opts\"] = set(self.params.get(\"compat_opts\", ()))\n+        self.params[\"http_headers\"] = HTTPHeaderDict(\n+            std_headers, self.params.get(\"http_headers\")\n+        )\n+        self._load_cookies(self.params[\"http_headers\"].get(\"Cookie\"))  # compat\n+        self.params[\"http_headers\"].pop(\"Cookie\", None)\n+        self._request_director = self.build_request_director(\n+            _REQUEST_HANDLERS.values(), _RH_PREFERENCES\n+        )\n \n-        if auto_init and auto_init != 'no_verbose_header':\n+        if auto_init and auto_init != \"no_verbose_header\":\n             self.print_debug_header()\n \n         def check_deprecated(param, option, suggestion):\n             if self.params.get(param) is not None:\n-                self.report_warning(f'{option} is deprecated. Use {suggestion} instead')\n+                self.report_warning(f\"{option} is deprecated. Use {suggestion} instead\")\n                 return True\n             return False\n \n-        if check_deprecated('cn_verification_proxy', '--cn-verification-proxy', '--geo-verification-proxy'):\n-            if self.params.get('geo_verification_proxy') is None:\n-                self.params['geo_verification_proxy'] = self.params['cn_verification_proxy']\n+        if check_deprecated(\n+            \"cn_verification_proxy\",\n+            \"--cn-verification-proxy\",\n+            \"--geo-verification-proxy\",\n+        ):\n+            if self.params.get(\"geo_verification_proxy\") is None:\n+                self.params[\"geo_verification_proxy\"] = self.params[\n+                    \"cn_verification_proxy\"\n+                ]\n \n-        check_deprecated('autonumber', '--auto-number', '-o \"%(autonumber)s-%(title)s.%(ext)s\"')\n-        check_deprecated('usetitle', '--title', '-o \"%(title)s-%(id)s.%(ext)s\"')\n-        check_deprecated('useid', '--id', '-o \"%(id)s.%(ext)s\"')\n+        check_deprecated(\n+            \"autonumber\", \"--auto-number\", '-o \"%(autonumber)s-%(title)s.%(ext)s\"'\n+        )\n+        check_deprecated(\"usetitle\", \"--title\", '-o \"%(title)s-%(id)s.%(ext)s\"')\n+        check_deprecated(\"useid\", \"--id\", '-o \"%(id)s.%(ext)s\"')\n \n-        for msg in self.params.get('_warnings', []):\n+        for msg in self.params.get(\"_warnings\", []):\n             self.report_warning(msg)\n-        for msg in self.params.get('_deprecation_warnings', []):\n+        for msg in self.params.get(\"_deprecation_warnings\", []):\n             self.deprecated_feature(msg)\n \n-        if 'list-formats' in self.params['compat_opts']:\n-            self.params['listformats_table'] = False\n+        if \"list-formats\" in self.params[\"compat_opts\"]:\n+            self.params[\"listformats_table\"] = False\n \n-        if 'overwrites' not in self.params and self.params.get('nooverwrites') is not None:\n+        if (\n+            \"overwrites\" not in self.params\n+            and self.params.get(\"nooverwrites\") is not None\n+        ):\n             # nooverwrites was unnecessarily changed to overwrites\n             # in 0c3d0f51778b153f65c21906031c2e091fcfb641\n             # This ensures compatibility with both keys\n-            self.params['overwrites'] = not self.params['nooverwrites']\n-        elif self.params.get('overwrites') is None:\n-            self.params.pop('overwrites', None)\n+            self.params[\"overwrites\"] = not self.params[\"nooverwrites\"]\n+        elif self.params.get(\"overwrites\") is None:\n+            self.params.pop(\"overwrites\", None)\n         else:\n-            self.params['nooverwrites'] = not self.params['overwrites']\n+            self.params[\"nooverwrites\"] = not self.params[\"overwrites\"]\n \n-        if self.params.get('simulate') is None and any((\n-            self.params.get('list_thumbnails'),\n-            self.params.get('listformats'),\n-            self.params.get('listsubtitles'),\n-        )):\n-            self.params['simulate'] = 'list_only'\n+        if self.params.get(\"simulate\") is None and any(\n+            (\n+                self.params.get(\"list_thumbnails\"),\n+                self.params.get(\"listformats\"),\n+                self.params.get(\"listsubtitles\"),\n+            )\n+        ):\n+            self.params[\"simulate\"] = \"list_only\"\n \n-        self.params.setdefault('forceprint', {})\n-        self.params.setdefault('print_to_file', {})\n+        self.params.setdefault(\"forceprint\", {})\n+        self.params.setdefault(\"print_to_file\", {})\n \n         # Compatibility with older syntax\n-        if not isinstance(params['forceprint'], dict):\n-            self.params['forceprint'] = {'video': params['forceprint']}\n+        if not isinstance(params[\"forceprint\"], dict):\n+            self.params[\"forceprint\"] = {\"video\": params[\"forceprint\"]}\n \n         if auto_init:\n             self.add_default_info_extractors()\n \n-        if (sys.platform != 'win32'\n-                and sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']\n-                and not self.params.get('restrictfilenames', False)):\n+        if (\n+            sys.platform != \"win32\"\n+            and sys.getfilesystemencoding() in [\"ascii\", \"ANSI_X3.4-1968\"]\n+            and not self.params.get(\"restrictfilenames\", False)\n+        ):\n             # Unicode filesystem API will throw errors (#1474, #13027)\n             self.report_warning(\n-                'Assuming --restrict-filenames since file system encoding '\n-                'cannot encode all characters. '\n-                'Set the LC_ALL environment variable to fix this.')\n-            self.params['restrictfilenames'] = True\n+                \"Assuming --restrict-filenames since file system encoding \"\n+                \"cannot encode all characters. \"\n+                \"Set the LC_ALL environment variable to fix this.\"\n+            )\n+            self.params[\"restrictfilenames\"] = True\n \n         self._parse_outtmpl()\n \n         # Creating format selector here allows us to catch syntax errors before the extraction\n         self.format_selector = (\n-            self.params.get('format') if self.params.get('format') in (None, '-')\n-            else self.params['format'] if callable(self.params['format'])\n-            else self.build_format_selector(self.params['format']))\n+            self.params.get(\"format\")\n+            if self.params.get(\"format\") in (None, \"-\")\n+            else self.params[\"format\"]\n+            if callable(self.params[\"format\"])\n+            else self.build_format_selector(self.params[\"format\"])\n+        )\n \n         hooks = {\n-            'post_hooks': self.add_post_hook,\n-            'progress_hooks': self.add_progress_hook,\n-            'postprocessor_hooks': self.add_postprocessor_hook,\n+            \"post_hooks\": self.add_post_hook,\n+            \"progress_hooks\": self.add_progress_hook,\n+            \"postprocessor_hooks\": self.add_postprocessor_hook,\n         }\n         for opt, fn in hooks.items():\n             for ph in self.params.get(opt, []):\n                 fn(ph)\n \n-        for pp_def_raw in self.params.get('postprocessors', []):\n+        for pp_def_raw in self.params.get(\"postprocessors\", []):\n             pp_def = dict(pp_def_raw)\n-            when = pp_def.pop('when', 'post_process')\n+            when = pp_def.pop(\"when\", \"post_process\")\n             self.add_post_processor(\n-                get_postprocessor(pp_def.pop('key'))(self, **pp_def),\n-                when=when)\n+                get_postprocessor(pp_def.pop(\"key\"))(self, **pp_def), when=when\n+            )\n \n         def preload_download_archive(fn):\n             \"\"\"Preload the archive, if any is specified\"\"\"\n@@ -780,9 +888,9 @@ def preload_download_archive(fn):\n             elif not is_path_like(fn):\n                 return fn\n \n-            self.write_debug(f'Loading archive file {fn!r}')\n+            self.write_debug(f\"Loading archive file {fn!r}\")\n             try:\n-                with locked_file(fn, 'r', encoding='utf-8') as archive_file:\n+                with locked_file(fn, \"r\", encoding=\"utf-8\") as archive_file:\n                     for line in archive_file:\n                         archive.add(line.strip())\n             except OSError as ioe:\n@@ -790,23 +898,23 @@ def preload_download_archive(fn):\n                     raise\n             return archive\n \n-        self.archive = preload_download_archive(self.params.get('download_archive'))\n+        self.archive = preload_download_archive(self.params.get(\"download_archive\"))\n \n     def warn_if_short_id(self, argv):\n         # short YouTube ID starting with dash?\n-        idxs = [\n-            i for i, a in enumerate(argv)\n-            if re.match(r'^-[0-9A-Za-z_-]{10}$', a)]\n+        idxs = [i for i, a in enumerate(argv) if re.match(r\"^-[0-9A-Za-z_-]{10}$\", a)]\n         if idxs:\n             correct_argv = (\n-                ['yt-dlp']\n+                [\"yt-dlp\"]\n                 + [a for i, a in enumerate(argv) if i not in idxs]\n-                + ['--'] + [argv[i] for i in idxs]\n+                + [\"--\"]\n+                + [argv[i] for i in idxs]\n             )\n             self.report_warning(\n-                'Long argument string detected. '\n-                'Use -- to separate parameters and URLs, like this:\\n%s' %\n-                args_to_str(correct_argv))\n+                \"Long argument string detected. \"\n+                \"Use -- to separate parameters and URLs, like this:\\n%s\"\n+                % args_to_str(correct_argv)\n+            )\n \n     def add_info_extractor(self, ie):\n         \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"\n@@ -833,22 +941,25 @@ def add_default_info_extractors(self):\n         Add the InfoExtractors returned by gen_extractors to the end of the list\n         \"\"\"\n         all_ies = {ie.IE_NAME.lower(): ie for ie in gen_extractor_classes()}\n-        all_ies['end'] = UnsupportedURLIE()\n+        all_ies[\"end\"] = UnsupportedURLIE()\n         try:\n             ie_names = orderedSet_from_options(\n-                self.params.get('allowed_extractors', ['default']), {\n-                    'all': list(all_ies),\n-                    'default': [name for name, ie in all_ies.items() if ie._ENABLED],\n-                }, use_regex=True)\n+                self.params.get(\"allowed_extractors\", [\"default\"]),\n+                {\n+                    \"all\": list(all_ies),\n+                    \"default\": [name for name, ie in all_ies.items() if ie._ENABLED],\n+                },\n+                use_regex=True,\n+            )\n         except re.error as e:\n-            raise ValueError(f'Wrong regex for allowed_extractors: {e.pattern}')\n+            raise ValueError(f\"Wrong regex for allowed_extractors: {e.pattern}\")\n         for name in ie_names:\n             self.add_info_extractor(all_ies[name])\n-        self.write_debug(f'Loaded {len(ie_names)} extractors')\n+        self.write_debug(f\"Loaded {len(ie_names)} extractors\")\n \n-    def add_post_processor(self, pp, when='post_process'):\n+    def add_post_processor(self, pp, when=\"post_process\"):\n         \"\"\"Add a PostProcessor object to the end of the chain.\"\"\"\n-        assert when in POSTPROCESS_WHEN, f'Invalid when={when}'\n+        assert when in POSTPROCESS_WHEN, f\"Invalid when={when}\"\n         self._pps[when].append(pp)\n         pp.set_downloader(self)\n \n@@ -868,87 +979,100 @@ def add_postprocessor_hook(self, ph):\n                 pp.add_progress_hook(ph)\n \n     def _bidi_workaround(self, message):\n-        if not hasattr(self, '_output_channel'):\n+        if not hasattr(self, \"_output_channel\"):\n             return message\n \n-        assert hasattr(self, '_output_process')\n+        assert hasattr(self, \"_output_process\")\n         assert isinstance(message, str)\n-        line_count = message.count('\\n') + 1\n-        self._output_process.stdin.write((message + '\\n').encode())\n+        line_count = message.count(\"\\n\") + 1\n+        self._output_process.stdin.write((message + \"\\n\").encode())\n         self._output_process.stdin.flush()\n-        res = ''.join(self._output_channel.readline().decode()\n-                      for _ in range(line_count))\n-        return res[:-len('\\n')]\n+        res = \"\".join(\n+            self._output_channel.readline().decode() for _ in range(line_count)\n+        )\n+        return res[: -len(\"\\n\")]\n \n     def _write_string(self, message, out=None, only_once=False):\n         if only_once:\n             if message in self._printed_messages:\n                 return\n             self._printed_messages.add(message)\n-        write_string(message, out=out, encoding=self.params.get('encoding'))\n+        write_string(message, out=out, encoding=self.params.get(\"encoding\"))\n \n     def to_stdout(self, message, skip_eol=False, quiet=None):\n         \"\"\"Print message to stdout\"\"\"\n         if quiet is not None:\n-            self.deprecation_warning('\"YoutubeDL.to_stdout\" no longer accepts the argument quiet. '\n-                                     'Use \"YoutubeDL.to_screen\" instead')\n+            self.deprecation_warning(\n+                '\"YoutubeDL.to_stdout\" no longer accepts the argument quiet. '\n+                'Use \"YoutubeDL.to_screen\" instead'\n+            )\n         if skip_eol is not False:\n-            self.deprecation_warning('\"YoutubeDL.to_stdout\" no longer accepts the argument skip_eol. '\n-                                     'Use \"YoutubeDL.to_screen\" instead')\n-        self._write_string(f'{self._bidi_workaround(message)}\\n', self._out_files.out)\n+            self.deprecation_warning(\n+                '\"YoutubeDL.to_stdout\" no longer accepts the argument skip_eol. '\n+                'Use \"YoutubeDL.to_screen\" instead'\n+            )\n+        self._write_string(f\"{self._bidi_workaround(message)}\\n\", self._out_files.out)\n \n     def to_screen(self, message, skip_eol=False, quiet=None, only_once=False):\n         \"\"\"Print message to screen if not in quiet mode\"\"\"\n-        if self.params.get('logger'):\n-            self.params['logger'].debug(message)\n+        if self.params.get(\"logger\"):\n+            self.params[\"logger\"].debug(message)\n             return\n-        if (self.params.get('quiet') if quiet is None else quiet) and not self.params.get('verbose'):\n+        if (\n+            self.params.get(\"quiet\") if quiet is None else quiet\n+        ) and not self.params.get(\"verbose\"):\n             return\n         self._write_string(\n-            '%s%s' % (self._bidi_workaround(message), ('' if skip_eol else '\\n')),\n-            self._out_files.screen, only_once=only_once)\n+            \"%s%s\" % (self._bidi_workaround(message), (\"\" if skip_eol else \"\\n\")),\n+            self._out_files.screen,\n+            only_once=only_once,\n+        )\n \n     def to_stderr(self, message, only_once=False):\n         \"\"\"Print message to stderr\"\"\"\n         assert isinstance(message, str)\n-        if self.params.get('logger'):\n-            self.params['logger'].error(message)\n+        if self.params.get(\"logger\"):\n+            self.params[\"logger\"].error(message)\n         else:\n-            self._write_string(f'{self._bidi_workaround(message)}\\n', self._out_files.error, only_once=only_once)\n+            self._write_string(\n+                f\"{self._bidi_workaround(message)}\\n\",\n+                self._out_files.error,\n+                only_once=only_once,\n+            )\n \n     def _send_console_code(self, code):\n-        if compat_os_name == 'nt' or not self._out_files.console:\n+        if compat_os_name == \"nt\" or not self._out_files.console:\n             return\n         self._write_string(code, self._out_files.console)\n \n     def to_console_title(self, message):\n-        if not self.params.get('consoletitle', False):\n+        if not self.params.get(\"consoletitle\", False):\n             return\n         message = remove_terminal_sequences(message)\n-        if compat_os_name == 'nt':\n+        if compat_os_name == \"nt\":\n             if ctypes.windll.kernel32.GetConsoleWindow():\n                 # c_wchar_p() might not be necessary if `message` is\n                 # already of type unicode()\n                 ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))\n         else:\n-            self._send_console_code(f'\\033]0;{message}\\007')\n+            self._send_console_code(f\"\\033]0;{message}\\007\")\n \n     def save_console_title(self):\n-        if not self.params.get('consoletitle') or self.params.get('simulate'):\n+        if not self.params.get(\"consoletitle\") or self.params.get(\"simulate\"):\n             return\n-        self._send_console_code('\\033[22;0t')  # Save the title on stack\n+        self._send_console_code(\"\\033[22;0t\")  # Save the title on stack\n \n     def restore_console_title(self):\n-        if not self.params.get('consoletitle') or self.params.get('simulate'):\n+        if not self.params.get(\"consoletitle\") or self.params.get(\"simulate\"):\n             return\n-        self._send_console_code('\\033[23;0t')  # Restore the title from stack\n+        self._send_console_code(\"\\033[23;0t\")  # Restore the title from stack\n \n     def __enter__(self):\n         self.save_console_title()\n         return self\n \n     def save_cookies(self):\n-        if self.params.get('cookiefile') is not None:\n+        if self.params.get(\"cookiefile\") is not None:\n             self.cookiejar.save()\n \n     def __exit__(self, *args):\n@@ -971,22 +1095,33 @@ def trouble(self, message=None, tb=None, is_error=True):\n         \"\"\"\n         if message is not None:\n             self.to_stderr(message)\n-        if self.params.get('verbose'):\n+        if self.params.get(\"verbose\"):\n             if tb is None:\n-                if sys.exc_info()[0]:  # if .trouble has been called from an except block\n-                    tb = ''\n-                    if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n-                        tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))\n+                if sys.exc_info()[\n+                    0\n+                ]:  # if .trouble has been called from an except block\n+                    tb = \"\"\n+                    if (\n+                        hasattr(sys.exc_info()[1], \"exc_info\")\n+                        and sys.exc_info()[1].exc_info[0]\n+                    ):\n+                        tb += \"\".join(\n+                            traceback.format_exception(*sys.exc_info()[1].exc_info)\n+                        )\n                     tb += encode_compat_str(traceback.format_exc())\n                 else:\n                     tb_data = traceback.format_list(traceback.extract_stack())\n-                    tb = ''.join(tb_data)\n+                    tb = \"\".join(tb_data)\n             if tb:\n                 self.to_stderr(tb)\n         if not is_error:\n             return\n-        if not self.params.get('ignoreerrors'):\n-            if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n+        if not self.params.get(\"ignoreerrors\"):\n+            if (\n+                sys.exc_info()[0]\n+                and hasattr(sys.exc_info()[1], \"exc_info\")\n+                and sys.exc_info()[1].exc_info[0]\n+            ):\n                 exc_info = sys.exc_info()[1].exc_info\n             else:\n                 exc_info = sys.exc_info()\n@@ -994,122 +1129,174 @@ def trouble(self, message=None, tb=None, is_error=True):\n         self._download_retcode = 1\n \n     Styles = Namespace(\n-        HEADERS='yellow',\n-        EMPHASIS='light blue',\n-        FILENAME='green',\n-        ID='green',\n-        DELIM='blue',\n-        ERROR='red',\n-        BAD_FORMAT='light red',\n-        WARNING='yellow',\n-        SUPPRESS='light black',\n+        HEADERS=\"yellow\",\n+        EMPHASIS=\"light blue\",\n+        FILENAME=\"green\",\n+        ID=\"green\",\n+        DELIM=\"blue\",\n+        ERROR=\"red\",\n+        BAD_FORMAT=\"light red\",\n+        WARNING=\"yellow\",\n+        SUPPRESS=\"light black\",\n     )\n \n-    def _format_text(self, handle, allow_colors, text, f, fallback=None, *, test_encoding=False):\n+    def _format_text(\n+        self, handle, allow_colors, text, f, fallback=None, *, test_encoding=False\n+    ):\n         text = str(text)\n         if test_encoding:\n             original_text = text\n             # handle.encoding can be None. See https://github.com/yt-dlp/yt-dlp/issues/2711\n-            encoding = self.params.get('encoding') or getattr(handle, 'encoding', None) or 'ascii'\n-            text = text.encode(encoding, 'ignore').decode(encoding)\n+            encoding = (\n+                self.params.get(\"encoding\")\n+                or getattr(handle, \"encoding\", None)\n+                or \"ascii\"\n+            )\n+            text = text.encode(encoding, \"ignore\").decode(encoding)\n             if fallback is not None and text != original_text:\n                 text = fallback\n-        return format_text(text, f) if allow_colors is True else text if fallback is None else fallback\n+        return (\n+            format_text(text, f)\n+            if allow_colors is True\n+            else text\n+            if fallback is None\n+            else fallback\n+        )\n \n     def _format_out(self, *args, **kwargs):\n-        return self._format_text(self._out_files.out, self._allow_colors.out, *args, **kwargs)\n+        return self._format_text(\n+            self._out_files.out, self._allow_colors.out, *args, **kwargs\n+        )\n \n     def _format_screen(self, *args, **kwargs):\n-        return self._format_text(self._out_files.screen, self._allow_colors.screen, *args, **kwargs)\n+        return self._format_text(\n+            self._out_files.screen, self._allow_colors.screen, *args, **kwargs\n+        )\n \n     def _format_err(self, *args, **kwargs):\n-        return self._format_text(self._out_files.error, self._allow_colors.error, *args, **kwargs)\n+        return self._format_text(\n+            self._out_files.error, self._allow_colors.error, *args, **kwargs\n+        )\n \n     def report_warning(self, message, only_once=False):\n-        '''\n+        \"\"\"\n         Print the message to stderr, it will be prefixed with 'WARNING:'\n         If stderr is a tty file the 'WARNING:' will be colored\n-        '''\n-        if self.params.get('logger') is not None:\n-            self.params['logger'].warning(message)\n+        \"\"\"\n+        if self.params.get(\"logger\") is not None:\n+            self.params[\"logger\"].warning(message)\n         else:\n-            if self.params.get('no_warnings'):\n+            if self.params.get(\"no_warnings\"):\n                 return\n-            self.to_stderr(f'{self._format_err(\"WARNING:\", self.Styles.WARNING)} {message}', only_once)\n+            self.to_stderr(\n+                f\"{self._format_err('WARNING:', self.Styles.WARNING)} {message}\",\n+                only_once,\n+            )\n \n     def deprecation_warning(self, message, *, stacklevel=0):\n         deprecation_warning(\n-            message, stacklevel=stacklevel + 1, printer=self.report_error, is_error=False)\n+            message,\n+            stacklevel=stacklevel + 1,\n+            printer=self.report_error,\n+            is_error=False,\n+        )\n \n     def deprecated_feature(self, message):\n-        if self.params.get('logger') is not None:\n-            self.params['logger'].warning(f'Deprecated Feature: {message}')\n-        self.to_stderr(f'{self._format_err(\"Deprecated Feature:\", self.Styles.ERROR)} {message}', True)\n+        if self.params.get(\"logger\") is not None:\n+            self.params[\"logger\"].warning(f\"Deprecated Feature: {message}\")\n+        self.to_stderr(\n+            f\"{self._format_err('Deprecated Feature:', self.Styles.ERROR)} {message}\",\n+            True,\n+        )\n \n     def report_error(self, message, *args, **kwargs):\n-        '''\n+        \"\"\"\n         Do the same as trouble, but prefixes the message with 'ERROR:', colored\n         in red if stderr is a tty file.\n-        '''\n-        self.trouble(f'{self._format_err(\"ERROR:\", self.Styles.ERROR)} {message}', *args, **kwargs)\n+        \"\"\"\n+        self.trouble(\n+            f\"{self._format_err('ERROR:', self.Styles.ERROR)} {message}\",\n+            *args,\n+            **kwargs,\n+        )\n \n     def write_debug(self, message, only_once=False):\n-        '''Log debug message or Print message to stderr'''\n-        if not self.params.get('verbose', False):\n+        \"\"\"Log debug message or Print message to stderr\"\"\"\n+        if not self.params.get(\"verbose\", False):\n             return\n-        message = f'[debug] {message}'\n-        if self.params.get('logger'):\n-            self.params['logger'].debug(message)\n+        message = f\"[debug] {message}\"\n+        if self.params.get(\"logger\"):\n+            self.params[\"logger\"].debug(message)\n         else:\n             self.to_stderr(message, only_once)\n \n     def report_file_already_downloaded(self, file_name):\n         \"\"\"Report file has already been fully downloaded.\"\"\"\n         try:\n-            self.to_screen('[download] %s has already been downloaded' % file_name)\n+            self.to_screen(\"[download] %s has already been downloaded\" % file_name)\n         except UnicodeEncodeError:\n-            self.to_screen('[download] The file has already been downloaded')\n+            self.to_screen(\"[download] The file has already been downloaded\")\n \n     def report_file_delete(self, file_name):\n         \"\"\"Report that existing file will be deleted.\"\"\"\n         try:\n-            self.to_screen('Deleting existing file %s' % file_name)\n+            self.to_screen(\"Deleting existing file %s\" % file_name)\n         except UnicodeEncodeError:\n-            self.to_screen('Deleting existing file')\n+            self.to_screen(\"Deleting existing file\")\n \n     def raise_no_formats(self, info, forced=False, *, msg=None):\n-        has_drm = info.get('_has_drm')\n-        ignored, expected = self.params.get('ignore_no_formats_error'), bool(msg)\n-        msg = msg or has_drm and 'This video is DRM protected' or 'No video formats found!'\n+        has_drm = info.get(\"_has_drm\")\n+        ignored, expected = self.params.get(\"ignore_no_formats_error\"), bool(msg)\n+        msg = (\n+            msg\n+            or has_drm\n+            and \"This video is DRM protected\"\n+            or \"No video formats found!\"\n+        )\n         if forced or not ignored:\n-            raise ExtractorError(msg, video_id=info['id'], ie=info['extractor'],\n-                                 expected=has_drm or ignored or expected)\n+            raise ExtractorError(\n+                msg,\n+                video_id=info[\"id\"],\n+                ie=info[\"extractor\"],\n+                expected=has_drm or ignored or expected,\n+            )\n         else:\n             self.report_warning(msg)\n \n     def parse_outtmpl(self):\n-        self.deprecation_warning('\"YoutubeDL.parse_outtmpl\" is deprecated and may be removed in a future version')\n+        self.deprecation_warning(\n+            '\"YoutubeDL.parse_outtmpl\" is deprecated and may be removed in a future version'\n+        )\n         self._parse_outtmpl()\n-        return self.params['outtmpl']\n+        return self.params[\"outtmpl\"]\n \n     def _parse_outtmpl(self):\n         sanitize = IDENTITY\n-        if self.params.get('restrictfilenames'):  # Remove spaces in the default template\n-            sanitize = lambda x: x.replace(' - ', ' ').replace(' ', '-')\n+        if self.params.get(\n+            \"restrictfilenames\"\n+        ):  # Remove spaces in the default template\n+            sanitize = lambda x: x.replace(\" - \", \" \").replace(\" \", \"-\")\n \n-        outtmpl = self.params.setdefault('outtmpl', {})\n+        outtmpl = self.params.setdefault(\"outtmpl\", {})\n         if not isinstance(outtmpl, dict):\n-            self.params['outtmpl'] = outtmpl = {'default': outtmpl}\n-        outtmpl.update({k: sanitize(v) for k, v in DEFAULT_OUTTMPL.items() if outtmpl.get(k) is None})\n+            self.params[\"outtmpl\"] = outtmpl = {\"default\": outtmpl}\n+        outtmpl.update(\n+            {\n+                k: sanitize(v)\n+                for k, v in DEFAULT_OUTTMPL.items()\n+                if outtmpl.get(k) is None\n+            }\n+        )\n \n-    def get_output_path(self, dir_type='', filename=None):\n-        paths = self.params.get('paths', {})\n+    def get_output_path(self, dir_type=\"\", filename=None):\n+        paths = self.params.get(\"paths\", {})\n         assert isinstance(paths, dict), '\"paths\" parameter must be a dictionary'\n         path = os.path.join(\n-            expand_path(paths.get('home', '').strip()),\n-            expand_path(paths.get(dir_type, '').strip()) if dir_type else '',\n-            filename or '')\n-        return sanitize_path(path, force=self.params.get('windowsfilenames'))\n+            expand_path(paths.get(\"home\", \"\").strip()),\n+            expand_path(paths.get(dir_type, \"\").strip()) if dir_type else \"\",\n+            filename or \"\",\n+        )\n+        return sanitize_path(path, force=self.params.get(\"windowsfilenames\"))\n \n     @staticmethod\n     def _outtmpl_expandpath(outtmpl):\n@@ -1117,30 +1304,32 @@ def _outtmpl_expandpath(outtmpl):\n         # correspondingly that is not what we want since we need to keep\n         # '%%' intact for template dict substitution step. Working around\n         # with boundary-alike separator hack.\n-        sep = ''.join(random.choices(string.ascii_letters, k=32))\n-        outtmpl = outtmpl.replace('%%', f'%{sep}%').replace('$$', f'${sep}$')\n+        sep = \"\".join(random.choices(string.ascii_letters, k=32))\n+        outtmpl = outtmpl.replace(\"%%\", f\"%{sep}%\").replace(\"$$\", f\"${sep}$\")\n \n         # outtmpl should be expand_path'ed before template dict substitution\n         # because meta fields may contain env variables we don't want to\n         # be expanded. E.g. for outtmpl \"%(title)s.%(ext)s\" and\n         # title \"Hello $PATH\", we don't want `$PATH` to be expanded.\n-        return expand_path(outtmpl).replace(sep, '')\n+        return expand_path(outtmpl).replace(sep, \"\")\n \n     @staticmethod\n     def escape_outtmpl(outtmpl):\n-        ''' Escape any remaining strings like %s, %abc% etc. '''\n+        \"\"\"Escape any remaining strings like %s, %abc% etc.\"\"\"\n         return re.sub(\n-            STR_FORMAT_RE_TMPL.format('', '(?![%(\\0])'),\n-            lambda mobj: ('' if mobj.group('has_key') else '%') + mobj.group(0),\n-            outtmpl)\n+            STR_FORMAT_RE_TMPL.format(\"\", \"(?![%(\\0])\"),\n+            lambda mobj: (\"\" if mobj.group(\"has_key\") else \"%\") + mobj.group(0),\n+            outtmpl,\n+        )\n \n     @classmethod\n     def validate_outtmpl(cls, outtmpl):\n-        ''' @return None or Exception object '''\n+        \"\"\"@return None or Exception object\"\"\"\n         outtmpl = re.sub(\n-            STR_FORMAT_RE_TMPL.format('[^)]*', '[ljhqBUDS]'),\n-            lambda mobj: f'{mobj.group(0)[:-1]}s',\n-            cls._outtmpl_expandpath(outtmpl))\n+            STR_FORMAT_RE_TMPL.format(\"[^)]*\", \"[ljhqBUDS]\"),\n+            lambda mobj: f\"{mobj.group(0)[:-1]}s\",\n+            cls._outtmpl_expandpath(outtmpl),\n+        )\n         try:\n             cls.escape_outtmpl(outtmpl) % collections.defaultdict(int)\n             return None\n@@ -1150,53 +1339,66 @@ def validate_outtmpl(cls, outtmpl):\n     @staticmethod\n     def _copy_infodict(info_dict):\n         info_dict = dict(info_dict)\n-        info_dict.pop('__postprocessors', None)\n-        info_dict.pop('__pending_error', None)\n+        info_dict.pop(\"__postprocessors\", None)\n+        info_dict.pop(\"__pending_error\", None)\n         return info_dict\n \n     def prepare_outtmpl(self, outtmpl, info_dict, sanitize=False):\n-        \"\"\" Make the outtmpl and info_dict suitable for substitution: ydl.escape_outtmpl(outtmpl) % info_dict\n+        \"\"\"Make the outtmpl and info_dict suitable for substitution: ydl.escape_outtmpl(outtmpl) % info_dict\n         @param sanitize    Whether to sanitize the output as a filename.\n                            For backward compatibility, a function can also be passed\n         \"\"\"\n \n-        info_dict.setdefault('epoch', int(time.time()))  # keep epoch consistent once set\n+        info_dict.setdefault(\n+            \"epoch\", int(time.time())\n+        )  # keep epoch consistent once set\n \n         info_dict = self._copy_infodict(info_dict)\n-        info_dict['duration_string'] = (  # %(duration>%H-%M-%S)s is wrong if duration > 24hrs\n-            formatSeconds(info_dict['duration'], '-' if sanitize else ':')\n-            if info_dict.get('duration', None) is not None\n-            else None)\n-        info_dict['autonumber'] = int(self.params.get('autonumber_start', 1) - 1 + self._num_downloads)\n-        info_dict['video_autonumber'] = self._num_videos\n-        if info_dict.get('resolution') is None:\n-            info_dict['resolution'] = self.format_resolution(info_dict, default=None)\n+        info_dict[\n+            \"duration_string\"\n+        ] = (  # %(duration>%H-%M-%S)s is wrong if duration > 24hrs\n+            formatSeconds(info_dict[\"duration\"], \"-\" if sanitize else \":\")\n+            if info_dict.get(\"duration\", None) is not None\n+            else None\n+        )\n+        info_dict[\"autonumber\"] = int(\n+            self.params.get(\"autonumber_start\", 1) - 1 + self._num_downloads\n+        )\n+        info_dict[\"video_autonumber\"] = self._num_videos\n+        if info_dict.get(\"resolution\") is None:\n+            info_dict[\"resolution\"] = self.format_resolution(info_dict, default=None)\n \n         # For fields playlist_index, playlist_autonumber and autonumber convert all occurrences\n         # of %(field)s to %(field)0Nd for backward compatibility\n         field_size_compat_map = {\n-            'playlist_index': number_of_digits(info_dict.get('__last_playlist_index') or 0),\n-            'playlist_autonumber': number_of_digits(info_dict.get('n_entries') or 0),\n-            'autonumber': self.params.get('autonumber_size') or 5,\n+            \"playlist_index\": number_of_digits(\n+                info_dict.get(\"__last_playlist_index\") or 0\n+            ),\n+            \"playlist_autonumber\": number_of_digits(info_dict.get(\"n_entries\") or 0),\n+            \"autonumber\": self.params.get(\"autonumber_size\") or 5,\n         }\n \n         TMPL_DICT = {}\n-        EXTERNAL_FORMAT_RE = re.compile(STR_FORMAT_RE_TMPL.format('[^)]*', f'[{STR_FORMAT_TYPES}ljhqBUDS]'))\n+        EXTERNAL_FORMAT_RE = re.compile(\n+            STR_FORMAT_RE_TMPL.format(\"[^)]*\", f\"[{STR_FORMAT_TYPES}ljhqBUDS]\")\n+        )\n         MATH_FUNCTIONS = {\n-            '+': float.__add__,\n-            '-': float.__sub__,\n-            '*': float.__mul__,\n+            \"+\": float.__add__,\n+            \"-\": float.__sub__,\n+            \"*\": float.__mul__,\n         }\n         # Field is of the form key1.key2...\n         # where keys (except first) can be string, int, slice or \"{field, ...}\"\n-        FIELD_INNER_RE = r'(?:\\w+|%(num)s|%(num)s?(?::%(num)s?){1,2})' % {'num': r'(?:-?\\d+)'}\n-        FIELD_RE = r'\\w*(?:\\.(?:%(inner)s|{%(field)s(?:,%(field)s)*}))*' % {\n-            'inner': FIELD_INNER_RE,\n-            'field': rf'\\w*(?:\\.{FIELD_INNER_RE})*'\n+        FIELD_INNER_RE = r\"(?:\\w+|%(num)s|%(num)s?(?::%(num)s?){1,2})\" % {\n+            \"num\": r\"(?:-?\\d+)\"\n+        }\n+        FIELD_RE = r\"\\w*(?:\\.(?:%(inner)s|{%(field)s(?:,%(field)s)*}))*\" % {\n+            \"inner\": FIELD_INNER_RE,\n+            \"field\": rf\"\\w*(?:\\.{FIELD_INNER_RE})*\",\n         }\n-        MATH_FIELD_RE = rf'(?:{FIELD_RE}|-?{NUMBER_RE})'\n-        MATH_OPERATORS_RE = r'(?:%s)' % '|'.join(map(re.escape, MATH_FUNCTIONS.keys()))\n-        INTERNAL_FORMAT_RE = re.compile(rf'''(?xs)\n+        MATH_FIELD_RE = rf\"(?:{FIELD_RE}|-?{NUMBER_RE})\"\n+        MATH_OPERATORS_RE = r\"(?:%s)\" % \"|\".join(map(re.escape, MATH_FUNCTIONS.keys()))\n+        INTERNAL_FORMAT_RE = re.compile(rf\"\"\"(?xs)\n             (?P<negate>-)?\n             (?P<fields>{FIELD_RE})\n             (?P<maths>(?:{MATH_OPERATORS_RE}{MATH_FIELD_RE})*)\n@@ -1205,55 +1407,61 @@ def prepare_outtmpl(self, outtmpl, info_dict, sanitize=False):\n                 (?P<alternate>(?<!\\\\),[^|&)]+)?\n                 (?:&(?P<replacement>.*?))?\n                 (?:\\|(?P<default>.*?))?\n-            )$''')\n+            )$\"\"\")\n \n         def _from_user_input(field):\n-            if field == ':':\n+            if field == \":\":\n                 return ...\n-            elif ':' in field:\n-                return slice(*map(int_or_none, field.split(':')))\n+            elif \":\" in field:\n+                return slice(*map(int_or_none, field.split(\":\")))\n             elif int_or_none(field) is not None:\n                 return int(field)\n             return field\n \n         def _traverse_infodict(fields):\n-            fields = [f for x in re.split(r'\\.({.+?})\\.?', fields)\n-                      for f in ([x] if x.startswith('{') else x.split('.'))]\n+            fields = [\n+                f\n+                for x in re.split(r\"\\.({.+?})\\.?\", fields)\n+                for f in ([x] if x.startswith(\"{\") else x.split(\".\"))\n+            ]\n             for i in (0, -1):\n                 if fields and not fields[i]:\n                     fields.pop(i)\n \n             for i, f in enumerate(fields):\n-                if not f.startswith('{'):\n+                if not f.startswith(\"{\"):\n                     fields[i] = _from_user_input(f)\n                     continue\n-                assert f.endswith('}'), f'No closing brace for {f} in {fields}'\n-                fields[i] = {k: list(map(_from_user_input, k.split('.'))) for k in f[1:-1].split(',')}\n+                assert f.endswith(\"}\"), f\"No closing brace for {f} in {fields}\"\n+                fields[i] = {\n+                    k: list(map(_from_user_input, k.split(\".\")))\n+                    for k in f[1:-1].split(\",\")\n+                }\n \n             return traverse_obj(info_dict, fields, traverse_string=True)\n \n         def get_value(mdict):\n             # Object traversal\n-            value = _traverse_infodict(mdict['fields'])\n+            value = _traverse_infodict(mdict[\"fields\"])\n             # Negative\n-            if mdict['negate']:\n+            if mdict[\"negate\"]:\n                 value = float_or_none(value)\n                 if value is not None:\n                     value *= -1\n             # Do maths\n-            offset_key = mdict['maths']\n+            offset_key = mdict[\"maths\"]\n             if offset_key:\n                 value = float_or_none(value)\n                 operator = None\n                 while offset_key:\n                     item = re.match(\n-                        MATH_FIELD_RE if operator else MATH_OPERATORS_RE,\n-                        offset_key).group(0)\n-                    offset_key = offset_key[len(item):]\n+                        MATH_FIELD_RE if operator else MATH_OPERATORS_RE, offset_key\n+                    ).group(0)\n+                    offset_key = offset_key[len(item) :]\n                     if operator is None:\n                         operator = MATH_FUNCTIONS[item]\n                         continue\n-                    item, multiplier = (item[1:], -1) if item[0] == '-' else (item, 1)\n+                    item, multiplier = (item[1:], -1) if item[0] == \"-\" else (item, 1)\n                     offset = float_or_none(item)\n                     if offset is None:\n                         offset = float_or_none(_traverse_infodict(item))\n@@ -1263,21 +1471,30 @@ def get_value(mdict):\n                         return None\n                     operator = None\n             # Datetime formatting\n-            if mdict['strf_format']:\n-                value = strftime_or_none(value, mdict['strf_format'].replace('\\\\,', ','))\n+            if mdict[\"strf_format\"]:\n+                value = strftime_or_none(\n+                    value, mdict[\"strf_format\"].replace(\"\\\\,\", \",\")\n+                )\n \n             # XXX: Workaround for https://github.com/yt-dlp/yt-dlp/issues/4485\n-            if sanitize and value == '':\n+            if sanitize and value == \"\":\n                 value = None\n             return value\n \n-        na = self.params.get('outtmpl_na_placeholder', 'NA')\n+        na = self.params.get(\"outtmpl_na_placeholder\", \"NA\")\n \n-        def filename_sanitizer(key, value, restricted=self.params.get('restrictfilenames')):\n-            return sanitize_filename(str(value), restricted=restricted, is_id=(\n-                bool(re.search(r'(^|[_.])id(\\.|$)', key))\n-                if 'filename-sanitization' in self.params['compat_opts']\n-                else NO_DEFAULT))\n+        def filename_sanitizer(\n+            key, value, restricted=self.params.get(\"restrictfilenames\")\n+        ):\n+            return sanitize_filename(\n+                str(value),\n+                restricted=restricted,\n+                is_id=(\n+                    bool(re.search(r\"(^|[_.])id(\\.|$)\", key))\n+                    if \"filename-sanitization\" in self.params[\"compat_opts\"]\n+                    else NO_DEFAULT\n+                ),\n+            )\n \n         sanitizer = sanitize if callable(sanitize) else filename_sanitizer\n         sanitize = bool(sanitize)\n@@ -1291,23 +1508,23 @@ class _ReplacementFormatter(string.Formatter):\n             def get_field(self, field_name, args, kwargs):\n                 if field_name.isdigit():\n                     return args[0], -1\n-                raise ValueError('Unsupported field')\n+                raise ValueError(\"Unsupported field\")\n \n         replacement_formatter = _ReplacementFormatter()\n \n         def create_key(outer_mobj):\n-            if not outer_mobj.group('has_key'):\n+            if not outer_mobj.group(\"has_key\"):\n                 return outer_mobj.group(0)\n-            key = outer_mobj.group('key')\n+            key = outer_mobj.group(\"key\")\n             mobj = re.match(INTERNAL_FORMAT_RE, key)\n-            value, replacement, default, last_field = None, None, na, ''\n+            value, replacement, default, last_field = None, None, na, \"\"\n             while mobj:\n                 mobj = mobj.groupdict()\n-                default = mobj['default'] if mobj['default'] is not None else default\n+                default = mobj[\"default\"] if mobj[\"default\"] is not None else default\n                 value = get_value(mobj)\n-                last_field, replacement = mobj['fields'], mobj['replacement']\n-                if value is None and mobj['alternate']:\n-                    mobj = re.match(INTERNAL_FORMAT_RE, mobj['remaining'][1:])\n+                last_field, replacement = mobj[\"fields\"], mobj[\"replacement\"]\n+                if value is None and mobj[\"alternate\"]:\n+                    mobj = re.match(INTERNAL_FORMAT_RE, mobj[\"remaining\"][1:])\n                 else:\n                     break\n \n@@ -1317,63 +1534,89 @@ def create_key(outer_mobj):\n                 except ValueError:\n                     value, default = None, na\n \n-            fmt = outer_mobj.group('format')\n-            if fmt == 's' and last_field in field_size_compat_map.keys() and isinstance(value, int):\n-                fmt = f'0{field_size_compat_map[last_field]:d}d'\n+            fmt = outer_mobj.group(\"format\")\n+            if (\n+                fmt == \"s\"\n+                and last_field in field_size_compat_map.keys()\n+                and isinstance(value, int)\n+            ):\n+                fmt = f\"0{field_size_compat_map[last_field]:d}d\"\n \n-            flags = outer_mobj.group('conversion') or ''\n-            str_fmt = f'{fmt[:-1]}s'\n+            flags = outer_mobj.group(\"conversion\") or \"\"\n+            str_fmt = f\"{fmt[:-1]}s\"\n             if value is None:\n-                value, fmt = default, 's'\n-            elif fmt[-1] == 'l':  # list\n-                delim = '\\n' if '#' in flags else ', '\n-                value, fmt = delim.join(map(str, variadic(value, allowed_types=(str, bytes)))), str_fmt\n-            elif fmt[-1] == 'j':  # json\n-                value, fmt = json.dumps(\n-                    value, default=_dumpjson_default,\n-                    indent=4 if '#' in flags else None, ensure_ascii='+' not in flags), str_fmt\n-            elif fmt[-1] == 'h':  # html\n+                value, fmt = default, \"s\"\n+            elif fmt[-1] == \"l\":  # list\n+                delim = \"\\n\" if \"#\" in flags else \", \"\n+                value, fmt = (\n+                    delim.join(map(str, variadic(value, allowed_types=(str, bytes)))),\n+                    str_fmt,\n+                )\n+            elif fmt[-1] == \"j\":  # json\n+                value, fmt = (\n+                    json.dumps(\n+                        value,\n+                        default=_dumpjson_default,\n+                        indent=4 if \"#\" in flags else None,\n+                        ensure_ascii=\"+\" not in flags,\n+                    ),\n+                    str_fmt,\n+                )\n+            elif fmt[-1] == \"h\":  # html\n                 value, fmt = escapeHTML(str(value)), str_fmt\n-            elif fmt[-1] == 'q':  # quoted\n-                value = map(str, variadic(value) if '#' in flags else [value])\n-                value, fmt = ' '.join(map(compat_shlex_quote, value)), str_fmt\n-            elif fmt[-1] == 'B':  # bytes\n-                value = f'%{str_fmt}'.encode() % str(value).encode()\n-                value, fmt = value.decode('utf-8', 'ignore'), 's'\n-            elif fmt[-1] == 'U':  # unicode normalized\n-                value, fmt = unicodedata.normalize(\n-                    # \"+\" = compatibility equivalence, \"#\" = NFD\n-                    'NF%s%s' % ('K' if '+' in flags else '', 'D' if '#' in flags else 'C'),\n-                    value), str_fmt\n-            elif fmt[-1] == 'D':  # decimal suffix\n-                num_fmt, fmt = fmt[:-1].replace('#', ''), 's'\n-                value = format_decimal_suffix(value, f'%{num_fmt}f%s' if num_fmt else '%d%s',\n-                                              factor=1024 if '#' in flags else 1000)\n-            elif fmt[-1] == 'S':  # filename sanitization\n-                value, fmt = filename_sanitizer(last_field, value, restricted='#' in flags), str_fmt\n-            elif fmt[-1] == 'c':\n+            elif fmt[-1] == \"q\":  # quoted\n+                value = map(str, variadic(value) if \"#\" in flags else [value])\n+                value, fmt = \" \".join(map(compat_shlex_quote, value)), str_fmt\n+            elif fmt[-1] == \"B\":  # bytes\n+                value = f\"%{str_fmt}\".encode() % str(value).encode()\n+                value, fmt = value.decode(\"utf-8\", \"ignore\"), \"s\"\n+            elif fmt[-1] == \"U\":  # unicode normalized\n+                value, fmt = (\n+                    unicodedata.normalize(\n+                        # \"+\" = compatibility equivalence, \"#\" = NFD\n+                        \"NF%s%s\"\n+                        % (\"K\" if \"+\" in flags else \"\", \"D\" if \"#\" in flags else \"C\"),\n+                        value,\n+                    ),\n+                    str_fmt,\n+                )\n+            elif fmt[-1] == \"D\":  # decimal suffix\n+                num_fmt, fmt = fmt[:-1].replace(\"#\", \"\"), \"s\"\n+                value = format_decimal_suffix(\n+                    value,\n+                    f\"%{num_fmt}f%s\" if num_fmt else \"%d%s\",\n+                    factor=1024 if \"#\" in flags else 1000,\n+                )\n+            elif fmt[-1] == \"S\":  # filename sanitization\n+                value, fmt = (\n+                    filename_sanitizer(last_field, value, restricted=\"#\" in flags),\n+                    str_fmt,\n+                )\n+            elif fmt[-1] == \"c\":\n                 if value:\n                     value = str(value)[0]\n                 else:\n                     fmt = str_fmt\n-            elif fmt[-1] not in 'rsa':  # numeric\n+            elif fmt[-1] not in \"rsa\":  # numeric\n                 value = float_or_none(value)\n                 if value is None:\n-                    value, fmt = default, 's'\n+                    value, fmt = default, \"s\"\n \n             if sanitize:\n                 # If value is an object, sanitize might convert it to a string\n                 # So we convert it to repr first\n-                if fmt[-1] == 'r':\n+                if fmt[-1] == \"r\":\n                     value, fmt = repr(value), str_fmt\n-                elif fmt[-1] == 'a':\n+                elif fmt[-1] == \"a\":\n                     value, fmt = ascii(value), str_fmt\n-                if fmt[-1] in 'csra':\n+                if fmt[-1] in \"csra\":\n                     value = sanitizer(last_field, value)\n \n-            key = '%s\\0%s' % (key.replace('%', '%\\0'), outer_mobj.group('format'))\n+            key = \"%s\\0%s\" % (key.replace(\"%\", \"%\\0\"), outer_mobj.group(\"format\"))\n             TMPL_DICT[key] = value\n-            return '{prefix}%({key}){fmt}'.format(key=key, fmt=fmt, prefix=outer_mobj.group('prefix'))\n+            return \"{prefix}%({key}){fmt}\".format(\n+                key=key, fmt=fmt, prefix=outer_mobj.group(\"prefix\")\n+            )\n \n         return EXTERNAL_FORMAT_RE.sub(create_key, outtmpl), TMPL_DICT\n \n@@ -1382,99 +1625,150 @@ def evaluate_outtmpl(self, outtmpl, info_dict, *args, **kwargs):\n         return self.escape_outtmpl(outtmpl) % info_dict\n \n     def _prepare_filename(self, info_dict, *, outtmpl=None, tmpl_type=None):\n-        assert None in (outtmpl, tmpl_type), 'outtmpl and tmpl_type are mutually exclusive'\n+        assert None in (outtmpl, tmpl_type), (\n+            \"outtmpl and tmpl_type are mutually exclusive\"\n+        )\n         if outtmpl is None:\n-            outtmpl = self.params['outtmpl'].get(tmpl_type or 'default', self.params['outtmpl']['default'])\n+            outtmpl = self.params[\"outtmpl\"].get(\n+                tmpl_type or \"default\", self.params[\"outtmpl\"][\"default\"]\n+            )\n         try:\n             outtmpl = self._outtmpl_expandpath(outtmpl)\n             filename = self.evaluate_outtmpl(outtmpl, info_dict, True)\n             if not filename:\n                 return None\n \n-            if tmpl_type in ('', 'temp'):\n-                final_ext, ext = self.params.get('final_ext'), info_dict.get('ext')\n-                if final_ext and ext and final_ext != ext and filename.endswith(f'.{final_ext}'):\n+            if tmpl_type in (\"\", \"temp\"):\n+                final_ext, ext = self.params.get(\"final_ext\"), info_dict.get(\"ext\")\n+                if (\n+                    final_ext\n+                    and ext\n+                    and final_ext != ext\n+                    and filename.endswith(f\".{final_ext}\")\n+                ):\n                     filename = replace_extension(filename, ext, final_ext)\n             elif tmpl_type:\n                 force_ext = OUTTMPL_TYPES[tmpl_type]\n                 if force_ext:\n-                    filename = replace_extension(filename, force_ext, info_dict.get('ext'))\n+                    filename = replace_extension(\n+                        filename, force_ext, info_dict.get(\"ext\")\n+                    )\n \n             # https://github.com/blackjack4494/youtube-dlc/issues/85\n-            trim_file_name = self.params.get('trim_file_name', False)\n+            trim_file_name = self.params.get(\"trim_file_name\", False)\n             if trim_file_name:\n-                no_ext, *ext = filename.rsplit('.', 2)\n-                filename = join_nonempty(no_ext[:trim_file_name], *ext, delim='.')\n+                no_ext, *ext = filename.rsplit(\".\", 2)\n+                filename = join_nonempty(no_ext[:trim_file_name], *ext, delim=\".\")\n \n             return filename\n         except ValueError as err:\n-            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')\n+            self.report_error(\n+                \"Error in output template: \"\n+                + str(err)\n+                + \" (encoding: \"\n+                + repr(preferredencoding())\n+                + \")\"\n+            )\n             return None\n \n-    def prepare_filename(self, info_dict, dir_type='', *, outtmpl=None, warn=False):\n+    def prepare_filename(self, info_dict, dir_type=\"\", *, outtmpl=None, warn=False):\n         \"\"\"Generate the output filename\"\"\"\n         if outtmpl:\n-            assert not dir_type, 'outtmpl and dir_type are mutually exclusive'\n+            assert not dir_type, \"outtmpl and dir_type are mutually exclusive\"\n             dir_type = None\n-        filename = self._prepare_filename(info_dict, tmpl_type=dir_type, outtmpl=outtmpl)\n-        if not filename and dir_type not in ('', 'temp'):\n-            return ''\n+        filename = self._prepare_filename(\n+            info_dict, tmpl_type=dir_type, outtmpl=outtmpl\n+        )\n+        if not filename and dir_type not in (\"\", \"temp\"):\n+            return \"\"\n \n         if warn:\n-            if not self.params.get('paths'):\n+            if not self.params.get(\"paths\"):\n                 pass\n-            elif filename == '-':\n-                self.report_warning('--paths is ignored when an outputting to stdout', only_once=True)\n+            elif filename == \"-\":\n+                self.report_warning(\n+                    \"--paths is ignored when an outputting to stdout\", only_once=True\n+                )\n             elif os.path.isabs(filename):\n-                self.report_warning('--paths is ignored since an absolute path is given in output template', only_once=True)\n-        if filename == '-' or not filename:\n+                self.report_warning(\n+                    \"--paths is ignored since an absolute path is given in output template\",\n+                    only_once=True,\n+                )\n+        if filename == \"-\" or not filename:\n             return filename\n \n         return self.get_output_path(dir_type, filename)\n \n     def _match_entry(self, info_dict, incomplete=False, silent=False):\n         \"\"\"Returns None if the file should be downloaded\"\"\"\n-        _type = 'video' if 'playlist-match-filter' in self.params['compat_opts'] else info_dict.get('_type', 'video')\n-        assert incomplete or _type == 'video', 'Only video result can be considered complete'\n+        _type = (\n+            \"video\"\n+            if \"playlist-match-filter\" in self.params[\"compat_opts\"]\n+            else info_dict.get(\"_type\", \"video\")\n+        )\n+        assert incomplete or _type == \"video\", (\n+            \"Only video result can be considered complete\"\n+        )\n \n-        video_title = info_dict.get('title', info_dict.get('id', 'entry'))\n+        video_title = info_dict.get(\"title\", info_dict.get(\"id\", \"entry\"))\n \n         def check_filter():\n-            if _type in ('playlist', 'multi_video'):\n+            if _type in (\"playlist\", \"multi_video\"):\n                 return\n-            elif _type in ('url', 'url_transparent') and not try_call(\n-                    lambda: self.get_info_extractor(info_dict['ie_key']).is_single_video(info_dict['url'])):\n+            elif _type in (\"url\", \"url_transparent\") and not try_call(\n+                lambda: self.get_info_extractor(info_dict[\"ie_key\"]).is_single_video(\n+                    info_dict[\"url\"]\n+                )\n+            ):\n                 return\n \n-            if 'title' in info_dict:\n+            if \"title\" in info_dict:\n                 # This can happen when we're just evaluating the playlist\n-                title = info_dict['title']\n-                matchtitle = self.params.get('matchtitle', False)\n+                title = info_dict[\"title\"]\n+                matchtitle = self.params.get(\"matchtitle\", False)\n                 if matchtitle:\n                     if not re.search(matchtitle, title, re.IGNORECASE):\n-                        return '\"' + title + '\" title did not match pattern \"' + matchtitle + '\"'\n-                rejecttitle = self.params.get('rejecttitle', False)\n+                        return (\n+                            '\"'\n+                            + title\n+                            + '\" title did not match pattern \"'\n+                            + matchtitle\n+                            + '\"'\n+                        )\n+                rejecttitle = self.params.get(\"rejecttitle\", False)\n                 if rejecttitle:\n                     if re.search(rejecttitle, title, re.IGNORECASE):\n-                        return '\"' + title + '\" title matched reject pattern \"' + rejecttitle + '\"'\n-\n-            date = info_dict.get('upload_date')\n+                        return (\n+                            '\"'\n+                            + title\n+                            + '\" title matched reject pattern \"'\n+                            + rejecttitle\n+                            + '\"'\n+                        )\n+\n+            date = info_dict.get(\"upload_date\")\n             if date is not None:\n-                dateRange = self.params.get('daterange', DateRange())\n+                dateRange = self.params.get(\"daterange\", DateRange())\n                 if date not in dateRange:\n-                    return f'{date_from_str(date).isoformat()} upload date is not in range {dateRange}'\n-            view_count = info_dict.get('view_count')\n+                    return f\"{date_from_str(date).isoformat()} upload date is not in range {dateRange}\"\n+            view_count = info_dict.get(\"view_count\")\n             if view_count is not None:\n-                min_views = self.params.get('min_views')\n+                min_views = self.params.get(\"min_views\")\n                 if min_views is not None and view_count < min_views:\n-                    return 'Skipping %s, because it has not reached minimum view count (%d/%d)' % (video_title, view_count, min_views)\n-                max_views = self.params.get('max_views')\n+                    return (\n+                        \"Skipping %s, because it has not reached minimum view count (%d/%d)\"\n+                        % (video_title, view_count, min_views)\n+                    )\n+                max_views = self.params.get(\"max_views\")\n                 if max_views is not None and view_count > max_views:\n-                    return 'Skipping %s, because it has exceeded the maximum view count (%d/%d)' % (video_title, view_count, max_views)\n-            if age_restricted(info_dict.get('age_limit'), self.params.get('age_limit')):\n+                    return (\n+                        \"Skipping %s, because it has exceeded the maximum view count (%d/%d)\"\n+                        % (video_title, view_count, max_views)\n+                    )\n+            if age_restricted(info_dict.get(\"age_limit\"), self.params.get(\"age_limit\")):\n                 return 'Skipping \"%s\" because it is age restricted' % video_title\n \n-            match_filter = self.params.get('match_filter')\n+            match_filter = self.params.get(\"match_filter\")\n             if match_filter is None:\n                 return None\n \n@@ -1492,45 +1786,72 @@ def check_filter():\n \n             if ret is NO_DEFAULT:\n                 while True:\n-                    filename = self._format_screen(self.prepare_filename(info_dict), self.Styles.FILENAME)\n-                    reply = input(self._format_screen(\n-                        f'Download \"{filename}\"? (Y/n): ', self.Styles.EMPHASIS)).lower().strip()\n-                    if reply in {'y', ''}:\n+                    filename = self._format_screen(\n+                        self.prepare_filename(info_dict), self.Styles.FILENAME\n+                    )\n+                    reply = (\n+                        input(\n+                            self._format_screen(\n+                                f'Download \"{filename}\"? (Y/n): ', self.Styles.EMPHASIS\n+                            )\n+                        )\n+                        .lower()\n+                        .strip()\n+                    )\n+                    if reply in {\"y\", \"\"}:\n                         return None\n-                    elif reply == 'n':\n+                    elif reply == \"n\":\n                         if cancelled:\n-                            raise type(cancelled)(f'Skipping {video_title}')\n-                        return f'Skipping {video_title}'\n+                            raise type(cancelled)(f\"Skipping {video_title}\")\n+                        return f\"Skipping {video_title}\"\n             return ret\n \n         if self.in_download_archive(info_dict):\n-            reason = ''.join((\n-                format_field(info_dict, 'id', f'{self._format_screen(\"%s\", self.Styles.ID)}: '),\n-                format_field(info_dict, 'title', f'{self._format_screen(\"%s\", self.Styles.EMPHASIS)} '),\n-                'has already been recorded in the archive'))\n-            break_opt, break_err = 'break_on_existing', ExistingVideoReached\n+            reason = \"\".join(\n+                (\n+                    format_field(\n+                        info_dict,\n+                        \"id\",\n+                        f\"{self._format_screen('%s', self.Styles.ID)}: \",\n+                    ),\n+                    format_field(\n+                        info_dict,\n+                        \"title\",\n+                        f\"{self._format_screen('%s', self.Styles.EMPHASIS)} \",\n+                    ),\n+                    \"has already been recorded in the archive\",\n+                )\n+            )\n+            break_opt, break_err = \"break_on_existing\", ExistingVideoReached\n         else:\n             try:\n                 reason = check_filter()\n             except DownloadCancelled as e:\n-                reason, break_opt, break_err = e.msg, 'match_filter', type(e)\n+                reason, break_opt, break_err = e.msg, \"match_filter\", type(e)\n             else:\n-                break_opt, break_err = 'break_on_reject', RejectedVideoReached\n+                break_opt, break_err = \"break_on_reject\", RejectedVideoReached\n         if reason is not None:\n             if not silent:\n-                self.to_screen('[download] ' + reason)\n+                self.to_screen(\"[download] \" + reason)\n             if self.params.get(break_opt, False):\n                 raise break_err()\n         return reason\n \n     @staticmethod\n     def add_extra_info(info_dict, extra_info):\n-        '''Set the keys from extra_info in info dict if they are missing'''\n+        \"\"\"Set the keys from extra_info in info dict if they are missing\"\"\"\n         for key, value in extra_info.items():\n             info_dict.setdefault(key, value)\n \n-    def extract_info(self, url, download=True, ie_key=None, extra_info=None,\n-                     process=True, force_generic_extractor=False):\n+    def extract_info(\n+        self,\n+        url,\n+        download=True,\n+        ie_key=None,\n+        extra_info=None,\n+        process=True,\n+        force_generic_extractor=False,\n+    ):\n         \"\"\"\n         Extract and return the information dictionary of the URL\n \n@@ -1551,7 +1872,7 @@ def extract_info(self, url, download=True, ie_key=None, extra_info=None,\n             extra_info = {}\n \n         if not ie_key and force_generic_extractor:\n-            ie_key = 'Generic'\n+            ie_key = \"Generic\"\n \n         if ie_key:\n             ies = {ie_key: self._ies[ie_key]} if ie_key in self._ies else {}\n@@ -1563,21 +1884,34 @@ def extract_info(self, url, download=True, ie_key=None, extra_info=None,\n                 continue\n \n             if not ie.working():\n-                self.report_warning('The program functionality for this site has been marked as broken, '\n-                                    'and will probably not work.')\n+                self.report_warning(\n+                    \"The program functionality for this site has been marked as broken, \"\n+                    \"and will probably not work.\"\n+                )\n \n             temp_id = ie.get_temp_id(url)\n-            if temp_id is not None and self.in_download_archive({'id': temp_id, 'ie_key': key}):\n-                self.to_screen(f'[download] {self._format_screen(temp_id, self.Styles.ID)}: '\n-                               'has already been recorded in the archive')\n-                if self.params.get('break_on_existing', False):\n+            if temp_id is not None and self.in_download_archive(\n+                {\"id\": temp_id, \"ie_key\": key}\n+            ):\n+                self.to_screen(\n+                    f\"[download] {self._format_screen(temp_id, self.Styles.ID)}: \"\n+                    \"has already been recorded in the archive\"\n+                )\n+                if self.params.get(\"break_on_existing\", False):\n                     raise ExistingVideoReached()\n                 break\n-            return self.__extract_info(url, self.get_info_extractor(key), download, extra_info, process)\n+            return self.__extract_info(\n+                url, self.get_info_extractor(key), download, extra_info, process\n+            )\n         else:\n-            extractors_restricted = self.params.get('allowed_extractors') not in (None, ['default'])\n-            self.report_error(f'No suitable extractor{format_field(ie_key, None, \" (%s)\")} found for URL {url}',\n-                              tb=False if extractors_restricted else None)\n+            extractors_restricted = self.params.get(\"allowed_extractors\") not in (\n+                None,\n+                [\"default\"],\n+            )\n+            self.report_error(\n+                f\"No suitable extractor{format_field(ie_key, None, ' (%s)')} found for URL {url}\",\n+                tb=False if extractors_restricted else None,\n+            )\n \n     def _handle_extraction_exceptions(func):\n         @functools.wraps(func)\n@@ -1589,72 +1923,90 @@ def wrapper(self, *args, **kwargs):\n                     raise\n                 except ReExtractInfo as e:\n                     if e.expected:\n-                        self.to_screen(f'{e}; Re-extracting data')\n+                        self.to_screen(f\"{e}; Re-extracting data\")\n                     else:\n-                        self.to_stderr('\\r')\n-                        self.report_warning(f'{e}; Re-extracting data')\n+                        self.to_stderr(\"\\r\")\n+                        self.report_warning(f\"{e}; Re-extracting data\")\n                     continue\n                 except GeoRestrictedError as e:\n                     msg = e.msg\n                     if e.countries:\n-                        msg += '\\nThis video is available in %s.' % ', '.join(\n-                            map(ISO3166Utils.short2full, e.countries))\n-                    msg += '\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.'\n+                        msg += \"\\nThis video is available in %s.\" % \", \".join(\n+                            map(ISO3166Utils.short2full, e.countries)\n+                        )\n+                    msg += \"\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.\"\n                     self.report_error(msg)\n                 except ExtractorError as e:  # An error we somewhat expected\n                     self.report_error(str(e), e.format_traceback())\n                 except Exception as e:\n-                    if self.params.get('ignoreerrors'):\n-                        self.report_error(str(e), tb=encode_compat_str(traceback.format_exc()))\n+                    if self.params.get(\"ignoreerrors\"):\n+                        self.report_error(\n+                            str(e), tb=encode_compat_str(traceback.format_exc())\n+                        )\n                     else:\n                         raise\n                 break\n+\n         return wrapper\n \n     def _wait_for_video(self, ie_result={}):\n-        if (not self.params.get('wait_for_video')\n-                or ie_result.get('_type', 'video') != 'video'\n-                or ie_result.get('formats') or ie_result.get('url')):\n+        if (\n+            not self.params.get(\"wait_for_video\")\n+            or ie_result.get(\"_type\", \"video\") != \"video\"\n+            or ie_result.get(\"formats\")\n+            or ie_result.get(\"url\")\n+        ):\n             return\n \n-        format_dur = lambda dur: '%02d:%02d:%02d' % timetuple_from_msec(dur * 1000)[:-1]\n-        last_msg = ''\n+        format_dur = lambda dur: \"%02d:%02d:%02d\" % timetuple_from_msec(dur * 1000)[:-1]\n+        last_msg = \"\"\n \n         def progress(msg):\n             nonlocal last_msg\n-            full_msg = f'{msg}\\n'\n-            if not self.params.get('noprogress'):\n-                full_msg = msg + ' ' * (len(last_msg) - len(msg)) + '\\r'\n+            full_msg = f\"{msg}\\n\"\n+            if not self.params.get(\"noprogress\"):\n+                full_msg = msg + \" \" * (len(last_msg) - len(msg)) + \"\\r\"\n             elif last_msg:\n                 return\n             self.to_screen(full_msg, skip_eol=True)\n             last_msg = msg\n \n-        min_wait, max_wait = self.params.get('wait_for_video')\n-        diff = try_get(ie_result, lambda x: x['release_timestamp'] - time.time())\n-        if diff is None and ie_result.get('live_status') == 'is_upcoming':\n-            diff = round(random.uniform(min_wait, max_wait) if (max_wait and min_wait) else (max_wait or min_wait), 0)\n-            self.report_warning('Release time of video is not known')\n+        min_wait, max_wait = self.params.get(\"wait_for_video\")\n+        diff = try_get(ie_result, lambda x: x[\"release_timestamp\"] - time.time())\n+        if diff is None and ie_result.get(\"live_status\") == \"is_upcoming\":\n+            diff = round(\n+                random.uniform(min_wait, max_wait)\n+                if (max_wait and min_wait)\n+                else (max_wait or min_wait),\n+                0,\n+            )\n+            self.report_warning(\"Release time of video is not known\")\n         elif ie_result and (diff or 0) <= 0:\n-            self.report_warning('Video should already be available according to extracted info')\n-        diff = min(max(diff or 0, min_wait or 0), max_wait or float('inf'))\n-        self.to_screen(f'[wait] Waiting for {format_dur(diff)} - Press Ctrl+C to try now')\n+            self.report_warning(\n+                \"Video should already be available according to extracted info\"\n+            )\n+        diff = min(max(diff or 0, min_wait or 0), max_wait or float(\"inf\"))\n+        self.to_screen(\n+            f\"[wait] Waiting for {format_dur(diff)} - Press Ctrl+C to try now\"\n+        )\n \n         wait_till = time.time() + diff\n         try:\n             while True:\n                 diff = wait_till - time.time()\n                 if diff <= 0:\n-                    progress('')\n-                    raise ReExtractInfo('[wait] Wait period ended', expected=True)\n-                progress(f'[wait] Remaining time until next attempt: {self._format_screen(format_dur(diff), self.Styles.EMPHASIS)}')\n+                    progress(\"\")\n+                    raise ReExtractInfo(\"[wait] Wait period ended\", expected=True)\n+                progress(\n+                    f\"[wait] Remaining time until next attempt: {self._format_screen(format_dur(diff), self.Styles.EMPHASIS)}\"\n+                )\n                 time.sleep(1)\n         except KeyboardInterrupt:\n-            progress('')\n-            raise ReExtractInfo('[wait] Interrupted by user', expected=True)\n+            progress(\"\")\n+            raise ReExtractInfo(\"[wait] Interrupted by user\", expected=True)\n         except BaseException as e:\n             if not isinstance(e, ReExtractInfo):\n-                self.to_screen('')\n+                self.to_screen(\"\")\n             raise\n \n     def _load_cookies(self, data, *, autoscope=True):\n@@ -1670,34 +2022,53 @@ def _load_cookies(self, data, *, autoscope=True):\n         \"\"\"\n         for cookie in LenientSimpleCookie(data).values():\n             if autoscope and any(cookie.values()):\n-                raise ValueError('Invalid syntax in Cookie Header')\n+                raise ValueError(\"Invalid syntax in Cookie Header\")\n \n-            domain = cookie.get('domain') or ''\n-            expiry = cookie.get('expires')\n-            if expiry == '':  # 0 is valid\n+            domain = cookie.get(\"domain\") or \"\"\n+            expiry = cookie.get(\"expires\")\n+            if expiry == \"\":  # 0 is valid\n                 expiry = None\n             prepared_cookie = http.cookiejar.Cookie(\n-                cookie.get('version') or 0, cookie.key, cookie.value, None, False,\n-                domain, True, True, cookie.get('path') or '', bool(cookie.get('path')),\n-                cookie.get('secure') or False, expiry, False, None, None, {})\n+                cookie.get(\"version\") or 0,\n+                cookie.key,\n+                cookie.value,\n+                None,\n+                False,\n+                domain,\n+                True,\n+                True,\n+                cookie.get(\"path\") or \"\",\n+                bool(cookie.get(\"path\")),\n+                cookie.get(\"secure\") or False,\n+                expiry,\n+                False,\n+                None,\n+                None,\n+                {},\n+            )\n \n             if domain:\n                 self.cookiejar.set_cookie(prepared_cookie)\n             elif autoscope is True:\n                 self.deprecated_feature(\n-                    'Passing cookies as a header is a potential security risk; '\n-                    'they will be scoped to the domain of the downloaded urls. '\n-                    'Please consider loading cookies from a file or browser instead.')\n+                    \"Passing cookies as a header is a potential security risk; \"\n+                    \"they will be scoped to the domain of the downloaded urls. \"\n+                    \"Please consider loading cookies from a file or browser instead.\"\n+                )\n                 self.__header_cookies.append(prepared_cookie)\n             elif autoscope:\n                 self.report_warning(\n-                    'The extractor result contains an unscoped cookie as an HTTP header. '\n-                    f'If you are using yt-dlp with an input URL{bug_reports_message(before=\",\")}',\n-                    only_once=True)\n+                    \"The extractor result contains an unscoped cookie as an HTTP header. \"\n+                    f\"If you are using yt-dlp with an input URL{bug_reports_message(before=',')}\",\n+                    only_once=True,\n+                )\n                 self._apply_header_cookies(autoscope, [prepared_cookie])\n             else:\n-                self.report_error('Unscoped cookies are not allowed; please specify some sort of scoping',\n-                                  tb=False, is_error=False)\n+                self.report_error(\n+                    \"Unscoped cookies are not allowed; please specify some sort of scoping\",\n+                    tb=False,\n+                    is_error=False,\n+                )\n \n     def _apply_header_cookies(self, url, cookies=None):\n         \"\"\"Applies stray header cookies to the provided url\n@@ -1711,7 +2082,7 @@ def _apply_header_cookies(self, url, cookies=None):\n             return\n \n         for cookie in map(copy.copy, cookies or self.__header_cookies):\n-            cookie.domain = f'.{parsed.hostname}'\n+            cookie.domain = f\".{parsed.hostname}\"\n             self.cookiejar.set_cookie(cookie)\n \n     @_handle_extraction_exceptions\n@@ -1722,21 +2093,25 @@ def __extract_info(self, url, ie, download, extra_info, process):\n             ie_result = ie.extract(url)\n         except UserNotLive as e:\n             if process:\n-                if self.params.get('wait_for_video'):\n+                if self.params.get(\"wait_for_video\"):\n                     self.report_warning(e)\n                 self._wait_for_video()\n             raise\n-        if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)\n-            self.report_warning(f'Extractor {ie.IE_NAME} returned nothing{bug_reports_message()}')\n+        if (\n+            ie_result is None\n+        ):  # Finished already (backwards compatibility; listformats and friends should be moved here)\n+            self.report_warning(\n+                f\"Extractor {ie.IE_NAME} returned nothing{bug_reports_message()}\"\n+            )\n             return\n         if isinstance(ie_result, list):\n             # Backwards compatibility: old IE result format\n             ie_result = {\n-                '_type': 'compat_list',\n-                'entries': ie_result,\n+                \"_type\": \"compat_list\",\n+                \"entries\": ie_result,\n             }\n-        if extra_info.get('original_url'):\n-            ie_result.setdefault('original_url', extra_info['original_url'])\n+        if extra_info.get(\"original_url\"):\n+            ie_result.setdefault(\"original_url\", extra_info[\"original_url\"])\n         self.add_default_extra_info(ie_result, ie, url)\n         if process:\n             self._wait_for_video(ie_result)\n@@ -1746,21 +2121,30 @@ def __extract_info(self, url, ie, download, extra_info, process):\n \n     def add_default_extra_info(self, ie_result, ie, url):\n         if url is not None:\n-            self.add_extra_info(ie_result, {\n-                'webpage_url': url,\n-                'original_url': url,\n-            })\n-        webpage_url = ie_result.get('webpage_url')\n+            self.add_extra_info(\n+                ie_result,\n+                {\n+                    \"webpage_url\": url,\n+                    \"original_url\": url,\n+                },\n+            )\n+        webpage_url = ie_result.get(\"webpage_url\")\n         if webpage_url:\n-            self.add_extra_info(ie_result, {\n-                'webpage_url_basename': url_basename(webpage_url),\n-                'webpage_url_domain': get_domain(webpage_url),\n-            })\n+            self.add_extra_info(\n+                ie_result,\n+                {\n+                    \"webpage_url_basename\": url_basename(webpage_url),\n+                    \"webpage_url_domain\": get_domain(webpage_url),\n+                },\n+            )\n         if ie is not None:\n-            self.add_extra_info(ie_result, {\n-                'extractor': ie.IE_NAME,\n-                'extractor_key': ie.ie_key(),\n-            })\n+            self.add_extra_info(\n+                ie_result,\n+                {\n+                    \"extractor\": ie.IE_NAME,\n+                    \"extractor_key\": ie.ie_key(),\n+                },\n+            )\n \n     def process_ie_result(self, ie_result, download=True, extra_info=None):\n         \"\"\"\n@@ -1772,62 +2156,78 @@ def process_ie_result(self, ie_result, download=True, extra_info=None):\n         \"\"\"\n         if extra_info is None:\n             extra_info = {}\n-        result_type = ie_result.get('_type', 'video')\n+        result_type = ie_result.get(\"_type\", \"video\")\n \n-        if result_type in ('url', 'url_transparent'):\n-            ie_result['url'] = sanitize_url(\n-                ie_result['url'], scheme='http' if self.params.get('prefer_insecure') else 'https')\n-            if ie_result.get('original_url') and not extra_info.get('original_url'):\n-                extra_info = {'original_url': ie_result['original_url'], **extra_info}\n+        if result_type in (\"url\", \"url_transparent\"):\n+            ie_result[\"url\"] = sanitize_url(\n+                ie_result[\"url\"],\n+                scheme=\"http\" if self.params.get(\"prefer_insecure\") else \"https\",\n+            )\n+            if ie_result.get(\"original_url\") and not extra_info.get(\"original_url\"):\n+                extra_info = {\"original_url\": ie_result[\"original_url\"], **extra_info}\n \n-            extract_flat = self.params.get('extract_flat', False)\n-            if ((extract_flat == 'in_playlist' and 'playlist' in extra_info)\n-                    or extract_flat is True):\n+            extract_flat = self.params.get(\"extract_flat\", False)\n+            if (\n+                extract_flat == \"in_playlist\" and \"playlist\" in extra_info\n+            ) or extract_flat is True:\n                 info_copy = ie_result.copy()\n-                ie = try_get(ie_result.get('ie_key'), self.get_info_extractor)\n-                if ie and not ie_result.get('id'):\n-                    info_copy['id'] = ie.get_temp_id(ie_result['url'])\n-                self.add_default_extra_info(info_copy, ie, ie_result['url'])\n+                ie = try_get(ie_result.get(\"ie_key\"), self.get_info_extractor)\n+                if ie and not ie_result.get(\"id\"):\n+                    info_copy[\"id\"] = ie.get_temp_id(ie_result[\"url\"])\n+                self.add_default_extra_info(info_copy, ie, ie_result[\"url\"])\n                 self.add_extra_info(info_copy, extra_info)\n                 info_copy, _ = self.pre_process(info_copy)\n                 self._fill_common_fields(info_copy, False)\n                 self.__forced_printings(info_copy)\n                 self._raise_pending_errors(info_copy)\n-                if self.params.get('force_write_download_archive', False):\n+                if self.params.get(\"force_write_download_archive\", False):\n                     self.record_download_archive(info_copy)\n                 return ie_result\n \n-        if result_type == 'video':\n+        if result_type == \"video\":\n             self.add_extra_info(ie_result, extra_info)\n             ie_result = self.process_video_result(ie_result, download=download)\n             self._raise_pending_errors(ie_result)\n-            additional_urls = (ie_result or {}).get('additional_urls')\n+            additional_urls = (ie_result or {}).get(\"additional_urls\")\n             if additional_urls:\n                 # TODO: Improve MetadataParserPP to allow setting a list\n                 if isinstance(additional_urls, str):\n                     additional_urls = [additional_urls]\n                 self.to_screen(\n-                    '[info] %s: %d additional URL(s) requested' % (ie_result['id'], len(additional_urls)))\n+                    \"[info] %s: %d additional URL(s) requested\"\n+                    % (ie_result[\"id\"], len(additional_urls))\n+                )\n                 self.write_debug('Additional URLs: \"%s\"' % '\", \"'.join(additional_urls))\n-                ie_result['additional_entries'] = [\n+                ie_result[\"additional_entries\"] = [\n                     self.extract_info(\n-                        url, download, extra_info=extra_info,\n-                        force_generic_extractor=self.params.get('force_generic_extractor'))\n+                        url,\n+                        download,\n+                        extra_info=extra_info,\n+                        force_generic_extractor=self.params.get(\n+                            \"force_generic_extractor\"\n+                        ),\n+                    )\n                     for url in additional_urls\n                 ]\n             return ie_result\n-        elif result_type == 'url':\n+        elif result_type == \"url\":\n             # We have to add extra_info to the results because it may be\n             # contained in a playlist\n             return self.extract_info(\n-                ie_result['url'], download,\n-                ie_key=ie_result.get('ie_key'),\n-                extra_info=extra_info)\n-        elif result_type == 'url_transparent':\n+                ie_result[\"url\"],\n+                download,\n+                ie_key=ie_result.get(\"ie_key\"),\n+                extra_info=extra_info,\n+            )\n+        elif result_type == \"url_transparent\":\n             # Use the information from the embedding page\n             info = self.extract_info(\n-                ie_result['url'], ie_key=ie_result.get('ie_key'),\n-                extra_info=extra_info, download=False, process=False)\n+                ie_result[\"url\"],\n+                ie_key=ie_result.get(\"ie_key\"),\n+                extra_info=extra_info,\n+                download=False,\n+                process=False,\n+            )\n \n             # extract_info may return None when ignoreerrors is enabled and\n             # extraction failed with an error, don't crash and return early\n@@ -1835,13 +2235,20 @@ def process_ie_result(self, ie_result, download=True, extra_info=None):\n             if not info:\n                 return info\n \n-            exempted_fields = {'_type', 'url', 'ie_key'}\n-            if not ie_result.get('section_end') and ie_result.get('section_start') is None:\n+            exempted_fields = {\"_type\", \"url\", \"ie_key\"}\n+            if (\n+                not ie_result.get(\"section_end\")\n+                and ie_result.get(\"section_start\") is None\n+            ):\n                 # For video clips, the id etc of the clip extractor should be used\n-                exempted_fields |= {'id', 'extractor', 'extractor_key'}\n+                exempted_fields |= {\"id\", \"extractor\", \"extractor_key\"}\n \n             new_result = info.copy()\n-            new_result.update(filter_dict(ie_result, lambda k, v: v is not None and k not in exempted_fields))\n+            new_result.update(\n+                filter_dict(\n+                    ie_result, lambda k, v: v is not None and k not in exempted_fields\n+                )\n+            )\n \n             # Extracted info may not be a video result (i.e.\n             # info.get('_type', 'video') != video) but rather an url or\n@@ -1849,19 +2256,24 @@ def process_ie_result(self, ie_result, download=True, extra_info=None):\n             # should be propagated to inner one (info). For this to happen\n             # _type of info should be overridden with url_transparent. This\n             # fixes issue from https://github.com/ytdl-org/youtube-dl/pull/11163.\n-            if new_result.get('_type') == 'url':\n-                new_result['_type'] = 'url_transparent'\n+            if new_result.get(\"_type\") == \"url\":\n+                new_result[\"_type\"] = \"url_transparent\"\n \n             return self.process_ie_result(\n-                new_result, download=download, extra_info=extra_info)\n-        elif result_type in ('playlist', 'multi_video'):\n+                new_result, download=download, extra_info=extra_info\n+            )\n+        elif result_type in (\"playlist\", \"multi_video\"):\n             # Protect from infinite recursion due to recursively nested playlists\n             # (see https://github.com/ytdl-org/youtube-dl/issues/27833)\n-            webpage_url = ie_result.get('webpage_url')  # Playlists maynot have webpage_url\n+            webpage_url = ie_result.get(\n+                \"webpage_url\"\n+            )  # Playlists maynot have webpage_url\n             if webpage_url and webpage_url in self._playlist_urls:\n                 self.to_screen(\n-                    '[download] Skipping already downloaded playlist: %s'\n-                    % ie_result.get('title') or ie_result.get('id'))\n+                    \"[download] Skipping already downloaded playlist: %s\"\n+                    % ie_result.get(\"title\")\n+                    or ie_result.get(\"id\")\n+                )\n                 return\n \n             self._playlist_level += 1\n@@ -1874,27 +2286,32 @@ def process_ie_result(self, ie_result, download=True, extra_info=None):\n                 self._playlist_level -= 1\n                 if not self._playlist_level:\n                     self._playlist_urls.clear()\n-        elif result_type == 'compat_list':\n+        elif result_type == \"compat_list\":\n             self.report_warning(\n-                'Extractor %s returned a compat_list result. '\n-                'It needs to be updated.' % ie_result.get('extractor'))\n+                \"Extractor %s returned a compat_list result. \"\n+                \"It needs to be updated.\" % ie_result.get(\"extractor\")\n+            )\n \n             def _fixup(r):\n-                self.add_extra_info(r, {\n-                    'extractor': ie_result['extractor'],\n-                    'webpage_url': ie_result['webpage_url'],\n-                    'webpage_url_basename': url_basename(ie_result['webpage_url']),\n-                    'webpage_url_domain': get_domain(ie_result['webpage_url']),\n-                    'extractor_key': ie_result['extractor_key'],\n-                })\n+                self.add_extra_info(\n+                    r,\n+                    {\n+                        \"extractor\": ie_result[\"extractor\"],\n+                        \"webpage_url\": ie_result[\"webpage_url\"],\n+                        \"webpage_url_basename\": url_basename(ie_result[\"webpage_url\"]),\n+                        \"webpage_url_domain\": get_domain(ie_result[\"webpage_url\"]),\n+                        \"extractor_key\": ie_result[\"extractor_key\"],\n+                    },\n+                )\n                 return r\n-            ie_result['entries'] = [\n+\n+            ie_result[\"entries\"] = [\n                 self.process_ie_result(_fixup(r), download, extra_info)\n-                for r in ie_result['entries']\n+                for r in ie_result[\"entries\"]\n             ]\n             return ie_result\n         else:\n-            raise Exception('Invalid result type: %s' % result_type)\n+            raise Exception(\"Invalid result type: %s\" % result_type)\n \n     def _ensure_dir_exists(self, path):\n         return make_dir(path, self.report_error)\n@@ -1902,226 +2319,284 @@ def _ensure_dir_exists(self, path):\n     @staticmethod\n     def _playlist_infodict(ie_result, strict=False, **kwargs):\n         info = {\n-            'playlist_count': ie_result.get('playlist_count'),\n-            'playlist': ie_result.get('title') or ie_result.get('id'),\n-            'playlist_id': ie_result.get('id'),\n-            'playlist_title': ie_result.get('title'),\n-            'playlist_uploader': ie_result.get('uploader'),\n-            'playlist_uploader_id': ie_result.get('uploader_id'),\n+            \"playlist_count\": ie_result.get(\"playlist_count\"),\n+            \"playlist\": ie_result.get(\"title\") or ie_result.get(\"id\"),\n+            \"playlist_id\": ie_result.get(\"id\"),\n+            \"playlist_title\": ie_result.get(\"title\"),\n+            \"playlist_uploader\": ie_result.get(\"uploader\"),\n+            \"playlist_uploader_id\": ie_result.get(\"uploader_id\"),\n             **kwargs,\n         }\n         if strict:\n             return info\n-        if ie_result.get('webpage_url'):\n-            info.update({\n-                'webpage_url': ie_result['webpage_url'],\n-                'webpage_url_basename': url_basename(ie_result['webpage_url']),\n-                'webpage_url_domain': get_domain(ie_result['webpage_url']),\n-            })\n+        if ie_result.get(\"webpage_url\"):\n+            info.update(\n+                {\n+                    \"webpage_url\": ie_result[\"webpage_url\"],\n+                    \"webpage_url_basename\": url_basename(ie_result[\"webpage_url\"]),\n+                    \"webpage_url_domain\": get_domain(ie_result[\"webpage_url\"]),\n+                }\n+            )\n         return {\n             **info,\n-            'playlist_index': 0,\n-            '__last_playlist_index': max(ie_result.get('requested_entries') or (0, 0)),\n-            'extractor': ie_result['extractor'],\n-            'extractor_key': ie_result['extractor_key'],\n+            \"playlist_index\": 0,\n+            \"__last_playlist_index\": max(ie_result.get(\"requested_entries\") or (0, 0)),\n+            \"extractor\": ie_result[\"extractor\"],\n+            \"extractor_key\": ie_result[\"extractor_key\"],\n         }\n \n     def __process_playlist(self, ie_result, download):\n         \"\"\"Process each entry in the playlist\"\"\"\n-        assert ie_result['_type'] in ('playlist', 'multi_video')\n+        assert ie_result[\"_type\"] in (\"playlist\", \"multi_video\")\n \n         common_info = self._playlist_infodict(ie_result, strict=True)\n-        title = common_info.get('playlist') or '<Untitled>'\n+        title = common_info.get(\"playlist\") or \"<Untitled>\"\n         if self._match_entry(common_info, incomplete=True) is not None:\n             return\n-        self.to_screen(f'[download] Downloading {ie_result[\"_type\"]}: {title}')\n+        self.to_screen(f\"[download] Downloading {ie_result['_type']}: {title}\")\n \n         all_entries = PlaylistEntries(self, ie_result)\n         entries = orderedSet(all_entries.get_requested_items(), lazy=True)\n \n-        lazy = self.params.get('lazy_playlist')\n+        lazy = self.params.get(\"lazy_playlist\")\n         if lazy:\n-            resolved_entries, n_entries = [], 'N/A'\n-            ie_result['requested_entries'], ie_result['entries'] = None, None\n+            resolved_entries, n_entries = [], \"N/A\"\n+            ie_result[\"requested_entries\"], ie_result[\"entries\"] = None, None\n         else:\n             entries = resolved_entries = list(entries)\n             n_entries = len(resolved_entries)\n-            ie_result['requested_entries'], ie_result['entries'] = tuple(zip(*resolved_entries)) or ([], [])\n-        if not ie_result.get('playlist_count'):\n+            ie_result[\"requested_entries\"], ie_result[\"entries\"] = tuple(\n+                zip(*resolved_entries)\n+            ) or ([], [])\n+        if not ie_result.get(\"playlist_count\"):\n             # Better to do this after potentially exhausting entries\n-            ie_result['playlist_count'] = all_entries.get_full_count()\n+            ie_result[\"playlist_count\"] = all_entries.get_full_count()\n \n         extra = self._playlist_infodict(ie_result, n_entries=int_or_none(n_entries))\n         ie_copy = collections.ChainMap(ie_result, extra)\n \n         _infojson_written = False\n-        write_playlist_files = self.params.get('allow_playlist_files', True)\n-        if write_playlist_files and self.params.get('list_thumbnails'):\n+        write_playlist_files = self.params.get(\"allow_playlist_files\", True)\n+        if write_playlist_files and self.params.get(\"list_thumbnails\"):\n             self.list_thumbnails(ie_result)\n-        if write_playlist_files and not self.params.get('simulate'):\n+        if write_playlist_files and not self.params.get(\"simulate\"):\n             _infojson_written = self._write_info_json(\n-                'playlist', ie_result, self.prepare_filename(ie_copy, 'pl_infojson'))\n+                \"playlist\", ie_result, self.prepare_filename(ie_copy, \"pl_infojson\")\n+            )\n             if _infojson_written is None:\n                 return\n-            if self._write_description('playlist', ie_result,\n-                                       self.prepare_filename(ie_copy, 'pl_description')) is None:\n+            if (\n+                self._write_description(\n+                    \"playlist\",\n+                    ie_result,\n+                    self.prepare_filename(ie_copy, \"pl_description\"),\n+                )\n+                is None\n+            ):\n                 return\n             # TODO: This should be passed to ThumbnailsConvertor if necessary\n-            self._write_thumbnails('playlist', ie_result, self.prepare_filename(ie_copy, 'pl_thumbnail'))\n+            self._write_thumbnails(\n+                \"playlist\", ie_result, self.prepare_filename(ie_copy, \"pl_thumbnail\")\n+            )\n \n         if lazy:\n-            if self.params.get('playlistreverse') or self.params.get('playlistrandom'):\n-                self.report_warning('playlistreverse and playlistrandom are not supported with lazy_playlist', only_once=True)\n-        elif self.params.get('playlistreverse'):\n+            if self.params.get(\"playlistreverse\") or self.params.get(\"playlistrandom\"):\n+                self.report_warning(\n+                    \"playlistreverse and playlistrandom are not supported with lazy_playlist\",\n+                    only_once=True,\n+                )\n+        elif self.params.get(\"playlistreverse\"):\n             entries.reverse()\n-        elif self.params.get('playlistrandom'):\n+        elif self.params.get(\"playlistrandom\"):\n             random.shuffle(entries)\n \n-        self.to_screen(f'[{ie_result[\"extractor\"]}] Playlist {title}: Downloading {n_entries} items'\n-                       f'{format_field(ie_result, \"playlist_count\", \" of %s\")}')\n+        self.to_screen(\n+            f\"[{ie_result['extractor']}] Playlist {title}: Downloading {n_entries} items\"\n+            f\"{format_field(ie_result, 'playlist_count', ' of %s')}\"\n+        )\n \n-        keep_resolved_entries = self.params.get('extract_flat') != 'discard'\n-        if self.params.get('extract_flat') == 'discard_in_playlist':\n-            keep_resolved_entries = ie_result['_type'] != 'playlist'\n+        keep_resolved_entries = self.params.get(\"extract_flat\") != \"discard\"\n+        if self.params.get(\"extract_flat\") == \"discard_in_playlist\":\n+            keep_resolved_entries = ie_result[\"_type\"] != \"playlist\"\n         if keep_resolved_entries:\n-            self.write_debug('The information of all playlist entries will be held in memory')\n+            self.write_debug(\n+                \"The information of all playlist entries will be held in memory\"\n+            )\n \n         failures = 0\n-        max_failures = self.params.get('skip_playlist_after_errors') or float('inf')\n+        max_failures = self.params.get(\"skip_playlist_after_errors\") or float(\"inf\")\n         for i, (playlist_index, entry) in enumerate(entries):\n             if lazy:\n                 resolved_entries.append((playlist_index, entry))\n             if not entry:\n                 continue\n \n-            entry['__x_forwarded_for_ip'] = ie_result.get('__x_forwarded_for_ip')\n-            if not lazy and 'playlist-index' in self.params['compat_opts']:\n-                playlist_index = ie_result['requested_entries'][i]\n-\n-            entry_copy = collections.ChainMap(entry, {\n-                **common_info,\n-                'n_entries': int_or_none(n_entries),\n-                'playlist_index': playlist_index,\n-                'playlist_autonumber': i + 1,\n-            })\n+            entry[\"__x_forwarded_for_ip\"] = ie_result.get(\"__x_forwarded_for_ip\")\n+            if not lazy and \"playlist-index\" in self.params[\"compat_opts\"]:\n+                playlist_index = ie_result[\"requested_entries\"][i]\n+\n+            entry_copy = collections.ChainMap(\n+                entry,\n+                {\n+                    **common_info,\n+                    \"n_entries\": int_or_none(n_entries),\n+                    \"playlist_index\": playlist_index,\n+                    \"playlist_autonumber\": i + 1,\n+                },\n+            )\n \n             if self._match_entry(entry_copy, incomplete=True) is not None:\n                 # For compatabilty with youtube-dl. See https://github.com/yt-dlp/yt-dlp/issues/4369\n                 resolved_entries[i] = (playlist_index, NO_DEFAULT)\n                 continue\n \n-            self.to_screen('[download] Downloading item %s of %s' % (\n-                self._format_screen(i + 1, self.Styles.ID), self._format_screen(n_entries, self.Styles.EMPHASIS)))\n+            self.to_screen(\n+                \"[download] Downloading item %s of %s\"\n+                % (\n+                    self._format_screen(i + 1, self.Styles.ID),\n+                    self._format_screen(n_entries, self.Styles.EMPHASIS),\n+                )\n+            )\n \n-            entry_result = self.__process_iterable_entry(entry, download, collections.ChainMap({\n-                'playlist_index': playlist_index,\n-                'playlist_autonumber': i + 1,\n-            }, extra))\n+            entry_result = self.__process_iterable_entry(\n+                entry,\n+                download,\n+                collections.ChainMap(\n+                    {\n+                        \"playlist_index\": playlist_index,\n+                        \"playlist_autonumber\": i + 1,\n+                    },\n+                    extra,\n+                ),\n+            )\n             if not entry_result:\n                 failures += 1\n             if failures >= max_failures:\n                 self.report_error(\n-                    f'Skipping the remaining entries in playlist \"{title}\" since {failures} items failed extraction')\n+                    f'Skipping the remaining entries in playlist \"{title}\" since {failures} items failed extraction'\n+                )\n                 break\n             if keep_resolved_entries:\n                 resolved_entries[i] = (playlist_index, entry_result)\n \n         # Update with processed data\n-        ie_result['entries'] = [e for _, e in resolved_entries if e is not NO_DEFAULT]\n-        ie_result['requested_entries'] = [i for i, e in resolved_entries if e is not NO_DEFAULT]\n-        if ie_result['requested_entries'] == try_call(lambda: list(range(1, ie_result['playlist_count'] + 1))):\n+        ie_result[\"entries\"] = [e for _, e in resolved_entries if e is not NO_DEFAULT]\n+        ie_result[\"requested_entries\"] = [\n+            i for i, e in resolved_entries if e is not NO_DEFAULT\n+        ]\n+        if ie_result[\"requested_entries\"] == try_call(\n+            lambda: list(range(1, ie_result[\"playlist_count\"] + 1))\n+        ):\n             # Do not set for full playlist\n-            ie_result.pop('requested_entries')\n+            ie_result.pop(\"requested_entries\")\n \n         # Write the updated info to json\n-        if _infojson_written is True and self._write_info_json(\n-                'updated playlist', ie_result,\n-                self.prepare_filename(ie_copy, 'pl_infojson'), overwrite=True) is None:\n+        if (\n+            _infojson_written is True\n+            and self._write_info_json(\n+                \"updated playlist\",\n+                ie_result,\n+                self.prepare_filename(ie_copy, \"pl_infojson\"),\n+                overwrite=True,\n+            )\n+            is None\n+        ):\n             return\n \n-        ie_result = self.run_all_pps('playlist', ie_result)\n-        self.to_screen(f'[download] Finished downloading playlist: {title}')\n+        ie_result = self.run_all_pps(\"playlist\", ie_result)\n+        self.to_screen(f\"[download] Finished downloading playlist: {title}\")\n         return ie_result\n \n     @_handle_extraction_exceptions\n     def __process_iterable_entry(self, entry, download, extra_info):\n-        return self.process_ie_result(\n-            entry, download=download, extra_info=extra_info)\n+        return self.process_ie_result(entry, download=download, extra_info=extra_info)\n \n     def _build_format_filter(self, filter_spec):\n-        \" Returns a function to filter the formats according to the filter_spec \"\n+        \"Returns a function to filter the formats according to the filter_spec\"\n \n         OPERATORS = {\n-            '<': operator.lt,\n-            '<=': operator.le,\n-            '>': operator.gt,\n-            '>=': operator.ge,\n-            '=': operator.eq,\n-            '!=': operator.ne,\n+            \"<\": operator.lt,\n+            \"<=\": operator.le,\n+            \">\": operator.gt,\n+            \">=\": operator.ge,\n+            \"=\": operator.eq,\n+            \"!=\": operator.ne,\n         }\n-        operator_rex = re.compile(r'''(?x)\\s*\n+        operator_rex = re.compile(\n+            r\"\"\"(?x)\\s*\n             (?P<key>[\\w.-]+)\\s*\n             (?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n             (?P<value>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)\\s*\n-            ''' % '|'.join(map(re.escape, OPERATORS.keys())))\n+            \"\"\"\n+            % \"|\".join(map(re.escape, OPERATORS.keys()))\n+        )\n         m = operator_rex.fullmatch(filter_spec)\n         if m:\n             try:\n-                comparison_value = int(m.group('value'))\n+                comparison_value = int(m.group(\"value\"))\n             except ValueError:\n-                comparison_value = parse_filesize(m.group('value'))\n+                comparison_value = parse_filesize(m.group(\"value\"))\n                 if comparison_value is None:\n-                    comparison_value = parse_filesize(m.group('value') + 'B')\n+                    comparison_value = parse_filesize(m.group(\"value\") + \"B\")\n                 if comparison_value is None:\n                     raise ValueError(\n-                        'Invalid value %r in format specification %r' % (\n-                            m.group('value'), filter_spec))\n-            op = OPERATORS[m.group('op')]\n+                        \"Invalid value %r in format specification %r\"\n+                        % (m.group(\"value\"), filter_spec)\n+                    )\n+            op = OPERATORS[m.group(\"op\")]\n \n         if not m:\n             STR_OPERATORS = {\n-                '=': operator.eq,\n-                '^=': lambda attr, value: attr.startswith(value),\n-                '$=': lambda attr, value: attr.endswith(value),\n-                '*=': lambda attr, value: value in attr,\n-                '~=': lambda attr, value: value.search(attr) is not None\n+                \"=\": operator.eq,\n+                \"^=\": lambda attr, value: attr.startswith(value),\n+                \"$=\": lambda attr, value: attr.endswith(value),\n+                \"*=\": lambda attr, value: value in attr,\n+                \"~=\": lambda attr, value: value.search(attr) is not None,\n             }\n-            str_operator_rex = re.compile(r'''(?x)\\s*\n+            str_operator_rex = re.compile(\n+                r\"\"\"(?x)\\s*\n                 (?P<key>[a-zA-Z0-9._-]+)\\s*\n                 (?P<negation>!\\s*)?(?P<op>%s)\\s*(?P<none_inclusive>\\?\\s*)?\n                 (?P<quote>[\"'])?\n                 (?P<value>(?(quote)(?:(?!(?P=quote))[^\\\\]|\\\\.)+|[\\w.-]+))\n                 (?(quote)(?P=quote))\\s*\n-                ''' % '|'.join(map(re.escape, STR_OPERATORS.keys())))\n+                \"\"\"\n+                % \"|\".join(map(re.escape, STR_OPERATORS.keys()))\n+            )\n             m = str_operator_rex.fullmatch(filter_spec)\n             if m:\n-                if m.group('op') == '~=':\n-                    comparison_value = re.compile(m.group('value'))\n+                if m.group(\"op\") == \"~=\":\n+                    comparison_value = re.compile(m.group(\"value\"))\n                 else:\n-                    comparison_value = re.sub(r'''\\\\([\\\\\"'])''', r'\\1', m.group('value'))\n-                str_op = STR_OPERATORS[m.group('op')]\n-                if m.group('negation'):\n+                    comparison_value = re.sub(\n+                        r\"\"\"\\\\([\\\\\"'])\"\"\", r\"\\1\", m.group(\"value\")\n+                    )\n+                str_op = STR_OPERATORS[m.group(\"op\")]\n+                if m.group(\"negation\"):\n                     op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op\n \n         if not m:\n-            raise SyntaxError('Invalid filter specification %r' % filter_spec)\n+            raise SyntaxError(\"Invalid filter specification %r\" % filter_spec)\n \n         def _filter(f):\n-            actual_value = f.get(m.group('key'))\n+            actual_value = f.get(m.group(\"key\"))\n             if actual_value is None:\n-                return m.group('none_inclusive')\n+                return m.group(\"none_inclusive\")\n             return op(actual_value, comparison_value)\n+\n         return _filter\n \n     def _check_formats(self, formats):\n         for f in formats:\n-            self.to_screen('[info] Testing format %s' % f['format_id'])\n-            path = self.get_output_path('temp')\n-            if not self._ensure_dir_exists(f'{path}/'):\n+            self.to_screen(\"[info] Testing format %s\" % f[\"format_id\"])\n+            path = self.get_output_path(\"temp\")\n+            if not self._ensure_dir_exists(f\"{path}/\"):\n                 continue\n-            temp_file = tempfile.NamedTemporaryFile(suffix='.tmp', delete=False, dir=path or None)\n+            temp_file = tempfile.NamedTemporaryFile(\n+                suffix=\".tmp\", delete=False, dir=path or None\n+            )\n             temp_file.close()\n             try:\n                 success, _ = self.dl(temp_file.name, f, test=True)\n@@ -2132,78 +2607,104 @@ def _check_formats(self, formats):\n                     try:\n                         os.remove(temp_file.name)\n                     except OSError:\n-                        self.report_warning('Unable to delete temporary file \"%s\"' % temp_file.name)\n+                        self.report_warning(\n+                            'Unable to delete temporary file \"%s\"' % temp_file.name\n+                        )\n             if success:\n                 yield f\n             else:\n-                self.to_screen('[info] Unable to download format %s. Skipping...' % f['format_id'])\n+                self.to_screen(\n+                    \"[info] Unable to download format %s. Skipping...\" % f[\"format_id\"]\n+                )\n \n     def _default_format_spec(self, info_dict, download=True):\n-\n         def can_merge():\n             merger = FFmpegMergerPP(self)\n             return merger.available and merger.can_merge()\n \n         prefer_best = (\n-            not self.params.get('simulate')\n+            not self.params.get(\"simulate\")\n             and download\n             and (\n                 not can_merge()\n-                or info_dict.get('is_live') and not self.params.get('live_from_start')\n-                or self.params['outtmpl']['default'] == '-'))\n+                or info_dict.get(\"is_live\")\n+                and not self.params.get(\"live_from_start\")\n+                or self.params[\"outtmpl\"][\"default\"] == \"-\"\n+            )\n+        )\n         compat = (\n             prefer_best\n-            or self.params.get('allow_multiple_audio_streams', False)\n-            or 'format-spec' in self.params['compat_opts'])\n+            or self.params.get(\"allow_multiple_audio_streams\", False)\n+            or \"format-spec\" in self.params[\"compat_opts\"]\n+        )\n \n         return (\n-            'best/bestvideo+bestaudio' if prefer_best\n-            else 'bestvideo*+bestaudio/best' if not compat\n-            else 'bestvideo+bestaudio/best')\n+            \"best/bestvideo+bestaudio\"\n+            if prefer_best\n+            else \"bestvideo*+bestaudio/best\"\n+            if not compat\n+            else \"bestvideo+bestaudio/best\"\n+        )\n \n     def build_format_selector(self, format_spec):\n         def syntax_error(note, start):\n-            message = (\n-                'Invalid format specification: '\n-                '{}\\n\\t{}\\n\\t{}^'.format(note, format_spec, ' ' * start[1]))\n+            message = \"Invalid format specification: {}\\n\\t{}\\n\\t{}^\".format(\n+                note, format_spec, \" \" * start[1]\n+            )\n             return SyntaxError(message)\n \n-        PICKFIRST = 'PICKFIRST'\n-        MERGE = 'MERGE'\n-        SINGLE = 'SINGLE'\n-        GROUP = 'GROUP'\n-        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])\n+        PICKFIRST = \"PICKFIRST\"\n+        MERGE = \"MERGE\"\n+        SINGLE = \"SINGLE\"\n+        GROUP = \"GROUP\"\n+        FormatSelector = collections.namedtuple(\n+            \"FormatSelector\", [\"type\", \"selector\", \"filters\"]\n+        )\n \n-        allow_multiple_streams = {'audio': self.params.get('allow_multiple_audio_streams', False),\n-                                  'video': self.params.get('allow_multiple_video_streams', False)}\n+        allow_multiple_streams = {\n+            \"audio\": self.params.get(\"allow_multiple_audio_streams\", False),\n+            \"video\": self.params.get(\"allow_multiple_video_streams\", False),\n+        }\n \n         def _parse_filter(tokens):\n             filter_parts = []\n             for type, string_, start, _, _ in tokens:\n-                if type == tokenize.OP and string_ == ']':\n-                    return ''.join(filter_parts)\n+                if type == tokenize.OP and string_ == \"]\":\n+                    return \"\".join(filter_parts)\n                 else:\n                     filter_parts.append(string_)\n \n         def _remove_unused_ops(tokens):\n             # Remove operators that we don't use and join them with the surrounding strings.\n             # E.g. 'mp4' '-' 'baseline' '-' '16x9' is converted to 'mp4-baseline-16x9'\n-            ALLOWED_OPS = ('/', '+', ',', '(', ')')\n+            ALLOWED_OPS = (\"/\", \"+\", \",\", \"(\", \")\")\n             last_string, last_start, last_end, last_line = None, None, None, None\n             for type, string_, start, end, line in tokens:\n-                if type == tokenize.OP and string_ == '[':\n+                if type == tokenize.OP and string_ == \"[\":\n                     if last_string:\n-                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n+                        yield (\n+                            tokenize.NAME,\n+                            last_string,\n+                            last_start,\n+                            last_end,\n+                            last_line,\n+                        )\n                         last_string = None\n                     yield type, string_, start, end, line\n                     # everything inside brackets will be handled by _parse_filter\n                     for type, string_, start, end, line in tokens:\n                         yield type, string_, start, end, line\n-                        if type == tokenize.OP and string_ == ']':\n+                        if type == tokenize.OP and string_ == \"]\":\n                             break\n                 elif type == tokenize.OP and string_ in ALLOWED_OPS:\n                     if last_string:\n-                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n+                        yield (\n+                            tokenize.NAME,\n+                            last_string,\n+                            last_start,\n+                            last_end,\n+                            last_line,\n+                        )\n                         last_string = None\n                     yield type, string_, start, end, line\n                 elif type in [tokenize.NAME, tokenize.NUMBER, tokenize.OP]:\n@@ -2216,58 +2717,72 @@ def _remove_unused_ops(tokens):\n             if last_string:\n                 yield tokenize.NAME, last_string, last_start, last_end, last_line\n \n-        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n+        def _parse_format_selection(\n+            tokens, inside_merge=False, inside_choice=False, inside_group=False\n+        ):\n             selectors = []\n             current_selector = None\n             for type, string_, start, _, _ in tokens:\n                 # ENCODING is only defined in python 3.x\n-                if type == getattr(tokenize, 'ENCODING', None):\n+                if type == getattr(tokenize, \"ENCODING\", None):\n                     continue\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string_, [])\n                 elif type == tokenize.OP:\n-                    if string_ == ')':\n+                    if string_ == \")\":\n                         if not inside_group:\n                             # ')' will be handled by the parentheses group\n                             tokens.restore_last_token()\n                         break\n-                    elif inside_merge and string_ in ['/', ',']:\n+                    elif inside_merge and string_ in [\"/\", \",\"]:\n                         tokens.restore_last_token()\n                         break\n-                    elif inside_choice and string_ == ',':\n+                    elif inside_choice and string_ == \",\":\n                         tokens.restore_last_token()\n                         break\n-                    elif string_ == ',':\n+                    elif string_ == \",\":\n                         if not current_selector:\n-                            raise syntax_error('\",\" must follow a format selector', start)\n+                            raise syntax_error(\n+                                '\",\" must follow a format selector', start\n+                            )\n                         selectors.append(current_selector)\n                         current_selector = None\n-                    elif string_ == '/':\n+                    elif string_ == \"/\":\n                         if not current_selector:\n-                            raise syntax_error('\"/\" must follow a format selector', start)\n+                            raise syntax_error(\n+                                '\"/\" must follow a format selector', start\n+                            )\n                         first_choice = current_selector\n-                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n-                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n-                    elif string_ == '[':\n+                        second_choice = _parse_format_selection(\n+                            tokens, inside_choice=True\n+                        )\n+                        current_selector = FormatSelector(\n+                            PICKFIRST, (first_choice, second_choice), []\n+                        )\n+                    elif string_ == \"[\":\n                         if not current_selector:\n-                            current_selector = FormatSelector(SINGLE, 'best', [])\n+                            current_selector = FormatSelector(SINGLE, \"best\", [])\n                         format_filter = _parse_filter(tokens)\n                         current_selector.filters.append(format_filter)\n-                    elif string_ == '(':\n+                    elif string_ == \"(\":\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                         group = _parse_format_selection(tokens, inside_group=True)\n                         current_selector = FormatSelector(GROUP, group, [])\n-                    elif string_ == '+':\n+                    elif string_ == \"+\":\n                         if not current_selector:\n                             raise syntax_error('Unexpected \"+\"', start)\n                         selector_1 = current_selector\n                         selector_2 = _parse_format_selection(tokens, inside_merge=True)\n                         if not selector_2:\n-                            raise syntax_error('Expected a selector', start)\n-                        current_selector = FormatSelector(MERGE, (selector_1, selector_2), [])\n+                            raise syntax_error(\"Expected a selector\", start)\n+                        current_selector = FormatSelector(\n+                            MERGE, (selector_1, selector_2), []\n+                        )\n                     else:\n-                        raise syntax_error(f'Operator not recognized: \"{string_}\"', start)\n+                        raise syntax_error(\n+                            f'Operator not recognized: \"{string_}\"', start\n+                        )\n                 elif type == tokenize.ENDMARKER:\n                     break\n             if current_selector:\n@@ -2278,17 +2793,23 @@ def _merge(formats_pair):\n             format_1, format_2 = formats_pair\n \n             formats_info = []\n-            formats_info.extend(format_1.get('requested_formats', (format_1,)))\n-            formats_info.extend(format_2.get('requested_formats', (format_2,)))\n-\n-            if not allow_multiple_streams['video'] or not allow_multiple_streams['audio']:\n-                get_no_more = {'video': False, 'audio': False}\n-                for (i, fmt_info) in enumerate(formats_info):\n-                    if fmt_info.get('acodec') == fmt_info.get('vcodec') == 'none':\n+            formats_info.extend(format_1.get(\"requested_formats\", (format_1,)))\n+            formats_info.extend(format_2.get(\"requested_formats\", (format_2,)))\n+\n+            if (\n+                not allow_multiple_streams[\"video\"]\n+                or not allow_multiple_streams[\"audio\"]\n+            ):\n+                get_no_more = {\"video\": False, \"audio\": False}\n+                for i, fmt_info in enumerate(formats_info):\n+                    if fmt_info.get(\"acodec\") == fmt_info.get(\"vcodec\") == \"none\":\n                         formats_info.pop(i)\n                         continue\n-                    for aud_vid in ['audio', 'video']:\n-                        if not allow_multiple_streams[aud_vid] and fmt_info.get(aud_vid[0] + 'codec') != 'none':\n+                    for aud_vid in [\"audio\", \"video\"]:\n+                        if (\n+                            not allow_multiple_streams[aud_vid]\n+                            and fmt_info.get(aud_vid[0] + \"codec\") != \"none\"\n+                        ):\n                             if get_no_more[aud_vid]:\n                                 formats_info.pop(i)\n                                 break\n@@ -2297,68 +2818,88 @@ def _merge(formats_pair):\n             if len(formats_info) == 1:\n                 return formats_info[0]\n \n-            video_fmts = [fmt_info for fmt_info in formats_info if fmt_info.get('vcodec') != 'none']\n-            audio_fmts = [fmt_info for fmt_info in formats_info if fmt_info.get('acodec') != 'none']\n+            video_fmts = [\n+                fmt_info\n+                for fmt_info in formats_info\n+                if fmt_info.get(\"vcodec\") != \"none\"\n+            ]\n+            audio_fmts = [\n+                fmt_info\n+                for fmt_info in formats_info\n+                if fmt_info.get(\"acodec\") != \"none\"\n+            ]\n \n             the_only_video = video_fmts[0] if len(video_fmts) == 1 else None\n             the_only_audio = audio_fmts[0] if len(audio_fmts) == 1 else None\n \n             output_ext = get_compatible_ext(\n-                vcodecs=[f.get('vcodec') for f in video_fmts],\n-                acodecs=[f.get('acodec') for f in audio_fmts],\n-                vexts=[f['ext'] for f in video_fmts],\n-                aexts=[f['ext'] for f in audio_fmts],\n-                preferences=(try_call(lambda: self.params['merge_output_format'].split('/'))\n-                             or self.params.get('prefer_free_formats') and ('webm', 'mkv')))\n+                vcodecs=[f.get(\"vcodec\") for f in video_fmts],\n+                acodecs=[f.get(\"acodec\") for f in audio_fmts],\n+                vexts=[f[\"ext\"] for f in video_fmts],\n+                aexts=[f[\"ext\"] for f in audio_fmts],\n+                preferences=(\n+                    try_call(lambda: self.params[\"merge_output_format\"].split(\"/\"))\n+                    or self.params.get(\"prefer_free_formats\")\n+                    and (\"webm\", \"mkv\")\n+                ),\n+            )\n \n-            filtered = lambda *keys: filter(None, (traverse_obj(fmt, *keys) for fmt in formats_info))\n+            filtered = lambda *keys: filter(\n+                None, (traverse_obj(fmt, *keys) for fmt in formats_info)\n+            )\n \n             new_dict = {\n-                'requested_formats': formats_info,\n-                'format': '+'.join(filtered('format')),\n-                'format_id': '+'.join(filtered('format_id')),\n-                'ext': output_ext,\n-                'protocol': '+'.join(map(determine_protocol, formats_info)),\n-                'language': '+'.join(orderedSet(filtered('language'))) or None,\n-                'format_note': '+'.join(orderedSet(filtered('format_note'))) or None,\n-                'filesize_approx': sum(filtered('filesize', 'filesize_approx')) or None,\n-                'tbr': sum(filtered('tbr', 'vbr', 'abr')),\n+                \"requested_formats\": formats_info,\n+                \"format\": \"+\".join(filtered(\"format\")),\n+                \"format_id\": \"+\".join(filtered(\"format_id\")),\n+                \"ext\": output_ext,\n+                \"protocol\": \"+\".join(map(determine_protocol, formats_info)),\n+                \"language\": \"+\".join(orderedSet(filtered(\"language\"))) or None,\n+                \"format_note\": \"+\".join(orderedSet(filtered(\"format_note\"))) or None,\n+                \"filesize_approx\": sum(filtered(\"filesize\", \"filesize_approx\")) or None,\n+                \"tbr\": sum(filtered(\"tbr\", \"vbr\", \"abr\")),\n             }\n \n             if the_only_video:\n-                new_dict.update({\n-                    'width': the_only_video.get('width'),\n-                    'height': the_only_video.get('height'),\n-                    'resolution': the_only_video.get('resolution') or self.format_resolution(the_only_video),\n-                    'fps': the_only_video.get('fps'),\n-                    'dynamic_range': the_only_video.get('dynamic_range'),\n-                    'vcodec': the_only_video.get('vcodec'),\n-                    'vbr': the_only_video.get('vbr'),\n-                    'stretched_ratio': the_only_video.get('stretched_ratio'),\n-                    'aspect_ratio': the_only_video.get('aspect_ratio'),\n-                })\n+                new_dict.update(\n+                    {\n+                        \"width\": the_only_video.get(\"width\"),\n+                        \"height\": the_only_video.get(\"height\"),\n+                        \"resolution\": the_only_video.get(\"resolution\")\n+                        or self.format_resolution(the_only_video),\n+                        \"fps\": the_only_video.get(\"fps\"),\n+                        \"dynamic_range\": the_only_video.get(\"dynamic_range\"),\n+                        \"vcodec\": the_only_video.get(\"vcodec\"),\n+                        \"vbr\": the_only_video.get(\"vbr\"),\n+                        \"stretched_ratio\": the_only_video.get(\"stretched_ratio\"),\n+                        \"aspect_ratio\": the_only_video.get(\"aspect_ratio\"),\n+                    }\n+                )\n \n             if the_only_audio:\n-                new_dict.update({\n-                    'acodec': the_only_audio.get('acodec'),\n-                    'abr': the_only_audio.get('abr'),\n-                    'asr': the_only_audio.get('asr'),\n-                    'audio_channels': the_only_audio.get('audio_channels')\n-                })\n+                new_dict.update(\n+                    {\n+                        \"acodec\": the_only_audio.get(\"acodec\"),\n+                        \"abr\": the_only_audio.get(\"abr\"),\n+                        \"asr\": the_only_audio.get(\"asr\"),\n+                        \"audio_channels\": the_only_audio.get(\"audio_channels\"),\n+                    }\n+                )\n \n             return new_dict\n \n         def _check_formats(formats):\n-            if self.params.get('check_formats') == 'selected':\n+            if self.params.get(\"check_formats\") == \"selected\":\n                 yield from self._check_formats(formats)\n                 return\n-            elif (self.params.get('check_formats') is not None\n-                    or self.params.get('allow_unplayable_formats')):\n+            elif self.params.get(\"check_formats\") is not None or self.params.get(\n+                \"allow_unplayable_formats\"\n+            ):\n                 yield from formats\n                 return\n \n             for f in formats:\n-                if f.get('has_drm') or f.get('__needs_testing'):\n+                if f.get(\"has_drm\") or f.get(\"__needs_testing\"):\n                     yield from self._check_formats([f])\n                 else:\n                     yield f\n@@ -2370,6 +2911,7 @@ def _build_selector_function(selector):\n                 def selector_function(ctx):\n                     for f in fs:\n                         yield from f(ctx)\n+\n                 return selector_function\n \n             elif selector.type == GROUP:  # ()\n@@ -2386,23 +2928,33 @@ def selector_function(ctx):\n                     return []\n \n             elif selector.type == MERGE:  # +\n-                selector_1, selector_2 = map(_build_selector_function, selector.selector)\n+                selector_1, selector_2 = map(\n+                    _build_selector_function, selector.selector\n+                )\n \n                 def selector_function(ctx):\n                     for pair in itertools.product(selector_1(ctx), selector_2(ctx)):\n                         yield _merge(pair)\n \n             elif selector.type == SINGLE:  # atom\n-                format_spec = selector.selector or 'best'\n+                format_spec = selector.selector or \"best\"\n \n                 # TODO: Add allvideo, allaudio etc by generalizing the code with best/worst selector\n-                if format_spec == 'all':\n+                if format_spec == \"all\":\n+\n                     def selector_function(ctx):\n-                        yield from _check_formats(ctx['formats'][::-1])\n-                elif format_spec == 'mergeall':\n+                        yield from _check_formats(ctx[\"formats\"][::-1])\n+                elif format_spec == \"mergeall\":\n+\n                     def selector_function(ctx):\n-                        formats = list(_check_formats(\n-                            f for f in ctx['formats'] if f.get('vcodec') != 'none' or f.get('acodec') != 'none'))\n+                        formats = list(\n+                            _check_formats(\n+                                f\n+                                for f in ctx[\"formats\"]\n+                                if f.get(\"vcodec\") != \"none\"\n+                                or f.get(\"acodec\") != \"none\"\n+                            )\n+                        )\n                         if not formats:\n                             return\n                         merged_format = formats[-1]\n@@ -2411,52 +2963,87 @@ def selector_function(ctx):\n                         yield merged_format\n \n                 else:\n-                    format_fallback, seperate_fallback, format_reverse, format_idx = False, None, True, 1\n+                    format_fallback, seperate_fallback, format_reverse, format_idx = (\n+                        False,\n+                        None,\n+                        True,\n+                        1,\n+                    )\n                     mobj = re.match(\n-                        r'(?P<bw>best|worst|b|w)(?P<type>video|audio|v|a)?(?P<mod>\\*)?(?:\\.(?P<n>[1-9]\\d*))?$',\n-                        format_spec)\n+                        r\"(?P<bw>best|worst|b|w)(?P<type>video|audio|v|a)?(?P<mod>\\*)?(?:\\.(?P<n>[1-9]\\d*))?$\",\n+                        format_spec,\n+                    )\n                     if mobj is not None:\n-                        format_idx = int_or_none(mobj.group('n'), default=1)\n-                        format_reverse = mobj.group('bw')[0] == 'b'\n-                        format_type = (mobj.group('type') or [None])[0]\n-                        not_format_type = {'v': 'a', 'a': 'v'}.get(format_type)\n-                        format_modified = mobj.group('mod') is not None\n-\n-                        format_fallback = not format_type and not format_modified  # for b, w\n+                        format_idx = int_or_none(mobj.group(\"n\"), default=1)\n+                        format_reverse = mobj.group(\"bw\")[0] == \"b\"\n+                        format_type = (mobj.group(\"type\") or [None])[0]\n+                        not_format_type = {\"v\": \"a\", \"a\": \"v\"}.get(format_type)\n+                        format_modified = mobj.group(\"mod\") is not None\n+\n+                        format_fallback = (\n+                            not format_type and not format_modified\n+                        )  # for b, w\n                         _filter_f = (\n-                            (lambda f: f.get('%scodec' % format_type) != 'none')\n+                            (lambda f: f.get(\"%scodec\" % format_type) != \"none\")\n                             if format_type and format_modified  # bv*, ba*, wv*, wa*\n-                            else (lambda f: f.get('%scodec' % not_format_type) == 'none')\n+                            else (\n+                                lambda f: f.get(\"%scodec\" % not_format_type) == \"none\"\n+                            )\n                             if format_type  # bv, ba, wv, wa\n-                            else (lambda f: f.get('vcodec') != 'none' and f.get('acodec') != 'none')\n+                            else (\n+                                lambda f: f.get(\"vcodec\") != \"none\"\n+                                and f.get(\"acodec\") != \"none\"\n+                            )\n                             if not format_modified  # b, w\n-                            else lambda f: True)  # b*, w*\n+                            else lambda f: True\n+                        )  # b*, w*\n                         filter_f = lambda f: _filter_f(f) and (\n-                            f.get('vcodec') != 'none' or f.get('acodec') != 'none')\n+                            f.get(\"vcodec\") != \"none\" or f.get(\"acodec\") != \"none\"\n+                        )\n                     else:\n-                        if format_spec in self._format_selection_exts['audio']:\n-                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') != 'none'\n-                        elif format_spec in self._format_selection_exts['video']:\n-                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') != 'none' and f.get('vcodec') != 'none'\n-                            seperate_fallback = lambda f: f.get('ext') == format_spec and f.get('vcodec') != 'none'\n-                        elif format_spec in self._format_selection_exts['storyboards']:\n-                            filter_f = lambda f: f.get('ext') == format_spec and f.get('acodec') == 'none' and f.get('vcodec') == 'none'\n+                        if format_spec in self._format_selection_exts[\"audio\"]:\n+                            filter_f = (\n+                                lambda f: f.get(\"ext\") == format_spec\n+                                and f.get(\"acodec\") != \"none\"\n+                            )\n+                        elif format_spec in self._format_selection_exts[\"video\"]:\n+                            filter_f = (\n+                                lambda f: f.get(\"ext\") == format_spec\n+                                and f.get(\"acodec\") != \"none\"\n+                                and f.get(\"vcodec\") != \"none\"\n+                            )\n+                            seperate_fallback = (\n+                                lambda f: f.get(\"ext\") == format_spec\n+                                and f.get(\"vcodec\") != \"none\"\n+                            )\n+                        elif format_spec in self._format_selection_exts[\"storyboards\"]:\n+                            filter_f = (\n+                                lambda f: f.get(\"ext\") == format_spec\n+                                and f.get(\"acodec\") == \"none\"\n+                                and f.get(\"vcodec\") == \"none\"\n+                            )\n                         else:\n-                            filter_f = lambda f: f.get('format_id') == format_spec  # id\n+                            filter_f = lambda f: f.get(\"format_id\") == format_spec  # id\n \n                     def selector_function(ctx):\n-                        formats = list(ctx['formats'])\n-                        matches = list(filter(filter_f, formats)) if filter_f is not None else formats\n+                        formats = list(ctx[\"formats\"])\n+                        matches = (\n+                            list(filter(filter_f, formats))\n+                            if filter_f is not None\n+                            else formats\n+                        )\n                         if not matches:\n-                            if format_fallback and ctx['incomplete_formats']:\n+                            if format_fallback and ctx[\"incomplete_formats\"]:\n                                 # for extractors with incomplete formats (audio only (soundcloud)\n                                 # or video only (imgur)) best/worst will fallback to\n                                 # best/worst {video,audio}-only format\n                                 matches = formats\n-                            elif seperate_fallback and not ctx['has_merged_format']:\n+                            elif seperate_fallback and not ctx[\"has_merged_format\"]:\n                                 # for compatibility with youtube-dl when there is no pre-merged format\n                                 matches = list(filter(seperate_fallback, formats))\n-                        matches = LazyList(_check_formats(matches[::-1 if format_reverse else 1]))\n+                        matches = LazyList(\n+                            _check_formats(matches[:: -1 if format_reverse else 1])\n+                        )\n                         try:\n                             yield matches[format_idx - 1]\n                         except LazyList.IndexError:\n@@ -2467,22 +3054,30 @@ def selector_function(ctx):\n             def final_selector(ctx):\n                 ctx_copy = dict(ctx)\n                 for _filter in filters:\n-                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n+                    ctx_copy[\"formats\"] = list(filter(_filter, ctx_copy[\"formats\"]))\n                 return selector_function(ctx_copy)\n+\n             return final_selector\n \n         # HACK: Python 3.12 changed the underlying parser, rendering '7_a' invalid\n         #       Prefix numbers with random letters to avoid it being classified as a number\n         #       See: https://github.com/yt-dlp/yt-dlp/pulls/8797\n         # TODO: Implement parser not reliant on tokenize.tokenize\n-        prefix = ''.join(random.choices(string.ascii_letters, k=32))\n-        stream = io.BytesIO(re.sub(r'\\d[_\\d]*', rf'{prefix}\\g<0>', format_spec).encode())\n+        prefix = \"\".join(random.choices(string.ascii_letters, k=32))\n+        stream = io.BytesIO(\n+            re.sub(r\"\\d[_\\d]*\", rf\"{prefix}\\g<0>\", format_spec).encode()\n+        )\n         try:\n-            tokens = list(_remove_unused_ops(\n-                token._replace(string=token.string.replace(prefix, ''))\n-                for token in tokenize.tokenize(stream.readline)))\n+            tokens = list(\n+                _remove_unused_ops(\n+                    token._replace(string=token.string.replace(prefix, \"\"))\n+                    for token in tokenize.tokenize(stream.readline)\n+                )\n+            )\n         except tokenize.TokenError:\n-            raise syntax_error('Missing closing/opening brackets or parenthesis', (0, len(format_spec)))\n+            raise syntax_error(\n+                \"Missing closing/opening brackets or parenthesis\", (0, len(format_spec))\n+            )\n \n         class TokenIterator:\n             def __init__(self, tokens):\n@@ -2508,116 +3103,135 @@ def restore_last_token(self):\n         return _build_selector_function(parsed_selector)\n \n     def _calc_headers(self, info_dict, load_cookies=False):\n-        res = HTTPHeaderDict(self.params['http_headers'], info_dict.get('http_headers'))\n+        res = HTTPHeaderDict(self.params[\"http_headers\"], info_dict.get(\"http_headers\"))\n         clean_headers(res)\n \n         if load_cookies:  # For --load-info-json\n-            self._load_cookies(res.get('Cookie'), autoscope=info_dict['url'])  # compat\n-            self._load_cookies(info_dict.get('cookies'), autoscope=False)\n+            self._load_cookies(res.get(\"Cookie\"), autoscope=info_dict[\"url\"])  # compat\n+            self._load_cookies(info_dict.get(\"cookies\"), autoscope=False)\n         # The `Cookie` header is removed to prevent leaks and unscoped cookies.\n         # See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj\n-        res.pop('Cookie', None)\n-        cookies = self.cookiejar.get_cookies_for_url(info_dict['url'])\n+        res.pop(\"Cookie\", None)\n+        cookies = self.cookiejar.get_cookies_for_url(info_dict[\"url\"])\n         if cookies:\n             encoder = LenientSimpleCookie()\n             values = []\n             for cookie in cookies:\n                 _, value = encoder.value_encode(cookie.value)\n-                values.append(f'{cookie.name}={value}')\n+                values.append(f\"{cookie.name}={value}\")\n                 if cookie.domain:\n-                    values.append(f'Domain={cookie.domain}')\n+                    values.append(f\"Domain={cookie.domain}\")\n                 if cookie.path:\n-                    values.append(f'Path={cookie.path}')\n+                    values.append(f\"Path={cookie.path}\")\n                 if cookie.secure:\n-                    values.append('Secure')\n+                    values.append(\"Secure\")\n                 if cookie.expires:\n-                    values.append(f'Expires={cookie.expires}')\n+                    values.append(f\"Expires={cookie.expires}\")\n                 if cookie.version:\n-                    values.append(f'Version={cookie.version}')\n-            info_dict['cookies'] = '; '.join(values)\n+                    values.append(f\"Version={cookie.version}\")\n+            info_dict[\"cookies\"] = \"; \".join(values)\n \n-        if 'X-Forwarded-For' not in res:\n-            x_forwarded_for_ip = info_dict.get('__x_forwarded_for_ip')\n+        if \"X-Forwarded-For\" not in res:\n+            x_forwarded_for_ip = info_dict.get(\"__x_forwarded_for_ip\")\n             if x_forwarded_for_ip:\n-                res['X-Forwarded-For'] = x_forwarded_for_ip\n+                res[\"X-Forwarded-For\"] = x_forwarded_for_ip\n \n         return res\n \n     def _calc_cookies(self, url):\n-        self.deprecation_warning('\"YoutubeDL._calc_cookies\" is deprecated and may be removed in a future version')\n+        self.deprecation_warning(\n+            '\"YoutubeDL._calc_cookies\" is deprecated and may be removed in a future version'\n+        )\n         return self.cookiejar.get_cookie_header(url)\n \n     def _sort_thumbnails(self, thumbnails):\n-        thumbnails.sort(key=lambda t: (\n-            t.get('preference') if t.get('preference') is not None else -1,\n-            t.get('width') if t.get('width') is not None else -1,\n-            t.get('height') if t.get('height') is not None else -1,\n-            t.get('id') if t.get('id') is not None else '',\n-            t.get('url')))\n+        thumbnails.sort(\n+            key=lambda t: (\n+                t.get(\"preference\") if t.get(\"preference\") is not None else -1,\n+                t.get(\"width\") if t.get(\"width\") is not None else -1,\n+                t.get(\"height\") if t.get(\"height\") is not None else -1,\n+                t.get(\"id\") if t.get(\"id\") is not None else \"\",\n+                t.get(\"url\"),\n+            )\n+        )\n \n     def _sanitize_thumbnails(self, info_dict):\n-        thumbnails = info_dict.get('thumbnails')\n+        thumbnails = info_dict.get(\"thumbnails\")\n         if thumbnails is None:\n-            thumbnail = info_dict.get('thumbnail')\n+            thumbnail = info_dict.get(\"thumbnail\")\n             if thumbnail:\n-                info_dict['thumbnails'] = thumbnails = [{'url': thumbnail}]\n+                info_dict[\"thumbnails\"] = thumbnails = [{\"url\": thumbnail}]\n         if not thumbnails:\n             return\n \n         def check_thumbnails(thumbnails):\n             for t in thumbnails:\n-                self.to_screen(f'[info] Testing thumbnail {t[\"id\"]}')\n+                self.to_screen(f\"[info] Testing thumbnail {t['id']}\")\n                 try:\n-                    self.urlopen(HEADRequest(t['url']))\n+                    self.urlopen(HEADRequest(t[\"url\"]))\n                 except network_exceptions as err:\n-                    self.to_screen(f'[info] Unable to connect to thumbnail {t[\"id\"]} URL {t[\"url\"]!r} - {err}. Skipping...')\n+                    self.to_screen(\n+                        f\"[info] Unable to connect to thumbnail {t['id']} URL {t['url']!r} - {err}. Skipping...\"\n+                    )\n                     continue\n                 yield t\n \n         self._sort_thumbnails(thumbnails)\n         for i, t in enumerate(thumbnails):\n-            if t.get('id') is None:\n-                t['id'] = '%d' % i\n-            if t.get('width') and t.get('height'):\n-                t['resolution'] = '%dx%d' % (t['width'], t['height'])\n-            t['url'] = sanitize_url(t['url'])\n-\n-        if self.params.get('check_formats') is True:\n-            info_dict['thumbnails'] = LazyList(check_thumbnails(thumbnails[::-1]), reverse=True)\n+            if t.get(\"id\") is None:\n+                t[\"id\"] = \"%d\" % i\n+            if t.get(\"width\") and t.get(\"height\"):\n+                t[\"resolution\"] = \"%dx%d\" % (t[\"width\"], t[\"height\"])\n+            t[\"url\"] = sanitize_url(t[\"url\"])\n+\n+        if self.params.get(\"check_formats\") is True:\n+            info_dict[\"thumbnails\"] = LazyList(\n+                check_thumbnails(thumbnails[::-1]), reverse=True\n+            )\n         else:\n-            info_dict['thumbnails'] = thumbnails\n+            info_dict[\"thumbnails\"] = thumbnails\n \n     def _fill_common_fields(self, info_dict, final=True):\n         # TODO: move sanitization here\n         if final:\n-            title = info_dict['fulltitle'] = info_dict.get('title')\n+            title = info_dict[\"fulltitle\"] = info_dict.get(\"title\")\n             if not title:\n-                if title == '':\n-                    self.write_debug('Extractor gave empty title. Creating a generic title')\n+                if title == \"\":\n+                    self.write_debug(\n+                        \"Extractor gave empty title. Creating a generic title\"\n+                    )\n                 else:\n-                    self.report_warning('Extractor failed to obtain \"title\". Creating a generic title instead')\n-                info_dict['title'] = f'{info_dict[\"extractor\"].replace(\":\", \"-\")} video #{info_dict[\"id\"]}'\n+                    self.report_warning(\n+                        'Extractor failed to obtain \"title\". Creating a generic title instead'\n+                    )\n+                info_dict[\"title\"] = (\n+                    f\"{info_dict['extractor'].replace(':', '-')} video #{info_dict['id']}\"\n+                )\n \n-        if info_dict.get('duration') is not None:\n-            info_dict['duration_string'] = formatSeconds(info_dict['duration'])\n+        if info_dict.get(\"duration\") is not None:\n+            info_dict[\"duration_string\"] = formatSeconds(info_dict[\"duration\"])\n \n         for ts_key, date_key in (\n-                ('timestamp', 'upload_date'),\n-                ('release_timestamp', 'release_date'),\n-                ('modified_timestamp', 'modified_date'),\n+            (\"timestamp\", \"upload_date\"),\n+            (\"release_timestamp\", \"release_date\"),\n+            (\"modified_timestamp\", \"modified_date\"),\n         ):\n             if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:\n                 # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n                 # see http://bugs.python.org/issue1646728)\n                 with contextlib.suppress(ValueError, OverflowError, OSError):\n-                    upload_date = datetime.datetime.fromtimestamp(info_dict[ts_key], datetime.timezone.utc)\n-                    info_dict[date_key] = upload_date.strftime('%Y%m%d')\n+                    upload_date = datetime.datetime.fromtimestamp(\n+                        info_dict[ts_key], datetime.timezone.utc\n+                    )\n+                    info_dict[date_key] = upload_date.strftime(\"%Y%m%d\")\n \n-        if not info_dict.get('release_year'):\n-            info_dict['release_year'] = traverse_obj(info_dict, ('release_date', {lambda x: int(x[:4])}))\n+        if not info_dict.get(\"release_year\"):\n+            info_dict[\"release_year\"] = traverse_obj(\n+                info_dict, (\"release_date\", {lambda x: int(x[:4])})\n+            )\n \n-        live_keys = ('is_live', 'was_live')\n-        live_status = info_dict.get('live_status')\n+        live_keys = (\"is_live\", \"was_live\")\n+        live_status = info_dict.get(\"live_status\")\n         if live_status is None:\n             for key in live_keys:\n                 if info_dict.get(key) is False:\n@@ -2626,50 +3240,65 @@ def _fill_common_fields(self, info_dict, final=True):\n                     live_status = key\n                 break\n             if all(info_dict.get(key) is False for key in live_keys):\n-                live_status = 'not_live'\n+                live_status = \"not_live\"\n         if live_status:\n-            info_dict['live_status'] = live_status\n+            info_dict[\"live_status\"] = live_status\n             for key in live_keys:\n                 if info_dict.get(key) is None:\n-                    info_dict[key] = (live_status == key)\n-        if live_status == 'post_live':\n-            info_dict['was_live'] = True\n+                    info_dict[key] = live_status == key\n+        if live_status == \"post_live\":\n+            info_dict[\"was_live\"] = True\n \n         # Auto generate title fields corresponding to the *_number fields when missing\n         # in order to always have clean titles. This is very common for TV series.\n-        for field in ('chapter', 'season', 'episode'):\n-            if final and info_dict.get('%s_number' % field) is not None and not info_dict.get(field):\n-                info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])\n+        for field in (\"chapter\", \"season\", \"episode\"):\n+            if (\n+                final\n+                and info_dict.get(\"%s_number\" % field) is not None\n+                and not info_dict.get(field)\n+            ):\n+                info_dict[field] = \"%s %d\" % (\n+                    field.capitalize(),\n+                    info_dict[\"%s_number\" % field],\n+                )\n \n     def _raise_pending_errors(self, info):\n-        err = info.pop('__pending_error', None)\n+        err = info.pop(\"__pending_error\", None)\n         if err:\n             self.report_error(err, tb=False)\n \n     def sort_formats(self, info_dict):\n         formats = self._get_formats(info_dict)\n-        formats.sort(key=FormatSorter(\n-            self, info_dict.get('_format_sort_fields') or []).calculate_preference)\n+        formats.sort(\n+            key=FormatSorter(\n+                self, info_dict.get(\"_format_sort_fields\") or []\n+            ).calculate_preference\n+        )\n \n     def process_video_result(self, info_dict, download=True):\n-        assert info_dict.get('_type', 'video') == 'video'\n+        assert info_dict.get(\"_type\", \"video\") == \"video\"\n         self._num_videos += 1\n \n-        if 'id' not in info_dict:\n-            raise ExtractorError('Missing \"id\" field in extractor result', ie=info_dict['extractor'])\n-        elif not info_dict.get('id'):\n-            raise ExtractorError('Extractor failed to obtain \"id\"', ie=info_dict['extractor'])\n+        if \"id\" not in info_dict:\n+            raise ExtractorError(\n+                'Missing \"id\" field in extractor result', ie=info_dict[\"extractor\"]\n+            )\n+        elif not info_dict.get(\"id\"):\n+            raise ExtractorError(\n+                'Extractor failed to obtain \"id\"', ie=info_dict[\"extractor\"]\n+            )\n \n         def report_force_conversion(field, field_not, conversion):\n             self.report_warning(\n                 '\"%s\" field is not %s - forcing %s conversion, there is an error in extractor'\n-                % (field, field_not, conversion))\n+                % (field, field_not, conversion)\n+            )\n \n         def sanitize_string_field(info, string_field):\n             field = info.get(string_field)\n             if field is None or isinstance(field, str):\n                 return\n-            report_force_conversion(string_field, 'a string', 'string')\n+            report_force_conversion(string_field, \"a string\", \"string\")\n             info[string_field] = str(field)\n \n         def sanitize_numeric_fields(info):\n@@ -2677,101 +3306,131 @@ def sanitize_numeric_fields(info):\n                 field = info.get(numeric_field)\n                 if field is None or isinstance(field, (int, float)):\n                     continue\n-                report_force_conversion(numeric_field, 'numeric', 'int')\n+                report_force_conversion(numeric_field, \"numeric\", \"int\")\n                 info[numeric_field] = int_or_none(field)\n \n-        sanitize_string_field(info_dict, 'id')\n+        sanitize_string_field(info_dict, \"id\")\n         sanitize_numeric_fields(info_dict)\n-        if info_dict.get('section_end') and info_dict.get('section_start') is not None:\n-            info_dict['duration'] = round(info_dict['section_end'] - info_dict['section_start'], 3)\n-        if (info_dict.get('duration') or 0) <= 0 and info_dict.pop('duration', None):\n-            self.report_warning('\"duration\" field is negative, there is an error in extractor')\n-\n-        chapters = info_dict.get('chapters') or []\n-        if chapters and chapters[0].get('start_time'):\n-            chapters.insert(0, {'start_time': 0})\n-\n-        dummy_chapter = {'end_time': 0, 'start_time': info_dict.get('duration')}\n-        for idx, (prev, current, next_) in enumerate(zip(\n-                (dummy_chapter, *chapters), chapters, (*chapters[1:], dummy_chapter)), 1):\n-            if current.get('start_time') is None:\n-                current['start_time'] = prev.get('end_time')\n-            if not current.get('end_time'):\n-                current['end_time'] = next_.get('start_time')\n-            if not current.get('title'):\n-                current['title'] = f'<Untitled Chapter {idx}>'\n-\n-        if 'playlist' not in info_dict:\n+        if info_dict.get(\"section_end\") and info_dict.get(\"section_start\") is not None:\n+            info_dict[\"duration\"] = round(\n+                info_dict[\"section_end\"] - info_dict[\"section_start\"], 3\n+            )\n+        if (info_dict.get(\"duration\") or 0) <= 0 and info_dict.pop(\"duration\", None):\n+            self.report_warning(\n+                '\"duration\" field is negative, there is an error in extractor'\n+            )\n+\n+        chapters = info_dict.get(\"chapters\") or []\n+        if chapters and chapters[0].get(\"start_time\"):\n+            chapters.insert(0, {\"start_time\": 0})\n+\n+        dummy_chapter = {\"end_time\": 0, \"start_time\": info_dict.get(\"duration\")}\n+        for idx, (prev, current, next_) in enumerate(\n+            zip((dummy_chapter, *chapters), chapters, (*chapters[1:], dummy_chapter)), 1\n+        ):\n+            if current.get(\"start_time\") is None:\n+                current[\"start_time\"] = prev.get(\"end_time\")\n+            if not current.get(\"end_time\"):\n+                current[\"end_time\"] = next_.get(\"start_time\")\n+            if not current.get(\"title\"):\n+                current[\"title\"] = f\"<Untitled Chapter {idx}>\"\n+\n+        if \"playlist\" not in info_dict:\n             # It isn't part of a playlist\n-            info_dict['playlist'] = None\n-            info_dict['playlist_index'] = None\n+            info_dict[\"playlist\"] = None\n+            info_dict[\"playlist_index\"] = None\n \n         self._sanitize_thumbnails(info_dict)\n \n-        thumbnail = info_dict.get('thumbnail')\n-        thumbnails = info_dict.get('thumbnails')\n+        thumbnail = info_dict.get(\"thumbnail\")\n+        thumbnails = info_dict.get(\"thumbnails\")\n         if thumbnail:\n-            info_dict['thumbnail'] = sanitize_url(thumbnail)\n+            info_dict[\"thumbnail\"] = sanitize_url(thumbnail)\n         elif thumbnails:\n-            info_dict['thumbnail'] = thumbnails[-1]['url']\n+            info_dict[\"thumbnail\"] = thumbnails[-1][\"url\"]\n \n-        if info_dict.get('display_id') is None and 'id' in info_dict:\n-            info_dict['display_id'] = info_dict['id']\n+        if info_dict.get(\"display_id\") is None and \"id\" in info_dict:\n+            info_dict[\"display_id\"] = info_dict[\"id\"]\n \n         self._fill_common_fields(info_dict)\n \n-        for cc_kind in ('subtitles', 'automatic_captions'):\n+        for cc_kind in (\"subtitles\", \"automatic_captions\"):\n             cc = info_dict.get(cc_kind)\n             if cc:\n                 for _, subtitle in cc.items():\n                     for subtitle_format in subtitle:\n-                        if subtitle_format.get('url'):\n-                            subtitle_format['url'] = sanitize_url(subtitle_format['url'])\n-                        if subtitle_format.get('ext') is None:\n-                            subtitle_format['ext'] = determine_ext(subtitle_format['url']).lower()\n-\n-        automatic_captions = info_dict.get('automatic_captions')\n-        subtitles = info_dict.get('subtitles')\n-\n-        info_dict['requested_subtitles'] = self.process_subtitles(\n-            info_dict['id'], subtitles, automatic_captions)\n+                        if subtitle_format.get(\"url\"):\n+                            subtitle_format[\"url\"] = sanitize_url(\n+                                subtitle_format[\"url\"]\n+                            )\n+                        if subtitle_format.get(\"ext\") is None:\n+                            subtitle_format[\"ext\"] = determine_ext(\n+                                subtitle_format[\"url\"]\n+                            ).lower()\n+\n+        automatic_captions = info_dict.get(\"automatic_captions\")\n+        subtitles = info_dict.get(\"subtitles\")\n+\n+        info_dict[\"requested_subtitles\"] = self.process_subtitles(\n+            info_dict[\"id\"], subtitles, automatic_captions\n+        )\n \n         formats = self._get_formats(info_dict)\n \n         # Backward compatibility with InfoExtractor._sort_formats\n-        field_preference = (formats or [{}])[0].pop('__sort_fields', None)\n+        field_preference = (formats or [{}])[0].pop(\"__sort_fields\", None)\n         if field_preference:\n-            info_dict['_format_sort_fields'] = field_preference\n+            info_dict[\"_format_sort_fields\"] = field_preference\n \n-        info_dict['_has_drm'] = any(  # or None ensures --clean-infojson removes it\n-            f.get('has_drm') and f['has_drm'] != 'maybe' for f in formats) or None\n-        if not self.params.get('allow_unplayable_formats'):\n-            formats = [f for f in formats if not f.get('has_drm') or f['has_drm'] == 'maybe']\n+        info_dict[\"_has_drm\"] = (\n+            any(  # or None ensures --clean-infojson removes it\n+                f.get(\"has_drm\") and f[\"has_drm\"] != \"maybe\" for f in formats\n+            )\n+            or None\n+        )\n+        if not self.params.get(\"allow_unplayable_formats\"):\n+            formats = [\n+                f for f in formats if not f.get(\"has_drm\") or f[\"has_drm\"] == \"maybe\"\n+            ]\n \n-        if formats and all(f.get('acodec') == f.get('vcodec') == 'none' for f in formats):\n+        if formats and all(\n+            f.get(\"acodec\") == f.get(\"vcodec\") == \"none\" for f in formats\n+        ):\n             self.report_warning(\n-                f'{\"This video is DRM protected and \" if info_dict[\"_has_drm\"] else \"\"}'\n-                'only images are available for download. Use --list-formats to see them'.capitalize())\n+                f\"{'This video is DRM protected and ' if info_dict['_has_drm'] else ''}\"\n+                \"only images are available for download. Use --list-formats to see them\".capitalize()\n+            )\n \n-        get_from_start = not info_dict.get('is_live') or bool(self.params.get('live_from_start'))\n+        get_from_start = not info_dict.get(\"is_live\") or bool(\n+            self.params.get(\"live_from_start\")\n+        )\n         if not get_from_start:\n-            info_dict['title'] += ' ' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n-        if info_dict.get('is_live') and formats:\n-            formats = [f for f in formats if bool(f.get('is_from_start')) == get_from_start]\n+            info_dict[\"title\"] += \" \" + datetime.datetime.now().strftime(\n+                \"%Y-%m-%d %H:%M\"\n+            )\n+        if info_dict.get(\"is_live\") and formats:\n+            formats = [\n+                f for f in formats if bool(f.get(\"is_from_start\")) == get_from_start\n+            ]\n             if get_from_start and not formats:\n-                self.raise_no_formats(info_dict, msg=(\n-                    '--live-from-start is passed, but there are no formats that can be downloaded from the start. '\n-                    'If you want to download from the current time, use --no-live-from-start'))\n+                self.raise_no_formats(\n+                    info_dict,\n+                    msg=(\n+                        \"--live-from-start is passed, but there are no formats that can be downloaded from the start. \"\n+                        \"If you want to download from the current time, use --no-live-from-start\"\n+                    ),\n+                )\n \n         def is_wellformed(f):\n-            url = f.get('url')\n+            url = f.get(\"url\")\n             if not url:\n                 self.report_warning(\n                     '\"url\" field is missing or empty - skipping format, '\n-                    'there is an error in extractor')\n+                    \"there is an error in extractor\"\n+                )\n                 return False\n             if isinstance(url, bytes):\n-                sanitize_string_field(f, 'url')\n+                sanitize_string_field(f, \"url\")\n             return True\n \n         # Filter out malformed formats for better extraction robustness\n@@ -2781,49 +3440,64 @@ def is_wellformed(f):\n             self.raise_no_formats(info_dict)\n \n         for format in formats:\n-            sanitize_string_field(format, 'format_id')\n+            sanitize_string_field(format, \"format_id\")\n             sanitize_numeric_fields(format)\n-            format['url'] = sanitize_url(format['url'])\n-            if format.get('ext') is None:\n-                format['ext'] = determine_ext(format['url']).lower()\n-            if format.get('protocol') is None:\n-                format['protocol'] = determine_protocol(format)\n-            if format.get('resolution') is None:\n-                format['resolution'] = self.format_resolution(format, default=None)\n-            if format.get('dynamic_range') is None and format.get('vcodec') != 'none':\n-                format['dynamic_range'] = 'SDR'\n-            if format.get('aspect_ratio') is None:\n-                format['aspect_ratio'] = try_call(lambda: round(format['width'] / format['height'], 2))\n+            format[\"url\"] = sanitize_url(format[\"url\"])\n+            if format.get(\"ext\") is None:\n+                format[\"ext\"] = determine_ext(format[\"url\"]).lower()\n+            if format.get(\"protocol\") is None:\n+                format[\"protocol\"] = determine_protocol(format)\n+            if format.get(\"resolution\") is None:\n+                format[\"resolution\"] = self.format_resolution(format, default=None)\n+            if format.get(\"dynamic_range\") is None and format.get(\"vcodec\") != \"none\":\n+                format[\"dynamic_range\"] = \"SDR\"\n+            if format.get(\"aspect_ratio\") is None:\n+                format[\"aspect_ratio\"] = try_call(\n+                    lambda: round(format[\"width\"] / format[\"height\"], 2)\n+                )\n             # For fragmented formats, \"tbr\" is often max bitrate and not average\n-            if (('manifest-filesize-approx' in self.params['compat_opts'] or not format.get('manifest_url'))\n-                    and info_dict.get('duration') and format.get('tbr')\n-                    and not format.get('filesize') and not format.get('filesize_approx')):\n-                format['filesize_approx'] = int(info_dict['duration'] * format['tbr'] * (1024 / 8))\n-            format['http_headers'] = self._calc_headers(collections.ChainMap(format, info_dict), load_cookies=True)\n+            if (\n+                (\n+                    \"manifest-filesize-approx\" in self.params[\"compat_opts\"]\n+                    or not format.get(\"manifest_url\")\n+                )\n+                and info_dict.get(\"duration\")\n+                and format.get(\"tbr\")\n+                and not format.get(\"filesize\")\n+                and not format.get(\"filesize_approx\")\n+            ):\n+                format[\"filesize_approx\"] = int(\n+                    info_dict[\"duration\"] * format[\"tbr\"] * (1024 / 8)\n+                )\n+            format[\"http_headers\"] = self._calc_headers(\n+                collections.ChainMap(format, info_dict), load_cookies=True\n+            )\n \n         # Safeguard against old/insecure infojson when using --load-info-json\n-        if info_dict.get('http_headers'):\n-            info_dict['http_headers'] = HTTPHeaderDict(info_dict['http_headers'])\n-            info_dict['http_headers'].pop('Cookie', None)\n+        if info_dict.get(\"http_headers\"):\n+            info_dict[\"http_headers\"] = HTTPHeaderDict(info_dict[\"http_headers\"])\n+            info_dict[\"http_headers\"].pop(\"Cookie\", None)\n \n         # This is copied to http_headers by the above _calc_headers and can now be removed\n-        if '__x_forwarded_for_ip' in info_dict:\n-            del info_dict['__x_forwarded_for_ip']\n+        if \"__x_forwarded_for_ip\" in info_dict:\n+            del info_dict[\"__x_forwarded_for_ip\"]\n \n-        self.sort_formats({\n-            'formats': formats,\n-            '_format_sort_fields': info_dict.get('_format_sort_fields')\n-        })\n+        self.sort_formats(\n+            {\n+                \"formats\": formats,\n+                \"_format_sort_fields\": info_dict.get(\"_format_sort_fields\"),\n+            }\n+        )\n \n         # Sanitize and group by format_id\n         formats_dict = {}\n         for i, format in enumerate(formats):\n-            if not format.get('format_id'):\n-                format['format_id'] = str(i)\n+            if not format.get(\"format_id\"):\n+                format[\"format_id\"] = str(i)\n             else:\n                 # Sanitize format_id from characters used in format selector expression\n-                format['format_id'] = re.sub(r'[\\s,/+\\[\\]()]', '_', format['format_id'])\n-            formats_dict.setdefault(format['format_id'], []).append(format)\n+                format[\"format_id\"] = re.sub(r\"[\\s,/+\\[\\]()]\", \"_\", format[\"format_id\"])\n+            formats_dict.setdefault(format[\"format_id\"], []).append(format)\n \n         # Make sure all formats have unique format_id\n         common_exts = set(itertools.chain(*self._format_selection_exts.values()))\n@@ -2831,20 +3505,23 @@ def is_wellformed(f):\n             ambigious_id = len(ambiguous_formats) > 1\n             for i, format in enumerate(ambiguous_formats):\n                 if ambigious_id:\n-                    format['format_id'] = '%s-%d' % (format_id, i)\n+                    format[\"format_id\"] = \"%s-%d\" % (format_id, i)\n                 # Ensure there is no conflict between id and ext in format selection\n                 # See https://github.com/yt-dlp/yt-dlp/issues/1282\n-                if format['format_id'] != format['ext'] and format['format_id'] in common_exts:\n-                    format['format_id'] = 'f%s' % format['format_id']\n+                if (\n+                    format[\"format_id\"] != format[\"ext\"]\n+                    and format[\"format_id\"] in common_exts\n+                ):\n+                    format[\"format_id\"] = \"f%s\" % format[\"format_id\"]\n \n-                if format.get('format') is None:\n-                    format['format'] = '{id} - {res}{note}'.format(\n-                        id=format['format_id'],\n+                if format.get(\"format\") is None:\n+                    format[\"format\"] = \"{id} - {res}{note}\".format(\n+                        id=format[\"format_id\"],\n                         res=self.format_resolution(format),\n-                        note=format_field(format, 'format_note', ' (%s)'),\n+                        note=format_field(format, \"format_note\", \" (%s)\"),\n                     )\n \n-        if self.params.get('check_formats') is True:\n+        if self.params.get(\"check_formats\") is True:\n             formats = LazyList(self._check_formats(formats[::-1]), reverse=True)\n \n         if not formats or formats[0] is not info_dict:\n@@ -2852,7 +3529,7 @@ def is_wellformed(f):\n             # otherwise we end up with a circular reference, the first (and unique)\n             # element in the 'formats' field in info_dict is info_dict itself,\n             # which can't be exported to json\n-            info_dict['formats'] = formats\n+            info_dict[\"formats\"] = formats\n \n         info_dict, _ = self.pre_process(info_dict)\n \n@@ -2860,21 +3537,22 @@ def is_wellformed(f):\n             return info_dict\n \n         self.post_extract(info_dict)\n-        info_dict, _ = self.pre_process(info_dict, 'after_filter')\n+        info_dict, _ = self.pre_process(info_dict, \"after_filter\")\n \n         # The pre-processors may have modified the formats\n         formats = self._get_formats(info_dict)\n \n-        list_only = self.params.get('simulate') == 'list_only'\n-        interactive_format_selection = not list_only and self.format_selector == '-'\n-        if self.params.get('list_thumbnails'):\n+        list_only = self.params.get(\"simulate\") == \"list_only\"\n+        interactive_format_selection = not list_only and self.format_selector == \"-\"\n+        if self.params.get(\"list_thumbnails\"):\n             self.list_thumbnails(info_dict)\n-        if self.params.get('listsubtitles'):\n-            if 'automatic_captions' in info_dict:\n+        if self.params.get(\"listsubtitles\"):\n+            if \"automatic_captions\" in info_dict:\n                 self.list_subtitles(\n-                    info_dict['id'], automatic_captions, 'automatic captions')\n-            self.list_subtitles(info_dict['id'], subtitles, 'subtitles')\n-        if self.params.get('listformats') or interactive_format_selection:\n+                    info_dict[\"id\"], automatic_captions, \"automatic captions\"\n+                )\n+            self.list_subtitles(info_dict[\"id\"], subtitles, \"subtitles\")\n+        if self.params.get(\"listformats\") or interactive_format_selection:\n             self.list_formats(info_dict)\n         if list_only:\n             # Without this printing, -F --print-json will not work\n@@ -2884,69 +3562,110 @@ def is_wellformed(f):\n         format_selector = self.format_selector\n         while True:\n             if interactive_format_selection:\n-                req_format = input(self._format_screen('\\nEnter format selector ', self.Styles.EMPHASIS)\n-                                   + '(Press ENTER for default, or Ctrl+C to quit)'\n-                                   + self._format_screen(': ', self.Styles.EMPHASIS))\n+                req_format = input(\n+                    self._format_screen(\n+                        \"\\nEnter format selector \", self.Styles.EMPHASIS\n+                    )\n+                    + \"(Press ENTER for default, or Ctrl+C to quit)\"\n+                    + self._format_screen(\": \", self.Styles.EMPHASIS)\n+                )\n                 try:\n-                    format_selector = self.build_format_selector(req_format) if req_format else None\n+                    format_selector = (\n+                        self.build_format_selector(req_format) if req_format else None\n+                    )\n                 except SyntaxError as err:\n                     self.report_error(err, tb=False, is_error=False)\n                     continue\n \n             if format_selector is None:\n                 req_format = self._default_format_spec(info_dict, download=download)\n-                self.write_debug(f'Default format spec: {req_format}')\n+                self.write_debug(f\"Default format spec: {req_format}\")\n                 format_selector = self.build_format_selector(req_format)\n \n-            formats_to_download = list(format_selector({\n-                'formats': formats,\n-                'has_merged_format': any('none' not in (f.get('acodec'), f.get('vcodec')) for f in formats),\n-                'incomplete_formats': (all(f.get('vcodec') == 'none' for f in formats)  # No formats with video\n-                                       or all(f.get('acodec') == 'none' for f in formats)),  # OR, No formats with audio\n-            }))\n+            formats_to_download = list(\n+                format_selector(\n+                    {\n+                        \"formats\": formats,\n+                        \"has_merged_format\": any(\n+                            \"none\" not in (f.get(\"acodec\"), f.get(\"vcodec\"))\n+                            for f in formats\n+                        ),\n+                        \"incomplete_formats\": (\n+                            all(\n+                                f.get(\"vcodec\") == \"none\" for f in formats\n+                            )  # No formats with video\n+                            or all(f.get(\"acodec\") == \"none\" for f in formats)\n+                        ),  # OR, No formats with audio\n+                    }\n+                )\n+            )\n             if interactive_format_selection and not formats_to_download:\n-                self.report_error('Requested format is not available', tb=False, is_error=False)\n+                self.report_error(\n+                    \"Requested format is not available\", tb=False, is_error=False\n+                )\n                 continue\n             break\n \n         if not formats_to_download:\n-            if not self.params.get('ignore_no_formats_error'):\n+            if not self.params.get(\"ignore_no_formats_error\"):\n                 raise ExtractorError(\n-                    'Requested format is not available. Use --list-formats for a list of available formats',\n-                    expected=True, video_id=info_dict['id'], ie=info_dict['extractor'])\n-            self.report_warning('Requested format is not available')\n+                    \"Requested format is not available. Use --list-formats for a list of available formats\",\n+                    expected=True,\n+                    video_id=info_dict[\"id\"],\n+                    ie=info_dict[\"extractor\"],\n+                )\n+            self.report_warning(\"Requested format is not available\")\n             # Process what we can, even without any available formats.\n             formats_to_download = [{}]\n \n-        requested_ranges = tuple(self.params.get('download_ranges', lambda *_: [{}])(info_dict, self))\n+        requested_ranges = tuple(\n+            self.params.get(\"download_ranges\", lambda *_: [{}])(info_dict, self)\n+        )\n         best_format, downloaded_formats = formats_to_download[-1], []\n         if download:\n             if best_format and requested_ranges:\n+\n                 def to_screen(*msg):\n-                    self.to_screen(f'[info] {info_dict[\"id\"]}: {\" \".join(\", \".join(variadic(m)) for m in msg)}')\n+                    self.to_screen(\n+                        f\"[info] {info_dict['id']}: {' '.join(', '.join(variadic(m)) for m in msg)}\"\n+                    )\n \n-                to_screen(f'Downloading {len(formats_to_download)} format(s):',\n-                          (f['format_id'] for f in formats_to_download))\n-                if requested_ranges != ({}, ):\n-                    to_screen(f'Downloading {len(requested_ranges)} time ranges:',\n-                              (f'{c[\"start_time\"]:.1f}-{c[\"end_time\"]:.1f}' for c in requested_ranges))\n+                to_screen(\n+                    f\"Downloading {len(formats_to_download)} format(s):\",\n+                    (f[\"format_id\"] for f in formats_to_download),\n+                )\n+                if requested_ranges != ({},):\n+                    to_screen(\n+                        f\"Downloading {len(requested_ranges)} time ranges:\",\n+                        (\n+                            f\"{c['start_time']:.1f}-{c['end_time']:.1f}\"\n+                            for c in requested_ranges\n+                        ),\n+                    )\n             max_downloads_reached = False\n \n-            for fmt, chapter in itertools.product(formats_to_download, requested_ranges):\n+            for fmt, chapter in itertools.product(\n+                formats_to_download, requested_ranges\n+            ):\n                 new_info = self._copy_infodict(info_dict)\n                 new_info.update(fmt)\n-                offset, duration = info_dict.get('section_start') or 0, info_dict.get('duration') or float('inf')\n-                end_time = offset + min(chapter.get('end_time', duration), duration)\n+                offset, duration = (\n+                    info_dict.get(\"section_start\") or 0,\n+                    info_dict.get(\"duration\") or float(\"inf\"),\n+                )\n+                end_time = offset + min(chapter.get(\"end_time\", duration), duration)\n                 # duration may not be accurate. So allow deviations <1sec\n-                if end_time == float('inf') or end_time > offset + duration + 1:\n+                if end_time == float(\"inf\") or end_time > offset + duration + 1:\n                     end_time = None\n                 if chapter or offset:\n-                    new_info.update({\n-                        'section_start': offset + chapter.get('start_time', 0),\n-                        'section_end': end_time,\n-                        'section_title': chapter.get('title'),\n-                        'section_number': chapter.get('index'),\n-                    })\n+                    new_info.update(\n+                        {\n+                            \"section_start\": offset + chapter.get(\"start_time\", 0),\n+                            \"section_end\": end_time,\n+                            \"section_title\": chapter.get(\"title\"),\n+                            \"section_number\": chapter.get(\"index\"),\n+                        }\n+                    )\n                 downloaded_formats.append(new_info)\n                 try:\n                     self.process_info(new_info)\n@@ -2960,13 +3679,15 @@ def to_screen(*msg):\n                 if max_downloads_reached:\n                     break\n \n-            write_archive = {f.get('__write_download_archive', False) for f in downloaded_formats}\n-            assert write_archive.issubset({True, False, 'ignore'})\n+            write_archive = {\n+                f.get(\"__write_download_archive\", False) for f in downloaded_formats\n+            }\n+            assert write_archive.issubset({True, False, \"ignore\"})\n             if True in write_archive and False not in write_archive:\n                 self.record_download_archive(info_dict)\n \n-            info_dict['requested_downloads'] = downloaded_formats\n-            info_dict = self.run_all_pps('after_video', info_dict)\n+            info_dict[\"requested_downloads\"] = downloaded_formats\n+            info_dict = self.run_all_pps(\"after_video\", info_dict)\n             if max_downloads_reached:\n                 raise MaxDownloadsReached()\n \n@@ -2977,52 +3698,61 @@ def to_screen(*msg):\n     def process_subtitles(self, video_id, normal_subtitles, automatic_captions):\n         \"\"\"Select the requested subtitles and their format\"\"\"\n         available_subs, normal_sub_langs = {}, []\n-        if normal_subtitles and self.params.get('writesubtitles'):\n+        if normal_subtitles and self.params.get(\"writesubtitles\"):\n             available_subs.update(normal_subtitles)\n             normal_sub_langs = tuple(normal_subtitles.keys())\n-        if automatic_captions and self.params.get('writeautomaticsub'):\n+        if automatic_captions and self.params.get(\"writeautomaticsub\"):\n             for lang, cap_info in automatic_captions.items():\n                 if lang not in available_subs:\n                     available_subs[lang] = cap_info\n \n         if not available_subs or (\n-                not self.params.get('writesubtitles')\n-                and not self.params.get('writeautomaticsub')):\n+            not self.params.get(\"writesubtitles\")\n+            and not self.params.get(\"writeautomaticsub\")\n+        ):\n             return None\n \n         all_sub_langs = tuple(available_subs.keys())\n-        if self.params.get('allsubtitles', False):\n+        if self.params.get(\"allsubtitles\", False):\n             requested_langs = all_sub_langs\n-        elif self.params.get('subtitleslangs', False):\n+        elif self.params.get(\"subtitleslangs\", False):\n             try:\n                 requested_langs = orderedSet_from_options(\n-                    self.params.get('subtitleslangs'), {'all': all_sub_langs}, use_regex=True)\n+                    self.params.get(\"subtitleslangs\"),\n+                    {\"all\": all_sub_langs},\n+                    use_regex=True,\n+                )\n             except re.error as e:\n-                raise ValueError(f'Wrong regex for subtitlelangs: {e.pattern}')\n+                raise ValueError(f\"Wrong regex for subtitlelangs: {e.pattern}\")\n         else:\n-            requested_langs = LazyList(itertools.chain(\n-                ['en'] if 'en' in normal_sub_langs else [],\n-                filter(lambda f: f.startswith('en'), normal_sub_langs),\n-                ['en'] if 'en' in all_sub_langs else [],\n-                filter(lambda f: f.startswith('en'), all_sub_langs),\n-                normal_sub_langs, all_sub_langs,\n-            ))[:1]\n+            requested_langs = LazyList(\n+                itertools.chain(\n+                    [\"en\"] if \"en\" in normal_sub_langs else [],\n+                    filter(lambda f: f.startswith(\"en\"), normal_sub_langs),\n+                    [\"en\"] if \"en\" in all_sub_langs else [],\n+                    filter(lambda f: f.startswith(\"en\"), all_sub_langs),\n+                    normal_sub_langs,\n+                    all_sub_langs,\n+                )\n+            )[:1]\n         if requested_langs:\n-            self.to_screen(f'[info] {video_id}: Downloading subtitles: {\", \".join(requested_langs)}')\n+            self.to_screen(\n+                f\"[info] {video_id}: Downloading subtitles: {', '.join(requested_langs)}\"\n+            )\n \n-        formats_query = self.params.get('subtitlesformat', 'best')\n-        formats_preference = formats_query.split('/') if formats_query else []\n+        formats_query = self.params.get(\"subtitlesformat\", \"best\")\n+        formats_preference = formats_query.split(\"/\") if formats_query else []\n         subs = {}\n         for lang in requested_langs:\n             formats = available_subs.get(lang)\n             if formats is None:\n-                self.report_warning(f'{lang} subtitles not available for {video_id}')\n+                self.report_warning(f\"{lang} subtitles not available for {video_id}\")\n                 continue\n             for ext in formats_preference:\n-                if ext == 'best':\n+                if ext == \"best\":\n                     f = formats[-1]\n                     break\n-                matches = list(filter(lambda f: f['ext'] == ext, formats))\n+                matches = list(filter(lambda f: f[\"ext\"] == ext, formats))\n                 if matches:\n                     f = matches[-1]\n                     break\n@@ -3030,7 +3760,8 @@ def process_subtitles(self, video_id, normal_subtitles, automatic_captions):\n                 f = formats[-1]\n                 self.report_warning(\n                     'No subtitle format found matching \"%s\" for language %s, '\n-                    'using %s' % (formats_query, lang, f['ext']))\n+                    \"using %s\" % (formats_query, lang, f[\"ext\"])\n+                )\n             subs[lang] = f\n         return subs\n \n@@ -3038,109 +3769,128 @@ def _forceprint(self, key, info_dict):\n         if info_dict is None:\n             return\n         info_copy = info_dict.copy()\n-        info_copy.setdefault('filename', self.prepare_filename(info_dict))\n-        if info_dict.get('requested_formats') is not None:\n+        info_copy.setdefault(\"filename\", self.prepare_filename(info_dict))\n+        if info_dict.get(\"requested_formats\") is not None:\n             # For RTMP URLs, also include the playpath\n-            info_copy['urls'] = '\\n'.join(f['url'] + f.get('play_path', '') for f in info_dict['requested_formats'])\n-        elif info_dict.get('url'):\n-            info_copy['urls'] = info_dict['url'] + info_dict.get('play_path', '')\n-        info_copy['formats_table'] = self.render_formats_table(info_dict)\n-        info_copy['thumbnails_table'] = self.render_thumbnails_table(info_dict)\n-        info_copy['subtitles_table'] = self.render_subtitles_table(info_dict.get('id'), info_dict.get('subtitles'))\n-        info_copy['automatic_captions_table'] = self.render_subtitles_table(info_dict.get('id'), info_dict.get('automatic_captions'))\n+            info_copy[\"urls\"] = \"\\n\".join(\n+                f[\"url\"] + f.get(\"play_path\", \"\")\n+                for f in info_dict[\"requested_formats\"]\n+            )\n+        elif info_dict.get(\"url\"):\n+            info_copy[\"urls\"] = info_dict[\"url\"] + info_dict.get(\"play_path\", \"\")\n+        info_copy[\"formats_table\"] = self.render_formats_table(info_dict)\n+        info_copy[\"thumbnails_table\"] = self.render_thumbnails_table(info_dict)\n+        info_copy[\"subtitles_table\"] = self.render_subtitles_table(\n+            info_dict.get(\"id\"), info_dict.get(\"subtitles\")\n+        )\n+        info_copy[\"automatic_captions_table\"] = self.render_subtitles_table(\n+            info_dict.get(\"id\"), info_dict.get(\"automatic_captions\")\n+        )\n \n         def format_tmpl(tmpl):\n-            mobj = re.fullmatch(r'([\\w.:,]|-\\d|(?P<dict>{([\\w.:,]|-\\d)+}))+=?', tmpl)\n+            mobj = re.fullmatch(r\"([\\w.:,]|-\\d|(?P<dict>{([\\w.:,]|-\\d)+}))+=?\", tmpl)\n             if not mobj:\n                 return tmpl\n \n-            fmt = '%({})s'\n-            if tmpl.startswith('{'):\n-                tmpl, fmt = f'.{tmpl}', '%({})j'\n-            if tmpl.endswith('='):\n-                tmpl, fmt = tmpl[:-1], '{0} = %({0})#j'\n-            return '\\n'.join(map(fmt.format, [tmpl] if mobj.group('dict') else tmpl.split(',')))\n+            fmt = \"%({})s\"\n+            if tmpl.startswith(\"{\"):\n+                tmpl, fmt = f\".{tmpl}\", \"%({})j\"\n+            if tmpl.endswith(\"=\"):\n+                tmpl, fmt = tmpl[:-1], \"{0} = %({0})#j\"\n+            return \"\\n\".join(\n+                map(fmt.format, [tmpl] if mobj.group(\"dict\") else tmpl.split(\",\"))\n+            )\n \n-        for tmpl in self.params['forceprint'].get(key, []):\n+        for tmpl in self.params[\"forceprint\"].get(key, []):\n             self.to_stdout(self.evaluate_outtmpl(format_tmpl(tmpl), info_copy))\n \n-        for tmpl, file_tmpl in self.params['print_to_file'].get(key, []):\n+        for tmpl, file_tmpl in self.params[\"print_to_file\"].get(key, []):\n             filename = self.prepare_filename(info_dict, outtmpl=file_tmpl)\n             tmpl = format_tmpl(tmpl)\n-            self.to_screen(f'[info] Writing {tmpl!r} to: {filename}')\n+            self.to_screen(f\"[info] Writing {tmpl!r} to: {filename}\")\n             if self._ensure_dir_exists(filename):\n-                with open(filename, 'a', encoding='utf-8', newline='') as f:\n+                with open(filename, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n                     f.write(self.evaluate_outtmpl(tmpl, info_copy) + os.linesep)\n \n         return info_copy\n \n     def __forced_printings(self, info_dict, filename=None, incomplete=True):\n-        if (self.params.get('forcejson')\n-                or self.params['forceprint'].get('video')\n-                or self.params['print_to_file'].get('video')):\n+        if (\n+            self.params.get(\"forcejson\")\n+            or self.params[\"forceprint\"].get(\"video\")\n+            or self.params[\"print_to_file\"].get(\"video\")\n+        ):\n             self.post_extract(info_dict)\n         if filename:\n-            info_dict['filename'] = filename\n-        info_copy = self._forceprint('video', info_dict)\n+            info_dict[\"filename\"] = filename\n+        info_copy = self._forceprint(\"video\", info_dict)\n \n         def print_field(field, actual_field=None, optional=False):\n             if actual_field is None:\n                 actual_field = field\n-            if self.params.get(f'force{field}') and (\n-                    info_copy.get(field) is not None or (not optional and not incomplete)):\n+            if self.params.get(f\"force{field}\") and (\n+                info_copy.get(field) is not None or (not optional and not incomplete)\n+            ):\n                 self.to_stdout(info_copy[actual_field])\n \n-        print_field('title')\n-        print_field('id')\n-        print_field('url', 'urls')\n-        print_field('thumbnail', optional=True)\n-        print_field('description', optional=True)\n-        print_field('filename')\n-        if self.params.get('forceduration') and info_copy.get('duration') is not None:\n-            self.to_stdout(formatSeconds(info_copy['duration']))\n-        print_field('format')\n-\n-        if self.params.get('forcejson'):\n+        print_field(\"title\")\n+        print_field(\"id\")\n+        print_field(\"url\", \"urls\")\n+        print_field(\"thumbnail\", optional=True)\n+        print_field(\"description\", optional=True)\n+        print_field(\"filename\")\n+        if self.params.get(\"forceduration\") and info_copy.get(\"duration\") is not None:\n+            self.to_stdout(formatSeconds(info_copy[\"duration\"]))\n+        print_field(\"format\")\n+\n+        if self.params.get(\"forcejson\"):\n             self.to_stdout(json.dumps(self.sanitize_info(info_dict)))\n \n     def dl(self, name, info, subtitle=False, test=False):\n-        if not info.get('url'):\n+        if not info.get(\"url\"):\n             self.raise_no_formats(info, True)\n \n         if test:\n-            verbose = self.params.get('verbose')\n+            verbose = self.params.get(\"verbose\")\n             params = {\n-                'test': True,\n-                'quiet': self.params.get('quiet') or not verbose,\n-                'verbose': verbose,\n-                'noprogress': not verbose,\n-                'nopart': True,\n-                'skip_unavailable_fragments': False,\n-                'keep_fragments': False,\n-                'overwrites': True,\n-                '_no_ytdl_file': True,\n+                \"test\": True,\n+                \"quiet\": self.params.get(\"quiet\") or not verbose,\n+                \"verbose\": verbose,\n+                \"noprogress\": not verbose,\n+                \"nopart\": True,\n+                \"skip_unavailable_fragments\": False,\n+                \"keep_fragments\": False,\n+                \"overwrites\": True,\n+                \"_no_ytdl_file\": True,\n             }\n         else:\n             params = self.params\n-        fd = get_suitable_downloader(info, params, to_stdout=(name == '-'))(self, params)\n+        fd = get_suitable_downloader(info, params, to_stdout=(name == \"-\"))(\n+            self, params\n+        )\n         if not test:\n             for ph in self._progress_hooks:\n                 fd.add_progress_hook(ph)\n             urls = '\", \"'.join(\n-                (f['url'].split(',')[0] + ',<data>' if f['url'].startswith('data:') else f['url'])\n-                for f in info.get('requested_formats', []) or [info])\n+                (\n+                    f[\"url\"].split(\",\")[0] + \",<data>\"\n+                    if f[\"url\"].startswith(\"data:\")\n+                    else f[\"url\"]\n+                )\n+                for f in info.get(\"requested_formats\", []) or [info]\n+            )\n             self.write_debug(f'Invoking {fd.FD_NAME} downloader on \"{urls}\"')\n \n         # Note: Ideally info should be a deep-copied so that hooks cannot modify it.\n         # But it may contain objects that are not deep-copyable\n         new_info = self._copy_infodict(info)\n-        if new_info.get('http_headers') is None:\n-            new_info['http_headers'] = self._calc_headers(new_info)\n+        if new_info.get(\"http_headers\") is None:\n+            new_info[\"http_headers\"] = self._calc_headers(new_info)\n         return fd.download(name, new_info, subtitle)\n \n     def existing_file(self, filepaths, *, default_overwrite=True):\n         existing_files = list(filter(os.path.exists, orderedSet(filepaths)))\n-        if existing_files and not self.params.get('overwrites', default_overwrite):\n+        if existing_files and not self.params.get(\"overwrites\", default_overwrite):\n             return existing_files[0]\n \n         for file in existing_files:\n@@ -3151,14 +3901,14 @@ def existing_file(self, filepaths, *, default_overwrite=True):\n     def process_info(self, info_dict):\n         \"\"\"Process a single resolved IE result. (Modifies it in-place)\"\"\"\n \n-        assert info_dict.get('_type', 'video') == 'video'\n+        assert info_dict.get(\"_type\", \"video\") == \"video\"\n         original_infodict = info_dict\n \n-        if 'format' not in info_dict and 'ext' in info_dict:\n-            info_dict['format'] = info_dict['ext']\n+        if \"format\" not in info_dict and \"ext\" in info_dict:\n+            info_dict[\"format\"] = info_dict[\"ext\"]\n \n         if self._match_entry(info_dict) is not None:\n-            info_dict['__write_download_archive'] = 'ignore'\n+            info_dict[\"__write_download_archive\"] = \"ignore\"\n             return\n \n         # Does nothing under normal operation - for backward compatibility of process_info\n@@ -3171,24 +3921,30 @@ def replace_info_dict(new_info):\n             info_dict.clear()\n             info_dict.update(new_info)\n \n-        new_info, _ = self.pre_process(info_dict, 'video')\n+        new_info, _ = self.pre_process(info_dict, \"video\")\n         replace_info_dict(new_info)\n         self._num_downloads += 1\n \n         # info_dict['_filename'] needs to be set for backward compatibility\n-        info_dict['_filename'] = full_filename = self.prepare_filename(info_dict, warn=True)\n-        temp_filename = self.prepare_filename(info_dict, 'temp')\n+        info_dict[\"_filename\"] = full_filename = self.prepare_filename(\n+            info_dict, warn=True\n+        )\n+        temp_filename = self.prepare_filename(info_dict, \"temp\")\n         files_to_move = {}\n \n         # Forced printings\n-        self.__forced_printings(info_dict, full_filename, incomplete=('format' not in info_dict))\n+        self.__forced_printings(\n+            info_dict, full_filename, incomplete=(\"format\" not in info_dict)\n+        )\n \n         def check_max_downloads():\n-            if self._num_downloads >= float(self.params.get('max_downloads') or 'inf'):\n+            if self._num_downloads >= float(self.params.get(\"max_downloads\") or \"inf\"):\n                 raise MaxDownloadsReached()\n \n-        if self.params.get('simulate'):\n-            info_dict['__write_download_archive'] = self.params.get('force_write_download_archive')\n+        if self.params.get(\"simulate\"):\n+            info_dict[\"__write_download_archive\"] = self.params.get(\n+                \"force_write_download_archive\"\n+            )\n             check_max_downloads()\n             return\n \n@@ -3199,8 +3955,12 @@ def check_max_downloads():\n         if not self._ensure_dir_exists(encodeFilename(temp_filename)):\n             return\n \n-        if self._write_description('video', info_dict,\n-                                   self.prepare_filename(info_dict, 'description')) is None:\n+        if (\n+            self._write_description(\n+                \"video\", info_dict, self.prepare_filename(info_dict, \"description\")\n+            )\n+            is None\n+        ):\n             return\n \n         sub_files = self._write_subtitles(info_dict, temp_filename)\n@@ -3209,203 +3969,286 @@ def check_max_downloads():\n         files_to_move.update(dict(sub_files))\n \n         thumb_files = self._write_thumbnails(\n-            'video', info_dict, temp_filename, self.prepare_filename(info_dict, 'thumbnail'))\n+            \"video\",\n+            info_dict,\n+            temp_filename,\n+            self.prepare_filename(info_dict, \"thumbnail\"),\n+        )\n         if thumb_files is None:\n             return\n         files_to_move.update(dict(thumb_files))\n \n-        infofn = self.prepare_filename(info_dict, 'infojson')\n-        _infojson_written = self._write_info_json('video', info_dict, infofn)\n+        infofn = self.prepare_filename(info_dict, \"infojson\")\n+        _infojson_written = self._write_info_json(\"video\", info_dict, infofn)\n         if _infojson_written:\n-            info_dict['infojson_filename'] = infofn\n+            info_dict[\"infojson_filename\"] = infofn\n             # For backward compatibility, even though it was a private field\n-            info_dict['__infojson_filename'] = infofn\n+            info_dict[\"__infojson_filename\"] = infofn\n         elif _infojson_written is None:\n             return\n \n         # Note: Annotations are deprecated\n         annofn = None\n-        if self.params.get('writeannotations', False):\n-            annofn = self.prepare_filename(info_dict, 'annotation')\n+        if self.params.get(\"writeannotations\", False):\n+            annofn = self.prepare_filename(info_dict, \"annotation\")\n         if annofn:\n             if not self._ensure_dir_exists(encodeFilename(annofn)):\n                 return\n-            if not self.params.get('overwrites', True) and os.path.exists(encodeFilename(annofn)):\n-                self.to_screen('[info] Video annotations are already present')\n-            elif not info_dict.get('annotations'):\n-                self.report_warning('There are no annotations to write.')\n+            if not self.params.get(\"overwrites\", True) and os.path.exists(\n+                encodeFilename(annofn)\n+            ):\n+                self.to_screen(\"[info] Video annotations are already present\")\n+            elif not info_dict.get(\"annotations\"):\n+                self.report_warning(\"There are no annotations to write.\")\n             else:\n                 try:\n-                    self.to_screen('[info] Writing video annotations to: ' + annofn)\n-                    with open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:\n-                        annofile.write(info_dict['annotations'])\n+                    self.to_screen(\"[info] Writing video annotations to: \" + annofn)\n+                    with open(\n+                        encodeFilename(annofn), \"w\", encoding=\"utf-8\"\n+                    ) as annofile:\n+                        annofile.write(info_dict[\"annotations\"])\n                 except (KeyError, TypeError):\n-                    self.report_warning('There are no annotations to write.')\n+                    self.report_warning(\"There are no annotations to write.\")\n                 except OSError:\n-                    self.report_error('Cannot write annotations file: ' + annofn)\n+                    self.report_error(\"Cannot write annotations file: \" + annofn)\n                     return\n \n         # Write internet shortcut files\n         def _write_link_file(link_type):\n-            url = try_get(info_dict['webpage_url'], iri_to_uri)\n+            url = try_get(info_dict[\"webpage_url\"], iri_to_uri)\n             if not url:\n                 self.report_warning(\n-                    f'Cannot write internet shortcut file because the actual URL of \"{info_dict[\"webpage_url\"]}\" is unknown')\n+                    f'Cannot write internet shortcut file because the actual URL of \"{info_dict[\"webpage_url\"]}\" is unknown'\n+                )\n                 return True\n-            linkfn = replace_extension(self.prepare_filename(info_dict, 'link'), link_type, info_dict.get('ext'))\n+            linkfn = replace_extension(\n+                self.prepare_filename(info_dict, \"link\"),\n+                link_type,\n+                info_dict.get(\"ext\"),\n+            )\n             if not self._ensure_dir_exists(encodeFilename(linkfn)):\n                 return False\n-            if self.params.get('overwrites', True) and os.path.exists(encodeFilename(linkfn)):\n-                self.to_screen(f'[info] Internet shortcut (.{link_type}) is already present')\n+            if self.params.get(\"overwrites\", True) and os.path.exists(\n+                encodeFilename(linkfn)\n+            ):\n+                self.to_screen(\n+                    f\"[info] Internet shortcut (.{link_type}) is already present\"\n+                )\n                 return True\n             try:\n-                self.to_screen(f'[info] Writing internet shortcut (.{link_type}) to: {linkfn}')\n-                with open(encodeFilename(to_high_limit_path(linkfn)), 'w', encoding='utf-8',\n-                          newline='\\r\\n' if link_type == 'url' else '\\n') as linkfile:\n-                    template_vars = {'url': url}\n-                    if link_type == 'desktop':\n-                        template_vars['filename'] = linkfn[:-(len(link_type) + 1)]\n+                self.to_screen(\n+                    f\"[info] Writing internet shortcut (.{link_type}) to: {linkfn}\"\n+                )\n+                with open(\n+                    encodeFilename(to_high_limit_path(linkfn)),\n+                    \"w\",\n+                    encoding=\"utf-8\",\n+                    newline=\"\\r\\n\" if link_type == \"url\" else \"\\n\",\n+                ) as linkfile:\n+                    template_vars = {\"url\": url}\n+                    if link_type == \"desktop\":\n+                        template_vars[\"filename\"] = linkfn[: -(len(link_type) + 1)]\n                     linkfile.write(LINK_TEMPLATES[link_type] % template_vars)\n             except OSError:\n-                self.report_error(f'Cannot write internet shortcut {linkfn}')\n+                self.report_error(f\"Cannot write internet shortcut {linkfn}\")\n                 return False\n             return True\n \n         write_links = {\n-            'url': self.params.get('writeurllink'),\n-            'webloc': self.params.get('writewebloclink'),\n-            'desktop': self.params.get('writedesktoplink'),\n+            \"url\": self.params.get(\"writeurllink\"),\n+            \"webloc\": self.params.get(\"writewebloclink\"),\n+            \"desktop\": self.params.get(\"writedesktoplink\"),\n         }\n-        if self.params.get('writelink'):\n-            link_type = ('webloc' if sys.platform == 'darwin'\n-                         else 'desktop' if sys.platform.startswith('linux')\n-                         else 'url')\n+        if self.params.get(\"writelink\"):\n+            link_type = (\n+                \"webloc\"\n+                if sys.platform == \"darwin\"\n+                else \"desktop\"\n+                if sys.platform.startswith(\"linux\")\n+                else \"url\"\n+            )\n             write_links[link_type] = True\n \n-        if any(should_write and not _write_link_file(link_type)\n-               for link_type, should_write in write_links.items()):\n+        if any(\n+            should_write and not _write_link_file(link_type)\n+            for link_type, should_write in write_links.items()\n+        ):\n             return\n \n-        new_info, files_to_move = self.pre_process(info_dict, 'before_dl', files_to_move)\n+        new_info, files_to_move = self.pre_process(\n+            info_dict, \"before_dl\", files_to_move\n+        )\n         replace_info_dict(new_info)\n \n-        if self.params.get('skip_download'):\n-            info_dict['filepath'] = temp_filename\n-            info_dict['__finaldir'] = os.path.dirname(os.path.abspath(encodeFilename(full_filename)))\n-            info_dict['__files_to_move'] = files_to_move\n-            replace_info_dict(self.run_pp(MoveFilesAfterDownloadPP(self, False), info_dict))\n-            info_dict['__write_download_archive'] = self.params.get('force_write_download_archive')\n+        if self.params.get(\"skip_download\"):\n+            info_dict[\"filepath\"] = temp_filename\n+            info_dict[\"__finaldir\"] = os.path.dirname(\n+                os.path.abspath(encodeFilename(full_filename))\n+            )\n+            info_dict[\"__files_to_move\"] = files_to_move\n+            replace_info_dict(\n+                self.run_pp(MoveFilesAfterDownloadPP(self, False), info_dict)\n+            )\n+            info_dict[\"__write_download_archive\"] = self.params.get(\n+                \"force_write_download_archive\"\n+            )\n         else:\n             # Download\n-            info_dict.setdefault('__postprocessors', [])\n+            info_dict.setdefault(\"__postprocessors\", [])\n             try:\n \n                 def existing_video_file(*filepaths):\n-                    ext = info_dict.get('ext')\n-                    converted = lambda file: replace_extension(file, self.params.get('final_ext') or ext, ext)\n-                    file = self.existing_file(itertools.chain(*zip(map(converted, filepaths), filepaths)),\n-                                              default_overwrite=False)\n+                    ext = info_dict.get(\"ext\")\n+                    converted = lambda file: replace_extension(\n+                        file, self.params.get(\"final_ext\") or ext, ext\n+                    )\n+                    file = self.existing_file(\n+                        itertools.chain(*zip(map(converted, filepaths), filepaths)),\n+                        default_overwrite=False,\n+                    )\n                     if file:\n-                        info_dict['ext'] = os.path.splitext(file)[1][1:]\n+                        info_dict[\"ext\"] = os.path.splitext(file)[1][1:]\n                     return file\n \n                 fd, success = None, True\n-                if info_dict.get('protocol') or info_dict.get('url'):\n-                    fd = get_suitable_downloader(info_dict, self.params, to_stdout=temp_filename == '-')\n-                    if fd != FFmpegFD and 'no-direct-merge' not in self.params['compat_opts'] and (\n-                            info_dict.get('section_start') or info_dict.get('section_end')):\n-                        msg = ('This format cannot be partially downloaded' if FFmpegFD.available()\n-                               else 'You have requested downloading the video partially, but ffmpeg is not installed')\n-                        self.report_error(f'{msg}. Aborting')\n+                if info_dict.get(\"protocol\") or info_dict.get(\"url\"):\n+                    fd = get_suitable_downloader(\n+                        info_dict, self.params, to_stdout=temp_filename == \"-\"\n+                    )\n+                    if (\n+                        fd != FFmpegFD\n+                        and \"no-direct-merge\" not in self.params[\"compat_opts\"]\n+                        and (\n+                            info_dict.get(\"section_start\")\n+                            or info_dict.get(\"section_end\")\n+                        )\n+                    ):\n+                        msg = (\n+                            \"This format cannot be partially downloaded\"\n+                            if FFmpegFD.available()\n+                            else \"You have requested downloading the video partially, but ffmpeg is not installed\"\n+                        )\n+                        self.report_error(f\"{msg}. Aborting\")\n                         return\n \n-                if info_dict.get('requested_formats') is not None:\n-                    old_ext = info_dict['ext']\n-                    if self.params.get('merge_output_format') is None:\n-                        if (info_dict['ext'] == 'webm'\n-                                and info_dict.get('thumbnails')\n-                                # check with type instead of pp_key, __name__, or isinstance\n-                                # since we dont want any custom PPs to trigger this\n-                                and any(type(pp) == EmbedThumbnailPP for pp in self._pps['post_process'])):  # noqa: E721\n-                            info_dict['ext'] = 'mkv'\n+                if info_dict.get(\"requested_formats\") is not None:\n+                    old_ext = info_dict[\"ext\"]\n+                    if self.params.get(\"merge_output_format\") is None:\n+                        if (\n+                            info_dict[\"ext\"] == \"webm\"\n+                            and info_dict.get(\"thumbnails\")\n+                            # check with type instead of pp_key, __name__, or isinstance\n+                            # since we dont want any custom PPs to trigger this\n+                            and any(\n+                                type(pp) == EmbedThumbnailPP\n+                                for pp in self._pps[\"post_process\"]\n+                            )\n+                        ):  # noqa: E721\n+                            info_dict[\"ext\"] = \"mkv\"\n                             self.report_warning(\n-                                'webm doesn\\'t support embedding a thumbnail, mkv will be used')\n-                    new_ext = info_dict['ext']\n+                                \"webm doesn't support embedding a thumbnail, mkv will be used\"\n+                            )\n+                    new_ext = info_dict[\"ext\"]\n \n                     def correct_ext(filename, ext=new_ext):\n-                        if filename == '-':\n+                        if filename == \"-\":\n                             return filename\n                         filename_real_ext = os.path.splitext(filename)[1][1:]\n                         filename_wo_ext = (\n                             os.path.splitext(filename)[0]\n                             if filename_real_ext in (old_ext, new_ext)\n-                            else filename)\n-                        return f'{filename_wo_ext}.{ext}'\n+                            else filename\n+                        )\n+                        return f\"{filename_wo_ext}.{ext}\"\n \n                     # Ensure filename always has a correct extension for successful merge\n                     full_filename = correct_ext(full_filename)\n                     temp_filename = correct_ext(temp_filename)\n                     dl_filename = existing_video_file(full_filename, temp_filename)\n \n-                    info_dict['__real_download'] = False\n+                    info_dict[\"__real_download\"] = False\n                     # NOTE: Copy so that original format dicts are not modified\n-                    info_dict['requested_formats'] = list(map(dict, info_dict['requested_formats']))\n+                    info_dict[\"requested_formats\"] = list(\n+                        map(dict, info_dict[\"requested_formats\"])\n+                    )\n \n                     merger = FFmpegMergerPP(self)\n                     downloaded = []\n                     if dl_filename is not None:\n                         self.report_file_already_downloaded(dl_filename)\n                     elif fd:\n-                        for f in info_dict['requested_formats'] if fd != FFmpegFD else []:\n-                            f['filepath'] = fname = prepend_extension(\n-                                correct_ext(temp_filename, info_dict['ext']),\n-                                'f%s' % f['format_id'], info_dict['ext'])\n+                        for f in (\n+                            info_dict[\"requested_formats\"] if fd != FFmpegFD else []\n+                        ):\n+                            f[\"filepath\"] = fname = prepend_extension(\n+                                correct_ext(temp_filename, info_dict[\"ext\"]),\n+                                \"f%s\" % f[\"format_id\"],\n+                                info_dict[\"ext\"],\n+                            )\n                             downloaded.append(fname)\n-                        info_dict['url'] = '\\n'.join(f['url'] for f in info_dict['requested_formats'])\n+                        info_dict[\"url\"] = \"\\n\".join(\n+                            f[\"url\"] for f in info_dict[\"requested_formats\"]\n+                        )\n                         success, real_download = self.dl(temp_filename, info_dict)\n-                        info_dict['__real_download'] = real_download\n+                        info_dict[\"__real_download\"] = real_download\n                     else:\n-                        if self.params.get('allow_unplayable_formats'):\n+                        if self.params.get(\"allow_unplayable_formats\"):\n                             self.report_warning(\n-                                'You have requested merging of multiple formats '\n-                                'while also allowing unplayable formats to be downloaded. '\n-                                'The formats won\\'t be merged to prevent data corruption.')\n+                                \"You have requested merging of multiple formats \"\n+                                \"while also allowing unplayable formats to be downloaded. \"\n+                                \"The formats won't be merged to prevent data corruption.\"\n+                            )\n                         elif not merger.available:\n-                            msg = 'You have requested merging of multiple formats but ffmpeg is not installed'\n-                            if not self.params.get('ignoreerrors'):\n-                                self.report_error(f'{msg}. Aborting due to --abort-on-error')\n+                            msg = \"You have requested merging of multiple formats but ffmpeg is not installed\"\n+                            if not self.params.get(\"ignoreerrors\"):\n+                                self.report_error(\n+                                    f\"{msg}. Aborting due to --abort-on-error\"\n+                                )\n                                 return\n-                            self.report_warning(f'{msg}. The formats won\\'t be merged')\n-\n-                        if temp_filename == '-':\n-                            reason = ('using a downloader other than ffmpeg' if FFmpegFD.can_merge_formats(info_dict, self.params)\n-                                      else 'but the formats are incompatible for simultaneous download' if merger.available\n-                                      else 'but ffmpeg is not installed')\n+                            self.report_warning(f\"{msg}. The formats won't be merged\")\n+\n+                        if temp_filename == \"-\":\n+                            reason = (\n+                                \"using a downloader other than ffmpeg\"\n+                                if FFmpegFD.can_merge_formats(info_dict, self.params)\n+                                else \"but the formats are incompatible for simultaneous download\"\n+                                if merger.available\n+                                else \"but ffmpeg is not installed\"\n+                            )\n                             self.report_warning(\n-                                f'You have requested downloading multiple formats to stdout {reason}. '\n-                                'The formats will be streamed one after the other')\n+                                f\"You have requested downloading multiple formats to stdout {reason}. \"\n+                                \"The formats will be streamed one after the other\"\n+                            )\n                             fname = temp_filename\n-                        for f in info_dict['requested_formats']:\n+                        for f in info_dict[\"requested_formats\"]:\n                             new_info = dict(info_dict)\n-                            del new_info['requested_formats']\n+                            del new_info[\"requested_formats\"]\n                             new_info.update(f)\n-                            if temp_filename != '-':\n+                            if temp_filename != \"-\":\n                                 fname = prepend_extension(\n-                                    correct_ext(temp_filename, new_info['ext']),\n-                                    'f%s' % f['format_id'], new_info['ext'])\n+                                    correct_ext(temp_filename, new_info[\"ext\"]),\n+                                    \"f%s\" % f[\"format_id\"],\n+                                    new_info[\"ext\"],\n+                                )\n                                 if not self._ensure_dir_exists(fname):\n                                     return\n-                                f['filepath'] = fname\n+                                f[\"filepath\"] = fname\n                                 downloaded.append(fname)\n                             partial_success, real_download = self.dl(fname, new_info)\n-                            info_dict['__real_download'] = info_dict['__real_download'] or real_download\n+                            info_dict[\"__real_download\"] = (\n+                                info_dict[\"__real_download\"] or real_download\n+                            )\n                             success = success and partial_success\n \n-                    if downloaded and merger.available and not self.params.get('allow_unplayable_formats'):\n-                        info_dict['__postprocessors'].append(merger)\n-                        info_dict['__files_to_merge'] = downloaded\n+                    if (\n+                        downloaded\n+                        and merger.available\n+                        and not self.params.get(\"allow_unplayable_formats\")\n+                    ):\n+                        info_dict[\"__postprocessors\"].append(merger)\n+                        info_dict[\"__files_to_merge\"] = downloaded\n                         # Even if there were no downloads, it is being merged only now\n-                        info_dict['__real_download'] = True\n+                        info_dict[\"__real_download\"] = True\n                     else:\n                         for file in downloaded:\n                             files_to_move[file] = None\n@@ -3416,97 +4259,137 @@ def correct_ext(filename, ext=new_ext):\n                         # dl_filename == temp_filename could mean that the file was partially downloaded with --no-part.\n                         # So we should try to resume the download\n                         success, real_download = self.dl(temp_filename, info_dict)\n-                        info_dict['__real_download'] = real_download\n+                        info_dict[\"__real_download\"] = real_download\n                     else:\n                         self.report_file_already_downloaded(dl_filename)\n \n                 dl_filename = dl_filename or temp_filename\n-                info_dict['__finaldir'] = os.path.dirname(os.path.abspath(encodeFilename(full_filename)))\n+                info_dict[\"__finaldir\"] = os.path.dirname(\n+                    os.path.abspath(encodeFilename(full_filename))\n+                )\n \n             except network_exceptions as err:\n-                self.report_error('unable to download video data: %s' % error_to_compat_str(err))\n+                self.report_error(\n+                    \"unable to download video data: %s\" % error_to_compat_str(err)\n+                )\n                 return\n             except OSError as err:\n                 raise UnavailableVideoError(err)\n-            except (ContentTooShortError, ) as err:\n-                self.report_error(f'content too short (expected {err.expected} bytes and served {err.downloaded})')\n+            except (ContentTooShortError,) as err:\n+                self.report_error(\n+                    f\"content too short (expected {err.expected} bytes and served {err.downloaded})\"\n+                )\n                 return\n \n             self._raise_pending_errors(info_dict)\n-            if success and full_filename != '-':\n+            if success and full_filename != \"-\":\n \n                 def fixup():\n                     do_fixup = True\n-                    fixup_policy = self.params.get('fixup')\n-                    vid = info_dict['id']\n+                    fixup_policy = self.params.get(\"fixup\")\n+                    vid = info_dict[\"id\"]\n \n-                    if fixup_policy in ('ignore', 'never'):\n+                    if fixup_policy in (\"ignore\", \"never\"):\n                         return\n-                    elif fixup_policy == 'warn':\n-                        do_fixup = 'warn'\n-                    elif fixup_policy != 'force':\n-                        assert fixup_policy in ('detect_or_warn', None)\n-                        if not info_dict.get('__real_download'):\n+                    elif fixup_policy == \"warn\":\n+                        do_fixup = \"warn\"\n+                    elif fixup_policy != \"force\":\n+                        assert fixup_policy in (\"detect_or_warn\", None)\n+                        if not info_dict.get(\"__real_download\"):\n                             do_fixup = False\n \n                     def ffmpeg_fixup(cndn, msg, cls):\n                         if not (do_fixup and cndn):\n                             return\n-                        elif do_fixup == 'warn':\n-                            self.report_warning(f'{vid}: {msg}')\n+                        elif do_fixup == \"warn\":\n+                            self.report_warning(f\"{vid}: {msg}\")\n                             return\n                         pp = cls(self)\n                         if pp.available:\n-                            info_dict['__postprocessors'].append(pp)\n+                            info_dict[\"__postprocessors\"].append(pp)\n                         else:\n-                            self.report_warning(f'{vid}: {msg}. Install ffmpeg to fix this automatically')\n-\n-                    stretched_ratio = info_dict.get('stretched_ratio')\n-                    ffmpeg_fixup(stretched_ratio not in (1, None),\n-                                 f'Non-uniform pixel ratio {stretched_ratio}',\n-                                 FFmpegFixupStretchedPP)\n+                            self.report_warning(\n+                                f\"{vid}: {msg}. Install ffmpeg to fix this automatically\"\n+                            )\n+\n+                    stretched_ratio = info_dict.get(\"stretched_ratio\")\n+                    ffmpeg_fixup(\n+                        stretched_ratio not in (1, None),\n+                        f\"Non-uniform pixel ratio {stretched_ratio}\",\n+                        FFmpegFixupStretchedPP,\n+                    )\n \n-                    downloader = get_suitable_downloader(info_dict, self.params) if 'protocol' in info_dict else None\n+                    downloader = (\n+                        get_suitable_downloader(info_dict, self.params)\n+                        if \"protocol\" in info_dict\n+                        else None\n+                    )\n                     downloader = downloader.FD_NAME if downloader else None\n \n-                    ext = info_dict.get('ext')\n-                    postprocessed_by_ffmpeg = info_dict.get('requested_formats') or any((\n-                        isinstance(pp, FFmpegVideoConvertorPP)\n-                        and resolve_recode_mapping(ext, pp.mapping)[0] not in (ext, None)\n-                    ) for pp in self._pps['post_process'])\n+                    ext = info_dict.get(\"ext\")\n+                    postprocessed_by_ffmpeg = info_dict.get(\"requested_formats\") or any(\n+                        (\n+                            isinstance(pp, FFmpegVideoConvertorPP)\n+                            and resolve_recode_mapping(ext, pp.mapping)[0]\n+                            not in (ext, None)\n+                        )\n+                        for pp in self._pps[\"post_process\"]\n+                    )\n \n                     if not postprocessed_by_ffmpeg:\n-                        ffmpeg_fixup(fd != FFmpegFD and ext == 'm4a'\n-                                     and info_dict.get('container') == 'm4a_dash',\n-                                     'writing DASH m4a. Only some players support this container',\n-                                     FFmpegFixupM4aPP)\n-                        ffmpeg_fixup(downloader == 'hlsnative' and not self.params.get('hls_use_mpegts')\n-                                     or info_dict.get('is_live') and self.params.get('hls_use_mpegts') is None,\n-                                     'Possible MPEG-TS in MP4 container or malformed AAC timestamps',\n-                                     FFmpegFixupM3u8PP)\n-                        ffmpeg_fixup(info_dict.get('is_live') and downloader == 'dashsegments',\n-                                     'Possible duplicate MOOV atoms', FFmpegFixupDuplicateMoovPP)\n-\n-                    ffmpeg_fixup(downloader == 'web_socket_fragment', 'Malformed timestamps detected', FFmpegFixupTimestampPP)\n-                    ffmpeg_fixup(downloader == 'web_socket_fragment', 'Malformed duration detected', FFmpegFixupDurationPP)\n+                        ffmpeg_fixup(\n+                            fd != FFmpegFD\n+                            and ext == \"m4a\"\n+                            and info_dict.get(\"container\") == \"m4a_dash\",\n+                            \"writing DASH m4a. Only some players support this container\",\n+                            FFmpegFixupM4aPP,\n+                        )\n+                        ffmpeg_fixup(\n+                            downloader == \"hlsnative\"\n+                            and not self.params.get(\"hls_use_mpegts\")\n+                            or info_dict.get(\"is_live\")\n+                            and self.params.get(\"hls_use_mpegts\") is None,\n+                            \"Possible MPEG-TS in MP4 container or malformed AAC timestamps\",\n+                            FFmpegFixupM3u8PP,\n+                        )\n+                        ffmpeg_fixup(\n+                            info_dict.get(\"is_live\") and downloader == \"dashsegments\",\n+                            \"Possible duplicate MOOV atoms\",\n+                            FFmpegFixupDuplicateMoovPP,\n+                        )\n+\n+                    ffmpeg_fixup(\n+                        downloader == \"web_socket_fragment\",\n+                        \"Malformed timestamps detected\",\n+                        FFmpegFixupTimestampPP,\n+                    )\n+                    ffmpeg_fixup(\n+                        downloader == \"web_socket_fragment\",\n+                        \"Malformed duration detected\",\n+                        FFmpegFixupDurationPP,\n+                    )\n \n                 fixup()\n                 try:\n-                    replace_info_dict(self.post_process(dl_filename, info_dict, files_to_move))\n+                    replace_info_dict(\n+                        self.post_process(dl_filename, info_dict, files_to_move)\n+                    )\n                 except PostProcessingError as err:\n-                    self.report_error('Postprocessing: %s' % str(err))\n+                    self.report_error(\"Postprocessing: %s\" % str(err))\n                     return\n                 try:\n                     for ph in self._post_hooks:\n-                        ph(info_dict['filepath'])\n+                        ph(info_dict[\"filepath\"])\n                 except Exception as err:\n-                    self.report_error('post hooks: %s' % str(err))\n+                    self.report_error(\"post hooks: %s\" % str(err))\n                     return\n-                info_dict['__write_download_archive'] = True\n+                info_dict[\"__write_download_archive\"] = True\n \n-        assert info_dict is original_infodict  # Make sure the info_dict was modified in-place\n-        if self.params.get('force_write_download_archive'):\n-            info_dict['__write_download_archive'] = True\n+        assert (\n+            info_dict is original_infodict\n+        )  # Make sure the info_dict was modified in-place\n+        if self.params.get(\"force_write_download_archive\"):\n+            info_dict[\"__write_download_archive\"] = True\n         check_max_downloads()\n \n     def __download_wrapper(self, func):\n@@ -3517,72 +4400,101 @@ def wrapper(*args, **kwargs):\n             except UnavailableVideoError as e:\n                 self.report_error(e)\n             except DownloadCancelled as e:\n-                self.to_screen(f'[info] {e}')\n-                if not self.params.get('break_per_url'):\n+                self.to_screen(f\"[info] {e}\")\n+                if not self.params.get(\"break_per_url\"):\n                     raise\n                 self._num_downloads = 0\n             else:\n-                if self.params.get('dump_single_json', False):\n+                if self.params.get(\"dump_single_json\", False):\n                     self.post_extract(res)\n                     self.to_stdout(json.dumps(self.sanitize_info(res)))\n+\n         return wrapper\n \n     def download(self, url_list):\n         \"\"\"Download a given list of URLs.\"\"\"\n         url_list = variadic(url_list)  # Passing a single URL is a common mistake\n-        outtmpl = self.params['outtmpl']['default']\n-        if (len(url_list) > 1\n-                and outtmpl != '-'\n-                and '%' not in outtmpl\n-                and self.params.get('max_downloads') != 1):\n+        outtmpl = self.params[\"outtmpl\"][\"default\"]\n+        if (\n+            len(url_list) > 1\n+            and outtmpl != \"-\"\n+            and \"%\" not in outtmpl\n+            and self.params.get(\"max_downloads\") != 1\n+        ):\n             raise SameFileError(outtmpl)\n \n         for url in url_list:\n             self.__download_wrapper(self.extract_info)(\n-                url, force_generic_extractor=self.params.get('force_generic_extractor', False))\n+                url,\n+                force_generic_extractor=self.params.get(\n+                    \"force_generic_extractor\", False\n+                ),\n+            )\n \n         return self._download_retcode\n \n     def download_with_info_file(self, info_filename):\n-        with contextlib.closing(fileinput.FileInput(\n-                [info_filename], mode='r',\n-                openhook=fileinput.hook_encoded('utf-8'))) as f:\n+        with contextlib.closing(\n+            fileinput.FileInput(\n+                [info_filename], mode=\"r\", openhook=fileinput.hook_encoded(\"utf-8\")\n+            )\n+        ) as f:\n             # FileInput doesn't have a read method, we can't call json.load\n-            infos = [self.sanitize_info(info, self.params.get('clean_infojson', True))\n-                     for info in variadic(json.loads('\\n'.join(f)))]\n+            infos = [\n+                self.sanitize_info(info, self.params.get(\"clean_infojson\", True))\n+                for info in variadic(json.loads(\"\\n\".join(f)))\n+            ]\n         for info in infos:\n             try:\n                 self.__download_wrapper(self.process_ie_result)(info, download=True)\n             except (DownloadError, EntryNotInPlaylist, ReExtractInfo) as e:\n                 if not isinstance(e, EntryNotInPlaylist):\n-                    self.to_stderr('\\r')\n-                webpage_url = info.get('webpage_url')\n+                    self.to_stderr(\"\\r\")\n+                webpage_url = info.get(\"webpage_url\")\n                 if webpage_url is None:\n                     raise\n-                self.report_warning(f'The info failed to download: {e}; trying with URL {webpage_url}')\n+                self.report_warning(\n+                    f\"The info failed to download: {e}; trying with URL {webpage_url}\"\n+                )\n                 self.download([webpage_url])\n         return self._download_retcode\n \n     @staticmethod\n     def sanitize_info(info_dict, remove_private_keys=False):\n-        ''' Sanitize the infodict for converting to json '''\n+        \"\"\"Sanitize the infodict for converting to json\"\"\"\n         if info_dict is None:\n             return info_dict\n-        info_dict.setdefault('epoch', int(time.time()))\n-        info_dict.setdefault('_type', 'video')\n-        info_dict.setdefault('_version', {\n-            'version': __version__,\n-            'current_git_head': current_git_head(),\n-            'release_git_head': RELEASE_GIT_HEAD,\n-            'repository': ORIGIN,\n-        })\n+        info_dict.setdefault(\"epoch\", int(time.time()))\n+        info_dict.setdefault(\"_type\", \"video\")\n+        info_dict.setdefault(\n+            \"_version\",\n+            {\n+                \"version\": __version__,\n+                \"current_git_head\": current_git_head(),\n+                \"release_git_head\": RELEASE_GIT_HEAD,\n+                \"repository\": ORIGIN,\n+            },\n+        )\n \n         if remove_private_keys:\n-            reject = lambda k, v: v is None or k.startswith('__') or k in {\n-                'requested_downloads', 'requested_formats', 'requested_subtitles', 'requested_entries',\n-                'entries', 'filepath', '_filename', 'filename', 'infojson_filename', 'original_url',\n-                'playlist_autonumber',\n-            }\n+            reject = (\n+                lambda k, v: v is None\n+                or k.startswith(\"__\")\n+                or k\n+                in {\n+                    \"requested_downloads\",\n+                    \"requested_formats\",\n+                    \"requested_subtitles\",\n+                    \"requested_entries\",\n+                    \"entries\",\n+                    \"filepath\",\n+                    \"_filename\",\n+                    \"filename\",\n+                    \"infojson_filename\",\n+                    \"original_url\",\n+                    \"playlist_autonumber\",\n+                }\n+            )\n         else:\n             reject = lambda k, v: False\n \n@@ -3600,7 +4512,7 @@ def filter_fn(obj):\n \n     @staticmethod\n     def filter_requested_info(info_dict, actually_filter=True):\n-        ''' Alias of sanitize_info for backward compatibility '''\n+        \"\"\"Alias of sanitize_info for backward compatibility\"\"\"\n         return YoutubeDL.sanitize_info(info_dict, actually_filter)\n \n     def _delete_downloaded_files(self, *files_to_delete, info={}, msg=None):\n@@ -3610,82 +4522,89 @@ def _delete_downloaded_files(self, *files_to_delete, info={}, msg=None):\n             try:\n                 os.remove(filename)\n             except OSError:\n-                self.report_warning(f'Unable to delete file {filename}')\n-            if filename in info.get('__files_to_move', []):  # NB: Delete even if None\n-                del info['__files_to_move'][filename]\n+                self.report_warning(f\"Unable to delete file {filename}\")\n+            if filename in info.get(\"__files_to_move\", []):  # NB: Delete even if None\n+                del info[\"__files_to_move\"][filename]\n \n     @staticmethod\n     def post_extract(info_dict):\n         def actual_post_extract(info_dict):\n-            if info_dict.get('_type') in ('playlist', 'multi_video'):\n-                for video_dict in info_dict.get('entries', {}):\n+            if info_dict.get(\"_type\") in (\"playlist\", \"multi_video\"):\n+                for video_dict in info_dict.get(\"entries\", {}):\n                     actual_post_extract(video_dict or {})\n                 return\n \n-            post_extractor = info_dict.pop('__post_extractor', None) or (lambda: {})\n+            post_extractor = info_dict.pop(\"__post_extractor\", None) or (lambda: {})\n             info_dict.update(post_extractor())\n \n         actual_post_extract(info_dict or {})\n \n     def run_pp(self, pp, infodict):\n         files_to_delete = []\n-        if '__files_to_move' not in infodict:\n-            infodict['__files_to_move'] = {}\n+        if \"__files_to_move\" not in infodict:\n+            infodict[\"__files_to_move\"] = {}\n         try:\n             files_to_delete, infodict = pp.run(infodict)\n         except PostProcessingError as e:\n             # Must be True and not 'only_download'\n-            if self.params.get('ignoreerrors') is True:\n+            if self.params.get(\"ignoreerrors\") is True:\n                 self.report_error(e)\n                 return infodict\n             raise\n \n         if not files_to_delete:\n             return infodict\n-        if self.params.get('keepvideo', False):\n+        if self.params.get(\"keepvideo\", False):\n             for f in files_to_delete:\n-                infodict['__files_to_move'].setdefault(f, '')\n+                infodict[\"__files_to_move\"].setdefault(f, \"\")\n         else:\n             self._delete_downloaded_files(\n-                *files_to_delete, info=infodict, msg='Deleting original file %s (pass -k to keep)')\n+                *files_to_delete,\n+                info=infodict,\n+                msg=\"Deleting original file %s (pass -k to keep)\",\n+            )\n         return infodict\n \n     def run_all_pps(self, key, info, *, additional_pps=None):\n-        if key != 'video':\n+        if key != \"video\":\n             self._forceprint(key, info)\n         for pp in (additional_pps or []) + self._pps[key]:\n             info = self.run_pp(pp, info)\n         return info\n \n-    def pre_process(self, ie_info, key='pre_process', files_to_move=None):\n+    def pre_process(self, ie_info, key=\"pre_process\", files_to_move=None):\n         info = dict(ie_info)\n-        info['__files_to_move'] = files_to_move or {}\n+        info[\"__files_to_move\"] = files_to_move or {}\n         try:\n             info = self.run_all_pps(key, info)\n         except PostProcessingError as err:\n-            msg = f'Preprocessing: {err}'\n-            info.setdefault('__pending_error', msg)\n+            msg = f\"Preprocessing: {err}\"\n+            info.setdefault(\"__pending_error\", msg)\n             self.report_error(msg, is_error=False)\n-        return info, info.pop('__files_to_move', None)\n+        return info, info.pop(\"__files_to_move\", None)\n \n     def post_process(self, filename, info, files_to_move=None):\n         \"\"\"Run all the postprocessors on the given file.\"\"\"\n-        info['filepath'] = filename\n-        info['__files_to_move'] = files_to_move or {}\n-        info = self.run_all_pps('post_process', info, additional_pps=info.get('__postprocessors'))\n+        info[\"filepath\"] = filename\n+        info[\"__files_to_move\"] = files_to_move or {}\n+        info = self.run_all_pps(\n+            \"post_process\", info, additional_pps=info.get(\"__postprocessors\")\n+        )\n         info = self.run_pp(MoveFilesAfterDownloadPP(self), info)\n-        del info['__files_to_move']\n-        return self.run_all_pps('after_move', info)\n+        del info[\"__files_to_move\"]\n+        return self.run_all_pps(\"after_move\", info)\n \n     def _make_archive_id(self, info_dict):\n-        video_id = info_dict.get('id')\n+        video_id = info_dict.get(\"id\")\n         if not video_id:\n             return\n         # Future-proof against any change in case\n         # and backwards compatibility with prior versions\n-        extractor = info_dict.get('extractor_key') or info_dict.get('ie_key')  # key in a playlist\n+        extractor = info_dict.get(\"extractor_key\") or info_dict.get(\n+            \"ie_key\"\n+        )  # key in a playlist\n         if extractor is None:\n-            url = str_or_none(info_dict.get('url'))\n+            url = str_or_none(info_dict.get(\"url\"))\n             if not url:\n                 return\n             # Try to find matching extractor for the URL and take its ie_key\n@@ -3702,216 +4621,289 @@ def in_download_archive(self, info_dict):\n             return False\n \n         vid_ids = [self._make_archive_id(info_dict)]\n-        vid_ids.extend(info_dict.get('_old_archive_ids') or [])\n+        vid_ids.extend(info_dict.get(\"_old_archive_ids\") or [])\n         return any(id_ in self.archive for id_ in vid_ids)\n \n     def record_download_archive(self, info_dict):\n-        fn = self.params.get('download_archive')\n+        fn = self.params.get(\"download_archive\")\n         if fn is None:\n             return\n         vid_id = self._make_archive_id(info_dict)\n         assert vid_id\n \n-        self.write_debug(f'Adding to archive: {vid_id}')\n+        self.write_debug(f\"Adding to archive: {vid_id}\")\n         if is_path_like(fn):\n-            with locked_file(fn, 'a', encoding='utf-8') as archive_file:\n-                archive_file.write(vid_id + '\\n')\n+            with locked_file(fn, \"a\", encoding=\"utf-8\") as archive_file:\n+                archive_file.write(vid_id + \"\\n\")\n         self.archive.add(vid_id)\n \n     @staticmethod\n-    def format_resolution(format, default='unknown'):\n-        if format.get('vcodec') == 'none' and format.get('acodec') != 'none':\n-            return 'audio only'\n-        if format.get('resolution') is not None:\n-            return format['resolution']\n-        if format.get('width') and format.get('height'):\n-            return '%dx%d' % (format['width'], format['height'])\n-        elif format.get('height'):\n-            return '%sp' % format['height']\n-        elif format.get('width'):\n-            return '%dx?' % format['width']\n+    def format_resolution(format, default=\"unknown\"):\n+        if format.get(\"vcodec\") == \"none\" and format.get(\"acodec\") != \"none\":\n+            return \"audio only\"\n+        if format.get(\"resolution\") is not None:\n+            return format[\"resolution\"]\n+        if format.get(\"width\") and format.get(\"height\"):\n+            return \"%dx%d\" % (format[\"width\"], format[\"height\"])\n+        elif format.get(\"height\"):\n+            return \"%sp\" % format[\"height\"]\n+        elif format.get(\"width\"):\n+            return \"%dx?\" % format[\"width\"]\n         return default\n \n     def _list_format_headers(self, *headers):\n-        if self.params.get('listformats_table', True) is not False:\n+        if self.params.get(\"listformats_table\", True) is not False:\n             return [self._format_out(header, self.Styles.HEADERS) for header in headers]\n         return headers\n \n     def _format_note(self, fdict):\n-        res = ''\n-        if fdict.get('ext') in ['f4f', 'f4m']:\n-            res += '(unsupported)'\n-        if fdict.get('language'):\n+        res = \"\"\n+        if fdict.get(\"ext\") in [\"f4f\", \"f4m\"]:\n+            res += \"(unsupported)\"\n+        if fdict.get(\"language\"):\n             if res:\n-                res += ' '\n-            res += '[%s]' % fdict['language']\n-        if fdict.get('format_note') is not None:\n+                res += \" \"\n+            res += \"[%s]\" % fdict[\"language\"]\n+        if fdict.get(\"format_note\") is not None:\n             if res:\n-                res += ' '\n-            res += fdict['format_note']\n-        if fdict.get('tbr') is not None:\n+                res += \" \"\n+            res += fdict[\"format_note\"]\n+        if fdict.get(\"tbr\") is not None:\n             if res:\n-                res += ', '\n-            res += '%4dk' % fdict['tbr']\n-        if fdict.get('container') is not None:\n+                res += \", \"\n+            res += \"%4dk\" % fdict[\"tbr\"]\n+        if fdict.get(\"container\") is not None:\n             if res:\n-                res += ', '\n-            res += '%s container' % fdict['container']\n-        if (fdict.get('vcodec') is not None\n-                and fdict.get('vcodec') != 'none'):\n+                res += \", \"\n+            res += \"%s container\" % fdict[\"container\"]\n+        if fdict.get(\"vcodec\") is not None and fdict.get(\"vcodec\") != \"none\":\n             if res:\n-                res += ', '\n-            res += fdict['vcodec']\n-            if fdict.get('vbr') is not None:\n-                res += '@'\n-        elif fdict.get('vbr') is not None and fdict.get('abr') is not None:\n-            res += 'video@'\n-        if fdict.get('vbr') is not None:\n-            res += '%4dk' % fdict['vbr']\n-        if fdict.get('fps') is not None:\n+                res += \", \"\n+            res += fdict[\"vcodec\"]\n+            if fdict.get(\"vbr\") is not None:\n+                res += \"@\"\n+        elif fdict.get(\"vbr\") is not None and fdict.get(\"abr\") is not None:\n+            res += \"video@\"\n+        if fdict.get(\"vbr\") is not None:\n+            res += \"%4dk\" % fdict[\"vbr\"]\n+        if fdict.get(\"fps\") is not None:\n             if res:\n-                res += ', '\n-            res += '%sfps' % fdict['fps']\n-        if fdict.get('acodec') is not None:\n+                res += \", \"\n+            res += \"%sfps\" % fdict[\"fps\"]\n+        if fdict.get(\"acodec\") is not None:\n             if res:\n-                res += ', '\n-            if fdict['acodec'] == 'none':\n-                res += 'video only'\n+                res += \", \"\n+            if fdict[\"acodec\"] == \"none\":\n+                res += \"video only\"\n             else:\n-                res += '%-5s' % fdict['acodec']\n-        elif fdict.get('abr') is not None:\n+                res += \"%-5s\" % fdict[\"acodec\"]\n+        elif fdict.get(\"abr\") is not None:\n             if res:\n-                res += ', '\n-            res += 'audio'\n-        if fdict.get('abr') is not None:\n-            res += '@%3dk' % fdict['abr']\n-        if fdict.get('asr') is not None:\n-            res += ' (%5dHz)' % fdict['asr']\n-        if fdict.get('filesize') is not None:\n+                res += \", \"\n+            res += \"audio\"\n+        if fdict.get(\"abr\") is not None:\n+            res += \"@%3dk\" % fdict[\"abr\"]\n+        if fdict.get(\"asr\") is not None:\n+            res += \" (%5dHz)\" % fdict[\"asr\"]\n+        if fdict.get(\"filesize\") is not None:\n             if res:\n-                res += ', '\n-            res += format_bytes(fdict['filesize'])\n-        elif fdict.get('filesize_approx') is not None:\n+                res += \", \"\n+            res += format_bytes(fdict[\"filesize\"])\n+        elif fdict.get(\"filesize_approx\") is not None:\n             if res:\n-                res += ', '\n-            res += '~' + format_bytes(fdict['filesize_approx'])\n+                res += \", \"\n+            res += \"~\" + format_bytes(fdict[\"filesize_approx\"])\n         return res\n \n     def _get_formats(self, info_dict):\n-        if info_dict.get('formats') is None:\n-            if info_dict.get('url') and info_dict.get('_type', 'video') == 'video':\n+        if info_dict.get(\"formats\") is None:\n+            if info_dict.get(\"url\") and info_dict.get(\"_type\", \"video\") == \"video\":\n                 return [info_dict]\n             return []\n-        return info_dict['formats']\n+        return info_dict[\"formats\"]\n \n     def render_formats_table(self, info_dict):\n         formats = self._get_formats(info_dict)\n         if not formats:\n             return\n-        if not self.params.get('listformats_table', True) is not False:\n+        if not self.params.get(\"listformats_table\", True) is not False:\n             table = [\n                 [\n-                    format_field(f, 'format_id'),\n-                    format_field(f, 'ext'),\n+                    format_field(f, \"format_id\"),\n+                    format_field(f, \"ext\"),\n                     self.format_resolution(f),\n-                    self._format_note(f)\n-                ] for f in formats if (f.get('preference') or 0) >= -1000]\n-            return render_table(['format code', 'extension', 'resolution', 'note'], table, extra_gap=1)\n+                    self._format_note(f),\n+                ]\n+                for f in formats\n+                if (f.get(\"preference\") or 0) >= -1000\n+            ]\n+            return render_table(\n+                [\"format code\", \"extension\", \"resolution\", \"note\"], table, extra_gap=1\n+            )\n \n         def simplified_codec(f, field):\n-            assert field in ('acodec', 'vcodec')\n+            assert field in (\"acodec\", \"vcodec\")\n             codec = f.get(field)\n             if not codec:\n-                return 'unknown'\n-            elif codec != 'none':\n-                return '.'.join(codec.split('.')[:4])\n-\n-            if field == 'vcodec' and f.get('acodec') == 'none':\n-                return 'images'\n-            elif field == 'acodec' and f.get('vcodec') == 'none':\n-                return ''\n-            return self._format_out('audio only' if field == 'vcodec' else 'video only',\n-                                    self.Styles.SUPPRESS)\n-\n-        delim = self._format_out('\\u2502', self.Styles.DELIM, '|', test_encoding=True)\n+                return \"unknown\"\n+            elif codec != \"none\":\n+                return \".\".join(codec.split(\".\")[:4])\n+\n+            if field == \"vcodec\" and f.get(\"acodec\") == \"none\":\n+                return \"images\"\n+            elif field == \"acodec\" and f.get(\"vcodec\") == \"none\":\n+                return \"\"\n+            return self._format_out(\n+                \"audio only\" if field == \"vcodec\" else \"video only\",\n+                self.Styles.SUPPRESS,\n+            )\n+\n+        delim = self._format_out(\"\\u2502\", self.Styles.DELIM, \"|\", test_encoding=True)\n         table = [\n             [\n-                self._format_out(format_field(f, 'format_id'), self.Styles.ID),\n-                format_field(f, 'ext'),\n-                format_field(f, func=self.format_resolution, ignore=('audio only', 'images')),\n-                format_field(f, 'fps', '\\t%d', func=round),\n-                format_field(f, 'dynamic_range', '%s', ignore=(None, 'SDR')).replace('HDR', ''),\n-                format_field(f, 'audio_channels', '\\t%s'),\n-                delim, (\n-                    format_field(f, 'filesize', ' \\t%s', func=format_bytes)\n-                    or format_field(f, 'filesize_approx', '\u2248\\t%s', func=format_bytes)\n-                    or format_field(try_call(lambda: format_bytes(int(info_dict['duration'] * f['tbr'] * (1024 / 8)))),\n-                                    None, self._format_out('~\\t%s', self.Styles.SUPPRESS))),\n-                format_field(f, 'tbr', '\\t%dk', func=round),\n-                shorten_protocol_name(f.get('protocol', '')),\n+                self._format_out(format_field(f, \"format_id\"), self.Styles.ID),\n+                format_field(f, \"ext\"),\n+                format_field(\n+                    f, func=self.format_resolution, ignore=(\"audio only\", \"images\")\n+                ),\n+                format_field(f, \"fps\", \"\\t%d\", func=round),\n+                format_field(f, \"dynamic_range\", \"%s\", ignore=(None, \"SDR\")).replace(\n+                    \"HDR\", \"\"\n+                ),\n+                format_field(f, \"audio_channels\", \"\\t%s\"),\n                 delim,\n-                simplified_codec(f, 'vcodec'),\n-                format_field(f, 'vbr', '\\t%dk', func=round),\n-                simplified_codec(f, 'acodec'),\n-                format_field(f, 'abr', '\\t%dk', func=round),\n-                format_field(f, 'asr', '\\t%s', func=format_decimal_suffix),\n-                join_nonempty(format_field(f, 'language', '[%s]'), join_nonempty(\n-                    self._format_out('UNSUPPORTED', self.Styles.BAD_FORMAT) if f.get('ext') in ('f4f', 'f4m') else None,\n-                    (self._format_out('Maybe DRM', self.Styles.WARNING) if f.get('has_drm') == 'maybe'\n-                     else self._format_out('DRM', self.Styles.BAD_FORMAT) if f.get('has_drm') else None),\n-                    format_field(f, 'format_note'),\n-                    format_field(f, 'container', ignore=(None, f.get('ext'))),\n-                    delim=', '), delim=' '),\n-            ] for f in formats if f.get('preference') is None or f['preference'] >= -1000]\n+                (\n+                    format_field(f, \"filesize\", \" \\t%s\", func=format_bytes)\n+                    or format_field(f, \"filesize_approx\", \"\u2248\\t%s\", func=format_bytes)\n+                    or format_field(\n+                        try_call(\n+                            lambda: format_bytes(\n+                                int(info_dict[\"duration\"] * f[\"tbr\"] * (1024 / 8))\n+                            )\n+                        ),\n+                        None,\n+                        self._format_out(\"~\\t%s\", self.Styles.SUPPRESS),\n+                    )\n+                ),\n+                format_field(f, \"tbr\", \"\\t%dk\", func=round),\n+                shorten_protocol_name(f.get(\"protocol\", \"\")),\n+                delim,\n+                simplified_codec(f, \"vcodec\"),\n+                format_field(f, \"vbr\", \"\\t%dk\", func=round),\n+                simplified_codec(f, \"acodec\"),\n+                format_field(f, \"abr\", \"\\t%dk\", func=round),\n+                format_field(f, \"asr\", \"\\t%s\", func=format_decimal_suffix),\n+                join_nonempty(\n+                    format_field(f, \"language\", \"[%s]\"),\n+                    join_nonempty(\n+                        self._format_out(\"UNSUPPORTED\", self.Styles.BAD_FORMAT)\n+                        if f.get(\"ext\") in (\"f4f\", \"f4m\")\n+                        else None,\n+                        (\n+                            self._format_out(\"Maybe DRM\", self.Styles.WARNING)\n+                            if f.get(\"has_drm\") == \"maybe\"\n+                            else self._format_out(\"DRM\", self.Styles.BAD_FORMAT)\n+                            if f.get(\"has_drm\")\n+                            else None\n+                        ),\n+                        format_field(f, \"format_note\"),\n+                        format_field(f, \"container\", ignore=(None, f.get(\"ext\"))),\n+                        delim=\", \",\n+                    ),\n+                    delim=\" \",\n+                ),\n+            ]\n+            for f in formats\n+            if f.get(\"preference\") is None or f[\"preference\"] >= -1000\n+        ]\n         header_line = self._list_format_headers(\n-            'ID', 'EXT', 'RESOLUTION', '\\tFPS', 'HDR', 'CH', delim, '\\tFILESIZE', '\\tTBR', 'PROTO',\n-            delim, 'VCODEC', '\\tVBR', 'ACODEC', '\\tABR', '\\tASR', 'MORE INFO')\n+            \"ID\",\n+            \"EXT\",\n+            \"RESOLUTION\",\n+            \"\\tFPS\",\n+            \"HDR\",\n+            \"CH\",\n+            delim,\n+            \"\\tFILESIZE\",\n+            \"\\tTBR\",\n+            \"PROTO\",\n+            delim,\n+            \"VCODEC\",\n+            \"\\tVBR\",\n+            \"ACODEC\",\n+            \"\\tABR\",\n+            \"\\tASR\",\n+            \"MORE INFO\",\n+        )\n \n         return render_table(\n-            header_line, table, hide_empty=True,\n-            delim=self._format_out('\\u2500', self.Styles.DELIM, '-', test_encoding=True))\n+            header_line,\n+            table,\n+            hide_empty=True,\n+            delim=self._format_out(\n+                \"\\u2500\", self.Styles.DELIM, \"-\", test_encoding=True\n+            ),\n+        )\n \n     def render_thumbnails_table(self, info_dict):\n-        thumbnails = list(info_dict.get('thumbnails') or [])\n+        thumbnails = list(info_dict.get(\"thumbnails\") or [])\n         if not thumbnails:\n             return None\n         return render_table(\n-            self._list_format_headers('ID', 'Width', 'Height', 'URL'),\n-            [[t.get('id'), t.get('width') or 'unknown', t.get('height') or 'unknown', t['url']] for t in thumbnails])\n+            self._list_format_headers(\"ID\", \"Width\", \"Height\", \"URL\"),\n+            [\n+                [\n+                    t.get(\"id\"),\n+                    t.get(\"width\") or \"unknown\",\n+                    t.get(\"height\") or \"unknown\",\n+                    t[\"url\"],\n+                ]\n+                for t in thumbnails\n+            ],\n+        )\n \n     def render_subtitles_table(self, video_id, subtitles):\n         def _row(lang, formats):\n-            exts, names = zip(*((f['ext'], f.get('name') or 'unknown') for f in reversed(formats)))\n+            exts, names = zip(\n+                *((f[\"ext\"], f.get(\"name\") or \"unknown\") for f in reversed(formats))\n+            )\n             if len(set(names)) == 1:\n-                names = [] if names[0] == 'unknown' else names[:1]\n-            return [lang, ', '.join(names), ', '.join(exts)]\n+                names = [] if names[0] == \"unknown\" else names[:1]\n+            return [lang, \", \".join(names), \", \".join(exts)]\n \n         if not subtitles:\n             return None\n         return render_table(\n-            self._list_format_headers('Language', 'Name', 'Formats'),\n+            self._list_format_headers(\"Language\", \"Name\", \"Formats\"),\n             [_row(lang, formats) for lang, formats in subtitles.items()],\n-            hide_empty=True)\n+            hide_empty=True,\n+        )\n \n     def __list_table(self, video_id, name, func, *args):\n         table = func(*args)\n         if not table:\n-            self.to_screen(f'{video_id} has no {name}')\n+            self.to_screen(f\"{video_id} has no {name}\")\n             return\n-        self.to_screen(f'[info] Available {name} for {video_id}:')\n+        self.to_screen(f\"[info] Available {name} for {video_id}:\")\n         self.to_stdout(table)\n \n     def list_formats(self, info_dict):\n-        self.__list_table(info_dict['id'], 'formats', self.render_formats_table, info_dict)\n+        self.__list_table(\n+            info_dict[\"id\"], \"formats\", self.render_formats_table, info_dict\n+        )\n \n     def list_thumbnails(self, info_dict):\n-        self.__list_table(info_dict['id'], 'thumbnails', self.render_thumbnails_table, info_dict)\n+        self.__list_table(\n+            info_dict[\"id\"], \"thumbnails\", self.render_thumbnails_table, info_dict\n+        )\n \n-    def list_subtitles(self, video_id, subtitles, name='subtitles'):\n-        self.__list_table(video_id, name, self.render_subtitles_table, video_id, subtitles)\n+    def list_subtitles(self, video_id, subtitles, name=\"subtitles\"):\n+        self.__list_table(\n+            video_id, name, self.render_subtitles_table, video_id, subtitles\n+        )\n \n     def print_debug_header(self):\n-        if not self.params.get('verbose'):\n+        if not self.params.get(\"verbose\"):\n             return\n \n         from . import _IN_CLI  # Must be delayed import\n@@ -3920,126 +4912,169 @@ def print_debug_header(self):\n         from .extractor.extractors import _LAZY_LOADER\n         from .extractor.extractors import (\n             _PLUGIN_CLASSES as plugin_ies,\n-            _PLUGIN_OVERRIDES as plugin_ie_overrides\n+            _PLUGIN_OVERRIDES as plugin_ie_overrides,\n         )\n \n         def get_encoding(stream):\n-            ret = str(getattr(stream, 'encoding', 'missing (%s)' % type(stream).__name__))\n+            ret = str(\n+                getattr(stream, \"encoding\", \"missing (%s)\" % type(stream).__name__)\n+            )\n             additional_info = []\n-            if os.environ.get('TERM', '').lower() == 'dumb':\n-                additional_info.append('dumb')\n+            if os.environ.get(\"TERM\", \"\").lower() == \"dumb\":\n+                additional_info.append(\"dumb\")\n             if not supports_terminal_sequences(stream):\n                 from .utils import WINDOWS_VT_MODE  # Must be imported locally\n-                additional_info.append('No VT' if WINDOWS_VT_MODE is False else 'No ANSI')\n+\n+                additional_info.append(\n+                    \"No VT\" if WINDOWS_VT_MODE is False else \"No ANSI\"\n+                )\n             if additional_info:\n-                ret = f'{ret} ({\",\".join(additional_info)})'\n+                ret = f\"{ret} ({','.join(additional_info)})\"\n             return ret\n \n-        encoding_str = 'Encodings: locale %s, fs %s, pref %s, %s' % (\n+        encoding_str = \"Encodings: locale %s, fs %s, pref %s, %s\" % (\n             locale.getpreferredencoding(),\n             sys.getfilesystemencoding(),\n             self.get_encoding(),\n-            ', '.join(\n-                f'{key} {get_encoding(stream)}' for key, stream in self._out_files.items_\n-                if stream is not None and key != 'console')\n+            \", \".join(\n+                f\"{key} {get_encoding(stream)}\"\n+                for key, stream in self._out_files.items_\n+                if stream is not None and key != \"console\"\n+            ),\n         )\n \n-        logger = self.params.get('logger')\n+        logger = self.params.get(\"logger\")\n         if logger:\n-            write_debug = lambda msg: logger.debug(f'[debug] {msg}')\n+            write_debug = lambda msg: logger.debug(f\"[debug] {msg}\")\n             write_debug(encoding_str)\n         else:\n-            write_string(f'[debug] {encoding_str}\\n', encoding=None)\n-            write_debug = lambda msg: self._write_string(f'[debug] {msg}\\n')\n+            write_string(f\"[debug] {encoding_str}\\n\", encoding=None)\n+            write_debug = lambda msg: self._write_string(f\"[debug] {msg}\\n\")\n \n         source = detect_variant()\n-        if VARIANT not in (None, 'pip'):\n-            source += '*'\n+        if VARIANT not in (None, \"pip\"):\n+            source += \"*\"\n         klass = type(self)\n-        write_debug(join_nonempty(\n-            f'{REPOSITORY.rpartition(\"/\")[2]} version',\n-            _make_label(ORIGIN, CHANNEL.partition('@')[2] or __version__, __version__),\n-            f'[{RELEASE_GIT_HEAD[:9]}]' if RELEASE_GIT_HEAD else '',\n-            '' if source == 'unknown' else f'({source})',\n-            '' if _IN_CLI else 'API' if klass == YoutubeDL else f'API:{self.__module__}.{klass.__qualname__}',\n-            delim=' '))\n+        write_debug(\n+            join_nonempty(\n+                f\"{REPOSITORY.rpartition('/')[2]} version\",\n+                _make_label(\n+                    ORIGIN, CHANNEL.partition(\"@\")[2] or __version__, __version__\n+                ),\n+                f\"[{RELEASE_GIT_HEAD[:9]}]\" if RELEASE_GIT_HEAD else \"\",\n+                \"\" if source == \"unknown\" else f\"({source})\",\n+                \"\"\n+                if _IN_CLI\n+                else \"API\"\n+                if klass == YoutubeDL\n+                else f\"API:{self.__module__}.{klass.__qualname__}\",\n+                delim=\" \",\n+            )\n+        )\n \n         if not _IN_CLI:\n-            write_debug(f'params: {self.params}')\n+            write_debug(f\"params: {self.params}\")\n \n         if not _LAZY_LOADER:\n-            if os.environ.get('YTDLP_NO_LAZY_EXTRACTORS'):\n-                write_debug('Lazy loading extractors is forcibly disabled')\n+            if os.environ.get(\"YTDLP_NO_LAZY_EXTRACTORS\"):\n+                write_debug(\"Lazy loading extractors is forcibly disabled\")\n             else:\n-                write_debug('Lazy loading extractors is disabled')\n-        if self.params['compat_opts']:\n-            write_debug('Compatibility options: %s' % ', '.join(self.params['compat_opts']))\n+                write_debug(\"Lazy loading extractors is disabled\")\n+        if self.params[\"compat_opts\"]:\n+            write_debug(\n+                \"Compatibility options: %s\" % \", \".join(self.params[\"compat_opts\"])\n+            )\n \n         if current_git_head():\n-            write_debug(f'Git HEAD: {current_git_head()}')\n+            write_debug(f\"Git HEAD: {current_git_head()}\")\n         write_debug(system_identifier())\n \n-        exe_versions, ffmpeg_features = FFmpegPostProcessor.get_versions_and_features(self)\n+        exe_versions, ffmpeg_features = FFmpegPostProcessor.get_versions_and_features(\n+            self\n+        )\n         ffmpeg_features = {key for key, val in ffmpeg_features.items() if val}\n         if ffmpeg_features:\n-            exe_versions['ffmpeg'] += ' (%s)' % ','.join(sorted(ffmpeg_features))\n+            exe_versions[\"ffmpeg\"] += \" (%s)\" % \",\".join(sorted(ffmpeg_features))\n \n-        exe_versions['rtmpdump'] = rtmpdump_version()\n-        exe_versions['phantomjs'] = PhantomJSwrapper._version()\n-        exe_str = ', '.join(\n-            f'{exe} {v}' for exe, v in sorted(exe_versions.items()) if v\n-        ) or 'none'\n-        write_debug('exe versions: %s' % exe_str)\n+        exe_versions[\"rtmpdump\"] = rtmpdump_version()\n+        exe_versions[\"phantomjs\"] = PhantomJSwrapper._version()\n+        exe_str = (\n+            \", \".join(f\"{exe} {v}\" for exe, v in sorted(exe_versions.items()) if v)\n+            or \"none\"\n+        )\n+        write_debug(\"exe versions: %s\" % exe_str)\n \n         from .compat.compat_utils import get_package_info\n         from .dependencies import available_dependencies\n \n-        write_debug('Optional libraries: %s' % (', '.join(sorted({\n-            join_nonempty(*get_package_info(m)) for m in available_dependencies.values()\n-        })) or 'none'))\n-\n-        write_debug(f'Proxy map: {self.proxies}')\n-        write_debug(f'Request Handlers: {\", \".join(rh.RH_NAME for rh in self._request_director.handlers.values())}')\n-        for plugin_type, plugins in {'Extractor': plugin_ies, 'Post-Processor': plugin_pps}.items():\n-            display_list = ['%s%s' % (\n-                klass.__name__, '' if klass.__name__ == name else f' as {name}')\n-                for name, klass in plugins.items()]\n-            if plugin_type == 'Extractor':\n-                display_list.extend(f'{plugins[-1].IE_NAME.partition(\"+\")[2]} ({parent.__name__})'\n-                                    for parent, plugins in plugin_ie_overrides.items())\n+        write_debug(\n+            \"Optional libraries: %s\"\n+            % (\n+                \", \".join(\n+                    sorted(\n+                        {\n+                            join_nonempty(*get_package_info(m))\n+                            for m in available_dependencies.values()\n+                        }\n+                    )\n+                )\n+                or \"none\"\n+            )\n+        )\n+\n+        write_debug(f\"Proxy map: {self.proxies}\")\n+        write_debug(\n+            f\"Request Handlers: {', '.join(rh.RH_NAME for rh in self._request_director.handlers.values())}\"\n+        )\n+        for plugin_type, plugins in {\n+            \"Extractor\": plugin_ies,\n+            \"Post-Processor\": plugin_pps,\n+        }.items():\n+            display_list = [\n+                \"%s%s\"\n+                % (klass.__name__, \"\" if klass.__name__ == name else f\" as {name}\")\n+                for name, klass in plugins.items()\n+            ]\n+            if plugin_type == \"Extractor\":\n+                display_list.extend(\n+                    f\"{plugins[-1].IE_NAME.partition('+')[2]} ({parent.__name__})\"\n+                    for parent, plugins in plugin_ie_overrides.items()\n+                )\n             if not display_list:\n                 continue\n-            write_debug(f'{plugin_type} Plugins: {\", \".join(sorted(display_list))}')\n+            write_debug(f\"{plugin_type} Plugins: {', '.join(sorted(display_list))}\")\n \n         plugin_dirs = plugin_directories()\n         if plugin_dirs:\n-            write_debug(f'Plugin directories: {plugin_dirs}')\n+            write_debug(f\"Plugin directories: {plugin_dirs}\")\n \n         # Not implemented\n-        if False and self.params.get('call_home'):\n-            ipaddr = self.urlopen('https://yt-dl.org/ip').read().decode()\n-            write_debug('Public IP address: %s' % ipaddr)\n-            latest_version = self.urlopen(\n-                'https://yt-dl.org/latest/version').read().decode()\n+        if False and self.params.get(\"call_home\"):\n+            ipaddr = self.urlopen(\"https://yt-dl.org/ip\").read().decode()\n+            write_debug(\"Public IP address: %s\" % ipaddr)\n+            latest_version = (\n+                self.urlopen(\"https://yt-dl.org/latest/version\").read().decode()\n+            )\n             if version_tuple(latest_version) > version_tuple(__version__):\n                 self.report_warning(\n-                    'You are using an outdated version (newest version: %s)! '\n-                    'See https://yt-dl.org/update if you need help updating.' %\n-                    latest_version)\n+                    \"You are using an outdated version (newest version: %s)! \"\n+                    \"See https://yt-dl.org/update if you need help updating.\"\n+                    % latest_version\n+                )\n \n     @functools.cached_property\n     def proxies(self):\n         \"\"\"Global proxy configuration\"\"\"\n-        opts_proxy = self.params.get('proxy')\n+        opts_proxy = self.params.get(\"proxy\")\n         if opts_proxy is not None:\n-            if opts_proxy == '':\n-                opts_proxy = '__noproxy__'\n-            proxies = {'all': opts_proxy}\n+            if opts_proxy == \"\":\n+                opts_proxy = \"__noproxy__\"\n+            proxies = {\"all\": opts_proxy}\n         else:\n             proxies = urllib.request.getproxies()\n             # compat. Set HTTPS_PROXY to __noproxy__ to revert\n-            if 'http' in proxies and 'https' not in proxies:\n-                proxies['https'] = proxies['http']\n+            if \"http\" in proxies and \"https\" not in proxies:\n+                proxies[\"https\"] = proxies[\"http\"]\n \n         return proxies\n \n@@ -4047,32 +5082,36 @@ def proxies(self):\n     def cookiejar(self):\n         \"\"\"Global cookiejar instance\"\"\"\n         return load_cookies(\n-            self.params.get('cookiefile'), self.params.get('cookiesfrombrowser'), self)\n+            self.params.get(\"cookiefile\"), self.params.get(\"cookiesfrombrowser\"), self\n+        )\n \n     @property\n     def _opener(self):\n         \"\"\"\n         Get a urllib OpenerDirector from the Urllib handler (deprecated).\n         \"\"\"\n-        self.deprecation_warning('YoutubeDL._opener is deprecated, use YoutubeDL.urlopen()')\n-        handler = self._request_director.handlers['Urllib']\n+        self.deprecation_warning(\n+            \"YoutubeDL._opener is deprecated, use YoutubeDL.urlopen()\"\n+        )\n+        handler = self._request_director.handlers[\"Urllib\"]\n         return handler._get_instance(cookiejar=self.cookiejar, proxies=self.proxies)\n \n     def urlopen(self, req):\n-        \"\"\" Start an HTTP download \"\"\"\n+        \"\"\"Start an HTTP download\"\"\"\n         if isinstance(req, str):\n             req = Request(req)\n         elif isinstance(req, urllib.request.Request):\n             self.deprecation_warning(\n-                'Passing a urllib.request.Request object to YoutubeDL.urlopen() is deprecated. '\n-                'Use yt_dlp.networking.common.Request instead.')\n+                \"Passing a urllib.request.Request object to YoutubeDL.urlopen() is deprecated. \"\n+                \"Use yt_dlp.networking.common.Request instead.\"\n+            )\n             req = urllib_req_to_req(req)\n         assert isinstance(req, Request)\n \n         # compat: Assume user:pass url params are basic auth\n         url, basic_auth_header = extract_basic_auth(req.url)\n         if basic_auth_header:\n-            req.headers['Authorization'] = basic_auth_header\n+            req.headers[\"Authorization\"] = basic_auth_header\n         req.url = sanitize_url(url)\n \n         clean_proxies(proxies=req.proxies, headers=req.headers)\n@@ -4085,66 +5124,85 @@ def urlopen(self, req):\n                 # FIXME: This depends on the order of errors.\n                 if not (ue.handler and ue.msg):\n                     continue\n-                if ue.handler.RH_KEY == 'Urllib' and 'unsupported url scheme: \"file\"' in ue.msg.lower():\n+                if (\n+                    ue.handler.RH_KEY == \"Urllib\"\n+                    and 'unsupported url scheme: \"file\"' in ue.msg.lower()\n+                ):\n                     raise RequestError(\n-                        'file:// URLs are disabled by default in yt-dlp for security reasons. '\n-                        'Use --enable-file-urls to enable at your own risk.', cause=ue) from ue\n+                        \"file:// URLs are disabled by default in yt-dlp for security reasons. \"\n+                        \"Use --enable-file-urls to enable at your own risk.\",\n+                        cause=ue,\n+                    ) from ue\n                 if 'unsupported proxy type: \"https\"' in ue.msg.lower():\n                     raise RequestError(\n-                        'To use an HTTPS proxy for this request, one of the following dependencies needs to be installed: requests')\n+                        \"To use an HTTPS proxy for this request, one of the following dependencies needs to be installed: requests\"\n+                    )\n \n                 elif (\n                     re.match(r'unsupported url scheme: \"wss?\"', ue.msg.lower())\n-                    and 'websockets' not in self._request_director.handlers\n+                    and \"websockets\" not in self._request_director.handlers\n                 ):\n                     raise RequestError(\n-                        'This request requires WebSocket support. '\n-                        'Ensure one of the following dependencies are installed: websockets',\n-                        cause=ue) from ue\n+                        \"This request requires WebSocket support. \"\n+                        \"Ensure one of the following dependencies are installed: websockets\",\n+                        cause=ue,\n+                    ) from ue\n             raise\n         except SSLError as e:\n-            if 'UNSAFE_LEGACY_RENEGOTIATION_DISABLED' in str(e):\n-                raise RequestError('UNSAFE_LEGACY_RENEGOTIATION_DISABLED: Try using --legacy-server-connect', cause=e) from e\n-            elif 'SSLV3_ALERT_HANDSHAKE_FAILURE' in str(e):\n+            if \"UNSAFE_LEGACY_RENEGOTIATION_DISABLED\" in str(e):\n+                raise RequestError(\n+                    \"UNSAFE_LEGACY_RENEGOTIATION_DISABLED: Try using --legacy-server-connect\",\n+                    cause=e,\n+                ) from e\n+            elif \"SSLV3_ALERT_HANDSHAKE_FAILURE\" in str(e):\n                 raise RequestError(\n-                    'SSLV3_ALERT_HANDSHAKE_FAILURE: The server may not support the current cipher list. '\n-                    'Try using --legacy-server-connect', cause=e) from e\n+                    \"SSLV3_ALERT_HANDSHAKE_FAILURE: The server may not support the current cipher list. \"\n+                    \"Try using --legacy-server-connect\",\n+                    cause=e,\n+                ) from e\n             raise\n         except HTTPError as e:  # TODO: Remove in a future release\n             raise _CompatHTTPError(e) from e\n \n     def build_request_director(self, handlers, preferences=None):\n         logger = _YDLLogger(self)\n-        headers = self.params['http_headers'].copy()\n+        headers = self.params[\"http_headers\"].copy()\n         proxies = self.proxies.copy()\n         clean_headers(headers)\n         clean_proxies(proxies, headers)\n \n-        director = RequestDirector(logger=logger, verbose=self.params.get('debug_printtraffic'))\n+        director = RequestDirector(\n+            logger=logger, verbose=self.params.get(\"debug_printtraffic\")\n+        )\n         for handler in handlers:\n-            director.add_handler(handler(\n-                logger=logger,\n-                headers=headers,\n-                cookiejar=self.cookiejar,\n-                proxies=proxies,\n-                prefer_system_certs='no-certifi' in self.params['compat_opts'],\n-                verify=not self.params.get('nocheckcertificate'),\n-                **traverse_obj(self.params, {\n-                    'verbose': 'debug_printtraffic',\n-                    'source_address': 'source_address',\n-                    'timeout': 'socket_timeout',\n-                    'legacy_ssl_support': 'legacyserverconnect',\n-                    'enable_file_urls': 'enable_file_urls',\n-                    'client_cert': {\n-                        'client_certificate': 'client_certificate',\n-                        'client_certificate_key': 'client_certificate_key',\n-                        'client_certificate_password': 'client_certificate_password',\n-                    },\n-                }),\n-            ))\n+            director.add_handler(\n+                handler(\n+                    logger=logger,\n+                    headers=headers,\n+                    cookiejar=self.cookiejar,\n+                    proxies=proxies,\n+                    prefer_system_certs=\"no-certifi\" in self.params[\"compat_opts\"],\n+                    verify=not self.params.get(\"nocheckcertificate\"),\n+                    **traverse_obj(\n+                        self.params,\n+                        {\n+                            \"verbose\": \"debug_printtraffic\",\n+                            \"source_address\": \"source_address\",\n+                            \"timeout\": \"socket_timeout\",\n+                            \"legacy_ssl_support\": \"legacyserverconnect\",\n+                            \"enable_file_urls\": \"enable_file_urls\",\n+                            \"client_cert\": {\n+                                \"client_certificate\": \"client_certificate\",\n+                                \"client_certificate_key\": \"client_certificate_key\",\n+                                \"client_certificate_password\": \"client_certificate_password\",\n+                            },\n+                        },\n+                    ),\n+                )\n+            )\n         director.preferences.update(preferences or [])\n-        if 'prefer-legacy-http-handler' in self.params['compat_opts']:\n-            director.preferences.add(lambda rh, _: 500 if rh.RH_KEY == 'Urllib' else 0)\n+        if \"prefer-legacy-http-handler\" in self.params[\"compat_opts\"]:\n+            director.preferences.add(lambda rh, _: 500 if rh.RH_KEY == \"Urllib\" else 0)\n         return director\n \n     def encode(self, s):\n@@ -4154,164 +5212,208 @@ def encode(self, s):\n         try:\n             return s.encode(self.get_encoding())\n         except UnicodeEncodeError as err:\n-            err.reason = err.reason + '. Check your system encoding configuration or use the --encoding option.'\n+            err.reason = (\n+                err.reason\n+                + \". Check your system encoding configuration or use the --encoding option.\"\n+            )\n             raise\n \n     def get_encoding(self):\n-        encoding = self.params.get('encoding')\n+        encoding = self.params.get(\"encoding\")\n         if encoding is None:\n             encoding = preferredencoding()\n         return encoding\n \n     def _write_info_json(self, label, ie_result, infofn, overwrite=None):\n-        ''' Write infojson and returns True = written, 'exists' = Already exists, False = skip, None = error '''\n+        \"\"\"Write infojson and returns True = written, 'exists' = Already exists, False = skip, None = error\"\"\"\n         if overwrite is None:\n-            overwrite = self.params.get('overwrites', True)\n-        if not self.params.get('writeinfojson'):\n+            overwrite = self.params.get(\"overwrites\", True)\n+        if not self.params.get(\"writeinfojson\"):\n             return False\n         elif not infofn:\n-            self.write_debug(f'Skipping writing {label} infojson')\n+            self.write_debug(f\"Skipping writing {label} infojson\")\n             return False\n         elif not self._ensure_dir_exists(infofn):\n             return None\n         elif not overwrite and os.path.exists(infofn):\n-            self.to_screen(f'[info] {label.title()} metadata is already present')\n-            return 'exists'\n+            self.to_screen(f\"[info] {label.title()} metadata is already present\")\n+            return \"exists\"\n \n-        self.to_screen(f'[info] Writing {label} metadata as JSON to: {infofn}')\n+        self.to_screen(f\"[info] Writing {label} metadata as JSON to: {infofn}\")\n         try:\n-            write_json_file(self.sanitize_info(ie_result, self.params.get('clean_infojson', True)), infofn)\n+            write_json_file(\n+                self.sanitize_info(ie_result, self.params.get(\"clean_infojson\", True)),\n+                infofn,\n+            )\n             return True\n         except OSError:\n-            self.report_error(f'Cannot write {label} metadata to JSON file {infofn}')\n+            self.report_error(f\"Cannot write {label} metadata to JSON file {infofn}\")\n             return None\n \n     def _write_description(self, label, ie_result, descfn):\n-        ''' Write description and returns True = written, False = skip, None = error '''\n-        if not self.params.get('writedescription'):\n+        \"\"\"Write description and returns True = written, False = skip, None = error\"\"\"\n+        if not self.params.get(\"writedescription\"):\n             return False\n         elif not descfn:\n-            self.write_debug(f'Skipping writing {label} description')\n+            self.write_debug(f\"Skipping writing {label} description\")\n             return False\n         elif not self._ensure_dir_exists(descfn):\n             return None\n-        elif not self.params.get('overwrites', True) and os.path.exists(descfn):\n-            self.to_screen(f'[info] {label.title()} description is already present')\n-        elif ie_result.get('description') is None:\n-            self.to_screen(f'[info] There\\'s no {label} description to write')\n+        elif not self.params.get(\"overwrites\", True) and os.path.exists(descfn):\n+            self.to_screen(f\"[info] {label.title()} description is already present\")\n+        elif ie_result.get(\"description\") is None:\n+            self.to_screen(f\"[info] There's no {label} description to write\")\n             return False\n         else:\n             try:\n-                self.to_screen(f'[info] Writing {label} description to: {descfn}')\n-                with open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:\n-                    descfile.write(ie_result['description'])\n+                self.to_screen(f\"[info] Writing {label} description to: {descfn}\")\n+                with open(encodeFilename(descfn), \"w\", encoding=\"utf-8\") as descfile:\n+                    descfile.write(ie_result[\"description\"])\n             except OSError:\n-                self.report_error(f'Cannot write {label} description file {descfn}')\n+                self.report_error(f\"Cannot write {label} description file {descfn}\")\n                 return None\n         return True\n \n     def _write_subtitles(self, info_dict, filename):\n-        ''' Write subtitles to file and return list of (sub_filename, final_sub_filename); or None if error'''\n+        \"\"\"Write subtitles to file and return list of (sub_filename, final_sub_filename); or None if error\"\"\"\n         ret = []\n-        subtitles = info_dict.get('requested_subtitles')\n-        if not (self.params.get('writesubtitles') or self.params.get('writeautomaticsub')):\n+        subtitles = info_dict.get(\"requested_subtitles\")\n+        if not (\n+            self.params.get(\"writesubtitles\") or self.params.get(\"writeautomaticsub\")\n+        ):\n             # subtitles download errors are already managed as troubles in relevant IE\n             # that way it will silently go on when used with unsupporting IE\n             return ret\n         elif not subtitles:\n-            self.to_screen('[info] There are no subtitles for the requested languages')\n+            self.to_screen(\"[info] There are no subtitles for the requested languages\")\n             return ret\n-        sub_filename_base = self.prepare_filename(info_dict, 'subtitle')\n+        sub_filename_base = self.prepare_filename(info_dict, \"subtitle\")\n         if not sub_filename_base:\n-            self.to_screen('[info] Skipping writing video subtitles')\n+            self.to_screen(\"[info] Skipping writing video subtitles\")\n             return ret\n \n         for sub_lang, sub_info in subtitles.items():\n-            sub_format = sub_info['ext']\n-            sub_filename = subtitles_filename(filename, sub_lang, sub_format, info_dict.get('ext'))\n-            sub_filename_final = subtitles_filename(sub_filename_base, sub_lang, sub_format, info_dict.get('ext'))\n+            sub_format = sub_info[\"ext\"]\n+            sub_filename = subtitles_filename(\n+                filename, sub_lang, sub_format, info_dict.get(\"ext\")\n+            )\n+            sub_filename_final = subtitles_filename(\n+                sub_filename_base, sub_lang, sub_format, info_dict.get(\"ext\")\n+            )\n             existing_sub = self.existing_file((sub_filename_final, sub_filename))\n             if existing_sub:\n-                self.to_screen(f'[info] Video subtitle {sub_lang}.{sub_format} is already present')\n-                sub_info['filepath'] = existing_sub\n+                self.to_screen(\n+                    f\"[info] Video subtitle {sub_lang}.{sub_format} is already present\"\n+                )\n+                sub_info[\"filepath\"] = existing_sub\n                 ret.append((existing_sub, sub_filename_final))\n                 continue\n \n-            self.to_screen(f'[info] Writing video subtitles to: {sub_filename}')\n-            if sub_info.get('data') is not None:\n+            self.to_screen(f\"[info] Writing video subtitles to: {sub_filename}\")\n+            if sub_info.get(\"data\") is not None:\n                 try:\n                     # Use newline='' to prevent conversion of newline characters\n                     # See https://github.com/ytdl-org/youtube-dl/issues/10268\n-                    with open(sub_filename, 'w', encoding='utf-8', newline='') as subfile:\n-                        subfile.write(sub_info['data'])\n-                    sub_info['filepath'] = sub_filename\n+                    with open(\n+                        sub_filename, \"w\", encoding=\"utf-8\", newline=\"\"\n+                    ) as subfile:\n+                        subfile.write(sub_info[\"data\"])\n+                    sub_info[\"filepath\"] = sub_filename\n                     ret.append((sub_filename, sub_filename_final))\n                     continue\n                 except OSError:\n-                    self.report_error(f'Cannot write video subtitles file {sub_filename}')\n+                    self.report_error(\n+                        f\"Cannot write video subtitles file {sub_filename}\"\n+                    )\n                     return None\n \n             try:\n                 sub_copy = sub_info.copy()\n-                sub_copy.setdefault('http_headers', info_dict.get('http_headers'))\n+                sub_copy.setdefault(\"http_headers\", info_dict.get(\"http_headers\"))\n                 self.dl(sub_filename, sub_copy, subtitle=True)\n-                sub_info['filepath'] = sub_filename\n+                sub_info[\"filepath\"] = sub_filename\n                 ret.append((sub_filename, sub_filename_final))\n-            except (DownloadError, ExtractorError, IOError, OSError, ValueError) + network_exceptions as err:\n-                msg = f'Unable to download video subtitles for {sub_lang!r}: {err}'\n-                if self.params.get('ignoreerrors') is not True:  # False or 'only_download'\n-                    if not self.params.get('ignoreerrors'):\n+            except (\n+                DownloadError,\n+                ExtractorError,\n+                IOError,\n+                OSError,\n+                ValueError,\n+            ) + network_exceptions as err:\n+                msg = f\"Unable to download video subtitles for {sub_lang!r}: {err}\"\n+                if (\n+                    self.params.get(\"ignoreerrors\") is not True\n+                ):  # False or 'only_download'\n+                    if not self.params.get(\"ignoreerrors\"):\n                         self.report_error(msg)\n                     raise DownloadError(msg)\n                 self.report_warning(msg)\n         return ret\n \n     def _write_thumbnails(self, label, info_dict, filename, thumb_filename_base=None):\n-        ''' Write thumbnails to file and return list of (thumb_filename, final_thumb_filename); or None if error '''\n-        write_all = self.params.get('write_all_thumbnails', False)\n+        \"\"\"Write thumbnails to file and return list of (thumb_filename, final_thumb_filename); or None if error\"\"\"\n+        write_all = self.params.get(\"write_all_thumbnails\", False)\n         thumbnails, ret = [], []\n-        if write_all or self.params.get('writethumbnail', False):\n-            thumbnails = info_dict.get('thumbnails') or []\n+        if write_all or self.params.get(\"writethumbnail\", False):\n+            thumbnails = info_dict.get(\"thumbnails\") or []\n             if not thumbnails:\n-                self.to_screen(f'[info] There are no {label} thumbnails to download')\n+                self.to_screen(f\"[info] There are no {label} thumbnails to download\")\n                 return ret\n         multiple = write_all and len(thumbnails) > 1\n \n         if thumb_filename_base is None:\n             thumb_filename_base = filename\n         if thumbnails and not thumb_filename_base:\n-            self.write_debug(f'Skipping writing {label} thumbnail')\n+            self.write_debug(f\"Skipping writing {label} thumbnail\")\n             return ret\n \n         if thumbnails and not self._ensure_dir_exists(filename):\n             return None\n \n         for idx, t in list(enumerate(thumbnails))[::-1]:\n-            thumb_ext = (f'{t[\"id\"]}.' if multiple else '') + determine_ext(t['url'], 'jpg')\n-            thumb_display_id = f'{label} thumbnail {t[\"id\"]}'\n-            thumb_filename = replace_extension(filename, thumb_ext, info_dict.get('ext'))\n-            thumb_filename_final = replace_extension(thumb_filename_base, thumb_ext, info_dict.get('ext'))\n+            thumb_ext = (f\"{t['id']}.\" if multiple else \"\") + determine_ext(\n+                t[\"url\"], \"jpg\"\n+            )\n+            thumb_display_id = f\"{label} thumbnail {t['id']}\"\n+            thumb_filename = replace_extension(\n+                filename, thumb_ext, info_dict.get(\"ext\")\n+            )\n+            thumb_filename_final = replace_extension(\n+                thumb_filename_base, thumb_ext, info_dict.get(\"ext\")\n+            )\n \n             existing_thumb = self.existing_file((thumb_filename_final, thumb_filename))\n             if existing_thumb:\n-                self.to_screen('[info] %s is already present' % (\n-                    thumb_display_id if multiple else f'{label} thumbnail').capitalize())\n-                t['filepath'] = existing_thumb\n+                self.to_screen(\n+                    \"[info] %s is already present\"\n+                    % (\n+                        thumb_display_id if multiple else f\"{label} thumbnail\"\n+                    ).capitalize()\n+                )\n+                t[\"filepath\"] = existing_thumb\n                 ret.append((existing_thumb, thumb_filename_final))\n             else:\n-                self.to_screen(f'[info] Downloading {thumb_display_id} ...')\n+                self.to_screen(f\"[info] Downloading {thumb_display_id} ...\")\n                 try:\n-                    uf = self.urlopen(Request(t['url'], headers=t.get('http_headers', {})))\n-                    self.to_screen(f'[info] Writing {thumb_display_id} to: {thumb_filename}')\n-                    with open(encodeFilename(thumb_filename), 'wb') as thumbf:\n+                    uf = self.urlopen(\n+                        Request(t[\"url\"], headers=t.get(\"http_headers\", {}))\n+                    )\n+                    self.to_screen(\n+                        f\"[info] Writing {thumb_display_id} to: {thumb_filename}\"\n+                    )\n+                    with open(encodeFilename(thumb_filename), \"wb\") as thumbf:\n                         shutil.copyfileobj(uf, thumbf)\n                     ret.append((thumb_filename, thumb_filename_final))\n-                    t['filepath'] = thumb_filename\n+                    t[\"filepath\"] = thumb_filename\n                 except network_exceptions as err:\n                     if isinstance(err, HTTPError) and err.status == 404:\n-                        self.to_screen(f'[info] {thumb_display_id.title()} does not exist')\n+                        self.to_screen(\n+                            f\"[info] {thumb_display_id.title()} does not exist\"\n+                        )\n                     else:\n-                        self.report_warning(f'Unable to download {thumb_display_id}: {err}')\n+                        self.report_warning(\n+                            f\"Unable to download {thumb_display_id}: {err}\"\n+                        )\n                     thumbnails.pop(idx)\n             if ret and not write_all:\n                 break\ndiff --git a/yt_dlp/__main__.py b/yt_dlp/__main__.py\nindex 78701df..2f219a8 100644\n--- a/yt_dlp/__main__.py\n+++ b/yt_dlp/__main__.py\n@@ -5,13 +5,14 @@\n \n import sys\n \n-if __package__ is None and not getattr(sys, 'frozen', False):\n+if __package__ is None and not getattr(sys, \"frozen\", False):\n     # direct call of __main__.py\n     import os.path\n+\n     path = os.path.realpath(os.path.abspath(__file__))\n     sys.path.insert(0, os.path.dirname(os.path.dirname(path)))\n \n import yt_dlp\n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     yt_dlp.main()\ndiff --git a/yt_dlp/extractor/_extractors.py b/yt_dlp/extractor/_extractors.py\nindex e5c6b21..ca13cb6 100644\n--- a/yt_dlp/extractor/_extractors.py\n+++ b/yt_dlp/extractor/_extractors.py\n@@ -89,10 +89,7 @@\n     AltCensoredIE,\n     AltCensoredChannelIE,\n )\n-from .alura import (\n-    AluraIE,\n-    AluraCourseIE\n-)\n+from .alura import AluraIE, AluraCourseIE\n from .amara import AmaraIE\n from .amcnetworks import AMCNetworksIE\n from .amazon import (\n@@ -284,14 +281,8 @@\n from .callin import CallinIE\n from .caltrans import CaltransIE\n from .cam4 import CAM4IE\n-from .camdemy import (\n-    CamdemyIE,\n-    CamdemyFolderIE\n-)\n-from .camfm import (\n-    CamFMEpisodeIE,\n-    CamFMShowIE\n-)\n+from .camdemy import CamdemyIE, CamdemyFolderIE\n+from .camfm import CamFMEpisodeIE, CamFMShowIE\n from .cammodels import CamModelsIE\n from .camsoda import CamsodaIE\n from .camtasia import CamtasiaEmbedIE\n@@ -421,10 +412,7 @@\n     CuriosityStreamSeriesIE,\n )\n from .cwtv import CWTVIE\n-from .cybrary import (\n-    CybraryIE,\n-    CybraryCourseIE\n-)\n+from .cybrary import CybraryIE, CybraryCourseIE\n from .dacast import (\n     DacastVODIE,\n     DacastPlaylistIE,\n@@ -506,25 +494,16 @@\n )\n from .dtube import DTubeIE\n from .dvtv import DVTVIE\n-from .duboku import (\n-    DubokuIE,\n-    DubokuPlaylistIE\n-)\n+from .duboku import DubokuIE, DubokuPlaylistIE\n from .dumpert import DumpertIE\n-from .deuxm import (\n-    DeuxMIE,\n-    DeuxMNewsIE\n-)\n+from .deuxm import DeuxMIE, DeuxMNewsIE\n from .digitalconcerthall import DigitalConcertHallIE\n from .discogs import DiscogsReleasePlaylistIE\n from .discovery import DiscoveryIE\n from .disney import DisneyIE\n from .dispeak import DigitallySpeakingIE\n from .dropbox import DropboxIE\n-from .dropout import (\n-    DropoutSeasonIE,\n-    DropoutIE\n-)\n+from .dropout import DropoutSeasonIE, DropoutIE\n from .duoplay import DuoplayIE\n from .dw import (\n     DWIE,\n@@ -589,10 +568,7 @@\n     FacebookRedirectURLIE,\n     FacebookReelIE,\n )\n-from .fancode import (\n-    FancodeVodIE,\n-    FancodeLiveIE\n-)\n+from .fancode import FancodeVodIE, FancodeLiveIE\n \n from .faz import FazIE\n from .fc2 import (\n@@ -647,7 +623,7 @@\n from .frontendmasters import (\n     FrontendMastersIE,\n     FrontendMastersLessonIE,\n-    FrontendMastersCourseIE\n+    FrontendMastersCourseIE,\n )\n from .freetv import (\n     FreeTvIE,\n@@ -699,7 +675,7 @@\n     GlobalPlayerLivePlaylistIE,\n     GlobalPlayerAudioIE,\n     GlobalPlayerAudioEpisodeIE,\n-    GlobalPlayerVideoIE\n+    GlobalPlayerVideoIE,\n )\n from .globo import (\n     GloboIE,\n@@ -725,11 +701,7 @@\n from .goshgay import GoshgayIE\n from .gotostage import GoToStageIE\n from .gputechconf import GPUTechConfIE\n-from .gronkh import (\n-    GronkhIE,\n-    GronkhFeedIE,\n-    GronkhVodsIE\n-)\n+from .gronkh import GronkhIE, GronkhFeedIE, GronkhVodsIE\n from .groupon import GrouponIE\n from .harpodeon import HarpodeonIE\n from .hbo import HBOIE\n@@ -795,10 +767,7 @@\n     IHeartRadioPodcastIE,\n )\n from .iltalehti import IltalehtiIE\n-from .imdb import (\n-    ImdbIE,\n-    ImdbListIE\n-)\n+from .imdb import ImdbIE, ImdbListIE\n from .imgur import (\n     ImgurIE,\n     ImgurAlbumIE,\n@@ -817,32 +786,19 @@\n )\n from .internazionale import InternazionaleIE\n from .internetvideoarchive import InternetVideoArchiveIE\n-from .iprima import (\n-    IPrimaIE,\n-    IPrimaCNNIE\n-)\n-from .iqiyi import (\n-    IqiyiIE,\n-    IqIE,\n-    IqAlbumIE\n-)\n+from .iprima import IPrimaIE, IPrimaCNNIE\n+from .iqiyi import IqiyiIE, IqIE, IqAlbumIE\n from .islamchannel import (\n     IslamChannelIE,\n     IslamChannelSeriesIE,\n )\n from .israelnationalnews import IsraelNationalNewsIE\n-from .itprotv import (\n-    ITProTVIE,\n-    ITProTVCourseIE\n-)\n+from .itprotv import ITProTVIE, ITProTVCourseIE\n from .itv import (\n     ITVIE,\n     ITVBTCCIE,\n )\n-from .ivi import (\n-    IviIE,\n-    IviCompilationIE\n-)\n+from .ivi import IviIE, IviCompilationIE\n from .ivideon import IvideonIE\n from .iwara import (\n     IwaraIE,\n@@ -960,10 +916,7 @@\n     LifeNewsIE,\n     LifeEmbedIE,\n )\n-from .likee import (\n-    LikeeIE,\n-    LikeeUserIE\n-)\n+from .likee import LikeeIE, LikeeUserIE\n from .limelight import (\n     LimelightMediaIE,\n     LimelightChannelIE,\n@@ -990,17 +943,9 @@\n )\n from .localnews8 import LocalNews8IE\n from .lovehomeporn import LoveHomePornIE\n-from .lrt import (\n-    LRTVODIE,\n-    LRTStreamIE\n-)\n-from .lumni import (\n-    LumniIE\n-)\n-from .lynda import (\n-    LyndaIE,\n-    LyndaCourseIE\n-)\n+from .lrt import LRTVODIE, LRTStreamIE\n+from .lumni import LumniIE\n+from .lynda import LyndaIE, LyndaCourseIE\n from .maariv import MaarivIE\n from .magellantv import MagellanTVIE\n from .magentamusik360 import MagentaMusik360IE\n@@ -1331,10 +1276,7 @@\n from .odatv import OdaTVIE\n from .odkmedia import OnDemandChinaEpisodeIE\n from .odnoklassniki import OdnoklassnikiIE\n-from .oftv import (\n-    OfTVIE,\n-    OfTVPlaylistIE\n-)\n+from .oftv import OfTVIE, OfTVPlaylistIE\n from .oktoberfesttv import OktoberfestTVIE\n from .olympics import OlympicsReplayIE\n from .on24 import On24IE\n@@ -1380,21 +1322,14 @@\n     PalcoMP3ArtistIE,\n     PalcoMP3VideoIE,\n )\n-from .panopto import (\n-    PanoptoIE,\n-    PanoptoListIE,\n-    PanoptoPlaylistIE\n-)\n+from .panopto import PanoptoIE, PanoptoListIE, PanoptoPlaylistIE\n from .paramountplus import (\n     ParamountPlusIE,\n     ParamountPlusSeriesIE,\n )\n from .parler import ParlerIE\n from .parlview import ParlviewIE\n-from .patreon import (\n-    PatreonIE,\n-    PatreonCampaignIE\n-)\n+from .patreon import PatreonIE, PatreonCampaignIE\n from .pbs import PBSIE, PBSKidsIE\n from .pearvideo import PearVideoIE\n from .peekvids import PeekVidsIE, PlayVidsIE\n@@ -1403,10 +1338,7 @@\n     PeerTubePlaylistIE,\n )\n from .peertv import PeerTVIE\n-from .peloton import (\n-    PelotonIE,\n-    PelotonLiveIE\n-)\n+from .peloton import PelotonIE, PelotonLiveIE\n from .performgroup import PerformGroupIE\n from .periscope import (\n     PeriscopeIE,\n@@ -1499,7 +1431,7 @@\n     PRXSeriesIE,\n     PRXAccountIE,\n     PRXStoriesSearchIE,\n-    PRXSeriesSearchIE\n+    PRXSeriesSearchIE,\n )\n from .puls4 import Puls4IE\n from .pyvideo import PyvideoIE\n@@ -1680,14 +1612,8 @@\n )\n from .rutv import RUTVIE\n from .ruutu import RuutuIE\n-from .ruv import (\n-    RuvIE,\n-    RuvSpilaIE\n-)\n-from .s4c import (\n-    S4CIE,\n-    S4CSeriesIE\n-)\n+from .ruv import RuvIE, RuvSpilaIE\n+from .s4c import S4CIE, S4CSeriesIE\n from .safari import (\n     SafariIE,\n     SafariApiIE,\n@@ -1788,22 +1714,16 @@\n     SoundcloudPlaylistIE,\n     SoundcloudSearchIE,\n )\n-from .soundgasm import (\n-    SoundgasmIE,\n-    SoundgasmProfileIE\n-)\n+from .soundgasm import SoundgasmIE, SoundgasmProfileIE\n from .southpark import (\n     SouthParkIE,\n     SouthParkDeIE,\n     SouthParkDkIE,\n     SouthParkEsIE,\n     SouthParkLatIE,\n-    SouthParkNlIE\n-)\n-from .sovietscloset import (\n-    SovietsClosetIE,\n-    SovietsClosetPlaylistIE\n+    SouthParkNlIE,\n )\n+from .sovietscloset import SovietsClosetIE, SovietsClosetPlaylistIE\n from .spankbang import (\n     SpankBangIE,\n     SpankBangPlaylistIE,\n@@ -2112,10 +2032,7 @@\n     TxxxIE,\n     PornTopIE,\n )\n-from .udemy import (\n-    UdemyIE,\n-    UdemyCourseIE\n-)\n+from .udemy import UdemyIE, UdemyCourseIE\n from .udn import UDNEmbedIE\n from .ufctv import (\n     UFCTVIE,\n@@ -2151,10 +2068,7 @@\n from .varzesh3 import Varzesh3IE\n from .vbox7 import Vbox7IE\n from .veo import VeoIE\n-from .veoh import (\n-    VeohIE,\n-    VeohUserIE\n-)\n+from .veoh import VeohIE, VeohUserIE\n from .vesti import VestiIE\n from .vevo import (\n     VevoIE,\n@@ -2192,11 +2106,7 @@\n     VideomoreSeasonIE,\n )\n from .videopress import VideoPressIE\n-from .vidio import (\n-    VidioIE,\n-    VidioPremierIE,\n-    VidioLiveIE\n-)\n+from .vidio import VidioIE, VidioPremierIE, VidioLiveIE\n from .vidlii import VidLiiIE\n from .vidly import VidlyIE\n from .viewlift import (\n@@ -2357,18 +2267,12 @@\n     XHamsterEmbedIE,\n     XHamsterUserIE,\n )\n-from .ximalaya import (\n-    XimalayaIE,\n-    XimalayaAlbumIE\n-)\n+from .ximalaya import XimalayaIE, XimalayaAlbumIE\n from .xinpianchang import XinpianchangIE\n from .xminus import XMinusIE\n from .xnxx import XNXXIE\n from .xstream import XstreamIE\n-from .xvideos import (\n-    XVideosIE,\n-    XVideosQuickiesIE\n-)\n+from .xvideos import XVideosIE, XVideosQuickiesIE\n from .xxxymovies import XXXYMoviesIE\n from .yahoo import (\n     YahooIE,\ndiff --git a/yt_dlp/extractor/extractors.py b/yt_dlp/extractor/extractors.py\nindex baa69d2..58d6f55 100644\n--- a/yt_dlp/extractor/extractors.py\n+++ b/yt_dlp/extractor/extractors.py\n@@ -4,21 +4,23 @@\n from ..plugins import load_plugins\n \n # NB: Must be before other imports so that plugins can be correctly injected\n-_PLUGIN_CLASSES = load_plugins('extractor', 'IE')\n+_PLUGIN_CLASSES = load_plugins(\"extractor\", \"IE\")\n \n _LAZY_LOADER = False\n-if not os.environ.get('YTDLP_NO_LAZY_EXTRACTORS'):\n+if not os.environ.get(\"YTDLP_NO_LAZY_EXTRACTORS\"):\n     with contextlib.suppress(ImportError):\n         from .lazy_extractors import *  # noqa: F403\n         from .lazy_extractors import _ALL_CLASSES\n+\n         _LAZY_LOADER = True\n \n if not _LAZY_LOADER:\n     from ._extractors import *  # noqa: F403\n+\n     _ALL_CLASSES = [  # noqa: F811\n         klass\n         for name, klass in globals().items()\n-        if name.endswith('IE') and name != 'GenericIE'\n+        if name.endswith(\"IE\") and name != \"GenericIE\"\n     ]\n     _ALL_CLASSES.append(GenericIE)  # noqa: F405\n \n"
    },
    {
        "id": 24,
        "sha_fail": "d985231d83ec0cb50784548dae26236dd03bd2a6",
        "diff": "diff --git a/tests/repositories/test_remove_repository.py b/tests/repositories/test_remove_repository.py\nindex 57effda..85fec92 100644\n--- a/tests/repositories/test_remove_repository.py\n+++ b/tests/repositories/test_remove_repository.py\n@@ -20,7 +20,7 @@ from tests.conftest import SnapshotFixture\n     \"category_test_data\",\n     category_test_data_parametrized(\n         skip_categories=[HacsCategory.PYTHON_SCRIPT],\n-        skip_reason=\"bug in cleanup, using repo name instad of file name.\",\n+        skip_reason=\"bug in cleanup, using repo name instead of file name.\",\n     ),\n )\n async def test_remove_repository(\n"
    },
    {
        "id": 25,
        "sha_fail": "5b9b7a0f0f73cc0257f1b41b4904dc9056e9baa1",
        "diff": "diff --git a/tests/common.py b/tests/common.py\nindex 190a2fb..3b3c42d 100644\n--- a/tests/common.py\n+++ b/tests/common.py\n@@ -13,7 +13,13 @@ from unittest.mock import AsyncMock, Mock, patch\n from aiohttp import ClientSession, ClientWebSocketResponse\n from aiohttp.typedefs import StrOrURL\n from awesomeversion import AwesomeVersion\n-from homeassistant import auth, bootstrap, config_entries, core as ha, config as ha_config\n+from homeassistant import (\n+    auth,\n+    bootstrap,\n+    config_entries,\n+    core as ha,\n+    config as ha_config,\n+)\n from homeassistant.auth import auth_store, models as auth_models\n from homeassistant.const import (\n     EVENT_HOMEASSISTANT_CLOSE,\n@@ -47,15 +53,19 @@ from custom_components.hacs.utils.logger import LOGGER\n _LOGGER = LOGGER\n TOKEN = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n INSTANCES = []\n-REQUEST_CONTEXT: ContextVar[pytest.FixtureRequest] = ContextVar(\"request_context\", default=None)\n+REQUEST_CONTEXT: ContextVar[pytest.FixtureRequest] = ContextVar(\n+    \"request_context\", default=None\n+)\n \n-IGNORED_BASE_FILES = set([\n+IGNORED_BASE_FILES = set(\n+    [\n         \"/config/automations.yaml\",\n         \"/config/configuration.yaml\",\n         \"/config/scenes.yaml\",\n         \"/config/scripts.yaml\",\n         \"/config/secrets.yaml\",\n-    ])\n+    ]\n+)\n \n \n def safe_json_dumps(data: dict | list) -> str:\n@@ -67,7 +77,9 @@ def safe_json_dumps(data: dict | list) -> str:\n     )\n \n \n-def recursive_remove_key(data: dict[str, Any], to_remove: Iterable[str]) -> dict[str, Any]:\n+def recursive_remove_key(\n+    data: dict[str, Any], to_remove: Iterable[str]\n+) -> dict[str, Any]:\n     if not isinstance(data, (Mapping, list)):\n         return data\n \n@@ -215,7 +227,9 @@ async def async_test_home_assistant(loop, tmpdir):\n     hass.data[bootstrap.DATA_REGISTRIES_LOADED] = None\n \n     hass.config_entries = config_entries.ConfigEntries(hass, {})\n-    hass.bus.async_listen_once(EVENT_HOMEASSISTANT_STOP, hass.config_entries._async_shutdown)\n+    hass.bus.async_listen_once(\n+        EVENT_HOMEASSISTANT_STOP, hass.config_entries._async_shutdown\n+    )\n \n     hass.state = ha.CoreState.running\n     await async_setup_component(hass, \"homeassistant\", {})\n@@ -275,24 +289,30 @@ def mock_storage(data=None):\n         \"\"\"Mock version of write data.\"\"\"\n         _LOGGER.info(\"Writing data to %s: %s\", store.key, data_to_write)\n         # To ensure that the data can be serialized\n-        data[store.key] = json_func.loads(json_func.dumps(data_to_write, cls=store._encoder))\n+        data[store.key] = json_func.loads(\n+            json_func.dumps(data_to_write, cls=store._encoder)\n+        )\n \n     async def mock_remove(store):\n         \"\"\"Remove data.\"\"\"\n         data.pop(store.key, None)\n \n-    with patch(\n-        \"homeassistant.helpers.storage.Store._async_load\",\n-        side_effect=mock_async_load,\n-        autospec=True,\n-    ), patch(\n-        \"homeassistant.helpers.storage.Store._write_data\",\n-        side_effect=mock_write_data,\n-        autospec=True,\n-    ), patch(\n-        \"homeassistant.helpers.storage.Store.async_remove\",\n-        side_effect=mock_remove,\n-        autospec=True,\n+    with (\n+        patch(\n+            \"homeassistant.helpers.storage.Store._async_load\",\n+            side_effect=mock_async_load,\n+            autospec=True,\n+        ),\n+        patch(\n+            \"homeassistant.helpers.storage.Store._write_data\",\n+            side_effect=mock_write_data,\n+            autospec=True,\n+        ),\n+        patch(\n+            \"homeassistant.helpers.storage.Store.async_remove\",\n+            side_effect=mock_remove,\n+            autospec=True,\n+        ),\n     ):\n         yield data\n \n@@ -332,7 +352,9 @@ class MockConfigEntry(config_entries.ConfigEntry):\n         if AwesomeVersion(HAVERSION) >= \"2023.10.0\":\n             hass.config_entries._domain_index.setdefault(self.domain, []).append(self)\n         else:\n-            hass.config_entries._domain_index.setdefault(self.domain, []).append(self.entry_id)\n+            hass.config_entries._domain_index.setdefault(self.domain, []).append(\n+                self.entry_id\n+            )\n \n \n class WSClient:\n@@ -359,7 +381,9 @@ class WSClient:\n \n             clientsession.detach()\n \n-        self.hass.bus.async_listen_once(EVENT_HOMEASSISTANT_STOP, _async_close_websession)\n+        self.hass.bus.async_listen_once(\n+            EVENT_HOMEASSISTANT_STOP, _async_close_websession\n+        )\n \n         self.client = await clientsession.ws_connect(\n             \"ws://localhost:8123/api/websocket\", timeout=1, autoclose=True\n@@ -379,7 +403,9 @@ class WSClient:\n     async def receive_json(self) -> dict[str, Any]:\n         return await self.client.receive_json()\n \n-    async def send_and_receive_json(self, type: str, payload: dict[str, Any]) -> dict[str, Any]:\n+    async def send_and_receive_json(\n+        self, type: str, payload: dict[str, Any]\n+    ) -> dict[str, Any]:\n         await self.send_json(type=type, payload=payload)\n         return await self.client.receive_json()\n \n@@ -562,10 +588,13 @@ def create_config_entry(\n         )\n \n \n-async def setup_integration(hass: ha.HomeAssistant, config_entry: MockConfigEntry) -> None:\n+async def setup_integration(\n+    hass: ha.HomeAssistant, config_entry: MockConfigEntry\n+) -> None:\n     mock_session = await client_session_proxy(hass)\n     with patch(\n-        \"homeassistant.helpers.aiohttp_client.async_get_clientsession\", return_value=mock_session\n+        \"homeassistant.helpers.aiohttp_client.async_get_clientsession\",\n+        return_value=mock_session,\n     ):\n         hass.data.pop(\"custom_components\", None)\n         config_entry.add_to_hass(hass)\n"
    },
    {
        "id": 26,
        "sha_fail": "5fea24b4a3fc4952e83474db5e7dc05af9ec76f6",
        "diff": "diff --git a/tests/repositories/test_get_documentation.py b/tests/repositories/test_get_documentation.py\nindex e289d29..36b3474 100644\n--- a/tests/repositories/test_get_documentation.py\n+++ b/tests/repositories/test_get_documentation.py\n@@ -1,4 +1,3 @@\n-\n from typing import Any\n import pytest\n from custom_components.hacs.base import HacsBase\n@@ -7,11 +6,13 @@ from custom_components.hacs.repositories.base import HacsRepository\n from tests.common import client_session_proxy\n \n \n-\n-@pytest.mark.parametrize(\"data,result\", [\n-    ({\"installed\": True, \"installed_version\": \"1.0.0\"}, \"Example readme file\"),\n-    ({\"installed\": False, \"last_version\": \"2.0.0\"}, \"Example readme file\")\n-])\n+@pytest.mark.parametrize(\n+    \"data,result\",\n+    [\n+        ({\"installed\": True, \"installed_version\": \"1.0.0\"}, \"Example readme file\"),\n+        ({\"installed\": False, \"last_version\": \"2.0.0\"}, \"Example readme file\"),\n+    ],\n+)\n @pytest.mark.asyncio\n async def test_validate_repository(hacs: HacsBase, data: dict[str, Any], result: str):\n     repository = HacsRepository(hacs=hacs)\ndiff --git a/tests/repositories/test_get_hacs_json.py b/tests/repositories/test_get_hacs_json.py\nindex 495f346..6615a43 100644\n--- a/tests/repositories/test_get_hacs_json.py\n+++ b/tests/repositories/test_get_hacs_json.py\n@@ -1,4 +1,3 @@\n-\n import pytest\n from custom_components.hacs.base import HacsBase\n from custom_components.hacs.repositories.base import HacsRepository\n@@ -6,11 +5,9 @@ from custom_components.hacs.repositories.base import HacsRepository\n from tests.common import client_session_proxy\n \n \n-\n-@pytest.mark.parametrize(\"version,name\", [\n-    (\"1.0.0\", \"Proxy integration\"),\n-    (\"99.99.99\", None)\n-])\n+@pytest.mark.parametrize(\n+    \"version,name\", [(\"1.0.0\", \"Proxy integration\"), (\"99.99.99\", None)]\n+)\n @pytest.mark.asyncio\n async def test_validate_repository(hacs: HacsBase, version: str, name: str | None):\n     repository = HacsRepository(hacs=hacs)\n"
    },
    {
        "id": 27,
        "sha_fail": "0b08b8e82f8e67d89dd4335e63ecd95ab6f5f048",
        "diff": "diff --git a/optuna/study/_optimize.py b/optuna/study/_optimize.py\nindex 059c681..f3f4ce3 100644\n--- a/optuna/study/_optimize.py\n+++ b/optuna/study/_optimize.py\n@@ -47,14 +47,20 @@ def _optimize(\n ) -> None:\n     if not isinstance(catch, tuple):\n         raise TypeError(\n-            \"The catch argument is of type '{}' but must be a tuple.\".format(type(catch).__name__)\n+            \"The catch argument is of type '{}' but must be a tuple.\".format(\n+                type(catch).__name__\n+            )\n         )\n \n     if study._thread_local.in_optimize_loop:\n-        raise RuntimeError(\"Nested invocation of `Study.optimize` method isn't allowed.\")\n+        raise RuntimeError(\n+            \"Nested invocation of `Study.optimize` method isn't allowed.\"\n+        )\n \n     if show_progress_bar and n_trials is None and timeout is not None and n_jobs != 1:\n-        warnings.warn(\"The timeout-based progress bar is not supported with n_jobs != 1.\")\n+        warnings.warn(\n+            \"The timeout-based progress bar is not supported with n_jobs != 1.\"\n+        )\n         show_progress_bar = False\n \n     progress_bar = pbar_module._ProgressBar(show_progress_bar, n_trials, timeout)\n@@ -89,7 +95,8 @@ def _optimize(\n \n                     if (\n                         timeout is not None\n-                        and (datetime.datetime.now() - time_start).total_seconds() > timeout\n+                        and (datetime.datetime.now() - time_start).total_seconds()\n+                        > timeout\n                     ):\n                         break\n \n@@ -223,7 +230,9 @@ def _run_trial(\n         if frozen_trial.state == TrialState.COMPLETE:\n             study._log_completed_trial(frozen_trial)\n         elif frozen_trial.state == TrialState.PRUNED:\n-            _logger.info(\"Trial {} pruned. {}\".format(frozen_trial.number, str(func_err)))\n+            _logger.info(\n+                \"Trial {} pruned. {}\".format(frozen_trial.number, str(func_err))\n+            )\n         elif frozen_trial.state == TrialState.FAIL:\n             if func_err is not None:\n                 _log_failed_trial(\n@@ -265,4 +274,6 @@ def _log_failed_trial(\n         exc_info=exc_info,\n     )\n \n-    _logger.warning(\"Trial {} failed with value {}.\".format(trial.number, repr(value_or_values)))\n+    _logger.warning(\n+        \"Trial {} failed with value {}.\".format(trial.number, repr(value_or_values))\n+    )\ndiff --git a/optuna/visualization/_terminator_improvement.py b/optuna/visualization/_terminator_improvement.py\nindex 4a086e0..13d9e59 100644\n--- a/optuna/visualization/_terminator_improvement.py\n+++ b/optuna/visualization/_terminator_improvement.py\n@@ -112,7 +112,9 @@ def plot_terminator_improvement(\n     \"\"\"\n     _imports.check()\n \n-    info = _get_improvement_info(study, plot_error, improvement_evaluator, error_evaluator)\n+    info = _get_improvement_info(\n+        study, plot_error, improvement_evaluator, error_evaluator\n+    )\n     return _get_improvement_plot(info, min_n_trials)\n \n \n@@ -123,7 +125,9 @@ def _get_improvement_info(\n     error_evaluator: BaseErrorEvaluator | None = None,\n ) -> _ImprovementInfo:\n     if study._is_multi_objective():\n-        raise ValueError(\"This function does not support multi-objective optimization study.\")\n+        raise ValueError(\n+            \"This function does not support multi-objective optimization study.\"\n+        )\n \n     if improvement_evaluator is None:\n         improvement_evaluator = RegretBoundEvaluator()\n"
    },
    {
        "id": 28,
        "sha_fail": "3137ef65975fc93e9e82b130e545028223cef408",
        "diff": "diff --git a/optuna/study/_multi_objective.py b/optuna/study/_multi_objective.py\nindex 4e4a365..7ae64b9 100644\n--- a/optuna/study/_multi_objective.py\n+++ b/optuna/study/_multi_objective.py\n@@ -63,8 +63,12 @@ def _get_pareto_front_trials_by_trials(\n     trials: Sequence[FrozenTrial], directions: Sequence[StudyDirection]\n ) -> List[FrozenTrial]:\n     if len(directions) == 2:\n-        return _get_pareto_front_trials_2d(trials, directions)  # Log-linear in number of trials.\n-    return _get_pareto_front_trials_nd(trials, directions)  # Quadratic in number of trials.\n+        return _get_pareto_front_trials_2d(\n+            trials, directions\n+        )  # Log-linear in number of trials.\n+    return _get_pareto_front_trials_nd(\n+        trials, directions\n+    )  # Quadratic in number of trials.\n \n \n def _get_pareto_front_trials(study: \"optuna.study.Study\") -> List[FrozenTrial]:\n@@ -85,7 +89,9 @@ def _fast_non_dominated_sort(\n     # First, we calculate the domination matrix for the objective values.\n     domination_mat = np.all(\n         objective_values[:, np.newaxis, :] >= objective_values[np.newaxis, :, :], axis=2\n-    ) & np.any(objective_values[:, np.newaxis, :] > objective_values[np.newaxis, :, :], axis=2)\n+    ) & np.any(\n+        objective_values[:, np.newaxis, :] > objective_values[np.newaxis, :, :], axis=2\n+    )\n     if penalty is not None:\n         # Filter the domination relations by the penalty on the constraints.\n         # When a penalty score does not exist, the trial is considered to be dominated by the\n@@ -97,7 +103,8 @@ def _fast_non_dominated_sort(\n         # scores are bounded, the domination relationship is discarded.\n         is_infeasible = penalty > 0\n         domination_mat &= ~(\n-            (penalty[:, np.newaxis] == penalty) & (is_infeasible[:, np.newaxis] | is_infeasible)\n+            (penalty[:, np.newaxis] == penalty)\n+            & (is_infeasible[:, np.newaxis] | is_infeasible)\n         )\n         # If the penalty score is dominated, the value domination relationship is overwritten.\n         penalty = np.where(is_nan, np.inf, penalty)\n@@ -139,7 +146,9 @@ def _dominates(\n     assert values1 is not None\n \n     if len(values0) != len(values1):\n-        raise ValueError(\"Trials with different numbers of objectives cannot be compared.\")\n+        raise ValueError(\n+            \"Trials with different numbers of objectives cannot be compared.\"\n+        )\n \n     if len(values0) != len(directions):\n         raise ValueError(\n"
    },
    {
        "id": 29,
        "sha_fail": "616eb3b10db94cf4a4c209377f36b2ce995bd01c",
        "diff": "diff --git a/tests/core/tests/test_admin_integration.py b/tests/core/tests/test_admin_integration.py\nindex c178c89..56c4122 100644\n--- a/tests/core/tests/test_admin_integration.py\n+++ b/tests/core/tests/test_admin_integration.py\n@@ -680,7 +680,9 @@ class ExportAdminIntegrationTest(AdminTestMixin, TestCase):\n             \"file_format\": \"0\",\n         }\n         date_str = datetime.now().strftime(\"%Y-%m-%d\")\n-        with self.assertNumQueries(7):  # Should not contain COUNT queries from ModelAdmin.get_results()\n+        with self.assertNumQueries(\n+            7\n+        ):  # Should not contain COUNT queries from ModelAdmin.get_results()\n             response = self.client.post(\"/admin/core/book/export/\", data)\n         self.assertEqual(response.status_code, 200)\n         self.assertTrue(response.has_header(\"Content-Disposition\"))\n@@ -1493,7 +1495,7 @@ class TestImportSkipConfirm(AdminTestMixin, TransactionTestCase):\n             follow=True,\n             encoding=\"utf-8-sig\",\n             regex_in_response=(\n-                \".*UnicodeDecodeError.* encountered \" \"while trying to read file\"\n+                \".*UnicodeDecodeError.* encountered while trying to read file\"\n             ),\n         )\n \n"
    },
    {
        "id": 30,
        "sha_fail": "d4ca3713b196de2aee52fd0344d0eb9a9eaada64",
        "diff": "diff --git a/tests/core/tests/test_resources/test_resources.py b/tests/core/tests/test_resources/test_resources.py\nindex 82ef08b..622bdaf 100644\n--- a/tests/core/tests/test_resources/test_resources.py\n+++ b/tests/core/tests/test_resources/test_resources.py\n@@ -1,7 +1,6 @@\n import json\n import sys\n from collections import OrderedDict\n-from copy import deepcopy\n from datetime import date\n from decimal import Decimal, InvalidOperation\n from unittest import mock, skipUnless\n@@ -1190,7 +1189,7 @@ class ModelResourceTest(TestCase):\n                     fields = (\"author__nonexistent\",)\n \n         self.assertEqual(\n-            \"Book.author.nonexistent: Author has no field named \" \"'nonexistent'\",\n+            \"Book.author.nonexistent: Author has no field named 'nonexistent'\",\n             cm.exception.args[0],\n         )\n \n"
    },
    {
        "id": 31,
        "sha_fail": "2a59b55e6124b33dca7f48c12845c78130b20fd5",
        "diff": "diff --git a/import_export/admin.py b/import_export/admin.py\nindex a1b288e..f23c55d 100644\n--- a/import_export/admin.py\n+++ b/import_export/admin.py\n@@ -248,9 +248,7 @@ class ImportMixin(BaseImportMixin, ImportExportMixinBase):\n     def add_success_message(self, result, request):\n         opts = self.model._meta\n \n-        success_message = _(\n-            \"Import finished, with {} new and \" \"{} updated {}.\"\n-        ).format(\n+        success_message = _(\"Import finished, with {} new and {} updated {}.\").format(\n             result.totals[RowResult.IMPORT_TYPE_NEW],\n             result.totals[RowResult.IMPORT_TYPE_UPDATE],\n             opts.verbose_name_plural,\n"
    },
    {
        "id": 33,
        "sha_fail": "c359d794dd0e4baf40be48d584193f88c2213f37",
        "diff": "diff --git a/tests/core/tests/test_resources/test_resources.py b/tests/core/tests/test_resources/test_resources.py\nindex 82ef08b..622bdaf 100644\n--- a/tests/core/tests/test_resources/test_resources.py\n+++ b/tests/core/tests/test_resources/test_resources.py\n@@ -1,7 +1,6 @@\n import json\n import sys\n from collections import OrderedDict\n-from copy import deepcopy\n from datetime import date\n from decimal import Decimal, InvalidOperation\n from unittest import mock, skipUnless\n@@ -1190,7 +1189,7 @@ class ModelResourceTest(TestCase):\n                     fields = (\"author__nonexistent\",)\n \n         self.assertEqual(\n-            \"Book.author.nonexistent: Author has no field named \" \"'nonexistent'\",\n+            \"Book.author.nonexistent: Author has no field named 'nonexistent'\",\n             cm.exception.args[0],\n         )\n \n"
    },
    {
        "id": 34,
        "sha_fail": "cfbbed910a5d84c08f9af237cf6737502c456f66",
        "diff": "diff --git a/import_export/admin.py b/import_export/admin.py\nindex c5151ef..10498fc 100644\n--- a/import_export/admin.py\n+++ b/import_export/admin.py\n@@ -248,9 +248,7 @@ class ImportMixin(BaseImportMixin, ImportExportMixinBase):\n     def add_success_message(self, result, request):\n         opts = self.model._meta\n \n-        success_message = _(\n-            \"Import finished, with {} new and \" \"{} updated {}.\"\n-        ).format(\n+        success_message = _(\"Import finished, with {} new and {} updated {}.\").format(\n             result.totals[RowResult.IMPORT_TYPE_NEW],\n             result.totals[RowResult.IMPORT_TYPE_UPDATE],\n             opts.verbose_name_plural,\n@@ -747,6 +745,7 @@ class ExportMixin(BaseExportMixin, ImportExportMixinBase):\n \n         class FakePaginator:\n             count = 0\n+\n         original_get_paginator = self.get_paginator\n         self.get_paginator = lambda request, queryset, per_page: FakePaginator()\n         cl = ChangeList(**changelist_kwargs)\n"
    },
    {
        "id": 36,
        "sha_fail": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
        "diff": "diff --git a/composer/__init__.py b/composer/__init__.py\nindex 67eb7f5..b5cba66 100644\n--- a/composer/__init__.py\n+++ b/composer/__init__.py\n@@ -4,23 +4,34 @@\n \"\"\"Some key classes are available directly in the ``composer`` namespace.\"\"\"\n \n from composer._version import __version__\n-from composer.core import Algorithm, Callback, DataSpec, Engine, Evaluator, Event, State, Time, Timestamp, TimeUnit\n+from composer.core import (\n+    Algorithm,\n+    Callback,\n+    DataSpec,\n+    Engine,\n+    Evaluator,\n+    Event,\n+    State,\n+    Time,\n+    Timestamp,\n+    TimeUnit,\n+)\n from composer.loggers import Logger\n from composer.models import ComposerModel\n from composer.trainer import Trainer\n \n __all__ = [\n-    'Algorithm',\n-    'Callback',\n-    'DataSpec',\n-    'Engine',\n-    'Evaluator',\n-    'Event',\n-    'State',\n-    'Time',\n-    'Timestamp',\n-    'TimeUnit',\n-    'Logger',\n-    'ComposerModel',\n-    'Trainer',\n+    \"Algorithm\",\n+    \"Callback\",\n+    \"DataSpec\",\n+    \"Engine\",\n+    \"Evaluator\",\n+    \"Event\",\n+    \"State\",\n+    \"Time\",\n+    \"Timestamp\",\n+    \"TimeUnit\",\n+    \"Logger\",\n+    \"ComposerModel\",\n+    \"Trainer\",\n ]\ndiff --git a/composer/callbacks/__init__.py b/composer/callbacks/__init__.py\nindex 0cd964a..b483d25 100644\n--- a/composer/callbacks/__init__.py\n+++ b/composer/callbacks/__init__.py\n@@ -6,6 +6,7 @@\n Each callback inherits from the :class:`.Callback` base class. See detailed description and\n examples for writing your own callbacks at the :class:`.Callback` base class.\n \"\"\"\n+\n from composer.callbacks.activation_monitor import ActivationMonitor\n from composer.callbacks.checkpoint_saver import CheckpointSaver\n from composer.callbacks.early_stopper import EarlyStopper\n@@ -26,22 +27,22 @@ from composer.callbacks.system_metrics_monitor import SystemMetricsMonitor\n from composer.callbacks.threshold_stopper import ThresholdStopper\n \n __all__ = [\n-    'ActivationMonitor',\n-    'OptimizerMonitor',\n-    'LRMonitor',\n-    'MemoryMonitor',\n-    'NaNMonitor',\n-    'SpeedMonitor',\n-    'CheckpointSaver',\n-    'MLPerfCallback',\n-    'EarlyStopper',\n-    'EvalOutputLogging',\n-    'ExportForInferenceCallback',\n-    'ThresholdStopper',\n-    'ImageVisualizer',\n-    'HealthChecker',\n-    'RuntimeEstimator',\n-    'SystemMetricsMonitor',\n-    'Generate',\n-    'FreeOutputs',\n+    \"ActivationMonitor\",\n+    \"OptimizerMonitor\",\n+    \"LRMonitor\",\n+    \"MemoryMonitor\",\n+    \"NaNMonitor\",\n+    \"SpeedMonitor\",\n+    \"CheckpointSaver\",\n+    \"MLPerfCallback\",\n+    \"EarlyStopper\",\n+    \"EvalOutputLogging\",\n+    \"ExportForInferenceCallback\",\n+    \"ThresholdStopper\",\n+    \"ImageVisualizer\",\n+    \"HealthChecker\",\n+    \"RuntimeEstimator\",\n+    \"SystemMetricsMonitor\",\n+    \"Generate\",\n+    \"FreeOutputs\",\n ]\ndiff --git a/composer/callbacks/eval_output_logging_callback.py b/composer/callbacks/eval_output_logging_callback.py\nindex 48e4f76..2d2f097 100644\n--- a/composer/callbacks/eval_output_logging_callback.py\n+++ b/composer/callbacks/eval_output_logging_callback.py\n@@ -13,18 +13,29 @@ from typing import Callable, Optional\n from torch.utils.data import DataLoader\n \n from composer.core import Callback, State\n-from composer.datasets.in_context_learning_evaluation import (InContextLearningCodeEvalDataset,\n-                                                              InContextLearningLMTaskDataset,\n-                                                              InContextLearningMultipleChoiceTaskDataset,\n-                                                              InContextLearningQATaskDataset,\n-                                                              InContextLearningSchemaTaskDataset)\n+from composer.datasets.in_context_learning_evaluation import (\n+    InContextLearningCodeEvalDataset,\n+    InContextLearningLMTaskDataset,\n+    InContextLearningMultipleChoiceTaskDataset,\n+    InContextLearningQATaskDataset,\n+    InContextLearningSchemaTaskDataset,\n+)\n from composer.loggers import Logger\n from composer.loggers.console_logger import ConsoleLogger\n-from composer.utils import MissingConditionalImportError, dist, maybe_create_object_store_from_uri, parse_uri\n-\n-ICLDatasetTypes = (InContextLearningLMTaskDataset, InContextLearningQATaskDataset,\n-                   InContextLearningMultipleChoiceTaskDataset, InContextLearningSchemaTaskDataset,\n-                   InContextLearningCodeEvalDataset)\n+from composer.utils import (\n+    MissingConditionalImportError,\n+    dist,\n+    maybe_create_object_store_from_uri,\n+    parse_uri,\n+)\n+\n+ICLDatasetTypes = (\n+    InContextLearningLMTaskDataset,\n+    InContextLearningQATaskDataset,\n+    InContextLearningMultipleChoiceTaskDataset,\n+    InContextLearningSchemaTaskDataset,\n+    InContextLearningCodeEvalDataset,\n+)\n \n \n def _write(destination_path, src_file):\n@@ -62,12 +73,16 @@ class EvalOutputLogging(Callback):\n         try:\n             import pandas as pd\n         except ImportError as e:\n-            raise MissingConditionalImportError(extra_deps_group='pandas',\n-                                                conda_package='pandas',\n-                                                conda_channel='conda-forge') from e\n+            raise MissingConditionalImportError(\n+                extra_deps_group=\"pandas\",\n+                conda_package=\"pandas\",\n+                conda_channel=\"conda-forge\",\n+            ) from e\n         # write tmp files\n-        self.hash.update((str(time.time()) + str(random.randint(0, 1_000_000))).encode('utf-8'))\n-        tmp_dir = os.getcwd() + '/' + self.hash.hexdigest()\n+        self.hash.update(\n+            (str(time.time()) + str(random.randint(0, 1_000_000))).encode(\"utf-8\")\n+        )\n+        tmp_dir = os.getcwd() + \"/\" + self.hash.hexdigest()\n \n         if not os.path.exists(tmp_dir):\n             with dist.local_rank_zero_download_and_wait(tmp_dir):\n@@ -75,24 +90,30 @@ class EvalOutputLogging(Callback):\n                     os.mkdir(tmp_dir)\n \n         full_df = pd.DataFrame()\n-        file_name = f'eval-outputs-ba{state.timestamp.batch.value}.tsv'\n+        file_name = f\"eval-outputs-ba{state.timestamp.batch.value}.tsv\"\n \n         for benchmark in self.table:\n             cols, rows = self.table[benchmark]\n-            rows = [[e.encode('unicode_escape') if isinstance(e, str) else e for e in row] for row in rows]\n+            rows = [\n+                [e.encode(\"unicode_escape\") if isinstance(e, str) else e for e in row]\n+                for row in rows\n+            ]\n             df = pd.DataFrame.from_records(data=rows, columns=cols)\n-            df['benchmark'] = benchmark\n+            df[\"benchmark\"] = benchmark\n             full_df = pd.concat([full_df, df], ignore_index=True)\n \n-        with dist.local_rank_zero_download_and_wait(f'{tmp_dir}/{file_name}'):\n+        with dist.local_rank_zero_download_and_wait(f\"{tmp_dir}/{file_name}\"):\n             if dist.get_local_rank() == 0:\n-                with open(f'{tmp_dir}/{file_name}', 'wb') as f:\n-                    full_df.to_csv(f, sep='\\t', index=False)\n+                with open(f\"{tmp_dir}/{file_name}\", \"wb\") as f:\n+                    full_df.to_csv(f, sep=\"\\t\", index=False)\n \n         # copy/upload tmp files\n-        _write(destination_path=f'{self.output_directory}/{file_name}', src_file=f'{tmp_dir}/{file_name}')\n-        os.remove(f'{tmp_dir}/{file_name}')\n-        self.destination_file = f'{self.output_directory}/{file_name}'\n+        _write(\n+            destination_path=f\"{self.output_directory}/{file_name}\",\n+            src_file=f\"{tmp_dir}/{file_name}\",\n+        )\n+        os.remove(f\"{tmp_dir}/{file_name}\")\n+        self.destination_file = f\"{self.output_directory}/{file_name}\"\n \n         # delete tmp files\n         os.rmdir(tmp_dir)\n@@ -100,7 +121,7 @@ class EvalOutputLogging(Callback):\n     def _prep_response_cache(self, state, cache):\n         benchmark = state.dataloader_label\n         for metric in state.eval_metrics[benchmark].values():\n-            if hasattr(metric, 'reset_response_cache'):\n+            if hasattr(metric, \"reset_response_cache\"):\n                 metric.reset_response_cache(cache)\n \n     def eval_start(self, state: State, logger: Logger) -> None:\n@@ -123,26 +144,34 @@ class EvalOutputLogging(Callback):\n         # during each eval, only a single dataloader/benchmark will be active\n         assert state.dataloader is not None\n         assert isinstance(state.dataloader, DataLoader)\n-        if hasattr(state.dataloader, 'dataset') and isinstance(state.dataloader.dataset, ICLDatasetTypes):\n+        if hasattr(state.dataloader, \"dataset\") and isinstance(\n+            state.dataloader.dataset, ICLDatasetTypes\n+        ):\n             assert isinstance(state.dataloader.dataset, ICLDatasetTypes)\n-            if hasattr(state.dataloader.dataset, 'tokenizer'):\n+            if hasattr(state.dataloader.dataset, \"tokenizer\"):\n                 tokenizer = state.dataloader.dataset.tokenizer\n                 benchmark = state.dataloader_label\n                 assert benchmark is not None\n                 assert isinstance(benchmark, str)\n                 for metric_name, metric in state.eval_metrics[benchmark].items():\n-                    if hasattr(metric, 'format_response_cache'):\n+                    if hasattr(metric, \"format_response_cache\"):\n                         assert isinstance(metric.format_response_cache, Callable)\n                         format_response_cache: Callable = metric.format_response_cache\n                         columns, rows = format_response_cache(tokenizer)\n \n                         if columns is not None and rows is not None:\n                             if self.subset_sample > 0:\n-                                rows = random.sample(rows, min(len(rows), self.subset_sample))\n+                                rows = random.sample(\n+                                    rows, min(len(rows), self.subset_sample)\n+                                )\n                             for destination in logger.destinations:\n                                 if not isinstance(destination, ConsoleLogger):\n                                     # don't log to console because it will pollute the console too much\n-                                    destination.log_table(columns, rows, f'icl_outputs/{benchmark}/{metric_name}')\n+                                    destination.log_table(\n+                                        columns,\n+                                        rows,\n+                                        f\"icl_outputs/{benchmark}/{metric_name}\",\n+                                    )\n \n-                            self.table[f'{benchmark}_{metric_name}'] = (columns, rows)\n+                            self.table[f\"{benchmark}_{metric_name}\"] = (columns, rows)\n         self._prep_response_cache(state, False)\ndiff --git a/composer/datasets/in_context_learning_evaluation.py b/composer/datasets/in_context_learning_evaluation.py\nindex e156650..08ae489 100644\n--- a/composer/datasets/in_context_learning_evaluation.py\n+++ b/composer/datasets/in_context_learning_evaluation.py\n@@ -25,37 +25,46 @@ if TYPE_CHECKING:\n _MAX_ANSWER_BUFFER_LENGTH = 10\n \n __all__ = [\n-    'InContextLearningLMTaskDataset',\n-    'InContextLearningMultipleChoiceTaskDataset',\n-    'InContextLearningCodeEvalDataset',\n-    'InContextLearningQATaskDataset',\n-    'get_icl_task_dataloader',\n+    \"InContextLearningLMTaskDataset\",\n+    \"InContextLearningMultipleChoiceTaskDataset\",\n+    \"InContextLearningCodeEvalDataset\",\n+    \"InContextLearningQATaskDataset\",\n+    \"get_icl_task_dataloader\",\n ]\n \n \n def strip_data(samples):\n-    return [{k: v.strip() if isinstance(v, str) else v for k, v in entry.items()} for entry in samples]\n+    return [\n+        {k: v.strip() if isinstance(v, str) else v for k, v in entry.items()}\n+        for entry in samples\n+    ]\n \n \n def _tokenizer_needs_prefix_space(tokenizer) -> bool:\n     # Test for whether a prefix space is needed before the continuation.\n     # sentencepiece tokenization should not have a prefix space, but gpt2 style BPE should\n-    return len(tokenizer(' a', add_special_tokens=False)['input_ids']) == 1\n+    return len(tokenizer(\" a\", add_special_tokens=False)[\"input_ids\"]) == 1\n \n \n-def _make_padded_input(context_enc, continuation_enc, max_seq_len, pad_tok_id, padding_side='right'):\n+def _make_padded_input(\n+    context_enc, continuation_enc, max_seq_len, pad_tok_id, padding_side=\"right\"\n+):\n     if len(continuation_enc) + len(context_enc) > max_seq_len:\n         # clip from the end\n         context_max_subseq_len = max_seq_len - len(continuation_enc)\n \n         if context_max_subseq_len < 0:\n-            raise Exception(f'Dataset included continuation longer than the max seq len')\n+            raise Exception(\n+                f\"Dataset included continuation longer than the max seq len\"\n+            )\n             # can't support continuations which are longer than the max seq len\n \n         context_enc = context_enc[-(context_max_subseq_len):]\n \n     # continuation span is the _inclusive_ range of indices corresponding to the continuation\n-    continuation_span = torch.tensor(range(len(context_enc), len(context_enc) + len(continuation_enc)))\n+    continuation_span = torch.tensor(\n+        range(len(context_enc), len(context_enc) + len(continuation_enc))\n+    )\n     inp = torch.tensor(\n         (context_enc + continuation_enc),\n         dtype=torch.long,\n@@ -63,7 +72,7 @@ def _make_padded_input(context_enc, continuation_enc, max_seq_len, pad_tok_id, p\n     (inp_len,) = inp.shape\n \n     # pad length from seq to padding_length\n-    if padding_side == 'right':\n+    if padding_side == \"right\":\n         inp = torch.cat(\n             [\n                 inp,  # [seq]\n@@ -71,7 +80,7 @@ def _make_padded_input(context_enc, continuation_enc, max_seq_len, pad_tok_id, p\n             ],\n             dim=0,\n         )\n-    elif padding_side == 'left':\n+    elif padding_side == \"left\":\n         inp = torch.cat(\n             [\n                 torch.LongTensor((max_seq_len - inp_len) * [pad_tok_id]),\n@@ -80,12 +89,16 @@ def _make_padded_input(context_enc, continuation_enc, max_seq_len, pad_tok_id, p\n             dim=0,\n         )\n     else:\n-        raise ValueError(f\"Unknown padding_side {padding_side}. padding_side must be either 'left' or 'right'\")\n+        raise ValueError(\n+            f\"Unknown padding_side {padding_side}. padding_side must be either 'left' or 'right'\"\n+        )\n \n     return inp, continuation_span\n \n \n-def _get_fewshot_sample_idxs(dataset_size: int, num_fewshot: int, sample_idx: int, rng: random.Random):\n+def _get_fewshot_sample_idxs(\n+    dataset_size: int, num_fewshot: int, sample_idx: int, rng: random.Random\n+):\n     # samples without replacement. if num_fewshot exceeds the number of unique samples,\n     # then we will have fewer than num_fewshot examples in context\n     num_fewshot = min(dataset_size - 1, num_fewshot)\n@@ -131,18 +144,22 @@ class InContextLearningQATaskDataset(Dataset):\n     def _read_dataset(self, dataset: Dataset) -> List[Dict[str, str]]:\n         result = []\n         for example in dataset:\n-            result.append({\n-                'context': example['context'],\n-                'answer': example['answer'],\n-                'aliases': set([example['answer']] + example.get('aliases', [])),\n-                'chain_of_thought': example.get('chain_of_thought', '')\n-            })\n+            result.append(\n+                {\n+                    \"context\": example[\"context\"],\n+                    \"answer\": example[\"answer\"],\n+                    \"aliases\": set([example[\"answer\"]] + example.get(\"aliases\", [])),\n+                    \"chain_of_thought\": example.get(\"chain_of_thought\", \"\"),\n+                }\n+            )\n         return result\n \n     def __init__(\n         self,\n         dataset_uri: str,\n-        tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n+        tokenizer: Union[\n+            transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+        ],\n         max_seq_len: int,\n         pad_tok_id: int,\n         num_fewshot: int,\n@@ -152,34 +169,52 @@ class InContextLearningQATaskDataset(Dataset):\n         destination_path: str,\n         question_prelimiter: str,\n         fewshot_random_seed: int,\n-        cot_delimiter: str = '',\n+        cot_delimiter: str = \"\",\n     ):\n         try:\n             from datasets import load_dataset  # pyright: ignore [reportGeneralTypeIssues]\n         except ImportError as e:\n-            raise MissingConditionalImportError(extra_deps_group='nlp',\n-                                                conda_package='datasets',\n-                                                conda_channel='conda-forge') from e\n+            raise MissingConditionalImportError(\n+                extra_deps_group=\"nlp\",\n+                conda_package=\"datasets\",\n+                conda_channel=\"conda-forge\",\n+            ) from e\n         with dist.local_rank_zero_download_and_wait(destination_path):\n             if dist.get_local_rank() == 0:\n                 get_file(dataset_uri, destination_path, overwrite=True)\n-        dataset = load_dataset('json', data_files=destination_path, split='train', streaming=False)\n+        dataset = load_dataset(\n+            \"json\", data_files=destination_path, split=\"train\", streaming=False\n+        )\n \n         self.samples = self._read_dataset(dataset)\n         self.samples = strip_data(self.samples)\n         self.tokenizer = tokenizer\n         self.max_seq_len = max_seq_len\n         self.pad_tok_id = pad_tok_id\n-        self.padding_side = 'left'\n+        self.padding_side = \"left\"\n         self.max_answer_length = 0\n         fewshot_rng = random.Random(fewshot_random_seed)\n-        self.encoded_dataset = self._prep_examples(num_fewshot, prompt_string, example_delimiter,\n-                                                   continuation_delimiter, question_prelimiter, fewshot_rng,\n-                                                   cot_delimiter)\n+        self.encoded_dataset = self._prep_examples(\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            question_prelimiter,\n+            fewshot_rng,\n+            cot_delimiter,\n+        )\n \n-    def _format_prompt_and_fewshot(self, num_fewshot: int, prompt_string: str, example_delimiter: str,\n-                                   continuation_delimiter: str, question_prelimiter: str, cot_delimiter: str,\n-                                   fewshot_rng: random.Random, sample_idx: int) -> str:\n+    def _format_prompt_and_fewshot(\n+        self,\n+        num_fewshot: int,\n+        prompt_string: str,\n+        example_delimiter: str,\n+        continuation_delimiter: str,\n+        question_prelimiter: str,\n+        cot_delimiter: str,\n+        fewshot_rng: random.Random,\n+        sample_idx: int,\n+    ) -> str:\n         \"\"\"Formats the prompt fewshot examples for test sample `sample_idx`.\n \n         Randomly select `num_fewshot` samples from the dataset (not including the sample at `sample_idx`) and format\n@@ -192,29 +227,33 @@ class InContextLearningQATaskDataset(Dataset):\n         prompt_and_fewshot = prompt_string\n \n         if num_fewshot > 0:\n-            fewshot_idxs = _get_fewshot_sample_idxs(len(self.samples), num_fewshot, sample_idx, fewshot_rng)\n+            fewshot_idxs = _get_fewshot_sample_idxs(\n+                len(self.samples), num_fewshot, sample_idx, fewshot_rng\n+            )\n             for fewshot_idx in fewshot_idxs:\n-                context = self.samples[fewshot_idx]['context']\n-                chain_of_thought = self.samples[fewshot_idx].get('chain_of_thought', '')\n-                answer = self.samples[fewshot_idx]['answer']\n+                context = self.samples[fewshot_idx][\"context\"]\n+                chain_of_thought = self.samples[fewshot_idx].get(\"chain_of_thought\", \"\")\n+                answer = self.samples[fewshot_idx][\"answer\"]\n \n                 if len(chain_of_thought) == 0:\n-                    cot_delimiter = ''\n-                context = f'{question_prelimiter}{context}'\n+                    cot_delimiter = \"\"\n+                context = f\"{question_prelimiter}{context}\"\n                 if len(prompt_and_fewshot) > 0:\n-                    context = f'{example_delimiter}{context}'\n-                prompt_and_fewshot += f'{context}{continuation_delimiter}{chain_of_thought}{cot_delimiter}{answer}'\n+                    context = f\"{example_delimiter}{context}\"\n+                prompt_and_fewshot += f\"{context}{continuation_delimiter}{chain_of_thought}{cot_delimiter}{answer}\"\n \n         return prompt_and_fewshot\n \n-    def _prep_examples(self,\n-                       num_fewshot: int,\n-                       prompt_string: str,\n-                       example_delimiter: str,\n-                       continuation_delimiter: str,\n-                       question_prelimiter: str,\n-                       fewshot_rng: random.Random,\n-                       cot_delimiter: str = '') -> List[Dict[str, Any]]:\n+    def _prep_examples(\n+        self,\n+        num_fewshot: int,\n+        prompt_string: str,\n+        example_delimiter: str,\n+        continuation_delimiter: str,\n+        question_prelimiter: str,\n+        fewshot_rng: random.Random,\n+        cot_delimiter: str = \"\",\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Prepares a set of language modeling tasks into tokenized format with prompt and fewshot examples.\n \n         Each task consists of a context and a continuation as well as an optional prompt and optional list of\n@@ -240,39 +279,55 @@ class InContextLearningQATaskDataset(Dataset):\n         for sample_idx in tqdm(range(len(self.samples))):\n             encoded_example = {}\n \n-            prompt_and_fewshot = self._format_prompt_and_fewshot(num_fewshot, prompt_string, example_delimiter,\n-                                                                 continuation_delimiter, question_prelimiter,\n-                                                                 cot_delimiter, fewshot_rng, sample_idx)\n-            ctxt = self.samples[sample_idx]['context']\n-            ctxt = f'{question_prelimiter}{ctxt}'\n+            prompt_and_fewshot = self._format_prompt_and_fewshot(\n+                num_fewshot,\n+                prompt_string,\n+                example_delimiter,\n+                continuation_delimiter,\n+                question_prelimiter,\n+                cot_delimiter,\n+                fewshot_rng,\n+                sample_idx,\n+            )\n+            ctxt = self.samples[sample_idx][\"context\"]\n+            ctxt = f\"{question_prelimiter}{ctxt}\"\n             if len(prompt_and_fewshot) > 0:\n-                ctxt = f'{example_delimiter}{ctxt}'\n+                ctxt = f\"{example_delimiter}{ctxt}\"\n \n             # rstrip the continuation delimiter, because the prompt ending in a space results in degenerate output\n             continuation_delimiter_stripped = continuation_delimiter.rstrip()\n-            ctxt = f'{ctxt}{continuation_delimiter_stripped}'\n+            ctxt = f\"{ctxt}{continuation_delimiter_stripped}\"\n \n             # If the preamble is empty then this will be a 0-length list, unless the tokenizer adds special tokens to empty strings (e.g. OPT tokenizer)\n-            encoded_example['preamble'] = self.tokenizer(prompt_and_fewshot)\n+            encoded_example[\"preamble\"] = self.tokenizer(prompt_and_fewshot)\n             # If there is an EOS token added, we need to remove it so it is not in the middle of the prompt\n-            if self.tokenizer.eos_token_id is not None and len(\n-                    encoded_example['preamble']\n-                ['input_ids']) > 1 and encoded_example['preamble']['input_ids'][-1] == self.tokenizer.eos_token_id:\n-                encoded_example['preamble']['input_ids'] = encoded_example['preamble']['input_ids'][:-1]\n-\n-            encoded_example['context'] = self.tokenizer(ctxt, add_special_tokens=False)\n-\n-            encoded_example['aliases'] = list(self.samples[sample_idx]['aliases'])\n-            encoded_example['cot_delimiter'] = cot_delimiter\n+            if (\n+                self.tokenizer.eos_token_id is not None\n+                and len(encoded_example[\"preamble\"][\"input_ids\"]) > 1\n+                and encoded_example[\"preamble\"][\"input_ids\"][-1]\n+                == self.tokenizer.eos_token_id\n+            ):\n+                encoded_example[\"preamble\"][\"input_ids\"] = encoded_example[\"preamble\"][\n+                    \"input_ids\"\n+                ][:-1]\n+\n+            encoded_example[\"context\"] = self.tokenizer(ctxt, add_special_tokens=False)\n+\n+            encoded_example[\"aliases\"] = list(self.samples[sample_idx][\"aliases\"])\n+            encoded_example[\"cot_delimiter\"] = cot_delimiter\n             examples.append(encoded_example)\n-            for answer in self.samples[sample_idx]['aliases']:\n+            for answer in self.samples[sample_idx][\"aliases\"]:\n                 response = f\"{self.samples[sample_idx]['chain_of_thought']}{cot_delimiter}{answer}\"\n-                max_answer_length = max(max_answer_length, len(self.tokenizer(response)['input_ids']))\n+                max_answer_length = max(\n+                    max_answer_length, len(self.tokenizer(response)[\"input_ids\"])\n+                )\n \n-            if len(self.samples[sample_idx]['chain_of_thought']) > 0:\n+            if len(self.samples[sample_idx][\"chain_of_thought\"]) > 0:\n                 has_cot = True\n \n-        self.max_answer_length = max_answer_length + (_MAX_ANSWER_BUFFER_LENGTH if has_cot else 0)\n+        self.max_answer_length = max_answer_length + (\n+            _MAX_ANSWER_BUFFER_LENGTH if has_cot else 0\n+        )\n         return examples\n \n     def __getitem__(self, index):\n@@ -283,48 +338,52 @@ class InContextLearningQATaskDataset(Dataset):\n \n     def collate_fn(self, data):\n         inputs, answers = [], []\n-        cot_delimiter = ''\n+        cot_delimiter = \"\"\n \n         for sample in data:\n-            preamble, context, aliases = (sample['preamble'], sample['context'], sample['aliases'])\n-            context_enc = preamble['input_ids'] + context['input_ids']\n-            inp, _ = _make_padded_input(context_enc, [],\n-                                        self.max_seq_len - self.max_answer_length,\n-                                        self.pad_tok_id,\n-                                        padding_side=self.padding_side)\n+            preamble, context, aliases = (\n+                sample[\"preamble\"],\n+                sample[\"context\"],\n+                sample[\"aliases\"],\n+            )\n+            context_enc = preamble[\"input_ids\"] + context[\"input_ids\"]\n+            inp, _ = _make_padded_input(\n+                context_enc,\n+                [],\n+                self.max_seq_len - self.max_answer_length,\n+                self.pad_tok_id,\n+                padding_side=self.padding_side,\n+            )\n \n             inputs.append(inp)\n             answers.append(aliases)\n \n             # We will search for the answer within the portion of the model response\n             # beginning with `cot_delimiter`\n-            cot_delimiter = sample['cot_delimiter']\n+            cot_delimiter = sample[\"cot_delimiter\"]\n \n         batch = {\n-            'input_ids': torch.stack(inputs),\n-            'mode': 'generate',\n-            'labels': answers,\n-            'cot_delimiter': cot_delimiter,\n-            'generation_length': self.max_answer_length,\n-            'generation_kwargs': {\n-                'pad_token_id': self.pad_tok_id,\n-                'use_cache': True\n-            }\n+            \"input_ids\": torch.stack(inputs),\n+            \"mode\": \"generate\",\n+            \"labels\": answers,\n+            \"cot_delimiter\": cot_delimiter,\n+            \"generation_length\": self.max_answer_length,\n+            \"generation_kwargs\": {\"pad_token_id\": self.pad_tok_id, \"use_cache\": True},\n         }\n \n-        batch['attention_mask'] = ~(batch['input_ids'] == self.pad_tok_id)\n+        batch[\"attention_mask\"] = ~(batch[\"input_ids\"] == self.pad_tok_id)\n         return batch\n \n     def get_num_samples_in_batch(self, batch) -> int:\n-        return batch['input_ids'].shape[0]\n+        return batch[\"input_ids\"].shape[0]\n \n     def split_batch(self, batch: Any, microbatch_size: int):\n         # Don't split kwargs that don't change\n         # Normally split torch tensors\n         # List split lists of strings\n-        no_split = ['mode', 'generation_length', 'generation_kwargs', 'cot_delimiter']\n-        normal_split = ['input_ids', 'attention_mask']\n-        list_split = ['labels']\n+        no_split = [\"mode\", \"generation_length\", \"generation_kwargs\", \"cot_delimiter\"]\n+        normal_split = [\"input_ids\", \"attention_mask\"]\n+        list_split = [\"labels\"]\n         chunked = {}\n         for k, v in batch.items():\n             if k in no_split:\n@@ -335,8 +394,8 @@ class InContextLearningQATaskDataset(Dataset):\n             elif k in normal_split:\n                 chunked[k] = _default_split_batch(v, microbatch_size)\n             else:\n-                raise ValueError(f'Unexpected key {k}')\n-        num_chunks = len(chunked['input_ids'])\n+                raise ValueError(f\"Unexpected key {k}\")\n+        num_chunks = len(chunked[\"input_ids\"])\n         for k, v in batch.items():\n             if isinstance(v, (int, float, str, bool, dict)):\n                 chunked[k] = [v] * num_chunks\n@@ -365,7 +424,9 @@ class InContextLearningLMTaskDataset(Dataset):\n     def __init__(\n         self,\n         dataset_uri: str,\n-        tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n+        tokenizer: Union[\n+            transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+        ],\n         max_seq_len: int,\n         pad_tok_id: int,\n         num_fewshot: int,\n@@ -378,18 +439,25 @@ class InContextLearningLMTaskDataset(Dataset):\n         try:\n             from datasets import load_dataset  # pyright: ignore [reportGeneralTypeIssues]\n         except ImportError as e:\n-            raise MissingConditionalImportError(extra_deps_group='nlp',\n-                                                conda_package='datasets',\n-                                                conda_channel='conda-forge') from e\n+            raise MissingConditionalImportError(\n+                extra_deps_group=\"nlp\",\n+                conda_package=\"datasets\",\n+                conda_channel=\"conda-forge\",\n+            ) from e\n         with dist.local_rank_zero_download_and_wait(destination_path):\n             if dist.get_local_rank() == 0:\n                 get_file(dataset_uri, destination_path, overwrite=True)\n-        dataset = load_dataset('json', data_files=destination_path, split='train', streaming=False)\n+        dataset = load_dataset(\n+            \"json\", data_files=destination_path, split=\"train\", streaming=False\n+        )\n         self.samples = list(\n-            dataset.map(lambda examples: {\n-                'continuation': examples['continuation'],\n-                'context': examples['context'],\n-            }))\n+            dataset.map(\n+                lambda examples: {\n+                    \"continuation\": examples[\"continuation\"],\n+                    \"context\": examples[\"context\"],\n+                }\n+            )\n+        )\n         self.samples = strip_data(self.samples)\n \n         self.tokenizer = tokenizer\n@@ -399,11 +467,22 @@ class InContextLearningLMTaskDataset(Dataset):\n \n         self.prefix_space = _tokenizer_needs_prefix_space(self.tokenizer)\n \n-        self.encoded_dataset = self.prep_examples(num_fewshot, prompt_string, example_delimiter, continuation_delimiter,\n-                                                  fewshot_rng)\n+        self.encoded_dataset = self.prep_examples(\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            fewshot_rng,\n+        )\n \n-    def prep_examples(self, num_fewshot: int, prompt_string: str, example_delimiter: str, continuation_delimiter: str,\n-                      fewshot_rng: random.Random):\n+    def prep_examples(\n+        self,\n+        num_fewshot: int,\n+        prompt_string: str,\n+        example_delimiter: str,\n+        continuation_delimiter: str,\n+        fewshot_rng: random.Random,\n+    ):\n         \"\"\"Prepares a set of language modeling tasks into tokenized format with prompt and fewshot examples.\n \n         Each task consists of a context and a continuation as well as an optional prompt and optional list of\n@@ -426,34 +505,49 @@ class InContextLearningLMTaskDataset(Dataset):\n             preamble = prompt_string\n \n             if num_fewshot > 0:\n-                fewshot_idxs = _get_fewshot_sample_idxs(len(self.samples), num_fewshot, sample_idx, fewshot_rng)\n+                fewshot_idxs = _get_fewshot_sample_idxs(\n+                    len(self.samples), num_fewshot, sample_idx, fewshot_rng\n+                )\n                 for fewshot_idx in fewshot_idxs:\n-                    ctxt, cont = self.samples[fewshot_idx]['context'], self.samples[fewshot_idx]['continuation']\n+                    ctxt, cont = (\n+                        self.samples[fewshot_idx][\"context\"],\n+                        self.samples[fewshot_idx][\"continuation\"],\n+                    )\n                     if len(preamble) > 0:\n-                        ctxt = f'{example_delimiter}{ctxt}'\n-                    preamble += f'{ctxt}{continuation_delimiter}{cont}'\n+                        ctxt = f\"{example_delimiter}{ctxt}\"\n+                    preamble += f\"{ctxt}{continuation_delimiter}{cont}\"\n \n-            ctxt, cont = self.samples[sample_idx]['context'], self.samples[sample_idx]['continuation']\n+            ctxt, cont = (\n+                self.samples[sample_idx][\"context\"],\n+                self.samples[sample_idx][\"continuation\"],\n+            )\n             if len(preamble) > 0:\n-                ctxt = f'{example_delimiter}{ctxt}'\n+                ctxt = f\"{example_delimiter}{ctxt}\"\n \n             # rstrip the continuation delimiter, because the prompt ending in a space results in degenerate output\n             continuation_delimiter_stripped = continuation_delimiter.rstrip()\n \n-            if self.prefix_space and not cont.startswith(' '):\n-                cont = f' {cont}'\n+            if self.prefix_space and not cont.startswith(\" \"):\n+                cont = f\" {cont}\"\n             ctxt += continuation_delimiter_stripped\n \n-            encoded_example['preamble'] = self.tokenizer(\n+            encoded_example[\"preamble\"] = self.tokenizer(\n                 preamble\n             )  # if the preamble is empty then these will be 0-length lists, unless the tokenizer adds special tokens to empty strings (e.g. OPT tokenizer)\n-            if self.tokenizer.eos_token_id is not None and len(\n-                    encoded_example['preamble']\n-                ['input_ids']) > 1 and encoded_example['preamble']['input_ids'][-1] == self.tokenizer.eos_token_id:\n-                encoded_example['preamble']['input_ids'] = encoded_example['preamble']['input_ids'][:-1]\n-\n-            encoded_example['context'] = self.tokenizer(ctxt, add_special_tokens=False)\n-            encoded_example['continuation'] = self.tokenizer(cont, add_special_tokens=False)\n+            if (\n+                self.tokenizer.eos_token_id is not None\n+                and len(encoded_example[\"preamble\"][\"input_ids\"]) > 1\n+                and encoded_example[\"preamble\"][\"input_ids\"][-1]\n+                == self.tokenizer.eos_token_id\n+            ):\n+                encoded_example[\"preamble\"][\"input_ids\"] = encoded_example[\"preamble\"][\n+                    \"input_ids\"\n+                ][:-1]\n+\n+            encoded_example[\"context\"] = self.tokenizer(ctxt, add_special_tokens=False)\n+            encoded_example[\"continuation\"] = self.tokenizer(\n+                cont, add_special_tokens=False\n+            )\n \n             examples.append(encoded_example)\n \n@@ -469,29 +563,34 @@ class InContextLearningLMTaskDataset(Dataset):\n         inputs = []\n         continuation_indices = []\n         for data_pair in data:\n-            preamble, context, continuation = (data_pair['preamble'], data_pair['context'], data_pair['continuation'])\n+            preamble, context, continuation = (\n+                data_pair[\"preamble\"],\n+                data_pair[\"context\"],\n+                data_pair[\"continuation\"],\n+            )\n \n-            context_enc = preamble['input_ids'] + context['input_ids']\n-            continuation_enc = continuation['input_ids']\n+            context_enc = preamble[\"input_ids\"] + context[\"input_ids\"]\n+            continuation_enc = continuation[\"input_ids\"]\n \n-            inp, continuation_span = _make_padded_input(context_enc, continuation_enc, self.max_seq_len,\n-                                                        self.pad_tok_id)\n+            inp, continuation_span = _make_padded_input(\n+                context_enc, continuation_enc, self.max_seq_len, self.pad_tok_id\n+            )\n \n             inputs.append(inp)\n             continuation_indices.append(continuation_span)\n \n         batch = {\n-            'input_ids': torch.stack(inputs),\n-            'continuation_indices': continuation_indices,\n-            'mode': 'icl_task',\n-            'labels': torch.stack(inputs),\n+            \"input_ids\": torch.stack(inputs),\n+            \"continuation_indices\": continuation_indices,\n+            \"mode\": \"icl_task\",\n+            \"labels\": torch.stack(inputs),\n         }\n \n-        batch['attention_mask'] = ~(batch['input_ids'] == self.pad_tok_id)\n+        batch[\"attention_mask\"] = ~(batch[\"input_ids\"] == self.pad_tok_id)\n         return batch\n \n     def get_num_samples_in_batch(self, batch) -> int:\n-        return batch['input_ids'].shape[0]\n+        return batch[\"input_ids\"].shape[0]\n \n \n class InContextLearningMultipleChoiceTaskDataset(Dataset):\n@@ -529,7 +628,9 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n     def __init__(\n         self,\n         dataset_uri: str,\n-        tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n+        tokenizer: Union[\n+            transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+        ],\n         max_seq_len: int,\n         pad_tok_id: int,\n         num_fewshot: int,\n@@ -542,23 +643,30 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n         try:\n             from datasets import load_dataset  # pyright: ignore [reportGeneralTypeIssues]\n         except ImportError as e:\n-            raise MissingConditionalImportError(extra_deps_group='nlp',\n-                                                conda_package='datasets',\n-                                                conda_channel='conda-forge') from e\n+            raise MissingConditionalImportError(\n+                extra_deps_group=\"nlp\",\n+                conda_package=\"datasets\",\n+                conda_channel=\"conda-forge\",\n+            ) from e\n \n         with dist.local_rank_zero_download_and_wait(destination_path):\n             if dist.get_local_rank() == 0:\n                 get_file(dataset_uri, destination_path, overwrite=True)\n-        dataset = load_dataset('json', data_files=destination_path, split='train', streaming=False)\n+        dataset = load_dataset(\n+            \"json\", data_files=destination_path, split=\"train\", streaming=False\n+        )\n         self.samples = list(\n-            dataset.map(lambda examples: {\n-                'query': examples['query'],\n-                'choices': examples['choices'],\n-                'gold': examples['gold']\n-            }))\n+            dataset.map(\n+                lambda examples: {\n+                    \"query\": examples[\"query\"],\n+                    \"choices\": examples[\"choices\"],\n+                    \"gold\": examples[\"gold\"],\n+                }\n+            )\n+        )\n         self.samples = strip_data(self.samples)\n \n-        self.num_choices = len(self.samples[0]['choices'])\n+        self.num_choices = len(self.samples[0][\"choices\"])\n         self.tokenizer = tokenizer\n         self.max_seq_len = max_seq_len\n         self.pad_tok_id = pad_tok_id\n@@ -566,11 +674,22 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n \n         self.prefix_space = _tokenizer_needs_prefix_space(self.tokenizer)\n \n-        self.encoded_dataset = self.prep_examples(num_fewshot, prompt_string, example_delimiter, continuation_delimiter,\n-                                                  fewshot_rng)\n+        self.encoded_dataset = self.prep_examples(\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            fewshot_rng,\n+        )\n \n-    def prep_examples(self, num_fewshot: int, prompt_string: str, example_delimiter: str, continuation_delimiter: str,\n-                      fewshot_rng: random.Random):\n+    def prep_examples(\n+        self,\n+        num_fewshot: int,\n+        prompt_string: str,\n+        example_delimiter: str,\n+        continuation_delimiter: str,\n+        fewshot_rng: random.Random,\n+    ):\n         \"\"\"Prepares a set of multiple choice questions into tokenized format with prompt and few shot examples.\n \n         Each question consists of a query and set of answer choices, only one of which is correct. At inference time\n@@ -593,42 +712,59 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n         \"\"\"\n         examples = []\n         for sample_idx in tqdm(range(len(self.samples))):\n-\n             preamble = prompt_string\n             if num_fewshot > 0:\n-                fewshot_idxs = _get_fewshot_sample_idxs(len(self.samples), num_fewshot, sample_idx, fewshot_rng)\n+                fewshot_idxs = _get_fewshot_sample_idxs(\n+                    len(self.samples), num_fewshot, sample_idx, fewshot_rng\n+                )\n                 for fewshot_idx in fewshot_idxs:\n-                    query, choices, gold_idx = self.samples[fewshot_idx]['query'], self.samples[fewshot_idx][\n-                        'choices'], self.samples[fewshot_idx]['gold']\n+                    query, choices, gold_idx = (\n+                        self.samples[fewshot_idx][\"query\"],\n+                        self.samples[fewshot_idx][\"choices\"],\n+                        self.samples[fewshot_idx][\"gold\"],\n+                    )\n                     if len(preamble) > 0:\n-                        query = f'{example_delimiter}{query}'\n+                        query = f\"{example_delimiter}{query}\"\n                     assert isinstance(gold_idx, int)\n-                    preamble += f'{query}{continuation_delimiter}{choices[gold_idx]}'\n+                    preamble += f\"{query}{continuation_delimiter}{choices[gold_idx]}\"\n             encoded_example = {}\n-            query, choices, gold_idx = self.samples[sample_idx]['query'], self.samples[sample_idx][\n-                'choices'], self.samples[sample_idx]['gold'],\n+            query, choices, gold_idx = (\n+                self.samples[sample_idx][\"query\"],\n+                self.samples[sample_idx][\"choices\"],\n+                self.samples[sample_idx][\"gold\"],\n+            )\n             if len(preamble) > 0:\n-                query = f'{example_delimiter}{query}'\n+                query = f\"{example_delimiter}{query}\"\n \n             # rstrip the continuation delimiter, because the prompt ending in a space results in degenerate output\n             continuation_delimiter_stripped = continuation_delimiter.rstrip()\n \n             if self.prefix_space:\n-                choices = [(f' {choice}' if not choice.startswith(' ') else choice) for choice in choices]\n+                choices = [\n+                    (f\" {choice}\" if not choice.startswith(\" \") else choice)\n+                    for choice in choices\n+                ]\n             query += continuation_delimiter_stripped\n-            encoded_example['preamble'] = self.tokenizer(\n+            encoded_example[\"preamble\"] = self.tokenizer(\n                 preamble\n             )  # if the preamble is empty then these will be 0-length lists, unless the tokenizer adds special tokens to empty strings (e.g. OPT tokenizer)\n \n-            if self.tokenizer.eos_token_id is not None and len(\n-                    encoded_example['preamble']\n-                ['input_ids']) > 1 and encoded_example['preamble']['input_ids'][-1] == self.tokenizer.eos_token_id:\n-                encoded_example['preamble']['input_ids'] = encoded_example['preamble']['input_ids'][:-1]\n+            if (\n+                self.tokenizer.eos_token_id is not None\n+                and len(encoded_example[\"preamble\"][\"input_ids\"]) > 1\n+                and encoded_example[\"preamble\"][\"input_ids\"][-1]\n+                == self.tokenizer.eos_token_id\n+            ):\n+                encoded_example[\"preamble\"][\"input_ids\"] = encoded_example[\"preamble\"][\n+                    \"input_ids\"\n+                ][:-1]\n \n-            encoded_example['gold_idx'] = gold_idx\n+            encoded_example[\"gold_idx\"] = gold_idx\n \n-            encoded_example['query'] = self.tokenizer(query, add_special_tokens=False)\n-            encoded_example['choices'] = [self.tokenizer(choice, add_special_tokens=False) for choice in choices]\n+            encoded_example[\"query\"] = self.tokenizer(query, add_special_tokens=False)\n+            encoded_example[\"choices\"] = [\n+                self.tokenizer(choice, add_special_tokens=False) for choice in choices\n+            ]\n \n             examples.append(encoded_example)\n \n@@ -646,16 +782,20 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n         gold_idxs = []\n         choice_groupings = []\n         for data_pair in data:\n-\n             choice_start_idx = len(continuation_indices)\n-            preamble, context, choices, gold_idx = (data_pair['preamble'], data_pair['query'], data_pair['choices'],\n-                                                    data_pair['gold_idx'])\n+            preamble, context, choices, gold_idx = (\n+                data_pair[\"preamble\"],\n+                data_pair[\"query\"],\n+                data_pair[\"choices\"],\n+                data_pair[\"gold_idx\"],\n+            )\n \n             for choice in choices:\n-                context_enc = preamble['input_ids'] + context['input_ids']\n-                continuation_enc = choice['input_ids']\n-                inp, continuation_span = _make_padded_input(context_enc, continuation_enc, self.max_seq_len,\n-                                                            self.pad_tok_id)\n+                context_enc = preamble[\"input_ids\"] + context[\"input_ids\"]\n+                continuation_enc = choice[\"input_ids\"]\n+                inp, continuation_span = _make_padded_input(\n+                    context_enc, continuation_enc, self.max_seq_len, self.pad_tok_id\n+                )\n \n                 inputs.append(inp)\n                 continuation_indices.append(continuation_span)\n@@ -672,18 +812,18 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n         # which contiguous sequences of elements in the batch correspond to which question\n         # gold_indices indicates which of the [0, N-1] choices is the correct one for each question.\n         batch = {\n-            'input_ids': torch.stack(inputs),\n-            'continuation_indices': continuation_indices,\n-            'mode': 'icl_task',\n-            'labels': torch.stack(inputs),\n-            'gold_indices': gold_idxs,\n-            'choice_groupings': choice_groupings\n+            \"input_ids\": torch.stack(inputs),\n+            \"continuation_indices\": continuation_indices,\n+            \"mode\": \"icl_task\",\n+            \"labels\": torch.stack(inputs),\n+            \"gold_indices\": gold_idxs,\n+            \"choice_groupings\": choice_groupings,\n         }\n-        batch['attention_mask'] = ~(batch['input_ids'] == self.pad_tok_id)\n+        batch[\"attention_mask\"] = ~(batch[\"input_ids\"] == self.pad_tok_id)\n         return batch\n \n     def get_num_samples_in_batch(self, batch) -> int:\n-        return batch['input_ids'].shape[0] // self.num_choices\n+        return batch[\"input_ids\"].shape[0] // self.num_choices\n \n     def split_batch(self, batch: Any, microbatch_size: int):\n         \"\"\"Split batch while ensuring all continuations are in the same microbatch.\n@@ -697,19 +837,19 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n         # Don't split kwargs that don't change\n         # Normally split torch tensors\n         # List split lists of strings\n-        no_split = ['mode']\n+        no_split = [\"mode\"]\n         # Real\n-        real = ['input_ids', 'labels', 'attention_mask']\n-        logical = ['gold_indices']\n+        real = [\"input_ids\", \"labels\", \"attention_mask\"]\n+        logical = [\"gold_indices\"]\n         chunked = {}\n         for k, v in batch.items():\n             if k in no_split:\n                 # Defer broadcasting primitives until we know num_chunks\n                 pass\n-            elif k == 'continuation_indices':\n+            elif k == \"continuation_indices\":\n                 # List of list, so we have to directly call _split_list\n                 chunked[k] = _split_list(v, microbatch_size * self.num_choices)\n-            elif k == 'choice_groupings':\n+            elif k == \"choice_groupings\":\n                 # List of list, so we have to directly call _split_list\n                 chunked[k] = _split_list(v, microbatch_size)\n             elif k in real:\n@@ -717,8 +857,8 @@ class InContextLearningMultipleChoiceTaskDataset(Dataset):\n             elif k in logical:\n                 chunked[k] = _default_split_batch(v, microbatch_size)\n             else:\n-                raise ValueError(f'Unexpected key {k}')\n-        num_chunks = len(chunked['input_ids'])\n+                raise ValueError(f\"Unexpected key {k}\")\n+        num_chunks = len(chunked[\"input_ids\"])\n         # Broadcast primitives to all chunks\n         for k, v in batch.items():\n             if isinstance(v, (int, float, str, bool)):\n@@ -757,7 +897,9 @@ class InContextLearningSchemaTaskDataset(InContextLearningMultipleChoiceTaskData\n     def __init__(\n         self,\n         dataset_uri: str,\n-        tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n+        tokenizer: Union[\n+            transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+        ],\n         max_seq_len: int,\n         pad_tok_id: int,\n         num_fewshot: int,\n@@ -770,24 +912,30 @@ class InContextLearningSchemaTaskDataset(InContextLearningMultipleChoiceTaskData\n         try:\n             from datasets import load_dataset  # pyright: ignore [reportGeneralTypeIssues]\n         except ImportError as e:\n-            raise MissingConditionalImportError(extra_deps_group='nlp',\n-                                                conda_package='datasets',\n-                                                conda_channel='conda-forge') from e\n+            raise MissingConditionalImportError(\n+                extra_deps_group=\"nlp\",\n+                conda_package=\"datasets\",\n+                conda_channel=\"conda-forge\",\n+            ) from e\n \n         with dist.local_rank_zero_download_and_wait(destination_path):\n             if dist.get_local_rank() == 0:\n                 get_file(dataset_uri, destination_path, overwrite=True)\n-        dataset = load_dataset('json', data_files=destination_path, split='train', streaming=False)\n+        dataset = load_dataset(\n+            \"json\", data_files=destination_path, split=\"train\", streaming=False\n+        )\n         self.samples = list(\n             dataset.map(\n                 lambda examples: {\n-                    'context_options': examples['context_options'],\n-                    'continuation': examples['continuation'],\n-                    'gold': examples['gold']\n-                }))\n+                    \"context_options\": examples[\"context_options\"],\n+                    \"continuation\": examples[\"continuation\"],\n+                    \"gold\": examples[\"gold\"],\n+                }\n+            )\n+        )\n         self.samples = strip_data(self.samples)\n \n-        self.num_choices = len(self.samples[0]['context_options'])\n+        self.num_choices = len(self.samples[0][\"context_options\"])\n         self.tokenizer = tokenizer\n         self.max_seq_len = max_seq_len\n         self.pad_tok_id = pad_tok_id\n@@ -795,11 +943,22 @@ class InContextLearningSchemaTaskDataset(InContextLearningMultipleChoiceTaskData\n \n         self.prefix_space = _tokenizer_needs_prefix_space(self.tokenizer)\n \n-        self.encoded_dataset = self.prep_examples(num_fewshot, prompt_string, example_delimiter, continuation_delimiter,\n-                                                  fewshot_rng)\n+        self.encoded_dataset = self.prep_examples(\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            fewshot_rng,\n+        )\n \n-    def prep_examples(self, num_fewshot: int, prompt_string: str, example_delimiter: str, continuation_delimiter: str,\n-                      fewshot_rng: random.Random):\n+    def prep_examples(\n+        self,\n+        num_fewshot: int,\n+        prompt_string: str,\n+        example_delimiter: str,\n+        continuation_delimiter: str,\n+        fewshot_rng: random.Random,\n+    ):\n         \"\"\"Prepares a set of schema questions into tokenized format with prompt and few shot examples.\n         Each question consists of a set of possible contexts followed by a continuation, only one of the contexts would logically permit the continuation.\n         At inference time we construct individual inference examples consisting of a single context option + the continuation,\n@@ -819,42 +978,65 @@ class InContextLearningSchemaTaskDataset(InContextLearningMultipleChoiceTaskData\n \n         examples = []\n         for sample_idx in tqdm(range(len(self.samples))):\n-\n             preamble = prompt_string\n             if num_fewshot > 0:\n-                fewshot_idxs = _get_fewshot_sample_idxs(len(self.samples), num_fewshot, sample_idx, fewshot_rng)\n+                fewshot_idxs = _get_fewshot_sample_idxs(\n+                    len(self.samples), num_fewshot, sample_idx, fewshot_rng\n+                )\n                 for fewshot_idx in fewshot_idxs:\n-                    context_options, continuation, gold_idx = self.samples[fewshot_idx][\n-                        'context_options'], self.samples[fewshot_idx]['continuation'], self.samples[fewshot_idx]['gold']\n+                    context_options, continuation, gold_idx = (\n+                        self.samples[fewshot_idx][\"context_options\"],\n+                        self.samples[fewshot_idx][\"continuation\"],\n+                        self.samples[fewshot_idx][\"gold\"],\n+                    )\n                     assert isinstance(gold_idx, int)\n                     context = context_options[gold_idx]\n                     if len(preamble) > 0:\n-                        context = f'{example_delimiter}{context}'\n-                    preamble += f'{context}{continuation_delimiter}{continuation}'\n+                        context = f\"{example_delimiter}{context}\"\n+                    preamble += f\"{context}{continuation_delimiter}{continuation}\"\n \n             encoded_example = {}\n-            context_options, continuation, gold_idx = self.samples[sample_idx]['context_options'], self.samples[\n-                sample_idx]['continuation'], self.samples[sample_idx]['gold'],\n+            context_options, continuation, gold_idx = (\n+                self.samples[sample_idx][\"context_options\"],\n+                self.samples[sample_idx][\"continuation\"],\n+                self.samples[sample_idx][\"gold\"],\n+            )\n \n             # rstrip the continuation delimiter, because the prompt ending in a space results in degenerate output\n             continuation_delimiter_stripped = continuation_delimiter.rstrip()\n \n             if len(preamble) > 0:\n-                context_options = [f'{example_delimiter}{c}{continuation_delimiter_stripped}' for c in context_options]\n-            encoded_example['preamble'] = self.tokenizer(\n+                context_options = [\n+                    f\"{example_delimiter}{c}{continuation_delimiter_stripped}\"\n+                    for c in context_options\n+                ]\n+            encoded_example[\"preamble\"] = self.tokenizer(\n                 preamble\n             )  # if the preamble is empty then these will be 0-length lists, unless the tokenizer adds special tokens to empty strings (e.g. OPT tokenizer)\n-            if self.tokenizer.eos_token_id is not None and len(\n-                    encoded_example['preamble']\n-                ['input_ids']) > 1 and encoded_example['preamble']['input_ids'][-1] == self.tokenizer.eos_token_id:\n-                encoded_example['preamble']['input_ids'] = encoded_example['preamble']['input_ids'][:-1]\n-\n-            encoded_example['gold_idx'] = gold_idx\n-            encoded_example['context_options'] = [self.tokenizer(c, add_special_tokens=False) for c in context_options]\n+            if (\n+                self.tokenizer.eos_token_id is not None\n+                and len(encoded_example[\"preamble\"][\"input_ids\"]) > 1\n+                and encoded_example[\"preamble\"][\"input_ids\"][-1]\n+                == self.tokenizer.eos_token_id\n+            ):\n+                encoded_example[\"preamble\"][\"input_ids\"] = encoded_example[\"preamble\"][\n+                    \"input_ids\"\n+                ][:-1]\n+\n+            encoded_example[\"gold_idx\"] = gold_idx\n+            encoded_example[\"context_options\"] = [\n+                self.tokenizer(c, add_special_tokens=False) for c in context_options\n+            ]\n \n             if self.prefix_space:\n-                continuation = f' {continuation}' if not continuation.startswith(' ') else continuation\n-            encoded_example['continuation'] = self.tokenizer(continuation, add_special_tokens=False)\n+                continuation = (\n+                    f\" {continuation}\"\n+                    if not continuation.startswith(\" \")\n+                    else continuation\n+                )\n+            encoded_example[\"continuation\"] = self.tokenizer(\n+                continuation, add_special_tokens=False\n+            )\n             examples.append(encoded_example)\n \n         return examples\n@@ -865,16 +1047,20 @@ class InContextLearningSchemaTaskDataset(InContextLearningMultipleChoiceTaskData\n         gold_idxs = []\n         choice_groupings = []\n         for data_pair in data:\n-\n             continuation_start_idx = len(continuation_indices)\n-            preamble, context_options, continuation, gold_idx = (data_pair['preamble'], data_pair['context_options'],\n-                                                                 data_pair['continuation'], data_pair['gold_idx'])\n+            preamble, context_options, continuation, gold_idx = (\n+                data_pair[\"preamble\"],\n+                data_pair[\"context_options\"],\n+                data_pair[\"continuation\"],\n+                data_pair[\"gold_idx\"],\n+            )\n \n             for ctxt in context_options:\n-                context_enc = preamble['input_ids'] + ctxt['input_ids']\n-                continuation_enc = continuation['input_ids']\n-                inp, continuation_span = _make_padded_input(context_enc, continuation_enc, self.max_seq_len,\n-                                                            self.pad_tok_id)\n+                context_enc = preamble[\"input_ids\"] + ctxt[\"input_ids\"]\n+                continuation_enc = continuation[\"input_ids\"]\n+                inp, continuation_span = _make_padded_input(\n+                    context_enc, continuation_enc, self.max_seq_len, self.pad_tok_id\n+                )\n \n                 inputs.append(inp)\n                 continuation_indices.append(continuation_span)\n@@ -891,19 +1077,19 @@ class InContextLearningSchemaTaskDataset(InContextLearningMultipleChoiceTaskData\n         # which contiguous sequences of elements in the batch correspond to which question\n         # gold_indices indicates which of the [0, N-1] choices is the correct one for each question.\n         batch = {\n-            'input_ids': torch.stack(inputs),\n-            'continuation_indices': continuation_indices,\n-            'mode': 'icl_task',\n-            'labels': torch.stack(inputs),\n-            'gold_indices': gold_idxs,\n-            'choice_groupings': choice_groupings\n+            \"input_ids\": torch.stack(inputs),\n+            \"continuation_indices\": continuation_indices,\n+            \"mode\": \"icl_task\",\n+            \"labels\": torch.stack(inputs),\n+            \"gold_indices\": gold_idxs,\n+            \"choice_groupings\": choice_groupings,\n         }\n-        batch['attention_mask'] = ~(batch['input_ids'] == self.pad_tok_id)\n+        batch[\"attention_mask\"] = ~(batch[\"input_ids\"] == self.pad_tok_id)\n         return batch\n \n \n class InContextLearningCodeEvalDataset(Dataset):\n-    \"\"\" A dataset that constructs batches for in-context learning code evaluation\n+    \"\"\"A dataset that constructs batches for in-context learning code evaluation\n \n     The input format is expected to be a jsonl file with the following fields:\n     - task_id: label of given task\n@@ -936,7 +1122,9 @@ class InContextLearningCodeEvalDataset(Dataset):\n     def __init__(\n         self,\n         dataset_uri: str,\n-        tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n+        tokenizer: Union[\n+            transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+        ],\n         max_seq_len: int,\n         pad_tok_id: int,\n         num_fewshot: int,\n@@ -953,29 +1141,35 @@ class InContextLearningCodeEvalDataset(Dataset):\n         try:\n             from datasets import load_dataset  # pyright: ignore [reportGeneralTypeIssues]\n         except ImportError as e:\n-            raise MissingConditionalImportError(extra_deps_group='nlp',\n-                                                conda_package='datasets',\n-                                                conda_channel='conda-forge') from e\n+            raise MissingConditionalImportError(\n+                extra_deps_group=\"nlp\",\n+                conda_package=\"datasets\",\n+                conda_channel=\"conda-forge\",\n+            ) from e\n         with dist.local_rank_zero_download_and_wait(destination_path):\n             if dist.get_local_rank() == 0:\n                 get_file(dataset_uri, destination_path, overwrite=True)\n-        dataset = load_dataset('json', data_files=destination_path, split='train', streaming=False)\n+        dataset = load_dataset(\n+            \"json\", data_files=destination_path, split=\"train\", streaming=False\n+        )\n         self.samples = list(\n             dataset.map(\n                 lambda examples: {\n-                    'task_id': examples['task_id'],\n-                    'prompt': examples['prompt'],\n-                    'canonical_solution': examples['canonical_solution'],\n-                    'test': examples['test'],\n-                    'entry_point': examples['entry_point'],\n-                    'test_inputs': examples['test_inputs'],\n-                    'test_outputs': examples['test_outputs'],\n-                    'language': examples['language'],\n-                }))\n+                    \"task_id\": examples[\"task_id\"],\n+                    \"prompt\": examples[\"prompt\"],\n+                    \"canonical_solution\": examples[\"canonical_solution\"],\n+                    \"test\": examples[\"test\"],\n+                    \"entry_point\": examples[\"entry_point\"],\n+                    \"test_inputs\": examples[\"test_inputs\"],\n+                    \"test_outputs\": examples[\"test_outputs\"],\n+                    \"language\": examples[\"language\"],\n+                }\n+            )\n+        )\n \n         if generations_per_sample < pass_at_k:\n             raise ValueError(\n-                f'generations_per_sample ({generations_per_sample}) must be greater than or equal to pass_at_k ({pass_at_k}) for code evaluation.'\n+                f\"generations_per_sample ({generations_per_sample}) must be greater than or equal to pass_at_k ({pass_at_k}) for code evaluation.\"\n             )\n \n         self.pass_at_k = pass_at_k\n@@ -984,16 +1178,23 @@ class InContextLearningCodeEvalDataset(Dataset):\n         self.tokenizer = tokenizer\n         self.max_seq_len = max_seq_len\n         self.pad_tok_id = pad_tok_id\n-        self.padding_side = 'left'\n+        self.padding_side = \"left\"\n         self.max_prompt_length = 0\n         self.top_p = top_p\n         self.top_k = top_k\n         fewshot_rng = random.Random(fewshot_random_seed)\n-        self.encoded_dataset = self.prep_examples(num_fewshot, prompt_string, example_delimiter, code_prelimiter,\n-                                                  fewshot_rng)\n+        self.encoded_dataset = self.prep_examples(\n+            num_fewshot, prompt_string, example_delimiter, code_prelimiter, fewshot_rng\n+        )\n \n-    def prep_examples(self, num_fewshot: int, prompt_string: str, example_delimiter: str, code_prelimiter: str,\n-                      fewshot_rng: random.Random):\n+    def prep_examples(\n+        self,\n+        num_fewshot: int,\n+        prompt_string: str,\n+        example_delimiter: str,\n+        code_prelimiter: str,\n+        fewshot_rng: random.Random,\n+    ):\n         \"\"\"Prepares a set of code evaluation tasks into tokenized format with prompt and fewshot examples.\n \n         Each task consists of a context as well as an optional prompt and optional list of\n@@ -1017,41 +1218,57 @@ class InContextLearningCodeEvalDataset(Dataset):\n             preamble = prompt_string\n \n             if num_fewshot > 0:\n-                fewshot_idxs = _get_fewshot_sample_idxs(len(self.samples), num_fewshot, sample_idx, fewshot_rng)\n+                fewshot_idxs = _get_fewshot_sample_idxs(\n+                    len(self.samples), num_fewshot, sample_idx, fewshot_rng\n+                )\n                 for fewshot_idx in fewshot_idxs:\n-                    ctxt, cont = self.samples[fewshot_idx]['prompt'], self.samples[fewshot_idx]['canonical_solution']\n-                    ctxt = f'{code_prelimiter}{ctxt}'\n+                    ctxt, cont = (\n+                        self.samples[fewshot_idx][\"prompt\"],\n+                        self.samples[fewshot_idx][\"canonical_solution\"],\n+                    )\n+                    ctxt = f\"{code_prelimiter}{ctxt}\"\n                     if len(preamble) > 0:\n-                        ctxt = f'{example_delimiter}{ctxt}'\n-                    preamble += f'{ctxt}{cont}'\n+                        ctxt = f\"{example_delimiter}{ctxt}\"\n+                    preamble += f\"{ctxt}{cont}\"\n \n-            ctxt = self.samples[sample_idx]['prompt']\n-            ctxt = f'{code_prelimiter}{ctxt}'\n+            ctxt = self.samples[sample_idx][\"prompt\"]\n+            ctxt = f\"{code_prelimiter}{ctxt}\"\n             if len(preamble) > 0:\n-                ctxt = f'{example_delimiter}{ctxt}'\n+                ctxt = f\"{example_delimiter}{ctxt}\"\n \n             # If the preamble is empty then this will be a 0-length list, unless the tokenizer adds special tokens to empty strings (e.g. OPT tokenizer)\n-            encoded_example['preamble'] = self.tokenizer(preamble)\n+            encoded_example[\"preamble\"] = self.tokenizer(preamble)\n             # If there is an EOS token added, we need to remove it so it is not in the middle of the prompt\n-            if self.tokenizer.eos_token_id is not None and len(\n-                    encoded_example['preamble']\n-                ['input_ids']) > 1 and encoded_example['preamble']['input_ids'][-1] == self.tokenizer.eos_token_id:\n-                encoded_example['preamble']['input_ids'] = encoded_example['preamble']['input_ids'][:-1]\n-\n-            encoded_example['prompt'] = self.tokenizer(ctxt, add_special_tokens=False)\n-            encoded_example['prompt_text'] = self.samples[sample_idx]['prompt']\n-            encoded_example['task_id'] = self.samples[sample_idx]['task_id']\n-            encoded_example['canonical_solution'] = self.samples[sample_idx]['canonical_solution']\n-            encoded_example['test'] = self.samples[sample_idx]['test']\n-            encoded_example['entry_point'] = self.samples[sample_idx]['entry_point']\n-            encoded_example['test_inputs'] = self.samples[sample_idx]['test_inputs']\n-            encoded_example['test_outputs'] = self.samples[sample_idx]['test_outputs']\n-            encoded_example['language'] = self.samples[sample_idx]['language']\n+            if (\n+                self.tokenizer.eos_token_id is not None\n+                and len(encoded_example[\"preamble\"][\"input_ids\"]) > 1\n+                and encoded_example[\"preamble\"][\"input_ids\"][-1]\n+                == self.tokenizer.eos_token_id\n+            ):\n+                encoded_example[\"preamble\"][\"input_ids\"] = encoded_example[\"preamble\"][\n+                    \"input_ids\"\n+                ][:-1]\n+\n+            encoded_example[\"prompt\"] = self.tokenizer(ctxt, add_special_tokens=False)\n+            encoded_example[\"prompt_text\"] = self.samples[sample_idx][\"prompt\"]\n+            encoded_example[\"task_id\"] = self.samples[sample_idx][\"task_id\"]\n+            encoded_example[\"canonical_solution\"] = self.samples[sample_idx][\n+                \"canonical_solution\"\n+            ]\n+            encoded_example[\"test\"] = self.samples[sample_idx][\"test\"]\n+            encoded_example[\"entry_point\"] = self.samples[sample_idx][\"entry_point\"]\n+            encoded_example[\"test_inputs\"] = self.samples[sample_idx][\"test_inputs\"]\n+            encoded_example[\"test_outputs\"] = self.samples[sample_idx][\"test_outputs\"]\n+            encoded_example[\"language\"] = self.samples[sample_idx][\"language\"]\n \n             examples.append(encoded_example)\n             max_prompt_length = max(\n                 max_prompt_length,\n-                len(encoded_example['preamble']['input_ids'] + encoded_example['prompt']['input_ids']))\n+                len(\n+                    encoded_example[\"preamble\"][\"input_ids\"]\n+                    + encoded_example[\"prompt\"][\"input_ids\"]\n+                ),\n+            )\n \n         self.max_prompt_length = max_prompt_length\n         return examples\n@@ -1063,24 +1280,46 @@ class InContextLearningCodeEvalDataset(Dataset):\n         return len(self.encoded_dataset)\n \n     def collate_fn(self, data):\n-        inputs, prompts, tests, canonical_solutions, entry_points, test_inputs, test_outputs, languages = [], [], [], [], [], [], [], []\n+        (\n+            inputs,\n+            prompts,\n+            tests,\n+            canonical_solutions,\n+            entry_points,\n+            test_inputs,\n+            test_outputs,\n+            languages,\n+        ) = [], [], [], [], [], [], [], []\n         for sample in data:\n-            preamble, prompt, text_prompt, canonical_solution, test, entry_point, test_input, test_output, language = (\n-                sample['preamble'],\n-                sample['prompt'],\n-                sample['prompt_text'],\n-                sample['canonical_solution'],\n-                sample['test'],\n-                sample['entry_point'],\n-                sample['test_inputs'],\n-                sample['test_outputs'],\n-                sample['language'],\n+            (\n+                preamble,\n+                prompt,\n+                text_prompt,\n+                canonical_solution,\n+                test,\n+                entry_point,\n+                test_input,\n+                test_output,\n+                language,\n+            ) = (\n+                sample[\"preamble\"],\n+                sample[\"prompt\"],\n+                sample[\"prompt_text\"],\n+                sample[\"canonical_solution\"],\n+                sample[\"test\"],\n+                sample[\"entry_point\"],\n+                sample[\"test_inputs\"],\n+                sample[\"test_outputs\"],\n+                sample[\"language\"],\n+            )\n+            context_enc = preamble[\"input_ids\"] + prompt[\"input_ids\"]\n+            inp, _ = _make_padded_input(\n+                context_enc,\n+                [],\n+                self.max_prompt_length,\n+                self.pad_tok_id,\n+                padding_side=self.padding_side,\n             )\n-            context_enc = preamble['input_ids'] + prompt['input_ids']\n-            inp, _ = _make_padded_input(context_enc, [],\n-                                        self.max_prompt_length,\n-                                        self.pad_tok_id,\n-                                        padding_side=self.padding_side)\n \n             inputs.append(inp)\n             tests.append(test)\n@@ -1092,44 +1331,50 @@ class InContextLearningCodeEvalDataset(Dataset):\n             languages.append(language)\n \n         batch = {\n-            'input_ids': torch.stack(inputs),\n-            'mode': 'generate',\n-            'labels': canonical_solutions,\n-            'prompts': prompts,  # list of prompts\n-            'tests': tests,  # list of tests\n-            'canonical_solutions': canonical_solutions,  # list of solutions\n-            'entry_points': entry_points,  # list of entry points\n-            'test_inputs': test_inputs,  # list of test inputs\n-            'test_outputs': test_outputs,  # list of test outputs\n-            'languages': languages,  # list of languages\n-            'pass_at_k': self.pass_at_k,\n-            'generation_length': self.max_seq_len - self.max_prompt_length,\n-            'generation_kwargs': {\n-                'pad_token_id': self.pad_tok_id,\n-                'num_beams': 1,  # single beam\n-                'num_return_sequences': self.generations_per_sample,  # how many gens per prompt\n-                'do_sample': True,\n-                'top_p': self.top_p,\n-                'top_k': self.top_k,\n-                'use_cache': True,\n-            }\n+            \"input_ids\": torch.stack(inputs),\n+            \"mode\": \"generate\",\n+            \"labels\": canonical_solutions,\n+            \"prompts\": prompts,  # list of prompts\n+            \"tests\": tests,  # list of tests\n+            \"canonical_solutions\": canonical_solutions,  # list of solutions\n+            \"entry_points\": entry_points,  # list of entry points\n+            \"test_inputs\": test_inputs,  # list of test inputs\n+            \"test_outputs\": test_outputs,  # list of test outputs\n+            \"languages\": languages,  # list of languages\n+            \"pass_at_k\": self.pass_at_k,\n+            \"generation_length\": self.max_seq_len - self.max_prompt_length,\n+            \"generation_kwargs\": {\n+                \"pad_token_id\": self.pad_tok_id,\n+                \"num_beams\": 1,  # single beam\n+                \"num_return_sequences\": self.generations_per_sample,  # how many gens per prompt\n+                \"do_sample\": True,\n+                \"top_p\": self.top_p,\n+                \"top_k\": self.top_k,\n+                \"use_cache\": True,\n+            },\n         }\n-        batch['attention_mask'] = ~(batch['input_ids'] == self.pad_tok_id)\n+        batch[\"attention_mask\"] = ~(batch[\"input_ids\"] == self.pad_tok_id)\n         return batch\n \n     def get_num_samples_in_batch(self, batch) -> int:\n         # Count number of inputs in the batch\n-        return batch['input_ids'].shape[0]\n+        return batch[\"input_ids\"].shape[0]\n \n     def split_batch(self, batch: Any, microbatch_size: int):\n         # Don't split kwargs that don't change\n         # Normally split torch tensors\n         # List split lists of strings\n-        no_split = ['mode', 'generation_length', 'pass_at_k', 'generation_kwargs']\n-        normal_split = ['input_ids', 'attention_mask']\n+        no_split = [\"mode\", \"generation_length\", \"pass_at_k\", \"generation_kwargs\"]\n+        normal_split = [\"input_ids\", \"attention_mask\"]\n         list_split = [\n-            'labels', 'tests', 'canonical_solutions', 'entry_points', 'test_inputs', 'test_outputs', 'prompts',\n-            'languages'\n+            \"labels\",\n+            \"tests\",\n+            \"canonical_solutions\",\n+            \"entry_points\",\n+            \"test_inputs\",\n+            \"test_outputs\",\n+            \"prompts\",\n+            \"languages\",\n         ]\n         chunked = {}\n         for k, v in batch.items():\n@@ -1141,8 +1386,8 @@ class InContextLearningCodeEvalDataset(Dataset):\n             elif k in normal_split:\n                 chunked[k] = _default_split_batch(v, microbatch_size)\n             else:\n-                raise ValueError(f'Unexpected key {k}')\n-        num_chunks = len(chunked['input_ids'])\n+                raise ValueError(f\"Unexpected key {k}\")\n+        num_chunks = len(chunked[\"input_ids\"])\n         for k, v in batch.items():\n             if isinstance(v, (int, float, str, bool, dict)):\n                 chunked[k] = [v] * num_chunks\n@@ -1153,7 +1398,9 @@ class InContextLearningCodeEvalDataset(Dataset):\n def build_icl_dataloader(\n     icl_task_type: str,\n     dataset_uri: str,\n-    tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n+    tokenizer: Union[\n+        transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+    ],\n     batch_size: int,\n     max_seq_len: int,\n     pad_tok_id: int,\n@@ -1162,87 +1409,102 @@ def build_icl_dataloader(\n     example_delimiter: str,  # e.g. '\\n'\n     continuation_delimiter: str,  # e.g. ''\n     destination_path: str,\n-    question_prelimiter: str = '',  # e.g. 'Question: '\n-    cot_delimiter: str = '',\n+    question_prelimiter: str = \"\",  # e.g. 'Question: '\n+    cot_delimiter: str = \"\",\n     fewshot_random_seed: int = 1234,\n     pass_at_k: int = 1,\n     generations_per_sample: int = 1,\n ) -> DataSpec:\n-    if icl_task_type == 'multiple_choice':\n-        dataset = InContextLearningMultipleChoiceTaskDataset(dataset_uri,\n-                                                             tokenizer,\n-                                                             max_seq_len,\n-                                                             pad_tok_id,\n-                                                             num_fewshot,\n-                                                             prompt_string,\n-                                                             example_delimiter,\n-                                                             continuation_delimiter,\n-                                                             destination_path=destination_path,\n-                                                             fewshot_random_seed=fewshot_random_seed)\n+    if icl_task_type == \"multiple_choice\":\n+        dataset = InContextLearningMultipleChoiceTaskDataset(\n+            dataset_uri,\n+            tokenizer,\n+            max_seq_len,\n+            pad_tok_id,\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            destination_path=destination_path,\n+            fewshot_random_seed=fewshot_random_seed,\n+        )\n         batch_size = max(dataset.num_choices, batch_size)\n         effective_batchsize = batch_size // dataset.num_choices\n-    elif icl_task_type == 'schema':\n-        dataset = InContextLearningSchemaTaskDataset(dataset_uri,\n-                                                     tokenizer,\n-                                                     max_seq_len,\n-                                                     pad_tok_id,\n-                                                     num_fewshot,\n-                                                     prompt_string,\n-                                                     example_delimiter,\n-                                                     continuation_delimiter,\n-                                                     destination_path=destination_path,\n-                                                     fewshot_random_seed=fewshot_random_seed)\n+    elif icl_task_type == \"schema\":\n+        dataset = InContextLearningSchemaTaskDataset(\n+            dataset_uri,\n+            tokenizer,\n+            max_seq_len,\n+            pad_tok_id,\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            destination_path=destination_path,\n+            fewshot_random_seed=fewshot_random_seed,\n+        )\n         batch_size = max(dataset.num_choices, batch_size)\n         effective_batchsize = batch_size // dataset.num_choices\n-    elif icl_task_type == 'language_modeling':\n-        dataset = InContextLearningLMTaskDataset(dataset_uri,\n-                                                 tokenizer,\n-                                                 max_seq_len,\n-                                                 pad_tok_id,\n-                                                 num_fewshot,\n-                                                 prompt_string,\n-                                                 example_delimiter,\n-                                                 continuation_delimiter,\n-                                                 destination_path=destination_path,\n-                                                 fewshot_random_seed=fewshot_random_seed)\n+    elif icl_task_type == \"language_modeling\":\n+        dataset = InContextLearningLMTaskDataset(\n+            dataset_uri,\n+            tokenizer,\n+            max_seq_len,\n+            pad_tok_id,\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            destination_path=destination_path,\n+            fewshot_random_seed=fewshot_random_seed,\n+        )\n         effective_batchsize = batch_size\n-    elif icl_task_type == 'question_answering':\n-        dataset = InContextLearningQATaskDataset(dataset_uri,\n-                                                 tokenizer,\n-                                                 max_seq_len,\n-                                                 pad_tok_id,\n-                                                 num_fewshot,\n-                                                 prompt_string,\n-                                                 example_delimiter,\n-                                                 continuation_delimiter,\n-                                                 destination_path=destination_path,\n-                                                 question_prelimiter=question_prelimiter,\n-                                                 fewshot_random_seed=fewshot_random_seed,\n-                                                 cot_delimiter=cot_delimiter)\n+    elif icl_task_type == \"question_answering\":\n+        dataset = InContextLearningQATaskDataset(\n+            dataset_uri,\n+            tokenizer,\n+            max_seq_len,\n+            pad_tok_id,\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            continuation_delimiter,\n+            destination_path=destination_path,\n+            question_prelimiter=question_prelimiter,\n+            fewshot_random_seed=fewshot_random_seed,\n+            cot_delimiter=cot_delimiter,\n+        )\n         effective_batchsize = batch_size\n-    elif icl_task_type == 'code_evaluation':\n-        dataset = InContextLearningCodeEvalDataset(dataset_uri,\n-                                                   tokenizer,\n-                                                   max_seq_len,\n-                                                   pad_tok_id,\n-                                                   num_fewshot,\n-                                                   prompt_string,\n-                                                   example_delimiter,\n-                                                   destination_path=destination_path,\n-                                                   code_prelimiter=question_prelimiter,\n-                                                   fewshot_random_seed=fewshot_random_seed,\n-                                                   pass_at_k=pass_at_k,\n-                                                   generations_per_sample=generations_per_sample)\n+    elif icl_task_type == \"code_evaluation\":\n+        dataset = InContextLearningCodeEvalDataset(\n+            dataset_uri,\n+            tokenizer,\n+            max_seq_len,\n+            pad_tok_id,\n+            num_fewshot,\n+            prompt_string,\n+            example_delimiter,\n+            destination_path=destination_path,\n+            code_prelimiter=question_prelimiter,\n+            fewshot_random_seed=fewshot_random_seed,\n+            pass_at_k=pass_at_k,\n+            generations_per_sample=generations_per_sample,\n+        )\n         effective_batchsize = batch_size\n     else:\n-        raise Exception(f'Unrecognized ICL task type: {icl_task_type}')\n+        raise Exception(f\"Unrecognized ICL task type: {icl_task_type}\")\n \n     sampler = dist.get_sampler(dataset, drop_last=False, shuffle=False)\n \n     split_batch = None\n     if isinstance(\n-            dataset,\n-        (InContextLearningMultipleChoiceTaskDataset, InContextLearningQATaskDataset, InContextLearningCodeEvalDataset)):\n+        dataset,\n+        (\n+            InContextLearningMultipleChoiceTaskDataset,\n+            InContextLearningQATaskDataset,\n+            InContextLearningCodeEvalDataset,\n+        ),\n+    ):\n         split_batch = dataset.split_batch\n \n     return DataSpec(\n@@ -1258,7 +1520,9 @@ def build_icl_dataloader(\n     )\n \n \n-def partition_dataset_by_category(dataset_uri: str, destination_path: str) -> Dict[str, str]:\n+def partition_dataset_by_category(\n+    dataset_uri: str, destination_path: str\n+) -> Dict[str, str]:\n     \"\"\"If has_categories is enabled, we partition the dataset into a separate dataset for each category value in the data and write each partition to a local file.\n \n     Args:\n@@ -1274,51 +1538,58 @@ def partition_dataset_by_category(dataset_uri: str, destination_path: str) -> Di\n     try:\n         from datasets import load_dataset  # pyright: ignore [reportGeneralTypeIssues]\n     except ImportError as e:\n-        raise MissingConditionalImportError(extra_deps_group='nlp',\n-                                            conda_package='datasets',\n-                                            conda_channel='conda-forge') from e\n+        raise MissingConditionalImportError(\n+            extra_deps_group=\"nlp\",\n+            conda_package=\"datasets\",\n+            conda_channel=\"conda-forge\",\n+        ) from e\n     with dist.local_rank_zero_download_and_wait(destination_path):\n         if dist.get_local_rank() == 0:\n             get_file(dataset_uri, destination_path, overwrite=True)\n-    dataset = load_dataset('json', data_files=destination_path, split='train', streaming=False)\n-    if 'category' not in dataset.features.keys():\n+    dataset = load_dataset(\n+        \"json\", data_files=destination_path, split=\"train\", streaming=False\n+    )\n+    if \"category\" not in dataset.features.keys():\n         raise Exception(\n             f\"Attempted to partition dataset by `category` but it doesn't have a `category` key. Got keys: {str(list(dataset.features.keys()))}\"\n         )\n-    categories = sorted(set(dataset['category']))\n+    categories = sorted(set(dataset[\"category\"]))\n     output_files = {}\n     for cat in categories:\n-        path = destination_path.split('/')\n-        cat_dest = '/'.join(path[:-1]) + f'/{cat}_{path[-1]}'\n+        path = destination_path.split(\"/\")\n+        cat_dest = \"/\".join(path[:-1]) + f\"/{cat}_{path[-1]}\"\n         tmp_path_to_broadcast = str(os.path.abspath(cat_dest))\n         gathered_paths = dist.all_gather_object(tmp_path_to_broadcast)\n         if dist.get_local_rank() == 0:\n-            subset = [l for l in dataset if l['category'] == cat]\n-            with open(gathered_paths[0], 'w', encoding='utf8') as f:\n+            subset = [l for l in dataset if l[\"category\"] == cat]\n+            with open(gathered_paths[0], \"w\", encoding=\"utf8\") as f:\n                 for l in subset:\n-                    f.write(json.dumps(l, ensure_ascii=False) + '\\n')\n+                    f.write(json.dumps(l, ensure_ascii=False) + \"\\n\")\n         output_files[cat] = cat_dest\n     return output_files\n \n \n def get_icl_task_dataloader(\n-        icl_task_type: str,\n-        dataset_uri: str,\n-        tokenizer: Union[transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast],\n-        batch_size: int,\n-        max_seq_len: int,\n-        pad_tok_id: int,\n-        num_fewshot: int,\n-        prompt_string: str,  # e.g. 'translate english to french:'\n-        example_delimiter: str,  # e.g. '\\n'\n-        continuation_delimiter: str = '',\n-        destination_path: str = '',\n-        question_prelimiter: str = '',  # e.g. 'Question: '\n-        fewshot_random_seed: int = 1234,\n-        pass_at_k: int = 1,\n-        generations_per_sample: int = 1,\n-        cot_delimiter: str = '',\n-        has_categories: bool = False) -> Union[DataSpec, Dict[str, DataSpec]]:\n+    icl_task_type: str,\n+    dataset_uri: str,\n+    tokenizer: Union[\n+        transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast\n+    ],\n+    batch_size: int,\n+    max_seq_len: int,\n+    pad_tok_id: int,\n+    num_fewshot: int,\n+    prompt_string: str,  # e.g. 'translate english to french:'\n+    example_delimiter: str,  # e.g. '\\n'\n+    continuation_delimiter: str = \"\",\n+    destination_path: str = \"\",\n+    question_prelimiter: str = \"\",  # e.g. 'Question: '\n+    fewshot_random_seed: int = 1234,\n+    pass_at_k: int = 1,\n+    generations_per_sample: int = 1,\n+    cot_delimiter: str = \"\",\n+    has_categories: bool = False,\n+) -> Union[DataSpec, Dict[str, DataSpec]]:\n     \"\"\"This constructs a dataloader (or dataloaders if has_categories is True) capable of evaluating LLMs on in-context learning language modeling tasks, for example LAMBADA. An example usage is below:\n \n     >>> dl = get_icl_task_dataloader(\n@@ -1382,7 +1653,7 @@ def get_icl_task_dataloader(\n                 prompt_string,\n                 example_delimiter,\n                 continuation_delimiter,\n-                partition_uri + '_tmp',\n+                partition_uri + \"_tmp\",\n                 question_prelimiter,\n                 cot_delimiter,\n                 fewshot_random_seed,\ndiff --git a/composer/loss/utils.py b/composer/loss/utils.py\nindex 614c2f9..135e3af 100644\n--- a/composer/loss/utils.py\n+++ b/composer/loss/utils.py\n@@ -10,7 +10,7 @@ from typing import Optional\n \n import torch\n \n-__all__ = ['infer_target_type', 'ensure_targets_one_hot', 'check_for_index_targets']\n+__all__ = [\"infer_target_type\", \"ensure_targets_one_hot\", \"check_for_index_targets\"]\n \n \n def infer_target_type(input: torch.Tensor, targets: torch.Tensor) -> str:\n@@ -19,19 +19,21 @@ def infer_target_type(input: torch.Tensor, targets: torch.Tensor) -> str:\n     Example indices format: [1, 4, 7] Example one_hot format [[0, 1, 0], [1, 0, 0], ...]\n     \"\"\"\n     if input.shape == targets.shape:\n-        return 'one_hot'\n+        return \"one_hot\"\n     elif input.ndim == targets.ndim + 1:\n-        return 'indices'\n+        return \"indices\"\n     else:\n-        raise RuntimeError(f'Unable to infer indices or one_hot. Targets has shape {targets.shape}'\n-                           f' and the inputs to cross entropy has shape {input.shape}. For one_hot, '\n-                           'expect targets.shape == inputs.shape. For indices, expect '\n-                           'inputs.ndim == targets.ndim + 1')\n+        raise RuntimeError(\n+            f\"Unable to infer indices or one_hot. Targets has shape {targets.shape}\"\n+            f\" and the inputs to cross entropy has shape {input.shape}. For one_hot, \"\n+            \"expect targets.shape == inputs.shape. For indices, expect \"\n+            \"inputs.ndim == targets.ndim + 1\"\n+        )\n \n \n-def ensure_targets_one_hot(input: torch.Tensor,\n-                           targets: torch.Tensor,\n-                           num_classes: Optional[int] = None) -> torch.Tensor:\n+def ensure_targets_one_hot(\n+    input: torch.Tensor, targets: torch.Tensor, num_classes: Optional[int] = None\n+) -> torch.Tensor:\n     r\"\"\"Ensures that the targets are in a one-hot format rather than an index format.\n \n     Args:\n@@ -46,7 +48,7 @@ def ensure_targets_one_hot(input: torch.Tensor,\n         num_classes (int, optional): Number of classes. If not specified, this will be inferred\n             from input. Default: ``None``\n     \"\"\"\n-    if infer_target_type(input, targets) == 'indices':\n+    if infer_target_type(input, targets) == \"indices\":\n         # If the number of classes isn't specified, attempt to infer it from the input\n         if num_classes is None:\n             num_classes = input.shape[1]\n@@ -62,7 +64,9 @@ def check_for_index_targets(targets: torch.Tensor) -> bool:\n     return targets.dtype in index_dtypes\n \n \n-def _one_hot(tensor: torch.Tensor, num_classes: int = -1, dim: int = -1) -> torch.Tensor:\n+def _one_hot(\n+    tensor: torch.Tensor, num_classes: int = -1, dim: int = -1\n+) -> torch.Tensor:\n     \"\"\"Converts a tensor of index class labels to a tensor of one-hot class labels.\n \n     Implementation is based on MONAI one-hot conversion function:\n@@ -80,19 +84,23 @@ def _one_hot(tensor: torch.Tensor, num_classes: int = -1, dim: int = -1) -> torc\n             extra dimension of size ``num_classes`` inserted after the first dimension\n     \"\"\"\n     if not check_for_index_targets(tensor):\n-        raise ValueError(f'tensor must be integer type, current type: {tensor.dtype}')\n+        raise ValueError(f\"tensor must be integer type, current type: {tensor.dtype}\")\n \n     max_index = tensor.max() + 1\n     if num_classes == -1:\n         num_classes = int(max_index)\n \n     if num_classes < max_index:\n-        raise ValueError(f'num_classes must be greater than or equal to tensor.max() + 1: {num_classes} < {max_index}')\n+        raise ValueError(\n+            f\"num_classes must be greater than or equal to tensor.max() + 1: {num_classes} < {max_index}\"\n+        )\n \n     # Remove negative indices\n     neg_indices = tensor.min() < 0\n     if neg_indices:\n-        warnings.warn('Negative label indices are being ignored in conversion to one-hot labels')\n+        warnings.warn(\n+            \"Negative label indices are being ignored in conversion to one-hot labels\"\n+        )\n         tensor = tensor.clone().long()\n         tensor[tensor < 0] = num_classes\n         num_classes += 1  # Add extra class for negative indices\n@@ -103,7 +111,9 @@ def _one_hot(tensor: torch.Tensor, num_classes: int = -1, dim: int = -1) -> torc\n     tensor_shape[dim] = num_classes\n \n     # Convert to one-hot\n-    one_hot_tensor = torch.zeros(size=tensor_shape, dtype=tensor.dtype, device=tensor.device)\n+    one_hot_tensor = torch.zeros(\n+        size=tensor_shape, dtype=tensor.dtype, device=tensor.device\n+    )\n     one_hot_tensor.scatter_(dim=dim, index=tensor, value=1)\n \n     # Remove negative indices\ndiff --git a/composer/trainer/__init__.py b/composer/trainer/__init__.py\nindex c5cf6ec..ad9745e 100644\n--- a/composer/trainer/__init__.py\n+++ b/composer/trainer/__init__.py\n@@ -5,4 +5,4 @@\n \n from composer.trainer.trainer import Trainer\n \n-__all__ = ['Trainer']\n+__all__ = [\"Trainer\"]\ndiff --git a/composer/trainer/trainer.py b/composer/trainer/trainer.py\nindex 081fe68..f7e21b3 100644\n--- a/composer/trainer/trainer.py\n+++ b/composer/trainer/trainer.py\n@@ -20,8 +20,21 @@ import warnings\n from collections import defaultdict\n from copy import deepcopy\n from pathlib import Path\n-from typing import (Any, Callable, ContextManager, Dict, Iterable, List, Mapping, Optional, Sequence, TextIO, Tuple,\n-                    Union, cast)\n+from typing import (\n+    Any,\n+    Callable,\n+    ContextManager,\n+    Dict,\n+    Iterable,\n+    List,\n+    Mapping,\n+    Optional,\n+    Sequence,\n+    TextIO,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n \n import coolname\n import torch\n@@ -35,27 +48,82 @@ from torch.utils.data import DataLoader, DistributedSampler\n from torchmetrics import Metric\n \n from composer.callbacks import CheckpointSaver, OptimizerMonitor\n-from composer.core import (Algorithm, AlgorithmPass, Batch, BreakEpochException, Callback, DataSpec, Engine, Evaluator,\n-                           Event, Precision, PyTorchScheduler, State, Time, Timestamp, TimeUnit, TrainerMode,\n-                           ensure_data_spec, ensure_evaluator, ensure_time, get_precision_context,\n-                           validate_eval_automicrobatching)\n+from composer.core import (\n+    Algorithm,\n+    AlgorithmPass,\n+    Batch,\n+    BreakEpochException,\n+    Callback,\n+    DataSpec,\n+    Engine,\n+    Evaluator,\n+    Event,\n+    Precision,\n+    PyTorchScheduler,\n+    State,\n+    Time,\n+    Timestamp,\n+    TimeUnit,\n+    TrainerMode,\n+    ensure_data_spec,\n+    ensure_evaluator,\n+    ensure_time,\n+    get_precision_context,\n+    validate_eval_automicrobatching,\n+)\n from composer.devices import Device, DeviceCPU, DeviceGPU, DeviceMPS, DeviceTPU\n-from composer.loggers import (ConsoleLogger, Logger, LoggerDestination, MosaicMLLogger, ProgressBarLogger,\n-                              RemoteUploaderDownloader, WandBLogger)\n-from composer.loggers.mosaicml_logger import MOSAICML_ACCESS_TOKEN_ENV_VAR, MOSAICML_PLATFORM_ENV_VAR\n+from composer.loggers import (\n+    ConsoleLogger,\n+    Logger,\n+    LoggerDestination,\n+    MosaicMLLogger,\n+    ProgressBarLogger,\n+    RemoteUploaderDownloader,\n+    WandBLogger,\n+)\n+from composer.loggers.mosaicml_logger import (\n+    MOSAICML_ACCESS_TOKEN_ENV_VAR,\n+    MOSAICML_PLATFORM_ENV_VAR,\n+)\n from composer.models import ComposerModel\n from composer.optim import ComposerScheduler, DecoupledSGDW, compile_composer_scheduler\n from composer.profiler import Profiler\n-from composer.trainer._deepspeed import _fix_batch_precision_for_deepspeed, _parse_deepspeed_config\n+from composer.trainer._deepspeed import (\n+    _fix_batch_precision_for_deepspeed,\n+    _parse_deepspeed_config,\n+)\n from composer.trainer._scale_schedule import scale_pytorch_scheduler\n from composer.trainer._scaler import ClosureGradScaler\n-from composer.trainer.dist_strategy import (DDPSyncStrategy, ddp_sync_context, prepare_ddp_module, prepare_fsdp_module,\n-                                            set_fsdp_default)\n-from composer.utils import (ExportFormat, MissingConditionalImportError, ObjectStore, Transform, checkpoint, dist,\n-                            ensure_tuple, export_with_logger, extract_hparams, format_name_with_dist,\n-                            get_composer_env_dict, get_device, get_file, is_tpu_installed, map_collection,\n-                            maybe_create_object_store_from_uri, maybe_create_remote_uploader_downloader_from_uri,\n-                            model_eval_mode, parse_uri, reproducibility, using_torch_2)\n+from composer.trainer.dist_strategy import (\n+    DDPSyncStrategy,\n+    ddp_sync_context,\n+    prepare_ddp_module,\n+    prepare_fsdp_module,\n+    set_fsdp_default,\n+)\n+from composer.utils import (\n+    ExportFormat,\n+    MissingConditionalImportError,\n+    ObjectStore,\n+    Transform,\n+    checkpoint,\n+    dist,\n+    ensure_tuple,\n+    export_with_logger,\n+    extract_hparams,\n+    format_name_with_dist,\n+    get_composer_env_dict,\n+    get_device,\n+    get_file,\n+    is_tpu_installed,\n+    map_collection,\n+    maybe_create_object_store_from_uri,\n+    maybe_create_remote_uploader_downloader_from_uri,\n+    model_eval_mode,\n+    parse_uri,\n+    reproducibility,\n+    using_torch_2,\n+)\n from composer.utils.misc import is_model_deepspeed\n \n if is_tpu_installed():\n@@ -64,17 +132,21 @@ if is_tpu_installed():\n \n log = logging.getLogger(__name__)\n \n-__all__ = ['Trainer']\n+__all__ = [\"Trainer\"]\n \n # syntax to shorten the Scheduler type annotations\n Scheduler = Union[ComposerScheduler, PyTorchScheduler]\n \n \n def _raise_missing_argument_exception(arg_name: str):\n-    raise ValueError((f'{arg_name} is a required argument and must be specified when constructing the '\n-                      f'{Trainer.__name__} or when calling {Trainer.__name__}.{Trainer.fit.__name__}(). '\n-                      f'To fix, please specify `{arg_name}` via {Trainer.__name__}({arg_name}=...) or '\n-                      f'{Trainer.__name__}.{Trainer.fit.__name__}({arg_name}=...).'))\n+    raise ValueError(\n+        (\n+            f\"{arg_name} is a required argument and must be specified when constructing the \"\n+            f\"{Trainer.__name__} or when calling {Trainer.__name__}.{Trainer.fit.__name__}(). \"\n+            f\"To fix, please specify `{arg_name}` via {Trainer.__name__}({arg_name}=...) or \"\n+            f\"{Trainer.__name__}.{Trainer.fit.__name__}({arg_name}=...).\"\n+        )\n+    )\n \n \n def _scale_max_duration_by_ssr(\n@@ -84,41 +156,61 @@ def _scale_max_duration_by_ssr(\n     if orig_max_duration is None:\n         return None\n     max_duration = cast(Time[int], orig_max_duration * scale_schedule_ratio)\n-    log.info(f'max_duration changed from {orig_max_duration} to {max_duration}')\n+    log.info(f\"max_duration changed from {orig_max_duration} to {max_duration}\")\n     if max_duration.value == 0:\n-        raise ValueError('Scale schedule has reduced the max_duration to 0. Set a higher ratio or use more epochs.')\n+        raise ValueError(\n+            \"Scale schedule has reduced the max_duration to 0. Set a higher ratio or use more epochs.\"\n+        )\n     return max_duration\n \n \n-def _get_default_scheduler_frequency(schedulers: Optional[Union[Scheduler, Sequence[Scheduler]]]):\n-    has_pytorch_scheduler = any(isinstance(scheduler, PyTorchScheduler) for scheduler in ensure_tuple(schedulers))\n+def _get_default_scheduler_frequency(\n+    schedulers: Optional[Union[Scheduler, Sequence[Scheduler]]],\n+):\n+    has_pytorch_scheduler = any(\n+        isinstance(scheduler, PyTorchScheduler)\n+        for scheduler in ensure_tuple(schedulers)\n+    )\n     if has_pytorch_scheduler:\n-        log.info(('Stepping schedulers every epoch, as a PyTorch scheduler was provided. '\n-                  'The trainer cannot automatically convert the parameters (e.g. step_size, T_max) of the '\n-                  'PyTorch scheduler to be in terms of batches. If the PyTorch scheduler should be stepped '\n-                  'every batch, set `step_schedulers_every_batch=True`.'))\n+        log.info(\n+            (\n+                \"Stepping schedulers every epoch, as a PyTorch scheduler was provided. \"\n+                \"The trainer cannot automatically convert the parameters (e.g. step_size, T_max) of the \"\n+                \"PyTorch scheduler to be in terms of batches. If the PyTorch scheduler should be stepped \"\n+                \"every batch, set `step_schedulers_every_batch=True`.\"\n+            )\n+        )\n         return TimeUnit.EPOCH\n     else:\n-        log.info(('Stepping schedulers every batch. '\n-                  'To step schedulers every epoch, set `step_schedulers_every_batch=False`.'))\n+        log.info(\n+            (\n+                \"Stepping schedulers every batch. \"\n+                \"To step schedulers every epoch, set `step_schedulers_every_batch=False`.\"\n+            )\n+        )\n         return TimeUnit.BATCH\n \n \n-def _filter_metrics(metrics: Dict[str, Metric], metric_names: Optional[List[str]]) -> Dict[str, Metric]:\n+def _filter_metrics(\n+    metrics: Dict[str, Metric], metric_names: Optional[List[str]]\n+) -> Dict[str, Metric]:\n     \"\"\"Filter the metrics based on the given metric_names as regex strings (e.g. 'Accuracy', 'f1' for 'BinaryF1Score', 'Top-.' for 'Top-1 Accuracy' and 'Top-2 Accuracy', etc). If no metric_names are provided, all metrics will be returned.\"\"\"\n     metrics = deepcopy(metrics)\n     if metric_names is None:\n         return metrics\n     filtered_metrics = {}\n     for name, metric in metrics.items():\n-        if any(re.match(f'.*{metric_name}.*', name, re.IGNORECASE) for metric_name in metric_names):\n+        if any(\n+            re.match(f\".*{metric_name}.*\", name, re.IGNORECASE)\n+            for metric_name in metric_names\n+        ):\n             filtered_metrics[name] = metric\n     return filtered_metrics\n \n \n def _validate_precision(precision: Precision, device: Device):\n     if isinstance(device, DeviceCPU) and precision != Precision.FP32:\n-        raise ValueError(f'{precision} is not supported for CPU training.')\n+        raise ValueError(f\"{precision} is not supported for CPU training.\")\n \n \n def _compile_schedulers(\n@@ -132,7 +224,9 @@ def _compile_schedulers(\n             scale_pytorch_scheduler(scheduler, scale_schedule_ratio)\n             compiled_schedulers.append(scheduler)\n         else:  # it's a composer scheduler\n-            compiled_schedulers.append(compile_composer_scheduler(scheduler, state, scale_schedule_ratio))\n+            compiled_schedulers.append(\n+                compile_composer_scheduler(scheduler, state, scale_schedule_ratio)\n+            )\n \n     return compiled_schedulers\n \n@@ -149,38 +243,49 @@ def _set_evaluator_interval_and_subset_num_batches(\n         if evaluator.eval_interval is None:\n             evaluator.eval_interval = eval_interval\n         eval_dataloader = evaluator.dataloader.dataloader\n-        if isinstance(eval_dataloader, collections.abc.Sized) and (evaluator.subset_num_batches is None or\n-                                                                   evaluator.subset_num_batches == -1):\n+        if isinstance(eval_dataloader, collections.abc.Sized) and (\n+            evaluator.subset_num_batches is None or evaluator.subset_num_batches == -1\n+        ):\n             try:\n                 dataloader_len = len(eval_dataloader)\n             except TypeError:\n                 dataloader_len = None\n             if dataloader_len == None:\n-                raise ValueError('eval_subset_num_batches must be set when using an infinite sized '\n-                                 'eval_dataloader where length is `None`. Otherwise, evaluation will '\n-                                 'run forever and never terminate.')\n-\n-\n-def _is_auto_microbatching(device_train_microbatch_size: Optional[Union[int, str]], device: Device):\n-    if device_train_microbatch_size == 'auto':\n-        warnings.warn((\"`device_train_microbatch_size='auto'` may potentially fail with unexpected \"\n-                       'CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory errors '\n-                       'and adjust the batch size, but it is possible CUDA will be put into an '\n-                       'irrecoverable state due to PyTorch bugs, e.g. integer overflow. In this case, '\n-                       'please manually set device_train_microbatch_size explicitly to an integer '\n-                       'instead.'))\n+                raise ValueError(\n+                    \"eval_subset_num_batches must be set when using an infinite sized \"\n+                    \"eval_dataloader where length is `None`. Otherwise, evaluation will \"\n+                    \"run forever and never terminate.\"\n+                )\n+\n+\n+def _is_auto_microbatching(\n+    device_train_microbatch_size: Optional[Union[int, str]], device: Device\n+):\n+    if device_train_microbatch_size == \"auto\":\n+        warnings.warn(\n+            (\n+                \"`device_train_microbatch_size='auto'` may potentially fail with unexpected \"\n+                \"CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory errors \"\n+                \"and adjust the batch size, but it is possible CUDA will be put into an \"\n+                \"irrecoverable state due to PyTorch bugs, e.g. integer overflow. In this case, \"\n+                \"please manually set device_train_microbatch_size explicitly to an integer \"\n+                \"instead.\"\n+            )\n+        )\n         if not isinstance(device, DeviceGPU):\n             raise ValueError(\n-                'Can only use adaptive device_train_microbatch_size on GPU. Please set device_train_microbatch_size >= 1.'\n+                \"Can only use adaptive device_train_microbatch_size on GPU. Please set device_train_microbatch_size >= 1.\"\n             )\n         return True\n     else:\n         return False\n \n \n-def _get_initial_device_train_microbatch_size(device_train_microbatch_size: Optional[Union[int, str]],\n-                                              auto_microbatching: bool,\n-                                              train_dataloader: Optional[Iterable]) -> Optional[int]:\n+def _get_initial_device_train_microbatch_size(\n+    device_train_microbatch_size: Optional[Union[int, str]],\n+    auto_microbatching: bool,\n+    train_dataloader: Optional[Iterable],\n+) -> Optional[int]:\n     \"\"\"Sets initial value of device_train_microbatch_size.\n \n     If auto_microbatching, sets initial `device_train_microbatch_size` to per rank batch size. If\n@@ -192,13 +297,13 @@ def _get_initial_device_train_microbatch_size(device_train_microbatch_size: Opti\n         if train_dataloader is None:\n             return None\n         try:\n-            batch_size = getattr(train_dataloader, 'batch_size')\n+            batch_size = getattr(train_dataloader, \"batch_size\")\n         except AttributeError as e:\n             # Error message when `device_train_microbatch_size` is None\n             # Note: This code path will be removed after `auto` is made default\n             if device_train_microbatch_size is None:\n                 raise ValueError(\n-                    '`device_train_microbatch_size` must be set when `state.train_dataloader` does not have a `batch_size` attribute.'\n+                    \"`device_train_microbatch_size` must be set when `state.train_dataloader` does not have a `batch_size` attribute.\"\n                 ) from e\n             # Error message when `device_train_microbatch_size` is 'auto'\n             raise AttributeError(\n@@ -213,15 +318,19 @@ def _get_initial_device_train_microbatch_size(device_train_microbatch_size: Opti\n \n def _is_cuda_oom(e: RuntimeError):\n     \"\"\"Determines if error is CUDA Out of Memory and if auto_microbatching is enabled.\"\"\"\n-    if 'CUDA out of memory' in str(e):\n+    if \"CUDA out of memory\" in str(e):\n         return True\n     # With batch_norm, large batch sizes sometimes result in cuDNN instead of Cuda OOMs.\n-    if 'cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.' in str(\n-            e):\n-        warnings.warn('Encountered \"cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in '\n-                      'a non-contiguous input.\" This can happen when the batch_size is too large for the GPU so auto '\n-                      'auto_microbatching will rerun with a smaller microbatch size value, but there may be a user '\n-                      'error with non-contiguous inputs.')\n+    if (\n+        \"cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\"\n+        in str(e)\n+    ):\n+        warnings.warn(\n+            'Encountered \"cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in '\n+            'a non-contiguous input.\" This can happen when the batch_size is too large for the GPU so auto '\n+            \"auto_microbatching will rerun with a smaller microbatch size value, but there may be a user \"\n+            \"error with non-contiguous inputs.\"\n+        )\n         return True\n     return False\n \n@@ -236,18 +345,25 @@ def _adjust_device_train_microbatch_size(state: State):\n     # if training 1 sample at a time still resulted in CUDA out of memory.\n     assert state.device_train_microbatch_size is not None\n     if state.device_train_microbatch_size == 1:\n-        raise RuntimeError(('CUDA out of memory. The train loop failed with an internal microbatch of size 1.'\n-                            'The GPU does not have enough memory to process even 1 sample during train.'))\n+        raise RuntimeError(\n+            (\n+                \"CUDA out of memory. The train loop failed with an internal microbatch of size 1.\"\n+                \"The GPU does not have enough memory to process even 1 sample during train.\"\n+            )\n+        )\n     else:\n         original_microbatch_size = state.device_train_microbatch_size\n         state.device_train_microbatch_size = max(int(original_microbatch_size / 2), 1)\n         warnings.warn(\n-            RuntimeWarning('CUDA out of memory detected. Train microbatch size will be decreased from '\n-                           f'{original_microbatch_size} -> {state.device_train_microbatch_size}.'))\n+            RuntimeWarning(\n+                \"CUDA out of memory detected. Train microbatch size will be decreased from \"\n+                f\"{original_microbatch_size} -> {state.device_train_microbatch_size}.\"\n+            )\n+        )\n     # Clear gradients in case failure happened during backwards pass\n-    if hasattr(state, 'outputs'):\n+    if hasattr(state, \"outputs\"):\n         del state.outputs\n-    if hasattr(state, 'loss'):\n+    if hasattr(state, \"loss\"):\n         del state.loss\n     for optimizer in state.optimizers:\n         optimizer.zero_grad(set_to_none=True)\n@@ -266,14 +382,23 @@ def _adjust_device_eval_microbatch_size(evaluator: Evaluator):\n     # if evaluating 1 sample at a time still resulted in CUDA out of memory.\n     assert evaluator.device_eval_microbatch_size is not None\n     if evaluator.device_eval_microbatch_size == 1:\n-        raise RuntimeError(('CUDA out of memory. The eval loop failed with an internal microbatch of size 1.'\n-                            'The GPU does not have enough memory to process even 1 sample during eval.'))\n+        raise RuntimeError(\n+            (\n+                \"CUDA out of memory. The eval loop failed with an internal microbatch of size 1.\"\n+                \"The GPU does not have enough memory to process even 1 sample during eval.\"\n+            )\n+        )\n     else:\n         original_microbatch_size = evaluator.device_eval_microbatch_size\n-        evaluator.device_eval_microbatch_size = max(int(original_microbatch_size / 2), 1)\n+        evaluator.device_eval_microbatch_size = max(\n+            int(original_microbatch_size / 2), 1\n+        )\n         warnings.warn(\n-            RuntimeWarning('CUDA out of memory detected. Train microbatch size will be decreased from '\n-                           f'{original_microbatch_size} -> {evaluator.device_eval_microbatch_size}.'))\n+            RuntimeWarning(\n+                \"CUDA out of memory detected. Train microbatch size will be decreased from \"\n+                f\"{original_microbatch_size} -> {evaluator.device_eval_microbatch_size}.\"\n+            )\n+        )\n     torch.cuda.empty_cache()\n \n \n@@ -286,7 +411,7 @@ def _distribute_and_get_random_seed(seed: Optional[int], device: Device):\n     # to restore seeds when resuming form checkpoints, since only the\n     # `rank_zero_seed` is stored on state.\n     if seed < 0 or seed > reproducibility.MAX_SEED:\n-        raise ValueError(f'Invalid seed: {seed}. It must be on [0; 2**32 - 1)')\n+        raise ValueError(f\"Invalid seed: {seed}. It must be on [0; 2**32 - 1)\")\n \n     # using int64 to prevent overflow\n     rank_zero_seed = device.tensor_to_device(torch.tensor([seed], dtype=torch.int64))\n@@ -298,7 +423,10 @@ def _distribute_and_get_random_seed(seed: Optional[int], device: Device):\n     return rank_zero_seed, seed\n \n \n-def _get_ddp_sync_strategy(ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]], find_unused_parameters: bool):\n+def _get_ddp_sync_strategy(\n+    ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]],\n+    find_unused_parameters: bool,\n+):\n     if ddp_sync_strategy is None:\n         if find_unused_parameters:\n             ddp_sync_strategy = DDPSyncStrategy.MULTI_AUTO_SYNC\n@@ -309,7 +437,11 @@ def _get_ddp_sync_strategy(ddp_sync_strategy: Optional[Union[str, DDPSyncStrateg\n     return ddp_sync_strategy\n \n \n-def _get_precision_context(precision: Precision, precision_config: Optional[Dict[str, Any]], deepspeed_enabled: bool):\n+def _get_precision_context(\n+    precision: Precision,\n+    precision_config: Optional[Dict[str, Any]],\n+    deepspeed_enabled: bool,\n+):\n     if deepspeed_enabled:\n         return contextlib.nullcontext()\n     return get_precision_context(precision, precision_config)\n@@ -319,7 +451,7 @@ def _generate_run_name() -> str:\n     # change coolname randomness for different names with same seed\n     coolname.replace_random(random.Random(os.urandom(128)))\n     # prefixing with the time so experiments sorted alphabetically will have the latest experiment last\n-    generated_run_name = str(int(time.time())) + '-' + coolname.generate_slug(2)\n+    generated_run_name = str(int(time.time())) + \"-\" + coolname.generate_slug(2)\n     run_name_list = [generated_run_name]\n     # ensure all ranks have the same experiment name\n     dist.broadcast_object_list(run_name_list)\n@@ -806,46 +938,50 @@ class Trainer:\n         *,\n         # The Model\n         model: ComposerModel,\n-\n         # Train Dataloader\n         train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,\n-        train_dataloader_label: str = 'train',\n+        train_dataloader_label: str = \"train\",\n         train_subset_num_batches: int = -1,\n         spin_dataloaders: bool = True,\n-\n         # Stopping Condition\n         max_duration: Optional[Union[int, str, Time]] = None,\n-\n         # Algorithms\n         algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,\n-\n         # Engine Pass Registration\n-        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],\n-                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,\n-\n+        algorithm_passes: Optional[\n+            Union[\n+                AlgorithmPass,\n+                Tuple[AlgorithmPass, int],\n+                Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]],\n+            ]\n+        ] = None,\n         # Optimizers and Scheduling\n         optimizers: Optional[torch.optim.Optimizer] = None,\n-        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,\n-                                                                                       PyTorchScheduler]]]] = None,\n+        schedulers: Optional[\n+            Union[\n+                ComposerScheduler,\n+                PyTorchScheduler,\n+                Sequence[Union[ComposerScheduler, PyTorchScheduler]],\n+            ]\n+        ] = None,\n         scale_schedule_ratio: float = 1.0,\n         step_schedulers_every_batch: Optional[bool] = None,\n-\n         # Evaluators\n-        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,\n+        eval_dataloader: Optional[\n+            Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]\n+        ] = None,\n         eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,\n         eval_subset_num_batches: int = -1,\n-\n         # Callbacks and Logging\n         callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,\n         loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,\n         run_name: Optional[str] = None,\n         progress_bar: bool = True,\n         log_to_console: bool = False,\n-        console_stream: Union[str, TextIO] = 'stderr',\n-        console_log_interval: Union[int, str, Time] = '1ba',\n+        console_stream: Union[str, TextIO] = \"stderr\",\n+        console_log_interval: Union[int, str, Time] = \"1ba\",\n         log_traces: bool = False,\n         auto_log_hparams: bool = False,\n-\n         # Load Checkpoint\n         load_path: Optional[str] = None,\n         load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,\n@@ -854,49 +990,39 @@ class Trainer:\n         load_progress_bar: bool = True,\n         load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,\n         load_exclude_algorithms: Optional[List[str]] = None,\n-\n         # Save Checkpoint\n         save_folder: Optional[str] = None,\n-        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',\n-        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',\n+        save_filename: str = \"ep{epoch}-ba{batch}-rank{rank}.pt\",\n+        save_latest_filename: Optional[str] = \"latest-rank{rank}.pt\",\n         save_overwrite: bool = False,\n-        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',\n+        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = \"1ep\",\n         save_weights_only: bool = False,\n         save_num_checkpoints_to_keep: int = -1,\n         save_metrics: bool = False,\n-\n         # Graceful Resumption\n         autoresume: bool = False,\n-\n         # DeepSpeed\n         deepspeed_config: Optional[Dict[str, Any]] = None,\n         fsdp_config: Optional[Dict[str, Any]] = None,\n         fsdp_auto_wrap: bool = True,\n-\n         # System/Numerics\n         device: Optional[Union[str, Device]] = None,\n         precision: Optional[Union[str, Precision]] = None,\n         precision_config: Optional[Dict[str, Any]] = None,\n         device_train_microbatch_size: Optional[Union[int, str]] = None,\n-\n         # Reproducibility\n         seed: Optional[int] = None,\n         deterministic_mode: bool = False,\n-\n         # Distributed Training\n         dist_timeout: float = 300.0,\n         ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,\n-\n         # Profiling\n         profiler: Optional[Profiler] = None,\n-\n         # Python logging\n         python_log_level: Optional[str] = None,\n-\n         # compile config for PyTorch 2.0 or higher\n         compile_config: Optional[Dict[str, Any]] = None,\n     ):\n-\n         self.auto_log_hparams = auto_log_hparams\n         self.python_log_level = python_log_level\n         if self.python_log_level is not None:\n@@ -905,10 +1031,9 @@ class Trainer:\n                 # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32\n                 # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background\n                 # threads / processes\n-                format=\n-                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'\n+                format=f\"%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s\"\n             )\n-            logging.getLogger('composer').setLevel(self.python_log_level.upper())\n+            logging.getLogger(\"composer\").setLevel(self.python_log_level.upper())\n \n         algorithms = list(ensure_tuple(algorithms))\n \n@@ -917,7 +1042,9 @@ class Trainer:\n \n         # Precision\n         if precision is None:\n-            precision = Precision.AMP_FP16 if isinstance(device, DeviceGPU) else Precision.FP32\n+            precision = (\n+                Precision.AMP_FP16 if isinstance(device, DeviceGPU) else Precision.FP32\n+            )\n         elif isinstance(precision, str):\n             precision = Precision(precision)\n         _validate_precision(precision, device)\n@@ -927,41 +1054,54 @@ class Trainer:\n         is_model_compiled = False\n         if is_torch_2_0:\n             from torch._dynamo import OptimizedModule\n+\n             if isinstance(model, OptimizedModule):\n-                log.warning(f'Provided `model` is already compiled with `torch.compile`. Ignoring ' +\n-                            f'parameter `compile_config` if provided. If you would like `Trainer` ' +\n-                            f'to takes care of model compilation, provide a not-compiled model and ' +\n-                            f'`compile_config` parameter.')\n+                log.warning(\n+                    f\"Provided `model` is already compiled with `torch.compile`. Ignoring \"\n+                    + f\"parameter `compile_config` if provided. If you would like `Trainer` \"\n+                    + f\"to takes care of model compilation, provide a not-compiled model and \"\n+                    + f\"`compile_config` parameter.\"\n+                )\n                 # The `torch.compile` function returns an object of type `torch._dynamo.OptimizedModule`\n                 # which wraps the original `nn.Module` object and later patches its forward method to\n                 # optimized `self.forward` method.\n                 is_model_compiled = True\n                 compiled_model = model._orig_mod\n                 if not isinstance(compiled_model, ComposerModel):\n-                    raise ValueError(f'Provided `model` must be a subclass of ComposerModel. ' +\n-                                     f'Instead found as type `{type(compiled_model)}`')\n-                compiled_model.forward = model.dynamo_ctx(\n-                    compiled_model.forward)  # pyright: ignore [reportGeneralTypeIssues]\n+                    raise ValueError(\n+                        f\"Provided `model` must be a subclass of ComposerModel. \"\n+                        + f\"Instead found as type `{type(compiled_model)}`\"\n+                    )\n+                compiled_model.forward = model.dynamo_ctx(compiled_model.forward)  # pyright: ignore [reportGeneralTypeIssues]\n                 model = compiled_model\n \n         # Microbatching\n-        auto_microbatching = _is_auto_microbatching(device_train_microbatch_size, device=device)\n+        auto_microbatching = _is_auto_microbatching(\n+            device_train_microbatch_size, device=device\n+        )\n         if auto_microbatching and profiler:\n-            raise ValueError(\"`device_train_microbatch_size='auto'` is not compatible with the profiler. It is \"\n-                             \"recommended to run a mini-run with `device_train_microbatch_size='auto'` to identify \"\n-                             'the optimal device_train_microbatch_size value and then manually specify that in a '\n-                             'second run with profiler.')\n+            raise ValueError(\n+                \"`device_train_microbatch_size='auto'` is not compatible with the profiler. It is \"\n+                \"recommended to run a mini-run with `device_train_microbatch_size='auto'` to identify \"\n+                \"the optimal device_train_microbatch_size value and then manually specify that in a \"\n+                \"second run with profiler.\"\n+            )\n         self.first_batch_complete = False\n         # If auto_microbatching is True or `device_train_microbatch_size` is not specified, the microbatch size\n         # will be determined when dataloader is specified. train_dataloader is parsed after `Event.INIT` or in\n         # fit()\n-        device_train_microbatch_size = _get_initial_device_train_microbatch_size(device_train_microbatch_size,\n-                                                                                 auto_microbatching, None)\n+        device_train_microbatch_size = _get_initial_device_train_microbatch_size(\n+            device_train_microbatch_size, auto_microbatching, None\n+        )\n \n         assert not isinstance(device_train_microbatch_size, str)\n \n         # Distributed\n-        if deepspeed_config is not None or fsdp_config is not None or dist.get_world_size() > 1:\n+        if (\n+            deepspeed_config is not None\n+            or fsdp_config is not None\n+            or dist.get_world_size() > 1\n+        ):\n             # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1\n             # And torch.distributed is always required for multi-rank training\n             dist.initialize_dist(device, dist_timeout)\n@@ -979,12 +1119,16 @@ class Trainer:\n             try:\n                 optimizers = DecoupledSGDW(model.parameters(), lr=0.1)\n                 # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning\n-                warnings.warn(('No optimizer was specified. Defaulting to '\n-                               f\"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})\"))\n+                warnings.warn(\n+                    (\n+                        \"No optimizer was specified. Defaulting to \"\n+                        f\"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})\"\n+                    )\n+                )\n             except ValueError as e:\n-                if 'optimizer got an empty parameter list' in str(e):\n+                if \"optimizer got an empty parameter list\" in str(e):\n                     warnings.warn(\n-                        'No optimizer was specified, and the model does not have parameters. Skipping auto-creating optimizer.'\n+                        \"No optimizer was specified, and the model does not have parameters. Skipping auto-creating optimizer.\"\n                     )\n                 else:\n                     raise\n@@ -992,14 +1136,19 @@ class Trainer:\n         if optimizers is not None:\n             num_optimizers = len(ensure_tuple(optimizers))\n             if num_optimizers != 1:\n-                raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')\n+                raise NotImplementedError(\n+                    f\"Only one optimizer is supported; found {num_optimizers} optimizers\"\n+                )\n \n         # Move the model and optimizers to the device\n         if deepspeed_config is None and fsdp_config is None:\n             # check if model is already on tpu\n-            if isinstance(device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):\n+            if isinstance(device, DeviceTPU) and \"xla\" not in str(\n+                next(model.parameters()).device\n+            ):\n                 raise ValueError(\n-                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')\n+                    \"Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.\"\n+                )\n             else:\n                 model = device.module_to_device(model)\n                 # Move any remaining optimizer parameters onto the device\n@@ -1008,13 +1157,17 @@ class Trainer:\n             optimizers = map_collection(optimizers, device.optimizer_to_device)\n \n         # Run Name\n-        run_name = os.getenv('COMPOSER_RUN_NAME', None) if run_name is None else run_name\n-        run_name = os.getenv('RUN_NAME', None) if run_name is None else run_name\n+        run_name = (\n+            os.getenv(\"COMPOSER_RUN_NAME\", None) if run_name is None else run_name\n+        )\n+        run_name = os.getenv(\"RUN_NAME\", None) if run_name is None else run_name\n         if run_name is None:\n             if autoresume:\n-                raise ValueError('When autoresume=True, the `run_name` must be specified.')\n+                raise ValueError(\n+                    \"When autoresume=True, the `run_name` must be specified.\"\n+                )\n             run_name = _generate_run_name()\n-        log.info('Run name: %s', run_name)\n+        log.info(\"Run name: %s\", run_name)\n \n         # Create the State\n         self.state = State(\n@@ -1031,7 +1184,9 @@ class Trainer:\n             run_name=run_name,\n             save_metrics=save_metrics,\n             deepspeed_config=deepspeed_config,\n-            fsdp_config=set_fsdp_default(fsdp_config) if fsdp_config is not None else None,\n+            fsdp_config=set_fsdp_default(fsdp_config)\n+            if fsdp_config is not None\n+            else None,\n             fsdp_auto_wrap=fsdp_auto_wrap,\n         )\n \n@@ -1040,54 +1195,80 @@ class Trainer:\n \n         # Profiler\n         if profiler is not None:\n-            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')\n+            warnings.warn(\n+                \"The profiler is enabled. Using the profiler adds additional overhead when training.\"\n+            )\n             self.state.profiler = profiler\n             for remote_uri in profiler.remote_filenames:\n-                remote_ud = maybe_create_remote_uploader_downloader_from_uri(uri=remote_uri, loggers=loggers)\n+                remote_ud = maybe_create_remote_uploader_downloader_from_uri(\n+                    uri=remote_uri, loggers=loggers\n+                )\n                 if remote_ud is not None:\n                     loggers.append(remote_ud)\n             self.state.profiler.bind_to_state(self.state)\n \n         if progress_bar and log_to_console:\n             warnings.warn(\n-                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'\n-                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'\n+                \"Setting both `progress_bar` and `log_to_console` both to True is not recommended and will\"\n+                \"lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.\"\n             )\n \n         if any(isinstance(x, ProgressBarLogger) for x in loggers):\n             warnings.warn(\n-                Warning((\n-                    f'Specifying the {ProgressBarLogger.__name__} via `loggers` is not recommended as '\n-                    'any values set for the following Trainer arguments will be ignored: `progress_bar`, `console_stream`, or `log_traces`. '\n-                    'The recommended way of enabling a progress bar is to set `progress_bar` to True instead of '\n-                    f'constructing a {ProgressBarLogger.__name__} instance.')))\n+                Warning(\n+                    (\n+                        f\"Specifying the {ProgressBarLogger.__name__} via `loggers` is not recommended as \"\n+                        \"any values set for the following Trainer arguments will be ignored: `progress_bar`, `console_stream`, or `log_traces`. \"\n+                        \"The recommended way of enabling a progress bar is to set `progress_bar` to True instead of \"\n+                        f\"constructing a {ProgressBarLogger.__name__} instance.\"\n+                    )\n+                )\n+            )\n         else:\n             if progress_bar:\n-                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))\n+                loggers.append(\n+                    ProgressBarLogger(stream=console_stream, log_traces=log_traces)\n+                )\n \n         # Console Logging\n         if any(isinstance(x, ConsoleLogger) for x in loggers):\n             warnings.warn(\n-                Warning((\n-                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is not recommended as '\n-                    'any values set for the following Trainer arguments will be ignored: `log_to_console`, `console_stream`, `log_traces`, and `console_log_interval`. '\n-                    'The recommended way of enabling a console logging is to set `log_to_console` to True instead of '\n-                    f'constructing a {ConsoleLogger.__name__} instance.')))\n+                Warning(\n+                    (\n+                        f\"Specifying the {ConsoleLogger.__name__} via `loggers` is not recommended as \"\n+                        \"any values set for the following Trainer arguments will be ignored: `log_to_console`, `console_stream`, `log_traces`, and `console_log_interval`. \"\n+                        \"The recommended way of enabling a console logging is to set `log_to_console` to True instead of \"\n+                        f\"constructing a {ConsoleLogger.__name__} instance.\"\n+                    )\n+                )\n+            )\n         else:\n             if log_to_console:\n                 loggers.append(\n-                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))\n+                    ConsoleLogger(\n+                        stream=console_stream,\n+                        log_interval=console_log_interval,\n+                        log_traces=log_traces,\n+                    )\n+                )\n \n         # MosaicML Logger\n         # Keep MosaicML logger above the RemoteUploaderDownloader so that fit end is reported before the final checkpoint begins uploading\n-        if os.environ.get(MOSAICML_PLATFORM_ENV_VAR, 'false').lower() == 'true' and os.environ.get(\n-                MOSAICML_ACCESS_TOKEN_ENV_VAR) is not None and not any(isinstance(x, MosaicMLLogger) for x in loggers):\n-            log.info('Detected run on MosaicML platform. Adding MosaicMLLogger to loggers.')\n+        if (\n+            os.environ.get(MOSAICML_PLATFORM_ENV_VAR, \"false\").lower() == \"true\"\n+            and os.environ.get(MOSAICML_ACCESS_TOKEN_ENV_VAR) is not None\n+            and not any(isinstance(x, MosaicMLLogger) for x in loggers)\n+        ):\n+            log.info(\n+                \"Detected run on MosaicML platform. Adding MosaicMLLogger to loggers.\"\n+            )\n             mosaicml_logger = MosaicMLLogger()\n             loggers.append(mosaicml_logger)\n \n         if save_folder is not None:\n-            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)\n+            remote_ud = maybe_create_remote_uploader_downloader_from_uri(\n+                save_folder, loggers\n+            )\n             if remote_ud is not None:\n                 loggers.append(remote_ud)\n \n@@ -1096,17 +1277,20 @@ class Trainer:\n \n         if save_latest_filename is not None:\n             remote_ud_has_format_string = [\n-                isinstance(logger_destination, RemoteUploaderDownloader) and\n-                logger_destination.file_path_format_string != '{remote_file_name}'\n+                isinstance(logger_destination, RemoteUploaderDownloader)\n+                and logger_destination.file_path_format_string != \"{remote_file_name}\"\n                 for logger_destination in self.logger.destinations\n             ]\n             if any(remote_ud_has_format_string):\n                 raise ValueError(\n-                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '\n-                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')\n+                    \"Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. \"\n+                    \"Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`\"\n+                )\n \n         # Callbacks\n-        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks\n+        self.state.callbacks[:] = (\n+            list(cast(List[Callback], loggers)) + self.state.callbacks\n+        )\n \n         # Checkpoint Saving\n         self._checkpoint_saver = None\n@@ -1114,7 +1298,7 @@ class Trainer:\n         if save_folder is not None:\n             if save_weights_only:\n                 log.info(\n-                    'save_weights_only=True now also saves metadata and integrations! Please adjust your workflow accordingly.'\n+                    \"save_weights_only=True now also saves metadata and integrations! Please adjust your workflow accordingly.\"\n                 )\n \n             _, _, parsed_save_folder = parse_uri(save_folder)\n@@ -1122,8 +1306,8 @@ class Trainer:\n             # If user passes a URI with s3:// and a bucket_name, but no other\n             # path then we assume they just want their checkpoints saved directly in their\n             # bucket.\n-            if parsed_save_folder == '':\n-                folder = '.'\n+            if parsed_save_folder == \"\":\n+                folder = \".\"\n                 remote_file_name = save_filename\n                 latest_remote_file_name = save_latest_filename\n \n@@ -1133,7 +1317,9 @@ class Trainer:\n                 folder = parsed_save_folder\n                 remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))\n                 if save_latest_filename is not None:\n-                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))\n+                    latest_remote_file_name = str(\n+                        Path(parsed_save_folder) / Path(save_latest_filename)\n+                    )\n                 else:\n                     latest_remote_file_name = None\n \n@@ -1151,7 +1337,9 @@ class Trainer:\n             self.state.callbacks.append(self._checkpoint_saver)\n \n         # The Engine\n-        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)\n+        self.engine = Engine(\n+            state=self.state, logger=self.logger, algorithm_passes=algorithm_passes\n+        )\n \n         # Set the logger\n         self.state.model.logger = self.logger\n@@ -1166,19 +1354,27 @@ class Trainer:\n \n         # Log composer version\n         composer_env_dict = get_composer_env_dict()\n-        self.logger.log_hyperparameters({'composer_version': composer_env_dict['composer_version']})\n-        self.logger.log_hyperparameters({'composer_commit_hash': str(composer_env_dict['composer_commit_hash'])})\n+        self.logger.log_hyperparameters(\n+            {\"composer_version\": composer_env_dict[\"composer_version\"]}\n+        )\n+        self.logger.log_hyperparameters(\n+            {\"composer_commit_hash\": str(composer_env_dict[\"composer_commit_hash\"])}\n+        )\n \n         # Log gpus and nodes.\n-        device_name = self.state.device.__class__.__name__.lstrip('Device').lower()\n-        self.logger.log_hyperparameters({\n-            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),\n-            f'num_{device_name}s_per_node': dist.get_local_world_size(),\n-            'node_name': os.environ.get('NODENAME', 'unknown because NODENAME environment variable not set')\n-        })\n+        device_name = self.state.device.__class__.__name__.lstrip(\"Device\").lower()\n+        self.logger.log_hyperparameters(\n+            {\n+                \"num_nodes\": int(dist.get_world_size() / dist.get_local_world_size()),\n+                f\"num_{device_name}s_per_node\": dist.get_local_world_size(),\n+                \"node_name\": os.environ.get(\n+                    \"NODENAME\", \"unknown because NODENAME environment variable not set\"\n+                ),\n+            }\n+        )\n \n         if not isinstance(self.state.model, ComposerModel):\n-            raise ValueError('Provided model must be a subclass of ComposerModel.')\n+            raise ValueError(\"Provided model must be a subclass of ComposerModel.\")\n \n         # After running Event.INIT, then set the \"optional\" elements of state that could be passed in on FIT instead of INIT\n         # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT\n@@ -1195,18 +1391,25 @@ class Trainer:\n             model_metric_names = [str(k) for k in eval_metrics.keys()]\n             eval_dataloader = ensure_tuple(eval_dataloader)\n \n-            evaluator_types = [isinstance(evaluator, Evaluator) for evaluator in eval_dataloader]\n+            evaluator_types = [\n+                isinstance(evaluator, Evaluator) for evaluator in eval_dataloader\n+            ]\n             if any(evaluator_types) and not all(evaluator_types):\n-                raise ValueError('Mixing Evaluator with other classes is not allowed, please wrap'\n-                                 'all other classes with the Evaluator class. These are the classes'\n-                                 'that were detected:' + str([type(evaluator) for evaluator in eval_dataloader]))\n+                raise ValueError(\n+                    \"Mixing Evaluator with other classes is not allowed, please wrap\"\n+                    \"all other classes with the Evaluator class. These are the classes\"\n+                    \"that were detected:\"\n+                    + str([type(evaluator) for evaluator in eval_dataloader])\n+                )\n \n             evaluators = [\n-                ensure_evaluator(evaluator, default_metric_names=model_metric_names) for evaluator in eval_dataloader\n+                ensure_evaluator(evaluator, default_metric_names=model_metric_names)\n+                for evaluator in eval_dataloader\n             ]\n             # match metric names to model metrics\n             self.state.eval_metrics = {\n-                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators\n+                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names)\n+                for evaluator in evaluators\n             }\n \n             _set_evaluator_interval_and_subset_num_batches(\n@@ -1216,75 +1419,116 @@ class Trainer:\n             )\n \n             for evaluator in evaluators:\n-                validate_eval_automicrobatching(evaluator.auto_microbatching, self.state.device)\n+                validate_eval_automicrobatching(\n+                    evaluator.auto_microbatching, self.state.device\n+                )\n         if len(evaluators) == 0:\n             if eval_subset_num_batches != -1:\n                 raise ValueError(\n-                    f'Specifying `eval_subset_num_batches={eval_subset_num_batches}` without an `eval_dataloader` '\n-                    'has no effect. If trying to run an evaluator, make sure `eval_dataloader` is specified. '\n-                    'Otherwise, set `eval_subset_num_batches` to default value -1.')\n+                    f\"Specifying `eval_subset_num_batches={eval_subset_num_batches}` without an `eval_dataloader` \"\n+                    \"has no effect. If trying to run an evaluator, make sure `eval_dataloader` is specified. \"\n+                    \"Otherwise, set `eval_subset_num_batches` to default value -1.\"\n+                )\n             if eval_interval != 0 and eval_interval != 1:\n                 raise ValueError(\n-                    f'Specifying `eval_interval={eval_interval}` without an `eval_dataloader` has no effect. '\n-                    'If trying to run an evaluator, make sure `eval_dataloader` is specified. Otherwise, '\n-                    'set `eval_interval` to 0 or default value 1.')\n+                    f\"Specifying `eval_interval={eval_interval}` without an `eval_dataloader` has no effect. \"\n+                    \"If trying to run an evaluator, make sure `eval_dataloader` is specified. Otherwise, \"\n+                    \"set `eval_interval` to 0 or default value 1.\"\n+                )\n \n         self.state.evaluators = evaluators\n \n         # Train Dataloader\n-        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)\n+        self._train_data_spec = (\n+            None if train_dataloader is None else ensure_data_spec(train_dataloader)\n+        )\n         if self._train_data_spec is not None:\n-            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,\n-                                      train_subset_num_batches)\n+            self.state.set_dataloader(\n+                self._train_data_spec.dataloader,\n+                train_dataloader_label,\n+                train_subset_num_batches,\n+            )\n             if isinstance(self.state.device, DeviceTPU):\n-                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())\n+                self.state.train_dataloader = pl.MpDeviceLoader(\n+                    self.state.dataloader, xm.xla_device()\n+                )\n             else:\n                 self.state.train_dataloader = self.state.dataloader\n-            self.state.device_train_microbatch_size = _get_initial_device_train_microbatch_size(\n-                self.state.device_train_microbatch_size, self.state.auto_microbatching, self.state.train_dataloader)\n+            self.state.device_train_microbatch_size = (\n+                _get_initial_device_train_microbatch_size(\n+                    self.state.device_train_microbatch_size,\n+                    self.state.auto_microbatching,\n+                    self.state.train_dataloader,\n+                )\n+            )\n         self.spin_dataloaders = spin_dataloaders\n \n         # Max Duration\n         if max_duration is not None:\n             self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)\n \n-        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})\n+        self.logger.log_hyperparameters({\"rank_zero_seed\": rank_zero_seed})\n \n         # Schedulers\n-        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)\n+        self.state.schedulers = _compile_schedulers(\n+            schedulers, self.state, scale_schedule_ratio\n+        )\n         if scale_schedule_ratio != 1.0:\n             if len(self.state.schedulers) == 0:\n-                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')\n-            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)\n+                raise ValueError(\n+                    \"Specifying `scale_schedule_ratio` without `schedulers` has no effect.\"\n+                )\n+            self.state.max_duration = _scale_max_duration_by_ssr(\n+                scale_schedule_ratio, self.state.max_duration\n+            )\n \n         if step_schedulers_every_batch is None:\n-            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)\n+            self._scheduler_step_frequency = _get_default_scheduler_frequency(\n+                schedulers\n+            )\n         else:\n-            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH\n+            self._scheduler_step_frequency = (\n+                TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH\n+            )\n \n         # Some algorithms require specific settings\n-        self._backwards_create_graph = any((x.backwards_create_graph for x in self.state.algorithms))\n-        self._find_unused_parameters = any((x.find_unused_parameters for x in self.state.algorithms))\n-        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)\n+        self._backwards_create_graph = any(\n+            (x.backwards_create_graph for x in self.state.algorithms)\n+        )\n+        self._find_unused_parameters = any(\n+            (x.find_unused_parameters for x in self.state.algorithms)\n+        )\n+        self._ddp_sync_strategy = _get_ddp_sync_strategy(\n+            ddp_sync_strategy, self._find_unused_parameters\n+        )\n \n         # Suppressing GradScaler warnings as they are always created\n         # self._use_grad_scaling() will raise a RuntimeError if grad scaling is not available when it is required\n-        warnings.filterwarnings(action='ignore', message='torch.cuda.amp.GradScaler')\n-        self.state.scaler = ClosureGradScaler() if self._use_closures() else GradScaler()\n+        warnings.filterwarnings(action=\"ignore\", message=\"torch.cuda.amp.GradScaler\")\n+        self.state.scaler = (\n+            ClosureGradScaler() if self._use_closures() else GradScaler()\n+        )\n \n         if self.state.fsdp_config is not None:\n-            if version.parse(torch.__version__) < version.parse('1.13.0'):\n-                raise RuntimeError('To use FSDP with Composer, you must use torch>=1.13.0.')\n+            if version.parse(torch.__version__) < version.parse(\"1.13.0\"):\n+                raise RuntimeError(\n+                    \"To use FSDP with Composer, you must use torch>=1.13.0.\"\n+                )\n             from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler\n \n             # This state should never be reached, but we raise a ValueError just in case\n             if self._use_closures() and self.state.precision == Precision.AMP_FP16:\n-                raise ValueError(f'Using closures and precision {self.state.precision} is not supported'\n-                                 f' with FSDP. Please use another optimizer or precision type.')\n+                raise ValueError(\n+                    f\"Using closures and precision {self.state.precision} is not supported\"\n+                    f\" with FSDP. Please use another optimizer or precision type.\"\n+                )\n             self.state.scaler = ShardedGradScaler()\n \n         # suppressing FSDP warning when auto grad accum exits the forward pass before completing\n-        warnings.filterwarnings(action='ignore', message='Forward order differs from that of the first iteration')\n+        warnings.filterwarnings(\n+            action=\"ignore\",\n+            message=\"Forward order differs from that of the first iteration\",\n+        )\n \n         # If using DDP or DeepSpeed, we need to wrap the ComposerModel but store a reference to the\n         # original model for functions like `eval_forward`, `get_metrics`, etc.\n@@ -1296,31 +1540,48 @@ class Trainer:\n         # checkpoint on rank 0 only, in which case the model be loaded before it is wrapped.\n \n         # FSDP wrap if not using monolith checkpoint on rank 0 only\n-        if self.state.fsdp_config is not None and fsdp_auto_wrap and not self.state.load_fsdp_monolith_rank0_only:\n-            prepare_fsdp_module(model, optimizers, self.state.fsdp_config, precision, device, auto_microbatching)\n+        if (\n+            self.state.fsdp_config is not None\n+            and fsdp_auto_wrap\n+            and not self.state.load_fsdp_monolith_rank0_only\n+        ):\n+            prepare_fsdp_module(\n+                model,\n+                optimizers,\n+                self.state.fsdp_config,\n+                precision,\n+                device,\n+                auto_microbatching,\n+            )\n \n         # Configure Deepspeed\n         if self.state.deepspeed_config is not None:\n             for callback in self.state.callbacks:\n                 if isinstance(callback, OptimizerMonitor):\n-                    raise ValueError('OptimizerMonitor is not supported with DeepSpeed because DeepSpeed clears '\n-                                     'the gradients before in the last call to .backward see: '\n-                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')\n+                    raise ValueError(\n+                        \"OptimizerMonitor is not supported with DeepSpeed because DeepSpeed clears \"\n+                        \"the gradients before in the last call to .backward see: \"\n+                        \"https://github.com/microsoft/DeepSpeed/issues/2329 for more details.\"\n+                    )\n \n             try:\n                 import deepspeed\n             except ImportError as e:\n                 raise MissingConditionalImportError(\n-                    extra_deps_group='deepspeed',\n-                    conda_package='deepspeed>=0.5.5',\n+                    extra_deps_group=\"deepspeed\",\n+                    conda_package=\"deepspeed>=0.5.5\",\n                     conda_channel=None,\n                 ) from e\n-            self.state.deepspeed_config = _parse_deepspeed_config(self.state.deepspeed_config, state=self.state)\n+            self.state.deepspeed_config = _parse_deepspeed_config(\n+                self.state.deepspeed_config, state=self.state\n+            )\n             optimizer = ensure_tuple(self.state.optimizers)[0]\n-            log.debug('Initializing deepspeed')\n-            (self.state.model, self.state.optimizers, _, _) = deepspeed.initialize(config=self.state.deepspeed_config,\n-                                                                                   model=self.state.model,\n-                                                                                   optimizer=optimizer)\n+            log.debug(\"Initializing deepspeed\")\n+            (self.state.model, self.state.optimizers, _, _) = deepspeed.initialize(\n+                config=self.state.deepspeed_config,\n+                model=self.state.model,\n+                optimizer=optimizer,\n+            )\n             # Since the DeepSpeed ZeRO optimizer does not inherit torch.optim.Optimizer, the schedulers must be\n             # compiled and bound BEFORE DeepSpeed initialization. However, this is OK, as the the DeepSpeed Zero\n             # optimizer uses the same underlying parameter groups as the original optimizer. See\n@@ -1328,69 +1589,96 @@ class Trainer:\n             # * https://github.com/microsoft/DeepSpeed/blob/ef17c89570ceae5b26a5f886e9d8cd0941afc0ac/deepspeed/runtime/zero/stage3.py#L2532-L2538\n             # In addition, the deepspeed engine is responsible for serializing the model and optimizer state,\n             # so these attributes should not be serialized with the composer state.\n-            if 'model' in self.state.serialized_attributes:\n-                self.state.serialized_attributes.remove('model')\n+            if \"model\" in self.state.serialized_attributes:\n+                self.state.serialized_attributes.remove(\"model\")\n \n-            if 'optimizers' in self.state.serialized_attributes:\n-                self.state.serialized_attributes.remove('optimizers')\n+            if \"optimizers\" in self.state.serialized_attributes:\n+                self.state.serialized_attributes.remove(\"optimizers\")\n \n         # Load Checkpoint\n         self._rng_state = None\n         # If autoresume is enabled, first check for existing checkpoints to load\n         if autoresume:\n-            log.info('Searching for a previous checkpoint to autoresume')\n-            error_message = ''\n+            log.info(\"Searching for a previous checkpoint to autoresume\")\n+            error_message = \"\"\n             if save_folder is None:\n-                error_message += 'The `save_folder` must be specified when autoresume is enabled. '\n+                error_message += (\n+                    \"The `save_folder` must be specified when autoresume is enabled. \"\n+                )\n             if save_overwrite:\n                 error_message += textwrap.dedent(\n-                    'The flag `save_overwrite` must be False when autoresume is enabled as autoresume always loads the '\n-                    'latest existing checkpoint in `save_folder`. ')\n+                    \"The flag `save_overwrite` must be False when autoresume is enabled as autoresume always loads the \"\n+                    \"latest existing checkpoint in `save_folder`. \"\n+                )\n             if save_latest_filename is None:\n-                error_message += 'The `save_latest_filename` must be specified so autoresume knows where to load checkpoints from. '\n+                error_message += \"The `save_latest_filename` must be specified so autoresume knows where to load checkpoints from. \"\n             if run_name is None:\n-                error_message += 'The `run_name` must be specified when using autoresume so Event.INIT is run with the correct run name. '\n-            if error_message != '':\n+                error_message += \"The `run_name` must be specified when using autoresume so Event.INIT is run with the correct run name. \"\n+            if error_message != \"\":\n                 raise ValueError(error_message)\n             assert save_folder is not None\n             assert save_latest_filename is not None\n \n             remote_ud_has_multiple_concurrent_uploads = [\n-                isinstance(logger_destination, RemoteUploaderDownloader) and\n-                logger_destination._num_concurrent_uploads != 1 for logger_destination in self.logger.destinations\n+                isinstance(logger_destination, RemoteUploaderDownloader)\n+                and logger_destination._num_concurrent_uploads != 1\n+                for logger_destination in self.logger.destinations\n             ]\n             if any(remote_ud_has_multiple_concurrent_uploads):\n                 raise ValueError(\n-                    'Multiple concurrent uploads is not currently supported when using autoresume. Please set `num_concurrent_uploads` to 1 '\n-                    'for all `RemoteUploaderDownloader` instances.')\n+                    \"Multiple concurrent uploads is not currently supported when using autoresume. Please set `num_concurrent_uploads` to 1 \"\n+                    \"for all `RemoteUploaderDownloader` instances.\"\n+                )\n             assert latest_remote_file_name is not None\n             if self.state.fsdp_elastic_sharded_enabled:\n                 ar_object_store = maybe_create_object_store_from_uri(save_folder)\n                 # Symlink is on object store.\n                 if ar_object_store is not None:\n                     with tempfile.TemporaryDirectory() as temp_dir:\n-                        local_symlink_file = str(Path(temp_dir) / Path('autoresume.symlink'))\n-                        formatted_latest_remote_file_name = format_name_with_dist(latest_remote_file_name,\n-                                                                                  self.state.run_name) + '.symlink'\n-                        rank0_formatted_latest_remote_file_name = dist.all_gather_object(\n-                            formatted_latest_remote_file_name)[0]\n+                        local_symlink_file = str(\n+                            Path(temp_dir) / Path(\"autoresume.symlink\")\n+                        )\n+                        formatted_latest_remote_file_name = (\n+                            format_name_with_dist(\n+                                latest_remote_file_name, self.state.run_name\n+                            )\n+                            + \".symlink\"\n+                        )\n+                        rank0_formatted_latest_remote_file_name = (\n+                            dist.all_gather_object(formatted_latest_remote_file_name)[0]\n+                        )\n                         try:\n-                            ar_object_store.download_object(rank0_formatted_latest_remote_file_name, local_symlink_file)\n-                            with open(local_symlink_file, 'r') as f:\n+                            ar_object_store.download_object(\n+                                rank0_formatted_latest_remote_file_name,\n+                                local_symlink_file,\n+                            )\n+                            with open(local_symlink_file, \"r\") as f:\n                                 real_path = f.read()\n-                                log.debug(f'Read path {real_path} from symlink file')\n-                            autoresume_checkpoint_path = ar_object_store.get_uri(real_path)\n+                                log.debug(f\"Read path {real_path} from symlink file\")\n+                            autoresume_checkpoint_path = ar_object_store.get_uri(\n+                                real_path\n+                            )\n                         except FileNotFoundError:\n                             autoresume_checkpoint_path = None\n                 # Symlink is local.\n                 else:\n-                    save_latest_filename = format_name_with_dist(save_latest_filename, self.state.run_name)\n-                    rank0_save_latest_filename = dist.all_gather_object(save_latest_filename)[0]\n-                    save_folder = format_name_with_dist(save_folder, self.state.run_name)\n-                    latest_checkpoint_path = os.path.join(save_folder, rank0_save_latest_filename)\n+                    save_latest_filename = format_name_with_dist(\n+                        save_latest_filename, self.state.run_name\n+                    )\n+                    rank0_save_latest_filename = dist.all_gather_object(\n+                        save_latest_filename\n+                    )[0]\n+                    save_folder = format_name_with_dist(\n+                        save_folder, self.state.run_name\n+                    )\n+                    latest_checkpoint_path = os.path.join(\n+                        save_folder, rank0_save_latest_filename\n+                    )\n                     if os.path.exists(latest_checkpoint_path):\n-                        latest_checkpoint_path = os.path.join(os.path.dirname(latest_checkpoint_path),\n-                                                              os.readlink(latest_checkpoint_path))\n+                        latest_checkpoint_path = os.path.join(\n+                            os.path.dirname(latest_checkpoint_path),\n+                            os.readlink(latest_checkpoint_path),\n+                        )\n                         autoresume_checkpoint_path = latest_checkpoint_path\n                     else:\n                         autoresume_checkpoint_path = None\n@@ -1402,7 +1690,8 @@ class Trainer:\n                     save_latest_filename=save_latest_filename,\n                     save_latest_remote_file_name=latest_remote_file_name,\n                     loggers=loggers,\n-                    load_progress_bar=load_progress_bar)\n+                    load_progress_bar=load_progress_bar,\n+                )\n \n             # Found latest checkpoint path, load that instead\n             if autoresume_checkpoint_path:\n@@ -1412,15 +1701,16 @@ class Trainer:\n                 load_object_store = None\n                 # Disable `load_weights_only` since this applies only to the initial training run\n                 load_weights_only = False\n-                log.info('Autoresuming training from checkpoint')\n+                log.info(\"Autoresuming training from checkpoint\")\n             else:\n-                log.info('No previous autoresume checkpoint found')\n+                log.info(\"No previous autoresume checkpoint found\")\n         # Actually load the checkpoint from potentially updated arguments\n         if load_path is not None:\n             if load_object_store is None:\n                 load_object_store = maybe_create_object_store_from_uri(load_path)\n             if isinstance(load_object_store, WandBLogger):\n                 import wandb\n+\n                 if wandb.run is None:\n                     load_object_store.init(self.state, self.logger)\n             _, _, parsed_load_path = parse_uri(load_path)\n@@ -1440,8 +1730,20 @@ class Trainer:\n \n         # FSDP wrap if model is not yet wrapped and FSDP is enabled. This can happen if\n         # load_fsdp_monolith_rank0_only=True but no checkpoint was loaded.\n-        if not self.state.fsdp_enabled and self.state.fsdp_config is not None and self.state.fsdp_auto_wrap and self.state.load_fsdp_monolith_rank0_only:\n-            prepare_fsdp_module(model, optimizers, self.state.fsdp_config, precision, device, auto_microbatching)\n+        if (\n+            not self.state.fsdp_enabled\n+            and self.state.fsdp_config is not None\n+            and self.state.fsdp_auto_wrap\n+            and self.state.load_fsdp_monolith_rank0_only\n+        ):\n+            prepare_fsdp_module(\n+                model,\n+                optimizers,\n+                self.state.fsdp_config,\n+                precision,\n+                device,\n+                auto_microbatching,\n+            )\n \n         self.engine.run_event(Event.AFTER_LOAD)\n \n@@ -1452,29 +1754,40 @@ class Trainer:\n         # 2. While resuming from a checkpoint, we want to spin dataloader and bring it back to the\n         # same state as at the time of the checkpoint. Therefore, spinning needs to start from the\n         # same rng state as in the original run.\n-        log.info(f'Setting seed to {self.state.seed}')\n+        log.info(f\"Setting seed to {self.state.seed}\")\n         reproducibility.seed_all(self.state.seed)\n \n         # DDP wrap if required\n-        if not self.state.deepspeed_enabled and not self.state.fsdp_enabled and dist.get_world_size() > 1:\n-            self.state.model = prepare_ddp_module(self.state.model, self._find_unused_parameters)\n+        if (\n+            not self.state.deepspeed_enabled\n+            and not self.state.fsdp_enabled\n+            and dist.get_world_size() > 1\n+        ):\n+            self.state.model = prepare_ddp_module(\n+                self.state.model, self._find_unused_parameters\n+            )\n \n         # The model would need to be torch.compile()'d after being wrapped in a distributed strategy\n         # to take advantage of any graph breaks.\n         if is_torch_2_0 and not is_model_compiled and compile_config is not None:\n             compiled_model = torch.compile(  # pyright: ignore [reportGeneralTypeIssues]\n-                self.state.model, **compile_config)\n+                self.state.model, **compile_config\n+            )\n             self.state.model = compiled_model._orig_mod\n-            self.state.model.forward = compiled_model.dynamo_ctx(self.state.model.forward)\n+            self.state.model.forward = compiled_model.dynamo_ctx(\n+                self.state.model.forward\n+            )\n             is_model_compiled = True\n             # update local_hparams to ensure the `is_model_compiled` is set correctly for\n             # debugging purpose and for unit test.\n             if self.auto_log_hparams:\n-                self.local_hparams['is_model_compiled'] = is_model_compiled\n+                self.local_hparams[\"is_model_compiled\"] = is_model_compiled\n         elif not is_torch_2_0 and compile_config is not None:\n-            raise ValueError(f'`torch.compile` is supported for PyTorch 2.0 or higher.' +\n-                             f'Either update your PyTorch version or disable parameter by providing ' +\n-                             f'`compile_config` to `None`.')\n+            raise ValueError(\n+                f\"`torch.compile` is supported for PyTorch 2.0 or higher.\"\n+                + f\"Either update your PyTorch version or disable parameter by providing \"\n+                + f\"`compile_config` to `None`.\"\n+            )\n \n     @property\n     def saved_checkpoints(self) -> List[str]:\n@@ -1489,11 +1802,16 @@ class Trainer:\n             return []\n         return self._checkpoint_saver.saved_checkpoints\n \n-    def _try_checkpoint_download(self, latest_checkpoint_path: str, save_latest_remote_file_name: str,\n-                                 loggers: Sequence[LoggerDestination], load_progress_bar: bool) -> None:\n+    def _try_checkpoint_download(\n+        self,\n+        latest_checkpoint_path: str,\n+        save_latest_remote_file_name: str,\n+        loggers: Sequence[LoggerDestination],\n+        load_progress_bar: bool,\n+    ) -> None:\n         \"\"\"Attempts to download the checkpoint from the logger destinations.\"\"\"\n         log.debug(\n-            f'Trying to download {save_latest_remote_file_name} to {latest_checkpoint_path} on rank {dist.get_global_rank()}'\n+            f\"Trying to download {save_latest_remote_file_name} to {latest_checkpoint_path} on rank {dist.get_global_rank()}\"\n         )\n         for logger in loggers:\n             try:\n@@ -1507,7 +1825,7 @@ class Trainer:\n                 )\n                 break\n             except (NotImplementedError, FileNotFoundError):\n-                log.info(f'Checkpoint not found in: {logger}')\n+                log.info(f\"Checkpoint not found in: {logger}\")\n                 # Ignore errors caused by no checkpoint saved with logger\n                 pass\n \n@@ -1528,34 +1846,52 @@ class Trainer:\n         Returns:\n             Optional[str]: The path to the latest checkpoint, if found, otherwise None.\n         \"\"\"\n-        save_latest_filename = format_name_with_dist(save_latest_filename, self.state.run_name)\n+        save_latest_filename = format_name_with_dist(\n+            save_latest_filename, self.state.run_name\n+        )\n         save_folder = format_name_with_dist(save_folder, self.state.run_name)\n-        save_latest_remote_file_name = format_name_with_dist(save_latest_remote_file_name, self.state.run_name)\n+        save_latest_remote_file_name = format_name_with_dist(\n+            save_latest_remote_file_name, self.state.run_name\n+        )\n         latest_checkpoint_path = os.path.join(save_folder, save_latest_filename)\n \n         log.info(\n-            f'Looking for autoresume checkpoint: {save_latest_remote_file_name} (remote), {latest_checkpoint_path} (local)'\n+            f\"Looking for autoresume checkpoint: {save_latest_remote_file_name} (remote), {latest_checkpoint_path} (local)\"\n         )\n \n         if self.state.deepspeed_enabled or self.state.fsdp_sharded_state_dict_enabled:\n             # If latest checkpoint is not saved locally, try to fetch from loggers\n             if not os.path.exists(latest_checkpoint_path):\n-                log.debug(f'Attempting to download the checkpoint on to rank {dist.get_global_rank()}')\n+                log.debug(\n+                    f\"Attempting to download the checkpoint on to rank {dist.get_global_rank()}\"\n+                )\n                 os.makedirs(save_folder, exist_ok=True)\n-                self._try_checkpoint_download(latest_checkpoint_path, save_latest_remote_file_name, loggers,\n-                                              load_progress_bar)\n+                self._try_checkpoint_download(\n+                    latest_checkpoint_path,\n+                    save_latest_remote_file_name,\n+                    loggers,\n+                    load_progress_bar,\n+                )\n \n             # List of whether the checkpoint exists on each rank\n-            latest_checkpoint_exists = dist.all_gather_object(os.path.exists(latest_checkpoint_path))\n+            latest_checkpoint_exists = dist.all_gather_object(\n+                os.path.exists(latest_checkpoint_path)\n+            )\n \n             if all(latest_checkpoint_exists):  # All paths exist, so return the path.\n                 return latest_checkpoint_path\n             # Require all ranks to have their own local checkpoint if we wish to restore from it for\n             # deepspeed or fsdp + sharding\n-            elif any(latest_checkpoint_exists):  # Some but not all exist, which is very bad.\n-                missing_ranks = [n for (n, exist) in enumerate(latest_checkpoint_exists) if not exist]\n-                mode = 'Deepspeed' if self.state.deepspeed_enabled else 'FSDP sharding'\n-                raise RuntimeError(f'{mode} was enabled, but checkpoints missing on ranks: {missing_ranks}')\n+            elif any(\n+                latest_checkpoint_exists\n+            ):  # Some but not all exist, which is very bad.\n+                missing_ranks = [\n+                    n for (n, exist) in enumerate(latest_checkpoint_exists) if not exist\n+                ]\n+                mode = \"Deepspeed\" if self.state.deepspeed_enabled else \"FSDP sharding\"\n+                raise RuntimeError(\n+                    f\"{mode} was enabled, but checkpoints missing on ranks: {missing_ranks}\"\n+                )\n             else:  # None of the paths exists, so no autoresume necessary.\n                 return None\n         else:\n@@ -1570,18 +1906,28 @@ class Trainer:\n             save_latest_remote_file_name = save_latest_remote_file_name_list[0]\n \n             # try to download the checkpoint on local rank 0 of all nodes\n-            if dist.get_local_rank() == 0 and not os.path.exists(latest_checkpoint_path):\n-                log.debug(f'Attempting to download the checkpoint {save_latest_remote_file_name} on to all nodes')\n+            if dist.get_local_rank() == 0 and not os.path.exists(\n+                latest_checkpoint_path\n+            ):\n+                log.debug(\n+                    f\"Attempting to download the checkpoint {save_latest_remote_file_name} on to all nodes\"\n+                )\n                 os.makedirs(save_folder, exist_ok=True)\n-                self._try_checkpoint_download(latest_checkpoint_path, save_latest_remote_file_name, loggers,\n-                                              load_progress_bar)\n+                self._try_checkpoint_download(\n+                    latest_checkpoint_path,\n+                    save_latest_remote_file_name,\n+                    loggers,\n+                    load_progress_bar,\n+                )\n \n-            signal_file_path = os.path.join(os.path.dirname(latest_checkpoint_path),\n-                                            f'.node_{dist.get_node_rank()}_local_rank0_completed_autoresume')\n+            signal_file_path = os.path.join(\n+                os.path.dirname(latest_checkpoint_path),\n+                f\".node_{dist.get_node_rank()}_local_rank0_completed_autoresume\",\n+            )\n             if dist.get_local_rank() == 0:\n                 os.makedirs(os.path.dirname(signal_file_path), exist_ok=True)\n-                with open(signal_file_path, 'wb') as f:\n-                    f.write(b'local_rank0_completed_autoresume')\n+                with open(signal_file_path, \"wb\") as f:\n+                    f.write(b\"local_rank0_completed_autoresume\")\n \n             # Avoid the collective call until the local rank zero has finished trying to download the checkpoint\n             # so that we don't timeout for large downloads. This syncs all processes on the node\n@@ -1595,17 +1941,19 @@ class Trainer:\n \n             # At this point the rank 0 filepath should exist on all ranks if the download succeeded\n             # list of whether the checkpoint exists on each rank\n-            latest_checkpoint_exists = dist.all_gather_object(os.path.exists(latest_checkpoint_path))\n+            latest_checkpoint_exists = dist.all_gather_object(\n+                os.path.exists(latest_checkpoint_path)\n+            )\n \n             log.debug(\n-                f'Checkpoint {latest_checkpoint_path} exists on rank {dist.get_global_rank()}? {os.path.exists(latest_checkpoint_path)}'\n+                f\"Checkpoint {latest_checkpoint_path} exists on rank {dist.get_global_rank()}? {os.path.exists(latest_checkpoint_path)}\"\n             )\n \n             if not latest_checkpoint_exists[0]:\n                 # If the checkpoint doesn't exist on rank 0, don't crash, so the initial autoresume run can succeed\n                 return None\n             elif not all(latest_checkpoint_exists):\n-                raise RuntimeError('Downloading the checkpoint to all nodes failed')\n+                raise RuntimeError(\"Downloading the checkpoint to all nodes failed\")\n \n             return latest_checkpoint_path\n \n@@ -1614,25 +1962,28 @@ class Trainer:\n         *,\n         # Train Dataloader\n         train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,\n-        train_dataloader_label: str = 'train',\n+        train_dataloader_label: str = \"train\",\n         train_subset_num_batches: Optional[int] = None,\n         spin_dataloaders: Optional[bool] = None,\n-\n         # Timing\n         duration: Optional[Union[int, str, Time[int]]] = None,\n         reset_time: bool = False,\n-\n         # Schedulers\n-        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,\n-                                                                                       PyTorchScheduler]]]] = None,\n+        schedulers: Optional[\n+            Union[\n+                ComposerScheduler,\n+                PyTorchScheduler,\n+                Sequence[Union[ComposerScheduler, PyTorchScheduler]],\n+            ]\n+        ] = None,\n         scale_schedule_ratio: float = 1.0,\n         step_schedulers_every_batch: Optional[bool] = None,\n-\n         # Evaluation\n-        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,\n+        eval_dataloader: Optional[\n+            Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]\n+        ] = None,\n         eval_subset_num_batches: int = -1,\n         eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,\n-\n         # Numerics\n         device_train_microbatch_size: Optional[Union[int, str]] = None,\n         precision: Optional[Union[str, Precision]] = None,\n@@ -1748,20 +2099,29 @@ class Trainer:\n         \"\"\"\n         # Check Optimizer\n         if len(self.state.optimizers) == 0:\n-            raise ValueError(f'No optimizer was specified when constructing the Trainer. As the '\n-                             'model had no parameters, SGD was not created by default. This trainer '\n-                             'object can only be used to evaluate or predict. Please specify a model '\n-                             'with parameters and an optimizer for training.')\n+            raise ValueError(\n+                f\"No optimizer was specified when constructing the Trainer. As the \"\n+                \"model had no parameters, SGD was not created by default. This trainer \"\n+                \"object can only be used to evaluate or predict. Please specify a model \"\n+                \"with parameters and an optimizer for training.\"\n+            )\n \n         # Train Dataloader\n         if train_dataloader is not None:\n             self._train_data_spec = ensure_data_spec(train_dataloader)\n-            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label)\n+            self.state.set_dataloader(\n+                self._train_data_spec.dataloader, train_dataloader_label\n+            )\n             self.state.train_dataloader = self.state.dataloader\n-            self.state.device_train_microbatch_size = _get_initial_device_train_microbatch_size(\n-                self.state.device_train_microbatch_size, self.state.auto_microbatching, self.state.train_dataloader)\n+            self.state.device_train_microbatch_size = (\n+                _get_initial_device_train_microbatch_size(\n+                    self.state.device_train_microbatch_size,\n+                    self.state.auto_microbatching,\n+                    self.state.train_dataloader,\n+                )\n+            )\n         if self._train_data_spec is None:\n-            _raise_missing_argument_exception('train_dataloader')\n+            _raise_missing_argument_exception(\"train_dataloader\")\n         if train_subset_num_batches is not None:\n             self.state.dataloader_len = train_subset_num_batches\n         if spin_dataloaders is not None:\n@@ -1781,42 +2141,69 @@ class Trainer:\n             self.state.max_duration = duration + self.state.timestamp.get(duration.unit)\n \n         if self.state.max_duration is None:\n-            _raise_missing_argument_exception('max_duration')\n+            _raise_missing_argument_exception(\"max_duration\")\n \n-        if self.state.dataloader_len is None and self.state.max_duration.unit == TimeUnit.EPOCH:\n+        if (\n+            self.state.dataloader_len is None\n+            and self.state.max_duration.unit == TimeUnit.EPOCH\n+        ):\n             raise ValueError(\n-                ('max_duration cannot be specified in epochs when using an infinite dataloader. Please either '\n-                 'provide a dataloader with a length, specify max_duration in batches, samples, or tokens, or provide '\n-                 'train_subset_num_batches.'))\n+                (\n+                    \"max_duration cannot be specified in epochs when using an infinite dataloader. Please either \"\n+                    \"provide a dataloader with a length, specify max_duration in batches, samples, or tokens, or provide \"\n+                    \"train_subset_num_batches.\"\n+                )\n+            )\n \n-        if self.state.max_duration <= self.state.timestamp.get(self.state.max_duration.unit) and not reset_time:\n+        if (\n+            self.state.max_duration\n+            <= self.state.timestamp.get(self.state.max_duration.unit)\n+            and not reset_time\n+        ):\n             raise ValueError(\n-                (f'The max_duration ({self.state.max_duration}) is less than or equal to the elapsed training duration '\n-                 f'({self.state.timestamp.get(self.state.max_duration.unit)}). No training would occur. '\n-                 'Please provide the `duration` or specify `reset_time=True` in Trainer.fit().'))\n+                (\n+                    f\"The max_duration ({self.state.max_duration}) is less than or equal to the elapsed training duration \"\n+                    f\"({self.state.timestamp.get(self.state.max_duration.unit)}). No training would occur. \"\n+                    \"Please provide the `duration` or specify `reset_time=True` in Trainer.fit().\"\n+                )\n+            )\n \n         # Scale Schedule Ratio and Schedulers\n         if scale_schedule_ratio != 1.0:\n             # Not scaling the schedulers if the ratio is 1.0 in case if the scheduler cannot be scaled\n             # (e.g. a custom LambdaLR). However, since 1.0 implies no scaling, it is still possible\n             # to train with it.\n-            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)\n+            self.state.max_duration = _scale_max_duration_by_ssr(\n+                scale_schedule_ratio, self.state.max_duration\n+            )\n         if schedulers is not None:\n-            self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)\n+            self.state.schedulers = _compile_schedulers(\n+                schedulers, self.state, scale_schedule_ratio\n+            )\n \n             if step_schedulers_every_batch is None:\n-                self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)\n+                self._scheduler_step_frequency = _get_default_scheduler_frequency(\n+                    schedulers\n+                )\n             else:\n-                self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH\n+                self._scheduler_step_frequency = (\n+                    TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH\n+                )\n         else:\n             if scale_schedule_ratio != 1.0:\n-                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')\n+                raise ValueError(\n+                    \"Specifying `scale_schedule_ratio` without `schedulers` has no effect.\"\n+                )\n \n             if step_schedulers_every_batch is not None:\n-                raise ValueError('Specifying `step_schedulers_every_batch` without `schedulers` has no effect.')\n+                raise ValueError(\n+                    \"Specifying `step_schedulers_every_batch` without `schedulers` has no effect.\"\n+                )\n \n             if step_schedulers_every_batch is not None:\n-                raise ValueError('Specifying `step_schedulers_every_batch` without `schedulers` has no effect.')\n+                raise ValueError(\n+                    \"Specifying `step_schedulers_every_batch` without `schedulers` has no effect.\"\n+                )\n \n         # Evaluators\n         if eval_dataloader is not None:\n@@ -1826,19 +2213,26 @@ class Trainer:\n             metric_names = [str(k) for k in eval_metrics.keys()]\n             eval_dataloader = ensure_tuple(eval_dataloader)\n \n-            evaluator_types = [isinstance(evaluator, Evaluator) for evaluator in eval_dataloader]\n+            evaluator_types = [\n+                isinstance(evaluator, Evaluator) for evaluator in eval_dataloader\n+            ]\n             if any(evaluator_types) and not all(evaluator_types):\n-                raise ValueError('Mixing Evaluator with other classes is not allowed, please wrap'\n-                                 'all other classes with the Evaluator class. These are the classes'\n-                                 'that were detected:' + str([type(evaluator) for evaluator in eval_dataloader]))\n+                raise ValueError(\n+                    \"Mixing Evaluator with other classes is not allowed, please wrap\"\n+                    \"all other classes with the Evaluator class. These are the classes\"\n+                    \"that were detected:\"\n+                    + str([type(evaluator) for evaluator in eval_dataloader])\n+                )\n \n             evaluators = [\n-                ensure_evaluator(evaluator, default_metric_names=metric_names) for evaluator in eval_dataloader\n+                ensure_evaluator(evaluator, default_metric_names=metric_names)\n+                for evaluator in eval_dataloader\n             ]\n \n             # match metric names to model metrics\n             self.state.eval_metrics = {\n-                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators\n+                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names)\n+                for evaluator in evaluators\n             }\n \n             _set_evaluator_interval_and_subset_num_batches(\n@@ -1848,39 +2242,57 @@ class Trainer:\n             )\n \n             for evaluator in evaluators:\n-                validate_eval_automicrobatching(evaluator.auto_microbatching, self.state.device)\n+                validate_eval_automicrobatching(\n+                    evaluator.auto_microbatching, self.state.device\n+                )\n \n             if len(evaluators) == 0:\n                 if eval_subset_num_batches != -1:\n-                    raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')\n+                    raise ValueError(\n+                        \"Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.\"\n+                    )\n                 if eval_interval != 1:\n-                    raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')\n+                    raise ValueError(\n+                        \"Specifying `eval_interval` without an `eval_dataloader` has no effect.\"\n+                    )\n \n             self.state.evaluators = evaluators\n \n         # Microbatching\n         if device_train_microbatch_size is not None:\n-            self.state.auto_microbatching = _is_auto_microbatching(device_train_microbatch_size,\n-                                                                   device=self.state.device)\n+            self.state.auto_microbatching = _is_auto_microbatching(\n+                device_train_microbatch_size, device=self.state.device\n+            )\n             if self.state.auto_microbatching and self.state.profiler:\n-                raise ValueError(\"`device_train_microbatch_size='auto'` is not compatible with the profiler. It is \"\n-                                 \"recommended to run a mini-run with `device_train_microbatch_size='auto'` to identify \"\n-                                 'the optimal device_train_microbatch_size value and then manually specify that in a '\n-                                 'second run with profiler.')\n-            self.state.device_train_microbatch_size = _get_initial_device_train_microbatch_size(\n-                device_train_microbatch_size, self.state.auto_microbatching, self.state.train_dataloader)\n+                raise ValueError(\n+                    \"`device_train_microbatch_size='auto'` is not compatible with the profiler. It is \"\n+                    \"recommended to run a mini-run with `device_train_microbatch_size='auto'` to identify \"\n+                    \"the optimal device_train_microbatch_size value and then manually specify that in a \"\n+                    \"second run with profiler.\"\n+                )\n+            self.state.device_train_microbatch_size = (\n+                _get_initial_device_train_microbatch_size(\n+                    device_train_microbatch_size,\n+                    self.state.auto_microbatching,\n+                    self.state.train_dataloader,\n+                )\n+            )\n \n         # Precision\n         if precision is not None:\n             if Precision(precision) != self.state.precision:\n                 if self.state.deepspeed_enabled:\n-                    raise ValueError('Changing the precision when using DeepSpeed is not supported')\n+                    raise ValueError(\n+                        \"Changing the precision when using DeepSpeed is not supported\"\n+                    )\n                 precision = Precision(precision)\n                 _validate_precision(precision, self.state.device)\n                 self.state.precision = precision\n \n             # update scaler since precision was provided\n-            self.state.scaler = ClosureGradScaler() if self._use_closures() else GradScaler()\n+            self.state.scaler = (\n+                ClosureGradScaler() if self._use_closures() else GradScaler()\n+            )\n \n         self.first_batch_complete = False\n         self._train_loop()\n@@ -1904,12 +2316,17 @@ class Trainer:\n                 # running with FP16, this tends to result in overflows. Let's assume FP32 is good enough.\n                 for key in metric._defaults:\n                     metric_data = getattr(metric, key)\n-                    if isinstance(metric_data, torch.Tensor) and metric_data.dtype == torch.float16:\n+                    if (\n+                        isinstance(metric_data, torch.Tensor)\n+                        and metric_data.dtype == torch.float16\n+                    ):\n                         metric_data = metric_data.to(torch.float32)  # type: ignore\n                         setattr(metric, key, metric_data)\n         return metrics\n \n-    def _compute_and_log_metrics(self, dataloader_label: str, metrics: Dict[str, Metric]):\n+    def _compute_and_log_metrics(\n+        self, dataloader_label: str, metrics: Dict[str, Metric]\n+    ):\n         \"\"\"Computes metrics, logs the results, and updates the state with the deep-copied metrics.\n \n         Args:\n@@ -1924,19 +2341,27 @@ class Trainer:\n             computed_metrics[metric_name] = metric.compute()\n \n         self.logger.log_metrics(\n-            {f'metrics/{dataloader_label}/{name}': val for (name, val) in computed_metrics.items()},)\n+            {\n+                f\"metrics/{dataloader_label}/{name}\": val\n+                for (name, val) in computed_metrics.items()\n+            },\n+        )\n \n         # store metric instances\n         for metric_name, metric in metrics.items():\n             assert isinstance(metric, Metric)\n-            if dataloader_label == 'train':\n+            if dataloader_label == \"train\":\n                 self.state.train_metrics[metric_name] = metric\n-                self.state.train_metric_values[metric_name] = computed_metrics[metric_name]\n+                self.state.train_metric_values[metric_name] = computed_metrics[\n+                    metric_name\n+                ]\n             else:\n                 if dataloader_label not in self.state.eval_metrics:\n                     self.state.eval_metrics[dataloader_label] = {}\n                 self.state.eval_metrics[dataloader_label][metric_name] = metric\n-                self.state.eval_metric_values[metric_name] = computed_metrics[metric_name]\n+                self.state.eval_metric_values[metric_name] = computed_metrics[\n+                    metric_name\n+                ]\n \n     def _spin_dataloaders_to_cur_epoch(self):\n         \"\"\"Spin the dataloaders to restore sampler state for current epoch.\n@@ -1944,14 +2369,16 @@ class Trainer:\n         Only one batch must be loaded to seed the sampler's generator. since only the first batch is being loaded, the\n         dataloader may not be completely iterated through.\n         \"\"\"\n-        log.debug('Spinning the dataloaders')\n+        log.debug(\"Spinning the dataloaders\")\n \n         # spin the evaluator dataloaders once to initialize its sampler deterministically\n         # so it does not affect any other RNG reads\n-        eval_state = self.state.dataset_resumption.get('eval', {})\n+        eval_state = self.state.dataset_resumption.get(\"eval\", {})\n         for evaluator in self.state.evaluators:\n             dataloader = evaluator.dataloader.dataloader\n-            if isinstance(dataloader, DataLoader) and isinstance(dataloader.sampler, DistributedSampler):\n+            if isinstance(dataloader, DataLoader) and isinstance(\n+                dataloader.sampler, DistributedSampler\n+            ):\n                 dataloader.sampler.set_epoch(0)\n             if evaluator.label not in eval_state:\n                 for _ in dataloader:\n@@ -1959,10 +2386,14 @@ class Trainer:\n \n         # spin the train dataloader's sampler to get to the state of the desired epoch\n         dataloader = self.state.dataloader\n-        assert dataloader is not None, 'train dataloader is set on state after FIT_START'\n-        if 'train' not in self.state.dataset_resumption:\n+        assert dataloader is not None, (\n+            \"train dataloader is set on state after FIT_START\"\n+        )\n+        if \"train\" not in self.state.dataset_resumption:\n             for epoch in range(int(self.state.timestamp.epoch)):\n-                if isinstance(dataloader, DataLoader) and isinstance(dataloader.sampler, DistributedSampler):\n+                if isinstance(dataloader, DataLoader) and isinstance(\n+                    dataloader.sampler, DistributedSampler\n+                ):\n                     dataloader.sampler.set_epoch(epoch)\n                 for _ in dataloader:\n                     break\n@@ -1980,28 +2411,46 @@ class Trainer:\n         # Samples and tokens should be summed\n         # Batch time should be the value from rank 0\n         sample_token_tensor = self.state.device.tensor_to_device(\n-            torch.tensor([num_samples, num_tokens], dtype=torch.int))\n-        dist.all_reduce(sample_token_tensor, reduce_operation='SUM')\n+            torch.tensor([num_samples, num_tokens], dtype=torch.int)\n+        )\n+        dist.all_reduce(sample_token_tensor, reduce_operation=\"SUM\")\n         batch_time_tensor = self.state.device.tensor_to_device(\n-            torch.tensor([batch_time.total_seconds()], dtype=torch.float32))\n+            torch.tensor([batch_time.total_seconds()], dtype=torch.float32)\n+        )\n         dist.broadcast(batch_time_tensor, src=0)\n         batch_time = datetime.timedelta(seconds=batch_time_tensor[0].cpu().item())\n \n-        return int(sample_token_tensor[0].cpu().item()), int(sample_token_tensor[1].cpu().item()), batch_time\n+        return (\n+            int(sample_token_tensor[0].cpu().item()),\n+            int(sample_token_tensor[1].cpu().item()),\n+            batch_time,\n+        )\n \n     def _train_loop(self) -> None:\n         \"\"\"Run training for the specified number of epochs and log results.\"\"\"\n         # print training start\n-        log.info('Using precision %s', self.state.precision)\n+        log.info(\"Using precision %s\", self.state.precision)\n         self.logger.log_hyperparameters(\n-            {'enabled_algorithms/' + algo.__class__.__name__: True for algo in self.state.algorithms})\n-        assert self.state.dataloader is not None, 'dataloader is set in __init__() or fit()'\n-        assert self._train_data_spec is not None, 'The train data spec is set in __init__() or fit()'\n-        assert self.state.scaler is not None, 'scaler should have been set in __init__()'\n+            {\n+                \"enabled_algorithms/\" + algo.__class__.__name__: True\n+                for algo in self.state.algorithms\n+            }\n+        )\n+        assert self.state.dataloader is not None, (\n+            \"dataloader is set in __init__() or fit()\"\n+        )\n+        assert self._train_data_spec is not None, (\n+            \"The train data spec is set in __init__() or fit()\"\n+        )\n+        assert self.state.scaler is not None, (\n+            \"scaler should have been set in __init__()\"\n+        )\n \n         self.engine.run_event(Event.FIT_START)\n \n-        use_grad_scaling = self._use_grad_scaling(self.state.precision, self.state.scaler)\n+        use_grad_scaling = self._use_grad_scaling(\n+            self.state.precision, self.state.scaler\n+        )\n \n         if self.spin_dataloaders:\n             self._spin_dataloaders_to_cur_epoch()\n@@ -2019,44 +2468,74 @@ class Trainer:\n             try:\n                 if int(self.state.timestamp.batch_in_epoch) == 0:\n                     self.engine.run_event(Event.EPOCH_START)\n-                    self.logger.log_metrics({'time/epoch': self.state.timestamp.epoch.value})\n+                    self.logger.log_metrics(\n+                        {\"time/epoch\": self.state.timestamp.epoch.value}\n+                    )\n \n                 dataloader = self.state.dataloader\n-                if isinstance(dataloader, DataLoader) and isinstance(dataloader.sampler, DistributedSampler):\n+                if isinstance(dataloader, DataLoader) and isinstance(\n+                    dataloader.sampler, DistributedSampler\n+                ):\n                     dataloader.sampler.set_epoch(int(self.state.timestamp.epoch))\n \n-                for batch_idx, self.state.batch in enumerate(self._iter_dataloader(TrainerMode.TRAIN)):\n+                for batch_idx, self.state.batch in enumerate(\n+                    self._iter_dataloader(TrainerMode.TRAIN)\n+                ):\n                     # Spin dataloader forward unless dataloader handles internally with dataset_resumption\n-                    if self.spin_dataloaders and 'train' not in self.state.dataset_resumption and batch_idx < int(\n-                            self.state.timestamp.batch_in_epoch):\n+                    if (\n+                        self.spin_dataloaders\n+                        and \"train\" not in self.state.dataset_resumption\n+                        and batch_idx < int(self.state.timestamp.batch_in_epoch)\n+                    ):\n                         # Restore the RNG state immediately before the next batch is yielded from the dataloader\n-                        if batch_idx + 1 == int(self.state.timestamp.batch_in_epoch) and self._rng_state is not None:\n+                        if (\n+                            batch_idx + 1 == int(self.state.timestamp.batch_in_epoch)\n+                            and self._rng_state is not None\n+                        ):\n                             reproducibility.load_rng_state(self._rng_state)\n                             self._rng_state = None\n                         continue\n \n-                    self.state.batch = self.state.device.batch_to_device(self.state.batch)\n-                    self.state.batch = self._train_data_spec.device_transforms(self.state.batch)\n-                    rank_num_samples = self._train_data_spec.get_num_samples_in_batch(self.state.batch)\n-                    rank_num_tokens = self._train_data_spec.get_num_tokens_in_batch(self.state.batch)\n+                    self.state.batch = self.state.device.batch_to_device(\n+                        self.state.batch\n+                    )\n+                    self.state.batch = self._train_data_spec.device_transforms(\n+                        self.state.batch\n+                    )\n+                    rank_num_samples = self._train_data_spec.get_num_samples_in_batch(\n+                        self.state.batch\n+                    )\n+                    rank_num_tokens = self._train_data_spec.get_num_tokens_in_batch(\n+                        self.state.batch\n+                    )\n \n                     if self.state.deepspeed_enabled:\n-                        self.state.batch = _fix_batch_precision_for_deepspeed(self.state.batch, self.state.precision)\n+                        self.state.batch = _fix_batch_precision_for_deepspeed(\n+                            self.state.batch, self.state.precision\n+                        )\n \n                     self.engine.run_event(Event.AFTER_DATALOADER)\n \n                     self.engine.run_event(Event.BATCH_START)\n \n                     # Log time values\n-                    self.logger.log_metrics({\n-                        'time/batch': self.state.timestamp.batch.value,\n-                        'time/sample': self.state.timestamp.sample.value,\n-                        'time/batch_in_epoch': self.state.timestamp.batch_in_epoch.value,\n-                        'time/sample_in_epoch': self.state.timestamp.sample_in_epoch.value,\n-                    })\n+                    self.logger.log_metrics(\n+                        {\n+                            \"time/batch\": self.state.timestamp.batch.value,\n+                            \"time/sample\": self.state.timestamp.sample.value,\n+                            \"time/batch_in_epoch\": self.state.timestamp.batch_in_epoch.value,\n+                            \"time/sample_in_epoch\": self.state.timestamp.sample_in_epoch.value,\n+                        }\n+                    )\n                     if rank_num_tokens > 0:\n-                        self.logger.log_metrics({'time/token': self.state.timestamp.token.value})\n-                        self.logger.log_metrics({'time/token_in_epoch': self.state.timestamp.token_in_epoch.value})\n+                        self.logger.log_metrics(\n+                            {\"time/token\": self.state.timestamp.token.value}\n+                        )\n+                        self.logger.log_metrics(\n+                            {\n+                                \"time/token_in_epoch\": self.state.timestamp.token_in_epoch.value\n+                            }\n+                        )\n \n                     total_loss_dict = self._train_batch(use_grad_scaling)\n \n@@ -2067,7 +2546,8 @@ class Trainer:\n                     if total_loss_dict is not None:\n                         map_collection(total_loss_dict, dist.all_reduce)\n                         total_loss_dict = {\n-                            k: loss.cpu().item() / dist.get_world_size() for k, loss in total_loss_dict.items()\n+                            k: loss.cpu().item() / dist.get_world_size()\n+                            for k, loss in total_loss_dict.items()\n                         }\n                         self.state.total_loss_dict = total_loss_dict\n                         self.logger.log_metrics(total_loss_dict)\n@@ -2080,10 +2560,12 @@ class Trainer:\n \n                     batch_time = now - last_wct\n \n-                    total_num_samples, total_num_tokens, batch_time = self._accumulate_time_across_ranks(\n-                        rank_num_samples,\n-                        rank_num_tokens,\n-                        batch_time,\n+                    total_num_samples, total_num_tokens, batch_time = (\n+                        self._accumulate_time_across_ranks(\n+                            rank_num_samples,\n+                            rank_num_tokens,\n+                            batch_time,\n+                        )\n                     )\n \n                     # `now` is actually in the past, but want to include the time it takes to perform this reduction\n@@ -2095,7 +2577,7 @@ class Trainer:\n \n                     if self.state.train_metrics is not None:\n                         self._compute_and_log_metrics(\n-                            dataloader_label='train',\n+                            dataloader_label=\"train\",\n                             metrics=self.state.train_metrics,\n                         )\n \n@@ -2123,15 +2605,22 @@ class Trainer:\n                         finished_epoch_early = True\n                         break\n \n-                if not finished_epoch_early or self.state.dataloader_len == self.state.timestamp.batch_in_epoch:\n+                if (\n+                    not finished_epoch_early\n+                    or self.state.dataloader_len == self.state.timestamp.batch_in_epoch\n+                ):\n                     # Trigger the epoch end events if the dataloader was exhausted.\n                     # This happens if the \"break\" did not trigger above, or if it\n                     # did (e.g. duration specified in samples/batches/tokens), but it is still\n                     # the end of the dataloader (i.e. next(dataloader) would raise StopIteration)\n                     if self.state.train_metrics is not None:\n-                        self.state.train_metrics = self._ensure_metrics_device_and_dtype(self.state.train_metrics)\n+                        self.state.train_metrics = (\n+                            self._ensure_metrics_device_and_dtype(\n+                                self.state.train_metrics\n+                            )\n+                        )\n                         self._compute_and_log_metrics(\n-                            dataloader_label='train',\n+                            dataloader_label=\"train\",\n                             metrics=self.state.train_metrics,\n                         )\n \n@@ -2152,31 +2641,54 @@ class Trainer:\n \n                     self.engine.run_event(Event.EPOCH_CHECKPOINT)\n             except BreakEpochException:\n-                log.info(f'Skipping the rest of Epoch {int(self.state.timestamp.epoch)}')\n+                log.info(\n+                    f\"Skipping the rest of Epoch {int(self.state.timestamp.epoch)}\"\n+                )\n \n         # Log final time values\n-        self.logger.log_metrics({\n-            'time/epoch': self.state.timestamp.epoch.value,\n-            'time/batch': self.state.timestamp.batch.value,\n-            'time/sample': self.state.timestamp.sample.value,\n-            'time/batch_in_epoch': self.state.timestamp.batch_in_epoch.value,\n-            'time/sample_in_epoch': self.state.timestamp.sample_in_epoch.value,\n-        })\n-        if self.state.previous_timestamp is not None and self.state.timestamp.token.value - self.state.previous_timestamp.token.value > 0:\n-            self.logger.log_metrics({'time/token': self.state.timestamp.token.value})\n-            self.logger.log_metrics({'time/token_in_epoch': self.state.timestamp.token_in_epoch.value})\n+        self.logger.log_metrics(\n+            {\n+                \"time/epoch\": self.state.timestamp.epoch.value,\n+                \"time/batch\": self.state.timestamp.batch.value,\n+                \"time/sample\": self.state.timestamp.sample.value,\n+                \"time/batch_in_epoch\": self.state.timestamp.batch_in_epoch.value,\n+                \"time/sample_in_epoch\": self.state.timestamp.sample_in_epoch.value,\n+            }\n+        )\n+        if (\n+            self.state.previous_timestamp is not None\n+            and self.state.timestamp.token.value\n+            - self.state.previous_timestamp.token.value\n+            > 0\n+        ):\n+            self.logger.log_metrics({\"time/token\": self.state.timestamp.token.value})\n+            self.logger.log_metrics(\n+                {\"time/token_in_epoch\": self.state.timestamp.token_in_epoch.value}\n+            )\n \n         self.engine.run_event(Event.FIT_END)\n         self._run_evaluators(Event.FIT_END)\n \n     def _eval_train_metrics(self, device_batch):\n-        assert self._train_data_spec is not None, 'The train data spec should be set on __init__ or fit()'\n-        assert self.state.train_metrics is not None, 'The train metrics should be set on __init__ or fit()'\n+        assert self._train_data_spec is not None, (\n+            \"The train data spec should be set on __init__ or fit()\"\n+        )\n+        assert self.state.train_metrics is not None, (\n+            \"The train metrics should be set on __init__ or fit()\"\n+        )\n \n-        with torch.no_grad(),\\\n-                model_eval_mode(self.state.model),\\\n-                _get_precision_context(self.state.precision, self.state.precision_config, self.state.deepspeed_enabled):\n-            eval_outputs = self._original_model.eval_forward(device_batch, self.state.outputs)\n+        with (\n+            torch.no_grad(),\n+            model_eval_mode(self.state.model),\n+            _get_precision_context(\n+                self.state.precision,\n+                self.state.precision_config,\n+                self.state.deepspeed_enabled,\n+            ),\n+        ):\n+            eval_outputs = self._original_model.eval_forward(\n+                device_batch, self.state.outputs\n+            )\n             for metric in self.state.train_metrics.values():\n                 self._original_model.update_metric(\n                     device_batch,\n@@ -2188,8 +2700,12 @@ class Trainer:\n         \"\"\"Runs evaluators periodically during training.\"\"\"\n         evaluators_executing = []\n         for evaluator in self.state.evaluators:\n-            assert evaluator.eval_interval is not None, 'eval_interval should have been set on __init__() or fit()'\n-            assert evaluator.subset_num_batches is not None, 'subset_num_batches should have been set on __init__() or fit()'\n+            assert evaluator.eval_interval is not None, (\n+                \"eval_interval should have been set on __init__() or fit()\"\n+            )\n+            assert evaluator.subset_num_batches is not None, (\n+                \"subset_num_batches should have been set on __init__() or fit()\"\n+            )\n             evaluators_executing.append(evaluator.eval_interval(self.state, event))\n         if not any(evaluators_executing):\n             return\n@@ -2216,7 +2732,9 @@ class Trainer:\n         Returns:\n             Dict[str, torch.Tensor]: a dictionary containing the total loss and individual losses if available.\n         \"\"\"\n-        assert self._train_data_spec is not None, 'The train data spec should be set on __init__ or fit()'\n+        assert self._train_data_spec is not None, (\n+            \"The train data spec should be set on __init__ or fit()\"\n+        )\n \n         # Cache the device batch, because `self.state.batch` gets overridden in microbatching loop.\n         # Any in-place changes to a microbatch will be reflected in the device batch.\n@@ -2230,21 +2748,37 @@ class Trainer:\n                 for metric in self.state.train_metrics.values():\n                     metric.reset()\n \n-            total_loss_dict = {'loss/train/total': self.state.device.tensor_to_device(torch.zeros(size=(1,)))}\n-            found_cuda_oom = 0  # int since bool BOR not supported on all torch.distributed backends\n+            total_loss_dict = {\n+                \"loss/train/total\": self.state.device.tensor_to_device(\n+                    torch.zeros(size=(1,))\n+                )\n+            }\n+            found_cuda_oom = (\n+                0  # int since bool BOR not supported on all torch.distributed backends\n+            )\n             try:\n                 assert self.state.scaler is not None\n                 assert self.state.device_train_microbatch_size is not None\n-                microbatches = self._train_data_spec.split_batch(device_batch, self.state.device_train_microbatch_size)\n+                microbatches = self._train_data_spec.split_batch(\n+                    device_batch, self.state.device_train_microbatch_size\n+                )\n                 if self._use_closures():\n                     for optimizer in self.state.optimizers:\n                         if use_grad_scaling:\n-                            self.state.scaler.step(optimizer,\n-                                                   closure=lambda loss_dict=total_loss_dict, **kwargs: self.\n-                                                   _train_microbatches(microbatches, loss_dict, **kwargs))\n+                            self.state.scaler.step(\n+                                optimizer,\n+                                closure=lambda loss_dict=total_loss_dict,\n+                                **kwargs: self._train_microbatches(\n+                                    microbatches, loss_dict, **kwargs\n+                                ),\n+                            )\n                         else:\n-                            optimizer.step(closure=lambda loss_dict=total_loss_dict, **kwargs: self._train_microbatches(\n-                                microbatches, loss_dict, **kwargs).item())\n+                            optimizer.step(\n+                                closure=lambda loss_dict=total_loss_dict,\n+                                **kwargs: self._train_microbatches(\n+                                    microbatches, loss_dict, **kwargs\n+                                ).item()\n+                            )\n                 else:\n                     self._train_microbatches(microbatches, total_loss_dict)\n                     if not self.state.deepspeed_enabled:\n@@ -2260,11 +2794,15 @@ class Trainer:\n                 if self.state.auto_microbatching and _is_cuda_oom(e):\n                     log.debug((f\"Rank {dist.get_global_rank()} OOM'd.\"))\n                     found_cuda_oom = 1\n-                elif self.state.auto_microbatching and ('cuda' in str(e).lower() or 'c10' in str(e).lower()):\n+                elif self.state.auto_microbatching and (\n+                    \"cuda\" in str(e).lower() or \"c10\" in str(e).lower()\n+                ):\n                     raise RuntimeError(\n                         textwrap.dedent(\n-                            'Encountered non-addressable cuda error while using auto microbatching. '\n-                            'If this repeatedly occurs, set `device_train_microbatch_size` manually.')) from e\n+                            \"Encountered non-addressable cuda error while using auto microbatching. \"\n+                            \"If this repeatedly occurs, set `device_train_microbatch_size` manually.\"\n+                        )\n+                    ) from e\n                 else:\n                     raise\n \n@@ -2273,13 +2811,16 @@ class Trainer:\n                 while not all_ranks_finished:\n                     # Propagate across all ranks if any rank hit CUDA OOM\n                     found_cuda_oom_tensor = self.state.device.tensor_to_device(\n-                        torch.tensor([found_cuda_oom], dtype=torch.uint8))\n-                    dist.all_reduce(found_cuda_oom_tensor, reduce_operation='MAX')\n+                        torch.tensor([found_cuda_oom], dtype=torch.uint8)\n+                    )\n+                    dist.all_reduce(found_cuda_oom_tensor, reduce_operation=\"MAX\")\n                     found_cuda_oom = found_cuda_oom_tensor.item()\n                     # Check if any rank is still not done with the batch. This may happen if only a\n                     # subset of ranks OOM, leaving some batches still in the forward pass\n-                    all_ranks_finished_tensor = self.state.device.tensor_to_device(torch.tensor([1], dtype=torch.uint8))\n-                    dist.all_reduce(all_ranks_finished_tensor, reduce_operation='MIN')\n+                    all_ranks_finished_tensor = self.state.device.tensor_to_device(\n+                        torch.tensor([1], dtype=torch.uint8)\n+                    )\n+                    dist.all_reduce(all_ranks_finished_tensor, reduce_operation=\"MIN\")\n                     all_ranks_finished = all_ranks_finished_tensor.item() == 1\n                 if found_cuda_oom == 1:\n                     _adjust_device_train_microbatch_size(self.state)\n@@ -2287,14 +2828,20 @@ class Trainer:\n                     continue\n             # Log microbatch and return loss if we've completed without OOMing.\n             assert self.state.device_train_microbatch_size is not None\n-            self.logger.log_metrics({'trainer/device_train_microbatch_size': self.state.device_train_microbatch_size})\n+            self.logger.log_metrics(\n+                {\n+                    \"trainer/device_train_microbatch_size\": self.state.device_train_microbatch_size\n+                }\n+            )\n             self.first_batch_complete = True\n             return total_loss_dict\n \n-    def _train_microbatches(self,\n-                            microbatches: Sequence[Batch],\n-                            total_loss_dict: Dict[str, torch.Tensor],\n-                            ddp_sync: bool = True) -> torch.Tensor:\n+    def _train_microbatches(\n+        self,\n+        microbatches: Sequence[Batch],\n+        total_loss_dict: Dict[str, torch.Tensor],\n+        ddp_sync: bool = True,\n+    ) -> torch.Tensor:\n         \"\"\"Iterate over microbatches and compute the loss that will be used to step the optimizer.\n \n         Args:\n@@ -2316,8 +2863,10 @@ class Trainer:\n                 # microbatch after the OOMs but an OOMing rank will have never completed (1) if\n                 # using `SINGLE_AUTO_SYNC`. To avoid this, we force a sync on every microbatch for\n                 # the first batch.\n-                log.info('Auto microbatching requires syncing every microbatch (`MULTI_AUTO_SYNC`)'\n-                         ' to avoid deadlock on first batch, so ddp_sync_strategy will be ignored.')\n+                log.info(\n+                    \"Auto microbatching requires syncing every microbatch (`MULTI_AUTO_SYNC`)\"\n+                    \" to avoid deadlock on first batch, so ddp_sync_strategy will be ignored.\"\n+                )\n                 context = contextlib.nullcontext\n             else:\n                 context = cast(Callable[[], ContextManager], self.state.model.no_sync)\n@@ -2330,7 +2879,9 @@ class Trainer:\n             assert self.state.optimizers is not None\n             assert self.state.scaler is not None\n \n-            use_grad_scaling = self._use_grad_scaling(self.state.precision, self.state.scaler)\n+            use_grad_scaling = self._use_grad_scaling(\n+                self.state.precision, self.state.scaler\n+            )\n \n             if not self.state.deepspeed_enabled:\n                 for optimizer in self.state.optimizers:\n@@ -2340,19 +2891,28 @@ class Trainer:\n                         optimizer.zero_grad()\n \n             # Tracker for gradient accumulation\n-            current_batch_size = sum([self._train_data_spec.get_num_samples_in_batch(batch) for batch in microbatches])\n+            current_batch_size = sum(\n+                [\n+                    self._train_data_spec.get_num_samples_in_batch(batch)\n+                    for batch in microbatches\n+                ]\n+            )\n             # Cache batch, which will be overwritten by microbatches. Restore after microbatches complete\n             current_batch = self.state.batch\n \n             for microbatch_idx, self.state.batch in enumerate(microbatches):\n                 is_final_microbatch = microbatch_idx + 1 == len(microbatches)\n-                microbatch_loss_dict = self._train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)\n+                microbatch_loss_dict = self._train_microbatch(\n+                    use_grad_scaling, current_batch_size, is_final_microbatch\n+                )\n \n                 # Aggregate each loss in microbatch_loss_dict into total_loss_dict\n                 for k, microbatch_loss in microbatch_loss_dict.items():\n-                    loss_key = f'loss/train/{k}'\n+                    loss_key = f\"loss/train/{k}\"\n                     if loss_key not in total_loss_dict:\n-                        total_loss_dict[loss_key] = self.state.device.tensor_to_device(torch.zeros(size=(1,)))\n+                        total_loss_dict[loss_key] = self.state.device.tensor_to_device(\n+                            torch.zeros(size=(1,))\n+                        )\n                     total_loss_dict[loss_key] += microbatch_loss\n \n             # Restore batch\n@@ -2365,10 +2925,11 @@ class Trainer:\n \n             self.engine.run_event(Event.AFTER_TRAIN_BATCH)\n \n-            return total_loss_dict['loss/train/total']\n+            return total_loss_dict[\"loss/train/total\"]\n \n-    def _train_microbatch(self, use_grad_scaling: bool, current_batch_size: int,\n-                          is_final_microbatch: bool) -> Dict[str, torch.Tensor]:\n+    def _train_microbatch(\n+        self, use_grad_scaling: bool, current_batch_size: int, is_final_microbatch: bool\n+    ) -> Dict[str, torch.Tensor]:\n         \"\"\"Train and compute the loss of ``state.batch``, which is assumed to be a single microbatch.\n \n         Args:\n@@ -2383,8 +2944,12 @@ class Trainer:\n         # Cache the device batch, because `self.state.batch` gets overridden in microbatching loop\n         device_batch = deepcopy(self.state.batch)\n \n-        microbatch_num_samples = self._train_data_spec.get_num_samples_in_batch(self.state.batch)\n-        if self.state.deepspeed_enabled or not isinstance(self.state.model, DistributedDataParallel):\n+        microbatch_num_samples = self._train_data_spec.get_num_samples_in_batch(\n+            self.state.batch\n+        )\n+        if self.state.deepspeed_enabled or not isinstance(\n+            self.state.model, DistributedDataParallel\n+        ):\n             sync_context = contextlib.nullcontext()\n         elif self.state.auto_microbatching and not self.first_batch_complete:\n             # PyTorch DDP rebuilds gradient reduction buckets after 1) a forward pass where the\n@@ -2394,8 +2959,10 @@ class Trainer:\n             # microbatch after the OOMs but an OOMing rank will have never completed (1) if\n             # using `SINGLE_AUTO_SYNC`. To avoid this, we force a sync on every microbatch for\n             # the first batch.\n-            log.info('Auto microbatching requires syncing every microbatch (`MULTI_AUTO_SYNC`)'\n-                     ' to avoid deadlock on first batch, so ddp_sync_strategy will be ignored.')\n+            log.info(\n+                \"Auto microbatching requires syncing every microbatch (`MULTI_AUTO_SYNC`)\"\n+                \" to avoid deadlock on first batch, so ddp_sync_strategy will be ignored.\"\n+            )\n             sync_context = contextlib.nullcontext()\n         else:\n             sync_context = ddp_sync_context(\n@@ -2408,8 +2975,11 @@ class Trainer:\n             # Forward pass\n             self.engine.run_event(Event.BEFORE_FORWARD)\n \n-            with _get_precision_context(self.state.precision, self.state.precision_config,\n-                                        self.state.deepspeed_enabled):\n+            with _get_precision_context(\n+                self.state.precision,\n+                self.state.precision_config,\n+                self.state.deepspeed_enabled,\n+            ):\n                 self.state.outputs = self.state.model(self.state.batch)\n \n             self.engine.run_event(Event.AFTER_FORWARD)\n@@ -2418,22 +2988,33 @@ class Trainer:\n             # happen when close to memory limit or with uneven memory usage across ranks\n             if self.state.auto_microbatching:\n                 # Check if any other rank hit an OOM\n-                found_cuda_oom_tensor = self.state.device.tensor_to_device(torch.tensor([0], dtype=torch.uint8))\n-                dist.all_reduce(found_cuda_oom_tensor, reduce_operation='MAX')\n+                found_cuda_oom_tensor = self.state.device.tensor_to_device(\n+                    torch.tensor([0], dtype=torch.uint8)\n+                )\n+                dist.all_reduce(found_cuda_oom_tensor, reduce_operation=\"MAX\")\n                 found_cuda_oom = found_cuda_oom_tensor.item()\n                 # Signal current rank is still in batch\n-                all_ranks_finished_tensor = self.state.device.tensor_to_device(torch.tensor([0], dtype=torch.uint8))\n-                dist.all_reduce(all_ranks_finished_tensor, reduce_operation='MIN')\n+                all_ranks_finished_tensor = self.state.device.tensor_to_device(\n+                    torch.tensor([0], dtype=torch.uint8)\n+                )\n+                dist.all_reduce(all_ranks_finished_tensor, reduce_operation=\"MIN\")\n \n                 if found_cuda_oom == 1:\n-                    raise RuntimeError('CUDA out of memory encountered on a different rank')\n+                    raise RuntimeError(\n+                        \"CUDA out of memory encountered on a different rank\"\n+                    )\n \n             # Loss\n             self.engine.run_event(Event.BEFORE_LOSS)\n \n-            with _get_precision_context(self.state.precision, self.state.precision_config,\n-                                        self.state.deepspeed_enabled):\n-                self.state.loss = self._original_model.loss(self.state.outputs, self.state.batch)\n+            with _get_precision_context(\n+                self.state.precision,\n+                self.state.precision_config,\n+                self.state.deepspeed_enabled,\n+            ):\n+                self.state.loss = self._original_model.loss(\n+                    self.state.outputs, self.state.batch\n+                )\n \n             assert self.state.loss is not None\n             self.engine.run_event(Event.AFTER_LOSS)\n@@ -2443,12 +3024,14 @@ class Trainer:\n \n             microbatch_loss_dict = {}\n             # If total loss key is present, copy loss\n-            if isinstance(self.state.loss, dict) and ('total' in self.state.loss):\n-                microbatch_loss = self.state.loss['total']  # type: ignore\n+            if isinstance(self.state.loss, dict) and (\"total\" in self.state.loss):\n+                microbatch_loss = self.state.loss[\"total\"]  # type: ignore\n                 microbatch_loss_dict = self.state.loss.copy()\n             # If total loss key is not present, sum individual losses\n             else:\n-                microbatch_loss = self.state.device.tensor_to_device(torch.zeros(size=(1,)))\n+                microbatch_loss = self.state.device.tensor_to_device(\n+                    torch.zeros(size=(1,))\n+                )\n                 for loss in ensure_tuple(self.state.loss):\n                     microbatch_loss.add_(loss.mean())\n \n@@ -2457,17 +3040,24 @@ class Trainer:\n                     microbatch_loss_dict = self.state.loss.copy()\n                 # If not, create a dictionary with generic loss names\n                 elif len(ensure_tuple(self.state.loss)) > 1:\n-                    microbatch_loss_dict = {f'loss{i}': loss for i, loss in enumerate(ensure_tuple(self.state.loss))}\n+                    microbatch_loss_dict = {\n+                        f\"loss{i}\": loss\n+                        for i, loss in enumerate(ensure_tuple(self.state.loss))\n+                    }\n \n                 # Include total loss\n-                microbatch_loss_dict['total'] = microbatch_loss\n+                microbatch_loss_dict[\"total\"] = microbatch_loss\n \n             # For each loss to log: detach, clone, mean, then multiply by (microbatch size) / (batch size)\n             for k, loss in microbatch_loss_dict.items():\n-                microbatch_loss_dict[k] = loss.detach().clone().mean() * (microbatch_num_samples / current_batch_size)\n+                microbatch_loss_dict[k] = loss.detach().clone().mean() * (\n+                    microbatch_num_samples / current_batch_size\n+                )\n \n             if use_grad_scaling:\n-                microbatch_loss = cast(torch.Tensor, self.state.scaler.scale(microbatch_loss))\n+                microbatch_loss = cast(\n+                    torch.Tensor, self.state.scaler.scale(microbatch_loss)\n+                )\n \n             if self.state.deepspeed_enabled:\n                 self.state.deepspeed_model.backward(microbatch_loss)\n@@ -2480,8 +3070,13 @@ class Trainer:\n             self.engine.run_event(Event.AFTER_BACKWARD)\n \n             # Use microbatch outputs to update training metrics\n-            if self.state.train_metrics is not None and len(self.state.train_metrics) != 0:\n-                self.state.train_metrics = self._ensure_metrics_device_and_dtype(self.state.train_metrics)\n+            if (\n+                self.state.train_metrics is not None\n+                and len(self.state.train_metrics) != 0\n+            ):\n+                self.state.train_metrics = self._ensure_metrics_device_and_dtype(\n+                    self.state.train_metrics\n+                )\n                 self._eval_train_metrics(device_batch)\n \n         if self.state.deepspeed_enabled:\n@@ -2569,8 +3164,8 @@ class Trainer:\n         original_dataloader = self.state.dataloader\n         original_dataloader_label = self.state.dataloader_label\n         original_dataloader_len = self.state.dataloader_len\n-        self.state.set_dataloader(data_spec.dataloader, 'predict', subset_num_batches)\n-        assert self.state.dataloader is not None, 'Already set the dataloader'\n+        self.state.set_dataloader(data_spec.dataloader, \"predict\", subset_num_batches)\n+        assert self.state.dataloader is not None, \"Already set the dataloader\"\n \n         # Reset the predict timestamp\n         self.state.predict_timestamp = Timestamp()\n@@ -2581,7 +3176,6 @@ class Trainer:\n         cpu_device = DeviceCPU()\n \n         with torch.no_grad(), model_eval_mode(self.state.model):\n-\n             self.engine.run_event(Event.PREDICT_START)\n \n             for self.state.batch in self._iter_dataloader(TrainerMode.PREDICT):\n@@ -2598,13 +3192,18 @@ class Trainer:\n \n                 # Fix the batch if using DeepSpeed\n                 if self.state.deepspeed_enabled:\n-                    self.state.batch = _fix_batch_precision_for_deepspeed(self.state.batch, self.state.precision)\n+                    self.state.batch = _fix_batch_precision_for_deepspeed(\n+                        self.state.batch, self.state.precision\n+                    )\n \n                 self.engine.run_event(Event.PREDICT_BATCH_START)\n \n                 self.engine.run_event(Event.PREDICT_BEFORE_FORWARD)\n-                with _get_precision_context(self.state.precision, self.state.precision_config,\n-                                            self.state.deepspeed_enabled):\n+                with _get_precision_context(\n+                    self.state.precision,\n+                    self.state.precision_config,\n+                    self.state.deepspeed_enabled,\n+                ):\n                     self.state.outputs = self.state.model(self.state.batch)\n                 self.engine.run_event(Event.PREDICT_AFTER_FORWARD)\n \n@@ -2614,17 +3213,23 @@ class Trainer:\n                 now = datetime.datetime.now()\n                 batch_time = now - last_wct\n \n-                total_num_samples, total_num_tokens, batch_time = self._accumulate_time_across_ranks(\n-                    num_samples=rank_num_samples,\n-                    num_tokens=rank_num_tokens,\n-                    batch_time=batch_time,\n+                total_num_samples, total_num_tokens, batch_time = (\n+                    self._accumulate_time_across_ranks(\n+                        num_samples=rank_num_samples,\n+                        num_tokens=rank_num_tokens,\n+                        batch_time=batch_time,\n+                    )\n                 )\n \n                 last_wct = now\n \n-                self.state.predict_timestamp = self.state.predict_timestamp.to_next_batch(samples=total_num_samples,\n-                                                                                          tokens=total_num_tokens,\n-                                                                                          duration=batch_time)\n+                self.state.predict_timestamp = (\n+                    self.state.predict_timestamp.to_next_batch(\n+                        samples=total_num_samples,\n+                        tokens=total_num_tokens,\n+                        duration=batch_time,\n+                    )\n+                )\n \n                 self.engine.run_event(Event.PREDICT_BATCH_END)\n \n@@ -2639,7 +3244,9 @@ class Trainer:\n \n     def eval(\n         self,\n-        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,\n+        eval_dataloader: Optional[\n+            Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]\n+        ] = None,\n         subset_num_batches: int = -1,\n     ):\n         \"\"\"Run evaluation loop.\n@@ -2734,54 +3341,78 @@ class Trainer:\n \n             eval_dataloader = ensure_tuple(eval_dataloader)\n \n-            evaluator_types = [isinstance(evaluator, Evaluator) for evaluator in eval_dataloader]\n+            evaluator_types = [\n+                isinstance(evaluator, Evaluator) for evaluator in eval_dataloader\n+            ]\n             if any(evaluator_types) and not all(evaluator_types):\n-                raise ValueError('Mixing Evaluator with other classes is not allowed, please wrap'\n-                                 'all other classes with the Evaluator class. These are the classes'\n-                                 'that were detected:' + str([type(evaluator) for evaluator in eval_dataloader]))\n+                raise ValueError(\n+                    \"Mixing Evaluator with other classes is not allowed, please wrap\"\n+                    \"all other classes with the Evaluator class. These are the classes\"\n+                    \"that were detected:\"\n+                    + str([type(evaluator) for evaluator in eval_dataloader])\n+                )\n \n             evaluators = [\n-                ensure_evaluator(evaluator, default_metric_names=metric_names) for evaluator in eval_dataloader\n+                ensure_evaluator(evaluator, default_metric_names=metric_names)\n+                for evaluator in eval_dataloader\n             ]\n \n             if self.state.eval_metrics:\n                 for evaluator in evaluators:\n                     if evaluator.label in self.state.eval_metrics:\n                         warnings.warn(\n-                            f'eval_dataloader label \\'{evaluator.label}\\' was already provided in '\n-                            'trainer initialization. Existing data for that label will be overwritten. '\n-                            'To prevent this in the future, assign unique label names.',\n-                            category=UserWarning)\n+                            f\"eval_dataloader label '{evaluator.label}' was already provided in \"\n+                            \"trainer initialization. Existing data for that label will be overwritten. \"\n+                            \"To prevent this in the future, assign unique label names.\",\n+                            category=UserWarning,\n+                        )\n \n             # match metric names to model metrics\n-            log.info(f'Added {[e.label for e in evaluators]} to eval_metrics.')\n-            self.state.eval_metrics.update({e.label: _filter_metrics(eval_metrics, e.metric_names) for e in evaluators})\n+            log.info(f\"Added {[e.label for e in evaluators]} to eval_metrics.\")\n+            self.state.eval_metrics.update(\n+                {\n+                    e.label: _filter_metrics(eval_metrics, e.metric_names)\n+                    for e in evaluators\n+                }\n+            )\n \n             _set_evaluator_interval_and_subset_num_batches(\n                 evaluators=evaluators,\n-                eval_interval='1ep',  # ignored\n+                eval_interval=\"1ep\",  # ignored\n                 subset_num_batches=subset_num_batches,\n             )\n \n             for evaluator in evaluators:\n-                validate_eval_automicrobatching(evaluator.auto_microbatching, self.state.device)\n+                validate_eval_automicrobatching(\n+                    evaluator.auto_microbatching, self.state.device\n+                )\n \n-            self.state.evaluators.extend(evaluators)  # Add evaluators to state.evaluators\n+            self.state.evaluators.extend(\n+                evaluators\n+            )  # Add evaluators to state.evaluators\n         else:\n             eval_passed_in = False\n             if not self.state.evaluators:\n-                raise ValueError('eval_dataloader must be provided to either Trainer init() or eval().')\n+                raise ValueError(\n+                    \"eval_dataloader must be provided to either Trainer init() or eval().\"\n+                )\n             evaluators = self.state.evaluators\n \n         for evaluator in evaluators:\n-            eval_subset_num_batches = evaluator.subset_num_batches if subset_num_batches == -1 else subset_num_batches\n+            eval_subset_num_batches = (\n+                evaluator.subset_num_batches\n+                if subset_num_batches == -1\n+                else subset_num_batches\n+            )\n             self._eval_loop(\n                 evaluator=evaluator,\n                 metrics=self.state.eval_metrics[evaluator.label],\n                 subset_num_batches=eval_subset_num_batches,\n             )\n             if eval_passed_in:\n-                self.state.evaluators.remove(evaluator)  # Remove them from state once eval is finished.\n+                self.state.evaluators.remove(\n+                    evaluator\n+                )  # Remove them from state once eval is finished.\n \n         self.engine.run_event(Event.EVAL_STANDALONE_END)\n \n@@ -2816,8 +3447,10 @@ class Trainer:\n         last_wct = datetime.datetime.now()\n \n         with torch.no_grad(), model_eval_mode(self.state.model):\n-            self.state.set_dataloader(data_spec.dataloader, evaluator.label, subset_num_batches)\n-            assert self.state.dataloader is not None, 'dataloader is set'\n+            self.state.set_dataloader(\n+                data_spec.dataloader, evaluator.label, subset_num_batches\n+            )\n+            assert self.state.dataloader is not None, \"dataloader is set\"\n \n             self.engine.run_event(Event.EVAL_START)\n \n@@ -2831,7 +3464,9 @@ class Trainer:\n             drop_last = None\n             dataset_len = None\n             last_batch = False\n-            if isinstance(dataloader, DataLoader) and isinstance(dataloader.sampler, DistributedSampler):\n+            if isinstance(dataloader, DataLoader) and isinstance(\n+                dataloader.sampler, DistributedSampler\n+            ):\n                 # The distributed sampler uses `set_epoch` to set the random seed\n                 # Because evaluation can run on each batch, we use the batch to seed the sampler\n                 # so each evaluation will get a proper shuffle.\n@@ -2845,11 +3480,13 @@ class Trainer:\n                     try:\n                         dataset_len = len(dist_sampler.dataset)  # type: ignore\n                     except AttributeError:\n-                        warnings.warn(\"DistributedSampler's dataset does not have length defined. When \"\n-                                      '`drop_last=False`, metrics may be incorrect, as DistributedSampler '\n-                                      'duplicates samples to make the dataset divisible by world size. To '\n-                                      'fix this, provide a dataset with a length attribute to the '\n-                                      'DistributedSampler to correctly drop duplicate samples.')\n+                        warnings.warn(\n+                            \"DistributedSampler's dataset does not have length defined. When \"\n+                            \"`drop_last=False`, metrics may be incorrect, as DistributedSampler \"\n+                            \"duplicates samples to make the dataset divisible by world size. To \"\n+                            \"fix this, provide a dataset with a length attribute to the \"\n+                            \"DistributedSampler to correctly drop duplicate samples.\"\n+                        )\n \n             for self.state.batch in self._iter_dataloader(TrainerMode.EVAL):\n                 self.state.batch = self.state.device.batch_to_device(self.state.batch)\n@@ -2861,14 +3498,25 @@ class Trainer:\n                 rank_num_tokens = data_spec.get_num_tokens_in_batch(self.state.batch)\n \n                 # If using a distributed sampler, keep track of last_batch for metrics update\n-                if dist_sampler is not None and drop_last == False and dataset_len is not None:\n-                    batch_num_samples_tensor = self.state.device.tensor_to_device(torch.tensor(rank_num_samples))\n-                    dist.all_reduce(batch_num_samples_tensor, reduce_operation='SUM')\n+                if (\n+                    dist_sampler is not None\n+                    and drop_last == False\n+                    and dataset_len is not None\n+                ):\n+                    batch_num_samples_tensor = self.state.device.tensor_to_device(\n+                        torch.tensor(rank_num_samples)\n+                    )\n+                    dist.all_reduce(batch_num_samples_tensor, reduce_operation=\"SUM\")\n                     batch_num_samples = batch_num_samples_tensor.item()\n-                    last_batch = self.state.eval_timestamp.sample + batch_num_samples >= dataset_len\n+                    last_batch = (\n+                        self.state.eval_timestamp.sample + batch_num_samples\n+                        >= dataset_len\n+                    )\n \n                 if self.state.deepspeed_enabled:\n-                    self.state.batch = _fix_batch_precision_for_deepspeed(self.state.batch, self.state.precision)\n+                    self.state.batch = _fix_batch_precision_for_deepspeed(\n+                        self.state.batch, self.state.precision\n+                    )\n \n                 self.engine.run_event(Event.EVAL_BATCH_START)\n \n@@ -2879,30 +3527,52 @@ class Trainer:\n                     # Note: We use uint8 instead of bool as BOR is not supported on all torch.distributed backends\n                     found_cuda_oom = 0\n                     try:\n-                        microbatches = data_spec.split_batch(device_batch, evaluator.device_eval_microbatch_size)\n+                        microbatches = data_spec.split_batch(\n+                            device_batch, evaluator.device_eval_microbatch_size\n+                        )\n                         for i, self.state.batch in enumerate(microbatches):\n                             last_microbatch = i == len(microbatches) - 1\n                             skip_metric_update = False\n                             # Distributed samplers pad batches to be the same size. If using a\n                             # distributed sampler and on last batch, remove the padding\n-                            if dist_sampler is not None and drop_last == False and dataset_len is not None and last_batch and last_microbatch:\n+                            if (\n+                                dist_sampler is not None\n+                                and drop_last == False\n+                                and dataset_len is not None\n+                                and last_batch\n+                                and last_microbatch\n+                            ):\n                                 padding = dist_sampler.total_size - dataset_len\n-                                if dist.get_global_rank() >= dist.get_world_size() - padding:\n+                                if (\n+                                    dist.get_global_rank()\n+                                    >= dist.get_world_size() - padding\n+                                ):\n                                     rank_num_samples -= 1\n-                                    num_samples_in_microbatch = data_spec.get_num_samples_in_batch(self.state.batch)\n+                                    num_samples_in_microbatch = (\n+                                        data_spec.get_num_samples_in_batch(\n+                                            self.state.batch\n+                                        )\n+                                    )\n                                     # Skip updating metric if batch is only padded samples\n                                     if num_samples_in_microbatch == 1:\n                                         skip_metric_update = True\n                                     # Remove padded samples from batch\n                                     else:\n-                                        self.state.batch = data_spec.split_batch(self.state.batch,\n-                                                                                 num_samples_in_microbatch - 1)[0]\n+                                        self.state.batch = data_spec.split_batch(\n+                                            self.state.batch,\n+                                            num_samples_in_microbatch - 1,\n+                                        )[0]\n \n                             self.engine.run_event(Event.EVAL_BEFORE_FORWARD)\n \n-                            with _get_precision_context(self.state.precision, self.state.precision_config,\n-                                                        self.state.deepspeed_enabled):\n-                                self.state.outputs = self._original_model.eval_forward(self.state.batch)\n+                            with _get_precision_context(\n+                                self.state.precision,\n+                                self.state.precision_config,\n+                                self.state.deepspeed_enabled,\n+                            ):\n+                                self.state.outputs = self._original_model.eval_forward(\n+                                    self.state.batch\n+                                )\n \n                             self.engine.run_event(Event.EVAL_AFTER_FORWARD)\n \n@@ -2912,8 +3582,11 @@ class Trainer:\n                                 continue\n \n                             # Run in same precision context to avoid NaNs\n-                            with _get_precision_context(self.state.precision, self.state.precision_config,\n-                                                        self.state.deepspeed_enabled):\n+                            with _get_precision_context(\n+                                self.state.precision,\n+                                self.state.precision_config,\n+                                self.state.deepspeed_enabled,\n+                            ):\n                                 if isinstance(self.state.device, DeviceMPS):\n                                     # torchmetrics math has numerical errors on M1 devices\n                                     # running the compute on CPU instead\n@@ -2940,37 +3613,45 @@ class Trainer:\n                         if evaluator.auto_microbatching and _is_cuda_oom(e):\n                             log.debug((f\"Rank {dist.get_global_rank()} OOM'd.\"))\n                             found_cuda_oom = 1\n-                        elif self.state.auto_microbatching and ('cuda' in str(e).lower() or 'c10' in str(e).lower()):\n+                        elif self.state.auto_microbatching and (\n+                            \"cuda\" in str(e).lower() or \"c10\" in str(e).lower()\n+                        ):\n                             raise ValueError(\n                                 textwrap.dedent(\n-                                    'Encountered non-addressable cuda error while using auto microbatching. '\n-                                    'If this repeatedly occurs, set `device_eval_microbatch_size` manually.')) from e\n+                                    \"Encountered non-addressable cuda error while using auto microbatching. \"\n+                                    \"If this repeatedly occurs, set `device_eval_microbatch_size` manually.\"\n+                                )\n+                            ) from e\n                         else:\n                             raise\n                     if evaluator.auto_microbatching:\n                         # Propagate across all ranks if any rank hit CUDA OOM\n                         found_cuda_oom = self.state.device.tensor_to_device(\n-                            torch.tensor([found_cuda_oom], dtype=torch.uint8))\n-                        dist.all_reduce(found_cuda_oom, reduce_operation='MAX')\n+                            torch.tensor([found_cuda_oom], dtype=torch.uint8)\n+                        )\n+                        dist.all_reduce(found_cuda_oom, reduce_operation=\"MAX\")\n                         if found_cuda_oom.item() == 1:\n                             _adjust_device_eval_microbatch_size(evaluator)\n                             # Skip return and rerun after handling oom\n                             continue\n                         # Log device_eval_microbatch_size if auto_microbatching is enabled\n-                        self.logger.log_metrics({\n-                            f'trainer/{evaluator.label}/device_eval_microbatch_size':\n-                                evaluator.device_eval_microbatch_size\n-                        })\n+                        self.logger.log_metrics(\n+                            {\n+                                f\"trainer/{evaluator.label}/device_eval_microbatch_size\": evaluator.device_eval_microbatch_size\n+                            }\n+                        )\n                     # Break if we've successfully completed eval without OOMing.\n                     break\n \n                 now = datetime.datetime.now()\n                 batch_time = now - last_wct\n \n-                total_num_samples, total_num_tokens, batch_time = self._accumulate_time_across_ranks(\n-                    num_samples=rank_num_samples,\n-                    num_tokens=rank_num_tokens,\n-                    batch_time=batch_time,\n+                total_num_samples, total_num_tokens, batch_time = (\n+                    self._accumulate_time_across_ranks(\n+                        num_samples=rank_num_samples,\n+                        num_tokens=rank_num_tokens,\n+                        batch_time=batch_time,\n+                    )\n                 )\n \n                 self.state.eval_timestamp = self.state.eval_timestamp.to_next_batch(\n@@ -2983,7 +3664,9 @@ class Trainer:\n \n                 self.engine.run_event(Event.EVAL_BATCH_END)\n \n-            self._compute_and_log_metrics(dataloader_label=evaluator.label, metrics=metrics)\n+            self._compute_and_log_metrics(\n+                dataloader_label=evaluator.label, metrics=metrics\n+            )\n \n             self.engine.run_event(Event.EVAL_END)\n \n@@ -2991,7 +3674,9 @@ class Trainer:\n         if original_num_batches is not None:\n             self.state.dataloader_len = original_num_batches\n \n-    def _use_grad_scaling(self, precision: Union[str, Precision], scaler: Optional[GradScaler]) -> bool:\n+    def _use_grad_scaling(\n+        self, precision: Union[str, Precision], scaler: Optional[GradScaler]\n+    ) -> bool:\n         \"\"\"Determines based on precision when to use grad scaling.\n \n         By default, the pytorch GradScaler is a no-op if running on\n@@ -3014,8 +3699,10 @@ class Trainer:\n         use_grad_scaling = precision == Precision.AMP_FP16\n \n         if use_grad_scaling and (scaler is None or not scaler.is_enabled()):\n-            raise RuntimeError(f'Attempting to use grad scaling with {precision}, but scaler is not enabled.'\n-                               f'Potentially your hardware does not support Precision {precision}.')\n+            raise RuntimeError(\n+                f\"Attempting to use grad scaling with {precision}, but scaler is not enabled.\"\n+                f\"Potentially your hardware does not support Precision {precision}.\"\n+            )\n         return use_grad_scaling\n \n     def _iter_dataloader(self, trainer_mode: TrainerMode):\n@@ -3027,12 +3714,16 @@ class Trainer:\n         Args:\n             trainer_mode (TrainerMode): Specifies which mode the trainer is in.\n         \"\"\"\n-        assert self.state.dataloader is not None, 'the dataloader should be set before calling this method'\n+        assert self.state.dataloader is not None, (\n+            \"the dataloader should be set before calling this method\"\n+        )\n \n         if self.state.dataloader_len is None:\n             dataloader_iter = iter(self.state.dataloader)\n         else:\n-            dataloader_iter = itertools.islice(self.state.dataloader, int(self.state.dataloader_len))\n+            dataloader_iter = itertools.islice(\n+                self.state.dataloader, int(self.state.dataloader_len)\n+            )\n \n         while True:\n             try:\n@@ -3068,15 +3759,18 @@ class Trainer:\n             return True\n \n         if self.state.optimizers is None:\n-            raise RuntimeError('state.optimizers must be set before `_use_closures` can be determined')\n+            raise RuntimeError(\n+                \"state.optimizers must be set before `_use_closures` can be determined\"\n+            )\n \n         return all(\n-            getattr(optimizer, '_step_supports_amp_closure', False)\n-            for optimizer in ensure_tuple(self.state.optimizers))\n+            getattr(optimizer, \"_step_supports_amp_closure\", False)\n+            for optimizer in ensure_tuple(self.state.optimizers)\n+        )\n \n     def save_checkpoint(\n         self,\n-        name: str = 'ep{epoch}-ba{batch}-rank{rank}',\n+        name: str = \"ep{epoch}-ba{batch}-rank{rank}\",\n         *,\n         weights_only: bool = False,\n     ):\n@@ -3106,7 +3800,8 @@ class Trainer:\n         \"\"\"\n         if self._checkpoint_saver is None:\n             raise ValueError(\n-                'In order to use save_checkpoint_to_save_folder you must pass a save_folder to the Trainer.')\n+                \"In order to use save_checkpoint_to_save_folder you must pass a save_folder to the Trainer.\"\n+            )\n         else:\n             self._checkpoint_saver._save_checkpoint(self.state, self.logger)\n \n@@ -3145,17 +3840,23 @@ class Trainer:\n         Returns:\n             None\n         \"\"\"\n-        export_model = self.state.model.module if self.state.is_model_ddp else self.state.model\n+        export_model = (\n+            self.state.model.module if self.state.is_model_ddp else self.state.model\n+        )\n         if not isinstance(export_model, nn.Module):\n-            raise ValueError(f'Exporting Model requires type torch.nn.Module, got {type(export_model)}')\n-        if sample_input == None and save_format == 'onnx':\n+            raise ValueError(\n+                f\"Exporting Model requires type torch.nn.Module, got {type(export_model)}\"\n+            )\n+        if sample_input == None and save_format == \"onnx\":\n             sample_input = self.state.batch\n-        export_with_logger(model=export_model,\n-                           save_format=save_format,\n-                           save_path=save_path,\n-                           logger=self.logger,\n-                           save_object_store=save_object_store,\n-                           sample_input=(sample_input, {}),\n-                           transforms=transforms,\n-                           input_names=input_names,\n-                           output_names=output_names)\n+        export_with_logger(\n+            model=export_model,\n+            save_format=save_format,\n+            save_path=save_path,\n+            logger=self.logger,\n+            save_object_store=save_object_store,\n+            sample_input=(sample_input, {}),\n+            transforms=transforms,\n+            input_names=input_names,\n+            output_names=output_names,\n+        )\ndiff --git a/tests/conftest.py b/tests/conftest.py\nindex 5cde3e4..2d1643a 100644\n--- a/tests/conftest.py\n+++ b/tests/conftest.py\n@@ -19,14 +19,16 @@ reproducibility.configure_deterministic_mode()\n \n # Add the path of any pytest fixture files you want to make global\n pytest_plugins = [\n-    'tests.fixtures.autouse_fixtures',\n-    'tests.fixtures.fixtures',\n+    \"tests.fixtures.autouse_fixtures\",\n+    \"tests.fixtures.fixtures\",\n ]\n \n \n-def _add_option(parser: pytest.Parser, name: str, help: str, choices: Optional[List[str]] = None):\n+def _add_option(\n+    parser: pytest.Parser, name: str, help: str, choices: Optional[List[str]] = None\n+):\n     parser.addoption(\n-        f'--{name}',\n+        f\"--{name}\",\n         default=None,\n         type=str,\n         choices=choices,\n@@ -35,7 +37,7 @@ def _add_option(parser: pytest.Parser, name: str, help: str, choices: Optional[L\n     parser.addini(\n         name=name,\n         help=help,\n-        type='string',\n+        type=\"string\",\n         default=None,\n     )\n \n@@ -50,31 +52,35 @@ def _get_option(config: pytest.Config, name: str, default: Optional[str] = None)\n         val = None\n     if val is None:\n         if default is None:\n-            pytest.fail(f'Config option {name} is not specified but is required')\n+            pytest.fail(f\"Config option {name} is not specified but is required\")\n         val = default\n     assert isinstance(val, str)\n     return val\n \n \n def pytest_addoption(parser: pytest.Parser) -> None:\n-    _add_option(parser,\n-                'seed',\n-                help=\"\"\"\\\n+    _add_option(\n+        parser,\n+        \"seed\",\n+        help=\"\"\"\\\n         Rank zero seed to use. `reproducibility.seed_all(seed + dist.get_global_rank())` will be invoked\n-        before each test.\"\"\")\n-    _add_option(parser, 's3_bucket', help='S3 Bucket for integration tests')\n+        before each test.\"\"\",\n+    )\n+    _add_option(parser, \"s3_bucket\", help=\"S3 Bucket for integration tests\")\n \n \n def _get_world_size(item: pytest.Item):\n     \"\"\"Returns the world_size of a test, defaults to 1.\"\"\"\n     _default = pytest.mark.world_size(1).mark\n-    return item.get_closest_marker('world_size', default=_default).args[0]\n+    return item.get_closest_marker(\"world_size\", default=_default).args[0]\n \n \n-def pytest_collection_modifyitems(config: pytest.Config, items: List[pytest.Item]) -> None:\n+def pytest_collection_modifyitems(\n+    config: pytest.Config, items: List[pytest.Item]\n+) -> None:\n     \"\"\"Filter tests by world_size (for multi-GPU tests) and duration (short, long, or all)\"\"\"\n \n-    world_size = int(os.environ.get('WORLD_SIZE', '1'))\n+    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n \n     conditions = [\n         lambda item: _get_world_size(item) == world_size,\n@@ -103,16 +109,25 @@ def pytest_collection_modifyitems(config: pytest.Config, items: List[pytest.Item\n def pytest_configure():\n     try:\n         import transformers\n+\n         del transformers\n         TRANSFORMERS_INSTALLED = True\n     except ImportError:\n         TRANSFORMERS_INSTALLED = False\n \n     if TRANSFORMERS_INSTALLED:\n-        from tests.fixtures.fixtures import (tiny_bert_config_helper, tiny_bert_model_helper,\n-                                             tiny_bert_tokenizer_helper, tiny_gpt2_config_helper,\n-                                             tiny_gpt2_model_helper, tiny_gpt2_tokenizer_helper, tiny_t5_config_helper,\n-                                             tiny_t5_model_helper, tiny_t5_tokenizer_helper)\n+        from tests.fixtures.fixtures import (\n+            tiny_bert_config_helper,\n+            tiny_bert_model_helper,\n+            tiny_bert_tokenizer_helper,\n+            tiny_gpt2_config_helper,\n+            tiny_gpt2_model_helper,\n+            tiny_gpt2_tokenizer_helper,\n+            tiny_t5_config_helper,\n+            tiny_t5_model_helper,\n+            tiny_t5_tokenizer_helper,\n+        )\n+\n         pytest.tiny_bert_config = tiny_bert_config_helper()  # type: ignore\n         pytest.tiny_bert_model = tiny_bert_model_helper(pytest.tiny_bert_config)  # type: ignore\n         pytest.tiny_bert_tokenizer = tiny_bert_tokenizer_helper()  # type: ignore\n"
    },
    {
        "id": 37,
        "sha_fail": "fd461e9133f1d191e3db194745f2306cde1772b6",
        "diff": "diff --git a/river/tree/nodes/sgt_nodes.py b/river/tree/nodes/sgt_nodes.py\nindex e5a0542..12aa073 100644\n--- a/river/tree/nodes/sgt_nodes.py\n+++ b/river/tree/nodes/sgt_nodes.py\n@@ -42,9 +42,10 @@ class SGTLeaf(Leaf):\n         )\n         self.last_split_attempt_at = 0\n \n-        self._split_stats: dict[\n-            FeatureName, dict[Hashable, GradHessStats] | DynamicQuantizer | StaticQuantizer\n-        ] | None = {}\n+        self._split_stats: (\n+            dict[FeatureName, dict[Hashable, GradHessStats] | DynamicQuantizer | StaticQuantizer]\n+            | None\n+        ) = {}\n         self._update_stats = GradHessStats()\n \n     def reset(self):\n"
    },
    {
        "id": 38,
        "sha_fail": "aa8a42bcf03f3b89575a9cce2f8af715a5121c59",
        "diff": "diff --git a/tests/client/test_redirects.py b/tests/client/test_redirects.py\nindex f658271..9d2c473 100644\n--- a/tests/client/test_redirects.py\n+++ b/tests/client/test_redirects.py\n@@ -435,13 +435,13 @@ def test_redirect_custom_scheme():\n     client = httpx.Client(transport=httpx.MockTransport(redirects))\n     with pytest.raises(httpx.UnsupportedProtocol) as e:\n         client.post(\"https://example.org/redirect_custom_scheme\", follow_redirects=True)\n-    assert str(e.value) == \"Scheme 'market' not supported.\"\n+    assert str(e.value) == \"Scheme 'https' not supported.\"\n \n \n @pytest.mark.anyio\n async def test_async_invalid_redirect():\n     async with httpx.AsyncClient(transport=httpx.MockTransport(redirects)) as client:\n-        with pytest.raises(httpx.RemoteProtocolError):\n+        with pytest.raises(httpx.UnsupportedProtocol):\n             await client.get(\n                 \"http://example.org/invalid_redirect\", follow_redirects=True\n             )\n"
    },
    {
        "id": 39,
        "sha_fail": "83b5e4bf130d204fbb25b26a341c62aee4fc2d0f",
        "diff": "diff --git a/httpx/_config.py b/httpx/_config.py\nindex 69c3c6f..1af8a45 100644\n--- a/httpx/_config.py\n+++ b/httpx/_config.py\n@@ -12,7 +12,6 @@ from ._types import CertTypes, HeaderTypes, TimeoutTypes, URLTypes, VerifyTypes\n from ._urls import URL\n from ._utils import get_ca_bundle_from_env\n \n-\n SOCKET_OPTION = typing.Union[\n     typing.Tuple[int, int, int],\n     typing.Tuple[int, int, typing.Union[bytes, bytearray]],\ndiff --git a/httpx/_transports/default.py b/httpx/_transports/default.py\nindex 7802026..4ad3460 100644\n--- a/httpx/_transports/default.py\n+++ b/httpx/_transports/default.py\n@@ -23,6 +23,7 @@ client = httpx.Client(transport=transport)\n transport = httpx.HTTPTransport(uds=\"socket.uds\")\n client = httpx.Client(transport=transport)\n \"\"\"\n+\n import contextlib\n import typing\n from types import TracebackType\n@@ -32,9 +33,9 @@ import httpcore\n from .._config import (\n     DEFAULT_LIMITS,\n     DEFAULT_NETWORK_OPTIONS,\n-    Proxy,\n     Limits,\n     NetworkOptions,\n+    Proxy,\n     create_ssl_context,\n )\n from .._exceptions import (\n"
    },
    {
        "id": 40,
        "sha_fail": "1afe2c9cb192d3760d59190cc7892e7ac37d5e27",
        "diff": "diff --git a/httpx/_transports/default.py b/httpx/_transports/default.py\nindex 5d5a841..9c29e1a 100644\n--- a/httpx/_transports/default.py\n+++ b/httpx/_transports/default.py\n@@ -23,6 +23,7 @@ client = httpx.Client(transport=transport)\n transport = httpx.HTTPTransport(uds=\"socket.uds\")\n client = httpx.Client(transport=transport)\n \"\"\"\n+\n import contextlib\n import ssl\n import typing\n"
    },
    {
        "id": 42,
        "sha_fail": "897a5deb406b53ea2f4675cdf0c2f1fa93fc6238",
        "diff": "diff --git a/httpx/__init__.py b/httpx/__init__.py\nindex bfb1867..23b9f31 100644\n--- a/httpx/__init__.py\n+++ b/httpx/__init__.py\n@@ -2,7 +2,7 @@ from .__version__ import __description__, __title__, __version__\n from ._api import delete, get, head, options, patch, post, put, request, stream\n from ._auth import Auth, BasicAuth, DigestAuth, NetRCAuth\n from ._client import USE_CLIENT_DEFAULT, AsyncClient, Client\n-from ._config import Limits, Proxy, Timeout, SSLContext\n+from ._config import Limits, Proxy, SSLContext, Timeout\n from ._content import ByteStream\n from ._exceptions import (\n     CloseError,\n"
    },
    {
        "id": 43,
        "sha_fail": "077f6aaac3ebb96626ac747fb126a0b4d752489c",
        "diff": "diff --git a/wandb/integration/ultralytics/bbox_utils.py b/wandb/integration/ultralytics/bbox_utils.py\nindex 798aa573..9cee9157 100644\n--- a/wandb/integration/ultralytics/bbox_utils.py\n+++ b/wandb/integration/ultralytics/bbox_utils.py\n@@ -3,6 +3,7 @@ from typing import Any, Dict, List, Optional, Tuple, Union\n import torch\n from ultralytics.engine.results import Results\n from ultralytics.models.yolo.detect import DetectionPredictor\n+\n try:\n     from ultralytics.yolo.utils import ops\n except ModuleNotFoundError:\ndiff --git a/wandb/integration/ultralytics/callback.py b/wandb/integration/ultralytics/callback.py\nindex ed8a2246..618fc5af 100644\n--- a/wandb/integration/ultralytics/callback.py\n+++ b/wandb/integration/ultralytics/callback.py\n@@ -42,6 +42,7 @@ try:\n         SegmentationValidator,\n     )\n     from ultralytics.utils.torch_utils import de_parallel\n+\n     try:\n         from ultralytics.yolo.utils import RANK, __version__\n     except ModuleNotFoundError:\n"
    },
    {
        "id": 44,
        "sha_fail": "c99ead9542bde331497f2456537fdbb0e37706d0",
        "diff": "diff --git a/wandb/sdk/data_types/image.py b/wandb/sdk/data_types/image.py\nindex 977251b4..67a9aa6f 100644\n--- a/wandb/sdk/data_types/image.py\n+++ b/wandb/sdk/data_types/image.py\n@@ -277,7 +277,7 @@ class Image(BatchableMedia):\n         )\n         if util.is_matplotlib_typename(util.get_full_typename(data)):\n             buf = BytesIO()\n-            util.ensure_matplotlib_figure(data).savefig(buf, format='png')\n+            util.ensure_matplotlib_figure(data).savefig(buf, format=\"png\")\n             self._image = pil_image.open(buf, formats=[\"PNG\"])\n         elif isinstance(data, pil_image.Image):\n             self._image = data\n"
    },
    {
        "id": 45,
        "sha_fail": "99ad8a351bb884f1e398c1d85c62d6b6e0bdd67e",
        "diff": "diff --git a/packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py b/packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py\nindex c00ad1b89..4847ce83c 100644\n--- a/packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py\n+++ b/packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py\n@@ -15,7 +15,7 @@\n #\n from __future__ import annotations\n \n-from typing import MutableMapping, MutableSequence\n+from typing import MutableSequence\n \n from google.protobuf import timestamp_pb2  # type: ignore\n import proto  # type: ignore\n@@ -280,6 +280,7 @@ class Condition(proto.Message):\n                 Supported by string only when ``CustomMetadata`` value type\n                 for the given key has a ``string_list_value``.\n         \"\"\"\n+\n         OPERATOR_UNSPECIFIED = 0\n         LESS = 1\n         LESS_EQUAL = 2\n@@ -352,6 +353,7 @@ class Chunk(proto.Message):\n             STATE_FAILED (10):\n                 ``Chunk`` failed processing.\n         \"\"\"\n+\n         STATE_UNSPECIFIED = 0\n         STATE_PENDING_PROCESSING = 1\n         STATE_ACTIVE = 2\n"
    },
    {
        "id": 47,
        "sha_fail": "3dd8e4404a0ce2e29db4911dc2cd7e94755be631",
        "diff": "diff --git a/tests/deepspeed/test_deepspeed.py b/tests/deepspeed/test_deepspeed.py\nindex 035965c..75b34f7 100644\n--- a/tests/deepspeed/test_deepspeed.py\n+++ b/tests/deepspeed/test_deepspeed.py\n@@ -36,9 +36,9 @@ from accelerate.test_utils.testing import (\n     AccelerateTestCase,\n     TempDirTestCase,\n     execute_subprocess_async,\n-    require_non_cpu,\n     require_deepspeed,\n     require_multi_device,\n+    require_non_cpu,\n     slow,\n )\n from accelerate.test_utils.training import RegressionDataset\n@@ -693,9 +693,9 @@ class DeepSpeedConfigIntegration(AccelerateTestCase):\n \n     def test_ds_config_assertions(self):\n         ambiguous_env = self.dist_env.copy()\n-        ambiguous_env[\n-            \"ACCELERATE_CONFIG_DS_FIELDS\"\n-        ] = \"gradient_accumulation_steps,gradient_clipping,zero_stage,offload_optimizer_device,offload_param_device,zero3_save_16bit_model,mixed_precision\"\n+        ambiguous_env[\"ACCELERATE_CONFIG_DS_FIELDS\"] = (\n+            \"gradient_accumulation_steps,gradient_clipping,zero_stage,offload_optimizer_device,offload_param_device,zero3_save_16bit_model,mixed_precision\"\n+        )\n \n         with mockenv_context(**ambiguous_env):\n             with self.assertRaises(ValueError) as cm:\n@@ -908,8 +908,8 @@ class DeepSpeedIntegrationTest(TempDirTestCase):\n                     ]\n                 )\n                 for i in range(3):\n-                    if f\"stage_{i+1}\" in spec:\n-                        cmd_stage.extend([f\"--zero_stage={i+1}\"])\n+                    if f\"stage_{i + 1}\" in spec:\n+                        cmd_stage.extend([f\"--zero_stage={i + 1}\"])\n                         break\n                 cmd_stage.extend(\n                     [\ndiff --git a/tests/fsdp/test_fsdp.py b/tests/fsdp/test_fsdp.py\nindex d939991..a4b6f05 100644\n--- a/tests/fsdp/test_fsdp.py\n+++ b/tests/fsdp/test_fsdp.py\n@@ -28,9 +28,9 @@ from accelerate.test_utils.testing import (\n     AccelerateTestCase,\n     TempDirTestCase,\n     execute_subprocess_async,\n-    require_non_cpu,\n     require_fsdp,\n     require_multi_device,\n+    require_non_cpu,\n     slow,\n )\n from accelerate.utils.constants import (\n@@ -201,7 +201,7 @@ class FSDPIntegrationTest(TempDirTestCase):\n             cmd_config = cmd.copy()\n             for i, strategy in enumerate(FSDP_SHARDING_STRATEGY):\n                 if strategy.lower() in config:\n-                    cmd_config.append(f\"--fsdp_sharding_strategy={i+1}\")\n+                    cmd_config.append(f\"--fsdp_sharding_strategy={i + 1}\")\n                     break\n \n             if \"fp32\" in config:\n@@ -247,7 +247,7 @@ class FSDPIntegrationTest(TempDirTestCase):\n \n         for i, strategy in enumerate(FSDP_SHARDING_STRATEGY):\n             cmd_config = cmd.copy()\n-            cmd_config.append(f\"--fsdp_sharding_strategy={i+1}\")\n+            cmd_config.append(f\"--fsdp_sharding_strategy={i + 1}\")\n             if strategy != \"FULL_SHARD\":\n                 continue\n             state_dict_config_index = len(cmd_config)\n@@ -301,7 +301,7 @@ class FSDPIntegrationTest(TempDirTestCase):\n                 cmd_config.extend([\"--use_fsdp\"])\n                 for i, strategy in enumerate(FSDP_SHARDING_STRATEGY):\n                     if strategy.lower() in spec:\n-                        cmd_config.append(f\"--fsdp_sharding_strategy={i+1}\")\n+                        cmd_config.append(f\"--fsdp_sharding_strategy={i + 1}\")\n                         break\n \n                 if \"cpu_offload\" in spec:\n"
    },
    {
        "id": 48,
        "sha_fail": "028ad1efee2c41691d78e5a4de90ebd6f8236cad",
        "diff": "diff --git a/src/accelerate/utils/__init__.py b/src/accelerate/utils/__init__.py\nindex ddf794a..7179afb 100644\n--- a/src/accelerate/utils/__init__.py\n+++ b/src/accelerate/utils/__init__.py\n@@ -59,7 +59,6 @@ from .imports import (\n     is_comet_ml_available,\n     is_cuda_available,\n     is_datasets_available,\n-    is_peft_available,\n     is_deepspeed_available,\n     is_dvclive_available,\n     is_fp8_available,\n@@ -70,6 +69,7 @@ from .imports import (\n     is_msamp_available,\n     is_npu_available,\n     is_pandas_available,\n+    is_peft_available,\n     is_rich_available,\n     is_sagemaker_available,\n     is_tensorboard_available,\n@@ -81,7 +81,6 @@ from .imports import (\n     is_xpu_available,\n )\n from .modeling import (\n-    is_peft_model,\n     calculate_maximum_sizes,\n     check_device_map,\n     check_tied_parameters_in_config,\n@@ -96,6 +95,7 @@ from .modeling import (\n     get_mixed_precision_context_manager,\n     id_tensor_storage,\n     infer_auto_device_map,\n+    is_peft_model,\n     load_checkpoint_in_model,\n     load_offloaded_weights,\n     load_state_dict,\ndiff --git a/src/accelerate/utils/modeling.py b/src/accelerate/utils/modeling.py\nindex 802b13c..03d3a39 100644\n--- a/src/accelerate/utils/modeling.py\n+++ b/src/accelerate/utils/modeling.py\n@@ -30,7 +30,7 @@ import torch.nn as nn\n from ..state import AcceleratorState\n from .constants import SAFE_WEIGHTS_NAME, WEIGHTS_NAME\n from .dataclasses import AutocastKwargs, CustomDtype, DistributedType\n-from .imports import is_mps_available, is_npu_available, is_xpu_available, is_peft_available\n+from .imports import is_mps_available, is_npu_available, is_peft_available, is_xpu_available\n from .offload import load_offloaded_weight, offload_weight, save_offload_index\n from .tqdm import is_tqdm_available, tqdm\n \n"
    },
    {
        "id": 49,
        "sha_fail": "c9991372b81edabb86965638db110ab930f8e165",
        "diff": "diff --git a/src/accelerate/utils/fsdp_utils.py b/src/accelerate/utils/fsdp_utils.py\nindex 35126fc..a638825 100644\n--- a/src/accelerate/utils/fsdp_utils.py\n+++ b/src/accelerate/utils/fsdp_utils.py\n@@ -17,7 +17,7 @@ import torch\n \n from ..logging import get_logger\n from .constants import FSDP_MODEL_NAME, FSDP_PYTORCH_VERSION, OPTIMIZER_NAME\n-from .imports import is_torch_distributed_available, is_peft_available\n+from .imports import is_peft_available, is_torch_distributed_available\n from .other import extract_model_from_parallel\n from .versions import is_torch_version\n \n"
    },
    {
        "id": 50,
        "sha_fail": "16a0c04d06205527ec5e379df2596b399ee5dadc",
        "diff": "diff --git a/dask/utils.py b/dask/utils.py\nindex 80ea6e8..4a0d776 100644\n--- a/dask/utils.py\n+++ b/dask/utils.py\n@@ -148,7 +148,7 @@ def _deprecated_kwarg(\n     new_arg_name: str | None = None,\n     mapping: Mapping[Any, Any] | Callable[[Any], Any] | None = None,\n     stacklevel: int = 2,\n-    comment: str | None = None\n+    comment: str | None = None,\n ) -> Callable[[F], F]:\n     \"\"\"\n     Decorator to deprecate a keyword argument of a function.\n@@ -1832,15 +1832,13 @@ timedelta_sizes.update({k.upper(): v for k, v in timedelta_sizes.items()})\n \n \n @overload\n-def parse_timedelta(s: None, default: str | Literal[False] = \"seconds\") -> None:\n-    ...\n+def parse_timedelta(s: None, default: str | Literal[False] = \"seconds\") -> None: ...\n \n \n @overload\n def parse_timedelta(\n     s: str | float | timedelta, default: str | Literal[False] = \"seconds\"\n-) -> float:\n-    ...\n+) -> float: ...\n \n \n def parse_timedelta(s, default=\"seconds\"):\n"
    }
]