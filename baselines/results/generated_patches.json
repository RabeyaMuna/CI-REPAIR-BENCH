[
    {
        "id": 1,
        "sha_fail": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
        "diff": "diff --git a/examples/community/ip_adapter_face_id.py b/examples/community/ip_adapter_face_id.py\nindex e3c5a2c..6d99b66 100644\n--- a/examples/community/ip_adapter_face_id.py\n+++ b/examples/community/ip_adapter_face_id.py\n@@ -14,33 +14,47 @@\n \n import inspect\n from typing import Any, Callable, Dict, List, Optional, Union\n-from safetensors import safe_open\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from packaging import version\n-from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n+from safetensors import safe_open\n+from transformers import (\n+    CLIPImageProcessor,\n+    CLIPTextModel,\n+    CLIPTokenizer,\n+    CLIPVisionModelWithProjection,\n+)\n \n from diffusers.configuration_utils import FrozenDict\n from diffusers.image_processor import VaeImageProcessor\n-from diffusers.loaders import FromSingleFileMixin, IPAdapterMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n+from diffusers.loaders import (\n+    FromSingleFileMixin,\n+    IPAdapterMixin,\n+    LoraLoaderMixin,\n+    TextualInversionLoaderMixin,\n+)\n from diffusers.models import AutoencoderKL, UNet2DConditionModel\n from diffusers.models.attention_processor import FusedAttnProcessor2_0\n-from diffusers.models.lora import adjust_lora_scale_text_encoder, LoRALinearLayer\n+from diffusers.models.lora import LoRALinearLayer, adjust_lora_scale_text_encoder\n+from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n+from diffusers.pipelines.stable_diffusion.pipeline_output import (\n+    StableDiffusionPipelineOutput,\n+)\n+from diffusers.pipelines.stable_diffusion.safety_checker import (\n+    StableDiffusionSafetyChecker,\n+)\n from diffusers.schedulers import KarrasDiffusionSchedulers\n from diffusers.utils import (\n-    _get_model_file,\n     USE_PEFT_BACKEND,\n+    _get_model_file,\n     deprecate,\n     logging,\n     scale_lora_layers,\n     unscale_lora_layers,\n )\n from diffusers.utils.torch_utils import randn_tensor\n-from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n-from diffusers.pipelines.stable_diffusion.pipeline_output import StableDiffusionPipelineOutput\n-from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -82,17 +96,27 @@ class LoRAIPAdapterAttnProcessor(nn.Module):\n         self.lora_scale = lora_scale\n \n         self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n-        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n-        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n-        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n+        self.to_k_lora = LoRALinearLayer(\n+            cross_attention_dim or hidden_size, hidden_size, rank, network_alpha\n+        )\n+        self.to_v_lora = LoRALinearLayer(\n+            cross_attention_dim or hidden_size, hidden_size, rank, network_alpha\n+        )\n+        self.to_out_lora = LoRALinearLayer(\n+            hidden_size, hidden_size, rank, network_alpha\n+        )\n \n         self.hidden_size = hidden_size\n         self.cross_attention_dim = cross_attention_dim\n         self.scale = scale\n         self.num_tokens = num_tokens\n \n-        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n-        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n+        self.to_k_ip = nn.Linear(\n+            cross_attention_dim or hidden_size, hidden_size, bias=False\n+        )\n+        self.to_v_ip = nn.Linear(\n+            cross_attention_dim or hidden_size, hidden_size, bias=False\n+        )\n \n     def __call__(\n         self,\n@@ -111,17 +135,27 @@ class LoRAIPAdapterAttnProcessor(nn.Module):\n \n         if input_ndim == 4:\n             batch_size, channel, height, width = hidden_states.shape\n-            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n+            hidden_states = hidden_states.view(\n+                batch_size, channel, height * width\n+            ).transpose(1, 2)\n \n         batch_size, sequence_length, _ = (\n-            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n+            hidden_states.shape\n+            if encoder_hidden_states is None\n+            else encoder_hidden_states.shape\n+        )\n+        attention_mask = attn.prepare_attention_mask(\n+            attention_mask, sequence_length, batch_size\n         )\n-        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n \n         if attn.group_norm is not None:\n-            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n+            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(\n+                1, 2\n+            )\n \n-        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n+        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(\n+            hidden_states\n+        )\n \n         if encoder_hidden_states is None:\n             encoder_hidden_states = hidden_states\n@@ -133,10 +167,16 @@ class LoRAIPAdapterAttnProcessor(nn.Module):\n                 encoder_hidden_states[:, end_pos:, :],\n             )\n             if attn.norm_cross:\n-                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n+                encoder_hidden_states = attn.norm_encoder_hidden_states(\n+                    encoder_hidden_states\n+                )\n \n-        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n-        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n+        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(\n+            encoder_hidden_states\n+        )\n+        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(\n+            encoder_hidden_states\n+        )\n \n         query = attn.head_to_batch_dim(query)\n         key = attn.head_to_batch_dim(key)\n@@ -160,12 +200,16 @@ class LoRAIPAdapterAttnProcessor(nn.Module):\n         hidden_states = hidden_states + self.scale * ip_hidden_states\n \n         # linear proj\n-        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n+        hidden_states = attn.to_out[0](\n+            hidden_states\n+        ) + self.lora_scale * self.to_out_lora(hidden_states)\n         # dropout\n         hidden_states = attn.to_out[1](hidden_states)\n \n         if input_ndim == 4:\n-            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n+            hidden_states = hidden_states.transpose(-1, -2).reshape(\n+                batch_size, channel, height, width\n+            )\n \n         if attn.residual_connection:\n             hidden_states = hidden_states + residual\n@@ -211,17 +255,27 @@ class LoRAIPAdapterAttnProcessor2_0(nn.Module):\n         self.lora_scale = lora_scale\n \n         self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n-        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n-        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n-        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n+        self.to_k_lora = LoRALinearLayer(\n+            cross_attention_dim or hidden_size, hidden_size, rank, network_alpha\n+        )\n+        self.to_v_lora = LoRALinearLayer(\n+            cross_attention_dim or hidden_size, hidden_size, rank, network_alpha\n+        )\n+        self.to_out_lora = LoRALinearLayer(\n+            hidden_size, hidden_size, rank, network_alpha\n+        )\n \n         self.hidden_size = hidden_size\n         self.cross_attention_dim = cross_attention_dim\n         self.scale = scale\n         self.num_tokens = num_tokens\n \n-        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n-        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n+        self.to_k_ip = nn.Linear(\n+            cross_attention_dim or hidden_size, hidden_size, bias=False\n+        )\n+        self.to_v_ip = nn.Linear(\n+            cross_attention_dim or hidden_size, hidden_size, bias=False\n+        )\n \n     def __call__(\n         self,\n@@ -240,22 +294,34 @@ class LoRAIPAdapterAttnProcessor2_0(nn.Module):\n \n         if input_ndim == 4:\n             batch_size, channel, height, width = hidden_states.shape\n-            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n+            hidden_states = hidden_states.view(\n+                batch_size, channel, height * width\n+            ).transpose(1, 2)\n \n         batch_size, sequence_length, _ = (\n-            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n+            hidden_states.shape\n+            if encoder_hidden_states is None\n+            else encoder_hidden_states.shape\n         )\n \n         if attention_mask is not None:\n-            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n+            attention_mask = attn.prepare_attention_mask(\n+                attention_mask, sequence_length, batch_size\n+            )\n             # scaled_dot_product_attention expects attention_mask shape to be\n             # (batch, heads, source_length, target_length)\n-            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n+            attention_mask = attention_mask.view(\n+                batch_size, attn.heads, -1, attention_mask.shape[-1]\n+            )\n \n         if attn.group_norm is not None:\n-            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n+            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(\n+                1, 2\n+            )\n \n-        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n+        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(\n+            hidden_states\n+        )\n \n         if encoder_hidden_states is None:\n             encoder_hidden_states = hidden_states\n@@ -267,10 +333,16 @@ class LoRAIPAdapterAttnProcessor2_0(nn.Module):\n                 encoder_hidden_states[:, end_pos:, :],\n             )\n             if attn.norm_cross:\n-                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n+                encoder_hidden_states = attn.norm_encoder_hidden_states(\n+                    encoder_hidden_states\n+                )\n \n-        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n-        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n+        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(\n+            encoder_hidden_states\n+        )\n+        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(\n+            encoder_hidden_states\n+        )\n \n         inner_dim = key.shape[-1]\n         head_dim = inner_dim // attn.heads\n@@ -285,7 +357,9 @@ class LoRAIPAdapterAttnProcessor2_0(nn.Module):\n             query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n         )\n \n-        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n+        hidden_states = hidden_states.transpose(1, 2).reshape(\n+            batch_size, -1, attn.heads * head_dim\n+        )\n         hidden_states = hidden_states.to(query.dtype)\n \n         # for ip-adapter\n@@ -301,18 +375,24 @@ class LoRAIPAdapterAttnProcessor2_0(nn.Module):\n             query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False\n         )\n \n-        ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n+        ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(\n+            batch_size, -1, attn.heads * head_dim\n+        )\n         ip_hidden_states = ip_hidden_states.to(query.dtype)\n \n         hidden_states = hidden_states + self.scale * ip_hidden_states\n \n         # linear proj\n-        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n+        hidden_states = attn.to_out[0](\n+            hidden_states\n+        ) + self.lora_scale * self.to_out_lora(hidden_states)\n         # dropout\n         hidden_states = attn.to_out[1](hidden_states)\n \n         if input_ndim == 4:\n-            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n+            hidden_states = hidden_states.transpose(-1, -2).reshape(\n+                batch_size, channel, height, width\n+            )\n \n         if attn.residual_connection:\n             hidden_states = hidden_states + residual\n@@ -323,13 +403,20 @@ class LoRAIPAdapterAttnProcessor2_0(nn.Module):\n \n \n class IPAdapterFullImageProjection(nn.Module):\n-    def __init__(self, image_embed_dim=1024, cross_attention_dim=1024, mult=1, num_tokens=1):\n+    def __init__(\n+        self, image_embed_dim=1024, cross_attention_dim=1024, mult=1, num_tokens=1\n+    ):\n         super().__init__()\n         from diffusers.models.attention import FeedForward\n \n         self.num_tokens = num_tokens\n         self.cross_attention_dim = cross_attention_dim\n-        self.ff = FeedForward(image_embed_dim, cross_attention_dim * num_tokens, mult=mult, activation_fn=\"gelu\")\n+        self.ff = FeedForward(\n+            image_embed_dim,\n+            cross_attention_dim * num_tokens,\n+            mult=mult,\n+            activation_fn=\"gelu\",\n+        )\n         self.norm = nn.LayerNorm(cross_attention_dim)\n \n     def forward(self, image_embeds: torch.FloatTensor):\n@@ -343,12 +430,16 @@ def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):\n     Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and\n     Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4\n     \"\"\"\n-    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)), keepdim=True)\n+    std_text = noise_pred_text.std(\n+        dim=list(range(1, noise_pred_text.ndim)), keepdim=True\n+    )\n     std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)\n     # rescale the results from guidance (fixes overexposure)\n     noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n     # mix with the original results from guidance by factor guidance_rescale to avoid \"plain looking\" images\n-    noise_cfg = guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n+    noise_cfg = (\n+        guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n+    )\n     return noise_cfg\n \n \n@@ -381,7 +472,9 @@ def retrieve_timesteps(\n         second element is the number of inference steps.\n     \"\"\"\n     if timesteps is not None:\n-        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        accepts_timesteps = \"timesteps\" in set(\n+            inspect.signature(scheduler.set_timesteps).parameters.keys()\n+        )\n         if not accepts_timesteps:\n             raise ValueError(\n                 f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n@@ -397,7 +490,11 @@ def retrieve_timesteps(\n \n \n class IPAdapterFaceIDStableDiffusionPipeline(\n-    DiffusionPipeline, TextualInversionLoaderMixin, LoraLoaderMixin, IPAdapterMixin, FromSingleFileMixin\n+    DiffusionPipeline,\n+    TextualInversionLoaderMixin,\n+    LoraLoaderMixin,\n+    IPAdapterMixin,\n+    FromSingleFileMixin,\n ):\n     r\"\"\"\n     Pipeline for text-to-image generation using Stable Diffusion.\n@@ -451,7 +548,10 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n     ):\n         super().__init__()\n \n-        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n+        if (\n+            hasattr(scheduler.config, \"steps_offset\")\n+            and scheduler.config.steps_offset != 1\n+        ):\n             deprecation_message = (\n                 f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                 f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n@@ -460,12 +560,17 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                 \" file\"\n             )\n-            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n+            deprecate(\n+                \"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False\n+            )\n             new_config = dict(scheduler.config)\n             new_config[\"steps_offset\"] = 1\n             scheduler._internal_dict = FrozenDict(new_config)\n \n-        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n+        if (\n+            hasattr(scheduler.config, \"clip_sample\")\n+            and scheduler.config.clip_sample is True\n+        ):\n             deprecation_message = (\n                 f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                 \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n@@ -473,7 +578,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                 \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n             )\n-            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n+            deprecate(\n+                \"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False\n+            )\n             new_config = dict(scheduler.config)\n             new_config[\"clip_sample\"] = False\n             scheduler._internal_dict = FrozenDict(new_config)\n@@ -494,10 +601,16 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n             )\n \n-        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n+        is_unet_version_less_0_9_0 = hasattr(\n+            unet.config, \"_diffusers_version\"\n+        ) and version.parse(\n             version.parse(unet.config._diffusers_version).base_version\n-        ) < version.parse(\"0.9.0.dev0\")\n-        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n+        ) < version.parse(\n+            \"0.9.0.dev0\"\n+        )\n+        is_unet_sample_size_less_64 = (\n+            hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n+        )\n         if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n             deprecation_message = (\n                 \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n@@ -510,7 +623,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 \" checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for\"\n                 \" the `unet/config.json` file\"\n             )\n-            deprecate(\"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False)\n+            deprecate(\n+                \"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False\n+            )\n             new_config = dict(unet.config)\n             new_config[\"sample_size\"] = 64\n             unet._internal_dict = FrozenDict(new_config)\n@@ -529,7 +644,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n         self.register_to_config(requires_safety_checker=requires_safety_checker)\n \n-    def load_ip_adapter_face_id(self, pretrained_model_name_or_path_or_dict, weight_name, **kwargs):\n+    def load_ip_adapter_face_id(\n+        self, pretrained_model_name_or_path_or_dict, weight_name, **kwargs\n+    ):\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n         resume_download = kwargs.pop(\"resume_download\", False)\n@@ -555,15 +672,19 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             revision=revision,\n             subfolder=subfolder,\n             user_agent=user_agent,\n-            )\n+        )\n         if weight_name.endswith(\".safetensors\"):\n             state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n             with safe_open(model_file, framework=\"pt\", device=\"cpu\") as f:\n                 for key in f.keys():\n                     if key.startswith(\"image_proj.\"):\n-                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n+                        state_dict[\"image_proj\"][\n+                            key.replace(\"image_proj.\", \"\")\n+                        ] = f.get_tensor(key)\n                     elif key.startswith(\"ip_adapter.\"):\n-                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n+                        state_dict[\"ip_adapter\"][\n+                            key.replace(\"ip_adapter.\", \"\")\n+                        ] = f.get_tensor(key)\n         else:\n             state_dict = torch.load(model_file, map_location=\"cpu\")\n         self._load_ip_adapter_weights(state_dict)\n@@ -593,10 +714,7 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         return image_projection\n \n     def _load_ip_adapter_weights(self, state_dict):\n-        from diffusers.models.attention_processor import (\n-            AttnProcessor,\n-            AttnProcessor2_0,\n-        )\n+        from diffusers.models.attention_processor import AttnProcessor, AttnProcessor2_0\n \n         num_image_text_embeds = 4\n \n@@ -606,21 +724,31 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         attn_procs = {}\n         key_id = 0\n         for name in self.unet.attn_processors.keys():\n-            cross_attention_dim = None if name.endswith(\"attn1.processor\") else self.unet.config.cross_attention_dim\n+            cross_attention_dim = (\n+                None\n+                if name.endswith(\"attn1.processor\")\n+                else self.unet.config.cross_attention_dim\n+            )\n             if name.startswith(\"mid_block\"):\n                 hidden_size = self.unet.config.block_out_channels[-1]\n             elif name.startswith(\"up_blocks\"):\n                 block_id = int(name[len(\"up_blocks.\")])\n-                hidden_size = list(reversed(self.unet.config.block_out_channels))[block_id]\n+                hidden_size = list(reversed(self.unet.config.block_out_channels))[\n+                    block_id\n+                ]\n             elif name.startswith(\"down_blocks\"):\n                 block_id = int(name[len(\"down_blocks.\")])\n                 hidden_size = self.unet.config.block_out_channels[block_id]\n             if cross_attention_dim is None or \"motion_modules\" in name:\n                 attn_processor_class = (\n-                    AttnProcessor2_0 if hasattr(F, \"scaled_dot_product_attention\") else AttnProcessor\n+                    AttnProcessor2_0\n+                    if hasattr(F, \"scaled_dot_product_attention\")\n+                    else AttnProcessor\n                 )\n                 attn_procs[name] = attn_processor_class()\n-                rank = state_dict[\"ip_adapter\"][f\"{key_id}.to_q_lora.down.weight\"].shape[0]\n+                rank = state_dict[\"ip_adapter\"][\n+                    f\"{key_id}.to_q_lora.down.weight\"\n+                ].shape[0]\n                 attn_module = self.unet\n                 for n in name.split(\".\")[:-1]:\n                     attn_module = getattr(attn_module, n)\n@@ -664,9 +792,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                     for lora_name, w in lora_layer.state_dict().items():\n                         value_dict.update(\n                             {\n-                                f\"{k}{index}lora_layer.{lora_name}\": state_dict[\"ip_adapter\"][\n-                                    f\"{key_id}.{k}_lora.{lora_name}\"\n-                                ]\n+                                f\"{k}{index}lora_layer.{lora_name}\": state_dict[\n+                                    \"ip_adapter\"\n+                                ][f\"{key_id}.{k}_lora.{lora_name}\"]\n                             }\n                         )\n \n@@ -674,7 +802,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 attn_module.to(dtype=self.dtype, device=self.device)\n                 key_id += 1\n             else:\n-                rank = state_dict[\"ip_adapter\"][f\"{key_id}.to_q_lora.down.weight\"].shape[0]\n+                rank = state_dict[\"ip_adapter\"][\n+                    f\"{key_id}.to_q_lora.down.weight\"\n+                ].shape[0]\n                 attn_processor_class = (\n                     LoRAIPAdapterAttnProcessor2_0\n                     if hasattr(F, \"scaled_dot_product_attention\")\n@@ -690,7 +820,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n \n                 value_dict = {}\n                 for k, w in attn_procs[name].state_dict().items():\n-                    value_dict.update({f\"{k}\": state_dict[\"ip_adapter\"][f\"{key_id}.{k}\"]})\n+                    value_dict.update(\n+                        {f\"{k}\": state_dict[\"ip_adapter\"][f\"{key_id}.{k}\"]}\n+                    )\n \n                 attn_procs[name].load_state_dict(value_dict)\n                 key_id += 1\n@@ -698,15 +830,22 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         self.unet.set_attn_processor(attn_procs)\n \n         # convert IP-Adapter Image Projection layers to diffusers\n-        image_projection = self.convert_ip_adapter_image_proj_to_diffusers(state_dict[\"image_proj\"])\n+        image_projection = self.convert_ip_adapter_image_proj_to_diffusers(\n+            state_dict[\"image_proj\"]\n+        )\n \n-        self.unet.encoder_hid_proj = image_projection.to(device=self.device, dtype=self.dtype)\n+        self.unet.encoder_hid_proj = image_projection.to(\n+            device=self.device, dtype=self.dtype\n+        )\n         self.unet.config.encoder_hid_dim_type = \"ip_image_proj\"\n \n     def set_ip_adapter_scale(self, scale):\n         unet = getattr(self, self.unet_name) if not hasattr(self, \"unet\") else self.unet\n         for attn_processor in unet.attn_processors.values():\n-            if isinstance(attn_processor, (LoRAIPAdapterAttnProcessor, LoRAIPAdapterAttnProcessor2_0)):\n+            if isinstance(\n+                attn_processor,\n+                (LoRAIPAdapterAttnProcessor, LoRAIPAdapterAttnProcessor2_0),\n+            ):\n                 attn_processor.scale = scale\n \n     def enable_vae_slicing(self):\n@@ -842,11 +981,13 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 return_tensors=\"pt\",\n             )\n             text_input_ids = text_inputs.input_ids\n-            untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n+            untruncated_ids = self.tokenizer(\n+                prompt, padding=\"longest\", return_tensors=\"pt\"\n+            ).input_ids\n \n-            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n-                text_input_ids, untruncated_ids\n-            ):\n+            if untruncated_ids.shape[-1] >= text_input_ids.shape[\n+                -1\n+            ] and not torch.equal(text_input_ids, untruncated_ids):\n                 removed_text = self.tokenizer.batch_decode(\n                     untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n                 )\n@@ -855,17 +996,24 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                     f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                 )\n \n-            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n+            if (\n+                hasattr(self.text_encoder.config, \"use_attention_mask\")\n+                and self.text_encoder.config.use_attention_mask\n+            ):\n                 attention_mask = text_inputs.attention_mask.to(device)\n             else:\n                 attention_mask = None\n \n             if clip_skip is None:\n-                prompt_embeds = self.text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n+                prompt_embeds = self.text_encoder(\n+                    text_input_ids.to(device), attention_mask=attention_mask\n+                )\n                 prompt_embeds = prompt_embeds[0]\n             else:\n                 prompt_embeds = self.text_encoder(\n-                    text_input_ids.to(device), attention_mask=attention_mask, output_hidden_states=True\n+                    text_input_ids.to(device),\n+                    attention_mask=attention_mask,\n+                    output_hidden_states=True,\n                 )\n                 # Access the `hidden_states` first, that contains a tuple of\n                 # all the hidden states from the encoder layers. Then index into\n@@ -875,7 +1023,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 # representations. The `last_hidden_states` that we typically use for\n                 # obtaining the final prompt representations passes through the LayerNorm\n                 # layer.\n-                prompt_embeds = self.text_encoder.text_model.final_layer_norm(prompt_embeds)\n+                prompt_embeds = self.text_encoder.text_model.final_layer_norm(\n+                    prompt_embeds\n+                )\n \n         if self.text_encoder is not None:\n             prompt_embeds_dtype = self.text_encoder.dtype\n@@ -889,7 +1039,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         bs_embed, seq_len, _ = prompt_embeds.shape\n         # duplicate text embeddings for each generation per prompt, using mps friendly method\n         prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n-        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds = prompt_embeds.view(\n+            bs_embed * num_images_per_prompt, seq_len, -1\n+        )\n \n         # get unconditional embeddings for classifier free guidance\n         if do_classifier_free_guidance and negative_prompt_embeds is None:\n@@ -925,7 +1077,10 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 return_tensors=\"pt\",\n             )\n \n-            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n+            if (\n+                hasattr(self.text_encoder.config, \"use_attention_mask\")\n+                and self.text_encoder.config.use_attention_mask\n+            ):\n                 attention_mask = uncond_input.attention_mask.to(device)\n             else:\n                 attention_mask = None\n@@ -940,10 +1095,16 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n             seq_len = negative_prompt_embeds.shape[1]\n \n-            negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n+            negative_prompt_embeds = negative_prompt_embeds.to(\n+                dtype=prompt_embeds_dtype, device=device\n+            )\n \n-            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n-            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+            negative_prompt_embeds = negative_prompt_embeds.repeat(\n+                1, num_images_per_prompt, 1\n+            )\n+            negative_prompt_embeds = negative_prompt_embeds.view(\n+                batch_size * num_images_per_prompt, seq_len, -1\n+            )\n \n         if isinstance(self, LoraLoaderMixin) and USE_PEFT_BACKEND:\n             # Retrieve the original scale by scaling back the LoRA layers\n@@ -951,7 +1112,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n \n         return prompt_embeds, negative_prompt_embeds\n \n-    def encode_image(self, image, device, num_images_per_prompt, output_hidden_states=None):\n+    def encode_image(\n+        self, image, device, num_images_per_prompt, output_hidden_states=None\n+    ):\n         dtype = next(self.image_encoder.parameters()).dtype\n \n         if not isinstance(image, torch.Tensor):\n@@ -959,13 +1122,19 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n \n         image = image.to(device=device, dtype=dtype)\n         if output_hidden_states:\n-            image_enc_hidden_states = self.image_encoder(image, output_hidden_states=True).hidden_states[-2]\n-            image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n+            image_enc_hidden_states = self.image_encoder(\n+                image, output_hidden_states=True\n+            ).hidden_states[-2]\n+            image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(\n+                num_images_per_prompt, dim=0\n+            )\n             uncond_image_enc_hidden_states = self.image_encoder(\n                 torch.zeros_like(image), output_hidden_states=True\n             ).hidden_states[-2]\n-            uncond_image_enc_hidden_states = uncond_image_enc_hidden_states.repeat_interleave(\n-                num_images_per_prompt, dim=0\n+            uncond_image_enc_hidden_states = (\n+                uncond_image_enc_hidden_states.repeat_interleave(\n+                    num_images_per_prompt, dim=0\n+                )\n             )\n             return image_enc_hidden_states, uncond_image_enc_hidden_states\n         else:\n@@ -980,10 +1149,14 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             has_nsfw_concept = None\n         else:\n             if torch.is_tensor(image):\n-                feature_extractor_input = self.image_processor.postprocess(image, output_type=\"pil\")\n+                feature_extractor_input = self.image_processor.postprocess(\n+                    image, output_type=\"pil\"\n+                )\n             else:\n                 feature_extractor_input = self.image_processor.numpy_to_pil(image)\n-            safety_checker_input = self.feature_extractor(feature_extractor_input, return_tensors=\"pt\").to(device)\n+            safety_checker_input = self.feature_extractor(\n+                feature_extractor_input, return_tensors=\"pt\"\n+            ).to(device)\n             image, has_nsfw_concept = self.safety_checker(\n                 images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n             )\n@@ -1006,13 +1179,17 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         # eta corresponds to \u03b7 in DDIM paper: https://arxiv.org/abs/2010.02502\n         # and should be between [0, 1]\n \n-        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        accepts_eta = \"eta\" in set(\n+            inspect.signature(self.scheduler.step).parameters.keys()\n+        )\n         extra_step_kwargs = {}\n         if accepts_eta:\n             extra_step_kwargs[\"eta\"] = eta\n \n         # check if the scheduler accepts generator\n-        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        accepts_generator = \"generator\" in set(\n+            inspect.signature(self.scheduler.step).parameters.keys()\n+        )\n         if accepts_generator:\n             extra_step_kwargs[\"generator\"] = generator\n         return extra_step_kwargs\n@@ -1029,15 +1206,20 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         callback_on_step_end_tensor_inputs=None,\n     ):\n         if height % 8 != 0 or width % 8 != 0:\n-            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n+            raise ValueError(\n+                f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\"\n+            )\n \n-        if callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0):\n+        if callback_steps is not None and (\n+            not isinstance(callback_steps, int) or callback_steps <= 0\n+        ):\n             raise ValueError(\n                 f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                 f\" {type(callback_steps)}.\"\n             )\n         if callback_on_step_end_tensor_inputs is not None and not all(\n-            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+            k in self._callback_tensor_inputs\n+            for k in callback_on_step_end_tensor_inputs\n         ):\n             raise ValueError(\n                 f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n@@ -1052,8 +1234,12 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             raise ValueError(\n                 \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n             )\n-        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n-            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif prompt is not None and (\n+            not isinstance(prompt, str) and not isinstance(prompt, list)\n+        ):\n+            raise ValueError(\n+                f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\"\n+            )\n \n         if negative_prompt is not None and negative_prompt_embeds is not None:\n             raise ValueError(\n@@ -1069,8 +1255,23 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                     f\" {negative_prompt_embeds.shape}.\"\n                 )\n \n-    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n-        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        shape = (\n+            batch_size,\n+            num_channels_latents,\n+            height // self.vae_scale_factor,\n+            width // self.vae_scale_factor,\n+        )\n         if isinstance(generator, list) and len(generator) != batch_size:\n             raise ValueError(\n                 f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n@@ -1078,7 +1279,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             )\n \n         if latents is None:\n-            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+            latents = randn_tensor(\n+                shape, generator=generator, device=device, dtype=dtype\n+            )\n         else:\n             latents = latents.to(device)\n \n@@ -1138,7 +1341,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n \n         if vae:\n             if not isinstance(self.vae, AutoencoderKL):\n-                raise ValueError(\"`fuse_qkv_projections()` is only supported for the VAE of type `AutoencoderKL`.\")\n+                raise ValueError(\n+                    \"`fuse_qkv_projections()` is only supported for the VAE of type `AutoencoderKL`.\"\n+                )\n \n             self.fusing_vae = True\n             self.vae.fuse_qkv_projections()\n@@ -1161,14 +1366,18 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         \"\"\"\n         if unet:\n             if not self.fusing_unet:\n-                logger.warning(\"The UNet was not initially fused for QKV projections. Doing nothing.\")\n+                logger.warning(\n+                    \"The UNet was not initially fused for QKV projections. Doing nothing.\"\n+                )\n             else:\n                 self.unet.unfuse_qkv_projections()\n                 self.fusing_unet = False\n \n         if vae:\n             if not self.fusing_vae:\n-                logger.warning(\"The VAE was not initially fused for QKV projections. Doing nothing.\")\n+                logger.warning(\n+                    \"The VAE was not initially fused for QKV projections. Doing nothing.\"\n+                )\n             else:\n                 self.vae.unfuse_qkv_projections()\n                 self.fusing_vae = False\n@@ -1389,7 +1598,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n \n         # 3. Encode input prompt\n         lora_scale = (\n-            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n+            self.cross_attention_kwargs.get(\"scale\", None)\n+            if self.cross_attention_kwargs is not None\n+            else None\n         )\n \n         prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n@@ -1411,15 +1622,17 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n             prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n \n         if image_embeds is not None:\n-            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0).to(\n-                device=device, dtype=prompt_embeds.dtype\n-            )\n+            image_embeds = image_embeds.repeat_interleave(\n+                num_images_per_prompt, dim=0\n+            ).to(device=device, dtype=prompt_embeds.dtype)\n             negative_image_embeds = torch.zeros_like(image_embeds)\n             if self.do_classifier_free_guidance:\n                 image_embeds = torch.cat([negative_image_embeds, image_embeds])\n \n         # 4. Prepare timesteps\n-        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler, num_inference_steps, device, timesteps\n+        )\n \n         # 5. Prepare latent variables\n         num_channels_latents = self.unet.config.in_channels\n@@ -1438,12 +1651,16 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n \n         # 6.1 Add image embeds for IP-Adapter\n-        added_cond_kwargs ={\"image_embeds\": image_embeds} if image_embeds is not None else None\n+        added_cond_kwargs = (\n+            {\"image_embeds\": image_embeds} if image_embeds is not None else None\n+        )\n \n         # 6.2 Optionally get Guidance Scale Embedding\n         timestep_cond = None\n         if self.unet.config.time_cond_proj_dim is not None:\n-            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n+            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(\n+                batch_size * num_images_per_prompt\n+            )\n             timestep_cond = self.get_guidance_scale_embedding(\n                 guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n             ).to(device=device, dtype=latents.dtype)\n@@ -1457,8 +1674,14 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                     continue\n \n                 # expand the latents if we are doing classifier free guidance\n-                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n-                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n+                latent_model_input = (\n+                    torch.cat([latents] * 2)\n+                    if self.do_classifier_free_guidance\n+                    else latents\n+                )\n+                latent_model_input = self.scheduler.scale_model_input(\n+                    latent_model_input, t\n+                )\n \n                 # predict the noise residual\n                 noise_pred = self.unet(\n@@ -1474,14 +1697,22 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n                 # perform guidance\n                 if self.do_classifier_free_guidance:\n                     noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n-                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n+                    noise_pred = noise_pred_uncond + self.guidance_scale * (\n+                        noise_pred_text - noise_pred_uncond\n+                    )\n \n                 if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n                     # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n-                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n+                    noise_pred = rescale_noise_cfg(\n+                        noise_pred,\n+                        noise_pred_text,\n+                        guidance_rescale=self.guidance_rescale,\n+                    )\n \n                 # compute the previous noisy sample x_t -> x_t-1\n-                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n+                latents = self.scheduler.step(\n+                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False\n+                )[0]\n \n                 if callback_on_step_end is not None:\n                     callback_kwargs = {}\n@@ -1491,20 +1722,28 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n \n                     latents = callback_outputs.pop(\"latents\", latents)\n                     prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n-                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\n+                        \"negative_prompt_embeds\", negative_prompt_embeds\n+                    )\n \n                 # call the callback, if provided\n-                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                if i == len(timesteps) - 1 or (\n+                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n+                ):\n                     progress_bar.update()\n                     if callback is not None and i % callback_steps == 0:\n                         step_idx = i // getattr(self.scheduler, \"order\", 1)\n                         callback(step_idx, t, latents)\n \n         if not output_type == \"latent\":\n-            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[\n-                0\n-            ]\n-            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n+            image = self.vae.decode(\n+                latents / self.vae.config.scaling_factor,\n+                return_dict=False,\n+                generator=generator,\n+            )[0]\n+            image, has_nsfw_concept = self.run_safety_checker(\n+                image, device, prompt_embeds.dtype\n+            )\n         else:\n             image = latents\n             has_nsfw_concept = None\n@@ -1514,7 +1753,9 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         else:\n             do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n \n-        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n+        image = self.image_processor.postprocess(\n+            image, output_type=output_type, do_denormalize=do_denormalize\n+        )\n \n         # Offload all models\n         self.maybe_free_model_hooks()\n@@ -1522,4 +1763,6 @@ class IPAdapterFaceIDStableDiffusionPipeline(\n         if not return_dict:\n             return (image, has_nsfw_concept)\n \n-        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n+        return StableDiffusionPipelineOutput(\n+            images=image, nsfw_content_detected=has_nsfw_concept\n+        )\n"
    }
]