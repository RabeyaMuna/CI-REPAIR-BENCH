[
    {
        "id": "276",
        "sha_fail": "2ee00eddc1bebd2fcffc4a026162e00378ebe3ae",
        "diff": "diff --git a/sqlglot/dialects/dialect.py b/sqlglot/dialects/dialect.py\nindex 371418bd3..a9638578e 100644\n--- a/sqlglot/dialects/dialect.py\n+++ b/sqlglot/dialects/dialect.py\n@@ -59,7 +59,9 @@ UNESCAPED_SEQUENCES = {\n }\n \n \n-def annotate_with_type_lambda(data_type: exp.DataType.Type) -> t.Callable[[TypeAnnotator, E], E]:\n+def annotate_with_type_lambda(\n+    data_type: exp.DataType.Type,\n+) -> t.Callable[[TypeAnnotator, E], E]:\n     return lambda self, e: self._annotate_with_type(e, data_type)\n \n \n@@ -205,7 +207,9 @@ class _Dialect(type):\n \n         base = seq_get(bases, 0)\n         base_tokenizer = (getattr(base, \"tokenizer_class\", Tokenizer),)\n-        base_jsonpath_tokenizer = (getattr(base, \"jsonpath_tokenizer_class\", JSONPathTokenizer),)\n+        base_jsonpath_tokenizer = (\n+            getattr(base, \"jsonpath_tokenizer_class\", JSONPathTokenizer),\n+        )\n         base_parser = (getattr(base, \"parser_class\", Parser),)\n         base_generator = (getattr(base, \"generator_class\", Generator),)\n \n@@ -215,17 +219,23 @@ class _Dialect(type):\n         klass.jsonpath_tokenizer_class = klass.__dict__.get(\n             \"JSONPathTokenizer\", type(\"JSONPathTokenizer\", base_jsonpath_tokenizer, {})\n         )\n-        klass.parser_class = klass.__dict__.get(\"Parser\", type(\"Parser\", base_parser, {}))\n+        klass.parser_class = klass.__dict__.get(\n+            \"Parser\", type(\"Parser\", base_parser, {})\n+        )\n         klass.generator_class = klass.__dict__.get(\n             \"Generator\", type(\"Generator\", base_generator, {})\n         )\n \n-        klass.QUOTE_START, klass.QUOTE_END = list(klass.tokenizer_class._QUOTES.items())[0]\n+        klass.QUOTE_START, klass.QUOTE_END = list(\n+            klass.tokenizer_class._QUOTES.items()\n+        )[0]\n         klass.IDENTIFIER_START, klass.IDENTIFIER_END = list(\n             klass.tokenizer_class._IDENTIFIERS.items()\n         )[0]\n \n-        def get_start_end(token_type: TokenType) -> t.Tuple[t.Optional[str], t.Optional[str]]:\n+        def get_start_end(\n+            token_type: TokenType,\n+        ) -> t.Tuple[t.Optional[str], t.Optional[str]]:\n             return next(\n                 (\n                     (s, e)\n@@ -258,7 +268,9 @@ class _Dialect(type):\n             klass.generator_class.SUPPORTS_UESCAPE = False\n \n         if enum not in (\"\", \"databricks\", \"hive\", \"spark\", \"spark2\"):\n-            modifier_transforms = klass.generator_class.AFTER_HAVING_MODIFIER_TRANSFORMS.copy()\n+            modifier_transforms = (\n+                klass.generator_class.AFTER_HAVING_MODIFIER_TRANSFORMS.copy()\n+            )\n             for modifier in (\"cluster\", \"distribute\", \"sort\"):\n                 modifier_transforms.pop(modifier, None)\n \n@@ -268,18 +280,24 @@ class _Dialect(type):\n             klass.parser_class.ID_VAR_TOKENS = klass.parser_class.ID_VAR_TOKENS | {\n                 TokenType.STRAIGHT_JOIN,\n             }\n-            klass.parser_class.TABLE_ALIAS_TOKENS = klass.parser_class.TABLE_ALIAS_TOKENS | {\n-                TokenType.STRAIGHT_JOIN,\n-            }\n+            klass.parser_class.TABLE_ALIAS_TOKENS = (\n+                klass.parser_class.TABLE_ALIAS_TOKENS\n+                | {\n+                    TokenType.STRAIGHT_JOIN,\n+                }\n+            )\n \n         if enum not in (\"\", \"databricks\", \"oracle\", \"redshift\", \"snowflake\", \"spark\"):\n             klass.generator_class.SUPPORTS_DECODE_CASE = False\n \n         if not klass.SUPPORTS_SEMI_ANTI_JOIN:\n-            klass.parser_class.TABLE_ALIAS_TOKENS = klass.parser_class.TABLE_ALIAS_TOKENS | {\n-                TokenType.ANTI,\n-                TokenType.SEMI,\n-            }\n+            klass.parser_class.TABLE_ALIAS_TOKENS = (\n+                klass.parser_class.TABLE_ALIAS_TOKENS\n+                | {\n+                    TokenType.ANTI,\n+                    TokenType.SEMI,\n+                }\n+            )\n \n         return klass\n \n@@ -792,11 +810,15 @@ class Dialect(metaclass=_Dialect):\n             for expr_type in expressions\n         },\n         exp.Abs: lambda self, e: self._annotate_by_args(e, \"this\"),\n-        exp.Anonymous: lambda self, e: self._annotate_with_type(e, exp.DataType.Type.UNKNOWN),\n+        exp.Anonymous: lambda self, e: self._annotate_with_type(\n+            e, exp.DataType.Type.UNKNOWN\n+        ),\n         exp.Array: lambda self, e: self._annotate_by_args(e, \"expressions\", array=True),\n         exp.AnyValue: lambda self, e: self._annotate_by_args(e, \"this\"),\n         exp.ArrayAgg: lambda self, e: self._annotate_by_args(e, \"this\", array=True),\n-        exp.ArrayConcat: lambda self, e: self._annotate_by_args(e, \"this\", \"expressions\"),\n+        exp.ArrayConcat: lambda self, e: self._annotate_by_args(\n+            e, \"this\", \"expressions\"\n+        ),\n         exp.ArrayConcatAgg: lambda self, e: self._annotate_by_args(e, \"this\"),\n         exp.ArrayFirst: lambda self, e: self._annotate_by_array_element(e),\n         exp.ArrayLast: lambda self, e: self._annotate_by_array_element(e),\n@@ -807,7 +829,10 @@ class Dialect(metaclass=_Dialect):\n         exp.Case: lambda self, e: self._annotate_by_args(e, \"default\", \"ifs\"),\n         exp.Coalesce: lambda self, e: self._annotate_by_args(e, \"this\", \"expressions\"),\n         exp.Count: lambda self, e: self._annotate_with_type(\n-            e, exp.DataType.Type.BIGINT if e.args.get(\"big_int\") else exp.DataType.Type.INT\n+            e,\n+            exp.DataType.Type.BIGINT\n+            if e.args.get(\"big_int\")\n+            else exp.DataType.Type.INT,\n         ),\n         exp.DataType: lambda self, e: self._annotate_with_type(e, e.copy()),\n         exp.DateAdd: lambda self, e: self._annotate_timeunit(e),\n@@ -839,13 +864,19 @@ class Dialect(metaclass=_Dialect):\n         exp.Null: lambda self, e: self._annotate_with_type(e, exp.DataType.Type.NULL),\n         exp.Nullif: lambda self, e: self._annotate_by_args(e, \"this\", \"expression\"),\n         exp.PropertyEQ: lambda self, e: self._annotate_by_args(e, \"expression\"),\n-        exp.Slice: lambda self, e: self._annotate_with_type(e, exp.DataType.Type.UNKNOWN),\n+        exp.Slice: lambda self, e: self._annotate_with_type(\n+            e, exp.DataType.Type.UNKNOWN\n+        ),\n         exp.Struct: lambda self, e: self._annotate_struct(e),\n-        exp.Sum: lambda self, e: self._annotate_by_args(e, \"this\", \"expressions\", promote=True),\n+        exp.Sum: lambda self, e: self._annotate_by_args(\n+            e, \"this\", \"expressions\", promote=True\n+        ),\n         exp.SortArray: lambda self, e: self._annotate_by_args(e, \"this\"),\n         exp.Timestamp: lambda self, e: self._annotate_with_type(\n             e,\n-            exp.DataType.Type.TIMESTAMPTZ if e.args.get(\"with_tz\") else exp.DataType.Type.TIMESTAMP,\n+            exp.DataType.Type.TIMESTAMPTZ\n+            if e.args.get(\"with_tz\")\n+            else exp.DataType.Type.TIMESTAMP,\n         ),\n         exp.ToMap: lambda self, e: self._annotate_to_map(e),\n         exp.TryCast: lambda self, e: self._annotate_with_type(e, e.args[\"to\"]),\n@@ -912,7 +943,9 @@ class Dialect(metaclass=_Dialect):\n \n             result = cls.get(dialect_name.strip())\n             if not result:\n-                suggest_closest_match_and_fail(\"dialect\", dialect_name, list(DIALECT_MODULE_NAMES))\n+                suggest_closest_match_and_fail(\n+                    \"dialect\", dialect_name, list(DIALECT_MODULE_NAMES)\n+                )\n \n             assert result is not None\n             return result(**kwargs)\n@@ -931,7 +964,9 @@ class Dialect(metaclass=_Dialect):\n             )\n \n         if expression and expression.is_string:\n-            return exp.Literal.string(format_time(expression.this, cls.TIME_MAPPING, cls.TIME_TRIE))\n+            return exp.Literal.string(\n+                format_time(expression.this, cls.TIME_MAPPING, cls.TIME_TRIE)\n+            )\n \n         return expression\n \n@@ -942,12 +977,16 @@ class Dialect(metaclass=_Dialect):\n         if normalization_strategy is None:\n             self.normalization_strategy = self.NORMALIZATION_STRATEGY\n         else:\n-            self.normalization_strategy = NormalizationStrategy(normalization_strategy.upper())\n+            self.normalization_strategy = NormalizationStrategy(\n+                normalization_strategy.upper()\n+            )\n \n         self.settings = kwargs\n \n         for unsupported_setting in kwargs.keys() - self.SUPPORTED_SETTINGS:\n-            suggest_closest_match_and_fail(\"setting\", unsupported_setting, self.SUPPORTED_SETTINGS)\n+            suggest_closest_match_and_fail(\n+                \"setting\", unsupported_setting, self.SUPPORTED_SETTINGS\n+            )\n \n     def __eq__(self, other: t.Any) -> bool:\n         # Does not currently take dialect state into account\n@@ -1042,16 +1081,22 @@ class Dialect(metaclass=_Dialect):\n             identify: If set to `False`, the quotes will only be added if the identifier is deemed\n                 \"unsafe\", with respect to its characters and this dialect's normalization strategy.\n         \"\"\"\n-        if isinstance(expression, exp.Identifier) and not isinstance(expression.parent, exp.Func):\n+        if isinstance(expression, exp.Identifier) and not isinstance(\n+            expression.parent, exp.Func\n+        ):\n             name = expression.this\n             expression.set(\n                 \"quoted\",\n-                identify or self.case_sensitive(name) or not exp.SAFE_IDENTIFIER_RE.match(name),\n+                identify\n+                or self.case_sensitive(name)\n+                or not exp.SAFE_IDENTIFIER_RE.match(name),\n             )\n \n         return expression\n \n-    def to_json_path(self, path: t.Optional[exp.Expression]) -> t.Optional[exp.Expression]:\n+    def to_json_path(\n+        self, path: t.Optional[exp.Expression]\n+    ) -> t.Optional[exp.Expression]:\n         if isinstance(path, exp.Literal):\n             path_text = path.name\n             if path.is_number:\n@@ -1131,10 +1176,16 @@ def if_sql(\n \n def arrow_json_extract_sql(self: Generator, expression: JSON_EXTRACT_TYPE) -> str:\n     this = expression.this\n-    if self.JSON_TYPE_REQUIRED_FOR_EXTRACTION and isinstance(this, exp.Literal) and this.is_string:\n+    if (\n+        self.JSON_TYPE_REQUIRED_FOR_EXTRACTION\n+        and isinstance(this, exp.Literal)\n+        and this.is_string\n+    ):\n         this.replace(exp.cast(this, exp.DataType.Type.JSON))\n \n-    return self.binary(expression, \"->\" if isinstance(expression, exp.JSONExtract) else \"->>\")\n+    return self.binary(\n+        expression, \"->\" if isinstance(expression, exp.JSONExtract) else \"->>\"\n+    )\n \n \n def inline_array_sql(self: Generator, expression: exp.Array) -> str:\n@@ -1151,7 +1202,8 @@ def inline_array_unless_query(self: Generator, expression: exp.Array) -> str:\n def no_ilike_sql(self: Generator, expression: exp.ILike) -> str:\n     return self.like_sql(\n         exp.Like(\n-            this=exp.Lower(this=expression.this), expression=exp.Lower(this=expression.expression)\n+            this=exp.Lower(this=expression.this),\n+            expression=exp.Lower(this=expression.expression),\n         )\n     )\n \n@@ -1221,16 +1273,24 @@ def strposition_sql(\n         string = exp.Substring(this=string, start=position)\n \n     if func_name == \"POSITION\" and use_ansi_position:\n-        func = exp.Anonymous(this=func_name, expressions=[exp.In(this=substr, field=string)])\n+        func = exp.Anonymous(\n+            this=func_name, expressions=[exp.In(this=substr, field=string)]\n+        )\n     else:\n-        args = [substr, string] if func_name in (\"LOCATE\", \"CHARINDEX\") else [string, substr]\n+        args = (\n+            [substr, string]\n+            if func_name in (\"LOCATE\", \"CHARINDEX\")\n+            else [string, substr]\n+        )\n         if supports_position:\n             args.append(position)\n         if occurrence:\n             if supports_occurrence:\n                 args.append(occurrence)\n             else:\n-                self.unsupported(f\"{func_name} does not support the occurrence parameter.\")\n+                self.unsupported(\n+                    f\"{func_name} does not support the occurrence parameter.\"\n+                )\n         func = exp.Anonymous(this=func_name, expressions=args)\n \n     if transpile_position:\n@@ -1242,9 +1302,7 @@ def strposition_sql(\n \n \n def struct_extract_sql(self: Generator, expression: exp.StructExtract) -> str:\n-    return (\n-        f\"{self.sql(expression, 'this')}.{self.sql(exp.to_identifier(expression.expression.name))}\"\n-    )\n+    return f\"{self.sql(expression, 'this')}.{self.sql(exp.to_identifier(expression.expression.name))}\"\n \n \n def var_map_sql(\n@@ -1284,7 +1342,9 @@ def build_formatted_time(\n             this=seq_get(args, 0),\n             format=Dialect[dialect].format_time(\n                 seq_get(args, 1)\n-                or (Dialect[dialect].TIME_FORMAT if default is True else default or None)\n+                or (\n+                    Dialect[dialect].TIME_FORMAT if default is True else default or None\n+                )\n             ),\n         )\n \n@@ -1294,13 +1354,19 @@ def build_formatted_time(\n def time_format(\n     dialect: DialectType = None,\n ) -> t.Callable[[Generator, exp.UnixToStr | exp.StrToUnix], t.Optional[str]]:\n-    def _time_format(self: Generator, expression: exp.UnixToStr | exp.StrToUnix) -> t.Optional[str]:\n+    def _time_format(\n+        self: Generator, expression: exp.UnixToStr | exp.StrToUnix\n+    ) -> t.Optional[str]:\n         \"\"\"\n         Returns the time format for a given expression, unless it's equivalent\n         to the default time format of the dialect of interest.\n         \"\"\"\n         time_format = self.format_time(expression)\n-        return time_format if time_format != Dialect.get_or_raise(dialect).TIME_FORMAT else None\n+        return (\n+            time_format\n+            if time_format != Dialect.get_or_raise(dialect).TIME_FORMAT\n+            else None\n+        )\n \n     return _time_format\n \n@@ -1318,7 +1384,11 @@ def build_date_delta(\n         unit = None\n         if unit_based or default_unit:\n             unit = args[0] if unit_based else exp.Literal.string(default_unit)\n-            unit = exp.var(unit_mapping.get(unit.name.lower(), unit.name)) if unit_mapping else unit\n+            unit = (\n+                exp.var(unit_mapping.get(unit.name.lower(), unit.name))\n+                if unit_mapping\n+                else unit\n+            )\n         expression = exp_class(this=this, expression=seq_get(args, 1), unit=unit)\n         if supports_timezone and has_timezone:\n             expression.set(\"zone\", args[-1])\n@@ -1339,7 +1409,9 @@ def build_date_delta_with_interval(\n         if not isinstance(interval, exp.Interval):\n             raise ParseError(f\"INTERVAL expression expected but got '{interval}'\")\n \n-        return expression_class(this=args[0], expression=interval.this, unit=unit_to_str(interval))\n+        return expression_class(\n+            this=args[0], expression=interval.this, unit=unit_to_str(interval)\n+        )\n \n     return _builder\n \n@@ -1358,13 +1430,17 @@ def date_add_interval_sql(\n ) -> t.Callable[[Generator, exp.Expression], str]:\n     def func(self: Generator, expression: exp.Expression) -> str:\n         this = self.sql(expression, \"this\")\n-        interval = exp.Interval(this=expression.expression, unit=unit_to_var(expression))\n+        interval = exp.Interval(\n+            this=expression.expression, unit=unit_to_var(expression)\n+        )\n         return f\"{data_type}_{kind}({this}, {self.sql(interval)})\"\n \n     return func\n \n \n-def timestamptrunc_sql(zone: bool = False) -> t.Callable[[Generator, exp.TimestampTrunc], str]:\n+def timestamptrunc_sql(\n+    zone: bool = False,\n+) -> t.Callable[[Generator, exp.TimestampTrunc], str]:\n     def _timestamptrunc_sql(self: Generator, expression: exp.TimestampTrunc) -> str:\n         args = [unit_to_str(expression), expression.this]\n         if zone:\n@@ -1380,7 +1456,8 @@ def no_timestamp_sql(self: Generator, expression: exp.Timestamp) -> str:\n         from sqlglot.optimizer.annotate_types import annotate_types\n \n         target_type = (\n-            annotate_types(expression, dialect=self.dialect).type or exp.DataType.Type.TIMESTAMP\n+            annotate_types(expression, dialect=self.dialect).type\n+            or exp.DataType.Type.TIMESTAMP\n         )\n         return self.sql(exp.cast(expression.this, target_type))\n     if zone.name.lower() in TIMEZONES:\n@@ -1397,7 +1474,8 @@ def no_time_sql(self: Generator, expression: exp.Time) -> str:\n     # Transpile BQ's TIME(timestamp, zone) to CAST(TIMESTAMPTZ <timestamp> AT TIME ZONE <zone> AS TIME)\n     this = exp.cast(expression.this, exp.DataType.Type.TIMESTAMPTZ)\n     expr = exp.cast(\n-        exp.AtTimeZone(this=this, zone=expression.args.get(\"zone\")), exp.DataType.Type.TIME\n+        exp.AtTimeZone(this=this, zone=expression.args.get(\"zone\")),\n+        exp.DataType.Type.TIME,\n     )\n     return self.sql(expr)\n \n@@ -1409,19 +1487,25 @@ def no_datetime_sql(self: Generator, expression: exp.Datetime) -> str:\n     if expr.name.lower() in TIMEZONES:\n         # Transpile BQ's DATETIME(timestamp, zone) to CAST(TIMESTAMPTZ <timestamp> AT TIME ZONE <zone> AS TIMESTAMP)\n         this = exp.cast(this, exp.DataType.Type.TIMESTAMPTZ)\n-        this = exp.cast(exp.AtTimeZone(this=this, zone=expr), exp.DataType.Type.TIMESTAMP)\n+        this = exp.cast(\n+            exp.AtTimeZone(this=this, zone=expr), exp.DataType.Type.TIMESTAMP\n+        )\n         return self.sql(this)\n \n     this = exp.cast(this, exp.DataType.Type.DATE)\n     expr = exp.cast(expr, exp.DataType.Type.TIME)\n \n-    return self.sql(exp.cast(exp.Add(this=this, expression=expr), exp.DataType.Type.TIMESTAMP))\n+    return self.sql(\n+        exp.cast(exp.Add(this=this, expression=expr), exp.DataType.Type.TIMESTAMP)\n+    )\n \n \n def left_to_substring_sql(self: Generator, expression: exp.Left) -> str:\n     return self.sql(\n         exp.Substring(\n-            this=expression.this, start=exp.Literal.number(1), length=expression.expression\n+            this=expression.this,\n+            start=exp.Literal.number(1),\n+            length=expression.expression,\n         )\n     )\n \n@@ -1430,7 +1514,8 @@ def right_to_substring_sql(self: Generator, expression: exp.Left) -> str:\n     return self.sql(\n         exp.Substring(\n             this=expression.this,\n-            start=exp.Length(this=expression.this) - exp.paren(expression.expression - 1),\n+            start=exp.Length(this=expression.this)\n+            - exp.paren(expression.expression - 1),\n         )\n     )\n \n@@ -1450,7 +1535,8 @@ def timestrtotime_sql(\n         precision = subsecond_precision(expression.this.name)\n         if precision > 0:\n             datatype = exp.DataType.build(\n-                datatype.this, expressions=[exp.DataTypeParam(this=exp.Literal.number(precision))]\n+                datatype.this,\n+                expressions=[exp.DataTypeParam(this=exp.Literal.number(precision))],\n             )\n \n     return self.sql(exp.cast(expression.this, datatype, dialect=self.dialect))\n@@ -1468,7 +1554,9 @@ def encode_decode_sql(\n     if charset and charset.name.lower() != \"utf-8\":\n         self.unsupported(f\"Expected utf-8 character set, got {charset}.\")\n \n-    return self.func(name, expression.this, expression.args.get(\"replace\") if replace else None)\n+    return self.func(\n+        name, expression.this, expression.args.get(\"replace\") if replace else None\n+    )\n \n \n def min_or_least(self: Generator, expression: exp.Min) -> str:\n@@ -1513,14 +1601,18 @@ def str_to_time_sql(self: Generator, expression: exp.Expression) -> str:\n \n \n def concat_to_dpipe_sql(self: Generator, expression: exp.Concat) -> str:\n-    return self.sql(reduce(lambda x, y: exp.DPipe(this=x, expression=y), expression.expressions))\n+    return self.sql(\n+        reduce(lambda x, y: exp.DPipe(this=x, expression=y), expression.expressions)\n+    )\n \n \n def concat_ws_to_dpipe_sql(self: Generator, expression: exp.ConcatWs) -> str:\n     delim, *rest_args = expression.expressions\n     return self.sql(\n         reduce(\n-            lambda x, y: exp.DPipe(this=x, expression=exp.DPipe(this=delim, expression=y)),\n+            lambda x, y: exp.DPipe(\n+                this=x, expression=exp.DPipe(this=delim, expression=y)\n+            ),\n             rest_args,\n         )\n     )\n@@ -1536,17 +1628,24 @@ def regexp_extract_sql(\n     if group and group.name == str(self.dialect.REGEXP_EXTRACT_DEFAULT_GROUP):\n         group = None\n \n-    return self.func(expression.sql_name(), expression.this, expression.expression, group)\n+    return self.func(\n+        expression.sql_name(), expression.this, expression.expression, group\n+    )\n \n \n @unsupported_args(\"position\", \"occurrence\", \"modifiers\")\n def regexp_replace_sql(self: Generator, expression: exp.RegexpReplace) -> str:\n     return self.func(\n-        \"REGEXP_REPLACE\", expression.this, expression.expression, expression.args[\"replacement\"]\n+        \"REGEXP_REPLACE\",\n+        expression.this,\n+        expression.expression,\n+        expression.args[\"replacement\"],\n     )\n \n \n-def pivot_column_names(aggregations: t.List[exp.Expression], dialect: DialectType) -> t.List[str]:\n+def pivot_column_names(\n+    aggregations: t.List[exp.Expression], dialect: DialectType\n+) -> t.List[str]:\n     names = []\n     for agg in aggregations:\n         if isinstance(agg, exp.Alias):\n@@ -1565,7 +1664,9 @@ def pivot_column_names(aggregations: t.List[exp.Expression], dialect: DialectTyp\n                     else node\n                 )\n             )\n-            names.append(agg_all_unquoted.sql(dialect=dialect, normalize_functions=\"lower\"))\n+            names.append(\n+                agg_all_unquoted.sql(dialect=dialect, normalize_functions=\"lower\")\n+            )\n \n     return names\n \n@@ -1607,9 +1708,13 @@ def generatedasidentitycolumnconstraint_sql(\n     return f\"IDENTITY({start}, {increment})\"\n \n \n-def arg_max_or_min_no_count(name: str) -> t.Callable[[Generator, exp.ArgMax | exp.ArgMin], str]:\n+def arg_max_or_min_no_count(\n+    name: str,\n+) -> t.Callable[[Generator, exp.ArgMax | exp.ArgMin], str]:\n     @unsupported_args(\"count\")\n-    def _arg_max_or_min_sql(self: Generator, expression: exp.ArgMax | exp.ArgMin) -> str:\n+    def _arg_max_or_min_sql(\n+        self: Generator, expression: exp.ArgMax | exp.ArgMin\n+    ) -> str:\n         return self.func(name, expression.this, expression.expression)\n \n     return _arg_max_or_min_sql\n@@ -1628,7 +1733,9 @@ def ts_or_ds_add_cast(expression: exp.TsOrDsAdd) -> exp.TsOrDsAdd:\n     return expression\n \n \n-def date_delta_sql(name: str, cast: bool = False) -> t.Callable[[Generator, DATE_ADD_OR_DIFF], str]:\n+def date_delta_sql(\n+    name: str, cast: bool = False\n+) -> t.Callable[[Generator, DATE_ADD_OR_DIFF], str]:\n     def _delta_sql(self: Generator, expression: DATE_ADD_OR_DIFF) -> str:\n         if cast and isinstance(expression, exp.TsOrDsAdd):\n             expression = ts_or_ds_add_cast(expression)\n@@ -1643,7 +1750,9 @@ def date_delta_sql(name: str, cast: bool = False) -> t.Callable[[Generator, DATE\n     return _delta_sql\n \n \n-def unit_to_str(expression: exp.Expression, default: str = \"DAY\") -> t.Optional[exp.Expression]:\n+def unit_to_str(\n+    expression: exp.Expression, default: str = \"DAY\"\n+) -> t.Optional[exp.Expression]:\n     unit = expression.args.get(\"unit\")\n \n     if isinstance(unit, exp.Placeholder):\n@@ -1653,7 +1762,9 @@ def unit_to_str(expression: exp.Expression, default: str = \"DAY\") -> t.Optional[\n     return exp.Literal.string(default) if default else None\n \n \n-def unit_to_var(expression: exp.Expression, default: str = \"DAY\") -> t.Optional[exp.Expression]:\n+def unit_to_var(\n+    expression: exp.Expression, default: str = \"DAY\"\n+) -> t.Optional[exp.Expression]:\n     unit = expression.args.get(\"unit\")\n \n     if isinstance(unit, (exp.Var, exp.Placeholder, exp.WeekStart)):\n@@ -1675,7 +1786,9 @@ def map_date_part(\n \n def map_date_part(part, dialect: DialectType = Dialect):\n     mapped = (\n-        Dialect.get_or_raise(dialect).DATE_PART_MAPPING.get(part.name.upper()) if part else None\n+        Dialect.get_or_raise(dialect).DATE_PART_MAPPING.get(part.name.upper())\n+        if part\n+        else None\n     )\n     if mapped:\n         return exp.Literal.string(mapped) if part.is_string else exp.var(mapped)\n@@ -1696,7 +1809,9 @@ def merge_without_target_sql(self: Generator, expression: exp.Merge) -> str:\n     alias = expression.this.args.get(\"alias\")\n \n     def normalize(identifier: t.Optional[exp.Identifier]) -> t.Optional[str]:\n-        return self.dialect.normalize_identifier(identifier).name if identifier else None\n+        return (\n+            self.dialect.normalize_identifier(identifier).name if identifier else None\n+        )\n \n     targets = {normalize(expression.this.this)}\n \n@@ -1728,7 +1843,9 @@ def merge_without_target_sql(self: Generator, expression: exp.Merge) -> str:\n \n \n def build_json_extract_path(\n-    expr_type: t.Type[F], zero_based_indexing: bool = True, arrow_req_json_type: bool = False\n+    expr_type: t.Type[F],\n+    zero_based_indexing: bool = True,\n+    arrow_req_json_type: bool = False,\n ) -> t.Callable[[t.List], F]:\n     def _builder(args: t.List) -> F:\n         segments: t.List[exp.JSONPathPart] = [exp.JSONPathRoot()]\n@@ -1741,7 +1858,9 @@ def build_json_extract_path(\n             if is_int(text) and (not arrow_req_json_type or not arg.is_string):\n                 index = int(text)\n                 segments.append(\n-                    exp.JSONPathSubscript(this=index if zero_based_indexing else index - 1)\n+                    exp.JSONPathSubscript(\n+                        this=index if zero_based_indexing else index - 1\n+                    )\n                 )\n             else:\n                 segments.append(exp.JSONPathKey(this=text))\n@@ -1812,7 +1931,9 @@ def filter_array_using_unnest(\n         return \"\"\n \n     unnest = exp.Unnest(expressions=[expression.this])\n-    filtered = exp.select(alias).from_(exp.alias_(unnest, None, table=[alias])).where(cond)\n+    filtered = (\n+        exp.select(alias).from_(exp.alias_(unnest, None, table=[alias])).where(cond)\n+    )\n     return self.sql(exp.Array(expressions=[filtered]))\n \n \n@@ -1821,7 +1942,8 @@ def remove_from_array_using_filter(self: Generator, expression: exp.ArrayRemove)\n     cond = exp.NEQ(this=lambda_id, expression=expression.expression)\n     return self.sql(\n         exp.ArrayFilter(\n-            this=expression.this, expression=exp.Lambda(this=cond, expressions=[lambda_id])\n+            this=expression.this,\n+            expression=exp.Lambda(this=cond, expressions=[lambda_id]),\n         )\n     )\n \n@@ -1861,7 +1983,9 @@ def sha256_sql(self: Generator, expression: exp.SHA2) -> str:\n     return self.func(f\"SHA{expression.text('length') or '256'}\", expression.this)\n \n \n-def sequence_sql(self: Generator, expression: exp.GenerateSeries | exp.GenerateDateArray) -> str:\n+def sequence_sql(\n+    self: Generator, expression: exp.GenerateSeries | exp.GenerateDateArray\n+) -> str:\n     start = expression.args.get(\"start\")\n     end = expression.args.get(\"end\")\n     step = expression.args.get(\"step\")\n@@ -1887,7 +2011,8 @@ def build_regexp_extract(expr_type: t.Type[E]) -> t.Callable[[t.List, Dialect],\n         return expr_type(\n             this=seq_get(args, 0),\n             expression=seq_get(args, 1),\n-            group=seq_get(args, 2) or exp.Literal.number(dialect.REGEXP_EXTRACT_DEFAULT_GROUP),\n+            group=seq_get(args, 2)\n+            or exp.Literal.number(dialect.REGEXP_EXTRACT_DEFAULT_GROUP),\n             parameters=seq_get(args, 3),\n         )\n \n@@ -1909,11 +2034,17 @@ def explode_to_unnest_sql(self: Generator, expression: exp.Lateral) -> str:\n     return self.lateral_sql(expression)\n \n \n-def timestampdiff_sql(self: Generator, expression: exp.DatetimeDiff | exp.TimestampDiff) -> str:\n-    return self.func(\"TIMESTAMPDIFF\", expression.unit, expression.expression, expression.this)\n+def timestampdiff_sql(\n+    self: Generator, expression: exp.DatetimeDiff | exp.TimestampDiff\n+) -> str:\n+    return self.func(\n+        \"TIMESTAMPDIFF\", expression.unit, expression.expression, expression.this\n+    )\n \n \n-def no_make_interval_sql(self: Generator, expression: exp.MakeInterval, sep: str = \", \") -> str:\n+def no_make_interval_sql(\n+    self: Generator, expression: exp.MakeInterval, sep: str = \", \"\n+) -> str:\n     args = []\n     for unit, value in expression.args.items():\n         if isinstance(value, exp.Kwarg):\n@@ -1941,7 +2072,9 @@ def groupconcat_sql(\n     separator = self.sql(expression.args.get(\"separator\") or exp.Literal.string(sep))\n \n     on_overflow_sql = self.sql(expression, \"on_overflow\")\n-    on_overflow_sql = f\" ON OVERFLOW {on_overflow_sql}\" if (on_overflow and on_overflow_sql) else \"\"\n+    on_overflow_sql = (\n+        f\" ON OVERFLOW {on_overflow_sql}\" if (on_overflow and on_overflow_sql) else \"\"\n+    )\n \n     order = this.find(exp.Order)\n \n@@ -1955,12 +2088,16 @@ def groupconcat_sql(\n         if within_group:\n             listagg = exp.WithinGroup(this=listagg, expression=order)\n         else:\n-            listagg.set(\"expressions\", [f\"{args}{self.sql(expression=expression.this)}\"])\n+            listagg.set(\n+                \"expressions\", [f\"{args}{self.sql(expression=expression.this)}\"]\n+            )\n \n     return self.sql(listagg)\n \n \n-def build_timetostr_or_tochar(args: t.List, dialect: Dialect) -> exp.TimeToStr | exp.ToChar:\n+def build_timetostr_or_tochar(\n+    args: t.List, dialect: Dialect\n+) -> exp.TimeToStr | exp.ToChar:\n     if len(args) == 2:\n         this = args[0]\n         if not this.type:\ndiff --git a/sqlglot/dialects/duckdb.py b/sqlglot/dialects/duckdb.py\nindex baaebed62..4136f3200 100644\n--- a/sqlglot/dialects/duckdb.py\n+++ b/sqlglot/dialects/duckdb.py\n@@ -46,7 +46,12 @@ from sqlglot.tokens import TokenType\n from sqlglot.parser import binary_range_parser\n \n DATETIME_DELTA = t.Union[\n-    exp.DateAdd, exp.TimeAdd, exp.DatetimeAdd, exp.TsOrDsAdd, exp.DateSub, exp.DatetimeSub\n+    exp.DateAdd,\n+    exp.TimeAdd,\n+    exp.DatetimeAdd,\n+    exp.TsOrDsAdd,\n+    exp.DateSub,\n+    exp.DatetimeSub,\n ]\n \n \n@@ -55,7 +60,9 @@ def _date_delta_sql(self: DuckDB.Generator, expression: DATETIME_DELTA) -> str:\n     unit = unit_to_var(expression)\n     op = (\n         \"+\"\n-        if isinstance(expression, (exp.DateAdd, exp.TimeAdd, exp.DatetimeAdd, exp.TsOrDsAdd))\n+        if isinstance(\n+            expression, (exp.DateAdd, exp.TimeAdd, exp.DatetimeAdd, exp.TsOrDsAdd)\n+        )\n         else \"-\"\n     )\n \n@@ -73,7 +80,9 @@ def _date_delta_sql(self: DuckDB.Generator, expression: DATETIME_DELTA) -> str:\n     this = exp.cast(this, to_type) if to_type else this\n \n     expr = expression.expression\n-    interval = expr if isinstance(expr, exp.Interval) else exp.Interval(this=expr, unit=unit)\n+    interval = (\n+        expr if isinstance(expr, exp.Interval) else exp.Interval(this=expr, unit=unit)\n+    )\n \n     return f\"{self.sql(this)} {op} {self.sql(interval)}\"\n \n@@ -109,7 +118,11 @@ def _array_sort_sql(self: DuckDB.Generator, expression: exp.ArraySort) -> str:\n \n \n def _sort_array_sql(self: DuckDB.Generator, expression: exp.SortArray) -> str:\n-    name = \"ARRAY_REVERSE_SORT\" if expression.args.get(\"asc\") == exp.false() else \"ARRAY_SORT\"\n+    name = (\n+        \"ARRAY_REVERSE_SORT\"\n+        if expression.args.get(\"asc\") == exp.false()\n+        else \"ARRAY_SORT\"\n+    )\n     return self.func(name, expression.this)\n \n \n@@ -118,10 +131,14 @@ def _build_sort_array_desc(args: t.List) -> exp.Expression:\n \n \n def _build_date_diff(args: t.List) -> exp.Expression:\n-    return exp.DateDiff(this=seq_get(args, 2), expression=seq_get(args, 1), unit=seq_get(args, 0))\n+    return exp.DateDiff(\n+        this=seq_get(args, 2), expression=seq_get(args, 1), unit=seq_get(args, 0)\n+    )\n \n \n-def _build_generate_series(end_exclusive: bool = False) -> t.Callable[[t.List], exp.GenerateSeries]:\n+def _build_generate_series(\n+    end_exclusive: bool = False,\n+) -> t.Callable[[t.List], exp.GenerateSeries]:\n     def _builder(args: t.List) -> exp.GenerateSeries:\n         # Check https://duckdb.org/docs/sql/functions/nested.html#range-functions\n         if len(args) == 1:\n@@ -150,7 +167,9 @@ def _build_make_timestamp(args: t.List) -> exp.Expression:\n     )\n \n \n-def _show_parser(*args: t.Any, **kwargs: t.Any) -> t.Callable[[DuckDB.Parser], exp.Show]:\n+def _show_parser(\n+    *args: t.Any, **kwargs: t.Any\n+) -> t.Callable[[DuckDB.Parser], exp.Show]:\n     def _parse(self: DuckDB.Parser) -> exp.Show:\n         return self._parse_show_duckdb(*args, **kwargs)\n \n@@ -226,13 +245,17 @@ def _unix_to_time_sql(self: DuckDB.Generator, expression: exp.UnixToTime) -> str\n     if scale == exp.UnixToTime.MICROS:\n         return self.func(\"MAKE_TIMESTAMP\", timestamp)\n \n-    return self.func(\"TO_TIMESTAMP\", exp.Div(this=timestamp, expression=exp.func(\"POW\", 10, scale)))\n+    return self.func(\n+        \"TO_TIMESTAMP\", exp.Div(this=timestamp, expression=exp.func(\"POW\", 10, scale))\n+    )\n \n \n WRAPPED_JSON_EXTRACT_EXPRESSIONS = (exp.Binary, exp.Bracket, exp.In)\n \n \n-def _arrow_json_extract_sql(self: DuckDB.Generator, expression: JSON_EXTRACT_TYPE) -> str:\n+def _arrow_json_extract_sql(\n+    self: DuckDB.Generator, expression: JSON_EXTRACT_TYPE\n+) -> str:\n     arrow_sql = arrow_json_extract_sql(self, expression)\n     if not expression.same_parent and isinstance(\n         expression.parent, WRAPPED_JSON_EXTRACT_EXPRESSIONS\n@@ -255,11 +278,16 @@ def _date_diff_sql(self: DuckDB.Generator, expression: exp.DateDiff) -> str:\n \n \n def _generate_datetime_array_sql(\n-    self: DuckDB.Generator, expression: t.Union[exp.GenerateDateArray, exp.GenerateTimestampArray]\n+    self: DuckDB.Generator,\n+    expression: t.Union[exp.GenerateDateArray, exp.GenerateTimestampArray],\n ) -> str:\n     is_generate_date_array = isinstance(expression, exp.GenerateDateArray)\n \n-    type = exp.DataType.Type.DATE if is_generate_date_array else exp.DataType.Type.TIMESTAMP\n+    type = (\n+        exp.DataType.Type.DATE\n+        if is_generate_date_array\n+        else exp.DataType.Type.TIMESTAMP\n+    )\n     start = _implicit_datetime_cast(expression.args.get(\"start\"), type=type)\n     end = _implicit_datetime_cast(expression.args.get(\"end\"), type=type)\n \n@@ -279,8 +307,12 @@ def _generate_datetime_array_sql(\n def _json_extract_value_array_sql(\n     self: DuckDB.Generator, expression: exp.JSONValueArray | exp.JSONExtractArray\n ) -> str:\n-    json_extract = exp.JSONExtract(this=expression.this, expression=expression.expression)\n-    data_type = \"ARRAY<STRING>\" if isinstance(expression, exp.JSONValueArray) else \"ARRAY<JSON>\"\n+    json_extract = exp.JSONExtract(\n+        this=expression.this, expression=expression.expression\n+    )\n+    data_type = (\n+        \"ARRAY<STRING>\" if isinstance(expression, exp.JSONValueArray) else \"ARRAY<JSON>\"\n+    )\n     return self.sql(exp.cast(json_extract, to=exp.DataType.build(data_type)))\n \n \n@@ -304,7 +336,9 @@ class DuckDB(Dialect):\n     }\n     DATE_PART_MAPPING.pop(\"WEEKDAY\")\n \n-    def to_json_path(self, path: t.Optional[exp.Expression]) -> t.Optional[exp.Expression]:\n+    def to_json_path(\n+        self, path: t.Optional[exp.Expression]\n+    ) -> t.Optional[exp.Expression]:\n         if isinstance(path, exp.Literal):\n             # DuckDB also supports the JSON pointer syntax, where every path starts with a `/`.\n             # Additionally, it allows accessing the back of lists using the `[#-i]` syntax.\n@@ -387,7 +421,10 @@ class DuckDB(Dialect):\n             TokenType.DSTAR: exp.Pow,\n         }\n \n-        FUNCTIONS_WITH_ALIASED_ARGS = {*parser.Parser.FUNCTIONS_WITH_ALIASED_ARGS, \"STRUCT_PACK\"}\n+        FUNCTIONS_WITH_ALIASED_ARGS = {\n+            *parser.Parser.FUNCTIONS_WITH_ALIASED_ARGS,\n+            \"STRUCT_PACK\",\n+        }\n \n         SHOW_PARSERS = {\n             \"TABLES\": _show_parser(\"TABLES\"),\n@@ -416,7 +453,9 @@ class DuckDB(Dialect):\n             \"GENERATE_SERIES\": _build_generate_series(),\n             \"JSON\": exp.ParseJSON.from_arg_list,\n             \"JSON_EXTRACT_PATH\": parser.build_extract_json_with_path(exp.JSONExtract),\n-            \"JSON_EXTRACT_STRING\": parser.build_extract_json_with_path(exp.JSONExtractScalar),\n+            \"JSON_EXTRACT_STRING\": parser.build_extract_json_with_path(\n+                exp.JSONExtractScalar\n+            ),\n             \"LIST_CONTAINS\": exp.ArrayContains.from_arg_list,\n             \"LIST_HAS\": exp.ArrayContains.from_arg_list,\n             \"LIST_HAS_ANY\": exp.ArrayOverlaps.from_arg_list,\n@@ -437,7 +476,9 @@ class DuckDB(Dialect):\n                 replacement=seq_get(args, 2),\n                 modifiers=seq_get(args, 3),\n             ),\n-            \"SHA256\": lambda args: exp.SHA2(this=seq_get(args, 0), length=exp.Literal.number(256)),\n+            \"SHA256\": lambda args: exp.SHA2(\n+                this=seq_get(args, 0), length=exp.Literal.number(256)\n+            ),\n             \"STRFTIME\": build_formatted_time(exp.TimeToStr, \"duckdb\"),\n             \"STRING_SPLIT\": exp.Split.from_arg_list,\n             \"STRING_SPLIT_REGEX\": exp.RegexpSplit.from_arg_list,\n@@ -458,7 +499,8 @@ class DuckDB(Dialect):\n         FUNCTION_PARSERS = {\n             **parser.Parser.FUNCTION_PARSERS,\n             **dict.fromkeys(\n-                (\"GROUP_CONCAT\", \"LISTAGG\", \"STRINGAGG\"), lambda self: self._parse_string_agg()\n+                (\"GROUP_CONCAT\", \"LISTAGG\", \"STRINGAGG\"),\n+                lambda self: self._parse_string_agg(),\n             ),\n         }\n         FUNCTION_PARSERS.pop(\"DECODE\")\n@@ -485,7 +527,9 @@ class DuckDB(Dialect):\n \n         TYPE_CONVERTERS = {\n             # https://duckdb.org/docs/sql/data_types/numeric\n-            exp.DataType.Type.DECIMAL: build_default_decimal_type(precision=18, scale=3),\n+            exp.DataType.Type.DECIMAL: build_default_decimal_type(\n+                precision=18, scale=3\n+            ),\n             # https://duckdb.org/docs/sql/data_types/text\n             exp.DataType.Type.TEXT: lambda dtype: exp.DataType.build(\"TEXT\"),\n         }\n@@ -513,7 +557,9 @@ class DuckDB(Dialect):\n                 return None\n \n             this = self._replace_lambda(self._parse_assignment(), expressions)\n-            return self.expression(exp.Lambda, this=this, expressions=expressions, colon=True)\n+            return self.expression(\n+                exp.Lambda, this=this, expressions=expressions, colon=True\n+            )\n \n         def _parse_expression(self) -> t.Optional[exp.Expression]:\n             # DuckDB supports prefix aliases, e.g. foo: 1\n@@ -527,7 +573,9 @@ class DuckDB(Dialect):\n                     # Moves the comment next to the alias in `alias: expr /* comment */`\n                     comments += this.pop_comments() or []\n \n-                return self.expression(exp.Alias, comments=comments, this=this, alias=alias)\n+                return self.expression(\n+                    exp.Alias, comments=comments, this=this, alias=alias\n+                )\n \n             return super()._parse_expression()\n \n@@ -568,7 +616,9 @@ class DuckDB(Dialect):\n \n             return table\n \n-        def _parse_table_sample(self, as_modifier: bool = False) -> t.Optional[exp.TableSample]:\n+        def _parse_table_sample(\n+            self, as_modifier: bool = False\n+        ) -> t.Optional[exp.TableSample]:\n             # https://duckdb.org/docs/sql/samples.html\n             sample = super()._parse_table_sample(as_modifier=as_modifier)\n             if sample and not sample.args.get(\"method\"):\n@@ -584,7 +634,9 @@ class DuckDB(Dialect):\n         ) -> t.Optional[exp.Expression]:\n             bracket = super()._parse_bracket(this)\n \n-            if self.dialect.version < Version(\"1.2.0\") and isinstance(bracket, exp.Bracket):\n+            if self.dialect.version < Version(\"1.2.0\") and isinstance(\n+                bracket, exp.Bracket\n+            ):\n                 # https://duckdb.org/2025/02/05/announcing-duckdb-120.html#breaking-changes\n                 bracket.set(\"returns_list_for_maps\", True)\n \n@@ -595,12 +647,18 @@ class DuckDB(Dialect):\n                 return self.expression(exp.ToMap, this=self._parse_bracket())\n \n             args = self._parse_wrapped_csv(self._parse_assignment)\n-            return self.expression(exp.Map, keys=seq_get(args, 0), values=seq_get(args, 1))\n+            return self.expression(\n+                exp.Map, keys=seq_get(args, 0), values=seq_get(args, 1)\n+            )\n \n-        def _parse_struct_types(self, type_required: bool = False) -> t.Optional[exp.Expression]:\n+        def _parse_struct_types(\n+            self, type_required: bool = False\n+        ) -> t.Optional[exp.Expression]:\n             return self._parse_field_def()\n \n-        def _pivot_column_names(self, aggregations: t.List[exp.Expression]) -> t.List[str]:\n+        def _pivot_column_names(\n+            self, aggregations: t.List[exp.Expression]\n+        ) -> t.List[str]:\n             if len(aggregations) == 1:\n                 return super()._pivot_column_names(aggregations)\n             return pivot_column_names(aggregations, dialect=\"duckdb\")\n@@ -623,7 +681,9 @@ class DuckDB(Dialect):\n                 expressions = None\n \n             return (\n-                self.expression(exp.Attach, this=this, exists=exists, expressions=expressions)\n+                self.expression(\n+                    exp.Attach, this=this, exists=exists, expressions=expressions\n+                )\n                 if is_attach\n                 else self.expression(exp.Detach, this=this, exists=exists)\n             )\n@@ -696,14 +756,22 @@ class DuckDB(Dialect):\n             exp.DatetimeAdd: _date_delta_sql,\n             exp.DateToDi: lambda self,\n             e: f\"CAST(STRFTIME({self.sql(e, 'this')}, {DuckDB.DATEINT_FORMAT}) AS INT)\",\n-            exp.Decode: lambda self, e: encode_decode_sql(self, e, \"DECODE\", replace=False),\n+            exp.Decode: lambda self, e: encode_decode_sql(\n+                self, e, \"DECODE\", replace=False\n+            ),\n             exp.DiToDate: lambda self,\n             e: f\"CAST(STRPTIME(CAST({self.sql(e, 'this')} AS TEXT), {DuckDB.DATEINT_FORMAT}) AS DATE)\",\n-            exp.Encode: lambda self, e: encode_decode_sql(self, e, \"ENCODE\", replace=False),\n+            exp.Encode: lambda self, e: encode_decode_sql(\n+                self, e, \"ENCODE\", replace=False\n+            ),\n             exp.GenerateDateArray: _generate_datetime_array_sql,\n             exp.GenerateTimestampArray: _generate_datetime_array_sql,\n-            exp.GroupConcat: lambda self, e: groupconcat_sql(self, e, within_group=False),\n-            exp.HexString: lambda self, e: self.hexstring_sql(e, binary_function_repr=\"FROM_HEX\"),\n+            exp.GroupConcat: lambda self, e: groupconcat_sql(\n+                self, e, within_group=False\n+            ),\n+            exp.HexString: lambda self, e: self.hexstring_sql(\n+                e, binary_function_repr=\"FROM_HEX\"\n+            ),\n             exp.Explode: rename_func(\"UNNEST\"),\n             exp.IntDiv: lambda self, e: self.binary(e, \"//\"),\n             exp.IsInf: rename_func(\"ISINF\"),\n@@ -743,7 +811,9 @@ class DuckDB(Dialect):\n             ),\n             exp.RegexpSplit: rename_func(\"STR_SPLIT_REGEX\"),\n             exp.Return: lambda self, e: self.sql(e, \"this\"),\n-            exp.ReturnsProperty: lambda self, e: \"TABLE\" if isinstance(e.this, exp.Schema) else \"\",\n+            exp.ReturnsProperty: lambda self, e: \"TABLE\"\n+            if isinstance(e.this, exp.Schema)\n+            else \"\",\n             exp.Rand: rename_func(\"RANDOM\"),\n             exp.SHA: rename_func(\"SHA1\"),\n             exp.SHA2: sha256_sql,\n@@ -763,12 +833,16 @@ class DuckDB(Dialect):\n                 \"DATE_DIFF\", exp.Literal.string(e.unit), e.expression, e.this\n             ),\n             exp.TimestampTrunc: timestamptrunc_sql(),\n-            exp.TimeStrToDate: lambda self, e: self.sql(exp.cast(e.this, exp.DataType.Type.DATE)),\n+            exp.TimeStrToDate: lambda self, e: self.sql(\n+                exp.cast(e.this, exp.DataType.Type.DATE)\n+            ),\n             exp.TimeStrToTime: timestrtotime_sql,\n             exp.TimeStrToUnix: lambda self, e: self.func(\n                 \"EPOCH\", exp.cast(e.this, exp.DataType.Type.TIMESTAMP)\n             ),\n-            exp.TimeToStr: lambda self, e: self.func(\"STRFTIME\", e.this, self.format_time(e)),\n+            exp.TimeToStr: lambda self, e: self.func(\n+                \"STRFTIME\", e.this, self.format_time(e)\n+            ),\n             exp.TimeToUnix: rename_func(\"EPOCH\"),\n             exp.TsOrDiToDi: lambda self,\n             e: f\"CAST(SUBSTR(REPLACE(CAST({self.sql(e, 'this')} AS TEXT), '-', ''), 1, 8) AS INT)\",\n@@ -783,16 +857,19 @@ class DuckDB(Dialect):\n                 \"STRFTIME\", self.func(\"TO_TIMESTAMP\", e.this), self.format_time(e)\n             ),\n             exp.DatetimeTrunc: lambda self, e: self.func(\n-                \"DATE_TRUNC\", unit_to_str(e), exp.cast(e.this, exp.DataType.Type.DATETIME)\n+                \"DATE_TRUNC\",\n+                unit_to_str(e),\n+                exp.cast(e.this, exp.DataType.Type.DATETIME),\n             ),\n             exp.UnixToTime: _unix_to_time_sql,\n-            exp.UnixToTimeStr: lambda self, e: f\"CAST(TO_TIMESTAMP({self.sql(e, 'this')}) AS TEXT)\",\n+            exp.UnixToTimeStr: lambda self,\n+            e: f\"CAST(TO_TIMESTAMP({self.sql(e, 'this')}) AS TEXT)\",\n             exp.VariancePop: rename_func(\"VAR_POP\"),\n             exp.WeekOfYear: rename_func(\"WEEKOFYEAR\"),\n             exp.Xor: bool_xor_sql,\n-            exp.Levenshtein: unsupported_args(\"ins_cost\", \"del_cost\", \"sub_cost\", \"max_dist\")(\n-                rename_func(\"LEVENSHTEIN\")\n-            ),\n+            exp.Levenshtein: unsupported_args(\n+                \"ins_cost\", \"del_cost\", \"sub_cost\", \"max_dist\"\n+            )(rename_func(\"LEVENSHTEIN\")),\n             exp.JSONObjectAgg: rename_func(\"JSON_GROUP_OBJECT\"),\n             exp.JSONBObjectAgg: rename_func(\"JSON_GROUP_OBJECT\"),\n             exp.DateBin: rename_func(\"TIME_BUCKET\"),\n@@ -963,14 +1040,18 @@ class DuckDB(Dialect):\n         def parsejson_sql(self, expression: exp.ParseJSON) -> str:\n             arg = expression.this\n             if expression.args.get(\"safe\"):\n-                return self.sql(exp.case().when(exp.func(\"json_valid\", arg), arg).else_(exp.null()))\n+                return self.sql(\n+                    exp.case().when(exp.func(\"json_valid\", arg), arg).else_(exp.null())\n+                )\n             return self.func(\"JSON\", arg)\n \n         def timefromparts_sql(self, expression: exp.TimeFromParts) -> str:\n             nano = expression.args.get(\"nano\")\n             if nano is not None:\n                 expression.set(\n-                    \"sec\", expression.args[\"sec\"] + nano.pop() / exp.Literal.number(1000000000.0)\n+                    \"sec\",\n+                    expression.args[\"sec\"]\n+                    + nano.pop() / exp.Literal.number(1000000000.0),\n                 )\n \n             return rename_func(\"MAKE_TIME\")(self, expression)\n@@ -1009,7 +1090,9 @@ class DuckDB(Dialect):\n                     )\n                     expression.set(\"method\", exp.var(\"RESERVOIR\"))\n \n-            return super().tablesample_sql(expression, tablesample_keyword=tablesample_keyword)\n+            return super().tablesample_sql(\n+                expression, tablesample_keyword=tablesample_keyword\n+            )\n \n         def columndef_sql(self, expression: exp.ColumnDef, sep: str = \" \") -> str:\n             if isinstance(expression.parent, exp.UserDefinedFunction):\n@@ -1131,7 +1214,9 @@ class DuckDB(Dialect):\n                 # In BigQuery, UNNESTing a nested array leads to explosion of the top-level array & struct\n                 # This is transpiled to DDB by transforming \"FROM UNNEST(...)\" to \"FROM (SELECT UNNEST(..., max_depth => 2))\"\n                 expression.expressions.append(\n-                    exp.Kwarg(this=exp.var(\"max_depth\"), expression=exp.Literal.number(2))\n+                    exp.Kwarg(\n+                        this=exp.var(\"max_depth\"), expression=exp.Literal.number(2)\n+                    )\n                 )\n \n                 # If BQ's UNNEST is aliased, we transform it from a column alias to a table alias in DDB\n@@ -1201,7 +1286,9 @@ class DuckDB(Dialect):\n             return self.function_fallback_sql(expression)\n \n         def autoincrementcolumnconstraint_sql(self, _) -> str:\n-            self.unsupported(\"The AUTOINCREMENT column constraint is not supported by DuckDB\")\n+            self.unsupported(\n+                \"The AUTOINCREMENT column constraint is not supported by DuckDB\"\n+            )\n             return \"\"\n \n         def aliases_sql(self, expression: exp.Aliases) -> str:\n@@ -1234,7 +1321,8 @@ class DuckDB(Dialect):\n             gen_subscripts = self.sql(\n                 exp.Alias(\n                     this=exp.Anonymous(\n-                        this=\"GENERATE_SUBSCRIPTS\", expressions=[this, exp.Literal.number(1)]\n+                        this=\"GENERATE_SUBSCRIPTS\",\n+                        expressions=[this, exp.Literal.number(1)],\n                     )\n                     - exp.Literal.number(1),\n                     alias=pos,\n@@ -1243,9 +1331,13 @@ class DuckDB(Dialect):\n \n             posexplode_sql = self.format_args(gen_subscripts, unnest_sql)\n \n-            if isinstance(parent, exp.From) or (parent and isinstance(parent.parent, exp.From)):\n+            if isinstance(parent, exp.From) or (\n+                parent and isinstance(parent.parent, exp.From)\n+            ):\n                 # SELECT * FROM POSEXPLODE(col) -> SELECT * FROM (SELECT GENERATE_SUBSCRIPTS(...), UNNEST(...))\n-                return self.sql(exp.Subquery(this=exp.Select(expressions=[posexplode_sql])))\n+                return self.sql(\n+                    exp.Subquery(this=exp.Select(expressions=[posexplode_sql]))\n+                )\n \n             return posexplode_sql\n \n@@ -1258,10 +1350,14 @@ class DuckDB(Dialect):\n                 this = annotate_types(this, dialect=self.dialect)\n \n             if this.is_type(*exp.DataType.TEXT_TYPES):\n-                this = exp.Cast(this=this, to=exp.DataType(this=exp.DataType.Type.TIMESTAMP))\n+                this = exp.Cast(\n+                    this=this, to=exp.DataType(this=exp.DataType.Type.TIMESTAMP)\n+                )\n \n             func = self.func(\n-                \"DATE_ADD\", this, exp.Interval(this=expression.expression, unit=exp.var(\"MONTH\"))\n+                \"DATE_ADD\",\n+                this,\n+                exp.Interval(this=expression.expression, unit=exp.var(\"MONTH\")),\n             )\n \n             # DuckDB's DATE_ADD function returns TIMESTAMP/DATETIME by default, even when the input is DATE\ndiff --git a/sqlglot/executor/python.py b/sqlglot/executor/python.py\nindex ce7a2a872..7ec52612c 100644\n--- a/sqlglot/executor/python.py\n+++ b/sqlglot/executor/python.py\n@@ -81,7 +81,9 @@ class PythonExecutor:\n \n     def table(self, expressions):\n         return Table(\n-            expression.alias_or_name if isinstance(expression, exp.Expression) else expression\n+            expression.alias_or_name\n+            if isinstance(expression, exp.Expression)\n+            else expression\n             for expression in expressions\n         )\n \n@@ -97,13 +99,17 @@ class PythonExecutor:\n             if not step.projections and not step.condition:\n                 return self.context({step.name: context.tables[source]})\n             table_iter = context.table_iter(source)\n-        elif isinstance(step.source, exp.Table) and isinstance(step.source.this, exp.ReadCSV):\n+        elif isinstance(step.source, exp.Table) and isinstance(\n+            step.source.this, exp.ReadCSV\n+        ):\n             table_iter = self.scan_csv(step)\n             context = next(table_iter)\n         else:\n             context, table_iter = self.scan_table(step)\n \n-        return self.context({step.name: self._project_and_filter(context, step, table_iter)})\n+        return self.context(\n+            {step.name: self._project_and_filter(context, step, table_iter)}\n+        )\n \n     def _project_and_filter(self, context, step, table_iter):\n         sink = self.table(step.projections if step.projections else context.columns)\n@@ -152,7 +158,10 @@ class PythonExecutor:\n \n                 # We can't cast empty values ('') to non-string types, so we convert them to None instead\n                 context.set_row(\n-                    tuple(None if (t is not str and v == \"\") else t(v) for t, v in zip(types, row))\n+                    tuple(\n+                        None if (t is not str and v == \"\") else t(v)\n+                        for t, v in zip(types, row)\n+                    )\n                 )\n                 yield context.table.reader\n \n@@ -226,7 +235,9 @@ class PythonExecutor:\n             results[ctx.eval_tuple(join_key)][1].append(reader.row)\n \n         table = Table(source_context.columns + join_context.columns)\n-        nulls = [(None,) * len(join_context.columns if left else source_context.columns)]\n+        nulls = [\n+            (None,) * len(join_context.columns if left else source_context.columns)\n+        ]\n \n         for a_group, b_group in results.values():\n             if left:\n@@ -300,7 +311,9 @@ class PythonExecutor:\n             context.set_range(0, 0)\n             table.append(context.eval_tuple(aggregations))\n \n-        context = self.context({step.name: table, **{name: table for name in context.tables}})\n+        context = self.context(\n+            {step.name: table, **{name: table for name in context.tables}}\n+        )\n \n         if step.projections or step.condition:\n             return self.scan(step, context)\n@@ -327,7 +340,9 @@ class PythonExecutor:\n \n         output = Table(\n             projection_columns,\n-            rows=[r[len(context.columns) : len(all_columns)] for r in sort_ctx.table.rows],\n+            rows=[\n+                r[len(context.columns) : len(all_columns)] for r in sort_ctx.table.rows\n+            ],\n         )\n         return self.context({step.name: output})\n \n@@ -370,7 +385,9 @@ def _rename(self, e):\n             return self.func(e.key, *values)\n \n         if isinstance(e, exp.Func) and e.is_var_len_args:\n-            args = itertools.chain.from_iterable(x if isinstance(x, list) else [x] for x in values)\n+            args = itertools.chain.from_iterable(\n+                x if isinstance(x, list) else [x] for x in values\n+            )\n             return self.func(e.key, *args)\n \n         return self.func(e.key, *values)\n@@ -396,7 +413,9 @@ def _lambda_sql(self, e: exp.Lambda) -> str:\n \n     e = e.transform(\n         lambda n: (\n-            exp.var(n.name) if isinstance(n, exp.Identifier) and n.name.lower() in names else n\n+            exp.var(n.name)\n+            if isinstance(n, exp.Identifier) and n.name.lower() in names\n+            else n\n         )\n     ).assert_is(exp.Lambda)\n \n@@ -431,7 +450,8 @@ class Python(Dialect):\n             exp.And: lambda self, e: self.binary(e, \"and\"),\n             exp.Between: _rename,\n             exp.Boolean: lambda self, e: \"True\" if e.this else \"False\",\n-            exp.Cast: lambda self, e: f\"CAST({self.sql(e.this)}, exp.DataType.Type.{e.args['to']})\",\n+            exp.Cast: lambda self,\n+            e: f\"CAST({self.sql(e.this)}, exp.DataType.Type.{e.args['to']})\",\n             exp.Column: lambda self,\n             e: f\"scope[{self.sql(e, 'table') or None}][{self.sql(e.this)}]\",\n             exp.Concat: lambda self, e: self.func(\n@@ -443,12 +463,18 @@ class Python(Dialect):\n             e: f\"EXTRACT('{e.name.lower()}', {self.sql(e, 'expression')})\",\n             exp.In: lambda self,\n             e: f\"{self.sql(e, 'this')} in {{{self.expressions(e, flat=True)}}}\",\n-            exp.Interval: lambda self, e: f\"INTERVAL({self.sql(e.this)}, '{self.sql(e.unit)}')\",\n+            exp.Interval: lambda self,\n+            e: f\"INTERVAL({self.sql(e.this)}, '{self.sql(e.unit)}')\",\n             exp.Is: lambda self, e: (\n-                self.binary(e, \"==\") if isinstance(e.this, exp.Literal) else self.binary(e, \"is\")\n+                self.binary(e, \"==\")\n+                if isinstance(e.this, exp.Literal)\n+                else self.binary(e, \"is\")\n+            ),\n+            exp.JSONExtract: lambda self, e: self.func(\n+                e.key, e.this, e.expression, *e.expressions\n             ),\n-            exp.JSONExtract: lambda self, e: self.func(e.key, e.this, e.expression, *e.expressions),\n-            exp.JSONPath: lambda self, e: f\"[{','.join(self.sql(p) for p in e.expressions[1:])}]\",\n+            exp.JSONPath: lambda self,\n+            e: f\"[{','.join(self.sql(p) for p in e.expressions[1:])}]\",\n             exp.JSONPathKey: lambda self, e: f\"'{self.sql(e.this)}'\",\n             exp.JSONPathSubscript: lambda self, e: f\"'{e.this}'\",\n             exp.Lambda: _lambda_sql,\ndiff --git a/sqlglot/expressions.py b/sqlglot/expressions.py\nindex eb2c2bad0..6802d16c6 100644\n--- a/sqlglot/expressions.py\n+++ b/sqlglot/expressions.py\n@@ -102,7 +102,16 @@ class Expression(metaclass=_Expression):\n \n     key = \"expression\"\n     arg_types = {\"this\": True}\n-    __slots__ = (\"args\", \"parent\", \"arg_key\", \"index\", \"comments\", \"_type\", \"_meta\", \"_hash\")\n+    __slots__ = (\n+        \"args\",\n+        \"parent\",\n+        \"arg_key\",\n+        \"index\",\n+        \"comments\",\n+        \"_type\",\n+        \"_meta\",\n+        \"_hash\",\n+    )\n \n     def __init__(self, **args: t.Any):\n         self.args: t.Dict[str, t.Any] = args\n@@ -201,7 +210,9 @@ class Expression(metaclass=_Expression):\n     @property\n     def is_star(self) -> bool:\n         \"\"\"Checks whether an expression is a star.\"\"\"\n-        return isinstance(self, Star) or (isinstance(self, Column) and isinstance(self.this, Star))\n+        return isinstance(self, Star) or (\n+            isinstance(self, Column) and isinstance(self.this, Star)\n+        )\n \n     @property\n     def alias(self) -> str:\n@@ -307,7 +318,9 @@ class Expression(metaclass=_Expression):\n         \"\"\"\n         return deepcopy(self)\n \n-    def add_comments(self, comments: t.Optional[t.List[str]] = None, prepend: bool = False) -> None:\n+    def add_comments(\n+        self, comments: t.Optional[t.List[str]] = None, prepend: bool = False\n+    ) -> None:\n         if self.comments is None:\n             self.comments = []\n \n@@ -391,7 +404,9 @@ class Expression(metaclass=_Expression):\n         self.args[arg_key] = value\n         self._set_parent(arg_key, value, index)\n \n-    def _set_parent(self, arg_key: str, value: t.Any, index: t.Optional[int] = None) -> None:\n+    def _set_parent(\n+        self, arg_key: str, value: t.Any, index: t.Optional[int] = None\n+    ) -> None:\n         if hasattr(value, \"parent\"):\n             value.parent = self\n             value.arg_key = arg_key\n@@ -584,9 +599,13 @@ class Expression(metaclass=_Expression):\n \n         A AND B AND C -> [A, B, C]\n         \"\"\"\n-        for node in self.dfs(prune=lambda n: n.parent and type(n) is not self.__class__):\n+        for node in self.dfs(\n+            prune=lambda n: n.parent and type(n) is not self.__class__\n+        ):\n             if type(node) is not self.__class__:\n-                yield node.unnest() if unnest and not isinstance(node, Subquery) else node\n+                yield (\n+                    node.unnest() if unnest and not isinstance(node, Subquery) else node\n+                )\n \n     def __str__(self) -> str:\n         return self.sql()\n@@ -616,7 +635,9 @@ class Expression(metaclass=_Expression):\n \n         return Dialect.get_or_raise(dialect).generate(self, **opts)\n \n-    def transform(self, fun: t.Callable, *args: t.Any, copy: bool = True, **kwargs) -> Expression:\n+    def transform(\n+        self, fun: t.Callable, *args: t.Any, copy: bool = True, **kwargs\n+    ) -> Expression:\n         \"\"\"\n         Visits all tree nodes (excluding already transformed ones)\n         and applies the given transformation function to each node.\n@@ -634,7 +655,9 @@ class Expression(metaclass=_Expression):\n         root = None\n         new_node = None\n \n-        for node in (self.copy() if copy else self).dfs(prune=lambda n: n is not new_node):\n+        for node in (self.copy() if copy else self).dfs(\n+            prune=lambda n: n is not new_node\n+        ):\n             parent, arg_key, index = node.parent, node.arg_key, node.index\n             new_node = fun(node, *args, **kwargs)\n \n@@ -861,7 +884,9 @@ class Expression(metaclass=_Expression):\n             The updated expression.\n         \"\"\"\n         if isinstance(other, Expression):\n-            self.meta.update({k: v for k, v in other.meta.items() if k in POSITION_META_KEYS})\n+            self.meta.update(\n+                {k: v for k, v in other.meta.items() if k in POSITION_META_KEYS}\n+            )\n         elif other is not None:\n             self.meta.update(\n                 {\n@@ -896,7 +921,8 @@ class Expression(metaclass=_Expression):\n \n     def __getitem__(self, other: ExpOrStr | t.Tuple[ExpOrStr]) -> Bracket:\n         return Bracket(\n-            this=self.copy(), expressions=[convert(e, copy=True) for e in ensure_list(other)]\n+            this=self.copy(),\n+            expressions=[convert(e, copy=True) for e in ensure_list(other)],\n         )\n \n     def __iter__(self) -> t.Iterator:\n@@ -1083,7 +1109,9 @@ class DerivedTable(Expression):\n \n \n class Query(Expression):\n-    def subquery(self, alias: t.Optional[ExpOrStr] = None, copy: bool = True) -> Subquery:\n+    def subquery(\n+        self, alias: t.Optional[ExpOrStr] = None, copy: bool = True\n+    ) -> Subquery:\n         \"\"\"\n         Returns a `Subquery` that wraps around this query.\n \n@@ -1103,7 +1131,11 @@ class Query(Expression):\n         return Subquery(this=instance, alias=alias)\n \n     def limit(\n-        self: Q, expression: ExpOrStr | int, dialect: DialectType = None, copy: bool = True, **opts\n+        self: Q,\n+        expression: ExpOrStr | int,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Q:\n         \"\"\"\n         Adds a LIMIT clause to this query.\n@@ -1137,7 +1169,11 @@ class Query(Expression):\n         )\n \n     def offset(\n-        self: Q, expression: ExpOrStr | int, dialect: DialectType = None, copy: bool = True, **opts\n+        self: Q,\n+        expression: ExpOrStr | int,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Q:\n         \"\"\"\n         Set the OFFSET expression.\n@@ -1344,7 +1380,11 @@ class Query(Expression):\n         )\n \n     def union(\n-        self, *expressions: ExpOrStr, distinct: bool = True, dialect: DialectType = None, **opts\n+        self,\n+        *expressions: ExpOrStr,\n+        distinct: bool = True,\n+        dialect: DialectType = None,\n+        **opts,\n     ) -> Union:\n         \"\"\"\n         Builds a UNION expression.\n@@ -1367,7 +1407,11 @@ class Query(Expression):\n         return union(self, *expressions, distinct=distinct, dialect=dialect, **opts)\n \n     def intersect(\n-        self, *expressions: ExpOrStr, distinct: bool = True, dialect: DialectType = None, **opts\n+        self,\n+        *expressions: ExpOrStr,\n+        distinct: bool = True,\n+        dialect: DialectType = None,\n+        **opts,\n     ) -> Intersect:\n         \"\"\"\n         Builds an INTERSECT expression.\n@@ -1390,7 +1434,11 @@ class Query(Expression):\n         return intersect(self, *expressions, distinct=distinct, dialect=dialect, **opts)\n \n     def except_(\n-        self, *expressions: ExpOrStr, distinct: bool = True, dialect: DialectType = None, **opts\n+        self,\n+        *expressions: ExpOrStr,\n+        distinct: bool = True,\n+        dialect: DialectType = None,\n+        **opts,\n     ) -> Except:\n         \"\"\"\n         Builds an EXCEPT expression.\n@@ -1455,7 +1503,9 @@ class DDL(Expression):\n         If this statement contains a query (e.g. a CTAS), this returns the output\n         names of the query's projections.\n         \"\"\"\n-        return self.expression.named_selects if isinstance(self.expression, Query) else []\n+        return (\n+            self.expression.named_selects if isinstance(self.expression, Query) else []\n+        )\n \n \n # https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Data-Manipulation-Language/Statement-Syntax/LOCKING-Request-Modifier/LOCKING-Request-Modifier-Syntax\n@@ -1717,7 +1767,13 @@ class UnicodeString(Condition):\n \n \n class Column(Condition):\n-    arg_types = {\"this\": True, \"table\": False, \"db\": False, \"catalog\": False, \"join_mark\": False}\n+    arg_types = {\n+        \"this\": True,\n+        \"table\": False,\n+        \"db\": False,\n+        \"catalog\": False,\n+        \"join_mark\": False,\n+    }\n \n     @property\n     def table(self) -> str:\n@@ -3298,7 +3354,9 @@ class Properties(Expression):\n             if property_cls:\n                 expressions.append(property_cls(this=convert(value)))\n             else:\n-                expressions.append(Property(this=Literal.string(key), value=convert(value)))\n+                expressions.append(\n+                    Property(this=Literal.string(key), value=convert(value))\n+                )\n \n         return cls(expressions=expressions)\n \n@@ -3504,7 +3562,9 @@ class SetOperation(Query):\n         **opts,\n     ) -> S:\n         this = maybe_copy(self, copy)\n-        this.this.unnest().select(*expressions, append=append, dialect=dialect, copy=False, **opts)\n+        this.this.unnest().select(\n+            *expressions, append=append, dialect=dialect, copy=False, **opts\n+        )\n         this.expression.unnest().select(\n             *expressions, append=append, dialect=dialect, copy=False, **opts\n         )\n@@ -3564,7 +3624,11 @@ class Update(DML):\n     }\n \n     def table(\n-        self, expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts\n+        self,\n+        expression: ExpOrStr,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Update:\n         \"\"\"\n         Set the table to update.\n@@ -3804,7 +3868,11 @@ class Select(Query):\n     }\n \n     def from_(\n-        self, expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts\n+        self,\n+        expression: ExpOrStr,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Select:\n         \"\"\"\n         Set the FROM expression.\n@@ -4211,7 +4279,11 @@ class Select(Query):\n             Select: the modified expression.\n         \"\"\"\n         instance = maybe_copy(self, copy)\n-        on = Tuple(expressions=[maybe_parse(on, copy=copy) for on in ons if on]) if ons else None\n+        on = (\n+            Tuple(expressions=[maybe_parse(on, copy=copy) for on in ons if on])\n+            if ons\n+            else None\n+        )\n         instance.set(\"distinct\", Distinct(on=on) if distinct else None)\n         return instance\n \n@@ -4278,7 +4350,9 @@ class Select(Query):\n \n         return inst\n \n-    def hint(self, *hints: ExpOrStr, dialect: DialectType = None, copy: bool = True) -> Select:\n+    def hint(\n+        self, *hints: ExpOrStr, dialect: DialectType = None, copy: bool = True\n+    ) -> Select:\n         \"\"\"\n         Set hints for this expression.\n \n@@ -4297,7 +4371,10 @@ class Select(Query):\n         \"\"\"\n         inst = maybe_copy(self, copy)\n         inst.set(\n-            \"hint\", Hint(expressions=[maybe_parse(h, copy=copy, dialect=dialect) for h in hints])\n+            \"hint\",\n+            Hint(\n+                expressions=[maybe_parse(h, copy=copy, dialect=dialect) for h in hints]\n+            ),\n         )\n \n         return inst\n@@ -4348,7 +4425,9 @@ class Subquery(DerivedTable, Query):\n         **opts,\n     ) -> Subquery:\n         this = maybe_copy(self, copy)\n-        this.unnest().select(*expressions, append=append, dialect=dialect, copy=False, **opts)\n+        this.unnest().select(\n+            *expressions, append=append, dialect=dialect, copy=False, **opts\n+        )\n         return this\n \n     @property\n@@ -4780,7 +4859,9 @@ class DataType(Expression):\n                 )\n             except ParseError:\n                 if udt:\n-                    return DataType(this=DataType.Type.USERDEFINED, kind=dtype, **kwargs)\n+                    return DataType(\n+                        this=DataType.Type.USERDEFINED, kind=dtype, **kwargs\n+                    )\n                 raise\n         elif isinstance(dtype, (Identifier, Dot)) and udt:\n             return DataType(this=DataType.Type.USERDEFINED, kind=dtype, **kwargs)\n@@ -4789,7 +4870,9 @@ class DataType(Expression):\n         elif isinstance(dtype, DataType):\n             return maybe_copy(dtype, copy)\n         else:\n-            raise ValueError(f\"Invalid data type: {type(dtype)}. Expected str or DataType.Type\")\n+            raise ValueError(\n+                f\"Invalid data type: {type(dtype)}. Expected str or DataType.Type\"\n+            )\n \n         return DataType(**{**data_type_exp.args, **kwargs})\n \n@@ -5370,10 +5453,14 @@ class Func(Condition):\n         if cls.is_var_len_args:\n             all_arg_keys = list(cls.arg_types)\n             # If this function supports variable length argument treat the last argument as such.\n-            non_var_len_arg_keys = all_arg_keys[:-1] if cls.is_var_len_args else all_arg_keys\n+            non_var_len_arg_keys = (\n+                all_arg_keys[:-1] if cls.is_var_len_args else all_arg_keys\n+            )\n             num_non_var = len(non_var_len_arg_keys)\n \n-            args_dict = {arg_key: arg for arg, arg_key in zip(args, non_var_len_arg_keys)}\n+            args_dict = {\n+                arg_key: arg for arg, arg_key in zip(args, non_var_len_arg_keys)\n+            }\n             args_dict[all_arg_keys[-1]] = args[num_non_var:]\n         else:\n             args_dict = {arg_key: arg for arg, arg_key in zip(args, cls.arg_types)}\n@@ -5521,7 +5608,12 @@ class List(Func):\n \n # String pad, kind True -> LPAD, False -> RPAD\n class Pad(Func):\n-    arg_types = {\"this\": True, \"expression\": True, \"fill_pattern\": False, \"is_left\": True}\n+    arg_types = {\n+        \"this\": True,\n+        \"expression\": True,\n+        \"fill_pattern\": False,\n+        \"is_left\": True,\n+    }\n \n \n # https://docs.snowflake.com/en/sql-reference/functions/to_char\n@@ -5737,7 +5829,9 @@ class NthValue(AggFunc):\n class Case(Func):\n     arg_types = {\"this\": False, \"ifs\": True, \"default\": False}\n \n-    def when(self, condition: ExpOrStr, then: ExpOrStr, copy: bool = True, **opts) -> Case:\n+    def when(\n+        self, condition: ExpOrStr, then: ExpOrStr, copy: bool = True, **opts\n+    ) -> Case:\n         instance = maybe_copy(self, copy)\n         instance.append(\n             \"ifs\",\n@@ -6176,7 +6270,12 @@ class FromBase64(Func):\n \n \n class FeaturesAtTime(Func):\n-    arg_types = {\"this\": True, \"time\": False, \"num_rows\": False, \"ignore_feature_nulls\": False}\n+    arg_types = {\n+        \"this\": True,\n+        \"time\": False,\n+        \"num_rows\": False,\n+        \"ignore_feature_nulls\": False,\n+    }\n \n \n class ToBase64(Func):\n@@ -6500,7 +6599,12 @@ class JSONExtractArray(Func):\n \n \n class JSONExtractScalar(Binary, Func):\n-    arg_types = {\"this\": True, \"expression\": True, \"only_json_types\": False, \"expressions\": False}\n+    arg_types = {\n+        \"this\": True,\n+        \"expression\": True,\n+        \"only_json_types\": False,\n+        \"expressions\": False,\n+    }\n     _sql_names = [\"JSON_EXTRACT_SCALAR\"]\n     is_var_len_args = True\n \n@@ -7301,13 +7405,17 @@ def maybe_copy(instance, copy=True):\n     return instance.copy() if copy and instance else instance\n \n \n-def _to_s(node: t.Any, verbose: bool = False, level: int = 0, repr_str: bool = False) -> str:\n+def _to_s(\n+    node: t.Any, verbose: bool = False, level: int = 0, repr_str: bool = False\n+) -> str:\n     \"\"\"Generate a textual representation of an Expression tree\"\"\"\n     indent = \"\\n\" + (\"  \" * (level + 1))\n     delim = f\",{indent}\"\n \n     if isinstance(node, Expression):\n-        args = {k: v for k, v in node.args.items() if (v is not None and v != []) or verbose}\n+        args = {\n+            k: v for k, v in node.args.items() if (v is not None and v != []) or verbose\n+        }\n \n         if (node.type or verbose) and not isinstance(node, DataType):\n             args[\"_type\"] = node.type\n@@ -7324,7 +7432,10 @@ def _to_s(node: t.Any, verbose: bool = False, level: int = 0, repr_str: bool = F\n \n         repr_str = node.is_string or (isinstance(node, Identifier) and node.quoted)\n         items = delim.join(\n-            [f\"{k}={_to_s(v, verbose, level + 1, repr_str=repr_str)}\" for k, v in args.items()]\n+            [\n+                f\"{k}={_to_s(v, verbose, level + 1, repr_str=repr_str)}\"\n+                for k, v in args.items()\n+            ]\n         )\n         return f\"{node.__class__.__name__}({indent}{items})\"\n \n@@ -7492,7 +7603,12 @@ def _apply_cte_builder(\n     if scalar and not isinstance(as_expression, Subquery):\n         # scalar CTE must be wrapped in a subquery\n         as_expression = Subquery(this=as_expression)\n-    cte = CTE(this=as_expression, alias=alias_expression, materialized=materialized, scalar=scalar)\n+    cte = CTE(\n+        this=as_expression,\n+        alias=alias_expression,\n+        materialized=materialized,\n+        scalar=scalar,\n+    )\n     return _apply_child_list_builder(\n         cte,\n         instance=instance,\n@@ -7522,7 +7638,9 @@ def _combine(\n     if rest and wrap:\n         this = _wrap(this, Connector)\n     for expression in rest:\n-        this = operator(this=this, expression=_wrap(expression, Connector) if wrap else expression)\n+        this = operator(\n+            this=this, expression=_wrap(expression, Connector) if wrap else expression\n+        )\n \n     return this\n \n@@ -7580,7 +7698,12 @@ def union(\n     \"\"\"\n     assert len(expressions) >= 2, \"At least two expressions are required by `union`.\"\n     return _apply_set_operation(\n-        *expressions, set_operation=Union, distinct=distinct, dialect=dialect, copy=copy, **opts\n+        *expressions,\n+        set_operation=Union,\n+        distinct=distinct,\n+        dialect=dialect,\n+        copy=copy,\n+        **opts,\n     )\n \n \n@@ -7609,9 +7732,16 @@ def intersect(\n     Returns:\n         The new Intersect instance.\n     \"\"\"\n-    assert len(expressions) >= 2, \"At least two expressions are required by `intersect`.\"\n+    assert len(expressions) >= 2, (\n+        \"At least two expressions are required by `intersect`.\"\n+    )\n     return _apply_set_operation(\n-        *expressions, set_operation=Intersect, distinct=distinct, dialect=dialect, copy=copy, **opts\n+        *expressions,\n+        set_operation=Intersect,\n+        distinct=distinct,\n+        dialect=dialect,\n+        copy=copy,\n+        **opts,\n     )\n \n \n@@ -7642,7 +7772,12 @@ def except_(\n     \"\"\"\n     assert len(expressions) >= 2, \"At least two expressions are required by `except_`.\"\n     return _apply_set_operation(\n-        *expressions, set_operation=Except, distinct=distinct, dialect=dialect, copy=copy, **opts\n+        *expressions,\n+        set_operation=Except,\n+        distinct=distinct,\n+        dialect=dialect,\n+        copy=copy,\n+        **opts,\n     )\n \n \n@@ -7741,7 +7876,9 @@ def update(\n         )\n     if with_:\n         cte_list = [\n-            alias_(CTE(this=maybe_parse(qry, dialect=dialect, **opts)), alias, table=True)\n+            alias_(\n+                CTE(this=maybe_parse(qry, dialect=dialect, **opts)), alias, table=True\n+            )\n             for alias, qry in with_.items()\n         ]\n         update_expr.set(\n@@ -7778,7 +7915,9 @@ def delete(\n     if where:\n         delete_expr = delete_expr.where(where, dialect=dialect, copy=False, **opts)\n     if returning:\n-        delete_expr = delete_expr.returning(returning, dialect=dialect, copy=False, **opts)\n+        delete_expr = delete_expr.returning(\n+            returning, dialect=dialect, copy=False, **opts\n+        )\n     return delete_expr\n \n \n@@ -7813,10 +7952,14 @@ def insert(\n         Insert: the syntax tree for the INSERT statement.\n     \"\"\"\n     expr = maybe_parse(expression, dialect=dialect, copy=copy, **opts)\n-    this: Table | Schema = maybe_parse(into, into=Table, dialect=dialect, copy=copy, **opts)\n+    this: Table | Schema = maybe_parse(\n+        into, into=Table, dialect=dialect, copy=copy, **opts\n+    )\n \n     if columns:\n-        this = Schema(this=this, expressions=[to_identifier(c, copy=copy) for c in columns])\n+        this = Schema(\n+            this=this, expressions=[to_identifier(c, copy=copy) for c in columns]\n+        )\n \n     insert = Insert(this=this, expression=expr, overwrite=overwrite)\n \n@@ -7862,8 +8005,12 @@ def merge(\n     \"\"\"\n     expressions: t.List[Expression] = []\n     for when_expr in when_exprs:\n-        expression = maybe_parse(when_expr, dialect=dialect, copy=copy, into=Whens, **opts)\n-        expressions.extend([expression] if isinstance(expression, When) else expression.expressions)\n+        expression = maybe_parse(\n+            when_expr, dialect=dialect, copy=copy, into=Whens, **opts\n+        )\n+        expressions.extend(\n+            [expression] if isinstance(expression, When) else expression.expressions\n+        )\n \n     merge = Merge(\n         this=maybe_parse(into, dialect=dialect, copy=copy, **opts),\n@@ -7941,7 +8088,9 @@ def and_(\n     Returns:\n         The new condition\n     \"\"\"\n-    return t.cast(Condition, _combine(expressions, And, dialect, copy=copy, wrap=wrap, **opts))\n+    return t.cast(\n+        Condition, _combine(expressions, And, dialect, copy=copy, wrap=wrap, **opts)\n+    )\n \n \n def or_(\n@@ -7971,7 +8120,9 @@ def or_(\n     Returns:\n         The new condition\n     \"\"\"\n-    return t.cast(Condition, _combine(expressions, Or, dialect, copy=copy, wrap=wrap, **opts))\n+    return t.cast(\n+        Condition, _combine(expressions, Or, dialect, copy=copy, wrap=wrap, **opts)\n+    )\n \n \n def xor(\n@@ -8001,10 +8152,14 @@ def xor(\n     Returns:\n         The new condition\n     \"\"\"\n-    return t.cast(Condition, _combine(expressions, Xor, dialect, copy=copy, wrap=wrap, **opts))\n+    return t.cast(\n+        Condition, _combine(expressions, Xor, dialect, copy=copy, wrap=wrap, **opts)\n+    )\n \n \n-def not_(expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts) -> Not:\n+def not_(\n+    expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts\n+) -> Not:\n     \"\"\"\n     Wrap a condition with a NOT operator.\n \n@@ -8054,7 +8209,9 @@ SAFE_IDENTIFIER_RE: t.Pattern[str] = re.compile(r\"^[_a-zA-Z][\\w]*$\")\n \n \n @t.overload\n-def to_identifier(name: None, quoted: t.Optional[bool] = None, copy: bool = True) -> None: ...\n+def to_identifier(\n+    name: None, quoted: t.Optional[bool] = None, copy: bool = True\n+) -> None: ...\n \n \n @t.overload\n@@ -8086,7 +8243,9 @@ def to_identifier(name, quoted=None, copy=True):\n             quoted=not SAFE_IDENTIFIER_RE.match(name) if quoted is None else quoted,\n         )\n     else:\n-        raise ValueError(f\"Name needs to be a string or an Identifier, got: {name.__class__}\")\n+        raise ValueError(\n+            f\"Name needs to be a string or an Identifier, got: {name.__class__}\"\n+        )\n     return identifier\n \n \n@@ -8280,7 +8439,9 @@ def subquery(\n         A new Select instance with the subquery expression included.\n     \"\"\"\n \n-    expression = maybe_parse(expression, dialect=dialect, **opts).subquery(alias, **opts)\n+    expression = maybe_parse(expression, dialect=dialect, **opts).subquery(\n+        alias, **opts\n+    )\n     return Select().from_(expression, dialect=dialect, **opts)\n \n \n@@ -8349,13 +8510,20 @@ def column(\n \n     if fields:\n         this = Dot.build(\n-            (this, *(to_identifier(field, quoted=quoted, copy=copy) for field in fields))\n+            (\n+                this,\n+                *(to_identifier(field, quoted=quoted, copy=copy) for field in fields),\n+            )\n         )\n     return this\n \n \n def cast(\n-    expression: ExpOrStr, to: DATA_TYPE, copy: bool = True, dialect: DialectType = None, **opts\n+    expression: ExpOrStr,\n+    to: DATA_TYPE,\n+    copy: bool = True,\n+    dialect: DialectType = None,\n+    **opts,\n ) -> Cast:\n     \"\"\"Cast an expression to a data type.\n \n@@ -8459,7 +8627,9 @@ def values(\n     return Values(\n         expressions=[convert(tup) for tup in values],\n         alias=(\n-            TableAlias(this=to_identifier(alias), columns=[to_identifier(x) for x in columns])\n+            TableAlias(\n+                this=to_identifier(alias), columns=[to_identifier(x) for x in columns]\n+            )\n             if columns\n             else (TableAlias(this=to_identifier(alias)) if alias else None)\n         ),\n@@ -8592,7 +8762,8 @@ def convert(value: t.Any, copy: bool = False) -> Expression:\n             return Struct(\n                 expressions=[\n                     PropertyEQ(\n-                        this=to_identifier(k), expression=convert(getattr(value, k), copy=copy)\n+                        this=to_identifier(k),\n+                        expression=convert(getattr(value, k), copy=copy),\n                     )\n                     for k in value._fields\n                 ]\n@@ -8632,7 +8803,9 @@ def replace_children(expression: Expression, fun: t.Callable, *args, **kwargs) -\n             else:\n                 new_child_nodes.append(cn)\n \n-        expression.set(k, new_child_nodes if is_list_arg else seq_get(new_child_nodes, 0))\n+        expression.set(\n+            k, new_child_nodes if is_list_arg else seq_get(new_child_nodes, 0)\n+        )\n \n \n def replace_tree(\n@@ -8684,7 +8857,9 @@ def column_table_names(expression: Expression, exclude: str = \"\") -> t.Set[str]:\n     }\n \n \n-def table_name(table: Table | str, dialect: DialectType = None, identify: bool = False) -> str:\n+def table_name(\n+    table: Table | str, dialect: DialectType = None, identify: bool = False\n+) -> str:\n     \"\"\"Get the full name of a table as a string.\n \n     Args:\n@@ -8718,7 +8893,9 @@ def table_name(table: Table | str, dialect: DialectType = None, identify: bool =\n     )\n \n \n-def normalize_table_name(table: str | Table, dialect: DialectType = None, copy: bool = True) -> str:\n+def normalize_table_name(\n+    table: str | Table, dialect: DialectType = None, copy: bool = True\n+) -> str:\n     \"\"\"Returns a case normalized table name without quotes.\n \n     Args:\n@@ -8741,7 +8918,10 @@ def normalize_table_name(table: str | Table, dialect: DialectType = None, copy:\n \n \n def replace_tables(\n-    expression: E, mapping: t.Dict[str, str], dialect: DialectType = None, copy: bool = True\n+    expression: E,\n+    mapping: t.Dict[str, str],\n+    dialect: DialectType = None,\n+    copy: bool = True,\n ) -> E:\n     \"\"\"Replace all tables in expression according to the mapping.\n \n@@ -8841,7 +9021,9 @@ def expand(\n     Returns:\n         The transformed expression.\n     \"\"\"\n-    normalized_sources = {normalize_table_name(k, dialect=dialect): v for k, v in sources.items()}\n+    normalized_sources = {\n+        normalize_table_name(k, dialect=dialect): v for k, v in sources.items()\n+    }\n \n     def _expand(node: Expression):\n         if isinstance(node, Table):\n@@ -8862,7 +9044,9 @@ def expand(\n     return expression.transform(_expand, copy=copy)\n \n \n-def func(name: str, *args, copy: bool = True, dialect: DialectType = None, **kwargs) -> Func:\n+def func(\n+    name: str, *args, copy: bool = True, dialect: DialectType = None, **kwargs\n+) -> Func:\n     \"\"\"\n     Returns a Func expression.\n \n@@ -8894,8 +9078,13 @@ def func(name: str, *args, copy: bool = True, dialect: DialectType = None, **kwa\n \n     dialect = Dialect.get_or_raise(dialect)\n \n-    converted: t.List[Expression] = [maybe_parse(arg, dialect=dialect, copy=copy) for arg in args]\n-    kwargs = {key: maybe_parse(value, dialect=dialect, copy=copy) for key, value in kwargs.items()}\n+    converted: t.List[Expression] = [\n+        maybe_parse(arg, dialect=dialect, copy=copy) for arg in args\n+    ]\n+    kwargs = {\n+        key: maybe_parse(value, dialect=dialect, copy=copy)\n+        for key, value in kwargs.items()\n+    }\n \n     constructor = dialect.parser_class.FUNCTIONS.get(name.upper())\n     if constructor:\ndiff --git a/sqlglot/lineage.py b/sqlglot/lineage.py\nindex 1642109ea..619029474 100644\n--- a/sqlglot/lineage.py\n+++ b/sqlglot/lineage.py\n@@ -7,7 +7,13 @@ from dataclasses import dataclass, field\n \n from sqlglot import Schema, exp, maybe_parse\n from sqlglot.errors import SqlglotError\n-from sqlglot.optimizer import Scope, build_scope, find_all_in_scope, normalize_identifiers, qualify\n+from sqlglot.optimizer import (\n+    Scope,\n+    build_scope,\n+    find_all_in_scope,\n+    normalize_identifiers,\n+    qualify,\n+)\n from sqlglot.optimizer.scope import ScopeType\n \n if t.TYPE_CHECKING:\n@@ -44,7 +50,9 @@ class Node:\n                 label = node.expression.sql(pretty=True, dialect=dialect)\n                 source = node.source.transform(\n                     lambda n: (\n-                        exp.Tag(this=n, prefix=\"<b>\", postfix=\"</b>\") if n is node.expression else n\n+                        exp.Tag(this=n, prefix=\"<b>\", postfix=\"</b>\")\n+                        if n is node.expression\n+                        else n\n                     ),\n                     copy=False,\n                 ).sql(pretty=True, dialect=dialect)\n@@ -97,7 +105,10 @@ def lineage(\n     if sources:\n         expression = exp.expand(\n             expression,\n-            {k: t.cast(exp.Query, maybe_parse(v, dialect=dialect)) for k, v in sources.items()},\n+            {\n+                k: t.cast(exp.Query, maybe_parse(v, dialect=dialect))\n+                for k, v in sources.items()\n+            },\n             dialect=dialect,\n         )\n \n@@ -136,7 +147,11 @@ def to_node(\n         scope.expression.selects[column]\n         if isinstance(column, int)\n         else next(\n-            (select for select in scope.expression.selects if select.alias_or_name == column),\n+            (\n+                select\n+                for select in scope.expression.selects\n+                if select.alias_or_name == column\n+            ),\n             exp.Star() if scope.expression.is_star else scope.expression,\n         )\n     )\n@@ -154,7 +169,9 @@ def to_node(\n             )\n     if isinstance(scope.expression, exp.SetOperation):\n         name = type(scope.expression).__name__.upper()\n-        upstream = upstream or Node(name=name, source=scope.expression, expression=select)\n+        upstream = upstream or Node(\n+            name=name, source=scope.expression, expression=select\n+        )\n \n         index = (\n             column\n@@ -207,7 +224,8 @@ def to_node(\n         upstream.downstream.append(node)\n \n     subquery_scopes = {\n-        id(subquery_scope.expression): subquery_scope for subquery_scope in scope.subquery_scopes\n+        id(subquery_scope.expression): subquery_scope\n+        for subquery_scope in scope.subquery_scopes\n     }\n \n     for subquery in find_all_in_scope(select, exp.UNWRAPPED_QUERIES):\n@@ -281,7 +299,10 @@ def to_node(\n \n         if isinstance(source, Scope):\n             reference_node_name = None\n-            if source.scope_type == ScopeType.DERIVED_TABLE and table not in source_names:\n+            if (\n+                source.scope_type == ScopeType.DERIVED_TABLE\n+                and table not in source_names\n+            ):\n                 reference_node_name = table\n             elif source.scope_type == ScopeType.CTE:\n                 selected_node, _ = scope.selected_sources.get(table, (None, None))\n@@ -307,7 +328,9 @@ def to_node(\n             else:\n                 # The column is not in the pivot, so it must be an implicit column of the\n                 # pivoted source -- adapt column to be from the implicit pivoted source.\n-                downstream_columns.append(exp.column(c.this, table=pivot.parent.alias_or_name))\n+                downstream_columns.append(\n+                    exp.column(c.this, table=pivot.parent.alias_or_name)\n+                )\n \n             for downstream_column in downstream_columns:\n                 table = downstream_column.table\n@@ -352,7 +375,11 @@ class GraphHTML:\n     \"\"\"\n \n     def __init__(\n-        self, nodes: t.Dict, edges: t.List, imports: bool = True, options: t.Optional[t.Dict] = None\n+        self,\n+        nodes: t.Dict,\n+        edges: t.List,\n+        imports: bool = True,\n+        options: t.Optional[t.Dict] = None,\n     ):\n         self.imports = imports\n \ndiff --git a/sqlglot/optimizer/qualify_columns.py b/sqlglot/optimizer/qualify_columns.py\nindex 743d009f7..177be2b00 100644\n--- a/sqlglot/optimizer/qualify_columns.py\n+++ b/sqlglot/optimizer/qualify_columns.py\n@@ -66,7 +66,9 @@ def qualify_columns(\n             # In Snowflake / Oracle queries that have a CONNECT BY clause, one can use the LEVEL\n             # pseudocolumn, which doesn't belong to a table, so we change it into an identifier\n             scope_expression.transform(\n-                lambda n: n.this if isinstance(n, exp.Column) and n.name == \"LEVEL\" else n,\n+                lambda n: n.this\n+                if isinstance(n, exp.Column) and n.name == \"LEVEL\"\n+                else n,\n                 copy=False,\n             )\n             scope.clear_cache()\n@@ -76,7 +78,9 @@ def qualify_columns(\n         _pop_table_column_aliases(scope.derived_tables)\n         using_column_tables = _expand_using(scope, resolver)\n \n-        if (schema.empty or dialect.FORCE_EARLY_ALIAS_REF_EXPANSION) and expand_alias_refs:\n+        if (\n+            schema.empty or dialect.FORCE_EARLY_ALIAS_REF_EXPANSION\n+        ) and expand_alias_refs:\n             _expand_alias_refs(\n                 scope,\n                 resolver,\n@@ -85,7 +89,9 @@ def qualify_columns(\n             )\n \n         _convert_columns_to_dots(scope, resolver)\n-        _qualify_columns(scope, resolver, allow_partial_qualification=allow_partial_qualification)\n+        _qualify_columns(\n+            scope, resolver, allow_partial_qualification=allow_partial_qualification\n+        )\n \n         if not schema.empty and expand_alias_refs:\n             _expand_alias_refs(scope, resolver, dialect)\n@@ -120,17 +126,25 @@ def validate_qualify_columns(expression: E) -> E:\n         if isinstance(scope.expression, exp.Select):\n             unqualified_columns = scope.unqualified_columns\n \n-            if scope.external_columns and not scope.is_correlated_subquery and not scope.pivots:\n+            if (\n+                scope.external_columns\n+                and not scope.is_correlated_subquery\n+                and not scope.pivots\n+            ):\n                 column = scope.external_columns[0]\n                 for_table = f\" for table: '{column.table}'\" if column.table else \"\"\n-                raise OptimizeError(f\"Column '{column}' could not be resolved{for_table}\")\n+                raise OptimizeError(\n+                    f\"Column '{column}' could not be resolved{for_table}\"\n+                )\n \n             if unqualified_columns and scope.pivots and scope.pivots[0].unpivot:\n                 # New columns produced by the UNPIVOT can't be qualified, but there may be columns\n                 # under the UNPIVOT's IN clause that can and should be qualified. We recompute\n                 # this list here to ensure those in the former category will be excluded.\n                 unpivot_columns = set(_unpivot_columns(scope.pivots[0]))\n-                unqualified_columns = [c for c in unqualified_columns if c not in unpivot_columns]\n+                unqualified_columns = [\n+                    c for c in unqualified_columns if c not in unpivot_columns\n+                ]\n \n             all_unqualified_columns.extend(unqualified_columns)\n \n@@ -158,7 +172,10 @@ def _pop_table_column_aliases(derived_tables: t.List[exp.CTE | exp.Subquery]) ->\n     For example, `col1` and `col2` will be dropped in SELECT ... FROM (SELECT ...) AS foo(col1, col2)\n     \"\"\"\n     for derived_table in derived_tables:\n-        if isinstance(derived_table.parent, exp.With) and derived_table.parent.recursive:\n+        if (\n+            isinstance(derived_table.parent, exp.With)\n+            and derived_table.parent.recursive\n+        ):\n             continue\n         table_alias = derived_table.args.get(\"alias\")\n         if table_alias:\n@@ -246,7 +263,9 @@ def _expand_using(scope: Scope, resolver: Resolver) -> t.Dict[str, t.Any]:\n         for column in scope.columns:\n             if not column.table and column.name in column_tables:\n                 tables = column_tables[column.name]\n-                coalesce_args = [exp.column(column.name, table=table) for table in tables]\n+                coalesce_args = [\n+                    exp.column(column.name, table=table) for table in tables\n+                ]\n                 replacement: exp.Expression = exp.func(\"coalesce\", *coalesce_args)\n \n                 if isinstance(column.parent, exp.Select):\n@@ -264,7 +283,10 @@ def _expand_using(scope: Scope, resolver: Resolver) -> t.Dict[str, t.Any]:\n \n \n def _expand_alias_refs(\n-    scope: Scope, resolver: Resolver, dialect: Dialect, expand_only_groupby: bool = False\n+    scope: Scope,\n+    resolver: Resolver,\n+    dialect: Dialect,\n+    expand_only_groupby: bool = False,\n ) -> None:\n     \"\"\"\n     Expand references to aliases.\n@@ -281,7 +303,9 @@ def _expand_alias_refs(\n     projections = {s.alias_or_name for s in expression.selects}\n \n     def replace_columns(\n-        node: t.Optional[exp.Expression], resolve_table: bool = False, literal_index: bool = False\n+        node: t.Optional[exp.Expression],\n+        resolve_table: bool = False,\n+        literal_index: bool = False,\n     ) -> None:\n         is_group_by = isinstance(node, exp.Group)\n         is_having = isinstance(node, exp.Having)\n@@ -300,14 +324,20 @@ def _expand_alias_refs(\n                 continue\n \n             skip_replace = False\n-            table = resolver.get_table(column.name) if resolve_table and not column.table else None\n+            table = (\n+                resolver.get_table(column.name)\n+                if resolve_table and not column.table\n+                else None\n+            )\n             alias_expr, i = alias_to_expression.get(column.name, (None, 1))\n \n             if alias_expr:\n                 skip_replace = bool(\n                     alias_expr.find(exp.AggFunc)\n                     and column.find_ancestor(exp.AggFunc)\n-                    and not isinstance(column.find_ancestor(exp.Window, exp.Select), exp.Window)\n+                    and not isinstance(\n+                        column.find_ancestor(exp.Window, exp.Select), exp.Window\n+                    )\n                 )\n \n                 # BigQuery's having clause gets confused if an alias matches a source.\n@@ -322,7 +352,9 @@ def _expand_alias_refs(\n             if table and (not alias_expr or skip_replace):\n                 column.set(\"table\", table)\n             elif not column.table and alias_expr and not skip_replace:\n-                if isinstance(alias_expr, exp.Literal) and (literal_index or resolve_table):\n+                if isinstance(alias_expr, exp.Literal) and (\n+                    literal_index or resolve_table\n+                ):\n                     if literal_index:\n                         column.replace(exp.Literal.number(i))\n                 else:\n@@ -367,7 +399,9 @@ def _expand_group_by(scope: Scope, dialect: DialectType) -> None:\n     if not group:\n         return\n \n-    group.set(\"expressions\", _expand_positional_references(scope, group.expressions, dialect))\n+    group.set(\n+        \"expressions\", _expand_positional_references(scope, group.expressions, dialect)\n+    )\n     expression.set(\"group\", group)\n \n \n@@ -398,7 +432,9 @@ def _expand_order_by_and_distinct_on(scope: Scope, resolver: Resolver) -> None:\n             original.replace(expanded)\n \n         if scope.expression.args.get(\"group\"):\n-            selects = {s.this: exp.column(s.alias_or_name) for s in scope.expression.selects}\n+            selects = {\n+                s.this: exp.column(s.alias_or_name) for s in scope.expression.selects\n+            }\n \n             for expression in modifier_expressions:\n                 expression.replace(\n@@ -409,7 +445,10 @@ def _expand_order_by_and_distinct_on(scope: Scope, resolver: Resolver) -> None:\n \n \n def _expand_positional_references(\n-    scope: Scope, expressions: t.Iterable[exp.Expression], dialect: DialectType, alias: bool = False\n+    scope: Scope,\n+    expressions: t.Iterable[exp.Expression],\n+    dialect: DialectType,\n+    alias: bool = False,\n ) -> t.List[exp.Expression]:\n     new_nodes: t.List[exp.Expression] = []\n     ambiguous_projections = None\n@@ -494,7 +533,9 @@ def _convert_columns_to_dots(scope: Scope, resolver: Resolver) -> None:\n \n             if column_table:\n                 converted = True\n-                column.replace(exp.Dot.build([exp.column(root, table=column_table), *parts]))\n+                column.replace(\n+                    exp.Dot.build([exp.column(root, table=column_table), *parts])\n+                )\n \n     if converted:\n         # We want to re-aggregate the converted columns, otherwise they'd be skipped in\n@@ -502,7 +543,9 @@ def _convert_columns_to_dots(scope: Scope, resolver: Resolver) -> None:\n         scope.clear_cache()\n \n \n-def _qualify_columns(scope: Scope, resolver: Resolver, allow_partial_qualification: bool) -> None:\n+def _qualify_columns(\n+    scope: Scope, resolver: Resolver, allow_partial_qualification: bool\n+) -> None:\n     \"\"\"Disambiguate columns, ensuring each column specifies a source\"\"\"\n     for column in scope.columns:\n         column_table = column.table\n@@ -551,7 +594,9 @@ def _expand_struct_stars_bigquery(\n     \"\"\"[BigQuery] Expand/Flatten foo.bar.* where bar is a struct column\"\"\"\n \n     dot_column = expression.find(exp.Column)\n-    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(exp.DataType.Type.STRUCT):\n+    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(\n+        exp.DataType.Type.STRUCT\n+    ):\n         return []\n \n     # All nested struct values are ColumnDefs, so normalize the first exp.Column in one\n@@ -608,7 +653,9 @@ def _expand_struct_stars_risingwave(expression: exp.Dot) -> t.List[exp.Alias]:\n \n     # find column definition to get data-type\n     dot_column = expression.find(exp.Column)\n-    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(exp.DataType.Type.STRUCT):\n+    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(\n+        exp.DataType.Type.STRUCT\n+    ):\n         return []\n \n     parent = dot_column.parent\n@@ -689,20 +736,28 @@ def _expand_stars(\n             for field in pivot.fields:\n                 if isinstance(field, exp.In):\n                     pivot_exclude_columns.update(\n-                        c.output_name for e in field.expressions for c in e.find_all(exp.Column)\n+                        c.output_name\n+                        for e in field.expressions\n+                        for c in e.find_all(exp.Column)\n                     )\n \n         else:\n-            pivot_exclude_columns = set(c.output_name for c in pivot.find_all(exp.Column))\n+            pivot_exclude_columns = set(\n+                c.output_name for c in pivot.find_all(exp.Column)\n+            )\n \n-            pivot_output_columns = [c.output_name for c in pivot.args.get(\"columns\", [])]\n+            pivot_output_columns = [\n+                c.output_name for c in pivot.args.get(\"columns\", [])\n+            ]\n             if not pivot_output_columns:\n                 pivot_output_columns = [c.alias_or_name for c in pivot.expressions]\n \n     is_bigquery = dialect == \"bigquery\"\n     is_risingwave = dialect == \"risingwave\"\n \n-    if (is_bigquery or is_risingwave) and any(isinstance(col, exp.Dot) for col in scope.stars):\n+    if (is_bigquery or is_risingwave) and any(\n+        isinstance(col, exp.Dot) for col in scope.stars\n+    ):\n         # Found struct expansion, annotate scope ahead of time\n         annotator.annotate_scope(scope)\n \n@@ -742,7 +797,9 @@ def _expand_stars(\n             columns = columns or scope.outer_columns\n \n             if pseudocolumns:\n-                columns = [name for name in columns if name.upper() not in pseudocolumns]\n+                columns = [\n+                    name for name in columns if name.upper() not in pseudocolumns\n+                ]\n \n             if not columns or \"*\" in columns:\n                 return\n@@ -754,7 +811,9 @@ def _expand_stars(\n \n             if pivot:\n                 if pivot_output_columns and pivot_exclude_columns:\n-                    pivot_columns = [c for c in columns if c not in pivot_exclude_columns]\n+                    pivot_columns = [\n+                        c for c in columns if c not in pivot_exclude_columns\n+                    ]\n                     pivot_columns.extend(pivot_output_columns)\n                 else:\n                     pivot_columns = pivot.alias_column_names\n@@ -776,11 +835,15 @@ def _expand_stars(\n                     coalesce_args = [exp.column(name, table=table) for table in tables]\n \n                     new_selections.append(\n-                        alias(exp.func(\"coalesce\", *coalesce_args), alias=name, copy=False)\n+                        alias(\n+                            exp.func(\"coalesce\", *coalesce_args), alias=name, copy=False\n+                        )\n                     )\n                 else:\n                     alias_ = renamed_columns.get(name, name)\n-                    selection_expr = replaced_columns.get(name) or exp.column(name, table=table)\n+                    selection_expr = replaced_columns.get(name) or exp.column(\n+                        name, table=table\n+                    )\n                     new_selections.append(\n                         alias(selection_expr, alias_, copy=False)\n                         if alias_ != name\n@@ -821,7 +884,9 @@ def _add_rename_columns(\n \n \n def _add_replace_columns(\n-    expression: exp.Expression, tables, replace_columns: t.Dict[int, t.Dict[str, exp.Alias]]\n+    expression: exp.Expression,\n+    tables,\n+    replace_columns: t.Dict[int, t.Dict[str, exp.Alias]],\n ) -> None:\n     replace = expression.args.get(\"replace\")\n \n@@ -852,7 +917,9 @@ def qualify_outputs(scope_or_expression: Scope | exp.Expression) -> None:\n \n         if isinstance(selection, exp.Subquery):\n             if not selection.output_name:\n-                selection.set(\"alias\", exp.TableAlias(this=exp.to_identifier(f\"_col_{i}\")))\n+                selection.set(\n+                    \"alias\", exp.TableAlias(this=exp.to_identifier(f\"_col_{i}\"))\n+                )\n         elif not isinstance(selection, exp.Alias) and not selection.is_star:\n             selection = alias(\n                 selection,\n@@ -868,7 +935,9 @@ def qualify_outputs(scope_or_expression: Scope | exp.Expression) -> None:\n         scope.expression.set(\"expressions\", new_selections)\n \n \n-def quote_identifiers(expression: E, dialect: DialectType = None, identify: bool = True) -> E:\n+def quote_identifiers(\n+    expression: E, dialect: DialectType = None, identify: bool = True\n+) -> E:\n     \"\"\"Makes sure all identifiers that need to be quoted are quoted.\"\"\"\n     return expression.transform(\n         Dialect.get_or_raise(dialect).quote_identifier, identify=identify, copy=False\n@@ -968,14 +1037,18 @@ class Resolver:\n         \"\"\"All available columns of all sources in this scope\"\"\"\n         if self._all_columns is None:\n             self._all_columns = {\n-                column for columns in self._get_all_source_columns().values() for column in columns\n+                column\n+                for columns in self._get_all_source_columns().values()\n+                for column in columns\n             }\n         return self._all_columns\n \n     def get_source_columns_from_set_op(self, expression: exp.Expression) -> t.List[str]:\n         if isinstance(expression, exp.Select):\n             return expression.named_selects\n-        if isinstance(expression, exp.Subquery) and isinstance(expression.this, exp.SetOperation):\n+        if isinstance(expression, exp.Subquery) and isinstance(\n+            expression.this, exp.SetOperation\n+        ):\n             # Different types of SET modifiers can be chained together if they're explicitly grouped by nesting\n             return self.get_source_columns_from_set_op(expression.this)\n         if not isinstance(expression, exp.SetOperation):\n@@ -1010,7 +1083,9 @@ class Resolver:\n \n         return columns\n \n-    def get_source_columns(self, name: str, only_visible: bool = False) -> t.Sequence[str]:\n+    def get_source_columns(\n+        self, name: str, only_visible: bool = False\n+    ) -> t.Sequence[str]:\n         \"\"\"Resolve the source columns for a given source `name`.\"\"\"\n         cache_key = (name, only_visible)\n         if cache_key not in self._get_source_columns_cache:\n@@ -1033,7 +1108,9 @@ class Resolver:\n                     if source.expression.is_type(exp.DataType.Type.STRUCT):\n                         for k in source.expression.type.expressions:  # type: ignore\n                             columns.append(k.name)\n-            elif isinstance(source, Scope) and isinstance(source.expression, exp.SetOperation):\n+            elif isinstance(source, Scope) and isinstance(\n+                source.expression, exp.SetOperation\n+            ):\n                 columns = self.get_source_columns_from_set_op(source.expression)\n \n             else:\n@@ -1042,7 +1119,11 @@ class Resolver:\n                 if isinstance(select, exp.QueryTransform):\n                     # https://spark.apache.org/docs/3.5.1/sql-ref-syntax-qry-select-transform.html\n                     schema = select.args.get(\"schema\")\n-                    columns = [c.name for c in schema.expressions] if schema else [\"key\", \"value\"]\n+                    columns = (\n+                        [c.name for c in schema.expressions]\n+                        if schema\n+                        else [\"key\", \"value\"]\n+                    )\n                 else:\n                     columns = source.expression.named_selects\n \n@@ -1071,7 +1152,8 @@ class Resolver:\n             self._source_columns = {\n                 source_name: self.get_source_columns(source_name)\n                 for source_name, source in itertools.chain(\n-                    self.scope.selected_sources.items(), self.scope.lateral_sources.items()\n+                    self.scope.selected_sources.items(),\n+                    self.scope.lateral_sources.items(),\n                 )\n             }\n         return self._source_columns\ndiff --git a/sqlglot/optimizer/scope.py b/sqlglot/optimizer/scope.py\nindex 2b2a01560..10c4db645 100644\n--- a/sqlglot/optimizer/scope.py\n+++ b/sqlglot/optimizer/scope.py\n@@ -104,7 +104,13 @@ class Scope:\n         self._semi_anti_join_tables = None\n \n     def branch(\n-        self, expression, scope_type, sources=None, cte_sources=None, lateral_sources=None, **kwargs\n+        self,\n+        expression,\n+        scope_type,\n+        sources=None,\n+        cte_sources=None,\n+        lateral_sources=None,\n+        **kwargs,\n     ):\n         \"\"\"Branch from the current scope to a new, inner scope\"\"\"\n         return Scope(\n@@ -142,7 +148,9 @@ class Scope:\n                     self._stars.append(node)\n                 else:\n                     self._raw_columns.append(node)\n-            elif isinstance(node, exp.Table) and not isinstance(node.parent, exp.JoinHint):\n+            elif isinstance(node, exp.Table) and not isinstance(\n+                node.parent, exp.JoinHint\n+            ):\n                 parent = node.parent\n                 if isinstance(parent, exp.Join) and parent.is_semi_or_anti_join:\n                     self._semi_anti_join_tables.add(node.alias_or_name)\n@@ -299,7 +307,10 @@ class Scope:\n                     not ancestor\n                     or column.table\n                     or isinstance(ancestor, exp.Select)\n-                    or (isinstance(ancestor, exp.Table) and not isinstance(ancestor.this, exp.Func))\n+                    or (\n+                        isinstance(ancestor, exp.Table)\n+                        and not isinstance(ancestor.this, exp.Func)\n+                    )\n                     or (\n                         isinstance(ancestor, (exp.Order, exp.Distinct))\n                         and (\n@@ -307,7 +318,10 @@ class Scope:\n                             or column.name not in named_selects\n                         )\n                     )\n-                    or (isinstance(ancestor, exp.Star) and not column.arg_key == \"except\")\n+                    or (\n+                        isinstance(ancestor, exp.Star)\n+                        and not column.arg_key == \"except\"\n+                    )\n                 ):\n                     self._columns.append(column)\n \n@@ -359,7 +373,9 @@ class Scope:\n                 self._references.append(\n                     (\n                         _get_source_alias(expression),\n-                        expression if expression.args.get(\"pivots\") else expression.unnest(),\n+                        expression\n+                        if expression.args.get(\"pivots\")\n+                        else expression.unnest(),\n                     )\n                 )\n \n@@ -414,7 +430,9 @@ class Scope:\n     def pivots(self):\n         if not self._pivots:\n             self._pivots = [\n-                pivot for _, node in self.references for pivot in node.args.get(\"pivots\") or []\n+                pivot\n+                for _, node in self.references\n+                for pivot in node.args.get(\"pivots\") or []\n             ]\n \n         return self._pivots\n@@ -597,7 +615,9 @@ def _traverse_scope(scope):\n     elif isinstance(expression, exp.DDL):\n         if isinstance(expression.expression, exp.Query):\n             yield from _traverse_ctes(scope)\n-            yield from _traverse_scope(Scope(expression.expression, cte_sources=scope.cte_sources))\n+            yield from _traverse_scope(\n+                Scope(expression.expression, cte_sources=scope.cte_sources)\n+            )\n         return\n     elif isinstance(expression, exp.DML):\n         yield from _traverse_ctes(scope)\n@@ -607,7 +627,9 @@ def _traverse_scope(scope):\n                 yield from _traverse_scope(Scope(query, cte_sources=scope.cte_sources))\n         return\n     else:\n-        logger.warning(\"Cannot traverse scope %s with type '%s'\", expression, type(expression))\n+        logger.warning(\n+            \"Cannot traverse scope %s with type '%s'\", expression, type(expression)\n+        )\n         return\n \n     yield scope\n@@ -753,7 +775,9 @@ def _traverse_tables(scope):\n \n             # Make sure to not include the joins twice\n             if expression is not scope.expression:\n-                expressions.extend(join.this for join in expression.args.get(\"joins\") or [])\n+                expressions.extend(\n+                    join.this for join in expression.args.get(\"joins\") or []\n+                )\n \n             continue\n \n@@ -804,7 +828,9 @@ def _traverse_tables(scope):\n def _traverse_subqueries(scope):\n     for subquery in scope.subqueries:\n         top = None\n-        for child_scope in _traverse_scope(scope.branch(subquery, scope_type=ScopeType.SUBQUERY)):\n+        for child_scope in _traverse_scope(\n+            scope.branch(subquery, scope_type=ScopeType.SUBQUERY)\n+        ):\n             yield child_scope\n             top = child_scope\n         scope.subquery_scopes.append(top)\n@@ -927,7 +953,11 @@ def _get_source_alias(expression):\n     alias_arg = expression.args.get(\"alias\")\n     alias_name = expression.alias\n \n-    if not alias_name and isinstance(alias_arg, exp.TableAlias) and len(alias_arg.columns) == 1:\n+    if (\n+        not alias_name\n+        and isinstance(alias_arg, exp.TableAlias)\n+        and len(alias_arg.columns) == 1\n+    ):\n         alias_name = alias_arg.columns[0].name\n \n     return alias_name\ndiff --git a/sqlglot/transforms.py b/sqlglot/transforms.py\nindex 464c66e68..1d201bb30 100644\n--- a/sqlglot/transforms.py\n+++ b/sqlglot/transforms.py\n@@ -56,12 +56,16 @@ def preprocess(\n \n             return transforms_handler(self, expression)\n \n-        raise ValueError(f\"Unsupported expression type {expression.__class__.__name__}.\")\n+        raise ValueError(\n+            f\"Unsupported expression type {expression.__class__.__name__}.\"\n+        )\n \n     return _to_sql\n \n \n-def unnest_generate_date_array_using_recursive_cte(expression: exp.Expression) -> exp.Expression:\n+def unnest_generate_date_array_using_recursive_cte(\n+    expression: exp.Expression,\n+) -> exp.Expression:\n     if isinstance(expression, exp.Select):\n         count = 0\n         recursive_ctes = []\n@@ -83,11 +87,16 @@ def unnest_generate_date_array_using_recursive_cte(expression: exp.Expression) -\n                 continue\n \n             alias = unnest.args.get(\"alias\")\n-            column_name = alias.columns[0] if isinstance(alias, exp.TableAlias) else \"date_value\"\n+            column_name = (\n+                alias.columns[0] if isinstance(alias, exp.TableAlias) else \"date_value\"\n+            )\n \n             start = exp.cast(start, \"date\")\n             date_add = exp.func(\n-                \"date_add\", column_name, exp.Literal.number(step.name), step.args.get(\"unit\")\n+                \"date_add\",\n+                column_name,\n+                exp.Literal.number(step.name),\n+                step.args.get(\"unit\"),\n             )\n             cast_date_add = exp.cast(date_add, \"date\")\n \n@@ -112,7 +121,9 @@ def unnest_generate_date_array_using_recursive_cte(expression: exp.Expression) -\n         if recursive_ctes:\n             with_expression = expression.args.get(\"with\") or exp.With()\n             with_expression.set(\"recursive\", True)\n-            with_expression.set(\"expressions\", [*recursive_ctes, *with_expression.expressions])\n+            with_expression.set(\n+                \"expressions\", [*recursive_ctes, *with_expression.expressions]\n+            )\n             expression.set(\"with\", with_expression)\n \n     return expression\n@@ -190,7 +201,9 @@ def eliminate_distinct_on(expression: exp.Expression) -> exp.Expression:\n         if order:\n             window.set(\"order\", order.pop())\n         else:\n-            window.set(\"order\", exp.Order(expressions=[c.copy() for c in distinct_cols]))\n+            window.set(\n+                \"order\", exp.Order(expressions=[c.copy() for c in distinct_cols])\n+            )\n \n         window = exp.alias_(window, row_number_window_alias)\n         expression.select(window, copy=False)\n@@ -205,7 +218,11 @@ def eliminate_distinct_on(expression: exp.Expression) -> exp.Expression:\n \n             if not isinstance(select, exp.Alias):\n                 alias = find_new_name(taken_names, select.output_name or \"_col\")\n-                quoted = select.this.args.get(\"quoted\") if isinstance(select, exp.Column) else None\n+                quoted = (\n+                    select.this.args.get(\"quoted\")\n+                    if isinstance(select, exp.Column)\n+                    else None\n+                )\n                 select = select.replace(exp.alias_(select, alias, quoted=quoted))\n \n             taken_names.add(select.output_name)\n@@ -249,7 +266,9 @@ def eliminate_qualify(expression: exp.Expression) -> exp.Expression:\n                 return exp.column(alias_or_name, quoted=identifier.args.get(\"quoted\"))\n             return alias_or_name\n \n-        outer_selects = exp.select(*list(map(_select_alias_or_name, expression.selects)))\n+        outer_selects = exp.select(\n+            *list(map(_select_alias_or_name, expression.selects))\n+        )\n         qualify_filters = expression.args[\"qualify\"].pop().this\n         expression_by_alias = {\n             select.alias: select.this\n@@ -257,7 +276,9 @@ def eliminate_qualify(expression: exp.Expression) -> exp.Expression:\n             if isinstance(select, exp.Alias)\n         }\n \n-        select_candidates = exp.Window if expression.is_star else (exp.Window, exp.Column)\n+        select_candidates = (\n+            exp.Window if expression.is_star else (exp.Window, exp.Column)\n+        )\n         for select_candidate in list(qualify_filters.find_all(select_candidates)):\n             if isinstance(select_candidate, exp.Window):\n                 if expression_by_alias:\n@@ -277,9 +298,9 @@ def eliminate_qualify(expression: exp.Expression) -> exp.Expression:\n             elif select_candidate.name not in expression.named_selects:\n                 expression.select(select_candidate.copy(), copy=False)\n \n-        return outer_selects.from_(expression.subquery(alias=\"_t\", copy=False), copy=False).where(\n-            qualify_filters, copy=False\n-        )\n+        return outer_selects.from_(\n+            expression.subquery(alias=\"_t\", copy=False), copy=False\n+        ).where(qualify_filters, copy=False)\n \n     return expression\n \n@@ -291,7 +312,8 @@ def remove_precision_parameterized_types(expression: exp.Expression) -> exp.Expr\n     \"\"\"\n     for node in expression.find_all(exp.DataType):\n         node.set(\n-            \"expressions\", [e for e in node.expressions if not isinstance(e, exp.DataTypeParam)]\n+            \"expressions\",\n+            [e for e in node.expressions if not isinstance(e, exp.DataTypeParam)],\n         )\n \n     return expression\n@@ -310,7 +332,10 @@ def unqualify_unnest(expression: exp.Expression) -> exp.Expression:\n         if unnest_aliases:\n             for column in expression.find_all(exp.Column):\n                 leftmost_part = column.parts[0]\n-                if leftmost_part.arg_key != \"this\" and leftmost_part.this in unnest_aliases:\n+                if (\n+                    leftmost_part.arg_key != \"this\"\n+                    and leftmost_part.this in unnest_aliases\n+                ):\n                     leftmost_part.pop()\n \n     return expression\n@@ -327,7 +352,9 @@ def unnest_to_explode(\n     ) -> t.List[exp.Expression]:\n         if has_multi_expr:\n             if not unnest_using_arrays_zip:\n-                raise UnsupportedError(\"Cannot transpile UNNEST with multiple input arrays\")\n+                raise UnsupportedError(\n+                    \"Cannot transpile UNNEST with multiple input arrays\"\n+                )\n \n             # Use INLINE(ARRAYS_ZIP(...)) for multiple expressions\n             zip_exprs: t.List[exp.Expression] = [\n@@ -356,7 +383,10 @@ def unnest_to_explode(\n             offset = unnest.args.get(\"offset\")\n             if offset:\n                 columns.insert(\n-                    0, offset if isinstance(offset, exp.Identifier) else exp.to_identifier(\"pos\")\n+                    0,\n+                    offset\n+                    if isinstance(offset, exp.Identifier)\n+                    else exp.to_identifier(\"pos\"),\n                 )\n \n             unnest.replace(\n@@ -365,7 +395,9 @@ def unnest_to_explode(\n                         this=this,\n                         expressions=expressions,\n                     ),\n-                    alias=exp.TableAlias(this=alias.this, columns=columns) if alias else None,\n+                    alias=exp.TableAlias(this=alias.this, columns=columns)\n+                    if alias\n+                    else None,\n                 )\n             )\n \n@@ -404,7 +436,9 @@ def unnest_to_explode(\n                 if offset:\n                     alias_cols.insert(\n                         0,\n-                        offset if isinstance(offset, exp.Identifier) else exp.to_identifier(\"pos\"),\n+                        offset\n+                        if isinstance(offset, exp.Identifier)\n+                        else exp.to_identifier(\"pos\"),\n                     )\n \n                 for e, column in zip(exprs, alias_cols):\n@@ -444,7 +478,9 @@ def explode_projection_to_unnest(\n             series_alias = new_name(taken_select_names, \"pos\")\n             series = exp.alias_(\n                 exp.Unnest(\n-                    expressions=[exp.GenerateSeries(start=exp.Literal.number(index_offset))]\n+                    expressions=[\n+                        exp.GenerateSeries(start=exp.Literal.number(index_offset))\n+                    ]\n                 ),\n                 new_name(taken_source_names, \"_u\"),\n                 table=[series_alias],\n@@ -480,7 +516,8 @@ def explode_projection_to_unnest(\n                         explode_arg = exp.func(\n                             \"IF\",\n                             exp.func(\n-                                \"ARRAY_SIZE\", exp.func(\"COALESCE\", explode_arg, exp.Array())\n+                                \"ARRAY_SIZE\",\n+                                exp.func(\"COALESCE\", explode_arg, exp.Array()),\n                             ).eq(0),\n                             exp.array(bracket, copy=False),\n                             explode_arg,\n@@ -518,9 +555,9 @@ def explode_projection_to_unnest(\n                         expressions.insert(\n                             expressions.index(alias) + 1,\n                             exp.If(\n-                                this=exp.column(series_alias, table=series_table_alias).eq(\n-                                    exp.column(pos_alias, table=unnest_source_alias)\n-                                ),\n+                                this=exp.column(\n+                                    series_alias, table=series_table_alias\n+                                ).eq(exp.column(pos_alias, table=unnest_source_alias)),\n                                 true=exp.column(pos_alias, table=unnest_source_alias),\n                             ).as_(pos_alias),\n                         )\n@@ -557,15 +594,22 @@ def explode_projection_to_unnest(\n                         exp.column(series_alias, table=series_table_alias)\n                         .eq(exp.column(pos_alias, table=unnest_source_alias))\n                         .or_(\n-                            (exp.column(series_alias, table=series_table_alias) > size).and_(\n-                                exp.column(pos_alias, table=unnest_source_alias).eq(size)\n+                            (\n+                                exp.column(series_alias, table=series_table_alias)\n+                                > size\n+                            ).and_(\n+                                exp.column(pos_alias, table=unnest_source_alias).eq(\n+                                    size\n+                                )\n                             )\n                         ),\n                         copy=False,\n                     )\n \n             if arrays:\n-                end: exp.Condition = exp.Greatest(this=arrays[0], expressions=arrays[1:])\n+                end: exp.Condition = exp.Greatest(\n+                    this=arrays[0], expressions=arrays[1:]\n+                )\n \n                 if index_offset != 1:\n                     end = end - (1 - index_offset)\n@@ -600,7 +644,9 @@ def remove_within_group_for_percentiles(expression: exp.Expression) -> exp.Expre\n     ):\n         quantile = expression.this.this\n         input_value = t.cast(exp.Ordered, expression.find(exp.Ordered)).this\n-        return expression.replace(exp.ApproxQuantile(this=input_value, quantile=quantile))\n+        return expression.replace(\n+            exp.ApproxQuantile(this=input_value, quantile=quantile)\n+        )\n \n     return expression\n \n@@ -618,7 +664,10 @@ def add_recursive_cte_column_names(expression: exp.Expression) -> exp.Expression\n \n                 cte.args[\"alias\"].set(\n                     \"columns\",\n-                    [exp.to_identifier(s.alias_or_name or next_name()) for s in query.selects],\n+                    [\n+                        exp.to_identifier(s.alias_or_name or next_name())\n+                        for s in query.selects\n+                    ],\n                 )\n \n     return expression\n@@ -671,7 +720,10 @@ def eliminate_full_outer_join(expression: exp.Expression) -> exp.Expression:\n             expression.set(\"limit\", None)\n             index, full_outer_join = full_outer_joins[0]\n \n-            tables = (expression.args[\"from\"].alias_or_name, full_outer_join.alias_or_name)\n+            tables = (\n+                expression.args[\"from\"].alias_or_name,\n+                full_outer_join.alias_or_name,\n+            )\n             join_conditions = full_outer_join.args.get(\"on\") or exp.and_(\n                 *[\n                     exp.column(col, tables[0]).eq(exp.column(col, tables[1]))\n@@ -680,9 +732,13 @@ def eliminate_full_outer_join(expression: exp.Expression) -> exp.Expression:\n             )\n \n             full_outer_join.set(\"side\", \"left\")\n-            anti_join_clause = exp.select(\"1\").from_(expression.args[\"from\"]).where(join_conditions)\n+            anti_join_clause = (\n+                exp.select(\"1\").from_(expression.args[\"from\"]).where(join_conditions)\n+            )\n             expression_copy.args[\"joins\"][index].set(\"side\", \"right\")\n-            expression_copy = expression_copy.where(exp.Exists(this=anti_join_clause).not_())\n+            expression_copy = expression_copy.where(\n+                exp.Exists(this=anti_join_clause).not_()\n+            )\n             expression_copy.args.pop(\"with\", None)  # remove CTEs from RIGHT side\n             expression.args.pop(\"order\", None)  # remove order by from LEFT side\n \n@@ -808,9 +864,15 @@ def move_schema_columns_to_partitioned_by(expression: exp.Expression) -> exp.Exp\n         if prop and prop.this and not isinstance(prop.this, exp.Schema):\n             schema = expression.this\n             columns = {v.name.upper() for v in prop.this.expressions}\n-            partitions = [col for col in schema.expressions if col.name.upper() in columns]\n-            schema.set(\"expressions\", [e for e in schema.expressions if e not in partitions])\n-            prop.replace(exp.PartitionedByProperty(this=exp.Schema(expressions=partitions)))\n+            partitions = [\n+                col for col in schema.expressions if col.name.upper() in columns\n+            ]\n+            schema.set(\n+                \"expressions\", [e for e in schema.expressions if e not in partitions]\n+            )\n+            prop.replace(\n+                exp.PartitionedByProperty(this=exp.Schema(expressions=partitions))\n+            )\n             expression.set(\"this\", schema)\n \n     return expression\n@@ -911,7 +973,9 @@ def eliminate_join_marks(expression: exp.Expression) -> exp.Expression:\n         assert not scope.is_correlated_subquery, \"Correlated queries are not supported\"\n \n         # nothing to do - we check it here after knockout above\n-        if not where or not any(c.args.get(\"join_mark\") for c in where.find_all(exp.Column)):\n+        if not where or not any(\n+            c.args.get(\"join_mark\") for c in where.find_all(exp.Column)\n+        ):\n             continue\n \n         # make sure we have AND of ORs to have clear join terms\n@@ -920,7 +984,9 @@ def eliminate_join_marks(expression: exp.Expression) -> exp.Expression:\n \n         joins_ons = defaultdict(list)  # dict of {name: list of join AND conditions}\n         for cond in [where] if not isinstance(where, exp.And) else where.flatten():\n-            join_cols = [col for col in cond.find_all(exp.Column) if col.args.get(\"join_mark\")]\n+            join_cols = [\n+                col for col in cond.find_all(exp.Column) if col.args.get(\"join_mark\")\n+            ]\n \n             left_join_table = set(col.table for col in join_cols)\n             if not left_join_table:\n@@ -990,7 +1056,9 @@ def any_to_exists(expression: exp.Expression) -> exp.Expression:\n     if isinstance(expression, exp.Select):\n         for any_expr in expression.find_all(exp.Any):\n             this = any_expr.this\n-            if isinstance(this, exp.Query) or isinstance(any_expr.parent, (exp.Like, exp.ILike)):\n+            if isinstance(this, exp.Query) or isinstance(\n+                any_expr.parent, (exp.Like, exp.ILike)\n+            ):\n                 continue\n \n             binop = any_expr.parent\n"
    },
    {
        "id": "277",
        "sha_fail": "bddc230cc11627fdbb8bdd98b67489afcad87d98",
        "diff": "diff --git a/tests/dialects/test_dialect.py b/tests/dialects/test_dialect.py\nindex 892fcdc5f..93d3338c9 100644\n--- a/tests/dialects/test_dialect.py\n+++ b/tests/dialects/test_dialect.py\n@@ -22,7 +22,12 @@ class Validator(unittest.TestCase):\n         return parse_one(sql, read=self.dialect, **kwargs)\n \n     def validate_identity(\n-        self, sql, write_sql=None, pretty=False, check_command_warning=False, identify=False\n+        self,\n+        sql,\n+        write_sql=None,\n+        pretty=False,\n+        check_command_warning=False,\n+        identify=False,\n     ):\n         if check_command_warning:\n             with self.assertLogs(parser_logger) as cm:\n@@ -32,7 +37,8 @@ class Validator(unittest.TestCase):\n             expression = self.parse_one(sql)\n \n         self.assertEqual(\n-            write_sql or sql, expression.sql(dialect=self.dialect, pretty=pretty, identify=identify)\n+            write_sql or sql,\n+            expression.sql(dialect=self.dialect, pretty=pretty, identify=identify),\n         )\n         return expression\n \n@@ -67,7 +73,9 @@ class Validator(unittest.TestCase):\n             with self.subTest(f\"{sql} -> {write_dialect}\"):\n                 if write_sql is UnsupportedError:\n                     with self.assertRaises(UnsupportedError):\n-                        expression.sql(write_dialect, unsupported_level=ErrorLevel.RAISE)\n+                        expression.sql(\n+                            write_dialect, unsupported_level=ErrorLevel.RAISE\n+                        )\n                 else:\n                     self.assertEqual(\n                         expression.sql(\n@@ -113,7 +121,9 @@ class TestDialect(Validator):\n         lowercase_mysql = Dialect.get_or_raise(\"mysql,normalization_strategy=lowercase\")\n         self.assertEqual(lowercase_mysql.normalization_strategy, \"LOWERCASE\")\n \n-        lowercase_mysql = Dialect.get_or_raise(\"mysql, normalization_strategy = lowercase\")\n+        lowercase_mysql = Dialect.get_or_raise(\n+            \"mysql, normalization_strategy = lowercase\"\n+        )\n         self.assertEqual(lowercase_mysql.normalization_strategy.value, \"LOWERCASE\")\n \n         with self.assertRaises(AttributeError) as cm:\n@@ -124,7 +134,9 @@ class TestDialect(Validator):\n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"myqsl\")\n \n-        self.assertEqual(str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\")\n+        self.assertEqual(\n+            str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\"\n+        )\n \n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"asdfjasodiufjsd\")\n@@ -140,7 +152,9 @@ class TestDialect(Validator):\n         class MyDialect(Dialect):\n             SUPPORTED_SETTINGS = {\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"}\n \n-        bool_settings = Dialect.get_or_raise(\"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\")\n+        bool_settings = Dialect.get_or_raise(\n+            \"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\"\n+        )\n         self.assertEqual(\n             bool_settings.settings,\n             {\"s1\": True, \"s2\": True, \"s3\": False, \"s4\": False, \"s5\": \"nonbool\"},\n@@ -450,7 +464,9 @@ class TestDialect(Validator):\n             },\n         )\n         self.validate_all(\"CAST(a AS TINYINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n-        self.validate_all(\"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n+        self.validate_all(\n+            \"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"}\n+        )\n         self.validate_all(\"CAST(a AS BIGINT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\"CAST(a AS INT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\n@@ -1411,11 +1427,15 @@ class TestDialect(Validator):\n         )\n \n         order_by_all_sql = \"SELECT * FROM t ORDER BY ALL\"\n-        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(exp.Column)\n+        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(\n+            exp.Column\n+        )\n \n         for dialect in (\"duckdb\", \"spark\", \"databricks\"):\n             with self.subTest(f\"Testing ORDER BY ALL in {dialect}\"):\n-                parse_one(order_by_all_sql, read=dialect).find(exp.Ordered).this.assert_is(exp.Var)\n+                parse_one(order_by_all_sql, read=dialect).find(\n+                    exp.Ordered\n+                ).this.assert_is(exp.Var)\n \n     def test_json(self):\n         self.validate_all(\n@@ -1590,9 +1610,13 @@ class TestDialect(Validator):\n         )\n \n         for dialect in (\"duckdb\", \"starrocks\"):\n-            with self.subTest(f\"Generating json extraction with digit-prefixed key ({dialect})\"):\n+            with self.subTest(\n+                f\"Generating json extraction with digit-prefixed key ({dialect})\"\n+            ):\n                 self.assertEqual(\n-                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(dialect=dialect),\n+                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(\n+                        dialect=dialect\n+                    ),\n                     \"\"\"SELECT '{\"0\": \"v\"}' -> '0'\"\"\",\n                 )\n \n@@ -1809,7 +1833,9 @@ class TestDialect(Validator):\n         )\n \n     def test_operators(self):\n-        self.validate_identity(\"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\")\n+        self.validate_identity(\n+            \"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\"\n+        )\n         self.validate_identity(\"some.column LIKE 'foo' + another.column + 'bar'\")\n \n         self.validate_all(\"LIKE(x, 'z')\", write={\"\": \"'z' LIKE x\"})\n@@ -2299,7 +2325,9 @@ class TestDialect(Validator):\n         )\n \n     def test_typeddiv(self):\n-        typed_div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True)\n+        typed_div = exp.Div(\n+            this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True\n+        )\n         div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"))\n         typed_div_dialect = \"presto\"\n         div_dialect = \"hive\"\n@@ -2324,7 +2352,9 @@ class TestDialect(Validator):\n             (div, (INT, FLOAT), typed_div_dialect, \"a / b\"),\n             (div, (INT, FLOAT), div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"\n+            ):\n                 expression = expression.copy()\n                 expression.left.type = types[0]\n                 expression.right.type = types[1]\n@@ -2342,7 +2372,9 @@ class TestDialect(Validator):\n             (div, safe_div_dialect, \"a / b\"),\n             (div, div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {dialect} -> {expected}\"\n+            ):\n                 self.assertEqual(expected, expression.sql(dialect=dialect))\n \n         self.assertEqual(\n@@ -2814,7 +2846,8 @@ SELECT\n         self.validate_identity(\"COUNT_IF(DISTINCT cond)\")\n \n         self.validate_all(\n-            \"SELECT COUNT_IF(cond) FILTER\", write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"}\n+            \"SELECT COUNT_IF(cond) FILTER\",\n+            write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"},\n         )\n         self.validate_all(\n             \"SELECT COUNT_IF(col % 2 = 0) FROM foo\",\n@@ -3038,8 +3071,12 @@ FROM subquery2\"\"\",\n         with_large_nulls = \"postgres\"\n \n         sql = \"SELECT * FROM t ORDER BY c\"\n-        sql_nulls_last = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n-        sql_nulls_first = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        sql_nulls_last = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n+        )\n+        sql_nulls_first = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        )\n \n         for read_dialect, desc, nulls_first, expected_sql in (\n             (with_last_nulls, False, None, sql_nulls_last),\n@@ -3072,7 +3109,9 @@ FROM subquery2\"\"\",\n                 )\n \n                 expected_sql = f\"{expected_sql}{sort_order}\"\n-                expression = parse_one(f\"{sql}{sort_order}{null_order}\", read=read_dialect)\n+                expression = parse_one(\n+                    f\"{sql}{sort_order}{null_order}\", read=read_dialect\n+                )\n \n                 self.assertEqual(expression.sql(dialect=\"mysql\"), expected_sql)\n                 self.assertEqual(expression.sql(dialect=\"tsql\"), expected_sql)\n@@ -3157,7 +3196,9 @@ FROM subquery2\"\"\",\n         self.validate_identity(\n             \"CREATE SEQUENCE seq START WITH 1 NO MINVALUE NO MAXVALUE CYCLE NO CACHE\"\n         )\n-        self.validate_identity(\"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\")\n+        self.validate_identity(\n+            \"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\"\n+        )\n         self.validate_identity(\n             \"CREATE OR REPLACE SEQUENCE IF NOT EXISTS seq COMMENT='test comment' ORDER\"\n         )\n@@ -3318,7 +3359,9 @@ FROM subquery2\"\"\",\n                     },\n                 )\n \n-        self.assertIsInstance(parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var)\n+        self.assertIsInstance(\n+            parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var\n+        )\n \n     def test_coalesce(self):\n         \"\"\"\n@@ -3425,7 +3468,9 @@ FROM subquery2\"\"\",\n \n     def test_escaped_identifier_delimiter(self):\n         for dialect in (\"databricks\", \"hive\", \"mysql\", \"spark2\", \"spark\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3447,7 +3492,9 @@ FROM subquery2\"\"\",\n             \"snowflake\",\n             \"sqlite\",\n         ):\n-            with self.subTest(f\"Testing escaped double-quote in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped double-quote in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x\"\"\"',\n                     read={\n@@ -3459,7 +3506,9 @@ FROM subquery2\"\"\",\n                 )\n \n         for dialect in (\"clickhouse\", \"sqlite\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3569,14 +3618,19 @@ FROM subquery2\"\"\",\n                 \"spark\",\n                 \"redshift\",\n             ):\n-                with self.subTest(f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"):\n+                with self.subTest(\n+                    f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"\n+                ):\n                     self.assertEqual(\n-                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect), \"SELECT 204\"\n+                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect),\n+                        \"SELECT 204\",\n                     )\n \n             for other_integer_dialects in integer_dialects:\n                 self.assertEqual(\n-                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(other_integer_dialects),\n+                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(\n+                        other_integer_dialects\n+                    ),\n                     \"SELECT 0xCC\",\n                 )\n \ndiff --git a/tests/dialects/test_dremio.py b/tests/dialects/test_dremio.py\nindex d5fc36208..60588fdaa 100644\n--- a/tests/dialects/test_dremio.py\n+++ b/tests/dialects/test_dremio.py\n@@ -63,7 +63,8 @@ class TestDremio(Validator):\n             \"SELECT * FROM t ORDER BY a NULLS LAST\", \"SELECT * FROM t ORDER BY a\"\n         )\n         self.validate_identity(\n-            \"SELECT * FROM t ORDER BY a DESC NULLS LAST\", \"SELECT * FROM t ORDER BY a DESC\"\n+            \"SELECT * FROM t ORDER BY a DESC NULLS LAST\",\n+            \"SELECT * FROM t ORDER BY a DESC\",\n         )\n \n         # If the clause is not the default, it must be kept\n@@ -138,20 +139,26 @@ class TestDremio(Validator):\n         to_char = self.validate_identity(\"TO_CHAR(3.14, '#.#')\").assert_is(exp.ToChar)\n         assert to_char.args[\"is_numeric\"] is True\n \n-        to_char = self.validate_identity(\"TO_CHAR(columnname, '#.##')\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(columnname, '#.##')\").assert_is(\n+            exp.ToChar\n+        )\n         assert to_char.args[\"is_numeric\"] is True\n \n         # Non-numeric formats or columns should have is_numeric=None or False\n         to_char = self.validate_identity(\"TO_CHAR(5555)\").assert_is(exp.ToChar)\n         assert not to_char.args.get(\"is_numeric\")\n \n-        to_char = self.validate_identity(\"TO_CHAR(3.14, columnname)\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(3.14, columnname)\").assert_is(\n+            exp.ToChar\n+        )\n         assert not to_char.args.get(\"is_numeric\")\n \n         to_char = self.validate_identity(\"TO_CHAR(123, 'abcd')\").assert_is(exp.ToChar)\n         assert not to_char.args.get(\"is_numeric\")\n \n-        to_char = self.validate_identity(\"TO_CHAR(3.14, UPPER('abcd'))\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(3.14, UPPER('abcd'))\").assert_is(\n+            exp.ToChar\n+        )\n         assert not to_char.args.get(\"is_numeric\")\n \n     def test_time_diff(self):\n@@ -175,7 +182,9 @@ class TestDremio(Validator):\n             \"SELECT DATE_SUB(col, 2, 'HOUR')\", \"SELECT TIMESTAMPADD(HOUR, -2, col)\"\n         )\n \n-        self.validate_identity(\"SELECT DATE_ADD(col, 2, 'DAY')\", \"SELECT DATE_ADD(col, 2)\")\n+        self.validate_identity(\n+            \"SELECT DATE_ADD(col, 2, 'DAY')\", \"SELECT DATE_ADD(col, 2)\"\n+        )\n \n         self.validate_identity(\n             \"SELECT DATE_SUB(col, a, 'HOUR')\", \"SELECT TIMESTAMPADD(HOUR, a * -1, col)\"\n"
    },
    {
        "id": "278",
        "sha_fail": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
        "diff": "diff --git a/tools/document_segmentation_server.py b/tools/document_segmentation_server.py\nindex 3ce17bf..59cb57b 100644\n--- a/tools/document_segmentation_server.py\n+++ b/tools/document_segmentation_server.py\n@@ -22,7 +22,7 @@ large research papers and technical documents that exceed LLM token limits.\n    - Stores segmentation index for efficient retrieval\n    - Returns: JSON with segmentation status, strategy used, and segment count\n \n-\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None, \n+\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None,\n                          max_segments: int = 3, max_total_chars: int = None)\n    Purpose: Intelligently retrieves relevant document segments based on query context\n    - query_type: \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n@@ -63,8 +63,7 @@ import re\n import json\n import sys\n import io\n-from pathlib import Path\n-from typing import Dict, Any, List, Optional, Tuple\n+from typing import Dict, List, Tuple\n import hashlib\n import logging\n from datetime import datetime\n@@ -92,9 +91,11 @@ logger = logging.getLogger(__name__)\n # Create FastMCP server instance\n mcp = FastMCP(\"document-segmentation-server\")\n \n+\n @dataclass\n class DocumentSegment:\n     \"\"\"Represents a document segment with metadata\"\"\"\n+\n     id: str\n     title: str\n     content: str\n@@ -106,9 +107,11 @@ class DocumentSegment:\n     relevance_scores: Dict[str, float]  # Scores for different query types\n     section_path: str  # e.g., \"3.2.1\" for nested sections\n \n-@dataclass \n+\n+@dataclass\n class DocumentIndex:\n     \"\"\"Document index containing all segments and metadata\"\"\"\n+\n     document_path: str\n     document_type: str  # \"academic_paper\", \"technical_doc\", \"code_doc\", \"general\"\n     segmentation_strategy: str\n@@ -116,66 +119,86 @@ class DocumentIndex:\n     total_chars: int\n     segments: List[DocumentSegment]\n     created_at: str\n-    \n+\n+\n class DocumentAnalyzer:\n     \"\"\"Enhanced document analyzer using semantic content analysis instead of mechanical structure detection\"\"\"\n-    \n+\n     # More precise semantic indicators, weighted by importance\n     ALGORITHM_INDICATORS = {\n-        \"high\": [\"algorithm\", \"procedure\", \"method\", \"approach\", \"technique\", \"framework\"],\n+        \"high\": [\n+            \"algorithm\",\n+            \"procedure\",\n+            \"method\",\n+            \"approach\",\n+            \"technique\",\n+            \"framework\",\n+        ],\n         \"medium\": [\"step\", \"process\", \"implementation\", \"computation\", \"calculation\"],\n-        \"low\": [\"example\", \"illustration\", \"demonstration\"]\n+        \"low\": [\"example\", \"illustration\", \"demonstration\"],\n     }\n-    \n+\n     TECHNICAL_CONCEPT_INDICATORS = {\n         \"high\": [\"formula\", \"equation\", \"theorem\", \"lemma\", \"proof\", \"definition\"],\n         \"medium\": [\"parameter\", \"variable\", \"function\", \"model\", \"architecture\"],\n-        \"low\": [\"notation\", \"symbol\", \"term\"]\n+        \"low\": [\"notation\", \"symbol\", \"term\"],\n     }\n-    \n+\n     IMPLEMENTATION_INDICATORS = {\n         \"high\": [\"code\", \"implementation\", \"programming\", \"software\", \"system\"],\n         \"medium\": [\"design\", \"structure\", \"module\", \"component\", \"interface\"],\n-        \"low\": [\"tool\", \"library\", \"package\"]\n+        \"low\": [\"tool\", \"library\", \"package\"],\n     }\n-    \n+\n     # Semantic features of document types (not just based on titles)\n     RESEARCH_PAPER_PATTERNS = [\n         r\"(?i)\\babstract\\b.*?\\n.*?(introduction|motivation|background)\",\n         r\"(?i)(methodology|method).*?(experiment|evaluation|result)\",\n         r\"(?i)(conclusion|future work|limitation).*?(reference|bibliography)\",\n-        r\"(?i)(related work|literature review|prior art)\"\n+        r\"(?i)(related work|literature review|prior art)\",\n     ]\n-    \n+\n     TECHNICAL_DOC_PATTERNS = [\n         r\"(?i)(getting started|installation|setup).*?(usage|example)\",\n         r\"(?i)(api|interface|specification).*?(parameter|endpoint)\",\n         r\"(?i)(tutorial|guide|walkthrough).*?(step|instruction)\",\n-        r\"(?i)(troubleshooting|faq|common issues)\"\n+        r\"(?i)(troubleshooting|faq|common issues)\",\n     ]\n-    \n+\n     def analyze_document_type(self, content: str) -> Tuple[str, float]:\n         \"\"\"\n         Enhanced document type analysis based on semantic content patterns\n-        \n+\n         Returns:\n             Tuple[str, float]: (document_type, confidence_score)\n         \"\"\"\n         content_lower = content.lower()\n-        \n+\n         # Calculate weighted semantic indicator scores\n-        algorithm_score = self._calculate_weighted_score(content_lower, self.ALGORITHM_INDICATORS)\n-        concept_score = self._calculate_weighted_score(content_lower, self.TECHNICAL_CONCEPT_INDICATORS)\n-        implementation_score = self._calculate_weighted_score(content_lower, self.IMPLEMENTATION_INDICATORS)\n-        \n+        algorithm_score = self._calculate_weighted_score(\n+            content_lower, self.ALGORITHM_INDICATORS\n+        )\n+        concept_score = self._calculate_weighted_score(\n+            content_lower, self.TECHNICAL_CONCEPT_INDICATORS\n+        )\n+        implementation_score = self._calculate_weighted_score(\n+            content_lower, self.IMPLEMENTATION_INDICATORS\n+        )\n+\n         # Detect semantic patterns of document types\n-        research_pattern_score = self._detect_pattern_score(content, self.RESEARCH_PAPER_PATTERNS)\n-        technical_pattern_score = self._detect_pattern_score(content, self.TECHNICAL_DOC_PATTERNS)\n-        \n+        research_pattern_score = self._detect_pattern_score(\n+            content, self.RESEARCH_PAPER_PATTERNS\n+        )\n+        technical_pattern_score = self._detect_pattern_score(\n+            content, self.TECHNICAL_DOC_PATTERNS\n+        )\n+\n         # Comprehensive evaluation of document type\n-        total_research_score = algorithm_score + concept_score + research_pattern_score * 2\n+        total_research_score = (\n+            algorithm_score + concept_score + research_pattern_score * 2\n+        )\n         total_technical_score = implementation_score + technical_pattern_score * 2\n-        \n+\n         # Determine document type based on content density and pattern matching\n         if research_pattern_score > 0.5 and total_research_score > 3.0:\n             return \"research_paper\", min(0.95, 0.6 + research_pattern_score * 0.35)\n@@ -187,17 +210,21 @@ class DocumentAnalyzer:\n             return \"implementation_guide\", 0.75\n         else:\n             return \"general_document\", 0.5\n-    \n-    def _calculate_weighted_score(self, content: str, indicators: Dict[str, List[str]]) -> float:\n+\n+    def _calculate_weighted_score(\n+        self, content: str, indicators: Dict[str, List[str]]\n+    ) -> float:\n         \"\"\"Calculate weighted semantic indicator scores\"\"\"\n         score = 0.0\n         for weight_level, terms in indicators.items():\n             weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[weight_level]\n             for term in terms:\n                 if term in content:\n-                    score += weight * (content.count(term) * 0.5 + 1)  # Consider term frequency\n+                    score += weight * (\n+                        content.count(term) * 0.5 + 1\n+                    )  # Consider term frequency\n         return score\n-    \n+\n     def _detect_pattern_score(self, content: str, patterns: List[str]) -> float:\n         \"\"\"Detect semantic pattern matching scores\"\"\"\n         matches = 0\n@@ -205,7 +232,7 @@ class DocumentAnalyzer:\n             if re.search(pattern, content, re.DOTALL):\n                 matches += 1\n         return matches / len(patterns)\n-    \n+\n     def detect_segmentation_strategy(self, content: str, doc_type: str) -> str:\n         \"\"\"\n         Intelligently determine the best segmentation strategy based on content semantics rather than mechanical structure\n@@ -213,8 +240,10 @@ class DocumentAnalyzer:\n         # Analyze content characteristics\n         algorithm_density = self._calculate_algorithm_density(content)\n         concept_complexity = self._calculate_concept_complexity(content)\n-        implementation_detail_level = self._calculate_implementation_detail_level(content)\n-        \n+        implementation_detail_level = self._calculate_implementation_detail_level(\n+            content\n+        )\n+\n         # Select strategy based on document type and content characteristics\n         if doc_type == \"research_paper\" and algorithm_density > 0.3:\n             return \"semantic_research_focused\"\n@@ -226,21 +255,21 @@ class DocumentAnalyzer:\n             return \"semantic_chunking_enhanced\"\n         else:\n             return \"content_aware_segmentation\"\n-    \n+\n     def _calculate_algorithm_density(self, content: str) -> float:\n         \"\"\"Calculate algorithm content density\"\"\"\n         total_chars = len(content)\n         algorithm_chars = 0\n-        \n+\n         # Identify algorithm blocks\n         algorithm_patterns = [\n-            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)',\n-            r'(?i)(step\\s+\\d+|phase\\s+\\d+)',\n-            r'(?i)(input:|output:|return:|initialize:)',\n-            r'(?i)(for\\s+each|while|if.*then|else)',\n-            r'(?i)(function|method|procedure).*\\('\n+            r\"(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)\",\n+            r\"(?i)(step\\s+\\d+|phase\\s+\\d+)\",\n+            r\"(?i)(input:|output:|return:|initialize:)\",\n+            r\"(?i)(for\\s+each|while|if.*then|else)\",\n+            r\"(?i)(function|method|procedure).*\\(\",\n         ]\n-        \n+\n         for pattern in algorithm_patterns:\n             matches = re.finditer(pattern, content)\n             for match in matches:\n@@ -248,44 +277,45 @@ class DocumentAnalyzer:\n                 start = max(0, match.start() - 200)\n                 end = min(len(content), match.end() + 800)\n                 algorithm_chars += end - start\n-        \n+\n         return min(1.0, algorithm_chars / total_chars)\n-    \n+\n     def _calculate_concept_complexity(self, content: str) -> float:\n         \"\"\"Calculate concept complexity\"\"\"\n         concept_indicators = self.TECHNICAL_CONCEPT_INDICATORS\n         complexity_score = 0.0\n-        \n+\n         for level, terms in concept_indicators.items():\n             weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[level]\n             for term in terms:\n                 complexity_score += content.lower().count(term) * weight\n-        \n+\n         # Normalize to 0-1 range\n         return min(1.0, complexity_score / 100)\n-    \n+\n     def _calculate_implementation_detail_level(self, content: str) -> float:\n         \"\"\"Calculate implementation detail level\"\"\"\n         implementation_patterns = [\n-            r'(?i)(code|implementation|programming)',\n-            r'(?i)(class|function|method|variable)',\n-            r'(?i)(import|include|library)',\n-            r'(?i)(parameter|argument|return)',\n-            r'(?i)(example|demo|tutorial)'\n+            r\"(?i)(code|implementation|programming)\",\n+            r\"(?i)(class|function|method|variable)\",\n+            r\"(?i)(import|include|library)\",\n+            r\"(?i)(parameter|argument|return)\",\n+            r\"(?i)(example|demo|tutorial)\",\n         ]\n-        \n+\n         detail_score = 0\n         for pattern in implementation_patterns:\n             detail_score += len(re.findall(pattern, content))\n-        \n+\n         return min(1.0, detail_score / 50)\n \n+\n class DocumentSegmenter:\n     \"\"\"Creates intelligent segments from documents\"\"\"\n-    \n+\n     def __init__(self):\n         self.analyzer = DocumentAnalyzer()\n-    \n+\n     def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:\n         \"\"\"\n         Perform intelligent segmentation using the specified strategy\n@@ -303,38 +333,45 @@ class DocumentSegmenter:\n         else:\n             # Compatibility with legacy strategies\n             return self._segment_by_enhanced_semantic_chunks(content)\n-    \n+\n     def _segment_by_headers(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment document based on markdown headers\"\"\"\n         segments = []\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n         current_segment = []\n         current_header = None\n         current_level = 0\n         char_pos = 0\n-        \n+\n         for line in lines:\n-            line_with_newline = line + '\\n'\n-            \n+            line_with_newline = line + \"\\n\"\n+\n             # Check if line is a header\n-            header_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n-            \n+            header_match = re.match(r\"^(#{1,6})\\s+(.+)$\", line)\n+\n             if header_match:\n                 # Save previous segment if exists\n                 if current_segment and current_header:\n-                    segment_content = '\\n'.join(current_segment).strip()\n+                    segment_content = \"\\n\".join(current_segment).strip()\n                     if segment_content:\n                         # Analyze content type and importance\n-                        content_type = self._classify_content_type(current_header, segment_content)\n-                        importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n-                        \n+                        content_type = self._classify_content_type(\n+                            current_header, segment_content\n+                        )\n+                        importance_score = (\n+                            0.8 if content_type in [\"algorithm\", \"formula\"] else 0.7\n+                        )\n+\n                         segment = self._create_enhanced_segment(\n-                            segment_content, current_header, \n-                            char_pos - len(segment_content.encode('utf-8')), char_pos,\n-                            importance_score, content_type\n+                            segment_content,\n+                            current_header,\n+                            char_pos - len(segment_content.encode(\"utf-8\")),\n+                            char_pos,\n+                            importance_score,\n+                            content_type,\n                         )\n                         segments.append(segment)\n-                \n+\n                 # Start new segment\n                 current_level = len(header_match.group(1))\n                 current_header = header_match.group(2).strip()\n@@ -342,128 +379,143 @@ class DocumentSegmenter:\n             else:\n                 if current_segment is not None:\n                     current_segment.append(line)\n-            \n-            char_pos += len(line_with_newline.encode('utf-8'))\n-        \n+\n+            char_pos += len(line_with_newline.encode(\"utf-8\"))\n+\n         # Add final segment\n         if current_segment and current_header:\n-            segment_content = '\\n'.join(current_segment).strip()\n+            segment_content = \"\\n\".join(current_segment).strip()\n             if segment_content:\n                 # Analyze content type and importance\n-                content_type = self._classify_content_type(current_header, segment_content)\n-                importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n-                \n+                content_type = self._classify_content_type(\n+                    current_header, segment_content\n+                )\n+                importance_score = (\n+                    0.8 if content_type in [\"algorithm\", \"formula\"] else 0.7\n+                )\n+\n                 segment = self._create_enhanced_segment(\n-                    segment_content, current_header, \n-                    char_pos - len(segment_content.encode('utf-8')), char_pos,\n-                    importance_score, content_type\n+                    segment_content,\n+                    current_header,\n+                    char_pos - len(segment_content.encode(\"utf-8\")),\n+                    char_pos,\n+                    importance_score,\n+                    content_type,\n                 )\n                 segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_preserve_algorithm_integrity(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_preserve_algorithm_integrity(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Smart segmentation strategy that preserves algorithm integrity\"\"\"\n         segments = []\n-        \n+\n         # 1. Identify algorithm blocks and related descriptions\n         algorithm_blocks = self._identify_algorithm_blocks(content)\n-        \n+\n         # 2. Identify concept definition groups\n         concept_groups = self._identify_concept_groups(content)\n-        \n+\n         # 3. Identify formula derivation chains\n         formula_chains = self._identify_formula_chains(content)\n-        \n+\n         # 4. Merge related content blocks to ensure integrity\n         content_blocks = self._merge_related_content_blocks(\n             algorithm_blocks, concept_groups, formula_chains, content\n         )\n-        \n+\n         # 5. Convert to DocumentSegment\n         for i, block in enumerate(content_blocks):\n             segment = self._create_enhanced_segment(\n-                block['content'], \n-                block['title'], \n-                block['start_pos'], \n-                block['end_pos'],\n-                block['importance_score'],\n-                block['content_type']\n+                block[\"content\"],\n+                block[\"title\"],\n+                block[\"start_pos\"],\n+                block[\"end_pos\"],\n+                block[\"importance_score\"],\n+                block[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_research_paper_semantically(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_research_paper_semantically(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Semantic segmentation specifically for research papers\"\"\"\n         segments = []\n-        \n+\n         # Identify semantic structure of research papers\n         paper_sections = self._identify_research_paper_sections(content)\n-        \n+\n         for section in paper_sections:\n             # Ensure each section contains sufficient context\n             enhanced_content = self._enhance_section_with_context(section, content)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                enhanced_content['content'],\n-                enhanced_content['title'],\n-                enhanced_content['start_pos'],\n-                enhanced_content['end_pos'],\n-                enhanced_content['importance_score'],\n-                enhanced_content['content_type']\n+                enhanced_content[\"content\"],\n+                enhanced_content[\"title\"],\n+                enhanced_content[\"start_pos\"],\n+                enhanced_content[\"end_pos\"],\n+                enhanced_content[\"importance_score\"],\n+                enhanced_content[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_concept_implementation_hybrid(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_concept_implementation_hybrid(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Intelligent segmentation combining concepts and implementation\"\"\"\n         segments = []\n-        \n+\n         # Identify concept-implementation correspondence\n         concept_impl_pairs = self._identify_concept_implementation_pairs(content)\n-        \n+\n         for pair in concept_impl_pairs:\n             # Merge related concepts and implementations into one segment\n             merged_content = self._merge_concept_with_implementation(pair, content)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                merged_content['content'],\n-                merged_content['title'],\n-                merged_content['start_pos'],\n-                merged_content['end_pos'],\n-                merged_content['importance_score'],\n-                merged_content['content_type']\n+                merged_content[\"content\"],\n+                merged_content[\"title\"],\n+                merged_content[\"start_pos\"],\n+                merged_content[\"end_pos\"],\n+                merged_content[\"importance_score\"],\n+                merged_content[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_by_enhanced_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_by_enhanced_semantic_chunks(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Enhanced semantic chunk segmentation\"\"\"\n         segments = []\n-        \n+\n         # Use improved semantic boundary detection\n         semantic_boundaries = self._detect_semantic_boundaries(content)\n-        \n+\n         current_start = 0\n         for i, boundary in enumerate(semantic_boundaries):\n-            chunk_content = content[current_start:boundary['position']]\n-            \n+            chunk_content = content[current_start : boundary[\"position\"]]\n+\n             if len(chunk_content.strip()) > 200:  # Minimum content threshold\n                 segment = self._create_enhanced_segment(\n                     chunk_content,\n-                    boundary['suggested_title'],\n+                    boundary[\"suggested_title\"],\n                     current_start,\n-                    boundary['position'],\n-                    boundary['importance_score'],\n-                    boundary['content_type']\n+                    boundary[\"position\"],\n+                    boundary[\"importance_score\"],\n+                    boundary[\"content_type\"],\n                 )\n                 segments.append(segment)\n-            \n-            current_start = boundary['position']\n-        \n+\n+            current_start = boundary[\"position\"]\n+\n         # Handle the final segment\n         if current_start < len(content):\n             final_content = content[current_start:]\n@@ -474,417 +526,484 @@ class DocumentSegmenter:\n                     current_start,\n                     len(content),\n                     0.7,\n-                    \"general\"\n+                    \"general\",\n                 )\n                 segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_content_aware(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Content-aware intelligent segmentation\"\"\"\n         segments = []\n-        \n+\n         # Adaptive segmentation size\n         optimal_chunk_size = self._calculate_optimal_chunk_size(content)\n-        \n+\n         # Segment based on content density\n         content_chunks = self._create_content_aware_chunks(content, optimal_chunk_size)\n-        \n+\n         for chunk in content_chunks:\n             segment = self._create_enhanced_segment(\n-                chunk['content'],\n-                chunk['title'],\n-                chunk['start_pos'],\n-                chunk['end_pos'],\n-                chunk['importance_score'],\n-                chunk['content_type']\n+                chunk[\"content\"],\n+                chunk[\"title\"],\n+                chunk[\"start_pos\"],\n+                chunk[\"end_pos\"],\n+                chunk[\"importance_score\"],\n+                chunk[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_academic_paper(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment academic paper using semantic understanding\"\"\"\n         # First try header-based segmentation\n-        headers = re.findall(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE)\n+        headers = re.findall(r\"^(#{1,6})\\s+(.+)$\", content, re.MULTILINE)\n         if len(headers) >= 2:\n             return self._segment_by_headers(content)\n-        \n+\n         # Fallback to semantic detection of academic sections\n         sections = self._detect_academic_sections(content)\n         segments = []\n-        \n+\n         for section in sections:\n             # Determine importance based on section type\n-            section_type = section.get('type', 'general')\n-            content_type = section_type if section_type in ['algorithm', 'formula', 'introduction', 'conclusion'] else 'general'\n+            section_type = section.get(\"type\", \"general\")\n+            content_type = (\n+                section_type\n+                if section_type\n+                in [\"algorithm\", \"formula\", \"introduction\", \"conclusion\"]\n+                else \"general\"\n+            )\n             importance_score = {\n-                'algorithm': 0.95,\n-                'formula': 0.9,\n-                'introduction': 0.85,\n-                'conclusion': 0.8\n+                \"algorithm\": 0.95,\n+                \"formula\": 0.9,\n+                \"introduction\": 0.85,\n+                \"conclusion\": 0.8,\n             }.get(content_type, 0.7)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                section['content'], \n-                section['title'], \n-                section['start_pos'], \n-                section['end_pos'],\n+                section[\"content\"],\n+                section[\"title\"],\n+                section[\"start_pos\"],\n+                section[\"end_pos\"],\n                 importance_score,\n-                content_type\n+                content_type,\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _detect_academic_sections(self, content: str) -> List[Dict]:\n         \"\"\"Detect academic paper sections even without clear headers\"\"\"\n         sections = []\n-        \n+\n         # Common academic section patterns\n         section_patterns = [\n-            (r'(?i)(abstract|\u6458\u8981)', 'introduction'),\n-            (r'(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)', 'introduction'),\n-            (r'(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)', 'background'),\n-            (r'(?i)(method|methodology|approach|\u65b9\u6cd5)', 'methodology'),\n-            (r'(?i)(algorithm|\u7b97\u6cd5)', 'algorithm'),\n-            (r'(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)', 'experiment'),\n-            (r'(?i)(result|\u7ed3\u679c|finding)', 'results'),\n-            (r'(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)', 'conclusion'),\n-            (r'(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)', 'references')\n+            (r\"(?i)(abstract|\u6458\u8981)\", \"introduction\"),\n+            (r\"(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)\", \"introduction\"),\n+            (r\"(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)\", \"background\"),\n+            (r\"(?i)(method|methodology|approach|\u65b9\u6cd5)\", \"methodology\"),\n+            (r\"(?i)(algorithm|\u7b97\u6cd5)\", \"algorithm\"),\n+            (r\"(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)\", \"experiment\"),\n+            (r\"(?i)(result|\u7ed3\u679c|finding)\", \"results\"),\n+            (r\"(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)\", \"conclusion\"),\n+            (r\"(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)\", \"references\"),\n         ]\n-        \n+\n         current_pos = 0\n         for i, (pattern, section_type) in enumerate(section_patterns):\n             match = re.search(pattern, content[current_pos:], re.IGNORECASE)\n             if match:\n                 start_pos = current_pos + match.start()\n-                \n+\n                 # Find end position (next section or end of document)\n                 next_pos = len(content)\n-                for next_pattern, _ in section_patterns[i+1:]:\n-                    next_match = re.search(next_pattern, content[start_pos+100:], re.IGNORECASE)\n+                for next_pattern, _ in section_patterns[i + 1 :]:\n+                    next_match = re.search(\n+                        next_pattern, content[start_pos + 100 :], re.IGNORECASE\n+                    )\n                     if next_match:\n                         next_pos = start_pos + 100 + next_match.start()\n                         break\n-                \n+\n                 section_content = content[start_pos:next_pos].strip()\n                 if len(section_content) > 50:  # Minimum content length\n                     # Calculate importance score and content type\n-                    importance_score = self._calculate_paragraph_importance(section_content, section_type)\n-                    content_type = self._classify_content_type(match.group(1), section_content)\n-                    \n-                    sections.append({\n-                        'title': match.group(1),\n-                        'content': section_content,\n-                        'start_pos': start_pos,\n-                        'end_pos': next_pos,\n-                        'type': section_type,\n-                        'importance_score': importance_score,\n-                        'content_type': content_type\n-                    })\n-                \n+                    importance_score = self._calculate_paragraph_importance(\n+                        section_content, section_type\n+                    )\n+                    content_type = self._classify_content_type(\n+                        match.group(1), section_content\n+                    )\n+\n+                    sections.append(\n+                        {\n+                            \"title\": match.group(1),\n+                            \"content\": section_content,\n+                            \"start_pos\": start_pos,\n+                            \"end_pos\": next_pos,\n+                            \"type\": section_type,\n+                            \"importance_score\": importance_score,\n+                            \"content_type\": content_type,\n+                        }\n+                    )\n+\n                 current_pos = next_pos\n-        \n+\n         return sections\n-    \n+\n     def _segment_by_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment long documents into semantic chunks\"\"\"\n         # Split into paragraphs first\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n-        \n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n+\n         segments = []\n         current_chunk = []\n         current_chunk_size = 0\n         chunk_size_limit = 3000  # characters\n         overlap_size = 200\n-        \n+\n         char_pos = 0\n-        \n+\n         for para in paragraphs:\n             para_size = len(para)\n-            \n+\n             # If adding this paragraph exceeds limit, create a segment\n             if current_chunk_size + para_size > chunk_size_limit and current_chunk:\n-                chunk_content = '\\n\\n'.join(current_chunk)\n+                chunk_content = \"\\n\\n\".join(current_chunk)\n                 # Analyze semantic chunk content type\n                 content_type = self._classify_paragraph_type(chunk_content)\n-                importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n-                \n+                importance_score = self._calculate_paragraph_importance(\n+                    chunk_content, content_type\n+                )\n+\n                 segment = self._create_enhanced_segment(\n-                    chunk_content, \n+                    chunk_content,\n                     f\"Section {len(segments) + 1}\",\n-                    char_pos - len(chunk_content.encode('utf-8')),\n+                    char_pos - len(chunk_content.encode(\"utf-8\")),\n                     char_pos,\n                     importance_score,\n-                    content_type\n+                    content_type,\n                 )\n                 segments.append(segment)\n-                \n+\n                 # Keep last part for overlap\n-                overlap_content = chunk_content[-overlap_size:] if len(chunk_content) > overlap_size else \"\"\n+                overlap_content = (\n+                    chunk_content[-overlap_size:]\n+                    if len(chunk_content) > overlap_size\n+                    else \"\"\n+                )\n                 current_chunk = [overlap_content, para] if overlap_content else [para]\n                 current_chunk_size = len(overlap_content) + para_size\n             else:\n                 current_chunk.append(para)\n                 current_chunk_size += para_size\n-            \n+\n             char_pos += para_size + 2  # +2 for \\n\\n\n-        \n+\n         # Add final chunk\n         if current_chunk:\n-            chunk_content = '\\n\\n'.join(current_chunk)\n+            chunk_content = \"\\n\\n\".join(current_chunk)\n             # Analyze final chunk content type\n             content_type = self._classify_paragraph_type(chunk_content)\n-            importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n-            \n+            importance_score = self._calculate_paragraph_importance(\n+                chunk_content, content_type\n+            )\n+\n             segment = self._create_enhanced_segment(\n                 chunk_content,\n                 f\"Section {len(segments) + 1}\",\n-                char_pos - len(chunk_content.encode('utf-8')),\n+                char_pos - len(chunk_content.encode(\"utf-8\")),\n                 char_pos,\n                 importance_score,\n-                content_type\n+                content_type,\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_by_paragraphs(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Simple paragraph-based segmentation for short documents\"\"\"\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n         segments = []\n         char_pos = 0\n-        \n+\n         for i, para in enumerate(paragraphs):\n             if len(para) > 100:  # Only include substantial paragraphs\n                 # Analyze paragraph type and importance\n                 content_type = self._classify_paragraph_type(para)\n-                importance_score = self._calculate_paragraph_importance(para, content_type)\n-                \n+                importance_score = self._calculate_paragraph_importance(\n+                    para, content_type\n+                )\n+\n                 segment = self._create_enhanced_segment(\n                     para,\n                     f\"Paragraph {i + 1}\",\n                     char_pos,\n-                    char_pos + len(para.encode('utf-8')),\n+                    char_pos + len(para.encode(\"utf-8\")),\n                     importance_score,\n-                    content_type\n+                    content_type,\n                 )\n                 segments.append(segment)\n-            char_pos += len(para.encode('utf-8')) + 2\n-        \n+            char_pos += len(para.encode(\"utf-8\")) + 2\n+\n         return segments\n-    \n+\n     # =============== Enhanced intelligent segmentation helper methods ===============\n-    \n+\n     def _identify_algorithm_blocks(self, content: str) -> List[Dict]:\n         \"\"\"Identify algorithm blocks and related descriptions\"\"\"\n         algorithm_blocks = []\n-        \n+\n         # Algorithm block identification patterns\n         algorithm_patterns = [\n-            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)',\n-            r'(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)',\n-            r'(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)',\n-            r'(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)'\n+            r\"(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)\",\n+            r\"(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)\",\n+            r\"(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)\",\n+            r\"(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)\",\n         ]\n-        \n+\n         for pattern in algorithm_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 # Expand context to include complete descriptions\n                 start = max(0, match.start() - 300)\n                 end = min(len(content), match.end() + 500)\n-                \n+\n                 # Find natural boundaries\n-                while start > 0 and content[start] not in '\\n.!?':\n+                while start > 0 and content[start] not in \"\\n.!?\":\n                     start -= 1\n-                while end < len(content) and content[end] not in '\\n.!?':\n+                while end < len(content) and content[end] not in \"\\n.!?\":\n                     end += 1\n-                \n-                algorithm_blocks.append({\n-                    'start_pos': start,\n-                    'end_pos': end,\n-                    'content': content[start:end].strip(),\n-                    'title': self._extract_algorithm_title(content[match.start():match.end()]),\n-                    'importance_score': 0.95,  # High importance for algorithm blocks\n-                    'content_type': 'algorithm'\n-                })\n-        \n+\n+                algorithm_blocks.append(\n+                    {\n+                        \"start_pos\": start,\n+                        \"end_pos\": end,\n+                        \"content\": content[start:end].strip(),\n+                        \"title\": self._extract_algorithm_title(\n+                            content[match.start() : match.end()]\n+                        ),\n+                        \"importance_score\": 0.95,  # High importance for algorithm blocks\n+                        \"content_type\": \"algorithm\",\n+                    }\n+                )\n+\n         return algorithm_blocks\n-    \n+\n     def _identify_concept_groups(self, content: str) -> List[Dict]:\n         \"\"\"Identify concept definition groups\"\"\"\n         concept_groups = []\n-        \n+\n         # Concept definition patterns\n         concept_patterns = [\n-            r'(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)',\n-            r'(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)',\n-            r'(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)'\n+            r\"(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)\",\n+            r\"(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)\",\n+            r\"(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)\",\n         ]\n-        \n+\n         for pattern in concept_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 # Expand context\n                 start = max(0, match.start() - 200)\n                 end = min(len(content), match.end() + 300)\n-                \n-                concept_groups.append({\n-                    'start_pos': start,\n-                    'end_pos': end,\n-                    'content': content[start:end].strip(),\n-                    'title': self._extract_concept_title(content[match.start():match.end()]),\n-                    'importance_score': 0.85,\n-                    'content_type': 'concept'\n-                })\n-        \n+\n+                concept_groups.append(\n+                    {\n+                        \"start_pos\": start,\n+                        \"end_pos\": end,\n+                        \"content\": content[start:end].strip(),\n+                        \"title\": self._extract_concept_title(\n+                            content[match.start() : match.end()]\n+                        ),\n+                        \"importance_score\": 0.85,\n+                        \"content_type\": \"concept\",\n+                    }\n+                )\n+\n         return concept_groups\n-    \n+\n     def _identify_formula_chains(self, content: str) -> List[Dict]:\n         \"\"\"Identify formula derivation chains\"\"\"\n         formula_chains = []\n-        \n+\n         # Formula patterns\n         formula_patterns = [\n-            r'\\$\\$.*?\\$\\$',  # Block-level mathematical formulas\n-            r'\\$[^$]+\\$',    # Inline mathematical formulas\n-            r'(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)',\n-            r'(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)'\n+            r\"\\$\\$.*?\\$\\$\",  # Block-level mathematical formulas\n+            r\"\\$[^$]+\\$\",  # Inline mathematical formulas\n+            r\"(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)\",\n+            r\"(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)\",\n         ]\n-        \n+\n         # Find dense formula regions\n         formula_positions = []\n         for pattern in formula_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 formula_positions.append((match.start(), match.end()))\n-        \n+\n         # Merge nearby formulas into formula chains\n         formula_positions.sort()\n         if formula_positions:\n             current_chain_start = formula_positions[0][0]\n             current_chain_end = formula_positions[0][1]\n-            \n+\n             for start, end in formula_positions[1:]:\n-                if start - current_chain_end < 500:  # Merge formulas within 500 characters\n+                if (\n+                    start - current_chain_end < 500\n+                ):  # Merge formulas within 500 characters\n                     current_chain_end = end\n                 else:\n                     # Save current chain\n-                    formula_chains.append({\n-                        'start_pos': max(0, current_chain_start - 200),\n-                        'end_pos': min(len(content), current_chain_end + 200),\n-                        'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n-                        'title': 'Mathematical Formulation',\n-                        'importance_score': 0.9,\n-                        'content_type': 'formula'\n-                    })\n+                    formula_chains.append(\n+                        {\n+                            \"start_pos\": max(0, current_chain_start - 200),\n+                            \"end_pos\": min(len(content), current_chain_end + 200),\n+                            \"content\": content[\n+                                max(0, current_chain_start - 200) : min(\n+                                    len(content), current_chain_end + 200\n+                                )\n+                            ].strip(),\n+                            \"title\": \"Mathematical Formulation\",\n+                            \"importance_score\": 0.9,\n+                            \"content_type\": \"formula\",\n+                        }\n+                    )\n                     current_chain_start = start\n                     current_chain_end = end\n-            \n+\n             # Add the last chain\n-            formula_chains.append({\n-                'start_pos': max(0, current_chain_start - 200),\n-                'end_pos': min(len(content), current_chain_end + 200),\n-                'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n-                'title': 'Mathematical Formulation',\n-                'importance_score': 0.9,\n-                'content_type': 'formula'\n-            })\n-        \n+            formula_chains.append(\n+                {\n+                    \"start_pos\": max(0, current_chain_start - 200),\n+                    \"end_pos\": min(len(content), current_chain_end + 200),\n+                    \"content\": content[\n+                        max(0, current_chain_start - 200) : min(\n+                            len(content), current_chain_end + 200\n+                        )\n+                    ].strip(),\n+                    \"title\": \"Mathematical Formulation\",\n+                    \"importance_score\": 0.9,\n+                    \"content_type\": \"formula\",\n+                }\n+            )\n+\n         return formula_chains\n-    \n-    def _merge_related_content_blocks(self, algorithm_blocks: List[Dict], concept_groups: List[Dict], \n-                                    formula_chains: List[Dict], content: str) -> List[Dict]:\n+\n+    def _merge_related_content_blocks(\n+        self,\n+        algorithm_blocks: List[Dict],\n+        concept_groups: List[Dict],\n+        formula_chains: List[Dict],\n+        content: str,\n+    ) -> List[Dict]:\n         \"\"\"Merge related content blocks to ensure integrity\"\"\"\n         all_blocks = algorithm_blocks + concept_groups + formula_chains\n-        all_blocks.sort(key=lambda x: x['start_pos'])\n-        \n+        all_blocks.sort(key=lambda x: x[\"start_pos\"])\n+\n         merged_blocks = []\n         i = 0\n-        \n+\n         while i < len(all_blocks):\n             current_block = all_blocks[i]\n-            \n+\n             # Check if can merge with the next block\n             while i + 1 < len(all_blocks):\n                 next_block = all_blocks[i + 1]\n-                \n+\n                 # If blocks are close or content related, merge them\n-                if (next_block['start_pos'] - current_block['end_pos'] < 300 or\n-                    self._are_blocks_related(current_block, next_block)):\n-                    \n+                if next_block[\"start_pos\"] - current_block[\n+                    \"end_pos\"\n+                ] < 300 or self._are_blocks_related(current_block, next_block):\n                     # Merge blocks\n-                    merged_content = content[current_block['start_pos']:next_block['end_pos']]\n+                    merged_content = content[\n+                        current_block[\"start_pos\"] : next_block[\"end_pos\"]\n+                    ]\n                     current_block = {\n-                        'start_pos': current_block['start_pos'],\n-                        'end_pos': next_block['end_pos'],\n-                        'content': merged_content.strip(),\n-                        'title': f\"{current_block['title']} & {next_block['title']}\",\n-                        'importance_score': max(current_block['importance_score'], next_block['importance_score']),\n-                        'content_type': 'merged'\n+                        \"start_pos\": current_block[\"start_pos\"],\n+                        \"end_pos\": next_block[\"end_pos\"],\n+                        \"content\": merged_content.strip(),\n+                        \"title\": f\"{current_block['title']} & {next_block['title']}\",\n+                        \"importance_score\": max(\n+                            current_block[\"importance_score\"],\n+                            next_block[\"importance_score\"],\n+                        ),\n+                        \"content_type\": \"merged\",\n                     }\n                     i += 1\n                 else:\n                     break\n-            \n+\n             merged_blocks.append(current_block)\n             i += 1\n-        \n+\n         return merged_blocks\n-    \n+\n     def _are_blocks_related(self, block1: Dict, block2: Dict) -> bool:\n         \"\"\"Determine if two content blocks are related\"\"\"\n         # Check content type associations\n         related_types = [\n-            ('algorithm', 'formula'),\n-            ('concept', 'algorithm'),\n-            ('formula', 'concept')\n+            (\"algorithm\", \"formula\"),\n+            (\"concept\", \"algorithm\"),\n+            (\"formula\", \"concept\"),\n         ]\n-        \n+\n         for type1, type2 in related_types:\n-            if ((block1['content_type'] == type1 and block2['content_type'] == type2) or\n-                (block1['content_type'] == type2 and block2['content_type'] == type1)):\n+            if (\n+                block1[\"content_type\"] == type1 and block2[\"content_type\"] == type2\n+            ) or (block1[\"content_type\"] == type2 and block2[\"content_type\"] == type1):\n                 return True\n-        \n+\n         return False\n-    \n+\n     def _extract_algorithm_title(self, text: str) -> str:\n         \"\"\"Extract title from algorithm text\"\"\"\n-        lines = text.split('\\n')[:3]  # First 3 lines\n+        lines = text.split(\"\\n\")[:3]  # First 3 lines\n         for line in lines:\n             line = line.strip()\n             if line and len(line) < 100:  # Reasonable title length\n                 # Clean title\n-                title = re.sub(r'[^\\w\\s-]', '', line)\n+                title = re.sub(r\"[^\\w\\s-]\", \"\", line)\n                 if title:\n                     return title[:50]  # Limit title length\n         return \"Algorithm Block\"\n-    \n+\n     def _extract_concept_title(self, text: str) -> str:\n         \"\"\"Extract title from concept text\"\"\"\n-        lines = text.split('\\n')[:2]\n+        lines = text.split(\"\\n\")[:2]\n         for line in lines:\n             line = line.strip()\n             if line and len(line) < 80:\n-                title = re.sub(r'[^\\w\\s-]', '', line)\n+                title = re.sub(r\"[^\\w\\s-]\", \"\", line)\n                 if title:\n                     return title[:50]\n         return \"Concept Definition\"\n-    \n-    def _create_enhanced_segment(self, content: str, title: str, start_pos: int, end_pos: int,\n-                               importance_score: float, content_type: str) -> DocumentSegment:\n+\n+    def _create_enhanced_segment(\n+        self,\n+        content: str,\n+        title: str,\n+        start_pos: int,\n+        end_pos: int,\n+        importance_score: float,\n+        content_type: str,\n+    ) -> DocumentSegment:\n         \"\"\"Create enhanced document segment\"\"\"\n         # Generate unique ID\n-        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()).hexdigest()[:8]\n-        \n+        segment_id = hashlib.md5(\n+            f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()\n+        ).hexdigest()[:8]\n+\n         # Extract keywords\n         keywords = self._extract_enhanced_keywords(content, content_type)\n-        \n+\n         # Calculate enhanced relevance scores\n-        relevance_scores = self._calculate_enhanced_relevance_scores(content, content_type, importance_score)\n-        \n+        relevance_scores = self._calculate_enhanced_relevance_scores(\n+            content, content_type, importance_score\n+        )\n+\n         return DocumentSegment(\n             id=segment_id,\n             title=title,\n@@ -895,150 +1014,199 @@ class DocumentSegmenter:\n             char_end=end_pos,\n             char_count=len(content),\n             relevance_scores=relevance_scores,\n-            section_path=title\n+            section_path=title,\n         )\n-    \n+\n     def _extract_enhanced_keywords(self, content: str, content_type: str) -> List[str]:\n         \"\"\"Extract enhanced keywords based on content type\"\"\"\n-        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n-        \n+        words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", content.lower())\n+\n         # Adjust stopwords based on content type\n-        if content_type == 'algorithm':\n-            algorithm_stopwords = {'step', 'then', 'else', 'end', 'begin', 'start', 'stop'}\n+        if content_type == \"algorithm\":\n+            algorithm_stopwords = {\n+                \"step\",\n+                \"then\",\n+                \"else\",\n+                \"end\",\n+                \"begin\",\n+                \"start\",\n+                \"stop\",\n+            }\n             words = [w for w in words if w not in algorithm_stopwords]\n-        elif content_type == 'formula':\n-            formula_keywords = ['equation', 'formula', 'where', 'given', 'such', 'that']\n+        elif content_type == \"formula\":\n+            formula_keywords = [\"equation\", \"formula\", \"where\", \"given\", \"such\", \"that\"]\n             words.extend(formula_keywords)\n-        \n+\n         # General stopwords\n-        general_stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n-        \n+        general_stopwords = {\n+            \"the\",\n+            \"and\",\n+            \"for\",\n+            \"are\",\n+            \"but\",\n+            \"not\",\n+            \"you\",\n+            \"all\",\n+            \"can\",\n+            \"her\",\n+            \"was\",\n+            \"one\",\n+            \"our\",\n+            \"had\",\n+            \"but\",\n+            \"have\",\n+            \"this\",\n+            \"that\",\n+            \"with\",\n+            \"from\",\n+            \"they\",\n+            \"she\",\n+            \"been\",\n+            \"were\",\n+            \"said\",\n+            \"each\",\n+            \"which\",\n+            \"their\",\n+        }\n+\n         keywords = [w for w in set(words) if w not in general_stopwords and len(w) > 3]\n         return keywords[:25]  # Increase keyword count\n-    \n-    def _calculate_enhanced_relevance_scores(self, content: str, content_type: str, importance_score: float) -> Dict[str, float]:\n+\n+    def _calculate_enhanced_relevance_scores(\n+        self, content: str, content_type: str, importance_score: float\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate enhanced relevance scores\"\"\"\n         content_lower = content.lower()\n-        \n+\n         base_scores = {\n-            'concept_analysis': 0.5,\n-            'algorithm_extraction': 0.5,\n-            'code_planning': 0.5\n+            \"concept_analysis\": 0.5,\n+            \"algorithm_extraction\": 0.5,\n+            \"code_planning\": 0.5,\n         }\n-        \n+\n         # Adjust base scores based on content type and importance\n-        if content_type == 'algorithm':\n-            base_scores['algorithm_extraction'] = importance_score\n-            base_scores['code_planning'] = importance_score * 0.9\n-            base_scores['concept_analysis'] = importance_score * 0.7\n-        elif content_type == 'concept':\n-            base_scores['concept_analysis'] = importance_score\n-            base_scores['algorithm_extraction'] = importance_score * 0.8\n-            base_scores['code_planning'] = importance_score * 0.6\n-        elif content_type == 'formula':\n-            base_scores['algorithm_extraction'] = importance_score\n-            base_scores['concept_analysis'] = importance_score * 0.8\n-            base_scores['code_planning'] = importance_score * 0.9\n-        elif content_type == 'merged':\n+        if content_type == \"algorithm\":\n+            base_scores[\"algorithm_extraction\"] = importance_score\n+            base_scores[\"code_planning\"] = importance_score * 0.9\n+            base_scores[\"concept_analysis\"] = importance_score * 0.7\n+        elif content_type == \"concept\":\n+            base_scores[\"concept_analysis\"] = importance_score\n+            base_scores[\"algorithm_extraction\"] = importance_score * 0.8\n+            base_scores[\"code_planning\"] = importance_score * 0.6\n+        elif content_type == \"formula\":\n+            base_scores[\"algorithm_extraction\"] = importance_score\n+            base_scores[\"concept_analysis\"] = importance_score * 0.8\n+            base_scores[\"code_planning\"] = importance_score * 0.9\n+        elif content_type == \"merged\":\n             # Merged content is usually important\n             base_scores = {k: importance_score * 0.95 for k in base_scores}\n-        \n+\n         # Additional bonus based on content density\n-        algorithm_indicators = ['algorithm', 'method', 'procedure', 'step', 'process']\n-        concept_indicators = ['definition', 'concept', 'framework', 'approach']\n-        implementation_indicators = ['implementation', 'code', 'function', 'design']\n-        \n+        algorithm_indicators = [\"algorithm\", \"method\", \"procedure\", \"step\", \"process\"]\n+        concept_indicators = [\"definition\", \"concept\", \"framework\", \"approach\"]\n+        implementation_indicators = [\"implementation\", \"code\", \"function\", \"design\"]\n+\n         for query_type, indicators in [\n-            ('algorithm_extraction', algorithm_indicators),\n-            ('concept_analysis', concept_indicators),\n-            ('code_planning', implementation_indicators)\n+            (\"algorithm_extraction\", algorithm_indicators),\n+            (\"concept_analysis\", concept_indicators),\n+            (\"code_planning\", implementation_indicators),\n         ]:\n-            density_bonus = sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            density_bonus = (\n+                sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            )\n             base_scores[query_type] = min(1.0, base_scores[query_type] + density_bonus)\n-        \n+\n         return base_scores\n-    \n+\n     # Placeholder methods - can be further implemented later\n     def _identify_research_paper_sections(self, content: str) -> List[Dict]:\n         \"\"\"Identify research paper sections - simplified implementation\"\"\"\n         # Temporarily use improved semantic detection\n         return self._detect_academic_sections(content)\n-    \n+\n     def _enhance_section_with_context(self, section: Dict, content: str) -> Dict:\n         \"\"\"Add context to sections - simplified implementation\"\"\"\n         return section\n-    \n+\n     def _identify_concept_implementation_pairs(self, content: str) -> List[Dict]:\n         \"\"\"Identify concept-implementation pairs - simplified implementation\"\"\"\n         return []\n-    \n+\n     def _merge_concept_with_implementation(self, pair: Dict, content: str) -> Dict:\n         \"\"\"Merge concepts with implementation - simplified implementation\"\"\"\n         return pair\n-    \n+\n     def _detect_semantic_boundaries(self, content: str) -> List[Dict]:\n         \"\"\"Detect semantic boundaries - based on paragraphs and logical separators\"\"\"\n         boundaries = []\n-        \n+\n         # Split paragraphs by double line breaks\n-        paragraphs = content.split('\\n\\n')\n+        paragraphs = content.split(\"\\n\\n\")\n         current_pos = 0\n-        \n+\n         for i, para in enumerate(paragraphs):\n             if len(para.strip()) > 100:  # Valid paragraph\n                 # Analyze paragraph type\n                 content_type = self._classify_paragraph_type(para)\n-                importance_score = self._calculate_paragraph_importance(para, content_type)\n-                \n-                boundaries.append({\n-                    'position': current_pos + len(para),\n-                    'suggested_title': self._extract_paragraph_title(para, i+1),\n-                    'importance_score': importance_score,\n-                    'content_type': content_type\n-                })\n-            \n+                importance_score = self._calculate_paragraph_importance(\n+                    para, content_type\n+                )\n+\n+                boundaries.append(\n+                    {\n+                        \"position\": current_pos + len(para),\n+                        \"suggested_title\": self._extract_paragraph_title(para, i + 1),\n+                        \"importance_score\": importance_score,\n+                        \"content_type\": content_type,\n+                    }\n+                )\n+\n             current_pos += len(para) + 2  # +2 for \\n\\n\n-        \n+\n         return boundaries\n-    \n+\n     def _classify_paragraph_type(self, paragraph: str) -> str:\n         \"\"\"Classify paragraph type\"\"\"\n         para_lower = paragraph.lower()\n-        \n-        if 'algorithm' in para_lower or 'procedure' in para_lower:\n-            return 'algorithm'\n-        elif 'formula' in para_lower or '$$' in paragraph:\n-            return 'formula'\n-        elif any(word in para_lower for word in ['introduction', 'overview', 'abstract']):\n-            return 'introduction'\n-        elif any(word in para_lower for word in ['conclusion', 'summary', 'result']):\n-            return 'conclusion'\n+\n+        if \"algorithm\" in para_lower or \"procedure\" in para_lower:\n+            return \"algorithm\"\n+        elif \"formula\" in para_lower or \"$$\" in paragraph:\n+            return \"formula\"\n+        elif any(\n+            word in para_lower for word in [\"introduction\", \"overview\", \"abstract\"]\n+        ):\n+            return \"introduction\"\n+        elif any(word in para_lower for word in [\"conclusion\", \"summary\", \"result\"]):\n+            return \"conclusion\"\n         else:\n-            return 'general'\n-    \n-    def _calculate_paragraph_importance(self, paragraph: str, content_type: str) -> float:\n+            return \"general\"\n+\n+    def _calculate_paragraph_importance(\n+        self, paragraph: str, content_type: str\n+    ) -> float:\n         \"\"\"Calculate paragraph importance\"\"\"\n-        if content_type == 'algorithm':\n+        if content_type == \"algorithm\":\n             return 0.95\n-        elif content_type == 'formula':\n+        elif content_type == \"formula\":\n             return 0.9\n-        elif content_type == 'introduction':\n+        elif content_type == \"introduction\":\n             return 0.85\n-        elif content_type == 'conclusion':\n+        elif content_type == \"conclusion\":\n             return 0.8\n         else:\n             return 0.7\n-    \n+\n     def _extract_paragraph_title(self, paragraph: str, index: int) -> str:\n         \"\"\"Extract paragraph title\"\"\"\n-        lines = paragraph.split('\\n')\n+        lines = paragraph.split(\"\\n\")\n         for line in lines[:2]:\n-            if line.startswith('#'):\n-                return line.strip('# ')\n+            if line.startswith(\"#\"):\n+                return line.strip(\"# \")\n             elif len(line) < 80 and line.strip():\n                 return line.strip()\n         return f\"Section {index}\"\n-    \n+\n     def _calculate_optimal_chunk_size(self, content: str) -> int:\n         \"\"\"Calculate optimal chunk size\"\"\"\n         # Dynamically adjust based on content complexity\n@@ -1049,65 +1217,73 @@ class DocumentSegmenter:\n             return 3000\n         else:\n             return 2000\n-    \n+\n     def _create_content_aware_chunks(self, content: str, chunk_size: int) -> List[Dict]:\n         \"\"\"Create content-aware chunks - simplified implementation\"\"\"\n         chunks = []\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n-        \n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n+\n         current_chunk = []\n         current_size = 0\n         start_pos = 0\n-        \n+\n         for para in paragraphs:\n             para_size = len(para)\n-            \n+\n             if current_size + para_size > chunk_size and current_chunk:\n-                chunk_content = '\\n\\n'.join(current_chunk)\n-                chunks.append({\n-                    'content': chunk_content,\n-                    'title': f\"Section {len(chunks) + 1}\",\n-                    'start_pos': start_pos,\n-                    'end_pos': start_pos + len(chunk_content),\n-                    'importance_score': 0.7,\n-                    'content_type': 'general'\n-                })\n-                \n+                chunk_content = \"\\n\\n\".join(current_chunk)\n+                chunks.append(\n+                    {\n+                        \"content\": chunk_content,\n+                        \"title\": f\"Section {len(chunks) + 1}\",\n+                        \"start_pos\": start_pos,\n+                        \"end_pos\": start_pos + len(chunk_content),\n+                        \"importance_score\": 0.7,\n+                        \"content_type\": \"general\",\n+                    }\n+                )\n+\n                 current_chunk = [para]\n                 current_size = para_size\n                 start_pos += len(chunk_content) + 2\n             else:\n                 current_chunk.append(para)\n                 current_size += para_size\n-        \n+\n         # Add the last chunk\n         if current_chunk:\n-            chunk_content = '\\n\\n'.join(current_chunk)\n-            chunks.append({\n-                'content': chunk_content,\n-                'title': f\"Section {len(chunks) + 1}\",\n-                'start_pos': start_pos,\n-                'end_pos': start_pos + len(chunk_content),\n-                'importance_score': 0.7,\n-                'content_type': 'general'\n-            })\n-        \n+            chunk_content = \"\\n\\n\".join(current_chunk)\n+            chunks.append(\n+                {\n+                    \"content\": chunk_content,\n+                    \"title\": f\"Section {len(chunks) + 1}\",\n+                    \"start_pos\": start_pos,\n+                    \"end_pos\": start_pos + len(chunk_content),\n+                    \"importance_score\": 0.7,\n+                    \"content_type\": \"general\",\n+                }\n+            )\n+\n         return chunks\n-    \n-    def _create_segment(self, content: str, title: str, start_pos: int, end_pos: int) -> DocumentSegment:\n+\n+    def _create_segment(\n+        self, content: str, title: str, start_pos: int, end_pos: int\n+    ) -> DocumentSegment:\n         \"\"\"Create a DocumentSegment with metadata\"\"\"\n         # Generate unique ID\n-        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[:8]\n-        \n+        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[\n+            :8\n+        ]\n+\n         # Extract keywords from content\n         keywords = self._extract_keywords(content)\n-        \n+\n         # Determine content type\n         content_type = self._classify_content_type(title, content)\n-        \n+\n         # Calculate relevance scores for different query types\n         relevance_scores = self._calculate_relevance_scores(content, content_type)\n-        \n+\n         return DocumentSegment(\n             id=segment_id,\n             title=title,\n@@ -1118,142 +1294,232 @@ class DocumentSegmenter:\n             char_end=end_pos,\n             char_count=len(content),\n             relevance_scores=relevance_scores,\n-            section_path=title  # Simplified for now\n+            section_path=title,  # Simplified for now\n         )\n-    \n+\n     def _extract_keywords(self, content: str) -> List[str]:\n         \"\"\"Extract relevant keywords from content\"\"\"\n         # Simple keyword extraction - could be enhanced with NLP\n-        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n-        \n+        words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", content.lower())\n+\n         # Remove common words\n-        stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n-        \n+        stopwords = {\n+            \"the\",\n+            \"and\",\n+            \"for\",\n+            \"are\",\n+            \"but\",\n+            \"not\",\n+            \"you\",\n+            \"all\",\n+            \"can\",\n+            \"her\",\n+            \"was\",\n+            \"one\",\n+            \"our\",\n+            \"had\",\n+            \"but\",\n+            \"have\",\n+            \"this\",\n+            \"that\",\n+            \"with\",\n+            \"from\",\n+            \"they\",\n+            \"she\",\n+            \"been\",\n+            \"were\",\n+            \"said\",\n+            \"each\",\n+            \"which\",\n+            \"their\",\n+        }\n+\n         keywords = [w for w in set(words) if w not in stopwords and len(w) > 3]\n         return keywords[:20]  # Top 20 keywords\n-    \n+\n     def _classify_content_type(self, title: str, content: str) -> str:\n         \"\"\"Classify the type of content based on title and content\"\"\"\n         title_lower = title.lower()\n         content_lower = content.lower()\n-        \n-        if any(word in title_lower for word in ['introduction', 'abstract', 'overview']):\n-            return 'introduction'\n-        elif any(word in title_lower for word in ['method', 'approach', 'algorithm']):\n-            return 'methodology'\n-        elif any(word in title_lower for word in ['experiment', 'evaluation', 'result']):\n-            return 'experiment'\n-        elif any(word in title_lower for word in ['conclusion', 'discussion', 'summary']):\n-            return 'conclusion'\n-        elif any(word in title_lower for word in ['reference', 'bibliography']):\n-            return 'references'\n-        elif 'algorithm' in content_lower or 'procedure' in content_lower:\n-            return 'algorithm'\n+\n+        if any(\n+            word in title_lower for word in [\"introduction\", \"abstract\", \"overview\"]\n+        ):\n+            return \"introduction\"\n+        elif any(word in title_lower for word in [\"method\", \"approach\", \"algorithm\"]):\n+            return \"methodology\"\n+        elif any(\n+            word in title_lower for word in [\"experiment\", \"evaluation\", \"result\"]\n+        ):\n+            return \"experiment\"\n+        elif any(\n+            word in title_lower for word in [\"conclusion\", \"discussion\", \"summary\"]\n+        ):\n+            return \"conclusion\"\n+        elif any(word in title_lower for word in [\"reference\", \"bibliography\"]):\n+            return \"references\"\n+        elif \"algorithm\" in content_lower or \"procedure\" in content_lower:\n+            return \"algorithm\"\n         else:\n-            return 'general'\n-    \n-    def _calculate_relevance_scores(self, content: str, content_type: str) -> Dict[str, float]:\n+            return \"general\"\n+\n+    def _calculate_relevance_scores(\n+        self, content: str, content_type: str\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate relevance scores for different query types\"\"\"\n         content_lower = content.lower()\n-        \n+\n         scores = {\n-            'concept_analysis': 0.5,\n-            'algorithm_extraction': 0.5,\n-            'code_planning': 0.5\n+            \"concept_analysis\": 0.5,\n+            \"algorithm_extraction\": 0.5,\n+            \"code_planning\": 0.5,\n         }\n-        \n+\n         # Concept analysis relevance\n-        concept_indicators = ['introduction', 'overview', 'architecture', 'system', 'framework', 'concept', 'approach']\n-        concept_score = sum(1 for indicator in concept_indicators if indicator in content_lower) / len(concept_indicators)\n-        scores['concept_analysis'] = min(1.0, concept_score + (0.8 if content_type == 'introduction' else 0))\n-        \n+        concept_indicators = [\n+            \"introduction\",\n+            \"overview\",\n+            \"architecture\",\n+            \"system\",\n+            \"framework\",\n+            \"concept\",\n+            \"approach\",\n+        ]\n+        concept_score = sum(\n+            1 for indicator in concept_indicators if indicator in content_lower\n+        ) / len(concept_indicators)\n+        scores[\"concept_analysis\"] = min(\n+            1.0, concept_score + (0.8 if content_type == \"introduction\" else 0)\n+        )\n+\n         # Algorithm extraction relevance\n-        algorithm_indicators = ['algorithm', 'method', 'procedure', 'formula', 'equation', 'step', 'process']\n-        algorithm_score = sum(1 for indicator in algorithm_indicators if indicator in content_lower) / len(algorithm_indicators)\n-        scores['algorithm_extraction'] = min(1.0, algorithm_score + (0.9 if content_type == 'methodology' else 0))\n-        \n+        algorithm_indicators = [\n+            \"algorithm\",\n+            \"method\",\n+            \"procedure\",\n+            \"formula\",\n+            \"equation\",\n+            \"step\",\n+            \"process\",\n+        ]\n+        algorithm_score = sum(\n+            1 for indicator in algorithm_indicators if indicator in content_lower\n+        ) / len(algorithm_indicators)\n+        scores[\"algorithm_extraction\"] = min(\n+            1.0, algorithm_score + (0.9 if content_type == \"methodology\" else 0)\n+        )\n+\n         # Code planning relevance\n-        code_indicators = ['implementation', 'code', 'function', 'class', 'module', 'structure', 'design']\n-        code_score = sum(1 for indicator in code_indicators if indicator in content_lower) / len(code_indicators)\n-        scores['code_planning'] = min(1.0, code_score + (0.7 if content_type in ['methodology', 'algorithm'] else 0))\n-        \n+        code_indicators = [\n+            \"implementation\",\n+            \"code\",\n+            \"function\",\n+            \"class\",\n+            \"module\",\n+            \"structure\",\n+            \"design\",\n+        ]\n+        code_score = sum(\n+            1 for indicator in code_indicators if indicator in content_lower\n+        ) / len(code_indicators)\n+        scores[\"code_planning\"] = min(\n+            1.0,\n+            code_score + (0.7 if content_type in [\"methodology\", \"algorithm\"] else 0),\n+        )\n+\n         return scores\n \n+\n # Global variables\n DOCUMENT_INDEXES: Dict[str, DocumentIndex] = {}\n segmenter = DocumentSegmenter()\n \n+\n def get_segments_dir(paper_dir: str) -> str:\n     \"\"\"Get the segments directory path\"\"\"\n     return os.path.join(paper_dir, \"document_segments\")\n \n+\n def ensure_segments_dir_exists(segments_dir: str):\n     \"\"\"Ensure segments directory exists\"\"\"\n     os.makedirs(segments_dir, exist_ok=True)\n \n+\n @mcp.tool()\n-async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = False) -> str:\n+async def analyze_and_segment_document(\n+    paper_dir: str, force_refresh: bool = False\n+) -> str:\n     \"\"\"\n     Analyze document structure and create intelligent segments\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         force_refresh: Whether to force re-analysis even if segments exist\n-        \n+\n     Returns:\n         JSON string with segmentation results\n     \"\"\"\n     try:\n         # Find markdown file in paper directory\n-        md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+        md_files = [f for f in os.listdir(paper_dir) if f.endswith(\".md\")]\n         if not md_files:\n-            return json.dumps({\n-                \"status\": \"error\",\n-                \"message\": f\"No markdown file found in {paper_dir}\"\n-            }, ensure_ascii=False, indent=2)\n-        \n+            return json.dumps(\n+                {\n+                    \"status\": \"error\",\n+                    \"message\": f\"No markdown file found in {paper_dir}\",\n+                },\n+                ensure_ascii=False,\n+                indent=2,\n+            )\n+\n         md_file_path = os.path.join(paper_dir, md_files[0])\n         segments_dir = get_segments_dir(paper_dir)\n         index_file_path = os.path.join(segments_dir, \"document_index.json\")\n-        \n+\n         # Check if analysis already exists and is recent\n         if not force_refresh and os.path.exists(index_file_path):\n             try:\n-                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n                     existing_index = json.load(f)\n-                    \n+\n                     # Compatibility handling: ensure segments data structure is correct\n-                    if 'segments' in existing_index:\n+                    if \"segments\" in existing_index:\n                         segments_data = []\n-                        for seg_data in existing_index['segments']:\n+                        for seg_data in existing_index[\"segments\"]:\n                             # Ensure all required fields exist\n                             segment_dict = dict(seg_data)\n-                            \n-                            if 'content_type' not in segment_dict:\n-                                segment_dict['content_type'] = 'general'\n-                            if 'keywords' not in segment_dict:\n-                                segment_dict['keywords'] = []\n-                            if 'relevance_scores' not in segment_dict:\n-                                segment_dict['relevance_scores'] = {\n-                                    'concept_analysis': 0.5,\n-                                    'algorithm_extraction': 0.5,\n-                                    'code_planning': 0.5\n+\n+                            if \"content_type\" not in segment_dict:\n+                                segment_dict[\"content_type\"] = \"general\"\n+                            if \"keywords\" not in segment_dict:\n+                                segment_dict[\"keywords\"] = []\n+                            if \"relevance_scores\" not in segment_dict:\n+                                segment_dict[\"relevance_scores\"] = {\n+                                    \"concept_analysis\": 0.5,\n+                                    \"algorithm_extraction\": 0.5,\n+                                    \"code_planning\": 0.5,\n                                 }\n-                            if 'section_path' not in segment_dict:\n-                                segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n-                            \n+                            if \"section_path\" not in segment_dict:\n+                                segment_dict[\"section_path\"] = segment_dict.get(\n+                                    \"title\", \"Unknown\"\n+                                )\n+\n                             segments_data.append(DocumentSegment(**segment_dict))\n-                        \n-                        existing_index['segments'] = segments_data\n-                    \n+\n+                        existing_index[\"segments\"] = segments_data\n+\n                     DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**existing_index)\n-                return json.dumps({\n-                    \"status\": \"success\",\n-                    \"message\": \"Using existing document analysis\",\n-                    \"segments_dir\": segments_dir,\n-                    \"total_segments\": existing_index[\"total_segments\"]\n-                }, ensure_ascii=False, indent=2)\n-            \n+                return json.dumps(\n+                    {\n+                        \"status\": \"success\",\n+                        \"message\": \"Using existing document analysis\",\n+                        \"segments_dir\": segments_dir,\n+                        \"total_segments\": existing_index[\"total_segments\"],\n+                    },\n+                    ensure_ascii=False,\n+                    indent=2,\n+                )\n+\n             except Exception as e:\n                 logger.error(f\"Failed to load existing index: {e}\")\n                 logger.info(\"Will perform fresh analysis instead\")\n@@ -1262,19 +1528,19 @@ async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = Fal\n                     os.remove(index_file_path)\n                 except:\n                     pass\n-        \n+\n         # Read document content\n-        with open(md_file_path, 'r', encoding='utf-8') as f:\n+        with open(md_file_path, \"r\", encoding=\"utf-8\") as f:\n             content = f.read()\n-        \n+\n         # Analyze document\n         analyzer = DocumentAnalyzer()\n         doc_type, confidence = analyzer.analyze_document_type(content)\n         strategy = analyzer.detect_segmentation_strategy(content, doc_type)\n-        \n+\n         # Create segments\n         segments = segmenter.segment_document(content, strategy)\n-        \n+\n         # Create document index\n         document_index = DocumentIndex(\n             document_path=md_file_path,\n@@ -1283,46 +1549,56 @@ async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = Fal\n             total_segments=len(segments),\n             total_chars=len(content),\n             segments=segments,\n-            created_at=datetime.now().isoformat()\n+            created_at=datetime.now().isoformat(),\n         )\n-        \n+\n         # Save segments\n         ensure_segments_dir_exists(segments_dir)\n-        \n+\n         # Save document index\n-        with open(index_file_path, 'w', encoding='utf-8') as f:\n-            json.dump(asdict(document_index), f, ensure_ascii=False, indent=2, default=str)\n-        \n+        with open(index_file_path, \"w\", encoding=\"utf-8\") as f:\n+            json.dump(\n+                asdict(document_index), f, ensure_ascii=False, indent=2, default=str\n+            )\n+\n         # Save individual segment files for fallback\n         for segment in segments:\n             segment_file_path = os.path.join(segments_dir, f\"segment_{segment.id}.md\")\n-            with open(segment_file_path, 'w', encoding='utf-8') as f:\n+            with open(segment_file_path, \"w\", encoding=\"utf-8\") as f:\n                 f.write(f\"# {segment.title}\\n\\n\")\n                 f.write(f\"**Content Type:** {segment.content_type}\\n\")\n                 f.write(f\"**Keywords:** {', '.join(segment.keywords[:10])}\\n\\n\")\n                 f.write(segment.content)\n-        \n+\n         # Store in memory\n         DOCUMENT_INDEXES[paper_dir] = document_index\n-        \n-        logger.info(f\"Document segmentation completed: {len(segments)} segments created\")\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"message\": f\"Document analysis completed with {strategy} strategy\",\n-            \"document_type\": doc_type,\n-            \"segmentation_strategy\": strategy,\n-            \"segments_dir\": segments_dir,\n-            \"total_segments\": len(segments),\n-            \"total_chars\": len(content)\n-        }, ensure_ascii=False, indent=2)\n-        \n+\n+        logger.info(\n+            f\"Document segmentation completed: {len(segments)} segments created\"\n+        )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"message\": f\"Document analysis completed with {strategy} strategy\",\n+                \"document_type\": doc_type,\n+                \"segmentation_strategy\": strategy,\n+                \"segments_dir\": segments_dir,\n+                \"total_segments\": len(segments),\n+                \"total_chars\": len(content),\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in analyze_and_segment_document: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to analyze document: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\"status\": \"error\", \"message\": f\"Failed to analyze document: {str(e)}\"},\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n @mcp.tool()\n async def read_document_segments(\n@@ -1330,18 +1606,18 @@ async def read_document_segments(\n     query_type: str,\n     keywords: List[str] = None,\n     max_segments: int = 3,\n-    max_total_chars: int = None\n+    max_total_chars: int = None,\n ) -> str:\n     \"\"\"\n     Intelligently retrieve relevant document segments based on query type\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         query_type: Type of query - \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n         keywords: Optional list of keywords to search for\n         max_segments: Maximum number of segments to return\n         max_total_chars: Maximum total characters to return\n-        \n+\n     Returns:\n         JSON string with selected segments\n     \"\"\"\n@@ -1350,100 +1626,113 @@ async def read_document_segments(\n         if paper_dir not in DOCUMENT_INDEXES:\n             segments_dir = get_segments_dir(paper_dir)\n             index_file_path = os.path.join(segments_dir, \"document_index.json\")\n-            \n+\n             if os.path.exists(index_file_path):\n-                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n                     index_data = json.load(f)\n                     # Convert dict back to DocumentIndex with backward compatibility\n                     segments_data = []\n-                    for seg_data in index_data.get('segments', []):\n+                    for seg_data in index_data.get(\"segments\", []):\n                         # Ensure all required fields exist, provide default values\n                         segment_dict = dict(seg_data)\n-                        \n+\n                         # Compatibility handling: add missing fields\n-                        if 'content_type' not in segment_dict:\n-                            segment_dict['content_type'] = 'general'\n-                        if 'keywords' not in segment_dict:\n-                            segment_dict['keywords'] = []\n-                        if 'relevance_scores' not in segment_dict:\n-                            segment_dict['relevance_scores'] = {\n-                                'concept_analysis': 0.5,\n-                                'algorithm_extraction': 0.5,\n-                                'code_planning': 0.5\n+                        if \"content_type\" not in segment_dict:\n+                            segment_dict[\"content_type\"] = \"general\"\n+                        if \"keywords\" not in segment_dict:\n+                            segment_dict[\"keywords\"] = []\n+                        if \"relevance_scores\" not in segment_dict:\n+                            segment_dict[\"relevance_scores\"] = {\n+                                \"concept_analysis\": 0.5,\n+                                \"algorithm_extraction\": 0.5,\n+                                \"code_planning\": 0.5,\n                             }\n-                        if 'section_path' not in segment_dict:\n-                            segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n-                        \n+                        if \"section_path\" not in segment_dict:\n+                            segment_dict[\"section_path\"] = segment_dict.get(\n+                                \"title\", \"Unknown\"\n+                            )\n+\n                         segment = DocumentSegment(**segment_dict)\n                         segments_data.append(segment)\n-                    \n-                    index_data['segments'] = segments_data\n+\n+                    index_data[\"segments\"] = segments_data\n                     DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**index_data)\n             else:\n                 # Auto-analyze if not found\n                 await analyze_and_segment_document(paper_dir)\n-        \n+\n         document_index = DOCUMENT_INDEXES[paper_dir]\n-        \n+\n         # Dynamically calculate character limit\n         if max_total_chars is None:\n             max_total_chars = _calculate_adaptive_char_limit(document_index, query_type)\n-        \n+\n         # Score and rank segments with enhanced algorithm\n         scored_segments = []\n         for segment in document_index.segments:\n             # Base relevance score (already enhanced in new system)\n             relevance_score = segment.relevance_scores.get(query_type, 0.5)\n-            \n+\n             # Enhanced keyword matching with position weighting\n             if keywords:\n                 keyword_score = _calculate_enhanced_keyword_score(segment, keywords)\n                 relevance_score += keyword_score\n-            \n+\n             # Content completeness bonus\n             completeness_bonus = _calculate_completeness_bonus(segment, document_index)\n             relevance_score += completeness_bonus\n-            \n+\n             scored_segments.append((segment, relevance_score))\n-        \n+\n         # Sort by enhanced relevance score\n         scored_segments.sort(key=lambda x: x[1], reverse=True)\n-        \n+\n         # Intelligent segment selection with integrity preservation\n         selected_segments = _select_segments_with_integrity(\n             scored_segments, max_segments, max_total_chars, query_type\n         )\n-        \n+\n         total_chars = sum(seg[\"char_count\"] for seg in selected_segments)\n-        \n-        logger.info(f\"Selected {len(selected_segments)} segments for {query_type} query\")\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"query_type\": query_type,\n-            \"keywords\": keywords or [],\n-            \"total_segments_available\": len(document_index.segments),\n-            \"segments_selected\": len(selected_segments),\n-            \"total_chars\": total_chars,\n-            \"max_chars_used\": max_total_chars,\n-            \"segments\": selected_segments\n-        }, ensure_ascii=False, indent=2)\n-        \n+\n+        logger.info(\n+            f\"Selected {len(selected_segments)} segments for {query_type} query\"\n+        )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"query_type\": query_type,\n+                \"keywords\": keywords or [],\n+                \"total_segments_available\": len(document_index.segments),\n+                \"segments_selected\": len(selected_segments),\n+                \"total_chars\": total_chars,\n+                \"max_chars_used\": max_total_chars,\n+                \"segments\": selected_segments,\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in read_document_segments: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to read document segments: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\n+                \"status\": \"error\",\n+                \"message\": f\"Failed to read document segments: {str(e)}\",\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n @mcp.tool()\n async def get_document_overview(paper_dir: str) -> str:\n     \"\"\"\n     Get overview of document structure and available segments\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n-        \n+\n     Returns:\n         JSON string with document overview\n     \"\"\"\n@@ -1451,45 +1740,59 @@ async def get_document_overview(paper_dir: str) -> str:\n         # Ensure document is analyzed\n         if paper_dir not in DOCUMENT_INDEXES:\n             await analyze_and_segment_document(paper_dir)\n-        \n+\n         document_index = DOCUMENT_INDEXES[paper_dir]\n-        \n+\n         # Create overview\n         segment_summaries = []\n         for segment in document_index.segments:\n-            segment_summaries.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content_type\": segment.content_type,\n-                \"char_count\": segment.char_count,\n-                \"keywords\": segment.keywords[:5],  # Top 5 keywords\n-                \"relevance_scores\": segment.relevance_scores\n-            })\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"document_path\": document_index.document_path,\n-            \"document_type\": document_index.document_type,\n-            \"segmentation_strategy\": document_index.segmentation_strategy,\n-            \"total_segments\": document_index.total_segments,\n-            \"total_chars\": document_index.total_chars,\n-            \"created_at\": document_index.created_at,\n-            \"segments_overview\": segment_summaries\n-        }, ensure_ascii=False, indent=2)\n-        \n+            segment_summaries.append(\n+                {\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content_type\": segment.content_type,\n+                    \"char_count\": segment.char_count,\n+                    \"keywords\": segment.keywords[:5],  # Top 5 keywords\n+                    \"relevance_scores\": segment.relevance_scores,\n+                }\n+            )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"document_path\": document_index.document_path,\n+                \"document_type\": document_index.document_type,\n+                \"segmentation_strategy\": document_index.segmentation_strategy,\n+                \"total_segments\": document_index.total_segments,\n+                \"total_chars\": document_index.total_chars,\n+                \"created_at\": document_index.created_at,\n+                \"segments_overview\": segment_summaries,\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in get_document_overview: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to get document overview: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\n+                \"status\": \"error\",\n+                \"message\": f\"Failed to get document overview: {str(e)}\",\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n # =============== Enhanced retrieval system helper methods ===============\n \n-def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: str) -> int:\n+\n+def _calculate_adaptive_char_limit(\n+    document_index: DocumentIndex, query_type: str\n+) -> int:\n     \"\"\"Dynamically calculate character limit based on document complexity and query type\"\"\"\n     base_limit = 6000\n-    \n+\n     # Adjust based on document type\n     if document_index.document_type == \"research_paper\":\n         base_limit = 10000\n@@ -1497,116 +1800,140 @@ def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: st\n         base_limit = 12000\n     elif document_index.segmentation_strategy == \"algorithm_preserve_integrity\":\n         base_limit = 15000\n-    \n+\n     # Adjust based on query type\n     query_multipliers = {\n         \"algorithm_extraction\": 1.5,  # Algorithms need more context\n         \"concept_analysis\": 1.2,\n-        \"code_planning\": 1.3\n+        \"code_planning\": 1.3,\n     }\n-    \n+\n     multiplier = query_multipliers.get(query_type, 1.0)\n     return int(base_limit * multiplier)\n \n-def _calculate_enhanced_keyword_score(segment: DocumentSegment, keywords: List[str]) -> float:\n+\n+def _calculate_enhanced_keyword_score(\n+    segment: DocumentSegment, keywords: List[str]\n+) -> float:\n     \"\"\"Calculate enhanced keyword matching score\"\"\"\n     score = 0.0\n     content_lower = segment.content.lower()\n     title_lower = segment.title.lower()\n-    \n+\n     for keyword in keywords:\n         keyword_lower = keyword.lower()\n-        \n+\n         # Title matching has higher weight\n         if keyword_lower in title_lower:\n             score += 0.3\n-        \n+\n         # Content matching\n         content_matches = content_lower.count(keyword_lower)\n         if content_matches > 0:\n             # Consider term frequency and position\n             frequency_score = min(0.2, content_matches * 0.05)\n-            \n+\n             # Check if in important position (first 25% of content)\n-            early_content = content_lower[:len(content_lower)//4]\n+            early_content = content_lower[: len(content_lower) // 4]\n             if keyword_lower in early_content:\n                 frequency_score += 0.1\n-            \n+\n             score += frequency_score\n-    \n+\n     return min(0.6, score)  # Limit maximum bonus\n \n-def _calculate_completeness_bonus(segment: DocumentSegment, document_index: DocumentIndex) -> float:\n+\n+def _calculate_completeness_bonus(\n+    segment: DocumentSegment, document_index: DocumentIndex\n+) -> float:\n     \"\"\"Calculate content completeness bonus\"\"\"\n     bonus = 0.0\n-    \n+\n     # Completeness bonus for algorithm and formula content\n-    if segment.content_type in ['algorithm', 'formula', 'merged']:\n+    if segment.content_type in [\"algorithm\", \"formula\", \"merged\"]:\n         bonus += 0.2\n-    \n+\n     # Long paragraphs usually contain more complete information\n     if segment.char_count > 2000:\n         bonus += 0.1\n     elif segment.char_count > 4000:\n         bonus += 0.15\n-    \n+\n     # High importance paragraph bonus\n-    if segment.relevance_scores.get('algorithm_extraction', 0) > 0.8:\n+    if segment.relevance_scores.get(\"algorithm_extraction\", 0) > 0.8:\n         bonus += 0.1\n-    \n+\n     return min(0.3, bonus)\n \n-def _select_segments_with_integrity(scored_segments: List[Tuple], max_segments: int, \n-                                  max_total_chars: int, query_type: str) -> List[Dict]:\n+\n+def _select_segments_with_integrity(\n+    scored_segments: List[Tuple],\n+    max_segments: int,\n+    max_total_chars: int,\n+    query_type: str,\n+) -> List[Dict]:\n     \"\"\"Intelligently select segments while maintaining content integrity\"\"\"\n     selected_segments = []\n     total_chars = 0\n-    \n+\n     # First select the highest scoring segments\n     for segment, score in scored_segments:\n         if len(selected_segments) >= max_segments:\n             break\n-            \n+\n         if total_chars + segment.char_count <= max_total_chars:\n-            selected_segments.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content\": segment.content,\n-                \"content_type\": segment.content_type,\n-                \"relevance_score\": score,\n-                \"char_count\": segment.char_count\n-            })\n+            selected_segments.append(\n+                {\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content\": segment.content,\n+                    \"content_type\": segment.content_type,\n+                    \"relevance_score\": score,\n+                    \"char_count\": segment.char_count,\n+                }\n+            )\n             total_chars += segment.char_count\n         elif len(selected_segments) == 0:\n             # If the first segment exceeds the limit, truncate but preserve it\n-            truncated_content = segment.content[:max_total_chars - 200] + \"\\n\\n[Content truncated for length...]\"\n-            selected_segments.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content\": truncated_content,\n-                \"content_type\": segment.content_type,\n-                \"relevance_score\": score,\n-                \"char_count\": len(truncated_content)\n-            })\n-            break\n-    \n-    # If there's remaining space, try to add relevant small segments\n-    remaining_chars = max_total_chars - total_chars\n-    if remaining_chars > 500 and len(selected_segments) < max_segments:\n-        for segment, score in scored_segments[len(selected_segments):]:\n-            if segment.char_count <= remaining_chars and len(selected_segments) < max_segments:\n-                selected_segments.append({\n+            truncated_content = (\n+                segment.content[: max_total_chars - 200]\n+                + \"\\n\\n[Content truncated for length...]\"\n+            )\n+            selected_segments.append(\n+                {\n                     \"id\": segment.id,\n                     \"title\": segment.title,\n-                    \"content\": segment.content,\n+                    \"content\": truncated_content,\n                     \"content_type\": segment.content_type,\n                     \"relevance_score\": score,\n-                    \"char_count\": segment.char_count\n-                })\n+                    \"char_count\": len(truncated_content),\n+                }\n+            )\n+            break\n+\n+    # If there's remaining space, try to add relevant small segments\n+    remaining_chars = max_total_chars - total_chars\n+    if remaining_chars > 500 and len(selected_segments) < max_segments:\n+        for segment, score in scored_segments[len(selected_segments) :]:\n+            if (\n+                segment.char_count <= remaining_chars\n+                and len(selected_segments) < max_segments\n+            ):\n+                selected_segments.append(\n+                    {\n+                        \"id\": segment.id,\n+                        \"title\": segment.title,\n+                        \"content\": segment.content,\n+                        \"content_type\": segment.content_type,\n+                        \"relevance_score\": score,\n+                        \"char_count\": segment.char_count,\n+                    }\n+                )\n                 remaining_chars -= segment.char_count\n-    \n+\n     return selected_segments\n \n+\n if __name__ == \"__main__\":\n     # Run the MCP server\n-    mcp.run()\n\\ No newline at end of file\n+    mcp.run()\ndiff --git a/utils/llm_utils.py b/utils/llm_utils.py\nindex 05d3d28..8d4a0cd 100644\n--- a/utils/llm_utils.py\n+++ b/utils/llm_utils.py\n@@ -56,10 +56,10 @@ def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\") -> Type\n def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n     \"\"\"\n     Get default models configuration from config file.\n-    \n+\n     Args:\n         config_path: Path to the main configuration file\n-        \n+\n     Returns:\n         dict: Default models configuration\n     \"\"\"\n@@ -67,14 +67,16 @@ def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n         if os.path.exists(config_path):\n             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-            \n+\n             # Extract model configurations\n             openai_config = config.get(\"openai\", {})\n             anthropic_config = config.get(\"anthropic\", {})\n-            \n+\n             return {\n-                \"anthropic\": anthropic_config.get(\"default_model\", \"claude-sonnet-4-20250514\"),\n-                \"openai\": openai_config.get(\"default_model\", \"o3-mini\")\n+                \"anthropic\": anthropic_config.get(\n+                    \"default_model\", \"claude-sonnet-4-20250514\"\n+                ),\n+                \"openai\": openai_config.get(\"default_model\", \"o3-mini\"),\n             }\n         else:\n             print(f\"\ud83e\udd16 Config file {config_path} not found, using default models\")\n@@ -86,13 +88,15 @@ def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n         return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n \n \n-def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\") -> Dict[str, Any]:\n+def get_document_segmentation_config(\n+    config_path: str = \"mcp_agent.config.yaml\",\n+) -> Dict[str, Any]:\n     \"\"\"\n     Get document segmentation configuration from config file.\n-    \n+\n     Args:\n         config_path: Path to the main configuration file\n-        \n+\n     Returns:\n         Dict containing segmentation configuration with default values\n     \"\"\"\n@@ -100,71 +104,83 @@ def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\")\n         if os.path.exists(config_path):\n             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-            \n+\n             # Get document segmentation config with defaults\n             seg_config = config.get(\"document_segmentation\", {})\n             return {\n                 \"enabled\": seg_config.get(\"enabled\", True),\n-                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000)\n+                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000),\n             }\n         else:\n-            print(f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\")\n+            print(\n+                f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\"\n+            )\n             return {\"enabled\": True, \"size_threshold_chars\": 50000}\n-            \n+\n     except Exception as e:\n         print(f\"\ud83d\udcc4 Error reading segmentation config from {config_path}: {e}\")\n         print(\"\ud83d\udcc4 Using default segmentation settings\")\n         return {\"enabled\": True, \"size_threshold_chars\": 50000}\n \n \n-def should_use_document_segmentation(document_content: str, config_path: str = \"mcp_agent.config.yaml\") -> Tuple[bool, str]:\n+def should_use_document_segmentation(\n+    document_content: str, config_path: str = \"mcp_agent.config.yaml\"\n+) -> Tuple[bool, str]:\n     \"\"\"\n     Determine whether to use document segmentation based on configuration and document size.\n-    \n+\n     Args:\n         document_content: The content of the document to analyze\n         config_path: Path to the configuration file\n-        \n+\n     Returns:\n         Tuple of (should_segment, reason) where:\n         - should_segment: Boolean indicating whether to use segmentation\n         - reason: String explaining the decision\n     \"\"\"\n     seg_config = get_document_segmentation_config(config_path)\n-    \n+\n     if not seg_config[\"enabled\"]:\n         return False, \"Document segmentation disabled in configuration\"\n-    \n+\n     doc_size = len(document_content)\n     threshold = seg_config[\"size_threshold_chars\"]\n-    \n+\n     if doc_size > threshold:\n-        return True, f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\"\n+        return (\n+            True,\n+            f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\",\n+        )\n     else:\n-        return False, f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\"\n+        return (\n+            False,\n+            f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\",\n+        )\n \n \n-def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list = None) -> Dict[str, list]:\n+def get_adaptive_agent_config(\n+    use_segmentation: bool, search_server_names: list = None\n+) -> Dict[str, list]:\n     \"\"\"\n     Get adaptive agent configuration based on whether to use document segmentation.\n-    \n+\n     Args:\n         use_segmentation: Whether to include document-segmentation server\n         search_server_names: Base search server names (from get_search_server_names)\n-        \n+\n     Returns:\n         Dict containing server configurations for different agents\n     \"\"\"\n     if search_server_names is None:\n         search_server_names = []\n-    \n+\n     # Base configuration\n     config = {\n         \"concept_analysis\": [],\n         \"algorithm_analysis\": search_server_names.copy(),\n-        \"code_planner\": search_server_names.copy()\n+        \"code_planner\": search_server_names.copy(),\n     }\n-    \n+\n     # Add document-segmentation server if needed\n     if use_segmentation:\n         config[\"concept_analysis\"] = [\"document-segmentation\"]\n@@ -172,39 +188,39 @@ def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list\n             config[\"algorithm_analysis\"].append(\"document-segmentation\")\n         if \"document-segmentation\" not in config[\"code_planner\"]:\n             config[\"code_planner\"].append(\"document-segmentation\")\n-    \n+\n     return config\n \n \n def get_adaptive_prompts(use_segmentation: bool) -> Dict[str, str]:\n     \"\"\"\n     Get appropriate prompt versions based on segmentation usage.\n-    \n+\n     Args:\n         use_segmentation: Whether to use segmented reading prompts\n-        \n+\n     Returns:\n         Dict containing prompt configurations\n     \"\"\"\n     # Import here to avoid circular imports\n     from prompts.code_prompts import (\n         PAPER_CONCEPT_ANALYSIS_PROMPT,\n-        PAPER_ALGORITHM_ANALYSIS_PROMPT, \n+        PAPER_ALGORITHM_ANALYSIS_PROMPT,\n         CODE_PLANNING_PROMPT,\n         PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n         PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n-        CODE_PLANNING_PROMPT_TRADITIONAL\n+        CODE_PLANNING_PROMPT_TRADITIONAL,\n     )\n-    \n+\n     if use_segmentation:\n         return {\n             \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT,\n             \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT,\n-            \"code_planning\": CODE_PLANNING_PROMPT\n+            \"code_planning\": CODE_PLANNING_PROMPT,\n         }\n     else:\n         return {\n             \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n             \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n-            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL\n-        }\n\\ No newline at end of file\n+            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL,\n+        }\ndiff --git a/workflows/agents/document_segmentation_agent.py b/workflows/agents/document_segmentation_agent.py\nindex a9e4164..6325a7e 100644\n--- a/workflows/agents/document_segmentation_agent.py\n+++ b/workflows/agents/document_segmentation_agent.py\n@@ -5,7 +5,6 @@ A lightweight agent that coordinates with the document segmentation MCP server\n to analyze document structure and prepare segments for other agents.\n \"\"\"\n \n-import asyncio\n import os\n import logging\n from typing import Dict, Any, Optional\n@@ -17,14 +16,14 @@ from utils.llm_utils import get_preferred_llm_class\n class DocumentSegmentationAgent:\n     \"\"\"\n     Intelligent document segmentation agent with semantic analysis capabilities.\n-    \n+\n     This enhanced agent provides:\n     1. **Semantic Document Classification**: Content-based document type identification\n     2. **Adaptive Segmentation Strategy**: Algorithm integrity and semantic coherence preservation\n     3. **Planning Agent Optimization**: Segment preparation specifically optimized for downstream agents\n     4. **Quality Intelligence Validation**: Advanced metrics for completeness and technical accuracy\n     5. **Algorithm Completeness Protection**: Ensures critical algorithms and formulas remain intact\n-    \n+\n     Key improvements over traditional segmentation:\n     - Semantic content analysis vs mechanical structure splitting\n     - Dynamic character limits based on content complexity\n@@ -32,26 +31,26 @@ class DocumentSegmentationAgent:\n     - Algorithm and formula integrity preservation\n     - Content type-aware segmentation strategies\n     \"\"\"\n-    \n+\n     def __init__(self, logger: Optional[logging.Logger] = None):\n         self.logger = logger or self._create_default_logger()\n         self.mcp_agent = None\n-    \n+\n     def _create_default_logger(self) -> logging.Logger:\n         \"\"\"Create default logger if none provided\"\"\"\n         logger = logging.getLogger(f\"{__name__}.DocumentSegmentationAgent\")\n         logger.setLevel(logging.INFO)\n         return logger\n-    \n+\n     async def __aenter__(self):\n         \"\"\"Async context manager entry\"\"\"\n         await self.initialize()\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         \"\"\"Async context manager exit\"\"\"\n         await self.cleanup()\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize the MCP agent connection\"\"\"\n         try:\n@@ -74,21 +73,21 @@ Your enhanced capabilities include:\n - Provide actionable quality assessments\n \n Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.\"\"\",\n-                server_names=[\"document-segmentation\", \"filesystem\"]\n+                server_names=[\"document-segmentation\", \"filesystem\"],\n             )\n-            \n+\n             # Initialize the agent context\n             await self.mcp_agent.__aenter__()\n-            \n+\n             # Attach LLM\n             self.llm = await self.mcp_agent.attach_llm(get_preferred_llm_class())\n-            \n+\n             self.logger.info(\"DocumentSegmentationAgent initialized successfully\")\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Failed to initialize DocumentSegmentationAgent: {e}\")\n             raise\n-    \n+\n     async def cleanup(self):\n         \"\"\"Cleanup resources\"\"\"\n         if self.mcp_agent:\n@@ -96,36 +95,34 @@ Use the enhanced document-segmentation tools to deliver superior segmentation re\n                 await self.mcp_agent.__aexit__(None, None, None)\n             except Exception as e:\n                 self.logger.warning(f\"Error during cleanup: {e}\")\n-    \n+\n     async def analyze_and_prepare_document(\n-        self, \n-        paper_dir: str, \n-        force_refresh: bool = False\n+        self, paper_dir: str, force_refresh: bool = False\n     ) -> Dict[str, Any]:\n         \"\"\"\n         Perform intelligent semantic analysis and create optimized document segments.\n-        \n+\n         This method coordinates with the enhanced document segmentation server to:\n         - Classify document type using semantic content analysis\n         - Select optimal segmentation strategy (semantic_research_focused, algorithm_preserve_integrity, etc.)\n         - Preserve algorithm and formula integrity\n         - Optimize segments for downstream planning agents\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n             force_refresh: Whether to force re-analysis with latest algorithms\n-            \n+\n         Returns:\n             Dict containing enhanced analysis results and intelligent segment information\n         \"\"\"\n         try:\n             self.logger.info(f\"Starting document analysis for: {paper_dir}\")\n-            \n+\n             # Check if markdown file exists\n-            md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+            md_files = [f for f in os.listdir(paper_dir) if f.endswith(\".md\")]\n             if not md_files:\n                 raise ValueError(f\"No markdown file found in {paper_dir}\")\n-            \n+\n             # Use the enhanced document segmentation tool\n             message = f\"\"\"Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}\n \n@@ -147,35 +144,35 @@ After segmentation, get a document overview and provide:\n - Algorithm/formula integrity verification\n - Recommendations for planning agent optimization\n - Technical content completeness evaluation\"\"\"\n-            \n+\n             result = await self.llm.generate_str(message=message)\n-            \n+\n             self.logger.info(\"Document analysis completed successfully\")\n-            \n+\n             # Parse the result and return structured information\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"analysis_result\": result,\n-                \"segments_available\": True\n+                \"segments_available\": True,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error in document analysis: {e}\")\n             return {\n                 \"status\": \"error\",\n                 \"paper_dir\": paper_dir,\n                 \"error_message\": str(e),\n-                \"segments_available\": False\n+                \"segments_available\": False,\n             }\n-    \n+\n     async def get_document_overview(self, paper_dir: str) -> Dict[str, Any]:\n         \"\"\"\n         Get overview of document structure and segments.\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n-            \n+\n         Returns:\n             Dict containing document overview information\n         \"\"\"\n@@ -194,40 +191,36 @@ Provide a comprehensive analysis focusing on:\n 2. Algorithm and formula integrity preservation\n 3. Segment relevance for downstream planning agents\n 4. Technical content distribution and completeness\"\"\"\n-            \n+\n             result = await self.llm.generate_str(message=message)\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n-                \"overview_result\": result\n+                \"overview_result\": result,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting document overview: {e}\")\n-            return {\n-                \"status\": \"error\",\n-                \"paper_dir\": paper_dir,\n-                \"error_message\": str(e)\n-            }\n-    \n+            return {\"status\": \"error\", \"paper_dir\": paper_dir, \"error_message\": str(e)}\n+\n     async def validate_segmentation_quality(self, paper_dir: str) -> Dict[str, Any]:\n         \"\"\"\n         Validate the quality of document segmentation.\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n-            \n+\n         Returns:\n             Dict containing validation results\n         \"\"\"\n         try:\n             # Get overview first\n             overview_result = await self.get_document_overview(paper_dir)\n-            \n+\n             if overview_result[\"status\"] != \"success\":\n                 return overview_result\n-            \n+\n             # Analyze enhanced segmentation quality\n             message = f\"\"\"Based on the intelligent document overview for {paper_dir}, please evaluate the enhanced segmentation quality using advanced criteria.\n \n@@ -244,38 +237,32 @@ Provide a comprehensive analysis focusing on:\n - Algorithm/formula integrity enhancements  \n - Planning agent optimization opportunities\n - Content distribution balance adjustments\"\"\"\n-            \n+\n             validation_result = await self.llm.generate_str(message=message)\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"validation_result\": validation_result,\n-                \"overview_data\": overview_result\n+                \"overview_data\": overview_result,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error validating segmentation quality: {e}\")\n-            return {\n-                \"status\": \"error\",\n-                \"paper_dir\": paper_dir,\n-                \"error_message\": str(e)\n-            }\n+            return {\"status\": \"error\", \"paper_dir\": paper_dir, \"error_message\": str(e)}\n \n \n async def run_document_segmentation_analysis(\n-    paper_dir: str, \n-    logger: Optional[logging.Logger] = None,\n-    force_refresh: bool = False\n+    paper_dir: str, logger: Optional[logging.Logger] = None, force_refresh: bool = False\n ) -> Dict[str, Any]:\n     \"\"\"\n     Convenience function to run document segmentation analysis.\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         logger: Optional logger instance\n         force_refresh: Whether to force re-analysis\n-        \n+\n     Returns:\n         Dict containing analysis results\n     \"\"\"\n@@ -284,78 +271,83 @@ async def run_document_segmentation_analysis(\n         analysis_result = await agent.analyze_and_prepare_document(\n             paper_dir, force_refresh=force_refresh\n         )\n-        \n+\n         if analysis_result[\"status\"] == \"success\":\n             # Validate segmentation quality\n             validation_result = await agent.validate_segmentation_quality(paper_dir)\n             analysis_result[\"validation\"] = validation_result\n-        \n+\n         return analysis_result\n \n \n # Utility function for integration with existing workflow\n async def prepare_document_segments(\n-    paper_dir: str,\n-    logger: Optional[logging.Logger] = None\n+    paper_dir: str, logger: Optional[logging.Logger] = None\n ) -> Dict[str, Any]:\n     \"\"\"\n     Prepare intelligent document segments optimized for planning agents.\n-    \n+\n     This enhanced function leverages semantic analysis to create segments that:\n     - Preserve algorithm and formula integrity\n     - Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent\n     - Use adaptive character limits based on content complexity\n     - Maintain technical content completeness\n-    \n-    Called from the orchestration engine (Phase 3.5) to prepare documents \n+\n+    Called from the orchestration engine (Phase 3.5) to prepare documents\n     before the planning phase with superior segmentation quality.\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory containing markdown file\n         logger: Optional logger instance for tracking\n-        \n+\n     Returns:\n         Dict containing enhanced preparation results and intelligent metadata\n     \"\"\"\n     try:\n         logger = logger or logging.getLogger(__name__)\n         logger.info(f\"Preparing document segments for: {paper_dir}\")\n-        \n+\n         # Run analysis\n         result = await run_document_segmentation_analysis(\n             paper_dir=paper_dir,\n             logger=logger,\n-            force_refresh=False  # Use cached analysis if available\n+            force_refresh=False,  # Use cached analysis if available\n         )\n-        \n+\n         if result[\"status\"] == \"success\":\n             logger.info(\"Document segments prepared successfully\")\n-            \n+\n             # Create metadata for downstream agents\n             segments_dir = os.path.join(paper_dir, \"document_segments\")\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"segments_dir\": segments_dir,\n                 \"segments_ready\": True,\n                 \"analysis_summary\": result.get(\"analysis_result\", \"\"),\n-                \"validation_summary\": result.get(\"validation\", {}).get(\"validation_result\", \"\")\n+                \"validation_summary\": result.get(\"validation\", {}).get(\n+                    \"validation_result\", \"\"\n+                ),\n             }\n         else:\n-            logger.error(f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\")\n+            logger.error(\n+                f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\"\n+            )\n             return {\n                 \"status\": \"error\",\n                 \"paper_dir\": paper_dir,\n                 \"segments_ready\": False,\n-                \"error_message\": result.get(\"error_message\", \"Document segmentation failed\")\n+                \"error_message\": result.get(\n+                    \"error_message\", \"Document segmentation failed\"\n+                ),\n             }\n-            \n+\n     except Exception as e:\n         logger.error(f\"Error preparing document segments: {e}\")\n         return {\n             \"status\": \"error\",\n             \"paper_dir\": paper_dir,\n             \"segments_ready\": False,\n-            \"error_message\": str(e)\n-        }\n\\ No newline at end of file\n+            \"error_message\": str(e),\n+        }\n"
    },
    {
        "id": "279",
        "sha_fail": "238a409674f147334f43788013fdfa766fd8035c",
        "diff": "diff --git a/workflows/agents/memory_agent_concise.py b/workflows/agents/memory_agent_concise.py\nindex 1be5423..841dc69 100644\n--- a/workflows/agents/memory_agent_concise.py\n+++ b/workflows/agents/memory_agent_concise.py\n@@ -695,7 +695,7 @@ class ConciseMemoryAgent:\n         summary = f\"\"\"# Code Implementation Summary\n **All Previously Implemented Files:**\n {implemented_files_list}\n-**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n+**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n **File Implemented**: {file_path}\n **Total Files Implemented**: {files_implemented}\n **Summary failed to generate.**\ndiff --git a/workflows/agents/memory_agent_concise_index.py b/workflows/agents/memory_agent_concise_index.py\nindex ad3c53f..b67fef9 100644\n--- a/workflows/agents/memory_agent_concise_index.py\n+++ b/workflows/agents/memory_agent_concise_index.py\n@@ -684,7 +684,7 @@ class ConciseMemoryAgent:\n         summary = f\"\"\"# Code Implementation Summary\n **All Previously Implemented Files:**\n {implemented_files_list}\n-**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n+**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n **File Implemented**: {file_path}\n **Total Files Implemented**: {files_implemented}\n **Summary failed to generate.**\n"
    },
    {
        "id": "280",
        "sha_fail": "219bc58c7f1bfe425ddc1d628ff5cda9639afc1e",
        "diff": "diff --git a/tools/pdf_downloader.py b/tools/pdf_downloader.py\nindex adb602b..301ca5d 100644\n--- a/tools/pdf_downloader.py\n+++ b/tools/pdf_downloader.py\n@@ -109,10 +109,10 @@ async def perform_document_conversion(\n         try:\n             with open(file_path, \"rb\") as f:\n                 header = f.read(8)\n-                is_pdf_file = header.startswith(b'%PDF')\n+                is_pdf_file = header.startswith(b\"%PDF\")\n         except Exception:\n             is_pdf_file = file_path.lower().endswith(\".pdf\")\n-    \n+\n     if is_pdf_file and PYPDF2_AVAILABLE:\n         try:\n             simple_converter = SimplePdfConverter()\n@@ -585,7 +585,7 @@ class DoclingConverter:\n                         ext = \"png\"\n \n                     # \u751f\u6210\u6587\u4ef6\u540d\n-                    filename = f\"image_{idx+1}.{ext}\"\n+                    filename = f\"image_{idx + 1}.{ext}\"\n                     filepath = os.path.join(images_dir, filename)\n \n                     # \u4fdd\u5b58\u56fe\u7247\u6570\u636e\n@@ -600,7 +600,7 @@ class DoclingConverter:\n                         image_map[img_id] = rel_path\n \n                 except Exception as img_error:\n-                    print(f\"Warning: Failed to extract image {idx+1}: {img_error}\")\n+                    print(f\"Warning: Failed to extract image {idx + 1}: {img_error}\")\n                     continue\n \n         except Exception as e:\ndiff --git a/utils/file_processor.py b/utils/file_processor.py\nindex b25346f..9963286 100644\n--- a/utils/file_processor.py\n+++ b/utils/file_processor.py\n@@ -190,8 +190,10 @@ class FileProcessor:\n             # Check if file is actually a PDF by reading the first few bytes\n             with open(file_path, \"rb\") as f:\n                 header = f.read(8)\n-                if header.startswith(b'%PDF'):\n-                    raise IOError(f\"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\")\n+                if header.startswith(b\"%PDF\"):\n+                    raise IOError(\n+                        f\"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\"\n+                    )\n \n             # Read file content\n             # Note: Using async with would be better for large files\n@@ -202,7 +204,9 @@ class FileProcessor:\n             return content\n \n         except UnicodeDecodeError as e:\n-            raise IOError(f\"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}\")\n+            raise IOError(\n+                f\"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}\"\n+            )\n         except Exception as e:\n             raise IOError(f\"Error reading file {file_path}: {str(e)}\")\n \ndiff --git a/workflows/agent_orchestration_engine.py b/workflows/agent_orchestration_engine.py\nindex 027775d..3f2d602 100644\n--- a/workflows/agent_orchestration_engine.py\n+++ b/workflows/agent_orchestration_engine.py\n@@ -664,9 +664,11 @@ async def orchestrate_document_preprocessing_agent(\n             # Check if file is actually a PDF by reading the first few bytes\n             with open(md_path, \"rb\") as f:\n                 header = f.read(8)\n-                if header.startswith(b'%PDF'):\n-                    raise IOError(f\"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\")\n-            \n+                if header.startswith(b\"%PDF\"):\n+                    raise IOError(\n+                        f\"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\"\n+                    )\n+\n             with open(md_path, \"r\", encoding=\"utf-8\") as f:\n                 document_content = f.read()\n         except Exception as e:\n"
    },
    {
        "id": "281",
        "sha_fail": "9ecbaeeb4d8db618ccdb6cb81f5edf4ba2c8b8de",
        "diff": "diff --git a/github/GithubObject.py b/github/GithubObject.py\nindex 697f1601..031e93ce 100644\n--- a/github/GithubObject.py\n+++ b/github/GithubObject.py\n@@ -149,13 +149,11 @@ camel_to_snake_case_regexp = re.compile(r\"(?<!^)(?=[A-Z])\")\n \n \n @overload\n-def as_rest_api_attributes(graphql_attributes: dict[str, Any]) -> dict[str, Any]:\n-    ...\n+def as_rest_api_attributes(graphql_attributes: dict[str, Any]) -> dict[str, Any]: ...\n \n \n @overload\n-def as_rest_api_attributes(graphql_attributes: None) -> None:\n-    ...\n+def as_rest_api_attributes(graphql_attributes: None) -> None: ...\n \n \n def as_rest_api_attributes(graphql_attributes: dict[str, Any] | None) -> dict[str, Any] | None:\ndiff --git a/github/GithubRetry.py b/github/GithubRetry.py\nindex fd7630d4..517b4cbd 100644\n--- a/github/GithubRetry.py\n+++ b/github/GithubRetry.py\n@@ -110,7 +110,7 @@ class GithubRetry(Retry):\n                     # Sleeping 'Retry-After' seconds is implemented in urllib3.Retry.sleep() and called by urllib3\n                     self.__log(\n                         logging.INFO,\n-                        f'Retrying after {response.headers.get(\"Retry-After\")} seconds',\n+                        f\"Retrying after {response.headers.get('Retry-After')} seconds\",\n                     )\n                 else:\n                     content = response.reason\ndiff --git a/scripts/openapi.py b/scripts/openapi.py\nindex af489426..90ef5c4f 100644\n--- a/scripts/openapi.py\n+++ b/scripts/openapi.py\n@@ -448,8 +448,7 @@ class IndexPythonClassesVisitor(CstVisitorBase):\n                         known_verb = f'\"{self._method_verbs[full_method_name]}\"'\n                         if known_verb != verb:\n                             print(\n-                                f\"Method {full_method_name} is known to call {known_verb}, \"\n-                                f\"but doc-string says {verb}\"\n+                                f\"Method {full_method_name} is known to call {known_verb}, but doc-string says {verb}\"\n                             )\n                     else:\n                         # detect method from code\ndiff --git a/tests/CheckRun.py b/tests/CheckRun.py\nindex 53a3d4a7..6a43323f 100644\n--- a/tests/CheckRun.py\n+++ b/tests/CheckRun.py\n@@ -203,15 +203,19 @@ class CheckRun(Framework.TestCase):\n         self.assertEqual(check_run.name, \"completed_check_run\")\n         self.assertEqual(check_run.head_sha, \"0283d46537193f1fed7d46859f15c5304b9836f9\")\n         self.assertEqual(check_run.status, \"completed\")\n-        self.assertEqual(\n-            check_run.started_at,\n-            datetime(2020, 10, 20, 10, 30, 29, tzinfo=timezone.utc),\n-        ),\n+        (\n+            self.assertEqual(\n+                check_run.started_at,\n+                datetime(2020, 10, 20, 10, 30, 29, tzinfo=timezone.utc),\n+            ),\n+        )\n         self.assertEqual(check_run.conclusion, \"success\")\n-        self.assertEqual(\n-            check_run.completed_at,\n-            datetime(2020, 10, 20, 11, 30, 50, tzinfo=timezone.utc),\n-        ),\n+        (\n+            self.assertEqual(\n+                check_run.completed_at,\n+                datetime(2020, 10, 20, 11, 30, 50, tzinfo=timezone.utc),\n+            ),\n+        )\n         self.assertEqual(check_run.output.annotations_count, 2)\n \n     def testUpdateCheckRunSuccess(self):\n"
    },
    {
        "id": "284",
        "sha_fail": "e5cf9e0da88f8a9835011d89c15146d25806f619",
        "diff": "diff --git a/sphinx/ext/mathjax.py b/sphinx/ext/mathjax.py\nindex cf0667013..193eb22c9 100644\n--- a/sphinx/ext/mathjax.py\n+++ b/sphinx/ext/mathjax.py\n@@ -111,7 +111,7 @@ def install_mathjax(\n             case str(config_filename):\n                 config_filepath = app.srcdir / config_filename\n                 if not config_filepath.exists():\n-                    raise ExtensionError(f'mathjax3_config file not found')\n+                    raise ExtensionError('mathjax3_config file not found')\n                 if not config_filepath.is_file():\n                     raise ExtensionError('mathjax3_config is not a file')\n                 if config_filepath.suffix != '.js':\n@@ -120,7 +120,7 @@ def install_mathjax(\n                     body = f.read()\n                 builder.add_js_file('', body=body)\n             case dict(config_dict):\n-                body = f\"window.MathJax = {json.dumps(config_dict)}\"\n+                body = f'window.MathJax = {json.dumps(config_dict)}'\n                 builder.add_js_file('', body=body)\n             case _:\n                 raise ExtensionError(\ndiff --git a/tests/test_extensions/test_ext_math.py b/tests/test_extensions/test_ext_math.py\nindex 529a59168..629997ea4 100644\n--- a/tests/test_extensions/test_ext_math.py\n+++ b/tests/test_extensions/test_ext_math.py\n@@ -381,7 +381,7 @@ def test_mathjax3_config(app: SphinxTestApp) -> None:\n     testroot='ext-math',\n     confoverrides={\n         'extensions': ['sphinx.ext.mathjax'],\n-        'mathjax3_config': \"_static/custom_mathjax_config.js\",\n+        'mathjax3_config': '_static/custom_mathjax_config.js',\n     },\n )\n def test_mathjax3_js_config(app: SphinxTestApp) -> None:\n"
    },
    {
        "id": "285",
        "sha_fail": "3ce7b9cb7b89ed8330621bdb0acb809eb830d687",
        "diff": "diff --git a/binance/async_client.py b/binance/async_client.py\nindex f0be76f..9f5932e 100644\n--- a/binance/async_client.py\n+++ b/binance/async_client.py\n@@ -42,15 +42,18 @@ class AsyncClient(BaseClient):\n         self.https_proxy = https_proxy\n         self.loop = loop or get_loop()\n         self._session_params: Dict[str, Any] = session_params or {}\n-        \n+\n         # Convert https_proxy to requests_params format for BaseClient\n         if https_proxy and requests_params is None:\n-            requests_params = {'proxies': {'http': https_proxy, 'https': https_proxy}}\n+            requests_params = {\"proxies\": {\"http\": https_proxy, \"https\": https_proxy}}\n         elif https_proxy and requests_params is not None:\n-            if 'proxies' not in requests_params:\n-                requests_params['proxies'] = {}\n-            requests_params['proxies'].update({'http': https_proxy, 'https': https_proxy})\n-        \n+            if \"proxies\" not in requests_params:\n+                requests_params[\"proxies\"] = {}\n+            requests_params[\"proxies\"].update({\n+                \"http\": https_proxy,\n+                \"https\": https_proxy,\n+            })\n+\n         super().__init__(\n             api_key,\n             api_secret,\n@@ -94,7 +97,7 @@ class AsyncClient(BaseClient):\n             private_key,\n             private_key_pass,\n             https_proxy,\n-            time_unit\n+            time_unit,\n         )\n         self.https_proxy = https_proxy  # move this to the constructor\n \n@@ -165,8 +168,8 @@ class AsyncClient(BaseClient):\n             data = f\"{url_encoded_data}&signature={signature}\"\n \n         # Remove proxies from kwargs since aiohttp uses 'proxy' parameter instead\n-        kwargs.pop('proxies', None)\n-        \n+        kwargs.pop(\"proxies\", None)\n+\n         async with getattr(self.session, method)(\n             yarl.URL(uri, encoded=True),\n             proxy=self.https_proxy,\n@@ -184,7 +187,7 @@ class AsyncClient(BaseClient):\n         \"\"\"\n         if not str(response.status).startswith(\"2\"):\n             raise BinanceAPIException(response, response.status, await response.text())\n-        \n+\n         text = await response.text()\n         if text == \"\":\n             return {}\n@@ -334,10 +337,10 @@ class AsyncClient(BaseClient):\n         params = {}\n         if symbol:\n             params[\"symbol\"] = symbol\n-        response = await self._get(\n-            \"ticker/price\", data=params\n-        )\n-        if isinstance(response, list) and all(isinstance(item, dict) for item in response):\n+        response = await self._get(\"ticker/price\", data=params)\n+        if isinstance(response, list) and all(\n+            isinstance(item, dict) for item in response\n+        ):\n             return response\n         raise TypeError(\"Expected a list of dictionaries\")\n \n@@ -349,9 +352,7 @@ class AsyncClient(BaseClient):\n             data[\"symbol\"] = params[\"symbol\"]\n         elif \"symbols\" in params:\n             data[\"symbols\"] = params[\"symbols\"]\n-        return await self._get(\n-            \"ticker/bookTicker\", data=data\n-        )\n+        return await self._get(\"ticker/bookTicker\", data=data)\n \n     get_orderbook_tickers.__doc__ = Client.get_orderbook_tickers.__doc__\n \n@@ -366,16 +367,12 @@ class AsyncClient(BaseClient):\n     get_recent_trades.__doc__ = Client.get_recent_trades.__doc__\n \n     async def get_historical_trades(self, **params) -> Dict:\n-        return await self._get(\n-            \"historicalTrades\", data=params\n-        )\n+        return await self._get(\"historicalTrades\", data=params)\n \n     get_historical_trades.__doc__ = Client.get_historical_trades.__doc__\n \n     async def get_aggregate_trades(self, **params) -> Dict:\n-        return await self._get(\n-            \"aggTrades\", data=params\n-        )\n+        return await self._get(\"aggTrades\", data=params)\n \n     get_aggregate_trades.__doc__ = Client.get_aggregate_trades.__doc__\n \n@@ -670,23 +667,17 @@ class AsyncClient(BaseClient):\n     _historical_klines_generator.__doc__ = Client._historical_klines_generator.__doc__\n \n     async def get_avg_price(self, **params):\n-        return await self._get(\n-            \"avgPrice\", data=params\n-        )\n+        return await self._get(\"avgPrice\", data=params)\n \n     get_avg_price.__doc__ = Client.get_avg_price.__doc__\n \n     async def get_ticker(self, **params):\n-        return await self._get(\n-            \"ticker/24hr\", data=params\n-        )\n+        return await self._get(\"ticker/24hr\", data=params)\n \n     get_ticker.__doc__ = Client.get_ticker.__doc__\n \n     async def get_symbol_ticker(self, **params):\n-        return await self._get(\n-            \"ticker/price\", data=params\n-        )\n+        return await self._get(\"ticker/price\", data=params)\n \n     get_symbol_ticker.__doc__ = Client.get_symbol_ticker.__doc__\n \n@@ -696,9 +687,7 @@ class AsyncClient(BaseClient):\n     get_symbol_ticker_window.__doc__ = Client.get_symbol_ticker_window.__doc__\n \n     async def get_orderbook_ticker(self, **params):\n-        return await self._get(\n-            \"ticker/bookTicker\", data=params\n-        )\n+        return await self._get(\"ticker/bookTicker\", data=params)\n \n     get_orderbook_ticker.__doc__ = Client.get_orderbook_ticker.__doc__\n \n@@ -1789,7 +1778,7 @@ class AsyncClient(BaseClient):\n         return await self._request_futures_data_api(\n             \"get\", \"globalLongShortAccountRatio\", data=params\n         )\n-        \n+\n     async def futures_taker_longshort_ratio(self, **params):\n         return await self._request_futures_data_api(\n             \"get\", \"takerlongshortRatio\", data=params\n@@ -1914,7 +1903,6 @@ class AsyncClient(BaseClient):\n         params[\"type\"] = \"MARKET\"\n         return await self._request_futures_api(\"post\", \"order\", True, data=params)\n \n-\n     async def futures_limit_buy_order(self, **params):\n         \"\"\"Send in a new futures limit buy order.\n \n@@ -2594,10 +2582,14 @@ class AsyncClient(BaseClient):\n \n     async def papi_get_balance(self, **params):\n         return await self._request_papi_api(\"get\", \"balance\", signed=True, data=params)\n+\n     papi_get_balance.__doc__ = Client.papi_get_balance.__doc__\n \n     async def papi_get_rate_limit(self, **params):\n-        return await self._request_papi_api(\"get\", \"rateLimit/order\", signed=True, data=params)\n+        return await self._request_papi_api(\n+            \"get\", \"rateLimit/order\", signed=True, data=params\n+        )\n+\n     papi_get_rate_limit.__doc__ = Client.papi_get_rate_limit.__doc__\n \n     async def papi_get_account(self, **params):\n@@ -2708,12 +2700,17 @@ class AsyncClient(BaseClient):\n             \"get\", \"portfolio/interest-history\", signed=True, data=params\n         )\n \n-\n     async def papi_get_portfolio_negative_balance_exchange_record(self, **params):\n         return await self._request_papi_api(\n-            \"get\", \"portfolio/negative-balance-exchange-record\", signed=True, data=params\n+            \"get\",\n+            \"portfolio/negative-balance-exchange-record\",\n+            signed=True,\n+            data=params,\n         )\n-    papi_get_portfolio_negative_balance_exchange_record.__doc__ = Client.papi_get_portfolio_negative_balance_exchange_record.__doc__\n+\n+    papi_get_portfolio_negative_balance_exchange_record.__doc__ = (\n+        Client.papi_get_portfolio_negative_balance_exchange_record.__doc__\n+    )\n \n     async def papi_fund_auto_collection(self, **params):\n         return await self._request_papi_api(\n@@ -3775,7 +3772,9 @@ class AsyncClient(BaseClient):\n     async def ws_get_account_rate_limits_orders(self, **params):\n         return await self._ws_api_request(\"account.rateLimits.orders\", True, params)\n \n-    ws_get_account_rate_limits_orders.__doc__ = Client.ws_get_account_rate_limits_orders.__doc__\n+    ws_get_account_rate_limits_orders.__doc__ = (\n+        Client.ws_get_account_rate_limits_orders.__doc__\n+    )\n \n     async def ws_get_all_orders(self, **params):\n         return await self._ws_api_request(\"allOrders\", True, params)\n@@ -4137,1317 +4136,2614 @@ class AsyncClient(BaseClient):\n     ####################################################\n \n     async def futures_historical_data_link(self, **params):\n-        return await self._request_margin_api(\"get\", \"futures/data/histDataLink\", signed=True, data=params)\n+        return await self._request_margin_api(\n+            \"get\", \"futures/data/histDataLink\", signed=True, data=params\n+        )\n \n     futures_historical_data_link.__doc__ = Client.futures_historical_data_link.__doc__\n \n     async def margin_v1_get_loan_vip_ongoing_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/vip/ongoing/orders\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_vip_ongoing_orders.__doc__ = Client.margin_v1_get_loan_vip_ongoing_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/vip/ongoing/orders\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_vip_ongoing_orders.__doc__ = (\n+        Client.margin_v1_get_loan_vip_ongoing_orders.__doc__\n+    )\n+\n     async def margin_v1_get_mining_payment_other(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/payment/other\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_payment_other.__doc__ = Client.margin_v1_get_mining_payment_other.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/payment/other\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_payment_other.__doc__ = (\n+        Client.margin_v1_get_mining_payment_other.__doc__\n+    )\n+\n     async def futures_coin_v1_get_income_asyn_id(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"income/asyn/id\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_get_income_asyn_id.__doc__ = Client.futures_coin_v1_get_income_asyn_id.__doc__\n-            \n-    async def margin_v1_get_simple_earn_flexible_history_subscription_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"simple-earn/flexible/history/subscriptionRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_simple_earn_flexible_history_subscription_record.__doc__ = Client.margin_v1_get_simple_earn_flexible_history_subscription_record.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"income/asyn/id\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_income_asyn_id.__doc__ = (\n+        Client.futures_coin_v1_get_income_asyn_id.__doc__\n+    )\n+\n+    async def margin_v1_get_simple_earn_flexible_history_subscription_record(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/flexible/history/subscriptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_simple_earn_flexible_history_subscription_record.__doc__ = (\n+        Client.margin_v1_get_simple_earn_flexible_history_subscription_record.__doc__\n+    )\n+\n     async def margin_v1_post_lending_auto_invest_one_off(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/auto-invest/one-off\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_auto_invest_one_off.__doc__ = Client.margin_v1_post_lending_auto_invest_one_off.__doc__\n-            \n-    async def margin_v1_post_broker_sub_account_api_commission_coin_futures(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/commission/coinFutures\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_commission_coin_futures.__doc__ = Client.margin_v1_post_broker_sub_account_api_commission_coin_futures.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"lending/auto-invest/one-off\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_lending_auto_invest_one_off.__doc__ = (\n+        Client.margin_v1_post_lending_auto_invest_one_off.__doc__\n+    )\n+\n+    async def margin_v1_post_broker_sub_account_api_commission_coin_futures(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/commission/coinFutures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_commission_coin_futures.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_commission_coin_futures.__doc__\n+    )\n+\n     async def v3_post_order_list_otoco(self, **params):\n-        return await self._request_api(\"post\", \"orderList/otoco\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"orderList/otoco\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_post_order_list_otoco.__doc__ = Client.v3_post_order_list_otoco.__doc__\n-            \n+\n     async def futures_v1_get_order_asyn(self, **params):\n-        return await self._request_futures_api(\"get\", \"order/asyn\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"order/asyn\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_order_asyn.__doc__ = Client.futures_v1_get_order_asyn.__doc__\n-            \n+\n     async def margin_v1_get_asset_custody_transfer_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"asset/custody/transfer-history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_asset_custody_transfer_history.__doc__ = Client.margin_v1_get_asset_custody_transfer_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"asset/custody/transfer-history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_asset_custody_transfer_history.__doc__ = (\n+        Client.margin_v1_get_asset_custody_transfer_history.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_blvt(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccount/blvt\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_blvt.__doc__ = Client.margin_v1_post_broker_sub_account_blvt.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"broker/subAccount/blvt\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_broker_sub_account_blvt.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_blvt.__doc__\n+    )\n+\n     async def margin_v1_post_sol_staking_sol_redeem(self, **params):\n-        return await self._request_margin_api(\"post\", \"sol-staking/sol/redeem\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sol_staking_sol_redeem.__doc__ = Client.margin_v1_post_sol_staking_sol_redeem.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"sol-staking/sol/redeem\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_sol_staking_sol_redeem.__doc__ = (\n+        Client.margin_v1_post_sol_staking_sol_redeem.__doc__\n+    )\n+\n     async def options_v1_get_countdown_cancel_all(self, **params):\n-        return await self._request_options_api(\"get\", \"countdownCancelAll\", signed=True, data=params)\n-        \n-    options_v1_get_countdown_cancel_all.__doc__ = Client.options_v1_get_countdown_cancel_all.__doc__\n-            \n+        return await self._request_options_api(\n+            \"get\", \"countdownCancelAll\", signed=True, data=params\n+        )\n+\n+    options_v1_get_countdown_cancel_all.__doc__ = (\n+        Client.options_v1_get_countdown_cancel_all.__doc__\n+    )\n+\n     async def margin_v1_get_margin_trade_coeff(self, **params):\n-        return await self._request_margin_api(\"get\", \"margin/tradeCoeff\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_margin_trade_coeff.__doc__ = Client.margin_v1_get_margin_trade_coeff.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"margin/tradeCoeff\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_margin_trade_coeff.__doc__ = (\n+        Client.margin_v1_get_margin_trade_coeff.__doc__\n+    )\n+\n     async def futures_coin_v1_get_order_amendment(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"orderAmendment\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_get_order_amendment.__doc__ = Client.futures_coin_v1_get_order_amendment.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"orderAmendment\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_order_amendment.__doc__ = (\n+        Client.futures_coin_v1_get_order_amendment.__doc__\n+    )\n+\n     async def margin_v1_get_margin_available_inventory(self, **params):\n-        return await self._request_margin_api(\"get\", \"margin/available-inventory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_margin_available_inventory.__doc__ = Client.margin_v1_get_margin_available_inventory.__doc__\n-            \n-    async def margin_v1_post_account_api_restrictions_ip_restriction_ip_list(self, **params):\n-        return await self._request_margin_api(\"post\", \"account/apiRestrictions/ipRestriction/ipList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_account_api_restrictions_ip_restriction_ip_list.__doc__ = Client.margin_v1_post_account_api_restrictions_ip_restriction_ip_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"margin/available-inventory\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_margin_available_inventory.__doc__ = (\n+        Client.margin_v1_get_margin_available_inventory.__doc__\n+    )\n+\n+    async def margin_v1_post_account_api_restrictions_ip_restriction_ip_list(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"account/apiRestrictions/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_account_api_restrictions_ip_restriction_ip_list.__doc__ = (\n+        Client.margin_v1_post_account_api_restrictions_ip_restriction_ip_list.__doc__\n+    )\n+\n     async def margin_v2_get_eth_staking_account(self, **params):\n-        return await self._request_margin_api(\"get\", \"eth-staking/account\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_eth_staking_account.__doc__ = Client.margin_v2_get_eth_staking_account.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"eth-staking/account\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_eth_staking_account.__doc__ = (\n+        Client.margin_v2_get_eth_staking_account.__doc__\n+    )\n+\n     async def margin_v1_get_loan_income(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/income\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"get\", \"loan/income\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_get_loan_income.__doc__ = Client.margin_v1_get_loan_income.__doc__\n-            \n+\n     async def futures_coin_v1_get_pm_account_info(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"pmAccountInfo\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_get_pm_account_info.__doc__ = Client.futures_coin_v1_get_pm_account_info.__doc__\n-            \n-    async def margin_v1_get_managed_subaccount_query_trans_log_for_investor(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/queryTransLogForInvestor\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_query_trans_log_for_investor.__doc__ = Client.margin_v1_get_managed_subaccount_query_trans_log_for_investor.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"pmAccountInfo\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_pm_account_info.__doc__ = (\n+        Client.futures_coin_v1_get_pm_account_info.__doc__\n+    )\n+\n+    async def margin_v1_get_managed_subaccount_query_trans_log_for_investor(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/queryTransLogForInvestor\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_managed_subaccount_query_trans_log_for_investor.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_query_trans_log_for_investor.__doc__\n+    )\n+\n     async def margin_v1_post_dci_product_auto_compound_edit_status(self, **params):\n-        return await self._request_margin_api(\"post\", \"dci/product/auto_compound/edit-status\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_dci_product_auto_compound_edit_status.__doc__ = Client.margin_v1_post_dci_product_auto_compound_edit_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"dci/product/auto_compound/edit-status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_dci_product_auto_compound_edit_status.__doc__ = (\n+        Client.margin_v1_post_dci_product_auto_compound_edit_status.__doc__\n+    )\n+\n     async def futures_v1_get_trade_asyn(self, **params):\n-        return await self._request_futures_api(\"get\", \"trade/asyn\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"trade/asyn\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_trade_asyn.__doc__ = Client.futures_v1_get_trade_asyn.__doc__\n-            \n+\n     async def margin_v1_get_loan_vip_request_interest_rate(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/vip/request/interestRate\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_vip_request_interest_rate.__doc__ = Client.margin_v1_get_loan_vip_request_interest_rate.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/vip/request/interestRate\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_vip_request_interest_rate.__doc__ = (\n+        Client.margin_v1_get_loan_vip_request_interest_rate.__doc__\n+    )\n+\n     async def futures_v1_get_funding_info(self, **params):\n-        return await self._request_futures_api(\"get\", \"fundingInfo\", signed=False, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"fundingInfo\", signed=False, data=params, version=1\n+        )\n+\n     futures_v1_get_funding_info.__doc__ = Client.futures_v1_get_funding_info.__doc__\n-            \n+\n     async def v3_get_all_orders(self, **params):\n-        return await self._request_api(\"get\", \"allOrders\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"allOrders\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v2_get_loan_flexible_repay_rate(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/repay/rate\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_repay_rate.__doc__ = Client.margin_v2_get_loan_flexible_repay_rate.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/flexible/repay/rate\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_loan_flexible_repay_rate.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_repay_rate.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_plan_id(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/plan/id\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_plan_id.__doc__ = Client.margin_v1_get_lending_auto_invest_plan_id.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"lending/auto-invest/plan/id\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_lending_auto_invest_plan_id.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_plan_id.__doc__\n+    )\n+\n     async def margin_v1_post_loan_adjust_ltv(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/adjust/ltv\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_loan_adjust_ltv.__doc__ = Client.margin_v1_post_loan_adjust_ltv.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/adjust/ltv\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_loan_adjust_ltv.__doc__ = (\n+        Client.margin_v1_post_loan_adjust_ltv.__doc__\n+    )\n+\n     async def margin_v1_get_mining_statistics_user_status(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/statistics/user/status\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_statistics_user_status.__doc__ = Client.margin_v1_get_mining_statistics_user_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/statistics/user/status\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_statistics_user_status.__doc__ = (\n+        Client.margin_v1_get_mining_statistics_user_status.__doc__\n+    )\n+\n     async def margin_v1_get_broker_transfer_futures(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/transfer/futures\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_transfer_futures.__doc__ = Client.margin_v1_get_broker_transfer_futures.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/transfer/futures\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_transfer_futures.__doc__ = (\n+        Client.margin_v1_get_broker_transfer_futures.__doc__\n+    )\n+\n     async def margin_v1_post_algo_spot_new_order_twap(self, **params):\n-        return await self._request_margin_api(\"post\", \"algo/spot/newOrderTwap\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_algo_spot_new_order_twap.__doc__ = Client.margin_v1_post_algo_spot_new_order_twap.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"algo/spot/newOrderTwap\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_algo_spot_new_order_twap.__doc__ = (\n+        Client.margin_v1_post_algo_spot_new_order_twap.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_target_asset_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/target-asset/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_target_asset_list.__doc__ = Client.margin_v1_get_lending_auto_invest_target_asset_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/target-asset/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_target_asset_list.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_target_asset_list.__doc__\n+    )\n+\n     async def margin_v1_get_capital_deposit_address_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"capital/deposit/address/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_capital_deposit_address_list.__doc__ = Client.margin_v1_get_capital_deposit_address_list.__doc__\n-            \n-    async def margin_v1_post_broker_sub_account_bnb_burn_margin_interest(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccount/bnbBurn/marginInterest\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_bnb_burn_margin_interest.__doc__ = Client.margin_v1_post_broker_sub_account_bnb_burn_margin_interest.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"capital/deposit/address/list\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_capital_deposit_address_list.__doc__ = (\n+        Client.margin_v1_get_capital_deposit_address_list.__doc__\n+    )\n+\n+    async def margin_v1_post_broker_sub_account_bnb_burn_margin_interest(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccount/bnbBurn/marginInterest\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_bnb_burn_margin_interest.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_bnb_burn_margin_interest.__doc__\n+    )\n+\n     async def margin_v2_post_loan_flexible_repay(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/flexible/repay\", signed=True, data=params, version=2)\n-        \n-    margin_v2_post_loan_flexible_repay.__doc__ = Client.margin_v2_post_loan_flexible_repay.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/flexible/repay\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_post_loan_flexible_repay.__doc__ = (\n+        Client.margin_v2_post_loan_flexible_repay.__doc__\n+    )\n+\n     async def margin_v2_get_loan_flexible_loanable_data(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/loanable/data\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_loanable_data.__doc__ = Client.margin_v2_get_loan_flexible_loanable_data.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/flexible/loanable/data\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_loan_flexible_loanable_data.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_loanable_data.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_api_permission(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/permission\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_permission.__doc__ = Client.margin_v1_post_broker_sub_account_api_permission.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/permission\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_permission.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_permission.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_api(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api.__doc__ = Client.margin_v1_post_broker_sub_account_api.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"broker/subAccountApi\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_broker_sub_account_api.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api.__doc__\n+    )\n+\n     async def margin_v1_get_dci_product_positions(self, **params):\n-        return await self._request_margin_api(\"get\", \"dci/product/positions\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_dci_product_positions.__doc__ = Client.margin_v1_get_dci_product_positions.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"dci/product/positions\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_dci_product_positions.__doc__ = (\n+        Client.margin_v1_get_dci_product_positions.__doc__\n+    )\n+\n     async def margin_v1_post_convert_limit_cancel_order(self, **params):\n-        return await self._request_margin_api(\"post\", \"convert/limit/cancelOrder\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_convert_limit_cancel_order.__doc__ = Client.margin_v1_post_convert_limit_cancel_order.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"convert/limit/cancelOrder\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_convert_limit_cancel_order.__doc__ = (\n+        Client.margin_v1_post_convert_limit_cancel_order.__doc__\n+    )\n+\n     async def v3_post_order_list_oto(self, **params):\n-        return await self._request_api(\"post\", \"orderList/oto\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"orderList/oto\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_post_order_list_oto.__doc__ = Client.v3_post_order_list_oto.__doc__\n-            \n+\n     async def margin_v1_get_mining_hash_transfer_config_details_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/hash-transfer/config/details/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_hash_transfer_config_details_list.__doc__ = Client.margin_v1_get_mining_hash_transfer_config_details_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"mining/hash-transfer/config/details/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_mining_hash_transfer_config_details_list.__doc__ = (\n+        Client.margin_v1_get_mining_hash_transfer_config_details_list.__doc__\n+    )\n+\n     async def margin_v1_get_mining_hash_transfer_profit_details(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/hash-transfer/profit/details\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_hash_transfer_profit_details.__doc__ = Client.margin_v1_get_mining_hash_transfer_profit_details.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"mining/hash-transfer/profit/details\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_mining_hash_transfer_profit_details.__doc__ = (\n+        Client.margin_v1_get_mining_hash_transfer_profit_details.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account.__doc__ = Client.margin_v1_get_broker_sub_account.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/subAccount\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_sub_account.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account.__doc__\n+    )\n+\n     async def margin_v1_get_portfolio_balance(self, **params):\n-        return await self._request_margin_api(\"get\", \"portfolio/balance\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_portfolio_balance.__doc__ = Client.margin_v1_get_portfolio_balance.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"portfolio/balance\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_portfolio_balance.__doc__ = (\n+        Client.margin_v1_get_portfolio_balance.__doc__\n+    )\n+\n     async def margin_v1_post_sub_account_eoptions_enable(self, **params):\n-        return await self._request_margin_api(\"post\", \"sub-account/eoptions/enable\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sub_account_eoptions_enable.__doc__ = Client.margin_v1_post_sub_account_eoptions_enable.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"sub-account/eoptions/enable\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_sub_account_eoptions_enable.__doc__ = (\n+        Client.margin_v1_post_sub_account_eoptions_enable.__doc__\n+    )\n+\n     async def papi_v1_post_ping(self, **params):\n-        return await self._request_papi_api(\"post\", \"ping\", signed=True, data=params, version=1)\n-        \n-    papi_v1_post_ping.__doc__ = Client.papi_v1_post_ping.__doc__\n-            \n-    async def margin_v1_get_loan_loanable_data(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/loanable/data\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_loanable_data.__doc__ = Client.margin_v1_get_loan_loanable_data.__doc__\n-            \n+        return await self._request_papi_api(\n+            \"post\", \"ping\", signed=True, data=params, version=1\n+        )\n+\n+    papi_v1_post_ping.__doc__ = Client.papi_v1_post_ping.__doc__\n+\n+    async def margin_v1_get_loan_loanable_data(self, **params):\n+        return await self._request_margin_api(\n+            \"get\", \"loan/loanable/data\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_loanable_data.__doc__ = (\n+        Client.margin_v1_get_loan_loanable_data.__doc__\n+    )\n+\n     async def margin_v1_post_eth_staking_wbeth_unwrap(self, **params):\n-        return await self._request_margin_api(\"post\", \"eth-staking/wbeth/unwrap\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_eth_staking_wbeth_unwrap.__doc__ = Client.margin_v1_post_eth_staking_wbeth_unwrap.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"eth-staking/wbeth/unwrap\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_eth_staking_wbeth_unwrap.__doc__ = (\n+        Client.margin_v1_post_eth_staking_wbeth_unwrap.__doc__\n+    )\n+\n     async def margin_v1_get_eth_staking_eth_history_staking_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"eth-staking/eth/history/stakingHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_eth_staking_eth_history_staking_history.__doc__ = Client.margin_v1_get_eth_staking_eth_history_staking_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"eth-staking/eth/history/stakingHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_eth_staking_eth_history_staking_history.__doc__ = (\n+        Client.margin_v1_get_eth_staking_eth_history_staking_history.__doc__\n+    )\n+\n     async def margin_v1_get_staking_staking_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"staking/stakingRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_staking_staking_record.__doc__ = Client.margin_v1_get_staking_staking_record.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"staking/stakingRecord\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_staking_staking_record.__doc__ = (\n+        Client.margin_v1_get_staking_staking_record.__doc__\n+    )\n+\n     async def margin_v1_get_broker_rebate_recent_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/rebate/recentRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_rebate_recent_record.__doc__ = Client.margin_v1_get_broker_rebate_recent_record.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/rebate/recentRecord\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_rebate_recent_record.__doc__ = (\n+        Client.margin_v1_get_broker_rebate_recent_record.__doc__\n+    )\n+\n     async def v3_delete_user_data_stream(self, **params):\n-        return await self._request_api(\"delete\", \"userDataStream\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"delete\", \"userDataStream\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def v3_get_open_order_list(self, **params):\n-        return await self._request_api(\"get\", \"openOrderList\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"openOrderList\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_loan_vip_collateral_account(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/vip/collateral/account\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_vip_collateral_account.__doc__ = Client.margin_v1_get_loan_vip_collateral_account.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/vip/collateral/account\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_vip_collateral_account.__doc__ = (\n+        Client.margin_v1_get_loan_vip_collateral_account.__doc__\n+    )\n+\n     async def margin_v1_get_algo_spot_open_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"algo/spot/openOrders\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_algo_spot_open_orders.__doc__ = Client.margin_v1_get_algo_spot_open_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"algo/spot/openOrders\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_algo_spot_open_orders.__doc__ = (\n+        Client.margin_v1_get_algo_spot_open_orders.__doc__\n+    )\n+\n     async def margin_v1_post_loan_repay(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/repay\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"post\", \"loan/repay\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_post_loan_repay.__doc__ = Client.margin_v1_post_loan_repay.__doc__\n-            \n+\n     async def futures_coin_v1_get_funding_info(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"fundingInfo\", signed=False, data=params, version=1)\n-        \n-    futures_coin_v1_get_funding_info.__doc__ = Client.futures_coin_v1_get_funding_info.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"fundingInfo\", signed=False, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_funding_info.__doc__ = (\n+        Client.futures_coin_v1_get_funding_info.__doc__\n+    )\n+\n     async def margin_v1_get_margin_leverage_bracket(self, **params):\n-        return await self._request_margin_api(\"get\", \"margin/leverageBracket\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_margin_leverage_bracket.__doc__ = Client.margin_v1_get_margin_leverage_bracket.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"margin/leverageBracket\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_margin_leverage_bracket.__doc__ = (\n+        Client.margin_v1_get_margin_leverage_bracket.__doc__\n+    )\n+\n     async def margin_v2_get_portfolio_collateral_rate(self, **params):\n-        return await self._request_margin_api(\"get\", \"portfolio/collateralRate\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_portfolio_collateral_rate.__doc__ = Client.margin_v2_get_portfolio_collateral_rate.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"portfolio/collateralRate\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_portfolio_collateral_rate.__doc__ = (\n+        Client.margin_v2_get_portfolio_collateral_rate.__doc__\n+    )\n+\n     async def margin_v2_post_loan_flexible_adjust_ltv(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/flexible/adjust/ltv\", signed=True, data=params, version=2)\n-        \n-    margin_v2_post_loan_flexible_adjust_ltv.__doc__ = Client.margin_v2_post_loan_flexible_adjust_ltv.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/flexible/adjust/ltv\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_post_loan_flexible_adjust_ltv.__doc__ = (\n+        Client.margin_v2_post_loan_flexible_adjust_ltv.__doc__\n+    )\n+\n     async def margin_v1_get_convert_order_status(self, **params):\n-        return await self._request_margin_api(\"get\", \"convert/orderStatus\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_convert_order_status.__doc__ = Client.margin_v1_get_convert_order_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"convert/orderStatus\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_convert_order_status.__doc__ = (\n+        Client.margin_v1_get_convert_order_status.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_api_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_api_ip_restriction.__doc__ = Client.margin_v1_get_broker_sub_account_api_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_sub_account_api_ip_restriction.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_api_ip_restriction.__doc__\n+    )\n+\n     async def margin_v1_post_dci_product_subscribe(self, **params):\n-        return await self._request_margin_api(\"post\", \"dci/product/subscribe\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_dci_product_subscribe.__doc__ = Client.margin_v1_post_dci_product_subscribe.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"dci/product/subscribe\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_dci_product_subscribe.__doc__ = (\n+        Client.margin_v1_post_dci_product_subscribe.__doc__\n+    )\n+\n     async def futures_v1_get_income_asyn_id(self, **params):\n-        return await self._request_futures_api(\"get\", \"income/asyn/id\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"income/asyn/id\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_income_asyn_id.__doc__ = Client.futures_v1_get_income_asyn_id.__doc__\n-            \n+\n     async def options_v1_post_countdown_cancel_all(self, **params):\n-        return await self._request_options_api(\"post\", \"countdownCancelAll\", signed=True, data=params)\n-        \n-    options_v1_post_countdown_cancel_all.__doc__ = Client.options_v1_post_countdown_cancel_all.__doc__\n-            \n+        return await self._request_options_api(\n+            \"post\", \"countdownCancelAll\", signed=True, data=params\n+        )\n+\n+    options_v1_post_countdown_cancel_all.__doc__ = (\n+        Client.options_v1_post_countdown_cancel_all.__doc__\n+    )\n+\n     async def margin_v1_post_mining_hash_transfer_config_cancel(self, **params):\n-        return await self._request_margin_api(\"post\", \"mining/hash-transfer/config/cancel\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_mining_hash_transfer_config_cancel.__doc__ = Client.margin_v1_post_mining_hash_transfer_config_cancel.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"mining/hash-transfer/config/cancel\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_mining_hash_transfer_config_cancel.__doc__ = (\n+        Client.margin_v1_post_mining_hash_transfer_config_cancel.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_deposit_hist(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/depositHist\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_deposit_hist.__doc__ = Client.margin_v1_get_broker_sub_account_deposit_hist.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/subAccount/depositHist\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_sub_account_deposit_hist.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_deposit_hist.__doc__\n+    )\n+\n     async def margin_v1_get_mining_payment_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/payment/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_payment_list.__doc__ = Client.margin_v1_get_mining_payment_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/payment/list\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_payment_list.__doc__ = (\n+        Client.margin_v1_get_mining_payment_list.__doc__\n+    )\n+\n     async def futures_v1_get_pm_account_info(self, **params):\n-        return await self._request_futures_api(\"get\", \"pmAccountInfo\", signed=True, data=params, version=1)\n-        \n-    futures_v1_get_pm_account_info.__doc__ = Client.futures_v1_get_pm_account_info.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"get\", \"pmAccountInfo\", signed=True, data=params, version=1\n+        )\n+\n+    futures_v1_get_pm_account_info.__doc__ = (\n+        Client.futures_v1_get_pm_account_info.__doc__\n+    )\n+\n     async def futures_coin_v1_get_adl_quantile(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"adlQuantile\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_get_adl_quantile.__doc__ = Client.futures_coin_v1_get_adl_quantile.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"adlQuantile\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_adl_quantile.__doc__ = (\n+        Client.futures_coin_v1_get_adl_quantile.__doc__\n+    )\n+\n     async def options_v1_get_income_asyn_id(self, **params):\n-        return await self._request_options_api(\"get\", \"income/asyn/id\", signed=True, data=params)\n-        \n+        return await self._request_options_api(\n+            \"get\", \"income/asyn/id\", signed=True, data=params\n+        )\n+\n     options_v1_get_income_asyn_id.__doc__ = Client.options_v1_get_income_asyn_id.__doc__\n-            \n+\n     async def v3_post_cancel_replace(self, **params):\n-        return await self._request_api(\"post\", \"cancelReplace\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"cancelReplace\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_post_cancel_replace.__doc__ = Client.v3_post_cancel_replace.__doc__\n-            \n+\n     async def v3_post_order_test(self, **params):\n-        return await self._request_api(\"post\", \"order/test\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"order/test\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_post_account_enable_fast_withdraw_switch(self, **params):\n-        return await self._request_margin_api(\"post\", \"account/enableFastWithdrawSwitch\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_account_enable_fast_withdraw_switch.__doc__ = Client.margin_v1_post_account_enable_fast_withdraw_switch.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"account/enableFastWithdrawSwitch\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_account_enable_fast_withdraw_switch.__doc__ = (\n+        Client.margin_v1_post_account_enable_fast_withdraw_switch.__doc__\n+    )\n+\n     async def margin_v1_post_broker_transfer_futures(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/transfer/futures\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_transfer_futures.__doc__ = Client.margin_v1_post_broker_transfer_futures.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"broker/transfer/futures\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_broker_transfer_futures.__doc__ = (\n+        Client.margin_v1_post_broker_transfer_futures.__doc__\n+    )\n+\n     async def margin_v1_get_margin_isolated_transfer(self, **params):\n-        return await self._request_margin_api(\"get\", \"margin/isolated/transfer\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"get\", \"margin/isolated/transfer\", signed=True, data=params, version=1\n+        )\n+\n     async def v3_post_order_cancel_replace(self, **params):\n-        return await self._request_api(\"post\", \"order/cancelReplace\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"order/cancelReplace\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_post_sol_staking_sol_stake(self, **params):\n-        return await self._request_margin_api(\"post\", \"sol-staking/sol/stake\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sol_staking_sol_stake.__doc__ = Client.margin_v1_post_sol_staking_sol_stake.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"sol-staking/sol/stake\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_sol_staking_sol_stake.__doc__ = (\n+        Client.margin_v1_post_sol_staking_sol_stake.__doc__\n+    )\n+\n     async def margin_v1_post_loan_borrow(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/borrow\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"post\", \"loan/borrow\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_post_loan_borrow.__doc__ = Client.margin_v1_post_loan_borrow.__doc__\n-            \n+\n     async def margin_v1_get_managed_subaccount_info(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/info\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_info.__doc__ = Client.margin_v1_get_managed_subaccount_info.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"managed-subaccount/info\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_managed_subaccount_info.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_info.__doc__\n+    )\n+\n     async def margin_v1_post_lending_auto_invest_plan_edit_status(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/auto-invest/plan/edit-status\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_auto_invest_plan_edit_status.__doc__ = Client.margin_v1_post_lending_auto_invest_plan_edit_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"lending/auto-invest/plan/edit-status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_lending_auto_invest_plan_edit_status.__doc__ = (\n+        Client.margin_v1_post_lending_auto_invest_plan_edit_status.__doc__\n+    )\n+\n     async def margin_v1_get_sol_staking_sol_history_unclaimed_rewards(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/history/unclaimedRewards\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_history_unclaimed_rewards.__doc__ = Client.margin_v1_get_sol_staking_sol_history_unclaimed_rewards.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/unclaimedRewards\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sol_staking_sol_history_unclaimed_rewards.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_history_unclaimed_rewards.__doc__\n+    )\n+\n     async def margin_v1_post_asset_convert_transfer_query_by_page(self, **params):\n-        return await self._request_margin_api(\"post\", \"asset/convert-transfer/queryByPage\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_asset_convert_transfer_query_by_page.__doc__ = Client.margin_v1_post_asset_convert_transfer_query_by_page.__doc__\n-            \n-    async def margin_v1_get_sol_staking_sol_history_boost_rewards_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/history/boostRewardsHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_history_boost_rewards_history.__doc__ = Client.margin_v1_get_sol_staking_sol_history_boost_rewards_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"asset/convert-transfer/queryByPage\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_asset_convert_transfer_query_by_page.__doc__ = (\n+        Client.margin_v1_post_asset_convert_transfer_query_by_page.__doc__\n+    )\n+\n+    async def margin_v1_get_sol_staking_sol_history_boost_rewards_history(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/boostRewardsHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sol_staking_sol_history_boost_rewards_history.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_history_boost_rewards_history.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_one_off_status(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/one-off/status\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_one_off_status.__doc__ = Client.margin_v1_get_lending_auto_invest_one_off_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/one-off/status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_one_off_status.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_one_off_status.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccount\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account.__doc__ = Client.margin_v1_post_broker_sub_account.__doc__\n-            \n-    async def margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page(self, **params):\n-        return await self._request_margin_api(\"get\", \"asset/ledger-transfer/cloud-mining/queryByPage\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page.__doc__ = Client.margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"broker/subAccount\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_broker_sub_account.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account.__doc__\n+    )\n+\n+    async def margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"asset/ledger-transfer/cloud-mining/queryByPage\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page.__doc__ = (\n+        Client.margin_v1_get_asset_ledger_transfer_cloud_mining_query_by_page.__doc__\n+    )\n+\n     async def margin_v1_get_mining_pub_coin_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/pub/coinList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_pub_coin_list.__doc__ = Client.margin_v1_get_mining_pub_coin_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/pub/coinList\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_pub_coin_list.__doc__ = (\n+        Client.margin_v1_get_mining_pub_coin_list.__doc__\n+    )\n+\n     async def margin_v2_get_loan_flexible_repay_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/repay/history\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_repay_history.__doc__ = Client.margin_v2_get_loan_flexible_repay_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/flexible/repay/history\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_loan_flexible_repay_history.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_repay_history.__doc__\n+    )\n+\n     async def v3_post_sor_order(self, **params):\n-        return await self._request_api(\"post\", \"sor/order\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"sor/order\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_post_sor_order.__doc__ = Client.v3_post_sor_order.__doc__\n-            \n+\n     async def margin_v1_post_capital_deposit_credit_apply(self, **params):\n-        return await self._request_margin_api(\"post\", \"capital/deposit/credit-apply\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_capital_deposit_credit_apply.__doc__ = Client.margin_v1_post_capital_deposit_credit_apply.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"capital/deposit/credit-apply\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_capital_deposit_credit_apply.__doc__ = (\n+        Client.margin_v1_post_capital_deposit_credit_apply.__doc__\n+    )\n+\n     async def futures_v1_put_batch_order(self, **params):\n-        return await self._request_futures_api(\"put\", \"batchOrder\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"put\", \"batchOrder\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_put_batch_order.__doc__ = Client.futures_v1_put_batch_order.__doc__\n-            \n+\n     async def v3_get_my_prevented_matches(self, **params):\n-        return await self._request_api(\"get\", \"myPreventedMatches\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"myPreventedMatches\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_mining_statistics_user_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/statistics/user/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_statistics_user_list.__doc__ = Client.margin_v1_get_mining_statistics_user_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/statistics/user/list\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_statistics_user_list.__doc__ = (\n+        Client.margin_v1_get_mining_statistics_user_list.__doc__\n+    )\n+\n     async def futures_v1_post_batch_order(self, **params):\n-        return await self._request_futures_api(\"post\", \"batchOrder\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"post\", \"batchOrder\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_post_batch_order.__doc__ = Client.futures_v1_post_batch_order.__doc__\n-            \n+\n     async def v3_get_ticker_trading_day(self, **params):\n-        return await self._request_api(\"get\", \"ticker/tradingDay\", signed=False, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"ticker/tradingDay\", signed=False, data=params, version=\"v3\"\n+        )\n+\n     v3_get_ticker_trading_day.__doc__ = Client.v3_get_ticker_trading_day.__doc__\n-            \n+\n     async def margin_v1_get_mining_worker_detail(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/worker/detail\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_worker_detail.__doc__ = Client.margin_v1_get_mining_worker_detail.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/worker/detail\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_worker_detail.__doc__ = (\n+        Client.margin_v1_get_mining_worker_detail.__doc__\n+    )\n+\n     async def margin_v1_get_managed_subaccount_fetch_future_asset(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/fetch-future-asset\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_fetch_future_asset.__doc__ = Client.margin_v1_get_managed_subaccount_fetch_future_asset.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/fetch-future-asset\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_managed_subaccount_fetch_future_asset.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_fetch_future_asset.__doc__\n+    )\n+\n     async def margin_v1_get_margin_rate_limit_order(self, **params):\n-        return await self._request_margin_api(\"get\", \"margin/rateLimit/order\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_margin_rate_limit_order.__doc__ = Client.margin_v1_get_margin_rate_limit_order.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"margin/rateLimit/order\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_margin_rate_limit_order.__doc__ = (\n+        Client.margin_v1_get_margin_rate_limit_order.__doc__\n+    )\n+\n     async def margin_v1_get_localentity_vasp(self, **params):\n-        return await self._request_margin_api(\"get\", \"localentity/vasp\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_localentity_vasp.__doc__ = Client.margin_v1_get_localentity_vasp.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"localentity/vasp\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_localentity_vasp.__doc__ = (\n+        Client.margin_v1_get_localentity_vasp.__doc__\n+    )\n+\n     async def margin_v1_get_sol_staking_sol_history_rate_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/history/rateHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_history_rate_history.__doc__ = Client.margin_v1_get_sol_staking_sol_history_rate_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/rateHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sol_staking_sol_history_rate_history.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_history_rate_history.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_api_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_ip_restriction.__doc__ = Client.margin_v1_post_broker_sub_account_api_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_ip_restriction.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_ip_restriction.__doc__\n+    )\n+\n     async def margin_v1_get_broker_transfer(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/transfer\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"get\", \"broker/transfer\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_get_broker_transfer.__doc__ = Client.margin_v1_get_broker_transfer.__doc__\n-            \n+\n     async def margin_v1_get_sol_staking_account(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/account\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_account.__doc__ = Client.margin_v1_get_sol_staking_account.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"sol-staking/account\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_sol_staking_account.__doc__ = (\n+        Client.margin_v1_get_sol_staking_account.__doc__\n+    )\n+\n     async def margin_v1_get_account_info(self, **params):\n-        return await self._request_margin_api(\"get\", \"account/info\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"get\", \"account/info\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_get_account_info.__doc__ = Client.margin_v1_get_account_info.__doc__\n-            \n+\n     async def margin_v1_post_portfolio_repay_futures_switch(self, **params):\n-        return await self._request_margin_api(\"post\", \"portfolio/repay-futures-switch\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_portfolio_repay_futures_switch.__doc__ = Client.margin_v1_post_portfolio_repay_futures_switch.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"portfolio/repay-futures-switch\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_portfolio_repay_futures_switch.__doc__ = (\n+        Client.margin_v1_post_portfolio_repay_futures_switch.__doc__\n+    )\n+\n     async def margin_v1_post_loan_vip_borrow(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/vip/borrow\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_loan_vip_borrow.__doc__ = Client.margin_v1_post_loan_vip_borrow.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/vip/borrow\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_loan_vip_borrow.__doc__ = (\n+        Client.margin_v1_post_loan_vip_borrow.__doc__\n+    )\n+\n     async def margin_v2_get_loan_flexible_ltv_adjustment_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/ltv/adjustment/history\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_ltv_adjustment_history.__doc__ = Client.margin_v2_get_loan_flexible_ltv_adjustment_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"loan/flexible/ltv/adjustment/history\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n+\n+    margin_v2_get_loan_flexible_ltv_adjustment_history.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_ltv_adjustment_history.__doc__\n+    )\n+\n     async def options_v1_delete_all_open_orders_by_underlying(self, **params):\n-        return await self._request_options_api(\"delete\", \"allOpenOrdersByUnderlying\", signed=True, data=params)\n-        \n-    options_v1_delete_all_open_orders_by_underlying.__doc__ = Client.options_v1_delete_all_open_orders_by_underlying.__doc__\n-            \n+        return await self._request_options_api(\n+            \"delete\", \"allOpenOrdersByUnderlying\", signed=True, data=params\n+        )\n+\n+    options_v1_delete_all_open_orders_by_underlying.__doc__ = (\n+        Client.options_v1_delete_all_open_orders_by_underlying.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_futures_summary(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/futuresSummary\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_futures_summary.__doc__ = Client.margin_v1_get_broker_sub_account_futures_summary.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/futuresSummary\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_sub_account_futures_summary.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_futures_summary.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_spot_summary(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/spotSummary\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_spot_summary.__doc__ = Client.margin_v1_get_broker_sub_account_spot_summary.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/subAccount/spotSummary\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_sub_account_spot_summary.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_spot_summary.__doc__\n+    )\n+\n     async def margin_v1_post_sub_account_blvt_enable(self, **params):\n-        return await self._request_margin_api(\"post\", \"sub-account/blvt/enable\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sub_account_blvt_enable.__doc__ = Client.margin_v1_post_sub_account_blvt_enable.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"sub-account/blvt/enable\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_sub_account_blvt_enable.__doc__ = (\n+        Client.margin_v1_post_sub_account_blvt_enable.__doc__\n+    )\n+\n     async def margin_v1_get_algo_spot_historical_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"algo/spot/historicalOrders\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_algo_spot_historical_orders.__doc__ = Client.margin_v1_get_algo_spot_historical_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"algo/spot/historicalOrders\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_algo_spot_historical_orders.__doc__ = (\n+        Client.margin_v1_get_algo_spot_historical_orders.__doc__\n+    )\n+\n     async def margin_v1_get_loan_vip_repay_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/vip/repay/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_vip_repay_history.__doc__ = Client.margin_v1_get_loan_vip_repay_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/vip/repay/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_vip_repay_history.__doc__ = (\n+        Client.margin_v1_get_loan_vip_repay_history.__doc__\n+    )\n+\n     async def margin_v1_get_loan_borrow_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/borrow/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_borrow_history.__doc__ = Client.margin_v1_get_loan_borrow_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/borrow/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_borrow_history.__doc__ = (\n+        Client.margin_v1_get_loan_borrow_history.__doc__\n+    )\n+\n     async def margin_v1_post_lending_auto_invest_redeem(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/auto-invest/redeem\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_auto_invest_redeem.__doc__ = Client.margin_v1_post_lending_auto_invest_redeem.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"lending/auto-invest/redeem\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_lending_auto_invest_redeem.__doc__ = (\n+        Client.margin_v1_post_lending_auto_invest_redeem.__doc__\n+    )\n+\n     async def v3_get_account(self, **params):\n-        return await self._request_api(\"get\", \"account\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"account\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def v3_delete_order(self, **params):\n-        return await self._request_api(\"delete\", \"order\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"delete\", \"order\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def futures_coin_v1_get_income_asyn(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"income/asyn\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_get_income_asyn.__doc__ = Client.futures_coin_v1_get_income_asyn.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"income/asyn\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_income_asyn.__doc__ = (\n+        Client.futures_coin_v1_get_income_asyn.__doc__\n+    )\n+\n     async def margin_v1_post_managed_subaccount_deposit(self, **params):\n-        return await self._request_margin_api(\"post\", \"managed-subaccount/deposit\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_managed_subaccount_deposit.__doc__ = Client.margin_v1_post_managed_subaccount_deposit.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"managed-subaccount/deposit\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_managed_subaccount_deposit.__doc__ = (\n+        Client.margin_v1_post_managed_subaccount_deposit.__doc__\n+    )\n+\n     async def margin_v1_post_lending_daily_purchase(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/daily/purchase\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_daily_purchase.__doc__ = Client.margin_v1_post_lending_daily_purchase.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"lending/daily/purchase\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_lending_daily_purchase.__doc__ = (\n+        Client.margin_v1_post_lending_daily_purchase.__doc__\n+    )\n+\n     async def futures_v1_get_trade_asyn_id(self, **params):\n-        return await self._request_futures_api(\"get\", \"trade/asyn/id\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"trade/asyn/id\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_trade_asyn_id.__doc__ = Client.futures_v1_get_trade_asyn_id.__doc__\n-            \n-    async def margin_v1_delete_sub_account_sub_account_api_ip_restriction_ip_list(self, **params):\n-        return await self._request_margin_api(\"delete\", \"sub-account/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n-        \n+\n+    async def margin_v1_delete_sub_account_sub_account_api_ip_restriction_ip_list(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"delete\",\n+            \"sub-account/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n     margin_v1_delete_sub_account_sub_account_api_ip_restriction_ip_list.__doc__ = Client.margin_v1_delete_sub_account_sub_account_api_ip_restriction_ip_list.__doc__\n-            \n+\n     async def margin_v1_get_copy_trading_futures_user_status(self, **params):\n-        return await self._request_margin_api(\"get\", \"copyTrading/futures/userStatus\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_copy_trading_futures_user_status.__doc__ = Client.margin_v1_get_copy_trading_futures_user_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"copyTrading/futures/userStatus\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_copy_trading_futures_user_status.__doc__ = (\n+        Client.margin_v1_get_copy_trading_futures_user_status.__doc__\n+    )\n+\n     async def options_v1_get_margin_account(self, **params):\n-        return await self._request_options_api(\"get\", \"marginAccount\", signed=True, data=params)\n-        \n+        return await self._request_options_api(\n+            \"get\", \"marginAccount\", signed=True, data=params\n+        )\n+\n     options_v1_get_margin_account.__doc__ = Client.options_v1_get_margin_account.__doc__\n-            \n+\n     async def margin_v1_post_localentity_withdraw_apply(self, **params):\n-        return await self._request_margin_api(\"post\", \"localentity/withdraw/apply\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_localentity_withdraw_apply.__doc__ = Client.margin_v1_post_localentity_withdraw_apply.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"localentity/withdraw/apply\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_localentity_withdraw_apply.__doc__ = (\n+        Client.margin_v1_post_localentity_withdraw_apply.__doc__\n+    )\n+\n     async def v3_put_user_data_stream(self, **params):\n-        return await self._request_api(\"put\", \"userDataStream\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"put\", \"userDataStream\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_asset_wallet_balance(self, **params):\n-        return await self._request_margin_api(\"get\", \"asset/wallet/balance\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_asset_wallet_balance.__doc__ = Client.margin_v1_get_asset_wallet_balance.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"asset/wallet/balance\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_asset_wallet_balance.__doc__ = (\n+        Client.margin_v1_get_asset_wallet_balance.__doc__\n+    )\n+\n     async def margin_v1_post_broker_transfer(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/transfer\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_transfer.__doc__ = Client.margin_v1_post_broker_transfer.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"broker/transfer\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_broker_transfer.__doc__ = (\n+        Client.margin_v1_post_broker_transfer.__doc__\n+    )\n+\n     async def margin_v1_post_lending_customized_fixed_purchase(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/customizedFixed/purchase\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_customized_fixed_purchase.__doc__ = Client.margin_v1_post_lending_customized_fixed_purchase.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"lending/customizedFixed/purchase\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_lending_customized_fixed_purchase.__doc__ = (\n+        Client.margin_v1_post_lending_customized_fixed_purchase.__doc__\n+    )\n+\n     async def margin_v1_post_algo_futures_new_order_twap(self, **params):\n-        return await self._request_margin_api(\"post\", \"algo/futures/newOrderTwap\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_algo_futures_new_order_twap.__doc__ = Client.margin_v1_post_algo_futures_new_order_twap.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"algo/futures/newOrderTwap\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_algo_futures_new_order_twap.__doc__ = (\n+        Client.margin_v1_post_algo_futures_new_order_twap.__doc__\n+    )\n+\n     async def margin_v2_post_eth_staking_eth_stake(self, **params):\n-        return await self._request_margin_api(\"post\", \"eth-staking/eth/stake\", signed=True, data=params, version=2)\n-        \n-    margin_v2_post_eth_staking_eth_stake.__doc__ = Client.margin_v2_post_eth_staking_eth_stake.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"eth-staking/eth/stake\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_post_eth_staking_eth_stake.__doc__ = (\n+        Client.margin_v2_post_eth_staking_eth_stake.__doc__\n+    )\n+\n     async def margin_v1_post_loan_flexible_repay_history(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/flexible/repay/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_loan_flexible_repay_history.__doc__ = Client.margin_v1_post_loan_flexible_repay_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/flexible/repay/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_loan_flexible_repay_history.__doc__ = (\n+        Client.margin_v1_post_loan_flexible_repay_history.__doc__\n+    )\n+\n     async def v3_post_user_data_stream(self, **params):\n-        return await self._request_api(\"post\", \"userDataStream\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"userDataStream\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_lending_auto_invest_index_info(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/index/info\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_index_info.__doc__ = Client.margin_v1_get_lending_auto_invest_index_info.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"lending/auto-invest/index/info\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_lending_auto_invest_index_info.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_index_info.__doc__\n+    )\n+\n     async def margin_v1_get_sol_staking_sol_history_redemption_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/history/redemptionHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_history_redemption_history.__doc__ = Client.margin_v1_get_sol_staking_sol_history_redemption_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/redemptionHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sol_staking_sol_history_redemption_history.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_history_redemption_history.__doc__\n+    )\n+\n     async def margin_v1_get_broker_rebate_futures_recent_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/rebate/futures/recentRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_rebate_futures_recent_record.__doc__ = Client.margin_v1_get_broker_rebate_futures_recent_record.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/rebate/futures/recentRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_rebate_futures_recent_record.__doc__ = (\n+        Client.margin_v1_get_broker_rebate_futures_recent_record.__doc__\n+    )\n+\n     async def margin_v3_get_broker_sub_account_futures_summary(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/futuresSummary\", signed=True, data=params, version=3)\n-        \n-    margin_v3_get_broker_sub_account_futures_summary.__doc__ = Client.margin_v3_get_broker_sub_account_futures_summary.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/futuresSummary\",\n+            signed=True,\n+            data=params,\n+            version=3,\n+        )\n+\n+    margin_v3_get_broker_sub_account_futures_summary.__doc__ = (\n+        Client.margin_v3_get_broker_sub_account_futures_summary.__doc__\n+    )\n+\n     async def margin_v1_post_margin_manual_liquidation(self, **params):\n-        return await self._request_margin_api(\"post\", \"margin/manual-liquidation\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"post\", \"margin/manual-liquidation\", signed=True, data=params, version=1\n+        )\n+\n     async def margin_v1_get_lending_auto_invest_target_asset_roi_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/target-asset/roi/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_target_asset_roi_list.__doc__ = Client.margin_v1_get_lending_auto_invest_target_asset_roi_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/target-asset/roi/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_target_asset_roi_list.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_target_asset_roi_list.__doc__\n+    )\n+\n     async def margin_v1_get_broker_universal_transfer(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/universalTransfer\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_universal_transfer.__doc__ = Client.margin_v1_get_broker_universal_transfer.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/universalTransfer\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_universal_transfer.__doc__ = (\n+        Client.margin_v1_get_broker_universal_transfer.__doc__\n+    )\n+\n     async def futures_v1_put_batch_orders(self, **params):\n-        return await self._request_futures_api(\"put\", \"batchOrders\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"put\", \"batchOrders\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_put_batch_orders.__doc__ = Client.futures_v1_put_batch_orders.__doc__\n-            \n+\n     async def options_v1_post_countdown_cancel_all_heart_beat(self, **params):\n-        return await self._request_options_api(\"post\", \"countdownCancelAllHeartBeat\", signed=True, data=params)\n-        \n-    options_v1_post_countdown_cancel_all_heart_beat.__doc__ = Client.options_v1_post_countdown_cancel_all_heart_beat.__doc__\n-            \n+        return await self._request_options_api(\n+            \"post\", \"countdownCancelAllHeartBeat\", signed=True, data=params\n+        )\n+\n+    options_v1_post_countdown_cancel_all_heart_beat.__doc__ = (\n+        Client.options_v1_post_countdown_cancel_all_heart_beat.__doc__\n+    )\n+\n     async def margin_v1_get_loan_collateral_data(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/collateral/data\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_collateral_data.__doc__ = Client.margin_v1_get_loan_collateral_data.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/collateral/data\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_collateral_data.__doc__ = (\n+        Client.margin_v1_get_loan_collateral_data.__doc__\n+    )\n+\n     async def margin_v1_get_loan_repay_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/repay/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_repay_history.__doc__ = Client.margin_v1_get_loan_repay_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/repay/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_repay_history.__doc__ = (\n+        Client.margin_v1_get_loan_repay_history.__doc__\n+    )\n+\n     async def margin_v1_post_convert_limit_place_order(self, **params):\n-        return await self._request_margin_api(\"post\", \"convert/limit/placeOrder\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_convert_limit_place_order.__doc__ = Client.margin_v1_post_convert_limit_place_order.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"convert/limit/placeOrder\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_convert_limit_place_order.__doc__ = (\n+        Client.margin_v1_post_convert_limit_place_order.__doc__\n+    )\n+\n     async def futures_v1_get_convert_exchange_info(self, **params):\n-        return await self._request_futures_api(\"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1)\n-        \n-    futures_v1_get_convert_exchange_info.__doc__ = Client.futures_v1_get_convert_exchange_info.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1\n+        )\n+\n+    futures_v1_get_convert_exchange_info.__doc__ = (\n+        Client.futures_v1_get_convert_exchange_info.__doc__\n+    )\n+\n     async def v3_get_all_order_list(self, **params):\n-        return await self._request_api(\"get\", \"allOrderList\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"allOrderList\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_get_all_order_list.__doc__ = Client.v3_get_all_order_list.__doc__\n-            \n-    async def margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list(self, **params):\n-        return await self._request_margin_api(\"delete\", \"broker/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list.__doc__ = Client.margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list.__doc__\n-            \n+\n+    async def margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"delete\",\n+            \"broker/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list.__doc__ = (\n+        Client.margin_v1_delete_broker_sub_account_api_ip_restriction_ip_list.__doc__\n+    )\n+\n     async def margin_v1_post_sub_account_virtual_sub_account(self, **params):\n-        return await self._request_margin_api(\"post\", \"sub-account/virtualSubAccount\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sub_account_virtual_sub_account.__doc__ = Client.margin_v1_post_sub_account_virtual_sub_account.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"sub-account/virtualSubAccount\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_sub_account_virtual_sub_account.__doc__ = (\n+        Client.margin_v1_post_sub_account_virtual_sub_account.__doc__\n+    )\n+\n     async def margin_v1_put_localentity_deposit_provide_info(self, **params):\n-        return await self._request_margin_api(\"put\", \"localentity/deposit/provide-info\", signed=True, data=params, version=1)\n-        \n-    margin_v1_put_localentity_deposit_provide_info.__doc__ = Client.margin_v1_put_localentity_deposit_provide_info.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"put\",\n+            \"localentity/deposit/provide-info\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_put_localentity_deposit_provide_info.__doc__ = (\n+        Client.margin_v1_put_localentity_deposit_provide_info.__doc__\n+    )\n+\n     async def margin_v1_post_portfolio_mint(self, **params):\n-        return await self._request_margin_api(\"post\", \"portfolio/mint\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"post\", \"portfolio/mint\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_post_portfolio_mint.__doc__ = Client.margin_v1_post_portfolio_mint.__doc__\n-            \n+\n     async def futures_v1_get_order_amendment(self, **params):\n-        return await self._request_futures_api(\"get\", \"orderAmendment\", signed=True, data=params, version=1)\n-        \n-    futures_v1_get_order_amendment.__doc__ = Client.futures_v1_get_order_amendment.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"get\", \"orderAmendment\", signed=True, data=params, version=1\n+        )\n+\n+    futures_v1_get_order_amendment.__doc__ = (\n+        Client.futures_v1_get_order_amendment.__doc__\n+    )\n+\n     async def margin_v1_post_sol_staking_sol_claim(self, **params):\n-        return await self._request_margin_api(\"post\", \"sol-staking/sol/claim\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sol_staking_sol_claim.__doc__ = Client.margin_v1_post_sol_staking_sol_claim.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"sol-staking/sol/claim\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_sol_staking_sol_claim.__doc__ = (\n+        Client.margin_v1_post_sol_staking_sol_claim.__doc__\n+    )\n+\n     async def margin_v1_post_lending_daily_redeem(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/daily/redeem\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_daily_redeem.__doc__ = Client.margin_v1_post_lending_daily_redeem.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"lending/daily/redeem\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_lending_daily_redeem.__doc__ = (\n+        Client.margin_v1_post_lending_daily_redeem.__doc__\n+    )\n+\n     async def margin_v1_post_mining_hash_transfer_config(self, **params):\n-        return await self._request_margin_api(\"post\", \"mining/hash-transfer/config\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_mining_hash_transfer_config.__doc__ = Client.margin_v1_post_mining_hash_transfer_config.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"mining/hash-transfer/config\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_mining_hash_transfer_config.__doc__ = (\n+        Client.margin_v1_post_mining_hash_transfer_config.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_rebalance_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/rebalance/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_rebalance_history.__doc__ = Client.margin_v1_get_lending_auto_invest_rebalance_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/rebalance/history\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_rebalance_history.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_rebalance_history.__doc__\n+    )\n+\n     async def margin_v1_get_loan_repay_collateral_rate(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/repay/collateral/rate\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_repay_collateral_rate.__doc__ = Client.margin_v1_get_loan_repay_collateral_rate.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/repay/collateral/rate\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_repay_collateral_rate.__doc__ = (\n+        Client.margin_v1_get_loan_repay_collateral_rate.__doc__\n+    )\n+\n     async def futures_v1_get_income_asyn(self, **params):\n-        return await self._request_futures_api(\"get\", \"income/asyn\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"income/asyn\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_income_asyn.__doc__ = Client.futures_v1_get_income_asyn.__doc__\n-            \n+\n     async def margin_v1_get_mining_payment_uid(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/payment/uid\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_payment_uid.__doc__ = Client.margin_v1_get_mining_payment_uid.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/payment/uid\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_payment_uid.__doc__ = (\n+        Client.margin_v1_get_mining_payment_uid.__doc__\n+    )\n+\n     async def margin_v2_get_loan_flexible_borrow_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/borrow/history\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_borrow_history.__doc__ = Client.margin_v2_get_loan_flexible_borrow_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/flexible/borrow/history\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_loan_flexible_borrow_history.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_borrow_history.__doc__\n+    )\n+\n     async def v3_get_order(self, **params):\n-        return await self._request_api(\"get\", \"order\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"order\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_capital_contract_convertible_coins(self, **params):\n-        return await self._request_margin_api(\"get\", \"capital/contract/convertible-coins\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_capital_contract_convertible_coins.__doc__ = Client.margin_v1_get_capital_contract_convertible_coins.__doc__\n-            \n-    async def margin_v1_post_broker_sub_account_api_permission_vanilla_options(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/permission/vanillaOptions\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_permission_vanilla_options.__doc__ = Client.margin_v1_post_broker_sub_account_api_permission_vanilla_options.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"capital/contract/convertible-coins\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_capital_contract_convertible_coins.__doc__ = (\n+        Client.margin_v1_get_capital_contract_convertible_coins.__doc__\n+    )\n+\n+    async def margin_v1_post_broker_sub_account_api_permission_vanilla_options(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/permission/vanillaOptions\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_permission_vanilla_options.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_permission_vanilla_options.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_redeem_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/redeem/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_redeem_history.__doc__ = Client.margin_v1_get_lending_auto_invest_redeem_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/redeem/history\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_redeem_history.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_redeem_history.__doc__\n+    )\n+\n     async def margin_v2_get_localentity_withdraw_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"localentity/withdraw/history\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_localentity_withdraw_history.__doc__ = Client.margin_v2_get_localentity_withdraw_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"localentity/withdraw/history\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_localentity_withdraw_history.__doc__ = (\n+        Client.margin_v2_get_localentity_withdraw_history.__doc__\n+    )\n+\n     async def margin_v1_get_eth_staking_eth_history_redemption_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"eth-staking/eth/history/redemptionHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_eth_staking_eth_history_redemption_history.__doc__ = Client.margin_v1_get_eth_staking_eth_history_redemption_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"eth-staking/eth/history/redemptionHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_eth_staking_eth_history_redemption_history.__doc__ = (\n+        Client.margin_v1_get_eth_staking_eth_history_redemption_history.__doc__\n+    )\n+\n     async def futures_v1_get_fee_burn(self, **params):\n-        return await self._request_futures_api(\"get\", \"feeBurn\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"feeBurn\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_fee_burn.__doc__ = Client.futures_v1_get_fee_burn.__doc__\n-            \n+\n     async def margin_v1_get_lending_auto_invest_index_user_summary(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/index/user-summary\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_index_user_summary.__doc__ = Client.margin_v1_get_lending_auto_invest_index_user_summary.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/index/user-summary\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_index_user_summary.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_index_user_summary.__doc__\n+    )\n+\n     async def margin_v2_post_loan_flexible_borrow(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/flexible/borrow\", signed=True, data=params, version=2)\n-        \n-    margin_v2_post_loan_flexible_borrow.__doc__ = Client.margin_v2_post_loan_flexible_borrow.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/flexible/borrow\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_post_loan_flexible_borrow.__doc__ = (\n+        Client.margin_v2_post_loan_flexible_borrow.__doc__\n+    )\n+\n     async def margin_v1_post_loan_vip_repay(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/vip/repay\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"post\", \"loan/vip/repay\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_post_loan_vip_repay.__doc__ = Client.margin_v1_post_loan_vip_repay.__doc__\n-            \n+\n     async def futures_coin_v1_get_commission_rate(self, **params):\n-        return await self._request_futures_coin_api(\"get\", \"commissionRate\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_get_commission_rate.__doc__ = Client.futures_coin_v1_get_commission_rate.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"get\", \"commissionRate\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_get_commission_rate.__doc__ = (\n+        Client.futures_coin_v1_get_commission_rate.__doc__\n+    )\n+\n     async def margin_v1_get_convert_asset_info(self, **params):\n-        return await self._request_margin_api(\"get\", \"convert/assetInfo\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_convert_asset_info.__doc__ = Client.margin_v1_get_convert_asset_info.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"convert/assetInfo\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_convert_asset_info.__doc__ = (\n+        Client.margin_v1_get_convert_asset_info.__doc__\n+    )\n+\n     async def v3_post_sor_order_test(self, **params):\n-        return await self._request_api(\"post\", \"sor/order/test\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"sor/order/test\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_post_sor_order_test.__doc__ = Client.v3_post_sor_order_test.__doc__\n-            \n+\n     async def margin_v1_post_broker_universal_transfer(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/universalTransfer\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_universal_transfer.__doc__ = Client.margin_v1_post_broker_universal_transfer.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"broker/universalTransfer\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_broker_universal_transfer.__doc__ = (\n+        Client.margin_v1_post_broker_universal_transfer.__doc__\n+    )\n+\n     async def margin_v1_post_account_disable_fast_withdraw_switch(self, **params):\n-        return await self._request_margin_api(\"post\", \"account/disableFastWithdrawSwitch\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_account_disable_fast_withdraw_switch.__doc__ = Client.margin_v1_post_account_disable_fast_withdraw_switch.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"account/disableFastWithdrawSwitch\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_account_disable_fast_withdraw_switch.__doc__ = (\n+        Client.margin_v1_post_account_disable_fast_withdraw_switch.__doc__\n+    )\n+\n     async def futures_v1_get_asset_index(self, **params):\n-        return await self._request_futures_api(\"get\", \"assetIndex\", signed=False, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"assetIndex\", signed=False, data=params, version=1\n+        )\n+\n     futures_v1_get_asset_index.__doc__ = Client.futures_v1_get_asset_index.__doc__\n-            \n+\n     async def v3_get_rate_limit_order(self, **params):\n-        return await self._request_api(\"get\", \"rateLimit/order\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"rateLimit/order\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_account_api_restrictions_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"get\", \"account/apiRestrictions/ipRestriction\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_account_api_restrictions_ip_restriction.__doc__ = Client.margin_v1_get_account_api_restrictions_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"account/apiRestrictions/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_account_api_restrictions_ip_restriction.__doc__ = (\n+        Client.margin_v1_get_account_api_restrictions_ip_restriction.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_bnb_burn_spot(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccount/bnbBurn/spot\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_bnb_burn_spot.__doc__ = Client.margin_v1_post_broker_sub_account_bnb_burn_spot.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccount/bnbBurn/spot\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_bnb_burn_spot.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_bnb_burn_spot.__doc__\n+    )\n+\n     async def futures_coin_v1_put_batch_orders(self, **params):\n-        return await self._request_futures_coin_api(\"put\", \"batchOrders\", signed=True, data=params, version=1)\n-        \n-    futures_coin_v1_put_batch_orders.__doc__ = Client.futures_coin_v1_put_batch_orders.__doc__\n-            \n+        return await self._request_futures_coin_api(\n+            \"put\", \"batchOrders\", signed=True, data=params, version=1\n+        )\n+\n+    futures_coin_v1_put_batch_orders.__doc__ = (\n+        Client.futures_coin_v1_put_batch_orders.__doc__\n+    )\n+\n     async def v3_delete_open_orders(self, **params):\n-        return await self._request_api(\"delete\", \"openOrders\", signed=True, data=params, version=\"v3\")\n-        \n-    async def margin_v1_post_broker_sub_account_api_permission_universal_transfer(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/permission/universalTransfer\", signed=True, data=params, version=1)\n-        \n+        return await self._request_api(\n+            \"delete\", \"openOrders\", signed=True, data=params, version=\"v3\"\n+        )\n+\n+    async def margin_v1_post_broker_sub_account_api_permission_universal_transfer(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/permission/universalTransfer\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n     margin_v1_post_broker_sub_account_api_permission_universal_transfer.__doc__ = Client.margin_v1_post_broker_sub_account_api_permission_universal_transfer.__doc__\n-            \n+\n     async def v3_get_my_allocations(self, **params):\n-        return await self._request_api(\"get\", \"myAllocations\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"myAllocations\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_loan_ltv_adjustment_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/ltv/adjustment/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_ltv_adjustment_history.__doc__ = Client.margin_v1_get_loan_ltv_adjustment_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/ltv/adjustment/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_ltv_adjustment_history.__doc__ = (\n+        Client.margin_v1_get_loan_ltv_adjustment_history.__doc__\n+    )\n+\n     async def margin_v1_get_localentity_withdraw_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"localentity/withdraw/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_localentity_withdraw_history.__doc__ = Client.margin_v1_get_localentity_withdraw_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"localentity/withdraw/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_localentity_withdraw_history.__doc__ = (\n+        Client.margin_v1_get_localentity_withdraw_history.__doc__\n+    )\n+\n     async def margin_v2_post_sub_account_sub_account_api_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"post\", \"sub-account/subAccountApi/ipRestriction\", signed=True, data=params, version=2)\n-        \n-    margin_v2_post_sub_account_sub_account_api_ip_restriction.__doc__ = Client.margin_v2_post_sub_account_sub_account_api_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"sub-account/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n+\n+    margin_v2_post_sub_account_sub_account_api_ip_restriction.__doc__ = (\n+        Client.margin_v2_post_sub_account_sub_account_api_ip_restriction.__doc__\n+    )\n+\n     async def futures_v1_get_rate_limit_order(self, **params):\n-        return await self._request_futures_api(\"get\", \"rateLimit/order\", signed=True, data=params, version=1)\n-        \n-    futures_v1_get_rate_limit_order.__doc__ = Client.futures_v1_get_rate_limit_order.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"get\", \"rateLimit/order\", signed=True, data=params, version=1\n+        )\n+\n+    futures_v1_get_rate_limit_order.__doc__ = (\n+        Client.futures_v1_get_rate_limit_order.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_api_commission_futures(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccountApi/commission/futures\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_api_commission_futures.__doc__ = Client.margin_v1_get_broker_sub_account_api_commission_futures.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccountApi/commission/futures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_sub_account_api_commission_futures.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_api_commission_futures.__doc__\n+    )\n+\n     async def margin_v1_get_sol_staking_sol_history_staking_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/history/stakingHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_history_staking_history.__doc__ = Client.margin_v1_get_sol_staking_sol_history_staking_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/stakingHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sol_staking_sol_history_staking_history.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_history_staking_history.__doc__\n+    )\n+\n     async def futures_v1_get_open_order(self, **params):\n-        return await self._request_futures_api(\"get\", \"openOrder\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"openOrder\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_open_order.__doc__ = Client.futures_v1_get_open_order.__doc__\n-            \n+\n     async def margin_v1_delete_algo_spot_order(self, **params):\n-        return await self._request_margin_api(\"delete\", \"algo/spot/order\", signed=True, data=params, version=1)\n-        \n-    margin_v1_delete_algo_spot_order.__doc__ = Client.margin_v1_delete_algo_spot_order.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"delete\", \"algo/spot/order\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_delete_algo_spot_order.__doc__ = (\n+        Client.margin_v1_delete_algo_spot_order.__doc__\n+    )\n+\n     async def v3_post_order(self, **params):\n-        return await self._request_api(\"post\", \"order\", signed=True, data=params, version=\"v3\")\n-        \n-    async def margin_v1_delete_account_api_restrictions_ip_restriction_ip_list(self, **params):\n-        return await self._request_margin_api(\"delete\", \"account/apiRestrictions/ipRestriction/ipList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_delete_account_api_restrictions_ip_restriction_ip_list.__doc__ = Client.margin_v1_delete_account_api_restrictions_ip_restriction_ip_list.__doc__\n-            \n+        return await self._request_api(\n+            \"post\", \"order\", signed=True, data=params, version=\"v3\"\n+        )\n+\n+    async def margin_v1_delete_account_api_restrictions_ip_restriction_ip_list(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"delete\",\n+            \"account/apiRestrictions/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_delete_account_api_restrictions_ip_restriction_ip_list.__doc__ = (\n+        Client.margin_v1_delete_account_api_restrictions_ip_restriction_ip_list.__doc__\n+    )\n+\n     async def margin_v1_post_capital_contract_convertible_coins(self, **params):\n-        return await self._request_margin_api(\"post\", \"capital/contract/convertible-coins\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_capital_contract_convertible_coins.__doc__ = Client.margin_v1_post_capital_contract_convertible_coins.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"capital/contract/convertible-coins\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_capital_contract_convertible_coins.__doc__ = (\n+        Client.margin_v1_post_capital_contract_convertible_coins.__doc__\n+    )\n+\n     async def margin_v1_get_managed_subaccount_margin_asset(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/marginAsset\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_margin_asset.__doc__ = Client.margin_v1_get_managed_subaccount_margin_asset.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"managed-subaccount/marginAsset\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_managed_subaccount_margin_asset.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_margin_asset.__doc__\n+    )\n+\n     async def v3_delete_order_list(self, **params):\n-        return await self._request_api(\"delete\", \"orderList\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"delete\", \"orderList\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_delete_order_list.__doc__ = Client.v3_delete_order_list.__doc__\n-            \n-    async def margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list(self, **params):\n-        return await self._request_margin_api(\"post\", \"sub-account/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list.__doc__ = Client.margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list.__doc__\n-            \n+\n+    async def margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"sub-account/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list.__doc__ = (\n+        Client.margin_v1_post_sub_account_sub_account_api_ip_restriction_ip_list.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_api_commission(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/commission\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_commission.__doc__ = Client.margin_v1_post_broker_sub_account_api_commission.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/commission\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_commission.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_commission.__doc__\n+    )\n+\n     async def futures_v1_post_fee_burn(self, **params):\n-        return await self._request_futures_api(\"post\", \"feeBurn\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"post\", \"feeBurn\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_post_fee_burn.__doc__ = Client.futures_v1_post_fee_burn.__doc__\n-            \n+\n     async def margin_v1_get_broker_sub_account_margin_summary(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/marginSummary\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_margin_summary.__doc__ = Client.margin_v1_get_broker_sub_account_margin_summary.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/marginSummary\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_sub_account_margin_summary.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_margin_summary.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_plan_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/plan/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_plan_list.__doc__ = Client.margin_v1_get_lending_auto_invest_plan_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"lending/auto-invest/plan/list\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_lending_auto_invest_plan_list.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_plan_list.__doc__\n+    )\n+\n     async def margin_v1_get_loan_vip_loanable_data(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/vip/loanable/data\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_vip_loanable_data.__doc__ = Client.margin_v1_get_loan_vip_loanable_data.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/vip/loanable/data\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_vip_loanable_data.__doc__ = (\n+        Client.margin_v1_get_loan_vip_loanable_data.__doc__\n+    )\n+\n     async def margin_v2_get_loan_flexible_collateral_data(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/collateral/data\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_collateral_data.__doc__ = Client.margin_v2_get_loan_flexible_collateral_data.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/flexible/collateral/data\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_loan_flexible_collateral_data.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_collateral_data.__doc__\n+    )\n+\n     async def margin_v1_delete_broker_sub_account_api(self, **params):\n-        return await self._request_margin_api(\"delete\", \"broker/subAccountApi\", signed=True, data=params, version=1)\n-        \n-    margin_v1_delete_broker_sub_account_api.__doc__ = Client.margin_v1_delete_broker_sub_account_api.__doc__\n-            \n-    async def margin_v1_get_sol_staking_sol_history_bnsol_rewards_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/history/bnsolRewardsHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_history_bnsol_rewards_history.__doc__ = Client.margin_v1_get_sol_staking_sol_history_bnsol_rewards_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"delete\", \"broker/subAccountApi\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_delete_broker_sub_account_api.__doc__ = (\n+        Client.margin_v1_delete_broker_sub_account_api.__doc__\n+    )\n+\n+    async def margin_v1_get_sol_staking_sol_history_bnsol_rewards_history(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sol-staking/sol/history/bnsolRewardsHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sol_staking_sol_history_bnsol_rewards_history.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_history_bnsol_rewards_history.__doc__\n+    )\n+\n     async def margin_v1_get_convert_limit_query_open_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"convert/limit/queryOpenOrders\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_convert_limit_query_open_orders.__doc__ = Client.margin_v1_get_convert_limit_query_open_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"convert/limit/queryOpenOrders\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_convert_limit_query_open_orders.__doc__ = (\n+        Client.margin_v1_get_convert_limit_query_open_orders.__doc__\n+    )\n+\n     async def v3_get_account_commission(self, **params):\n-        return await self._request_api(\"get\", \"account/commission\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"account/commission\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_get_account_commission.__doc__ = Client.v3_get_account_commission.__doc__\n-            \n+\n     async def v3_post_order_list_oco(self, **params):\n-        return await self._request_api(\"post\", \"orderList/oco\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"post\", \"orderList/oco\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_managed_subaccount_query_trans_log(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/query-trans-log\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_query_trans_log.__doc__ = Client.margin_v1_get_managed_subaccount_query_trans_log.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/query-trans-log\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_managed_subaccount_query_trans_log.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_query_trans_log.__doc__\n+    )\n+\n     async def margin_v2_post_broker_sub_account_api_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/ipRestriction\", signed=True, data=params, version=2)\n-        \n-    margin_v2_post_broker_sub_account_api_ip_restriction.__doc__ = Client.margin_v2_post_broker_sub_account_api_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n+\n+    margin_v2_post_broker_sub_account_api_ip_restriction.__doc__ = (\n+        Client.margin_v2_post_broker_sub_account_api_ip_restriction.__doc__\n+    )\n+\n     async def margin_v1_get_lending_auto_invest_all_asset(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/all/asset\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_all_asset.__doc__ = Client.margin_v1_get_lending_auto_invest_all_asset.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"lending/auto-invest/all/asset\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_lending_auto_invest_all_asset.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_all_asset.__doc__\n+    )\n+\n     async def futures_v1_post_convert_accept_quote(self, **params):\n-        return await self._request_futures_api(\"post\", \"convert/acceptQuote\", signed=True, data=params, version=1)\n-        \n-    futures_v1_post_convert_accept_quote.__doc__ = Client.futures_v1_post_convert_accept_quote.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"post\", \"convert/acceptQuote\", signed=True, data=params, version=1\n+        )\n+\n+    futures_v1_post_convert_accept_quote.__doc__ = (\n+        Client.futures_v1_post_convert_accept_quote.__doc__\n+    )\n+\n     async def margin_v1_get_spot_delist_schedule(self, **params):\n-        return await self._request_margin_api(\"get\", \"spot/delist-schedule\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_spot_delist_schedule.__doc__ = Client.margin_v1_get_spot_delist_schedule.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"spot/delist-schedule\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_spot_delist_schedule.__doc__ = (\n+        Client.margin_v1_get_spot_delist_schedule.__doc__\n+    )\n+\n     async def margin_v1_post_account_api_restrictions_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"post\", \"account/apiRestrictions/ipRestriction\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_account_api_restrictions_ip_restriction.__doc__ = Client.margin_v1_post_account_api_restrictions_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"account/apiRestrictions/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_account_api_restrictions_ip_restriction.__doc__ = (\n+        Client.margin_v1_post_account_api_restrictions_ip_restriction.__doc__\n+    )\n+\n     async def margin_v1_get_dci_product_accounts(self, **params):\n-        return await self._request_margin_api(\"get\", \"dci/product/accounts\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_dci_product_accounts.__doc__ = Client.margin_v1_get_dci_product_accounts.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"dci/product/accounts\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_dci_product_accounts.__doc__ = (\n+        Client.margin_v1_get_dci_product_accounts.__doc__\n+    )\n+\n     async def margin_v1_get_sub_account_sub_account_api_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"get\", \"sub-account/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sub_account_sub_account_api_ip_restriction.__doc__ = Client.margin_v1_get_sub_account_sub_account_api_ip_restriction.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sub-account/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sub_account_sub_account_api_ip_restriction.__doc__ = (\n+        Client.margin_v1_get_sub_account_sub_account_api_ip_restriction.__doc__\n+    )\n+\n     async def margin_v1_get_sub_account_transaction_statistics(self, **params):\n-        return await self._request_margin_api(\"get\", \"sub-account/transaction-statistics\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sub_account_transaction_statistics.__doc__ = Client.margin_v1_get_sub_account_transaction_statistics.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"sub-account/transaction-statistics\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_sub_account_transaction_statistics.__doc__ = (\n+        Client.margin_v1_get_sub_account_transaction_statistics.__doc__\n+    )\n+\n     async def margin_v1_get_managed_subaccount_deposit_address(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/deposit/address\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_deposit_address.__doc__ = Client.margin_v1_get_managed_subaccount_deposit_address.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/deposit/address\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_managed_subaccount_deposit_address.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_deposit_address.__doc__\n+    )\n+\n     async def margin_v2_get_portfolio_account(self, **params):\n-        return await self._request_margin_api(\"get\", \"portfolio/account\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_portfolio_account.__doc__ = Client.margin_v2_get_portfolio_account.__doc__\n-            \n-    async def margin_v1_get_simple_earn_locked_history_redemption_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"simple-earn/locked/history/redemptionRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_simple_earn_locked_history_redemption_record.__doc__ = Client.margin_v1_get_simple_earn_locked_history_redemption_record.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"portfolio/account\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_portfolio_account.__doc__ = (\n+        Client.margin_v2_get_portfolio_account.__doc__\n+    )\n+\n+    async def margin_v1_get_simple_earn_locked_history_redemption_record(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/locked/history/redemptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_simple_earn_locked_history_redemption_record.__doc__ = (\n+        Client.margin_v1_get_simple_earn_locked_history_redemption_record.__doc__\n+    )\n+\n     async def futures_v1_get_order_asyn_id(self, **params):\n-        return await self._request_futures_api(\"get\", \"order/asyn/id\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"get\", \"order/asyn/id\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_get_order_asyn_id.__doc__ = Client.futures_v1_get_order_asyn_id.__doc__\n-            \n+\n     async def margin_v1_post_managed_subaccount_withdraw(self, **params):\n-        return await self._request_margin_api(\"post\", \"managed-subaccount/withdraw\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_managed_subaccount_withdraw.__doc__ = Client.margin_v1_post_managed_subaccount_withdraw.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"managed-subaccount/withdraw\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_managed_subaccount_withdraw.__doc__ = (\n+        Client.margin_v1_post_managed_subaccount_withdraw.__doc__\n+    )\n+\n     async def margin_v1_get_localentity_deposit_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"localentity/deposit/history\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_localentity_deposit_history.__doc__ = Client.margin_v1_get_localentity_deposit_history.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"localentity/deposit/history\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_localentity_deposit_history.__doc__ = (\n+        Client.margin_v1_get_localentity_deposit_history.__doc__\n+    )\n+\n     async def margin_v1_post_eth_staking_wbeth_wrap(self, **params):\n-        return await self._request_margin_api(\"post\", \"eth-staking/wbeth/wrap\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_eth_staking_wbeth_wrap.__doc__ = Client.margin_v1_post_eth_staking_wbeth_wrap.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"eth-staking/wbeth/wrap\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_eth_staking_wbeth_wrap.__doc__ = (\n+        Client.margin_v1_post_eth_staking_wbeth_wrap.__doc__\n+    )\n+\n     async def margin_v1_post_simple_earn_locked_set_redeem_option(self, **params):\n-        return await self._request_margin_api(\"post\", \"simple-earn/locked/setRedeemOption\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_simple_earn_locked_set_redeem_option.__doc__ = Client.margin_v1_post_simple_earn_locked_set_redeem_option.__doc__\n-            \n-    async def margin_v1_post_broker_sub_account_api_ip_restriction_ip_list(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/ipRestriction/ipList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_ip_restriction_ip_list.__doc__ = Client.margin_v1_post_broker_sub_account_api_ip_restriction_ip_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"simple-earn/locked/setRedeemOption\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_simple_earn_locked_set_redeem_option.__doc__ = (\n+        Client.margin_v1_post_simple_earn_locked_set_redeem_option.__doc__\n+    )\n+\n+    async def margin_v1_post_broker_sub_account_api_ip_restriction_ip_list(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/ipRestriction/ipList\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_ip_restriction_ip_list.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_ip_restriction_ip_list.__doc__\n+    )\n+\n     async def margin_v1_post_broker_sub_account_api_commission_futures(self, **params):\n-        return await self._request_margin_api(\"post\", \"broker/subAccountApi/commission/futures\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_broker_sub_account_api_commission_futures.__doc__ = Client.margin_v1_post_broker_sub_account_api_commission_futures.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"broker/subAccountApi/commission/futures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_broker_sub_account_api_commission_futures.__doc__ = (\n+        Client.margin_v1_post_broker_sub_account_api_commission_futures.__doc__\n+    )\n+\n     async def v3_get_open_orders(self, **params):\n-        return await self._request_api(\"get\", \"openOrders\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"openOrders\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_lending_auto_invest_history_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/history/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_history_list.__doc__ = Client.margin_v1_get_lending_auto_invest_history_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/history/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_history_list.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_history_list.__doc__\n+    )\n+\n     async def margin_v1_post_loan_customize_margin_call(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/customize/margin_call\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_loan_customize_margin_call.__doc__ = Client.margin_v1_post_loan_customize_margin_call.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"loan/customize/margin_call\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_loan_customize_margin_call.__doc__ = (\n+        Client.margin_v1_post_loan_customize_margin_call.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_bnb_burn_status(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/bnbBurn/status\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_bnb_burn_status.__doc__ = Client.margin_v1_get_broker_sub_account_bnb_burn_status.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/bnbBurn/status\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_sub_account_bnb_burn_status.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_bnb_burn_status.__doc__\n+    )\n+\n     async def margin_v1_get_managed_subaccount_account_snapshot(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/accountSnapshot\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_account_snapshot.__doc__ = Client.margin_v1_get_managed_subaccount_account_snapshot.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/accountSnapshot\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_managed_subaccount_account_snapshot.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_account_snapshot.__doc__\n+    )\n+\n     async def margin_v1_post_asset_convert_transfer(self, **params):\n-        return await self._request_margin_api(\"post\", \"asset/convert-transfer\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_asset_convert_transfer.__doc__ = Client.margin_v1_post_asset_convert_transfer.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"asset/convert-transfer\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_asset_convert_transfer.__doc__ = (\n+        Client.margin_v1_post_asset_convert_transfer.__doc__\n+    )\n+\n     async def options_v1_get_income_asyn(self, **params):\n-        return await self._request_options_api(\"get\", \"income/asyn\", signed=True, data=params)\n-        \n+        return await self._request_options_api(\n+            \"get\", \"income/asyn\", signed=True, data=params\n+        )\n+\n     options_v1_get_income_asyn.__doc__ = Client.options_v1_get_income_asyn.__doc__\n-            \n-    async def margin_v1_get_broker_sub_account_api_commission_coin_futures(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccountApi/commission/coinFutures\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_api_commission_coin_futures.__doc__ = Client.margin_v1_get_broker_sub_account_api_commission_coin_futures.__doc__\n-            \n+\n+    async def margin_v1_get_broker_sub_account_api_commission_coin_futures(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccountApi/commission/coinFutures\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_broker_sub_account_api_commission_coin_futures.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_api_commission_coin_futures.__doc__\n+    )\n+\n     async def margin_v2_get_broker_sub_account_futures_summary(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccount/futuresSummary\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_broker_sub_account_futures_summary.__doc__ = Client.margin_v2_get_broker_sub_account_futures_summary.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"broker/subAccount/futuresSummary\",\n+            signed=True,\n+            data=params,\n+            version=2,\n+        )\n+\n+    margin_v2_get_broker_sub_account_futures_summary.__doc__ = (\n+        Client.margin_v2_get_broker_sub_account_futures_summary.__doc__\n+    )\n+\n     async def margin_v1_get_loan_ongoing_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/ongoing/orders\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_loan_ongoing_orders.__doc__ = Client.margin_v1_get_loan_ongoing_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/ongoing/orders\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_loan_ongoing_orders.__doc__ = (\n+        Client.margin_v1_get_loan_ongoing_orders.__doc__\n+    )\n+\n     async def margin_v2_get_loan_flexible_ongoing_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"loan/flexible/ongoing/orders\", signed=True, data=params, version=2)\n-        \n-    margin_v2_get_loan_flexible_ongoing_orders.__doc__ = Client.margin_v2_get_loan_flexible_ongoing_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"loan/flexible/ongoing/orders\", signed=True, data=params, version=2\n+        )\n+\n+    margin_v2_get_loan_flexible_ongoing_orders.__doc__ = (\n+        Client.margin_v2_get_loan_flexible_ongoing_orders.__doc__\n+    )\n+\n     async def margin_v1_post_algo_futures_new_order_vp(self, **params):\n-        return await self._request_margin_api(\"post\", \"algo/futures/newOrderVp\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_algo_futures_new_order_vp.__doc__ = Client.margin_v1_post_algo_futures_new_order_vp.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"algo/futures/newOrderVp\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_algo_futures_new_order_vp.__doc__ = (\n+        Client.margin_v1_post_algo_futures_new_order_vp.__doc__\n+    )\n+\n     async def futures_v1_post_convert_get_quote(self, **params):\n-        return await self._request_futures_api(\"post\", \"convert/getQuote\", signed=True, data=params, version=1)\n-        \n-    futures_v1_post_convert_get_quote.__doc__ = Client.futures_v1_post_convert_get_quote.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"post\", \"convert/getQuote\", signed=True, data=params, version=1\n+        )\n+\n+    futures_v1_post_convert_get_quote.__doc__ = (\n+        Client.futures_v1_post_convert_get_quote.__doc__\n+    )\n+\n     async def margin_v1_get_algo_spot_sub_orders(self, **params):\n-        return await self._request_margin_api(\"get\", \"algo/spot/subOrders\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_algo_spot_sub_orders.__doc__ = Client.margin_v1_get_algo_spot_sub_orders.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"algo/spot/subOrders\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_algo_spot_sub_orders.__doc__ = (\n+        Client.margin_v1_get_algo_spot_sub_orders.__doc__\n+    )\n+\n     async def margin_v1_post_portfolio_redeem(self, **params):\n-        return await self._request_margin_api(\"post\", \"portfolio/redeem\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_portfolio_redeem.__doc__ = Client.margin_v1_post_portfolio_redeem.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"portfolio/redeem\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_portfolio_redeem.__doc__ = (\n+        Client.margin_v1_post_portfolio_redeem.__doc__\n+    )\n+\n     async def margin_v1_post_lending_auto_invest_plan_add(self, **params):\n-        return await self._request_margin_api(\"post\", \"lending/auto-invest/plan/add\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_lending_auto_invest_plan_add.__doc__ = Client.margin_v1_post_lending_auto_invest_plan_add.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"lending/auto-invest/plan/add\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_lending_auto_invest_plan_add.__doc__ = (\n+        Client.margin_v1_post_lending_auto_invest_plan_add.__doc__\n+    )\n+\n     async def v3_get_order_list(self, **params):\n-        return await self._request_api(\"get\", \"orderList\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"orderList\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     v3_get_order_list.__doc__ = Client.v3_get_order_list.__doc__\n-            \n+\n     async def v3_get_my_trades(self, **params):\n-        return await self._request_api(\"get\", \"myTrades\", signed=True, data=params, version=\"v3\")\n-        \n+        return await self._request_api(\n+            \"get\", \"myTrades\", signed=True, data=params, version=\"v3\"\n+        )\n+\n     async def margin_v1_get_lending_auto_invest_source_asset_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"lending/auto-invest/source-asset/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_lending_auto_invest_source_asset_list.__doc__ = Client.margin_v1_get_lending_auto_invest_source_asset_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\",\n+            \"lending/auto-invest/source-asset/list\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_lending_auto_invest_source_asset_list.__doc__ = (\n+        Client.margin_v1_get_lending_auto_invest_source_asset_list.__doc__\n+    )\n+\n     async def margin_v1_get_margin_all_order_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"margin/allOrderList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_margin_all_order_list.__doc__ = Client.margin_v1_get_margin_all_order_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"margin/allOrderList\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_margin_all_order_list.__doc__ = (\n+        Client.margin_v1_get_margin_all_order_list.__doc__\n+    )\n+\n     async def margin_v1_post_eth_staking_eth_redeem(self, **params):\n-        return await self._request_margin_api(\"post\", \"eth-staking/eth/redeem\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_eth_staking_eth_redeem.__doc__ = Client.margin_v1_post_eth_staking_eth_redeem.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\", \"eth-staking/eth/redeem\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_post_eth_staking_eth_redeem.__doc__ = (\n+        Client.margin_v1_post_eth_staking_eth_redeem.__doc__\n+    )\n+\n     async def margin_v1_get_broker_rebate_historical_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/rebate/historicalRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_rebate_historical_record.__doc__ = Client.margin_v1_get_broker_rebate_historical_record.__doc__\n-            \n-    async def margin_v1_get_simple_earn_locked_history_subscription_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"simple-earn/locked/history/subscriptionRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_simple_earn_locked_history_subscription_record.__doc__ = Client.margin_v1_get_simple_earn_locked_history_subscription_record.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/rebate/historicalRecord\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_rebate_historical_record.__doc__ = (\n+        Client.margin_v1_get_broker_rebate_historical_record.__doc__\n+    )\n+\n+    async def margin_v1_get_simple_earn_locked_history_subscription_record(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/locked/history/subscriptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_simple_earn_locked_history_subscription_record.__doc__ = (\n+        Client.margin_v1_get_simple_earn_locked_history_subscription_record.__doc__\n+    )\n+\n     async def futures_coin_v1_put_order(self, **params):\n-        return await self._request_futures_coin_api(\"put\", \"order\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_coin_api(\n+            \"put\", \"order\", signed=True, data=params, version=1\n+        )\n+\n     futures_coin_v1_put_order.__doc__ = Client.futures_coin_v1_put_order.__doc__\n-            \n+\n     async def margin_v1_get_managed_subaccount_asset(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/asset\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_asset.__doc__ = Client.margin_v1_get_managed_subaccount_asset.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"managed-subaccount/asset\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_managed_subaccount_asset.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_asset.__doc__\n+    )\n+\n     async def margin_v1_get_sol_staking_sol_quota(self, **params):\n-        return await self._request_margin_api(\"get\", \"sol-staking/sol/quota\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_sol_staking_sol_quota.__doc__ = Client.margin_v1_get_sol_staking_sol_quota.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"sol-staking/sol/quota\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_sol_staking_sol_quota.__doc__ = (\n+        Client.margin_v1_get_sol_staking_sol_quota.__doc__\n+    )\n+\n     async def margin_v1_post_loan_vip_renew(self, **params):\n-        return await self._request_margin_api(\"post\", \"loan/vip/renew\", signed=True, data=params, version=1)\n-        \n+        return await self._request_margin_api(\n+            \"post\", \"loan/vip/renew\", signed=True, data=params, version=1\n+        )\n+\n     margin_v1_post_loan_vip_renew.__doc__ = Client.margin_v1_post_loan_vip_renew.__doc__\n-            \n-    async def margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent(self, **params):\n-        return await self._request_margin_api(\"get\", \"managed-subaccount/queryTransLogForTradeParent\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent.__doc__ = Client.margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent.__doc__\n-            \n+\n+    async def margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"managed-subaccount/queryTransLogForTradeParent\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent.__doc__ = (\n+        Client.margin_v1_get_managed_subaccount_query_trans_log_for_trade_parent.__doc__\n+    )\n+\n     async def margin_v1_post_sub_account_sub_account_api_ip_restriction(self, **params):\n-        return await self._request_margin_api(\"post\", \"sub-account/subAccountApi/ipRestriction\", signed=True, data=params, version=1)\n-        \n-    margin_v1_post_sub_account_sub_account_api_ip_restriction.__doc__ = Client.margin_v1_post_sub_account_sub_account_api_ip_restriction.__doc__\n-            \n-    async def margin_v1_get_simple_earn_flexible_history_redemption_record(self, **params):\n-        return await self._request_margin_api(\"get\", \"simple-earn/flexible/history/redemptionRecord\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_simple_earn_flexible_history_redemption_record.__doc__ = Client.margin_v1_get_simple_earn_flexible_history_redemption_record.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"post\",\n+            \"sub-account/subAccountApi/ipRestriction\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_post_sub_account_sub_account_api_ip_restriction.__doc__ = (\n+        Client.margin_v1_post_sub_account_sub_account_api_ip_restriction.__doc__\n+    )\n+\n+    async def margin_v1_get_simple_earn_flexible_history_redemption_record(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"simple-earn/flexible/history/redemptionRecord\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_simple_earn_flexible_history_redemption_record.__doc__ = (\n+        Client.margin_v1_get_simple_earn_flexible_history_redemption_record.__doc__\n+    )\n+\n     async def margin_v1_get_broker_sub_account_api(self, **params):\n-        return await self._request_margin_api(\"get\", \"broker/subAccountApi\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_broker_sub_account_api.__doc__ = Client.margin_v1_get_broker_sub_account_api.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"broker/subAccountApi\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_broker_sub_account_api.__doc__ = (\n+        Client.margin_v1_get_broker_sub_account_api.__doc__\n+    )\n+\n     async def options_v1_get_exercise_history(self, **params):\n-        return await self._request_options_api(\"get\", \"exerciseHistory\", signed=False, data=params)\n-        \n-    options_v1_get_exercise_history.__doc__ = Client.options_v1_get_exercise_history.__doc__\n-            \n+        return await self._request_options_api(\n+            \"get\", \"exerciseHistory\", signed=False, data=params\n+        )\n+\n+    options_v1_get_exercise_history.__doc__ = (\n+        Client.options_v1_get_exercise_history.__doc__\n+    )\n+\n     async def margin_v1_get_convert_exchange_info(self, **params):\n-        return await self._request_margin_api(\"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1)\n-        \n-    margin_v1_get_convert_exchange_info.__doc__ = Client.margin_v1_get_convert_exchange_info.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"convert/exchangeInfo\", signed=False, data=params, version=1\n+        )\n+\n+    margin_v1_get_convert_exchange_info.__doc__ = (\n+        Client.margin_v1_get_convert_exchange_info.__doc__\n+    )\n+\n     async def futures_v1_delete_batch_order(self, **params):\n-        return await self._request_futures_api(\"delete\", \"batchOrder\", signed=True, data=params, version=1)\n-        \n+        return await self._request_futures_api(\n+            \"delete\", \"batchOrder\", signed=True, data=params, version=1\n+        )\n+\n     futures_v1_delete_batch_order.__doc__ = Client.futures_v1_delete_batch_order.__doc__\n-            \n-    async def margin_v1_get_eth_staking_eth_history_wbeth_rewards_history(self, **params):\n-        return await self._request_margin_api(\"get\", \"eth-staking/eth/history/wbethRewardsHistory\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_eth_staking_eth_history_wbeth_rewards_history.__doc__ = Client.margin_v1_get_eth_staking_eth_history_wbeth_rewards_history.__doc__\n-            \n+\n+    async def margin_v1_get_eth_staking_eth_history_wbeth_rewards_history(\n+        self, **params\n+    ):\n+        return await self._request_margin_api(\n+            \"get\",\n+            \"eth-staking/eth/history/wbethRewardsHistory\",\n+            signed=True,\n+            data=params,\n+            version=1,\n+        )\n+\n+    margin_v1_get_eth_staking_eth_history_wbeth_rewards_history.__doc__ = (\n+        Client.margin_v1_get_eth_staking_eth_history_wbeth_rewards_history.__doc__\n+    )\n+\n     async def margin_v1_get_mining_pub_algo_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/pub/algoList\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_pub_algo_list.__doc__ = Client.margin_v1_get_mining_pub_algo_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/pub/algoList\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_pub_algo_list.__doc__ = (\n+        Client.margin_v1_get_mining_pub_algo_list.__doc__\n+    )\n+\n     async def options_v1_get_block_trades(self, **params):\n-        return await self._request_options_api(\"get\", \"blockTrades\", signed=False, data=params)\n-        \n+        return await self._request_options_api(\n+            \"get\", \"blockTrades\", signed=False, data=params\n+        )\n+\n     options_v1_get_block_trades.__doc__ = Client.options_v1_get_block_trades.__doc__\n-            \n+\n     async def margin_v1_get_copy_trading_futures_lead_symbol(self, **params):\n-        return await self._request_margin_api(\"get\", \"copyTrading/futures/leadSymbol\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_copy_trading_futures_lead_symbol.__doc__ = Client.margin_v1_get_copy_trading_futures_lead_symbol.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"copyTrading/futures/leadSymbol\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_copy_trading_futures_lead_symbol.__doc__ = (\n+        Client.margin_v1_get_copy_trading_futures_lead_symbol.__doc__\n+    )\n+\n     async def margin_v1_get_mining_worker_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"mining/worker/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_mining_worker_list.__doc__ = Client.margin_v1_get_mining_worker_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"mining/worker/list\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_mining_worker_list.__doc__ = (\n+        Client.margin_v1_get_mining_worker_list.__doc__\n+    )\n+\n     async def margin_v1_get_dci_product_list(self, **params):\n-        return await self._request_margin_api(\"get\", \"dci/product/list\", signed=True, data=params, version=1)\n-        \n-    margin_v1_get_dci_product_list.__doc__ = Client.margin_v1_get_dci_product_list.__doc__\n-            \n+        return await self._request_margin_api(\n+            \"get\", \"dci/product/list\", signed=True, data=params, version=1\n+        )\n+\n+    margin_v1_get_dci_product_list.__doc__ = (\n+        Client.margin_v1_get_dci_product_list.__doc__\n+    )\n+\n     async def futures_v1_get_convert_order_status(self, **params):\n-        return await self._request_futures_api(\"get\", \"convert/orderStatus\", signed=True, data=params, version=1)\n-        \n-    futures_v1_get_convert_order_status.__doc__ = Client.futures_v1_get_convert_order_status.__doc__\n-            \n+        return await self._request_futures_api(\n+            \"get\", \"convert/orderStatus\", signed=True, data=params, version=1\n+        )\n+\n+    futures_v1_get_convert_order_status.__doc__ = (\n+        Client.futures_v1_get_convert_order_status.__doc__\n+    )\ndiff --git a/tests/test_async_client_futures.py b/tests/test_async_client_futures.py\nindex 618a184..8c671eb 100644\n--- a/tests/test_async_client_futures.py\n+++ b/tests/test_async_client_futures.py\n@@ -6,145 +6,184 @@ from .test_get_order_book import assert_ob\n \n pytestmark = [pytest.mark.futures, pytest.mark.asyncio]\n \n+\n async def test_futures_ping(futuresClientAsync):\n     await futuresClientAsync.futures_ping()\n \n+\n async def test_futures_time(futuresClientAsync):\n     await futuresClientAsync.futures_time()\n+\n+\n async def test_futures_exchange_info(futuresClientAsync):\n     await futuresClientAsync.futures_exchange_info()\n \n+\n async def test_futures_order_book(futuresClientAsync):\n     order_book = await futuresClientAsync.futures_order_book(symbol=\"BTCUSDT\")\n     assert_ob(order_book)\n \n+\n async def test_futures_recent_trades(futuresClientAsync):\n     await futuresClientAsync.futures_recent_trades(symbol=\"BTCUSDT\")\n \n+\n async def test_futures_historical_trades(futuresClientAsync):\n     await futuresClientAsync.futures_historical_trades(symbol=\"BTCUSDT\")\n \n+\n async def test_futures_aggregate_trades(futuresClientAsync):\n     await futuresClientAsync.futures_aggregate_trades(symbol=\"BTCUSDT\")\n \n+\n async def test_futures_klines(futuresClientAsync):\n     await futuresClientAsync.futures_klines(symbol=\"BTCUSDT\", interval=\"1h\")\n \n+\n async def test_futures_continuous_klines(futuresClientAsync):\n     await futuresClientAsync.futures_continuous_klines(\n         pair=\"BTCUSDT\", contractType=\"PERPETUAL\", interval=\"1h\"\n     )\n \n+\n async def test_futures_historical_klines(futuresClientAsync):\n     await futuresClientAsync.futures_historical_klines(\n         symbol=\"BTCUSDT\", interval=\"1h\", start_str=datetime.now().strftime(\"%Y-%m-%d\")\n     )\n \n+\n async def test_futures_historical_klines_generator(futuresClientAsync):\n     await futuresClientAsync.futures_historical_klines_generator(\n         symbol=\"BTCUSDT\", interval=\"1h\", start_str=datetime.now().strftime(\"%Y-%m-%d\")\n     )\n \n+\n async def test_futures_mark_price(futuresClientAsync):\n     await futuresClientAsync.futures_mark_price()\n \n+\n async def test_futures_funding_rate(futuresClientAsync):\n     await futuresClientAsync.futures_funding_rate()\n \n+\n @pytest.mark.skip(reason=\"No Sandbox Environment to test\")\n async def test_futures_top_longshort_account_ratio(futuresClientAsync):\n     await futuresClientAsync.futures_top_longshort_account_ratio(\n         symbol=\"BTCUSDT\", period=\"5m\"\n     )\n \n+\n @pytest.mark.skip(reason=\"No Sandbox Environment to test\")\n async def test_futures_top_longshort_position_ratio(futuresClientAsync):\n     await futuresClientAsync.futures_top_longshort_position_ratio(\n         symbol=\"BTCUSDT\", period=\"5m\"\n     )\n \n+\n @pytest.mark.skip(reason=\"No Sandbox Environment to test\")\n async def test_futures_global_longshort_ratio(futuresClientAsync):\n     await futuresClientAsync.futures_global_longshort_ratio(\n         symbol=\"BTCUSDT\", period=\"5m\"\n     )\n \n+\n @pytest.mark.skip(reason=\"No Sandbox Environment to test\")\n async def test_futures_taker_longshort_ratio(futuresClientAsync):\n     await futuresClientAsync.futures_taker_longshort_ratio(\n         symbol=\"BTCUSDT\", period=\"5m\"\n     )\n \n+\n async def test_futures_ticker(futuresClientAsync):\n     await futuresClientAsync.futures_ticker()\n \n+\n async def test_futures_symbol_ticker(futuresClientAsync):\n     await futuresClientAsync.futures_symbol_ticker()\n \n+\n async def test_futures_orderbook_ticker(futuresClientAsync):\n     await futuresClientAsync.futures_orderbook_ticker()\n \n+\n async def test_futures_index_index_price_constituents(futuresClientAsync):\n     await futuresClientAsync.futures_index_price_constituents(symbol=\"BTCUSD\")\n \n+\n async def test_futures_liquidation_orders(futuresClientAsync):\n     await futuresClientAsync.futures_liquidation_orders()\n \n+\n @pytest.mark.skip(reason=\"Temporary skip due to issues with api\")\n async def test_futures_api_trading_status(futuresClientAsync):\n     await futuresClientAsync.futures_api_trading_status()\n \n+\n async def test_futures_commission_rate(futuresClientAsync):\n     await futuresClientAsync.futures_commission_rate(symbol=\"BTCUSDT\")\n \n+\n async def test_futures_adl_quantile_estimate(futuresClientAsync):\n     await futuresClientAsync.futures_adl_quantile_estimate()\n \n+\n async def test_futures_open_interest(futuresClientAsync):\n     await futuresClientAsync.futures_open_interest(symbol=\"BTCUSDT\")\n \n+\n async def test_futures_index_info(futuresClientAsync):\n     await futuresClientAsync.futures_index_info()\n \n+\n @pytest.mark.skip(reason=\"No Sandbox Environment to test\")\n async def test_futures_open_interest_hist(futuresClientAsync):\n     await futuresClientAsync.futures_open_interest_hist(symbol=\"BTCUSDT\", period=\"5m\")\n \n+\n async def test_futures_leverage_bracket(futuresClientAsync):\n     await futuresClientAsync.futures_leverage_bracket()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_account_transfer(futuresClientAsync):\n     await futuresClientAsync.futures_account_transfer()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_transfer_history(client):\n     client.transfer_history()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_loan_borrow_history(futuresClientAsync):\n     await futuresClientAsync.futures_loan_borrow_history()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_loan_repay_history(futuresClientAsync):\n     await futuresClientAsync.futures_loan_repay_history()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_loan_wallet(futuresClientAsync):\n     await futuresClientAsync.futures_loan_wallet()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_cross_collateral_adjust_history(futuresClientAsync):\n     await futuresClientAsync.futures_cross_collateral_adjust_history()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_cross_collateral_liquidation_history(futuresClientAsync):\n     await futuresClientAsync.futures_cross_collateral_liquidation_history()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_loan_interest_history(futuresClientAsync):\n     await futuresClientAsync.futures_loan_interest_history()\n \n+\n async def test_futures_create_get_edit_cancel_order(futuresClientAsync):\n     ticker = await futuresClientAsync.futures_ticker(symbol=\"LTCUSDT\")\n     positions = await futuresClientAsync.futures_position_information(symbol=\"LTCUSDT\")\n@@ -174,6 +213,7 @@ async def test_futures_create_get_edit_cancel_order(futuresClientAsync):\n         orderid=order[\"orderId\"], symbol=order[\"symbol\"]\n     )\n \n+\n async def test_futures_create_test_order(futuresClientAsync):\n     ticker = await futuresClientAsync.futures_ticker(symbol=\"LTCUSDT\")\n     positions = await futuresClientAsync.futures_position_information(symbol=\"LTCUSDT\")\n@@ -187,6 +227,7 @@ async def test_futures_create_test_order(futuresClientAsync):\n         price=str(round(float(ticker[\"lastPrice\"]) - 1, 0)),\n     )\n \n+\n async def test_futures_place_batch_order_and_cancel(futuresClientAsync):\n     ticker = await futuresClientAsync.futures_ticker(symbol=\"LTCUSDT\")\n     positions = await futuresClientAsync.futures_position_information(symbol=\"LTCUSDT\")\n@@ -230,30 +271,38 @@ async def test_futures_place_batch_order_and_cancel(futuresClientAsync):\n     for order in cancelled_orders:\n         assert_contract_order(futuresClientAsync, order)\n \n+\n async def test_futures_get_open_orders(futuresClientAsync):\n     await futuresClientAsync.futures_get_open_orders()\n \n+\n async def test_futures_get_all_orders(futuresClientAsync):\n     orders = futuresClientAsync.futures_get_all_orders()\n     print(orders)\n \n+\n async def test_futures_cancel_all_open_orders(futuresClientAsync):\n     await futuresClientAsync.futures_cancel_all_open_orders(symbol=\"LTCUSDT\")\n \n+\n async def test_futures_countdown_cancel_all(futuresClientAsync):\n     await futuresClientAsync.futures_countdown_cancel_all(\n         symbol=\"LTCUSDT\", countdownTime=10\n     )\n \n+\n async def test_futures_account_balance(futuresClientAsync):\n     await futuresClientAsync.futures_account_balance()\n \n+\n async def test_futures_account(futuresClientAsync):\n     await futuresClientAsync.futures_account()\n \n+\n async def test_futures_change_leverage(futuresClientAsync):\n     await futuresClientAsync.futures_change_leverage(symbol=\"LTCUSDT\", leverage=10)\n \n+\n async def test_futures_change_margin_type(futuresClientAsync):\n     try:\n         await futuresClientAsync.futures_change_margin_type(\n@@ -264,20 +313,25 @@ async def test_futures_change_margin_type(futuresClientAsync):\n             symbol=\"XRPUSDT\", marginType=\"ISOLATED\"\n         )\n \n+\n async def test_futures_position_margin_history(futuresClientAsync):\n     position = await futuresClientAsync.futures_position_margin_history(\n         symbol=\"LTCUSDT\"\n     )\n \n+\n async def test_futures_position_information(futuresClientAsync):\n     await futuresClientAsync.futures_position_information()\n \n+\n async def test_futures_account_trades(futuresClientAsync):\n     await futuresClientAsync.futures_account_trades()\n \n+\n async def test_futures_income_history(futuresClientAsync):\n     await futuresClientAsync.futures_income_history()\n \n+\n async def close_all_futures_positions(futuresClientAsync):\n     # Get all open positions\n     positions = await futuresClientAsync.futures_position_information(symbol=\"LTCUSDT\")\n@@ -299,6 +353,7 @@ async def close_all_futures_positions(futuresClientAsync):\n             except Exception as e:\n                 print(f\"Failed to close position for {symbol}: {e}\")\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_get_and_change_position_mode(futuresClientAsync):\n     mode = await futuresClientAsync.futures_get_position_mode()\n@@ -306,101 +361,129 @@ async def test_futures_get_and_change_position_mode(futuresClientAsync):\n         dualSidePosition=not mode[\"dualSidePosition\"]\n     )\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_change_multi_assets_mode(futuresClientAsync):\n     await futuresClientAsync.futures_change_multi_assets_mode()\n \n+\n async def test_futures_get_multi_assets_mode(futuresClientAsync):\n     await futuresClientAsync.futures_get_multi_assets_mode()\n \n+\n async def test_futures_stream_get_listen_key(futuresClientAsync):\n     await futuresClientAsync.futures_stream_get_listen_key()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_stream_close(futuresClientAsync):\n     await futuresClientAsync.futures_stream_close()\n \n+\n # new methods\n async def test_futures_account_config(futuresClientAsync):\n     await futuresClientAsync.futures_account_config()\n \n+\n async def test_futures_symbol_config(futuresClientAsync):\n     await futuresClientAsync.futures_symbol_config()\n \n+\n # COIN Futures API\n async def test_futures_coin_ping(futuresClientAsync):\n     await futuresClientAsync.futures_coin_ping()\n \n+\n async def test_futures_coin_time(futuresClientAsync):\n     await futuresClientAsync.futures_coin_time()\n \n+\n async def test_futures_coin_exchange_info(futuresClientAsync):\n     await futuresClientAsync.futures_coin_exchange_info()\n \n+\n async def test_futures_coin_order_book(futuresClientAsync):\n     order_book = await futuresClientAsync.futures_coin_order_book(symbol=\"BTCUSD_PERP\")\n     assert_ob(order_book)\n \n+\n async def test_futures_coin_recent_trades(futuresClientAsync):\n     await futuresClientAsync.futures_coin_recent_trades(symbol=\"BTCUSD_PERP\")\n \n+\n async def test_futures_coin_historical_trades(futuresClientAsync):\n     await futuresClientAsync.futures_coin_historical_trades(symbol=\"BTCUSD_PERP\")\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_aggregate_trades(futuresClientAsync):\n     await futuresClientAsync.futures_coin_aggregate_trades(symbol=\"BTCUSD_PERP\")\n \n+\n async def test_futures_coin_klines(futuresClientAsync):\n     await futuresClientAsync.futures_coin_klines(symbol=\"BTCUSD_PERP\", interval=\"1h\")\n \n+\n async def test_futures_coin_continous_klines(futuresClientAsync):\n     await futuresClientAsync.futures_coin_continous_klines(\n         pair=\"BTCUSD\", contractType=\"PERPETUAL\", interval=\"1h\"\n     )\n \n+\n async def test_futures_coin_index_price_klines(futuresClientAsync):\n     await futuresClientAsync.futures_coin_index_price_klines(\n         pair=\"BTCUSD\", interval=\"1m\"\n     )\n \n+\n async def test_futures_coin_mark_price_klines(futuresClientAsync):\n     await futuresClientAsync.futures_coin_mark_price_klines(\n         symbol=\"BTCUSD_PERP\", interval=\"1m\"\n     )\n \n+\n async def test_futures_coin_mark_price(futuresClientAsync):\n     await futuresClientAsync.futures_coin_mark_price()\n \n+\n async def test_futures_coin_funding_rate(futuresClientAsync):\n     await futuresClientAsync.futures_coin_funding_rate(symbol=\"BTCUSD_PERP\")\n \n+\n async def test_futures_coin_ticker(futuresClientAsync):\n     await futuresClientAsync.futures_coin_ticker()\n \n+\n async def test_futures_coin_symbol_ticker(futuresClientAsync):\n     await futuresClientAsync.futures_coin_symbol_ticker()\n \n+\n async def test_futures_coin_orderbook_ticker(futuresClientAsync):\n     await futuresClientAsync.futures_coin_orderbook_ticker()\n \n+\n async def test_futures_coin_index_index_price_constituents(futuresClientAsync):\n     await futuresClientAsync.futures_coin_index_price_constituents(symbol=\"BTCUSD\")\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_liquidation_orders(futuresClientAsync):\n     await futuresClientAsync.futures_coin_liquidation_orders()\n \n+\n async def test_futures_coin_open_interest(futuresClientAsync):\n     await futuresClientAsync.futures_coin_open_interest(symbol=\"BTCUSD_PERP\")\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_open_interest_hist(futuresClientAsync):\n     await futuresClientAsync.futures_coin_open_interest_hist(symbol=\"BTCUSD_PERP\")\n \n+\n async def test_futures_coin_leverage_bracket(futuresClientAsync):\n     await futuresClientAsync.futures_coin_leverage_bracket()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_create_order(futuresClientAsync):\n     positions = await futuresClientAsync.futures_coin_position_information()\n@@ -430,88 +513,111 @@ async def test_futures_coin_create_order(futuresClientAsync):\n         orderid=order[\"orderId\"], symbol=order[\"symbol\"]\n     )\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_place_batch_order(futuresClientAsync):\n     await futuresClientAsync.futures_coin_place_batch_order()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_get_order(futuresClientAsync):\n     await futuresClientAsync.futures_coin_get_order()\n \n+\n async def test_futures_coin_get_open_orders(futuresClientAsync):\n     await futuresClientAsync.futures_coin_get_open_orders()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_get_all_orders(futuresClientAsync):\n     await futuresClientAsync.futures_coin_get_all_orders()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_cancel_order(futuresClientAsync):\n     await futuresClientAsync.futures_coin_cancel_order()\n \n+\n async def test_futures_coin_cancel_all_open_orders(futuresClientAsync):\n     await futuresClientAsync.futures_coin_cancel_all_open_orders(symbol=\"BTCUSD_PERP\")\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_cancel_orders(futuresClientAsync):\n     await futuresClientAsync.futures_coin_cancel_orders()\n \n+\n async def test_futures_coin_account_balance(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account_balance()\n \n+\n async def test_futures_coin_account(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_change_leverage(futuresClientAsync):\n     await futuresClientAsync.futures_coin_change_leverage(symbol=\"XRPUSDT\", leverage=10)\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_change_margin_type(futuresClientAsync):\n     await futuresClientAsync.futures_coin_change_margin_type()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_change_position_margin(futuresClientAsync):\n     await futuresClientAsync.futures_coin_change_position_margin()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_position_margin_history(futuresClientAsync):\n     await futuresClientAsync.futures_coin_position_margin_history()\n \n+\n async def test_futures_coin_position_information(futuresClientAsync):\n     await futuresClientAsync.futures_coin_position_information()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_account_trades(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account_trades()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_income_history(futuresClientAsync):\n     await futuresClientAsync.futures_coin_income_history()\n \n+\n @pytest.mark.skip(reason=\"Not implemented\")\n async def test_futures_coin_change_position_mode(futuresClientAsync):\n     await futuresClientAsync.futures_coin_change_position_mode()\n \n+\n async def test_futures_coin_get_position_mode(futuresClientAsync):\n     await futuresClientAsync.futures_coin_get_position_mode()\n \n+\n async def test_futures_coin_stream_close(futuresClientAsync):\n     listen_key = await futuresClientAsync.futures_coin_stream_get_listen_key()\n     await futuresClientAsync.futures_coin_stream_close(listenKey=listen_key)\n \n+\n @pytest.mark.skip(reason=\"No sandbox support\")\n async def test_futures_coin_account_order_history_download(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account_order_download()\n \n+\n @pytest.mark.skip(reason=\"No sandbox support\")\n async def test_futures_coin_account_order_download_id(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account_order_download_link(downloadId=\"123\")\n \n+\n @pytest.mark.skip(reason=\"No sandbox support\")\n async def test_futures_coin_account_trade_history_download(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account_trade_history_download()\n \n+\n @pytest.mark.skip(reason=\"No sandbox support\")\n async def test_futures_coin_account_trade_download_id(futuresClientAsync):\n     await futuresClientAsync.futures_coin_account_trade_history_download_link(\n"
    },
    {
        "id": "286",
        "sha_fail": "58de72d97f1b19b44a141fbaa4cdd0411748f6ea",
        "diff": "diff --git a/tests/functional/customer/test_order_status.py b/tests/functional/customer/test_order_status.py\nindex 59f77017a..6dbe3fbc0 100644\n--- a/tests/functional/customer/test_order_status.py\n+++ b/tests/functional/customer/test_order_status.py\n@@ -21,7 +21,7 @@ class TestAnAnonymousUser(WebTestCase):\n             kwargs={\"order_number\": order.number, \"hash\": order.verification_hash()},\n         )\n         response = self.app.get(path)\n-        self.assertEqual(http_client.OK, response.status_code)\n+        self.assertEqual(http_client.FOUND, response.status_code)\n \n     def test_gets_404_when_using_incorrect_hash(self):\n         order = create_order()\n"
    },
    {
        "id": "292",
        "sha_fail": "ce28f30ca68a2e0469f14beb1b4d65a4633d211e",
        "diff": "diff --git a/lightrag/api/lightrag_server.py b/lightrag/api/lightrag_server.py\nindex 60fae05d..0ed20a28 100644\n--- a/lightrag/api/lightrag_server.py\n+++ b/lightrag/api/lightrag_server.py\n@@ -644,13 +644,20 @@ def create_app(args):\n                     from lightrag.llm.jina import jina_embed\n \n                     return await jina_embed(\n-                        texts, embedding_dim=embedding_dim, base_url=host, api_key=api_key\n+                        texts,\n+                        embedding_dim=embedding_dim,\n+                        base_url=host,\n+                        api_key=api_key,\n                     )\n                 else:  # openai and compatible\n                     from lightrag.llm.openai import openai_embed\n \n                     return await openai_embed(\n-                        texts, model=model, base_url=host, api_key=api_key, embedding_dim=embedding_dim\n+                        texts,\n+                        model=model,\n+                        base_url=host,\n+                        api_key=api_key,\n+                        embedding_dim=embedding_dim,\n                     )\n             except ImportError as e:\n                 raise Exception(f\"Failed to import {binding} embedding: {e}\")\n"
    },
    {
        "id": "293",
        "sha_fail": "a0593ec1c9613ae8ab78fddd30e47114ded16f00",
        "diff": "diff --git a/lightrag/kg/postgres_impl.py b/lightrag/kg/postgres_impl.py\nindex 4663aadb..30418d88 100644\n--- a/lightrag/kg/postgres_impl.py\n+++ b/lightrag/kg/postgres_impl.py\n@@ -828,8 +828,8 @@ class PostgreSQLDB:\n \n                     # Execute the migration\n                     alter_sql = f\"\"\"\n-                    ALTER TABLE {migration['table']}\n-                    ALTER COLUMN {migration['column']} TYPE {migration['new_type']}\n+                    ALTER TABLE {migration[\"table\"]}\n+                    ALTER COLUMN {migration[\"column\"]} TYPE {migration[\"new_type\"]}\n                     \"\"\"\n \n                     await self.execute(alter_sql)\n@@ -3387,7 +3387,9 @@ class PGGraphStorage(BaseGraphStorage):\n                 logger.error(f\"[{self.workspace}] Error during edge deletion: {str(e)}\")\n                 raise\n \n-    async def get_nodes_batch(self, node_ids: list[str], batch_size: int = 1000) -> dict[str, dict]:\n+    async def get_nodes_batch(\n+        self, node_ids: list[str], batch_size: int = 1000\n+    ) -> dict[str, dict]:\n         \"\"\"\n         Retrieve multiple nodes in one query using UNWIND.\n \n@@ -3453,7 +3455,9 @@ class PGGraphStorage(BaseGraphStorage):\n \n         return nodes_dict\n \n-    async def node_degrees_batch(self, node_ids: list[str], batch_size: int = 500) -> dict[str, int]:\n+    async def node_degrees_batch(\n+        self, node_ids: list[str], batch_size: int = 500\n+    ) -> dict[str, int]:\n         \"\"\"\n         Retrieve the degree for multiple nodes in a single query using UNWIND.\n         Calculates the total degree by counting distinct relationships.\n@@ -3482,7 +3486,7 @@ class PGGraphStorage(BaseGraphStorage):\n         in_degrees = {}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n \n             query = f\"\"\"\n                     WITH input(v, ord) AS (\n@@ -3602,7 +3606,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_dict: dict[tuple[str, str], dict] = {}\n \n         for i in range(0, len(uniq_pairs), batch_size):\n-            batch = uniq_pairs[i:i + batch_size]\n+            batch = uniq_pairs[i : i + batch_size]\n \n             pairs = [{\"src\": p[\"src\"], \"tgt\": p[\"tgt\"]} for p in batch]\n \n@@ -3709,7 +3713,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_norm: dict[str, list[tuple[str, str]]] = {n: [] for n in unique_ids}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n             # Format node IDs for the query\n             formatted_ids = \", \".join([f'\"{n}\"' for n in batch])\n \n"
    },
    {
        "id": "294",
        "sha_fail": "60564cf453626802e147b5b929b8b1c6427fb3d8",
        "diff": "diff --git a/lightrag/kg/postgres_impl.py b/lightrag/kg/postgres_impl.py\nindex 3ab8bfb8..bf590095 100644\n--- a/lightrag/kg/postgres_impl.py\n+++ b/lightrag/kg/postgres_impl.py\n@@ -828,8 +828,8 @@ class PostgreSQLDB:\n \n                     # Execute the migration\n                     alter_sql = f\"\"\"\n-                    ALTER TABLE {migration['table']}\n-                    ALTER COLUMN {migration['column']} TYPE {migration['new_type']}\n+                    ALTER TABLE {migration[\"table\"]}\n+                    ALTER COLUMN {migration[\"column\"]} TYPE {migration[\"new_type\"]}\n                     \"\"\"\n \n                     await self.execute(alter_sql)\n@@ -3387,7 +3387,9 @@ class PGGraphStorage(BaseGraphStorage):\n                 logger.error(f\"[{self.workspace}] Error during edge deletion: {str(e)}\")\n                 raise\n \n-    async def get_nodes_batch(self, node_ids: list[str], batch_size: int = 1000) -> dict[str, dict]:\n+    async def get_nodes_batch(\n+        self, node_ids: list[str], batch_size: int = 1000\n+    ) -> dict[str, dict]:\n         \"\"\"\n         Retrieve multiple nodes in one query using UNWIND.\n \n@@ -3453,7 +3455,9 @@ class PGGraphStorage(BaseGraphStorage):\n \n         return nodes_dict\n \n-    async def node_degrees_batch(self, node_ids: list[str], batch_size: int = 500) -> dict[str, int]:\n+    async def node_degrees_batch(\n+        self, node_ids: list[str], batch_size: int = 500\n+    ) -> dict[str, int]:\n         \"\"\"\n         Retrieve the degree for multiple nodes in a single query using UNWIND.\n         Calculates the total degree by counting distinct relationships.\n@@ -3482,7 +3486,7 @@ class PGGraphStorage(BaseGraphStorage):\n         in_degrees = {}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n \n             query = f\"\"\"\n                     WITH input(v, ord) AS (\n@@ -3602,7 +3606,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_dict: dict[tuple[str, str], dict] = {}\n \n         for i in range(0, len(uniq_pairs), batch_size):\n-            batch = uniq_pairs[i:i + batch_size]\n+            batch = uniq_pairs[i : i + batch_size]\n \n             pairs = [{\"src\": p[\"src\"], \"tgt\": p[\"tgt\"]} for p in batch]\n \n@@ -3709,7 +3713,7 @@ class PGGraphStorage(BaseGraphStorage):\n         edges_norm: dict[str, list[tuple[str, str]]] = {n: [] for n in unique_ids}\n \n         for i in range(0, len(unique_ids), batch_size):\n-            batch = unique_ids[i:i + batch_size]\n+            batch = unique_ids[i : i + batch_size]\n             # Format node IDs for the query\n             formatted_ids = \", \".join([f'\"{n}\"' for n in batch])\n \n"
    },
    {
        "id": "296",
        "sha_fail": "f6adbaa7d295c83c481fbd8ad452eb1883f42a2c",
        "diff": "diff --git a/libs/agno/agno/models/dashscope/dashscope.py b/libs/agno/agno/models/dashscope/dashscope.py\nindex a1799e070..68ff72099 100644\n--- a/libs/agno/agno/models/dashscope/dashscope.py\n+++ b/libs/agno/agno/models/dashscope/dashscope.py\n@@ -6,7 +6,6 @@ from pydantic import BaseModel\n \n from agno.exceptions import ModelProviderError\n from agno.models.openai.like import OpenAILike\n-from agno.models.response import ModelResponse\n \n \n @dataclass\n@@ -84,7 +83,7 @@ class DashScope(OpenAILike):\n             params[\"extra_body\"] = {\n                 \"enable_thinking\": self.enable_thinking,\n             }\n-            \n+\n             if self.thinking_budget is not None:\n                 params[\"extra_body\"][\"thinking_budget\"] = self.thinking_budget\n \n"
    },
    {
        "id": "298",
        "sha_fail": "6f27d0e8dbbb74ef34579d283621da82ebf3ba26",
        "diff": "diff --git a/tests/unittests/sources/test_smartos.py b/tests/unittests/sources/test_smartos.py\nindex 1f4418eba..bc3f5c79f 100644\n--- a/tests/unittests/sources/test_smartos.py\n+++ b/tests/unittests/sources/test_smartos.py\n@@ -814,13 +814,10 @@ def joyent_metadata(mocker, m_serial):\n             payloadstr = \" {0}\".format(\n                 res.response_parts[\"payload\"]  # pylint: disable=E1146,E1136\n             )\n-        return (\n-            \"V2 {length} {crc} {request_id} \"\n-            \"{command}{payloadstr}\\n\".format(\n-                payloadstr=payloadstr,\n-                **res.response_parts  # pylint: disable=E1134\n-            ).encode(\"ascii\")\n-        )\n+        return \"V2 {length} {crc} {request_id} {command}{payloadstr}\\n\".format(\n+            payloadstr=payloadstr,\n+            **res.response_parts,  # pylint: disable=E1134\n+        ).encode(\"ascii\")\n \n     res.metasource_data = None\n \n@@ -865,7 +862,6 @@ def joyent_serial_client(joyent_metadata):\n \n @pytest.mark.usefixtures(\"fake_filesystem\")\n class TestJoyentMetadataClient:\n-\n     invalid = b\"invalid command\\n\"\n     failure = b\"FAILURE\\n\"\n     v2_ok = b\"V2_OK\\n\"\n@@ -876,9 +872,9 @@ class TestJoyentMetadataClient:\n         )\n \n     def assertStartsWith(self, haystack, prefix):\n-        assert haystack.startswith(\n-            prefix\n-        ), \"{0} does not start with '{1}'\".format(repr(haystack), prefix)\n+        assert haystack.startswith(prefix), (\n+            \"{0} does not start with '{1}'\".format(repr(haystack), prefix)\n+        )\n \n     def assertNoMoreSideEffects(self, obj):\n         with pytest.raises(StopIteration):\n"
    },
    {
        "id": "276",
        "sha_fail": "2ee00eddc1bebd2fcffc4a026162e00378ebe3ae",
        "diff": "diff --git a/sqlglot/dialects/dialect.py b/sqlglot/dialects/dialect.py\nindex 371418bd3..a9638578e 100644\n--- a/sqlglot/dialects/dialect.py\n+++ b/sqlglot/dialects/dialect.py\n@@ -59,7 +59,9 @@ UNESCAPED_SEQUENCES = {\n }\n \n \n-def annotate_with_type_lambda(data_type: exp.DataType.Type) -> t.Callable[[TypeAnnotator, E], E]:\n+def annotate_with_type_lambda(\n+    data_type: exp.DataType.Type,\n+) -> t.Callable[[TypeAnnotator, E], E]:\n     return lambda self, e: self._annotate_with_type(e, data_type)\n \n \n@@ -205,7 +207,9 @@ class _Dialect(type):\n \n         base = seq_get(bases, 0)\n         base_tokenizer = (getattr(base, \"tokenizer_class\", Tokenizer),)\n-        base_jsonpath_tokenizer = (getattr(base, \"jsonpath_tokenizer_class\", JSONPathTokenizer),)\n+        base_jsonpath_tokenizer = (\n+            getattr(base, \"jsonpath_tokenizer_class\", JSONPathTokenizer),\n+        )\n         base_parser = (getattr(base, \"parser_class\", Parser),)\n         base_generator = (getattr(base, \"generator_class\", Generator),)\n \n@@ -215,17 +219,23 @@ class _Dialect(type):\n         klass.jsonpath_tokenizer_class = klass.__dict__.get(\n             \"JSONPathTokenizer\", type(\"JSONPathTokenizer\", base_jsonpath_tokenizer, {})\n         )\n-        klass.parser_class = klass.__dict__.get(\"Parser\", type(\"Parser\", base_parser, {}))\n+        klass.parser_class = klass.__dict__.get(\n+            \"Parser\", type(\"Parser\", base_parser, {})\n+        )\n         klass.generator_class = klass.__dict__.get(\n             \"Generator\", type(\"Generator\", base_generator, {})\n         )\n \n-        klass.QUOTE_START, klass.QUOTE_END = list(klass.tokenizer_class._QUOTES.items())[0]\n+        klass.QUOTE_START, klass.QUOTE_END = list(\n+            klass.tokenizer_class._QUOTES.items()\n+        )[0]\n         klass.IDENTIFIER_START, klass.IDENTIFIER_END = list(\n             klass.tokenizer_class._IDENTIFIERS.items()\n         )[0]\n \n-        def get_start_end(token_type: TokenType) -> t.Tuple[t.Optional[str], t.Optional[str]]:\n+        def get_start_end(\n+            token_type: TokenType,\n+        ) -> t.Tuple[t.Optional[str], t.Optional[str]]:\n             return next(\n                 (\n                     (s, e)\n@@ -258,7 +268,9 @@ class _Dialect(type):\n             klass.generator_class.SUPPORTS_UESCAPE = False\n \n         if enum not in (\"\", \"databricks\", \"hive\", \"spark\", \"spark2\"):\n-            modifier_transforms = klass.generator_class.AFTER_HAVING_MODIFIER_TRANSFORMS.copy()\n+            modifier_transforms = (\n+                klass.generator_class.AFTER_HAVING_MODIFIER_TRANSFORMS.copy()\n+            )\n             for modifier in (\"cluster\", \"distribute\", \"sort\"):\n                 modifier_transforms.pop(modifier, None)\n \n@@ -268,18 +280,24 @@ class _Dialect(type):\n             klass.parser_class.ID_VAR_TOKENS = klass.parser_class.ID_VAR_TOKENS | {\n                 TokenType.STRAIGHT_JOIN,\n             }\n-            klass.parser_class.TABLE_ALIAS_TOKENS = klass.parser_class.TABLE_ALIAS_TOKENS | {\n-                TokenType.STRAIGHT_JOIN,\n-            }\n+            klass.parser_class.TABLE_ALIAS_TOKENS = (\n+                klass.parser_class.TABLE_ALIAS_TOKENS\n+                | {\n+                    TokenType.STRAIGHT_JOIN,\n+                }\n+            )\n \n         if enum not in (\"\", \"databricks\", \"oracle\", \"redshift\", \"snowflake\", \"spark\"):\n             klass.generator_class.SUPPORTS_DECODE_CASE = False\n \n         if not klass.SUPPORTS_SEMI_ANTI_JOIN:\n-            klass.parser_class.TABLE_ALIAS_TOKENS = klass.parser_class.TABLE_ALIAS_TOKENS | {\n-                TokenType.ANTI,\n-                TokenType.SEMI,\n-            }\n+            klass.parser_class.TABLE_ALIAS_TOKENS = (\n+                klass.parser_class.TABLE_ALIAS_TOKENS\n+                | {\n+                    TokenType.ANTI,\n+                    TokenType.SEMI,\n+                }\n+            )\n \n         return klass\n \n@@ -792,11 +810,15 @@ class Dialect(metaclass=_Dialect):\n             for expr_type in expressions\n         },\n         exp.Abs: lambda self, e: self._annotate_by_args(e, \"this\"),\n-        exp.Anonymous: lambda self, e: self._annotate_with_type(e, exp.DataType.Type.UNKNOWN),\n+        exp.Anonymous: lambda self, e: self._annotate_with_type(\n+            e, exp.DataType.Type.UNKNOWN\n+        ),\n         exp.Array: lambda self, e: self._annotate_by_args(e, \"expressions\", array=True),\n         exp.AnyValue: lambda self, e: self._annotate_by_args(e, \"this\"),\n         exp.ArrayAgg: lambda self, e: self._annotate_by_args(e, \"this\", array=True),\n-        exp.ArrayConcat: lambda self, e: self._annotate_by_args(e, \"this\", \"expressions\"),\n+        exp.ArrayConcat: lambda self, e: self._annotate_by_args(\n+            e, \"this\", \"expressions\"\n+        ),\n         exp.ArrayConcatAgg: lambda self, e: self._annotate_by_args(e, \"this\"),\n         exp.ArrayFirst: lambda self, e: self._annotate_by_array_element(e),\n         exp.ArrayLast: lambda self, e: self._annotate_by_array_element(e),\n@@ -807,7 +829,10 @@ class Dialect(metaclass=_Dialect):\n         exp.Case: lambda self, e: self._annotate_by_args(e, \"default\", \"ifs\"),\n         exp.Coalesce: lambda self, e: self._annotate_by_args(e, \"this\", \"expressions\"),\n         exp.Count: lambda self, e: self._annotate_with_type(\n-            e, exp.DataType.Type.BIGINT if e.args.get(\"big_int\") else exp.DataType.Type.INT\n+            e,\n+            exp.DataType.Type.BIGINT\n+            if e.args.get(\"big_int\")\n+            else exp.DataType.Type.INT,\n         ),\n         exp.DataType: lambda self, e: self._annotate_with_type(e, e.copy()),\n         exp.DateAdd: lambda self, e: self._annotate_timeunit(e),\n@@ -839,13 +864,19 @@ class Dialect(metaclass=_Dialect):\n         exp.Null: lambda self, e: self._annotate_with_type(e, exp.DataType.Type.NULL),\n         exp.Nullif: lambda self, e: self._annotate_by_args(e, \"this\", \"expression\"),\n         exp.PropertyEQ: lambda self, e: self._annotate_by_args(e, \"expression\"),\n-        exp.Slice: lambda self, e: self._annotate_with_type(e, exp.DataType.Type.UNKNOWN),\n+        exp.Slice: lambda self, e: self._annotate_with_type(\n+            e, exp.DataType.Type.UNKNOWN\n+        ),\n         exp.Struct: lambda self, e: self._annotate_struct(e),\n-        exp.Sum: lambda self, e: self._annotate_by_args(e, \"this\", \"expressions\", promote=True),\n+        exp.Sum: lambda self, e: self._annotate_by_args(\n+            e, \"this\", \"expressions\", promote=True\n+        ),\n         exp.SortArray: lambda self, e: self._annotate_by_args(e, \"this\"),\n         exp.Timestamp: lambda self, e: self._annotate_with_type(\n             e,\n-            exp.DataType.Type.TIMESTAMPTZ if e.args.get(\"with_tz\") else exp.DataType.Type.TIMESTAMP,\n+            exp.DataType.Type.TIMESTAMPTZ\n+            if e.args.get(\"with_tz\")\n+            else exp.DataType.Type.TIMESTAMP,\n         ),\n         exp.ToMap: lambda self, e: self._annotate_to_map(e),\n         exp.TryCast: lambda self, e: self._annotate_with_type(e, e.args[\"to\"]),\n@@ -912,7 +943,9 @@ class Dialect(metaclass=_Dialect):\n \n             result = cls.get(dialect_name.strip())\n             if not result:\n-                suggest_closest_match_and_fail(\"dialect\", dialect_name, list(DIALECT_MODULE_NAMES))\n+                suggest_closest_match_and_fail(\n+                    \"dialect\", dialect_name, list(DIALECT_MODULE_NAMES)\n+                )\n \n             assert result is not None\n             return result(**kwargs)\n@@ -931,7 +964,9 @@ class Dialect(metaclass=_Dialect):\n             )\n \n         if expression and expression.is_string:\n-            return exp.Literal.string(format_time(expression.this, cls.TIME_MAPPING, cls.TIME_TRIE))\n+            return exp.Literal.string(\n+                format_time(expression.this, cls.TIME_MAPPING, cls.TIME_TRIE)\n+            )\n \n         return expression\n \n@@ -942,12 +977,16 @@ class Dialect(metaclass=_Dialect):\n         if normalization_strategy is None:\n             self.normalization_strategy = self.NORMALIZATION_STRATEGY\n         else:\n-            self.normalization_strategy = NormalizationStrategy(normalization_strategy.upper())\n+            self.normalization_strategy = NormalizationStrategy(\n+                normalization_strategy.upper()\n+            )\n \n         self.settings = kwargs\n \n         for unsupported_setting in kwargs.keys() - self.SUPPORTED_SETTINGS:\n-            suggest_closest_match_and_fail(\"setting\", unsupported_setting, self.SUPPORTED_SETTINGS)\n+            suggest_closest_match_and_fail(\n+                \"setting\", unsupported_setting, self.SUPPORTED_SETTINGS\n+            )\n \n     def __eq__(self, other: t.Any) -> bool:\n         # Does not currently take dialect state into account\n@@ -1042,16 +1081,22 @@ class Dialect(metaclass=_Dialect):\n             identify: If set to `False`, the quotes will only be added if the identifier is deemed\n                 \"unsafe\", with respect to its characters and this dialect's normalization strategy.\n         \"\"\"\n-        if isinstance(expression, exp.Identifier) and not isinstance(expression.parent, exp.Func):\n+        if isinstance(expression, exp.Identifier) and not isinstance(\n+            expression.parent, exp.Func\n+        ):\n             name = expression.this\n             expression.set(\n                 \"quoted\",\n-                identify or self.case_sensitive(name) or not exp.SAFE_IDENTIFIER_RE.match(name),\n+                identify\n+                or self.case_sensitive(name)\n+                or not exp.SAFE_IDENTIFIER_RE.match(name),\n             )\n \n         return expression\n \n-    def to_json_path(self, path: t.Optional[exp.Expression]) -> t.Optional[exp.Expression]:\n+    def to_json_path(\n+        self, path: t.Optional[exp.Expression]\n+    ) -> t.Optional[exp.Expression]:\n         if isinstance(path, exp.Literal):\n             path_text = path.name\n             if path.is_number:\n@@ -1131,10 +1176,16 @@ def if_sql(\n \n def arrow_json_extract_sql(self: Generator, expression: JSON_EXTRACT_TYPE) -> str:\n     this = expression.this\n-    if self.JSON_TYPE_REQUIRED_FOR_EXTRACTION and isinstance(this, exp.Literal) and this.is_string:\n+    if (\n+        self.JSON_TYPE_REQUIRED_FOR_EXTRACTION\n+        and isinstance(this, exp.Literal)\n+        and this.is_string\n+    ):\n         this.replace(exp.cast(this, exp.DataType.Type.JSON))\n \n-    return self.binary(expression, \"->\" if isinstance(expression, exp.JSONExtract) else \"->>\")\n+    return self.binary(\n+        expression, \"->\" if isinstance(expression, exp.JSONExtract) else \"->>\"\n+    )\n \n \n def inline_array_sql(self: Generator, expression: exp.Array) -> str:\n@@ -1151,7 +1202,8 @@ def inline_array_unless_query(self: Generator, expression: exp.Array) -> str:\n def no_ilike_sql(self: Generator, expression: exp.ILike) -> str:\n     return self.like_sql(\n         exp.Like(\n-            this=exp.Lower(this=expression.this), expression=exp.Lower(this=expression.expression)\n+            this=exp.Lower(this=expression.this),\n+            expression=exp.Lower(this=expression.expression),\n         )\n     )\n \n@@ -1221,16 +1273,24 @@ def strposition_sql(\n         string = exp.Substring(this=string, start=position)\n \n     if func_name == \"POSITION\" and use_ansi_position:\n-        func = exp.Anonymous(this=func_name, expressions=[exp.In(this=substr, field=string)])\n+        func = exp.Anonymous(\n+            this=func_name, expressions=[exp.In(this=substr, field=string)]\n+        )\n     else:\n-        args = [substr, string] if func_name in (\"LOCATE\", \"CHARINDEX\") else [string, substr]\n+        args = (\n+            [substr, string]\n+            if func_name in (\"LOCATE\", \"CHARINDEX\")\n+            else [string, substr]\n+        )\n         if supports_position:\n             args.append(position)\n         if occurrence:\n             if supports_occurrence:\n                 args.append(occurrence)\n             else:\n-                self.unsupported(f\"{func_name} does not support the occurrence parameter.\")\n+                self.unsupported(\n+                    f\"{func_name} does not support the occurrence parameter.\"\n+                )\n         func = exp.Anonymous(this=func_name, expressions=args)\n \n     if transpile_position:\n@@ -1242,9 +1302,7 @@ def strposition_sql(\n \n \n def struct_extract_sql(self: Generator, expression: exp.StructExtract) -> str:\n-    return (\n-        f\"{self.sql(expression, 'this')}.{self.sql(exp.to_identifier(expression.expression.name))}\"\n-    )\n+    return f\"{self.sql(expression, 'this')}.{self.sql(exp.to_identifier(expression.expression.name))}\"\n \n \n def var_map_sql(\n@@ -1284,7 +1342,9 @@ def build_formatted_time(\n             this=seq_get(args, 0),\n             format=Dialect[dialect].format_time(\n                 seq_get(args, 1)\n-                or (Dialect[dialect].TIME_FORMAT if default is True else default or None)\n+                or (\n+                    Dialect[dialect].TIME_FORMAT if default is True else default or None\n+                )\n             ),\n         )\n \n@@ -1294,13 +1354,19 @@ def build_formatted_time(\n def time_format(\n     dialect: DialectType = None,\n ) -> t.Callable[[Generator, exp.UnixToStr | exp.StrToUnix], t.Optional[str]]:\n-    def _time_format(self: Generator, expression: exp.UnixToStr | exp.StrToUnix) -> t.Optional[str]:\n+    def _time_format(\n+        self: Generator, expression: exp.UnixToStr | exp.StrToUnix\n+    ) -> t.Optional[str]:\n         \"\"\"\n         Returns the time format for a given expression, unless it's equivalent\n         to the default time format of the dialect of interest.\n         \"\"\"\n         time_format = self.format_time(expression)\n-        return time_format if time_format != Dialect.get_or_raise(dialect).TIME_FORMAT else None\n+        return (\n+            time_format\n+            if time_format != Dialect.get_or_raise(dialect).TIME_FORMAT\n+            else None\n+        )\n \n     return _time_format\n \n@@ -1318,7 +1384,11 @@ def build_date_delta(\n         unit = None\n         if unit_based or default_unit:\n             unit = args[0] if unit_based else exp.Literal.string(default_unit)\n-            unit = exp.var(unit_mapping.get(unit.name.lower(), unit.name)) if unit_mapping else unit\n+            unit = (\n+                exp.var(unit_mapping.get(unit.name.lower(), unit.name))\n+                if unit_mapping\n+                else unit\n+            )\n         expression = exp_class(this=this, expression=seq_get(args, 1), unit=unit)\n         if supports_timezone and has_timezone:\n             expression.set(\"zone\", args[-1])\n@@ -1339,7 +1409,9 @@ def build_date_delta_with_interval(\n         if not isinstance(interval, exp.Interval):\n             raise ParseError(f\"INTERVAL expression expected but got '{interval}'\")\n \n-        return expression_class(this=args[0], expression=interval.this, unit=unit_to_str(interval))\n+        return expression_class(\n+            this=args[0], expression=interval.this, unit=unit_to_str(interval)\n+        )\n \n     return _builder\n \n@@ -1358,13 +1430,17 @@ def date_add_interval_sql(\n ) -> t.Callable[[Generator, exp.Expression], str]:\n     def func(self: Generator, expression: exp.Expression) -> str:\n         this = self.sql(expression, \"this\")\n-        interval = exp.Interval(this=expression.expression, unit=unit_to_var(expression))\n+        interval = exp.Interval(\n+            this=expression.expression, unit=unit_to_var(expression)\n+        )\n         return f\"{data_type}_{kind}({this}, {self.sql(interval)})\"\n \n     return func\n \n \n-def timestamptrunc_sql(zone: bool = False) -> t.Callable[[Generator, exp.TimestampTrunc], str]:\n+def timestamptrunc_sql(\n+    zone: bool = False,\n+) -> t.Callable[[Generator, exp.TimestampTrunc], str]:\n     def _timestamptrunc_sql(self: Generator, expression: exp.TimestampTrunc) -> str:\n         args = [unit_to_str(expression), expression.this]\n         if zone:\n@@ -1380,7 +1456,8 @@ def no_timestamp_sql(self: Generator, expression: exp.Timestamp) -> str:\n         from sqlglot.optimizer.annotate_types import annotate_types\n \n         target_type = (\n-            annotate_types(expression, dialect=self.dialect).type or exp.DataType.Type.TIMESTAMP\n+            annotate_types(expression, dialect=self.dialect).type\n+            or exp.DataType.Type.TIMESTAMP\n         )\n         return self.sql(exp.cast(expression.this, target_type))\n     if zone.name.lower() in TIMEZONES:\n@@ -1397,7 +1474,8 @@ def no_time_sql(self: Generator, expression: exp.Time) -> str:\n     # Transpile BQ's TIME(timestamp, zone) to CAST(TIMESTAMPTZ <timestamp> AT TIME ZONE <zone> AS TIME)\n     this = exp.cast(expression.this, exp.DataType.Type.TIMESTAMPTZ)\n     expr = exp.cast(\n-        exp.AtTimeZone(this=this, zone=expression.args.get(\"zone\")), exp.DataType.Type.TIME\n+        exp.AtTimeZone(this=this, zone=expression.args.get(\"zone\")),\n+        exp.DataType.Type.TIME,\n     )\n     return self.sql(expr)\n \n@@ -1409,19 +1487,25 @@ def no_datetime_sql(self: Generator, expression: exp.Datetime) -> str:\n     if expr.name.lower() in TIMEZONES:\n         # Transpile BQ's DATETIME(timestamp, zone) to CAST(TIMESTAMPTZ <timestamp> AT TIME ZONE <zone> AS TIMESTAMP)\n         this = exp.cast(this, exp.DataType.Type.TIMESTAMPTZ)\n-        this = exp.cast(exp.AtTimeZone(this=this, zone=expr), exp.DataType.Type.TIMESTAMP)\n+        this = exp.cast(\n+            exp.AtTimeZone(this=this, zone=expr), exp.DataType.Type.TIMESTAMP\n+        )\n         return self.sql(this)\n \n     this = exp.cast(this, exp.DataType.Type.DATE)\n     expr = exp.cast(expr, exp.DataType.Type.TIME)\n \n-    return self.sql(exp.cast(exp.Add(this=this, expression=expr), exp.DataType.Type.TIMESTAMP))\n+    return self.sql(\n+        exp.cast(exp.Add(this=this, expression=expr), exp.DataType.Type.TIMESTAMP)\n+    )\n \n \n def left_to_substring_sql(self: Generator, expression: exp.Left) -> str:\n     return self.sql(\n         exp.Substring(\n-            this=expression.this, start=exp.Literal.number(1), length=expression.expression\n+            this=expression.this,\n+            start=exp.Literal.number(1),\n+            length=expression.expression,\n         )\n     )\n \n@@ -1430,7 +1514,8 @@ def right_to_substring_sql(self: Generator, expression: exp.Left) -> str:\n     return self.sql(\n         exp.Substring(\n             this=expression.this,\n-            start=exp.Length(this=expression.this) - exp.paren(expression.expression - 1),\n+            start=exp.Length(this=expression.this)\n+            - exp.paren(expression.expression - 1),\n         )\n     )\n \n@@ -1450,7 +1535,8 @@ def timestrtotime_sql(\n         precision = subsecond_precision(expression.this.name)\n         if precision > 0:\n             datatype = exp.DataType.build(\n-                datatype.this, expressions=[exp.DataTypeParam(this=exp.Literal.number(precision))]\n+                datatype.this,\n+                expressions=[exp.DataTypeParam(this=exp.Literal.number(precision))],\n             )\n \n     return self.sql(exp.cast(expression.this, datatype, dialect=self.dialect))\n@@ -1468,7 +1554,9 @@ def encode_decode_sql(\n     if charset and charset.name.lower() != \"utf-8\":\n         self.unsupported(f\"Expected utf-8 character set, got {charset}.\")\n \n-    return self.func(name, expression.this, expression.args.get(\"replace\") if replace else None)\n+    return self.func(\n+        name, expression.this, expression.args.get(\"replace\") if replace else None\n+    )\n \n \n def min_or_least(self: Generator, expression: exp.Min) -> str:\n@@ -1513,14 +1601,18 @@ def str_to_time_sql(self: Generator, expression: exp.Expression) -> str:\n \n \n def concat_to_dpipe_sql(self: Generator, expression: exp.Concat) -> str:\n-    return self.sql(reduce(lambda x, y: exp.DPipe(this=x, expression=y), expression.expressions))\n+    return self.sql(\n+        reduce(lambda x, y: exp.DPipe(this=x, expression=y), expression.expressions)\n+    )\n \n \n def concat_ws_to_dpipe_sql(self: Generator, expression: exp.ConcatWs) -> str:\n     delim, *rest_args = expression.expressions\n     return self.sql(\n         reduce(\n-            lambda x, y: exp.DPipe(this=x, expression=exp.DPipe(this=delim, expression=y)),\n+            lambda x, y: exp.DPipe(\n+                this=x, expression=exp.DPipe(this=delim, expression=y)\n+            ),\n             rest_args,\n         )\n     )\n@@ -1536,17 +1628,24 @@ def regexp_extract_sql(\n     if group and group.name == str(self.dialect.REGEXP_EXTRACT_DEFAULT_GROUP):\n         group = None\n \n-    return self.func(expression.sql_name(), expression.this, expression.expression, group)\n+    return self.func(\n+        expression.sql_name(), expression.this, expression.expression, group\n+    )\n \n \n @unsupported_args(\"position\", \"occurrence\", \"modifiers\")\n def regexp_replace_sql(self: Generator, expression: exp.RegexpReplace) -> str:\n     return self.func(\n-        \"REGEXP_REPLACE\", expression.this, expression.expression, expression.args[\"replacement\"]\n+        \"REGEXP_REPLACE\",\n+        expression.this,\n+        expression.expression,\n+        expression.args[\"replacement\"],\n     )\n \n \n-def pivot_column_names(aggregations: t.List[exp.Expression], dialect: DialectType) -> t.List[str]:\n+def pivot_column_names(\n+    aggregations: t.List[exp.Expression], dialect: DialectType\n+) -> t.List[str]:\n     names = []\n     for agg in aggregations:\n         if isinstance(agg, exp.Alias):\n@@ -1565,7 +1664,9 @@ def pivot_column_names(aggregations: t.List[exp.Expression], dialect: DialectTyp\n                     else node\n                 )\n             )\n-            names.append(agg_all_unquoted.sql(dialect=dialect, normalize_functions=\"lower\"))\n+            names.append(\n+                agg_all_unquoted.sql(dialect=dialect, normalize_functions=\"lower\")\n+            )\n \n     return names\n \n@@ -1607,9 +1708,13 @@ def generatedasidentitycolumnconstraint_sql(\n     return f\"IDENTITY({start}, {increment})\"\n \n \n-def arg_max_or_min_no_count(name: str) -> t.Callable[[Generator, exp.ArgMax | exp.ArgMin], str]:\n+def arg_max_or_min_no_count(\n+    name: str,\n+) -> t.Callable[[Generator, exp.ArgMax | exp.ArgMin], str]:\n     @unsupported_args(\"count\")\n-    def _arg_max_or_min_sql(self: Generator, expression: exp.ArgMax | exp.ArgMin) -> str:\n+    def _arg_max_or_min_sql(\n+        self: Generator, expression: exp.ArgMax | exp.ArgMin\n+    ) -> str:\n         return self.func(name, expression.this, expression.expression)\n \n     return _arg_max_or_min_sql\n@@ -1628,7 +1733,9 @@ def ts_or_ds_add_cast(expression: exp.TsOrDsAdd) -> exp.TsOrDsAdd:\n     return expression\n \n \n-def date_delta_sql(name: str, cast: bool = False) -> t.Callable[[Generator, DATE_ADD_OR_DIFF], str]:\n+def date_delta_sql(\n+    name: str, cast: bool = False\n+) -> t.Callable[[Generator, DATE_ADD_OR_DIFF], str]:\n     def _delta_sql(self: Generator, expression: DATE_ADD_OR_DIFF) -> str:\n         if cast and isinstance(expression, exp.TsOrDsAdd):\n             expression = ts_or_ds_add_cast(expression)\n@@ -1643,7 +1750,9 @@ def date_delta_sql(name: str, cast: bool = False) -> t.Callable[[Generator, DATE\n     return _delta_sql\n \n \n-def unit_to_str(expression: exp.Expression, default: str = \"DAY\") -> t.Optional[exp.Expression]:\n+def unit_to_str(\n+    expression: exp.Expression, default: str = \"DAY\"\n+) -> t.Optional[exp.Expression]:\n     unit = expression.args.get(\"unit\")\n \n     if isinstance(unit, exp.Placeholder):\n@@ -1653,7 +1762,9 @@ def unit_to_str(expression: exp.Expression, default: str = \"DAY\") -> t.Optional[\n     return exp.Literal.string(default) if default else None\n \n \n-def unit_to_var(expression: exp.Expression, default: str = \"DAY\") -> t.Optional[exp.Expression]:\n+def unit_to_var(\n+    expression: exp.Expression, default: str = \"DAY\"\n+) -> t.Optional[exp.Expression]:\n     unit = expression.args.get(\"unit\")\n \n     if isinstance(unit, (exp.Var, exp.Placeholder, exp.WeekStart)):\n@@ -1675,7 +1786,9 @@ def map_date_part(\n \n def map_date_part(part, dialect: DialectType = Dialect):\n     mapped = (\n-        Dialect.get_or_raise(dialect).DATE_PART_MAPPING.get(part.name.upper()) if part else None\n+        Dialect.get_or_raise(dialect).DATE_PART_MAPPING.get(part.name.upper())\n+        if part\n+        else None\n     )\n     if mapped:\n         return exp.Literal.string(mapped) if part.is_string else exp.var(mapped)\n@@ -1696,7 +1809,9 @@ def merge_without_target_sql(self: Generator, expression: exp.Merge) -> str:\n     alias = expression.this.args.get(\"alias\")\n \n     def normalize(identifier: t.Optional[exp.Identifier]) -> t.Optional[str]:\n-        return self.dialect.normalize_identifier(identifier).name if identifier else None\n+        return (\n+            self.dialect.normalize_identifier(identifier).name if identifier else None\n+        )\n \n     targets = {normalize(expression.this.this)}\n \n@@ -1728,7 +1843,9 @@ def merge_without_target_sql(self: Generator, expression: exp.Merge) -> str:\n \n \n def build_json_extract_path(\n-    expr_type: t.Type[F], zero_based_indexing: bool = True, arrow_req_json_type: bool = False\n+    expr_type: t.Type[F],\n+    zero_based_indexing: bool = True,\n+    arrow_req_json_type: bool = False,\n ) -> t.Callable[[t.List], F]:\n     def _builder(args: t.List) -> F:\n         segments: t.List[exp.JSONPathPart] = [exp.JSONPathRoot()]\n@@ -1741,7 +1858,9 @@ def build_json_extract_path(\n             if is_int(text) and (not arrow_req_json_type or not arg.is_string):\n                 index = int(text)\n                 segments.append(\n-                    exp.JSONPathSubscript(this=index if zero_based_indexing else index - 1)\n+                    exp.JSONPathSubscript(\n+                        this=index if zero_based_indexing else index - 1\n+                    )\n                 )\n             else:\n                 segments.append(exp.JSONPathKey(this=text))\n@@ -1812,7 +1931,9 @@ def filter_array_using_unnest(\n         return \"\"\n \n     unnest = exp.Unnest(expressions=[expression.this])\n-    filtered = exp.select(alias).from_(exp.alias_(unnest, None, table=[alias])).where(cond)\n+    filtered = (\n+        exp.select(alias).from_(exp.alias_(unnest, None, table=[alias])).where(cond)\n+    )\n     return self.sql(exp.Array(expressions=[filtered]))\n \n \n@@ -1821,7 +1942,8 @@ def remove_from_array_using_filter(self: Generator, expression: exp.ArrayRemove)\n     cond = exp.NEQ(this=lambda_id, expression=expression.expression)\n     return self.sql(\n         exp.ArrayFilter(\n-            this=expression.this, expression=exp.Lambda(this=cond, expressions=[lambda_id])\n+            this=expression.this,\n+            expression=exp.Lambda(this=cond, expressions=[lambda_id]),\n         )\n     )\n \n@@ -1861,7 +1983,9 @@ def sha256_sql(self: Generator, expression: exp.SHA2) -> str:\n     return self.func(f\"SHA{expression.text('length') or '256'}\", expression.this)\n \n \n-def sequence_sql(self: Generator, expression: exp.GenerateSeries | exp.GenerateDateArray) -> str:\n+def sequence_sql(\n+    self: Generator, expression: exp.GenerateSeries | exp.GenerateDateArray\n+) -> str:\n     start = expression.args.get(\"start\")\n     end = expression.args.get(\"end\")\n     step = expression.args.get(\"step\")\n@@ -1887,7 +2011,8 @@ def build_regexp_extract(expr_type: t.Type[E]) -> t.Callable[[t.List, Dialect],\n         return expr_type(\n             this=seq_get(args, 0),\n             expression=seq_get(args, 1),\n-            group=seq_get(args, 2) or exp.Literal.number(dialect.REGEXP_EXTRACT_DEFAULT_GROUP),\n+            group=seq_get(args, 2)\n+            or exp.Literal.number(dialect.REGEXP_EXTRACT_DEFAULT_GROUP),\n             parameters=seq_get(args, 3),\n         )\n \n@@ -1909,11 +2034,17 @@ def explode_to_unnest_sql(self: Generator, expression: exp.Lateral) -> str:\n     return self.lateral_sql(expression)\n \n \n-def timestampdiff_sql(self: Generator, expression: exp.DatetimeDiff | exp.TimestampDiff) -> str:\n-    return self.func(\"TIMESTAMPDIFF\", expression.unit, expression.expression, expression.this)\n+def timestampdiff_sql(\n+    self: Generator, expression: exp.DatetimeDiff | exp.TimestampDiff\n+) -> str:\n+    return self.func(\n+        \"TIMESTAMPDIFF\", expression.unit, expression.expression, expression.this\n+    )\n \n \n-def no_make_interval_sql(self: Generator, expression: exp.MakeInterval, sep: str = \", \") -> str:\n+def no_make_interval_sql(\n+    self: Generator, expression: exp.MakeInterval, sep: str = \", \"\n+) -> str:\n     args = []\n     for unit, value in expression.args.items():\n         if isinstance(value, exp.Kwarg):\n@@ -1941,7 +2072,9 @@ def groupconcat_sql(\n     separator = self.sql(expression.args.get(\"separator\") or exp.Literal.string(sep))\n \n     on_overflow_sql = self.sql(expression, \"on_overflow\")\n-    on_overflow_sql = f\" ON OVERFLOW {on_overflow_sql}\" if (on_overflow and on_overflow_sql) else \"\"\n+    on_overflow_sql = (\n+        f\" ON OVERFLOW {on_overflow_sql}\" if (on_overflow and on_overflow_sql) else \"\"\n+    )\n \n     order = this.find(exp.Order)\n \n@@ -1955,12 +2088,16 @@ def groupconcat_sql(\n         if within_group:\n             listagg = exp.WithinGroup(this=listagg, expression=order)\n         else:\n-            listagg.set(\"expressions\", [f\"{args}{self.sql(expression=expression.this)}\"])\n+            listagg.set(\n+                \"expressions\", [f\"{args}{self.sql(expression=expression.this)}\"]\n+            )\n \n     return self.sql(listagg)\n \n \n-def build_timetostr_or_tochar(args: t.List, dialect: Dialect) -> exp.TimeToStr | exp.ToChar:\n+def build_timetostr_or_tochar(\n+    args: t.List, dialect: Dialect\n+) -> exp.TimeToStr | exp.ToChar:\n     if len(args) == 2:\n         this = args[0]\n         if not this.type:\ndiff --git a/sqlglot/dialects/duckdb.py b/sqlglot/dialects/duckdb.py\nindex baaebed62..4136f3200 100644\n--- a/sqlglot/dialects/duckdb.py\n+++ b/sqlglot/dialects/duckdb.py\n@@ -46,7 +46,12 @@ from sqlglot.tokens import TokenType\n from sqlglot.parser import binary_range_parser\n \n DATETIME_DELTA = t.Union[\n-    exp.DateAdd, exp.TimeAdd, exp.DatetimeAdd, exp.TsOrDsAdd, exp.DateSub, exp.DatetimeSub\n+    exp.DateAdd,\n+    exp.TimeAdd,\n+    exp.DatetimeAdd,\n+    exp.TsOrDsAdd,\n+    exp.DateSub,\n+    exp.DatetimeSub,\n ]\n \n \n@@ -55,7 +60,9 @@ def _date_delta_sql(self: DuckDB.Generator, expression: DATETIME_DELTA) -> str:\n     unit = unit_to_var(expression)\n     op = (\n         \"+\"\n-        if isinstance(expression, (exp.DateAdd, exp.TimeAdd, exp.DatetimeAdd, exp.TsOrDsAdd))\n+        if isinstance(\n+            expression, (exp.DateAdd, exp.TimeAdd, exp.DatetimeAdd, exp.TsOrDsAdd)\n+        )\n         else \"-\"\n     )\n \n@@ -73,7 +80,9 @@ def _date_delta_sql(self: DuckDB.Generator, expression: DATETIME_DELTA) -> str:\n     this = exp.cast(this, to_type) if to_type else this\n \n     expr = expression.expression\n-    interval = expr if isinstance(expr, exp.Interval) else exp.Interval(this=expr, unit=unit)\n+    interval = (\n+        expr if isinstance(expr, exp.Interval) else exp.Interval(this=expr, unit=unit)\n+    )\n \n     return f\"{self.sql(this)} {op} {self.sql(interval)}\"\n \n@@ -109,7 +118,11 @@ def _array_sort_sql(self: DuckDB.Generator, expression: exp.ArraySort) -> str:\n \n \n def _sort_array_sql(self: DuckDB.Generator, expression: exp.SortArray) -> str:\n-    name = \"ARRAY_REVERSE_SORT\" if expression.args.get(\"asc\") == exp.false() else \"ARRAY_SORT\"\n+    name = (\n+        \"ARRAY_REVERSE_SORT\"\n+        if expression.args.get(\"asc\") == exp.false()\n+        else \"ARRAY_SORT\"\n+    )\n     return self.func(name, expression.this)\n \n \n@@ -118,10 +131,14 @@ def _build_sort_array_desc(args: t.List) -> exp.Expression:\n \n \n def _build_date_diff(args: t.List) -> exp.Expression:\n-    return exp.DateDiff(this=seq_get(args, 2), expression=seq_get(args, 1), unit=seq_get(args, 0))\n+    return exp.DateDiff(\n+        this=seq_get(args, 2), expression=seq_get(args, 1), unit=seq_get(args, 0)\n+    )\n \n \n-def _build_generate_series(end_exclusive: bool = False) -> t.Callable[[t.List], exp.GenerateSeries]:\n+def _build_generate_series(\n+    end_exclusive: bool = False,\n+) -> t.Callable[[t.List], exp.GenerateSeries]:\n     def _builder(args: t.List) -> exp.GenerateSeries:\n         # Check https://duckdb.org/docs/sql/functions/nested.html#range-functions\n         if len(args) == 1:\n@@ -150,7 +167,9 @@ def _build_make_timestamp(args: t.List) -> exp.Expression:\n     )\n \n \n-def _show_parser(*args: t.Any, **kwargs: t.Any) -> t.Callable[[DuckDB.Parser], exp.Show]:\n+def _show_parser(\n+    *args: t.Any, **kwargs: t.Any\n+) -> t.Callable[[DuckDB.Parser], exp.Show]:\n     def _parse(self: DuckDB.Parser) -> exp.Show:\n         return self._parse_show_duckdb(*args, **kwargs)\n \n@@ -226,13 +245,17 @@ def _unix_to_time_sql(self: DuckDB.Generator, expression: exp.UnixToTime) -> str\n     if scale == exp.UnixToTime.MICROS:\n         return self.func(\"MAKE_TIMESTAMP\", timestamp)\n \n-    return self.func(\"TO_TIMESTAMP\", exp.Div(this=timestamp, expression=exp.func(\"POW\", 10, scale)))\n+    return self.func(\n+        \"TO_TIMESTAMP\", exp.Div(this=timestamp, expression=exp.func(\"POW\", 10, scale))\n+    )\n \n \n WRAPPED_JSON_EXTRACT_EXPRESSIONS = (exp.Binary, exp.Bracket, exp.In)\n \n \n-def _arrow_json_extract_sql(self: DuckDB.Generator, expression: JSON_EXTRACT_TYPE) -> str:\n+def _arrow_json_extract_sql(\n+    self: DuckDB.Generator, expression: JSON_EXTRACT_TYPE\n+) -> str:\n     arrow_sql = arrow_json_extract_sql(self, expression)\n     if not expression.same_parent and isinstance(\n         expression.parent, WRAPPED_JSON_EXTRACT_EXPRESSIONS\n@@ -255,11 +278,16 @@ def _date_diff_sql(self: DuckDB.Generator, expression: exp.DateDiff) -> str:\n \n \n def _generate_datetime_array_sql(\n-    self: DuckDB.Generator, expression: t.Union[exp.GenerateDateArray, exp.GenerateTimestampArray]\n+    self: DuckDB.Generator,\n+    expression: t.Union[exp.GenerateDateArray, exp.GenerateTimestampArray],\n ) -> str:\n     is_generate_date_array = isinstance(expression, exp.GenerateDateArray)\n \n-    type = exp.DataType.Type.DATE if is_generate_date_array else exp.DataType.Type.TIMESTAMP\n+    type = (\n+        exp.DataType.Type.DATE\n+        if is_generate_date_array\n+        else exp.DataType.Type.TIMESTAMP\n+    )\n     start = _implicit_datetime_cast(expression.args.get(\"start\"), type=type)\n     end = _implicit_datetime_cast(expression.args.get(\"end\"), type=type)\n \n@@ -279,8 +307,12 @@ def _generate_datetime_array_sql(\n def _json_extract_value_array_sql(\n     self: DuckDB.Generator, expression: exp.JSONValueArray | exp.JSONExtractArray\n ) -> str:\n-    json_extract = exp.JSONExtract(this=expression.this, expression=expression.expression)\n-    data_type = \"ARRAY<STRING>\" if isinstance(expression, exp.JSONValueArray) else \"ARRAY<JSON>\"\n+    json_extract = exp.JSONExtract(\n+        this=expression.this, expression=expression.expression\n+    )\n+    data_type = (\n+        \"ARRAY<STRING>\" if isinstance(expression, exp.JSONValueArray) else \"ARRAY<JSON>\"\n+    )\n     return self.sql(exp.cast(json_extract, to=exp.DataType.build(data_type)))\n \n \n@@ -304,7 +336,9 @@ class DuckDB(Dialect):\n     }\n     DATE_PART_MAPPING.pop(\"WEEKDAY\")\n \n-    def to_json_path(self, path: t.Optional[exp.Expression]) -> t.Optional[exp.Expression]:\n+    def to_json_path(\n+        self, path: t.Optional[exp.Expression]\n+    ) -> t.Optional[exp.Expression]:\n         if isinstance(path, exp.Literal):\n             # DuckDB also supports the JSON pointer syntax, where every path starts with a `/`.\n             # Additionally, it allows accessing the back of lists using the `[#-i]` syntax.\n@@ -387,7 +421,10 @@ class DuckDB(Dialect):\n             TokenType.DSTAR: exp.Pow,\n         }\n \n-        FUNCTIONS_WITH_ALIASED_ARGS = {*parser.Parser.FUNCTIONS_WITH_ALIASED_ARGS, \"STRUCT_PACK\"}\n+        FUNCTIONS_WITH_ALIASED_ARGS = {\n+            *parser.Parser.FUNCTIONS_WITH_ALIASED_ARGS,\n+            \"STRUCT_PACK\",\n+        }\n \n         SHOW_PARSERS = {\n             \"TABLES\": _show_parser(\"TABLES\"),\n@@ -416,7 +453,9 @@ class DuckDB(Dialect):\n             \"GENERATE_SERIES\": _build_generate_series(),\n             \"JSON\": exp.ParseJSON.from_arg_list,\n             \"JSON_EXTRACT_PATH\": parser.build_extract_json_with_path(exp.JSONExtract),\n-            \"JSON_EXTRACT_STRING\": parser.build_extract_json_with_path(exp.JSONExtractScalar),\n+            \"JSON_EXTRACT_STRING\": parser.build_extract_json_with_path(\n+                exp.JSONExtractScalar\n+            ),\n             \"LIST_CONTAINS\": exp.ArrayContains.from_arg_list,\n             \"LIST_HAS\": exp.ArrayContains.from_arg_list,\n             \"LIST_HAS_ANY\": exp.ArrayOverlaps.from_arg_list,\n@@ -437,7 +476,9 @@ class DuckDB(Dialect):\n                 replacement=seq_get(args, 2),\n                 modifiers=seq_get(args, 3),\n             ),\n-            \"SHA256\": lambda args: exp.SHA2(this=seq_get(args, 0), length=exp.Literal.number(256)),\n+            \"SHA256\": lambda args: exp.SHA2(\n+                this=seq_get(args, 0), length=exp.Literal.number(256)\n+            ),\n             \"STRFTIME\": build_formatted_time(exp.TimeToStr, \"duckdb\"),\n             \"STRING_SPLIT\": exp.Split.from_arg_list,\n             \"STRING_SPLIT_REGEX\": exp.RegexpSplit.from_arg_list,\n@@ -458,7 +499,8 @@ class DuckDB(Dialect):\n         FUNCTION_PARSERS = {\n             **parser.Parser.FUNCTION_PARSERS,\n             **dict.fromkeys(\n-                (\"GROUP_CONCAT\", \"LISTAGG\", \"STRINGAGG\"), lambda self: self._parse_string_agg()\n+                (\"GROUP_CONCAT\", \"LISTAGG\", \"STRINGAGG\"),\n+                lambda self: self._parse_string_agg(),\n             ),\n         }\n         FUNCTION_PARSERS.pop(\"DECODE\")\n@@ -485,7 +527,9 @@ class DuckDB(Dialect):\n \n         TYPE_CONVERTERS = {\n             # https://duckdb.org/docs/sql/data_types/numeric\n-            exp.DataType.Type.DECIMAL: build_default_decimal_type(precision=18, scale=3),\n+            exp.DataType.Type.DECIMAL: build_default_decimal_type(\n+                precision=18, scale=3\n+            ),\n             # https://duckdb.org/docs/sql/data_types/text\n             exp.DataType.Type.TEXT: lambda dtype: exp.DataType.build(\"TEXT\"),\n         }\n@@ -513,7 +557,9 @@ class DuckDB(Dialect):\n                 return None\n \n             this = self._replace_lambda(self._parse_assignment(), expressions)\n-            return self.expression(exp.Lambda, this=this, expressions=expressions, colon=True)\n+            return self.expression(\n+                exp.Lambda, this=this, expressions=expressions, colon=True\n+            )\n \n         def _parse_expression(self) -> t.Optional[exp.Expression]:\n             # DuckDB supports prefix aliases, e.g. foo: 1\n@@ -527,7 +573,9 @@ class DuckDB(Dialect):\n                     # Moves the comment next to the alias in `alias: expr /* comment */`\n                     comments += this.pop_comments() or []\n \n-                return self.expression(exp.Alias, comments=comments, this=this, alias=alias)\n+                return self.expression(\n+                    exp.Alias, comments=comments, this=this, alias=alias\n+                )\n \n             return super()._parse_expression()\n \n@@ -568,7 +616,9 @@ class DuckDB(Dialect):\n \n             return table\n \n-        def _parse_table_sample(self, as_modifier: bool = False) -> t.Optional[exp.TableSample]:\n+        def _parse_table_sample(\n+            self, as_modifier: bool = False\n+        ) -> t.Optional[exp.TableSample]:\n             # https://duckdb.org/docs/sql/samples.html\n             sample = super()._parse_table_sample(as_modifier=as_modifier)\n             if sample and not sample.args.get(\"method\"):\n@@ -584,7 +634,9 @@ class DuckDB(Dialect):\n         ) -> t.Optional[exp.Expression]:\n             bracket = super()._parse_bracket(this)\n \n-            if self.dialect.version < Version(\"1.2.0\") and isinstance(bracket, exp.Bracket):\n+            if self.dialect.version < Version(\"1.2.0\") and isinstance(\n+                bracket, exp.Bracket\n+            ):\n                 # https://duckdb.org/2025/02/05/announcing-duckdb-120.html#breaking-changes\n                 bracket.set(\"returns_list_for_maps\", True)\n \n@@ -595,12 +647,18 @@ class DuckDB(Dialect):\n                 return self.expression(exp.ToMap, this=self._parse_bracket())\n \n             args = self._parse_wrapped_csv(self._parse_assignment)\n-            return self.expression(exp.Map, keys=seq_get(args, 0), values=seq_get(args, 1))\n+            return self.expression(\n+                exp.Map, keys=seq_get(args, 0), values=seq_get(args, 1)\n+            )\n \n-        def _parse_struct_types(self, type_required: bool = False) -> t.Optional[exp.Expression]:\n+        def _parse_struct_types(\n+            self, type_required: bool = False\n+        ) -> t.Optional[exp.Expression]:\n             return self._parse_field_def()\n \n-        def _pivot_column_names(self, aggregations: t.List[exp.Expression]) -> t.List[str]:\n+        def _pivot_column_names(\n+            self, aggregations: t.List[exp.Expression]\n+        ) -> t.List[str]:\n             if len(aggregations) == 1:\n                 return super()._pivot_column_names(aggregations)\n             return pivot_column_names(aggregations, dialect=\"duckdb\")\n@@ -623,7 +681,9 @@ class DuckDB(Dialect):\n                 expressions = None\n \n             return (\n-                self.expression(exp.Attach, this=this, exists=exists, expressions=expressions)\n+                self.expression(\n+                    exp.Attach, this=this, exists=exists, expressions=expressions\n+                )\n                 if is_attach\n                 else self.expression(exp.Detach, this=this, exists=exists)\n             )\n@@ -696,14 +756,22 @@ class DuckDB(Dialect):\n             exp.DatetimeAdd: _date_delta_sql,\n             exp.DateToDi: lambda self,\n             e: f\"CAST(STRFTIME({self.sql(e, 'this')}, {DuckDB.DATEINT_FORMAT}) AS INT)\",\n-            exp.Decode: lambda self, e: encode_decode_sql(self, e, \"DECODE\", replace=False),\n+            exp.Decode: lambda self, e: encode_decode_sql(\n+                self, e, \"DECODE\", replace=False\n+            ),\n             exp.DiToDate: lambda self,\n             e: f\"CAST(STRPTIME(CAST({self.sql(e, 'this')} AS TEXT), {DuckDB.DATEINT_FORMAT}) AS DATE)\",\n-            exp.Encode: lambda self, e: encode_decode_sql(self, e, \"ENCODE\", replace=False),\n+            exp.Encode: lambda self, e: encode_decode_sql(\n+                self, e, \"ENCODE\", replace=False\n+            ),\n             exp.GenerateDateArray: _generate_datetime_array_sql,\n             exp.GenerateTimestampArray: _generate_datetime_array_sql,\n-            exp.GroupConcat: lambda self, e: groupconcat_sql(self, e, within_group=False),\n-            exp.HexString: lambda self, e: self.hexstring_sql(e, binary_function_repr=\"FROM_HEX\"),\n+            exp.GroupConcat: lambda self, e: groupconcat_sql(\n+                self, e, within_group=False\n+            ),\n+            exp.HexString: lambda self, e: self.hexstring_sql(\n+                e, binary_function_repr=\"FROM_HEX\"\n+            ),\n             exp.Explode: rename_func(\"UNNEST\"),\n             exp.IntDiv: lambda self, e: self.binary(e, \"//\"),\n             exp.IsInf: rename_func(\"ISINF\"),\n@@ -743,7 +811,9 @@ class DuckDB(Dialect):\n             ),\n             exp.RegexpSplit: rename_func(\"STR_SPLIT_REGEX\"),\n             exp.Return: lambda self, e: self.sql(e, \"this\"),\n-            exp.ReturnsProperty: lambda self, e: \"TABLE\" if isinstance(e.this, exp.Schema) else \"\",\n+            exp.ReturnsProperty: lambda self, e: \"TABLE\"\n+            if isinstance(e.this, exp.Schema)\n+            else \"\",\n             exp.Rand: rename_func(\"RANDOM\"),\n             exp.SHA: rename_func(\"SHA1\"),\n             exp.SHA2: sha256_sql,\n@@ -763,12 +833,16 @@ class DuckDB(Dialect):\n                 \"DATE_DIFF\", exp.Literal.string(e.unit), e.expression, e.this\n             ),\n             exp.TimestampTrunc: timestamptrunc_sql(),\n-            exp.TimeStrToDate: lambda self, e: self.sql(exp.cast(e.this, exp.DataType.Type.DATE)),\n+            exp.TimeStrToDate: lambda self, e: self.sql(\n+                exp.cast(e.this, exp.DataType.Type.DATE)\n+            ),\n             exp.TimeStrToTime: timestrtotime_sql,\n             exp.TimeStrToUnix: lambda self, e: self.func(\n                 \"EPOCH\", exp.cast(e.this, exp.DataType.Type.TIMESTAMP)\n             ),\n-            exp.TimeToStr: lambda self, e: self.func(\"STRFTIME\", e.this, self.format_time(e)),\n+            exp.TimeToStr: lambda self, e: self.func(\n+                \"STRFTIME\", e.this, self.format_time(e)\n+            ),\n             exp.TimeToUnix: rename_func(\"EPOCH\"),\n             exp.TsOrDiToDi: lambda self,\n             e: f\"CAST(SUBSTR(REPLACE(CAST({self.sql(e, 'this')} AS TEXT), '-', ''), 1, 8) AS INT)\",\n@@ -783,16 +857,19 @@ class DuckDB(Dialect):\n                 \"STRFTIME\", self.func(\"TO_TIMESTAMP\", e.this), self.format_time(e)\n             ),\n             exp.DatetimeTrunc: lambda self, e: self.func(\n-                \"DATE_TRUNC\", unit_to_str(e), exp.cast(e.this, exp.DataType.Type.DATETIME)\n+                \"DATE_TRUNC\",\n+                unit_to_str(e),\n+                exp.cast(e.this, exp.DataType.Type.DATETIME),\n             ),\n             exp.UnixToTime: _unix_to_time_sql,\n-            exp.UnixToTimeStr: lambda self, e: f\"CAST(TO_TIMESTAMP({self.sql(e, 'this')}) AS TEXT)\",\n+            exp.UnixToTimeStr: lambda self,\n+            e: f\"CAST(TO_TIMESTAMP({self.sql(e, 'this')}) AS TEXT)\",\n             exp.VariancePop: rename_func(\"VAR_POP\"),\n             exp.WeekOfYear: rename_func(\"WEEKOFYEAR\"),\n             exp.Xor: bool_xor_sql,\n-            exp.Levenshtein: unsupported_args(\"ins_cost\", \"del_cost\", \"sub_cost\", \"max_dist\")(\n-                rename_func(\"LEVENSHTEIN\")\n-            ),\n+            exp.Levenshtein: unsupported_args(\n+                \"ins_cost\", \"del_cost\", \"sub_cost\", \"max_dist\"\n+            )(rename_func(\"LEVENSHTEIN\")),\n             exp.JSONObjectAgg: rename_func(\"JSON_GROUP_OBJECT\"),\n             exp.JSONBObjectAgg: rename_func(\"JSON_GROUP_OBJECT\"),\n             exp.DateBin: rename_func(\"TIME_BUCKET\"),\n@@ -963,14 +1040,18 @@ class DuckDB(Dialect):\n         def parsejson_sql(self, expression: exp.ParseJSON) -> str:\n             arg = expression.this\n             if expression.args.get(\"safe\"):\n-                return self.sql(exp.case().when(exp.func(\"json_valid\", arg), arg).else_(exp.null()))\n+                return self.sql(\n+                    exp.case().when(exp.func(\"json_valid\", arg), arg).else_(exp.null())\n+                )\n             return self.func(\"JSON\", arg)\n \n         def timefromparts_sql(self, expression: exp.TimeFromParts) -> str:\n             nano = expression.args.get(\"nano\")\n             if nano is not None:\n                 expression.set(\n-                    \"sec\", expression.args[\"sec\"] + nano.pop() / exp.Literal.number(1000000000.0)\n+                    \"sec\",\n+                    expression.args[\"sec\"]\n+                    + nano.pop() / exp.Literal.number(1000000000.0),\n                 )\n \n             return rename_func(\"MAKE_TIME\")(self, expression)\n@@ -1009,7 +1090,9 @@ class DuckDB(Dialect):\n                     )\n                     expression.set(\"method\", exp.var(\"RESERVOIR\"))\n \n-            return super().tablesample_sql(expression, tablesample_keyword=tablesample_keyword)\n+            return super().tablesample_sql(\n+                expression, tablesample_keyword=tablesample_keyword\n+            )\n \n         def columndef_sql(self, expression: exp.ColumnDef, sep: str = \" \") -> str:\n             if isinstance(expression.parent, exp.UserDefinedFunction):\n@@ -1131,7 +1214,9 @@ class DuckDB(Dialect):\n                 # In BigQuery, UNNESTing a nested array leads to explosion of the top-level array & struct\n                 # This is transpiled to DDB by transforming \"FROM UNNEST(...)\" to \"FROM (SELECT UNNEST(..., max_depth => 2))\"\n                 expression.expressions.append(\n-                    exp.Kwarg(this=exp.var(\"max_depth\"), expression=exp.Literal.number(2))\n+                    exp.Kwarg(\n+                        this=exp.var(\"max_depth\"), expression=exp.Literal.number(2)\n+                    )\n                 )\n \n                 # If BQ's UNNEST is aliased, we transform it from a column alias to a table alias in DDB\n@@ -1201,7 +1286,9 @@ class DuckDB(Dialect):\n             return self.function_fallback_sql(expression)\n \n         def autoincrementcolumnconstraint_sql(self, _) -> str:\n-            self.unsupported(\"The AUTOINCREMENT column constraint is not supported by DuckDB\")\n+            self.unsupported(\n+                \"The AUTOINCREMENT column constraint is not supported by DuckDB\"\n+            )\n             return \"\"\n \n         def aliases_sql(self, expression: exp.Aliases) -> str:\n@@ -1234,7 +1321,8 @@ class DuckDB(Dialect):\n             gen_subscripts = self.sql(\n                 exp.Alias(\n                     this=exp.Anonymous(\n-                        this=\"GENERATE_SUBSCRIPTS\", expressions=[this, exp.Literal.number(1)]\n+                        this=\"GENERATE_SUBSCRIPTS\",\n+                        expressions=[this, exp.Literal.number(1)],\n                     )\n                     - exp.Literal.number(1),\n                     alias=pos,\n@@ -1243,9 +1331,13 @@ class DuckDB(Dialect):\n \n             posexplode_sql = self.format_args(gen_subscripts, unnest_sql)\n \n-            if isinstance(parent, exp.From) or (parent and isinstance(parent.parent, exp.From)):\n+            if isinstance(parent, exp.From) or (\n+                parent and isinstance(parent.parent, exp.From)\n+            ):\n                 # SELECT * FROM POSEXPLODE(col) -> SELECT * FROM (SELECT GENERATE_SUBSCRIPTS(...), UNNEST(...))\n-                return self.sql(exp.Subquery(this=exp.Select(expressions=[posexplode_sql])))\n+                return self.sql(\n+                    exp.Subquery(this=exp.Select(expressions=[posexplode_sql]))\n+                )\n \n             return posexplode_sql\n \n@@ -1258,10 +1350,14 @@ class DuckDB(Dialect):\n                 this = annotate_types(this, dialect=self.dialect)\n \n             if this.is_type(*exp.DataType.TEXT_TYPES):\n-                this = exp.Cast(this=this, to=exp.DataType(this=exp.DataType.Type.TIMESTAMP))\n+                this = exp.Cast(\n+                    this=this, to=exp.DataType(this=exp.DataType.Type.TIMESTAMP)\n+                )\n \n             func = self.func(\n-                \"DATE_ADD\", this, exp.Interval(this=expression.expression, unit=exp.var(\"MONTH\"))\n+                \"DATE_ADD\",\n+                this,\n+                exp.Interval(this=expression.expression, unit=exp.var(\"MONTH\")),\n             )\n \n             # DuckDB's DATE_ADD function returns TIMESTAMP/DATETIME by default, even when the input is DATE\ndiff --git a/sqlglot/executor/python.py b/sqlglot/executor/python.py\nindex ce7a2a872..7ec52612c 100644\n--- a/sqlglot/executor/python.py\n+++ b/sqlglot/executor/python.py\n@@ -81,7 +81,9 @@ class PythonExecutor:\n \n     def table(self, expressions):\n         return Table(\n-            expression.alias_or_name if isinstance(expression, exp.Expression) else expression\n+            expression.alias_or_name\n+            if isinstance(expression, exp.Expression)\n+            else expression\n             for expression in expressions\n         )\n \n@@ -97,13 +99,17 @@ class PythonExecutor:\n             if not step.projections and not step.condition:\n                 return self.context({step.name: context.tables[source]})\n             table_iter = context.table_iter(source)\n-        elif isinstance(step.source, exp.Table) and isinstance(step.source.this, exp.ReadCSV):\n+        elif isinstance(step.source, exp.Table) and isinstance(\n+            step.source.this, exp.ReadCSV\n+        ):\n             table_iter = self.scan_csv(step)\n             context = next(table_iter)\n         else:\n             context, table_iter = self.scan_table(step)\n \n-        return self.context({step.name: self._project_and_filter(context, step, table_iter)})\n+        return self.context(\n+            {step.name: self._project_and_filter(context, step, table_iter)}\n+        )\n \n     def _project_and_filter(self, context, step, table_iter):\n         sink = self.table(step.projections if step.projections else context.columns)\n@@ -152,7 +158,10 @@ class PythonExecutor:\n \n                 # We can't cast empty values ('') to non-string types, so we convert them to None instead\n                 context.set_row(\n-                    tuple(None if (t is not str and v == \"\") else t(v) for t, v in zip(types, row))\n+                    tuple(\n+                        None if (t is not str and v == \"\") else t(v)\n+                        for t, v in zip(types, row)\n+                    )\n                 )\n                 yield context.table.reader\n \n@@ -226,7 +235,9 @@ class PythonExecutor:\n             results[ctx.eval_tuple(join_key)][1].append(reader.row)\n \n         table = Table(source_context.columns + join_context.columns)\n-        nulls = [(None,) * len(join_context.columns if left else source_context.columns)]\n+        nulls = [\n+            (None,) * len(join_context.columns if left else source_context.columns)\n+        ]\n \n         for a_group, b_group in results.values():\n             if left:\n@@ -300,7 +311,9 @@ class PythonExecutor:\n             context.set_range(0, 0)\n             table.append(context.eval_tuple(aggregations))\n \n-        context = self.context({step.name: table, **{name: table for name in context.tables}})\n+        context = self.context(\n+            {step.name: table, **{name: table for name in context.tables}}\n+        )\n \n         if step.projections or step.condition:\n             return self.scan(step, context)\n@@ -327,7 +340,9 @@ class PythonExecutor:\n \n         output = Table(\n             projection_columns,\n-            rows=[r[len(context.columns) : len(all_columns)] for r in sort_ctx.table.rows],\n+            rows=[\n+                r[len(context.columns) : len(all_columns)] for r in sort_ctx.table.rows\n+            ],\n         )\n         return self.context({step.name: output})\n \n@@ -370,7 +385,9 @@ def _rename(self, e):\n             return self.func(e.key, *values)\n \n         if isinstance(e, exp.Func) and e.is_var_len_args:\n-            args = itertools.chain.from_iterable(x if isinstance(x, list) else [x] for x in values)\n+            args = itertools.chain.from_iterable(\n+                x if isinstance(x, list) else [x] for x in values\n+            )\n             return self.func(e.key, *args)\n \n         return self.func(e.key, *values)\n@@ -396,7 +413,9 @@ def _lambda_sql(self, e: exp.Lambda) -> str:\n \n     e = e.transform(\n         lambda n: (\n-            exp.var(n.name) if isinstance(n, exp.Identifier) and n.name.lower() in names else n\n+            exp.var(n.name)\n+            if isinstance(n, exp.Identifier) and n.name.lower() in names\n+            else n\n         )\n     ).assert_is(exp.Lambda)\n \n@@ -431,7 +450,8 @@ class Python(Dialect):\n             exp.And: lambda self, e: self.binary(e, \"and\"),\n             exp.Between: _rename,\n             exp.Boolean: lambda self, e: \"True\" if e.this else \"False\",\n-            exp.Cast: lambda self, e: f\"CAST({self.sql(e.this)}, exp.DataType.Type.{e.args['to']})\",\n+            exp.Cast: lambda self,\n+            e: f\"CAST({self.sql(e.this)}, exp.DataType.Type.{e.args['to']})\",\n             exp.Column: lambda self,\n             e: f\"scope[{self.sql(e, 'table') or None}][{self.sql(e.this)}]\",\n             exp.Concat: lambda self, e: self.func(\n@@ -443,12 +463,18 @@ class Python(Dialect):\n             e: f\"EXTRACT('{e.name.lower()}', {self.sql(e, 'expression')})\",\n             exp.In: lambda self,\n             e: f\"{self.sql(e, 'this')} in {{{self.expressions(e, flat=True)}}}\",\n-            exp.Interval: lambda self, e: f\"INTERVAL({self.sql(e.this)}, '{self.sql(e.unit)}')\",\n+            exp.Interval: lambda self,\n+            e: f\"INTERVAL({self.sql(e.this)}, '{self.sql(e.unit)}')\",\n             exp.Is: lambda self, e: (\n-                self.binary(e, \"==\") if isinstance(e.this, exp.Literal) else self.binary(e, \"is\")\n+                self.binary(e, \"==\")\n+                if isinstance(e.this, exp.Literal)\n+                else self.binary(e, \"is\")\n+            ),\n+            exp.JSONExtract: lambda self, e: self.func(\n+                e.key, e.this, e.expression, *e.expressions\n             ),\n-            exp.JSONExtract: lambda self, e: self.func(e.key, e.this, e.expression, *e.expressions),\n-            exp.JSONPath: lambda self, e: f\"[{','.join(self.sql(p) for p in e.expressions[1:])}]\",\n+            exp.JSONPath: lambda self,\n+            e: f\"[{','.join(self.sql(p) for p in e.expressions[1:])}]\",\n             exp.JSONPathKey: lambda self, e: f\"'{self.sql(e.this)}'\",\n             exp.JSONPathSubscript: lambda self, e: f\"'{e.this}'\",\n             exp.Lambda: _lambda_sql,\ndiff --git a/sqlglot/expressions.py b/sqlglot/expressions.py\nindex eb2c2bad0..6802d16c6 100644\n--- a/sqlglot/expressions.py\n+++ b/sqlglot/expressions.py\n@@ -102,7 +102,16 @@ class Expression(metaclass=_Expression):\n \n     key = \"expression\"\n     arg_types = {\"this\": True}\n-    __slots__ = (\"args\", \"parent\", \"arg_key\", \"index\", \"comments\", \"_type\", \"_meta\", \"_hash\")\n+    __slots__ = (\n+        \"args\",\n+        \"parent\",\n+        \"arg_key\",\n+        \"index\",\n+        \"comments\",\n+        \"_type\",\n+        \"_meta\",\n+        \"_hash\",\n+    )\n \n     def __init__(self, **args: t.Any):\n         self.args: t.Dict[str, t.Any] = args\n@@ -201,7 +210,9 @@ class Expression(metaclass=_Expression):\n     @property\n     def is_star(self) -> bool:\n         \"\"\"Checks whether an expression is a star.\"\"\"\n-        return isinstance(self, Star) or (isinstance(self, Column) and isinstance(self.this, Star))\n+        return isinstance(self, Star) or (\n+            isinstance(self, Column) and isinstance(self.this, Star)\n+        )\n \n     @property\n     def alias(self) -> str:\n@@ -307,7 +318,9 @@ class Expression(metaclass=_Expression):\n         \"\"\"\n         return deepcopy(self)\n \n-    def add_comments(self, comments: t.Optional[t.List[str]] = None, prepend: bool = False) -> None:\n+    def add_comments(\n+        self, comments: t.Optional[t.List[str]] = None, prepend: bool = False\n+    ) -> None:\n         if self.comments is None:\n             self.comments = []\n \n@@ -391,7 +404,9 @@ class Expression(metaclass=_Expression):\n         self.args[arg_key] = value\n         self._set_parent(arg_key, value, index)\n \n-    def _set_parent(self, arg_key: str, value: t.Any, index: t.Optional[int] = None) -> None:\n+    def _set_parent(\n+        self, arg_key: str, value: t.Any, index: t.Optional[int] = None\n+    ) -> None:\n         if hasattr(value, \"parent\"):\n             value.parent = self\n             value.arg_key = arg_key\n@@ -584,9 +599,13 @@ class Expression(metaclass=_Expression):\n \n         A AND B AND C -> [A, B, C]\n         \"\"\"\n-        for node in self.dfs(prune=lambda n: n.parent and type(n) is not self.__class__):\n+        for node in self.dfs(\n+            prune=lambda n: n.parent and type(n) is not self.__class__\n+        ):\n             if type(node) is not self.__class__:\n-                yield node.unnest() if unnest and not isinstance(node, Subquery) else node\n+                yield (\n+                    node.unnest() if unnest and not isinstance(node, Subquery) else node\n+                )\n \n     def __str__(self) -> str:\n         return self.sql()\n@@ -616,7 +635,9 @@ class Expression(metaclass=_Expression):\n \n         return Dialect.get_or_raise(dialect).generate(self, **opts)\n \n-    def transform(self, fun: t.Callable, *args: t.Any, copy: bool = True, **kwargs) -> Expression:\n+    def transform(\n+        self, fun: t.Callable, *args: t.Any, copy: bool = True, **kwargs\n+    ) -> Expression:\n         \"\"\"\n         Visits all tree nodes (excluding already transformed ones)\n         and applies the given transformation function to each node.\n@@ -634,7 +655,9 @@ class Expression(metaclass=_Expression):\n         root = None\n         new_node = None\n \n-        for node in (self.copy() if copy else self).dfs(prune=lambda n: n is not new_node):\n+        for node in (self.copy() if copy else self).dfs(\n+            prune=lambda n: n is not new_node\n+        ):\n             parent, arg_key, index = node.parent, node.arg_key, node.index\n             new_node = fun(node, *args, **kwargs)\n \n@@ -861,7 +884,9 @@ class Expression(metaclass=_Expression):\n             The updated expression.\n         \"\"\"\n         if isinstance(other, Expression):\n-            self.meta.update({k: v for k, v in other.meta.items() if k in POSITION_META_KEYS})\n+            self.meta.update(\n+                {k: v for k, v in other.meta.items() if k in POSITION_META_KEYS}\n+            )\n         elif other is not None:\n             self.meta.update(\n                 {\n@@ -896,7 +921,8 @@ class Expression(metaclass=_Expression):\n \n     def __getitem__(self, other: ExpOrStr | t.Tuple[ExpOrStr]) -> Bracket:\n         return Bracket(\n-            this=self.copy(), expressions=[convert(e, copy=True) for e in ensure_list(other)]\n+            this=self.copy(),\n+            expressions=[convert(e, copy=True) for e in ensure_list(other)],\n         )\n \n     def __iter__(self) -> t.Iterator:\n@@ -1083,7 +1109,9 @@ class DerivedTable(Expression):\n \n \n class Query(Expression):\n-    def subquery(self, alias: t.Optional[ExpOrStr] = None, copy: bool = True) -> Subquery:\n+    def subquery(\n+        self, alias: t.Optional[ExpOrStr] = None, copy: bool = True\n+    ) -> Subquery:\n         \"\"\"\n         Returns a `Subquery` that wraps around this query.\n \n@@ -1103,7 +1131,11 @@ class Query(Expression):\n         return Subquery(this=instance, alias=alias)\n \n     def limit(\n-        self: Q, expression: ExpOrStr | int, dialect: DialectType = None, copy: bool = True, **opts\n+        self: Q,\n+        expression: ExpOrStr | int,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Q:\n         \"\"\"\n         Adds a LIMIT clause to this query.\n@@ -1137,7 +1169,11 @@ class Query(Expression):\n         )\n \n     def offset(\n-        self: Q, expression: ExpOrStr | int, dialect: DialectType = None, copy: bool = True, **opts\n+        self: Q,\n+        expression: ExpOrStr | int,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Q:\n         \"\"\"\n         Set the OFFSET expression.\n@@ -1344,7 +1380,11 @@ class Query(Expression):\n         )\n \n     def union(\n-        self, *expressions: ExpOrStr, distinct: bool = True, dialect: DialectType = None, **opts\n+        self,\n+        *expressions: ExpOrStr,\n+        distinct: bool = True,\n+        dialect: DialectType = None,\n+        **opts,\n     ) -> Union:\n         \"\"\"\n         Builds a UNION expression.\n@@ -1367,7 +1407,11 @@ class Query(Expression):\n         return union(self, *expressions, distinct=distinct, dialect=dialect, **opts)\n \n     def intersect(\n-        self, *expressions: ExpOrStr, distinct: bool = True, dialect: DialectType = None, **opts\n+        self,\n+        *expressions: ExpOrStr,\n+        distinct: bool = True,\n+        dialect: DialectType = None,\n+        **opts,\n     ) -> Intersect:\n         \"\"\"\n         Builds an INTERSECT expression.\n@@ -1390,7 +1434,11 @@ class Query(Expression):\n         return intersect(self, *expressions, distinct=distinct, dialect=dialect, **opts)\n \n     def except_(\n-        self, *expressions: ExpOrStr, distinct: bool = True, dialect: DialectType = None, **opts\n+        self,\n+        *expressions: ExpOrStr,\n+        distinct: bool = True,\n+        dialect: DialectType = None,\n+        **opts,\n     ) -> Except:\n         \"\"\"\n         Builds an EXCEPT expression.\n@@ -1455,7 +1503,9 @@ class DDL(Expression):\n         If this statement contains a query (e.g. a CTAS), this returns the output\n         names of the query's projections.\n         \"\"\"\n-        return self.expression.named_selects if isinstance(self.expression, Query) else []\n+        return (\n+            self.expression.named_selects if isinstance(self.expression, Query) else []\n+        )\n \n \n # https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Data-Manipulation-Language/Statement-Syntax/LOCKING-Request-Modifier/LOCKING-Request-Modifier-Syntax\n@@ -1717,7 +1767,13 @@ class UnicodeString(Condition):\n \n \n class Column(Condition):\n-    arg_types = {\"this\": True, \"table\": False, \"db\": False, \"catalog\": False, \"join_mark\": False}\n+    arg_types = {\n+        \"this\": True,\n+        \"table\": False,\n+        \"db\": False,\n+        \"catalog\": False,\n+        \"join_mark\": False,\n+    }\n \n     @property\n     def table(self) -> str:\n@@ -3298,7 +3354,9 @@ class Properties(Expression):\n             if property_cls:\n                 expressions.append(property_cls(this=convert(value)))\n             else:\n-                expressions.append(Property(this=Literal.string(key), value=convert(value)))\n+                expressions.append(\n+                    Property(this=Literal.string(key), value=convert(value))\n+                )\n \n         return cls(expressions=expressions)\n \n@@ -3504,7 +3562,9 @@ class SetOperation(Query):\n         **opts,\n     ) -> S:\n         this = maybe_copy(self, copy)\n-        this.this.unnest().select(*expressions, append=append, dialect=dialect, copy=False, **opts)\n+        this.this.unnest().select(\n+            *expressions, append=append, dialect=dialect, copy=False, **opts\n+        )\n         this.expression.unnest().select(\n             *expressions, append=append, dialect=dialect, copy=False, **opts\n         )\n@@ -3564,7 +3624,11 @@ class Update(DML):\n     }\n \n     def table(\n-        self, expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts\n+        self,\n+        expression: ExpOrStr,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Update:\n         \"\"\"\n         Set the table to update.\n@@ -3804,7 +3868,11 @@ class Select(Query):\n     }\n \n     def from_(\n-        self, expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts\n+        self,\n+        expression: ExpOrStr,\n+        dialect: DialectType = None,\n+        copy: bool = True,\n+        **opts,\n     ) -> Select:\n         \"\"\"\n         Set the FROM expression.\n@@ -4211,7 +4279,11 @@ class Select(Query):\n             Select: the modified expression.\n         \"\"\"\n         instance = maybe_copy(self, copy)\n-        on = Tuple(expressions=[maybe_parse(on, copy=copy) for on in ons if on]) if ons else None\n+        on = (\n+            Tuple(expressions=[maybe_parse(on, copy=copy) for on in ons if on])\n+            if ons\n+            else None\n+        )\n         instance.set(\"distinct\", Distinct(on=on) if distinct else None)\n         return instance\n \n@@ -4278,7 +4350,9 @@ class Select(Query):\n \n         return inst\n \n-    def hint(self, *hints: ExpOrStr, dialect: DialectType = None, copy: bool = True) -> Select:\n+    def hint(\n+        self, *hints: ExpOrStr, dialect: DialectType = None, copy: bool = True\n+    ) -> Select:\n         \"\"\"\n         Set hints for this expression.\n \n@@ -4297,7 +4371,10 @@ class Select(Query):\n         \"\"\"\n         inst = maybe_copy(self, copy)\n         inst.set(\n-            \"hint\", Hint(expressions=[maybe_parse(h, copy=copy, dialect=dialect) for h in hints])\n+            \"hint\",\n+            Hint(\n+                expressions=[maybe_parse(h, copy=copy, dialect=dialect) for h in hints]\n+            ),\n         )\n \n         return inst\n@@ -4348,7 +4425,9 @@ class Subquery(DerivedTable, Query):\n         **opts,\n     ) -> Subquery:\n         this = maybe_copy(self, copy)\n-        this.unnest().select(*expressions, append=append, dialect=dialect, copy=False, **opts)\n+        this.unnest().select(\n+            *expressions, append=append, dialect=dialect, copy=False, **opts\n+        )\n         return this\n \n     @property\n@@ -4780,7 +4859,9 @@ class DataType(Expression):\n                 )\n             except ParseError:\n                 if udt:\n-                    return DataType(this=DataType.Type.USERDEFINED, kind=dtype, **kwargs)\n+                    return DataType(\n+                        this=DataType.Type.USERDEFINED, kind=dtype, **kwargs\n+                    )\n                 raise\n         elif isinstance(dtype, (Identifier, Dot)) and udt:\n             return DataType(this=DataType.Type.USERDEFINED, kind=dtype, **kwargs)\n@@ -4789,7 +4870,9 @@ class DataType(Expression):\n         elif isinstance(dtype, DataType):\n             return maybe_copy(dtype, copy)\n         else:\n-            raise ValueError(f\"Invalid data type: {type(dtype)}. Expected str or DataType.Type\")\n+            raise ValueError(\n+                f\"Invalid data type: {type(dtype)}. Expected str or DataType.Type\"\n+            )\n \n         return DataType(**{**data_type_exp.args, **kwargs})\n \n@@ -5370,10 +5453,14 @@ class Func(Condition):\n         if cls.is_var_len_args:\n             all_arg_keys = list(cls.arg_types)\n             # If this function supports variable length argument treat the last argument as such.\n-            non_var_len_arg_keys = all_arg_keys[:-1] if cls.is_var_len_args else all_arg_keys\n+            non_var_len_arg_keys = (\n+                all_arg_keys[:-1] if cls.is_var_len_args else all_arg_keys\n+            )\n             num_non_var = len(non_var_len_arg_keys)\n \n-            args_dict = {arg_key: arg for arg, arg_key in zip(args, non_var_len_arg_keys)}\n+            args_dict = {\n+                arg_key: arg for arg, arg_key in zip(args, non_var_len_arg_keys)\n+            }\n             args_dict[all_arg_keys[-1]] = args[num_non_var:]\n         else:\n             args_dict = {arg_key: arg for arg, arg_key in zip(args, cls.arg_types)}\n@@ -5521,7 +5608,12 @@ class List(Func):\n \n # String pad, kind True -> LPAD, False -> RPAD\n class Pad(Func):\n-    arg_types = {\"this\": True, \"expression\": True, \"fill_pattern\": False, \"is_left\": True}\n+    arg_types = {\n+        \"this\": True,\n+        \"expression\": True,\n+        \"fill_pattern\": False,\n+        \"is_left\": True,\n+    }\n \n \n # https://docs.snowflake.com/en/sql-reference/functions/to_char\n@@ -5737,7 +5829,9 @@ class NthValue(AggFunc):\n class Case(Func):\n     arg_types = {\"this\": False, \"ifs\": True, \"default\": False}\n \n-    def when(self, condition: ExpOrStr, then: ExpOrStr, copy: bool = True, **opts) -> Case:\n+    def when(\n+        self, condition: ExpOrStr, then: ExpOrStr, copy: bool = True, **opts\n+    ) -> Case:\n         instance = maybe_copy(self, copy)\n         instance.append(\n             \"ifs\",\n@@ -6176,7 +6270,12 @@ class FromBase64(Func):\n \n \n class FeaturesAtTime(Func):\n-    arg_types = {\"this\": True, \"time\": False, \"num_rows\": False, \"ignore_feature_nulls\": False}\n+    arg_types = {\n+        \"this\": True,\n+        \"time\": False,\n+        \"num_rows\": False,\n+        \"ignore_feature_nulls\": False,\n+    }\n \n \n class ToBase64(Func):\n@@ -6500,7 +6599,12 @@ class JSONExtractArray(Func):\n \n \n class JSONExtractScalar(Binary, Func):\n-    arg_types = {\"this\": True, \"expression\": True, \"only_json_types\": False, \"expressions\": False}\n+    arg_types = {\n+        \"this\": True,\n+        \"expression\": True,\n+        \"only_json_types\": False,\n+        \"expressions\": False,\n+    }\n     _sql_names = [\"JSON_EXTRACT_SCALAR\"]\n     is_var_len_args = True\n \n@@ -7301,13 +7405,17 @@ def maybe_copy(instance, copy=True):\n     return instance.copy() if copy and instance else instance\n \n \n-def _to_s(node: t.Any, verbose: bool = False, level: int = 0, repr_str: bool = False) -> str:\n+def _to_s(\n+    node: t.Any, verbose: bool = False, level: int = 0, repr_str: bool = False\n+) -> str:\n     \"\"\"Generate a textual representation of an Expression tree\"\"\"\n     indent = \"\\n\" + (\"  \" * (level + 1))\n     delim = f\",{indent}\"\n \n     if isinstance(node, Expression):\n-        args = {k: v for k, v in node.args.items() if (v is not None and v != []) or verbose}\n+        args = {\n+            k: v for k, v in node.args.items() if (v is not None and v != []) or verbose\n+        }\n \n         if (node.type or verbose) and not isinstance(node, DataType):\n             args[\"_type\"] = node.type\n@@ -7324,7 +7432,10 @@ def _to_s(node: t.Any, verbose: bool = False, level: int = 0, repr_str: bool = F\n \n         repr_str = node.is_string or (isinstance(node, Identifier) and node.quoted)\n         items = delim.join(\n-            [f\"{k}={_to_s(v, verbose, level + 1, repr_str=repr_str)}\" for k, v in args.items()]\n+            [\n+                f\"{k}={_to_s(v, verbose, level + 1, repr_str=repr_str)}\"\n+                for k, v in args.items()\n+            ]\n         )\n         return f\"{node.__class__.__name__}({indent}{items})\"\n \n@@ -7492,7 +7603,12 @@ def _apply_cte_builder(\n     if scalar and not isinstance(as_expression, Subquery):\n         # scalar CTE must be wrapped in a subquery\n         as_expression = Subquery(this=as_expression)\n-    cte = CTE(this=as_expression, alias=alias_expression, materialized=materialized, scalar=scalar)\n+    cte = CTE(\n+        this=as_expression,\n+        alias=alias_expression,\n+        materialized=materialized,\n+        scalar=scalar,\n+    )\n     return _apply_child_list_builder(\n         cte,\n         instance=instance,\n@@ -7522,7 +7638,9 @@ def _combine(\n     if rest and wrap:\n         this = _wrap(this, Connector)\n     for expression in rest:\n-        this = operator(this=this, expression=_wrap(expression, Connector) if wrap else expression)\n+        this = operator(\n+            this=this, expression=_wrap(expression, Connector) if wrap else expression\n+        )\n \n     return this\n \n@@ -7580,7 +7698,12 @@ def union(\n     \"\"\"\n     assert len(expressions) >= 2, \"At least two expressions are required by `union`.\"\n     return _apply_set_operation(\n-        *expressions, set_operation=Union, distinct=distinct, dialect=dialect, copy=copy, **opts\n+        *expressions,\n+        set_operation=Union,\n+        distinct=distinct,\n+        dialect=dialect,\n+        copy=copy,\n+        **opts,\n     )\n \n \n@@ -7609,9 +7732,16 @@ def intersect(\n     Returns:\n         The new Intersect instance.\n     \"\"\"\n-    assert len(expressions) >= 2, \"At least two expressions are required by `intersect`.\"\n+    assert len(expressions) >= 2, (\n+        \"At least two expressions are required by `intersect`.\"\n+    )\n     return _apply_set_operation(\n-        *expressions, set_operation=Intersect, distinct=distinct, dialect=dialect, copy=copy, **opts\n+        *expressions,\n+        set_operation=Intersect,\n+        distinct=distinct,\n+        dialect=dialect,\n+        copy=copy,\n+        **opts,\n     )\n \n \n@@ -7642,7 +7772,12 @@ def except_(\n     \"\"\"\n     assert len(expressions) >= 2, \"At least two expressions are required by `except_`.\"\n     return _apply_set_operation(\n-        *expressions, set_operation=Except, distinct=distinct, dialect=dialect, copy=copy, **opts\n+        *expressions,\n+        set_operation=Except,\n+        distinct=distinct,\n+        dialect=dialect,\n+        copy=copy,\n+        **opts,\n     )\n \n \n@@ -7741,7 +7876,9 @@ def update(\n         )\n     if with_:\n         cte_list = [\n-            alias_(CTE(this=maybe_parse(qry, dialect=dialect, **opts)), alias, table=True)\n+            alias_(\n+                CTE(this=maybe_parse(qry, dialect=dialect, **opts)), alias, table=True\n+            )\n             for alias, qry in with_.items()\n         ]\n         update_expr.set(\n@@ -7778,7 +7915,9 @@ def delete(\n     if where:\n         delete_expr = delete_expr.where(where, dialect=dialect, copy=False, **opts)\n     if returning:\n-        delete_expr = delete_expr.returning(returning, dialect=dialect, copy=False, **opts)\n+        delete_expr = delete_expr.returning(\n+            returning, dialect=dialect, copy=False, **opts\n+        )\n     return delete_expr\n \n \n@@ -7813,10 +7952,14 @@ def insert(\n         Insert: the syntax tree for the INSERT statement.\n     \"\"\"\n     expr = maybe_parse(expression, dialect=dialect, copy=copy, **opts)\n-    this: Table | Schema = maybe_parse(into, into=Table, dialect=dialect, copy=copy, **opts)\n+    this: Table | Schema = maybe_parse(\n+        into, into=Table, dialect=dialect, copy=copy, **opts\n+    )\n \n     if columns:\n-        this = Schema(this=this, expressions=[to_identifier(c, copy=copy) for c in columns])\n+        this = Schema(\n+            this=this, expressions=[to_identifier(c, copy=copy) for c in columns]\n+        )\n \n     insert = Insert(this=this, expression=expr, overwrite=overwrite)\n \n@@ -7862,8 +8005,12 @@ def merge(\n     \"\"\"\n     expressions: t.List[Expression] = []\n     for when_expr in when_exprs:\n-        expression = maybe_parse(when_expr, dialect=dialect, copy=copy, into=Whens, **opts)\n-        expressions.extend([expression] if isinstance(expression, When) else expression.expressions)\n+        expression = maybe_parse(\n+            when_expr, dialect=dialect, copy=copy, into=Whens, **opts\n+        )\n+        expressions.extend(\n+            [expression] if isinstance(expression, When) else expression.expressions\n+        )\n \n     merge = Merge(\n         this=maybe_parse(into, dialect=dialect, copy=copy, **opts),\n@@ -7941,7 +8088,9 @@ def and_(\n     Returns:\n         The new condition\n     \"\"\"\n-    return t.cast(Condition, _combine(expressions, And, dialect, copy=copy, wrap=wrap, **opts))\n+    return t.cast(\n+        Condition, _combine(expressions, And, dialect, copy=copy, wrap=wrap, **opts)\n+    )\n \n \n def or_(\n@@ -7971,7 +8120,9 @@ def or_(\n     Returns:\n         The new condition\n     \"\"\"\n-    return t.cast(Condition, _combine(expressions, Or, dialect, copy=copy, wrap=wrap, **opts))\n+    return t.cast(\n+        Condition, _combine(expressions, Or, dialect, copy=copy, wrap=wrap, **opts)\n+    )\n \n \n def xor(\n@@ -8001,10 +8152,14 @@ def xor(\n     Returns:\n         The new condition\n     \"\"\"\n-    return t.cast(Condition, _combine(expressions, Xor, dialect, copy=copy, wrap=wrap, **opts))\n+    return t.cast(\n+        Condition, _combine(expressions, Xor, dialect, copy=copy, wrap=wrap, **opts)\n+    )\n \n \n-def not_(expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts) -> Not:\n+def not_(\n+    expression: ExpOrStr, dialect: DialectType = None, copy: bool = True, **opts\n+) -> Not:\n     \"\"\"\n     Wrap a condition with a NOT operator.\n \n@@ -8054,7 +8209,9 @@ SAFE_IDENTIFIER_RE: t.Pattern[str] = re.compile(r\"^[_a-zA-Z][\\w]*$\")\n \n \n @t.overload\n-def to_identifier(name: None, quoted: t.Optional[bool] = None, copy: bool = True) -> None: ...\n+def to_identifier(\n+    name: None, quoted: t.Optional[bool] = None, copy: bool = True\n+) -> None: ...\n \n \n @t.overload\n@@ -8086,7 +8243,9 @@ def to_identifier(name, quoted=None, copy=True):\n             quoted=not SAFE_IDENTIFIER_RE.match(name) if quoted is None else quoted,\n         )\n     else:\n-        raise ValueError(f\"Name needs to be a string or an Identifier, got: {name.__class__}\")\n+        raise ValueError(\n+            f\"Name needs to be a string or an Identifier, got: {name.__class__}\"\n+        )\n     return identifier\n \n \n@@ -8280,7 +8439,9 @@ def subquery(\n         A new Select instance with the subquery expression included.\n     \"\"\"\n \n-    expression = maybe_parse(expression, dialect=dialect, **opts).subquery(alias, **opts)\n+    expression = maybe_parse(expression, dialect=dialect, **opts).subquery(\n+        alias, **opts\n+    )\n     return Select().from_(expression, dialect=dialect, **opts)\n \n \n@@ -8349,13 +8510,20 @@ def column(\n \n     if fields:\n         this = Dot.build(\n-            (this, *(to_identifier(field, quoted=quoted, copy=copy) for field in fields))\n+            (\n+                this,\n+                *(to_identifier(field, quoted=quoted, copy=copy) for field in fields),\n+            )\n         )\n     return this\n \n \n def cast(\n-    expression: ExpOrStr, to: DATA_TYPE, copy: bool = True, dialect: DialectType = None, **opts\n+    expression: ExpOrStr,\n+    to: DATA_TYPE,\n+    copy: bool = True,\n+    dialect: DialectType = None,\n+    **opts,\n ) -> Cast:\n     \"\"\"Cast an expression to a data type.\n \n@@ -8459,7 +8627,9 @@ def values(\n     return Values(\n         expressions=[convert(tup) for tup in values],\n         alias=(\n-            TableAlias(this=to_identifier(alias), columns=[to_identifier(x) for x in columns])\n+            TableAlias(\n+                this=to_identifier(alias), columns=[to_identifier(x) for x in columns]\n+            )\n             if columns\n             else (TableAlias(this=to_identifier(alias)) if alias else None)\n         ),\n@@ -8592,7 +8762,8 @@ def convert(value: t.Any, copy: bool = False) -> Expression:\n             return Struct(\n                 expressions=[\n                     PropertyEQ(\n-                        this=to_identifier(k), expression=convert(getattr(value, k), copy=copy)\n+                        this=to_identifier(k),\n+                        expression=convert(getattr(value, k), copy=copy),\n                     )\n                     for k in value._fields\n                 ]\n@@ -8632,7 +8803,9 @@ def replace_children(expression: Expression, fun: t.Callable, *args, **kwargs) -\n             else:\n                 new_child_nodes.append(cn)\n \n-        expression.set(k, new_child_nodes if is_list_arg else seq_get(new_child_nodes, 0))\n+        expression.set(\n+            k, new_child_nodes if is_list_arg else seq_get(new_child_nodes, 0)\n+        )\n \n \n def replace_tree(\n@@ -8684,7 +8857,9 @@ def column_table_names(expression: Expression, exclude: str = \"\") -> t.Set[str]:\n     }\n \n \n-def table_name(table: Table | str, dialect: DialectType = None, identify: bool = False) -> str:\n+def table_name(\n+    table: Table | str, dialect: DialectType = None, identify: bool = False\n+) -> str:\n     \"\"\"Get the full name of a table as a string.\n \n     Args:\n@@ -8718,7 +8893,9 @@ def table_name(table: Table | str, dialect: DialectType = None, identify: bool =\n     )\n \n \n-def normalize_table_name(table: str | Table, dialect: DialectType = None, copy: bool = True) -> str:\n+def normalize_table_name(\n+    table: str | Table, dialect: DialectType = None, copy: bool = True\n+) -> str:\n     \"\"\"Returns a case normalized table name without quotes.\n \n     Args:\n@@ -8741,7 +8918,10 @@ def normalize_table_name(table: str | Table, dialect: DialectType = None, copy:\n \n \n def replace_tables(\n-    expression: E, mapping: t.Dict[str, str], dialect: DialectType = None, copy: bool = True\n+    expression: E,\n+    mapping: t.Dict[str, str],\n+    dialect: DialectType = None,\n+    copy: bool = True,\n ) -> E:\n     \"\"\"Replace all tables in expression according to the mapping.\n \n@@ -8841,7 +9021,9 @@ def expand(\n     Returns:\n         The transformed expression.\n     \"\"\"\n-    normalized_sources = {normalize_table_name(k, dialect=dialect): v for k, v in sources.items()}\n+    normalized_sources = {\n+        normalize_table_name(k, dialect=dialect): v for k, v in sources.items()\n+    }\n \n     def _expand(node: Expression):\n         if isinstance(node, Table):\n@@ -8862,7 +9044,9 @@ def expand(\n     return expression.transform(_expand, copy=copy)\n \n \n-def func(name: str, *args, copy: bool = True, dialect: DialectType = None, **kwargs) -> Func:\n+def func(\n+    name: str, *args, copy: bool = True, dialect: DialectType = None, **kwargs\n+) -> Func:\n     \"\"\"\n     Returns a Func expression.\n \n@@ -8894,8 +9078,13 @@ def func(name: str, *args, copy: bool = True, dialect: DialectType = None, **kwa\n \n     dialect = Dialect.get_or_raise(dialect)\n \n-    converted: t.List[Expression] = [maybe_parse(arg, dialect=dialect, copy=copy) for arg in args]\n-    kwargs = {key: maybe_parse(value, dialect=dialect, copy=copy) for key, value in kwargs.items()}\n+    converted: t.List[Expression] = [\n+        maybe_parse(arg, dialect=dialect, copy=copy) for arg in args\n+    ]\n+    kwargs = {\n+        key: maybe_parse(value, dialect=dialect, copy=copy)\n+        for key, value in kwargs.items()\n+    }\n \n     constructor = dialect.parser_class.FUNCTIONS.get(name.upper())\n     if constructor:\ndiff --git a/sqlglot/lineage.py b/sqlglot/lineage.py\nindex 1642109ea..619029474 100644\n--- a/sqlglot/lineage.py\n+++ b/sqlglot/lineage.py\n@@ -7,7 +7,13 @@ from dataclasses import dataclass, field\n \n from sqlglot import Schema, exp, maybe_parse\n from sqlglot.errors import SqlglotError\n-from sqlglot.optimizer import Scope, build_scope, find_all_in_scope, normalize_identifiers, qualify\n+from sqlglot.optimizer import (\n+    Scope,\n+    build_scope,\n+    find_all_in_scope,\n+    normalize_identifiers,\n+    qualify,\n+)\n from sqlglot.optimizer.scope import ScopeType\n \n if t.TYPE_CHECKING:\n@@ -44,7 +50,9 @@ class Node:\n                 label = node.expression.sql(pretty=True, dialect=dialect)\n                 source = node.source.transform(\n                     lambda n: (\n-                        exp.Tag(this=n, prefix=\"<b>\", postfix=\"</b>\") if n is node.expression else n\n+                        exp.Tag(this=n, prefix=\"<b>\", postfix=\"</b>\")\n+                        if n is node.expression\n+                        else n\n                     ),\n                     copy=False,\n                 ).sql(pretty=True, dialect=dialect)\n@@ -97,7 +105,10 @@ def lineage(\n     if sources:\n         expression = exp.expand(\n             expression,\n-            {k: t.cast(exp.Query, maybe_parse(v, dialect=dialect)) for k, v in sources.items()},\n+            {\n+                k: t.cast(exp.Query, maybe_parse(v, dialect=dialect))\n+                for k, v in sources.items()\n+            },\n             dialect=dialect,\n         )\n \n@@ -136,7 +147,11 @@ def to_node(\n         scope.expression.selects[column]\n         if isinstance(column, int)\n         else next(\n-            (select for select in scope.expression.selects if select.alias_or_name == column),\n+            (\n+                select\n+                for select in scope.expression.selects\n+                if select.alias_or_name == column\n+            ),\n             exp.Star() if scope.expression.is_star else scope.expression,\n         )\n     )\n@@ -154,7 +169,9 @@ def to_node(\n             )\n     if isinstance(scope.expression, exp.SetOperation):\n         name = type(scope.expression).__name__.upper()\n-        upstream = upstream or Node(name=name, source=scope.expression, expression=select)\n+        upstream = upstream or Node(\n+            name=name, source=scope.expression, expression=select\n+        )\n \n         index = (\n             column\n@@ -207,7 +224,8 @@ def to_node(\n         upstream.downstream.append(node)\n \n     subquery_scopes = {\n-        id(subquery_scope.expression): subquery_scope for subquery_scope in scope.subquery_scopes\n+        id(subquery_scope.expression): subquery_scope\n+        for subquery_scope in scope.subquery_scopes\n     }\n \n     for subquery in find_all_in_scope(select, exp.UNWRAPPED_QUERIES):\n@@ -281,7 +299,10 @@ def to_node(\n \n         if isinstance(source, Scope):\n             reference_node_name = None\n-            if source.scope_type == ScopeType.DERIVED_TABLE and table not in source_names:\n+            if (\n+                source.scope_type == ScopeType.DERIVED_TABLE\n+                and table not in source_names\n+            ):\n                 reference_node_name = table\n             elif source.scope_type == ScopeType.CTE:\n                 selected_node, _ = scope.selected_sources.get(table, (None, None))\n@@ -307,7 +328,9 @@ def to_node(\n             else:\n                 # The column is not in the pivot, so it must be an implicit column of the\n                 # pivoted source -- adapt column to be from the implicit pivoted source.\n-                downstream_columns.append(exp.column(c.this, table=pivot.parent.alias_or_name))\n+                downstream_columns.append(\n+                    exp.column(c.this, table=pivot.parent.alias_or_name)\n+                )\n \n             for downstream_column in downstream_columns:\n                 table = downstream_column.table\n@@ -352,7 +375,11 @@ class GraphHTML:\n     \"\"\"\n \n     def __init__(\n-        self, nodes: t.Dict, edges: t.List, imports: bool = True, options: t.Optional[t.Dict] = None\n+        self,\n+        nodes: t.Dict,\n+        edges: t.List,\n+        imports: bool = True,\n+        options: t.Optional[t.Dict] = None,\n     ):\n         self.imports = imports\n \ndiff --git a/sqlglot/optimizer/qualify_columns.py b/sqlglot/optimizer/qualify_columns.py\nindex 743d009f7..177be2b00 100644\n--- a/sqlglot/optimizer/qualify_columns.py\n+++ b/sqlglot/optimizer/qualify_columns.py\n@@ -66,7 +66,9 @@ def qualify_columns(\n             # In Snowflake / Oracle queries that have a CONNECT BY clause, one can use the LEVEL\n             # pseudocolumn, which doesn't belong to a table, so we change it into an identifier\n             scope_expression.transform(\n-                lambda n: n.this if isinstance(n, exp.Column) and n.name == \"LEVEL\" else n,\n+                lambda n: n.this\n+                if isinstance(n, exp.Column) and n.name == \"LEVEL\"\n+                else n,\n                 copy=False,\n             )\n             scope.clear_cache()\n@@ -76,7 +78,9 @@ def qualify_columns(\n         _pop_table_column_aliases(scope.derived_tables)\n         using_column_tables = _expand_using(scope, resolver)\n \n-        if (schema.empty or dialect.FORCE_EARLY_ALIAS_REF_EXPANSION) and expand_alias_refs:\n+        if (\n+            schema.empty or dialect.FORCE_EARLY_ALIAS_REF_EXPANSION\n+        ) and expand_alias_refs:\n             _expand_alias_refs(\n                 scope,\n                 resolver,\n@@ -85,7 +89,9 @@ def qualify_columns(\n             )\n \n         _convert_columns_to_dots(scope, resolver)\n-        _qualify_columns(scope, resolver, allow_partial_qualification=allow_partial_qualification)\n+        _qualify_columns(\n+            scope, resolver, allow_partial_qualification=allow_partial_qualification\n+        )\n \n         if not schema.empty and expand_alias_refs:\n             _expand_alias_refs(scope, resolver, dialect)\n@@ -120,17 +126,25 @@ def validate_qualify_columns(expression: E) -> E:\n         if isinstance(scope.expression, exp.Select):\n             unqualified_columns = scope.unqualified_columns\n \n-            if scope.external_columns and not scope.is_correlated_subquery and not scope.pivots:\n+            if (\n+                scope.external_columns\n+                and not scope.is_correlated_subquery\n+                and not scope.pivots\n+            ):\n                 column = scope.external_columns[0]\n                 for_table = f\" for table: '{column.table}'\" if column.table else \"\"\n-                raise OptimizeError(f\"Column '{column}' could not be resolved{for_table}\")\n+                raise OptimizeError(\n+                    f\"Column '{column}' could not be resolved{for_table}\"\n+                )\n \n             if unqualified_columns and scope.pivots and scope.pivots[0].unpivot:\n                 # New columns produced by the UNPIVOT can't be qualified, but there may be columns\n                 # under the UNPIVOT's IN clause that can and should be qualified. We recompute\n                 # this list here to ensure those in the former category will be excluded.\n                 unpivot_columns = set(_unpivot_columns(scope.pivots[0]))\n-                unqualified_columns = [c for c in unqualified_columns if c not in unpivot_columns]\n+                unqualified_columns = [\n+                    c for c in unqualified_columns if c not in unpivot_columns\n+                ]\n \n             all_unqualified_columns.extend(unqualified_columns)\n \n@@ -158,7 +172,10 @@ def _pop_table_column_aliases(derived_tables: t.List[exp.CTE | exp.Subquery]) ->\n     For example, `col1` and `col2` will be dropped in SELECT ... FROM (SELECT ...) AS foo(col1, col2)\n     \"\"\"\n     for derived_table in derived_tables:\n-        if isinstance(derived_table.parent, exp.With) and derived_table.parent.recursive:\n+        if (\n+            isinstance(derived_table.parent, exp.With)\n+            and derived_table.parent.recursive\n+        ):\n             continue\n         table_alias = derived_table.args.get(\"alias\")\n         if table_alias:\n@@ -246,7 +263,9 @@ def _expand_using(scope: Scope, resolver: Resolver) -> t.Dict[str, t.Any]:\n         for column in scope.columns:\n             if not column.table and column.name in column_tables:\n                 tables = column_tables[column.name]\n-                coalesce_args = [exp.column(column.name, table=table) for table in tables]\n+                coalesce_args = [\n+                    exp.column(column.name, table=table) for table in tables\n+                ]\n                 replacement: exp.Expression = exp.func(\"coalesce\", *coalesce_args)\n \n                 if isinstance(column.parent, exp.Select):\n@@ -264,7 +283,10 @@ def _expand_using(scope: Scope, resolver: Resolver) -> t.Dict[str, t.Any]:\n \n \n def _expand_alias_refs(\n-    scope: Scope, resolver: Resolver, dialect: Dialect, expand_only_groupby: bool = False\n+    scope: Scope,\n+    resolver: Resolver,\n+    dialect: Dialect,\n+    expand_only_groupby: bool = False,\n ) -> None:\n     \"\"\"\n     Expand references to aliases.\n@@ -281,7 +303,9 @@ def _expand_alias_refs(\n     projections = {s.alias_or_name for s in expression.selects}\n \n     def replace_columns(\n-        node: t.Optional[exp.Expression], resolve_table: bool = False, literal_index: bool = False\n+        node: t.Optional[exp.Expression],\n+        resolve_table: bool = False,\n+        literal_index: bool = False,\n     ) -> None:\n         is_group_by = isinstance(node, exp.Group)\n         is_having = isinstance(node, exp.Having)\n@@ -300,14 +324,20 @@ def _expand_alias_refs(\n                 continue\n \n             skip_replace = False\n-            table = resolver.get_table(column.name) if resolve_table and not column.table else None\n+            table = (\n+                resolver.get_table(column.name)\n+                if resolve_table and not column.table\n+                else None\n+            )\n             alias_expr, i = alias_to_expression.get(column.name, (None, 1))\n \n             if alias_expr:\n                 skip_replace = bool(\n                     alias_expr.find(exp.AggFunc)\n                     and column.find_ancestor(exp.AggFunc)\n-                    and not isinstance(column.find_ancestor(exp.Window, exp.Select), exp.Window)\n+                    and not isinstance(\n+                        column.find_ancestor(exp.Window, exp.Select), exp.Window\n+                    )\n                 )\n \n                 # BigQuery's having clause gets confused if an alias matches a source.\n@@ -322,7 +352,9 @@ def _expand_alias_refs(\n             if table and (not alias_expr or skip_replace):\n                 column.set(\"table\", table)\n             elif not column.table and alias_expr and not skip_replace:\n-                if isinstance(alias_expr, exp.Literal) and (literal_index or resolve_table):\n+                if isinstance(alias_expr, exp.Literal) and (\n+                    literal_index or resolve_table\n+                ):\n                     if literal_index:\n                         column.replace(exp.Literal.number(i))\n                 else:\n@@ -367,7 +399,9 @@ def _expand_group_by(scope: Scope, dialect: DialectType) -> None:\n     if not group:\n         return\n \n-    group.set(\"expressions\", _expand_positional_references(scope, group.expressions, dialect))\n+    group.set(\n+        \"expressions\", _expand_positional_references(scope, group.expressions, dialect)\n+    )\n     expression.set(\"group\", group)\n \n \n@@ -398,7 +432,9 @@ def _expand_order_by_and_distinct_on(scope: Scope, resolver: Resolver) -> None:\n             original.replace(expanded)\n \n         if scope.expression.args.get(\"group\"):\n-            selects = {s.this: exp.column(s.alias_or_name) for s in scope.expression.selects}\n+            selects = {\n+                s.this: exp.column(s.alias_or_name) for s in scope.expression.selects\n+            }\n \n             for expression in modifier_expressions:\n                 expression.replace(\n@@ -409,7 +445,10 @@ def _expand_order_by_and_distinct_on(scope: Scope, resolver: Resolver) -> None:\n \n \n def _expand_positional_references(\n-    scope: Scope, expressions: t.Iterable[exp.Expression], dialect: DialectType, alias: bool = False\n+    scope: Scope,\n+    expressions: t.Iterable[exp.Expression],\n+    dialect: DialectType,\n+    alias: bool = False,\n ) -> t.List[exp.Expression]:\n     new_nodes: t.List[exp.Expression] = []\n     ambiguous_projections = None\n@@ -494,7 +533,9 @@ def _convert_columns_to_dots(scope: Scope, resolver: Resolver) -> None:\n \n             if column_table:\n                 converted = True\n-                column.replace(exp.Dot.build([exp.column(root, table=column_table), *parts]))\n+                column.replace(\n+                    exp.Dot.build([exp.column(root, table=column_table), *parts])\n+                )\n \n     if converted:\n         # We want to re-aggregate the converted columns, otherwise they'd be skipped in\n@@ -502,7 +543,9 @@ def _convert_columns_to_dots(scope: Scope, resolver: Resolver) -> None:\n         scope.clear_cache()\n \n \n-def _qualify_columns(scope: Scope, resolver: Resolver, allow_partial_qualification: bool) -> None:\n+def _qualify_columns(\n+    scope: Scope, resolver: Resolver, allow_partial_qualification: bool\n+) -> None:\n     \"\"\"Disambiguate columns, ensuring each column specifies a source\"\"\"\n     for column in scope.columns:\n         column_table = column.table\n@@ -551,7 +594,9 @@ def _expand_struct_stars_bigquery(\n     \"\"\"[BigQuery] Expand/Flatten foo.bar.* where bar is a struct column\"\"\"\n \n     dot_column = expression.find(exp.Column)\n-    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(exp.DataType.Type.STRUCT):\n+    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(\n+        exp.DataType.Type.STRUCT\n+    ):\n         return []\n \n     # All nested struct values are ColumnDefs, so normalize the first exp.Column in one\n@@ -608,7 +653,9 @@ def _expand_struct_stars_risingwave(expression: exp.Dot) -> t.List[exp.Alias]:\n \n     # find column definition to get data-type\n     dot_column = expression.find(exp.Column)\n-    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(exp.DataType.Type.STRUCT):\n+    if not isinstance(dot_column, exp.Column) or not dot_column.is_type(\n+        exp.DataType.Type.STRUCT\n+    ):\n         return []\n \n     parent = dot_column.parent\n@@ -689,20 +736,28 @@ def _expand_stars(\n             for field in pivot.fields:\n                 if isinstance(field, exp.In):\n                     pivot_exclude_columns.update(\n-                        c.output_name for e in field.expressions for c in e.find_all(exp.Column)\n+                        c.output_name\n+                        for e in field.expressions\n+                        for c in e.find_all(exp.Column)\n                     )\n \n         else:\n-            pivot_exclude_columns = set(c.output_name for c in pivot.find_all(exp.Column))\n+            pivot_exclude_columns = set(\n+                c.output_name for c in pivot.find_all(exp.Column)\n+            )\n \n-            pivot_output_columns = [c.output_name for c in pivot.args.get(\"columns\", [])]\n+            pivot_output_columns = [\n+                c.output_name for c in pivot.args.get(\"columns\", [])\n+            ]\n             if not pivot_output_columns:\n                 pivot_output_columns = [c.alias_or_name for c in pivot.expressions]\n \n     is_bigquery = dialect == \"bigquery\"\n     is_risingwave = dialect == \"risingwave\"\n \n-    if (is_bigquery or is_risingwave) and any(isinstance(col, exp.Dot) for col in scope.stars):\n+    if (is_bigquery or is_risingwave) and any(\n+        isinstance(col, exp.Dot) for col in scope.stars\n+    ):\n         # Found struct expansion, annotate scope ahead of time\n         annotator.annotate_scope(scope)\n \n@@ -742,7 +797,9 @@ def _expand_stars(\n             columns = columns or scope.outer_columns\n \n             if pseudocolumns:\n-                columns = [name for name in columns if name.upper() not in pseudocolumns]\n+                columns = [\n+                    name for name in columns if name.upper() not in pseudocolumns\n+                ]\n \n             if not columns or \"*\" in columns:\n                 return\n@@ -754,7 +811,9 @@ def _expand_stars(\n \n             if pivot:\n                 if pivot_output_columns and pivot_exclude_columns:\n-                    pivot_columns = [c for c in columns if c not in pivot_exclude_columns]\n+                    pivot_columns = [\n+                        c for c in columns if c not in pivot_exclude_columns\n+                    ]\n                     pivot_columns.extend(pivot_output_columns)\n                 else:\n                     pivot_columns = pivot.alias_column_names\n@@ -776,11 +835,15 @@ def _expand_stars(\n                     coalesce_args = [exp.column(name, table=table) for table in tables]\n \n                     new_selections.append(\n-                        alias(exp.func(\"coalesce\", *coalesce_args), alias=name, copy=False)\n+                        alias(\n+                            exp.func(\"coalesce\", *coalesce_args), alias=name, copy=False\n+                        )\n                     )\n                 else:\n                     alias_ = renamed_columns.get(name, name)\n-                    selection_expr = replaced_columns.get(name) or exp.column(name, table=table)\n+                    selection_expr = replaced_columns.get(name) or exp.column(\n+                        name, table=table\n+                    )\n                     new_selections.append(\n                         alias(selection_expr, alias_, copy=False)\n                         if alias_ != name\n@@ -821,7 +884,9 @@ def _add_rename_columns(\n \n \n def _add_replace_columns(\n-    expression: exp.Expression, tables, replace_columns: t.Dict[int, t.Dict[str, exp.Alias]]\n+    expression: exp.Expression,\n+    tables,\n+    replace_columns: t.Dict[int, t.Dict[str, exp.Alias]],\n ) -> None:\n     replace = expression.args.get(\"replace\")\n \n@@ -852,7 +917,9 @@ def qualify_outputs(scope_or_expression: Scope | exp.Expression) -> None:\n \n         if isinstance(selection, exp.Subquery):\n             if not selection.output_name:\n-                selection.set(\"alias\", exp.TableAlias(this=exp.to_identifier(f\"_col_{i}\")))\n+                selection.set(\n+                    \"alias\", exp.TableAlias(this=exp.to_identifier(f\"_col_{i}\"))\n+                )\n         elif not isinstance(selection, exp.Alias) and not selection.is_star:\n             selection = alias(\n                 selection,\n@@ -868,7 +935,9 @@ def qualify_outputs(scope_or_expression: Scope | exp.Expression) -> None:\n         scope.expression.set(\"expressions\", new_selections)\n \n \n-def quote_identifiers(expression: E, dialect: DialectType = None, identify: bool = True) -> E:\n+def quote_identifiers(\n+    expression: E, dialect: DialectType = None, identify: bool = True\n+) -> E:\n     \"\"\"Makes sure all identifiers that need to be quoted are quoted.\"\"\"\n     return expression.transform(\n         Dialect.get_or_raise(dialect).quote_identifier, identify=identify, copy=False\n@@ -968,14 +1037,18 @@ class Resolver:\n         \"\"\"All available columns of all sources in this scope\"\"\"\n         if self._all_columns is None:\n             self._all_columns = {\n-                column for columns in self._get_all_source_columns().values() for column in columns\n+                column\n+                for columns in self._get_all_source_columns().values()\n+                for column in columns\n             }\n         return self._all_columns\n \n     def get_source_columns_from_set_op(self, expression: exp.Expression) -> t.List[str]:\n         if isinstance(expression, exp.Select):\n             return expression.named_selects\n-        if isinstance(expression, exp.Subquery) and isinstance(expression.this, exp.SetOperation):\n+        if isinstance(expression, exp.Subquery) and isinstance(\n+            expression.this, exp.SetOperation\n+        ):\n             # Different types of SET modifiers can be chained together if they're explicitly grouped by nesting\n             return self.get_source_columns_from_set_op(expression.this)\n         if not isinstance(expression, exp.SetOperation):\n@@ -1010,7 +1083,9 @@ class Resolver:\n \n         return columns\n \n-    def get_source_columns(self, name: str, only_visible: bool = False) -> t.Sequence[str]:\n+    def get_source_columns(\n+        self, name: str, only_visible: bool = False\n+    ) -> t.Sequence[str]:\n         \"\"\"Resolve the source columns for a given source `name`.\"\"\"\n         cache_key = (name, only_visible)\n         if cache_key not in self._get_source_columns_cache:\n@@ -1033,7 +1108,9 @@ class Resolver:\n                     if source.expression.is_type(exp.DataType.Type.STRUCT):\n                         for k in source.expression.type.expressions:  # type: ignore\n                             columns.append(k.name)\n-            elif isinstance(source, Scope) and isinstance(source.expression, exp.SetOperation):\n+            elif isinstance(source, Scope) and isinstance(\n+                source.expression, exp.SetOperation\n+            ):\n                 columns = self.get_source_columns_from_set_op(source.expression)\n \n             else:\n@@ -1042,7 +1119,11 @@ class Resolver:\n                 if isinstance(select, exp.QueryTransform):\n                     # https://spark.apache.org/docs/3.5.1/sql-ref-syntax-qry-select-transform.html\n                     schema = select.args.get(\"schema\")\n-                    columns = [c.name for c in schema.expressions] if schema else [\"key\", \"value\"]\n+                    columns = (\n+                        [c.name for c in schema.expressions]\n+                        if schema\n+                        else [\"key\", \"value\"]\n+                    )\n                 else:\n                     columns = source.expression.named_selects\n \n@@ -1071,7 +1152,8 @@ class Resolver:\n             self._source_columns = {\n                 source_name: self.get_source_columns(source_name)\n                 for source_name, source in itertools.chain(\n-                    self.scope.selected_sources.items(), self.scope.lateral_sources.items()\n+                    self.scope.selected_sources.items(),\n+                    self.scope.lateral_sources.items(),\n                 )\n             }\n         return self._source_columns\ndiff --git a/sqlglot/optimizer/scope.py b/sqlglot/optimizer/scope.py\nindex 2b2a01560..10c4db645 100644\n--- a/sqlglot/optimizer/scope.py\n+++ b/sqlglot/optimizer/scope.py\n@@ -104,7 +104,13 @@ class Scope:\n         self._semi_anti_join_tables = None\n \n     def branch(\n-        self, expression, scope_type, sources=None, cte_sources=None, lateral_sources=None, **kwargs\n+        self,\n+        expression,\n+        scope_type,\n+        sources=None,\n+        cte_sources=None,\n+        lateral_sources=None,\n+        **kwargs,\n     ):\n         \"\"\"Branch from the current scope to a new, inner scope\"\"\"\n         return Scope(\n@@ -142,7 +148,9 @@ class Scope:\n                     self._stars.append(node)\n                 else:\n                     self._raw_columns.append(node)\n-            elif isinstance(node, exp.Table) and not isinstance(node.parent, exp.JoinHint):\n+            elif isinstance(node, exp.Table) and not isinstance(\n+                node.parent, exp.JoinHint\n+            ):\n                 parent = node.parent\n                 if isinstance(parent, exp.Join) and parent.is_semi_or_anti_join:\n                     self._semi_anti_join_tables.add(node.alias_or_name)\n@@ -299,7 +307,10 @@ class Scope:\n                     not ancestor\n                     or column.table\n                     or isinstance(ancestor, exp.Select)\n-                    or (isinstance(ancestor, exp.Table) and not isinstance(ancestor.this, exp.Func))\n+                    or (\n+                        isinstance(ancestor, exp.Table)\n+                        and not isinstance(ancestor.this, exp.Func)\n+                    )\n                     or (\n                         isinstance(ancestor, (exp.Order, exp.Distinct))\n                         and (\n@@ -307,7 +318,10 @@ class Scope:\n                             or column.name not in named_selects\n                         )\n                     )\n-                    or (isinstance(ancestor, exp.Star) and not column.arg_key == \"except\")\n+                    or (\n+                        isinstance(ancestor, exp.Star)\n+                        and not column.arg_key == \"except\"\n+                    )\n                 ):\n                     self._columns.append(column)\n \n@@ -359,7 +373,9 @@ class Scope:\n                 self._references.append(\n                     (\n                         _get_source_alias(expression),\n-                        expression if expression.args.get(\"pivots\") else expression.unnest(),\n+                        expression\n+                        if expression.args.get(\"pivots\")\n+                        else expression.unnest(),\n                     )\n                 )\n \n@@ -414,7 +430,9 @@ class Scope:\n     def pivots(self):\n         if not self._pivots:\n             self._pivots = [\n-                pivot for _, node in self.references for pivot in node.args.get(\"pivots\") or []\n+                pivot\n+                for _, node in self.references\n+                for pivot in node.args.get(\"pivots\") or []\n             ]\n \n         return self._pivots\n@@ -597,7 +615,9 @@ def _traverse_scope(scope):\n     elif isinstance(expression, exp.DDL):\n         if isinstance(expression.expression, exp.Query):\n             yield from _traverse_ctes(scope)\n-            yield from _traverse_scope(Scope(expression.expression, cte_sources=scope.cte_sources))\n+            yield from _traverse_scope(\n+                Scope(expression.expression, cte_sources=scope.cte_sources)\n+            )\n         return\n     elif isinstance(expression, exp.DML):\n         yield from _traverse_ctes(scope)\n@@ -607,7 +627,9 @@ def _traverse_scope(scope):\n                 yield from _traverse_scope(Scope(query, cte_sources=scope.cte_sources))\n         return\n     else:\n-        logger.warning(\"Cannot traverse scope %s with type '%s'\", expression, type(expression))\n+        logger.warning(\n+            \"Cannot traverse scope %s with type '%s'\", expression, type(expression)\n+        )\n         return\n \n     yield scope\n@@ -753,7 +775,9 @@ def _traverse_tables(scope):\n \n             # Make sure to not include the joins twice\n             if expression is not scope.expression:\n-                expressions.extend(join.this for join in expression.args.get(\"joins\") or [])\n+                expressions.extend(\n+                    join.this for join in expression.args.get(\"joins\") or []\n+                )\n \n             continue\n \n@@ -804,7 +828,9 @@ def _traverse_tables(scope):\n def _traverse_subqueries(scope):\n     for subquery in scope.subqueries:\n         top = None\n-        for child_scope in _traverse_scope(scope.branch(subquery, scope_type=ScopeType.SUBQUERY)):\n+        for child_scope in _traverse_scope(\n+            scope.branch(subquery, scope_type=ScopeType.SUBQUERY)\n+        ):\n             yield child_scope\n             top = child_scope\n         scope.subquery_scopes.append(top)\n@@ -927,7 +953,11 @@ def _get_source_alias(expression):\n     alias_arg = expression.args.get(\"alias\")\n     alias_name = expression.alias\n \n-    if not alias_name and isinstance(alias_arg, exp.TableAlias) and len(alias_arg.columns) == 1:\n+    if (\n+        not alias_name\n+        and isinstance(alias_arg, exp.TableAlias)\n+        and len(alias_arg.columns) == 1\n+    ):\n         alias_name = alias_arg.columns[0].name\n \n     return alias_name\ndiff --git a/sqlglot/transforms.py b/sqlglot/transforms.py\nindex 464c66e68..1d201bb30 100644\n--- a/sqlglot/transforms.py\n+++ b/sqlglot/transforms.py\n@@ -56,12 +56,16 @@ def preprocess(\n \n             return transforms_handler(self, expression)\n \n-        raise ValueError(f\"Unsupported expression type {expression.__class__.__name__}.\")\n+        raise ValueError(\n+            f\"Unsupported expression type {expression.__class__.__name__}.\"\n+        )\n \n     return _to_sql\n \n \n-def unnest_generate_date_array_using_recursive_cte(expression: exp.Expression) -> exp.Expression:\n+def unnest_generate_date_array_using_recursive_cte(\n+    expression: exp.Expression,\n+) -> exp.Expression:\n     if isinstance(expression, exp.Select):\n         count = 0\n         recursive_ctes = []\n@@ -83,11 +87,16 @@ def unnest_generate_date_array_using_recursive_cte(expression: exp.Expression) -\n                 continue\n \n             alias = unnest.args.get(\"alias\")\n-            column_name = alias.columns[0] if isinstance(alias, exp.TableAlias) else \"date_value\"\n+            column_name = (\n+                alias.columns[0] if isinstance(alias, exp.TableAlias) else \"date_value\"\n+            )\n \n             start = exp.cast(start, \"date\")\n             date_add = exp.func(\n-                \"date_add\", column_name, exp.Literal.number(step.name), step.args.get(\"unit\")\n+                \"date_add\",\n+                column_name,\n+                exp.Literal.number(step.name),\n+                step.args.get(\"unit\"),\n             )\n             cast_date_add = exp.cast(date_add, \"date\")\n \n@@ -112,7 +121,9 @@ def unnest_generate_date_array_using_recursive_cte(expression: exp.Expression) -\n         if recursive_ctes:\n             with_expression = expression.args.get(\"with\") or exp.With()\n             with_expression.set(\"recursive\", True)\n-            with_expression.set(\"expressions\", [*recursive_ctes, *with_expression.expressions])\n+            with_expression.set(\n+                \"expressions\", [*recursive_ctes, *with_expression.expressions]\n+            )\n             expression.set(\"with\", with_expression)\n \n     return expression\n@@ -190,7 +201,9 @@ def eliminate_distinct_on(expression: exp.Expression) -> exp.Expression:\n         if order:\n             window.set(\"order\", order.pop())\n         else:\n-            window.set(\"order\", exp.Order(expressions=[c.copy() for c in distinct_cols]))\n+            window.set(\n+                \"order\", exp.Order(expressions=[c.copy() for c in distinct_cols])\n+            )\n \n         window = exp.alias_(window, row_number_window_alias)\n         expression.select(window, copy=False)\n@@ -205,7 +218,11 @@ def eliminate_distinct_on(expression: exp.Expression) -> exp.Expression:\n \n             if not isinstance(select, exp.Alias):\n                 alias = find_new_name(taken_names, select.output_name or \"_col\")\n-                quoted = select.this.args.get(\"quoted\") if isinstance(select, exp.Column) else None\n+                quoted = (\n+                    select.this.args.get(\"quoted\")\n+                    if isinstance(select, exp.Column)\n+                    else None\n+                )\n                 select = select.replace(exp.alias_(select, alias, quoted=quoted))\n \n             taken_names.add(select.output_name)\n@@ -249,7 +266,9 @@ def eliminate_qualify(expression: exp.Expression) -> exp.Expression:\n                 return exp.column(alias_or_name, quoted=identifier.args.get(\"quoted\"))\n             return alias_or_name\n \n-        outer_selects = exp.select(*list(map(_select_alias_or_name, expression.selects)))\n+        outer_selects = exp.select(\n+            *list(map(_select_alias_or_name, expression.selects))\n+        )\n         qualify_filters = expression.args[\"qualify\"].pop().this\n         expression_by_alias = {\n             select.alias: select.this\n@@ -257,7 +276,9 @@ def eliminate_qualify(expression: exp.Expression) -> exp.Expression:\n             if isinstance(select, exp.Alias)\n         }\n \n-        select_candidates = exp.Window if expression.is_star else (exp.Window, exp.Column)\n+        select_candidates = (\n+            exp.Window if expression.is_star else (exp.Window, exp.Column)\n+        )\n         for select_candidate in list(qualify_filters.find_all(select_candidates)):\n             if isinstance(select_candidate, exp.Window):\n                 if expression_by_alias:\n@@ -277,9 +298,9 @@ def eliminate_qualify(expression: exp.Expression) -> exp.Expression:\n             elif select_candidate.name not in expression.named_selects:\n                 expression.select(select_candidate.copy(), copy=False)\n \n-        return outer_selects.from_(expression.subquery(alias=\"_t\", copy=False), copy=False).where(\n-            qualify_filters, copy=False\n-        )\n+        return outer_selects.from_(\n+            expression.subquery(alias=\"_t\", copy=False), copy=False\n+        ).where(qualify_filters, copy=False)\n \n     return expression\n \n@@ -291,7 +312,8 @@ def remove_precision_parameterized_types(expression: exp.Expression) -> exp.Expr\n     \"\"\"\n     for node in expression.find_all(exp.DataType):\n         node.set(\n-            \"expressions\", [e for e in node.expressions if not isinstance(e, exp.DataTypeParam)]\n+            \"expressions\",\n+            [e for e in node.expressions if not isinstance(e, exp.DataTypeParam)],\n         )\n \n     return expression\n@@ -310,7 +332,10 @@ def unqualify_unnest(expression: exp.Expression) -> exp.Expression:\n         if unnest_aliases:\n             for column in expression.find_all(exp.Column):\n                 leftmost_part = column.parts[0]\n-                if leftmost_part.arg_key != \"this\" and leftmost_part.this in unnest_aliases:\n+                if (\n+                    leftmost_part.arg_key != \"this\"\n+                    and leftmost_part.this in unnest_aliases\n+                ):\n                     leftmost_part.pop()\n \n     return expression\n@@ -327,7 +352,9 @@ def unnest_to_explode(\n     ) -> t.List[exp.Expression]:\n         if has_multi_expr:\n             if not unnest_using_arrays_zip:\n-                raise UnsupportedError(\"Cannot transpile UNNEST with multiple input arrays\")\n+                raise UnsupportedError(\n+                    \"Cannot transpile UNNEST with multiple input arrays\"\n+                )\n \n             # Use INLINE(ARRAYS_ZIP(...)) for multiple expressions\n             zip_exprs: t.List[exp.Expression] = [\n@@ -356,7 +383,10 @@ def unnest_to_explode(\n             offset = unnest.args.get(\"offset\")\n             if offset:\n                 columns.insert(\n-                    0, offset if isinstance(offset, exp.Identifier) else exp.to_identifier(\"pos\")\n+                    0,\n+                    offset\n+                    if isinstance(offset, exp.Identifier)\n+                    else exp.to_identifier(\"pos\"),\n                 )\n \n             unnest.replace(\n@@ -365,7 +395,9 @@ def unnest_to_explode(\n                         this=this,\n                         expressions=expressions,\n                     ),\n-                    alias=exp.TableAlias(this=alias.this, columns=columns) if alias else None,\n+                    alias=exp.TableAlias(this=alias.this, columns=columns)\n+                    if alias\n+                    else None,\n                 )\n             )\n \n@@ -404,7 +436,9 @@ def unnest_to_explode(\n                 if offset:\n                     alias_cols.insert(\n                         0,\n-                        offset if isinstance(offset, exp.Identifier) else exp.to_identifier(\"pos\"),\n+                        offset\n+                        if isinstance(offset, exp.Identifier)\n+                        else exp.to_identifier(\"pos\"),\n                     )\n \n                 for e, column in zip(exprs, alias_cols):\n@@ -444,7 +478,9 @@ def explode_projection_to_unnest(\n             series_alias = new_name(taken_select_names, \"pos\")\n             series = exp.alias_(\n                 exp.Unnest(\n-                    expressions=[exp.GenerateSeries(start=exp.Literal.number(index_offset))]\n+                    expressions=[\n+                        exp.GenerateSeries(start=exp.Literal.number(index_offset))\n+                    ]\n                 ),\n                 new_name(taken_source_names, \"_u\"),\n                 table=[series_alias],\n@@ -480,7 +516,8 @@ def explode_projection_to_unnest(\n                         explode_arg = exp.func(\n                             \"IF\",\n                             exp.func(\n-                                \"ARRAY_SIZE\", exp.func(\"COALESCE\", explode_arg, exp.Array())\n+                                \"ARRAY_SIZE\",\n+                                exp.func(\"COALESCE\", explode_arg, exp.Array()),\n                             ).eq(0),\n                             exp.array(bracket, copy=False),\n                             explode_arg,\n@@ -518,9 +555,9 @@ def explode_projection_to_unnest(\n                         expressions.insert(\n                             expressions.index(alias) + 1,\n                             exp.If(\n-                                this=exp.column(series_alias, table=series_table_alias).eq(\n-                                    exp.column(pos_alias, table=unnest_source_alias)\n-                                ),\n+                                this=exp.column(\n+                                    series_alias, table=series_table_alias\n+                                ).eq(exp.column(pos_alias, table=unnest_source_alias)),\n                                 true=exp.column(pos_alias, table=unnest_source_alias),\n                             ).as_(pos_alias),\n                         )\n@@ -557,15 +594,22 @@ def explode_projection_to_unnest(\n                         exp.column(series_alias, table=series_table_alias)\n                         .eq(exp.column(pos_alias, table=unnest_source_alias))\n                         .or_(\n-                            (exp.column(series_alias, table=series_table_alias) > size).and_(\n-                                exp.column(pos_alias, table=unnest_source_alias).eq(size)\n+                            (\n+                                exp.column(series_alias, table=series_table_alias)\n+                                > size\n+                            ).and_(\n+                                exp.column(pos_alias, table=unnest_source_alias).eq(\n+                                    size\n+                                )\n                             )\n                         ),\n                         copy=False,\n                     )\n \n             if arrays:\n-                end: exp.Condition = exp.Greatest(this=arrays[0], expressions=arrays[1:])\n+                end: exp.Condition = exp.Greatest(\n+                    this=arrays[0], expressions=arrays[1:]\n+                )\n \n                 if index_offset != 1:\n                     end = end - (1 - index_offset)\n@@ -600,7 +644,9 @@ def remove_within_group_for_percentiles(expression: exp.Expression) -> exp.Expre\n     ):\n         quantile = expression.this.this\n         input_value = t.cast(exp.Ordered, expression.find(exp.Ordered)).this\n-        return expression.replace(exp.ApproxQuantile(this=input_value, quantile=quantile))\n+        return expression.replace(\n+            exp.ApproxQuantile(this=input_value, quantile=quantile)\n+        )\n \n     return expression\n \n@@ -618,7 +664,10 @@ def add_recursive_cte_column_names(expression: exp.Expression) -> exp.Expression\n \n                 cte.args[\"alias\"].set(\n                     \"columns\",\n-                    [exp.to_identifier(s.alias_or_name or next_name()) for s in query.selects],\n+                    [\n+                        exp.to_identifier(s.alias_or_name or next_name())\n+                        for s in query.selects\n+                    ],\n                 )\n \n     return expression\n@@ -671,7 +720,10 @@ def eliminate_full_outer_join(expression: exp.Expression) -> exp.Expression:\n             expression.set(\"limit\", None)\n             index, full_outer_join = full_outer_joins[0]\n \n-            tables = (expression.args[\"from\"].alias_or_name, full_outer_join.alias_or_name)\n+            tables = (\n+                expression.args[\"from\"].alias_or_name,\n+                full_outer_join.alias_or_name,\n+            )\n             join_conditions = full_outer_join.args.get(\"on\") or exp.and_(\n                 *[\n                     exp.column(col, tables[0]).eq(exp.column(col, tables[1]))\n@@ -680,9 +732,13 @@ def eliminate_full_outer_join(expression: exp.Expression) -> exp.Expression:\n             )\n \n             full_outer_join.set(\"side\", \"left\")\n-            anti_join_clause = exp.select(\"1\").from_(expression.args[\"from\"]).where(join_conditions)\n+            anti_join_clause = (\n+                exp.select(\"1\").from_(expression.args[\"from\"]).where(join_conditions)\n+            )\n             expression_copy.args[\"joins\"][index].set(\"side\", \"right\")\n-            expression_copy = expression_copy.where(exp.Exists(this=anti_join_clause).not_())\n+            expression_copy = expression_copy.where(\n+                exp.Exists(this=anti_join_clause).not_()\n+            )\n             expression_copy.args.pop(\"with\", None)  # remove CTEs from RIGHT side\n             expression.args.pop(\"order\", None)  # remove order by from LEFT side\n \n@@ -808,9 +864,15 @@ def move_schema_columns_to_partitioned_by(expression: exp.Expression) -> exp.Exp\n         if prop and prop.this and not isinstance(prop.this, exp.Schema):\n             schema = expression.this\n             columns = {v.name.upper() for v in prop.this.expressions}\n-            partitions = [col for col in schema.expressions if col.name.upper() in columns]\n-            schema.set(\"expressions\", [e for e in schema.expressions if e not in partitions])\n-            prop.replace(exp.PartitionedByProperty(this=exp.Schema(expressions=partitions)))\n+            partitions = [\n+                col for col in schema.expressions if col.name.upper() in columns\n+            ]\n+            schema.set(\n+                \"expressions\", [e for e in schema.expressions if e not in partitions]\n+            )\n+            prop.replace(\n+                exp.PartitionedByProperty(this=exp.Schema(expressions=partitions))\n+            )\n             expression.set(\"this\", schema)\n \n     return expression\n@@ -911,7 +973,9 @@ def eliminate_join_marks(expression: exp.Expression) -> exp.Expression:\n         assert not scope.is_correlated_subquery, \"Correlated queries are not supported\"\n \n         # nothing to do - we check it here after knockout above\n-        if not where or not any(c.args.get(\"join_mark\") for c in where.find_all(exp.Column)):\n+        if not where or not any(\n+            c.args.get(\"join_mark\") for c in where.find_all(exp.Column)\n+        ):\n             continue\n \n         # make sure we have AND of ORs to have clear join terms\n@@ -920,7 +984,9 @@ def eliminate_join_marks(expression: exp.Expression) -> exp.Expression:\n \n         joins_ons = defaultdict(list)  # dict of {name: list of join AND conditions}\n         for cond in [where] if not isinstance(where, exp.And) else where.flatten():\n-            join_cols = [col for col in cond.find_all(exp.Column) if col.args.get(\"join_mark\")]\n+            join_cols = [\n+                col for col in cond.find_all(exp.Column) if col.args.get(\"join_mark\")\n+            ]\n \n             left_join_table = set(col.table for col in join_cols)\n             if not left_join_table:\n@@ -990,7 +1056,9 @@ def any_to_exists(expression: exp.Expression) -> exp.Expression:\n     if isinstance(expression, exp.Select):\n         for any_expr in expression.find_all(exp.Any):\n             this = any_expr.this\n-            if isinstance(this, exp.Query) or isinstance(any_expr.parent, (exp.Like, exp.ILike)):\n+            if isinstance(this, exp.Query) or isinstance(\n+                any_expr.parent, (exp.Like, exp.ILike)\n+            ):\n                 continue\n \n             binop = any_expr.parent\n"
    },
    {
        "id": "277",
        "sha_fail": "bddc230cc11627fdbb8bdd98b67489afcad87d98",
        "diff": "diff --git a/tests/dialects/test_dialect.py b/tests/dialects/test_dialect.py\nindex 892fcdc5f..93d3338c9 100644\n--- a/tests/dialects/test_dialect.py\n+++ b/tests/dialects/test_dialect.py\n@@ -22,7 +22,12 @@ class Validator(unittest.TestCase):\n         return parse_one(sql, read=self.dialect, **kwargs)\n \n     def validate_identity(\n-        self, sql, write_sql=None, pretty=False, check_command_warning=False, identify=False\n+        self,\n+        sql,\n+        write_sql=None,\n+        pretty=False,\n+        check_command_warning=False,\n+        identify=False,\n     ):\n         if check_command_warning:\n             with self.assertLogs(parser_logger) as cm:\n@@ -32,7 +37,8 @@ class Validator(unittest.TestCase):\n             expression = self.parse_one(sql)\n \n         self.assertEqual(\n-            write_sql or sql, expression.sql(dialect=self.dialect, pretty=pretty, identify=identify)\n+            write_sql or sql,\n+            expression.sql(dialect=self.dialect, pretty=pretty, identify=identify),\n         )\n         return expression\n \n@@ -67,7 +73,9 @@ class Validator(unittest.TestCase):\n             with self.subTest(f\"{sql} -> {write_dialect}\"):\n                 if write_sql is UnsupportedError:\n                     with self.assertRaises(UnsupportedError):\n-                        expression.sql(write_dialect, unsupported_level=ErrorLevel.RAISE)\n+                        expression.sql(\n+                            write_dialect, unsupported_level=ErrorLevel.RAISE\n+                        )\n                 else:\n                     self.assertEqual(\n                         expression.sql(\n@@ -113,7 +121,9 @@ class TestDialect(Validator):\n         lowercase_mysql = Dialect.get_or_raise(\"mysql,normalization_strategy=lowercase\")\n         self.assertEqual(lowercase_mysql.normalization_strategy, \"LOWERCASE\")\n \n-        lowercase_mysql = Dialect.get_or_raise(\"mysql, normalization_strategy = lowercase\")\n+        lowercase_mysql = Dialect.get_or_raise(\n+            \"mysql, normalization_strategy = lowercase\"\n+        )\n         self.assertEqual(lowercase_mysql.normalization_strategy.value, \"LOWERCASE\")\n \n         with self.assertRaises(AttributeError) as cm:\n@@ -124,7 +134,9 @@ class TestDialect(Validator):\n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"myqsl\")\n \n-        self.assertEqual(str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\")\n+        self.assertEqual(\n+            str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\"\n+        )\n \n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"asdfjasodiufjsd\")\n@@ -140,7 +152,9 @@ class TestDialect(Validator):\n         class MyDialect(Dialect):\n             SUPPORTED_SETTINGS = {\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"}\n \n-        bool_settings = Dialect.get_or_raise(\"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\")\n+        bool_settings = Dialect.get_or_raise(\n+            \"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\"\n+        )\n         self.assertEqual(\n             bool_settings.settings,\n             {\"s1\": True, \"s2\": True, \"s3\": False, \"s4\": False, \"s5\": \"nonbool\"},\n@@ -450,7 +464,9 @@ class TestDialect(Validator):\n             },\n         )\n         self.validate_all(\"CAST(a AS TINYINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n-        self.validate_all(\"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n+        self.validate_all(\n+            \"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"}\n+        )\n         self.validate_all(\"CAST(a AS BIGINT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\"CAST(a AS INT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\n@@ -1411,11 +1427,15 @@ class TestDialect(Validator):\n         )\n \n         order_by_all_sql = \"SELECT * FROM t ORDER BY ALL\"\n-        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(exp.Column)\n+        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(\n+            exp.Column\n+        )\n \n         for dialect in (\"duckdb\", \"spark\", \"databricks\"):\n             with self.subTest(f\"Testing ORDER BY ALL in {dialect}\"):\n-                parse_one(order_by_all_sql, read=dialect).find(exp.Ordered).this.assert_is(exp.Var)\n+                parse_one(order_by_all_sql, read=dialect).find(\n+                    exp.Ordered\n+                ).this.assert_is(exp.Var)\n \n     def test_json(self):\n         self.validate_all(\n@@ -1590,9 +1610,13 @@ class TestDialect(Validator):\n         )\n \n         for dialect in (\"duckdb\", \"starrocks\"):\n-            with self.subTest(f\"Generating json extraction with digit-prefixed key ({dialect})\"):\n+            with self.subTest(\n+                f\"Generating json extraction with digit-prefixed key ({dialect})\"\n+            ):\n                 self.assertEqual(\n-                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(dialect=dialect),\n+                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(\n+                        dialect=dialect\n+                    ),\n                     \"\"\"SELECT '{\"0\": \"v\"}' -> '0'\"\"\",\n                 )\n \n@@ -1809,7 +1833,9 @@ class TestDialect(Validator):\n         )\n \n     def test_operators(self):\n-        self.validate_identity(\"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\")\n+        self.validate_identity(\n+            \"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\"\n+        )\n         self.validate_identity(\"some.column LIKE 'foo' + another.column + 'bar'\")\n \n         self.validate_all(\"LIKE(x, 'z')\", write={\"\": \"'z' LIKE x\"})\n@@ -2299,7 +2325,9 @@ class TestDialect(Validator):\n         )\n \n     def test_typeddiv(self):\n-        typed_div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True)\n+        typed_div = exp.Div(\n+            this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True\n+        )\n         div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"))\n         typed_div_dialect = \"presto\"\n         div_dialect = \"hive\"\n@@ -2324,7 +2352,9 @@ class TestDialect(Validator):\n             (div, (INT, FLOAT), typed_div_dialect, \"a / b\"),\n             (div, (INT, FLOAT), div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"\n+            ):\n                 expression = expression.copy()\n                 expression.left.type = types[0]\n                 expression.right.type = types[1]\n@@ -2342,7 +2372,9 @@ class TestDialect(Validator):\n             (div, safe_div_dialect, \"a / b\"),\n             (div, div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {dialect} -> {expected}\"\n+            ):\n                 self.assertEqual(expected, expression.sql(dialect=dialect))\n \n         self.assertEqual(\n@@ -2814,7 +2846,8 @@ SELECT\n         self.validate_identity(\"COUNT_IF(DISTINCT cond)\")\n \n         self.validate_all(\n-            \"SELECT COUNT_IF(cond) FILTER\", write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"}\n+            \"SELECT COUNT_IF(cond) FILTER\",\n+            write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"},\n         )\n         self.validate_all(\n             \"SELECT COUNT_IF(col % 2 = 0) FROM foo\",\n@@ -3038,8 +3071,12 @@ FROM subquery2\"\"\",\n         with_large_nulls = \"postgres\"\n \n         sql = \"SELECT * FROM t ORDER BY c\"\n-        sql_nulls_last = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n-        sql_nulls_first = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        sql_nulls_last = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n+        )\n+        sql_nulls_first = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        )\n \n         for read_dialect, desc, nulls_first, expected_sql in (\n             (with_last_nulls, False, None, sql_nulls_last),\n@@ -3072,7 +3109,9 @@ FROM subquery2\"\"\",\n                 )\n \n                 expected_sql = f\"{expected_sql}{sort_order}\"\n-                expression = parse_one(f\"{sql}{sort_order}{null_order}\", read=read_dialect)\n+                expression = parse_one(\n+                    f\"{sql}{sort_order}{null_order}\", read=read_dialect\n+                )\n \n                 self.assertEqual(expression.sql(dialect=\"mysql\"), expected_sql)\n                 self.assertEqual(expression.sql(dialect=\"tsql\"), expected_sql)\n@@ -3157,7 +3196,9 @@ FROM subquery2\"\"\",\n         self.validate_identity(\n             \"CREATE SEQUENCE seq START WITH 1 NO MINVALUE NO MAXVALUE CYCLE NO CACHE\"\n         )\n-        self.validate_identity(\"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\")\n+        self.validate_identity(\n+            \"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\"\n+        )\n         self.validate_identity(\n             \"CREATE OR REPLACE SEQUENCE IF NOT EXISTS seq COMMENT='test comment' ORDER\"\n         )\n@@ -3318,7 +3359,9 @@ FROM subquery2\"\"\",\n                     },\n                 )\n \n-        self.assertIsInstance(parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var)\n+        self.assertIsInstance(\n+            parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var\n+        )\n \n     def test_coalesce(self):\n         \"\"\"\n@@ -3425,7 +3468,9 @@ FROM subquery2\"\"\",\n \n     def test_escaped_identifier_delimiter(self):\n         for dialect in (\"databricks\", \"hive\", \"mysql\", \"spark2\", \"spark\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3447,7 +3492,9 @@ FROM subquery2\"\"\",\n             \"snowflake\",\n             \"sqlite\",\n         ):\n-            with self.subTest(f\"Testing escaped double-quote in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped double-quote in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x\"\"\"',\n                     read={\n@@ -3459,7 +3506,9 @@ FROM subquery2\"\"\",\n                 )\n \n         for dialect in (\"clickhouse\", \"sqlite\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3569,14 +3618,19 @@ FROM subquery2\"\"\",\n                 \"spark\",\n                 \"redshift\",\n             ):\n-                with self.subTest(f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"):\n+                with self.subTest(\n+                    f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"\n+                ):\n                     self.assertEqual(\n-                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect), \"SELECT 204\"\n+                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect),\n+                        \"SELECT 204\",\n                     )\n \n             for other_integer_dialects in integer_dialects:\n                 self.assertEqual(\n-                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(other_integer_dialects),\n+                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(\n+                        other_integer_dialects\n+                    ),\n                     \"SELECT 0xCC\",\n                 )\n \ndiff --git a/tests/dialects/test_dremio.py b/tests/dialects/test_dremio.py\nindex d5fc36208..60588fdaa 100644\n--- a/tests/dialects/test_dremio.py\n+++ b/tests/dialects/test_dremio.py\n@@ -63,7 +63,8 @@ class TestDremio(Validator):\n             \"SELECT * FROM t ORDER BY a NULLS LAST\", \"SELECT * FROM t ORDER BY a\"\n         )\n         self.validate_identity(\n-            \"SELECT * FROM t ORDER BY a DESC NULLS LAST\", \"SELECT * FROM t ORDER BY a DESC\"\n+            \"SELECT * FROM t ORDER BY a DESC NULLS LAST\",\n+            \"SELECT * FROM t ORDER BY a DESC\",\n         )\n \n         # If the clause is not the default, it must be kept\n@@ -138,20 +139,26 @@ class TestDremio(Validator):\n         to_char = self.validate_identity(\"TO_CHAR(3.14, '#.#')\").assert_is(exp.ToChar)\n         assert to_char.args[\"is_numeric\"] is True\n \n-        to_char = self.validate_identity(\"TO_CHAR(columnname, '#.##')\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(columnname, '#.##')\").assert_is(\n+            exp.ToChar\n+        )\n         assert to_char.args[\"is_numeric\"] is True\n \n         # Non-numeric formats or columns should have is_numeric=None or False\n         to_char = self.validate_identity(\"TO_CHAR(5555)\").assert_is(exp.ToChar)\n         assert not to_char.args.get(\"is_numeric\")\n \n-        to_char = self.validate_identity(\"TO_CHAR(3.14, columnname)\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(3.14, columnname)\").assert_is(\n+            exp.ToChar\n+        )\n         assert not to_char.args.get(\"is_numeric\")\n \n         to_char = self.validate_identity(\"TO_CHAR(123, 'abcd')\").assert_is(exp.ToChar)\n         assert not to_char.args.get(\"is_numeric\")\n \n-        to_char = self.validate_identity(\"TO_CHAR(3.14, UPPER('abcd'))\").assert_is(exp.ToChar)\n+        to_char = self.validate_identity(\"TO_CHAR(3.14, UPPER('abcd'))\").assert_is(\n+            exp.ToChar\n+        )\n         assert not to_char.args.get(\"is_numeric\")\n \n     def test_time_diff(self):\n@@ -175,7 +182,9 @@ class TestDremio(Validator):\n             \"SELECT DATE_SUB(col, 2, 'HOUR')\", \"SELECT TIMESTAMPADD(HOUR, -2, col)\"\n         )\n \n-        self.validate_identity(\"SELECT DATE_ADD(col, 2, 'DAY')\", \"SELECT DATE_ADD(col, 2)\")\n+        self.validate_identity(\n+            \"SELECT DATE_ADD(col, 2, 'DAY')\", \"SELECT DATE_ADD(col, 2)\"\n+        )\n \n         self.validate_identity(\n             \"SELECT DATE_SUB(col, a, 'HOUR')\", \"SELECT TIMESTAMPADD(HOUR, a * -1, col)\"\n"
    },
    {
        "id": "278",
        "sha_fail": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
        "diff": "diff --git a/tools/document_segmentation_server.py b/tools/document_segmentation_server.py\nindex 3ce17bf..59cb57b 100644\n--- a/tools/document_segmentation_server.py\n+++ b/tools/document_segmentation_server.py\n@@ -22,7 +22,7 @@ large research papers and technical documents that exceed LLM token limits.\n    - Stores segmentation index for efficient retrieval\n    - Returns: JSON with segmentation status, strategy used, and segment count\n \n-\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None, \n+\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None,\n                          max_segments: int = 3, max_total_chars: int = None)\n    Purpose: Intelligently retrieves relevant document segments based on query context\n    - query_type: \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n@@ -63,8 +63,7 @@ import re\n import json\n import sys\n import io\n-from pathlib import Path\n-from typing import Dict, Any, List, Optional, Tuple\n+from typing import Dict, List, Tuple\n import hashlib\n import logging\n from datetime import datetime\n@@ -92,9 +91,11 @@ logger = logging.getLogger(__name__)\n # Create FastMCP server instance\n mcp = FastMCP(\"document-segmentation-server\")\n \n+\n @dataclass\n class DocumentSegment:\n     \"\"\"Represents a document segment with metadata\"\"\"\n+\n     id: str\n     title: str\n     content: str\n@@ -106,9 +107,11 @@ class DocumentSegment:\n     relevance_scores: Dict[str, float]  # Scores for different query types\n     section_path: str  # e.g., \"3.2.1\" for nested sections\n \n-@dataclass \n+\n+@dataclass\n class DocumentIndex:\n     \"\"\"Document index containing all segments and metadata\"\"\"\n+\n     document_path: str\n     document_type: str  # \"academic_paper\", \"technical_doc\", \"code_doc\", \"general\"\n     segmentation_strategy: str\n@@ -116,66 +119,86 @@ class DocumentIndex:\n     total_chars: int\n     segments: List[DocumentSegment]\n     created_at: str\n-    \n+\n+\n class DocumentAnalyzer:\n     \"\"\"Enhanced document analyzer using semantic content analysis instead of mechanical structure detection\"\"\"\n-    \n+\n     # More precise semantic indicators, weighted by importance\n     ALGORITHM_INDICATORS = {\n-        \"high\": [\"algorithm\", \"procedure\", \"method\", \"approach\", \"technique\", \"framework\"],\n+        \"high\": [\n+            \"algorithm\",\n+            \"procedure\",\n+            \"method\",\n+            \"approach\",\n+            \"technique\",\n+            \"framework\",\n+        ],\n         \"medium\": [\"step\", \"process\", \"implementation\", \"computation\", \"calculation\"],\n-        \"low\": [\"example\", \"illustration\", \"demonstration\"]\n+        \"low\": [\"example\", \"illustration\", \"demonstration\"],\n     }\n-    \n+\n     TECHNICAL_CONCEPT_INDICATORS = {\n         \"high\": [\"formula\", \"equation\", \"theorem\", \"lemma\", \"proof\", \"definition\"],\n         \"medium\": [\"parameter\", \"variable\", \"function\", \"model\", \"architecture\"],\n-        \"low\": [\"notation\", \"symbol\", \"term\"]\n+        \"low\": [\"notation\", \"symbol\", \"term\"],\n     }\n-    \n+\n     IMPLEMENTATION_INDICATORS = {\n         \"high\": [\"code\", \"implementation\", \"programming\", \"software\", \"system\"],\n         \"medium\": [\"design\", \"structure\", \"module\", \"component\", \"interface\"],\n-        \"low\": [\"tool\", \"library\", \"package\"]\n+        \"low\": [\"tool\", \"library\", \"package\"],\n     }\n-    \n+\n     # Semantic features of document types (not just based on titles)\n     RESEARCH_PAPER_PATTERNS = [\n         r\"(?i)\\babstract\\b.*?\\n.*?(introduction|motivation|background)\",\n         r\"(?i)(methodology|method).*?(experiment|evaluation|result)\",\n         r\"(?i)(conclusion|future work|limitation).*?(reference|bibliography)\",\n-        r\"(?i)(related work|literature review|prior art)\"\n+        r\"(?i)(related work|literature review|prior art)\",\n     ]\n-    \n+\n     TECHNICAL_DOC_PATTERNS = [\n         r\"(?i)(getting started|installation|setup).*?(usage|example)\",\n         r\"(?i)(api|interface|specification).*?(parameter|endpoint)\",\n         r\"(?i)(tutorial|guide|walkthrough).*?(step|instruction)\",\n-        r\"(?i)(troubleshooting|faq|common issues)\"\n+        r\"(?i)(troubleshooting|faq|common issues)\",\n     ]\n-    \n+\n     def analyze_document_type(self, content: str) -> Tuple[str, float]:\n         \"\"\"\n         Enhanced document type analysis based on semantic content patterns\n-        \n+\n         Returns:\n             Tuple[str, float]: (document_type, confidence_score)\n         \"\"\"\n         content_lower = content.lower()\n-        \n+\n         # Calculate weighted semantic indicator scores\n-        algorithm_score = self._calculate_weighted_score(content_lower, self.ALGORITHM_INDICATORS)\n-        concept_score = self._calculate_weighted_score(content_lower, self.TECHNICAL_CONCEPT_INDICATORS)\n-        implementation_score = self._calculate_weighted_score(content_lower, self.IMPLEMENTATION_INDICATORS)\n-        \n+        algorithm_score = self._calculate_weighted_score(\n+            content_lower, self.ALGORITHM_INDICATORS\n+        )\n+        concept_score = self._calculate_weighted_score(\n+            content_lower, self.TECHNICAL_CONCEPT_INDICATORS\n+        )\n+        implementation_score = self._calculate_weighted_score(\n+            content_lower, self.IMPLEMENTATION_INDICATORS\n+        )\n+\n         # Detect semantic patterns of document types\n-        research_pattern_score = self._detect_pattern_score(content, self.RESEARCH_PAPER_PATTERNS)\n-        technical_pattern_score = self._detect_pattern_score(content, self.TECHNICAL_DOC_PATTERNS)\n-        \n+        research_pattern_score = self._detect_pattern_score(\n+            content, self.RESEARCH_PAPER_PATTERNS\n+        )\n+        technical_pattern_score = self._detect_pattern_score(\n+            content, self.TECHNICAL_DOC_PATTERNS\n+        )\n+\n         # Comprehensive evaluation of document type\n-        total_research_score = algorithm_score + concept_score + research_pattern_score * 2\n+        total_research_score = (\n+            algorithm_score + concept_score + research_pattern_score * 2\n+        )\n         total_technical_score = implementation_score + technical_pattern_score * 2\n-        \n+\n         # Determine document type based on content density and pattern matching\n         if research_pattern_score > 0.5 and total_research_score > 3.0:\n             return \"research_paper\", min(0.95, 0.6 + research_pattern_score * 0.35)\n@@ -187,17 +210,21 @@ class DocumentAnalyzer:\n             return \"implementation_guide\", 0.75\n         else:\n             return \"general_document\", 0.5\n-    \n-    def _calculate_weighted_score(self, content: str, indicators: Dict[str, List[str]]) -> float:\n+\n+    def _calculate_weighted_score(\n+        self, content: str, indicators: Dict[str, List[str]]\n+    ) -> float:\n         \"\"\"Calculate weighted semantic indicator scores\"\"\"\n         score = 0.0\n         for weight_level, terms in indicators.items():\n             weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[weight_level]\n             for term in terms:\n                 if term in content:\n-                    score += weight * (content.count(term) * 0.5 + 1)  # Consider term frequency\n+                    score += weight * (\n+                        content.count(term) * 0.5 + 1\n+                    )  # Consider term frequency\n         return score\n-    \n+\n     def _detect_pattern_score(self, content: str, patterns: List[str]) -> float:\n         \"\"\"Detect semantic pattern matching scores\"\"\"\n         matches = 0\n@@ -205,7 +232,7 @@ class DocumentAnalyzer:\n             if re.search(pattern, content, re.DOTALL):\n                 matches += 1\n         return matches / len(patterns)\n-    \n+\n     def detect_segmentation_strategy(self, content: str, doc_type: str) -> str:\n         \"\"\"\n         Intelligently determine the best segmentation strategy based on content semantics rather than mechanical structure\n@@ -213,8 +240,10 @@ class DocumentAnalyzer:\n         # Analyze content characteristics\n         algorithm_density = self._calculate_algorithm_density(content)\n         concept_complexity = self._calculate_concept_complexity(content)\n-        implementation_detail_level = self._calculate_implementation_detail_level(content)\n-        \n+        implementation_detail_level = self._calculate_implementation_detail_level(\n+            content\n+        )\n+\n         # Select strategy based on document type and content characteristics\n         if doc_type == \"research_paper\" and algorithm_density > 0.3:\n             return \"semantic_research_focused\"\n@@ -226,21 +255,21 @@ class DocumentAnalyzer:\n             return \"semantic_chunking_enhanced\"\n         else:\n             return \"content_aware_segmentation\"\n-    \n+\n     def _calculate_algorithm_density(self, content: str) -> float:\n         \"\"\"Calculate algorithm content density\"\"\"\n         total_chars = len(content)\n         algorithm_chars = 0\n-        \n+\n         # Identify algorithm blocks\n         algorithm_patterns = [\n-            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)',\n-            r'(?i)(step\\s+\\d+|phase\\s+\\d+)',\n-            r'(?i)(input:|output:|return:|initialize:)',\n-            r'(?i)(for\\s+each|while|if.*then|else)',\n-            r'(?i)(function|method|procedure).*\\('\n+            r\"(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)\",\n+            r\"(?i)(step\\s+\\d+|phase\\s+\\d+)\",\n+            r\"(?i)(input:|output:|return:|initialize:)\",\n+            r\"(?i)(for\\s+each|while|if.*then|else)\",\n+            r\"(?i)(function|method|procedure).*\\(\",\n         ]\n-        \n+\n         for pattern in algorithm_patterns:\n             matches = re.finditer(pattern, content)\n             for match in matches:\n@@ -248,44 +277,45 @@ class DocumentAnalyzer:\n                 start = max(0, match.start() - 200)\n                 end = min(len(content), match.end() + 800)\n                 algorithm_chars += end - start\n-        \n+\n         return min(1.0, algorithm_chars / total_chars)\n-    \n+\n     def _calculate_concept_complexity(self, content: str) -> float:\n         \"\"\"Calculate concept complexity\"\"\"\n         concept_indicators = self.TECHNICAL_CONCEPT_INDICATORS\n         complexity_score = 0.0\n-        \n+\n         for level, terms in concept_indicators.items():\n             weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[level]\n             for term in terms:\n                 complexity_score += content.lower().count(term) * weight\n-        \n+\n         # Normalize to 0-1 range\n         return min(1.0, complexity_score / 100)\n-    \n+\n     def _calculate_implementation_detail_level(self, content: str) -> float:\n         \"\"\"Calculate implementation detail level\"\"\"\n         implementation_patterns = [\n-            r'(?i)(code|implementation|programming)',\n-            r'(?i)(class|function|method|variable)',\n-            r'(?i)(import|include|library)',\n-            r'(?i)(parameter|argument|return)',\n-            r'(?i)(example|demo|tutorial)'\n+            r\"(?i)(code|implementation|programming)\",\n+            r\"(?i)(class|function|method|variable)\",\n+            r\"(?i)(import|include|library)\",\n+            r\"(?i)(parameter|argument|return)\",\n+            r\"(?i)(example|demo|tutorial)\",\n         ]\n-        \n+\n         detail_score = 0\n         for pattern in implementation_patterns:\n             detail_score += len(re.findall(pattern, content))\n-        \n+\n         return min(1.0, detail_score / 50)\n \n+\n class DocumentSegmenter:\n     \"\"\"Creates intelligent segments from documents\"\"\"\n-    \n+\n     def __init__(self):\n         self.analyzer = DocumentAnalyzer()\n-    \n+\n     def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:\n         \"\"\"\n         Perform intelligent segmentation using the specified strategy\n@@ -303,38 +333,45 @@ class DocumentSegmenter:\n         else:\n             # Compatibility with legacy strategies\n             return self._segment_by_enhanced_semantic_chunks(content)\n-    \n+\n     def _segment_by_headers(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment document based on markdown headers\"\"\"\n         segments = []\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n         current_segment = []\n         current_header = None\n         current_level = 0\n         char_pos = 0\n-        \n+\n         for line in lines:\n-            line_with_newline = line + '\\n'\n-            \n+            line_with_newline = line + \"\\n\"\n+\n             # Check if line is a header\n-            header_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n-            \n+            header_match = re.match(r\"^(#{1,6})\\s+(.+)$\", line)\n+\n             if header_match:\n                 # Save previous segment if exists\n                 if current_segment and current_header:\n-                    segment_content = '\\n'.join(current_segment).strip()\n+                    segment_content = \"\\n\".join(current_segment).strip()\n                     if segment_content:\n                         # Analyze content type and importance\n-                        content_type = self._classify_content_type(current_header, segment_content)\n-                        importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n-                        \n+                        content_type = self._classify_content_type(\n+                            current_header, segment_content\n+                        )\n+                        importance_score = (\n+                            0.8 if content_type in [\"algorithm\", \"formula\"] else 0.7\n+                        )\n+\n                         segment = self._create_enhanced_segment(\n-                            segment_content, current_header, \n-                            char_pos - len(segment_content.encode('utf-8')), char_pos,\n-                            importance_score, content_type\n+                            segment_content,\n+                            current_header,\n+                            char_pos - len(segment_content.encode(\"utf-8\")),\n+                            char_pos,\n+                            importance_score,\n+                            content_type,\n                         )\n                         segments.append(segment)\n-                \n+\n                 # Start new segment\n                 current_level = len(header_match.group(1))\n                 current_header = header_match.group(2).strip()\n@@ -342,128 +379,143 @@ class DocumentSegmenter:\n             else:\n                 if current_segment is not None:\n                     current_segment.append(line)\n-            \n-            char_pos += len(line_with_newline.encode('utf-8'))\n-        \n+\n+            char_pos += len(line_with_newline.encode(\"utf-8\"))\n+\n         # Add final segment\n         if current_segment and current_header:\n-            segment_content = '\\n'.join(current_segment).strip()\n+            segment_content = \"\\n\".join(current_segment).strip()\n             if segment_content:\n                 # Analyze content type and importance\n-                content_type = self._classify_content_type(current_header, segment_content)\n-                importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n-                \n+                content_type = self._classify_content_type(\n+                    current_header, segment_content\n+                )\n+                importance_score = (\n+                    0.8 if content_type in [\"algorithm\", \"formula\"] else 0.7\n+                )\n+\n                 segment = self._create_enhanced_segment(\n-                    segment_content, current_header, \n-                    char_pos - len(segment_content.encode('utf-8')), char_pos,\n-                    importance_score, content_type\n+                    segment_content,\n+                    current_header,\n+                    char_pos - len(segment_content.encode(\"utf-8\")),\n+                    char_pos,\n+                    importance_score,\n+                    content_type,\n                 )\n                 segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_preserve_algorithm_integrity(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_preserve_algorithm_integrity(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Smart segmentation strategy that preserves algorithm integrity\"\"\"\n         segments = []\n-        \n+\n         # 1. Identify algorithm blocks and related descriptions\n         algorithm_blocks = self._identify_algorithm_blocks(content)\n-        \n+\n         # 2. Identify concept definition groups\n         concept_groups = self._identify_concept_groups(content)\n-        \n+\n         # 3. Identify formula derivation chains\n         formula_chains = self._identify_formula_chains(content)\n-        \n+\n         # 4. Merge related content blocks to ensure integrity\n         content_blocks = self._merge_related_content_blocks(\n             algorithm_blocks, concept_groups, formula_chains, content\n         )\n-        \n+\n         # 5. Convert to DocumentSegment\n         for i, block in enumerate(content_blocks):\n             segment = self._create_enhanced_segment(\n-                block['content'], \n-                block['title'], \n-                block['start_pos'], \n-                block['end_pos'],\n-                block['importance_score'],\n-                block['content_type']\n+                block[\"content\"],\n+                block[\"title\"],\n+                block[\"start_pos\"],\n+                block[\"end_pos\"],\n+                block[\"importance_score\"],\n+                block[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_research_paper_semantically(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_research_paper_semantically(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Semantic segmentation specifically for research papers\"\"\"\n         segments = []\n-        \n+\n         # Identify semantic structure of research papers\n         paper_sections = self._identify_research_paper_sections(content)\n-        \n+\n         for section in paper_sections:\n             # Ensure each section contains sufficient context\n             enhanced_content = self._enhance_section_with_context(section, content)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                enhanced_content['content'],\n-                enhanced_content['title'],\n-                enhanced_content['start_pos'],\n-                enhanced_content['end_pos'],\n-                enhanced_content['importance_score'],\n-                enhanced_content['content_type']\n+                enhanced_content[\"content\"],\n+                enhanced_content[\"title\"],\n+                enhanced_content[\"start_pos\"],\n+                enhanced_content[\"end_pos\"],\n+                enhanced_content[\"importance_score\"],\n+                enhanced_content[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_concept_implementation_hybrid(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_concept_implementation_hybrid(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Intelligent segmentation combining concepts and implementation\"\"\"\n         segments = []\n-        \n+\n         # Identify concept-implementation correspondence\n         concept_impl_pairs = self._identify_concept_implementation_pairs(content)\n-        \n+\n         for pair in concept_impl_pairs:\n             # Merge related concepts and implementations into one segment\n             merged_content = self._merge_concept_with_implementation(pair, content)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                merged_content['content'],\n-                merged_content['title'],\n-                merged_content['start_pos'],\n-                merged_content['end_pos'],\n-                merged_content['importance_score'],\n-                merged_content['content_type']\n+                merged_content[\"content\"],\n+                merged_content[\"title\"],\n+                merged_content[\"start_pos\"],\n+                merged_content[\"end_pos\"],\n+                merged_content[\"importance_score\"],\n+                merged_content[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n-    def _segment_by_enhanced_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n+\n+    def _segment_by_enhanced_semantic_chunks(\n+        self, content: str\n+    ) -> List[DocumentSegment]:\n         \"\"\"Enhanced semantic chunk segmentation\"\"\"\n         segments = []\n-        \n+\n         # Use improved semantic boundary detection\n         semantic_boundaries = self._detect_semantic_boundaries(content)\n-        \n+\n         current_start = 0\n         for i, boundary in enumerate(semantic_boundaries):\n-            chunk_content = content[current_start:boundary['position']]\n-            \n+            chunk_content = content[current_start : boundary[\"position\"]]\n+\n             if len(chunk_content.strip()) > 200:  # Minimum content threshold\n                 segment = self._create_enhanced_segment(\n                     chunk_content,\n-                    boundary['suggested_title'],\n+                    boundary[\"suggested_title\"],\n                     current_start,\n-                    boundary['position'],\n-                    boundary['importance_score'],\n-                    boundary['content_type']\n+                    boundary[\"position\"],\n+                    boundary[\"importance_score\"],\n+                    boundary[\"content_type\"],\n                 )\n                 segments.append(segment)\n-            \n-            current_start = boundary['position']\n-        \n+\n+            current_start = boundary[\"position\"]\n+\n         # Handle the final segment\n         if current_start < len(content):\n             final_content = content[current_start:]\n@@ -474,417 +526,484 @@ class DocumentSegmenter:\n                     current_start,\n                     len(content),\n                     0.7,\n-                    \"general\"\n+                    \"general\",\n                 )\n                 segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_content_aware(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Content-aware intelligent segmentation\"\"\"\n         segments = []\n-        \n+\n         # Adaptive segmentation size\n         optimal_chunk_size = self._calculate_optimal_chunk_size(content)\n-        \n+\n         # Segment based on content density\n         content_chunks = self._create_content_aware_chunks(content, optimal_chunk_size)\n-        \n+\n         for chunk in content_chunks:\n             segment = self._create_enhanced_segment(\n-                chunk['content'],\n-                chunk['title'],\n-                chunk['start_pos'],\n-                chunk['end_pos'],\n-                chunk['importance_score'],\n-                chunk['content_type']\n+                chunk[\"content\"],\n+                chunk[\"title\"],\n+                chunk[\"start_pos\"],\n+                chunk[\"end_pos\"],\n+                chunk[\"importance_score\"],\n+                chunk[\"content_type\"],\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_academic_paper(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment academic paper using semantic understanding\"\"\"\n         # First try header-based segmentation\n-        headers = re.findall(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE)\n+        headers = re.findall(r\"^(#{1,6})\\s+(.+)$\", content, re.MULTILINE)\n         if len(headers) >= 2:\n             return self._segment_by_headers(content)\n-        \n+\n         # Fallback to semantic detection of academic sections\n         sections = self._detect_academic_sections(content)\n         segments = []\n-        \n+\n         for section in sections:\n             # Determine importance based on section type\n-            section_type = section.get('type', 'general')\n-            content_type = section_type if section_type in ['algorithm', 'formula', 'introduction', 'conclusion'] else 'general'\n+            section_type = section.get(\"type\", \"general\")\n+            content_type = (\n+                section_type\n+                if section_type\n+                in [\"algorithm\", \"formula\", \"introduction\", \"conclusion\"]\n+                else \"general\"\n+            )\n             importance_score = {\n-                'algorithm': 0.95,\n-                'formula': 0.9,\n-                'introduction': 0.85,\n-                'conclusion': 0.8\n+                \"algorithm\": 0.95,\n+                \"formula\": 0.9,\n+                \"introduction\": 0.85,\n+                \"conclusion\": 0.8,\n             }.get(content_type, 0.7)\n-            \n+\n             segment = self._create_enhanced_segment(\n-                section['content'], \n-                section['title'], \n-                section['start_pos'], \n-                section['end_pos'],\n+                section[\"content\"],\n+                section[\"title\"],\n+                section[\"start_pos\"],\n+                section[\"end_pos\"],\n                 importance_score,\n-                content_type\n+                content_type,\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _detect_academic_sections(self, content: str) -> List[Dict]:\n         \"\"\"Detect academic paper sections even without clear headers\"\"\"\n         sections = []\n-        \n+\n         # Common academic section patterns\n         section_patterns = [\n-            (r'(?i)(abstract|\u6458\u8981)', 'introduction'),\n-            (r'(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)', 'introduction'),\n-            (r'(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)', 'background'),\n-            (r'(?i)(method|methodology|approach|\u65b9\u6cd5)', 'methodology'),\n-            (r'(?i)(algorithm|\u7b97\u6cd5)', 'algorithm'),\n-            (r'(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)', 'experiment'),\n-            (r'(?i)(result|\u7ed3\u679c|finding)', 'results'),\n-            (r'(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)', 'conclusion'),\n-            (r'(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)', 'references')\n+            (r\"(?i)(abstract|\u6458\u8981)\", \"introduction\"),\n+            (r\"(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)\", \"introduction\"),\n+            (r\"(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)\", \"background\"),\n+            (r\"(?i)(method|methodology|approach|\u65b9\u6cd5)\", \"methodology\"),\n+            (r\"(?i)(algorithm|\u7b97\u6cd5)\", \"algorithm\"),\n+            (r\"(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)\", \"experiment\"),\n+            (r\"(?i)(result|\u7ed3\u679c|finding)\", \"results\"),\n+            (r\"(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)\", \"conclusion\"),\n+            (r\"(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)\", \"references\"),\n         ]\n-        \n+\n         current_pos = 0\n         for i, (pattern, section_type) in enumerate(section_patterns):\n             match = re.search(pattern, content[current_pos:], re.IGNORECASE)\n             if match:\n                 start_pos = current_pos + match.start()\n-                \n+\n                 # Find end position (next section or end of document)\n                 next_pos = len(content)\n-                for next_pattern, _ in section_patterns[i+1:]:\n-                    next_match = re.search(next_pattern, content[start_pos+100:], re.IGNORECASE)\n+                for next_pattern, _ in section_patterns[i + 1 :]:\n+                    next_match = re.search(\n+                        next_pattern, content[start_pos + 100 :], re.IGNORECASE\n+                    )\n                     if next_match:\n                         next_pos = start_pos + 100 + next_match.start()\n                         break\n-                \n+\n                 section_content = content[start_pos:next_pos].strip()\n                 if len(section_content) > 50:  # Minimum content length\n                     # Calculate importance score and content type\n-                    importance_score = self._calculate_paragraph_importance(section_content, section_type)\n-                    content_type = self._classify_content_type(match.group(1), section_content)\n-                    \n-                    sections.append({\n-                        'title': match.group(1),\n-                        'content': section_content,\n-                        'start_pos': start_pos,\n-                        'end_pos': next_pos,\n-                        'type': section_type,\n-                        'importance_score': importance_score,\n-                        'content_type': content_type\n-                    })\n-                \n+                    importance_score = self._calculate_paragraph_importance(\n+                        section_content, section_type\n+                    )\n+                    content_type = self._classify_content_type(\n+                        match.group(1), section_content\n+                    )\n+\n+                    sections.append(\n+                        {\n+                            \"title\": match.group(1),\n+                            \"content\": section_content,\n+                            \"start_pos\": start_pos,\n+                            \"end_pos\": next_pos,\n+                            \"type\": section_type,\n+                            \"importance_score\": importance_score,\n+                            \"content_type\": content_type,\n+                        }\n+                    )\n+\n                 current_pos = next_pos\n-        \n+\n         return sections\n-    \n+\n     def _segment_by_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Segment long documents into semantic chunks\"\"\"\n         # Split into paragraphs first\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n-        \n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n+\n         segments = []\n         current_chunk = []\n         current_chunk_size = 0\n         chunk_size_limit = 3000  # characters\n         overlap_size = 200\n-        \n+\n         char_pos = 0\n-        \n+\n         for para in paragraphs:\n             para_size = len(para)\n-            \n+\n             # If adding this paragraph exceeds limit, create a segment\n             if current_chunk_size + para_size > chunk_size_limit and current_chunk:\n-                chunk_content = '\\n\\n'.join(current_chunk)\n+                chunk_content = \"\\n\\n\".join(current_chunk)\n                 # Analyze semantic chunk content type\n                 content_type = self._classify_paragraph_type(chunk_content)\n-                importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n-                \n+                importance_score = self._calculate_paragraph_importance(\n+                    chunk_content, content_type\n+                )\n+\n                 segment = self._create_enhanced_segment(\n-                    chunk_content, \n+                    chunk_content,\n                     f\"Section {len(segments) + 1}\",\n-                    char_pos - len(chunk_content.encode('utf-8')),\n+                    char_pos - len(chunk_content.encode(\"utf-8\")),\n                     char_pos,\n                     importance_score,\n-                    content_type\n+                    content_type,\n                 )\n                 segments.append(segment)\n-                \n+\n                 # Keep last part for overlap\n-                overlap_content = chunk_content[-overlap_size:] if len(chunk_content) > overlap_size else \"\"\n+                overlap_content = (\n+                    chunk_content[-overlap_size:]\n+                    if len(chunk_content) > overlap_size\n+                    else \"\"\n+                )\n                 current_chunk = [overlap_content, para] if overlap_content else [para]\n                 current_chunk_size = len(overlap_content) + para_size\n             else:\n                 current_chunk.append(para)\n                 current_chunk_size += para_size\n-            \n+\n             char_pos += para_size + 2  # +2 for \\n\\n\n-        \n+\n         # Add final chunk\n         if current_chunk:\n-            chunk_content = '\\n\\n'.join(current_chunk)\n+            chunk_content = \"\\n\\n\".join(current_chunk)\n             # Analyze final chunk content type\n             content_type = self._classify_paragraph_type(chunk_content)\n-            importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n-            \n+            importance_score = self._calculate_paragraph_importance(\n+                chunk_content, content_type\n+            )\n+\n             segment = self._create_enhanced_segment(\n                 chunk_content,\n                 f\"Section {len(segments) + 1}\",\n-                char_pos - len(chunk_content.encode('utf-8')),\n+                char_pos - len(chunk_content.encode(\"utf-8\")),\n                 char_pos,\n                 importance_score,\n-                content_type\n+                content_type,\n             )\n             segments.append(segment)\n-        \n+\n         return segments\n-    \n+\n     def _segment_by_paragraphs(self, content: str) -> List[DocumentSegment]:\n         \"\"\"Simple paragraph-based segmentation for short documents\"\"\"\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n         segments = []\n         char_pos = 0\n-        \n+\n         for i, para in enumerate(paragraphs):\n             if len(para) > 100:  # Only include substantial paragraphs\n                 # Analyze paragraph type and importance\n                 content_type = self._classify_paragraph_type(para)\n-                importance_score = self._calculate_paragraph_importance(para, content_type)\n-                \n+                importance_score = self._calculate_paragraph_importance(\n+                    para, content_type\n+                )\n+\n                 segment = self._create_enhanced_segment(\n                     para,\n                     f\"Paragraph {i + 1}\",\n                     char_pos,\n-                    char_pos + len(para.encode('utf-8')),\n+                    char_pos + len(para.encode(\"utf-8\")),\n                     importance_score,\n-                    content_type\n+                    content_type,\n                 )\n                 segments.append(segment)\n-            char_pos += len(para.encode('utf-8')) + 2\n-        \n+            char_pos += len(para.encode(\"utf-8\")) + 2\n+\n         return segments\n-    \n+\n     # =============== Enhanced intelligent segmentation helper methods ===============\n-    \n+\n     def _identify_algorithm_blocks(self, content: str) -> List[Dict]:\n         \"\"\"Identify algorithm blocks and related descriptions\"\"\"\n         algorithm_blocks = []\n-        \n+\n         # Algorithm block identification patterns\n         algorithm_patterns = [\n-            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)',\n-            r'(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)',\n-            r'(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)',\n-            r'(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)'\n+            r\"(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)\",\n+            r\"(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)\",\n+            r\"(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)\",\n+            r\"(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)\",\n         ]\n-        \n+\n         for pattern in algorithm_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 # Expand context to include complete descriptions\n                 start = max(0, match.start() - 300)\n                 end = min(len(content), match.end() + 500)\n-                \n+\n                 # Find natural boundaries\n-                while start > 0 and content[start] not in '\\n.!?':\n+                while start > 0 and content[start] not in \"\\n.!?\":\n                     start -= 1\n-                while end < len(content) and content[end] not in '\\n.!?':\n+                while end < len(content) and content[end] not in \"\\n.!?\":\n                     end += 1\n-                \n-                algorithm_blocks.append({\n-                    'start_pos': start,\n-                    'end_pos': end,\n-                    'content': content[start:end].strip(),\n-                    'title': self._extract_algorithm_title(content[match.start():match.end()]),\n-                    'importance_score': 0.95,  # High importance for algorithm blocks\n-                    'content_type': 'algorithm'\n-                })\n-        \n+\n+                algorithm_blocks.append(\n+                    {\n+                        \"start_pos\": start,\n+                        \"end_pos\": end,\n+                        \"content\": content[start:end].strip(),\n+                        \"title\": self._extract_algorithm_title(\n+                            content[match.start() : match.end()]\n+                        ),\n+                        \"importance_score\": 0.95,  # High importance for algorithm blocks\n+                        \"content_type\": \"algorithm\",\n+                    }\n+                )\n+\n         return algorithm_blocks\n-    \n+\n     def _identify_concept_groups(self, content: str) -> List[Dict]:\n         \"\"\"Identify concept definition groups\"\"\"\n         concept_groups = []\n-        \n+\n         # Concept definition patterns\n         concept_patterns = [\n-            r'(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)',\n-            r'(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)',\n-            r'(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)'\n+            r\"(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)\",\n+            r\"(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)\",\n+            r\"(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)\",\n         ]\n-        \n+\n         for pattern in concept_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 # Expand context\n                 start = max(0, match.start() - 200)\n                 end = min(len(content), match.end() + 300)\n-                \n-                concept_groups.append({\n-                    'start_pos': start,\n-                    'end_pos': end,\n-                    'content': content[start:end].strip(),\n-                    'title': self._extract_concept_title(content[match.start():match.end()]),\n-                    'importance_score': 0.85,\n-                    'content_type': 'concept'\n-                })\n-        \n+\n+                concept_groups.append(\n+                    {\n+                        \"start_pos\": start,\n+                        \"end_pos\": end,\n+                        \"content\": content[start:end].strip(),\n+                        \"title\": self._extract_concept_title(\n+                            content[match.start() : match.end()]\n+                        ),\n+                        \"importance_score\": 0.85,\n+                        \"content_type\": \"concept\",\n+                    }\n+                )\n+\n         return concept_groups\n-    \n+\n     def _identify_formula_chains(self, content: str) -> List[Dict]:\n         \"\"\"Identify formula derivation chains\"\"\"\n         formula_chains = []\n-        \n+\n         # Formula patterns\n         formula_patterns = [\n-            r'\\$\\$.*?\\$\\$',  # Block-level mathematical formulas\n-            r'\\$[^$]+\\$',    # Inline mathematical formulas\n-            r'(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)',\n-            r'(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)'\n+            r\"\\$\\$.*?\\$\\$\",  # Block-level mathematical formulas\n+            r\"\\$[^$]+\\$\",  # Inline mathematical formulas\n+            r\"(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)\",\n+            r\"(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)\",\n         ]\n-        \n+\n         # Find dense formula regions\n         formula_positions = []\n         for pattern in formula_patterns:\n             matches = re.finditer(pattern, content, re.DOTALL)\n             for match in matches:\n                 formula_positions.append((match.start(), match.end()))\n-        \n+\n         # Merge nearby formulas into formula chains\n         formula_positions.sort()\n         if formula_positions:\n             current_chain_start = formula_positions[0][0]\n             current_chain_end = formula_positions[0][1]\n-            \n+\n             for start, end in formula_positions[1:]:\n-                if start - current_chain_end < 500:  # Merge formulas within 500 characters\n+                if (\n+                    start - current_chain_end < 500\n+                ):  # Merge formulas within 500 characters\n                     current_chain_end = end\n                 else:\n                     # Save current chain\n-                    formula_chains.append({\n-                        'start_pos': max(0, current_chain_start - 200),\n-                        'end_pos': min(len(content), current_chain_end + 200),\n-                        'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n-                        'title': 'Mathematical Formulation',\n-                        'importance_score': 0.9,\n-                        'content_type': 'formula'\n-                    })\n+                    formula_chains.append(\n+                        {\n+                            \"start_pos\": max(0, current_chain_start - 200),\n+                            \"end_pos\": min(len(content), current_chain_end + 200),\n+                            \"content\": content[\n+                                max(0, current_chain_start - 200) : min(\n+                                    len(content), current_chain_end + 200\n+                                )\n+                            ].strip(),\n+                            \"title\": \"Mathematical Formulation\",\n+                            \"importance_score\": 0.9,\n+                            \"content_type\": \"formula\",\n+                        }\n+                    )\n                     current_chain_start = start\n                     current_chain_end = end\n-            \n+\n             # Add the last chain\n-            formula_chains.append({\n-                'start_pos': max(0, current_chain_start - 200),\n-                'end_pos': min(len(content), current_chain_end + 200),\n-                'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n-                'title': 'Mathematical Formulation',\n-                'importance_score': 0.9,\n-                'content_type': 'formula'\n-            })\n-        \n+            formula_chains.append(\n+                {\n+                    \"start_pos\": max(0, current_chain_start - 200),\n+                    \"end_pos\": min(len(content), current_chain_end + 200),\n+                    \"content\": content[\n+                        max(0, current_chain_start - 200) : min(\n+                            len(content), current_chain_end + 200\n+                        )\n+                    ].strip(),\n+                    \"title\": \"Mathematical Formulation\",\n+                    \"importance_score\": 0.9,\n+                    \"content_type\": \"formula\",\n+                }\n+            )\n+\n         return formula_chains\n-    \n-    def _merge_related_content_blocks(self, algorithm_blocks: List[Dict], concept_groups: List[Dict], \n-                                    formula_chains: List[Dict], content: str) -> List[Dict]:\n+\n+    def _merge_related_content_blocks(\n+        self,\n+        algorithm_blocks: List[Dict],\n+        concept_groups: List[Dict],\n+        formula_chains: List[Dict],\n+        content: str,\n+    ) -> List[Dict]:\n         \"\"\"Merge related content blocks to ensure integrity\"\"\"\n         all_blocks = algorithm_blocks + concept_groups + formula_chains\n-        all_blocks.sort(key=lambda x: x['start_pos'])\n-        \n+        all_blocks.sort(key=lambda x: x[\"start_pos\"])\n+\n         merged_blocks = []\n         i = 0\n-        \n+\n         while i < len(all_blocks):\n             current_block = all_blocks[i]\n-            \n+\n             # Check if can merge with the next block\n             while i + 1 < len(all_blocks):\n                 next_block = all_blocks[i + 1]\n-                \n+\n                 # If blocks are close or content related, merge them\n-                if (next_block['start_pos'] - current_block['end_pos'] < 300 or\n-                    self._are_blocks_related(current_block, next_block)):\n-                    \n+                if next_block[\"start_pos\"] - current_block[\n+                    \"end_pos\"\n+                ] < 300 or self._are_blocks_related(current_block, next_block):\n                     # Merge blocks\n-                    merged_content = content[current_block['start_pos']:next_block['end_pos']]\n+                    merged_content = content[\n+                        current_block[\"start_pos\"] : next_block[\"end_pos\"]\n+                    ]\n                     current_block = {\n-                        'start_pos': current_block['start_pos'],\n-                        'end_pos': next_block['end_pos'],\n-                        'content': merged_content.strip(),\n-                        'title': f\"{current_block['title']} & {next_block['title']}\",\n-                        'importance_score': max(current_block['importance_score'], next_block['importance_score']),\n-                        'content_type': 'merged'\n+                        \"start_pos\": current_block[\"start_pos\"],\n+                        \"end_pos\": next_block[\"end_pos\"],\n+                        \"content\": merged_content.strip(),\n+                        \"title\": f\"{current_block['title']} & {next_block['title']}\",\n+                        \"importance_score\": max(\n+                            current_block[\"importance_score\"],\n+                            next_block[\"importance_score\"],\n+                        ),\n+                        \"content_type\": \"merged\",\n                     }\n                     i += 1\n                 else:\n                     break\n-            \n+\n             merged_blocks.append(current_block)\n             i += 1\n-        \n+\n         return merged_blocks\n-    \n+\n     def _are_blocks_related(self, block1: Dict, block2: Dict) -> bool:\n         \"\"\"Determine if two content blocks are related\"\"\"\n         # Check content type associations\n         related_types = [\n-            ('algorithm', 'formula'),\n-            ('concept', 'algorithm'),\n-            ('formula', 'concept')\n+            (\"algorithm\", \"formula\"),\n+            (\"concept\", \"algorithm\"),\n+            (\"formula\", \"concept\"),\n         ]\n-        \n+\n         for type1, type2 in related_types:\n-            if ((block1['content_type'] == type1 and block2['content_type'] == type2) or\n-                (block1['content_type'] == type2 and block2['content_type'] == type1)):\n+            if (\n+                block1[\"content_type\"] == type1 and block2[\"content_type\"] == type2\n+            ) or (block1[\"content_type\"] == type2 and block2[\"content_type\"] == type1):\n                 return True\n-        \n+\n         return False\n-    \n+\n     def _extract_algorithm_title(self, text: str) -> str:\n         \"\"\"Extract title from algorithm text\"\"\"\n-        lines = text.split('\\n')[:3]  # First 3 lines\n+        lines = text.split(\"\\n\")[:3]  # First 3 lines\n         for line in lines:\n             line = line.strip()\n             if line and len(line) < 100:  # Reasonable title length\n                 # Clean title\n-                title = re.sub(r'[^\\w\\s-]', '', line)\n+                title = re.sub(r\"[^\\w\\s-]\", \"\", line)\n                 if title:\n                     return title[:50]  # Limit title length\n         return \"Algorithm Block\"\n-    \n+\n     def _extract_concept_title(self, text: str) -> str:\n         \"\"\"Extract title from concept text\"\"\"\n-        lines = text.split('\\n')[:2]\n+        lines = text.split(\"\\n\")[:2]\n         for line in lines:\n             line = line.strip()\n             if line and len(line) < 80:\n-                title = re.sub(r'[^\\w\\s-]', '', line)\n+                title = re.sub(r\"[^\\w\\s-]\", \"\", line)\n                 if title:\n                     return title[:50]\n         return \"Concept Definition\"\n-    \n-    def _create_enhanced_segment(self, content: str, title: str, start_pos: int, end_pos: int,\n-                               importance_score: float, content_type: str) -> DocumentSegment:\n+\n+    def _create_enhanced_segment(\n+        self,\n+        content: str,\n+        title: str,\n+        start_pos: int,\n+        end_pos: int,\n+        importance_score: float,\n+        content_type: str,\n+    ) -> DocumentSegment:\n         \"\"\"Create enhanced document segment\"\"\"\n         # Generate unique ID\n-        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()).hexdigest()[:8]\n-        \n+        segment_id = hashlib.md5(\n+            f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()\n+        ).hexdigest()[:8]\n+\n         # Extract keywords\n         keywords = self._extract_enhanced_keywords(content, content_type)\n-        \n+\n         # Calculate enhanced relevance scores\n-        relevance_scores = self._calculate_enhanced_relevance_scores(content, content_type, importance_score)\n-        \n+        relevance_scores = self._calculate_enhanced_relevance_scores(\n+            content, content_type, importance_score\n+        )\n+\n         return DocumentSegment(\n             id=segment_id,\n             title=title,\n@@ -895,150 +1014,199 @@ class DocumentSegmenter:\n             char_end=end_pos,\n             char_count=len(content),\n             relevance_scores=relevance_scores,\n-            section_path=title\n+            section_path=title,\n         )\n-    \n+\n     def _extract_enhanced_keywords(self, content: str, content_type: str) -> List[str]:\n         \"\"\"Extract enhanced keywords based on content type\"\"\"\n-        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n-        \n+        words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", content.lower())\n+\n         # Adjust stopwords based on content type\n-        if content_type == 'algorithm':\n-            algorithm_stopwords = {'step', 'then', 'else', 'end', 'begin', 'start', 'stop'}\n+        if content_type == \"algorithm\":\n+            algorithm_stopwords = {\n+                \"step\",\n+                \"then\",\n+                \"else\",\n+                \"end\",\n+                \"begin\",\n+                \"start\",\n+                \"stop\",\n+            }\n             words = [w for w in words if w not in algorithm_stopwords]\n-        elif content_type == 'formula':\n-            formula_keywords = ['equation', 'formula', 'where', 'given', 'such', 'that']\n+        elif content_type == \"formula\":\n+            formula_keywords = [\"equation\", \"formula\", \"where\", \"given\", \"such\", \"that\"]\n             words.extend(formula_keywords)\n-        \n+\n         # General stopwords\n-        general_stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n-        \n+        general_stopwords = {\n+            \"the\",\n+            \"and\",\n+            \"for\",\n+            \"are\",\n+            \"but\",\n+            \"not\",\n+            \"you\",\n+            \"all\",\n+            \"can\",\n+            \"her\",\n+            \"was\",\n+            \"one\",\n+            \"our\",\n+            \"had\",\n+            \"but\",\n+            \"have\",\n+            \"this\",\n+            \"that\",\n+            \"with\",\n+            \"from\",\n+            \"they\",\n+            \"she\",\n+            \"been\",\n+            \"were\",\n+            \"said\",\n+            \"each\",\n+            \"which\",\n+            \"their\",\n+        }\n+\n         keywords = [w for w in set(words) if w not in general_stopwords and len(w) > 3]\n         return keywords[:25]  # Increase keyword count\n-    \n-    def _calculate_enhanced_relevance_scores(self, content: str, content_type: str, importance_score: float) -> Dict[str, float]:\n+\n+    def _calculate_enhanced_relevance_scores(\n+        self, content: str, content_type: str, importance_score: float\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate enhanced relevance scores\"\"\"\n         content_lower = content.lower()\n-        \n+\n         base_scores = {\n-            'concept_analysis': 0.5,\n-            'algorithm_extraction': 0.5,\n-            'code_planning': 0.5\n+            \"concept_analysis\": 0.5,\n+            \"algorithm_extraction\": 0.5,\n+            \"code_planning\": 0.5,\n         }\n-        \n+\n         # Adjust base scores based on content type and importance\n-        if content_type == 'algorithm':\n-            base_scores['algorithm_extraction'] = importance_score\n-            base_scores['code_planning'] = importance_score * 0.9\n-            base_scores['concept_analysis'] = importance_score * 0.7\n-        elif content_type == 'concept':\n-            base_scores['concept_analysis'] = importance_score\n-            base_scores['algorithm_extraction'] = importance_score * 0.8\n-            base_scores['code_planning'] = importance_score * 0.6\n-        elif content_type == 'formula':\n-            base_scores['algorithm_extraction'] = importance_score\n-            base_scores['concept_analysis'] = importance_score * 0.8\n-            base_scores['code_planning'] = importance_score * 0.9\n-        elif content_type == 'merged':\n+        if content_type == \"algorithm\":\n+            base_scores[\"algorithm_extraction\"] = importance_score\n+            base_scores[\"code_planning\"] = importance_score * 0.9\n+            base_scores[\"concept_analysis\"] = importance_score * 0.7\n+        elif content_type == \"concept\":\n+            base_scores[\"concept_analysis\"] = importance_score\n+            base_scores[\"algorithm_extraction\"] = importance_score * 0.8\n+            base_scores[\"code_planning\"] = importance_score * 0.6\n+        elif content_type == \"formula\":\n+            base_scores[\"algorithm_extraction\"] = importance_score\n+            base_scores[\"concept_analysis\"] = importance_score * 0.8\n+            base_scores[\"code_planning\"] = importance_score * 0.9\n+        elif content_type == \"merged\":\n             # Merged content is usually important\n             base_scores = {k: importance_score * 0.95 for k in base_scores}\n-        \n+\n         # Additional bonus based on content density\n-        algorithm_indicators = ['algorithm', 'method', 'procedure', 'step', 'process']\n-        concept_indicators = ['definition', 'concept', 'framework', 'approach']\n-        implementation_indicators = ['implementation', 'code', 'function', 'design']\n-        \n+        algorithm_indicators = [\"algorithm\", \"method\", \"procedure\", \"step\", \"process\"]\n+        concept_indicators = [\"definition\", \"concept\", \"framework\", \"approach\"]\n+        implementation_indicators = [\"implementation\", \"code\", \"function\", \"design\"]\n+\n         for query_type, indicators in [\n-            ('algorithm_extraction', algorithm_indicators),\n-            ('concept_analysis', concept_indicators),\n-            ('code_planning', implementation_indicators)\n+            (\"algorithm_extraction\", algorithm_indicators),\n+            (\"concept_analysis\", concept_indicators),\n+            (\"code_planning\", implementation_indicators),\n         ]:\n-            density_bonus = sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            density_bonus = (\n+                sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            )\n             base_scores[query_type] = min(1.0, base_scores[query_type] + density_bonus)\n-        \n+\n         return base_scores\n-    \n+\n     # Placeholder methods - can be further implemented later\n     def _identify_research_paper_sections(self, content: str) -> List[Dict]:\n         \"\"\"Identify research paper sections - simplified implementation\"\"\"\n         # Temporarily use improved semantic detection\n         return self._detect_academic_sections(content)\n-    \n+\n     def _enhance_section_with_context(self, section: Dict, content: str) -> Dict:\n         \"\"\"Add context to sections - simplified implementation\"\"\"\n         return section\n-    \n+\n     def _identify_concept_implementation_pairs(self, content: str) -> List[Dict]:\n         \"\"\"Identify concept-implementation pairs - simplified implementation\"\"\"\n         return []\n-    \n+\n     def _merge_concept_with_implementation(self, pair: Dict, content: str) -> Dict:\n         \"\"\"Merge concepts with implementation - simplified implementation\"\"\"\n         return pair\n-    \n+\n     def _detect_semantic_boundaries(self, content: str) -> List[Dict]:\n         \"\"\"Detect semantic boundaries - based on paragraphs and logical separators\"\"\"\n         boundaries = []\n-        \n+\n         # Split paragraphs by double line breaks\n-        paragraphs = content.split('\\n\\n')\n+        paragraphs = content.split(\"\\n\\n\")\n         current_pos = 0\n-        \n+\n         for i, para in enumerate(paragraphs):\n             if len(para.strip()) > 100:  # Valid paragraph\n                 # Analyze paragraph type\n                 content_type = self._classify_paragraph_type(para)\n-                importance_score = self._calculate_paragraph_importance(para, content_type)\n-                \n-                boundaries.append({\n-                    'position': current_pos + len(para),\n-                    'suggested_title': self._extract_paragraph_title(para, i+1),\n-                    'importance_score': importance_score,\n-                    'content_type': content_type\n-                })\n-            \n+                importance_score = self._calculate_paragraph_importance(\n+                    para, content_type\n+                )\n+\n+                boundaries.append(\n+                    {\n+                        \"position\": current_pos + len(para),\n+                        \"suggested_title\": self._extract_paragraph_title(para, i + 1),\n+                        \"importance_score\": importance_score,\n+                        \"content_type\": content_type,\n+                    }\n+                )\n+\n             current_pos += len(para) + 2  # +2 for \\n\\n\n-        \n+\n         return boundaries\n-    \n+\n     def _classify_paragraph_type(self, paragraph: str) -> str:\n         \"\"\"Classify paragraph type\"\"\"\n         para_lower = paragraph.lower()\n-        \n-        if 'algorithm' in para_lower or 'procedure' in para_lower:\n-            return 'algorithm'\n-        elif 'formula' in para_lower or '$$' in paragraph:\n-            return 'formula'\n-        elif any(word in para_lower for word in ['introduction', 'overview', 'abstract']):\n-            return 'introduction'\n-        elif any(word in para_lower for word in ['conclusion', 'summary', 'result']):\n-            return 'conclusion'\n+\n+        if \"algorithm\" in para_lower or \"procedure\" in para_lower:\n+            return \"algorithm\"\n+        elif \"formula\" in para_lower or \"$$\" in paragraph:\n+            return \"formula\"\n+        elif any(\n+            word in para_lower for word in [\"introduction\", \"overview\", \"abstract\"]\n+        ):\n+            return \"introduction\"\n+        elif any(word in para_lower for word in [\"conclusion\", \"summary\", \"result\"]):\n+            return \"conclusion\"\n         else:\n-            return 'general'\n-    \n-    def _calculate_paragraph_importance(self, paragraph: str, content_type: str) -> float:\n+            return \"general\"\n+\n+    def _calculate_paragraph_importance(\n+        self, paragraph: str, content_type: str\n+    ) -> float:\n         \"\"\"Calculate paragraph importance\"\"\"\n-        if content_type == 'algorithm':\n+        if content_type == \"algorithm\":\n             return 0.95\n-        elif content_type == 'formula':\n+        elif content_type == \"formula\":\n             return 0.9\n-        elif content_type == 'introduction':\n+        elif content_type == \"introduction\":\n             return 0.85\n-        elif content_type == 'conclusion':\n+        elif content_type == \"conclusion\":\n             return 0.8\n         else:\n             return 0.7\n-    \n+\n     def _extract_paragraph_title(self, paragraph: str, index: int) -> str:\n         \"\"\"Extract paragraph title\"\"\"\n-        lines = paragraph.split('\\n')\n+        lines = paragraph.split(\"\\n\")\n         for line in lines[:2]:\n-            if line.startswith('#'):\n-                return line.strip('# ')\n+            if line.startswith(\"#\"):\n+                return line.strip(\"# \")\n             elif len(line) < 80 and line.strip():\n                 return line.strip()\n         return f\"Section {index}\"\n-    \n+\n     def _calculate_optimal_chunk_size(self, content: str) -> int:\n         \"\"\"Calculate optimal chunk size\"\"\"\n         # Dynamically adjust based on content complexity\n@@ -1049,65 +1217,73 @@ class DocumentSegmenter:\n             return 3000\n         else:\n             return 2000\n-    \n+\n     def _create_content_aware_chunks(self, content: str, chunk_size: int) -> List[Dict]:\n         \"\"\"Create content-aware chunks - simplified implementation\"\"\"\n         chunks = []\n-        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n-        \n+        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n+\n         current_chunk = []\n         current_size = 0\n         start_pos = 0\n-        \n+\n         for para in paragraphs:\n             para_size = len(para)\n-            \n+\n             if current_size + para_size > chunk_size and current_chunk:\n-                chunk_content = '\\n\\n'.join(current_chunk)\n-                chunks.append({\n-                    'content': chunk_content,\n-                    'title': f\"Section {len(chunks) + 1}\",\n-                    'start_pos': start_pos,\n-                    'end_pos': start_pos + len(chunk_content),\n-                    'importance_score': 0.7,\n-                    'content_type': 'general'\n-                })\n-                \n+                chunk_content = \"\\n\\n\".join(current_chunk)\n+                chunks.append(\n+                    {\n+                        \"content\": chunk_content,\n+                        \"title\": f\"Section {len(chunks) + 1}\",\n+                        \"start_pos\": start_pos,\n+                        \"end_pos\": start_pos + len(chunk_content),\n+                        \"importance_score\": 0.7,\n+                        \"content_type\": \"general\",\n+                    }\n+                )\n+\n                 current_chunk = [para]\n                 current_size = para_size\n                 start_pos += len(chunk_content) + 2\n             else:\n                 current_chunk.append(para)\n                 current_size += para_size\n-        \n+\n         # Add the last chunk\n         if current_chunk:\n-            chunk_content = '\\n\\n'.join(current_chunk)\n-            chunks.append({\n-                'content': chunk_content,\n-                'title': f\"Section {len(chunks) + 1}\",\n-                'start_pos': start_pos,\n-                'end_pos': start_pos + len(chunk_content),\n-                'importance_score': 0.7,\n-                'content_type': 'general'\n-            })\n-        \n+            chunk_content = \"\\n\\n\".join(current_chunk)\n+            chunks.append(\n+                {\n+                    \"content\": chunk_content,\n+                    \"title\": f\"Section {len(chunks) + 1}\",\n+                    \"start_pos\": start_pos,\n+                    \"end_pos\": start_pos + len(chunk_content),\n+                    \"importance_score\": 0.7,\n+                    \"content_type\": \"general\",\n+                }\n+            )\n+\n         return chunks\n-    \n-    def _create_segment(self, content: str, title: str, start_pos: int, end_pos: int) -> DocumentSegment:\n+\n+    def _create_segment(\n+        self, content: str, title: str, start_pos: int, end_pos: int\n+    ) -> DocumentSegment:\n         \"\"\"Create a DocumentSegment with metadata\"\"\"\n         # Generate unique ID\n-        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[:8]\n-        \n+        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[\n+            :8\n+        ]\n+\n         # Extract keywords from content\n         keywords = self._extract_keywords(content)\n-        \n+\n         # Determine content type\n         content_type = self._classify_content_type(title, content)\n-        \n+\n         # Calculate relevance scores for different query types\n         relevance_scores = self._calculate_relevance_scores(content, content_type)\n-        \n+\n         return DocumentSegment(\n             id=segment_id,\n             title=title,\n@@ -1118,142 +1294,232 @@ class DocumentSegmenter:\n             char_end=end_pos,\n             char_count=len(content),\n             relevance_scores=relevance_scores,\n-            section_path=title  # Simplified for now\n+            section_path=title,  # Simplified for now\n         )\n-    \n+\n     def _extract_keywords(self, content: str) -> List[str]:\n         \"\"\"Extract relevant keywords from content\"\"\"\n         # Simple keyword extraction - could be enhanced with NLP\n-        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n-        \n+        words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", content.lower())\n+\n         # Remove common words\n-        stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n-        \n+        stopwords = {\n+            \"the\",\n+            \"and\",\n+            \"for\",\n+            \"are\",\n+            \"but\",\n+            \"not\",\n+            \"you\",\n+            \"all\",\n+            \"can\",\n+            \"her\",\n+            \"was\",\n+            \"one\",\n+            \"our\",\n+            \"had\",\n+            \"but\",\n+            \"have\",\n+            \"this\",\n+            \"that\",\n+            \"with\",\n+            \"from\",\n+            \"they\",\n+            \"she\",\n+            \"been\",\n+            \"were\",\n+            \"said\",\n+            \"each\",\n+            \"which\",\n+            \"their\",\n+        }\n+\n         keywords = [w for w in set(words) if w not in stopwords and len(w) > 3]\n         return keywords[:20]  # Top 20 keywords\n-    \n+\n     def _classify_content_type(self, title: str, content: str) -> str:\n         \"\"\"Classify the type of content based on title and content\"\"\"\n         title_lower = title.lower()\n         content_lower = content.lower()\n-        \n-        if any(word in title_lower for word in ['introduction', 'abstract', 'overview']):\n-            return 'introduction'\n-        elif any(word in title_lower for word in ['method', 'approach', 'algorithm']):\n-            return 'methodology'\n-        elif any(word in title_lower for word in ['experiment', 'evaluation', 'result']):\n-            return 'experiment'\n-        elif any(word in title_lower for word in ['conclusion', 'discussion', 'summary']):\n-            return 'conclusion'\n-        elif any(word in title_lower for word in ['reference', 'bibliography']):\n-            return 'references'\n-        elif 'algorithm' in content_lower or 'procedure' in content_lower:\n-            return 'algorithm'\n+\n+        if any(\n+            word in title_lower for word in [\"introduction\", \"abstract\", \"overview\"]\n+        ):\n+            return \"introduction\"\n+        elif any(word in title_lower for word in [\"method\", \"approach\", \"algorithm\"]):\n+            return \"methodology\"\n+        elif any(\n+            word in title_lower for word in [\"experiment\", \"evaluation\", \"result\"]\n+        ):\n+            return \"experiment\"\n+        elif any(\n+            word in title_lower for word in [\"conclusion\", \"discussion\", \"summary\"]\n+        ):\n+            return \"conclusion\"\n+        elif any(word in title_lower for word in [\"reference\", \"bibliography\"]):\n+            return \"references\"\n+        elif \"algorithm\" in content_lower or \"procedure\" in content_lower:\n+            return \"algorithm\"\n         else:\n-            return 'general'\n-    \n-    def _calculate_relevance_scores(self, content: str, content_type: str) -> Dict[str, float]:\n+            return \"general\"\n+\n+    def _calculate_relevance_scores(\n+        self, content: str, content_type: str\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate relevance scores for different query types\"\"\"\n         content_lower = content.lower()\n-        \n+\n         scores = {\n-            'concept_analysis': 0.5,\n-            'algorithm_extraction': 0.5,\n-            'code_planning': 0.5\n+            \"concept_analysis\": 0.5,\n+            \"algorithm_extraction\": 0.5,\n+            \"code_planning\": 0.5,\n         }\n-        \n+\n         # Concept analysis relevance\n-        concept_indicators = ['introduction', 'overview', 'architecture', 'system', 'framework', 'concept', 'approach']\n-        concept_score = sum(1 for indicator in concept_indicators if indicator in content_lower) / len(concept_indicators)\n-        scores['concept_analysis'] = min(1.0, concept_score + (0.8 if content_type == 'introduction' else 0))\n-        \n+        concept_indicators = [\n+            \"introduction\",\n+            \"overview\",\n+            \"architecture\",\n+            \"system\",\n+            \"framework\",\n+            \"concept\",\n+            \"approach\",\n+        ]\n+        concept_score = sum(\n+            1 for indicator in concept_indicators if indicator in content_lower\n+        ) / len(concept_indicators)\n+        scores[\"concept_analysis\"] = min(\n+            1.0, concept_score + (0.8 if content_type == \"introduction\" else 0)\n+        )\n+\n         # Algorithm extraction relevance\n-        algorithm_indicators = ['algorithm', 'method', 'procedure', 'formula', 'equation', 'step', 'process']\n-        algorithm_score = sum(1 for indicator in algorithm_indicators if indicator in content_lower) / len(algorithm_indicators)\n-        scores['algorithm_extraction'] = min(1.0, algorithm_score + (0.9 if content_type == 'methodology' else 0))\n-        \n+        algorithm_indicators = [\n+            \"algorithm\",\n+            \"method\",\n+            \"procedure\",\n+            \"formula\",\n+            \"equation\",\n+            \"step\",\n+            \"process\",\n+        ]\n+        algorithm_score = sum(\n+            1 for indicator in algorithm_indicators if indicator in content_lower\n+        ) / len(algorithm_indicators)\n+        scores[\"algorithm_extraction\"] = min(\n+            1.0, algorithm_score + (0.9 if content_type == \"methodology\" else 0)\n+        )\n+\n         # Code planning relevance\n-        code_indicators = ['implementation', 'code', 'function', 'class', 'module', 'structure', 'design']\n-        code_score = sum(1 for indicator in code_indicators if indicator in content_lower) / len(code_indicators)\n-        scores['code_planning'] = min(1.0, code_score + (0.7 if content_type in ['methodology', 'algorithm'] else 0))\n-        \n+        code_indicators = [\n+            \"implementation\",\n+            \"code\",\n+            \"function\",\n+            \"class\",\n+            \"module\",\n+            \"structure\",\n+            \"design\",\n+        ]\n+        code_score = sum(\n+            1 for indicator in code_indicators if indicator in content_lower\n+        ) / len(code_indicators)\n+        scores[\"code_planning\"] = min(\n+            1.0,\n+            code_score + (0.7 if content_type in [\"methodology\", \"algorithm\"] else 0),\n+        )\n+\n         return scores\n \n+\n # Global variables\n DOCUMENT_INDEXES: Dict[str, DocumentIndex] = {}\n segmenter = DocumentSegmenter()\n \n+\n def get_segments_dir(paper_dir: str) -> str:\n     \"\"\"Get the segments directory path\"\"\"\n     return os.path.join(paper_dir, \"document_segments\")\n \n+\n def ensure_segments_dir_exists(segments_dir: str):\n     \"\"\"Ensure segments directory exists\"\"\"\n     os.makedirs(segments_dir, exist_ok=True)\n \n+\n @mcp.tool()\n-async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = False) -> str:\n+async def analyze_and_segment_document(\n+    paper_dir: str, force_refresh: bool = False\n+) -> str:\n     \"\"\"\n     Analyze document structure and create intelligent segments\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         force_refresh: Whether to force re-analysis even if segments exist\n-        \n+\n     Returns:\n         JSON string with segmentation results\n     \"\"\"\n     try:\n         # Find markdown file in paper directory\n-        md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+        md_files = [f for f in os.listdir(paper_dir) if f.endswith(\".md\")]\n         if not md_files:\n-            return json.dumps({\n-                \"status\": \"error\",\n-                \"message\": f\"No markdown file found in {paper_dir}\"\n-            }, ensure_ascii=False, indent=2)\n-        \n+            return json.dumps(\n+                {\n+                    \"status\": \"error\",\n+                    \"message\": f\"No markdown file found in {paper_dir}\",\n+                },\n+                ensure_ascii=False,\n+                indent=2,\n+            )\n+\n         md_file_path = os.path.join(paper_dir, md_files[0])\n         segments_dir = get_segments_dir(paper_dir)\n         index_file_path = os.path.join(segments_dir, \"document_index.json\")\n-        \n+\n         # Check if analysis already exists and is recent\n         if not force_refresh and os.path.exists(index_file_path):\n             try:\n-                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n                     existing_index = json.load(f)\n-                    \n+\n                     # Compatibility handling: ensure segments data structure is correct\n-                    if 'segments' in existing_index:\n+                    if \"segments\" in existing_index:\n                         segments_data = []\n-                        for seg_data in existing_index['segments']:\n+                        for seg_data in existing_index[\"segments\"]:\n                             # Ensure all required fields exist\n                             segment_dict = dict(seg_data)\n-                            \n-                            if 'content_type' not in segment_dict:\n-                                segment_dict['content_type'] = 'general'\n-                            if 'keywords' not in segment_dict:\n-                                segment_dict['keywords'] = []\n-                            if 'relevance_scores' not in segment_dict:\n-                                segment_dict['relevance_scores'] = {\n-                                    'concept_analysis': 0.5,\n-                                    'algorithm_extraction': 0.5,\n-                                    'code_planning': 0.5\n+\n+                            if \"content_type\" not in segment_dict:\n+                                segment_dict[\"content_type\"] = \"general\"\n+                            if \"keywords\" not in segment_dict:\n+                                segment_dict[\"keywords\"] = []\n+                            if \"relevance_scores\" not in segment_dict:\n+                                segment_dict[\"relevance_scores\"] = {\n+                                    \"concept_analysis\": 0.5,\n+                                    \"algorithm_extraction\": 0.5,\n+                                    \"code_planning\": 0.5,\n                                 }\n-                            if 'section_path' not in segment_dict:\n-                                segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n-                            \n+                            if \"section_path\" not in segment_dict:\n+                                segment_dict[\"section_path\"] = segment_dict.get(\n+                                    \"title\", \"Unknown\"\n+                                )\n+\n                             segments_data.append(DocumentSegment(**segment_dict))\n-                        \n-                        existing_index['segments'] = segments_data\n-                    \n+\n+                        existing_index[\"segments\"] = segments_data\n+\n                     DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**existing_index)\n-                return json.dumps({\n-                    \"status\": \"success\",\n-                    \"message\": \"Using existing document analysis\",\n-                    \"segments_dir\": segments_dir,\n-                    \"total_segments\": existing_index[\"total_segments\"]\n-                }, ensure_ascii=False, indent=2)\n-            \n+                return json.dumps(\n+                    {\n+                        \"status\": \"success\",\n+                        \"message\": \"Using existing document analysis\",\n+                        \"segments_dir\": segments_dir,\n+                        \"total_segments\": existing_index[\"total_segments\"],\n+                    },\n+                    ensure_ascii=False,\n+                    indent=2,\n+                )\n+\n             except Exception as e:\n                 logger.error(f\"Failed to load existing index: {e}\")\n                 logger.info(\"Will perform fresh analysis instead\")\n@@ -1262,19 +1528,19 @@ async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = Fal\n                     os.remove(index_file_path)\n                 except:\n                     pass\n-        \n+\n         # Read document content\n-        with open(md_file_path, 'r', encoding='utf-8') as f:\n+        with open(md_file_path, \"r\", encoding=\"utf-8\") as f:\n             content = f.read()\n-        \n+\n         # Analyze document\n         analyzer = DocumentAnalyzer()\n         doc_type, confidence = analyzer.analyze_document_type(content)\n         strategy = analyzer.detect_segmentation_strategy(content, doc_type)\n-        \n+\n         # Create segments\n         segments = segmenter.segment_document(content, strategy)\n-        \n+\n         # Create document index\n         document_index = DocumentIndex(\n             document_path=md_file_path,\n@@ -1283,46 +1549,56 @@ async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = Fal\n             total_segments=len(segments),\n             total_chars=len(content),\n             segments=segments,\n-            created_at=datetime.now().isoformat()\n+            created_at=datetime.now().isoformat(),\n         )\n-        \n+\n         # Save segments\n         ensure_segments_dir_exists(segments_dir)\n-        \n+\n         # Save document index\n-        with open(index_file_path, 'w', encoding='utf-8') as f:\n-            json.dump(asdict(document_index), f, ensure_ascii=False, indent=2, default=str)\n-        \n+        with open(index_file_path, \"w\", encoding=\"utf-8\") as f:\n+            json.dump(\n+                asdict(document_index), f, ensure_ascii=False, indent=2, default=str\n+            )\n+\n         # Save individual segment files for fallback\n         for segment in segments:\n             segment_file_path = os.path.join(segments_dir, f\"segment_{segment.id}.md\")\n-            with open(segment_file_path, 'w', encoding='utf-8') as f:\n+            with open(segment_file_path, \"w\", encoding=\"utf-8\") as f:\n                 f.write(f\"# {segment.title}\\n\\n\")\n                 f.write(f\"**Content Type:** {segment.content_type}\\n\")\n                 f.write(f\"**Keywords:** {', '.join(segment.keywords[:10])}\\n\\n\")\n                 f.write(segment.content)\n-        \n+\n         # Store in memory\n         DOCUMENT_INDEXES[paper_dir] = document_index\n-        \n-        logger.info(f\"Document segmentation completed: {len(segments)} segments created\")\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"message\": f\"Document analysis completed with {strategy} strategy\",\n-            \"document_type\": doc_type,\n-            \"segmentation_strategy\": strategy,\n-            \"segments_dir\": segments_dir,\n-            \"total_segments\": len(segments),\n-            \"total_chars\": len(content)\n-        }, ensure_ascii=False, indent=2)\n-        \n+\n+        logger.info(\n+            f\"Document segmentation completed: {len(segments)} segments created\"\n+        )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"message\": f\"Document analysis completed with {strategy} strategy\",\n+                \"document_type\": doc_type,\n+                \"segmentation_strategy\": strategy,\n+                \"segments_dir\": segments_dir,\n+                \"total_segments\": len(segments),\n+                \"total_chars\": len(content),\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in analyze_and_segment_document: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to analyze document: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\"status\": \"error\", \"message\": f\"Failed to analyze document: {str(e)}\"},\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n @mcp.tool()\n async def read_document_segments(\n@@ -1330,18 +1606,18 @@ async def read_document_segments(\n     query_type: str,\n     keywords: List[str] = None,\n     max_segments: int = 3,\n-    max_total_chars: int = None\n+    max_total_chars: int = None,\n ) -> str:\n     \"\"\"\n     Intelligently retrieve relevant document segments based on query type\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         query_type: Type of query - \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n         keywords: Optional list of keywords to search for\n         max_segments: Maximum number of segments to return\n         max_total_chars: Maximum total characters to return\n-        \n+\n     Returns:\n         JSON string with selected segments\n     \"\"\"\n@@ -1350,100 +1626,113 @@ async def read_document_segments(\n         if paper_dir not in DOCUMENT_INDEXES:\n             segments_dir = get_segments_dir(paper_dir)\n             index_file_path = os.path.join(segments_dir, \"document_index.json\")\n-            \n+\n             if os.path.exists(index_file_path):\n-                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n                     index_data = json.load(f)\n                     # Convert dict back to DocumentIndex with backward compatibility\n                     segments_data = []\n-                    for seg_data in index_data.get('segments', []):\n+                    for seg_data in index_data.get(\"segments\", []):\n                         # Ensure all required fields exist, provide default values\n                         segment_dict = dict(seg_data)\n-                        \n+\n                         # Compatibility handling: add missing fields\n-                        if 'content_type' not in segment_dict:\n-                            segment_dict['content_type'] = 'general'\n-                        if 'keywords' not in segment_dict:\n-                            segment_dict['keywords'] = []\n-                        if 'relevance_scores' not in segment_dict:\n-                            segment_dict['relevance_scores'] = {\n-                                'concept_analysis': 0.5,\n-                                'algorithm_extraction': 0.5,\n-                                'code_planning': 0.5\n+                        if \"content_type\" not in segment_dict:\n+                            segment_dict[\"content_type\"] = \"general\"\n+                        if \"keywords\" not in segment_dict:\n+                            segment_dict[\"keywords\"] = []\n+                        if \"relevance_scores\" not in segment_dict:\n+                            segment_dict[\"relevance_scores\"] = {\n+                                \"concept_analysis\": 0.5,\n+                                \"algorithm_extraction\": 0.5,\n+                                \"code_planning\": 0.5,\n                             }\n-                        if 'section_path' not in segment_dict:\n-                            segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n-                        \n+                        if \"section_path\" not in segment_dict:\n+                            segment_dict[\"section_path\"] = segment_dict.get(\n+                                \"title\", \"Unknown\"\n+                            )\n+\n                         segment = DocumentSegment(**segment_dict)\n                         segments_data.append(segment)\n-                    \n-                    index_data['segments'] = segments_data\n+\n+                    index_data[\"segments\"] = segments_data\n                     DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**index_data)\n             else:\n                 # Auto-analyze if not found\n                 await analyze_and_segment_document(paper_dir)\n-        \n+\n         document_index = DOCUMENT_INDEXES[paper_dir]\n-        \n+\n         # Dynamically calculate character limit\n         if max_total_chars is None:\n             max_total_chars = _calculate_adaptive_char_limit(document_index, query_type)\n-        \n+\n         # Score and rank segments with enhanced algorithm\n         scored_segments = []\n         for segment in document_index.segments:\n             # Base relevance score (already enhanced in new system)\n             relevance_score = segment.relevance_scores.get(query_type, 0.5)\n-            \n+\n             # Enhanced keyword matching with position weighting\n             if keywords:\n                 keyword_score = _calculate_enhanced_keyword_score(segment, keywords)\n                 relevance_score += keyword_score\n-            \n+\n             # Content completeness bonus\n             completeness_bonus = _calculate_completeness_bonus(segment, document_index)\n             relevance_score += completeness_bonus\n-            \n+\n             scored_segments.append((segment, relevance_score))\n-        \n+\n         # Sort by enhanced relevance score\n         scored_segments.sort(key=lambda x: x[1], reverse=True)\n-        \n+\n         # Intelligent segment selection with integrity preservation\n         selected_segments = _select_segments_with_integrity(\n             scored_segments, max_segments, max_total_chars, query_type\n         )\n-        \n+\n         total_chars = sum(seg[\"char_count\"] for seg in selected_segments)\n-        \n-        logger.info(f\"Selected {len(selected_segments)} segments for {query_type} query\")\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"query_type\": query_type,\n-            \"keywords\": keywords or [],\n-            \"total_segments_available\": len(document_index.segments),\n-            \"segments_selected\": len(selected_segments),\n-            \"total_chars\": total_chars,\n-            \"max_chars_used\": max_total_chars,\n-            \"segments\": selected_segments\n-        }, ensure_ascii=False, indent=2)\n-        \n+\n+        logger.info(\n+            f\"Selected {len(selected_segments)} segments for {query_type} query\"\n+        )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"query_type\": query_type,\n+                \"keywords\": keywords or [],\n+                \"total_segments_available\": len(document_index.segments),\n+                \"segments_selected\": len(selected_segments),\n+                \"total_chars\": total_chars,\n+                \"max_chars_used\": max_total_chars,\n+                \"segments\": selected_segments,\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in read_document_segments: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to read document segments: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\n+                \"status\": \"error\",\n+                \"message\": f\"Failed to read document segments: {str(e)}\",\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n @mcp.tool()\n async def get_document_overview(paper_dir: str) -> str:\n     \"\"\"\n     Get overview of document structure and available segments\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n-        \n+\n     Returns:\n         JSON string with document overview\n     \"\"\"\n@@ -1451,45 +1740,59 @@ async def get_document_overview(paper_dir: str) -> str:\n         # Ensure document is analyzed\n         if paper_dir not in DOCUMENT_INDEXES:\n             await analyze_and_segment_document(paper_dir)\n-        \n+\n         document_index = DOCUMENT_INDEXES[paper_dir]\n-        \n+\n         # Create overview\n         segment_summaries = []\n         for segment in document_index.segments:\n-            segment_summaries.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content_type\": segment.content_type,\n-                \"char_count\": segment.char_count,\n-                \"keywords\": segment.keywords[:5],  # Top 5 keywords\n-                \"relevance_scores\": segment.relevance_scores\n-            })\n-        \n-        return json.dumps({\n-            \"status\": \"success\",\n-            \"document_path\": document_index.document_path,\n-            \"document_type\": document_index.document_type,\n-            \"segmentation_strategy\": document_index.segmentation_strategy,\n-            \"total_segments\": document_index.total_segments,\n-            \"total_chars\": document_index.total_chars,\n-            \"created_at\": document_index.created_at,\n-            \"segments_overview\": segment_summaries\n-        }, ensure_ascii=False, indent=2)\n-        \n+            segment_summaries.append(\n+                {\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content_type\": segment.content_type,\n+                    \"char_count\": segment.char_count,\n+                    \"keywords\": segment.keywords[:5],  # Top 5 keywords\n+                    \"relevance_scores\": segment.relevance_scores,\n+                }\n+            )\n+\n+        return json.dumps(\n+            {\n+                \"status\": \"success\",\n+                \"document_path\": document_index.document_path,\n+                \"document_type\": document_index.document_type,\n+                \"segmentation_strategy\": document_index.segmentation_strategy,\n+                \"total_segments\": document_index.total_segments,\n+                \"total_chars\": document_index.total_chars,\n+                \"created_at\": document_index.created_at,\n+                \"segments_overview\": segment_summaries,\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n     except Exception as e:\n         logger.error(f\"Error in get_document_overview: {e}\")\n-        return json.dumps({\n-            \"status\": \"error\",\n-            \"message\": f\"Failed to get document overview: {str(e)}\"\n-        }, ensure_ascii=False, indent=2)\n+        return json.dumps(\n+            {\n+                \"status\": \"error\",\n+                \"message\": f\"Failed to get document overview: {str(e)}\",\n+            },\n+            ensure_ascii=False,\n+            indent=2,\n+        )\n+\n \n # =============== Enhanced retrieval system helper methods ===============\n \n-def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: str) -> int:\n+\n+def _calculate_adaptive_char_limit(\n+    document_index: DocumentIndex, query_type: str\n+) -> int:\n     \"\"\"Dynamically calculate character limit based on document complexity and query type\"\"\"\n     base_limit = 6000\n-    \n+\n     # Adjust based on document type\n     if document_index.document_type == \"research_paper\":\n         base_limit = 10000\n@@ -1497,116 +1800,140 @@ def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: st\n         base_limit = 12000\n     elif document_index.segmentation_strategy == \"algorithm_preserve_integrity\":\n         base_limit = 15000\n-    \n+\n     # Adjust based on query type\n     query_multipliers = {\n         \"algorithm_extraction\": 1.5,  # Algorithms need more context\n         \"concept_analysis\": 1.2,\n-        \"code_planning\": 1.3\n+        \"code_planning\": 1.3,\n     }\n-    \n+\n     multiplier = query_multipliers.get(query_type, 1.0)\n     return int(base_limit * multiplier)\n \n-def _calculate_enhanced_keyword_score(segment: DocumentSegment, keywords: List[str]) -> float:\n+\n+def _calculate_enhanced_keyword_score(\n+    segment: DocumentSegment, keywords: List[str]\n+) -> float:\n     \"\"\"Calculate enhanced keyword matching score\"\"\"\n     score = 0.0\n     content_lower = segment.content.lower()\n     title_lower = segment.title.lower()\n-    \n+\n     for keyword in keywords:\n         keyword_lower = keyword.lower()\n-        \n+\n         # Title matching has higher weight\n         if keyword_lower in title_lower:\n             score += 0.3\n-        \n+\n         # Content matching\n         content_matches = content_lower.count(keyword_lower)\n         if content_matches > 0:\n             # Consider term frequency and position\n             frequency_score = min(0.2, content_matches * 0.05)\n-            \n+\n             # Check if in important position (first 25% of content)\n-            early_content = content_lower[:len(content_lower)//4]\n+            early_content = content_lower[: len(content_lower) // 4]\n             if keyword_lower in early_content:\n                 frequency_score += 0.1\n-            \n+\n             score += frequency_score\n-    \n+\n     return min(0.6, score)  # Limit maximum bonus\n \n-def _calculate_completeness_bonus(segment: DocumentSegment, document_index: DocumentIndex) -> float:\n+\n+def _calculate_completeness_bonus(\n+    segment: DocumentSegment, document_index: DocumentIndex\n+) -> float:\n     \"\"\"Calculate content completeness bonus\"\"\"\n     bonus = 0.0\n-    \n+\n     # Completeness bonus for algorithm and formula content\n-    if segment.content_type in ['algorithm', 'formula', 'merged']:\n+    if segment.content_type in [\"algorithm\", \"formula\", \"merged\"]:\n         bonus += 0.2\n-    \n+\n     # Long paragraphs usually contain more complete information\n     if segment.char_count > 2000:\n         bonus += 0.1\n     elif segment.char_count > 4000:\n         bonus += 0.15\n-    \n+\n     # High importance paragraph bonus\n-    if segment.relevance_scores.get('algorithm_extraction', 0) > 0.8:\n+    if segment.relevance_scores.get(\"algorithm_extraction\", 0) > 0.8:\n         bonus += 0.1\n-    \n+\n     return min(0.3, bonus)\n \n-def _select_segments_with_integrity(scored_segments: List[Tuple], max_segments: int, \n-                                  max_total_chars: int, query_type: str) -> List[Dict]:\n+\n+def _select_segments_with_integrity(\n+    scored_segments: List[Tuple],\n+    max_segments: int,\n+    max_total_chars: int,\n+    query_type: str,\n+) -> List[Dict]:\n     \"\"\"Intelligently select segments while maintaining content integrity\"\"\"\n     selected_segments = []\n     total_chars = 0\n-    \n+\n     # First select the highest scoring segments\n     for segment, score in scored_segments:\n         if len(selected_segments) >= max_segments:\n             break\n-            \n+\n         if total_chars + segment.char_count <= max_total_chars:\n-            selected_segments.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content\": segment.content,\n-                \"content_type\": segment.content_type,\n-                \"relevance_score\": score,\n-                \"char_count\": segment.char_count\n-            })\n+            selected_segments.append(\n+                {\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content\": segment.content,\n+                    \"content_type\": segment.content_type,\n+                    \"relevance_score\": score,\n+                    \"char_count\": segment.char_count,\n+                }\n+            )\n             total_chars += segment.char_count\n         elif len(selected_segments) == 0:\n             # If the first segment exceeds the limit, truncate but preserve it\n-            truncated_content = segment.content[:max_total_chars - 200] + \"\\n\\n[Content truncated for length...]\"\n-            selected_segments.append({\n-                \"id\": segment.id,\n-                \"title\": segment.title,\n-                \"content\": truncated_content,\n-                \"content_type\": segment.content_type,\n-                \"relevance_score\": score,\n-                \"char_count\": len(truncated_content)\n-            })\n-            break\n-    \n-    # If there's remaining space, try to add relevant small segments\n-    remaining_chars = max_total_chars - total_chars\n-    if remaining_chars > 500 and len(selected_segments) < max_segments:\n-        for segment, score in scored_segments[len(selected_segments):]:\n-            if segment.char_count <= remaining_chars and len(selected_segments) < max_segments:\n-                selected_segments.append({\n+            truncated_content = (\n+                segment.content[: max_total_chars - 200]\n+                + \"\\n\\n[Content truncated for length...]\"\n+            )\n+            selected_segments.append(\n+                {\n                     \"id\": segment.id,\n                     \"title\": segment.title,\n-                    \"content\": segment.content,\n+                    \"content\": truncated_content,\n                     \"content_type\": segment.content_type,\n                     \"relevance_score\": score,\n-                    \"char_count\": segment.char_count\n-                })\n+                    \"char_count\": len(truncated_content),\n+                }\n+            )\n+            break\n+\n+    # If there's remaining space, try to add relevant small segments\n+    remaining_chars = max_total_chars - total_chars\n+    if remaining_chars > 500 and len(selected_segments) < max_segments:\n+        for segment, score in scored_segments[len(selected_segments) :]:\n+            if (\n+                segment.char_count <= remaining_chars\n+                and len(selected_segments) < max_segments\n+            ):\n+                selected_segments.append(\n+                    {\n+                        \"id\": segment.id,\n+                        \"title\": segment.title,\n+                        \"content\": segment.content,\n+                        \"content_type\": segment.content_type,\n+                        \"relevance_score\": score,\n+                        \"char_count\": segment.char_count,\n+                    }\n+                )\n                 remaining_chars -= segment.char_count\n-    \n+\n     return selected_segments\n \n+\n if __name__ == \"__main__\":\n     # Run the MCP server\n-    mcp.run()\n\\ No newline at end of file\n+    mcp.run()\ndiff --git a/utils/llm_utils.py b/utils/llm_utils.py\nindex 05d3d28..8d4a0cd 100644\n--- a/utils/llm_utils.py\n+++ b/utils/llm_utils.py\n@@ -56,10 +56,10 @@ def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\") -> Type\n def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n     \"\"\"\n     Get default models configuration from config file.\n-    \n+\n     Args:\n         config_path: Path to the main configuration file\n-        \n+\n     Returns:\n         dict: Default models configuration\n     \"\"\"\n@@ -67,14 +67,16 @@ def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n         if os.path.exists(config_path):\n             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-            \n+\n             # Extract model configurations\n             openai_config = config.get(\"openai\", {})\n             anthropic_config = config.get(\"anthropic\", {})\n-            \n+\n             return {\n-                \"anthropic\": anthropic_config.get(\"default_model\", \"claude-sonnet-4-20250514\"),\n-                \"openai\": openai_config.get(\"default_model\", \"o3-mini\")\n+                \"anthropic\": anthropic_config.get(\n+                    \"default_model\", \"claude-sonnet-4-20250514\"\n+                ),\n+                \"openai\": openai_config.get(\"default_model\", \"o3-mini\"),\n             }\n         else:\n             print(f\"\ud83e\udd16 Config file {config_path} not found, using default models\")\n@@ -86,13 +88,15 @@ def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n         return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n \n \n-def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\") -> Dict[str, Any]:\n+def get_document_segmentation_config(\n+    config_path: str = \"mcp_agent.config.yaml\",\n+) -> Dict[str, Any]:\n     \"\"\"\n     Get document segmentation configuration from config file.\n-    \n+\n     Args:\n         config_path: Path to the main configuration file\n-        \n+\n     Returns:\n         Dict containing segmentation configuration with default values\n     \"\"\"\n@@ -100,71 +104,83 @@ def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\")\n         if os.path.exists(config_path):\n             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-            \n+\n             # Get document segmentation config with defaults\n             seg_config = config.get(\"document_segmentation\", {})\n             return {\n                 \"enabled\": seg_config.get(\"enabled\", True),\n-                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000)\n+                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000),\n             }\n         else:\n-            print(f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\")\n+            print(\n+                f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\"\n+            )\n             return {\"enabled\": True, \"size_threshold_chars\": 50000}\n-            \n+\n     except Exception as e:\n         print(f\"\ud83d\udcc4 Error reading segmentation config from {config_path}: {e}\")\n         print(\"\ud83d\udcc4 Using default segmentation settings\")\n         return {\"enabled\": True, \"size_threshold_chars\": 50000}\n \n \n-def should_use_document_segmentation(document_content: str, config_path: str = \"mcp_agent.config.yaml\") -> Tuple[bool, str]:\n+def should_use_document_segmentation(\n+    document_content: str, config_path: str = \"mcp_agent.config.yaml\"\n+) -> Tuple[bool, str]:\n     \"\"\"\n     Determine whether to use document segmentation based on configuration and document size.\n-    \n+\n     Args:\n         document_content: The content of the document to analyze\n         config_path: Path to the configuration file\n-        \n+\n     Returns:\n         Tuple of (should_segment, reason) where:\n         - should_segment: Boolean indicating whether to use segmentation\n         - reason: String explaining the decision\n     \"\"\"\n     seg_config = get_document_segmentation_config(config_path)\n-    \n+\n     if not seg_config[\"enabled\"]:\n         return False, \"Document segmentation disabled in configuration\"\n-    \n+\n     doc_size = len(document_content)\n     threshold = seg_config[\"size_threshold_chars\"]\n-    \n+\n     if doc_size > threshold:\n-        return True, f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\"\n+        return (\n+            True,\n+            f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\",\n+        )\n     else:\n-        return False, f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\"\n+        return (\n+            False,\n+            f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\",\n+        )\n \n \n-def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list = None) -> Dict[str, list]:\n+def get_adaptive_agent_config(\n+    use_segmentation: bool, search_server_names: list = None\n+) -> Dict[str, list]:\n     \"\"\"\n     Get adaptive agent configuration based on whether to use document segmentation.\n-    \n+\n     Args:\n         use_segmentation: Whether to include document-segmentation server\n         search_server_names: Base search server names (from get_search_server_names)\n-        \n+\n     Returns:\n         Dict containing server configurations for different agents\n     \"\"\"\n     if search_server_names is None:\n         search_server_names = []\n-    \n+\n     # Base configuration\n     config = {\n         \"concept_analysis\": [],\n         \"algorithm_analysis\": search_server_names.copy(),\n-        \"code_planner\": search_server_names.copy()\n+        \"code_planner\": search_server_names.copy(),\n     }\n-    \n+\n     # Add document-segmentation server if needed\n     if use_segmentation:\n         config[\"concept_analysis\"] = [\"document-segmentation\"]\n@@ -172,39 +188,39 @@ def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list\n             config[\"algorithm_analysis\"].append(\"document-segmentation\")\n         if \"document-segmentation\" not in config[\"code_planner\"]:\n             config[\"code_planner\"].append(\"document-segmentation\")\n-    \n+\n     return config\n \n \n def get_adaptive_prompts(use_segmentation: bool) -> Dict[str, str]:\n     \"\"\"\n     Get appropriate prompt versions based on segmentation usage.\n-    \n+\n     Args:\n         use_segmentation: Whether to use segmented reading prompts\n-        \n+\n     Returns:\n         Dict containing prompt configurations\n     \"\"\"\n     # Import here to avoid circular imports\n     from prompts.code_prompts import (\n         PAPER_CONCEPT_ANALYSIS_PROMPT,\n-        PAPER_ALGORITHM_ANALYSIS_PROMPT, \n+        PAPER_ALGORITHM_ANALYSIS_PROMPT,\n         CODE_PLANNING_PROMPT,\n         PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n         PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n-        CODE_PLANNING_PROMPT_TRADITIONAL\n+        CODE_PLANNING_PROMPT_TRADITIONAL,\n     )\n-    \n+\n     if use_segmentation:\n         return {\n             \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT,\n             \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT,\n-            \"code_planning\": CODE_PLANNING_PROMPT\n+            \"code_planning\": CODE_PLANNING_PROMPT,\n         }\n     else:\n         return {\n             \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n             \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n-            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL\n-        }\n\\ No newline at end of file\n+            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL,\n+        }\ndiff --git a/workflows/agents/document_segmentation_agent.py b/workflows/agents/document_segmentation_agent.py\nindex a9e4164..6325a7e 100644\n--- a/workflows/agents/document_segmentation_agent.py\n+++ b/workflows/agents/document_segmentation_agent.py\n@@ -5,7 +5,6 @@ A lightweight agent that coordinates with the document segmentation MCP server\n to analyze document structure and prepare segments for other agents.\n \"\"\"\n \n-import asyncio\n import os\n import logging\n from typing import Dict, Any, Optional\n@@ -17,14 +16,14 @@ from utils.llm_utils import get_preferred_llm_class\n class DocumentSegmentationAgent:\n     \"\"\"\n     Intelligent document segmentation agent with semantic analysis capabilities.\n-    \n+\n     This enhanced agent provides:\n     1. **Semantic Document Classification**: Content-based document type identification\n     2. **Adaptive Segmentation Strategy**: Algorithm integrity and semantic coherence preservation\n     3. **Planning Agent Optimization**: Segment preparation specifically optimized for downstream agents\n     4. **Quality Intelligence Validation**: Advanced metrics for completeness and technical accuracy\n     5. **Algorithm Completeness Protection**: Ensures critical algorithms and formulas remain intact\n-    \n+\n     Key improvements over traditional segmentation:\n     - Semantic content analysis vs mechanical structure splitting\n     - Dynamic character limits based on content complexity\n@@ -32,26 +31,26 @@ class DocumentSegmentationAgent:\n     - Algorithm and formula integrity preservation\n     - Content type-aware segmentation strategies\n     \"\"\"\n-    \n+\n     def __init__(self, logger: Optional[logging.Logger] = None):\n         self.logger = logger or self._create_default_logger()\n         self.mcp_agent = None\n-    \n+\n     def _create_default_logger(self) -> logging.Logger:\n         \"\"\"Create default logger if none provided\"\"\"\n         logger = logging.getLogger(f\"{__name__}.DocumentSegmentationAgent\")\n         logger.setLevel(logging.INFO)\n         return logger\n-    \n+\n     async def __aenter__(self):\n         \"\"\"Async context manager entry\"\"\"\n         await self.initialize()\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         \"\"\"Async context manager exit\"\"\"\n         await self.cleanup()\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize the MCP agent connection\"\"\"\n         try:\n@@ -74,21 +73,21 @@ Your enhanced capabilities include:\n - Provide actionable quality assessments\n \n Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.\"\"\",\n-                server_names=[\"document-segmentation\", \"filesystem\"]\n+                server_names=[\"document-segmentation\", \"filesystem\"],\n             )\n-            \n+\n             # Initialize the agent context\n             await self.mcp_agent.__aenter__()\n-            \n+\n             # Attach LLM\n             self.llm = await self.mcp_agent.attach_llm(get_preferred_llm_class())\n-            \n+\n             self.logger.info(\"DocumentSegmentationAgent initialized successfully\")\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Failed to initialize DocumentSegmentationAgent: {e}\")\n             raise\n-    \n+\n     async def cleanup(self):\n         \"\"\"Cleanup resources\"\"\"\n         if self.mcp_agent:\n@@ -96,36 +95,34 @@ Use the enhanced document-segmentation tools to deliver superior segmentation re\n                 await self.mcp_agent.__aexit__(None, None, None)\n             except Exception as e:\n                 self.logger.warning(f\"Error during cleanup: {e}\")\n-    \n+\n     async def analyze_and_prepare_document(\n-        self, \n-        paper_dir: str, \n-        force_refresh: bool = False\n+        self, paper_dir: str, force_refresh: bool = False\n     ) -> Dict[str, Any]:\n         \"\"\"\n         Perform intelligent semantic analysis and create optimized document segments.\n-        \n+\n         This method coordinates with the enhanced document segmentation server to:\n         - Classify document type using semantic content analysis\n         - Select optimal segmentation strategy (semantic_research_focused, algorithm_preserve_integrity, etc.)\n         - Preserve algorithm and formula integrity\n         - Optimize segments for downstream planning agents\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n             force_refresh: Whether to force re-analysis with latest algorithms\n-            \n+\n         Returns:\n             Dict containing enhanced analysis results and intelligent segment information\n         \"\"\"\n         try:\n             self.logger.info(f\"Starting document analysis for: {paper_dir}\")\n-            \n+\n             # Check if markdown file exists\n-            md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+            md_files = [f for f in os.listdir(paper_dir) if f.endswith(\".md\")]\n             if not md_files:\n                 raise ValueError(f\"No markdown file found in {paper_dir}\")\n-            \n+\n             # Use the enhanced document segmentation tool\n             message = f\"\"\"Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}\n \n@@ -147,35 +144,35 @@ After segmentation, get a document overview and provide:\n - Algorithm/formula integrity verification\n - Recommendations for planning agent optimization\n - Technical content completeness evaluation\"\"\"\n-            \n+\n             result = await self.llm.generate_str(message=message)\n-            \n+\n             self.logger.info(\"Document analysis completed successfully\")\n-            \n+\n             # Parse the result and return structured information\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"analysis_result\": result,\n-                \"segments_available\": True\n+                \"segments_available\": True,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error in document analysis: {e}\")\n             return {\n                 \"status\": \"error\",\n                 \"paper_dir\": paper_dir,\n                 \"error_message\": str(e),\n-                \"segments_available\": False\n+                \"segments_available\": False,\n             }\n-    \n+\n     async def get_document_overview(self, paper_dir: str) -> Dict[str, Any]:\n         \"\"\"\n         Get overview of document structure and segments.\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n-            \n+\n         Returns:\n             Dict containing document overview information\n         \"\"\"\n@@ -194,40 +191,36 @@ Provide a comprehensive analysis focusing on:\n 2. Algorithm and formula integrity preservation\n 3. Segment relevance for downstream planning agents\n 4. Technical content distribution and completeness\"\"\"\n-            \n+\n             result = await self.llm.generate_str(message=message)\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n-                \"overview_result\": result\n+                \"overview_result\": result,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting document overview: {e}\")\n-            return {\n-                \"status\": \"error\",\n-                \"paper_dir\": paper_dir,\n-                \"error_message\": str(e)\n-            }\n-    \n+            return {\"status\": \"error\", \"paper_dir\": paper_dir, \"error_message\": str(e)}\n+\n     async def validate_segmentation_quality(self, paper_dir: str) -> Dict[str, Any]:\n         \"\"\"\n         Validate the quality of document segmentation.\n-        \n+\n         Args:\n             paper_dir: Path to the paper directory\n-            \n+\n         Returns:\n             Dict containing validation results\n         \"\"\"\n         try:\n             # Get overview first\n             overview_result = await self.get_document_overview(paper_dir)\n-            \n+\n             if overview_result[\"status\"] != \"success\":\n                 return overview_result\n-            \n+\n             # Analyze enhanced segmentation quality\n             message = f\"\"\"Based on the intelligent document overview for {paper_dir}, please evaluate the enhanced segmentation quality using advanced criteria.\n \n@@ -244,38 +237,32 @@ Provide a comprehensive analysis focusing on:\n - Algorithm/formula integrity enhancements  \n - Planning agent optimization opportunities\n - Content distribution balance adjustments\"\"\"\n-            \n+\n             validation_result = await self.llm.generate_str(message=message)\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"validation_result\": validation_result,\n-                \"overview_data\": overview_result\n+                \"overview_data\": overview_result,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error validating segmentation quality: {e}\")\n-            return {\n-                \"status\": \"error\",\n-                \"paper_dir\": paper_dir,\n-                \"error_message\": str(e)\n-            }\n+            return {\"status\": \"error\", \"paper_dir\": paper_dir, \"error_message\": str(e)}\n \n \n async def run_document_segmentation_analysis(\n-    paper_dir: str, \n-    logger: Optional[logging.Logger] = None,\n-    force_refresh: bool = False\n+    paper_dir: str, logger: Optional[logging.Logger] = None, force_refresh: bool = False\n ) -> Dict[str, Any]:\n     \"\"\"\n     Convenience function to run document segmentation analysis.\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory\n         logger: Optional logger instance\n         force_refresh: Whether to force re-analysis\n-        \n+\n     Returns:\n         Dict containing analysis results\n     \"\"\"\n@@ -284,78 +271,83 @@ async def run_document_segmentation_analysis(\n         analysis_result = await agent.analyze_and_prepare_document(\n             paper_dir, force_refresh=force_refresh\n         )\n-        \n+\n         if analysis_result[\"status\"] == \"success\":\n             # Validate segmentation quality\n             validation_result = await agent.validate_segmentation_quality(paper_dir)\n             analysis_result[\"validation\"] = validation_result\n-        \n+\n         return analysis_result\n \n \n # Utility function for integration with existing workflow\n async def prepare_document_segments(\n-    paper_dir: str,\n-    logger: Optional[logging.Logger] = None\n+    paper_dir: str, logger: Optional[logging.Logger] = None\n ) -> Dict[str, Any]:\n     \"\"\"\n     Prepare intelligent document segments optimized for planning agents.\n-    \n+\n     This enhanced function leverages semantic analysis to create segments that:\n     - Preserve algorithm and formula integrity\n     - Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent\n     - Use adaptive character limits based on content complexity\n     - Maintain technical content completeness\n-    \n-    Called from the orchestration engine (Phase 3.5) to prepare documents \n+\n+    Called from the orchestration engine (Phase 3.5) to prepare documents\n     before the planning phase with superior segmentation quality.\n-    \n+\n     Args:\n         paper_dir: Path to the paper directory containing markdown file\n         logger: Optional logger instance for tracking\n-        \n+\n     Returns:\n         Dict containing enhanced preparation results and intelligent metadata\n     \"\"\"\n     try:\n         logger = logger or logging.getLogger(__name__)\n         logger.info(f\"Preparing document segments for: {paper_dir}\")\n-        \n+\n         # Run analysis\n         result = await run_document_segmentation_analysis(\n             paper_dir=paper_dir,\n             logger=logger,\n-            force_refresh=False  # Use cached analysis if available\n+            force_refresh=False,  # Use cached analysis if available\n         )\n-        \n+\n         if result[\"status\"] == \"success\":\n             logger.info(\"Document segments prepared successfully\")\n-            \n+\n             # Create metadata for downstream agents\n             segments_dir = os.path.join(paper_dir, \"document_segments\")\n-            \n+\n             return {\n                 \"status\": \"success\",\n                 \"paper_dir\": paper_dir,\n                 \"segments_dir\": segments_dir,\n                 \"segments_ready\": True,\n                 \"analysis_summary\": result.get(\"analysis_result\", \"\"),\n-                \"validation_summary\": result.get(\"validation\", {}).get(\"validation_result\", \"\")\n+                \"validation_summary\": result.get(\"validation\", {}).get(\n+                    \"validation_result\", \"\"\n+                ),\n             }\n         else:\n-            logger.error(f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\")\n+            logger.error(\n+                f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\"\n+            )\n             return {\n                 \"status\": \"error\",\n                 \"paper_dir\": paper_dir,\n                 \"segments_ready\": False,\n-                \"error_message\": result.get(\"error_message\", \"Document segmentation failed\")\n+                \"error_message\": result.get(\n+                    \"error_message\", \"Document segmentation failed\"\n+                ),\n             }\n-            \n+\n     except Exception as e:\n         logger.error(f\"Error preparing document segments: {e}\")\n         return {\n             \"status\": \"error\",\n             \"paper_dir\": paper_dir,\n             \"segments_ready\": False,\n-            \"error_message\": str(e)\n-        }\n\\ No newline at end of file\n+            \"error_message\": str(e),\n+        }\n"
    },
    {
        "id": "279",
        "sha_fail": "238a409674f147334f43788013fdfa766fd8035c",
        "diff": "diff --git a/workflows/agents/memory_agent_concise.py b/workflows/agents/memory_agent_concise.py\nindex 1be5423..841dc69 100644\n--- a/workflows/agents/memory_agent_concise.py\n+++ b/workflows/agents/memory_agent_concise.py\n@@ -695,7 +695,7 @@ class ConciseMemoryAgent:\n         summary = f\"\"\"# Code Implementation Summary\n **All Previously Implemented Files:**\n {implemented_files_list}\n-**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n+**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n **File Implemented**: {file_path}\n **Total Files Implemented**: {files_implemented}\n **Summary failed to generate.**\ndiff --git a/workflows/agents/memory_agent_concise_index.py b/workflows/agents/memory_agent_concise_index.py\nindex ad3c53f..b67fef9 100644\n--- a/workflows/agents/memory_agent_concise_index.py\n+++ b/workflows/agents/memory_agent_concise_index.py\n@@ -684,7 +684,7 @@ class ConciseMemoryAgent:\n         summary = f\"\"\"# Code Implementation Summary\n **All Previously Implemented Files:**\n {implemented_files_list}\n-**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n+**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n **File Implemented**: {file_path}\n **Total Files Implemented**: {files_implemented}\n **Summary failed to generate.**\n"
    },
    {
        "id": "280",
        "sha_fail": "219bc58c7f1bfe425ddc1d628ff5cda9639afc1e",
        "diff": "diff --git a/tools/pdf_downloader.py b/tools/pdf_downloader.py\nindex adb602b..301ca5d 100644\n--- a/tools/pdf_downloader.py\n+++ b/tools/pdf_downloader.py\n@@ -109,10 +109,10 @@ async def perform_document_conversion(\n         try:\n             with open(file_path, \"rb\") as f:\n                 header = f.read(8)\n-                is_pdf_file = header.startswith(b'%PDF')\n+                is_pdf_file = header.startswith(b\"%PDF\")\n         except Exception:\n             is_pdf_file = file_path.lower().endswith(\".pdf\")\n-    \n+\n     if is_pdf_file and PYPDF2_AVAILABLE:\n         try:\n             simple_converter = SimplePdfConverter()\n@@ -585,7 +585,7 @@ class DoclingConverter:\n                         ext = \"png\"\n \n                     # \u751f\u6210\u6587\u4ef6\u540d\n-                    filename = f\"image_{idx+1}.{ext}\"\n+                    filename = f\"image_{idx + 1}.{ext}\"\n                     filepath = os.path.join(images_dir, filename)\n \n                     # \u4fdd\u5b58\u56fe\u7247\u6570\u636e\n@@ -600,7 +600,7 @@ class DoclingConverter:\n                         image_map[img_id] = rel_path\n \n                 except Exception as img_error:\n-                    print(f\"Warning: Failed to extract image {idx+1}: {img_error}\")\n+                    print(f\"Warning: Failed to extract image {idx + 1}: {img_error}\")\n                     continue\n \n         except Exception as e:\ndiff --git a/utils/file_processor.py b/utils/file_processor.py\nindex b25346f..9963286 100644\n--- a/utils/file_processor.py\n+++ b/utils/file_processor.py\n@@ -190,8 +190,10 @@ class FileProcessor:\n             # Check if file is actually a PDF by reading the first few bytes\n             with open(file_path, \"rb\") as f:\n                 header = f.read(8)\n-                if header.startswith(b'%PDF'):\n-                    raise IOError(f\"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\")\n+                if header.startswith(b\"%PDF\"):\n+                    raise IOError(\n+                        f\"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\"\n+                    )\n \n             # Read file content\n             # Note: Using async with would be better for large files\n@@ -202,7 +204,9 @@ class FileProcessor:\n             return content\n \n         except UnicodeDecodeError as e:\n-            raise IOError(f\"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}\")\n+            raise IOError(\n+                f\"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}\"\n+            )\n         except Exception as e:\n             raise IOError(f\"Error reading file {file_path}: {str(e)}\")\n \ndiff --git a/workflows/agent_orchestration_engine.py b/workflows/agent_orchestration_engine.py\nindex 027775d..3f2d602 100644\n--- a/workflows/agent_orchestration_engine.py\n+++ b/workflows/agent_orchestration_engine.py\n@@ -664,9 +664,11 @@ async def orchestrate_document_preprocessing_agent(\n             # Check if file is actually a PDF by reading the first few bytes\n             with open(md_path, \"rb\") as f:\n                 header = f.read(8)\n-                if header.startswith(b'%PDF'):\n-                    raise IOError(f\"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\")\n-            \n+                if header.startswith(b\"%PDF\"):\n+                    raise IOError(\n+                        f\"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools.\"\n+                    )\n+\n             with open(md_path, \"r\", encoding=\"utf-8\") as f:\n                 document_content = f.read()\n         except Exception as e:\n"
    },
    {
        "id": "281",
        "sha_fail": "9ecbaeeb4d8db618ccdb6cb81f5edf4ba2c8b8de",
        "diff": "diff --git a/github/GithubObject.py b/github/GithubObject.py\nindex 697f1601..031e93ce 100644\n--- a/github/GithubObject.py\n+++ b/github/GithubObject.py\n@@ -149,13 +149,11 @@ camel_to_snake_case_regexp = re.compile(r\"(?<!^)(?=[A-Z])\")\n \n \n @overload\n-def as_rest_api_attributes(graphql_attributes: dict[str, Any]) -> dict[str, Any]:\n-    ...\n+def as_rest_api_attributes(graphql_attributes: dict[str, Any]) -> dict[str, Any]: ...\n \n \n @overload\n-def as_rest_api_attributes(graphql_attributes: None) -> None:\n-    ...\n+def as_rest_api_attributes(graphql_attributes: None) -> None: ...\n \n \n def as_rest_api_attributes(graphql_attributes: dict[str, Any] | None) -> dict[str, Any] | None:\ndiff --git a/github/GithubRetry.py b/github/GithubRetry.py\nindex fd7630d4..517b4cbd 100644\n--- a/github/GithubRetry.py\n+++ b/github/GithubRetry.py\n@@ -110,7 +110,7 @@ class GithubRetry(Retry):\n                     # Sleeping 'Retry-After' seconds is implemented in urllib3.Retry.sleep() and called by urllib3\n                     self.__log(\n                         logging.INFO,\n-                        f'Retrying after {response.headers.get(\"Retry-After\")} seconds',\n+                        f\"Retrying after {response.headers.get('Retry-After')} seconds\",\n                     )\n                 else:\n                     content = response.reason\ndiff --git a/scripts/openapi.py b/scripts/openapi.py\nindex af489426..90ef5c4f 100644\n--- a/scripts/openapi.py\n+++ b/scripts/openapi.py\n@@ -448,8 +448,7 @@ class IndexPythonClassesVisitor(CstVisitorBase):\n                         known_verb = f'\"{self._method_verbs[full_method_name]}\"'\n                         if known_verb != verb:\n                             print(\n-                                f\"Method {full_method_name} is known to call {known_verb}, \"\n-                                f\"but doc-string says {verb}\"\n+                                f\"Method {full_method_name} is known to call {known_verb}, but doc-string says {verb}\"\n                             )\n                     else:\n                         # detect method from code\ndiff --git a/tests/CheckRun.py b/tests/CheckRun.py\nindex 53a3d4a7..6a43323f 100644\n--- a/tests/CheckRun.py\n+++ b/tests/CheckRun.py\n@@ -203,15 +203,19 @@ class CheckRun(Framework.TestCase):\n         self.assertEqual(check_run.name, \"completed_check_run\")\n         self.assertEqual(check_run.head_sha, \"0283d46537193f1fed7d46859f15c5304b9836f9\")\n         self.assertEqual(check_run.status, \"completed\")\n-        self.assertEqual(\n-            check_run.started_at,\n-            datetime(2020, 10, 20, 10, 30, 29, tzinfo=timezone.utc),\n-        ),\n+        (\n+            self.assertEqual(\n+                check_run.started_at,\n+                datetime(2020, 10, 20, 10, 30, 29, tzinfo=timezone.utc),\n+            ),\n+        )\n         self.assertEqual(check_run.conclusion, \"success\")\n-        self.assertEqual(\n-            check_run.completed_at,\n-            datetime(2020, 10, 20, 11, 30, 50, tzinfo=timezone.utc),\n-        ),\n+        (\n+            self.assertEqual(\n+                check_run.completed_at,\n+                datetime(2020, 10, 20, 11, 30, 50, tzinfo=timezone.utc),\n+            ),\n+        )\n         self.assertEqual(check_run.output.annotations_count, 2)\n \n     def testUpdateCheckRunSuccess(self):\n"
    },
    {
        "id": "302",
        "sha_fail": "3fecab1b8e5668d6fa49529e87195f6ee7eba268",
        "diff": "diff --git a/tests/core/tests/admin_integration/test_action_export.py b/tests/core/tests/admin_integration/test_action_export.py\nindex c566cf0..14fc6b2 100644\n--- a/tests/core/tests/admin_integration/test_action_export.py\n+++ b/tests/core/tests/admin_integration/test_action_export.py\n@@ -379,8 +379,7 @@ class TestExportButtonOnChangeForm(AdminTestMixin, TestCase):\n             args=[self.cat1.id],\n         )\n         self.target_str = (\n-            '<input type=\"submit\" value=\"Export\" '\n-            'class=\"default\" name=\"_export-item\">'\n+            '<input type=\"submit\" value=\"Export\" class=\"default\" name=\"_export-item\">'\n         )\n \n     def test_export_button_on_change_form(self):\n@@ -460,13 +459,13 @@ class TestSkipExportFormFromAction(AdminTestMixin, TestCase):\n     def test_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n     def test_skip_export_form_from_action_setting_enabled(self):\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n \n@@ -500,7 +499,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     def test_export_button_on_change_form_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n@@ -509,7 +508,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     ):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_EXPORT_UI=True)\n"
    },
    {
        "id": "303",
        "sha_fail": "dadeba110a5a98dbc810f9b8f3896424175318c1",
        "diff": "diff --git a/tests/core/tests/admin_integration/test_action_export.py b/tests/core/tests/admin_integration/test_action_export.py\nindex c566cf0..ca7801d 100644\n--- a/tests/core/tests/admin_integration/test_action_export.py\n+++ b/tests/core/tests/admin_integration/test_action_export.py\n@@ -350,7 +350,7 @@ class TestExportFilterPreservation(AdminTestMixin, TestCase):\n             final_response = self._post_url_response(export_url, export_data)\n \n         # Should get CSV export that respects the filter context\n-        self.assertEqual(final_response[\"Content-Type\"], \"text/csv\")\n+        self.assertEqual(final_response.get(\"Content-Type\"), \"text/csv\")\n         content = final_response.content.decode()\n \n         # Verify the export contains the expected filtered data\n@@ -379,8 +379,7 @@ class TestExportButtonOnChangeForm(AdminTestMixin, TestCase):\n             args=[self.cat1.id],\n         )\n         self.target_str = (\n-            '<input type=\"submit\" value=\"Export\" '\n-            'class=\"default\" name=\"_export-item\">'\n+            '<input type=\"submit\" value=\"Export\" class=\"default\" name=\"_export-item\">'\n         )\n \n     def test_export_button_on_change_form(self):\n@@ -460,13 +459,13 @@ class TestSkipExportFormFromAction(AdminTestMixin, TestCase):\n     def test_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n     def test_skip_export_form_from_action_setting_enabled(self):\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n \n@@ -500,7 +499,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     def test_export_button_on_change_form_skip_export_form_from_action_enabled(self):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_ACTION_EXPORT_UI=True)\n@@ -509,7 +508,7 @@ class TestSkipExportFormFromChangeForm(AdminTestMixin, TestCase):\n     ):\n         self.model_admin.skip_export_form_from_action = True\n         response = self.model_admin.export_admin_action(self.request, self.queryset)\n-        target_file_contents = \"id,name\\r\\n\" f\"{self.cat1.id},Cat 1\\r\\n\"\n+        target_file_contents = f\"id,name\\r\\n{self.cat1.id},Cat 1\\r\\n\"\n         self.assertEqual(target_file_contents.encode(), response.content)\n \n     @override_settings(IMPORT_EXPORT_SKIP_ADMIN_EXPORT_UI=True)\n"
    },
    {
        "id": "304",
        "sha_fail": "16c48e2d1190bbc6fd907ae612921df5dd3f42f4",
        "diff": "diff --git a/tests/middleware/test_cors.py b/tests/middleware/test_cors.py\nindex f2c5fa2..749c627 100644\n--- a/tests/middleware/test_cors.py\n+++ b/tests/middleware/test_cors.py\n@@ -201,8 +201,8 @@ def test_cors_allow_specific_origin(\n def test_cors_disallowed_preflight(\n     test_client_factory: TestClientFactory,\n ) -> None:\n-    def homepage(request: Request) -> None:\n-        pass  # pragma: no cover\n+    def homepage(request: Request) -> PlainTextResponse:\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -251,8 +251,8 @@ def test_cors_disallowed_preflight(\n def test_cors_preflight_allow_all_methods(\n     test_client_factory: TestClientFactory,\n ) -> None:\n-    def homepage(request: Request) -> None:\n-        pass  # pragma: no cover\n+    def homepage(request: Request) -> PlainTextResponse:\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -460,7 +460,7 @@ def test_cors_preflight_vary_with_wildcard_origins_specific_methods(\n     test_client_factory: TestClientFactory,\n ) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -481,7 +481,7 @@ def test_cors_preflight_vary_with_specific_origins_wildcard_methods(\n     test_client_factory: TestClientFactory,\n ) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -694,7 +694,7 @@ def test_cors_null_origin_explicitly_allowed(test_client_factory: TestClientFact\n \n def test_cors_method_case_sensitive(test_client_factory: TestClientFactory) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -776,7 +776,7 @@ def test_cors_origins_list_and_regex_both_accepted(test_client_factory: TestClie\n \n def test_cors_max_age_header(test_client_factory: TestClientFactory) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app_default = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n@@ -817,7 +817,7 @@ def test_cors_no_origin_header_no_cors_processing(test_client_factory: TestClien\n \n def test_cors_header_name_case_insensitive(test_client_factory: TestClientFactory) -> None:\n     def homepage(request: Request) -> PlainTextResponse:\n-        pass  # pragma: no cover\n+        return PlainTextResponse(\"Homepage\", status_code=200)\n \n     app = Starlette(\n         routes=[Route(\"/\", endpoint=homepage)],\n"
    },
    {
        "id": "305",
        "sha_fail": "207b861fbc10acc867a6f04eae0b4c866d1e508d",
        "diff": "diff --git a/tests/supervisors/test_multiprocess.py b/tests/supervisors/test_multiprocess.py\nindex e1f594e..bd481d8 100644\n--- a/tests/supervisors/test_multiprocess.py\n+++ b/tests/supervisors/test_multiprocess.py\n@@ -133,7 +133,7 @@ def test_multiprocess_sighup() -> None:\n     pids = [p.pid for p in supervisor.processes]\n     supervisor.signal_queue.append(signal.SIGHUP)\n     time.sleep(1)\n-    assert pids != [p.pid for p in supervisor.processes]\n+    assert len(pids) == len(supervisor.processes) and pids != [p.pid for p in supervisor.processes]\n     supervisor.signal_queue.append(signal.SIGINT)\n     supervisor.join_all()\n \n"
    },
    {
        "id": "308",
        "sha_fail": "df5e9e422993a051ab7e2fc7b1dc70485a40c7d3",
        "diff": "diff --git a/tests/test_cli_lip_syncer.py b/tests/test_cli_lip_syncer.py\nindex b61a255..f4a7bdc 100644\n--- a/tests/test_cli_lip_syncer.py\n+++ b/tests/test_cli_lip_syncer.py\n@@ -5,36 +5,84 @@ import pytest\n \n from facefusion.download import conditional_download\n from facefusion.jobs.job_manager import clear_jobs, init_jobs\n-from .helper import get_test_example_file, get_test_examples_directory, get_test_jobs_directory, get_test_output_file, is_test_output_file, prepare_test_output_directory\n+from .helper import (\n+    get_test_example_file,\n+    get_test_examples_directory,\n+    get_test_jobs_directory,\n+    get_test_output_file,\n+    is_test_output_file,\n+    prepare_test_output_directory,\n+)\n \n \n-@pytest.fixture(scope = 'module', autouse = True)\n+@pytest.fixture(scope=\"module\", autouse=True)\n def before_all() -> None:\n-\tconditional_download(get_test_examples_directory(),\n-\t[\n-\t\t'https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.jpg',\n-\t\t'https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.mp3',\n-\t\t'https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/target-240p.mp4'\n-\t])\n-\tsubprocess.run([ 'ffmpeg', '-i', get_test_example_file('target-240p.mp4'), '-vframes', '1', get_test_example_file('target-240p.jpg') ])\n+    conditional_download(\n+        get_test_examples_directory(),\n+        [\n+            \"https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.jpg\",\n+            \"https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/source.mp3\",\n+            \"https://github.com/facefusion/facefusion-assets/releases/download/examples-3.0.0/target-240p.mp4\",\n+        ],\n+    )\n+    subprocess.run(\n+        [\n+            \"ffmpeg\",\n+            \"-i\",\n+            get_test_example_file(\"target-240p.mp4\"),\n+            \"-vframes\",\n+            \"1\",\n+            get_test_example_file(\"target-240p.jpg\"),\n+        ]\n+    )\n \n \n-@pytest.fixture(scope = 'function', autouse = True)\n+@pytest.fixture(scope=\"function\", autouse=True)\n def before_each() -> None:\n-\tclear_jobs(get_test_jobs_directory())\n-\tinit_jobs(get_test_jobs_directory())\n-\tprepare_test_output_directory()\n+    clear_jobs(get_test_jobs_directory())\n+    init_jobs(get_test_jobs_directory())\n+    prepare_test_output_directory()\n \n \n def test_sync_lip_to_image() -> None:\n-\tcommands = [ sys.executable, 'facefusion.py', 'run', '--jobs-path', get_test_jobs_directory(), '--processors', 'lip_syncer', '-s', get_test_example_file('source.mp3'), '-t', get_test_example_file('target-240p.jpg'), '-o', get_test_output_file('test_sync_lip_to_image.jpg') ]\n+    commands = [\n+        sys.executable,\n+        \"facefusion.py\",\n+        \"run\",\n+        \"--jobs-path\",\n+        get_test_jobs_directory(),\n+        \"--processors\",\n+        \"lip_syncer\",\n+        \"-s\",\n+        get_test_example_file(\"source.mp3\"),\n+        \"-t\",\n+        get_test_example_file(\"target-240p.jpg\"),\n+        \"-o\",\n+        get_test_output_file(\"test_sync_lip_to_image.jpg\"),\n+    ]\n \n-\tassert subprocess.run(commands).returncode == 0\n-\tassert is_test_output_file('test_sync_lip_to_image.jpg') is True\n+    assert subprocess.run(commands).returncode == 0\n+    assert is_test_output_file(\"test_sync_lip_to_image.jpg\") is True\n \n \n def test_sync_lip_to_video() -> None:\n-\tcommands = [ sys.executable, 'facefusion.py', 'run', '--jobs-path', get_test_jobs_directory(), '--processors', 'lip_syncer', '-s', get_test_example_file('source.mp3'), '-t', get_test_example_file('target-240p.mp4'), '-o', get_test_output_file('test_sync_lip_to_video.mp4'), '--trim-frame-end', '1' ]\n+    commands = [\n+        sys.executable,\n+        \"facefusion.py\",\n+        \"run\",\n+        \"--jobs-path\",\n+        get_test_jobs_directory(),\n+        \"--processors\",\n+        \"lip_syncer\",\n+        \"-s\",\n+        get_test_example_file(\"source.mp3\"),\n+        \"-t\",\n+        get_test_example_file(\"target-240p.mp4\"),\n+        \"-o\",\n+        get_test_output_file(\"test_sync_lip_to_video.mp4\"),\n+        \"--trim-frame-end\",\n+        \"1\",\n+    ]\n \n-\tassert subprocess.run(commands).returncode == 0\n-\tassert is_test_output_file('test_sync_lip_to_video.mp4') is True\n+    assert subprocess.run(commands).returncode == 0\n+    assert is_test_output_file(\"test_sync_lip_to_video.mp4\") is True\n"
    },
    {
        "id": "309",
        "sha_fail": "064c3b62f835ae5315b5893b3b4bd98b54e34ffa",
        "diff": "diff --git a/cli/cli_interface.py b/cli/cli_interface.py\nindex e53773a..df5b60d 100644\n--- a/cli/cli_interface.py\n+++ b/cli/cli_interface.py\n@@ -40,10 +40,10 @@ class CLIInterface:\n         self.is_running = True\n         self.processing_history = []\n         self.enable_indexing = True  # Default configuration\n-        \n+\n         # Load segmentation config from the same source as UI\n         self._load_segmentation_config()\n-        \n+\n         # Initialize tkinter availability\n         self._init_tkinter()\n \n@@ -51,6 +51,7 @@ class CLIInterface:\n         \"\"\"Load segmentation configuration from mcp_agent.config.yaml\"\"\"\n         try:\n             from utils.llm_utils import get_document_segmentation_config\n+\n             seg_config = get_document_segmentation_config()\n             self.segmentation_enabled = seg_config.get(\"enabled\", True)\n             self.segmentation_threshold = seg_config.get(\"size_threshold_chars\", 50000)\n@@ -64,7 +65,7 @@ class CLIInterface:\n         \"\"\"Save segmentation configuration to mcp_agent.config.yaml\"\"\"\n         import yaml\n         import os\n-        \n+\n         # Get the project root directory (where mcp_agent.config.yaml is located)\n         current_file = os.path.abspath(__file__)\n         cli_dir = os.path.dirname(current_file)  # cli directory\n@@ -81,16 +82,22 @@ class CLIInterface:\n                 config[\"document_segmentation\"] = {}\n \n             config[\"document_segmentation\"][\"enabled\"] = self.segmentation_enabled\n-            config[\"document_segmentation\"][\"size_threshold_chars\"] = self.segmentation_threshold\n+            config[\"document_segmentation\"][\"size_threshold_chars\"] = (\n+                self.segmentation_threshold\n+            )\n \n             # Write updated config\n             with open(config_path, \"w\", encoding=\"utf-8\") as f:\n                 yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n \n-            print(f\"{Colors.OKGREEN}\u2705 Document segmentation configuration updated{Colors.ENDC}\")\n+            print(\n+                f\"{Colors.OKGREEN}\u2705 Document segmentation configuration updated{Colors.ENDC}\"\n+            )\n \n         except Exception as e:\n-            print(f\"{Colors.WARNING}\u26a0\ufe0f Failed to update segmentation config: {str(e)}{Colors.ENDC}\")\n+            print(\n+                f\"{Colors.WARNING}\u26a0\ufe0f Failed to update segmentation config: {str(e)}{Colors.ENDC}\"\n+            )\n \n     def _init_tkinter(self):\n         \"\"\"Initialize tkinter availability check\"\"\"\n@@ -790,8 +797,8 @@ class CLIInterface:\n \u2551         \u2717 Smart segmentation (Disabled)                                      \u2551\n \u2551                                                                               \u2551\n \u2551  {Colors.YELLOW}Current Settings:{Colors.CYAN}                                                         \u2551\n-\u2551    Pipeline: {'\ud83e\udde0 Comprehensive Mode' if self.enable_indexing else '\u26a1 Optimized Mode'}                                          \u2551\n-\u2551    Document: {'\ud83d\udcc4 Smart Segmentation' if segmentation_enabled else '\ud83d\udccb Traditional Processing'}                                \u2551\n+\u2551    Pipeline: {\"\ud83e\udde0 Comprehensive Mode\" if self.enable_indexing else \"\u26a1 Optimized Mode\"}                                          \u2551\n+\u2551    Document: {\"\ud83d\udcc4 Smart Segmentation\" if segmentation_enabled else \"\ud83d\udccb Traditional Processing\"}                                \u2551\n \u2551    Threshold: {segmentation_threshold} characters                                    \u2551\n \u2551                                                                               \u2551\n \u2551  {Colors.OKGREEN}[T] Toggle Pipeline    {Colors.BLUE}[S] Toggle Segmentation    {Colors.FAIL}[B] Back{Colors.CYAN}     \u2551\ndiff --git a/workflows/agent_orchestration_engine.py b/workflows/agent_orchestration_engine.py\nindex b766b64..f225a8d 100644\n--- a/workflows/agent_orchestration_engine.py\n+++ b/workflows/agent_orchestration_engine.py\n@@ -65,76 +65,98 @@ os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"  # Prevent .pyc file generation\n def _assess_output_completeness(text: str) -> float:\n     \"\"\"\n     \u667a\u80fd\u8bc4\u4f30\u8f93\u51fa\u5b8c\u6574\u6027\u7684\u9ad8\u7ea7\u7b97\u6cd5\n-    \n+\n     \u4f7f\u7528\u591a\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u68c0\u6d4b\u8f93\u51fa\u662f\u5426\u88ab\u622a\u65ad\uff1a\n     1. \u7ed3\u6784\u5316\u6807\u8bb0\u5b8c\u6574\u6027\u68c0\u67e5\n-    2. \u53e5\u5b50\u5b8c\u6574\u6027\u5206\u6790 \n+    2. \u53e5\u5b50\u5b8c\u6574\u6027\u5206\u6790\n     3. \u4ee3\u7801\u5757\u5b8c\u6574\u6027\u9a8c\u8bc1\n     4. \u9884\u671f\u5185\u5bb9\u5143\u7d20\u68c0\u67e5\n-    \n+\n     Returns:\n         float: \u5b8c\u6574\u6027\u5206\u6570 (0.0-1.0)\uff0c\u8d8a\u9ad8\u8868\u793a\u8d8a\u5b8c\u6574\n     \"\"\"\n     if not text or len(text.strip()) < 100:\n         return 0.0\n-    \n+\n     score = 0.0\n     factors = 0\n-    \n+\n     # 1. \u57fa\u672c\u957f\u5ea6\u68c0\u67e5 (\u6743\u91cd: 0.2)\n     if len(text) > 5000:  # \u671f\u671b\u7684\u6700\u5c0f\u8f93\u51fa\u957f\u5ea6\n         score += 0.2\n     elif len(text) > 2000:\n         score += 0.1\n     factors += 1\n-    \n+\n     # 2. \u7ed3\u6784\u5b8c\u6574\u6027\u68c0\u67e5 (\u6743\u91cd: 0.3)\n     structure_indicators = [\n-        \"## 1.\", \"## 2.\", \"## 3.\",  # \u7ae0\u8282\u6807\u9898\n-        \"```\", \"file_structure\", \"implementation\",\n-        \"algorithm\", \"method\", \"function\"\n+        \"## 1.\",\n+        \"## 2.\",\n+        \"## 3.\",  # \u7ae0\u8282\u6807\u9898\n+        \"```\",\n+        \"file_structure\",\n+        \"implementation\",\n+        \"algorithm\",\n+        \"method\",\n+        \"function\",\n     ]\n-    structure_count = sum(1 for indicator in structure_indicators if indicator.lower() in text.lower())\n+    structure_count = sum(\n+        1 for indicator in structure_indicators if indicator.lower() in text.lower()\n+    )\n     if structure_count >= 6:\n         score += 0.3\n     elif structure_count >= 3:\n         score += 0.15\n     factors += 1\n-    \n+\n     # 3. \u53e5\u5b50\u5b8c\u6574\u6027\u68c0\u67e5 (\u6743\u91cd: 0.2)\n-    lines = text.strip().split('\\n')\n+    lines = text.strip().split(\"\\n\")\n     if lines:\n         last_line = lines[-1].strip()\n         # \u68c0\u67e5\u6700\u540e\u4e00\u884c\u662f\u5426\u662f\u5b8c\u6574\u7684\u53e5\u5b50\u6216\u7ed3\u6784\u5316\u5185\u5bb9\n-        if (last_line.endswith(('.', ':', '```', '!', '?')) or \n-            last_line.startswith(('##', '-', '*', '`')) or\n-            len(last_line) < 10):  # \u5f88\u77ed\u7684\u884c\u53ef\u80fd\u662f\u5217\u8868\u9879\n+        if (\n+            last_line.endswith((\".\", \":\", \"```\", \"!\", \"?\"))\n+            or last_line.startswith((\"##\", \"-\", \"*\", \"`\"))\n+            or len(last_line) < 10\n+        ):  # \u5f88\u77ed\u7684\u884c\u53ef\u80fd\u662f\u5217\u8868\u9879\n             score += 0.2\n-        elif len(last_line) > 50 and not last_line.endswith(('.', ':', '```', '!', '?')):\n+        elif len(last_line) > 50 and not last_line.endswith(\n+            (\".\", \":\", \"```\", \"!\", \"?\")\n+        ):\n             # \u957f\u884c\u4f46\u6ca1\u6709\u9002\u5f53\u7ed3\u5c3e\uff0c\u53ef\u80fd\u88ab\u622a\u65ad\n             score += 0.05\n     factors += 1\n-    \n+\n     # 4. \u4ee3\u7801\u5b9e\u73b0\u8ba1\u5212\u5b8c\u6574\u6027 (\u6743\u91cd: 0.3)\n     implementation_keywords = [\n-        \"file structure\", \"architecture\", \"implementation\", \n-        \"requirements\", \"dependencies\", \"setup\", \"main\",\n-        \"class\", \"function\", \"method\", \"algorithm\"\n+        \"file structure\",\n+        \"architecture\",\n+        \"implementation\",\n+        \"requirements\",\n+        \"dependencies\",\n+        \"setup\",\n+        \"main\",\n+        \"class\",\n+        \"function\",\n+        \"method\",\n+        \"algorithm\",\n     ]\n-    impl_count = sum(1 for keyword in implementation_keywords if keyword.lower() in text.lower())\n+    impl_count = sum(\n+        1 for keyword in implementation_keywords if keyword.lower() in text.lower()\n+    )\n     if impl_count >= 8:\n         score += 0.3\n     elif impl_count >= 4:\n         score += 0.15\n     factors += 1\n-    \n+\n     return min(score, 1.0)  # \u786e\u4fdd\u4e0d\u8d85\u8fc71.0\n \n \n def _adjust_params_for_retry(params: RequestParams, retry_count: int) -> RequestParams:\n     \"\"\"\n     \u52a8\u6001\u8c03\u6574\u8bf7\u6c42\u53c2\u6570\u4ee5\u63d0\u9ad8\u6210\u529f\u7387\n-    \n+\n     \u57fa\u4e8e\u91cd\u8bd5\u6b21\u6570\u667a\u80fd\u8c03\u6574\u53c2\u6570\uff1a\n     - \u589e\u52a0token\u9650\u5236\n     - \u8c03\u6574temperature\n@@ -142,15 +164,17 @@ def _adjust_params_for_retry(params: RequestParams, retry_count: int) -> Request\n     \"\"\"\n     # \u57fa\u7840token\u589e\u91cf\uff1a\u6bcf\u6b21\u91cd\u8bd5\u589e\u52a0\u66f4\u591atokens\n     token_increment = 4096 * (retry_count + 1)\n-    new_max_tokens = min(params.max_tokens + token_increment, 32768)  # \u4e0d\u8d85\u8fc732K\u7684\u5408\u7406\u9650\u5236\n-    \n+    new_max_tokens = min(\n+        params.max_tokens + token_increment, 32768\n+    )  # \u4e0d\u8d85\u8fc732K\u7684\u5408\u7406\u9650\u5236\n+\n     # \u968f\u7740\u91cd\u8bd5\u6b21\u6570\u589e\u52a0\uff0c\u964d\u4f4etemperature\u4ee5\u83b7\u5f97\u66f4\u4e00\u81f4\u7684\u8f93\u51fa\n     new_temperature = max(params.temperature - (retry_count * 0.1), 0.1)\n-    \n+\n     print(f\"\ud83d\udd27 Adjusting parameters for retry {retry_count + 1}:\")\n     print(f\"   Token limit: {params.max_tokens} \u2192 {new_max_tokens}\")\n     print(f\"   Temperature: {params.temperature} \u2192 {new_temperature}\")\n-    \n+\n     return RequestParams(\n         max_tokens=new_max_tokens,\n         temperature=new_temperature,\n@@ -483,13 +507,15 @@ async def run_code_analyzer(\n         # \u5206\u6bb5\u6a21\u5f0f\uff1a\u53ef\u4ee5\u4f7f\u7528\u66f4\u9ad8\u7684token\u9650\u5236\uff0c\u56e0\u4e3a\u8f93\u5165\u5df2\u7ecf\u88ab\u4f18\u5316\n         max_tokens_limit = 16384  # \u4f7f\u7528\u66f4\u9ad8\u9650\u5236\uff0c\u56e0\u4e3a\u5206\u6bb5\u51cf\u5c11\u4e86\u8f93\u5165\u590d\u6742\u6027\n         temperature = 0.2  # \u7a0d\u5fae\u964d\u4f4etemperature\u4ee5\u63d0\u9ad8\u4e00\u81f4\u6027\n-        print(\"\ud83e\udde0 Using SEGMENTED mode: Higher token limit (16384) with optimized inputs\")\n+        print(\n+            \"\ud83e\udde0 Using SEGMENTED mode: Higher token limit (16384) with optimized inputs\"\n+        )\n     else:\n         # \u4f20\u7edf\u6a21\u5f0f\uff1a\u4f7f\u7528\u4fdd\u5b88\u7684token\u9650\u5236\u5e76\u542f\u7528\u589e\u91cf\u751f\u6210\n         max_tokens_limit = 12288  # \u4e2d\u7b49\u9650\u5236\uff0c\u4e3a\u805a\u5408\u8f93\u51fa\u7559\u51fa\u7a7a\u95f4\n         temperature = 0.3\n         print(\"\ud83e\udde0 Using TRADITIONAL mode: Moderate token limit (12288)\")\n-    \n+\n     enhanced_params = RequestParams(\n         max_tokens=max_tokens_limit,\n         temperature=temperature,\n@@ -509,33 +535,39 @@ The goal is to create a reproduction plan detailed enough for independent implem\n     # \u667a\u80fd\u8f93\u51fa\u5b8c\u6574\u6027\u68c0\u67e5\u548c\u91cd\u8bd5\u673a\u5236\n     max_retries = 3\n     retry_count = 0\n-    \n+\n     while retry_count < max_retries:\n         try:\n-            print(f\"\ud83d\ude80 Attempting code analysis (attempt {retry_count + 1}/{max_retries})\")\n+            print(\n+                f\"\ud83d\ude80 Attempting code analysis (attempt {retry_count + 1}/{max_retries})\"\n+            )\n             result = await code_aggregator_agent.generate_str(\n                 message=message, request_params=enhanced_params\n             )\n-            \n+\n             # \u68c0\u67e5\u8f93\u51fa\u5b8c\u6574\u6027\u7684\u9ad8\u7ea7\u6307\u6807\n             completeness_score = _assess_output_completeness(result)\n             print(f\"\ud83d\udcca Output completeness score: {completeness_score:.2f}/1.0\")\n-            \n+\n             if completeness_score >= 0.8:  # \u8f93\u51fa\u88ab\u8ba4\u4e3a\u662f\u5b8c\u6574\u7684\n-                print(f\"\u2705 Code analysis completed successfully (length: {len(result)} chars)\")\n+                print(\n+                    f\"\u2705 Code analysis completed successfully (length: {len(result)} chars)\"\n+                )\n                 return result\n             else:\n-                print(f\"\u26a0\ufe0f Output appears truncated (score: {completeness_score:.2f}), retrying with enhanced parameters...\")\n+                print(\n+                    f\"\u26a0\ufe0f Output appears truncated (score: {completeness_score:.2f}), retrying with enhanced parameters...\"\n+                )\n                 # \u52a8\u6001\u8c03\u6574\u53c2\u6570\u8fdb\u884c\u91cd\u8bd5\n                 enhanced_params = _adjust_params_for_retry(enhanced_params, retry_count)\n                 retry_count += 1\n-                \n+\n         except Exception as e:\n             print(f\"\u274c Error in code analysis attempt {retry_count + 1}: {e}\")\n             retry_count += 1\n             if retry_count >= max_retries:\n                 raise\n-    \n+\n     # \u5982\u679c\u6240\u6709\u91cd\u8bd5\u90fd\u5931\u8d25\uff0c\u8fd4\u56de\u6700\u540e\u4e00\u6b21\u7684\u7ed3\u679c\n     print(f\"\u26a0\ufe0f Returning potentially incomplete result after {max_retries} attempts\")\n     return result\n"
    },
    {
        "id": "313",
        "sha_fail": "4ffacbbe801b90aa147af206809b8c610ccbcefb",
        "diff": "diff --git a/.github/workflows/ci.py b/.github/workflows/ci.py\nindex c83f522..624da9c 100644\n--- a/.github/workflows/ci.py\n+++ b/.github/workflows/ci.py\n@@ -52,6 +52,7 @@ def run(*a: str, print_crash_reports: bool = False) -> None:\n     if ret != 0:\n         if ret < 0:\n             import signal\n+\n             try:\n                 sig = signal.Signals(-ret)\n             except ValueError:\n@@ -65,6 +66,7 @@ def run(*a: str, print_crash_reports: bool = False) -> None:\n \n def download_with_retry(url: str | Request, count: int = 5) -> bytes:\n     from urllib.request import urlopen\n+\n     for i in range(count):\n         try:\n             print('Downloading', url, flush=True)\n@@ -105,15 +107,18 @@ def install_deps() -> None:\n             openssl = 'openssl'\n             items.remove('go')  # already installed by ci.yml\n             import ssl\n+\n             if ssl.OPENSSL_VERSION_INFO[0] == 1:\n                 openssl += '@1.1'\n             run('brew', 'install', 'fish', openssl, *items)\n     else:\n         run('sudo apt-get update')\n-        run('sudo apt-get install -y libgl1-mesa-dev libxi-dev libxrandr-dev libxinerama-dev ca-certificates'\n+        run(\n+            'sudo apt-get install -y libgl1-mesa-dev libxi-dev libxrandr-dev libxinerama-dev ca-certificates'\n             ' libxcursor-dev libxcb-xkb-dev libdbus-1-dev libxkbcommon-dev libharfbuzz-dev libx11-xcb-dev zsh'\n             ' libpng-dev liblcms2-dev libfontconfig-dev libxkbcommon-x11-dev libcanberra-dev libxxhash-dev uuid-dev'\n-            ' libsimde-dev libsystemd-dev libcairo2-dev zsh bash dash systemd-coredump gdb')\n+            ' libsimde-dev libsystemd-dev libcairo2-dev zsh bash dash systemd-coredump gdb'\n+        )\n         # for some reason these directories are world writable which causes zsh\n         # compinit to break\n         run('sudo chmod -R og-w /usr/share/zsh')\n@@ -197,9 +202,12 @@ def install_bundle(dest: str = '', which: str = '') -> None:\n \n def install_grype() -> str:\n     dest = os.path.join(SW, 'bin')\n-    rq = Request('https://api.github.com/repos/anchore/grype/releases/latest', headers={\n-        'Accept': 'application/vnd.github.v3+json',\n-    })\n+    rq = Request(\n+        'https://api.github.com/repos/anchore/grype/releases/latest',\n+        headers={\n+            'Accept': 'application/vnd.github.v3+json',\n+        },\n+    )\n     m = json.loads(download_with_retry(rq))\n     for asset in m['assets']:\n         if asset['name'].endswith('_linux_amd64.tar.gz'):\n@@ -216,10 +224,10 @@ def install_grype() -> str:\n \n IGNORED_DEPENDENCY_CVES = [\n     # Python stdlib\n-    'CVE-2025-8194', # DoS in tarfile\n-    'CVE-2025-6069', # DoS in HTMLParser\n+    'CVE-2025-8194',  # DoS in tarfile\n+    'CVE-2025-6069',  # DoS in HTMLParser\n     # glib\n-    'CVE-2025-4056', # Only affects Windows, on which we dont run\n+    'CVE-2025-4056',  # Only affects Windows, on which we dont run\n ]\n \n \n@@ -234,12 +242,17 @@ def check_dependencies() -> None:\n     install_bundle(dest, os.path.basename(dest))\n     dest = os.path.join(SW, 'macos')\n     os.makedirs(dest, exist_ok=True)\n-    install_bundle(dest, os.path.basename(dest))\n+    try:\n+        install_bundle(dest, os.path.basename(dest))\n+    except Exception as e:\n+        print(f'Error installing bundle: {e}')\n+        raise SystemExit(1)\n     cmdline = [grype, '--by-cve', '--config', gc, '--fail-on', 'medium', '--only-fixed', '--add-cpes-if-none']\n     if (cp := subprocess.run(cmdline + ['dir:' + SW])).returncode != 0:\n         raise SystemExit(cp.returncode)\n     # Now test against the SBOM\n     import runpy\n+\n     orig = sys.argv, sys.stdout\n     sys.argv = ['bypy', 'sbom', 'myproject', '1.0.0']\n     buf = io.StringIO()\ndiff --git a/kitty/utils.py b/kitty/utils.py\nindex 8836ddc..ecd9cd0 100644\n--- a/kitty/utils.py\n+++ b/kitty/utils.py\n@@ -45,7 +45,6 @@\n \n \n class Flag:\n-\n     def __init__(self, initial_val: bool = True) -> None:\n         self.val = initial_val\n \n@@ -63,9 +62,9 @@ def __bool__(self) -> bool:\n \n \n def expandvars(val: str, env: Mapping[str, str] = {}, fallback_to_os_env: bool = True) -> str:\n-    '''\n+    \"\"\"\n     Expand $VAR and ${VAR} Use $$ for a literal $\n-    '''\n+    \"\"\"\n \n     def sub(m: 'Match[str]') -> str:\n         key = m.group(1) or m.group(2)\n@@ -100,10 +99,12 @@ def kitty_ansi_sanitizer_pat() -> 're.Pattern[str]':\n def platform_window_id(os_window_id: int) -> int | None:\n     if is_macos:\n         from .fast_data_types import cocoa_window_id\n+\n         with suppress(Exception):\n             return cocoa_window_id(os_window_id)\n     if not is_wayland():\n         from .fast_data_types import x11_window_id\n+\n         with suppress(Exception):\n             return x11_window_id(os_window_id)\n     return None\n@@ -116,6 +117,7 @@ def safe_print(*a: Any, **k: Any) -> None:\n \n def log_error(*a: Any, **k: str) -> None:\n     from .fast_data_types import log_error_string\n+\n     output = getattr(log_error, 'redirect', log_error_string)\n     with suppress(Exception):\n         msg = k.get('sep', ' ').join(map(str, a)) + k.get('end', '')\n@@ -144,7 +146,7 @@ def sanitize_title(x: str) -> str:\n \n \n def color_as_int(val: Color) -> int:\n-    return int(val) & 0xffffff\n+    return int(val) & 0xFFFFFF\n \n \n def color_from_int(val: int) -> Color:\n@@ -164,6 +166,7 @@ def read_screen_size(fd: int = -1) -> ScreenSize:\n     import array\n     import fcntl\n     import termios\n+\n     buf = array.array('H', [0, 0, 0, 0])\n     if fd < 0:\n         fd = sys.stdout.fileno()\n@@ -197,6 +200,7 @@ def screen_size_function(fd: int | None = None) -> ScreenSizeGetter:\n \n def fit_image(width: int, height: int, pwidth: int, pheight: int) -> tuple[int, int]:\n     from math import floor\n+\n     if height > pheight:\n         corrf = pheight / float(height)\n         width, height = floor(corrf * width), pheight\n@@ -210,11 +214,7 @@ def fit_image(width: int, height: int, pwidth: int, pheight: int) -> tuple[int,\n     return int(width), int(height)\n \n \n-def base64_encode(\n-    integer: int,\n-    chars: str = string.ascii_uppercase + string.ascii_lowercase + string.digits +\n-    '+/'\n-) -> str:\n+def base64_encode(integer: int, chars: str = string.ascii_uppercase + string.ascii_lowercase + string.digits + '+/') -> str:\n     ans = ''\n     while True:\n         integer, remainder = divmod(integer, 64)\n@@ -227,6 +227,7 @@ def base64_encode(\n def command_for_open(program: str | list[str] = 'default') -> list[str]:\n     if isinstance(program, str):\n         from .conf.utils import to_cmdline\n+\n         program = to_cmdline(program)\n     if program == ['default']:\n         cmd = ['open'] if is_macos else ['xdg-open']\n@@ -235,9 +236,11 @@ def command_for_open(program: str | list[str] = 'default') -> list[str]:\n     return cmd\n \n \n-def open_cmd(cmd: Iterable[str] | list[str], arg: None | Iterable[str] | str = None,\n-             cwd: str | None = None, extra_env: dict[str, str] | None = None) -> 'PopenType[bytes]':\n+def open_cmd(\n+    cmd: Iterable[str] | list[str], arg: None | Iterable[str] | str = None, cwd: str | None = None, extra_env: dict[str, str] | None = None\n+) -> 'PopenType[bytes]':\n     import subprocess\n+\n     if arg is not None:\n         cmd = list(cmd)\n         if isinstance(arg, str):\n@@ -249,8 +252,8 @@ def open_cmd(cmd: Iterable[str] | list[str], arg: None | Iterable[str] | str = N\n         env = os.environ.copy()\n         env.update(extra_env)\n     return subprocess.Popen(\n-        tuple(cmd), stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, cwd=cwd or None,\n-        preexec_fn=clear_handled_signals, env=env)\n+        tuple(cmd), stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, cwd=cwd or None, preexec_fn=clear_handled_signals, env=env\n+    )\n \n \n def open_url(url: str, program: str | list[str] = 'default', cwd: str | None = None, extra_env: dict[str, str] | None = None) -> 'PopenType[bytes]':\n@@ -260,10 +263,12 @@ def open_url(url: str, program: str | list[str] = 'default', cwd: str | None = N\n def init_startup_notification_x11(window_handle: int, startup_id: str | None = None) -> Optional['StartupCtx']:\n     # https://specifications.freedesktop.org/startup-notification-spec/startup-notification-latest.txt\n     from kitty.fast_data_types import init_x11_startup_notification\n+\n     sid = startup_id or os.environ.pop('DESKTOP_STARTUP_ID', None)  # ensure child processes don't get this env var\n     if not sid:\n         return None\n     from .fast_data_types import x11_display\n+\n     display = x11_display()\n     if not display:\n         return None\n@@ -272,6 +277,7 @@ def init_startup_notification_x11(window_handle: int, startup_id: str | None = N\n \n def end_startup_notification_x11(ctx: 'StartupCtx') -> None:\n     from kitty.fast_data_types import end_x11_startup_notification\n+\n     end_x11_startup_notification(ctx)\n \n \n@@ -285,13 +291,15 @@ def init_startup_notification(window_handle: int | None, startup_id: str | None\n         try:\n             return init_startup_notification_x11(window_handle, startup_id)\n         except OSError as e:\n-            if not str(e).startswith(\"Failed to load libstartup-notification\"):\n+            if not str(e).startswith('Failed to load libstartup-notification'):\n                 raise e\n             log_error(\n                 f'{e}. This has two main effects:',\n-                'There will be no startup feedback and when using --single-instance, kitty windows may start on an incorrect desktop/workspace.')\n+                'There will be no startup feedback and when using --single-instance, kitty windows may start on an incorrect desktop/workspace.',\n+            )\n     except Exception:\n         import traceback\n+\n         traceback.print_exc()\n     return None\n \n@@ -305,11 +313,11 @@ def end_startup_notification(ctx: Optional['StartupCtx']) -> None:\n         end_startup_notification_x11(ctx)\n     except Exception:\n         import traceback\n+\n         traceback.print_exc()\n \n \n class startup_notification_handler:\n-\n     # WARNING: This only works on X11 on other platforms extra_callback will be called\n     # after the window is shown, not before, as they do not do two stage window\n     # creation.\n@@ -321,7 +329,6 @@ def __init__(self, do_notify: bool = True, startup_id: str | None = None, extra_\n         self.ctx: Optional['StartupCtx'] = None\n \n     def __enter__(self) -> Callable[[int], None]:\n-\n         def pre_show_callback(window_handle: int) -> None:\n             if self.extra_callback is not None:\n                 self.extra_callback(window_handle)\n@@ -337,10 +344,12 @@ def __exit__(self, *a: Any) -> None:\n \n def unix_socket_directories() -> Iterator[str]:\n     import tempfile\n+\n     home = os.path.expanduser('~')\n     candidates = [tempfile.gettempdir(), home]\n     if is_macos:\n         from .fast_data_types import user_cache_dir\n+\n         candidates = [user_cache_dir(), '/Library/Caches']\n     else:\n         if os.environ.get('XDG_RUNTIME_DIR'):\n@@ -359,6 +368,7 @@ def unix_socket_paths(name: str, ext: str = '.lock') -> Generator[str, None, Non\n \n def parse_address_spec(spec: str) -> tuple[AddressFamily, tuple[str, int] | str, str | None]:\n     import socket\n+\n     try:\n         protocol, rest = spec.split(':', 1)\n     except ValueError:\n@@ -421,7 +431,6 @@ def write_all(fd: int, data: str | bytes, block_until_written: bool = True) -> N\n \n \n class TTYIO:\n-\n     def __init__(self, read_with_timeout: bool = True):\n         self.read_with_timeout = read_with_timeout\n \n@@ -431,12 +440,14 @@ def __enter__(self) -> 'TTYIO':\n \n     def __exit__(self, *a: Any) -> None:\n         from .fast_data_types import close_tty\n+\n         close_tty(self.tty_fd, self.original_termios)\n \n     def wait_till_read_available(self) -> bool:\n         if self.read_with_timeout:\n             raise ValueError('Cannot wait when TTY is set to read with timeout')\n         import select\n+\n         rd = select.select([self.tty_fd], [], [])[0]\n         return bool(rd)\n \n@@ -463,6 +474,7 @@ def recv(self, more_needed: Callable[[bytes], bool], timeout: float, sz: int = 1\n \n def set_echo(fd: int = -1, on: bool = False) -> tuple[int, list[int | list[bytes | int]]]:\n     import termios\n+\n     if fd < 0:\n         fd = sys.stdin.fileno()\n     old = termios.tcgetattr(fd)\n@@ -478,6 +490,7 @@ def set_echo(fd: int = -1, on: bool = False) -> tuple[int, list[int | list[bytes\n @contextmanager\n def no_echo(fd: int = -1) -> Iterator[None]:\n     import termios\n+\n     fd, old = set_echo(fd)\n     try:\n         yield\n@@ -486,7 +499,6 @@ def no_echo(fd: int = -1) -> Iterator[None]:\n \n \n def natsort_ints(iterable: Iterable[str]) -> list[str]:\n-\n     def convert(text: str) -> int | str:\n         return int(text) if text.isdigit() else text\n \n@@ -498,6 +510,7 @@ def alphanum_key(key: str) -> tuple[int | str, ...]:\n \n def get_hostname(fallback: str = '') -> str:\n     import socket\n+\n     try:\n         return socket.gethostname() or fallback\n     except Exception:\n@@ -506,6 +519,7 @@ def get_hostname(fallback: str = '') -> str:\n \n def resolve_editor_cmd(editor: str, shell_env: Mapping[str, str]) -> str | None:\n     import shlex\n+\n     editor_cmd = list(shlex_split(editor))\n     editor_exe = (editor_cmd or ('',))[0]\n     if editor_exe and os.path.isabs(editor_exe):\n@@ -523,6 +537,7 @@ def patched(exe: str) -> str:\n             return patched(q)\n     elif 'PATH' in shell_env:\n         import shutil\n+\n         q = shutil.which(editor_exe, path=shell_env['PATH'])\n         if q:\n             return patched(q)\n@@ -560,6 +575,7 @@ def get_editor(opts: Options | None = None, path_to_edit: str = '', line_number:\n         except RuntimeError:\n             # we are in a kitten\n             from .cli import create_default_opts\n+\n             opts = create_default_opts()\n     if opts.editor == '.':\n         ans = get_editor_from_env_vars()\n@@ -588,6 +604,7 @@ def abspath(x: str | None) -> str:\n         return x or ''\n \n     import tempfile\n+\n     path = abspath(path)\n     candidates = frozenset(map(abspath, ('/tmp', '/dev/shm', os.environ.get('TMPDIR', None), tempfile.gettempdir())))\n     for q in candidates:\n@@ -598,6 +615,7 @@ def abspath(x: str | None) -> str:\n \n def is_ok_to_read_image_file(path: str, fd: int) -> bool:\n     import stat\n+\n     path = os.path.abspath(os.path.realpath(path))\n     try:\n         path_stat = os.stat(path, follow_symlinks=True)\n@@ -653,9 +671,12 @@ def resolved_shell(opts: Options | None = None) -> list[str]:\n             env['HOME'] = os.path.expanduser('~')\n         if 'USER' not in os.environ:\n             import pwd\n+\n             env['USER'] = pwd.getpwuid(os.geteuid()).pw_name\n+\n         def expand(x: str) -> str:\n             return expandvars(x, env)\n+\n         ans = list(map(expand, shlex_split(q)))\n     return ans\n \n@@ -676,6 +697,7 @@ def add_from_file(x: str) -> None:\n                     if os.path.isdir(line):\n                         seen.add(line)\n                         entries.append(line)\n+\n     try:\n         files = os.listdir('/etc/paths.d')\n     except (FileNotFoundError, PermissionError):\n@@ -747,9 +769,11 @@ def read_shell_environment(opts: Options | None = None) -> dict[str, str]:\n     ans: dict[str, str] | None = getattr(read_shell_environment, 'ans', None)\n     if ans is None:\n         from .child import openpty\n+\n         ans = {}\n         setattr(read_shell_environment, 'ans', ans)\n         import subprocess\n+\n         shell = resolved_shell(opts)\n         master, slave = openpty()\n         os.set_blocking(master, False)\n@@ -759,14 +783,15 @@ def read_shell_environment(opts: Options | None = None) -> dict[str, str]:\n             shell += ['-i']\n         try:\n             p = subprocess.Popen(\n-                shell + ['-c', 'env'], stdout=slave, stdin=slave, stderr=slave, start_new_session=True, close_fds=True,\n-                preexec_fn=clear_handled_signals)\n+                shell + ['-c', 'env'], stdout=slave, stdin=slave, stderr=slave, start_new_session=True, close_fds=True, preexec_fn=clear_handled_signals\n+            )\n         except FileNotFoundError:\n             log_error('Could not find shell to read environment')\n             return ans\n         with os.fdopen(master, 'rb') as stdout, os.fdopen(slave, 'wb'):\n             raw = b''\n             from time import monotonic\n+\n             start_time = monotonic()\n             while monotonic() - start_time < 1.5:\n                 try:\n@@ -800,8 +825,9 @@ def read_shell_environment(opts: Options | None = None) -> dict[str, str]:\n \n \n def parse_uri_list(text: str) -> Generator[str, None, None]:\n-    ' Get paths from file:// URLs '\n+    \"Get paths from file:// URLs\"\n     from urllib.parse import unquote, urlparse\n+\n     for line in text.splitlines():\n         if not line or line.startswith('#'):\n             continue\n@@ -819,6 +845,7 @@ def parse_uri_list(text: str) -> Generator[str, None, None]:\n \n def edit_config_file() -> None:\n     from kitty.config import prepare_config_file_for_editing\n+\n     p = prepare_config_file_for_editing()\n     editor = get_editor()\n     os.execvp(editor[0], editor + [p])\n@@ -855,6 +882,7 @@ def get_new_os_window_size(\n def get_all_processes() -> Iterable[int]:\n     if is_macos:\n         from kitty.fast_data_types import get_all_processes as f\n+\n         yield from f()\n     else:\n         for c in os.listdir('/proc'):\n@@ -906,20 +934,27 @@ def hold_till_enter() -> None:\n     import subprocess\n \n     from .constants import kitten_exe\n+\n     subprocess.Popen([kitten_exe(), '__hold_till_enter__']).wait()\n \n \n def cleanup_ssh_control_masters() -> None:\n     import glob\n     import subprocess\n+\n     try:\n-        files = frozenset(glob.glob(os.path.join(runtime_dir(), ssh_control_master_template.format(\n-            kitty_pid=os.getpid(), ssh_placeholder='*'))))\n+        files = frozenset(glob.glob(os.path.join(runtime_dir(), ssh_control_master_template.format(kitty_pid=os.getpid(), ssh_placeholder='*'))))\n     except OSError:\n         return\n-    workers = tuple(subprocess.Popen([\n-        'ssh', '-o', f'ControlPath={x}', '-O', 'exit', 'kitty-unused-host-name'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n-        preexec_fn=clear_handled_signals) for x in files)\n+    workers = tuple(\n+        subprocess.Popen(\n+            ['ssh', '-o', f'ControlPath={x}', '-O', 'exit', 'kitty-unused-host-name'],\n+            stdout=subprocess.DEVNULL,\n+            stderr=subprocess.DEVNULL,\n+            preexec_fn=clear_handled_signals,\n+        )\n+        for x in files\n+    )\n     for w in workers:\n         w.wait()\n     for x in files:\n@@ -934,6 +969,7 @@ def path_from_osc7_url(url: str | bytes) -> str:\n         return '/' + url.split('/', 3)[-1]\n     if url.startswith('file://'):\n         from urllib.parse import unquote, urlparse\n+\n         return unquote(urlparse(url).path)\n     return ''\n \n@@ -942,6 +978,7 @@ def path_from_osc7_url(url: str | bytes) -> str:\n def macos_version() -> tuple[int, ...]:\n     # platform.mac_ver does not work thanks to Apple's stupid \"hardening\", so just use sw_vers\n     import subprocess\n+\n     try:\n         o = subprocess.check_output(['sw_vers', '-productVersion'], stderr=subprocess.STDOUT).decode()\n     except Exception:\n@@ -952,6 +989,7 @@ def macos_version() -> tuple[int, ...]:\n @lru_cache(maxsize=2)\n def less_version(less_exe: str = 'less') -> int:\n     import subprocess\n+\n     o = subprocess.check_output([less_exe, '-V'], stderr=subprocess.STDOUT).decode()\n     m = re.match(r'less (\\d+)', o)\n     if m is None:\n@@ -974,10 +1012,12 @@ def safer_fork() -> int:\n     if pid:\n         # master\n         import ssl\n+\n         ssl.RAND_add(os.urandom(32), 0.0)\n     else:\n         # child\n         import atexit\n+\n         atexit._clear()\n     return pid\n \n@@ -987,6 +1027,7 @@ def docs_url(which: str = '', local_docs_root: str | None = '') -> str:\n \n     from .conf.types import resolve_ref\n     from .constants import local_docs, website_url\n+\n     if local_docs_root is None:\n         ld = ''\n     else:\n@@ -1023,6 +1064,7 @@ def sanitize_for_bracketed_paste(text: bytes) -> bytes:\n @lru_cache(maxsize=64)\n def sanitize_url_for_display_to_user(url: str) -> str:\n     from urllib.parse import unquote, urlparse, urlunparse\n+\n     try:\n         purl = urlparse(url)\n         if purl.netloc:\n@@ -1045,7 +1087,7 @@ def is_within_directory(directory: str, target: str) -> bool:\n         prefix = os.path.commonprefix((abs_directory, abs_target))\n         return prefix == abs_directory\n \n-    def safe_extract(tar: 'tarfile.TarFile', path: str = \".\", numeric_owner: bool = False) -> None:\n+    def safe_extract(tar: 'tarfile.TarFile', path: str = '.', numeric_owner: bool = False) -> None:\n         for member in tar.getmembers():\n             member_path = os.path.join(path, member.name)\n             if not is_within_directory(path, member_path):\n@@ -1069,9 +1111,11 @@ def cmdline_for_hold(cmd: Sequence[str] = (), opts: Optional['Options'] = None)\n             opts = get_options()\n     if opts is None:\n         from .options.types import defaults\n+\n         opts = defaults\n     ksi = ' '.join(opts.shell_integration)\n     import shlex\n+\n     shell = shlex.join(resolved_shell(opts))\n     return [kitten_exe(), 'run-shell', f'--shell={shell}', f'--shell-integration={ksi}', '--env=KITTY_HOLD=1'] + list(cmd)\n \n@@ -1098,8 +1142,7 @@ def get_custom_window_icon() -> tuple[float, str] | tuple[None, None]:\n \n def key_val_matcher(items: Iterable[tuple[str, str]], key_pat: 're.Pattern[str]', val_pat: Optional['re.Pattern[str]']) -> bool:\n     for key, val in items:\n-        if key_pat.search(key) is not None and (\n-                val_pat is None or val_pat.search(val) is not None):\n+        if key_pat.search(key) is not None and (val_pat is None or val_pat.search(val) is not None):\n             return True\n     return False\n \n"
    },
    {
        "id": "316",
        "sha_fail": "026bbd5ffd1394338d51c1c0c5c8209205ae33ed",
        "diff": "diff --git a/flask_admin/base.py b/flask_admin/base.py\nindex 7881744..aba10a9 100644\n--- a/flask_admin/base.py\n+++ b/flask_admin/base.py\n@@ -881,7 +881,7 @@ class Admin:\n         for p in admins:\n             if p.endpoint == self.endpoint:\n                 raise Exception(\n-                    \"Cannot have two Admin() instances with same\" \" endpoint name.\"\n+                    \"Cannot have two Admin() instances with same endpoint name.\"\n                 )\n \n             if p.url == self.url and p.subdomain == self.subdomain:\n"
    },
    {
        "id": "320",
        "sha_fail": "6066f06de3275801b19af5f23ccb5e3940991e60",
        "diff": "diff --git a/Lib/pathlib/types.py b/Lib/pathlib/types.py\nindex 559677bc..acd879be 100644\n--- a/Lib/pathlib/types.py\n+++ b/Lib/pathlib/types.py\n@@ -9,12 +9,16 @@\n #\n # Three ABCs are provided -- _JoinablePath, _ReadablePath and _WritablePath\n \n-\n from abc import ABC, abstractmethod\n from glob import _GlobberBase\n from io import text_encoding\n-from pathlib._os import (vfsopen, vfspath, ensure_distinct_paths,\n-                         ensure_different_files, copyfileobj)\n+from pathlib._os import (\n+    vfsopen,\n+    vfspath,\n+    ensure_distinct_paths,\n+    ensure_different_files,\n+    copyfileobj,\n+)\n from pathlib import PurePath, Path\n from typing import Optional, Protocol, runtime_checkable\n \n@@ -45,6 +49,7 @@ class _PathParser(Protocol):\n \n     sep: str\n     altsep: Optional[str]\n+\n     def split(self, path: str) -> tuple[str, str]: ...\n     def splitext(self, path: str) -> tuple[str, str]: ...\n     def normcase(self, path: str) -> str: ...\n@@ -55,6 +60,7 @@ class PathInfo(Protocol):\n     \"\"\"Protocol for path info objects, which support querying the file type.\n     Methods may return cached results.\n     \"\"\"\n+\n     def exists(self, *, follow_symlinks: bool = True) -> bool: ...\n     def is_dir(self, *, follow_symlinks: bool = True) -> bool: ...\n     def is_file(self, *, follow_symlinks: bool = True) -> bool: ...\n@@ -62,8 +68,7 @@ def is_symlink(self) -> bool: ...\n \n \n class _PathGlobber(_GlobberBase):\n-    \"\"\"Provides shell-style pattern matching and globbing for ReadablePath.\n-    \"\"\"\n+    \"\"\"Provides shell-style pattern matching and globbing for ReadablePath.\"\"\"\n \n     @staticmethod\n     def lexists(path):\n@@ -87,6 +92,7 @@ class _JoinablePath(ABC):\n     its implementation PurePath. They are: __init__, __fspath__, __bytes__,\n     __reduce__, __hash__, __eq__, __lt__, __le__, __gt__, __ge__.\n     \"\"\"\n+\n     __slots__ = ()\n \n     @property\n@@ -178,17 +184,17 @@ def with_suffix(self, suffix):\n         if not stem:\n             # If the stem is empty, we can't make the suffix non-empty.\n             raise ValueError(f\"{self!r} has an empty name\")\n-        elif suffix and not suffix.startswith('.'):\n+        elif suffix and not suffix.startswith(\".\"):\n             raise ValueError(f\"Invalid suffix {suffix!r}\")\n         else:\n             return self.with_name(stem + suffix)\n \n     def without_suffix(self):\n-        \"\"\"Return a new path without the file suffix.  Readable alternative \n+        \"\"\"Return a new path without the file suffix.  Readable alternative\n         for providing empty string to with_suffix.\n         \"\"\"\n-        return self.with_suffix('')\n-    \n+        return self.with_suffix(\"\")\n+\n     @property\n     def parts(self):\n         \"\"\"An object providing sequence-like access to the\n@@ -245,7 +251,7 @@ def full_match(self, pattern):\n         Return True if this path matches the given glob-style pattern. The\n         pattern is matched against the entire path.\n         \"\"\"\n-        case_sensitive = self.parser.normcase('Aa') == 'Aa'\n+        case_sensitive = self.parser.normcase(\"Aa\") == \"Aa\"\n         globber = _PathGlobber(self.parser.sep, case_sensitive, recursive=True)\n         match = globber.compile(pattern, altsep=self.parser.altsep)\n         return match(vfspath(self)) is not None\n@@ -258,6 +264,7 @@ class _ReadablePath(_JoinablePath):\n     create subclasses to implement readable virtual filesystem paths, such as\n     paths in archive files or on remote storage systems.\n     \"\"\"\n+\n     __slots__ = ()\n \n     @property\n@@ -281,7 +288,7 @@ def read_bytes(self):\n         \"\"\"\n         Open the file in bytes mode, read it, and close the file.\n         \"\"\"\n-        with vfsopen(self, mode='rb') as f:\n+        with vfsopen(self, mode=\"rb\") as f:\n             return f.read()\n \n     def read_text(self, encoding=None, errors=None, newline=None):\n@@ -291,7 +298,9 @@ def read_text(self, encoding=None, errors=None, newline=None):\n         # Call io.text_encoding() here to ensure any warning is raised at an\n         # appropriate stack level.\n         encoding = text_encoding(encoding)\n-        with vfsopen(self, mode='r', encoding=encoding, errors=errors, newline=newline) as f:\n+        with vfsopen(\n+            self, mode=\"r\", encoding=encoding, errors=errors, newline=newline\n+        ) as f:\n             return f.read()\n \n     @abstractmethod\n@@ -314,10 +323,10 @@ def glob(self, pattern, *, recurse_symlinks=True):\n             raise ValueError(f\"Unacceptable pattern: {pattern!r}\")\n         elif not recurse_symlinks:\n             raise NotImplementedError(\"recurse_symlinks=False is unsupported\")\n-        case_sensitive = self.parser.normcase('Aa') == 'Aa'\n+        case_sensitive = self.parser.normcase(\"Aa\") == \"Aa\"\n         globber = _PathGlobber(self.parser.sep, case_sensitive, recursive=True)\n         select = globber.selector(parts)\n-        return select(self.joinpath(''))\n+        return select(self.joinpath(\"\"))\n \n     def walk(self, top_down=True, on_error=None, follow_symlinks=False):\n         \"\"\"Walk the directory tree from this directory, similar to os.walk().\"\"\"\n@@ -382,6 +391,7 @@ class _WritablePath(_JoinablePath):\n     create subclasses to implement writable virtual filesystem paths, such as\n     paths in archive files or on remote storage systems.\n     \"\"\"\n+\n     __slots__ = ()\n \n     @abstractmethod\n@@ -413,7 +423,7 @@ def write_bytes(self, data):\n         \"\"\"\n         # type-check for the buffer interface before truncating the file\n         view = memoryview(data)\n-        with vfsopen(self, mode='wb') as f:\n+        with vfsopen(self, mode=\"wb\") as f:\n             return f.write(view)\n \n     def write_text(self, data, encoding=None, errors=None, newline=None):\n@@ -424,9 +434,12 @@ def write_text(self, data, encoding=None, errors=None, newline=None):\n         # appropriate stack level.\n         encoding = text_encoding(encoding)\n         if not isinstance(data, str):\n-            raise TypeError('data must be str, not %s' %\n-                            data.__class__.__name__)\n-        with vfsopen(self, mode='w', encoding=encoding, errors=errors, newline=newline) as f:\n+            raise TypeError(\n+                \"data must be str, not %s\" % data.__class__.__name__\n+            )\n+        with vfsopen(\n+            self, mode=\"w\", encoding=encoding, errors=errors, newline=newline\n+        ) as f:\n             return f.write(data)\n \n     def _copy_from(self, source, follow_symlinks=True):\n@@ -445,8 +458,8 @@ def _copy_from(self, source, follow_symlinks=True):\n                     stack.append((child, dst.joinpath(child.name)))\n             else:\n                 ensure_different_files(src, dst)\n-                with vfsopen(src, 'rb') as source_f:\n-                    with vfsopen(dst, 'wb') as target_f:\n+                with vfsopen(src, \"rb\") as source_f:\n+                    with vfsopen(dst, \"wb\") as target_f:\n                         copyfileobj(source_f, target_f)\n \n \n"
    },
    {
        "id": "322",
        "sha_fail": "d01a6ec49f1aac932816c81c0354de64c0183373",
        "diff": "diff --git a/lib/core/ncgui.py b/lib/core/ncgui.py\nindex 708f021..440cedb 100644\n--- a/lib/core/ncgui.py\n+++ b/lib/core/ncgui.py\n@@ -5,7 +5,7 @@ Copyright (c) 2006-2025 sqlmap developers (https://sqlmap.org)\n See the file 'LICENSE' for copying permission\n \"\"\"\n \n-import curses\n+# import curses\n import os\n import subprocess\n import sys\n@@ -19,9 +19,9 @@ from lib.core.enums import MKSTEMP_PREFIX\n from lib.core.exception import SqlmapMissingDependence\n from lib.core.exception import SqlmapSystemException\n from lib.core.settings import IS_WIN\n-from thirdparty.six.moves import queue as _queue\n from thirdparty.six.moves import configparser as _configparser\n \n+\n class NcursesUI:\n     def __init__(self, stdscr, parser):\n         self.stdscr = stdscr\n@@ -37,13 +37,13 @@ class NcursesUI:\n \n         # Initialize colors\n         curses.start_color()\n-        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_CYAN)    # Header\n-        curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_BLUE)    # Active tab\n-        curses.init_pair(3, curses.COLOR_BLACK, curses.COLOR_WHITE)   # Inactive tab\n+        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_CYAN)  # Header\n+        curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_BLUE)  # Active tab\n+        curses.init_pair(3, curses.COLOR_BLACK, curses.COLOR_WHITE)  # Inactive tab\n         curses.init_pair(4, curses.COLOR_YELLOW, curses.COLOR_BLACK)  # Selected field\n-        curses.init_pair(5, curses.COLOR_GREEN, curses.COLOR_BLACK)   # Help text\n-        curses.init_pair(6, curses.COLOR_RED, curses.COLOR_BLACK)     # Error/Important\n-        curses.init_pair(7, curses.COLOR_CYAN, curses.COLOR_BLACK)    # Label\n+        curses.init_pair(5, curses.COLOR_GREEN, curses.COLOR_BLACK)  # Help text\n+        curses.init_pair(6, curses.COLOR_RED, curses.COLOR_BLACK)  # Error/Important\n+        curses.init_pair(7, curses.COLOR_CYAN, curses.COLOR_BLACK)  # Label\n \n         # Setup curses\n         curses.curs_set(1)\n@@ -56,21 +56,27 @@ class NcursesUI:\n         \"\"\"Parse command line options into tabs and fields\"\"\"\n         for group in self.parser.option_groups:\n             tab_data = {\n-                'title': group.title,\n-                'description': group.get_description() if hasattr(group, 'get_description') and group.get_description() else \"\",\n-                'options': []\n+                \"title\": group.title,\n+                \"description\": group.get_description()\n+                if hasattr(group, \"get_description\") and group.get_description()\n+                else \"\",\n+                \"options\": [],\n             }\n \n             for option in group.option_list:\n                 field_data = {\n-                    'dest': option.dest,\n-                    'label': self._format_option_strings(option),\n-                    'help': option.help if option.help else \"\",\n-                    'type': option.type if hasattr(option, 'type') and option.type else 'bool',\n-                    'value': '',\n-                    'default': defaults.get(option.dest) if defaults.get(option.dest) else None\n+                    \"dest\": option.dest,\n+                    \"label\": self._format_option_strings(option),\n+                    \"help\": option.help if option.help else \"\",\n+                    \"type\": option.type\n+                    if hasattr(option, \"type\") and option.type\n+                    else \"bool\",\n+                    \"value\": \"\",\n+                    \"default\": defaults.get(option.dest)\n+                    if defaults.get(option.dest)\n+                    else None,\n                 }\n-                tab_data['options'].append(field_data)\n+                tab_data[\"options\"].append(field_data)\n                 self.fields[(group.title, option.dest)] = field_data\n \n             self.tabs.append(tab_data)\n@@ -78,11 +84,11 @@ class NcursesUI:\n     def _format_option_strings(self, option):\n         \"\"\"Format option strings for display\"\"\"\n         parts = []\n-        if hasattr(option, '_short_opts') and option._short_opts:\n+        if hasattr(option, \"_short_opts\") and option._short_opts:\n             parts.extend(option._short_opts)\n-        if hasattr(option, '_long_opts') and option._long_opts:\n+        if hasattr(option, \"_long_opts\") and option._long_opts:\n             parts.extend(option._long_opts)\n-        return ', '.join(parts)\n+        return \", \".join(parts)\n \n     def _draw_header(self):\n         \"\"\"Draw the header bar\"\"\"\n@@ -99,7 +105,7 @@ class NcursesUI:\n         x = 0\n \n         for i, tab in enumerate(self.tabs):\n-            tab_text = \" %s \" % tab['title']\n+            tab_text = \" %s \" % tab[\"title\"]\n \n             # Check if tab exceeds width, wrap to next line\n             if x + len(tab_text) >= width:\n@@ -120,7 +126,7 @@ class NcursesUI:\n         x = 0\n \n         for i, tab in enumerate(self.tabs):\n-            tab_text = \" %s \" % tab['title']\n+            tab_text = \" %s \" % tab[\"title\"]\n \n             # Check if tab exceeds width, wrap to next line\n             if x + len(tab_text) >= width:\n@@ -178,8 +184,8 @@ class NcursesUI:\n         y = start_y\n \n         # Draw description if exists\n-        if tab['description']:\n-            desc_lines = self._wrap_text(tab['description'], width - 4)\n+        if tab[\"description\"]:\n+            desc_lines = self._wrap_text(tab[\"description\"], width - 4)\n             for line in desc_lines[:2]:  # Limit to 2 lines\n                 try:\n                     self.stdscr.attron(curses.color_pair(5))\n@@ -194,14 +200,16 @@ class NcursesUI:\n         visible_start = self.scroll_offset\n         visible_end = visible_start + (height - y - 2)\n \n-        for i, option in enumerate(tab['options'][visible_start:visible_end], visible_start):\n+        for i, option in enumerate(\n+            tab[\"options\"][visible_start:visible_end], visible_start\n+        ):\n             if y >= height - 2:\n                 break\n \n-            is_selected = (i == self.current_field)\n+            is_selected = i == self.current_field\n \n             # Draw label\n-            label = option['label'][:25].ljust(25)\n+            label = option[\"label\"][:25].ljust(25)\n             try:\n                 if is_selected:\n                     self.stdscr.attron(curses.color_pair(4) | curses.A_BOLD)\n@@ -219,12 +227,12 @@ class NcursesUI:\n \n             # Draw value\n             value_str = \"\"\n-            if option['type'] == 'bool':\n-                value_str = \"[X]\" if option['value'] else \"[ ]\"\n+            if option[\"type\"] == \"bool\":\n+                value_str = \"[X]\" if option[\"value\"] else \"[ ]\"\n             else:\n-                value_str = str(option['value']) if option['value'] else \"\"\n-                if option['default'] and not option['value']:\n-                    value_str = \"(%s)\" % str(option['default'])\n+                value_str = str(option[\"value\"]) if option[\"value\"] else \"\"\n+                if option[\"default\"] and not option[\"value\"]:\n+                    value_str = \"(%s)\" % str(option[\"default\"])\n \n             value_str = value_str[:30]\n \n@@ -239,7 +247,7 @@ class NcursesUI:\n \n             # Draw help text\n             if width > 65:\n-                help_text = option['help'][:width-62] if option['help'] else \"\"\n+                help_text = option[\"help\"][: width - 62] if option[\"help\"] else \"\"\n                 try:\n                     self.stdscr.attron(curses.color_pair(5))\n                     self.stdscr.addstr(y, 60, help_text)\n@@ -250,7 +258,7 @@ class NcursesUI:\n             y += 1\n \n         # Draw scroll indicator\n-        if len(tab['options']) > visible_end - visible_start:\n+        if len(tab[\"options\"]) > visible_end - visible_start:\n             try:\n                 self.stdscr.attron(curses.color_pair(6))\n                 self.stdscr.addstr(height - 2, width - 10, \"[More...]\")\n@@ -280,14 +288,14 @@ class NcursesUI:\n     def _edit_field(self):\n         \"\"\"Edit the current field\"\"\"\n         tab = self.tabs[self.current_tab]\n-        if self.current_field >= len(tab['options']):\n+        if self.current_field >= len(tab[\"options\"]):\n             return\n \n-        option = tab['options'][self.current_field]\n+        option = tab[\"options\"][self.current_field]\n \n-        if option['type'] == 'bool':\n+        if option[\"type\"] == \"bool\":\n             # Toggle boolean\n-            option['value'] = not option['value']\n+            option[\"value\"] = not option[\"value\"]\n         else:\n             # Text input\n             height, width = self.stdscr.getmaxyx()\n@@ -296,7 +304,7 @@ class NcursesUI:\n             input_win = curses.newwin(5, width - 20, height // 2 - 2, 10)\n             input_win.box()\n             input_win.attron(curses.color_pair(2))\n-            input_win.addstr(0, 2, \" Edit %s \" % option['label'][:20])\n+            input_win.addstr(0, 2, \" Edit %s \" % option[\"label\"][:20])\n             input_win.attroff(curses.color_pair(2))\n             input_win.addstr(2, 2, \"Value:\")\n             input_win.refresh()\n@@ -306,26 +314,26 @@ class NcursesUI:\n             curses.curs_set(1)\n \n             # Pre-fill with existing value\n-            current_value = str(option['value']) if option['value'] else \"\"\n+            current_value = str(option[\"value\"]) if option[\"value\"] else \"\"\n             input_win.addstr(2, 9, current_value)\n             input_win.move(2, 9)\n \n             try:\n-                new_value = input_win.getstr(2, 9, width - 32).decode('utf-8')\n+                new_value = input_win.getstr(2, 9, width - 32).decode(\"utf-8\")\n \n                 # Validate and convert based on type\n-                if option['type'] == 'int':\n+                if option[\"type\"] == \"int\":\n                     try:\n-                        option['value'] = int(new_value) if new_value else None\n+                        option[\"value\"] = int(new_value) if new_value else None\n                     except ValueError:\n-                        option['value'] = None\n-                elif option['type'] == 'float':\n+                        option[\"value\"] = None\n+                elif option[\"type\"] == \"float\":\n                     try:\n-                        option['value'] = float(new_value) if new_value else None\n+                        option[\"value\"] = float(new_value) if new_value else None\n                     except ValueError:\n-                        option['value'] = None\n+                        option[\"value\"] = None\n                 else:\n-                    option['value'] = new_value if new_value else None\n+                    option[\"value\"] = new_value if new_value else None\n             except:\n                 pass\n \n@@ -355,21 +363,25 @@ class NcursesUI:\n         curses.curs_set(1)\n \n         try:\n-            filename = input_win.getstr(2, 8, width - 32).decode('utf-8').strip()\n+            filename = input_win.getstr(2, 8, width - 32).decode(\"utf-8\").strip()\n \n             if filename:\n                 # Collect all field values\n                 config = {}\n                 for tab in self.tabs:\n-                    for option in tab['options']:\n-                        dest = option['dest']\n-                        value = option['value'] if option['value'] else option.get('default')\n-\n-                        if option['type'] == 'bool':\n+                    for option in tab[\"options\"]:\n+                        dest = option[\"dest\"]\n+                        value = (\n+                            option[\"value\"]\n+                            if option[\"value\"]\n+                            else option.get(\"default\")\n+                        )\n+\n+                        if option[\"type\"] == \"bool\":\n                             config[dest] = bool(value)\n-                        elif option['type'] == 'int':\n+                        elif option[\"type\"] == \"int\":\n                             config[dest] = int(value) if value else None\n-                        elif option['type'] == 'float':\n+                        elif option[\"type\"] == \"float\":\n                             config[dest] = float(value) if value else None\n                         else:\n                             config[dest] = value\n@@ -390,7 +402,7 @@ class NcursesUI:\n                     input_win.addstr(0, 2, \" Export Successful \")\n                     input_win.attroff(curses.color_pair(5))\n                     input_win.addstr(2, 2, \"Configuration exported to:\")\n-                    input_win.addstr(3, 2, filename[:width - 26])\n+                    input_win.addstr(3, 2, filename[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n                 except Exception as ex:\n@@ -400,7 +412,7 @@ class NcursesUI:\n                     input_win.attron(curses.color_pair(6))\n                     input_win.addstr(0, 2, \" Export Failed \")\n                     input_win.attroff(curses.color_pair(6))\n-                    input_win.addstr(2, 2, str(getSafeExString(ex))[:width - 26])\n+                    input_win.addstr(2, 2, str(getSafeExString(ex))[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n         except:\n@@ -432,7 +444,7 @@ class NcursesUI:\n         curses.curs_set(1)\n \n         try:\n-            filename = input_win.getstr(2, 8, width - 32).decode('utf-8').strip()\n+            filename = input_win.getstr(2, 8, width - 32).decode(\"utf-8\").strip()\n \n             if filename and os.path.isfile(filename):\n                 try:\n@@ -444,8 +456,8 @@ class NcursesUI:\n \n                     # Load values into fields\n                     for tab in self.tabs:\n-                        for option in tab['options']:\n-                            dest = option['dest']\n+                        for option in tab[\"options\"]:\n+                            dest = option[\"dest\"]\n \n                             # Search for option in all sections\n                             for section in config.sections():\n@@ -453,20 +465,29 @@ class NcursesUI:\n                                     value = config.get(section, dest)\n \n                                     # Convert based on type\n-                                    if option['type'] == 'bool':\n-                                        option['value'] = value.lower() in ('true', '1', 'yes', 'on')\n-                                    elif option['type'] == 'int':\n+                                    if option[\"type\"] == \"bool\":\n+                                        option[\"value\"] = value.lower() in (\n+                                            \"true\",\n+                                            \"1\",\n+                                            \"yes\",\n+                                            \"on\",\n+                                        )\n+                                    elif option[\"type\"] == \"int\":\n                                         try:\n-                                            option['value'] = int(value) if value else None\n+                                            option[\"value\"] = (\n+                                                int(value) if value else None\n+                                            )\n                                         except ValueError:\n-                                            option['value'] = None\n-                                    elif option['type'] == 'float':\n+                                            option[\"value\"] = None\n+                                    elif option[\"type\"] == \"float\":\n                                         try:\n-                                            option['value'] = float(value) if value else None\n+                                            option[\"value\"] = (\n+                                                float(value) if value else None\n+                                            )\n                                         except ValueError:\n-                                            option['value'] = None\n+                                            option[\"value\"] = None\n                                     else:\n-                                        option['value'] = value if value else None\n+                                        option[\"value\"] = value if value else None\n \n                                     imported_count += 1\n                                     break\n@@ -478,7 +499,7 @@ class NcursesUI:\n                     input_win.addstr(0, 2, \" Import Successful \")\n                     input_win.attroff(curses.color_pair(5))\n                     input_win.addstr(2, 2, \"Imported %d options from:\" % imported_count)\n-                    input_win.addstr(3, 2, filename[:width - 26])\n+                    input_win.addstr(3, 2, filename[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n \n@@ -489,7 +510,7 @@ class NcursesUI:\n                     input_win.attron(curses.color_pair(6))\n                     input_win.addstr(0, 2, \" Import Failed \")\n                     input_win.attroff(curses.color_pair(6))\n-                    input_win.addstr(2, 2, str(getSafeExString(ex))[:width - 26])\n+                    input_win.addstr(2, 2, str(getSafeExString(ex))[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n             elif filename:\n@@ -500,7 +521,7 @@ class NcursesUI:\n                 input_win.addstr(0, 2, \" File Not Found \")\n                 input_win.attroff(curses.color_pair(6))\n                 input_win.addstr(2, 2, \"File does not exist:\")\n-                input_win.addstr(3, 2, filename[:width - 26])\n+                input_win.addstr(3, 2, filename[: width - 26])\n                 input_win.refresh()\n                 curses.napms(2000)\n         except:\n@@ -520,15 +541,15 @@ class NcursesUI:\n \n         # Collect all field values\n         for tab in self.tabs:\n-            for option in tab['options']:\n-                dest = option['dest']\n-                value = option['value'] if option['value'] else option.get('default')\n+            for option in tab[\"options\"]:\n+                dest = option[\"dest\"]\n+                value = option[\"value\"] if option[\"value\"] else option.get(\"default\")\n \n-                if option['type'] == 'bool':\n+                if option[\"type\"] == \"bool\":\n                     config[dest] = bool(value)\n-                elif option['type'] == 'int':\n+                elif option[\"type\"] == \"int\":\n                     config[dest] = int(value) if value else None\n-                elif option['type'] == 'float':\n+                elif option[\"type\"] == \"float\":\n                     config[dest] = float(value) if value else None\n                 else:\n                     config[dest] = value\n@@ -567,17 +588,23 @@ class NcursesUI:\n         # Start sqlmap process\n         try:\n             process = subprocess.Popen(\n-                [sys.executable or \"python\", os.path.join(paths.SQLMAP_ROOT_PATH, \"sqlmap.py\"), \"-c\", configFile],\n+                [\n+                    sys.executable or \"python\",\n+                    os.path.join(paths.SQLMAP_ROOT_PATH, \"sqlmap.py\"),\n+                    \"-c\",\n+                    configFile,\n+                ],\n                 shell=False,\n                 stdout=subprocess.PIPE,\n                 stderr=subprocess.STDOUT,\n                 stdin=subprocess.PIPE,\n                 bufsize=1,\n-                close_fds=not IS_WIN\n+                close_fds=not IS_WIN,\n             )\n \n             # Make it non-blocking\n             import fcntl\n+\n             flags = fcntl.fcntl(process.stdout, fcntl.F_GETFL)\n             fcntl.fcntl(process.stdout, fcntl.F_SETFL, flags | os.O_NONBLOCK)\n \n@@ -591,7 +618,7 @@ class NcursesUI:\n                 # Check for user input\n                 try:\n                     key = console_win.getch()\n-                    if key in (ord('q'), ord('Q')):\n+                    if key in (ord(\"q\"), ord(\"Q\")):\n                         # Kill process\n                         process.terminate()\n                         break\n@@ -599,7 +626,7 @@ class NcursesUI:\n                         # Send newline to process\n                         if process.poll() is None:\n                             try:\n-                                process.stdin.write(b'\\n')\n+                                process.stdin.write(b\"\\n\")\n                                 process.stdin.flush()\n                             except:\n                                 pass\n@@ -610,11 +637,11 @@ class NcursesUI:\n                 try:\n                     chunk = process.stdout.read(1024)\n                     if chunk:\n-                        current_line += chunk.decode('utf-8', errors='ignore')\n+                        current_line += chunk.decode(\"utf-8\", errors=\"ignore\")\n \n                         # Split into lines\n-                        while '\\n' in current_line:\n-                            line, current_line = current_line.split('\\n', 1)\n+                        while \"\\n\" in current_line:\n+                            line, current_line = current_line.split(\"\\n\", 1)\n                             lines.append(line)\n \n                             # Keep only last N lines\n@@ -626,7 +653,7 @@ class NcursesUI:\n                             start_line = max(0, len(lines) - (height - 10))\n                             for i, l in enumerate(lines[start_line:]):\n                                 try:\n-                                    output_win.addstr(i, 0, l[:width-10])\n+                                    output_win.addstr(i, 0, l[: width - 10])\n                                 except:\n                                     pass\n                             output_win.refresh()\n@@ -640,8 +667,8 @@ class NcursesUI:\n                     try:\n                         remaining = process.stdout.read()\n                         if remaining:\n-                            current_line += remaining.decode('utf-8', errors='ignore')\n-                            for line in current_line.split('\\n'):\n+                            current_line += remaining.decode(\"utf-8\", errors=\"ignore\")\n+                            for line in current_line.split(\"\\n\"):\n                                 if line:\n                                     lines.append(line)\n                     except:\n@@ -652,11 +679,13 @@ class NcursesUI:\n                     start_line = max(0, len(lines) - (height - 10))\n                     for i, l in enumerate(lines[start_line:]):\n                         try:\n-                            output_win.addstr(i, 0, l[:width-10])\n+                            output_win.addstr(i, 0, l[: width - 10])\n                         except:\n                             pass\n \n-                    output_win.addstr(height - 9, 0, \"--- Process finished. Press Q to close ---\")\n+                    output_win.addstr(\n+                        height - 9, 0, \"--- Process finished. Press Q to close ---\"\n+                    )\n                     output_win.refresh()\n                     console_win.refresh()\n \n@@ -664,7 +693,7 @@ class NcursesUI:\n                     console_win.nodelay(False)\n                     while True:\n                         key = console_win.getch()\n-                        if key in (ord('q'), ord('Q')):\n+                        if key in (ord(\"q\"), ord(\"Q\")):\n                             break\n \n                     break\n@@ -711,7 +740,7 @@ class NcursesUI:\n             # Handle input\n             if key == curses.KEY_F10 or key == 27:  # F10 or ESC\n                 break\n-            elif key == ord('\\t') or key == curses.KEY_RIGHT:  # Tab or Right arrow\n+            elif key == ord(\"\\t\") or key == curses.KEY_RIGHT:  # Tab or Right arrow\n                 self.current_tab = (self.current_tab + 1) % len(self.tabs)\n                 self.current_field = 0\n                 self.scroll_offset = 0\n@@ -726,7 +755,7 @@ class NcursesUI:\n                     if self.current_field < self.scroll_offset:\n                         self.scroll_offset = self.current_field\n             elif key == curses.KEY_DOWN:  # Down arrow\n-                if self.current_field < len(tab['options']) - 1:\n+                if self.current_field < len(tab[\"options\"]) - 1:\n                     self.current_field += 1\n                     # Adjust scroll if needed\n                     height, width = self.stdscr.getmaxyx()\n@@ -741,10 +770,11 @@ class NcursesUI:\n                 self._export_config()\n             elif key == curses.KEY_F4:  # F4 to import\n                 self._import_config()\n-            elif key == ord(' '):  # Space for boolean toggle\n-                option = tab['options'][self.current_field]\n-                if option['type'] == 'bool':\n-                    option['value'] = not option['value']\n+            elif key == ord(\" \"):  # Space for boolean toggle\n+                option = tab[\"options\"][self.current_field]\n+                if option[\"type\"] == \"bool\":\n+                    option[\"value\"] = not option[\"value\"]\n+\n \n def runNcGui(parser):\n     \"\"\"Main entry point for ncurses GUI\"\"\"\n@@ -752,7 +782,9 @@ def runNcGui(parser):\n         # Check if ncurses is available\n         import curses\n     except ImportError:\n-        raise SqlmapMissingDependence(\"missing 'curses' module (try installing 'windows-curses' on Windows)\")\n+        raise SqlmapMissingDependence(\n+            \"missing 'curses' module (try installing 'windows-curses' on Windows)\"\n+        )\n \n     try:\n         # Initialize and run\n"
    },
    {
        "id": "323",
        "sha_fail": "71b7fd4a926acb2c018af855f325502bfe03d417",
        "diff": "diff --git a/dspy/clients/lm.py b/dspy/clients/lm.py\nindex f1cebd5..acd0fe5 100644\n--- a/dspy/clients/lm.py\n+++ b/dspy/clients/lm.py\n@@ -13,7 +13,7 @@ import dspy\n from dspy.clients.cache import request_cache\n from dspy.clients.openai import OpenAIProvider\n from dspy.clients.provider import Provider, ReinforceJob, TrainingJob\n-from dspy.clients.utils_finetune import TrainDataFormat, MultiGPUConfig\n+from dspy.clients.utils_finetune import MultiGPUConfig, TrainDataFormat\n from dspy.dsp.utils.settings import settings\n from dspy.utils.callback import BaseCallback\n \n@@ -104,11 +104,7 @@ class LM(BaseLM):\n         self._warn_zero_temp_rollout(self.kwargs.get(\"temperature\"), self.kwargs.get(\"rollout_id\"))\n \n     def _warn_zero_temp_rollout(self, temperature: float | None, rollout_id):\n-        if (\n-            not self._warned_zero_temp_rollout\n-            and rollout_id is not None\n-            and (temperature is None or temperature == 0)\n-        ):\n+        if not self._warned_zero_temp_rollout and rollout_id is not None and (temperature is None or temperature == 0):\n             warnings.warn(\n                 \"rollout_id has no effect when temperature=0; set temperature>0 to bypass the cache.\",\n                 stacklevel=3,\n@@ -134,10 +130,7 @@ class LM(BaseLM):\n \n         messages = messages or [{\"role\": \"user\", \"content\": prompt}]\n         if self.use_developer_role and self.model_type == \"responses\":\n-            messages = [\n-                {**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m\n-                for m in messages\n-            ]\n+            messages = [{**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m for m in messages]\n         kwargs = {**self.kwargs, **kwargs}\n         self._warn_zero_temp_rollout(kwargs.get(\"temperature\"), kwargs.get(\"rollout_id\"))\n         if kwargs.get(\"rollout_id\") is None:\n@@ -170,10 +163,7 @@ class LM(BaseLM):\n \n         messages = messages or [{\"role\": \"user\", \"content\": prompt}]\n         if self.use_developer_role and self.model_type == \"responses\":\n-            messages = [\n-                {**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m\n-                for m in messages\n-            ]\n+            messages = [{**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m for m in messages]\n         kwargs = {**self.kwargs, **kwargs}\n         self._warn_zero_temp_rollout(kwargs.get(\"temperature\"), kwargs.get(\"rollout_id\"))\n         if kwargs.get(\"rollout_id\") is None:\n@@ -237,7 +227,9 @@ class LM(BaseLM):\n \n         return job\n \n-    def reinforce(self, train_kwargs, gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1)) -> ReinforceJob:\n+    def reinforce(\n+        self, train_kwargs, gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1)\n+    ) -> ReinforceJob:\n         # TODO(GRPO Team): Should we return an initialized job here?\n         from dspy import settings as settings\n \n@@ -424,6 +416,7 @@ async def alitellm_text_completion(request: dict[str, Any], num_retries: int, ca\n         **request,\n     )\n \n+\n def litellm_responses_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None):\n     cache = cache or {\"no-cache\": True, \"no-store\": True}\n     request = dict(request)\n@@ -451,6 +444,7 @@ async def alitellm_responses_completion(request: dict[str, Any], num_retries: in\n         **request,\n     )\n \n+\n def _convert_chat_request_to_responses_request(request: dict[str, Any]):\n     request = dict(request)\n     if \"messages\" in request:\n"
    },
    {
        "id": "324",
        "sha_fail": "a7b902770d8b2769920794cf4e2016525e3ea1d9",
        "diff": "diff --git a/tests/dialects/test_clickhouse.py b/tests/dialects/test_clickhouse.py\nindex 7d7b252..a5bd004 100644\n--- a/tests/dialects/test_clickhouse.py\n+++ b/tests/dialects/test_clickhouse.py\n@@ -12,11 +12,15 @@ class TestClickhouse(Validator):\n     dialect = \"clickhouse\"\n \n     def test_clickhouse(self):\n-        expr = quote_identifiers(self.parse_one(\"{start_date:String}\"), dialect=\"clickhouse\")\n+        expr = quote_identifiers(\n+            self.parse_one(\"{start_date:String}\"), dialect=\"clickhouse\"\n+        )\n         self.assertEqual(expr.sql(\"clickhouse\"), \"{start_date: String}\")\n \n         for string_type_enum in ClickHouse.Generator.STRING_TYPE_MAPPING:\n-            self.validate_identity(f\"CAST(x AS {string_type_enum.value})\", \"CAST(x AS String)\")\n+            self.validate_identity(\n+                f\"CAST(x AS {string_type_enum.value})\", \"CAST(x AS String)\"\n+            )\n \n         # Arrays, maps and tuples can't be Nullable in ClickHouse\n         for non_nullable_type in (\"ARRAY<INT>\", \"MAP<INT, INT>\", \"STRUCT(a: INT)\"):\n@@ -24,10 +28,23 @@ class TestClickhouse(Validator):\n             target_type = try_cast.to.sql(\"clickhouse\")\n             self.assertEqual(try_cast.sql(\"clickhouse\"), f\"CAST(x AS {target_type})\")\n \n-        for nullable_type in (\"INT\", \"UINT\", \"BIGINT\", \"FLOAT\", \"DOUBLE\", \"TEXT\", \"DATE\", \"UUID\"):\n+        for nullable_type in (\n+            \"INT\",\n+            \"UINT\",\n+            \"BIGINT\",\n+            \"FLOAT\",\n+            \"DOUBLE\",\n+            \"TEXT\",\n+            \"DATE\",\n+            \"UUID\",\n+        ):\n             try_cast = parse_one(f\"TRY_CAST(x AS {nullable_type})\")\n-            target_type = exp.DataType.build(nullable_type, dialect=\"clickhouse\").sql(\"clickhouse\")\n-            self.assertEqual(try_cast.sql(\"clickhouse\"), f\"CAST(x AS Nullable({target_type}))\")\n+            target_type = exp.DataType.build(nullable_type, dialect=\"clickhouse\").sql(\n+                \"clickhouse\"\n+            )\n+            self.assertEqual(\n+                try_cast.sql(\"clickhouse\"), f\"CAST(x AS Nullable({target_type}))\"\n+            )\n \n         expr = parse_one(\"count(x)\")\n         self.assertEqual(expr.sql(dialect=\"clickhouse\"), \"COUNT(x)\")\n@@ -40,8 +57,12 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"SELECT json.a.:JSON.b.:Int64\")\n         self.validate_identity(\"WITH arrayJoin([(1, [2, 3])]) AS arr SELECT arr\")\n         self.validate_identity(\"CAST(1 AS Bool)\")\n-        self.validate_identity(\"SELECT toString(CHAR(104.1, 101, 108.9, 108.9, 111, 32))\")\n-        self.validate_identity(\"@macro\").assert_is(exp.Parameter).this.assert_is(exp.Var)\n+        self.validate_identity(\n+            \"SELECT toString(CHAR(104.1, 101, 108.9, 108.9, 111, 32))\"\n+        )\n+        self.validate_identity(\"@macro\").assert_is(exp.Parameter).this.assert_is(\n+            exp.Var\n+        )\n         self.validate_identity(\"SELECT toFloat(like)\")\n         self.validate_identity(\"SELECT like\")\n         self.validate_identity(\"SELECT STR_TO_DATE(str, fmt, tz)\")\n@@ -49,14 +70,20 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"SELECT EXTRACT(YEAR FROM toDateTime('2023-02-01'))\")\n         self.validate_identity(\"extract(haystack, pattern)\")\n         self.validate_identity(\"SELECT * FROM x LIMIT 1 UNION ALL SELECT * FROM y\")\n-        self.validate_identity(\"SELECT CAST(x AS Tuple(String, Array(Nullable(Float64))))\")\n+        self.validate_identity(\n+            \"SELECT CAST(x AS Tuple(String, Array(Nullable(Float64))))\"\n+        )\n         self.validate_identity(\"countIf(x, y)\")\n         self.validate_identity(\"x = y\")\n         self.validate_identity(\"x <> y\")\n         self.validate_identity(\"SELECT * FROM (SELECT a FROM b SAMPLE 0.01)\")\n-        self.validate_identity(\"SELECT * FROM (SELECT a FROM b SAMPLE 1 / 10 OFFSET 1 / 2)\")\n+        self.validate_identity(\n+            \"SELECT * FROM (SELECT a FROM b SAMPLE 1 / 10 OFFSET 1 / 2)\"\n+        )\n         self.validate_identity(\"SELECT sum(foo * bar) FROM bla SAMPLE 10000000\")\n-        self.validate_identity(\"CAST(x AS Nested(ID UInt32, Serial UInt32, EventTime DateTime))\")\n+        self.validate_identity(\n+            \"CAST(x AS Nested(ID UInt32, Serial UInt32, EventTime DateTime))\"\n+        )\n         self.validate_identity(\"CAST(x AS Enum('hello' = 1, 'world' = 2))\")\n         self.validate_identity(\"CAST(x AS Enum('hello', 'world'))\")\n         self.validate_identity(\"CAST(x AS Enum('hello' = 1, 'world'))\")\n@@ -92,17 +119,27 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"SELECT histogram(5)(a)\")\n         self.validate_identity(\"SELECT groupUniqArray(2)(a)\")\n         self.validate_identity(\"SELECT exponentialTimeDecayedAvg(60)(a, b)\")\n-        self.validate_identity(\"levenshteinDistance(col1, col2)\", \"editDistance(col1, col2)\")\n-        self.validate_identity(\"SELECT * FROM foo WHERE x GLOBAL IN (SELECT * FROM bar)\")\n-        self.validate_identity(\"SELECT * FROM foo WHERE x GLOBAL NOT IN (SELECT * FROM bar)\")\n+        self.validate_identity(\n+            \"levenshteinDistance(col1, col2)\", \"editDistance(col1, col2)\"\n+        )\n+        self.validate_identity(\n+            \"SELECT * FROM foo WHERE x GLOBAL IN (SELECT * FROM bar)\"\n+        )\n+        self.validate_identity(\n+            \"SELECT * FROM foo WHERE x GLOBAL NOT IN (SELECT * FROM bar)\"\n+        )\n         self.validate_identity(\"POSITION(haystack, needle)\")\n         self.validate_identity(\"POSITION(haystack, needle, position)\")\n         self.validate_identity(\"CAST(x AS DATETIME)\", \"CAST(x AS DateTime)\")\n         self.validate_identity(\"CAST(x AS TIMESTAMPTZ)\", \"CAST(x AS DateTime)\")\n         self.validate_identity(\"CAST(x as MEDIUMINT)\", \"CAST(x AS Int32)\")\n         self.validate_identity(\"CAST(x AS DECIMAL(38, 2))\", \"CAST(x AS Decimal(38, 2))\")\n-        self.validate_identity(\"SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src\")\n-        self.validate_identity(\"\"\"SELECT JSONExtractString('{\"x\": {\"y\": 1}}', 'x', 'y')\"\"\")\n+        self.validate_identity(\n+            \"SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src\"\n+        )\n+        self.validate_identity(\n+            \"\"\"SELECT JSONExtractString('{\"x\": {\"y\": 1}}', 'x', 'y')\"\"\"\n+        )\n         self.validate_identity(\"SELECT * FROM table LIMIT 1 BY a, b\")\n         self.validate_identity(\"SELECT * FROM table LIMIT 2 OFFSET 1 BY a, b\")\n         self.validate_identity(\"TRUNCATE TABLE t1 ON CLUSTER test_cluster\")\n@@ -110,7 +147,9 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"TRUNCATE DATABASE db\")\n         self.validate_identity(\"TRUNCATE DATABASE db ON CLUSTER test_cluster\")\n         self.validate_identity(\"TRUNCATE DATABASE db ON CLUSTER '{cluster}'\")\n-        self.validate_identity(\"EXCHANGE TABLES x.a AND y.b\", check_command_warning=True)\n+        self.validate_identity(\n+            \"EXCHANGE TABLES x.a AND y.b\", check_command_warning=True\n+        )\n         self.validate_identity(\"CREATE TABLE test (id UInt8) ENGINE=Null()\")\n         self.validate_identity(\n             \"SELECT * FROM foo ORDER BY bar OFFSET 0 ROWS FETCH NEXT 10 ROWS WITH TIES\"\n@@ -483,7 +522,9 @@ class TestClickhouse(Validator):\n         self.validate_identity(\n             \"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result = 'break'\"\n         )\n-        self.validate_identity(\"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result_\")\n+        self.validate_identity(\n+            \"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result_\"\n+        )\n         self.validate_identity(\"SELECT * FROM x FORMAT PrettyCompact\")\n         self.validate_identity(\n             \"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result_ FORMAT PrettyCompact\"\n@@ -494,11 +535,15 @@ class TestClickhouse(Validator):\n         )\n         self.validate_all(\n             \"SELECT * FROM foo ANY LEFT JOIN bla ON foo.c1 = bla.c2\",\n-            write={\"clickhouse\": \"SELECT * FROM foo LEFT ANY JOIN bla ON foo.c1 = bla.c2\"},\n+            write={\n+                \"clickhouse\": \"SELECT * FROM foo LEFT ANY JOIN bla ON foo.c1 = bla.c2\"\n+            },\n         )\n         self.validate_all(\n             \"SELECT * FROM foo GLOBAL ANY LEFT JOIN bla ON foo.c1 = bla.c2\",\n-            write={\"clickhouse\": \"SELECT * FROM foo GLOBAL LEFT ANY JOIN bla ON foo.c1 = bla.c2\"},\n+            write={\n+                \"clickhouse\": \"SELECT * FROM foo GLOBAL LEFT ANY JOIN bla ON foo.c1 = bla.c2\"\n+            },\n         )\n         self.validate_all(\n             \"\"\"\n@@ -585,19 +630,34 @@ class TestClickhouse(Validator):\n         )\n         self.validate_identity(\"ALTER TABLE visits DROP PARTITION ID '201901'\")\n \n-        self.validate_identity(\"ALTER TABLE visits REPLACE PARTITION 201901 FROM visits_tmp\")\n-        self.validate_identity(\"ALTER TABLE visits REPLACE PARTITION ALL FROM visits_tmp\")\n+        self.validate_identity(\n+            \"ALTER TABLE visits REPLACE PARTITION 201901 FROM visits_tmp\"\n+        )\n+        self.validate_identity(\n+            \"ALTER TABLE visits REPLACE PARTITION ALL FROM visits_tmp\"\n+        )\n         self.validate_identity(\n             \"ALTER TABLE visits REPLACE PARTITION tuple(toYYYYMM(toDate('2019-01-25'))) FROM visits_tmp\"\n         )\n-        self.validate_identity(\"ALTER TABLE visits REPLACE PARTITION ID '201901' FROM visits_tmp\")\n-        self.validate_identity(\"ALTER TABLE visits ON CLUSTER test_cluster DROP COLUMN col1\")\n-        self.validate_identity(\"ALTER TABLE visits ON CLUSTER '{cluster}' DROP COLUMN col1\")\n-        self.validate_identity(\"DELETE FROM tbl ON CLUSTER test_cluster WHERE date = '2019-01-01'\")\n-        self.validate_identity(\"DELETE FROM tbl ON CLUSTER '{cluster}' WHERE date = '2019-01-01'\")\n+        self.validate_identity(\n+            \"ALTER TABLE visits REPLACE PARTITION ID '201901' FROM visits_tmp\"\n+        )\n+        self.validate_identity(\n+            \"ALTER TABLE visits ON CLUSTER test_cluster DROP COLUMN col1\"\n+        )\n+        self.validate_identity(\n+            \"ALTER TABLE visits ON CLUSTER '{cluster}' DROP COLUMN col1\"\n+        )\n+        self.validate_identity(\n+            \"DELETE FROM tbl ON CLUSTER test_cluster WHERE date = '2019-01-01'\"\n+        )\n+        self.validate_identity(\n+            \"DELETE FROM tbl ON CLUSTER '{cluster}' WHERE date = '2019-01-01'\"\n+        )\n \n         self.assertIsInstance(\n-            parse_one(\"Tuple(select Int64)\", into=exp.DataType, read=\"clickhouse\"), exp.DataType\n+            parse_one(\"Tuple(select Int64)\", into=exp.DataType, read=\"clickhouse\"),\n+            exp.DataType,\n         )\n \n         self.validate_identity(\n@@ -616,11 +676,15 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"current_timestamp\").assert_is(exp.Column)\n \n         self.validate_identity(\"SELECT * APPLY(sum) FROM columns_transformers\")\n-        self.validate_identity(\"SELECT COLUMNS('[jk]') APPLY(toString) FROM columns_transformers\")\n+        self.validate_identity(\n+            \"SELECT COLUMNS('[jk]') APPLY(toString) FROM columns_transformers\"\n+        )\n         self.validate_identity(\n             \"SELECT COLUMNS('[jk]') APPLY(toString) APPLY(length) APPLY(max) FROM columns_transformers\"\n         )\n-        self.validate_identity(\"SELECT * APPLY(sum), COLUMNS('col') APPLY(sum) APPLY(avg) FROM t\")\n+        self.validate_identity(\n+            \"SELECT * APPLY(sum), COLUMNS('col') APPLY(sum) APPLY(avg) FROM t\"\n+        )\n         self.validate_identity(\n             \"SELECT * FROM ABC WHERE hasAny(COLUMNS('.*field') APPLY(toUInt64) APPLY(to), (SELECT groupUniqArray(toUInt64(field))))\"\n         )\n@@ -637,7 +701,9 @@ class TestClickhouse(Validator):\n         )\n         self.validate_identity(\"SELECT arrayConcat([1, 2], [3, 4])\")\n \n-        self.validate_identity(\"SELECT parseDateTime('2021-01-04+23:00:00', '%Y-%m-%d+%H:%i:%s')\")\n+        self.validate_identity(\n+            \"SELECT parseDateTime('2021-01-04+23:00:00', '%Y-%m-%d+%H:%i:%s')\"\n+        )\n         self.validate_identity(\n             \"SELECT parseDateTime('2021-01-04+23:00:00', '%Y-%m-%d+%H:%i:%s', 'Asia/Istanbul')\"\n         )\n@@ -686,7 +752,9 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"WITH ['c'] AS field_names SELECT field_names\")\n         self.validate_identity(\"WITH SUM(bytes) AS foo SELECT foo FROM system.parts\")\n         self.validate_identity(\"WITH (SELECT foo) AS bar SELECT bar + 5\")\n-        self.validate_identity(\"WITH test1 AS (SELECT i + 1, j + 1 FROM test1) SELECT * FROM test1\")\n+        self.validate_identity(\n+            \"WITH test1 AS (SELECT i + 1, j + 1 FROM test1) SELECT * FROM test1\"\n+        )\n \n         query = parse_one(\"\"\"WITH (SELECT 1) AS y SELECT * FROM y\"\"\", read=\"clickhouse\")\n         self.assertIsInstance(query.args[\"with\"].expressions[0].this, exp.Subquery)\n@@ -695,14 +763,18 @@ class TestClickhouse(Validator):\n         query = \"WITH 1 AS var SELECT var\"\n         for error_level in [ErrorLevel.IGNORE, ErrorLevel.RAISE, ErrorLevel.IMMEDIATE]:\n             self.assertEqual(\n-                self.parse_one(query, error_level=error_level).sql(dialect=self.dialect),\n+                self.parse_one(query, error_level=error_level).sql(\n+                    dialect=self.dialect\n+                ),\n                 query,\n             )\n \n         self.validate_identity(\"arraySlice(x, 1)\")\n \n     def test_ternary(self):\n-        self.validate_all(\"x ? 1 : 2\", write={\"clickhouse\": \"CASE WHEN x THEN 1 ELSE 2 END\"})\n+        self.validate_all(\n+            \"x ? 1 : 2\", write={\"clickhouse\": \"CASE WHEN x THEN 1 ELSE 2 END\"}\n+        )\n         self.validate_all(\n             \"IF(BAR(col), sign > 0 ? FOO() : 0, 1)\",\n             write={\n@@ -715,7 +787,9 @@ class TestClickhouse(Validator):\n         )\n         self.validate_all(\n             \"x ? (y ? 1 : 2) : 3\",\n-            write={\"clickhouse\": \"CASE WHEN x THEN (CASE WHEN y THEN 1 ELSE 2 END) ELSE 3 END\"},\n+            write={\n+                \"clickhouse\": \"CASE WHEN x THEN (CASE WHEN y THEN 1 ELSE 2 END) ELSE 3 END\"\n+            },\n         )\n         self.validate_all(\n             \"x AND (foo() ? FALSE : TRUE) ? (y ? 1 : 2) : 3\",\n@@ -737,7 +811,9 @@ class TestClickhouse(Validator):\n         self.assertIsInstance(nested_ternary.args[\"true\"], exp.Literal)\n         self.assertIsInstance(nested_ternary.args[\"false\"], exp.Literal)\n \n-        parse_one(\"a and b ? 1 : 2\", read=\"clickhouse\").assert_is(exp.If).this.assert_is(exp.And)\n+        parse_one(\"a and b ? 1 : 2\", read=\"clickhouse\").assert_is(\n+            exp.If\n+        ).this.assert_is(exp.And)\n \n     def test_parameterization(self):\n         self.validate_all(\n@@ -774,7 +850,14 @@ class TestClickhouse(Validator):\n             )\n \n     def test_geom_types(self):\n-        data_types = [\"Point\", \"Ring\", \"LineString\", \"MultiLineString\", \"Polygon\", \"MultiPolygon\"]\n+        data_types = [\n+            \"Point\",\n+            \"Ring\",\n+            \"LineString\",\n+            \"MultiLineString\",\n+            \"Polygon\",\n+            \"MultiPolygon\",\n+        ]\n         for data_type in data_types:\n             with self.subTest(f\"Casting to ClickHouse {data_type}\"):\n                 self.validate_identity(f\"SELECT CAST(val AS {data_type})\")\n@@ -828,17 +911,25 @@ ORDER BY (\n         create_with_cluster = exp.Create(\n             this=db_table_expr,\n             kind=\"DATABASE\",\n-            properties=exp.Properties(expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]),\n+            properties=exp.Properties(\n+                expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]\n+            ),\n+        )\n+        self.assertEqual(\n+            create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\"\n         )\n-        self.assertEqual(create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\")\n \n         # Transpiled CREATE SCHEMA may have OnCluster property set\n         create_with_cluster = exp.Create(\n             this=db_table_expr,\n             kind=\"SCHEMA\",\n-            properties=exp.Properties(expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]),\n+            properties=exp.Properties(\n+                expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]\n+            ),\n+        )\n+        self.assertEqual(\n+            create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\"\n         )\n-        self.assertEqual(create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\")\n \n         ctas_with_comment = exp.Create(\n             this=exp.table_(\"foo\"),\n@@ -856,12 +947,22 @@ ORDER BY (\n             \"CREATE TABLE foo ENGINE=Memory AS (SELECT * FROM db.other_table) COMMENT 'foo'\",\n         )\n \n-        self.validate_identity(\"CREATE FUNCTION linear_equation AS (x, k, b) -> k * x + b\")\n-        self.validate_identity(\"CREATE MATERIALIZED VIEW a.b TO a.c (c Int32) AS SELECT * FROM a.d\")\n-        self.validate_identity(\"\"\"CREATE TABLE ip_data (ip4 IPv4, ip6 IPv6) ENGINE=TinyLog()\"\"\")\n+        self.validate_identity(\n+            \"CREATE FUNCTION linear_equation AS (x, k, b) -> k * x + b\"\n+        )\n+        self.validate_identity(\n+            \"CREATE MATERIALIZED VIEW a.b TO a.c (c Int32) AS SELECT * FROM a.d\"\n+        )\n+        self.validate_identity(\n+            \"\"\"CREATE TABLE ip_data (ip4 IPv4, ip6 IPv6) ENGINE=TinyLog()\"\"\"\n+        )\n         self.validate_identity(\"\"\"CREATE TABLE dates (dt1 Date32) ENGINE=TinyLog()\"\"\")\n-        self.validate_identity(\"CREATE TABLE named_tuples (a Tuple(select String, i Int64))\")\n-        self.validate_identity(\"\"\"CREATE TABLE t (a String) EMPTY AS SELECT * FROM dummy\"\"\")\n+        self.validate_identity(\n+            \"CREATE TABLE named_tuples (a Tuple(select String, i Int64))\"\n+        )\n+        self.validate_identity(\n+            \"\"\"CREATE TABLE t (a String) EMPTY AS SELECT * FROM dummy\"\"\"\n+        )\n         self.validate_identity(\n             \"CREATE TABLE t1 (a String EPHEMERAL, b String EPHEMERAL func(), c String MATERIALIZED func(), d String ALIAS func()) ENGINE=TinyLog()\"\n         )\n@@ -1217,9 +1318,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         )\n \n         self.assertIsNotNone(\n-            self.validate_identity(\"CREATE TABLE t1 (a String MATERIALIZED func())\").find(\n-                exp.ColumnConstraint\n-            )\n+            self.validate_identity(\n+                \"CREATE TABLE t1 (a String MATERIALIZED func())\"\n+            ).find(exp.ColumnConstraint)\n         )\n \n     def test_agg_functions(self):\n@@ -1227,7 +1328,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n             return parse_one(query, read=\"clickhouse\").selects[0].this\n \n         self.assertIsInstance(\n-            extract_agg_func(\"select quantileGK(100, 0.95) OVER (PARTITION BY id) FROM table\"),\n+            extract_agg_func(\n+                \"select quantileGK(100, 0.95) OVER (PARTITION BY id) FROM table\"\n+            ),\n             exp.AnonymousAggFunc,\n         )\n         self.assertIsInstance(\n@@ -1237,7 +1340,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n             exp.ParameterizedAgg,\n         )\n         self.assertIsInstance(\n-            extract_agg_func(\"select quantileGKIf(100, 0.95) OVER (PARTITION BY id) FROM table\"),\n+            extract_agg_func(\n+                \"select quantileGKIf(100, 0.95) OVER (PARTITION BY id) FROM table\"\n+            ),\n             exp.CombinedAggFunc,\n         )\n         self.assertIsInstance(\n@@ -1253,7 +1358,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         for creatable in (\"DATABASE\", \"TABLE\", \"VIEW\", \"DICTIONARY\", \"FUNCTION\"):\n             with self.subTest(f\"Test DROP {creatable} ON CLUSTER\"):\n                 self.validate_identity(f\"DROP {creatable} test ON CLUSTER test_cluster\")\n-                self.validate_identity(f\"DROP {creatable} test ON CLUSTER '{{cluster}}'\")\n+                self.validate_identity(\n+                    f\"DROP {creatable} test ON CLUSTER '{{cluster}}'\"\n+                )\n \n     def test_datetime_funcs(self):\n         # Each datetime func has an alias that is roundtripped to the original name e.g. (DATE_SUB, DATESUB) -> DATE_SUB\n@@ -1269,7 +1376,11 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n                 )\n \n         # 3-arg functions of type <func>(unit, value, date)\n-        for func in (*datetime_funcs, (\"DATE_DIFF\", \"DATEDIFF\"), (\"TIMESTAMP_SUB\", \"TIMESTAMPSUB\")):\n+        for func in (\n+            *datetime_funcs,\n+            (\"DATE_DIFF\", \"DATEDIFF\"),\n+            (\"TIMESTAMP_SUB\", \"TIMESTAMPSUB\"),\n+        ):\n             func_name = func[0]\n             for func_alias in func:\n                 with self.subTest(f\"Test 3-arg date-time function {func_alias}\"):\n@@ -1298,7 +1409,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n             \"CAST('2020-01-01 00:00:01' AS DateTime64(6))\",\n         )\n         self.assertEqual(\n-            convert(datetime(2020, 1, 1, 0, 0, 1, tzinfo=timezone.utc)).sql(dialect=self.dialect),\n+            convert(datetime(2020, 1, 1, 0, 0, 1, tzinfo=timezone.utc)).sql(\n+                dialect=self.dialect\n+            ),\n             \"CAST('2020-01-01 00:00:01' AS DateTime64(6, 'UTC'))\",\n         )\n \n@@ -1379,7 +1492,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n                 )\n \n     def test_grant(self):\n-        self.validate_identity(\"GRANT SELECT(x, y) ON db.table TO john WITH GRANT OPTION\")\n+        self.validate_identity(\n+            \"GRANT SELECT(x, y) ON db.table TO john WITH GRANT OPTION\"\n+        )\n         self.validate_identity(\"GRANT INSERT(x, y) ON db.table TO john\")\n \n     def test_array_join(self):\n@@ -1401,7 +1516,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         self.assertIsInstance(join.expressions[1].this, exp.Array)\n \n         self.validate_identity(\"SELECT s, arr FROM arrays_test ARRAY JOIN arr\")\n-        self.validate_identity(\"SELECT s, arr, a FROM arrays_test LEFT ARRAY JOIN arr AS a\")\n+        self.validate_identity(\n+            \"SELECT s, arr, a FROM arrays_test LEFT ARRAY JOIN arr AS a\"\n+        )\n         self.validate_identity(\n             \"SELECT s, arr_external FROM arrays_test ARRAY JOIN [1, 2, 3] AS arr_external\"\n         )\n@@ -1424,7 +1541,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         )\n \n     def test_functions(self):\n-        self.validate_identity(\"SELECT TRANSFORM(foo, [1, 2], ['first', 'second']) FROM table\")\n+        self.validate_identity(\n+            \"SELECT TRANSFORM(foo, [1, 2], ['first', 'second']) FROM table\"\n+        )\n         self.validate_identity(\n             \"SELECT TRANSFORM(foo, [1, 2], ['first', 'second'], 'default') FROM table\"\n         )\n"
    },
    {
        "id": "327",
        "sha_fail": "c7fbe73582650fe0c431fad4d0e290caa3efb3bb",
        "diff": "diff --git a/build.py b/build.py\nindex 1145a8d..ecb08e1 100755\n--- a/build.py\n+++ b/build.py\n@@ -297,7 +297,7 @@ class BuildScript:\n         env_args = []\n         for k in (\"TRT_VERSION\", \"CMAKE_TOOLCHAIN_FILE\", \"VCPKG_TARGET_TRIPLET\"):\n             env_args += [f'\"-D{k}={self.envvar_ref(k)}\"']\n-        self.cmd(f'cmake {\" \".join(env_args)} {\" \".join(args)}', check_exitcode=True)\n+        self.cmd(f\"cmake {' '.join(env_args)} {' '.join(args)}\", check_exitcode=True)\n \n     def makeinstall(self, target=\"install\"):\n         verbose_flag = \"-v\" if self._verbose else \"\"\n@@ -329,7 +329,7 @@ class BuildScript:\n             self.cmd(\"}\" if target_platform() == \"windows\" else \"fi\")\n             self.cwd(subdir)\n             self.cmd(f\"git fetch origin {tag}:tritonbuildref\", check_exitcode=True)\n-            self.cmd(f\"git checkout tritonbuildref\", check_exitcode=True)\n+            self.cmd(\"git checkout tritonbuildref\", check_exitcode=True)\n         else:\n             self.cmd(\n                 f\"  git clone --recursive --single-branch --depth=1 -b {tag} {org}/{repo}.git {subdir}; git --git-dir {subdir}/.git log --oneline -1\",\n@@ -861,9 +861,7 @@ RUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/\n     && dnf install --assumeyes \\\\\n                  datacenter-gpu-manager-4-core=1:{} \\\\\n                  datacenter-gpu-manager-4-devel=1:{}\n-\"\"\".format(\n-                    dcgm_version, dcgm_version, dcgm_version\n-                )\n+\"\"\".format(dcgm_version, dcgm_version, dcgm_version)\n             else:\n                 return \"\"\"\n ENV DCGM_VERSION {}\n@@ -873,9 +871,7 @@ RUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/\n     && dnf install --assumeyes \\\\\n                  datacenter-gpu-manager-4-core=1:{} \\\\\n                  datacenter-gpu-manager-4-devel=1:{}\n-\"\"\".format(\n-                    dcgm_version, dcgm_version, dcgm_version\n-                )\n+\"\"\".format(dcgm_version, dcgm_version, dcgm_version)\n         else:\n             if target_machine == \"aarch64\":\n                 return \"\"\"\n@@ -889,9 +885,7 @@ RUN curl -o /tmp/cuda-keyring.deb \\\\\n       && apt install --yes --no-install-recommends \\\\\n                   datacenter-gpu-manager-4-core=1:{} \\\\\n                   datacenter-gpu-manager-4-dev=1:{}\n-\"\"\".format(\n-                    dcgm_version, dcgm_version, dcgm_version\n-                )\n+\"\"\".format(dcgm_version, dcgm_version, dcgm_version)\n             else:\n                 return \"\"\"\n ENV DCGM_VERSION {}\n@@ -904,9 +898,7 @@ RUN curl -o /tmp/cuda-keyring.deb \\\\\n       && apt install --yes --no-install-recommends \\\\\n                    datacenter-gpu-manager-4-core=1:{} \\\\\n                    datacenter-gpu-manager-4-dev=1:{}\n-\"\"\".format(\n-                    dcgm_version, dcgm_version, dcgm_version\n-                )\n+\"\"\".format(dcgm_version, dcgm_version, dcgm_version)\n \n \n def create_dockerfile_buildbase_rhel(ddir, dockerfile_name, argmap):\n@@ -982,9 +974,7 @@ ENV CCACHE_REMOTE_ONLY=\"true\" \\\\\n     CMAKE_CUDA_COMPILER_LAUNCHER=\"ccache\"\n \n RUN ccache -p\n-\"\"\".format(\n-            os.getenv(\"CCACHE_REMOTE_STORAGE\")\n-        )\n+\"\"\".format(os.getenv(\"CCACHE_REMOTE_STORAGE\"))\n     # Requires openssl-devel to be installed first for pyenv build to be successful\n     df += change_default_python_version_rhel(FLAGS.rhel_py_version)\n     df += \"\"\"\n@@ -1138,9 +1128,7 @@ ENV CCACHE_REMOTE_ONLY=\"true\" \\\\\n RUN apt-get update \\\\\n       && apt-get install -y --no-install-recommends ccache && ccache -p \\\\\n       && rm -rf /var/lib/apt/lists/*\n-\"\"\".format(\n-            os.getenv(\"CCACHE_REMOTE_STORAGE\")\n-        )\n+\"\"\".format(os.getenv(\"CCACHE_REMOTE_STORAGE\"))\n \n     # Copy in the triton source. We remove existing contents first in\n     # case the FROM container has something there already.\n@@ -1216,9 +1204,7 @@ ARG BASE_IMAGE={}\n ############################################################################\n FROM {} AS min_container\n \n-\"\"\".format(\n-            argmap[\"GPU_BASE_IMAGE\"]\n-        )\n+\"\"\".format(argmap[\"GPU_BASE_IMAGE\"])\n \n     df += \"\"\"\n ############################################################################\n@@ -1342,9 +1328,7 @@ RUN userdel tensorrt-server > /dev/null 2>&1 || true \\\\\n         fi \\\\\n       && [ `id -u $TRITON_SERVER_USER` -eq 1000 ] \\\\\n       && [ `id -g $TRITON_SERVER_USER` -eq 1000 ]\n-\"\"\".format(\n-        gpu_enabled=gpu_enabled\n-    )\n+\"\"\".format(gpu_enabled=gpu_enabled)\n \n     if target_platform() == \"rhel\":\n         df += \"\"\"\n@@ -1389,9 +1373,7 @@ RUN apt-get update \\\\\n               {backend_dependencies} \\\\\n               python3-pip \\\\\n       && rm -rf /var/lib/apt/lists/*\n-\"\"\".format(\n-            backend_dependencies=backend_dependencies\n-        )\n+\"\"\".format(backend_dependencies=backend_dependencies)\n \n     df += \"\"\"\n # Set TCMALLOC_RELEASE_RATE for users setting LD_PRELOAD with tcmalloc\n@@ -1583,9 +1565,7 @@ RUN apt-get update \\\\\n RUN pip3 install patchelf==0.17.2\n \n ENV LD_LIBRARY_PATH /usr/local/cuda/targets/{cuda_arch}-linux/lib:/usr/local/cuda/lib64/stubs:${{LD_LIBRARY_PATH}}\n-\"\"\".format(\n-            cuda_arch=cuda_arch, libs_arch=libs_arch\n-        )\n+\"\"\".format(cuda_arch=cuda_arch, libs_arch=libs_arch)\n \n     if \"pytorch\" in backends:\n         # Add NCCL dependency for pytorch backend.\n@@ -1595,9 +1575,7 @@ ENV LD_LIBRARY_PATH /usr/local/cuda/targets/{cuda_arch}-linux/lib:/usr/local/cud\n         # we must copy it from the Triton min container ourselves.\n         df += \"\"\"\n COPY --from=min_container /usr/lib/{libs_arch}-linux-gnu/libnccl.so.2 /usr/lib/{libs_arch}-linux-gnu/libnccl.so.2\n-\"\"\".format(\n-            libs_arch=libs_arch\n-        )\n+\"\"\".format(libs_arch=libs_arch)\n \n     return df\n \n@@ -1894,9 +1872,9 @@ def create_docker_build_script(script_name, container_install_dir, container_ci_\n         if secrets:\n             finalargs += [\n                 f\"--secret id=req,src={requirements}\",\n-                f\"--secret id=VLLM_INDEX_URL\",\n-                f\"--secret id=PYTORCH_TRITON_URL\",\n-                f\"--secret id=NVPL_SLIM_URL\",\n+                \"--secret id=VLLM_INDEX_URL\",\n+                \"--secret id=PYTORCH_TRITON_URL\",\n+                \"--secret id=NVPL_SLIM_URL\",\n                 f\"--build-arg BUILD_PUBLIC_VLLM={build_public_vllm}\",\n             ]\n         finalargs += [\ndiff --git a/compose.py b/compose.py\nindex e2321cf..1bbaa8f 100755\n--- a/compose.py\n+++ b/compose.py\n@@ -67,9 +67,7 @@ ARG TRITON_VERSION={}\n ARG TRITON_CONTAINER_VERSION={}\n \n FROM {} AS full\n-\"\"\".format(\n-        argmap[\"TRITON_VERSION\"], argmap[\"TRITON_CONTAINER_VERSION\"], images[\"full\"]\n-    )\n+\"\"\".format(argmap[\"TRITON_VERSION\"], argmap[\"TRITON_CONTAINER_VERSION\"], images[\"full\"])\n \n     # PyTorch backends need extra CUDA and other\n     # dependencies during runtime that are missing in the CPU-only base container.\n@@ -78,17 +76,13 @@ FROM {} AS full\n         df += \"\"\"\n FROM {} AS min_container\n \n-\"\"\".format(\n-            images[\"gpu-min\"]\n-        )\n+\"\"\".format(images[\"gpu-min\"])\n \n     df += \"\"\"\n FROM {}\n \n ENV PIP_BREAK_SYSTEM_PACKAGES=1\n-\"\"\".format(\n-        images[\"min\"]\n-    )\n+\"\"\".format(images[\"min\"])\n \n     import build\n \n@@ -113,9 +107,7 @@ def add_requested_backends(ddir, dockerfile_name, backends):\n     df = \"# Copying over backends \\n\"\n     for backend in backends:\n         df += \"\"\"COPY --chown=1000:1000 --from=full /opt/tritonserver/backends/{} /opt/tritonserver/backends/{}\n-\"\"\".format(\n-            backend, backend\n-        )\n+\"\"\".format(backend, backend)\n     if len(backends) > 0:\n         df += \"\"\"\n # Top-level /opt/tritonserver/backends not copied so need to explicitly set permissions here\n@@ -129,9 +121,7 @@ def add_requested_repoagents(ddir, dockerfile_name, repoagents):\n     df = \"#  Copying over repoagents \\n\"\n     for ra in repoagents:\n         df += \"\"\"COPY --chown=1000:1000 --from=full /opt/tritonserver/repoagents/{} /opt/tritonserver/repoagents/{}\n-\"\"\".format(\n-            ra, ra\n-        )\n+\"\"\".format(ra, ra)\n     if len(repoagents) > 0:\n         df += \"\"\"\n # Top-level /opt/tritonserver/repoagents not copied so need to explicitly set permissions here\n@@ -145,9 +135,7 @@ def add_requested_caches(ddir, dockerfile_name, caches):\n     df = \"#  Copying over caches \\n\"\n     for cache in caches:\n         df += \"\"\"COPY --chown=1000:1000 --from=full /opt/tritonserver/caches/{} /opt/tritonserver/caches/{}\n-\"\"\".format(\n-            cache, cache\n-        )\n+\"\"\".format(cache, cache)\n     if len(caches) > 0:\n         df += \"\"\"\n # Top-level /opt/tritonserver/caches not copied so need to explicitly set permissions here\ndiff --git a/deploy/mlflow-triton-plugin/mlflow_triton/deployments.py b/deploy/mlflow-triton-plugin/mlflow_triton/deployments.py\nindex bebe559..4e0a7c1 100755\n--- a/deploy/mlflow-triton-plugin/mlflow_triton/deployments.py\n+++ b/deploy/mlflow-triton-plugin/mlflow_triton/deployments.py\n@@ -378,9 +378,7 @@ class TritonPlugin(BaseDeploymentClient):\n                 config = \"\"\"\n backend: \"onnxruntime\"\n default_model_filename: \"{}\"\n-\"\"\".format(\n-                    model_file\n-                )\n+\"\"\".format(model_file)\n                 with open(\n                     os.path.join(triton_deployment_dir, \"config.pbtxt\"), \"w\"\n                 ) as cfile:\ndiff --git a/docs/conf.py b/docs/conf.py\nindex 0b44f7c..1b5cf44 100755\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -44,10 +44,7 @@ from datetime import date\n # documentation root, use os.path.abspath to make it absolute, like shown here.\n #\n import httplib2\n-import nvidia_sphinx_theme\n-from docutils import nodes\n from packaging.version import Version\n-from sphinx import search\n \n # import sys\n # sys.path.insert(0, os.path.abspath('.'))\ndiff --git a/docs/generate_docs.py b/docs/generate_docs.py\nindex b7abe51..a4d20f5 100755\n--- a/docs/generate_docs.py\n+++ b/docs/generate_docs.py\n@@ -31,7 +31,6 @@ import logging\n import os\n import re\n import subprocess\n-from collections import defaultdict\n from functools import partial\n \n # Global constants\ndiff --git a/qa/L0_lifecycle/lifecycle_test.py b/qa/L0_lifecycle/lifecycle_test.py\nindex 2dba1c2..c243fd8 100755\n--- a/qa/L0_lifecycle/lifecycle_test.py\n+++ b/qa/L0_lifecycle/lifecycle_test.py\n@@ -2361,9 +2361,7 @@ class LifeCycleTest(tu.TestResultCollector):\n                         model_name,\n                         config=\"\"\"\n {{\"backend\":\"{backend}\",\"version_policy\":{{\"specific\" : {{ \"versions\": [2] }} }} }}\n-\"\"\".format(\n-                            backend=base[1]\n-                        ),\n+\"\"\".format(backend=base[1]),\n                     )\n                 except Exception as ex:\n                     self.assertTrue(False, \"unexpected error {}\".format(ex))\n@@ -3362,7 +3360,6 @@ class LifeCycleTest(tu.TestResultCollector):\n     def test_shutdown_with_live_connection(self):\n         model_name = \"add_sub\"\n         model_shape = (16,)\n-        from geventhttpclient.response import HTTPConnectionClosed\n \n         input_data = np.ones(shape=model_shape, dtype=np.float32)\n         inputs = [\n"
    }
]