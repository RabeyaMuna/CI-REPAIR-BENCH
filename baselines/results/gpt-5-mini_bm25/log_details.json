[
    {
        "sha_fail": "3137ef65975fc93e9e82b130e545028223cef408",
        "error_context": [
            "The isort check failed: isort reported that imports in optuna/study/_multi_objective.py are incorrectly sorted/formatted, causing the isort step to exit with code 1. The log shows an isort diff where a single import 'from optuna.trial import FrozenTrial, TrialState' was split into two lines, and the run ended with 'Process completed with exit code 1.'"
        ],
        "relevant_files": [
            {
                "file": "optuna/study/_multi_objective.py",
                "line_number": null,
                "reason": "Log contains: \"ERROR: /home/runner/work/optuna/optuna/optuna/study/_multi_objective.py Imports are incorrectly sorted and/or formatted.\" and the accompanying diff shows changed import lines (e.g. splitting 'from optuna.trial import FrozenTrial, TrialState' into separate lines), linking this file directly to the isort failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import sorting (isort)",
                "evidence": "Context shows the isort command ('isort . --check --diff') and the error: \"ERROR: /home/runner/work/optuna/optuna/optuna/study/_multi_objective.py Imports are incorrectly sorted and/or formatted.\" plus the diff hunk showing import reordering."
            }
        ],
        "failed_job": [
            {
                "job": "checks",
                "step": "isort",
                "command": "isort . --check --diff"
            }
        ]
    },
    {
        "sha_fail": "616eb3b10db94cf4a4c209377f36b2ce995bd01c",
        "error_context": [
            "The pre-commit action failed because one or more hooks modified tracked files (pre-commit reported \"All changes made by hooks:\" and produced a diff). The diff shows a formatting change in tests/core/tests/test_admin_integration.py around line 680 (replacement of a single-line assertNumQueries(7) with a multi-line with self.assertNumQueries(\\n            7\\n        ):). The step exited with code 1, causing the job to fail."
        ],
        "relevant_files": [
            {
                "file": "tests/core/tests/test_admin_integration.py",
                "line_number": 680,
                "reason": "Pre-commit output contains a diff for this file: \"diff --git a/tests/core/tests/test_admin_integration.py b/tests/core/tests/test_admin_integration.py\" and the hunk header \"@@ -680,7 +680,9 @@\" plus removed line \"-        with self.assertNumQueries(7):\" and added lines showing the hook reformatted it to a multi-line with self.assertNumQueries(...)."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Pre-commit hook modified file / auto-formatting change",
                "evidence": "\"All changes made by hooks:\" followed by a git diff (e.g. \"diff --git a/tests/core/tests/test_admin_integration.py b/tests/core/tests/test_admin_integration.py\" and \"@@ -680,7 +680,9 @@\") and the final message \"##[error]Process completed with exit code 1.\" indicating pre-commit failed because hooks changed files."
            }
        ],
        "failed_job": [
            {
                "job": "main",
                "step": "main/4_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit/action@v3.0.0"
            }
        ]
    },
    {
        "sha_fail": "d4ca3713b196de2aee52fd0344d0eb9a9eaada64",
        "error_context": [
            "The pre-commit step failed because the flake8 hook reported a lint error (F401 unused import) and pre-commit exited with code 1. The failing message points to tests/core/tests/test_resources/test_resources.py at line 4: \"F401 'copy.deepcopy' imported but unused.\"",
            "This failure occurred while running the repository's pre-commit action (pre-commit/action@v3.0.0), which runs multiple hooks (black, isort, flake8); black and isort passed but flake8 failed, causing the job to fail."
        ],
        "relevant_files": [
            {
                "file": "tests/core/tests/test_resources/test_resources.py",
                "line_number": 4,
                "reason": "Log shows a flake8 error pointing to this file and line: \"tests/core/tests/test_resources/test_resources.py:4:1: F401 'copy.deepcopy' imported but unused\" \u2013 the F401 unused-import is the reported failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting",
                "subcategory": "Unused import (flake8 F401)",
                "evidence": "Context: \"flake8...................................................................Failed\" and \"- hook id: flake8\" plus the error line: \"tests/core/tests/test_resources/test_resources.py:4:1: F401 'copy.deepcopy' imported but unused\""
            }
        ],
        "failed_job": [
            {
                "job": "main",
                "step": "Run pre-commit action (pre-commit/action@v3.0.0)",
                "command": "pre-commit (flake8 hook) \u2014 flake8 reported F401 and pre-commit exited with code 1"
            }
        ]
    },
    {
        "sha_fail": "2a59b55e6124b33dca7f48c12845c78130b20fd5",
        "error_context": [
            "The pre-commit step failed because the flake8 hook reported E501 (line too long) violations in import_export/admin.py. The pre-commit action exited with code 1 after flake8 reported lines exceeding the 88-character limit (e.g. E501 line too long (96 > 88) and (98 > 88))."
        ],
        "relevant_files": [
            {
                "file": "import_export/admin.py",
                "line_number": 749,
                "reason": "Log shows flake8 errors referencing import_export/admin.py:749:89 and import_export/admin.py:756:89 with 'E501 line too long (96 > 88 characters)' and 'E501 line too long (98 > 88 characters)', proving this file caused the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Line length exceeded (flake8 E501)",
                "evidence": "Context lines: \"- hook id: flake8\", \"- exit code: 1\", and \"import_export/admin.py:749:89: E501 line too long (96 > 88 characters)\" (and similarly for line 756: 'E501 line too long (98 > 88 characters)')."
            }
        ],
        "failed_job": [
            {
                "job": "main",
                "step": "main/4_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit action (flake8 hook)"
            }
        ]
    },
    {
        "sha_fail": "2f0605c9ec79b7a675728cb525ad55b36ade2e93",
        "error_context": [
            "The pre-commit step failed because the flake8 hook reported a line-length violation (E501). The log shows black and isort passed but flake8 failed with \"E501 line too long (101 > 88 characters)\" in import_export/resources.py on line 1360, causing the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "import_export/resources.py",
                "line_number": 1360,
                "reason": "Flake8 error reported in the log: \"import_export/resources.py:1360:89: E501 line too long (101 > 88 characters)\" \u2014 this traceback line ties the file and line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Line Length Exceeded (flake8 E501)",
                "evidence": "Log shows flake8 failed and reported: \"- hook id: flake8\" and \"import_export/resources.py:1360:89: E501 line too long (101 > 88 characters)\"; pre-commit then exited with code 1 (\"Process completed with exit code 1\")."
            }
        ],
        "failed_job": [
            {
                "job": "main",
                "step": "main/4_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit (flake8 hook)"
            }
        ]
    },
    {
        "sha_fail": "c359d794dd0e4baf40be48d584193f88c2213f37",
        "error_context": [
            "The pre-commit step failed because the flake8 hook reported an unused import (F401) in a test file. The log shows 'flake8...Failed' and the specific error: tests/core/tests/test_resources/test_resources.py:4:1: F401 'copy.deepcopy' imported but unused. The job exited with code 1."
        ],
        "relevant_files": [
            {
                "file": "tests/core/tests/test_resources/test_resources.py",
                "line_number": 4,
                "reason": "Log shows a flake8 error referencing this file and line: \"tests/core/tests/test_resources/test_resources.py:4:1: F401 'copy.deepcopy' imported but unused\", tying the file to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Code Quality",
                "subcategory": "Unused import (flake8 F401)",
                "evidence": "Context lines contain: \"flake8...................................................................Failed\" and \"tests/core/tests/test_resources/test_resources.py:4:1: F401 'copy.deepcopy' imported but unused\", which is the flake8 unused-import error code F401."
            }
        ],
        "failed_job": [
            {
                "job": "main",
                "step": "main/4_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit (flake8 hook)"
            }
        ]
    },
    {
        "sha_fail": "cfbbed910a5d84c08f9af237cf6737502c456f66",
        "error_context": [
            "The CI pre-commit step failed because the Black formatter (run via the pre-commit action) modified a tracked file, causing the pre-commit hook to exit non-zero. Logs show Black reported \"Failed\" and \"- files were modified by this hook\" and specifically \"reformatted import_export/admin.py\". The workflow ended with \"##[error]Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "import_export/admin.py",
                "line_number": 747,
                "reason": "Log shows a diff hunk for import_export/admin.py (@@ -747,6 +747,7 @@) and the pre-commit output states \"reformatted import_export/admin.py\" and \"- files were modified by this hook\", directly tying this file to the black formatting change that caused the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Black autoformat changed file / formatting hook failure",
                "evidence": "\"black....................................................................Failed\" and \"- hook id: black\" followed by \"- files were modified by this hook\" and \"reformatted import_export/admin.py\" in the step logs."
            },
            {
                "category": "Configuration / CI",
                "subcategory": "Pre-commit hook failure (non-zero exit code)",
                "evidence": "The step ended with \"##[error]Process completed with exit code 1.\", originating from the pre-commit action (pre-commit/action@v3.0.0) that ran the Black hook."
            }
        ],
        "failed_job": [
            {
                "job": "main",
                "step": "main/4_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit action (black hook) \u2014 Black formatter run via pre-commit"
            }
        ]
    },
    {
        "sha_fail": "76e35eca93562514943c5842cf2b0b8ec94a4763",
        "error_context": [
            "Pytest ran during the \"Run Tests\" step and 2 unit tests failed (\"2 failed, 165 passed, 1 skipped\"). The failures come from tests/api/gef_memory.py where a TypeError occurs when evaluating the line \"if self.gdb_version < (11, 0)\": the logs show \"E       TypeError: '<' not supported between instances of 'list' and 'tuple'\" (seen at tests/api/gef_memory.py:36 and tests/api/gef_memory.py:71). The test-run (pytest) is the tool that produced the failing result. The captured stderr also shows a separate runtime message \"Error while writing index for `/tmp/default.out': mkstemp: No such file or directory.\", but the explicit test failures are caused by the TypeError in tests/api/gef_memory.py."
        ],
        "relevant_files": [
            {
                "file": "tests/api/gef_memory.py",
                "line_number": 36,
                "reason": "Test traceback and summary show failing tests in this file: e.g. \"E       TypeError: '<' not supported between instances of 'list' and 'tuple'\" and \"tests/api/gef_memory.py:36: TypeError\" (also referenced at tests/api/gef_memory.py:71 in another failing test). The logs show the failing statement \"if self.gdb_version < (11, 0):\" in that file."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "TypeError raised during unit test",
                "evidence": "\"E       TypeError: '<' not supported between instances of 'list' and 'tuple'\" and the frames showing the failing code \"if self.gdb_version < (11, 0):\" with references to tests/api/gef_memory.py:36 and tests/api/gef_memory.py:71."
            },
            {
                "category": "Runtime Error",
                "subcategory": "Filesystem/temporary file creation error (mkstemp)",
                "evidence": "Captured stderr contains: \"Error while writing index for `/tmp/default.out': mkstemp: No such file or directory.\" (shown repeatedly in the test output)."
            }
        ],
        "failed_job": [
            {
                "job": "Run Unit tests on ubuntu-22.04",
                "step": "Run Tests",
                "command": "python${{ env.PY_VER }} -m pytest --forked -n ${{ env.GEF_CI_NB_CPU }} -v -k \"not benchmark\" tests/"
            },
            {
                "job": "Run Unit tests on ubuntu-20.04",
                "step": "Run Tests",
                "command": "python${{ env.PY_VER }} -m pytest --forked -n ${{ env.GEF_CI_NB_CPU }} -v -k \"not benchmark\" tests/"
            }
        ]
    },
    {
        "sha_fail": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
        "error_context": [
            "The smoketest job failed during the \"Run checks\" step while running pytest (command: \"pytest tests/test_smoketest.py\"). Test collection/import failed because a ModuleNotFoundError was raised for the external dependency \"transformers\". The traceback shows the import chain from tests/conftest.py -> composer/__init__.py -> composer/trainer/... -> composer/datasets/in_context_learning_evaluation.py where the missing import occurs (composer/datasets/in_context_learning_evaluation.py:13).",
            "As a result, pytest aborted during conftest import (log: \"ImportError while loading conftest '/home/runner/work/composer/composer/tests/conftest.py'\"), causing the job to exit with code 4."
        ],
        "relevant_files": [
            {
                "file": "composer/datasets/in_context_learning_evaluation.py",
                "line_number": 13,
                "reason": "Log shows \"composer/datasets/in_context_learning_evaluation.py:13: in <module>\" followed by \"import transformers\" and the error \"E   ModuleNotFoundError: No module named 'transformers'\", directly tying this file/line to the failure."
            },
            {
                "file": "composer/callbacks/eval_output_logging_callback.py",
                "line_number": 16,
                "reason": "Log shows \"composer/callbacks/eval_output_logging_callback.py:16: in <module>\" which imports the dataset that triggers the missing- dependency error (appears in the same traceback leading to ModuleNotFoundError)."
            },
            {
                "file": "composer/callbacks/__init__.py",
                "line_number": 12,
                "reason": "Log shows \"composer/callbacks/__init__.py:12: in <module>\" in the import chain before the failing callback import, indicating this file is part of the module import path that led to the error."
            },
            {
                "file": "composer/trainer/trainer.py",
                "line_number": 37,
                "reason": "Log shows \"composer/trainer/trainer.py:37: in <module>\" as part of the traceback invoking composer.callbacks, linking this file to the import chain causing the failure."
            },
            {
                "file": "composer/trainer/__init__.py",
                "line_number": 6,
                "reason": "Log shows \"composer/trainer/__init__.py:6: in <module>\" in the traceback that leads to importing composer.trainer.trainer, placing this file in the import chain that culminated in the ModuleNotFoundError."
            },
            {
                "file": "composer/__init__.py",
                "line_number": 10,
                "reason": "Log shows \"composer/__init__.py:10: in <module>\" where composer.__init__ imports Trainer, contributing to the import chain during conftest loading."
            },
            {
                "file": "tests/conftest.py",
                "line_number": 9,
                "reason": "Log shows \"tests/conftest.py:9: in <module>\" and the message \"ImportError while loading conftest '/home/runner/work/composer/composer/tests/conftest.py'\", proving test collection failed while importing this file."
            }
        ],
        "error_types": [
            {
                "category": "Dependency Error",
                "subcategory": "ImportError / ModuleNotFoundError for external package",
                "evidence": "Log lines: \"composer/datasets/in_context_learning_evaluation.py:13: in <module>\" then \"import transformers\" and \"E   ModuleNotFoundError: No module named 'transformers'\" show an external dependency is missing."
            },
            {
                "category": "Test Failure",
                "subcategory": "ImportError during pytest collection (conftest import)",
                "evidence": "Log message: \"ImportError while loading conftest '/home/runner/work/composer/composer/tests/conftest.py'.\" and traceback entries from tests/conftest.py through composer modules demonstrate pytest aborted during test collection due to the import error."
            }
        ],
        "failed_job": [
            {
                "job": "smoketest",
                "step": "Run checks",
                "command": "pytest tests/test_smoketest.py"
            }
        ]
    },
    {
        "sha_fail": "fd461e9133f1d191e3db194745f2306cde1772b6",
        "error_context": [
            "The pre-commit step failed because the mypy pre-commit hook reported a type-checking error. Mypy produced: \"river/anomaly/sad.py:68: error: Incompatible types in assignment (expression has type \\\"Quantile\\\", variable has type \\\"Mean\\\")\", and the mypy hook exited with code 1, causing the \"Run pre-commit on all files\" step to fail.",
            "Mypy output also contains notes for other files (e.g. river/drift/kswin.py:112) about unchecked untyped function bodies, but the single blocking error is in river/anomaly/sad.py at line 68."
        ],
        "relevant_files": [
            {
                "file": "river/anomaly/sad.py",
                "line_number": 68,
                "reason": "Mypy error reported: \"river/anomaly/sad.py:68: error: Incompatible types in assignment (expression has type \\\"Quantile\\\", variable has type \\\"Mean\\\")\" \u2014 this is the single error shown by mypy and is the cause of the hook failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch",
                "evidence": "\"river/anomaly/sad.py:68: error: Incompatible types in assignment (expression has type \\\"Quantile\\\", variable has type \\\"Mean\\\")\" (mypy output in pre-commit logs)."
            },
            {
                "category": "CI Hook Failure",
                "subcategory": "Pre-commit mypy hook exited non-zero",
                "evidence": "\"- hook id: mypy\" and \"- exit code: 1\" in the step output, and \"##[error]Process completed with exit code 1.\" \u2014 pre-commit run failed because the mypy hook returned a non-zero exit code."
            }
        ],
        "failed_job": [
            {
                "job": "ubuntu",
                "step": "Run pre-commit on all files",
                "command": "pre-commit run --all-files (mypy pre-commit hook failed)"
            }
        ]
    },
    {
        "sha_fail": "aa8a42bcf03f3b89575a9cce2f8af715a5121c59",
        "error_context": [
            "The test suite failed: pytest reported assertion and runtime errors during the 'Run tests' step. tests/client/test_redirects.py at line 416 failed with an AssertionError because response.text was 'Not logged in' but the test expected 'Logged in'. Separately, cookie-related tests failed due to a KeyError ('example-name') raised in httpx/_models.py at line 1154 (seen from tests/client/test_cookies.py failures). These failures occurred when running the project's test command (coverage run -m pytest) in the Python matrix jobs (e.g. Python 3.8 and 3.11)."
        ],
        "relevant_files": [
            {
                "file": "tests/client/test_redirects.py",
                "line_number": 416,
                "reason": "Log shows a failing assertion at this file and line: \"E       AssertionError: assert 'Not logged in' == 'Logged in'\" and \"tests/client/test_redirects.py:416: AssertionError\" (response.text mismatch)."
            },
            {
                "file": "tests/client/test_cookies.py",
                "line_number": 148,
                "reason": "Tests in this file produced failures referencing cookie access (seen at tests/client/test_cookies.py:148 and :164) with assertions like \">       assert client.cookies[\"example-name\"] == \\\"example-value\\\"\" and stack frames pointing to a KeyError."
            },
            {
                "file": "httpx/_models.py",
                "line_number": 1154,
                "reason": "Traceback in the logs shows a KeyError raised here: \">           raise KeyError(name)\" followed by \"E           KeyError: 'example-name'\" and \"httpx/_models.py:1154: KeyError\", tying this module to the cookie test failures."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "\"E       AssertionError: assert 'Not logged in' == 'Logged in'\" and \"tests/client/test_redirects.py:416: AssertionError\" show an assertion in tests/client/test_redirects.py expecting 'Logged in' but receiving 'Not logged in'."
            },
            {
                "category": "Runtime Error",
                "subcategory": "Unhandled KeyError during tests",
                "evidence": "\"E           KeyError: 'example-name'\" and \"httpx/_models.py:1154: KeyError\" in the test output indicate a KeyError was raised in httpx/_models.py while running cookie-related tests (tests/client/test_cookies.py)."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.8",
                "step": "Run tests",
                "command": "coverage run -m pytest"
            },
            {
                "job": "Python 3.11",
                "step": "Run tests",
                "command": "coverage run -m pytest"
            }
        ]
    },
    {
        "sha_fail": "83b5e4bf130d204fbb25b26a341c62aee4fc2d0f",
        "error_context": [
            "The linting step failed: ruff reported two import-sorting/formatting issues which made the 'Run linting checks' step exit with code 1. The CI log shows ruff check output lines: \"httpx/_config.py:1:1: I001 [*] Import block is un-sorted or un-formatted\" and \"httpx/_transports/default.py:26:1: I001 [*] Import block is un-sorted or un-formatted\". The workflow runs the \"Run linting checks\" step (scripts/check) which invoked `ruff check httpx tests`, causing the job to fail."
        ],
        "relevant_files": [
            {
                "file": "httpx/_config.py",
                "line_number": 1,
                "reason": "Log shows a ruff error for this file: \"httpx/_config.py:1:1: I001 [*] Import block is un-sorted or un-formatted\", tying this file and line to the lint failure."
            },
            {
                "file": "httpx/_transports/default.py",
                "line_number": 26,
                "reason": "Log shows a ruff error for this file: \"httpx/_transports/default.py:26:1: I001 [*] Import block is un-sorted or un-formatted\", tying this file and line to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import block unsorted (ruff I001)",
                "evidence": "CI log: \"httpx/_config.py:1:1: I001 [*] Import block is un-sorted or un-formatted\" and \"httpx/_transports/default.py:26:1: I001 [*] Import block is un-sorted or un-formatted\"; followed by \"Found 2 errors.\" and \"[*] 2 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run linting checks",
                "command": "ruff check httpx tests"
            }
        ]
    },
    {
        "sha_fail": "1afe2c9cb192d3760d59190cc7892e7ac37d5e27",
        "error_context": [
            "Mypy type-checking failed during the 'Run linting checks' step: code in httpx/_client.py is passing keyword arguments (\"verify\", \"cert\") to AsyncHTTPTransport, but mypy reports the transport does not accept those keywords and also reports Name errors for those identifiers. The mypy output points to httpx/_client.py lines 1456, 1458, 1459 and notes AsyncHTTPTransport is defined in httpx/_transports/default.py at line 260.",
            "The failure occurred while running the repository lint/type checks (scripts/check -> ruff format then mypy); mypy produced 4 errors and caused the step to exit with code 1 across the Python 3.10/3.11/3.12 matrix runs."
        ],
        "relevant_files": [
            {
                "file": "httpx/_client.py",
                "line_number": 1456,
                "reason": "Mypy error lines referencing this file: \"httpx/_client.py:1456: error: Unexpected keyword argument \\\"verify\\\" for \\\"AsyncHTTPTransport\\\"  [call-arg]\" and \"httpx/_client.py:1458: error: Name \\\"verify\\\" is not defined  [name-defined]\" and \"httpx/_client.py:1459: error: Name \\\"cert\\\" is not defined  [name-defined]\" \u2014 these tie this file and those line numbers directly to the type-check failures."
            },
            {
                "file": "httpx/_transports/default.py",
                "line_number": 260,
                "reason": "Mypy output contains a note pointing to the transport definition: \"httpx/_transports/default.py:260: note: \\\"AsyncHTTPTransport\\\" defined here\", indicating the reported Unexpected keyword argument error is caused by a mismatch between how AsyncHTTPTransport is defined here and how it's called in httpx/_client.py."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: unexpected keyword argument / name undefined",
                "evidence": "\"httpx/_client.py:1456: error: Unexpected keyword argument \\\"verify\\\" for \\\"AsyncHTTPTransport\\\"  [call-arg]\" and \"httpx/_client.py:1458: error: Name \\\"verify\\\" is not defined  [name-defined]\" (also similar for \\\"cert\\\") \u2014 these are mypy errors reported in the CI log."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.10",
                "step": "Run linting checks",
                "command": "mypy httpx tests"
            },
            {
                "job": "Python 3.11",
                "step": "Run linting checks",
                "command": "mypy httpx tests"
            },
            {
                "job": "Python 3.12",
                "step": "Run linting checks",
                "command": "mypy httpx tests"
            }
        ]
    },
    {
        "sha_fail": "7f351340260c165e18ccd7c83dc783bb371b3797",
        "error_context": [
            "The CI coverage enforcement step failed because the coverage report produced a total coverage of 99%, which is below the required fail-under=100 threshold. The coverage output shows httpx/_config.py with missing line(s) (listed as '139'), contributing to the single missing line that caused the overall 99% total. The failing step is the 'Enforce coverage' step (invoked via scripts/coverage) which runs the command 'coverage report --show-missing --skip-covered --fail-under=100'.",
            "This failure appears across multiple matrix jobs (Python 3.8, 3.9, 3.10, 3.11) as the same coverage check exits with code 2 in each case."
        ],
        "relevant_files": [
            {
                "file": "httpx/_config.py",
                "line_number": 139,
                "reason": "Coverage report line: 'httpx/_config.py     133      1    99%   139' \u2014 shows httpx/_config.py has 1 missing line (listed as 139) and is at 99% coverage, which the 'coverage' check reports as contributing to the overall coverage failure."
            }
        ],
        "error_types": [
            {
                "category": "Configuration Error",
                "subcategory": "Coverage check failed (coverage below required threshold)",
                "evidence": "'+ coverage report --show-missing --skip-covered --fail-under=100' and 'Coverage failure: total of 99 is less than fail-under=100' in the CI log indicate the coverage enforcement command failed because total coverage (99%) is below the configured fail-under=100."
            }
        ],
        "failed_job": [
            {
                "job": "tests (Python 3.8)",
                "step": "Enforce coverage",
                "command": "coverage report --show-missing --skip-covered --fail-under=100 (invoked via scripts/coverage)"
            },
            {
                "job": "tests (Python 3.9)",
                "step": "Enforce coverage",
                "command": "coverage report --show-missing --skip-covered --fail-under=100 (invoked via scripts/coverage)"
            },
            {
                "job": "tests (Python 3.10)",
                "step": "Enforce coverage",
                "command": "coverage report --show-missing --skip-covered --fail-under=100 (invoked via scripts/coverage)"
            },
            {
                "job": "tests (Python 3.11)",
                "step": "Enforce coverage",
                "command": "coverage report --show-missing --skip-covered --fail-under=100 (invoked via scripts/coverage)"
            }
        ]
    },
    {
        "sha_fail": "897a5deb406b53ea2f4675cdf0c2f1fa93fc6238",
        "error_context": [
            "The CI 'Run linting checks' step failed because the ruff linter reported a formatting/import-order issue in httpx/__init__.py (httpx/__init__.py:1:1). The logs show ruff produced an I001 diagnostic and the job exited with code 1; the issue is marked as fixable with --fix. This failure occurred during the scripts/check invocation (which ran 'ruff check httpx tests') across the Python matrix."
        ],
        "relevant_files": [
            {
                "file": "httpx/__init__.py",
                "line_number": 1,
                "reason": "Log shows a ruff lint error for this file: \"httpx/__init__.py:1:1: I001 [*] Import block is un-sorted or un-formatted\" and follow-up lines \"Found 1 error.\" and \"[*] 1 fixable with the `--fix` option.\" which tie this file directly to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import block unsorted / linter (ruff) diagnostic I001",
                "evidence": "\"httpx/__init__.py:1:1: I001 [*] Import block is un-sorted or un-formatted\" plus \"Found 1 error.\" and \"[*] 1 fixable with the `--fix` option.\" in the 'Run linting checks' log; the step then ended with \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "Python ${{ matrix.python-version }} (job id: tests)",
                "step": "Run linting checks",
                "command": "ruff check httpx tests (invoked via scripts/check)"
            }
        ]
    },
    {
        "sha_fail": "077f6aaac3ebb96626ac747fb126a0b4d752489c",
        "error_context": [
            "The pre-commit step failed because formatting/lint hooks modified files, causing the action to exit non\u2011zero. Logs show ruff reported \"files were modified by this hook\" and \"Found 1 error (1 fixed, 0 remaining)\", while black-jupyter reformatted wandb/integration/ultralytics/callback.py (hunk around line 42). The pre-commit action therefore failed with \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "wandb/integration/ultralytics/callback.py",
                "line_number": 42,
                "reason": "Shown in the diff/hunk header in the logs (\"--- a/wandb/integration/ultralytics/callback.py\", \"+++ b/wandb/integration/ultralytics/callback.py\", \"@@ -42,6 +42,7 @@\") and explicitly reported as reformatted by black-jupyter (\"reformatted wandb/integration/ultralytics/callback.py\")."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Pre-commit hooks modified files / auto-fixes (formatter/linter changes)",
                "evidence": "\"ruff... Failed\" with \"- files were modified by this hook\" and \"Found 1 error (1 fixed, 0 remaining).\" plus \"reformatted wandb/integration/ultralytics/callback.py\" from black-jupyter; overall \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "pre-commit",
                "step": "pre-commit/6_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit action (run with extra_args: --hook-stage pre-push --all-files) executing hooks ruff and black-jupyter"
            }
        ]
    },
    {
        "sha_fail": "c99ead9542bde331497f2456537fdbb0e37706d0",
        "error_context": [
            "The pre-commit job failed because a pre-commit hook modified file formatting and caused the pre-commit action to exit non-zero. The CI log shows a unified-diff for wandb/sdk/data_types/image.py (around line 277) changing the quote style in a savefig call, and the step ended with \"##[error]Process completed with exit code 1.\"",
            "The failing step is the pre-commit action (pre-commit/action@v3.0.0) run with extra_args '--hook-stage pre-push --all-files', which returned exit code 1 after hooks reported/produced changes."
        ],
        "relevant_files": [
            {
                "file": "wandb/sdk/data_types/image.py",
                "line_number": 277,
                "reason": "Log shows a unified diff for this file: '+++ b/wandb/sdk/data_types/image.py' and the changed lines: '-            util.ensure_matplotlib_figure(data).savefig(buf, format=\\'png\\')' -> '+            util.ensure_matplotlib_figure(data).savefig(buf, format=\"png\")'. This diff is shown immediately before '##[error]Process completed with exit code 1.' indicating the file was modified by a pre-commit hook and tied to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Pre-commit hook caused formatting change / hook-modified files",
                "evidence": "Unified diff in logs for wandb/sdk/data_types/image.py showing a formatting/quote change and the step ending with '##[error]Process completed with exit code 1.' plus the workflow using pre-commit/action@v3.0.0 with '--all-files' indicates a pre-commit hook modified files and caused the action to fail."
            }
        ],
        "failed_job": [
            {
                "job": "pre-commit",
                "step": "pre-commit/6_Run pre-commitaction@v3.0.0.txt",
                "command": "pre-commit action (pre-commit/action@v3.0.0) run with extra_args '--hook-stage pre-push --all-files'"
            }
        ]
    },
    {
        "sha_fail": "99ad8a351bb884f1e398c1d85c62d6b6e0bdd67e",
        "error_context": [
            "The docs build failed because Sphinx raised a parsing warning in a docstring that was treated as an error. Logs show a Sphinx warning pointing to packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py (docstring of Document) at line 6: \"Inline emphasis start-string without end-string.\"",
            "The failing command was sphinx-build (invoked by nox), which exited with code 2 and caused the 'Run docs' step of the 'docs' job to fail."
        ],
        "relevant_files": [
            {
                "file": "packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py",
                "line_number": 6,
                "reason": "Sphinx error message: \"sphinx.errors.SphinxWarning: /home/runner/work/.../packages/google-ai-generativelanguage/google/ai/generativelanguage_v1beta/types/retriever.py:docstring of google.ai.generativelanguage_v1beta.types.retriever.Document:6:Inline emphasis start-string without end-string.\" \u2014 ties this file and docstring line 6 directly to the build failure."
            }
        ],
        "error_types": [
            {
                "category": "Documentation Build Error",
                "subcategory": "Sphinx docstring parsing error (inline emphasis start-string without end-string) treated as error",
                "evidence": "\"sphinx.errors.SphinxWarning: .../packages/google-ai-generativelanguage/.../retriever.py:docstring ...:6:Inline emphasis start-string without end-string.\" and \"Warning, treated as error:\" followed by \"sphinx-build ... failed with exit code 2\" in the logs."
            }
        ],
        "failed_job": [
            {
                "job": "docs",
                "step": "Run docs",
                "command": "sphinx-build -W -T -N -b html -d docs/_build/doctrees/ docs/ docs/_build/html/"
            }
        ]
    },
    {
        "sha_fail": "44b56e01683771fb4ca583f9ea57c67dcee8e779",
        "error_context": [
            "The Quality check job failed because the style tool (doc-builder) reported that one file needs restyling: the log shows \"doc-builder style src/accelerate docs/source --max_len 119 --check_only\" followed by \"ValueError: 1 files should be restyled!\", which caused `make quality` to exit and the step to fail. The failure propagated to the job with \"make: *** [Makefile:17: quality] Error 1\" and \"##[error]Process completed with exit code 2.\" No repository file path or line number for the offending file is printed in the logs."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Style-check failure (restyle required)",
                "evidence": "\"doc-builder style src/accelerate docs/source --max_len 119 --check_only\" and \"ValueError: 1 files should be restyled!\" in the step log; followed by \"make: *** [Makefile:17: quality] Error 1\" and \"Process completed with exit code 2.\""
            }
        ],
        "failed_job": [
            {
                "job": "quality",
                "step": "Run Quality check",
                "command": "doc-builder style src/accelerate docs/source --max_len 119 --check_only (invoked via 'make quality')"
            }
        ]
    },
    {
        "sha_fail": "3dd8e4404a0ce2e29db4911dc2cd7e94755be631",
        "error_context": [
            "The Quality check step failed because the linter ruff reported two formatting/import-order issues, causing `make quality` to exit with error. Ruff reported I001 (import block un-sorted or un-formatted) in tests/deepspeed/test_deepspeed.py at line 15 and tests/fsdp/test_fsdp.py at line 16, and the make target returned Error 1."
        ],
        "relevant_files": [
            {
                "file": "tests/deepspeed/test_deepspeed.py",
                "line_number": 15,
                "reason": "Log shows: \"tests/deepspeed/test_deepspeed.py:15:1: I001 [*] Import block is un-sorted or un-formatted\" \u2014 ruff flagged this file/line as a formatting/import-order error."
            },
            {
                "file": "tests/fsdp/test_fsdp.py",
                "line_number": 16,
                "reason": "Log shows: \"tests/fsdp/test_fsdp.py:16:1: I001 [*] Import block is un-sorted or un-formatted\" \u2014 ruff flagged this file/line as a formatting/import-order error."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import block not sorted / import-order (ruff I001)",
                "evidence": "\"tests/deepspeed/test_deepspeed.py:15:1: I001 [*] Import block is un-sorted or un-formatted\" and \"tests/fsdp/test_fsdp.py:16:1: I001 [*] Import block is un-sorted or un-formatted\"; ruff reported \"Found 2 errors.\" and make returned \"Error 1\"."
            }
        ],
        "failed_job": [
            {
                "job": "quality",
                "step": "Run Quality check",
                "command": "ruff tests src examples benchmarks utils"
            }
        ]
    },
    {
        "sha_fail": "028ad1efee2c41691d78e5a4de90ebd6f8236cad",
        "error_context": [
            "The Quality check job failed because the linter ruff reported import-block formatting errors. Black completed successfully (118 files unchanged), but ruff produced two I001 errors: one in src/accelerate/utils/__init__.py and one in src/accelerate/utils/modeling.py, causing `make quality` to exit with error.",
            "The failing step executed the repository's quality target (make quality) which runs black then ruff; ruff's output triggered the job failure (exit code 2 / Makefile error)."
        ],
        "relevant_files": [
            {
                "file": "src/accelerate/utils/__init__.py",
                "line_number": 1,
                "reason": "Log shows a ruff error for this file: \"src/accelerate/utils/__init__.py:1:1: I001 [*] Import block is un-sorted or un-formatted\" which directly ties the file to the lint failure."
            },
            {
                "file": "src/accelerate/utils/modeling.py",
                "line_number": 15,
                "reason": "Log shows a ruff error for this file: \"src/accelerate/utils/modeling.py:15:1: I001 [*] Import block is un-sorted or un-formatted\" which directly ties the file to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import sorting / Un-sorted import block (ruff I001)",
                "evidence": "\"src/accelerate/utils/__init__.py:1:1: I001 [*] Import block is un-sorted or un-formatted\" and \"src/accelerate/utils/modeling.py:15:1: I001 [*] Import block is un-sorted or un-formatted\"; followed by \"Found 2 errors.\" and \"[*] 2 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "quality",
                "step": "Run Quality check",
                "command": "ruff tests src examples benchmarks utils"
            }
        ]
    },
    {
        "sha_fail": "c9991372b81edabb86965638db110ab930f8e165",
        "error_context": [
            "The Quality check job failed because the linter (ruff) reported a formatting/import-order issue. CI log shows: \"src/accelerate/utils/fsdp_utils.py:14:1: I001 [*] Import block is un-sorted or un-formatted\", and Make aborted with \"Error 1\" causing the step to exit with code 2. The failing command was executed as part of the 'Run Quality check' step (make quality) which runs ruff over tests, src, examples, benchmarks, and utils."
        ],
        "relevant_files": [
            {
                "file": "src/accelerate/utils/fsdp_utils.py",
                "line_number": 14,
                "reason": "Log shows a ruff lint error for this file: \"src/accelerate/utils/fsdp_utils.py:14:1: I001 [*] Import block is un-sorted or un-formatted\" which ties this file directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import ordering / unsorted import block (ruff I001)",
                "evidence": "CI log: \"src/accelerate/utils/fsdp_utils.py:14:1: I001 [*] Import block is un-sorted or un-formatted\" and \"[*] 1 fixable with the `--fix` option.\" followed by \"make: *** [Makefile:16: quality] Error 1\" and \"Process completed with exit code 2.\""
            }
        ],
        "failed_job": [
            {
                "job": "quality",
                "step": "Run Quality check",
                "command": "ruff tests src examples benchmarks utils (invoked via 'make quality')"
            }
        ]
    },
    {
        "sha_fail": "16a0c04d06205527ec5e379df2596b399ee5dadc",
        "error_context": [
            "The pre-commit job failed because the mypy pre-commit hook reported type-checking errors in dask/utils.py, causing the pre-commit action to exit with code 1. The logs show two mypy errors: at dask/utils.py:231 and dask/utils.py:257, both 'Unsupported operand types for + (\"str\" and \"None\")' (right operand Optional[str]).",
            "Pre-commit also reformatted dask/utils.py and reported \"pre-commit hook(s) made changes,\" but the CI failure is driven by the mypy type-check errors."
        ],
        "relevant_files": [
            {
                "file": "dask/utils.py",
                "line_number": 231,
                "reason": "Mypy errors reported for this file in the logs: 'dask/utils.py:231: error: Unsupported operand types for + (\"str\" and \"None\")' and 'dask/utils.py:257: error: Unsupported operand types for + (\"str\" and \"None\")'."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch (unsupported operand types)",
                "evidence": "Logs: 'dask/utils.py:231: error: Unsupported operand types for + (\"str\" and \"None\")' and 'dask/utils.py:257: error: Unsupported operand types for + (\"str\" and \"None\")' with mypy hook exit code 1 ('- hook id: mypy', '- exit code: 1')."
            },
            {
                "category": "Code Formatting",
                "subcategory": "Pre-commit auto-formatting made changes",
                "evidence": "Logs show 'reformatted dask/utils.py' and 'pre-commit hook(s) made changes.' (All changes made by hooks: diff --git a/dask/utils.py b/dask/utils.py)."
            }
        ],
        "failed_job": [
            {
                "job": "pre-commit hooks",
                "step": "pre-commit hooks/4_Run pre-commitaction@v3.0.0.txt",
                "command": "mypy (run via pre-commit hook / pre-commit/action@v3.0.0)"
            }
        ]
    },
    {
        "sha_fail": "e21f666b44b5c2ddf22f9a9d057787811dc92a30",
        "error_context": [
            "The linting/type-check step failed because mypy reported type/syntax errors when running 'mypy starlette'. The logs show \"Found 7 errors in 3 files\" with specific mypy messages referencing starlette/concurrency.py:32, starlette/_utils.py:77 and multiple locations in starlette/exceptions.py (e.g. lines 12,13,30,59). Several errors indicate use of Python 3.10 union syntax (X | Y) which is not valid under older Python, plus type/subscriptability issues (e.g. \"\\\"dict\\\" is not subscriptable\") and an unpacking error in concurrency.py."
        ],
        "relevant_files": [
            {
                "file": "starlette/concurrency.py",
                "line_number": 32,
                "reason": "Log shows: \"starlette/concurrency.py:32: error: Need more than 1 value to unpack (2 expected)  [misc]\", proving concurrency.py at line 32 triggered a mypy/type-check error."
            },
            {
                "file": "starlette/_utils.py",
                "line_number": 77,
                "reason": "Log shows: \"starlette/_utils.py:77: error: X | Y syntax for unions requires Python 3.10  [syntax]\", tying _utils.py line 77 to a mypy syntax/version error."
            },
            {
                "file": "starlette/exceptions.py",
                "line_number": 12,
                "reason": "Multiple mypy messages reference this file (e.g. \"starlette/exceptions.py:12: error: X | Y syntax for unions requires Python 3.10\" and \"starlette/exceptions.py:13: error: \\\"dict\\\" is not subscriptable, use \\\"typing.Dict\\\" instead\"), showing exceptions.py lines 12/13/30/59 are involved; first reported line is 12."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type/syntax errors",
                "evidence": "Logs show mypy run and errors: \"+ mypy starlette\" and \"Found 7 errors in 3 files (checked 35 source files)\" plus specific messages like \"starlette/concurrency.py:32: error: Need more than 1 value to unpack (2 expected)  [misc]\" and \"starlette/exceptions.py:13: error: \\\"dict\\\" is not subscriptable, use \\\"typing.Dict\\\" instead  [misc]\"."
            },
            {
                "category": "Compatibility / Syntax",
                "subcategory": "Use of Python 3.10 union operator (X | Y) not supported on older Python",
                "evidence": "Multiple messages state: \"error: X | Y syntax for unions requires Python 3.10  [syntax]\" (e.g. \"starlette/_utils.py:77: error: X | Y syntax for unions requires Python 3.10  [syntax]\"), indicating code uses Python 3.10-only syntax while the environment runs an older interpreter for some matrix entries."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run linting checks",
                "command": "mypy starlette"
            }
        ]
    },
    {
        "sha_fail": "cc0b066c05947c2a356d063d0137685205709c3e",
        "error_context": [
            "The CI run failed during the test step: pytest (invoked via coverage run -m pytest) produced many unit-test failures across the test suite. Multiple tests assert an expected 200 response but received 404 (e.g. \"E       assert 404 == 200\"), causing AssertionError failures in tests/test_routing.py and tests/test_applications.py.",
            "There are also test-time exceptions: a test expecting an exception did not receive one (\"E       Failed: DID NOT RAISE <class 'Exception'>\" in tests/test_routing.py), WebSocket sessions closed raising WebSocketDisconnect (starlette.testclient frames), and JSON decoding errors when attempting to parse 'Not Found' responses (json.decoder.JSONDecodeError).",
            "These failures were produced by the CI job matrix entries for Python (seen for Python 3.9 and 3.11) in the workflow step named \"Run tests\" (the step runs the project test script which invokes pytest via coverage run -m pytest)."
        ],
        "relevant_files": [
            {
                "file": "tests/test_routing.py",
                "line_number": 1053,
                "reason": "Log shows a failing test frame and assertion in this file: \"E           Failed: DID NOT RAISE <class 'Exception'>\" and \"tests/test_routing.py:1053: Failed\" (test_exception_on_mounted_apps / mounted-middleware failures)."
            },
            {
                "file": "tests/middleware/test_session.py",
                "line_number": 134,
                "reason": "Test failure assertion recorded here: \">       assert response.status_code == 200\" followed by \"E       assert 404 == 200\" and \"tests/middleware/test_session.py:134: AssertionError\" (session cookie subpath test)."
            },
            {
                "file": "tests/test_applications.py",
                "line_number": 163,
                "reason": "Multiple assertion failures reference this file and line: \">       assert response.status_code == 200\" then \"E       assert 404 == 200\" and \"tests/test_applications.py:163: AssertionError\" (test_mounted_route)."
            },
            {
                "file": "starlette/testclient.py",
                "line_number": 94,
                "reason": "Appears in traceback for websocket test failures: \"starlette/testclient.py:94: in __enter__    self._raise_on_close(message)\" and later \"starlette/testclient.py:135: WebSocketDisconnect\" showing the test client raising WebSocketDisconnect during websocket tests."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/json/decoder.py",
                "line_number": 355,
                "reason": "JSON decode errors are logged from here: \"E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\" and \"/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/json/decoder.py:355: JSONDecodeError\" indicating a test attempted to parse non-JSON 'Not Found' responses."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.11.7/x64/lib/python3.11/site-packages/httpx/_models.py",
                "line_number": 761,
                "reason": "Stack shows httpx trying to parse response JSON before the JSONDecodeError: \"/opt/hostedtoolcache/.../site-packages/httpx/_models.py:761: in json    return jsonlib.loads(self.content, **kwargs)\" \u2014 ties the JSONDecodeError to HTTP response content used in tests."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit tests (HTTP status mismatch)",
                "evidence": "Many test frames show assertions failing because response.status_code was 404 not 200, e.g. \"E       assert 404 == 200\" and \"tests/test_routing.py:854: AssertionError\", \"tests/test_applications.py:163: AssertionError\"."
            },
            {
                "category": "Test Failure",
                "subcategory": "Failed expectation of exception (pytest.raises DID NOT RAISE)",
                "evidence": "Log contains \"E       Failed: DID NOT RAISE <class 'Exception'>\" with \"tests/test_routing.py:1053: Failed\", showing a test expecting an exception did not receive it."
            },
            {
                "category": "Runtime Error (during tests)",
                "subcategory": "WebSocketDisconnect raised in test client",
                "evidence": "Traceback shows the test client raising a WebSocketDisconnect: \"E           starlette.websockets.WebSocketDisconnect: (1000, '')\" and frames \"starlette/testclient.py:94\" and \"starlette/testclient.py:135\"."
            },
            {
                "category": "Runtime Error (during tests)",
                "subcategory": "JSONDecodeError when parsing HTTP response content",
                "evidence": "Logs show \"E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\" from \"/opt/hostedtoolcache/.../json/decoder.py:355\" and httpx frame \"/opt/.../httpx/_models.py:761: in json\" \u2014 tests attempted to parse 'Not Found' body as JSON and failed."
            }
        ],
        "failed_job": [
            {
                "job": "tests (Python 3.9)",
                "step": "Run tests",
                "command": "coverage run -m pytest"
            },
            {
                "job": "tests (Python 3.11)",
                "step": "Run tests",
                "command": "coverage run -m pytest"
            }
        ]
    },
    {
        "sha_fail": "58c7cde15084b1c07373c00028dbd19b85fd5e1b",
        "error_context": [
            "The test run failed because two parametrizations of the file-response test in tests/test_responses.py raised a TypeError: \"unhashable type: 'dict'\". The failure is triggered at tests/test_responses.py:358 when the test calls await app(...) with an ASGI message whose \"extensions\" value is written as {\"http.response.pathsend\", {}} (a set containing an unhashable dict), causing the runtime TypeError during test execution. The failing step is the test step (pytest) executed by the CI 'Run tests' command."
        ],
        "relevant_files": [
            {
                "file": "tests/test_responses.py",
                "line_number": 358,
                "reason": "Log shows the traceback and failure at tests/test_responses.py:358 with the error message \"E       TypeError: unhashable type: 'dict'\" and the failing call context: await app({\"type\": \"http\", \"method\": \"get\", \"extensions\": {\"http.response.pathsend\", {}}}, receive, send,)."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Runtime TypeError in unit test (TypeError: unhashable type: 'dict')",
                "evidence": "CI log lines show the failing call and exception: \"await app(..., \\n>           {\\\"type\\\": \\\"http\\\", \\\"method\\\": \\\"get\\\", \\\"extensions\\\": {\\\"http.response.pathsend\\\", {}}},\\n...\\nE       TypeError: unhashable type: 'dict'\" and \"tests/test_responses.py:358: TypeError\"."
            }
        ],
        "failed_job": [
            {
                "job": "tests (matrix job: Python 3.9 shown in logs as 'Python 3.9')",
                "step": "Run tests",
                "command": "scripts/test (invoked in logs as: coverage run -m pytest -> pytest)"
            }
        ]
    },
    {
        "sha_fail": "0b93d2da3b721c80dcb6a2993a23876a97498dd5",
        "error_context": [
            "Pytest test run failed: 6 tests failed and 559 passed (\"=================== 6 failed, 559 passed ...\"). Failures originate from tests/protocols/test_websocket.py (e.g. test frames referencing tests/protocols/test_websocket.py:1219, :1298, :1348) where websocket client code raised network/protocol exceptions during the handshake. The logs show httpx/httpcore and websockets internals raising RemoteProtocolError / InvalidMessage / EOFError (e.g. \"httpx.RemoteProtocolError: Server disconnected without sending a response.\", \"websockets.exceptions.InvalidMessage: did not receive a valid HTTP response\", \"EOFError: connection closed while reading HTTP status line\"), so the immediate cause is runtime protocol parsing/connection errors during websocket tests executed by pytest (Run tests step).",
            "The failing traces reference both test code (tests/protocols/test_websocket.py) and runtime libraries (httpx, httpcore, websockets) plus uvicorn server modules that logged connection rejections (uvicorn server/websockets_impl), indicating the issue occurs during test-run interactions between the test harness, uvicorn server, and websocket/http libraries."
        ],
        "relevant_files": [
            {
                "file": "tests/protocols/test_websocket.py",
                "line_number": 1219,
                "reason": "Multiple trace frames point to this test file and line (e.g. \"tests/protocols/test_websocket.py:1219\" and frames showing \"async with run_server(config):\" then \">           await websocket_session(f\"ws://127.0.0.1:{unused_tcp_port}\")\"). The pytest failure summary notes tests in this module failed (\"6 failed, 559 passed\")."
            },
            {
                "file": "uvicorn/uvicorn/protocols/websockets/websockets_impl.py",
                "line_number": 317,
                "reason": "Log lines show uvicorn logging from websockets_impl.py (\"uvicorn.error:websockets_impl.py:317 ('127.0.0.1', 48684) - \\\"WebSocket /\\\" 404\") tying the server websocket implementation to the connection rejections observed during tests."
            },
            {
                "file": "uvicorn/uvicorn/server.py",
                "line_number": 77,
                "reason": "Uvicorn server log entries appear in the captured logs (\"INFO     uvicorn.error:server.py:77 Started server process [1904]\" and \"uvicorn.error:server.py:229 connection rejected (404 Not Found)\"), showing the server process and connection rejection events during the failing tests."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/httpx/_transports/default.py",
                "line_number": 84,
                "reason": "Trace contains an exception mapping in httpx transports and the raised RemoteProtocolError originates here (e.g. \"E           httpx.RemoteProtocolError: Server disconnected without sending a response.\", \"/.../site-packages/httpx/_transports/default.py:84: RemoteProtocolError\")."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/httpcore/_async/http11.py",
                "line_number": 226,
                "reason": "The logs show httpcore raising RemoteProtocolError at this location (\"E                   httpcore.RemoteProtocolError: Server disconnected without sending a response.\", \"/.../site-packages/httpcore/_async/http11.py:226: RemoteProtocolError\"), which is part of the chain that caused httpx/httpcore to fail the requests used by the tests."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/websockets/legacy/http.py",
                "line_number": 122,
                "reason": "The traceback shows websockets raising EOFError here (\"E           EOFError: connection closed while reading HTTP status line\", \"/.../site-packages/websockets/legacy/http.py:122: EOFError\"), tying malformed/closed HTTP responses to the failing websocket handshake tests."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/websockets/legacy/client.py",
                "line_number": 141,
                "reason": "Logs include websockets client raising InvalidMessage and showing frames from client.py (\"E           websockets.exceptions.InvalidMessage: did not receive a valid HTTP response\", \"/.../site-packages/websockets/legacy/client.py:141: InvalidMessage\"), which is part of the failure chain when the test attempted to open websocket connections."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Assertion/interaction failure in unit tests (websocket handshake)",
                "evidence": "\"=================== 6 failed, 559 passed in ...\" and stack frames showing test code calling await websocket_session(...) in tests/protocols/test_websocket.py (e.g. \"tests/protocols/test_websocket.py:1219: ... await websocket_session(...)\"), indicating pytest-reported failing tests."
            },
            {
                "category": "Runtime Error",
                "subcategory": "RemoteProtocolError / Connection closed by server",
                "evidence": "Multiple frames show httpcore/httpx raising RemoteProtocolError: \"httpcore.RemoteProtocolError: Server disconnected without sending a response.\" and \"httpx.RemoteProtocolError: Server disconnected without sending a response.\" (e.g. \"/.../site-packages/httpcore/_async/http11.py:226: RemoteProtocolError\" and \"/.../site-packages/httpx/_transports/default.py:84: RemoteProtocolError\")."
            },
            {
                "category": "Protocol Parsing Error",
                "subcategory": "Malformed or incomplete HTTP response during WebSocket handshake (EOF/InvalidMessage)",
                "evidence": "The websockets library raised EOFError and InvalidMessage during read_http_response (\"E           EOFError: connection closed while reading HTTP status line\" at \"/.../websockets/legacy/http.py:122\" and \"E           websockets.exceptions.InvalidMessage: did not receive a valid HTTP response\" at \"/.../websockets/legacy/client.py:141\"), showing the HTTP response was malformed or truncated."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests",
                "command": "pytest (invoked by scripts/test)"
            }
        ]
    },
    {
        "sha_fail": "e5b5fcb646e6fa9cab60fb4fff930888149b88fe",
        "error_context": [
            "The check_requirements step failed because the requirements-checking script reported an unused dependency: \"DEP002 'sentence-transformers' defined as a dependency but not used in the codebase\" for the handler requirements file mindsdb/integrations/handlers/rag_handler/requirements.txt, causing the script to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "mindsdb/integrations/handlers/rag_handler/requirements.txt",
                "line_number": null,
                "reason": "Log shows: \"- mindsdb/integrations/handlers/rag_handler/requirements.txt\" followed by \"DEP002 'sentence-transformers' defined as a dependency but not used in the codebase\", linking this requirements file to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Dependency Error",
                "subcategory": "Unused dependency declared in requirements (DEP002)",
                "evidence": "Context lines: \"- mindsdb/integrations/handlers/rag_handler/requirements.txt\" and \"None:None: DEP002 'sentence-transformers' defined as a dependency but not used in the codebase\"; the step then reports \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "check_requirements",
                "step": "Check main requirements",
                "command": "python tests/scripts/check_requirements.py"
            }
        ]
    },
    {
        "sha_fail": "6cbb12e47665eda2c687b4431d6ce789e74ea4a4",
        "error_context": [
            "A test run (pytest via 'make test') failed: tests/test_categorical.py::TestBarPlot::test_datetime_native_scale_axis raised ValueError: \"Invalid frequency: ME\". The test calls pd.date_range(..., freq=\"ME\") (log shows tests/test_categorical.py:2081).",
            "The traceback shows an underlying KeyError 'ME' in pandas internals (pandas/_libs/tslibs/offsets.pyx:3502) which is then reported as ValueError: Invalid frequency: ME in pandas' to_offset code paths (frames in pandas/core/arrays/datetimes.py and pandas/core/indexes/datetimes.py)."
        ],
        "relevant_files": [
            {
                "file": "tests/test_categorical.py",
                "line_number": 2081,
                "reason": "The failing test is reported as 'FAILED tests/test_categorical.py::TestBarPlot::test_datetime_native_scale_axis' and the traceback shows the test line 'x = pd.date_range(\"2010-01-01\", periods=20, freq=\"ME\")' at tests/test_categorical.py:2081."
            },
            {
                "file": "pandas/_libs/tslibs/offsets.pyx",
                "line_number": 3502,
                "reason": "Traceback shows \"E   KeyError: 'ME'\" at pandas/_libs/tslibs/offsets.pyx:3502 and later \"E   ValueError: Invalid frequency: ME\" referencing offsets.pyx lines (3508/3612), tying this Cython offsets code to the error."
            },
            {
                "file": "pandas/core/arrays/datetimes.py",
                "line_number": 377,
                "reason": "Traceback includes '/.../site-packages/pandas/core/arrays/datetimes.py:377: in _generate_range' and shows the call 'freq = to_offset(freq)', indicating this frame is part of the call chain that raised the ValueError."
            },
            {
                "file": "pandas/core/indexes/datetimes.py",
                "line_number": 1069,
                "reason": "Traceback includes '/.../site-packages/pandas/core/indexes/datetimes.py:1069: in date_range', indicating date_range called into DatetimeArray._generate_range which led to the Invalid frequency error."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Assertion/Exception in unit test (ValueError)",
                "evidence": "Log shows 'FAILED tests/test_categorical.py::TestBarPlot::test_datetime_native_scale_axis - ValueError: Invalid frequency: ME' and the test line 'x = pd.date_range(\"2010-01-01\", periods=20, freq=\"ME\")' (tests/test_categorical.py:2081)."
            },
            {
                "category": "Runtime Error (library)",
                "subcategory": "pandas raised ValueError/KeyError in frequency parsing",
                "evidence": "Traceback contains 'E   KeyError: 'ME'' at pandas/_libs/tslibs/offsets.pyx:3502 and 'E   ValueError: Invalid frequency: ME' in pandas/_libs/tslibs/offsets.pyx (3508/3612), showing the error originates in pandas' offset parsing."
            }
        ],
        "failed_job": [
            {
                "job": "run-tests",
                "step": "Run tests",
                "command": "make test"
            }
        ]
    },
    {
        "sha_fail": "2201be21886bb82201f3c3487f5f1468f6e6ac81",
        "error_context": [
            "A NameError caused by an undefined identifier get_layout_engine in seaborn/_core/plot.py led to multiple CI failures: flake8 flagged F821 during lint, pytest failed three tests in tests/_core/test_plot.py, and the docs build (notebook preprocessing) crashed when executing a notebook via tools/nb_to_doc.py. The failing name is referenced at seaborn/_core/plot.py:1815 in both lint output and test tracebacks."
        ],
        "relevant_files": [
            {
                "file": "seaborn/_core/plot.py",
                "line_number": 1815,
                "reason": "Logs show the error frame and lint report pointing to this file and line: 'seaborn/_core/plot.py:1815:22: F821 undefined name 'get_layout_engine'' and traceback lines 'E           NameError: name 'get_layout_engine' is not defined' with 'seaborn/_core/plot.py:1815: NameError'."
            },
            {
                "file": "tests/_core/test_plot.py",
                "line_number": 1114,
                "reason": "Tests failing reference this test file and test cases: 'FAILED tests/_core/test_plot.py::TestPlotting::test_layout_extent - NameError: name 'get_layout_engine' is not defined' and context includes 'tests/_core/test_plot.py:1114' near the failing test call."
            },
            {
                "file": "tools/nb_to_doc.py",
                "line_number": 126,
                "reason": "Doc build traceback shows the script invoking notebook execution failed: 'File \"/home/runner/work/seaborn/seaborn/doc/_docstrings/../tools/nb_to_doc.py\", line 126, in <module>' and then the nbconvert/nbclient CellExecutionError reporting the NameError originating in seaborn/_core/plot.py."
            }
        ],
        "error_types": [
            {
                "category": "Runtime Error",
                "subcategory": "NameError: undefined name at runtime",
                "evidence": "pytest and nbclient tracebacks: 'E           NameError: name 'get_layout_engine' is not defined' and stack frames pointing to 'seaborn/_core/plot.py:1815', showing a runtime NameError during test and notebook execution."
            },
            {
                "category": "Test Failure",
                "subcategory": "Unit tests failing due to runtime exception",
                "evidence": "Pytest summary: 'FAILED tests/_core/test_plot.py::TestPlotting::test_layout_extent - NameError: name 'get_layout_engine' is not defined' (3 failed, 2299 passed...)."
            },
            {
                "category": "Lint Error",
                "subcategory": "Undefined name (flake8 F821)",
                "evidence": "Flake8 reported 'seaborn/_core/plot.py:1815:22: F821 undefined name 'get_layout_engine'' causing 'make lint' to fail."
            },
            {
                "category": "Documentation Build Failure",
                "subcategory": "Notebook execution error during docs build",
                "evidence": "Doc build logs show nbclient CellExecutionError while running '../tools/nb_to_doc.py ...' with traceback and then 'make: *** [Makefile:60: docstrings] Error 2', indicating notebook preprocessing failed because of the NameError."
            }
        ],
        "failed_job": [
            {
                "job": "lint",
                "step": "Flake8",
                "command": "make lint (runs 'flake8 seaborn/ tests/')"
            },
            {
                "job": "run-tests",
                "step": "Run tests",
                "command": "make test (invokes pytest)"
            },
            {
                "job": "build-docs",
                "step": "Build docs",
                "command": "cd doc; make -j `nproc` notebooks (invoked tools/nb_to_doc.py to preprocess notebooks)"
            }
        ]
    },
    {
        "sha_fail": "785242b646b54a33547ff1298cb945a05c24aa4c",
        "error_context": [
            "The run-tests job failed while running the test suite: the 'Run tests' step executed 'make test' (which runs pytest) and a single test failed. Pytest reports: \"FAILED tests/test_relational.py::TestLinePlotter::test_weights - ZeroDivisionError: Weights sum to zero, can't be normalized\".",
            "The failure is a runtime ZeroDivisionError raised inside NumPy's average implementation (traceback shows a raise at numpy/lib/function_base.py:409) after tests/test_relational.py called np.average (tests/test_relational.py:1077). The Makefile/CI step then exited with error (make: *** [Makefile:4: test] Error 1; Process completed with exit code 2)."
        ],
        "relevant_files": [
            {
                "file": "tests/test_relational.py",
                "line_number": 1077,
                "reason": "Test failure shown in logs: \"FAILED tests/test_relational.py::TestLinePlotter::test_weights - ZeroDivisionError...\" and context shows the failing call at tests/test_relational.py:1077: \">           expected = np.average(pos_df[\\\"y\\\"], weights=pos_df[\\\"x\\\"])\"."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/numpy/lib/function_base.py",
                "line_number": 409,
                "reason": "Traceback shows the exception originates in NumPy: \"/opt/hostedtoolcache/.../site-packages/numpy/lib/function_base.py:409: ZeroDivisionError\" and log lines show the raise: \"raise ZeroDivisionError( 'Weights sum to zero, can't be normalized')\"."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Runtime Error (ZeroDivisionError) in unit test",
                "evidence": "\"FAILED tests/test_relational.py::TestLinePlotter::test_weights - ZeroDivisionError: Weights sum to zero, can't be normalized\" and traceback showing the raise in numpy's function_base.py (\"raise ZeroDivisionError(\\\"Weights sum to zero, can't be normalized\\\")\")."
            },
            {
                "category": "CI Step Failure",
                "subcategory": "Makefile/test command exited non-zero",
                "evidence": "\"make: *** [Makefile:4: test] Error 1\" and \"##[error]Process completed with exit code 2.\" indicate the 'make test' command failed and caused the job to fail."
            }
        ],
        "failed_job": [
            {
                "job": "run-tests",
                "step": "Run tests",
                "command": "make test"
            }
        ]
    },
    {
        "sha_fail": "3ed7a88e343c89b7153efea25db1b6287b2f0823",
        "error_context": [
            "The CI Build step failed because a linter reported an unused local variable in a test file, causing the build script to exit with code 1. The logs show the lint section produced: \"tests/test_octodns_processor_filter.py:199:13: local variable 'filter_private' is assigned to but never used\". This lint error appears in the CI matrix runs for multiple Python versions, and the failing step is the repository's CI Build (./script/cibuild) run."
        ],
        "relevant_files": [
            {
                "file": "tests/test_octodns_processor_filter.py",
                "line_number": 199,
                "reason": "Log shows a lint error pointing to this file and line: \"tests/test_octodns_processor_filter.py:199:13: local variable 'filter_private' is assigned to but never used\", linking the file to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused local variable (assigned but never used)",
                "evidence": "\"tests/test_octodns_processor_filter.py:199:13: local variable 'filter_private' is assigned to but never used\" (appears under the CI 'lint' section and is followed by \"##[error]Process completed with exit code 1.\")"
            }
        ],
        "failed_job": [
            {
                "job": "ci",
                "step": "CI Build",
                "command": null
            }
        ]
    },
    {
        "sha_fail": "9e1aa7b8edfb723656f41f97bab57f9a653d5e1b",
        "error_context": [
            "A unit test failed: tests/test_octodns_manager.py::TestManager::test_missing_zone raised an AssertionError. The test expected the ManagerException message to contain the substring 'Requested zone:' but it did not (assert at tests/test_octodns_manager.py:87). This failure occurred while running the project's test suite (pytest) inside the CI build steps, causing the process to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "tests/test_octodns_manager.py",
                "line_number": 87,
                "reason": "The test failure is reported at tests/test_octodns_manager.py:87 with the assertion '>       self.assertTrue('Requested zone:' in str(ctx.exception))' and 'E       AssertionError: False is not true' (log lines: \"tests/test_octodns_manager.py:87: AssertionError\" and \"FAILED tests/test_octodns_manager.py::TestManager::test_missing_zone - AssertionError: False is not true\")."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (expected substring missing from exception message)",
                "evidence": "Log shows the failing assertion and test summary: \">       self.assertTrue('Requested zone:' in str(ctx.exception))\" followed by \"E       AssertionError: False is not true\" and \"FAILED tests/test_octodns_manager.py::TestManager::test_missing_zone - AssertionError: False is not true\"."
            }
        ],
        "failed_job": [
            {
                "job": "setup-py",
                "step": "CI setup.py",
                "command": "./script/cibuild-setup-py"
            },
            {
                "job": "ci",
                "step": "CI Build",
                "command": "./script/cibuild"
            }
        ]
    },
    {
        "sha_fail": "f2f8b63c3d579f9e8f1d4319592e44e39591ee38",
        "error_context": [
            "Two distinct failures occurred: (1) On an Ubuntu-22.04 test run pytest reported multiple unit-test assertion failures in tests/test_context_commands.py (AssertionError at tests/test_context_commands.py:85 comparing pwndbg.gdblib.config.context_sections.value). (2) On an Ubuntu-20.04 test run pytest failed during collection with a TypeError caused by usage of PEP 604 union syntax (\"int | None\") in pwndbg/gdblib/heap_tracking.py:88 \u2014 the runner used Python 3.8.10 which does not support that runtime expression, so imports aborted and 27 collection errors were reported."
        ],
        "relevant_files": [
            {
                "file": "tests/test_context_commands.py",
                "line_number": 85,
                "reason": "pytest traceback shows an AssertionError at tests/test_context_commands.py:85: \"assert pwndbg.gdblib.config.context_sections.value == default_ctx_sects\" and the failure message comparing the two strings (missing/extra 'heap-tracker'), e.g. \"tests/test_context_commands.py:85: AssertionError\" and the diff lines in the logs."
            },
            {
                "file": "pwndbg/gdblib/heap_tracking.py",
                "line_number": 88,
                "reason": "Collection-time TypeError points directly to this file and line: the log shows \"../../pwndbg/gdblib/heap_tracking.py:88: in <module>    def resolve_address(name: str) -> int | None:\" followed by \"TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\"."
            },
            {
                "file": "pwndbg/commands/ai.py",
                "line_number": 21,
                "reason": "Import chain in the traceback shows pwndbg.commands.ai importing context: \"../../pwndbg/commands/ai.py:21: in <module>    from pwndbg.commands import context\"; this module appears in the stack leading to the TypeError during collection."
            },
            {
                "file": "pwndbg/commands/context.py",
                "line_number": 24,
                "reason": "Traceback shows context.py is importing heap_tracking (\"../../pwndbg/commands/context.py:24: in <module>    import pwndbg.gdblib.heap_tracking\"), tying this file to the collection-time failure."
            },
            {
                "file": "pwndbg/commands/__init__.py",
                "line_number": 633,
                "reason": "Traceback repeatedly shows load_commands() in this file as the entry point for importing commands: \"../../pwndbg/commands/__init__.py:633: in load_commands\" which triggers the import chain that leads to the heap_tracking TypeError."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "Logs show failing pytest tests and assertion: \"tests/test_context_commands.py:85: AssertionError\" and \"FAILED tests/test_context_commands.py::test_empty_context_sections[''] - AssertionError: assert '... heap-tracker' == '...'\" (diff lines show the extra 'heap-tracker')."
            },
            {
                "category": "Runtime / Compatibility Error",
                "subcategory": "Incompatible Python syntax (PEP 604 '|' union) causing import failure",
                "evidence": "During collection pytest raised: \"../../pwndbg/gdblib/heap_tracking.py:88: in <module>    def resolve_address(name: str) -> int | None:\" followed by \"TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\" and logs indicate runner Python version \"Python 3.8.10\" which does not support the 'int | None' syntax at runtime."
            }
        ],
        "failed_job": [
            {
                "job": "tests (ubuntu-22.04)",
                "step": "Run tests",
                "command": "./tests.sh --cov (invokes pytest) \u2014 pytest raised AssertionError(s) in tests/test_context_commands.py"
            },
            {
                "job": "tests (ubuntu-20.04)",
                "step": "Run tests",
                "command": "./tests.sh --cov (invokes pytest) \u2014 pytest failed during collection due to TypeError from 'int | None' usage"
            }
        ]
    },
    {
        "sha_fail": "fc6215f93ad9e2be8a32dc18b75a3f5bf6381a16",
        "error_context": [
            "The CI run failed during the pylint pre-commit hook: pre-commit's 'pylint-with-spelling' hook reported a spelling/lint error in the repository module pylint/checkers/nested_min_max.py at line 70, flagging the misspelled word 'redunant' in a comment. The pre-commit hook exited with a non-zero status (hook exit code 16 and overall process exit code 1), causing the 'pylint' job step to fail."
        ],
        "relevant_files": [
            {
                "file": "pylint/checkers/nested_min_max.py",
                "line_number": 70,
                "reason": "Log shows: \"pylint/checkers/nested_min_max.py:70:0: C0401: Wrong spelling of a word 'redunant' in a comment\" and the comment line is printed: \"# Meaning, redunant call only if parent max call has more than 1 arg.\" \u2014 this ties the file and line directly to the pylint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Quality / Linting",
                "subcategory": "Pylint spelling/lint error (wrong-spelling-in-comment)",
                "evidence": "\"C0401: Wrong spelling of a word 'redunant' in a comment\" and \"Did you mean: ''redundant' or 'redundancy''? (wrong-spelling-in-comment)\" from the CI log for pylint/checkers/nested_min_max.py:70:0."
            },
            {
                "category": "CI Hook Failure",
                "subcategory": "pre-commit hook failed",
                "evidence": "The workflow ran \"pre-commit run --hook-stage manual pylint-with-spelling --all-files\" and logs include \"- hook id: pylint\", \"- exit code: 16\" and \"##[error]Process completed with exit code 1.\" indicating the pre-commit hook caused the job to fail."
            }
        ],
        "failed_job": [
            {
                "job": "pylint",
                "step": "Run pylint checks",
                "command": "pre-commit run --hook-stage manual pylint-with-spelling --all-files"
            }
        ]
    },
    {
        "sha_fail": "a14be35a9de01a87991618a5dbd6b96470d0f799",
        "error_context": [
            "The Codestyle step failed because the formatter `black --check` reported that one file would be reformatted, causing exit code 1. The log shows: \"would reformat /home/runner/work/errbot/errbot/errbot/core_plugins/vcheck.py\" and \"1 file would be reformatted...\", so the codestyle tox environment failed.",
            "The failing command was run as part of the workflow's Codestyle step (tox -e codestyle), which invoked `black --check errbot/ tests/ tools/` and returned a non-zero exit code."
        ],
        "relevant_files": [
            {
                "file": "errbot/core_plugins/vcheck.py",
                "line_number": null,
                "reason": "Log output from the codestyle step: \"would reformat /home/runner/work/errbot/errbot/errbot/core_plugins/vcheck.py\" and \"1 file would be reformatted, 121 files would be left unchanged.\" These lines tie this file directly to the black formatting failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Black would reformat file (formatting mismatch)",
                "evidence": "Context lines: \"would reformat /home/runner/work/errbot/errbot/errbot/core_plugins/vcheck.py\", \"1 file would be reformatted...\", and \"codestyle: commands[0]> black --check errbot/ tests/ tools/\"; black returned exit code 1."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Codestyle",
                "command": "black --check errbot/ tests/ tools/ (run inside tox -e codestyle)"
            }
        ]
    },
    {
        "sha_fail": "8a04007d606de7a355f904407294f8ad5d2b7374",
        "error_context": [
            "The Codestyle step in the build job (matrix python-version=3.9) failed because Black's check reported an unformatted file. The log shows Black ran via the codestyle tox environment and printed \"would reformat /home/runner/work/errbot/errbot/tests/commands_test.py\" and \"1 file would be reformatted\", causing exit code 1.",
            "Concretely, running tox -e codestyle (which invoked `black --check errbot/ tests/ tools/`) produced a non-zero exit due to formatting required for tests/commands_test.py."
        ],
        "relevant_files": [
            {
                "file": "tests/commands_test.py",
                "line_number": null,
                "reason": "Log message: \"would reformat /home/runner/work/errbot/errbot/tests/commands_test.py\" and summary lines \"1 file would be reformatted\" and \"codestyle: commands[0]> black --check errbot/ tests/ tools/\" tie this file to the Black formatting failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Black check failed (file needs reformatting)",
                "evidence": "Logs show Black was run (\"codestyle: commands[0]> black --check errbot/ tests/ tools/\") and reported \"would reformat /home/runner/work/errbot/errbot/tests/commands_test.py\" and \"1 file would be reformatted\", followed by \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "build (python-version=3.9)",
                "step": "Codestyle",
                "command": "tox -e codestyle (invoked 'black --check errbot/ tests/ tools/')"
            }
        ]
    },
    {
        "sha_fail": "ce191b811e255722bfb4f5c2c7c30bcb7a4a3d59",
        "error_context": [
            "The CI job failed during the 'Pip install and run pylint' step: pylint reported a lint violation 'C0301: Line too long' in the repository file openvino/tools/accuracy_checker/annotation_converters/amazon.py (line 42). Pylint then exited with a non\u2011zero exit code (Process completed with exit code 16), causing the step and job to fail."
        ],
        "relevant_files": [
            {
                "file": "openvino/tools/accuracy_checker/annotation_converters/amazon.py",
                "line_number": 42,
                "reason": "Log shows a pylint error: 'openvino/tools/accuracy_checker/annotation_converters/amazon.py:42:0: C0301: Line too long (125/120) (line-too-long)', which ties this file and line directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Pylint C0301 - Line too long",
                "evidence": "Log line: 'openvino/tools/accuracy_checker/annotation_converters/amazon.py:42:0: C0301: Line too long (125/120) (line-too-long)'. Also the step ended with '##[error]Process completed with exit code 16.'"
            }
        ],
        "failed_job": [
            {
                "job": "accuracy_checker",
                "step": "Pip install and run pylint",
                "command": "PYTHONPATH=. python -m pylint --rcfile=.pylintrc `find -wholename '?*/**/*.py' -not -path \"./tests/*\" -not -path \"./build/*\"`"
            }
        ]
    },
    {
        "sha_fail": "07bc10c0e7858b22e9345812af8e6bb6c4ef18be",
        "error_context": [
            "The CI job failed because pylint reported a linting error: a trailing whitespace (pylint C0303) in openvino/tools/accuracy_checker/evaluators/model_evaluator.py at line 805. The 'Pip install and run pylint' step exited non\u2011zero (Process completed with exit code 16) after running pylint.",
            "The failing step ran the repository's pylint invocation (PYTHONPATH=. python -m pylint --rcfile=.pylintrc ...) which returned an error due to the reported C0303 violation; this caused the job to fail."
        ],
        "relevant_files": [
            {
                "file": "openvino/tools/accuracy_checker/evaluators/model_evaluator.py",
                "line_number": 805,
                "reason": "Pylint output in the CI log shows: \"openvino/tools/accuracy_checker/evaluators/model_evaluator.py:805:0: C0303: Trailing whitespace (trailing-whitespace)\", tying this file and line directly to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Quality / Linting",
                "subcategory": "Pylint style violation (C0303: Trailing whitespace)",
                "evidence": "\"openvino/tools/accuracy_checker/evaluators/model_evaluator.py:805:0: C0303: Trailing whitespace (trailing-whitespace)\" and \"##[error]Process completed with exit code 16.\" in the pylint step logs."
            }
        ],
        "failed_job": [
            {
                "job": "accuracy_checker",
                "step": "Pip install and run pylint",
                "command": "PYTHONPATH=. python -m pylint --rcfile=.pylintrc `find -wholename '?*/**/*.py' -not -path \"./tests/*\" -not -path \"./build/*\"`"
            }
        ]
    },
    {
        "sha_fail": "386bf6f0815368b78261be43bf90e203dfe9c13f",
        "error_context": [
            "The CI run failed while executing the examples runner step. Running examples/graph_prediction/general_gnn.py (inside the 'Run all examples' step) triggered spektral.datasets.TUDataset which called scikit-learn's OneHotEncoder with the deprecated keyword argument 'sparse'. The interpreter raised a TypeError: \"OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\" (trace shows spektral/datasets/tudataset.py calling _normalize and constructing OneHotEncoder at line 219)."
        ],
        "relevant_files": [
            {
                "file": "spektral/examples/graph_prediction/general_gnn.py",
                "line_number": 39,
                "reason": "Traceback shows this example file invoking TUDataset: \"File \"/home/runner/work/spektral/spektral/examples/graph_prediction/general_gnn.py\", line 39, in <module> data = TUDataset(\"PROTEINS\")\" \u2014 it is the user script that started the failing call."
            },
            {
                "file": "spektral/datasets/tudataset.py",
                "line_number": 128,
                "reason": "The traceback shows spektral's tudataset.read at line 128 calling _normalize and then at line 219 constructing OneHotEncoder: \"File .../site-packages/spektral/datasets/tudataset.py\", line 128, in read\" and \"File .../site-packages/spektral/datasets/tudataset.py\", line 219, in _normalize ... OneHotEncoder(sparse=False, categories=\\\"auto\\\")\" \u2014 this is where the unexpected-kwarg TypeError originates."
            },
            {
                "file": "spektral/data/dataset.py",
                "line_number": 118,
                "reason": "Traceback includes \"File \"/opt/hostedtoolcache/.../site-packages/spektral/data/dataset.py\", line 118, in __init__ self.graphs = self.read()\" indicating spektral/data/dataset.py initiated the read path that led to the OneHotEncoder TypeError."
            }
        ],
        "error_types": [
            {
                "category": "Runtime Error",
                "subcategory": "TypeError: unexpected keyword argument",
                "evidence": "\"TypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\" (log context lines show the exact TypeError and the call site in spektral/datasets/tudataset.py where OneHotEncoder(sparse=False, ...) is invoked)."
            },
            {
                "category": "Dependency Error",
                "subcategory": "Third-party API incompatibility (scikit-learn OneHotEncoder parameter change)",
                "evidence": "Logs show spektral code calls OneHotEncoder with 'sparse=False' (\"OneHotEncoder(sparse=False, categories=\\\"auto\\\")\") and Python raises an unexpected keyword argument error, indicating the installed scikit-learn version removed/renamed that parameter."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Run all examples",
                "command": "python examples/graph_prediction/general_gnn.py"
            }
        ]
    },
    {
        "sha_fail": "7431ad26a48d20a2850fe0ab242636d708727521",
        "error_context": [
            "The test suite run by pytest failed: tests/test_per_worker_seed.py::test_dataloader_epoch_diversity raised an AttributeError while multiprocessing attempted to pickle a locally defined class. CI logs show the error \"AttributeError: Can't get local object 'test_dataloader_epoch_diversity.<locals>.SimpleDataset'\", and the traceback points into the Python multiprocessing reduction/popen/spawn code on Windows.",
            "The failing command was pytest in the \"Test and lint (windows-latest, 3.12)\" job; the underlying cause is that SimpleDataset is defined as a local class inside the test function and therefore cannot be pickled by the spawn-based worker start method on Windows."
        ],
        "relevant_files": [
            {
                "file": "tests/test_per_worker_seed.py",
                "line_number": null,
                "reason": "Test failure reported as: \"FAILED tests/test_per_worker_seed.py::test_dataloader_epoch_diversity - AttributeError: Can't get local object 'test_dataloader_epoch_diversity.<locals>.SimpleDataset'\" (log message). This ties the failing test to this file."
            },
            {
                "file": "C:/hostedtoolcache/windows/Python/3.12.10/x64/Lib/multiprocessing/reduction.py",
                "line_number": 60,
                "reason": "Traceback shows the AttributeError raised in reduction.py: \"C:\\hostedtoolcache\\windows\\Python\\3.12.10\\x64\\Lib\\multiprocessing\\reduction.py:60: AttributeError\" and the frame with ForkingPickler.dump triggering the error."
            },
            {
                "file": "C:/hostedtoolcache/windows/Python/3.12.10/x64/Lib/multiprocessing/popen_spawn_win32.py",
                "line_number": 95,
                "reason": "Captured traceback includes a frame in popen_spawn_win32.py initiating the reduction dump: \"C:\\hostedtoolcache\\windows\\Python\\3.12.10\\x64\\Lib\\multiprocessing\\popen_spawn_win32.py:95: in __init__\" which is part of the multiprocessing spawn path used on Windows."
            },
            {
                "file": "C:/hostedtoolcache/windows/Python/3.12.10/x64/Lib/multiprocessing/spawn.py",
                "line_number": 122,
                "reason": "Captured stderr teardown includes spawn.py frames: \"File \\\"C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.12.10\\\\x64\\\\Lib\\\\multiprocessing\\\\spawn.py\\\", line 122, in spawn_main\" showing the worker spawn process involved in the failure."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AttributeError from multiprocessing pickling (can't get local object)",
                "evidence": "\"FAILED tests/test_per_worker_seed.py::test_dataloader_epoch_diversity - AttributeError: Can't get local object 'test_dataloader_epoch_diversity.<locals>.SimpleDataset'\" and traceback lines in reduction.py showing ForkingPickler.dump triggered the AttributeError (reduction.py:60)."
            }
        ],
        "failed_job": [
            {
                "job": "Test and lint (windows-latest, 3.12)",
                "step": "Run PyTest",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "bd46af653e25571f377664c7b7e9228ae8b28e96",
        "error_context": [
            "The style-check job failed during linting: ruff reported lint errors (F821 undefined name) in source code. The log shows a code snippet with the annotation 'params: Dict[str, Any] = {}' and the marker '^^^ F821' at that line, and the run ends with 'Found 2 errors.'",
            "Ruff format ran and reformatted 1 file, but the subsequent 'ruff check .' step exited non\u2011zero due to the lint errors. The logs do not include a file path for the failing frame, so the exact file causing F821 is not present in the provided log excerpts."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Undefined name (F821)",
                "evidence": "Log shows the lint marker '^^^ F821' under the line 'params: Dict[str, Any] = {}' and the lines 'Found 2 errors.' plus the step 'ruff check .' in the run, indicating ruff reported undefined-name lint errors."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
        "error_context": [
            "The style-check job failed because ruff reported two unused-import lint errors (F401) and exited with a non-zero status. The logs show unused import of `pydantic.BaseModel` in agno/models/meta/llama.py (line 7) and in agno/models/perplexity/perplexity.py (line 5), and ruff finished with \"Found 2 errors.\" and \"Process completed with exit code 1.\"",
            "These are formatting/linting issues (fixable by removing or using the imported symbol); the failing step was the 'Ruff check' step which runs `ruff check .`."
        ],
        "relevant_files": [
            {
                "file": "agno/models/meta/llama.py",
                "line_number": 7,
                "reason": "Log shows: \"agno/models/meta/llama.py:7:22: F401 [*] `pydantic.BaseModel` imported but unused\" and the shown source lines highlight `from pydantic import BaseModel` with the F401 marker and advice to remove the unused import."
            },
            {
                "file": "agno/models/perplexity/perplexity.py",
                "line_number": 5,
                "reason": "Log shows: \"agno/models/perplexity/perplexity.py:5:22: F401 [*] `pydantic.BaseModel` imported but unused\" and adjacent context displays the import line flagged by ruff and the message recommending removal."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "Log lines: \"agno/models/meta/llama.py:7:22: F401 ... `pydantic.BaseModel` imported but unused\" and \"agno/models/perplexity/perplexity.py:5:22: F401 ... `pydantic.BaseModel` imported but unused\" plus \"Found 2 errors.\" and \"[*] 2 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "c27ea5332c1d979ad2fc0c2b09ff571c9538f423",
        "error_context": [
            "A unit test failed during the 'tests' job while running pytest: libs/agno/tests/unit/utils/test_string.py::test_parse_concatenated_reasoning_steps failed with an AssertionError (assert 1 == 2) at libs/agno/tests/unit/utils/test_string.py:251. The test attempted to parse two concatenated JSON reasoning-step objects but only one ReasoningStep was returned.",
            "Captured test output shows a JSON parsing problem immediately before the assertion: \"WARNING  Failed to parse cleaned JSON: Extra data: line 1 column 59 (char 58)\", indicating the parser encountered extra data when trying to combine/clean the concatenated JSON, which likely caused the missing second reasoning step."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/utils/test_string.py",
                "line_number": 251,
                "reason": "The pytest failure and traceback point directly to this test file and line: \"libs/agno/tests/unit/utils/test_string.py:251: AssertionError\" and the test is reported as failed: \"FAILED libs/agno/tests/unit/utils/test_string.py::test_parse_concatenated_reasoning_steps - AssertionError: assert 1 == 2\". The test source lines around the failure show the failing assertion and the call to parse_response_model_str."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "\"FAILED libs/agno/tests/unit/utils/test_string.py::test_parse_concatenated_reasoning_steps - AssertionError: assert 1 == 2\" and the traceback line \"libs/agno/tests/unit/utils/test_string.py:251: AssertionError\" in the CI logs."
            },
            {
                "category": "Parsing Error",
                "subcategory": "JSON parsing error (Extra data)",
                "evidence": "Captured stdout immediately before the assertion: \"WARNING  Failed to parse cleaned JSON: Extra data: line 1 column 59 (char 58)\", indicating the JSON parser encountered extra data while processing the concatenated JSON input."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "python -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "c434e89bee35d93f4e741c32dc36c5a9a68404df",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error in agno/app/base.py:194 \u2014 \"BaseAPIApp\" has no attribute \"type\". The mypy step ran `mypy .`, checked 490 files and reported: \"Found 1 error in 1 file\", causing the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/app/base.py",
                "line_number": 194,
                "reason": "Log shows a mypy error frame: \"agno/app/base.py:194: error: \\\"BaseAPIApp\\\" has no attribute \\\"type\\\"  [attr-defined]\" and the next line \"Found 1 error in 1 file (checked 490 source files)\" ties this file/line directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy attribute error (attr-defined) \u2014 missing attribute on class",
                "evidence": "Log line: \"agno/app/base.py:194: error: \\\"BaseAPIApp\\\" has no attribute \\\"type\\\"  [attr-defined]\" and \"Found 1 error in 1 file (checked 490 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "ca75b3dc2cfaf4d6a9409109f10b285bdf6a8097",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: an incompatible return type in agno/utils/chain.py at line 84. The Mypy step ('mypy .') produced the message 'agno/utils/chain.py:84: error: Incompatible return value type (got \"RunResponse\", expected \"str\")' and the job exited with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/utils/chain.py",
                "line_number": 84,
                "reason": "Log shows a mypy error at this file and line: 'agno/utils/chain.py:84: error: Incompatible return value type (got \"RunResponse\", expected \"str\")', and mypy reported 'Found 1 error in 1 file'."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: Incompatible return value type",
                "evidence": "Log line: 'agno/utils/chain.py:84: error: Incompatible return value type (got \"RunResponse\", expected \"str\")  [return-value]' and 'Found 1 error in 1 file (checked 431 source files)'."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "d1e88b8766d152d7d4e7911a0072591db1eb4bcd",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: a coroutine value is not awaited in agno/agent/agent.py at line 6551 (error: \"Value of type \\\"Coroutine[Any, Any, None]\\\" must be used [unused-coroutine]\"). Mypy exited with an error (Found 1 error in 1 file) which caused the style-check (Mypy) step to terminate with exit code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 6551,
                "reason": "Log shows a mypy error pointing to this file and line: \"agno/agent/agent.py:6551: error: Value of type \\\"Coroutine[Any, Any, None]\\\" must be used  [unused-coroutine]\" and the follow-up note \"Are you missing an await?\""
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy unused-coroutine / missing await",
                "evidence": "\"agno/agent/agent.py:6551: error: Value of type \\\"Coroutine[Any, Any, None]\\\" must be used  [unused-coroutine]\" and \"agno/agent/agent.py:6551: note: Are you missing an await?\" plus \"Found 1 error in 1 file (checked 522 source files)\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "d78ebf9508e112a8d2526ec26bcc37d8e8a2bfa2",
        "error_context": [
            "The CI style-check job failed during the Mypy type-checking step: mypy reported 22 type errors all from agno/knowledge/document.py (e.g. lines 29, 32, 33, 34, 35, 43, 68\u201382).",
            "The errors indicate incorrect handling of a Union[Document, dict[str, Any]] (mypy 'union-attr', 'dict-item', and 'list-item' complaints), causing the 'Mypy' step (command 'mypy .') to exit with code 1 and fail the style-check job."
        ],
        "relevant_files": [
            {
                "file": "agno/knowledge/document.py",
                "line_number": 29,
                "reason": "Mypy errors in the logs reference this file multiple times (e.g. 'agno/knowledge/document.py:29: error: Item \"dict[str, Any]\" of \"Union[Document, dict[str, Any]]\" has no attribute \"name\"  [union-attr]' and many further lines up to ':82'), and the summary 'Found 22 errors in 1 file' ties the failure to this single file."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: union attribute / dict-item / list-item type errors",
                "evidence": "Log lines show mypy errors such as 'agno/knowledge/document.py:29: error: Item \"dict[str, Any]\" of \"Union[Document, dict[str, Any]]\" has no attribute \"name\"  [union-attr]' and 'agno/knowledge/document.py:35: error: Unpacked dict entry 1 has incompatible type \"Union[Document, dict[str, Any]]\"; expected \"SupportsKeysAndGetItem[str, Any]\"  [dict-item]'; the run ended with 'Found 22 errors in 1 file' and 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
        "error_context": [
            "The style-check job failed when running mypy: mypy reported two type errors in agno/workflow/v2/workflow.py and exited with a non-zero status. The logs show errors at lines 3220 and 3226 complaining that an Optional[dict[str, Any]] was passed where dict[str, Any] was expected, and the job ended with \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "agno/workflow/v2/workflow.py",
                "line_number": 3220,
                "reason": "Mypy error lines in the CI log: \"agno/workflow/v2/workflow.py:3220: error: Argument 1 to \\\"merge_dictionaries\\\" has incompatible type \\\"Optional[dict[str, Any]]\\\"; expected \\\"dict[str, Any]\\\"\" (also a second error at line 3226), and \"Found 2 errors in 1 file\" ties this file directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy argument type mismatch (arg-type)",
                "evidence": "\"agno/workflow/v2/workflow.py:3220: error: Argument 1 to \\\"merge_dictionaries\\\" has incompatible type \\\"Optional[dict[str, Any]]\\\"; expected \\\"dict[str, Any]\\\"  [arg-type]\" and \"agno/workflow/v2/workflow.py:3226: error: Argument 1 to \\\"merge_dictionaries\\\" has incompatible type \\\"Optional[dict[str, Any]]\\\"; expected \\\"dict[str, Any]\\\"\"; CI also reports \"Found 2 errors in 1 file\" and the job ended with \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "dc07b7a4e5314d29edb7e9c30d36e12a6dec3dd7",
        "error_context": [
            "The tests job failed during pytest collection because imports in repository code could not be satisfied: agno.tools.crawl4ai attempted to import BrowserConfig from the installed package crawl4ai but the package's __init__ did not provide it, and agno.reranker.infinity raised a ModuleNotFoundError for infinity_client. The style-check job also failed: mypy reported missing imports for infinity_client and type errors in playground router code."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/tools/test_crawl4ai.py",
                "line_number": 7,
                "reason": "Pytest error: \"ImportError while importing test module '/home/runner/work/agno/agno/libs/agno/tests/unit/tools/test_crawl4ai.py'\" and log shows \"libs/agno/tests/unit/tools/test_crawl4ai.py:7: in <module> from agno.tools.crawl4ai import Crawl4aiTools\" \u2014 this test module failed to import due to the downstream import error."
            },
            {
                "file": "libs/agno/agno/tools/crawl4ai.py",
                "line_number": 8,
                "reason": "Traceback frame: \"libs/agno/agno/tools/crawl4ai.py:8: in <module>     from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\" followed by \"ImportError: cannot import name 'BrowserConfig' from 'crawl4ai' (/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/crawl4ai/__init__.py)\" \u2014 ties this file to the import failure."
            },
            {
                "file": ".venv/lib/python3.12/site-packages/crawl4ai/__init__.py",
                "line_number": null,
                "reason": "Import error originates from the installed package: log states \"cannot import name 'BrowserConfig' from 'crawl4ai' (/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/crawl4ai/__init__.py)\", indicating an API mismatch or missing symbol in that package installation."
            },
            {
                "file": "libs/agno/tests/unit/vectordb/test_chromadb.py",
                "line_number": 8,
                "reason": "Pytest error: \"ImportError while importing test module '/home/runner/work/agno/agno/libs/agno/tests/unit/vectordb/test_chromadb.py'\" and trace shows \"libs/agno/tests/unit/vectordb/test_chromadb.py:8: in <module>     from agno.vectordb.chroma import ChromaDb\" \u2014 test collection failed due to import errors in project code."
            },
            {
                "file": "libs/agno/agno/reranker/infinity.py",
                "line_number": 9,
                "reason": "Traceback shows \"libs/agno/agno/reranker/infinity.py:9: in <module>     from infinity_client import AuthenticatedClient, Client\" followed by \"E   ModuleNotFoundError: No module named 'infinity_client'\" and later an explicit \"ImportError: infinity_client not installed, please run `pip install infinity_client`\" \u2014 this file raised the missing-dependency error."
            },
            {
                "file": "libs/agno/agno/vectordb/chroma/chromadb.py",
                "line_number": 17,
                "reason": "Traceback frame: \"libs/agno/agno/vectordb/chroma/chromadb.py:17: in <module>     from agno.reranker.base import Reranker\" which links the ChromaDb import chain to the reranker/infinity import failure reported by pytest."
            },
            {
                "file": "libs/agno/agno/reranker/__init__.py",
                "line_number": 3,
                "reason": "Traceback shows \"libs/agno/agno/reranker/__init__.py:3: in <module>\" immediately before the infinity module import error, tying this package initializer into the import failure chain."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "ImportError during test collection (cannot import symbol from installed package)",
                "evidence": "\"ImportError while importing test module '/home/runner/work/agno/agno/libs/agno/tests/unit/tools/test_crawl4ai.py'.\" and \"E   ImportError: cannot import name 'BrowserConfig' from 'crawl4ai' (/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/crawl4ai/__init__.py)\""
            },
            {
                "category": "Dependency Error",
                "subcategory": "Missing dependency (ModuleNotFoundError for infinity_client)",
                "evidence": "\"libs/agno/agno/reranker/infinity.py:9: in <module>     from infinity_client import AuthenticatedClient, Client\" followed by \"E   ModuleNotFoundError: No module named 'infinity_client'\" and \"E   ImportError: infinity_client not installed, please run `pip install infinity_client`\""
            },
            {
                "category": "Type Checking",
                "subcategory": "Mypy missing-imports and arg-type errors",
                "evidence": "Style-check logs: \"agno/reranker/infinity.py:9: error: Cannot find implementation or library stub for module named \\\"infinity_client\\\"  [import-not-found]\" and \"agno/app/playground/sync_router.py:556: error: Argument 1 to \\\"append\\\" of \\\"list\\\" has incompatible type \\\"dict[str, Union[str, Any, int, None]]\\\"; expected \\\"WorkflowSessionResponse\\\"  [arg-type]\""
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "python -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            },
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "dc63689a775dcb8f90cac9824149e21c3a868cc1",
        "error_context": [
            "The style-check job failed because mypy reported type-checking errors. Mypy output shows two errors in agno/team/team.py (lines 587 and 598), and the job ended with \"Process completed with exit code 1.\" The failing step is the Mypy step (command: \"mypy .\") in the style-check job."
        ],
        "relevant_files": [
            {
                "file": "agno/team/team.py",
                "line_number": 587,
                "reason": "Log shows mypy errors pointing to this file: \"agno/team/team.py:587: error: Need type annotation for \\\"effective_filters\\\" (...)\" and \"agno/team/team.py:598: error: Incompatible types in assignment (...)\" \u2014 these messages tie this file and those lines directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type errors (missing annotation and incompatible assignment)",
                "evidence": "Logs: \"agno/team/team.py:587: error: Need type annotation for \\\"effective_filters\\\" (...)  [var-annotated]\" and \"agno/team/team.py:598: error: Incompatible types in assignment (expression has type \\\"Optional[dict[str, Any]]\\\", variable has type \\\"dict[Any, Any]\\\")  [assignment]\"; \"Found 2 errors in 1 file\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "e22825fa77b3e549db08147a90c061e695726fb2",
        "error_context": [
            "The style-check job failed because the ruff linter reported lint errors (F821 undefined name). The log shows a code snippet where EvalType is flagged with '^^^^^^^ F821' on the line 'eval_type=EvalType.PERFORMANCE,' (around line 332 of a linted file) and the run ended with 'Found 3 errors.' and 'Process completed with exit code 1.' The failing step is the 'Ruff check' step run in the style-check (3.9) job."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Undefined name (F821) reported by ruff",
                "evidence": "Log shows the lint frame: '332 |                 eval_type=EvalType.PERFORMANCE,' with a caret pointing under 'EvalType' and the marker '^^^^^^^ F821' followed by 'Found 3 errors.' and '##[error]Process completed with exit code 1.' \u2014 this is the standard ruff/flake8-style undefined-name error (F821)."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "e36b14dc3ee04beb3f0d8c2b89252eb864ea5c1a",
        "error_context": [
            "The style-check job failed during the Mypy step: mypy reported import-not-found errors for external modules used in the codebase. Specifically, agno/document/chunking/markdown.py (lines 6-7) references unstructured.chunking.title and unstructured.partition.md which mypy cannot find, and agno/vectordb/qdrant/qdrant.py (line 124) references fastembed which mypy cannot find. The mypy run ended with \"Found 3 errors in 2 files\" and the step exited with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/document/chunking/markdown.py",
                "line_number": 6,
                "reason": "Log shows: \"agno/document/chunking/markdown.py:6: error: Cannot find implementation or library stub for module named \\\"unstructured.chunking.title\\\"  [import-not-found]\" tying this file and line to the mypy failure."
            },
            {
                "file": "agno/document/chunking/markdown.py",
                "line_number": 7,
                "reason": "Log shows: \"agno/document/chunking/markdown.py:7: error: Cannot find implementation or library stub for module named \\\"unstructured.partition.md\\\"  [import-not-found]\" tying this file and line to the mypy failure."
            },
            {
                "file": "agno/vectordb/qdrant/qdrant.py",
                "line_number": 124,
                "reason": "Log shows: \"agno/vectordb/qdrant/qdrant.py:124: error: Cannot find implementation or library stub for module named \\\"fastembed\\\"  [import-not-found]\" tying this file and line to the mypy failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Import-not-found (mypy missing implementation or stub)",
                "evidence": "mypy output: \"agno/document/chunking/markdown.py:6: error: Cannot find implementation or library stub for module named \\\"unstructured.chunking.title\\\" [import-not-found]\", \"agno/document/chunking/markdown.py:7: error: Cannot find implementation or library stub for module named \\\"unstructured.partition.md\\\" [import-not-found]\", and \"agno/vectordb/qdrant/qdrant.py:124: error: Cannot find implementation or library stub for module named \\\"fastembed\\\" [import-not-found]\"; summary line: \"Found 3 errors in 2 files (checked 476 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "ede117552a48bee7f674fbfab87d9586f2fabe19",
        "error_context": [
            "The style-check job failed during the Mypy step: mypy reported a name redefinition in agno/models/anthropic/claude.py. The log shows \"agno/models/anthropic/claude.py:61: error: Name \\\"default_headers\\\" already defined on line 59 [no-redef]\", and mypy reported \"Found 1 error in 1 file (checked 471 source files)\", causing the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/models/anthropic/claude.py",
                "line_number": 61,
                "reason": "Log output: \"agno/models/anthropic/claude.py:61: error: Name \\\"default_headers\\\" already defined on line 59  [no-redef]\" \u2014 this traceback/error message ties this file and line to the mypy failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy name redefinition (no-redef)",
                "evidence": "Log: \"agno/models/anthropic/claude.py:61: error: Name \\\"default_headers\\\" already defined on line 59  [no-redef]\" and \"Found 1 error in 1 file (checked 471 source files)\" \u2014 indicates mypy detected a redefined name and failed."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "f06bfb4ef15132a04a3983b4aa40f2e385ef7c04",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: a name redefinition of \"run_response\" in agno/agent/agent.py (defined on line 1108 and again on line 1116). The mypy step (command 'mypy .') produced the error and exited with code 1: \"Found 1 error in 1 file (checked 434 source files)\"."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 1116,
                "reason": "Log shows a mypy error pointing at this file and line: \"agno/agent/agent.py:1116: error: Name \\\"run_response\\\" already defined on line 1108  [no-redef]\" \u2014 proving this file/lines caused the mypy failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy name redefinition (no-redef)",
                "evidence": "Exact log: \"agno/agent/agent.py:1116: error: Name \\\"run_response\\\" already defined on line 1108  [no-redef]\" and \"Found 1 error in 1 file (checked 434 source files)\" \u2014 indicates mypy detected a duplicate name definition."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "f2436c62292765f014a0dd30a013035abd13c33f",
        "error_context": [
            "The style-check job failed because mypy (static type checking) reported a type error and exited with a non\u2011zero status. Mypy produced the message \"agno/eval/accuracy.py:460: error: Return value expected  [return-value]\" and then \"Found 1 error in 1 file (checked 482 source files)\", causing the job to stop with exit code 1.",
            "The failing step is the Mypy step in the style-check job (Python 3.9), run with the command \"mypy .\" from the libs/agno working directory."
        ],
        "relevant_files": [
            {
                "file": "agno/eval/accuracy.py",
                "line_number": 460,
                "reason": "Log shows a mypy error pointing at this file and line: \"agno/eval/accuracy.py:460: error: Return value expected  [return-value]\" and \"Found 1 error in 1 file (checked 482 source files)\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy 'Return value expected' (return-value)",
                "evidence": "Exact mypy output: \"agno/eval/accuracy.py:460: error: Return value expected  [return-value]\" and \"Found 1 error in 1 file (checked 482 source files)\"; the workflow shows the step ran \"mypy .\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check (3.9)",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "f6f8da08fb440f8856510d0837876c41eb182dfc",
        "error_context": [
            "The style-check job failed during the Mypy type-checking step: mypy reported a type error in agno/agent/agent.py at line 995, saying Item \"None\" of \"Optional[Model]\" has no attribute \"aresponse\". Mypy found 1 error in 1 file and the job exited with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 995,
                "reason": "Log shows a mypy error at this location: \"agno/agent/agent.py:995: error: Item \\\"None\\\" of \\\"Optional[Model]\\\" has no attribute \\\"aresponse\\\"  [union-attr]\" and \"Found 1 error in 1 file (checked 444 source files)\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attribute error (union-attr)",
                "evidence": "\"agno/agent/agent.py:995: error: Item \\\"None\\\" of \\\"Optional[Model]\\\" has no attribute \\\"aresponse\\\"  [union-attr]\" and \"Found 1 error in 1 file (checked 444 source files)\" from the style-check job logs."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "fb10c6e8e2f7ca6d855f8cfeda8bae8f4a644e7c",
        "error_context": [
            "The style-check job failed during the Mypy step: mypy reported 5 type/syntax errors in agno/app/discord/client.py and the step exited with a non\u2011zero status. Logs show specific mypy messages (e.g. at lines 144, 146, 147, 153, 178) including a type incompatibility where TeamRunResponse is used where RunResponse is expected and a syntax error reporting that the X | Y union syntax requires Python 3.10 while the job runs Python 3.9.",
            "Because the workflow's style-check matrix uses python-version 3.9, the X | Y union syntax reported by mypy is a Python-version compatibility issue in addition to concrete type-mismatch errors; mypy's failures caused the process to exit with code 1 (\"Found 5 errors in 1 file\" -> process completed with exit code 1)."
        ],
        "relevant_files": [
            {
                "file": "agno/app/discord/client.py",
                "line_number": 144,
                "reason": "Mypy output lines show errors in this file: e.g. \"agno/app/discord/client.py:144: error: Argument 1 to \\\"_handle_response_in_thread\\\" of \\\"DiscordClient\\\" has incompatible type \\\"TeamRunResponse\\\"; expected \\\"RunResponse\\\"\" and additional errors at lines 146, 147, 153, 178; logs also state \"Found 5 errors in 1 file\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch (argument type incompatible / union attribute missing)",
                "evidence": "Log: \"agno/app/discord/client.py:144: error: Argument 1 to \\\"_handle_response_in_thread\\\" of \\\"DiscordClient\\\" has incompatible type \\\"TeamRunResponse\\\"; expected \\\"RunResponse\\\"\" and \"agno/app/discord/client.py:147: error: Item \\\"TeamRunResponse\\\" of \\\"Union[RunResponse, TeamRunResponse]\\\" has no attribute \\\"tools_requiring_confirmation\\\"\"."
            },
            {
                "category": "Configuration Error",
                "subcategory": "Python version incompatibility (syntax unsupported on Python 3.9)",
                "evidence": "Log: \"agno/app/discord/client.py:146: error: X | Y syntax for unions requires Python 3.10  [syntax]\" while workflow's style-check job matrix uses python-version: \"3.9\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
        "error_context": [
            "Pytest ran the unit test suite and three tests in libs/agno/tests/unit/reader/test_pdf_reader.py failed with AssertionError: the tests assert that document names equal \"ThaiRecipes\" but the assertion evaluated to False (see libs/agno/tests/unit/reader/test_pdf_reader.py:46, :57, :148).",
            "The failures occurred during the 'Run tests for Agno' step (pytest) in the 'tests' job (Python 3.12) \u2014 CI shows \"FAILED libs/agno/tests/unit/reader/test_pdf_reader.py::... - assert False\" and the final summary \"====== 3 failed, 838 passed... ======\"."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/reader/test_pdf_reader.py",
                "line_number": 46,
                "reason": "Pytest traceback and failure lines reference this test file directly: e.g. \"libs/agno/tests/unit/reader/test_pdf_reader.py:46: AssertionError\" and \"FAILED libs/agno/tests/unit/reader/test_pdf_reader.py::test_pdf_reader_read_file - assert False\"; the logged failing assertion is 'assert all(doc.name == \"ThaiRecipes\" for doc in documents)'."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "Multiple log lines show assertion failures and pytest failure summary, e.g. \"E       assert False\" and \"libs/agno/tests/unit/reader/test_pdf_reader.py:46: AssertionError\" plus \"FAILED libs/agno/tests/unit/reader/test_pdf_reader.py::test_pdf_reader_read_file - assert False\" and final summary \"====== 3 failed, 838 passed... ======\"."
            },
            {
                "category": "Warnings",
                "subcategory": "DeprecationWarning / RuntimeWarning from dependencies",
                "evidence": "Several non-fatal warnings are present in the logs (not the cause of the test failures) such as \"/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\" and \"libs/agno/agno/storage/postgres.py:205: SADeprecationWarning: The Column.copy() method is deprecated\"."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "python -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "ff8929ce168fb87cf50f6a52e4cc3b12b8fd5e2e",
        "error_context": [
            "The style-check job failed because ruff reported lint errors and returned a non\u2011zero exit code. Logs show ruff check produced two errors (one F841 and one F401) and the step exited with \"Process completed with exit code 1.\"",
            "Specifically, agno/utils/functions.py has an unused local variable assignment at line 33 (F841) and tests/unit/utils/test_functions.py imports typing.Optional but does not use it (F401 at line 3). These lint errors caused the 'Ruff check' step to fail."
        ],
        "relevant_files": [
            {
                "file": "agno/utils/functions.py",
                "line_number": 33,
                "reason": "Log shows \"agno/utils/functions.py:33:33: F841 [*] Local variable `e` is assigned to but never used\" and displays the frame \"33 |             except Exception as e:\" with the F841 marker, tying this file/line to the ruff lint failure."
            },
            {
                "file": "tests/unit/utils/test_functions.py",
                "line_number": 3,
                "reason": "Log shows \"tests/unit/utils/test_functions.py:3:26: F401 [*] `typing.Optional` imported but unused\" and displays the import line \"3 | from typing import Dict, Optional\", tying this test file/line to the ruff lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused local variable (ruff F841)",
                "evidence": "\"agno/utils/functions.py:33:33: F841 [*] Local variable `e` is assigned to but never used\" and the context line \"33 |             except Exception as e:\" in the logs."
            },
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "\"tests/unit/utils/test_functions.py:3:26: F401 [*] `typing.Optional` imported but unused\" and the context lines showing \"3 | from typing import Dict, Optional\" and the help: \"Remove unused import: `typing.Optional`\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "48376e59c2d51260b1c6d2a14dcf58ab5066f88e",
        "error_context": [
            "A pytest unit test failed: tests/basic/test_openrouter.py::test_openrouter_get_model_info_from_cache. The assertion at tests/basic/test_openrouter.py:42 expected info[\"input_cost_per_token\"] == 0.0001 but the test observed 100.0 (E       assert 100.0 == 0.0001). This failure occurred while running the test suite in the build job (Python 3.12) during the 'Run tests' step (pytest)."
        ],
        "relevant_files": [
            {
                "file": "tests/basic/test_openrouter.py",
                "line_number": 42,
                "reason": "Log shows the failing assertion and traceback: \">       assert info[\\\"input_cost_per_token\\\"] == 0.0001\" followed by \"E       assert 100.0 == 0.0001\" and \"tests/basic/test_openrouter.py:42: AssertionError\"; pytest also reports \"FAILED tests/basic/test_openrouter.py::test_openrouter_get_model_info_from_cache - assert 100.0 == 0.0001\"."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "Pytest output: \"E       assert 100.0 == 0.0001\" and \"tests/basic/test_openrouter.py:42: AssertionError\" plus \"FAILED tests/basic/test_openrouter.py::test_openrouter_get_model_info_from_cache - assert 100.0 == 0.0001\"."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.12)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "4ab8faf21e04489caf1d2f0dfbace03521e90b25",
        "error_context": [
            "The test suite (pytest) failed: tests/basic/test_repomap.py::TestRepoMapAllLanguages::test_language_elixir raised an AssertionError. The failure is caused by an assertion self.assertGreater(len(result.strip().splitlines()), 1) failing because the result contains only one line.",
            "Log evidence shows the pytest failure and captured stdout: 'E   AssertionError: 1 not greater than 1' and 'FAILED tests/basic/test_repomap.py::TestRepoMapAllLanguages::test_language_elixir - AssertionError: 1 not greater than 1', and the captured output lists only a single line 'test.ex', which explains the assertion failure."
        ],
        "relevant_files": [
            {
                "file": "tests/basic/test_repomap.py",
                "line_number": 300,
                "reason": "The pytest traceback references this file and lines where the test is invoked and the assertion fails: 'tests/basic/test_repomap.py:300:' and 'tests/basic/test_repomap.py:415: in _test_language_repo_map' with the failing assertion 'self.assertGreater(len(result.strip().splitlines()), 1)' and error 'E   AssertionError: 1 not greater than 1'."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (unexpected output length)",
                "evidence": "Pytest output: 'E   AssertionError: 1 not greater than 1' and 'FAILED tests/basic/test_repomap.py::TestRepoMapAllLanguages::test_language_elixir - AssertionError: 1 not greater than 1'. The traceback shows the failing assertion 'self.assertGreater(len(result.strip().splitlines()), 1)' in tests/basic/test_repomap.py."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.9)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "5548acee0b31576cae313185aa3c859f88818939",
        "error_context": [
            "A pytest unit test failed: tests/basic/test_repo.py::TestRepo::test_commit_with_custom_committer_name. The test expected the commit author name to be \"Test User (aider)\" but the actual value was \"Test User\", causing an AssertionError at tests/basic/test_repo.py:217. The failure occurred while running the test suite (pytest) in the build job (python 3.12 matrix entry)."
        ],
        "relevant_files": [
            {
                "file": "tests/basic/test_repo.py",
                "line_number": 217,
                "reason": "The CI log shows an AssertionError at tests/basic/test_repo.py:217 and the failing test is reported as FAILED tests/basic/test_repo.py::TestRepo::test_commit_with_custom_committer_name; the log contains the failing assertion line \"self.assertEqual(commit.author.name, \\\"Test User (aider)\\\")\" and the error \"AssertionError: 'Test User' != 'Test User (aider)'\"."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "Logs show \"tests/basic/test_repo.py:217: AssertionError\" and \"FAILED tests/basic/test_repo.py::TestRepo::test_commit_with_custom_committer_name - AssertionError: 'Test User' != 'Test User (aider)'\", proving a pytest assertion failure."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.12)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "73f1acb9539c928be57de3ecb4e9d2b49da62d9f",
        "error_context": [
            "Pytest test suite failed: multiple unit tests in tests/basic/test_models.py and tests/basic/test_sendchat.py asserted that a mocked completion() was called with specific keyword arguments but observed calls included an extra tools=[{'googleSearch': {}}] argument, causing many AssertionError: \"expected call not found.\" messages. Separately, a TypeError was raised when calling litellm.completion in aider/models.py:982: \"<MagicMock name='completion' ...> got multiple values for keyword argument 'tools'\", indicating the mock was invoked with duplicate/ambiguous 'tools' values (leading to both assertion failures and a runtime TypeError). The failing step ran pytest (test runner) in the build job (build (3.10)), which terminated the job with exit code 1."
        ],
        "relevant_files": [
            {
                "file": "tests/basic/test_models.py",
                "line_number": null,
                "reason": "Multiple failure lines show this test file failing (e.g. \"FAILED tests/basic/test_models.py::TestModels::test_non_ollama_no_num_ctx - AssertionError: expected call not found.\" and other FAILED entries referencing tests/basic/test_models.py), proving the tests in this file failed due to mismatched mock calls."
            },
            {
                "file": "tests/basic/test_sendchat.py",
                "line_number": null,
                "reason": "Log shows a failing test in this file: \"FAILED tests/basic/test_sendchat.py::TestSendChat::test_send_completion_with_functions - TypeError: <MagicMock name='completion' ...> got multiple values for keyword argument 'tools'\", tying this test file to the failure."
            },
            {
                "file": "aider/models.py",
                "line_number": 982,
                "reason": "The TypeError is reported at aider/models.py:982 in the logs: \"aider/models.py:982: TypeError\" and the code line preceding it is shown: \"res = litellm.completion(tools=[{'googleSearch': {}}], **kwargs)\", which directly links this file/line to the runtime error."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.10.17/x64/lib/python3.10/unittest/mock.py",
                "line_number": 929,
                "reason": "The assertion failures originate from unittest.mock utilities; logs include \"/opt/hostedtoolcache/.../unittest/mock.py:929: AssertionError\" and the accompanying messages \"AssertionError: expected call not found.\" and Expected/Actual call dumps, showing the mock assertion failed in the standard library code invoked by the tests."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (mock assert_called_with mismatch)",
                "evidence": "Many lines show assertion failures from tests, e.g. \"E AssertionError: expected call not found.\" and repeated lines like \"Expected: completion(model='gpt-4', ... )\" vs \"Actual: completion(tools=[{'googleSearch': {}}], model='gpt-4', ...)\", proving tests expected a call without 'tools' but actual calls included 'tools'."
            },
            {
                "category": "Runtime Error",
                "subcategory": "TypeError raised when calling mocked function (multiple values for keyword argument)",
                "evidence": "Log shows \"TypeError: <MagicMock name='completion' id='139697145654544'> got multiple values for keyword argument 'tools'\" and identifies the location as \"aider/models.py:982\", proving a runtime TypeError occurred when invoking litellm.completion with duplicate/ambiguous 'tools' arguments."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.10)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "803a8db60cb2ce21c0e3b0ed2b773d2ecc5142bd",
        "error_context": [
            "A pytest unit test failed: tests/basic/test_models.py::TestModels::test_model_aliases raised an AssertionError. The test expected model.name == \"anthropic/claude-3-7-sonnet-20250219\" but got \"anthropic/claude-sonnet-4-20250514\" (see tests/basic/test_models.py:141). The failure occurred while running the test suite in the build matrix job for Python 3.10 (step: Run tests) using the pytest command."
        ],
        "relevant_files": [
            {
                "file": "tests/basic/test_models.py",
                "line_number": 141,
                "reason": "Log shows an AssertionError at tests/basic/test_models.py:141 and the short test summary: \"FAILED tests/basic/test_models.py::TestModels::test_model_aliases - AssertionError: 'anthropic/claude-sonnet-4-20250514' != 'anthropic/claude-3-7-sonnet-20250219'\"."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "CI log: \"tests/basic/test_models.py:141: AssertionError\" and \"FAILED tests/basic/test_models.py::TestModels::test_model_aliases - AssertionError: 'anthropic/claude-sonnet-4-20250514' != 'anthropic/claude-3-7-sonnet-20250219'\"; summary also shows \"1 failed, 477 passed, 1 skipped\"."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.10)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "82f33c12206d1ab7a19d4b5369a40c3016b255ee",
        "error_context": [
            "The pytest test suite failed: tests/basic/test_urls.py::test_urls raised an AssertionError because an HTTP request returned status code 503 instead of 200. The logs show the failing assertion at tests\\basic\\test_urls.py:15 and the message 'URL https://aider.chat/assets/icons/favicon-32x32.png returned status code 503', causing pytest to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "tests/basic/test_urls.py",
                "line_number": 15,
                "reason": "Log shows the traceback and failing assertion in this file: 'tests\\\\basic\\\\test_urls.py:15: AssertionError' and the assertion line 'assert response.status_code == 200, f\"URL {url} returned status code {response.status_code}\"' with the error 'AssertionError: URL https://aider.chat/assets/icons/favicon-32x32.png returned status code 503'."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (HTTP status mismatch)",
                "evidence": "'E           AssertionError: URL https://aider.chat/assets/icons/favicon-32x32.png returned status code 503' and 'E           assert 503 == 200' in the pytest output; summary 'FAILED tests/basic/test_urls.py::test_urls' and '1 failed, 465 passed, 5 skipped'."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.10)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "852f8655c69f4704965b19c0bbfadca6777ef23e",
        "error_context": [
            "Pytest run failed because multiple unit tests raised a runtime UnboundLocalError originating in aider/io.py:92. The test tracebacks show tests (tests/basic/test_commands.py and tests/basic/test_io.py) calling Commands.cmd_add and InputOutput.confirm_ask which enter a wrapper in aider/io.py; the wrapper's finally block references orig_buf_append even when it was never assigned, causing UnboundLocalError and making 8 tests fail.",
            "The failure occurred during the CI 'Run tests' step where pytest was executed (the job labeled in logs as \"build (3.9)\")."
        ],
        "relevant_files": [
            {
                "file": "aider/io.py",
                "line_number": 92,
                "reason": "Traceback shows the exception at aider/io.py:92: \"UnboundLocalError: local variable 'orig_buf_append' referenced before assignment\" and context lines show the finally block 'if orig_buf_append:' causing the error."
            },
            {
                "file": "aider/commands.py",
                "line_number": 823,
                "reason": "Stack trace shows aider/commands.py:823 in cmd_add calling self.io.confirm_ask, which delegates into the wrapper in aider/io.py that raises the UnboundLocalError."
            },
            {
                "file": "tests/basic/test_commands.py",
                "line_number": 44,
                "reason": "Log shows a failing test frame at tests/basic/test_commands.py:44 (test_cmd_add) where commands.cmd_add(...) is invoked and the subsequent traceback leads to aider/commands.py and then aider/io.py causing UnboundLocalError."
            },
            {
                "file": "tests/basic/test_commands.py",
                "line_number": 210,
                "reason": "Log shows tests/basic/test_commands.py:210 (test_cmd_add_no_match_but_make_it) calling commands.cmd_add which flows into aider/commands.py and the wrapper in aider/io.py that triggers the UnboundLocalError."
            },
            {
                "file": "tests/basic/test_commands.py",
                "line_number": 440,
                "reason": "Log shows tests/basic/test_commands.py:440 (another test invoking commands.cmd_add(\"temp.txt\")) with a traceback that includes aider/commands.py:823 and the failing wrapper in aider/io.py."
            },
            {
                "file": "tests/basic/test_io.py",
                "line_number": 181,
                "reason": "Log shows tests/basic/test_io.py:181 (test_confirm_ask_explicit_yes_required) calling io.confirm_ask; traceback points to aider/io.py:65 (wrapper) and then the UnboundLocalError at aider/io.py:92."
            },
            {
                "file": "tests/basic/test_io.py",
                "line_number": 255,
                "reason": "Log shows tests/basic/test_io.py:255 (test_confirm_ask_yes_no) invoking io.confirm_ask and the call chain ends at aider/io.py where the UnboundLocalError occurs."
            }
        ],
        "error_types": [
            {
                "category": "Runtime Error",
                "subcategory": "UnboundLocalError from referencing an uninitialized local variable",
                "evidence": "Multiple context lines and the traceback: \"E           UnboundLocalError: local variable 'orig_buf_append' referenced before assignment\" and code context showing orig_buf_append is only assigned in try but referenced in finally ('if orig_buf_append:') in aider/io.py."
            },
            {
                "category": "Test Failure",
                "subcategory": "Unit tests failing due to runtime exception in application code",
                "evidence": "Pytest summary: \"FAILED tests/basic/test_commands.py::TestCommands::test_cmd_add - UnboundLocalError...\" and \"================== 8 failed, 448 passed in 228.42s\" \u2014 the failures are direct results of the runtime UnboundLocalError."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.9)",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "b79f8486bf2063658bf3b2e658c07b7cfbe519bc",
        "error_context": [
            "Pytest (run by the 'Run tests' step) reported 8 failing tests (8 failed, 448 passed). All failures are AttributeError exceptions caused when code in aider/io.py attempts to access prompt_session.default_buffer but prompt_session is None. The failures surface from tests in tests/basic/test_commands.py and tests/basic/test_io.py and originate at aider/io.py:79, triggered during pytest execution."
        ],
        "relevant_files": [
            {
                "file": "aider/io.py",
                "line_number": 79,
                "reason": "Log shows the exception frame and location: \"aider/io.py:79: AttributeError\" and the error line in context: \"orig_buf_append = self.prompt_session.default_buffer.append_to_history\" followed by \"AttributeError: 'NoneType' object has no attribute 'default_buffer'\"."
            },
            {
                "file": "tests/basic/test_commands.py",
                "line_number": 44,
                "reason": "Multiple failing test traces reference this test file (examples in logs: \"tests/basic/test_commands.py:44\", \"tests/basic/test_commands.py:210\", \"tests/basic/test_commands.py:440\") and pytest summary lists several failures from tests/basic/test_commands.py (e.g. \"FAILED tests/basic/test_commands.py::TestCommands::test_cmd_add - AttributeError: 'NoneType' object has no attribute 'default_buffer'\")."
            },
            {
                "file": "tests/basic/test_io.py",
                "line_number": 181,
                "reason": "Failing test traces reference this file (examples in logs: \"tests/basic/test_io.py:181\", \"tests/basic/test_io.py:214\", \"tests/basic/test_io.py:255\") and pytest summary shows failures from tests/basic/test_io.py (e.g. \"FAILED tests/basic/test_io.py::TestInputOutput::test_confirm_ask_allow_never - AttributeError: 'NoneType' object has no attribute 'default_buffer'\")."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AttributeError during test execution (NoneType access)",
                "evidence": "\"E       AttributeError: 'NoneType' object has no attribute 'default_buffer'\" and \"aider/io.py:79: AttributeError\" appear in multiple test traces; pytest summary: \"8 failed, 448 passed in 221.93s\"."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Run tests",
                "command": "pytest"
            }
        ]
    },
    {
        "sha_fail": "8c0707ba9879994f0106a79126e917559b0b0bb9",
        "error_context": [
            "A test script tests/#655.py exited with a non-zero status during the test run, causing the job to fail (log: \"Error: tests/#655.py exited with a non-zero status.\"). The test-run step (tests/testall.sh) produced the failing test result and the runner terminated the job with \"##[error]Process completed with exit code 1.\"",
            "Other test scripts (tests/#511.py and tests/#588.py) show success in the logs, indicating the failure is isolated to tests/#655.py."
        ],
        "relevant_files": [
            {
                "file": "tests/#655.py",
                "line_number": null,
                "reason": "Direct log evidence: \"Error: tests/#655.py exited with a non-zero status.\" This message ties this test file to the failure."
            },
            {
                "file": "tests/testall.sh",
                "line_number": null,
                "reason": "The test-runner script was executed in the failing step (log group shows \"Run tests/testall.sh\" and the testing output that follows); it invoked the test that exited non-zero."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Test script exited with non-zero status / failing unit/integration test",
                "evidence": "\"Error: tests/#655.py exited with a non-zero status.\" and \"##[error]Process completed with exit code 1.\" from the CI logs show a test script returned a failure status and caused the job to stop."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.10, ubuntu-latest)",
                "step": "Run tests",
                "command": "tests/testall.sh"
            }
        ]
    },
    {
        "sha_fail": "e999cb700a84f2d25b71be17cbe17fa9832b2950",
        "error_context": [
            "The CI run failed because a test script returned a non\u2011zero exit code: the test runner executed tests/#655.py and the run reported \"Error: tests/#655.py exited with a non-zero status.\" This occurred during the test step (the workflow step that runs tests/testall.sh), which caused the job to terminate with \"Process completed with exit code 1.\"",
            "Other tests (e.g. tests/#511.py and tests/#588.py) are shown as successful in the logs, indicating the failure is localized to tests/#655.py rather than a global installation or dependency error."
        ],
        "relevant_files": [
            {
                "file": "tests/#655.py",
                "line_number": null,
                "reason": "Log shows the failing test directly: \"Error: tests/#655.py exited with a non-zero status.\" and earlier lines show the runner starting that test: \"Testing tests/#655.py...\""
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Test script exited with non-zero status (failing unit/test case)",
                "evidence": "\"Testing tests/#655.py...\" followed by \"Error: tests/#655.py exited with a non-zero status.\" and final \"##[error]Process completed with exit code 1.\" in the CI logs."
            }
        ],
        "failed_job": [
            {
                "job": "build (3.10, ubuntu-latest)",
                "step": "Run Test",
                "command": "tests/testall.sh"
            }
        ]
    },
    {
        "sha_fail": "07c55e7c8bee5b60d9d9647d647f89bec137ef4d",
        "error_context": [
            "The CI job failed because the linter ruff reported a lint error (F632) in py/flwr/supernode/start_client_internal.py at line 389: \"Use `==` to compare constant literals\". The linting step (run via ./framework/dev/test.sh as part of the \"Lint + Test\" step) returned a non-zero exit code, causing the \"Python 3.11\" job to fail."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/start_client_internal.py",
                "line_number": 389,
                "reason": "Log shows a ruff lint error for this file and line: \"py/flwr/supernode/start_client_internal.py:389:23: F632 [*] Use `==` to compare constant literals\"; this ties the file directly to the CI failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Code Style",
                "subcategory": "Ruff rule F632 \u2014 use == to compare constant literals",
                "evidence": "Log line: \"py/flwr/supernode/start_client_internal.py:389:23: F632 [*] Use `==` to compare constant literals\" and earlier context \"- ruff: start\" plus \"Found 1 error.\" show a ruff lint rule violation caused the failure."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.11",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked as part of ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "1366645090f139b99f4606faf8cb1f054330213e",
        "error_context": [
            "The CI job failed during the type-checking step: mypy reported two errors that caused the job to exit with code 1. Mypy messages show that code in two files attempts to use flwr.common.logger.mask_string, but mypy reports that the module has no attribute 'mask_string'. The failing files and line numbers are py/flwr/supernode/runtime/run_clientapp.py:35 and py/flwr/supernode/cli/flwr_clientapp.py:24."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/runtime/run_clientapp.py",
                "line_number": 35,
                "reason": "Log shows a mypy error at this file and line: \"py/flwr/supernode/runtime/run_clientapp.py:35: error: Module \\\"flwr.common.logger\\\" has no attribute \\\"mask_string\\\"  [attr-defined]\"."
            },
            {
                "file": "py/flwr/supernode/cli/flwr_clientapp.py",
                "line_number": 24,
                "reason": "Log shows a mypy error at this file and line: \"py/flwr/supernode/cli/flwr_clientapp.py:24: error: Module \\\"flwr.common.logger\\\" has no attribute \\\"mask_string\\\"  [attr-defined]\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: missing attribute on imported module (attr-defined)",
                "evidence": "\"py/flwr/supernode/runtime/run_clientapp.py:35: error: Module \\\"flwr.common.logger\\\" has no attribute \\\"mask_string\\\"  [attr-defined]\" and \"py/flwr/supernode/cli/flwr_clientapp.py:24: error: Module \\\"flwr.common.logger\\\" has no attribute \\\"mask_string\\\"  [attr-defined]\"; followed by \"Found 2 errors in 2 files\" and the process exit code 1."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.11",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked via ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "14df8e5918cb5d6c4ca62d23b5a1f653a0e92212",
        "error_context": [
            "The CI run failed during the lint/test pipeline when ruff attempted to parse a test file and encountered a Python syntax error. Ruff reported a parse error in py/flwr/server/grid/grpc_grid_test.py at line 223: \"E999 SyntaxError: Positional argument cannot follow keyword argument\", which caused the Lint + Test step to exit with code 1.",
            "The failing tool (ruff) was invoked as part of the repository's Lint + Test step (./framework/dev/test.sh) in the Python job, and the pipeline terminated with \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/grid/grpc_grid_test.py",
                "line_number": 223,
                "reason": "Log shows ruff parse error for this file and line: \"py/flwr/server/grid/grpc_grid_test.py:223:13: E999 SyntaxError: Positional argument cannot follow keyword argument\" and \"Failed to parse py/flwr/server/grid/grpc_grid_test.py:223:13: Positional argument cannot follow keyword argument\"."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting / Static Analysis",
                "subcategory": "SyntaxError (parse error) reported by ruff \u2014 'Positional argument cannot follow keyword argument'",
                "evidence": "Logs: \"error: Failed to parse py/flwr/server/grid/grpc_grid_test.py:223:13: Positional argument cannot follow keyword argument\" and \"py/flwr/server/grid/grpc_grid_test.py:223:13: E999 SyntaxError: Positional argument cannot follow keyword argument\". The ruff step then reported \"Found 1 error.\""
            }
        ],
        "failed_job": [
            {
                "job": "test_core",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked inside ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "424e2bec035589620a6ae25f51d4f389adc3a12e",
        "error_context": [
            "The CI job failed because the code formatter Black reported that one file would be reformatted, causing the lint/test step to exit non\u2011zero. The logs show Black output: \"would reformat /home/runner/work/flower/flower/framework/py/flwr/cli/build.py\" and \"1 file would be reformatted, 360 files would be left unchanged.\", followed by \"##[error]Process completed with exit code 1.\"",
            "This occurred during the repository's lint/test step (the Lint + Test step invoked by ./framework/dev/test.sh) in the Python 3.11 matrix job."
        ],
        "relevant_files": [
            {
                "file": "framework/py/flwr/cli/build.py",
                "line_number": null,
                "reason": "Black output in the logs explicitly names this file: \"would reformat /home/runner/work/flower/flower/framework/py/flwr/cli/build.py\" and the summary line \"1 file would be reformatted...\" ties this file directly to the formatting failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Black check failure (file would be reformatted)",
                "evidence": "\"would reformat /home/runner/work/flower/flower/framework/py/flwr/cli/build.py\" and \"1 file would be reformatted, 360 files would be left unchanged.\" followed by \"##[error]Process completed with exit code 1.\" in the Python 3.11 job logs."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.11",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "./framework/dev/test.sh (invoked Black; logs show Black reporting \"would reformat ...\")"
            }
        ]
    },
    {
        "sha_fail": "444017d8b29540c809f8a7b6479fcb124f229ab8",
        "error_context": [
            "The CI Lint+Test step failed during the docformatter stage: the log shows \"- docformatter: start\" followed by the path \"flwr_datasets/partitioner/continuous_partitioner.py\" and then \"##[error]Process completed with exit code 3.\" This indicates a code-formatting/docstring-formatting issue detected by docformatter inside the ./framework/dev/test.sh run (the Lint + Test step).",
            "Black completed successfully earlier (\"All done! ... 61 files would be left unchanged.\"), so the failure is specific to the docformatter check rather than black or dependency installation."
        ],
        "relevant_files": [
            {
                "file": "flwr_datasets/partitioner/continuous_partitioner.py",
                "line_number": null,
                "reason": "Log shows docformatter starting then this file path (\"- docformatter: start\" followed by \"flwr_datasets/partitioner/continuous_partitioner.py\") immediately before the error line \"##[error]Process completed with exit code 3.\", indicating docformatter flagged this file."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Docstring/formatting check failure (docformatter)",
                "evidence": "Logs contain \"- docformatter: start\" and list \"flwr_datasets/partitioner/continuous_partitioner.py\" immediately before \"##[error]Process completed with exit code 3.\", indicating docformatter reported a formatting/docstring issue for that file during the Lint+Test step."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.9",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "./framework/dev/test.sh (docformatter stage reported the issue)"
            },
            {
                "job": "Python 3.10",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "./framework/dev/test.sh (docformatter stage reported the issue)"
            }
        ]
    },
    {
        "sha_fail": "44cc14aa3f36dbd14049106933dcd12bdb1fead4",
        "error_context": [
            "Mypy type-checking failed: the mypy run reports that py/flwr/server/superlink/serverappio/serverappio_grpc.py (line 43) attempts to instantiate an abstract class ServerAppIoServicer that is missing implementations for PullObject and PushObject. This mypy error caused the test/lint step to exit with code 1 ('Found 1 error in 1 file' and 'Process completed with exit code 1'). The failure occurred during the repository's 'Lint + Test' pipeline (mypy invoked from ./framework/dev/test.sh) on the Python matrix runs (examples shown for Python 3.9 and 3.10)."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/superlink/serverappio/serverappio_grpc.py",
                "line_number": 43,
                "reason": "Mypy error message in the logs: 'py/flwr/server/superlink/serverappio/serverappio_grpc.py:43: error: Cannot instantiate abstract class \"ServerAppIoServicer\" with abstract attributes \"PullObject\" and \"PushObject\"' \u2014 directly ties this file and line to the type-check failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: instantiation of abstract class / missing abstract method implementations",
                "evidence": "Log line: 'py/flwr/server/superlink/serverappio/serverappio_grpc.py:43: error: Cannot instantiate abstract class \"ServerAppIoServicer\" with abstract attributes \"PullObject\" and \"PushObject\"' and 'Found 1 error in 1 file (checked 343 source files)' followed by 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "test_core (matrix run: Python 3.9)",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            },
            {
                "job": "test_core (matrix run: Python 3.10)",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "44eb41928c49ec0438728ea76283495461dc2e19",
        "error_context": [
            "The CI linting step failed: ruff reported 3 errors in py/flwr/supernode/start_client_internal.py (two F401 unused-imports and one E402 module-level import not at top). These linter errors caused the job to exit with code 1.",
            "The failure occurred in the Python 3.10 run when the repository's lint/test script executed ruff (as part of the 'Lint + Test' step invoked by ./framework/dev/test.sh)."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/start_client_internal.py",
                "line_number": 34,
                "reason": "Log shows ruff errors for this file: 'py/flwr/supernode/start_client_internal.py:34:28: F401 `flwr.app.error.Error` imported but unused', 'py/flwr/supernode/start_client_internal.py:35:35: F401 `flwr.cli.config_utils.get_fab_metadata` imported but unused', and 'py/flwr/supernode/start_client_internal.py:65:1: E402 Module level import not at top of file'."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused import (F401) and module-level import location (E402) reported by ruff",
                "evidence": "Log lines: 'py/flwr/supernode/start_client_internal.py:34:28: F401 `flwr.app.error.Error` imported but unused', 'py/flwr/supernode/start_client_internal.py:35:35: F401 `flwr.cli.config_utils.get_fab_metadata` imported but unused', 'py/flwr/supernode/start_client_internal.py:65:1: E402 Module level import not at top of file' and 'Found 3 errors.' followed by 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "test_core (Python 3.10)",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "5f98672f68c7f4acdf3e6f68fd79e66b56d0b529",
        "error_context": [
            "The CI run failed during the linting step: ruff reported two E501 (line-too-long) errors in a test file, causing the pipeline to exit with code 1. The failing file is py/flwr/server/superlink/serverappio/serverappio_servicer_test.py at lines 334 and 336. The lint step was executed as part of the repository's Lint + Test script (./framework/dev/test.sh)."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/superlink/serverappio/serverappio_servicer_test.py",
                "line_number": 334,
                "reason": "Log shows ruff output referencing this file with line-length errors: \"py/flwr/server/superlink/serverappio/serverappio_servicer_test.py:334:89: E501 Line too long (94 > 88)\" and \"...:336:89: E501 Line too long (92 > 88)\"; ruff reported \"Found 2 errors.\""
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Line Length Exceeded (E501)",
                "evidence": "CI log lines: \"py/flwr/server/superlink/serverappio/serverappio_servicer_test.py:334:89: E501 Line too long (94 > 88)\" and \"py/flwr/server/superlink/serverappio/serverappio_servicer_test.py:336:89: E501 Line too long (92 > 88)\" and \"Found 2 errors.\""
            }
        ],
        "failed_job": [
            {
                "job": "test_core (Python 3.11)",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "63b90b943bab2b7897e968fcea280f6a8ecc229b",
        "error_context": [
            "The CI run failed because the linter (ruff) reported a line-length violation (E501) in a test file. The failing message is: \"py/flwr/superexec/exec_event_log_interceptor_test.py:95:89: E501 Line too long (89 > 88)\". This occurred during the Lint + Test step (./framework/dev/test.sh) in the Python 3.11 job, causing the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/superexec/exec_event_log_interceptor_test.py",
                "line_number": 95,
                "reason": "Log shows ruff output pointing to this file and line: \"py/flwr/superexec/exec_event_log_interceptor_test.py:95:89: E501 Line too long (89 > 88)\" \u2014 a direct linter error causing the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Line Length Exceeded (E501)",
                "evidence": "Log line from ruff: \"py/flwr/superexec/exec_event_log_interceptor_test.py:95:89: E501 Line too long (89 > 88)\". The presence of \"Found 1 error.\" and then \"##[error]Process completed with exit code 1.\" confirms the linter violation caused the job failure."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.11",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "./framework/dev/test.sh (ran ruff; ruff produced: \"E501 Line too long (89 > 88)\")"
            }
        ]
    },
    {
        "sha_fail": "66718a2530fcc65c0adfa0a6ea380e1978c12ebe",
        "error_context": [
            "A pytest unit test failed with an unhandled IndexError when test_pull_clientapp_inputs tried to run code that accessed an empty list. The traceback shows run_clientapp.py line 226 performing object_tree = pull_msg_res.message_object_trees[0], which raised IndexError: list index out of range. This failure occurred during the CI step that runs tests (the Lint + Test step invoked via ./framework/dev/test.sh) for the Python 3.11 matrix job, causing the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/runtime/run_clientapp.py",
                "line_number": 226,
                "reason": "Traceback frame and error: \">           object_tree = pull_msg_res.message_object_trees[0]\" followed by \"E           IndexError: list index out of range\" and the log line \"py/flwr/supernode/runtime/run_clientapp.py:226: IndexError\" tie this file and line to the runtime exception."
            },
            {
                "file": "py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py",
                "line_number": null,
                "reason": "Pytest summary shows the failing test: \"FAILED py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py::TestClientAppIoServicer::test_pull_clientapp_inputs - IndexError: list index out of range\", indicating this test exercised the failing code."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Unit test raised an uncaught exception (IndexError)",
                "evidence": "\"FAILED py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py::TestClientAppIoServicer::test_pull_clientapp_inputs - IndexError: list index out of range\" (from the pytest short test summary)"
            },
            {
                "category": "Runtime Error",
                "subcategory": "IndexError: list index out of range (out-of-bounds list access)",
                "evidence": "Traceback shows the failing statement \">           object_tree = pull_msg_res.message_object_trees[0]\" and the message \"E           IndexError: list index out of range\" with file/line \"py/flwr/supernode/runtime/run_clientapp.py:226: IndexError\"."
            }
        ],
        "failed_job": [
            {
                "job": "test_core (matrix python=3.11)",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "pytest (invoked via ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "6aec23d104a077af68fbcf236630ed69d6d965ac",
        "error_context": [
            "The CI job failed because the ruff linter reported two F811 \"Redefinition of unused\" errors in py/flwr/supernode/servicer/clientappio/clientappio_servicer.py (lines shown at 129 and 146), which caused the lint step to exit non\u2011zero. The log shows ruff output identifying the exact file and lines and then \"Found 2 errors.\" followed by \"Process completed with exit code 1.\"",
            "The failing tool was ruff invoked as part of the project's lint/test script (./framework/dev/test.sh) during the Python job, so the pipeline stopped due to linting rule violations rather than a test or runtime error."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/servicer/clientappio/clientappio_servicer.py",
                "line_number": 129,
                "reason": "Ruff error reported in the log: \"py/flwr/supernode/servicer/clientappio/clientappio_servicer.py:129:9: F811 Redefinition of unused `GetRunIdsWithPendingMessages` from line 92\" (also a second F811 at line 146); these messages tie this file directly to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Ruff F811 - Redefinition of unused symbol",
                "evidence": "Log lines: \"py/flwr/supernode/servicer/clientappio/clientappio_servicer.py:129:9: F811 Redefinition of unused `GetRunIdsWithPendingMessages` from line 92\" and \"...:146:9: F811 Redefinition of unused `RequestToken` from line 109\" plus \"Found 2 errors.\" and \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "test_core",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "6aee1d58e8ce6402c48325c8c479dae84596d352",
        "error_context": [
            "The CI mypy type-checking step failed: mypy reported a missing return type annotation in py/flwr/common/inflatable_test.py at line 60. The Lint + Test step (which runs ./framework/dev/test.sh and invokes mypy) exited with code 1 after mypy reported \"Found 1 error in 1 file (checked 342 source files)\"."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/common/inflatable_test.py",
                "line_number": 60,
                "reason": "Mypy error in logs: \"py/flwr/common/inflatable_test.py:60: error: Function is missing a return type annotation  [no-untyped-def]\" and the follow-up note \"Use \"-> None\" if function does not return a value\" tie this file and line directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: missing return type annotation ([no-untyped-def])",
                "evidence": "\"py/flwr/common/inflatable_test.py:60: error: Function is missing a return type annotation  [no-untyped-def]\" and \"Found 1 error in 1 file (checked 342 source files)\" in the CI logs during the mypy phase."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.10",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            },
            {
                "job": "Python 3.12",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "78131a41338361909ae6e81399a5eac2579498bf",
        "error_context": [
            "An end-to-end test script (test_serverapp_heartbeat.py) raised an AssertionError: \"Run statuses are not updated correctly\" causing the job to exit with code 1. The test failure is preceded by runtime errors in the flwr heartbeat code: the heartbeat logic raised RuntimeError(\"Heartbeat failed unexpectedly...\") after receiving gRPC PERMISSION_DENIED responses (details = \"Run is finished.\"), and a RunNotRunningException from flwr's retry_invoker, which ultimately led to the heartbeat sender being reported as not running.",
            "The failing CI step ran the command 'python ../test_serverapp_heartbeat.py simulation' (Python 3.9); the assertion in the test (framework/e2e/test_serverapp_heartbeat.py) is the immediate failure, while the flwr package stack traces (heartbeat.py, retry_invoker.py, simulation/app.py, grpc/_channel.py) show the underlying runtime/gRPC errors that caused the test to observe invalid run statuses."
        ],
        "relevant_files": [
            {
                "file": "framework/e2e/test_serverapp_heartbeat.py",
                "line_number": 180,
                "reason": "Test script raised an assertion: log shows 'File \"/home/runner/work/flower/flower/framework/e2e/e2e-serverapp-heartbeat/../test_serverapp_heartbeat.py\", line 180, in main' and immediately 'assert is_valid, \"Run statuses are not updated correctly\"' followed by 'AssertionError: Run statuses are not updated correctly'."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/flwr/common/heartbeat.py",
                "line_number": 97,
                "reason": "Heartbeat thread frames appear in the traceback: 'File \"/opt/hostedtoolcache/.../site-packages/flwr/common/heartbeat.py\", line 97, in _run' and later the code raises 'RuntimeError: Heartbeat failed unexpectedly. The SuperLink could not find the provided run ID, or the run status is invalid.'"
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/flwr/common/retry_invoker.py",
                "line_number": 278,
                "reason": "Retry invoker is in the stack leading to the heartbeat failure: 'File \"/opt/hostedtoolcache/.../site-packages/flwr/common/retry_invoker.py\", line 278, in invoke' and other frames show giveup_check/_should_giveup logic raising RunNotRunningException."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/grpc/_channel.py",
                "line_number": 1181,
                "reason": "gRPC call failed inside the simulation: 'File \"/opt/hostedtoolcache/.../site-packages/grpc/_channel.py\", line 1181, in __call__' and logs include 'status = StatusCode.PERMISSION_DENIED' with 'details = \"Run is finished.\"', which is cited as the reason for the failure in the retry flow."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/flwr/simulation/app.py",
                "line_number": 245,
                "reason": "Simulation process frame appears in the traceback: 'File \"/opt/hostedtoolcache/.../site-packages/flwr/simulation/app.py\", line 245, in run_simulation_process' where a PushSimulationOutputs RPC is invoked and subsequently triggers the retry/gRPC errors."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.9.23/x64/bin/flwr-simulation",
                "line_number": 8,
                "reason": "Top-level simulation entrypoint is in the traceback: 'File \"/opt/hostedtoolcache/.../bin/flwr-simulation\", line 8, in <module>' showing the script invoked flwr_simulation() which led to the inner simulation/app.py frames."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in end-to-end test",
                "evidence": "Log shows: 'assert is_valid, \"Run statuses are not updated correctly\"' followed by 'AssertionError: Run statuses are not updated correctly' (File .../test_serverapp_heartbeat.py, line 180)."
            },
            {
                "category": "Runtime Error",
                "subcategory": "Heartbeat runtime failure / third-party library exception",
                "evidence": "Logs include 'RuntimeError: Heartbeat failed unexpectedly. The SuperLink could not find the provided run ID, or the run status is invalid.' and stack frames in flwr/common/heartbeat.py and flwr/common/retry_invoker.py indicating the heartbeat thread failed."
            },
            {
                "category": "Remote Procedure Call (gRPC) Error",
                "subcategory": "Permission denied / RPC returned 'Run is finished.'",
                "evidence": "gRPC stack shows: 'grpc._channel._InactiveRpcError' with 'status = StatusCode.PERMISSION_DENIED' and 'details = \"Run is finished.\"' in grpc/_channel.py frames, which is propagated through retry_invoker and simulation code."
            }
        ],
        "failed_job": [
            {
                "job": "ServerApp/run heartbeat test with simulation runtime / Python 3.9",
                "step": "Run python ../test_serverapp_heartbeat.py simulation",
                "command": "python ../test_serverapp_heartbeat.py simulation"
            }
        ]
    },
    {
        "sha_fail": "992b9a36ee14ce62ac8639c1ec88ba83e375e525",
        "error_context": [
            "The CI job failed during static type checking: mypy reported an incompatible assignment in py/flwr/server/serverapp/app.py (line 253) where an expression of type None is being assigned to a variable typed as Context. This error occurred while running the repository's lint/test script (the mypy step) inside the Python 3.11 runner, causing the step to exit with code 1.",
            "A DeprecationWarning from mypy (/opt/.../mypy/__main__.py:37) is present in the logs but is informational; the blocking failure is the mypy type error reported for py/flwr/server/serverapp/app.py."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/serverapp/app.py",
                "line_number": 253,
                "reason": "Mypy output: \"py/flwr/server/serverapp/app.py:253: error: Incompatible types in assignment (expression has type \\\"None\\\", variable has type \\\"Context\\\")\" \u2014 the log shows this exact error line tying this file and line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch (Incompatible types in assignment)",
                "evidence": "Log message: \"py/flwr/server/serverapp/app.py:253: error: Incompatible types in assignment (expression has type \\\"None\\\", variable has type \\\"Context\\\")\" and \"Found 1 error in 1 file (checked 393 source files)\" indicating mypy reported a type error that caused the job to fail."
            }
        ],
        "failed_job": [
            {
                "job": "test_core",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest) [mypy]",
                "command": "./framework/dev/test.sh (mypy invoked inside this script)"
            }
        ]
    },
    {
        "sha_fail": "a02c53a0eed80e7332e03b39bac351260afde289",
        "error_context": [
            "The CI job failed because the linter (ruff) reported a lint error: an unused import in a test file. The log shows ruff produced `F401` for py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py at line 38, and the pipeline exited with code 1. This occurred during the Lint + Test step (the project's ./framework/dev/test.sh), which runs ruff as part of the checks."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py",
                "line_number": 38,
                "reason": "Log shows: \"py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py:38:5: F401 [*] `flwr.proto.message_pb2.ObjectIDs` imported but unused\" \u2014 ruff flagged an unused import at this file and line, which is tied to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused import (F401)",
                "evidence": "Log line: \"py/flwr/supernode/servicer/clientappio/clientappio_servicer_test.py:38:5: F401 [*] `flwr.proto.message_pb2.ObjectIDs` imported but unused\" and subsequent \"Found 1 error.\" / \"[*] 1 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.11",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "a6517d5fc23bbb6be3424924264025b961df3fe2",
        "error_context": [
            "The CI job failed because pylint reported a rule violation in py/flwr/client/start_client_internal.py: specifically R0913 (Too many arguments) at line 93. This lint error occurred during the 'Lint + Test' step (./framework/dev/test.sh) and the job ended with \"Process completed with exit code 8.\"",
            "Mypy completed successfully (\"Success: no issues found in 359 source files\"), but the subsequent pylint run produced the R0913 message which caused the pipeline to exit non\u2011zero."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/client/start_client_internal.py",
                "line_number": 93,
                "reason": "Pylint output in the logs: \"py/flwr/client/start_client_internal.py:93:0: R0913: Too many arguments (15/5) (too-many-arguments)\" \u2014 this ties the file and line directly to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Static Analysis",
                "subcategory": "Pylint rule R0913 \u2014 Too many arguments",
                "evidence": "Log shows: \"py/flwr/client/start_client_internal.py:93:0: R0913: Too many arguments (15/5) (too-many-arguments)\" and the job then reports \"##[error]Process completed with exit code 8.\""
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.9",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "pylint (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "a7a139ca8f5d0acdebb205d469166fbe034c2372",
        "error_context": [
            "The Python 3.10 CI job failed during the Lint + Test step when mypy reported a type-checking error. mypy found a missing positional argument 'flwr_aid' in a call to Run in py/flwr/server/grid/inmemory_grid_test.py at line 84, and exited with a non-zero status (Found 1 error in 1 file), causing the job to fail."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/grid/inmemory_grid_test.py",
                "line_number": 84,
                "reason": "Log shows a mypy error for this file and line: \"py/flwr/server/grid/inmemory_grid_test.py:84: error: Missing positional argument \\\"flwr_aid\\\" in call to \\\"Run\\\"  [call-arg]\" and \"Found 1 error in 1 file (checked 375 source files)\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: missing positional argument in function call ([call-arg])",
                "evidence": "\"py/flwr/server/grid/inmemory_grid_test.py:84: error: Missing positional argument \\\"flwr_aid\\\" in call to \\\"Run\\\"  [call-arg]\" and \"Found 1 error in 1 file (checked 375 source files)\" from the CI logs."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.10",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "af6a0b457a537777e1e18111b49dbe9abfffb2cd",
        "error_context": [
            "The CI failed because the linter ruff reported unused-import errors (F401) in py/flwr/server/superlink/serverappio/serverappio_servicer.py (lines 25, 32 and 39). The failures occurred during the Lint + Test step (./framework/dev/test.sh) inside the test_core job (Python matrix), causing the job to exit with code 1 (\"##[error]Process completed with exit code 1.\"). The same ruff errors appear in the logs for multiple Python matrix runs (3.10, 3.11, 3.12)."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/superlink/serverappio/serverappio_servicer.py",
                "line_number": 25,
                "reason": "Ruff output in the CI logs shows unused-import errors in this file: e.g. \"py/flwr/server/superlink/serverappio/serverappio_servicer.py:25:25: F401 `flwr.common.ConfigRecord` imported but unused\", plus similar entries at \":32:5\" and \":39:5\" indicating the file and lines are the cause."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Code Quality",
                "subcategory": "Unused import (F401) reported by ruff",
                "evidence": "Log lines: \"py/flwr/server/superlink/serverappio/serverappio_servicer.py:25:25: F401 `flwr.common.ConfigRecord` imported but unused\", \"...:32:5: F401 `flwr.common.serde.fab_from_proto` imported but unused\", \"...:39:5: F401 `flwr.common.serde.user_config_from_proto` imported but unused\" and \"Found 3 errors.\" followed by \"[*] 3 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "test_core",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "ruff (invoked as part of ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "b2a09db583937f76160cef9629dc7d172b86dbed",
        "error_context": [
            "The CI Lint+Test step failed because the Black formatter encountered a parse error when trying to format framework/py/flwr/common/message.py. The logs show Black reported: \"error: cannot format /home/runner/work/flower/flower/framework/py/flwr/common/message.py: Cannot parse: 406:12:             error = None\", causing the test script to exit with code 123. This failure occurred in the test_core job's Python matrix (Python 3.10 and Python 3.11) during the Lint + Test step (./framework/dev/test.sh)."
        ],
        "relevant_files": [
            {
                "file": "framework/py/flwr/common/message.py",
                "line_number": 406,
                "reason": "Black parse error in CI logs: \"error: cannot format /home/runner/work/flower/flower/framework/py/flwr/common/message.py: Cannot parse: 406:12:             error = None\" \u2014 ties this file and line 406 directly to the formatting failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Black parse error / cannot format file",
                "evidence": "Log line: \"error: cannot format /home/runner/work/flower/flower/framework/py/flwr/common/message.py: Cannot parse: 406:12:             error = None\" and subsequent message \"1 file would fail to reformat.\""
            }
        ],
        "failed_job": [
            {
                "job": "test_core (Python 3.10 matrix run shown as 'Python 3.10')",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "black (invoked via ./framework/dev/test.sh)"
            },
            {
                "job": "test_core (Python 3.11 matrix run shown as 'Python 3.11')",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "black (invoked via ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "c3a3944a6d50067aa5937e22e97eac0a64631f47",
        "error_context": [
            "The CI matrix job running Python 3.9 failed during the repository's lint/test step when mypy reported syntax errors using the newer union 'X | Y' syntax. Mypy produced four errors in a single file: py/flwr/common/inflatable_rest_utils.py (lines 32, 57, 70, 97) stating \"X | Y syntax for unions requires Python 3.10\". The failure caused the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/common/inflatable_rest_utils.py",
                "line_number": 32,
                "reason": "Mypy error lines in the log: \"py/flwr/common/inflatable_rest_utils.py:32: error: X | Y syntax for unions requires Python 3.10\" (also reported at lines 57, 70, 97) and \"Found 4 errors in 1 file (checked 375 source files)\" \u2014 directly ties this file to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Syntax / Python-version incompatibility (mypy: union '|' requires Python 3.10)",
                "evidence": "Log shows mypy errors: \"py/flwr/common/inflatable_rest_utils.py:32: error: X | Y syntax for unions requires Python 3.10\", repeated for lines 57, 70, 97, followed by \"Found 4 errors in 1 file (checked 375 source files)\"; the job ran under Python 3.9 as shown by the job name."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.9",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "cbb7561e4e0d81a027fbd7ff6482fea13ee17398",
        "error_context": [
            "The CI run failed during the framework lint/test step when mypy reported a type-checking error: py/flwr/server/app.py at line 76 references Module \"flwr.superexec\" with no attribute \"load_executor\". The mypy run produced \"Found 1 error in 1 file\" which caused the test step (./framework/dev/test.sh) to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/server/app.py",
                "line_number": 76,
                "reason": "Mypy error reported in the logs: \"py/flwr/server/app.py:76: error: Module \\\"flwr.superexec\\\" has no attribute \\\"load_executor\\\"\" \u2014 this links that file/line directly to the failing type-check."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: missing attribute / attr-defined",
                "evidence": "Log line: \"py/flwr/server/app.py:76: error: Module \\\"flwr.superexec\\\" has no attribute \\\"load_executor\\\"  [attr-defined]\" and \"Found 1 error in 1 file (checked 398 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "test_core (Python 3.11)",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "d4593aedbc14dbd885f63009dcb0a4b962a04be6",
        "error_context": [
            "The CI job failed because mypy reported a typing check error: an unused \"type: ignore\" comment in a test file. The log shows: \"py/flwr/common/record/recorddict_test.py:277: error: Unused \\\"type: ignore\\\" comment  [unused-ignore]\" which caused mypy to exit non-zero and the workflow to finish with exit code 1. The failure occurred during the repository's Lint + Test step (the test.sh script) where mypy was run."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/common/record/recorddict_test.py",
                "line_number": 277,
                "reason": "Log line: \"py/flwr/common/record/recorddict_test.py:277: error: Unused \\\"type: ignore\\\" comment  [unused-ignore]\" \u2014 mypy reported this specific file and line as the source of the error."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: unused \"type: ignore\" comment ([unused-ignore])",
                "evidence": "\"py/flwr/common/record/recorddict_test.py:277: error: Unused \\\"type: ignore\\\" comment  [unused-ignore]\" and \"Found 1 error in 1 file (checked 340 source files)\" from the CI logs."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.9",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "mypy (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "da7ae359227a089cedf0b7aba53962766e7eb0b2",
        "error_context": [
            "The end-to-end 'e2e-bare' test run crashed with a gRPC connection failure: the client trace shows grpc._channel._MultiThreadedRendezvous with status = StatusCode.UNAVAILABLE and details indicating connection refused to 127.0.0.1:8080. The traceback ties the failure to the test client (framework/e2e/e2e-bare/e2e_bare/client_app.py line 65) calling into flwr client code, and the job subsequently finished with exit code 124 (timeout)."
        ],
        "relevant_files": [
            {
                "file": "framework/e2e/e2e-bare/e2e_bare/client_app.py",
                "line_number": 65,
                "reason": "Traceback shows: \"File \"/home/runner/work/flower/flower/framework/e2e/e2e-bare/e2e_bare/client_app.py\", line 65, in <module> start_client(\" \u2014 the failing traceback originates in this repository file that started the client."
            },
            {
                "file": "opt/hostedtoolcache/Python/3.9.22/x64/lib/python3.9/site-packages/flwr/client/app.py",
                "line_number": 201,
                "reason": "Traceback includes: \"File \"/opt/hostedtoolcache/.../site-packages/flwr/client/app.py\", line 201, in start_client start_client_internal(\" \u2014 shows flwr client entrypoint was involved in the error path."
            },
            {
                "file": "opt/hostedtoolcache/Python/3.9.22/x64/lib/python3.9/site-packages/flwr/client/app.py",
                "line_number": 438,
                "reason": "Traceback includes: \"File \"/opt/hostedtoolcache/.../site-packages/flwr/client/app.py\", line 438, in start_client_internal message = receive()\" \u2014 the flwr client internal call to receive() is on the failing stack."
            },
            {
                "file": "opt/hostedtoolcache/Python/3.9.22/x64/lib/python3.9/site-packages/flwr/client/grpc_client/connection.py",
                "line_number": 142,
                "reason": "Traceback includes: \"File \"/opt/hostedtoolcache/.../site-packages/flwr/client/grpc_client/connection.py\", line 142, in receive proto = next(server_message_iterator)\" \u2014 shows the gRPC receive code in flwr was executing when the error occurred."
            },
            {
                "file": "opt/hostedtoolcache/Python/3.9.22/x64/lib/python3.9/site-packages/grpc/_channel.py",
                "line_number": 543,
                "reason": "Traceback includes multiple frames in grpc: e.g. \"File \"/opt/hostedtoolcache/.../site-packages/grpc/_channel.py\", line 543, in __next__ return self._next()\" and later the _MultiThreadedRendezvous status = StatusCode.UNAVAILABLE / details = \"failed to connect... Connection refused (111)\" \u2014 this is the direct error source."
            }
        ],
        "error_types": [
            {
                "category": "Runtime Error",
                "subcategory": "gRPC connection failure (StatusCode.UNAVAILABLE / Connection refused)",
                "evidence": "\"grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.UNAVAILABLE\\n\\tdetails = \\\"failed to connect to all addresses; ... Connection refused (111)\\\"\" (from context_lines) and traceback frames in flwr client and grpc/_channel.py."
            },
            {
                "category": "CI Timeout / Infrastructure",
                "subcategory": "Process exited with code 124 (timeout/forced termination)",
                "evidence": "\"##[error]Process completed with exit code 124.\" appears in the logs after the gRPC error, indicating the job timed out / was terminated."
            }
        ],
        "failed_job": [
            {
                "job": "Framework / e2e-bare",
                "step": "Run ./../../test_legacy.sh \"e2e-bare\"",
                "command": "./../../test_legacy.sh \"e2e-bare\""
            }
        ]
    },
    {
        "sha_fail": "df0344c9bd80f87fed394e512d67a3138a1d8176",
        "error_context": [
            "The linting step (pylint) failed because pylint reported E0611 \"No name 'X' in module 'flwr.proto.recorddict_pb2'\" for multiple names imported at py/flwr/common/serde_utils.py:22. Those pylint errors caused the Lint + Test step to exit non\u2011zero and the job to finish with exit code 2 (##[error]Process completed with exit code 2)."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/common/serde_utils.py",
                "line_number": 22,
                "reason": "Pylint error lines in the log show: \"py/flwr/common/serde_utils.py:22:0: E0611: No name 'BoolList' in module 'flwr.proto.recorddict_pb2' (no-name-in-module)\" (and similar E0611 messages for BytesList, DoubleList, SintList, StringList, UintList), tying this file and line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Quality / Linting",
                "subcategory": "pylint E0611: no-name-in-module (import not found)",
                "evidence": "Log shows: \"py/flwr/common/serde_utils.py:22:0: E0611: No name 'BoolList' in module 'flwr.proto.recorddict_pb2' (no-name-in-module)\" (repeated for other names)."
            },
            {
                "category": "CI Infrastructure",
                "subcategory": "Non-zero exit code from step",
                "evidence": "Runner output: \"##[error]Process completed with exit code 2.\" indicating the lint/test step failed and terminated the job with a non-zero exit code."
            }
        ],
        "failed_job": [
            {
                "job": "test_core",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "pylint (invoked by ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "ed13d89504c50a1b3f5d9757b74c7aaf38356150",
        "error_context": [
            "The CI run failed because pylint reported a no-member error in py/flwr/cli/utils.py (line 323): \"E1101: Instance of 'RpcError' has no 'details' member (no-member)\". This pylint error occurred during the repository's Lint + Test step (the framework/dev/test.sh invocation that runs pylint), which caused the job to exit with code 2.",
            "Logs show the pylint section beginning (\"- pylint: start\") followed by the pylint module output for py/flwr/cli/utils.py and then the overall failure message \"##[error]Process completed with exit code 2.\", tying the linter failure to the job failure on Python 3.9."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/cli/utils.py",
                "line_number": 323,
                "reason": "CI log contains the pylint error: \"py/flwr/cli/utils.py:323:24: E1101: Instance of 'RpcError' has no 'details' member (no-member)\" which directly links this file/line to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Lint / Static Analysis",
                "subcategory": "pylint no-member (E1101)",
                "evidence": "Log: \"py/flwr/cli/utils.py:323:24: E1101: Instance of 'RpcError' has no 'details' member (no-member)\" and preceding line \"- pylint: start\"."
            },
            {
                "category": "CI Infrastructure / Job Failure",
                "subcategory": "Non-zero exit code from step",
                "evidence": "Log shows final failure: \"##[error]Process completed with exit code 2.\", indicating the pipeline failed due to the linter step exiting non-zero."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.9",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "pylint"
            }
        ]
    },
    {
        "sha_fail": "f231d50356ccfb76ed54d27e9fd32dd40e824f28",
        "error_context": [
            "The CI job failed because the lint step (pylint) reported a warning in py/flwr/supercore/scheduler/run_scheduler.py (W0108: \"Lambda may not be necessary\") and the overall test script exited with code 4. The failing step is the 'Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)' step which runs ./framework/dev/test.sh; logs show the pylint message \"py/flwr/supercore/scheduler/run_scheduler.py:66:23: W0108: Lambda may not be necessary (unnecessary-lambda)\" immediately before \"##[error]Process completed with exit code 4.\""
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supercore/scheduler/run_scheduler.py",
                "line_number": 66,
                "reason": "Pylint output in the logs: \"py/flwr/supercore/scheduler/run_scheduler.py:66:23: W0108: Lambda may not be necessary (unnecessary-lambda)\" \u2014 this ties that file and line to the lint warning that triggered the CI failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Code Quality",
                "subcategory": "Pylint warning treated as failure (W0108: unnecessary-lambda)",
                "evidence": "Log shows a pylint message: \"py/flwr/supercore/scheduler/run_scheduler.py:66:23: W0108: Lambda may not be necessary (unnecessary-lambda)\" and the job ends with \"##[error]Process completed with exit code 4.\" (pylint uses exit code bit 4 for warnings)."
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.11",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "pylint (invoked inside ./framework/dev/test.sh)"
            }
        ]
    },
    {
        "sha_fail": "f539ef9be2b510a9bef42e224e7c601f5965bcda",
        "error_context": [
            "The CI job failed because the ruff linter reported an unused import (F401) in py/flwr/supernode/nodestate/in_memory_nodestate.py at line 21, causing the Lint + Test step to exit with code 1. The failure occurred within the Python 3.10 matrix job while running the repository's test/lint script.",
            "Logs show: \"py/flwr/supernode/nodestate/in_memory_nodestate.py:21:30: F401 [*] `typing.Union` imported but unused\" and \"Found 1 error.\" followed by \"Process completed with exit code 1.\" which indicates the lint error is the root cause and is fixable with ruff's --fix option."
        ],
        "relevant_files": [
            {
                "file": "py/flwr/supernode/nodestate/in_memory_nodestate.py",
                "line_number": 21,
                "reason": "Log shows a ruff lint error referencing this file and line: \"py/flwr/supernode/nodestate/in_memory_nodestate.py:21:30: F401 [*] `typing.Union` imported but unused\" tying the file directly to the CI failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "\"py/flwr/supernode/nodestate/in_memory_nodestate.py:21:30: F401 [*] `typing.Union` imported but unused\" and \"Found 1 error.\" from the ruff output; CI then shows \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "Python 3.10",
                "step": "Lint + Test (isort/black/docformatter/mypy/pylint/flake8/pytest)",
                "command": "./framework/dev/test.sh"
            }
        ]
    },
    {
        "sha_fail": "0309bd9e095b3da3cb01d220541674d3b8e0a803",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: agno/cli/auth_server.py line 102 triggers \"BaseServer\" has no attribute \"running\". The mypy step returned a non-zero exit (Found 1 error in 1 file), causing the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/cli/auth_server.py",
                "line_number": 102,
                "reason": "Log shows a mypy error: \"agno/cli/auth_server.py:102: error: \\\"BaseServer\\\" has no attribute \\\"running\\\"  [attr-defined]\", linking this file and line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy attribute error (attr-defined): missing attribute on class",
                "evidence": "\"agno/cli/auth_server.py:102: error: \\\"BaseServer\\\" has no attribute \\\"running\\\"  [attr-defined]\" and \"Found 1 error in 1 file (checked 440 source files)\" in the style-check logs."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "00dff2ac803dca7ae44435c5dea311c633926b87",
        "error_context": [
            "The Codespell job ('Check for spelling errors') failed because codespell detected a spelling mistake in the repository. The log shows a codespell error indicating the word 'presense' should be 'presence' in tabular/src/autogluon/tabular/models/tabm/tabm_reference.py at line 533. The codespell action returned a non-zero exit (Codespell found one or more problems), causing the step to fail."
        ],
        "relevant_files": [
            {
                "file": "tabular/src/autogluon/tabular/models/tabm/tabm_reference.py",
                "line_number": 533,
                "reason": "Log error message: \"##[error]./tabular/src/autogluon/tabular/models/tabm/tabm_reference.py:533: presense ==> presence\" and subsequent log line \"Codespell found one or more problems\" tie this file and line directly to the codespell failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Quality",
                "subcategory": "Spelling error (codespell)",
                "evidence": "Log contains: \"##[error]./tabular/src/autogluon/tabular/models/tabm/tabm_reference.py:533: presense ==> presence\" and \"Codespell found one or more problems\" which indicates a spelling violation detected by codespell."
            }
        ],
        "failed_job": [
            {
                "job": "Check for spelling errors",
                "step": "Codespell",
                "command": "codespell (codespell-project/actions-codespell@v2)"
            }
        ]
    },
    {
        "sha_fail": "6b71b0ed836b8c61f092e369e77501911dc9d5f3",
        "error_context": [
            "The CI linter step failed: ruff reported undefined name errors (F821) in webui.py, causing the 'ruff check .' command to exit with code 1. The logs show multiple F821 occurrences (e.g. webui.py:45:21 and other lines) and a summary 'Found 13 errors.' which caused the job to fail."
        ],
        "relevant_files": [
            {
                "file": "webui.py",
                "line_number": 45,
                "reason": "Log shows lint errors in webui.py (e.g. 'webui.py:45:21: F821 Undefined name `cmd_opts`') and additional F821 occurrences at lines 46,47,48,49,53,55,67; these tie webui.py directly to the ruff failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Quality / Linting",
                "subcategory": "Undefined name (ruff F821)",
                "evidence": "Context lines: 'webui.py:45:21: F821 Undefined name `cmd_opts`' and repeated F821 messages for webui.py plus 'Found 13 errors.' and 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "ruff",
                "step": "Run Ruff",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "1559358d5679d353417ef140c1894997b4c7160f",
        "error_context": [
            "The CI linter step failed: the ruff action reported a single linting error (I001) related to import organization and exited with code 1. The log shows a code snippet with a caret pointing at an import line and the message \"= help: Organize imports\" plus \"Found 1 error.\" and \"[*] 1 fixable with the `--fix` option.\"",
            "This occurred in the 'partial-tests' job's linter step (chartboost/ruff-action@v1 running ruff check). The log snippet references the import line (line 30) \"from taipy.exceptions import NoGuiDefinedInEventConsumer\" but the surrounding logs do not print a filename, so the exact file path is not shown in the captured log."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import organization (ruff / I001)",
                "evidence": "Log shows a lint diagnostic with code 'I001' and the help text '= help: Organize imports' pointing at an import line (caret under the import). Also: 'Found 1 error.' and '[*] 1 fixable with the `--fix` option.' followed by 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests",
                "step": "partial-tests / linter",
                "command": "chartboost/ruff-action@v1 (runs 'ruff' with args: check) "
            }
        ]
    },
    {
        "sha_fail": "166f99e7023eb1ab623926aa8ce743e7fb2a6d50",
        "error_context": [
            "The linter step (ruff) failed during the partial-tests / linter job because ruff reported an import-sorting/formatting issue. The log shows ruff exited with code 1 after reporting: \"tests/gui/gui_specific/test_notification_on_close.py:12:1: I001 Import block is un-sorted or un-formatted\". The failing command was ruff check run by the astral-sh/ruff-action."
        ],
        "relevant_files": [
            {
                "file": "tests/gui/gui_specific/test_notification_on_close.py",
                "line_number": 12,
                "reason": "Log contains the ruff error: \"tests/gui/gui_specific/test_notification_on_close.py:12:1: I001 Import block is un-sorted or un-formatted\", tying this file and line directly to the linter failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import block not sorted (ruff I001)",
                "evidence": "##[error]tests/gui/gui_specific/test_notification_on_close.py:12:1: I001 Import block is un-sorted or un-formatted and ##[error]The process '/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff' failed with exit code 1 appear in the logs."
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests",
                "step": "Run astral-sh/ruff-action@v3",
                "command": "/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff check /home/runner/work/taipy/taipy"
            }
        ]
    },
    {
        "sha_fail": "23546b560fa453bd3fb05e7b8661bf77f77fb1a6",
        "error_context": [
            "Pytest failed during collection because an ImportError occurred in the test module tests/gui/e2e/renderers/test_html_rendering.py. The test file attempts to import taipy.gui.servers.fastapi and the interpreter raised ModuleNotFoundError: No module named 'taipy.gui.servers.fastapi' (see log: \"tests/gui/e2e/renderers/test_html_rendering.py:25: in <module> from taipy.gui.servers.fastapi import _FastAPIServer\" and \"E   ModuleNotFoundError: No module named 'taipy.gui.servers.fastapi'\").",
            "This ImportError aborted test collection (\"Interrupted: 1 error during collection\") and caused pytest to exit with a non\u2011zero status (Process completed with exit code 2) in multiple CI jobs/steps (coverage and partial-tests matrix runs)."
        ],
        "relevant_files": [
            {
                "file": "tests/gui/e2e/renderers/test_html_rendering.py",
                "line_number": 25,
                "reason": "Log shows test collection failing in this file and points to the import at line 25: \"tests/gui/e2e/renderers/test_html_rendering.py:25: in <module>     from taipy.gui.servers.fastapi import _FastAPIServer\" followed by \"E   ModuleNotFoundError: No module named 'taipy.gui.servers.fastapi'\"."
            },
            {
                "file": "taipy/gui/servers/fastapi.py",
                "line_number": null,
                "reason": "The error is a ModuleNotFoundError for module path 'taipy.gui.servers.fastapi' (log: \"E   ModuleNotFoundError: No module named 'taipy.gui.servers.fastapi'\"). This ties the missing module (expected file taipy/gui/servers/fastapi.py) to the failure even though the file path/line is not shown directly in the logs."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "ImportError / ModuleNotFoundError",
                "evidence": "\"E   ModuleNotFoundError: No module named 'taipy.gui.servers.fastapi'\" appears in pytest collection traceback (e.g. tests/gui/e2e/renderers/test_html_rendering.py:25: in <module> from taipy.gui.servers.fastapi import _FastAPIServer)."
            },
            {
                "category": "Test Collection",
                "subcategory": "ImportError during test collection",
                "evidence": "Pytest aborted collection: \"!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\" and short test summary shows \"ERROR tests/gui/e2e/renderers/test_html_rendering.py\" immediately before the ModuleNotFoundError."
            }
        ],
        "failed_job": [
            {
                "job": "coverage",
                "step": "Pytest",
                "command": "pipenv run pytest --cov=taipy --cov-report=xml:${{ github.workspace }}/coverage.xml --cov-config=.coveragerc"
            },
            {
                "job": "partial-tests / tests (3.11, ubuntu-latest)",
                "step": "Pytest",
                "command": "pipenv run pytest -m \"teste2e\" tests/gui"
            }
        ]
    },
    {
        "sha_fail": "700c695eea898caaa6fe527ec9957c47e9c3002c",
        "error_context": [
            "The linter step failed because ruff reported import-block formatting issues in taipy/gui/gui.py (I001). The log shows ruff was run via 'ruff check /home/runner/work/taipy/taipy' and exited with code 1, with specific messages pointing to taipy/gui/gui.py:12 and taipy/gui/gui.py:44.",
            "Mypy completed successfully (\"Success: no issues found ...\"), so the failure is localized to the ruff/formatting check in the partial-tests / linter job."
        ],
        "relevant_files": [
            {
                "file": "taipy/gui/gui.py",
                "line_number": 12,
                "reason": "Log shows ruff reporting formatting errors for this file: \"##[error]taipy/gui/gui.py:12:1: I001 Import block is un-sorted or un-formatted\" (also referenced at line 44)."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import block unsorted/un-formatted (ruff I001)",
                "evidence": "\"##[error]taipy/gui/gui.py:12:1: I001 Import block is un-sorted or un-formatted\" and \"##[error]taipy/gui/gui.py:44:1: I001 Import block is un-sorted or un-formatted\""
            },
            {
                "category": "Linter Failure",
                "subcategory": "Tool exited with non-zero status (ruff exit code 1)",
                "evidence": "\"##[error]The process '/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff' failed with exit code 1\" and the preceding ruff invocation: \"/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff check /home/runner/work/taipy/taipy\""
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests / linter",
                "step": "Run astral-sh/ruff-action@v3",
                "command": "/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff check /home/runner/work/taipy/taipy"
            }
        ]
    },
    {
        "sha_fail": "7c228365e8de18c41dfe96d6035bc7c528663f2b",
        "error_context": [
            "The coverage job failed its pull-request coverage check: tools/coverage_check.py reported \"Coverage for changed files is below 80.0%: 73.33%\" (coverage for gui/utils/datatype.py is 73.33%) and \"No coverage data found for doc/gui/examples/controls/chat_discuss.py\", causing the step to exit with code 1.",
            "During the test run several runtime exceptions/warnings were emitted (tracebacks in taipy/gui/gui.py showing AttributeError: 'Gui' object has no attribute '_server' and a JSON serialization TypeError in taipy/gui/_renderers/json.py), which appear in pytest output but the immediate CI failure is the coverage-check quality gate."
        ],
        "relevant_files": [
            {
                "file": "gui/utils/datatype.py",
                "line_number": null,
                "reason": "CI log: \"Coverage for gui/utils/datatype.py: 73.33%\" and then \"Coverage for changed files is below 80.0%: 73.33%\" \u2014 shows this file's coverage is below the threshold and directly tied to the coverage-check failure."
            },
            {
                "file": "doc/gui/examples/controls/chat_discuss.py",
                "line_number": null,
                "reason": "CI log: \"No coverage data found for doc/gui/examples/controls/chat_discuss.py\" \u2014 the coverage tool could not find coverage for this file, referenced by the failing coverage check."
            },
            {
                "file": "taipy/gui/gui.py",
                "line_number": 1213,
                "reason": "Multiple log tracebacks reference this file (e.g. \"/home/runner/.../taipy/gui/gui.py:1213: TaipyGuiWarning: Error transforming data: Conversion failure\" and later frames at line 1323/1326 showing a traceback ending in \"AttributeError: 'Gui' object has no attribute '_server'\"). These tracebacks tie this file to runtime exceptions during tests."
            },
            {
                "file": "taipy/gui/_renderers/json.py",
                "line_number": 72,
                "reason": "CI log shows a traceback: \"File \\\"/home/runner/work/taipy/taipy/taipy/gui/_renderers/json.py\\\", line 72, in parse\" followed by \"TypeError: Object of type _DoNotUpdate is not JSON serializable\" \u2014 linking this file to a JSON serialization error in tests."
            },
            {
                "file": "taipy/gui_core/_context.py",
                "line_number": 137,
                "reason": "CI log: \"/home/runner/work/taipy/taipy/taipy/gui_core/_context.py:137: TaipyGuiWarning: Access to sequence 'SEQUENCE_... ' failed:\" and a following traceback \u2014 shows this file appears in a failing traceback during test events."
            },
            {
                "file": "taipy/core/_entity/_reload.py",
                "line_number": 103,
                "reason": "CI traceback shows: \"File \"/home/runner/work/taipy/taipy/taipy/core/_entity/_reload.py\", line 103, in _do_reload\" as part of the sequence-access failure \u2014 ties this file to the same error chain."
            },
            {
                "file": "taipy/core/sequence/_sequence_manager.py",
                "line_number": 247,
                "reason": "CI traceback shows: \"File \"/home/runner/work/taipy/taipy/taipy/core/sequence/_sequence_manager.py\", line 247, in _get\" as part of the sequence-access failure reported in the logs."
            },
            {
                "file": "taipy/core/taipy.py",
                "line_number": 405,
                "reason": "CI traceback shows: \"File \"/home/runner/work/taipy/taipy/taipy/core/taipy.py\", line 405, in get\" in the chain leading to the sequence-access error reported by tests."
            }
        ],
        "error_types": [
            {
                "category": "Quality Gate / Coverage",
                "subcategory": "Coverage below threshold for changed files",
                "evidence": "\"Coverage for gui/utils/datatype.py: 73.33%\" and \"Coverage for changed files is below 80.0%: 73.33%\" followed by \"##[error]Process completed with exit code 1.\" in the coverage job logs."
            },
            {
                "category": "Runtime Error (during tests)",
                "subcategory": "AttributeError in WebSocket communication",
                "evidence": "Multiple tracebacks: \"File \\\".../taipy/gui/gui.py\\\", line 1323, in __broadcast_ws ... AttributeError: 'Gui' object has no attribute '_server'\" (shown repeatedly in pytest output)."
            },
            {
                "category": "Runtime Error (during tests)",
                "subcategory": "JSON serialization TypeError",
                "evidence": "\"File \\\"/home/runner/work/taipy/taipy/taipy/gui/_renderers/json.py\\\", line 72, in parse\" and \"TypeError: Object of type _DoNotUpdate is not JSON serializable (value: Taipy: Do not update).\" in the test logs."
            }
        ],
        "failed_job": [
            {
                "job": "coverage",
                "step": "Check pull request coverage",
                "command": "python tools/coverage_check.py check-changed --coverage-file ${{ github.workspace }}/coverage.xml --threshold 80"
            }
        ]
    },
    {
        "sha_fail": "92f010054982b709ab300f53b9a8bd407ab51e3f",
        "error_context": [
            "The CI linter step failed because ruff reported an unused-import lint error: tests/gui/config/test_filename.py contains an unused import of `pytest` at line 14. Ruff returned exit code 1, causing the partial-tests / linter job to fail.",
            "The failing command was ruff check (installed via astral-sh/ruff-action@v3) run against the repository root (/home/runner/work/taipy/taipy)."
        ],
        "relevant_files": [
            {
                "file": "tests/gui/config/test_filename.py",
                "line_number": 14,
                "reason": "Log shows a ruff lint error: \"tests/gui/config/test_filename.py:14:8: F401 `pytest` imported but unused\" and the subsequent line shows ruff exited with code 1, tying this file and line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused import (F401)",
                "evidence": "\"##[error]tests/gui/config/test_filename.py:14:8: F401 `pytest` imported but unused\" and \"The process '/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff' failed with exit code 1\" from the CI logs."
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests",
                "step": "partial-tests / linter",
                "command": "[command]/opt/hostedtoolcache/ruff/0.6.4/x86_64/ruff check /home/runner/work/taipy/taipy"
            }
        ]
    },
    {
        "sha_fail": "bbe7b82b778adbafd56d40bd00bc3aabcd5cb9c9",
        "error_context": [
            "The CI linter step failed because the ruff linter reported an import-organization issue (I001) in a source file, and ruff exited non\u2011zero. Logs show a caret pointing at the import line \"from tests.core.utils.NotifyMock import NotifyMock\" and the messages \"Found 1 error.\" and \"[*] 1 fixable with the `--fix` option.\", followed by \"Process completed with exit code 1.\" The failure occurred in the partial-tests / linter job while running the chartboost/ruff-action with args: check."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Import ordering / Organize imports (I001)",
                "evidence": "\"53 | | from tests.core.utils.NotifyMock import NotifyMock\" with an underline and \"^ I001\" in the log context; followed by \"Found 1 error.\" and \"[*] 1 fixable with the `--fix` option.\" and \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests",
                "step": "partial-tests / linter (Run chartboost/ruff-action@v1)",
                "command": "ruff check (chartboost/ruff-action@v1 with args: check)"
            }
        ]
    },
    {
        "sha_fail": "c9766a5e3d0c982cd148a391e63fb0f7c386b17e",
        "error_context": [
            "The CI linter step failed: the ruff linter reported a single lint error (I001) related to import organization and returned a non-zero exit code, causing the job to fail. Logs show \"Found 1 error.\", \"I001\", and \"[*] 1 fixable with the `--fix` option.\" No Python file path is printed in the captured log snippet, so the exact file name/line is not present in the logs."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Import organization / import-order (ruff I001)",
                "evidence": "\"|________________________________________________________________^ I001\", \"= help: Organize imports\", \"Found 1 error.\", and \"[*] 1 fixable with the `--fix` option.\" from the linter output; followed by \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests",
                "step": "partial-tests / linter (chartboost/ruff-action@v1)",
                "command": "ruff check (invoked via chartboost/ruff-action@v1 with args: check)"
            }
        ]
    },
    {
        "sha_fail": "e5031012aded0867f1d3677586c7851da537bd82",
        "error_context": [
            "The linter step failed: the Ruff action (chartboost/ruff-action@v1 running with args: check) reported a T201 lint error for a print usage. The log shows the offending line 'print(f\"Node Env: ${os.environ['NODE_OPTIONS']}\")' (marked with '^^^^^ T201') and 'Found 1 error.' which caused 'Process completed with exit code 1.' The CI log snippet does not include a filename for the failing snippet, so the exact file path cannot be determined from the logs."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Linting / Code Style",
                "subcategory": "Ruff rule T201 \u2014 use of print statement flagged",
                "evidence": "Context lines show the printed code and ruff marker: \"45 |         print(f\\\"Node Env: ${os.environ['NODE_OPTIONS']}\\\")\" followed by \"    |         ^^^^^ T201\" and later \"Found 1 error.\" and \"No fixes available (...)\"; the run is from chartboost/ruff-action@v1 with args: check."
            }
        ],
        "failed_job": [
            {
                "job": "partial-tests",
                "step": "partial-tests / linter",
                "command": "ruff check (chartboost/ruff-action@v1, args: check)"
            }
        ]
    },
    {
        "sha_fail": "076846e0a5144086c33090675a893d97305c8d52",
        "error_context": [
            "Pytest setup failed for tests in backends/build_system/functional/test_aws_cli_venv.py because evaluating pytest 'skipif' conditions raised SyntaxError(s) (e.g. \"invalid syntax (<skipif condition>, line 1)\") which produced repeated \"Error evaluating 'skipif' condition\" messages and caused tests to error. The failing pytest invocation returned a non-zero exit status, and CI wrapper scripts (scripts/ci/run-tests at line 129 and scripts/ci/run-build-system-tests at line 43) propagated the failure as subprocess.CalledProcessError. In some jobs an earlier dependency installation (python scripts/ci/install-build-system -> pip install --no-build-isolation -r requirements-dev-lock.txt) also failed (traceback at line 58)."
        ],
        "relevant_files": [
            {
                "file": "backends/build_system/functional/test_aws_cli_venv.py",
                "line_number": 971,
                "reason": "Pytest error lines reference this test module: e.g. \"ERROR backends\\build_system\\functional\\test_aws_cli_venv.py::TestAwsCliVenv::test_create_windows - Failed: Error evaluating 'skipif' condition\" and logs show \"invalid syntax (<skipif condition>, line 1)\" / \"SyntaxError: invalid syntax\" while evaluating its skipif conditions."
            },
            {
                "file": "scripts/ci/run-tests",
                "line_number": 129,
                "reason": "Traceback shows this script invoked the failing pytest: \"File \\\".../scripts/ci/run-tests\\\", line 129, in <module>\" and the log contains \"subprocess.CalledProcessError: Command 'pytest ... backends/build_system/' returned non-zero exit status 1.\""
            },
            {
                "file": "scripts/ci/run-build-system-tests",
                "line_number": 43,
                "reason": "Wrapper script appears in traceback calling run-tests: \"File \\\".../scripts/ci/run-build-system-tests\\\", line 43, in <module>\" and the CalledProcessError shows the run-tests invocation returned non-zero, surfacing the CI failure."
            },
            {
                "file": "scripts/ci/install-build-system",
                "line_number": 58,
                "reason": "Logs show a failing install step in this script: \"File \\\".../scripts/ci/install-build-system\\\", line 58, in <module>\" and \"subprocess.CalledProcessError: Command 'pip install --no-build-isolation -r requirements-dev-lock.txt' returned non-zero exit status 1.\" (observed in several Windows jobs)."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.12.10/x64/lib/python3.12/subprocess.py",
                "line_number": 413,
                "reason": "Traceback frame in the Python stdlib shows subprocess.check_call raising the error: \"File \\\".../subprocess.py\\\", line 413, in check_call\" and the message \"subprocess.CalledProcessError: Command 'pytest ...' returned non-zero exit status 1.\" This ties the non-zero pytest return to the raised CalledProcessError."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Test setup error \u2013 SyntaxError evaluating pytest 'skipif' condition",
                "evidence": "Multiple pytest error lines: \"Error evaluating 'skipif' condition\" and \"invalid syntax (<skipif condition>, line 1)\" reported for backends/build_system/functional/test_aws_cli_venv.py (e.g. \"ERROR backends/build_system/functional/test_aws_cli_venv.py::TestAwsCliVenv::test_create - Failed: Error evaluating 'skipif' condition\")."
            },
            {
                "category": "Runtime Error",
                "subcategory": "CalledProcessError \u2013 test runner returned non-zero exit status",
                "evidence": "Logs show the wrapper raised subprocess.CalledProcessError: \"Command 'pytest --numprocesses=auto --dist=loadfile --maxprocesses=4 backends/build_system/' returned non-zero exit status 1.\" and final CI failure: \"##[error]Process completed with exit code 1.\""
            },
            {
                "category": "Dependency Error",
                "subcategory": "pip install failed during install-build-system",
                "evidence": "Traceback messages show \"subprocess.CalledProcessError: Command 'pip install --no-build-isolation -r requirements-dev-lock.txt' returned non-zero exit status 1.\" from scripts/ci/install-build-system in several job logs."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Run build-system tests",
                "command": "pytest --numprocesses=auto --dist=loadfile --maxprocesses=4 backends/build_system/"
            }
        ]
    },
    {
        "sha_fail": "0e6f97d05d68c3d66d270865c31e9060c5a5b4de",
        "error_context": [
            "A pytest functional test failed: functional/botocore/test_s3.py::test_retries_reuse_request_checksum raised an AssertionError because the test expected mock_urllib3_session_send.call_count == 2 but observed 1 (log: \"FAILED functional/botocore/test_s3.py::test_retries_reuse_request_checksum - AssertionError: assert 1 == 2\" and \"functional/botocore/test_s3.py:2216: AssertionError\").",
            "The repository test-run wrapper (scripts/ci/run-tests) invoked pytest and propagated the non-zero exit, producing a subprocess.CalledProcessError and making the CI step fail (log: \"subprocess.CalledProcessError: Command 'pytest ...' returned non-zero exit status 1.\").",
            "Additional, separate runtime/test issues appear in some runs: token refresh errors (botocore TokenRetrievalError and a KeyError for 'expiresIn') and many DeprecationWarning entries about datetime.datetime.utcnow(), but the immediate failing condition for this CI failure is the pytest AssertionError in functional/botocore/test_s3.py."
        ],
        "relevant_files": [
            {
                "file": "functional/botocore/test_s3.py",
                "line_number": 2216,
                "reason": "Pytest shows the failing test and assertion at this file and line: \"FAILED functional/botocore/test_s3.py::test_retries_reuse_request_checksum - AssertionError: assert 1 == 2\" and \"functional/botocore/test_s3.py:2216: AssertionError\" with the assertion \">       assert mock_urllib3_session_send.call_count == 2\" (call_count was 1)."
            },
            {
                "file": "scripts/ci/run-tests",
                "line_number": 129,
                "reason": "Traceback shows the CI test-run wrapper invoked pytest and propagated the error: \"File \\\"/home/runner/work/aws-cli/aws-cli/scripts/ci/run-tests\\\", line 129, in <module>\" and logs contain \"subprocess.CalledProcessError: Command 'pytest ...' returned non-zero exit status 1.\""
            },
            {
                "file": "awscli/botocore/tokens.py",
                "line_number": 83,
                "reason": "Logs include token parsing/refresh traceback frames referencing tokens.py (e.g. \"File \\\".../awscli/botocore/tokens.py\\\", line 83\") and later show a KeyError during token creation (KeyError: 'expiresIn' from tokens.py:263), tying this file to token-related failures seen in some test runs."
            },
            {
                "file": "awscli/botocore/credentials.py",
                "line_number": 567,
                "reason": "Captured traceback and warnings reference credentials refresh in credentials.py (e.g. \"File \\\".../awscli/botocore/credentials.py\\\", line 567, in _protected_refresh\") and show a propagated TokenRetrievalError: \"Error when retrieving token from sso: Token has expired and refresh failed.\""
            },
            {
                "file": "awscli/botocore/signers.py",
                "line_number": 716,
                "reason": "Multiple test-run log lines show DeprecationWarning referencing signers.py: \".../awscli/botocore/signers.py:716: DeprecationWarning: datetime.datetime.utcnow() is deprecated\", linking this module to warnings emitted during the failing test runs."
            },
            {
                "file": "awscli/botocore/crt/auth.py",
                "line_number": 259,
                "reason": "DeprecationWarning in test output points to this file: \".../awscli/botocore/crt/auth.py:259: DeprecationWarning: datetime.datetime.utcnow() is deprecated\", appearing in the same pytest run that produced the failing test."
            },
            {
                "file": "awscli/customizations/datapipeline/__init__.py",
                "line_number": 200,
                "reason": "Log shows a DeprecationWarning originating from this customization: \".../awscli/customizations/datapipeline/__init__.py:200: DeprecationWarning: datetime.datetime.utcnow() is deprecated\" during the test run."
            },
            {
                "file": "awscli/customizations/eks/get_token.py",
                "line_number": 108,
                "reason": "DeprecationWarning in the test output references this file: \".../awscli/customizations/eks/get_token.py:108: DeprecationWarning: datetime.datetime.utcnow() is deprecated\" and the context line 'token_expiration = datetime.utcnow() + timedelta(' appears in logs."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Functional test assertion failure",
                "evidence": "Pytest reported the failing test and assertion: \"FAILED functional/botocore/test_s3.py::test_retries_reuse_request_checksum - AssertionError: assert 1 == 2\" and the trace shows \"functional/botocore/test_s3.py:2216: AssertionError\" with the assertion line \"assert mock_urllib3_session_send.call_count == 2\"."
            },
            {
                "category": "Runtime Error",
                "subcategory": "subprocess.CalledProcessError \u2013 test runner exited non-zero",
                "evidence": "The run-tests wrapper propagated pytest's non-zero exit: \"subprocess.CalledProcessError: Command 'pytest ...' returned non-zero exit status 1.\" and CI shows \"##[error]Process completed with exit code 1.\""
            },
            {
                "category": "Runtime Error",
                "subcategory": "Token retrieval / parsing errors (botocore.exceptions.TokenRetrievalError, KeyError)",
                "evidence": "Some runs show token-related exceptions: \"botocore.exceptions.TokenRetrievalError: Error when retrieving token from sso: Token has expired and refresh failed\" and a KeyError: \"KeyError: 'expiresIn'\" tied to awscli/botocore/tokens.py (e.g. tokens.py:263)."
            },
            {
                "category": "Runtime Warning",
                "subcategory": "DeprecationWarning \u2013 datetime.datetime.utcnow() usage",
                "evidence": "Numerous DeprecationWarning lines in the test output reference datetime.utcnow(): e.g. \"/.../awscli/botocore/signers.py:716: DeprecationWarning: datetime.datetime.utcnow() is deprecated\" and similar warnings for other awscli modules and tests."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Run tests",
                "command": "python scripts/ci/run-tests --with-cov (invokes pytest)"
            }
        ]
    },
    {
        "sha_fail": "4a32a9357914d448b98bb5823f454902b92368e6",
        "error_context": [
            "The build step failed when the CI ran the install script (python scripts/ci/install) which invoked `python -m build`. The packaging/build process raised a SyntaxError while importing awscli code: logs show \"SyntaxError: f-string: unmatched '['\" coming from awscli/customizations/s3/subcommands.py at line 1638. Because that syntax error occurs during wheel building/import, the build subprocess exits and is reported as a CalledProcessError (`Command 'python -m build' returned non-zero exit status 1`), causing the job step to fail on multiple matrix runners."
        ],
        "relevant_files": [
            {
                "file": "awscli/customizations/s3/subcommands.py",
                "line_number": 1638,
                "reason": "Log shows a SyntaxError frame pointing to this file and line: \"File \\\".../awscli/customizations/s3/subcommands.py\\\", line 1638\" followed by the f-string error and caret: \"f'The user-provided path {params['src']} does not exist.'\" and \"SyntaxError: f-string: unmatched '['\"."
            },
            {
                "file": "awscli/handlers.py",
                "line_number": 113,
                "reason": "This file appears in the import traceback that leads to the SyntaxError: \"File \\\".../awscli/handlers.py\\\", line 113, in <module>\" then \"from awscli.customizations.s3.s3 import s3_plugin_initialize\", showing the import chain that triggered the error in subcommands.py."
            },
            {
                "file": "backends/pep517.py",
                "line_number": 65,
                "reason": "The build backend invoked by pep517 reports calls into this file: \"File \\\".../backends/pep517.py\\\", line 65, in build_wheel\" and later calls `_inject_wheel_extras`, demonstrating the wheel-building process that attempted to import awscli and hit the SyntaxError."
            },
            {
                "file": "scripts/ci/install",
                "line_number": 32,
                "reason": "The CI step runs this install script which invoked the failing command: log shows \"File \\\".../scripts/ci/install\\\", line 32, in <module>\" and the call `run(\"python -m build\")`, and the final error is `subprocess.CalledProcessError: Command 'python -m build' returned non-zero exit status 1.`"
            }
        ],
        "error_types": [
            {
                "category": "Syntax Error",
                "subcategory": "SyntaxError in f-string (unmatched '[')",
                "evidence": "Multiple runners report: \"SyntaxError: f-string: unmatched '['\" with a traceback frame: \"File \\\".../awscli/customizations/s3/subcommands.py\\\", line 1638\" and the offending f-string `f'The user-provided path {params['src']} does not exist.'` (caret under the '['), proving a source-code syntax error."
            },
            {
                "category": "Build / Packaging Error",
                "subcategory": "Build wheel failed (subprocess.CalledProcessError)",
                "evidence": "The install/build step shows `subprocess.CalledProcessError: Command 'python -m build' returned non-zero exit status 1.` and repeated log lines \"ERROR Backend subproccess exited when trying to invoke build_wheel\" indicating the wheel build failed due to the underlying syntax error."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Install dependencies",
                "command": "python -m build (invoked by `python scripts/ci/install`)"
            }
        ]
    },
    {
        "sha_fail": "6daeaefa76ab8bfe9a6212b1e4263743079149ce",
        "error_context": [
            "A pytest run failed because a single functional test functional/test_telemetry.py::TestCLISessionDatabaseConnection::test_timeout_does_not_raise_exception raised a TypeError: \"'NoneType' object is not subscriptable\".",
            "The TypeError originates in awscli.telemetry where the code attempts to subscript the result of cur.fetchone() (log shows \"host_id_ct = cur.fetchone()[0]\" and \"TypeError: 'NoneType' object is not subscriptable\" at awscli/telemetry.py:114), which caused pytest to exit non-zero and the CI test-run wrapper scripts/ci/run-tests to raise a subprocess.CalledProcessError."
        ],
        "relevant_files": [
            {
                "file": "functional/test_telemetry.py",
                "line_number": null,
                "reason": "Pytest short-summary names this test as failing: \"FAILED functional/test_telemetry.py::TestCLISessionDatabaseConnection::test_timeout_does_not_raise_exception - TypeError: 'NoneType' object is not subscriptable\", directly tying the failing test case to this file."
            },
            {
                "file": "awscli/telemetry.py",
                "line_number": 114,
                "reason": "Traceback in the logs shows the runtime error at awscli.telemetry: the failing line is shown as \"host_id_ct = cur.fetchone()[0]\" followed by \"TypeError: 'NoneType' object is not subscriptable\" and references awscli/telemetry.py:114."
            },
            {
                "file": "scripts/ci/run-tests",
                "line_number": 129,
                "reason": "The CI wrapper script invoked pytest and propagated the failure; logs show \"File \\\".../scripts/ci/run-tests\\\", line 129\" and a subprocess.CalledProcessError: \"Command 'pytest ...' returned non-zero exit status 1.\" indicating this script raised after pytest failed."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "Functional test failure \u2013 TypeError raised in test",
                "evidence": "Pytest short-summary: \"FAILED functional/test_telemetry.py::TestCLISessionDatabaseConnection::test_timeout_does_not_raise_exception - TypeError: 'NoneType' object is not subscriptable\"."
            },
            {
                "category": "Runtime Error",
                "subcategory": "TypeError \u2013 'NoneType' object is not subscriptable in awscli.telemetry",
                "evidence": "Traceback shows \"host_id_ct = cur.fetchone()[0]\" then \"TypeError: 'NoneType' object is not subscriptable\" at awscli/telemetry.py:114 (log: \"/.../site-packages/awscli/telemetry.py:114: TypeError\")."
            },
            {
                "category": "CI Step Failure",
                "subcategory": "CalledProcessError \u2013 pytest returned non-zero exit status",
                "evidence": "Run-tests wrapper raised: \"subprocess.CalledProcessError: Command 'pytest ...' returned non-zero exit status 1.\" and \"##[error]Process completed with exit code 1.\""
            },
            {
                "category": "Runtime Warning",
                "subcategory": "DeprecationWarning \u2013 datetime.datetime.utcnow() is deprecated (reported during tests)",
                "evidence": "Multiple DeprecationWarning lines in logs such as \"/.../site-packages/awscli/botocore/auth.py:422: DeprecationWarning: datetime.datetime.utcnow() is deprecated\" and similar warnings in awscli/customizations and test files."
            }
        ],
        "failed_job": [
            {
                "job": "build",
                "step": "Run tests",
                "command": "python scripts/ci/run-tests --with-cov"
            }
        ]
    },
    {
        "sha_fail": "7751beb21807fa7f206079b8f69bf887ec16a199",
        "error_context": [
            "The CI run ended with a non\u2011zero exit (##[error]Process completed with exit code 1) while the pipeline was inflating/downloading test-result artifacts (multiple \"inflating: windows-latest-.../pytest.xml\" lines). The logs show an EOF/read error during archive extraction: \"(EOF or read error, treating as \\\"[N]one\\\" ...)\", which precedes the final exit code.",
            "The failure happened while the Publish Test Results action (EnricoMi/publish-unit-test-result-action) was being pulled/run (logs show the action download and docker image pull), indicating the problem occurred in the publish-results step while processing artifacts rather than in a specific Python test or source file."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Runtime Error",
                "subcategory": "Archive extraction / I/O error leading to non-zero exit",
                "evidence": "\"(EOF or read error, treating as \\\"[N]one\\\" ... )\" followed immediately by \"##[error]Process completed with exit code 1.\" \u2014 shows an EOF/read error during inflating archives and a subsequent non-zero exit."
            },
            {
                "category": "CI Action Failure",
                "subcategory": "Publish-test-results action (artifact processing) failure",
                "evidence": "Logs show the publish-unit-test-result-action being downloaded and its image pulled (\"Download action repository 'EnricoMi/publish-unit-test-result-action@v2'\" and \"Pull down action image 'ghcr.io/enricomi/publish-unit-test-result-action:v2.20.0'\"), and the job later exits with code 1 while inflating pytest.xml/env.yaml \u2014 implicating the publish-results action step."
            }
        ],
        "failed_job": [
            {
                "job": "Publish test results",
                "step": "Publish test results",
                "command": "EnricoMi/publish-unit-test-result-action (docker image ghcr.io/enricomi/publish-unit-test-result-action:v2.20.0) \u2014 artifact extraction / inflating of pytest.xml/env.yaml (internal to the action)"
            }
        ]
    },
    {
        "sha_fail": "0cc4b5faec6ff58c1d667b048e5ee7df4ba664a7",
        "error_context": [
            "The CI job failed while creating the Conda environment with micromamba: libmamba encountered a file-copy conflict for a compiled Python file (sessions.cpython-39.pyc), causing micromamba to exit with code 1 and abort environment creation. Logs also show cache restore/save issues (cache restore responded with 400 and save returned an error) and a post-job cleanup message about a missing micromamba activation entry in the runner's .bash_profile."
        ],
        "relevant_files": [
            {
                "file": "C:/Users/runneradmin/micromamba-bin/micromamba.exe",
                "line_number": null,
                "reason": "Log reports the failing process explicitly: \"##[error]The process 'C:\\Users\\runneradmin\\micromamba-bin\\micromamba.exe' failed with exit code 1\" \u2014 the micromamba executable invocation is the command that failed."
            },
            {
                "file": "C:/Users/runneradmin/micromamba/pkgs/tomli-2.2.1-py39haa95532_0/Lib/site-packages/pip/_vendor/requests/__pycache__/sessions.cpython-39.pyc",
                "line_number": null,
                "reason": "The critical error message names this file as one side of the copy conflict: \"critical libmamba copy: The file exists.: \"C:\\Users\\runneradmin\\micromamba\\pkgs\\tomli-2.2.1-py39...\\sessions.cpython-39.pyc\", ...\""
            },
            {
                "file": "C:/Users/runneradmin/micromamba/envs/test_env/Lib/site-packages/pip/_vendor/requests/__pycache__/sessions.cpython-39.pyc",
                "line_number": null,
                "reason": "The critical error message names this file as the other side of the copy conflict: \"..., \\\"C:\\Users\\runneradmin\\micromamba\\envs\\test_env\\Lib\\site-packages\\pip\\_vendor\\requests\\__pycache__\\sessions.cpython-39.pyc\\\"\" \u2014 showing an existing-target collision during libmamba copy."
            },
            {
                "file": "C:/Users/runneradmin/.bash_profile",
                "line_number": null,
                "reason": "Post-job cleanup logs contain an error tied to this file: \"##[error]Could not find micromamba activate test_env in C:\\Users\\runneradmin\\.bash_profile\" (cleanup/deinit step issue)."
            }
        ],
        "error_types": [
            {
                "category": "Dependency / Environment Error",
                "subcategory": "Package install file-copy conflict (libmamba)",
                "evidence": "\"critical libmamba copy: The file exists.: \\\"C:\\\\Users\\\\runneradmin\\\\micromamba\\\\pkgs\\\\tomli-2.2.1-py39...\\\\sessions.cpython-39.pyc\\\", \\\"C:\\\\Users\\\\runneradmin\\\\micromamba\\\\envs\\\\test_env\\\\Lib\\\\site-packages\\\\pip\\\\_vendor\\\\requests\\\\__pycache__\\\\sessions.cpython-39.pyc\\\"\" and \"##[error]The process 'C:\\\\Users\\\\runneradmin\\\\micromamba-bin\\\\micromamba.exe' failed with exit code 1\""
            },
            {
                "category": "CI Cache / Infrastructure Error",
                "subcategory": "Cache restore/save failure",
                "evidence": "\"##[warning]Failed to restore: Cache service responded with 400\" and later \"##[warning]Failed to save: <h2>Our services aren't available right now</h2>...\" followed by \"Saved cache with ID `-1`\" in the logs."
            },
            {
                "category": "Configuration / Cleanup Error",
                "subcategory": "Missing activation entry during micromamba deinit",
                "evidence": "\"##[error]Could not find micromamba activate test_env in C:\\\\Users\\\\runneradmin\\\\.bash_profile\" appears during post job cleanup / shell deinit."
            }
        ],
        "failed_job": [
            {
                "job": "test",
                "step": "Install Conda environment with Micromamba",
                "command": "C:\\Users\\runneradmin\\micromamba-bin\\micromamba.exe create -y -r C:\\Users\\runneradmin\\micromamba -f ci/requirements-py3.9.yml python=3.9 --log-level warning --rc-file D:\\a\\_temp\\setup-micromamba\\.condarc"
            }
        ]
    },
    {
        "sha_fail": "d178af5ad25dedf29ddb5fe3f71e9634f765bc0e",
        "error_context": [
            "The test suite failed: a unit test assertion in test/test_all_urls.py (line 83) failed with an AssertionError causing the test-run to exit non\u2011zero. test/test_execution.py (line 52) attempted to run test/test_all_urls.py via subprocess and raised a CalledProcessError when that subprocess returned exit status 1, which made the CI 'Run tests' step fail.",
            "Logs show the failing assertion: \"AssertionError: False is not true : FranceTVEmbedIE should match URL 'http://embed.francetv.fr/...'\", and the subprocess error: \"Command '['.../python', 'test/test_all_urls.py']' returned non-zero exit status 1.\""
        ],
        "relevant_files": [
            {
                "file": "test/test_all_urls.py",
                "line_number": 83,
                "reason": "The log shows a failing test frame: \"File \"/home/runner/work/youtube-dl/youtube-dl/test/test_all_urls.py\", line 83, in test_no_duplicates\" followed by \"AssertionError: False is not true : FranceTVEmbedIE should match URL 'http://embed.francetv.fr/?ue=...'\", directly tying this file/line to the failing unit test."
            },
            {
                "file": "test/test_execution.py",
                "line_number": 52,
                "reason": "The log contains the traceback: \"File \"/home/runner/work/youtube-dl/youtube-dl/test/test_execution.py\", line 52, in test_lazy_extractors\" showing it called subprocess.check_call(...) to run 'test/test_all_urls.py', and the subsequent subprocess.CalledProcessError indicates this step invoked the failing test."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "\"AssertionError: False is not true : FranceTVEmbedIE should match URL 'http://embed.francetv.fr/?ue=...'\" with frame \"File .../test/test_all_urls.py, line 83, in test_no_duplicates\" (from context_lines)."
            },
            {
                "category": "Runtime / Test Runner Error",
                "subcategory": "subprocess.CalledProcessError (test subprocess exited non-zero)",
                "evidence": "\"subprocess.CalledProcessError: Command '['/opt/hostedtoolcache/Python/3.9.22/x64/bin/python', 'test/test_all_urls.py']' returned non-zero exit status 1.\" (shown in message/context_lines), indicating the test-runner subprocess failed and caused the CI step to exit 1."
            }
        ],
        "failed_job": [
            {
                "job": "Run tests",
                "step": "Run tests",
                "command": "python test/test_all_urls.py (invoked by the test runner; the CI step executes ./devscripts/run_tests.sh which invoked the python subprocess)"
            }
        ]
    },
    {
        "sha_fail": "073ad2a4332ae2ea545fd18c601fdf2171d150c6",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error in agno/agent/agent.py:4185. Mypy flagged an attribute access: a Union includes agno.storage.session.v2.workflow.WorkflowSession which does not have attribute \"memory\", causing 'Found 1 error in 1 file' and the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 4185,
                "reason": "Log shows a mypy error at this file and line: \"agno/agent/agent.py:4185: error: Item \\\"agno.storage.session.v2.workflow.WorkflowSession\\\" of \\\"Union[...]\\\" has no attribute \\\"memory\\\"  [union-attr]\" \u2014 directly tying this file/line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attribute error (attribute not found on one union member)",
                "evidence": "Log message: \"agno/agent/agent.py:4185: error: Item \\\"agno.storage.session.v2.workflow.WorkflowSession\\\" of \\\"Union[...]\\\" has no attribute \\\"memory\\\"  [union-attr]\" and \"Found 1 error in 1 file (checked 541 source files)\" \u2014 indicates a mypy type-checking failure."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "09c3a7624b042f0af173fb95362fbefbb3d32522",
        "error_context": [
            "The style-check job failed because the Mypy type-checker reported 6 'Incompatible return value type' errors in agno/agent/agent.py (lines 1099, 1111, 1116, 1765, 1780, 1784).",
            "The failure occurred during the workflow step 'Mypy' (command: 'mypy .'), which exited with a non-zero status: 'Found 6 errors in 1 file' and 'Process completed with exit code 1.'"
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 1099,
                "reason": "Mypy error frames in the logs: e.g. 'agno/agent/agent.py:1099: error: Incompatible return value type (...)  [return-value]' and additional errors at lines 1111, 1116, 1765, 1780, 1784; logs also state 'Found 6 errors in 1 file'."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy incompatible return value type (return-value)",
                "evidence": "Log lines: 'agno/agent/agent.py:1099: error: Incompatible return value type (...)  [return-value]' (repeated for 1111,1116,1765,1780,1784) and 'Found 6 errors in 1 file (checked 520 source files)'."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "110e09997f8a22a617e261dec9e301129bbead65",
        "error_context": [
            "The style-check job failed because mypy reported type-checking errors. Mypy produced 3 errors across 2 files: agno/agent/agent.py (errors at lines 3003 and 3132, with a note at 3109) and agno/team/team.py (error at line 1834). The CI step running was the 'Mypy' step which executed the command `mypy .`."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 3003,
                "reason": "Mypy error reported in logs: \"agno/agent/agent.py:3003: error: Unexpected keyword argument \\\"stream_model_response\\\" for \\\"_handle_model_response_chunk\\\" of \\\"Agent\\\"; did you mean \\\"model_response\\\"?  [call-arg]\" (also additional mypy frames at lines 3109 and 3132)."
            },
            {
                "file": "agno/agent/agent.py",
                "line_number": 3132,
                "reason": "Mypy error reported in logs: \"agno/agent/agent.py:3132: error: Item \\\"None\\\" of \\\"Optional[type[BaseModel]]\\\" has no attribute \\\"__name__\\\"  [union-attr]\"."
            },
            {
                "file": "agno/team/team.py",
                "line_number": 1834,
                "reason": "Mypy error reported in logs: \"agno/team/team.py:1834: error: Item \\\"None\\\" of \\\"Optional[type[BaseModel]]\\\" has no attribute \\\"__name__\\\"  [union-attr]\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy call-arg error (unexpected keyword argument)",
                "evidence": "\"agno/agent/agent.py:3003: error: Unexpected keyword argument \\\"stream_model_response\\\" for \\\"_handle_model_response_chunk\\\" of \\\"Agent\\\"; did you mean \\\"model_response\\\"?  [call-arg]\""
            },
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attr error (Optional[...] may be None; attribute access on None)",
                "evidence": "\"agno/agent/agent.py:3132: error: Item \\\"None\\\" of \\\"Optional[type[BaseModel]]\\\" has no attribute \\\"__name__\\\"  [union-attr]\" and \"agno/team/team.py:1834: error: Item \\\"None\\\" of \\\"Optional[type[BaseModel]]\\\" has no attribute \\\"__name__\\\"  [union-attr]\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "18f4596992d6c36ccf9da84ae30083ef65878130",
        "error_context": [
            "The style-check job failed because ruff (lint) reported a style error E712 in tests/integration/embedder/test_jina_embedder.py (line 17) \u2014 an equality comparison to False. The log shows: \"tests/integration/embedder/test_jina_embedder.py:17:12: E712 Avoid equality comparisons to `False`; use `not embedder.late_chunking:`\" and ruff finished with \"Found 1 error.\" causing the step to exit with code 1.",
            "The failing step was the 'Ruff check' step (command: `ruff check .`) in the style-check (3.9) job; earlier `ruff format .` reformatted 1 file but the subsequent `ruff check .` still found the E712 lint violation."
        ],
        "relevant_files": [
            {
                "file": "tests/integration/embedder/test_jina_embedder.py",
                "line_number": 17,
                "reason": "Log contains the lint error: \"tests/integration/embedder/test_jina_embedder.py:17:12: E712 Avoid equality comparisons to `False`; use `not embedder.late_chunking:`\", tying this file and line to the ruff failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting / Style",
                "subcategory": "E712 Avoid equality comparison to False (ruff/flake8 rule)",
                "evidence": "\"tests/integration/embedder/test_jina_embedder.py:17:12: E712 Avoid equality comparisons to `False`; use `not embedder.late_chunking:`\" and \"Found 1 error.\" followed by \"##[error]Process completed with exit code 1.\" in the style-check logs."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "1a41bfdce3fbdd13b1269f0acc04885762c39dd1",
        "error_context": [
            "The style-check job failed during the Mypy static type check: mypy reported 3 type errors across two files. The log shows mypy errors indicating Optional[str] values may be None and are used with .lower(), e.g. agno/memory/agent.py:276 and :289 and agno/memory/team.py:329. The failing step was the 'Mypy' step running the command `mypy .`, which caused the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/memory/agent.py",
                "line_number": 276,
                "reason": "Log line shows a mypy error for this file and line: \"agno/memory/agent.py:276: error: Item \"None\" of \"Optional[str]\" has no attribute \"lower\"  [union-attr]\"."
            },
            {
                "file": "agno/memory/agent.py",
                "line_number": 289,
                "reason": "Log line shows a second mypy error in the same file: \"agno/memory/agent.py:289: error: Item \"None\" of \"Optional[str]\" has no attribute \"lower\"  [union-attr]\"."
            },
            {
                "file": "agno/memory/team.py",
                "line_number": 329,
                "reason": "Log line shows a mypy error for this file and line: \"agno/memory/team.py:329: error: Item \"None\" of \"Optional[str]\" has no attribute \"lower\"  [union-attr]\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attr (Optional[str] used without None-check)",
                "evidence": "\"mypy .\" is shown in the logs and the errors are: \"agno/memory/agent.py:276: error: Item \\\"None\\\" of \\\"Optional[str]\\\" has no attribute \\\"lower\\\"  [union-attr]\"; similar messages at agent.py:289 and team.py:329; log also states \"Found 3 errors in 2 files\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "1a6efefa726610d7ca262beb8e6adf1607829dbb",
        "error_context": [
            "The style-check job failed because mypy reported type-checking errors in agno/knowledge/agent.py. Mypy produced three errors (lines 192, 276, 610) complaining that a Coroutine value must be used (unused-coroutine / likely missing await), and the job ended with \"Process completed with exit code 1.\"",
            "Ruff formatting ran earlier and reformatted one file but did not cause the failure; the visible failing tool is mypy during the 'Mypy' step."
        ],
        "relevant_files": [
            {
                "file": "agno/knowledge/agent.py",
                "line_number": 192,
                "reason": "Log shows mypy errors pointing to this file: \"agno/knowledge/agent.py:192: error: Value of type \\\"Coroutine[Any, Any, None]\\\" must be used  [unused-coroutine]\" (also referenced at lines 276 and 610), and the summary line: \"Found 3 errors in 1 file (checked 549 source files)\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy unused-coroutine / missing await",
                "evidence": "Multiple context lines from the mypy run: \"agno/knowledge/agent.py:192: error: Value of type \\\"Coroutine[Any, Any, None]\\\" must be used  [unused-coroutine]\" and similar messages for lines 276 and 610; followed by \"Found 3 errors in 1 file (checked 549 source files)\" and \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
        "error_context": [
            "The style-check job failed because ruff reported a lint error: an unused import (F401) in tests/unit/reader/test_url_reader.py (line 1). The ruff check returned \"Found 1 error.\" and the job exited with code 1.",
            "The specific failing step was the 'Ruff check' step which ran the command `ruff check .`; logs show the F401 message and note that the issue is fixable with `--fix`."
        ],
        "relevant_files": [
            {
                "file": "tests/unit/reader/test_url_reader.py",
                "line_number": 1,
                "reason": "Log shows: \"tests/unit/reader/test_url_reader.py:1:27: F401 [*] `unittest.mock.AsyncMock` imported but unused\" and the source line: \"1 | from unittest.mock import AsyncMock, Mock, patch\" \u2014 indicating this file/line caused the ruff F401 failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Unused Import (lint error, ruff F401)",
                "evidence": "Log: \"tests/unit/reader/test_url_reader.py:1:27: F401 [*] `unittest.mock.AsyncMock` imported but unused\" and \"Found 1 error.\" plus \"[*] 1 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "1d0fff71ae9ea258fad3d54257be9ca9b3eb499e",
        "error_context": [
            "The style-check job failed because ruff reported a lint error: an unused import (F401) in tests/unit/tools/models/test_gemini.py at line 11 (import of agno.models.message.Message). Ruff returned \"Found 1 error.\" and the step exited with code 1, causing the job to fail. Ruff format ran earlier and reformatted one file, but ruff check afterwards caught the unused-import issue."
        ],
        "relevant_files": [
            {
                "file": "tests/unit/tools/models/test_gemini.py",
                "line_number": 11,
                "reason": "Log shows: \"tests/unit/tools/models/test_gemini.py:11:33: F401 [*] `agno.models.message.Message` imported but unused\" and the displayed source line \"11 | from agno.models.message import Message\" with caret marking the F401 error."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "CI log: \"tests/unit/tools/models/test_gemini.py:11:33: F401 [*] `agno.models.message.Message` imported but unused\" and \"Found 1 error.\" plus \"[*] 1 fixable with the `--fix` option.\"; ruff then caused the step to exit with code 1 (\"Process completed with exit code 1\")."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "1da36493c0905f61ee6fa58ebf792f993d3579dc",
        "error_context": [
            "The style-check job failed because mypy reported type-checking errors. Mypy found 2 errors in agno/knowledge/csv_url.py (lines 77 and 107), both complaining that a str was passed where a Path was expected.",
            "Mypy returned a non-zero exit (\"Found 2 errors in 1 file\" and \"Process completed with exit code 1.\"), causing the style-check (3.9) job to fail during the \"Mypy\" step that runs the command `mypy .`."
        ],
        "relevant_files": [
            {
                "file": "agno/knowledge/csv_url.py",
                "line_number": 77,
                "reason": "Mypy error lines in the CI log: \"agno/knowledge/csv_url.py:77: error: Argument 1 to \\\"prepare_load\\\" of \\\"AgentKnowledge\\\" has incompatible type \\\"str\\\"; expected \\\"Path\\\" [arg-type]\" and another at line 107; log also shows \"Found 2 errors in 1 file\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch (arg-type: incompatible argument type)",
                "evidence": "\"agno/knowledge/csv_url.py:77: error: Argument 1 to \\\"prepare_load\\\" of \\\"AgentKnowledge\\\" has incompatible type \\\"str\\\"; expected \\\"Path\\\"  [arg-type]\" and \"agno/knowledge/csv_url.py:107: error: Argument 1 to \\\"aprepare_load\\\" of \\\"AgentKnowledge\\\" has incompatible type \\\"str\\\"; expected \\\"Path\\\"\"; followed by \"Found 2 errors in 1 file\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "233c76da6b91def1e9d09a3fe170578c0bded0aa",
        "error_context": [
            "The style-check job failed because Ruff reported a lint error: an unused import in agno/utils/models/claude.py (line 6).",
            "Ruff produced an F401 (imported but unused) for `agno.utils.log.log_info`, the check found 1 error and the job exited with code 1; the error is fixable with `--fix`."
        ],
        "relevant_files": [
            {
                "file": "agno/utils/models/claude.py",
                "line_number": 6,
                "reason": "Log shows: \"agno/utils/models/claude.py:6:39: F401 [*] `agno.utils.log.log_info` imported but unused\" and the code context with a caret pointing at the unused import, tying this file/line to the Ruff failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused import (Ruff F401)",
                "evidence": "\"agno/utils/models/claude.py:6:39: F401 [*] `agno.utils.log.log_info` imported but unused\" and \"Found 1 error.\" plus \"[*] 1 fixable with the `--fix` option.\" from the Ruff check logs."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "235e19d6998790dd527e8a43710990aef29359b1",
        "error_context": [
            "The test suite run by pytest failed: the unit test libs/agno/tests/unit/reader/test_csv_reader.py::test_async_read_multi_page_csv contains an assertion mismatch at line 150 (expected documents[0].id == \"multi_page_page1_1\" but got '5fde5c88-8d6a-4139-88bb-0850c934a08e_1').",
            "Pytest (the 'Run tests for Agno' step) returned a non-zero exit code, causing the tests job (tests / python 3.12 matrix) to fail. The test output also shows that repr() of the CSVReader raised an AttributeError (\"'CSVReader' object has no attribute 'separators'\"), which may indicate an additional runtime/initialization issue with the CSVReader object used by the failing test."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/reader/test_csv_reader.py",
                "line_number": 150,
                "reason": "Test failure reported directly in the logs: \"FAILED libs/agno/tests/unit/reader/test_csv_reader.py::test_async_read_multi_page_csv\" and \"libs/agno/tests/unit/reader/test_csv_reader.py:150: AssertionError\". The assertion showing expected vs actual (\"- multi_page_page1_1\" vs \"+ 5fde5c88-..._1\") ties this file and line to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test",
                "evidence": "Log shows: \"FAILED libs/agno/tests/unit/reader/test_csv_reader.py::test_async_read_multi_page_csv - AssertionError: assert '5fde5c88-8d6...850c934a08e_1' == 'multi_page_page1_1'\" and \"libs/agno/tests/unit/reader/test_csv_reader.py:150: AssertionError\"."
            },
            {
                "category": "Runtime Error (secondary)",
                "subcategory": "AttributeError raised in object repr / test fixture",
                "evidence": "Log shows the test fixture/object repr contains an AttributeError: \"csv_reader = <[AttributeError(\"'CSVReader' object has no attribute 'separators'\") raised in repr()] CSVReader object at 0x7fc580305e80>\", indicating an AttributeError occurred when representing the CSVReader used by the test."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "source .venv/bin/activate\npython -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "3045ad82fb7ebc1182bf5d0a1d64713bec621512",
        "error_context": [
            "The style-check job failed because ruff reported a lint error: an unused import of pydantic.BaseModel in agno/app/playground/async_router.py (line 10).",
            "The failing step was 'Ruff check' (command: `ruff check .`), which returned a non-zero exit (Process completed with exit code 1). The log notes the issue is fixable (\"[*] 1 fixable with the `--fix` option\")."
        ],
        "relevant_files": [
            {
                "file": "agno/app/playground/async_router.py",
                "line_number": 10,
                "reason": "Log shows: \"agno/app/playground/async_router.py:10:22: F401 [*] `pydantic.BaseModel` imported but unused\" and context lines indicate the import at line 10 and the suggestion: \"Remove unused import: `pydantic.BaseModel`\"."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused import (F401)",
                "evidence": "\"agno/app/playground/async_router.py:10:22: F401 [*] `pydantic.BaseModel` imported but unused\" and \"= help: Remove unused import: `pydantic.BaseModel`\" from the ruff check output; ruff then reported \"Found 1 error.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "31d6eba6f9be2ea8fc3bf80efc498075f6241cb5",
        "error_context": [
            "The style-check job failed because the linter 'ruff' reported an unused import in agno/tools/nebius.py (line 9) \u2014 the log shows \"agno/tools/nebius.py:9:28: F401 `agno.utils.log.log_debug` imported but unused\". Ruff returned exit code 1 (\"Process completed with exit code 1\"), causing the 'style-check' job to fail. The message also notes the issue is fixable with the `--fix` option."
        ],
        "relevant_files": [
            {
                "file": "agno/tools/nebius.py",
                "line_number": 9,
                "reason": "Log shows a ruff lint error for this file: \"agno/tools/nebius.py:9:28: F401 `agno.utils.log.log_debug` imported but unused\" and the following help: \"Remove unused import: `agno.utils.log.log_debug`\" \u2014 this ties the file and line directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting",
                "subcategory": "Unused import (F401)",
                "evidence": "CI log: \"agno/tools/nebius.py:9:28: F401 `agno.utils.log.log_debug` imported but unused\" and \"= help: Remove unused import: `agno.utils.log.log_debug`\"; ruff reported \"Found 1 error.\" and the job ended with \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "33ed019c7f623b9fb89b1430b38c7e7a7aefa57c",
        "error_context": [
            "The style-check job failed because mypy reported type-checking errors. Mypy produced two errors in agno/agent/agent.py (lines 378 and 2083), causing the 'Mypy' step to exit with code 1.",
            "Logs show: \"agno/agent/agent.py:378: error: Incompatible types in assignment (expression has type \\\"Optional[bool]\\\", variable has type \\\"bool\\\")\" and \"agno/agent/agent.py:2083: error: Argument \\\"number_of_sessions\\\" ... has incompatible type \\\"Optional[int]\\\"; expected \\\"int\\\"\", followed by \"Found 2 errors in 1 file (checked 467 source files)\" and \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 378,
                "reason": "Mypy reported errors in this file: \"agno/agent/agent.py:378: error: Incompatible types in assignment (expression has type \\\"Optional[bool]\\\", variable has type \\\"bool\\\")\" and also \"agno/agent/agent.py:2083: error: Argument \\\"number_of_sessions\\\" ... has incompatible type \\\"Optional[int]\\\"; expected \\\"int\\\"\" (log shows \"Found 2 errors in 1 file\")."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch (incompatible assignment / arg-type)",
                "evidence": "\"agno/agent/agent.py:378: error: Incompatible types in assignment (expression has type \\\"Optional[bool]\\\", variable has type \\\"bool\\\")\" and \"agno/agent/agent.py:2083: error: Argument \\\"number_of_sessions\\\" ... has incompatible type \\\"Optional[int]\\\"; expected \\\"int\\\"\"; plus \"Found 2 errors in 1 file (checked 467 source files)\" and \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "344c0994f4fce22a64ad9c57270679e51b8e66e6",
        "error_context": [
            "A unit test failed under the 'tests' job when running pytest. The test libs/agno/tests/unit/reader/test_firecrawl_reader.py::test_scrape_with_api_key_and_formats_params raised an AssertionError because a mocked FirecrawlApp.scrape_url call was made with keyword args (waitUntil, formats) instead of a single params dict (Expected: scrape_url(..., params={...}) \u2014 Actual: scrape_url(..., waitUntil=..., formats=[...])).",
            "The AssertionError originates from unittest.mock assert_called_* machinery (traceback frames in unittest/mock.py) which raised \"AssertionError: expected call not found.\" as shown in the logs."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/reader/test_firecrawl_reader.py",
                "line_number": 99,
                "reason": "Test failure reported directly in the logs: \"libs/agno/tests/unit/reader/test_firecrawl_reader.py:99: AssertionError\" and multiple lines show the failing test name and assertion: \"FAILED libs/agno/tests/unit/reader/test_firecrawl_reader.py::test_scrape_with_api_key_and_formats_params - AssertionError: expected call not found.\" plus the Expected/Actual lines."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.12.10/x64/lib/python3.12/unittest/mock.py",
                "line_number": 949,
                "reason": "Traceback frames and assertion come from unittest.mock: logs show \"/opt/hostedtoolcache/.../unittest/mock.py:949: AssertionError\" and \"/opt/.../unittest/mock.py:961: AssertionError\", and the message \"AssertionError: expected call not found.\" is raised there when mock assertions fail."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (mock call mismatch)",
                "evidence": "\"FAILED libs/agno/tests/unit/reader/test_firecrawl_reader.py::test_scrape_with_api_key_and_formats_params - AssertionError: expected call not found.\" and the Expected/Actual lines: \"Expected: scrape_url('https://example.com', params={'waitUntil': 'networkidle2', 'formats': ['markdown']})\" vs \"Actual: scrape_url('https://example.com', waitUntil='networkidle2', formats=['markdown'])\"."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "python -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "39d992d415c335b61f670c31a68c0edc578e5062",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error. Mypy produced: \"agno/agent/agent.py:4810: error: Argument 1 to \\\"get_json_output_prompt\\\" has incompatible type \\\"type[BaseModel]\\\"; expected \\\"Union[str, list[Any], BaseModel]\\\"\", and the run ended with \"Found 1 error in 1 file\" and exit code 1. The failing step was the Mypy step (command: \"mypy .\")."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 4810,
                "reason": "Log shows a mypy error pointing to this file and line: \"agno/agent/agent.py:4810: error: Argument 1 to \\\"get_json_output_prompt\\\" has incompatible type \\\"type[BaseModel]\\\"; expected \\\"Union[str, list[Any], BaseModel]\\\"\" \u2014 this ties the file directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy argument-type mismatch (arg-type)",
                "evidence": "\"agno/agent/agent.py:4810: error: Argument 1 to \\\"get_json_output_prompt\\\" has incompatible type \\\"type[BaseModel]\\\"; expected \\\"Union[str, list[Any], BaseModel]\\\"\" and \"Found 1 error in 1 file (checked 498 source files)\" from the CI log."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "3a9a0b4124d28c2f4735ffad30b83bf4b1c82477",
        "error_context": [
            "The style-check job failed because the linter (ruff) reported an undefined name in agno/storage/singlestore.py. The log shows: \"agno/storage/singlestore.py:299:52: F821 Undefined name `num_history_sessions`\" and \"Found 1 error.\" which caused the step to exit with code 1.",
            "The failing step is the Ruff check step (run command: `ruff check .`) in the style-check job; this lint error is the root cause, not tests or mypy."
        ],
        "relevant_files": [
            {
                "file": "agno/storage/singlestore.py",
                "line_number": 299,
                "reason": "Log contains the lint error frame: \"agno/storage/singlestore.py:299:52: F821 Undefined name `num_history_sessions`\" and shows the source lines (lines 297-301) with the F821 marker under the undefined name."
            }
        ],
        "error_types": [
            {
                "category": "Linting",
                "subcategory": "Undefined name (F821)",
                "evidence": "Log line: \"agno/storage/singlestore.py:299:52: F821 Undefined name `num_history_sessions`\" and subsequent \"Found 1 error.\" followed by \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "3ab735577ed448887a8f1f463c73a611a4ca253e",
        "error_context": [
            "The style-check job failed because the linter reported undefined-name errors (F821) in a Python source file: the log shows a type-annotation line (line 12) with a caret pointing at the Union(...) contents and the marker 'F821', followed by 'Found 3 errors.' The run ended with 'Process completed with exit code 1.'",
            "These lint errors were produced while running the repository-style checks under the style-check job (working directory defaults to libs/agno), so the failure originated from the Ruff/flake8-style linter step before subsequent steps (e.g., mypy) could run."
        ],
        "relevant_files": [],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Linter error: undefined name (F821) in type annotation",
                "evidence": "Log context shows the annotated line and linter marker: '12 |     result: Union(\"AccuracyResult\", \"PerformanceResult\", \"ReliabilityResult\"),' with a caret under the Union contents and '^^^^^^^^^^^^^^^^ F821' and the summary 'Found 3 errors.' followed by 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "557f5305cd8dc547495ea9bbfec35bee632b63be",
        "error_context": [
            "The style-check job failed because ruff reported a linting error: F821 Undefined name `tool` in agno/team/team.py at line 5637. The failing line in the log is: \"yield \",\".join([tool.result for tool in member_agent_run_response.tools and tool.result])  # type: ignore\". This occurred during the \"ruff check .\" step (style-check job) and caused the job to exit with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/team/team.py",
                "line_number": 5637,
                "reason": "Log shows a ruff error: \"agno/team/team.py:5637:101: F821 Undefined name `tool`\" and the context includes the exact failing line: \"yield \",\".join([tool.result for tool in member_agent_run_response.tools and tool.result])  # type: ignore\"."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Undefined name (ruff F821)",
                "evidence": "Log line: \"agno/team/team.py:5637:101: F821 Undefined name `tool`\" and \"Found 1 error.\" plus the runner message \"##[error]Process completed with exit code 1.\" indicate a ruff lint error (F821) caused the failure."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
        "error_context": [
            "The tests job (Python 3.12) failed because a unit test assertion failed in libs/agno/tests/unit/tools/test_mem0.py. The test TestMem0Toolkit.test_add_memory_invalid_message_type expects the memory.add mock to be called with content '123' (string) but the actual call used the integer 123, causing an AssertionError at libs/agno/tests/unit/tools/test_mem0.py:131. Pytest reported: \"1 failed, 917 passed...\" and the job exited with code 1."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/tools/test_mem0.py",
                "line_number": 131,
                "reason": "Pytest failure points directly to this test file and line: \"libs/agno/tests/unit/tools/test_mem0.py:131: AssertionError\" and the log shows the failing assertion and the assert call: \"mock_memory_instance.add.assert_called_once_with([{\"role\": \"user\", \"content\": \"123\"}], ... )\" with \"E       AssertionError: expected call not found.\" and the diff showing expected '123' vs actual 123."
            },
            {
                "file": "/opt/hostedtoolcache/Python/3.12.10/x64/lib/python3.12/unittest/mock.py",
                "line_number": 949,
                "reason": "The unittest.mock implementation raised the AssertionError used by the test; logs include the traceback frame \"/opt/hostedtoolcache/Python/3.12.10/x64/lib/python3.12/unittest/mock.py:949: AssertionError\" and show the mock assertion failing (Expected vs Actual) as part of the failure stack."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (mock assertion mismatch \u2014 expected string vs int)",
                "evidence": "\"FAILED libs/agno/tests/unit/tools/test_mem0.py::TestMem0Toolkit::test_add_memory_invalid_message_type - AssertionError: expected call not found.\" and \"Expected: add([{'role': 'user', 'content': '123'}]...)  Actual: add([{'role': 'user', 'content': 123}]...)\" (log lines showing expected '123' vs actual 123)."
            },
            {
                "category": "Warnings (non-fatal)",
                "subcategory": "Deprecation / RuntimeWarnings reported during test run",
                "evidence": "Logs include multiple warning/deprecation messages (e.g. \"/home/runner/work/agno/agno/libs/agno/agno/storage/postgres.py:205: SADeprecationWarning\" and \"/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/youtube_transcript_api/_api.py:298: DeprecationWarning\" and RuntimeWarning about ffmpeg). These appear in the test output but are not the root cause of the failing job."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "source .venv/bin/activate\npython -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "5f24cca27fe01045314ee6ddb7619e164ada0c91",
        "error_context": [
            "A unit test failed during the test matrix for Python 3.12: libs/agno/tests/unit/tools/test_google_bigquery.py::test_run_sql_query_success raised an AssertionError because a mock assertion expected the BigQuery client to be called with only the cleaned SQL string but the actual call included an extra QueryJobConfig object. Pytest ended with \"1 failed\" and the runner exited with code 1, causing the tests job to fail."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/tools/test_google_bigquery.py",
                "line_number": 51,
                "reason": "The test file is the failing test: logs show \"FAILED libs/agno/tests/unit/tools/test_google_bigquery.py::test_run_sql_query_success - AssertionError: expected call not found.\" and an assertion trace pointing to \"libs/agno/tests/unit/tools/test_google_bigquery.py:51: AssertionError\" where the test calls mock_bq_client.query.assert_called_once_with(cleaned_query)."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (mock assertion mismatch)",
                "evidence": "\"FAILED libs/agno/tests/unit/tools/test_google_bigquery.py::test_run_sql_query_success - AssertionError: expected call not found.\" and pytest output showing Expected: query('SELECT product_name, quantity FROM sales') but Actual: query('SELECT product_name, quantity FROM sales', <google.cloud.bigquery.job.query.QueryJobConfig object ...>)"
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "python -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "601a9c6986f1659f3051449238c0c0c5d2ef4124",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: an 'Incompatible types in assignment' at agno/models/langdb/langdb.py line 30. The CI shows the mypy run produced the message 'Found 1 error in 1 file' and the job exited with code 1.",
            "The failing step was the 'Mypy' step that ran the command 'mypy .', which returned a non-zero exit (Process completed with exit code 1) after reporting the assignment type mismatch."
        ],
        "relevant_files": [
            {
                "file": "agno/models/langdb/langdb.py",
                "line_number": 30,
                "reason": "Log shows a mypy error targeting this file and line: 'agno/models/langdb/langdb.py:30: error: Incompatible types in assignment (expression has type \"None\", variable has type \"str\")'."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type mismatch (Incompatible types in assignment)",
                "evidence": "Log entry: 'agno/models/langdb/langdb.py:30: error: Incompatible types in assignment (expression has type \"None\", variable has type \"str\")' and 'Found 1 error in 1 file (checked 523 source files)'."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "67b1780b01f5d7461dc9b3d189d25acecbdb171d",
        "error_context": [
            "The style-check job failed during the Mypy step: `mypy .` reported 8 type-checking errors across 2 files and exited with code 1. The log shows an argument-type mismatch in agno/agent/agent.py and several type/union-attribute errors in agno/team/team.py (e.g. accesses on Optional[TeamRunResponse] that may be None).",
            "Specifically, agno/agent/agent.py:4533 reports an incompatible argument type (list[Union[dict[str, Any], str]] vs expected list[dict[str, Any]]), and agno/team/team.py has multiple 'Item \"None\" of \"Optional[TeamRunResponse]\" has no attribute \"extra_data\"' messages (lines ~4813, 4834-4838, 4869)."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 4533,
                "reason": "Mypy error logged: \"agno/agent/agent.py:4533: error: Argument 1 to \\\"convert_documents_to_string\\\" of \\\"Agent\\\" has incompatible type \\\"list[Union[dict[str, Any], str]]\\\"; expected \\\"list[dict[str, Any]]\\\"\" \u2014 this ties the file and line to the failing type-check."
            },
            {
                "file": "agno/team/team.py",
                "line_number": 4813,
                "reason": "Multiple mypy errors reference this file (e.g. \"agno/team/team.py:4813: error: Incompatible types in assignment ...\", and several \"agno/team/team.py:4834: error: Item \\\"None\\\" of \\\"Optional[TeamRunResponse]\\\" has no attribute \\\"extra_data\\\"\"), indicating team.py is involved in the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy argument type mismatch",
                "evidence": "\"agno/agent/agent.py:4533: error: Argument 1 to \\\"convert_documents_to_string\\\" of \\\"Agent\\\" has incompatible type \\\"list[Union[dict[str, Any], str]]\\\"; expected \\\"list[dict[str, Any]]\\\"\" (from log context_lines)."
            },
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attr / Optional attribute access",
                "evidence": "\"agno/team/team.py:4834: error: Item \\\"None\\\" of \\\"Optional[TeamRunResponse]\\\" has no attribute \\\"extra_data\\\"\" (and repeated similar messages for lines 4835-4838), showing code is accessing attributes on a value that mypy considers possibly None."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "6865351833c002710b5e861a4ff6bc05b74afd4b",
        "error_context": [
            "The style-check job failed because ruff reported a lint error: an unused import of `json` in agno/embedder/huggingface.py. The log shows \"agno/embedder/huggingface.py:1:8: F401 `json` imported but unused\" and \"Found 1 error.\" which caused the step to exit with code 1. Ruff format had reformatted one file earlier, but the blocking failure is from `ruff check .` discovering the F401 unused-import."
        ],
        "relevant_files": [
            {
                "file": "agno/embedder/huggingface.py",
                "line_number": 1,
                "reason": "Log contains: \"agno/embedder/huggingface.py:1:8: F401 [*] `json` imported but unused\" and shows the code context \"1 | import json\" with the F401 pointer and the help message \"Remove unused import: `json\"; this directly ties this file and line to the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "CI logs: \"agno/embedder/huggingface.py:1:8: F401 `json` imported but unused\"; follow-up messages: \"= help: Remove unused import: `json`\", \"Found 1 error.\", and \"[*] 1 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "69a56c3a1a48f96da4e8c6f71b19dfd71f3f2b8c",
        "error_context": [
            "The style-check job failed because ruff (the linter) reported an unused-import lint error: F401 for `typing.Union` in agno/tools/toolkit.py (line 2). Ruff reported \"Found 1 error.\" and the job exited with code 1; the log also states the issue is fixable with `--fix`."
        ],
        "relevant_files": [
            {
                "file": "agno/tools/toolkit.py",
                "line_number": 2,
                "reason": "Log shows: \"agno/tools/toolkit.py:2:57: F401 [*] `typing.Union` imported but unused\" and the surrounding lines display the import statement (`from typing import Any, Callable, Dict, List, Optional, Union`), tying this file and line to the ruff F401 failure."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Code Style",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "\"agno/tools/toolkit.py:2:57: F401 [*] `typing.Union` imported but unused\"; followed by \"Found 1 error.\" and \"[*] 1 fixable with the `--fix` option.\" in the ruff check output."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "6ad4e8d6bb1ea167fa88d68f5396968b713dd7ba",
        "error_context": [
            "The style-check job failed during the Mypy type-check step: mypy reported a type error in agno/tools/mcp.py at line 185 (\"None\" has no attribute \"__aenter__\"), causing the job to exit with code 1. The logs show \"Found 1 error in 1 file (checked 467 source files)\" and the runner produced \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "agno/tools/mcp.py",
                "line_number": 185,
                "reason": "Mypy reported an error for this file in the logs: \"agno/tools/mcp.py:185: error: \\\"None\\\" has no attribute \\\"__aenter__\\\"  [attr-defined]\" and the logs state \"Found 1 error in 1 file (checked 467 source files)\"."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy attribute error (None has no attribute __aenter__)",
                "evidence": "Log lines from the Mypy step: \"mypy .\" then \"agno/tools/mcp.py:185: error: \\\"None\\\" has no attribute \\\"__aenter__\\\"  [attr-defined]\" and \"Found 1 error in 1 file (checked 467 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "6e5f3fc6534025b5711d84f144d3071f1ff403d2",
        "error_context": [
            "The style-check job failed during the Mypy type-check step: mypy reported 3 errors in a single file, agno/tools/daytona.py. The errors include two missing-import/type-stub issues for 'daytona_sdk' and 'daytona_sdk.common.process' (reported at lines 5 and 19) and a redefinition of attribute \"result\" (reported at line 131, noting a previous definition at line 120).",
            "This failure comes from the 'style-check (3.9)' job running the 'Mypy' step (command: 'mypy .'), which returned exit code 1 after printing the above errors."
        ],
        "relevant_files": [
            {
                "file": "agno/tools/daytona.py",
                "line_number": 5,
                "reason": "Mypy error lines reference this file: 'agno/tools/daytona.py:5: error: Cannot find implementation or library stub for module named \"daytona_sdk\" [import-not-found]' and additional errors at lines 19 and 131 (e.g. 'agno/tools/daytona.py:19: error: Cannot find implementation or library stub for module named \"daytona_sdk.common.process\"' and 'agno/tools/daytona.py:131: error: Attribute \"result\" already defined on line 120 [no-redef]')."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Missing import / missing type stub ('import-not-found')",
                "evidence": "Log: 'agno/tools/daytona.py:5: error: Cannot find implementation or library stub for module named \"daytona_sdk\"  [import-not-found]' and 'agno/tools/daytona.py:19: error: Cannot find implementation or library stub for module named \"daytona_sdk.common.process\"  [import-not-found]'."
            },
            {
                "category": "Type Checking",
                "subcategory": "Symbol redefinition detected by mypy ('no-redef')",
                "evidence": "Log: 'agno/tools/daytona.py:131: error: Attribute \"result\" already defined on line 120  [no-redef]' (mypy reports attribute redefinition referencing line 120)."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "6f19da809ef086cec3d81173f2c1b849e81c896d",
        "error_context": [
            "The style-check job failed during static type checking: mypy reported 3 errors in agno/models/cerebras/cerebras.py. Mypy errors include \"Value of type \\\"Optional[Any]\\\" is not indexable\" at line 163 and 165 and \"Unsupported right operand type for in (\\\"Optional[Any]\\\")\" at line 163, causing the 'Mypy' step (command 'mypy .') to exit with code 1.",
            "This is a type-checking failure (mypy) rather than a formatting or test failure; the logs show 'Found 3 errors in 1 file' and the mypy output lines referencing the file and line numbers."
        ],
        "relevant_files": [
            {
                "file": "agno/models/cerebras/cerebras.py",
                "line_number": 163,
                "reason": "Mypy output lines: 'agno/models/cerebras/cerebras.py:163: error: Value of type \"Optional[Any]\" is not indexable  [index]' and 'agno/models/cerebras/cerebras.py:163: error: Unsupported right operand type for in (\"Optional[Any]\")  [operator]'; line 165 also reported 'Value of type \"Optional[Any]\" is not indexable'."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy type errors (indexing Optional, unsupported operand for 'in')",
                "evidence": "'agno/models/cerebras/cerebras.py:163: error: Value of type \"Optional[Any]\" is not indexable  [index]'; 'agno/models/cerebras/cerebras.py:163: error: Unsupported right operand type for in (\"Optional[Any]\")  [operator]'; 'agno/models/cerebras/cerebras.py:165: error: Value of type \"Optional[Any]\" is not indexable  [index]'."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "6fe4e00e8863c8da28d241cb65632725de6db64b",
        "error_context": [
            "The style-check job failed because the ruff linter (ruff check .) reported lint errors and returned a non-zero exit code. Logs show two F841 errors for an unused local variable `task` in agno/workflow/v2/workflow.py (lines 1238 and 1339), and the runner exited with \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "agno/workflow/v2/workflow.py",
                "line_number": 1238,
                "reason": "Log contains ruff error lines: \"agno/workflow/v2/workflow.py:1238:13: F841 Local variable `task` is assigned to but never used\" and a second occurrence at line 1339; ruff output and the surrounding context (showing the source lines) tie this file to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused local variable (ruff F841)",
                "evidence": "\"agno/workflow/v2/workflow.py:1238:13: F841 Local variable `task` is assigned to but never used\" and \"agno/workflow/v2/workflow.py:1339:13: F841 Local variable `task` is assigned to but never used\"; logs also show \"Found 2 errors.\" and \"##[error]Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "78c5d662bf2145c91356e5185db05a17950b3dc4",
        "error_context": [
            "The style-check job failed because mypy type checking reported two type errors and the workflow exited with code 1. Mypy produced \"No overload variant of \\\"run\\\" of \\\"Team\\\" matches argument types\" for agno/app/agui/sync_router.py:63 and a similar \"arun\" error for agno/app/agui/async_router.py:63. The failing step ran the command `mypy .` as part of the \"Mypy\" step in the style-check job."
        ],
        "relevant_files": [
            {
                "file": "agno/app/agui/sync_router.py",
                "line_number": 63,
                "reason": "Mypy error in logs: \"agno/app/agui/sync_router.py:63: error: No overload variant of \\\"run\\\" of \\\"Team\\\" matches argument types \\\"list[Message]\\\", \\\"Any\\\", \\\"bool\\\", \\\"bool\\\"  [call-overload]\" (shown in the mypy output)."
            },
            {
                "file": "agno/app/agui/async_router.py",
                "line_number": 63,
                "reason": "Mypy error in logs: \"agno/app/agui/async_router.py:63: error: No overload variant of \\\"arun\\\" of \\\"Team\\\" matches argument types \\\"list[Message]\\\", \\\"Any\\\", \\\"bool\\\", \\\"bool\\\"  [call-overload]\" (shown in the mypy output)."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy call-overload / argument type mismatch",
                "evidence": "Log lines: \"agno/app/agui/sync_router.py:63: error: No overload variant of \\\"run\\\" of \\\"Team\\\" matches argument types \\\"list[Message]\\\", \\\"Any\\\", \\\"bool\\\", \\\"bool\\\"  [call-overload]\" and \"agno/app/agui/async_router.py:63: error: No overload variant of \\\"arun\\\" of \\\"Team\\\" matches argument types \\\"list[Message]\\\", \\\"Any\\\", \\\"bool\\\", \\\"bool\\\"  [call-overload]\"; these are mypy messages shown after the `mypy .` command."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "7b8834b321a97bc57d7a5ac82680c24aff793b91",
        "error_context": [
            "The style-check job failed during the Mypy type-check step: mypy reported two type errors in a single file and the job exited with code 1. Logs show incompatible-assignment errors in agno/agent/agent.py at lines 6940 and 7389 and the runner reported \"Found 2 errors in 1 file (checked 522 source files)\" followed by \"Process completed with exit code 1.\""
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 6940,
                "reason": "Mypy error frames in the CI log: \"agno/agent/agent.py:6940: error: Incompatible types in assignment (expression has type \\\"Union[str, JSON, Markdown]\\\", variable has type \\\"str\\\")  [assignment]\" and a second similar error at \"agno/agent/agent.py:7389\". The log also states \"Found 2 errors in 1 file (checked 522 source files)\" tying these lines to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy incompatible types in assignment",
                "evidence": "Log lines: \"agno/agent/agent.py:6940: error: Incompatible types in assignment (expression has type \\\"Union[str, JSON, Markdown]\\\", variable has type \\\"str\\\")  [assignment]\" and \"agno/agent/agent.py:7389: error: Incompatible types in assignment (...)\"; plus \"Found 2 errors in 1 file (checked 522 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "7d51d48b09bbc5e6311becec1c1a4150fde89b4e",
        "error_context": [
            "A unit test failed: test_generate_image_missing_credentials in libs/agno/tests/unit/tools/test_azure_openai_tools.py asserted that the result should contain the string \"not properly initialized\", but the actual result returned an Azure API 401 error message. The pytest output shows the failing assertion and the full 401 error text, indicating the code under test attempted an actual call (or returned an Azure-formatted error) instead of the expected local error handling.",
            "The failure occurred during the test run step (pytest) in the CI job; unrelated deprecation and runtime warnings were emitted during the run but did not cause the job to fail."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/tools/test_azure_openai_tools.py",
                "line_number": 192,
                "reason": "Pytest reports the AssertionError at this file and line: \"libs/agno/tests/unit/tools/test_azure_openai_tools.py:192: AssertionError\" and shows the failing assertion and actual value: \"E       assert 'not properly initialized' in 'Error 401: {\"error\":{\"code\":\"401\",\"message\":\"Access denied due to invalid subscription key or wrong API endpoint...\"}}'\"."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "AssertionError in unit test (unexpected external API 401 response)",
                "evidence": "Log: \"E       assert 'not properly initialized' in 'Error 401: {\"error\":{\"code\":\"401\",\"message\":\"Access denied due to invalid subscription key or wrong API endpoint...\"}}'\" and \"FAILED libs/agno/tests/unit/tools/test_azure_openai_tools.py::test_generate_image_missing_credentials\"."
            },
            {
                "category": "Warnings",
                "subcategory": "Deprecation / Runtime Warnings (non-fatal)",
                "evidence": "Multiple warning lines in the logs such as \"/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/youtube_transcript_api/_api.py:298: DeprecationWarning\" and \"/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\"; these are present in context but are warnings, not the failing error."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "python -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "7da71b58b1eb9b56ac1b4423afd9da61679a706e",
        "error_context": [
            "The style-check job failed because ruff reported a lint error: an unused import. Logs show \"agno/workflow/v2/types.py:2:20: F401 `typing.TYPE_CHECKING` imported but unused\" and a suggestion to remove the import. The failing step was the \"Ruff check\" step (command: `ruff check .`) which exited with code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/workflow/v2/types.py",
                "line_number": 2,
                "reason": "Log shows a ruff lint error for this file: \"agno/workflow/v2/types.py:2:20: F401 [*] `typing.TYPE_CHECKING` imported but unused\" and the surrounding context shows the import on line 2 and the F401 marker."
            }
        ],
        "error_types": [
            {
                "category": "Code Linting",
                "subcategory": "Unused import (F401)",
                "evidence": "Log line: \"agno/workflow/v2/types.py:2:20: F401 [*] `typing.TYPE_CHECKING` imported but unused\" and \"= help: Remove unused import: `typing.TYPE_CHECKING`\"; ruff then exited with code 1."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
        "error_context": [
            "The style-check job failed because ruff (the linter) reported unused-import errors (F401) in agno/app/discord/client.py. The logs show two F401 errors (lines 11 and 12) and the job exited with code 1.",
            "Ruff format ran earlier (reformatted 1 file) but the failing step is 'Ruff check' which produced the unused-import errors and caused the process to complete with exit code 1."
        ],
        "relevant_files": [
            {
                "file": "agno/app/discord/client.py",
                "line_number": 11,
                "reason": "Log shows ruff errors referencing this file: 'agno/app/discord/client.py:11:20: F401 `typing.List` imported but unused' and 'agno/app/discord/client.py:12:33: F401 `agno.tools.function.UserInputField` imported but unused', and the log culminates with 'Found 2 errors.'"
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused import (F401)",
                "evidence": "Log lines: 'agno/app/discord/client.py:11:20: F401 [*] `typing.List` imported but unused' and 'agno/app/discord/client.py:12:33: F401 [*] `agno.tools.function.UserInputField` imported but unused' plus 'Found 2 errors.' and 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "7f4d750eaf81c6f8384ed8246180c29bb45ea7bf",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: at agno/eval/accuracy.py line 307 an Optional[Agent] value may be None but the code accesses its .run attribute. The failure occurred during the Mypy step (command: mypy .) which returned a non-zero exit code, causing the step/job to fail."
        ],
        "relevant_files": [
            {
                "file": "agno/eval/accuracy.py",
                "line_number": 307,
                "reason": "Log shows a mypy error referencing this file and line: \"agno/eval/accuracy.py:307: error: Item \\\"None\\\" of \\\"Optional[Agent]\\\" has no attribute \\\"run\\\"  [union-attr]\" which ties the file to the type-check failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attr (Optional value may be None; attribute access on None)",
                "evidence": "\"agno/eval/accuracy.py:307: error: Item \\\"None\\\" of \\\"Optional[Agent]\\\" has no attribute \\\"run\\\"  [union-attr]\" and \"Found 1 error in 1 file (checked 467 source files)\" from the CI logs."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "82609349d7098fd5ad1c71f0e057f50cd0074404",
        "error_context": [
            "The style-check job failed because the linter (ruff) reported a lint error: an unused import. Logs show ruff check produced \"agno/workflow/workflow.py:8:25: F401` typing.AsyncGenerator` imported but unused\" and \"Found 1 error.\" causing the job to exit with code 1. The error is marked fixable (\"[*] 1 fixable with the `--fix` option\")."
        ],
        "relevant_files": [
            {
                "file": "agno/workflow/workflow.py",
                "line_number": 8,
                "reason": "Log shows a ruff lint failure: \"agno/workflow/workflow.py:8:25: F401 [*] `typing.AsyncGenerator` imported but unused\" and the following context lines show the import at line 8."
            }
        ],
        "error_types": [
            {
                "category": "Linting / Code Style",
                "subcategory": "Unused import (F401)",
                "evidence": "\"agno/workflow/workflow.py:8:25: F401 [*] `typing.AsyncGenerator` imported but unused\" and \"Found 1 error.\" from the ruff check output; also \"[*] 1 fixable with the `--fix` option.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "82bdfed0cf9ff57528d78ac9ad9f1179c8e117e3",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error: agno/vectordb/mongodb/cosmos_mongodb.py line 87 - 'None' has no attribute 'get_collection'.",
            "Mypy produced the error and the step exited non\u2011zero (Found 1 error in 1 file; Process completed with exit code 1), causing the style-check (3.9) job to fail."
        ],
        "relevant_files": [
            {
                "file": "agno/vectordb/mongodb/cosmos_mongodb.py",
                "line_number": 87,
                "reason": "Log shows a mypy error for this file and line: \"agno/vectordb/mongodb/cosmos_mongodb.py:87: error: \\\"None\\\" has no attribute \\\"get_collection\\\"  [attr-defined]\"; mypy then reports \"Found 1 error in 1 file\" and the job exits with code 1."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy attribute access on possibly None (attr-defined)",
                "evidence": "Log message: \"agno/vectordb/mongodb/cosmos_mongodb.py:87: error: \\\"None\\\" has no attribute \\\"get_collection\\\"  [attr-defined]\" and \"Found 1 error in 1 file (checked 440 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check (3.9)",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "840ad511b904f43678ec438464deb893fda58c8b",
        "error_context": [
            "The style-check job failed because the Ruff linter (ruff check) reported a lint error F841 (unused local variable) in a test file. Logs show: \"tests/unit/tools/models/test_morph.py:200:79: F841 Local variable `mock_file` is assigned to but never used\" and the job ended with \"Process completed with exit code 1.\"",
            "Ruff reported \"Found 1 error.\" and \"No fixes available (1 hidden fix can be enabled with the `--unsafe-fixes` option).\", so the ruff check step failed the CI even though ruff format ran earlier."
        ],
        "relevant_files": [
            {
                "file": "tests/unit/tools/models/test_morph.py",
                "line_number": 200,
                "reason": "Log shows a ruff lint failure: \"tests/unit/tools/models/test_morph.py:200:79: F841 Local variable `mock_file` is assigned to but never used\" and the following context lines point to the with-patch line assigning mock_file (code snippet included in logs)."
            }
        ],
        "error_types": [
            {
                "category": "Linting",
                "subcategory": "Unused local variable (ruff F841)",
                "evidence": "CI log: \"tests/unit/tools/models/test_morph.py:200:79: F841 Local variable `mock_file` is assigned to but never used\" and \"Found 1 error.\" followed by \"##[error]Process completed with exit code 1.\" indicating ruff check produced the failure."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "8474427eeab92e28766b8051d92e6b46379581e9",
        "error_context": [
            "The style-check job failed because ruff reported lint errors (unused imports) and exited with code 1. Ruff output shows two F401 errors in agno/models/anthropic/claude.py (unused `pathlib.Path` at line 6 and unused `agno.media.File` at line 16) and reports \"Found 2 errors.\" with \"[*] 2 fixable with the `--fix` option.\""
        ],
        "relevant_files": [
            {
                "file": "agno/models/anthropic/claude.py",
                "line_number": 6,
                "reason": "Ruff reported lint failures in this file: \"agno/models/anthropic/claude.py:6:21: F401 [*] `pathlib.Path` imported but unused\" and \"agno/models/anthropic/claude.py:16:24: F401 [*] `agno.media.File` imported but unused\" (shows both offending lines and F401 codes)."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused Import (ruff F401)",
                "evidence": "\"agno/models/anthropic/claude.py:6:21: F401 [*] `pathlib.Path` imported but unused\" and \"agno/models/anthropic/claude.py:16:24: F401 [*] `agno.media.File` imported but unused\" plus \"Found 2 errors.\" and \"[*] 2 fixable with the `--fix` option.\" from the ruff output."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "84d26c4b6b5881367cdabfc56d29b50389f1ba92",
        "error_context": [
            "The style-check job failed during the Mypy type-check step: mypy reported a type error in agno/team/team.py at line 6173, complaining that an item of type Union[Agent, Team] (specifically \"Team\") has no attribute \"agent_id\". The mypy command returned a non\u2011zero exit (Found 1 error in 1 file -> Process completed with exit code 1)."
        ],
        "relevant_files": [
            {
                "file": "agno/team/team.py",
                "line_number": 6173,
                "reason": "Log shows a mypy error pointing to this file and line: \"agno/team/team.py:6173: error: Item \\\"Team\\\" of \\\"Union[Agent, Team]\\\" has no attribute \\\"agent_id\\\"  [union-attr]\"; followed by \"Found 1 error in 1 file\" and process exit code 1."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy union-attribute error (accessing attribute on a Union member)",
                "evidence": "\"agno/team/team.py:6173: error: Item \\\"Team\\\" of \\\"Union[Agent, Team]\\\" has no attribute \\\"agent_id\\\"  [union-attr]\" and \"Found 1 error in 1 file (checked 520 source files)\" from the style-check job logs."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
        "error_context": [
            "The test matrix job 'tests' failed during the 'Run tests for Agno' step when pytest aborted collection with import errors. Pytest reported two collection-time failures: (1) a TypeError from the protobuf runtime (\"TypeError: Descriptors cannot be created directly\") triggered when importing generated _pb2 code used by opentelemetry/chromadb, and (2) an ImportError from the project's Milvus wrapper when attempting to import AsyncMilvusClient from the installed pymilvus (\"cannot import name 'AsyncMilvusClient' ... Did you mean: 'MilvusClient'?\" and an explicit raised ImportError saying pymilvus is not installed).",
            "Files shown in the traceback tie the failures to tests under libs/agno/tests/unit/vectordb (test_chromadb.py and test_milvusdb.py) which import project modules libs/agno/agno/vectordb/chroma/chromadb.py and libs/agno/agno/vectordb/milvus/milvus.py, and to dependency files in the virtualenv (google protobuf and pymilvus)."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/vectordb/test_chromadb.py",
                "line_number": 8,
                "reason": "Pytest traceback: \"libs/agno/tests/unit/vectordb/test_chromadb.py:8: in <module>     from agno.vectordb.chroma import ChromaDb\" \u2014 the test import triggered the chromadb import chain that produced the protobuf TypeError."
            },
            {
                "file": "libs/agno/tests/unit/vectordb/test_milvusdb.py",
                "line_number": 8,
                "reason": "Pytest traceback: \"ImportError while importing test module '/home/runner/work/agno/agno/libs/agno/tests/unit/vectordb/test_milvusdb.py'\" and \"libs/agno/tests/unit/vectordb/test_milvusdb.py:8: in <module>     from agno.vectordb.milvus import Milvus\" \u2014 shows this test failed during import."
            },
            {
                "file": "libs/agno/agno/vectordb/chroma/chromadb.py",
                "line_number": 6,
                "reason": "Traceback: \"libs/agno/agno/vectordb/chroma/chromadb.py:6: in <module>     from chromadb import Client as ChromaDbClient\" which leads into chromadb's opentelemetry imports that hit the protobuf error."
            },
            {
                "file": "libs/agno/agno/vectordb/milvus/milvus.py",
                "line_number": 8,
                "reason": "Traceback shows \"libs/agno/agno/vectordb/milvus/milvus.py:8: in <module>     from pymilvus import AsyncMilvusClient, MilvusClient\" and later the module raises an ImportError at line 10: \"raise ImportError('The `pymilvus` package is not installed...')\" \u2014 tying this file to the ImportError reported by pytest."
            },
            {
                "file": "/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/opentelemetry/proto/common/v1/common_pb2.py",
                "line_number": 36,
                "reason": "Traceback into vendor-generated proto: \".venv/lib/python3.12/site-packages/opentelemetry/proto/common/v1/common_pb2.py:36: in <module>     _descriptor.FieldDescriptor(...)\" \u2014 shows generated _pb2 code being executed when the protobuf TypeError occurred."
            },
            {
                "file": "/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/google/protobuf/descriptor.py",
                "line_number": 621,
                "reason": "Error originates in protobuf runtime: \".venv/lib/python3.12/site-packages/google/protobuf/descriptor.py:621: in __new__     _message.Message._CheckCalledFromGeneratedFile()\" followed by \"TypeError: Descriptors cannot be created directly.\""
            },
            {
                "file": "/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/pymilvus/__init__.py",
                "line_number": null,
                "reason": "Pytest/import error: \"ImportError: cannot import name 'AsyncMilvusClient' from 'pymilvus' (/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/pymilvus/__init__.py). Did you mean: 'MilvusClient'?\" \u2014 implicates the installed pymilvus package API mismatch."
            }
        ],
        "error_types": [
            {
                "category": "Test Failure",
                "subcategory": "ImportError during test collection",
                "evidence": "\"ImportError while importing test module '/home/runner/work/agno/agno/libs/agno/tests/unit/vectordb/test_milvusdb.py'\" and \"E   ImportError: The `pymilvus` package is not installed. Please install it via `pip install pymilvus`.\" (logs show pytest aborted collection and reported these import errors)."
            },
            {
                "category": "Dependency Error",
                "subcategory": "Missing or incompatible package API (pymilvus)",
                "evidence": "\"E   ImportError: cannot import name 'AsyncMilvusClient' from 'pymilvus' (/home/runner/work/agno/agno/.venv/lib/python3.12/site-packages/pymilvus/__init__.py). Did you mean: 'MilvusClient'?\" \u2014 indicates the installed pymilvus does not provide the expected AsyncMilvusClient symbol or a mismatched version."
            },
            {
                "category": "Runtime Error / Dependency Compatibility",
                "subcategory": "Protobuf runtime vs generated code mismatch (Descriptors cannot be created directly)",
                "evidence": "\"E   TypeError: Descriptors cannot be created directly.\" and \"If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\" (logs reference common_pb2.py and google/protobuf/descriptor.py:621)."
            }
        ],
        "failed_job": [
            {
                "job": "tests",
                "step": "Run tests for Agno",
                "command": "source .venv/bin/activate\npython -m pytest --cov=agno --cov-report=json:coverage-agno.json ./libs/agno/tests/unit"
            }
        ]
    },
    {
        "sha_fail": "96cab6eebb5bd0c37a60a93f3e3495fdb08793f9",
        "error_context": [
            "The style-check job failed because mypy reported a type-checking error. Mypy found an attempt to call .append on a value typed as Optional[list[DocumentCitation]] in agno/models/anthropic/claude.py at line 519. This caused the Mypy step to exit non\u2011zero and the style-check job to fail (Process completed with exit code 1)."
        ],
        "relevant_files": [
            {
                "file": "agno/models/anthropic/claude.py",
                "line_number": 519,
                "reason": "Log shows a mypy error referencing this file and line: \"agno/models/anthropic/claude.py:519: error: Item \"None\" of \"Optional[list[DocumentCitation]]\" has no attribute \"append\"\" \u2014 this ties the file and line directly to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy 'union-attr' (calling an attribute on Optional that may be None)",
                "evidence": "Log: \"agno/models/anthropic/claude.py:519: error: Item \\\"None\\\" of \\\"Optional[list[DocumentCitation]]\\\" has no attribute \\\"append\\\"  [union-attr]\" and \"Found 1 error in 1 file (checked 431 source files)\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "978b0f29982c558b12d775a884ebeea1869c16da",
        "error_context": [
            "The style-check job failed because ruff reported formatting/linting errors (F541: f-string without any placeholders) in the test file tests/unit/tools/models/test_gemini.py. The ruff check step produced two F541 errors (lines 125 and 211) and exited with code 1; the log also notes \"Found 2 errors.\" and that both are fixable with \"--fix\".",
            "Ruff format ran earlier and reformatted 1 file, but the subsequent \"ruff check .\" step still failed on the f-string issues, causing the style-check job to fail."
        ],
        "relevant_files": [
            {
                "file": "libs/agno/tests/unit/tools/models/test_gemini.py",
                "line_number": 125,
                "reason": "Log shows ruff errors referencing this file: \"tests/unit/tools/models/test_gemini.py:125:26: F541 [*] f-string without any placeholders\" and another occurrence at line 211; context shows the offending lines (e.g. \"125 |         assert result == f\\\"Image generated successfully\\\"\" and \"211 |         assert result == f\\\"Video generated successfully\\\"\") and the ruff help \"Remove extraneous `f` prefix\"."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Ruff linter error: F541 f-string without any placeholders",
                "evidence": "Log lines: \"tests/unit/tools/models/test_gemini.py:125:26: F541 [*] f-string without any placeholders\" and \"tests/unit/tools/models/test_gemini.py:211:26: F541 [*] f-string without any placeholders\"; also \"Found 2 errors.\" and \"[*] 2 fixable with the `--fix` option.\" and the step invocation \"ruff check .\"."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "98c0877ed94c574c7a6eae2f2eb662bfcef1ba87",
        "error_context": [
            "The style-check job failed because the Mypy type checker reported two type errors in agno/agent/agent.py, causing the 'Mypy' step to exit with code 1. Logs show Mypy running (\"mypy .\") and the messages: \"agno/agent/agent.py:1105: error: Item \\\"None\\\" of \\\"Optional[AgentKnowledge]\\\" has no attribute \\\"valid_metadata_filters\\\"\" and a second similar error at line 1739. This produced \"Found 2 errors in 1 file ...\" and the job terminated."
        ],
        "relevant_files": [
            {
                "file": "agno/agent/agent.py",
                "line_number": 1105,
                "reason": "Mypy error lines in the CI log: \"agno/agent/agent.py:1105: error: Item \\\"None\\\" of \\\"Optional[AgentKnowledge]\\\" has no attribute \\\"valid_metadata_filters\\\"\" and a second error at \"agno/agent/agent.py:1739\", followed by \"Found 2 errors in 1 file\" \u2014 directly tying this file (lines 1105 and 1739) to the failure."
            }
        ],
        "error_types": [
            {
                "category": "Type Checking",
                "subcategory": "Mypy: attribute access on Optional (union-attr)",
                "evidence": "Log lines from the Mypy step: \"agno/agent/agent.py:1105: error: Item \\\"None\\\" of \\\"Optional[AgentKnowledge]\\\" has no attribute \\\"valid_metadata_filters\\\" [union-attr]\" and \"Found 2 errors in 1 file (checked 431 source files)\"; command shown as \"mypy .\" in the step context."
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Mypy",
                "command": "mypy ."
            }
        ]
    },
    {
        "sha_fail": "9b52669238946cd01f9cc5e99a4b6babf0942cdf",
        "error_context": [
            "The style-check job failed because ruff reported unused-import lint errors (F401) in multiple test files, causing 'ruff check .' to exit with code 1. The logs show specific F401 messages (e.g. tests/integration/models/cerebras/test_tool_use.py:1:20 and tests/integration/models/cerebras_openai/test_basic.py:6:33) and 'Found 4 errors.' / '[*] 4 fixable with the `--fix` option.' followed by 'Process completed with exit code 1.'"
        ],
        "relevant_files": [
            {
                "file": "tests/integration/models/cerebras/test_tool_use.py",
                "line_number": 1,
                "reason": "Log shows 'tests/integration/models/cerebras/test_tool_use.py:1:20: F401 `typing.Optional` imported but unused' and later 'tests/integration/models/cerebras/test_tool_use.py:7:35: F401 `agno.tools.duckduckgo.DuckDuckGoTools` imported but unused', tying this file to the ruff F401 failures."
            },
            {
                "file": "tests/integration/models/cerebras_openai/test_basic.py",
                "line_number": 6,
                "reason": "Log shows 'tests/integration/models/cerebras_openai/test_basic.py:6:33: F401 `agno.storage.sqlite.SqliteStorage` imported but unused', indicating this file triggered a ruff unused-import error."
            },
            {
                "file": "tests/integration/models/cerebras_openai/test_tool_use.py",
                "line_number": 5,
                "reason": "Log shows 'tests/integration/models/cerebras_openai/test_tool_use.py:5:35: F401 `agno.tools.duckduckgo.DuckDuckGoTools` imported but unused', indicating this file is involved in the lint failure."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting",
                "subcategory": "Linting \u2014 Unused Import (F401)",
                "evidence": "Multiple log lines from ruff: e.g. 'tests/integration/models/cerebras/test_tool_use.py:1:20: F401 `typing.Optional` imported but unused' and 'Found 4 errors.' plus '[*] 4 fixable with the `--fix` option.' followed by 'Process completed with exit code 1.'"
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    },
    {
        "sha_fail": "9ebc254bb14ed052a0ac459d8c109ae8e0c12233",
        "error_context": [
            "The style-check job failed because ruff (the linter) reported unused-import errors (F401) in two files. The logs show ruff output lines like \"agno/app/playground/async_router.py:42:27: F401 ... `agno.run.team.TeamRunResponseEvent` imported but unused\" and a final \"Found 2 errors.\" followed by \"Process completed with exit code 1.\", so the failure is caused by linter-detected unused imports that made the `ruff check .` step exit non\u2011zero."
        ],
        "relevant_files": [
            {
                "file": "agno/app/playground/async_router.py",
                "line_number": 42,
                "reason": "Log shows: \"agno/app/playground/async_router.py:42:27: F401 [*] `agno.run.team.TeamRunResponseEvent` imported but unused\" and the context lines show the import at line 42 and the ruff F401 marker pointing to it."
            },
            {
                "file": "agno/app/playground/sync_router.py",
                "line_number": 42,
                "reason": "Log shows: \"agno/app/playground/sync_router.py:42:27: F401 [*] `agno.run.team.TeamRunResponseEvent` imported but unused\" with context lines showing the import at line 42 and the F401 lint marker."
            }
        ],
        "error_types": [
            {
                "category": "Code Formatting / Linting",
                "subcategory": "Unused import (ruff F401)",
                "evidence": "Logs contain ruff output: \"agno/app/playground/async_router.py:42:27: F401 ... imported but unused\" and \"agno/app/playground/sync_router.py:42:27: F401 ... imported but unused\", plus \"Found 2 errors.\" and final \"Process completed with exit code 1.\""
            }
        ],
        "failed_job": [
            {
                "job": "style-check",
                "step": "Ruff check",
                "command": "ruff check ."
            }
        ]
    }
]