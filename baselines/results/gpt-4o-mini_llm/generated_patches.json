[
    {
        "id": "320",
        "sha_fail": "6066f06de3275801b19af5f23ccb5e3940991e60",
        "diff": "diff --git a/Lib/pathlib/types.py b/Lib/pathlib/types.py\nindex 559677bc..acd879be 100644\n--- a/Lib/pathlib/types.py\n+++ b/Lib/pathlib/types.py\n@@ -9,12 +9,16 @@\n #\n # Three ABCs are provided -- _JoinablePath, _ReadablePath and _WritablePath\n \n-\n from abc import ABC, abstractmethod\n from glob import _GlobberBase\n from io import text_encoding\n-from pathlib._os import (vfsopen, vfspath, ensure_distinct_paths,\n-                         ensure_different_files, copyfileobj)\n+from pathlib._os import (\n+    vfsopen,\n+    vfspath,\n+    ensure_distinct_paths,\n+    ensure_different_files,\n+    copyfileobj,\n+)\n from pathlib import PurePath, Path\n from typing import Optional, Protocol, runtime_checkable\n \n@@ -45,6 +49,7 @@ class _PathParser(Protocol):\n \n     sep: str\n     altsep: Optional[str]\n+\n     def split(self, path: str) -> tuple[str, str]: ...\n     def splitext(self, path: str) -> tuple[str, str]: ...\n     def normcase(self, path: str) -> str: ...\n@@ -55,6 +60,7 @@ class PathInfo(Protocol):\n     \"\"\"Protocol for path info objects, which support querying the file type.\n     Methods may return cached results.\n     \"\"\"\n+\n     def exists(self, *, follow_symlinks: bool = True) -> bool: ...\n     def is_dir(self, *, follow_symlinks: bool = True) -> bool: ...\n     def is_file(self, *, follow_symlinks: bool = True) -> bool: ...\n@@ -62,8 +68,7 @@ def is_symlink(self) -> bool: ...\n \n \n class _PathGlobber(_GlobberBase):\n-    \"\"\"Provides shell-style pattern matching and globbing for ReadablePath.\n-    \"\"\"\n+    \"\"\"Provides shell-style pattern matching and globbing for ReadablePath.\"\"\"\n \n     @staticmethod\n     def lexists(path):\n@@ -87,6 +92,7 @@ class _JoinablePath(ABC):\n     its implementation PurePath. They are: __init__, __fspath__, __bytes__,\n     __reduce__, __hash__, __eq__, __lt__, __le__, __gt__, __ge__.\n     \"\"\"\n+\n     __slots__ = ()\n \n     @property\n@@ -178,17 +184,17 @@ def with_suffix(self, suffix):\n         if not stem:\n             # If the stem is empty, we can't make the suffix non-empty.\n             raise ValueError(f\"{self!r} has an empty name\")\n-        elif suffix and not suffix.startswith('.'):\n+        elif suffix and not suffix.startswith(\".\"):\n             raise ValueError(f\"Invalid suffix {suffix!r}\")\n         else:\n             return self.with_name(stem + suffix)\n \n     def without_suffix(self):\n-        \"\"\"Return a new path without the file suffix.  Readable alternative \n+        \"\"\"Return a new path without the file suffix.  Readable alternative\n         for providing empty string to with_suffix.\n         \"\"\"\n-        return self.with_suffix('')\n-    \n+        return self.with_suffix(\"\")\n+\n     @property\n     def parts(self):\n         \"\"\"An object providing sequence-like access to the\n@@ -245,7 +251,7 @@ def full_match(self, pattern):\n         Return True if this path matches the given glob-style pattern. The\n         pattern is matched against the entire path.\n         \"\"\"\n-        case_sensitive = self.parser.normcase('Aa') == 'Aa'\n+        case_sensitive = self.parser.normcase(\"Aa\") == \"Aa\"\n         globber = _PathGlobber(self.parser.sep, case_sensitive, recursive=True)\n         match = globber.compile(pattern, altsep=self.parser.altsep)\n         return match(vfspath(self)) is not None\n@@ -258,6 +264,7 @@ class _ReadablePath(_JoinablePath):\n     create subclasses to implement readable virtual filesystem paths, such as\n     paths in archive files or on remote storage systems.\n     \"\"\"\n+\n     __slots__ = ()\n \n     @property\n@@ -281,7 +288,7 @@ def read_bytes(self):\n         \"\"\"\n         Open the file in bytes mode, read it, and close the file.\n         \"\"\"\n-        with vfsopen(self, mode='rb') as f:\n+        with vfsopen(self, mode=\"rb\") as f:\n             return f.read()\n \n     def read_text(self, encoding=None, errors=None, newline=None):\n@@ -291,7 +298,9 @@ def read_text(self, encoding=None, errors=None, newline=None):\n         # Call io.text_encoding() here to ensure any warning is raised at an\n         # appropriate stack level.\n         encoding = text_encoding(encoding)\n-        with vfsopen(self, mode='r', encoding=encoding, errors=errors, newline=newline) as f:\n+        with vfsopen(\n+            self, mode=\"r\", encoding=encoding, errors=errors, newline=newline\n+        ) as f:\n             return f.read()\n \n     @abstractmethod\n@@ -314,10 +323,10 @@ def glob(self, pattern, *, recurse_symlinks=True):\n             raise ValueError(f\"Unacceptable pattern: {pattern!r}\")\n         elif not recurse_symlinks:\n             raise NotImplementedError(\"recurse_symlinks=False is unsupported\")\n-        case_sensitive = self.parser.normcase('Aa') == 'Aa'\n+        case_sensitive = self.parser.normcase(\"Aa\") == \"Aa\"\n         globber = _PathGlobber(self.parser.sep, case_sensitive, recursive=True)\n         select = globber.selector(parts)\n-        return select(self.joinpath(''))\n+        return select(self.joinpath(\"\"))\n \n     def walk(self, top_down=True, on_error=None, follow_symlinks=False):\n         \"\"\"Walk the directory tree from this directory, similar to os.walk().\"\"\"\n@@ -382,6 +391,7 @@ class _WritablePath(_JoinablePath):\n     create subclasses to implement writable virtual filesystem paths, such as\n     paths in archive files or on remote storage systems.\n     \"\"\"\n+\n     __slots__ = ()\n \n     @abstractmethod\n@@ -413,7 +423,7 @@ def write_bytes(self, data):\n         \"\"\"\n         # type-check for the buffer interface before truncating the file\n         view = memoryview(data)\n-        with vfsopen(self, mode='wb') as f:\n+        with vfsopen(self, mode=\"wb\") as f:\n             return f.write(view)\n \n     def write_text(self, data, encoding=None, errors=None, newline=None):\n@@ -424,9 +434,12 @@ def write_text(self, data, encoding=None, errors=None, newline=None):\n         # appropriate stack level.\n         encoding = text_encoding(encoding)\n         if not isinstance(data, str):\n-            raise TypeError('data must be str, not %s' %\n-                            data.__class__.__name__)\n-        with vfsopen(self, mode='w', encoding=encoding, errors=errors, newline=newline) as f:\n+            raise TypeError(\n+                \"data must be str, not %s\" % data.__class__.__name__\n+            )\n+        with vfsopen(\n+            self, mode=\"w\", encoding=encoding, errors=errors, newline=newline\n+        ) as f:\n             return f.write(data)\n \n     def _copy_from(self, source, follow_symlinks=True):\n@@ -445,8 +458,8 @@ def _copy_from(self, source, follow_symlinks=True):\n                     stack.append((child, dst.joinpath(child.name)))\n             else:\n                 ensure_different_files(src, dst)\n-                with vfsopen(src, 'rb') as source_f:\n-                    with vfsopen(dst, 'wb') as target_f:\n+                with vfsopen(src, \"rb\") as source_f:\n+                    with vfsopen(dst, \"wb\") as target_f:\n                         copyfileobj(source_f, target_f)\n \n \n"
    },
    {
        "id": "322",
        "sha_fail": "d01a6ec49f1aac932816c81c0354de64c0183373",
        "diff": "diff --git a/lib/core/ncgui.py b/lib/core/ncgui.py\nindex 708f021..3683ac1 100644\n--- a/lib/core/ncgui.py\n+++ b/lib/core/ncgui.py\n@@ -19,9 +19,9 @@ from lib.core.enums import MKSTEMP_PREFIX\n from lib.core.exception import SqlmapMissingDependence\n from lib.core.exception import SqlmapSystemException\n from lib.core.settings import IS_WIN\n-from thirdparty.six.moves import queue as _queue\n from thirdparty.six.moves import configparser as _configparser\n \n+\n class NcursesUI:\n     def __init__(self, stdscr, parser):\n         self.stdscr = stdscr\n@@ -37,13 +37,13 @@ class NcursesUI:\n \n         # Initialize colors\n         curses.start_color()\n-        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_CYAN)    # Header\n-        curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_BLUE)    # Active tab\n-        curses.init_pair(3, curses.COLOR_BLACK, curses.COLOR_WHITE)   # Inactive tab\n+        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_CYAN)  # Header\n+        curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_BLUE)  # Active tab\n+        curses.init_pair(3, curses.COLOR_BLACK, curses.COLOR_WHITE)  # Inactive tab\n         curses.init_pair(4, curses.COLOR_YELLOW, curses.COLOR_BLACK)  # Selected field\n-        curses.init_pair(5, curses.COLOR_GREEN, curses.COLOR_BLACK)   # Help text\n-        curses.init_pair(6, curses.COLOR_RED, curses.COLOR_BLACK)     # Error/Important\n-        curses.init_pair(7, curses.COLOR_CYAN, curses.COLOR_BLACK)    # Label\n+        curses.init_pair(5, curses.COLOR_GREEN, curses.COLOR_BLACK)  # Help text\n+        curses.init_pair(6, curses.COLOR_RED, curses.COLOR_BLACK)  # Error/Important\n+        curses.init_pair(7, curses.COLOR_CYAN, curses.COLOR_BLACK)  # Label\n \n         # Setup curses\n         curses.curs_set(1)\n@@ -56,21 +56,27 @@ class NcursesUI:\n         \"\"\"Parse command line options into tabs and fields\"\"\"\n         for group in self.parser.option_groups:\n             tab_data = {\n-                'title': group.title,\n-                'description': group.get_description() if hasattr(group, 'get_description') and group.get_description() else \"\",\n-                'options': []\n+                \"title\": group.title,\n+                \"description\": group.get_description()\n+                if hasattr(group, \"get_description\") and group.get_description()\n+                else \"\",\n+                \"options\": [],\n             }\n \n             for option in group.option_list:\n                 field_data = {\n-                    'dest': option.dest,\n-                    'label': self._format_option_strings(option),\n-                    'help': option.help if option.help else \"\",\n-                    'type': option.type if hasattr(option, 'type') and option.type else 'bool',\n-                    'value': '',\n-                    'default': defaults.get(option.dest) if defaults.get(option.dest) else None\n+                    \"dest\": option.dest,\n+                    \"label\": self._format_option_strings(option),\n+                    \"help\": option.help if option.help else \"\",\n+                    \"type\": option.type\n+                    if hasattr(option, \"type\") and option.type\n+                    else \"bool\",\n+                    \"value\": \"\",\n+                    \"default\": defaults.get(option.dest)\n+                    if defaults.get(option.dest)\n+                    else None,\n                 }\n-                tab_data['options'].append(field_data)\n+                tab_data[\"options\"].append(field_data)\n                 self.fields[(group.title, option.dest)] = field_data\n \n             self.tabs.append(tab_data)\n@@ -78,11 +84,11 @@ class NcursesUI:\n     def _format_option_strings(self, option):\n         \"\"\"Format option strings for display\"\"\"\n         parts = []\n-        if hasattr(option, '_short_opts') and option._short_opts:\n+        if hasattr(option, \"_short_opts\") and option._short_opts:\n             parts.extend(option._short_opts)\n-        if hasattr(option, '_long_opts') and option._long_opts:\n+        if hasattr(option, \"_long_opts\") and option._long_opts:\n             parts.extend(option._long_opts)\n-        return ', '.join(parts)\n+        return \", \".join(parts)\n \n     def _draw_header(self):\n         \"\"\"Draw the header bar\"\"\"\n@@ -99,7 +105,7 @@ class NcursesUI:\n         x = 0\n \n         for i, tab in enumerate(self.tabs):\n-            tab_text = \" %s \" % tab['title']\n+            tab_text = \" %s \" % tab[\"title\"]\n \n             # Check if tab exceeds width, wrap to next line\n             if x + len(tab_text) >= width:\n@@ -120,7 +126,7 @@ class NcursesUI:\n         x = 0\n \n         for i, tab in enumerate(self.tabs):\n-            tab_text = \" %s \" % tab['title']\n+            tab_text = \" %s \" % tab[\"title\"]\n \n             # Check if tab exceeds width, wrap to next line\n             if x + len(tab_text) >= width:\n@@ -178,8 +184,8 @@ class NcursesUI:\n         y = start_y\n \n         # Draw description if exists\n-        if tab['description']:\n-            desc_lines = self._wrap_text(tab['description'], width - 4)\n+        if tab[\"description\"]:\n+            desc_lines = self._wrap_text(tab[\"description\"], width - 4)\n             for line in desc_lines[:2]:  # Limit to 2 lines\n                 try:\n                     self.stdscr.attron(curses.color_pair(5))\n@@ -194,14 +200,16 @@ class NcursesUI:\n         visible_start = self.scroll_offset\n         visible_end = visible_start + (height - y - 2)\n \n-        for i, option in enumerate(tab['options'][visible_start:visible_end], visible_start):\n+        for i, option in enumerate(\n+            tab[\"options\"][visible_start:visible_end], visible_start\n+        ):\n             if y >= height - 2:\n                 break\n \n-            is_selected = (i == self.current_field)\n+            is_selected = i == self.current_field\n \n             # Draw label\n-            label = option['label'][:25].ljust(25)\n+            label = option[\"label\"][:25].ljust(25)\n             try:\n                 if is_selected:\n                     self.stdscr.attron(curses.color_pair(4) | curses.A_BOLD)\n@@ -219,12 +227,12 @@ class NcursesUI:\n \n             # Draw value\n             value_str = \"\"\n-            if option['type'] == 'bool':\n-                value_str = \"[X]\" if option['value'] else \"[ ]\"\n+            if option[\"type\"] == \"bool\":\n+                value_str = \"[X]\" if option[\"value\"] else \"[ ]\"\n             else:\n-                value_str = str(option['value']) if option['value'] else \"\"\n-                if option['default'] and not option['value']:\n-                    value_str = \"(%s)\" % str(option['default'])\n+                value_str = str(option[\"value\"]) if option[\"value\"] else \"\"\n+                if option[\"default\"] and not option[\"value\"]:\n+                    value_str = \"(%s)\" % str(option[\"default\"])\n \n             value_str = value_str[:30]\n \n@@ -239,7 +247,7 @@ class NcursesUI:\n \n             # Draw help text\n             if width > 65:\n-                help_text = option['help'][:width-62] if option['help'] else \"\"\n+                help_text = option[\"help\"][: width - 62] if option[\"help\"] else \"\"\n                 try:\n                     self.stdscr.attron(curses.color_pair(5))\n                     self.stdscr.addstr(y, 60, help_text)\n@@ -250,7 +258,7 @@ class NcursesUI:\n             y += 1\n \n         # Draw scroll indicator\n-        if len(tab['options']) > visible_end - visible_start:\n+        if len(tab[\"options\"]) > visible_end - visible_start:\n             try:\n                 self.stdscr.attron(curses.color_pair(6))\n                 self.stdscr.addstr(height - 2, width - 10, \"[More...]\")\n@@ -280,14 +288,14 @@ class NcursesUI:\n     def _edit_field(self):\n         \"\"\"Edit the current field\"\"\"\n         tab = self.tabs[self.current_tab]\n-        if self.current_field >= len(tab['options']):\n+        if self.current_field >= len(tab[\"options\"]):\n             return\n \n-        option = tab['options'][self.current_field]\n+        option = tab[\"options\"][self.current_field]\n \n-        if option['type'] == 'bool':\n+        if option[\"type\"] == \"bool\":\n             # Toggle boolean\n-            option['value'] = not option['value']\n+            option[\"value\"] = not option[\"value\"]\n         else:\n             # Text input\n             height, width = self.stdscr.getmaxyx()\n@@ -296,7 +304,7 @@ class NcursesUI:\n             input_win = curses.newwin(5, width - 20, height // 2 - 2, 10)\n             input_win.box()\n             input_win.attron(curses.color_pair(2))\n-            input_win.addstr(0, 2, \" Edit %s \" % option['label'][:20])\n+            input_win.addstr(0, 2, \" Edit %s \" % option[\"label\"][:20])\n             input_win.attroff(curses.color_pair(2))\n             input_win.addstr(2, 2, \"Value:\")\n             input_win.refresh()\n@@ -306,26 +314,26 @@ class NcursesUI:\n             curses.curs_set(1)\n \n             # Pre-fill with existing value\n-            current_value = str(option['value']) if option['value'] else \"\"\n+            current_value = str(option[\"value\"]) if option[\"value\"] else \"\"\n             input_win.addstr(2, 9, current_value)\n             input_win.move(2, 9)\n \n             try:\n-                new_value = input_win.getstr(2, 9, width - 32).decode('utf-8')\n+                new_value = input_win.getstr(2, 9, width - 32).decode(\"utf-8\")\n \n                 # Validate and convert based on type\n-                if option['type'] == 'int':\n+                if option[\"type\"] == \"int\":\n                     try:\n-                        option['value'] = int(new_value) if new_value else None\n+                        option[\"value\"] = int(new_value) if new_value else None\n                     except ValueError:\n-                        option['value'] = None\n-                elif option['type'] == 'float':\n+                        option[\"value\"] = None\n+                elif option[\"type\"] == \"float\":\n                     try:\n-                        option['value'] = float(new_value) if new_value else None\n+                        option[\"value\"] = float(new_value) if new_value else None\n                     except ValueError:\n-                        option['value'] = None\n+                        option[\"value\"] = None\n                 else:\n-                    option['value'] = new_value if new_value else None\n+                    option[\"value\"] = new_value if new_value else None\n             except:\n                 pass\n \n@@ -355,21 +363,25 @@ class NcursesUI:\n         curses.curs_set(1)\n \n         try:\n-            filename = input_win.getstr(2, 8, width - 32).decode('utf-8').strip()\n+            filename = input_win.getstr(2, 8, width - 32).decode(\"utf-8\").strip()\n \n             if filename:\n                 # Collect all field values\n                 config = {}\n                 for tab in self.tabs:\n-                    for option in tab['options']:\n-                        dest = option['dest']\n-                        value = option['value'] if option['value'] else option.get('default')\n-\n-                        if option['type'] == 'bool':\n+                    for option in tab[\"options\"]:\n+                        dest = option[\"dest\"]\n+                        value = (\n+                            option[\"value\"]\n+                            if option[\"value\"]\n+                            else option.get(\"default\")\n+                        )\n+\n+                        if option[\"type\"] == \"bool\":\n                             config[dest] = bool(value)\n-                        elif option['type'] == 'int':\n+                        elif option[\"type\"] == \"int\":\n                             config[dest] = int(value) if value else None\n-                        elif option['type'] == 'float':\n+                        elif option[\"type\"] == \"float\":\n                             config[dest] = float(value) if value else None\n                         else:\n                             config[dest] = value\n@@ -390,7 +402,7 @@ class NcursesUI:\n                     input_win.addstr(0, 2, \" Export Successful \")\n                     input_win.attroff(curses.color_pair(5))\n                     input_win.addstr(2, 2, \"Configuration exported to:\")\n-                    input_win.addstr(3, 2, filename[:width - 26])\n+                    input_win.addstr(3, 2, filename[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n                 except Exception as ex:\n@@ -400,7 +412,7 @@ class NcursesUI:\n                     input_win.attron(curses.color_pair(6))\n                     input_win.addstr(0, 2, \" Export Failed \")\n                     input_win.attroff(curses.color_pair(6))\n-                    input_win.addstr(2, 2, str(getSafeExString(ex))[:width - 26])\n+                    input_win.addstr(2, 2, str(getSafeExString(ex))[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n         except:\n@@ -432,7 +444,7 @@ class NcursesUI:\n         curses.curs_set(1)\n \n         try:\n-            filename = input_win.getstr(2, 8, width - 32).decode('utf-8').strip()\n+            filename = input_win.getstr(2, 8, width - 32).decode(\"utf-8\").strip()\n \n             if filename and os.path.isfile(filename):\n                 try:\n@@ -444,8 +456,8 @@ class NcursesUI:\n \n                     # Load values into fields\n                     for tab in self.tabs:\n-                        for option in tab['options']:\n-                            dest = option['dest']\n+                        for option in tab[\"options\"]:\n+                            dest = option[\"dest\"]\n \n                             # Search for option in all sections\n                             for section in config.sections():\n@@ -453,20 +465,29 @@ class NcursesUI:\n                                     value = config.get(section, dest)\n \n                                     # Convert based on type\n-                                    if option['type'] == 'bool':\n-                                        option['value'] = value.lower() in ('true', '1', 'yes', 'on')\n-                                    elif option['type'] == 'int':\n+                                    if option[\"type\"] == \"bool\":\n+                                        option[\"value\"] = value.lower() in (\n+                                            \"true\",\n+                                            \"1\",\n+                                            \"yes\",\n+                                            \"on\",\n+                                        )\n+                                    elif option[\"type\"] == \"int\":\n                                         try:\n-                                            option['value'] = int(value) if value else None\n+                                            option[\"value\"] = (\n+                                                int(value) if value else None\n+                                            )\n                                         except ValueError:\n-                                            option['value'] = None\n-                                    elif option['type'] == 'float':\n+                                            option[\"value\"] = None\n+                                    elif option[\"type\"] == \"float\":\n                                         try:\n-                                            option['value'] = float(value) if value else None\n+                                            option[\"value\"] = (\n+                                                float(value) if value else None\n+                                            )\n                                         except ValueError:\n-                                            option['value'] = None\n+                                            option[\"value\"] = None\n                                     else:\n-                                        option['value'] = value if value else None\n+                                        option[\"value\"] = value if value else None\n \n                                     imported_count += 1\n                                     break\n@@ -478,7 +499,7 @@ class NcursesUI:\n                     input_win.addstr(0, 2, \" Import Successful \")\n                     input_win.attroff(curses.color_pair(5))\n                     input_win.addstr(2, 2, \"Imported %d options from:\" % imported_count)\n-                    input_win.addstr(3, 2, filename[:width - 26])\n+                    input_win.addstr(3, 2, filename[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n \n@@ -489,7 +510,7 @@ class NcursesUI:\n                     input_win.attron(curses.color_pair(6))\n                     input_win.addstr(0, 2, \" Import Failed \")\n                     input_win.attroff(curses.color_pair(6))\n-                    input_win.addstr(2, 2, str(getSafeExString(ex))[:width - 26])\n+                    input_win.addstr(2, 2, str(getSafeExString(ex))[: width - 26])\n                     input_win.refresh()\n                     curses.napms(2000)\n             elif filename:\n@@ -500,7 +521,7 @@ class NcursesUI:\n                 input_win.addstr(0, 2, \" File Not Found \")\n                 input_win.attroff(curses.color_pair(6))\n                 input_win.addstr(2, 2, \"File does not exist:\")\n-                input_win.addstr(3, 2, filename[:width - 26])\n+                input_win.addstr(3, 2, filename[: width - 26])\n                 input_win.refresh()\n                 curses.napms(2000)\n         except:\n@@ -520,15 +541,15 @@ class NcursesUI:\n \n         # Collect all field values\n         for tab in self.tabs:\n-            for option in tab['options']:\n-                dest = option['dest']\n-                value = option['value'] if option['value'] else option.get('default')\n+            for option in tab[\"options\"]:\n+                dest = option[\"dest\"]\n+                value = option[\"value\"] if option[\"value\"] else option.get(\"default\")\n \n-                if option['type'] == 'bool':\n+                if option[\"type\"] == \"bool\":\n                     config[dest] = bool(value)\n-                elif option['type'] == 'int':\n+                elif option[\"type\"] == \"int\":\n                     config[dest] = int(value) if value else None\n-                elif option['type'] == 'float':\n+                elif option[\"type\"] == \"float\":\n                     config[dest] = float(value) if value else None\n                 else:\n                     config[dest] = value\n@@ -567,17 +588,23 @@ class NcursesUI:\n         # Start sqlmap process\n         try:\n             process = subprocess.Popen(\n-                [sys.executable or \"python\", os.path.join(paths.SQLMAP_ROOT_PATH, \"sqlmap.py\"), \"-c\", configFile],\n+                [\n+                    sys.executable or \"python\",\n+                    os.path.join(paths.SQLMAP_ROOT_PATH, \"sqlmap.py\"),\n+                    \"-c\",\n+                    configFile,\n+                ],\n                 shell=False,\n                 stdout=subprocess.PIPE,\n                 stderr=subprocess.STDOUT,\n                 stdin=subprocess.PIPE,\n                 bufsize=1,\n-                close_fds=not IS_WIN\n+                close_fds=not IS_WIN,\n             )\n \n             # Make it non-blocking\n             import fcntl\n+\n             flags = fcntl.fcntl(process.stdout, fcntl.F_GETFL)\n             fcntl.fcntl(process.stdout, fcntl.F_SETFL, flags | os.O_NONBLOCK)\n \n@@ -591,7 +618,7 @@ class NcursesUI:\n                 # Check for user input\n                 try:\n                     key = console_win.getch()\n-                    if key in (ord('q'), ord('Q')):\n+                    if key in (ord(\"q\"), ord(\"Q\")):\n                         # Kill process\n                         process.terminate()\n                         break\n@@ -599,7 +626,7 @@ class NcursesUI:\n                         # Send newline to process\n                         if process.poll() is None:\n                             try:\n-                                process.stdin.write(b'\\n')\n+                                process.stdin.write(b\"\\n\")\n                                 process.stdin.flush()\n                             except:\n                                 pass\n@@ -610,11 +637,11 @@ class NcursesUI:\n                 try:\n                     chunk = process.stdout.read(1024)\n                     if chunk:\n-                        current_line += chunk.decode('utf-8', errors='ignore')\n+                        current_line += chunk.decode(\"utf-8\", errors=\"ignore\")\n \n                         # Split into lines\n-                        while '\\n' in current_line:\n-                            line, current_line = current_line.split('\\n', 1)\n+                        while \"\\n\" in current_line:\n+                            line, current_line = current_line.split(\"\\n\", 1)\n                             lines.append(line)\n \n                             # Keep only last N lines\n@@ -626,7 +653,7 @@ class NcursesUI:\n                             start_line = max(0, len(lines) - (height - 10))\n                             for i, l in enumerate(lines[start_line:]):\n                                 try:\n-                                    output_win.addstr(i, 0, l[:width-10])\n+                                    output_win.addstr(i, 0, l[: width - 10])\n                                 except:\n                                     pass\n                             output_win.refresh()\n@@ -640,8 +667,8 @@ class NcursesUI:\n                     try:\n                         remaining = process.stdout.read()\n                         if remaining:\n-                            current_line += remaining.decode('utf-8', errors='ignore')\n-                            for line in current_line.split('\\n'):\n+                            current_line += remaining.decode(\"utf-8\", errors=\"ignore\")\n+                            for line in current_line.split(\"\\n\"):\n                                 if line:\n                                     lines.append(line)\n                     except:\n@@ -652,11 +679,13 @@ class NcursesUI:\n                     start_line = max(0, len(lines) - (height - 10))\n                     for i, l in enumerate(lines[start_line:]):\n                         try:\n-                            output_win.addstr(i, 0, l[:width-10])\n+                            output_win.addstr(i, 0, l[: width - 10])\n                         except:\n                             pass\n \n-                    output_win.addstr(height - 9, 0, \"--- Process finished. Press Q to close ---\")\n+                    output_win.addstr(\n+                        height - 9, 0, \"--- Process finished. Press Q to close ---\"\n+                    )\n                     output_win.refresh()\n                     console_win.refresh()\n \n@@ -664,7 +693,7 @@ class NcursesUI:\n                     console_win.nodelay(False)\n                     while True:\n                         key = console_win.getch()\n-                        if key in (ord('q'), ord('Q')):\n+                        if key in (ord(\"q\"), ord(\"Q\")):\n                             break\n \n                     break\n@@ -711,7 +740,7 @@ class NcursesUI:\n             # Handle input\n             if key == curses.KEY_F10 or key == 27:  # F10 or ESC\n                 break\n-            elif key == ord('\\t') or key == curses.KEY_RIGHT:  # Tab or Right arrow\n+            elif key == ord(\"\\t\") or key == curses.KEY_RIGHT:  # Tab or Right arrow\n                 self.current_tab = (self.current_tab + 1) % len(self.tabs)\n                 self.current_field = 0\n                 self.scroll_offset = 0\n@@ -726,7 +755,7 @@ class NcursesUI:\n                     if self.current_field < self.scroll_offset:\n                         self.scroll_offset = self.current_field\n             elif key == curses.KEY_DOWN:  # Down arrow\n-                if self.current_field < len(tab['options']) - 1:\n+                if self.current_field < len(tab[\"options\"]) - 1:\n                     self.current_field += 1\n                     # Adjust scroll if needed\n                     height, width = self.stdscr.getmaxyx()\n@@ -741,10 +770,11 @@ class NcursesUI:\n                 self._export_config()\n             elif key == curses.KEY_F4:  # F4 to import\n                 self._import_config()\n-            elif key == ord(' '):  # Space for boolean toggle\n-                option = tab['options'][self.current_field]\n-                if option['type'] == 'bool':\n-                    option['value'] = not option['value']\n+            elif key == ord(\" \"):  # Space for boolean toggle\n+                option = tab[\"options\"][self.current_field]\n+                if option[\"type\"] == \"bool\":\n+                    option[\"value\"] = not option[\"value\"]\n+\n \n def runNcGui(parser):\n     \"\"\"Main entry point for ncurses GUI\"\"\"\n@@ -752,7 +782,9 @@ def runNcGui(parser):\n         # Check if ncurses is available\n         import curses\n     except ImportError:\n-        raise SqlmapMissingDependence(\"missing 'curses' module (try installing 'windows-curses' on Windows)\")\n+        raise SqlmapMissingDependence(\n+            \"missing 'curses' module (try installing 'windows-curses' on Windows)\"\n+        )\n \n     try:\n         # Initialize and run\n"
    },
    {
        "id": "323",
        "sha_fail": "71b7fd4a926acb2c018af855f325502bfe03d417",
        "diff": "diff --git a/dspy/clients/lm.py b/dspy/clients/lm.py\nindex f1cebd5..acd0fe5 100644\n--- a/dspy/clients/lm.py\n+++ b/dspy/clients/lm.py\n@@ -13,7 +13,7 @@ import dspy\n from dspy.clients.cache import request_cache\n from dspy.clients.openai import OpenAIProvider\n from dspy.clients.provider import Provider, ReinforceJob, TrainingJob\n-from dspy.clients.utils_finetune import TrainDataFormat, MultiGPUConfig\n+from dspy.clients.utils_finetune import MultiGPUConfig, TrainDataFormat\n from dspy.dsp.utils.settings import settings\n from dspy.utils.callback import BaseCallback\n \n@@ -104,11 +104,7 @@ class LM(BaseLM):\n         self._warn_zero_temp_rollout(self.kwargs.get(\"temperature\"), self.kwargs.get(\"rollout_id\"))\n \n     def _warn_zero_temp_rollout(self, temperature: float | None, rollout_id):\n-        if (\n-            not self._warned_zero_temp_rollout\n-            and rollout_id is not None\n-            and (temperature is None or temperature == 0)\n-        ):\n+        if not self._warned_zero_temp_rollout and rollout_id is not None and (temperature is None or temperature == 0):\n             warnings.warn(\n                 \"rollout_id has no effect when temperature=0; set temperature>0 to bypass the cache.\",\n                 stacklevel=3,\n@@ -134,10 +130,7 @@ class LM(BaseLM):\n \n         messages = messages or [{\"role\": \"user\", \"content\": prompt}]\n         if self.use_developer_role and self.model_type == \"responses\":\n-            messages = [\n-                {**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m\n-                for m in messages\n-            ]\n+            messages = [{**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m for m in messages]\n         kwargs = {**self.kwargs, **kwargs}\n         self._warn_zero_temp_rollout(kwargs.get(\"temperature\"), kwargs.get(\"rollout_id\"))\n         if kwargs.get(\"rollout_id\") is None:\n@@ -170,10 +163,7 @@ class LM(BaseLM):\n \n         messages = messages or [{\"role\": \"user\", \"content\": prompt}]\n         if self.use_developer_role and self.model_type == \"responses\":\n-            messages = [\n-                {**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m\n-                for m in messages\n-            ]\n+            messages = [{**m, \"role\": \"developer\"} if m.get(\"role\") == \"system\" else m for m in messages]\n         kwargs = {**self.kwargs, **kwargs}\n         self._warn_zero_temp_rollout(kwargs.get(\"temperature\"), kwargs.get(\"rollout_id\"))\n         if kwargs.get(\"rollout_id\") is None:\n@@ -237,7 +227,9 @@ class LM(BaseLM):\n \n         return job\n \n-    def reinforce(self, train_kwargs, gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1)) -> ReinforceJob:\n+    def reinforce(\n+        self, train_kwargs, gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1)\n+    ) -> ReinforceJob:\n         # TODO(GRPO Team): Should we return an initialized job here?\n         from dspy import settings as settings\n \n@@ -424,6 +416,7 @@ async def alitellm_text_completion(request: dict[str, Any], num_retries: int, ca\n         **request,\n     )\n \n+\n def litellm_responses_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None):\n     cache = cache or {\"no-cache\": True, \"no-store\": True}\n     request = dict(request)\n@@ -451,6 +444,7 @@ async def alitellm_responses_completion(request: dict[str, Any], num_retries: in\n         **request,\n     )\n \n+\n def _convert_chat_request_to_responses_request(request: dict[str, Any]):\n     request = dict(request)\n     if \"messages\" in request:\ndiff --git a/dspy/clients/lm_local_arbor.py b/dspy/clients/lm_local_arbor.py\nindex bc5380e..0fdfa00 100644\n--- a/dspy/clients/lm_local_arbor.py\n+++ b/dspy/clients/lm_local_arbor.py\n@@ -8,7 +8,7 @@ import requests\n \n import dspy\n from dspy.clients.provider import Provider, ReinforceJob, TrainingJob\n-from dspy.clients.utils_finetune import GRPOGroup, TrainDataFormat, TrainingStatus, save_data, MultiGPUConfig\n+from dspy.clients.utils_finetune import GRPOGroup, MultiGPUConfig, TrainDataFormat, TrainingStatus, save_data\n \n if TYPE_CHECKING:\n     from dspy.clients.lm import LM\n@@ -71,7 +71,12 @@ class ArborReinforceJob(ReinforceJob):\n         \"lora\": False,\n     }\n \n-    def __init__(self, lm: \"LM\", train_kwargs: GRPOTrainKwargs, gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1)):\n+    def __init__(\n+        self,\n+        lm: \"LM\",\n+        train_kwargs: GRPOTrainKwargs,\n+        gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1),\n+    ):\n         # The teleprompter must ensure that this is set\n         if \"num_generations\" not in train_kwargs:\n             raise ValueError(\"num_generations must be set in the training kwargs\")\n@@ -157,9 +162,7 @@ class ArborReinforceJob(ReinforceJob):\n         self.lm.model = ArborProvider._add_provider_prefix(response[\"current_model\"])\n         self.provider_job_id = response.get(\"job_id\")\n \n-    def _run_grpo_step_one_group(\n-        self, train_group: GRPOGroup, train_data_format: TrainDataFormat | str | None = None\n-    ):\n+    def _run_grpo_step_one_group(self, train_group: GRPOGroup, train_data_format: TrainDataFormat | str | None = None):\n         # TODO: Check that the data follows the intended format\n         api_base = self.lm.kwargs[\"api_base\"]\n         # api_key = self.lm.kwargs[\"api_key\"]\n@@ -184,9 +187,9 @@ class ArborReinforceJob(ReinforceJob):\n         # different step methods or changing our smallets data format to be the\n         # GRPO group.\n         # TODO: Support step on the server side\n-        assert (\n-            train_data_format == TrainDataFormat.GRPO_CHAT\n-        ), f\"GRPO only supports the GRPO_CHAT data format. Got {train_data_format} instead.\"\n+        assert train_data_format == TrainDataFormat.GRPO_CHAT, (\n+            f\"GRPO only supports the GRPO_CHAT data format. Got {train_data_format} instead.\"\n+        )\n         for group in train_data:\n             self._run_grpo_step_one_group(group, train_data_format)\n \n@@ -254,7 +257,9 @@ class ArborProvider(Provider):\n         launch_kwargs = launch_kwargs or lm.launch_kwargs\n \n         # Make request to launch endpoint\n-        response = requests.post(urljoin(api_base, \"chat/launch\"), json={\"model\": model, \"launch_kwargs\": launch_kwargs})\n+        response = requests.post(\n+            urljoin(api_base, \"chat/launch\"), json={\"model\": model, \"launch_kwargs\": launch_kwargs}\n+        )\n \n         if response.status_code != 200:\n             raise Exception(f\"Failed to launch model. Status code: {response.status_code}, Response: {response.text}\")\ndiff --git a/dspy/clients/provider.py b/dspy/clients/provider.py\nindex 3b3ac54..3779989 100644\n--- a/dspy/clients/provider.py\n+++ b/dspy/clients/provider.py\n@@ -3,7 +3,7 @@ from concurrent.futures import Future\n from threading import Thread\n from typing import TYPE_CHECKING, Any\n \n-from dspy.clients.utils_finetune import TrainDataFormat, MultiGPUConfig\n+from dspy.clients.utils_finetune import MultiGPUConfig, TrainDataFormat\n \n if TYPE_CHECKING:\n     from dspy.clients.lm import LM\n@@ -36,7 +36,12 @@ class TrainingJob(Future):\n \n \n class ReinforceJob:\n-    def __init__(self, lm: \"LM\", train_kwargs: dict[str, Any] | None = None, gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1)):\n+    def __init__(\n+        self,\n+        lm: \"LM\",\n+        train_kwargs: dict[str, Any] | None = None,\n+        gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1),\n+    ):\n         self.lm = lm\n         self.train_kwargs = train_kwargs or {}\n         self.gpu_config = gpu_config\n@@ -44,7 +49,6 @@ class ReinforceJob:\n         self.last_checkpoint = None\n         self.gpu_config = gpu_config\n \n-\n     @abstractmethod\n     def initialize(self):\n         raise NotImplementedError\ndiff --git a/dspy/teleprompt/grpo.py b/dspy/teleprompt/grpo.py\nindex 8ae9c60..e193e76 100644\n--- a/dspy/teleprompt/grpo.py\n+++ b/dspy/teleprompt/grpo.py\n@@ -6,7 +6,7 @@ from typing import Any, Callable, Literal\n from dspy.adapters.base import Adapter\n from dspy.adapters.chat_adapter import ChatAdapter\n from dspy.clients.lm import LM\n-from dspy.clients.utils_finetune import GRPOGroup, TrainDataFormat, MultiGPUConfig\n+from dspy.clients.utils_finetune import GRPOGroup, MultiGPUConfig, TrainDataFormat\n from dspy.dsp.utils.settings import settings\n from dspy.evaluate.evaluate import Evaluate\n from dspy.primitives.example import Example\n@@ -39,7 +39,9 @@ class GRPO(FinetuneTeleprompter):\n         report_train_scores: bool = False,\n         failure_score: float = 0,\n         format_failure_score: float = -1,\n-        variably_invoked_predictor_grouping_mode: Literal[\"truncate\"] | Literal[\"fill\"] | Literal[\"ragged\"] = \"truncate\",\n+        variably_invoked_predictor_grouping_mode: Literal[\"truncate\"]\n+        | Literal[\"fill\"]\n+        | Literal[\"ragged\"] = \"truncate\",\n         variably_invoked_predictor_fill_strategy: Literal[\"randint\"] | Literal[\"max\"] | None = None,\n         gpu_config: MultiGPUConfig = MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1),\n     ):\n@@ -60,20 +62,28 @@ class GRPO(FinetuneTeleprompter):\n         self.format_failure_score = format_failure_score\n         self.gpu_config = gpu_config\n \n-        assert failure_score > format_failure_score, \"failure_score must be greater than format_failure_score since the range [format_failure_score, failure_score] is used to provide dspy formatting rewards\"\n+        assert failure_score > format_failure_score, (\n+            \"failure_score must be greater than format_failure_score since the range [format_failure_score, failure_score] is used to provide dspy formatting rewards\"\n+        )\n \n         if self.use_train_as_val:\n             assert report_train_scores, \"If use_train_as_val is True, report_train_scores must be True.\"\n \n         assert exclude_demos, \"exclude_demos==False is not supported yet. Please set it to True.\"\n-        assert multitask, \"independent GRPO training jobs for each predictor in the student program is not supported yet. Please set multitask=True.\"\n+        assert multitask, (\n+            \"independent GRPO training jobs for each predictor in the student program is not supported yet. Please set multitask=True.\"\n+        )\n \n         # The backend will be called with a batch of (num_dspy_examples_per_grpo_step * num_rollouts_per_grpo_step * num_predictors) per training set if multitask is True\n         # If multitask is False, the backend will be called with a batch of (num_dspy_examples_per_grpo_step * num_rollouts_per_grpo_step) per training job\n         self.variably_invoked_predictor_grouping_mode = variably_invoked_predictor_grouping_mode\n         if variably_invoked_predictor_grouping_mode == \"fill\":\n-            assert variably_invoked_predictor_fill_strategy is not None, \"variably_invoked_predictor_fill_strategy must be set when variably_invoked_predictor_grouping_mode is 'fill'\"\n-            assert variably_invoked_predictor_fill_strategy in [\"randint\", \"max\"], \"variably_invoked_predictor_fill_strategy must be either 'randint' or 'max'\"\n+            assert variably_invoked_predictor_fill_strategy is not None, (\n+                \"variably_invoked_predictor_fill_strategy must be set when variably_invoked_predictor_grouping_mode is 'fill'\"\n+            )\n+            assert variably_invoked_predictor_fill_strategy in [\"randint\", \"max\"], (\n+                \"variably_invoked_predictor_fill_strategy must be either 'randint' or 'max'\"\n+            )\n         self.variably_invoked_predictor_fill_strategy = variably_invoked_predictor_fill_strategy\n \n         self.shuffled_trainset_ids = []\n@@ -90,16 +100,26 @@ class GRPO(FinetuneTeleprompter):\n     ):\n         # At this point, trace_data: list[example_idx -> list[teacher_idx -> [num_samples_per_input * Dict(example, prediction, trace, example_ind, score)]]]\n         # Shape of trace is: [dspy_module_invocation_idx -> Tuple[Predictor, PredictorInputs, Prediction]]\n-        assert len(trace_data) == len(subsample_training_dataset), f\"Trace data length {len(trace_data)} does not match the number of examples {len(subsample_training_dataset)}\"\n-        assert len(trace_data[0]) == num_teachers, f\"Trace data length {len(trace_data[0])} does not match the number of teachers {num_teachers}\"\n+        assert len(trace_data) == len(subsample_training_dataset), (\n+            f\"Trace data length {len(trace_data)} does not match the number of examples {len(subsample_training_dataset)}\"\n+        )\n+        assert len(trace_data[0]) == num_teachers, (\n+            f\"Trace data length {len(trace_data[0])} does not match the number of teachers {num_teachers}\"\n+        )\n         # TODO(GRPO Team): Ideally, once the dspy format issue is fixed, this change should be reverted back to being a normal assert.\n         if len(trace_data[0][0]) == 0:\n-            logger.warning(f\"Trace data for example {0} and teacher {0} is empty. This is likely due to all examples in the training set input, resulting in the model generating output not following the dspy response format.\")\n+            logger.warning(\n+                f\"Trace data for example {0} and teacher {0} is empty. This is likely due to all examples in the training set input, resulting in the model generating output not following the dspy response format.\"\n+            )\n         elif len(trace_data[0][0]) != num_samples_per_input:\n-            logger.warning(f\"Trace data length {len(trace_data[0][0])} does not match the expected number of samples per input {num_samples_per_input}\")\n+            logger.warning(\n+                f\"Trace data length {len(trace_data[0][0])} does not match the expected number of samples per input {num_samples_per_input}\"\n+            )\n             assert \"trace\" in trace_data[0][0][0], \"Trace data does not contain the 'trace' key\"\n             assert len(trace_data[0][0][0][\"trace\"]) > 0, \"Trace data is empty\"\n-            assert len(trace_data[0][0][0][\"trace\"][0]) == 3, f\"Trace tuple length {len(trace_data[0][0][0]['trace'][0])} does not match the expected length 3\"\n+            assert len(trace_data[0][0][0][\"trace\"][0]) == 3, (\n+                f\"Trace tuple length {len(trace_data[0][0][0]['trace'][0])} does not match the expected length 3\"\n+            )\n \n         for example_data in trace_data:\n             for teacher_data in example_data:\n@@ -116,33 +136,43 @@ class GRPO(FinetuneTeleprompter):\n         if valset is not None:\n             # Validation set provided by user\n             assert not self.use_train_as_val, \"If valset is provided, use_train_as_val must be False.\"\n-            assert isinstance(self.num_steps_for_val, int) and self.num_steps_for_val > 0, \"num_steps_for_val must be a positive integer.\"\n+            assert isinstance(self.num_steps_for_val, int) and self.num_steps_for_val > 0, (\n+                \"num_steps_for_val must be a positive integer.\"\n+            )\n             if self.report_train_scores:\n                 if step_idx == -1:\n-                    logger.info(\"Using user provided validation set and reporting train scores for every validation step in addition.\")\n+                    logger.info(\n+                        \"Using user provided validation set and reporting train scores for every validation step in addition.\"\n+                    )\n                 valset_evaluator = Evaluate(\n                     devset=valset + trainset,\n                     num_threads=self.num_threads,\n                     display_progress=True,\n                     provide_traceback=False,  # TODO(check with team)\n-                    max_errors=len(valset)*10,  # TODO(check with team)\n-                    failure_score=self.failure_score\n+                    max_errors=len(valset) * 10,  # TODO(check with team)\n+                    failure_score=self.failure_score,\n                 )\n                 if step_idx == -1:\n                     logger.info(\"Evaluating the student program on the train+validation set before training loop...\")\n                 else:\n-                    logger.info(f\"Evaluating the student program on the validation set after training step {step_idx + 1}/{self.num_train_steps}\")\n+                    logger.info(\n+                        f\"Evaluating the student program on the validation set after training step {step_idx + 1}/{self.num_train_steps}\"\n+                    )\n                 valset_evaluation = valset_evaluator(student, metric=self.metric)\n-                trainset_scores = [r[-1] for r in valset_evaluation.results[len(valset):]]\n-                valset_scores = [r[-1] for r in valset_evaluation.results[:len(valset)]]\n+                trainset_scores = [r[-1] for r in valset_evaluation.results[len(valset) :]]\n+                valset_scores = [r[-1] for r in valset_evaluation.results[: len(valset)]]\n                 trainset_agg = sum(trainset_scores) / len(trainset_scores)\n                 valset_agg = sum(valset_scores) / len(valset_scores)\n                 if step_idx == -1:\n                     logger.info(f\"Student program training set score before training loop: {trainset_agg}\")\n                     logger.info(f\"Student program validation set score before training loop: {valset_agg}\")\n                 else:\n-                    logger.info(f\"Student program training set score after training step {step_idx + 1}/{self.num_train_steps}: {trainset_agg}\")\n-                    logger.info(f\"Student program validation set score after training step {step_idx + 1}/{self.num_train_steps}: {valset_agg}\")\n+                    logger.info(\n+                        f\"Student program training set score after training step {step_idx + 1}/{self.num_train_steps}: {trainset_agg}\"\n+                    )\n+                    logger.info(\n+                        f\"Student program validation set score after training step {step_idx + 1}/{self.num_train_steps}: {valset_agg}\"\n+                    )\n             else:\n                 if step_idx == -1:\n                     logger.info(\"Using user provided validation set and not reporting train scores.\")\n@@ -151,23 +181,31 @@ class GRPO(FinetuneTeleprompter):\n                     num_threads=self.num_threads,\n                     display_progress=True,\n                     provide_traceback=False,  # TODO(check with team)\n-                    max_errors=len(valset)*10,  # TODO(check with team)\n-                    failure_score=self.failure_score\n+                    max_errors=len(valset) * 10,  # TODO(check with team)\n+                    failure_score=self.failure_score,\n                 )\n                 if step_idx == -1:\n                     logger.info(\"Evaluating the student program on the validation set before training loop...\")\n                 else:\n-                    logger.info(f\"Evaluating the student program on the validation set after training step {step_idx + 1}/{self.num_train_steps}\")\n+                    logger.info(\n+                        f\"Evaluating the student program on the validation set after training step {step_idx + 1}/{self.num_train_steps}\"\n+                    )\n                 valset_evaluation = valset_evaluator(student, metric=self.metric)\n                 if step_idx == -1:\n                     logger.info(f\"Student program validation set score before training loop: {valset_evaluation.score}\")\n                 else:\n-                    logger.info(f\"Student program validation set score after training step {step_idx + 1}/{self.num_train_steps}: {valset_evaluation.score}\")\n+                    logger.info(\n+                        f\"Student program validation set score after training step {step_idx + 1}/{self.num_train_steps}: {valset_evaluation.score}\"\n+                    )\n         else:\n             # No validation set provided by user\n             if self.report_train_scores:\n-                assert self.use_train_as_val, \"If report_train_scores is True, use_train_as_val must be True when valset is not provided explicitly.\"\n-                assert isinstance(self.num_steps_for_val, int) and self.num_steps_for_val > 0, \"num_steps_for_val must be a positive integer.\"\n+                assert self.use_train_as_val, (\n+                    \"If report_train_scores is True, use_train_as_val must be True when valset is not provided explicitly.\"\n+                )\n+                assert isinstance(self.num_steps_for_val, int) and self.num_steps_for_val > 0, (\n+                    \"num_steps_for_val must be a positive integer.\"\n+                )\n                 if step_idx == -1:\n                     logger.info(\"Using trainset as validation set.\")\n                 valset_evaluator = Evaluate(\n@@ -175,18 +213,22 @@ class GRPO(FinetuneTeleprompter):\n                     num_threads=self.num_threads,\n                     display_progress=True,\n                     provide_traceback=False,  # TODO(check with team)\n-                    max_errors=len(trainset)*10,  # TODO(check with team)\n-                    failure_score=self.failure_score\n+                    max_errors=len(trainset) * 10,  # TODO(check with team)\n+                    failure_score=self.failure_score,\n                 )\n                 if step_idx == -1:\n                     logger.info(\"Evaluating the student program on the validation set before training loop...\")\n                 else:\n-                    logger.info(f\"Evaluating the student program on the validation set after training step {step_idx + 1}/{self.num_train_steps}\")\n+                    logger.info(\n+                        f\"Evaluating the student program on the validation set after training step {step_idx + 1}/{self.num_train_steps}\"\n+                    )\n                 valset_evaluation = valset_evaluator(student, metric=self.metric)\n                 if step_idx == -1:\n                     logger.info(f\"Student program training set score before training loop: {valset_evaluation.score}\")\n                 else:\n-                    logger.info(f\"Student program training set score after training step {step_idx + 1}/{self.num_train_steps}: {valset_evaluation.score}\")\n+                    logger.info(\n+                        f\"Student program training set score after training step {step_idx + 1}/{self.num_train_steps}: {valset_evaluation.score}\"\n+                    )\n             else:\n                 # No valset provided, and not using train as val\n                 assert not self.use_train_as_val, \"If report_train_scores is False, use_train_as_val must be False.\"\n@@ -199,7 +241,9 @@ class GRPO(FinetuneTeleprompter):\n         for id in self.shuffled_trainset_ids:\n             self.id_freqs[id] += 1\n \n-        num_to_pad = self.num_dspy_examples_per_grpo_step - (len(original_trainset) % self.num_dspy_examples_per_grpo_step)\n+        num_to_pad = self.num_dspy_examples_per_grpo_step - (\n+            len(original_trainset) % self.num_dspy_examples_per_grpo_step\n+        )\n         if num_to_pad > 0:\n             # Select ids based on least frequent ids\n             for _ in range(num_to_pad):\n@@ -222,12 +266,18 @@ class GRPO(FinetuneTeleprompter):\n             self.epoch = curr_epoch\n             self.update_shuffled_trainset(original_trainset)\n \n-        assert len(self.shuffled_trainset_ids) >= self.num_dspy_examples_per_grpo_step, f\"Shuffled trainset length {len(self.shuffled_trainset_ids)} is less than num_dspy_examples_per_grpo_step {self.num_dspy_examples_per_grpo_step}\"\n-        assert len(self.shuffled_trainset_ids) % self.num_dspy_examples_per_grpo_step == 0, f\"Shuffled trainset length {len(self.shuffled_trainset_ids)} is not divisible by num_dspy_examples_per_grpo_step {self.num_dspy_examples_per_grpo_step}\"\n+        assert len(self.shuffled_trainset_ids) >= self.num_dspy_examples_per_grpo_step, (\n+            f\"Shuffled trainset length {len(self.shuffled_trainset_ids)} is less than num_dspy_examples_per_grpo_step {self.num_dspy_examples_per_grpo_step}\"\n+        )\n+        assert len(self.shuffled_trainset_ids) % self.num_dspy_examples_per_grpo_step == 0, (\n+            f\"Shuffled trainset length {len(self.shuffled_trainset_ids)} is not divisible by num_dspy_examples_per_grpo_step {self.num_dspy_examples_per_grpo_step}\"\n+        )\n \n         base_idx = base_idx % len(self.shuffled_trainset_ids)\n         end_idx = base_idx + self.num_dspy_examples_per_grpo_step\n-        assert end_idx <= len(self.shuffled_trainset_ids), f\"End index {end_idx} is out of bounds for shuffled trainset length {len(self.shuffled_trainset_ids)}\"\n+        assert end_idx <= len(self.shuffled_trainset_ids), (\n+            f\"End index {end_idx} is out of bounds for shuffled trainset length {len(self.shuffled_trainset_ids)}\"\n+        )\n         selected_ids = self.shuffled_trainset_ids[base_idx:end_idx]\n         selected_trainset = [original_trainset[i] for i in selected_ids]\n         return selected_trainset\n@@ -240,14 +290,16 @@ class GRPO(FinetuneTeleprompter):\n         valset: list[Example] | None = None,\n         **kwargs,\n     ) -> Module:\n-        logger.info(\"Starting the GRPO compilation process... The LM(s) for the student program will be updated in place at the end of the training.\")\n+        logger.info(\n+            \"Starting the GRPO compilation process... The LM(s) for the student program will be updated in place at the end of the training.\"\n+        )\n         logger.info(\"Validating the inputs...\")\n \n         assert len(trainset) > 0, \"Training set is empty. Please provide a non-empty training set.\"\n \n         if len(trainset) < self.num_dspy_examples_per_grpo_step:\n             logger.warning(\n-            f\"Number of training examples {len(trainset)} is less than the number of examples per GRPO step {self.num_dspy_examples_per_grpo_step}. \"\n+                f\"Number of training examples {len(trainset)} is less than the number of examples per GRPO step {self.num_dspy_examples_per_grpo_step}. \"\n                 \"Repeating the training set to fill the GRPO step. This could lead to overfitting and training instability.\"\n             )\n             multiplier = (self.num_dspy_examples_per_grpo_step + len(trainset) - 1) // len(trainset)\n@@ -292,7 +344,9 @@ class GRPO(FinetuneTeleprompter):\n         pred_signature_hash_to_ind = {hash(pred.signature): ind for ind, pred in enumerate(student.predictors())}\n         num_student_predictors = len(student.predictors())\n \n-        logging.info(\"Preparing the teacher program(s)... We will ensure that the provided programs have the same program structure as the student program.\")\n+        logging.info(\n+            \"Preparing the teacher program(s)... We will ensure that the provided programs have the same program structure as the student program.\"\n+        )\n         if (isinstance(teacher, list) and len(teacher) == 0) or teacher is None:\n             teacher = student\n         teachers = teacher if isinstance(teacher, list) else [teacher]\n@@ -301,7 +355,9 @@ class GRPO(FinetuneTeleprompter):\n             all_predictors_have_lms(t)\n \n         # Ensure that the teachers list contain the student program\n-        assert student in teachers, f\"Student program {student} is not in the list of teachers {teachers}. Please provide the student program as one of the teachers. Alternatively, you can leave the teacher argument as None, and the student program will be used as the teacher program.\"\n+        assert student in teachers, (\n+            f\"Student program {student} is not in the list of teachers {teachers}. Please provide the student program as one of the teachers. Alternatively, you can leave the teacher argument as None, and the student program will be used as the teacher program.\"\n+        )\n         assert self.num_rollouts_per_grpo_step % len(teachers) == 0, (\n             f\"The GRPO group size (num_rollouts_per_grpo_step) {self.num_rollouts_per_grpo_step} is not divisible by the number of teachers {len(teachers)}. \"\n             \"This is required to ensure that each teacher gets the same number of examples.\"\n@@ -357,13 +413,15 @@ class GRPO(FinetuneTeleprompter):\n             logger.info(\"Bootstrapping data...\")\n             trace_data = [[[] for _ in range(len(teachers))] for _ in range(len(subsample_training_dataset))]\n             for tind, teacher in enumerate(teachers):\n-                subsample_training_dataset_repeated = [example for _ in range(num_samples_per_input) for example in subsample_training_dataset]\n+                subsample_training_dataset_repeated = [\n+                    example for _ in range(num_samples_per_input) for example in subsample_training_dataset\n+                ]\n                 round_data = bootstrap_trace_data(\n                     program=teacher,\n                     dataset=subsample_training_dataset_repeated,\n                     metric=self.metric,\n                     num_threads=self.num_threads,\n-                    raise_on_error=False, # TODO(GRPO Team): This should be True, once the dspy format issue is fixed\n+                    raise_on_error=False,  # TODO(GRPO Team): This should be True, once the dspy format issue is fixed\n                     capture_failed_parses=True,\n                     failure_score=self.failure_score,\n                     format_failure_score=self.format_failure_score,\n@@ -402,38 +460,60 @@ class GRPO(FinetuneTeleprompter):\n                         for sample in teacher_data:\n                             # Each sample is a Dict(example, prediction, trace, example_ind, score)\n                             # sample['prediction'] is module_level prediction\n-                            assert sample[\"example_ind\"] == example_ind, f\"Example index {sample['example_ind']} does not match the expected index {example_ind}\"\n+                            assert sample[\"example_ind\"] == example_ind, (\n+                                f\"Example index {sample['example_ind']} does not match the expected index {example_ind}\"\n+                            )\n \n-                            trace_instances_for_current_pred = [(*t, sample[\"score\"]) for t in sample[\"trace\"] if hash(t[0].signature) == hash(student.predictors()[pred_id].signature)]\n+                            trace_instances_for_current_pred = [\n+                                (*t, sample[\"score\"])\n+                                for t in sample[\"trace\"]\n+                                if hash(t[0].signature) == hash(student.predictors()[pred_id].signature)\n+                            ]\n \n                             predictor_example_invocations.append(trace_instances_for_current_pred)\n \n                     if len(predictor_example_invocations) == 0:\n-                        logger.warning(f\"Skipping example {example_ind} for predictor {pred_id} as it has no invocations. This is likely due to all examples in the training set input, resulting in the model generating output not following the dspy response format.\")\n+                        logger.warning(\n+                            f\"Skipping example {example_ind} for predictor {pred_id} as it has no invocations. This is likely due to all examples in the training set input, resulting in the model generating output not following the dspy response format.\"\n+                        )\n                         continue\n                     elif len(predictor_example_invocations) != self.num_rollouts_per_grpo_step:\n-                        logger.warning(f\"Number of predictor example invocations {len(predictor_example_invocations)} does not match the expected batch size {self.num_rollouts_per_grpo_step}. This is likely due to all examples in the training set input, resulting in the model generating output not following the dspy response format.\")\n-\n-                    min_len = min([len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))])\n-                    max_len = max([len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))])\n+                        logger.warning(\n+                            f\"Number of predictor example invocations {len(predictor_example_invocations)} does not match the expected batch size {self.num_rollouts_per_grpo_step}. This is likely due to all examples in the training set input, resulting in the model generating output not following the dspy response format.\"\n+                        )\n+\n+                    min_len = min(\n+                        [len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))]\n+                    )\n+                    max_len = max(\n+                        [len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))]\n+                    )\n                     if min_len == 0:\n-                        logger.warning(f\"Skipping example {example_ind} for predictor {pred_id} as it has no invocations.\")\n+                        logger.warning(\n+                            f\"Skipping example {example_ind} for predictor {pred_id} as it has no invocations.\"\n+                        )\n                         continue\n \n                     if self.variably_invoked_predictor_grouping_mode == \"truncate\":\n-                        predictor_example_invocations = [invocation[:min_len] for invocation in predictor_example_invocations]\n+                        predictor_example_invocations = [\n+                            invocation[:min_len] for invocation in predictor_example_invocations\n+                        ]\n                     elif self.variably_invoked_predictor_grouping_mode == \"fill\":\n                         if self.variably_invoked_predictor_fill_strategy == \"randint\":\n-                            selector = lambda l: self.rng.choice(l) # noqa: E731, E741\n+                            selector = lambda l: self.rng.choice(l)  # noqa: E731, E741\n                         else:\n-                            selector = lambda l: l[-1] # noqa: E731, E741\n+                            selector = lambda l: l[-1]  # noqa: E731, E741\n                         predictor_example_invocations = [\n                             invocation + [selector(invocation) for _ in range(max_len - len(invocation))]\n                             for invocation in predictor_example_invocations\n                         ]\n                     else:\n-                        assert self.variably_invoked_predictor_grouping_mode == \"ragged\", f\"Unknown variably invoked predictor grouping mode {self.variably_invoked_predictor_grouping_mode}\"\n-                    max_len = max([len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))])\n+                        assert self.variably_invoked_predictor_grouping_mode == \"ragged\", (\n+                            f\"Unknown variably invoked predictor grouping mode {self.variably_invoked_predictor_grouping_mode}\"\n+                        )\n+                    max_len = max(\n+                        [len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))]\n+                    )\n \n                     example_training_data: list[GRPOGroup] = [[] for _ in range(max_len)]\n \n@@ -449,60 +529,78 @@ class GRPO(FinetuneTeleprompter):\n                             predictor = trace_instance[0]\n                             pred_lm = predictor.lm\n                             adapter = self.adapter[pred_lm] or settings.adapter or ChatAdapter()\n-                            assert isinstance(adapter, ChatAdapter), f\"Adapter {adapter} is not a ChatAdapter. GRPO training is not supported for this adapter.\"\n+                            assert isinstance(adapter, ChatAdapter), (\n+                                f\"Adapter {adapter} is not a ChatAdapter. GRPO training is not supported for this adapter.\"\n+                            )\n                             # TODO(Lakshya): Currently we exclude demos from the training data\n                             # TODO(GRPO Team): Use build_call_data_from_trace (from bootstrap_finetune) instead of\n                             # dealing with the message formatting ourselves.\n                             inp_messages = adapter.format(\n                                 signature=trace_instance[0].signature,\n                                 inputs=trace_instance[1],\n-                                demos=[] # TODO: Add support for demos\n+                                demos=[],  # TODO: Add support for demos\n                             )\n \n                             if isinstance(trace_instance[2], FailedPrediction):\n                                 score = trace_instance[2].format_reward or self.format_failure_score\n-                                example_training_data[group_idx].append({\n-                                    \"messages\": inp_messages,\n-                                    \"completion\": {\n-                                        \"role\": \"assistant\",\n-                                        \"content\": trace_instance[2].completion_text,\n-                                    },\n-                                    \"reward\": float(score),\n-                                })\n-                                logger.warning(f\"Adding a format failure example to the training data for predictor {pred_id} and example {example_ind}.\")\n+                                example_training_data[group_idx].append(\n+                                    {\n+                                        \"messages\": inp_messages,\n+                                        \"completion\": {\n+                                            \"role\": \"assistant\",\n+                                            \"content\": trace_instance[2].completion_text,\n+                                        },\n+                                        \"reward\": float(score),\n+                                    }\n+                                )\n+                                logger.warning(\n+                                    f\"Adding a format failure example to the training data for predictor {pred_id} and example {example_ind}.\"\n+                                )\n                             else:\n                                 all_messages = adapter.format_finetune_data(\n                                     signature=trace_instance[0].signature,\n                                     inputs=trace_instance[1],\n                                     outputs=trace_instance[2],\n-                                    demos=[] # TODO: Add support for demos\n+                                    demos=[],  # TODO: Add support for demos\n                                 )[\"messages\"]\n \n-                                assert all_messages[:-1] == inp_messages, f\"Input messages {inp_messages} do not match the expected messages {all_messages[:-1]}\"\n-\n-                                example_training_data[group_idx].append({\n-                                    \"messages\": inp_messages,\n-                                    \"completion\": {\n-                                        \"role\": all_messages[-1][\"role\"],\n-                                        \"content\": all_messages[-1][\"content\"],\n-                                    },\n-                                    \"reward\": float(score),\n-                                })\n+                                assert all_messages[:-1] == inp_messages, (\n+                                    f\"Input messages {inp_messages} do not match the expected messages {all_messages[:-1]}\"\n+                                )\n+\n+                                example_training_data[group_idx].append(\n+                                    {\n+                                        \"messages\": inp_messages,\n+                                        \"completion\": {\n+                                            \"role\": all_messages[-1][\"role\"],\n+                                            \"content\": all_messages[-1][\"content\"],\n+                                        },\n+                                        \"reward\": float(score),\n+                                    }\n+                                )\n \n                     train_batch_per_predictor[pred_id].extend(example_training_data)\n \n             if not any(train_batch_per_predictor):\n-                logger.warning(\"No training data found for this training step. This means that the model did not generate valid formatted responses for any of the examples in the training set. This is a critical error. Please check the model and the training set.\")\n+                logger.warning(\n+                    \"No training data found for this training step. This means that the model did not generate valid formatted responses for any of the examples in the training set. This is a critical error. Please check the model and the training set.\"\n+                )\n                 continue\n \n             for predictor_train_batch in train_batch_per_predictor:\n                 for grpo_train_group in predictor_train_batch:\n                     if len(grpo_train_group) != self.num_rollouts_per_grpo_step:\n-                        logger.warning(f\"Number of completions {len(grpo_train_group)} does not match the expected number num_rollouts_per_grpo_step={self.num_rollouts_per_grpo_step}\")\n-                        assert len(grpo_train_group) <= self.num_rollouts_per_grpo_step, f\"Number of completions {len(grpo_train_group)} is greater than the expected number num_rollouts_per_grpo_step={self.num_rollouts_per_grpo_step}\"\n+                        logger.warning(\n+                            f\"Number of completions {len(grpo_train_group)} does not match the expected number num_rollouts_per_grpo_step={self.num_rollouts_per_grpo_step}\"\n+                        )\n+                        assert len(grpo_train_group) <= self.num_rollouts_per_grpo_step, (\n+                            f\"Number of completions {len(grpo_train_group)} is greater than the expected number num_rollouts_per_grpo_step={self.num_rollouts_per_grpo_step}\"\n+                        )\n                     if len(set(map(repr, grpo_train_group))) < 2:\n                         # TODO(GRPO Team): How can we avoid this warning?\n-                        logger.warning(f\"GRPOGroup has no diversity. This could be due to low temperature, or low number of rollouts, or the cache could be enabled inadvertently. The GRPOGroup is {grpo_train_group}.\")\n+                        logger.warning(\n+                            f\"GRPOGroup has no diversity. This could be due to low temperature, or low number of rollouts, or the cache could be enabled inadvertently. The GRPOGroup is {grpo_train_group}.\"\n+                        )\n \n             # We now run the GRPO step. Notes:\n             # * The job here has a reference to a particular M that's attached\n@@ -520,7 +618,9 @@ class GRPO(FinetuneTeleprompter):\n             #   LM.\n             logger.info(\"Invoking GRPO training step...\")\n             for (_, data_key), job in grpo_training_jobs.items():\n-                train_data: list[GRPOGroup] = sum(train_batch_per_predictor, []) if data_key is None else train_batch_per_predictor[data_key] #noqa: RUF017\n+                train_data: list[GRPOGroup] = (\n+                    sum(train_batch_per_predictor, []) if data_key is None else train_batch_per_predictor[data_key]\n+                )  # noqa: RUF017\n                 for group in train_data:\n                     if len(group) != self.num_rollouts_per_grpo_step:\n                         # TODO(GRPO Team): This is very undesirable. This occurs only because in some of the generations, the model does not follow the correct dspy format.\n@@ -528,8 +628,10 @@ class GRPO(FinetuneTeleprompter):\n \n                         # Pad the group to the expected number of generations by repeating the whole group, might require multiple iterations\n                         while len(group) < self.num_rollouts_per_grpo_step:\n-                            group.extend(group[:min(self.num_rollouts_per_grpo_step - len(group), len(group))])\n-                    assert len(group) == self.num_rollouts_per_grpo_step, f\"Number of completions {len(group)} does not match the expected number self.num_rollouts_per_grpo_step={self.num_rollouts_per_grpo_step}\"\n+                            group.extend(group[: min(self.num_rollouts_per_grpo_step - len(group), len(group))])\n+                    assert len(group) == self.num_rollouts_per_grpo_step, (\n+                        f\"Number of completions {len(group)} does not match the expected number self.num_rollouts_per_grpo_step={self.num_rollouts_per_grpo_step}\"\n+                    )\n \n                 job.step(train_data=train_data, train_data_format=TrainDataFormat.GRPO_CHAT)\n \n"
    },
    {
        "id": "324",
        "sha_fail": "a7b902770d8b2769920794cf4e2016525e3ea1d9",
        "diff": "diff --git a/tests/dialects/test_clickhouse.py b/tests/dialects/test_clickhouse.py\nindex 7d7b252..a5bd004 100644\n--- a/tests/dialects/test_clickhouse.py\n+++ b/tests/dialects/test_clickhouse.py\n@@ -12,11 +12,15 @@ class TestClickhouse(Validator):\n     dialect = \"clickhouse\"\n \n     def test_clickhouse(self):\n-        expr = quote_identifiers(self.parse_one(\"{start_date:String}\"), dialect=\"clickhouse\")\n+        expr = quote_identifiers(\n+            self.parse_one(\"{start_date:String}\"), dialect=\"clickhouse\"\n+        )\n         self.assertEqual(expr.sql(\"clickhouse\"), \"{start_date: String}\")\n \n         for string_type_enum in ClickHouse.Generator.STRING_TYPE_MAPPING:\n-            self.validate_identity(f\"CAST(x AS {string_type_enum.value})\", \"CAST(x AS String)\")\n+            self.validate_identity(\n+                f\"CAST(x AS {string_type_enum.value})\", \"CAST(x AS String)\"\n+            )\n \n         # Arrays, maps and tuples can't be Nullable in ClickHouse\n         for non_nullable_type in (\"ARRAY<INT>\", \"MAP<INT, INT>\", \"STRUCT(a: INT)\"):\n@@ -24,10 +28,23 @@ class TestClickhouse(Validator):\n             target_type = try_cast.to.sql(\"clickhouse\")\n             self.assertEqual(try_cast.sql(\"clickhouse\"), f\"CAST(x AS {target_type})\")\n \n-        for nullable_type in (\"INT\", \"UINT\", \"BIGINT\", \"FLOAT\", \"DOUBLE\", \"TEXT\", \"DATE\", \"UUID\"):\n+        for nullable_type in (\n+            \"INT\",\n+            \"UINT\",\n+            \"BIGINT\",\n+            \"FLOAT\",\n+            \"DOUBLE\",\n+            \"TEXT\",\n+            \"DATE\",\n+            \"UUID\",\n+        ):\n             try_cast = parse_one(f\"TRY_CAST(x AS {nullable_type})\")\n-            target_type = exp.DataType.build(nullable_type, dialect=\"clickhouse\").sql(\"clickhouse\")\n-            self.assertEqual(try_cast.sql(\"clickhouse\"), f\"CAST(x AS Nullable({target_type}))\")\n+            target_type = exp.DataType.build(nullable_type, dialect=\"clickhouse\").sql(\n+                \"clickhouse\"\n+            )\n+            self.assertEqual(\n+                try_cast.sql(\"clickhouse\"), f\"CAST(x AS Nullable({target_type}))\"\n+            )\n \n         expr = parse_one(\"count(x)\")\n         self.assertEqual(expr.sql(dialect=\"clickhouse\"), \"COUNT(x)\")\n@@ -40,8 +57,12 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"SELECT json.a.:JSON.b.:Int64\")\n         self.validate_identity(\"WITH arrayJoin([(1, [2, 3])]) AS arr SELECT arr\")\n         self.validate_identity(\"CAST(1 AS Bool)\")\n-        self.validate_identity(\"SELECT toString(CHAR(104.1, 101, 108.9, 108.9, 111, 32))\")\n-        self.validate_identity(\"@macro\").assert_is(exp.Parameter).this.assert_is(exp.Var)\n+        self.validate_identity(\n+            \"SELECT toString(CHAR(104.1, 101, 108.9, 108.9, 111, 32))\"\n+        )\n+        self.validate_identity(\"@macro\").assert_is(exp.Parameter).this.assert_is(\n+            exp.Var\n+        )\n         self.validate_identity(\"SELECT toFloat(like)\")\n         self.validate_identity(\"SELECT like\")\n         self.validate_identity(\"SELECT STR_TO_DATE(str, fmt, tz)\")\n@@ -49,14 +70,20 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"SELECT EXTRACT(YEAR FROM toDateTime('2023-02-01'))\")\n         self.validate_identity(\"extract(haystack, pattern)\")\n         self.validate_identity(\"SELECT * FROM x LIMIT 1 UNION ALL SELECT * FROM y\")\n-        self.validate_identity(\"SELECT CAST(x AS Tuple(String, Array(Nullable(Float64))))\")\n+        self.validate_identity(\n+            \"SELECT CAST(x AS Tuple(String, Array(Nullable(Float64))))\"\n+        )\n         self.validate_identity(\"countIf(x, y)\")\n         self.validate_identity(\"x = y\")\n         self.validate_identity(\"x <> y\")\n         self.validate_identity(\"SELECT * FROM (SELECT a FROM b SAMPLE 0.01)\")\n-        self.validate_identity(\"SELECT * FROM (SELECT a FROM b SAMPLE 1 / 10 OFFSET 1 / 2)\")\n+        self.validate_identity(\n+            \"SELECT * FROM (SELECT a FROM b SAMPLE 1 / 10 OFFSET 1 / 2)\"\n+        )\n         self.validate_identity(\"SELECT sum(foo * bar) FROM bla SAMPLE 10000000\")\n-        self.validate_identity(\"CAST(x AS Nested(ID UInt32, Serial UInt32, EventTime DateTime))\")\n+        self.validate_identity(\n+            \"CAST(x AS Nested(ID UInt32, Serial UInt32, EventTime DateTime))\"\n+        )\n         self.validate_identity(\"CAST(x AS Enum('hello' = 1, 'world' = 2))\")\n         self.validate_identity(\"CAST(x AS Enum('hello', 'world'))\")\n         self.validate_identity(\"CAST(x AS Enum('hello' = 1, 'world'))\")\n@@ -92,17 +119,27 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"SELECT histogram(5)(a)\")\n         self.validate_identity(\"SELECT groupUniqArray(2)(a)\")\n         self.validate_identity(\"SELECT exponentialTimeDecayedAvg(60)(a, b)\")\n-        self.validate_identity(\"levenshteinDistance(col1, col2)\", \"editDistance(col1, col2)\")\n-        self.validate_identity(\"SELECT * FROM foo WHERE x GLOBAL IN (SELECT * FROM bar)\")\n-        self.validate_identity(\"SELECT * FROM foo WHERE x GLOBAL NOT IN (SELECT * FROM bar)\")\n+        self.validate_identity(\n+            \"levenshteinDistance(col1, col2)\", \"editDistance(col1, col2)\"\n+        )\n+        self.validate_identity(\n+            \"SELECT * FROM foo WHERE x GLOBAL IN (SELECT * FROM bar)\"\n+        )\n+        self.validate_identity(\n+            \"SELECT * FROM foo WHERE x GLOBAL NOT IN (SELECT * FROM bar)\"\n+        )\n         self.validate_identity(\"POSITION(haystack, needle)\")\n         self.validate_identity(\"POSITION(haystack, needle, position)\")\n         self.validate_identity(\"CAST(x AS DATETIME)\", \"CAST(x AS DateTime)\")\n         self.validate_identity(\"CAST(x AS TIMESTAMPTZ)\", \"CAST(x AS DateTime)\")\n         self.validate_identity(\"CAST(x as MEDIUMINT)\", \"CAST(x AS Int32)\")\n         self.validate_identity(\"CAST(x AS DECIMAL(38, 2))\", \"CAST(x AS Decimal(38, 2))\")\n-        self.validate_identity(\"SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src\")\n-        self.validate_identity(\"\"\"SELECT JSONExtractString('{\"x\": {\"y\": 1}}', 'x', 'y')\"\"\")\n+        self.validate_identity(\n+            \"SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src\"\n+        )\n+        self.validate_identity(\n+            \"\"\"SELECT JSONExtractString('{\"x\": {\"y\": 1}}', 'x', 'y')\"\"\"\n+        )\n         self.validate_identity(\"SELECT * FROM table LIMIT 1 BY a, b\")\n         self.validate_identity(\"SELECT * FROM table LIMIT 2 OFFSET 1 BY a, b\")\n         self.validate_identity(\"TRUNCATE TABLE t1 ON CLUSTER test_cluster\")\n@@ -110,7 +147,9 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"TRUNCATE DATABASE db\")\n         self.validate_identity(\"TRUNCATE DATABASE db ON CLUSTER test_cluster\")\n         self.validate_identity(\"TRUNCATE DATABASE db ON CLUSTER '{cluster}'\")\n-        self.validate_identity(\"EXCHANGE TABLES x.a AND y.b\", check_command_warning=True)\n+        self.validate_identity(\n+            \"EXCHANGE TABLES x.a AND y.b\", check_command_warning=True\n+        )\n         self.validate_identity(\"CREATE TABLE test (id UInt8) ENGINE=Null()\")\n         self.validate_identity(\n             \"SELECT * FROM foo ORDER BY bar OFFSET 0 ROWS FETCH NEXT 10 ROWS WITH TIES\"\n@@ -483,7 +522,9 @@ class TestClickhouse(Validator):\n         self.validate_identity(\n             \"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result = 'break'\"\n         )\n-        self.validate_identity(\"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result_\")\n+        self.validate_identity(\n+            \"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result_\"\n+        )\n         self.validate_identity(\"SELECT * FROM x FORMAT PrettyCompact\")\n         self.validate_identity(\n             \"SELECT * FROM x LIMIT 10 SETTINGS max_results = 100, result_ FORMAT PrettyCompact\"\n@@ -494,11 +535,15 @@ class TestClickhouse(Validator):\n         )\n         self.validate_all(\n             \"SELECT * FROM foo ANY LEFT JOIN bla ON foo.c1 = bla.c2\",\n-            write={\"clickhouse\": \"SELECT * FROM foo LEFT ANY JOIN bla ON foo.c1 = bla.c2\"},\n+            write={\n+                \"clickhouse\": \"SELECT * FROM foo LEFT ANY JOIN bla ON foo.c1 = bla.c2\"\n+            },\n         )\n         self.validate_all(\n             \"SELECT * FROM foo GLOBAL ANY LEFT JOIN bla ON foo.c1 = bla.c2\",\n-            write={\"clickhouse\": \"SELECT * FROM foo GLOBAL LEFT ANY JOIN bla ON foo.c1 = bla.c2\"},\n+            write={\n+                \"clickhouse\": \"SELECT * FROM foo GLOBAL LEFT ANY JOIN bla ON foo.c1 = bla.c2\"\n+            },\n         )\n         self.validate_all(\n             \"\"\"\n@@ -585,19 +630,34 @@ class TestClickhouse(Validator):\n         )\n         self.validate_identity(\"ALTER TABLE visits DROP PARTITION ID '201901'\")\n \n-        self.validate_identity(\"ALTER TABLE visits REPLACE PARTITION 201901 FROM visits_tmp\")\n-        self.validate_identity(\"ALTER TABLE visits REPLACE PARTITION ALL FROM visits_tmp\")\n+        self.validate_identity(\n+            \"ALTER TABLE visits REPLACE PARTITION 201901 FROM visits_tmp\"\n+        )\n+        self.validate_identity(\n+            \"ALTER TABLE visits REPLACE PARTITION ALL FROM visits_tmp\"\n+        )\n         self.validate_identity(\n             \"ALTER TABLE visits REPLACE PARTITION tuple(toYYYYMM(toDate('2019-01-25'))) FROM visits_tmp\"\n         )\n-        self.validate_identity(\"ALTER TABLE visits REPLACE PARTITION ID '201901' FROM visits_tmp\")\n-        self.validate_identity(\"ALTER TABLE visits ON CLUSTER test_cluster DROP COLUMN col1\")\n-        self.validate_identity(\"ALTER TABLE visits ON CLUSTER '{cluster}' DROP COLUMN col1\")\n-        self.validate_identity(\"DELETE FROM tbl ON CLUSTER test_cluster WHERE date = '2019-01-01'\")\n-        self.validate_identity(\"DELETE FROM tbl ON CLUSTER '{cluster}' WHERE date = '2019-01-01'\")\n+        self.validate_identity(\n+            \"ALTER TABLE visits REPLACE PARTITION ID '201901' FROM visits_tmp\"\n+        )\n+        self.validate_identity(\n+            \"ALTER TABLE visits ON CLUSTER test_cluster DROP COLUMN col1\"\n+        )\n+        self.validate_identity(\n+            \"ALTER TABLE visits ON CLUSTER '{cluster}' DROP COLUMN col1\"\n+        )\n+        self.validate_identity(\n+            \"DELETE FROM tbl ON CLUSTER test_cluster WHERE date = '2019-01-01'\"\n+        )\n+        self.validate_identity(\n+            \"DELETE FROM tbl ON CLUSTER '{cluster}' WHERE date = '2019-01-01'\"\n+        )\n \n         self.assertIsInstance(\n-            parse_one(\"Tuple(select Int64)\", into=exp.DataType, read=\"clickhouse\"), exp.DataType\n+            parse_one(\"Tuple(select Int64)\", into=exp.DataType, read=\"clickhouse\"),\n+            exp.DataType,\n         )\n \n         self.validate_identity(\n@@ -616,11 +676,15 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"current_timestamp\").assert_is(exp.Column)\n \n         self.validate_identity(\"SELECT * APPLY(sum) FROM columns_transformers\")\n-        self.validate_identity(\"SELECT COLUMNS('[jk]') APPLY(toString) FROM columns_transformers\")\n+        self.validate_identity(\n+            \"SELECT COLUMNS('[jk]') APPLY(toString) FROM columns_transformers\"\n+        )\n         self.validate_identity(\n             \"SELECT COLUMNS('[jk]') APPLY(toString) APPLY(length) APPLY(max) FROM columns_transformers\"\n         )\n-        self.validate_identity(\"SELECT * APPLY(sum), COLUMNS('col') APPLY(sum) APPLY(avg) FROM t\")\n+        self.validate_identity(\n+            \"SELECT * APPLY(sum), COLUMNS('col') APPLY(sum) APPLY(avg) FROM t\"\n+        )\n         self.validate_identity(\n             \"SELECT * FROM ABC WHERE hasAny(COLUMNS('.*field') APPLY(toUInt64) APPLY(to), (SELECT groupUniqArray(toUInt64(field))))\"\n         )\n@@ -637,7 +701,9 @@ class TestClickhouse(Validator):\n         )\n         self.validate_identity(\"SELECT arrayConcat([1, 2], [3, 4])\")\n \n-        self.validate_identity(\"SELECT parseDateTime('2021-01-04+23:00:00', '%Y-%m-%d+%H:%i:%s')\")\n+        self.validate_identity(\n+            \"SELECT parseDateTime('2021-01-04+23:00:00', '%Y-%m-%d+%H:%i:%s')\"\n+        )\n         self.validate_identity(\n             \"SELECT parseDateTime('2021-01-04+23:00:00', '%Y-%m-%d+%H:%i:%s', 'Asia/Istanbul')\"\n         )\n@@ -686,7 +752,9 @@ class TestClickhouse(Validator):\n         self.validate_identity(\"WITH ['c'] AS field_names SELECT field_names\")\n         self.validate_identity(\"WITH SUM(bytes) AS foo SELECT foo FROM system.parts\")\n         self.validate_identity(\"WITH (SELECT foo) AS bar SELECT bar + 5\")\n-        self.validate_identity(\"WITH test1 AS (SELECT i + 1, j + 1 FROM test1) SELECT * FROM test1\")\n+        self.validate_identity(\n+            \"WITH test1 AS (SELECT i + 1, j + 1 FROM test1) SELECT * FROM test1\"\n+        )\n \n         query = parse_one(\"\"\"WITH (SELECT 1) AS y SELECT * FROM y\"\"\", read=\"clickhouse\")\n         self.assertIsInstance(query.args[\"with\"].expressions[0].this, exp.Subquery)\n@@ -695,14 +763,18 @@ class TestClickhouse(Validator):\n         query = \"WITH 1 AS var SELECT var\"\n         for error_level in [ErrorLevel.IGNORE, ErrorLevel.RAISE, ErrorLevel.IMMEDIATE]:\n             self.assertEqual(\n-                self.parse_one(query, error_level=error_level).sql(dialect=self.dialect),\n+                self.parse_one(query, error_level=error_level).sql(\n+                    dialect=self.dialect\n+                ),\n                 query,\n             )\n \n         self.validate_identity(\"arraySlice(x, 1)\")\n \n     def test_ternary(self):\n-        self.validate_all(\"x ? 1 : 2\", write={\"clickhouse\": \"CASE WHEN x THEN 1 ELSE 2 END\"})\n+        self.validate_all(\n+            \"x ? 1 : 2\", write={\"clickhouse\": \"CASE WHEN x THEN 1 ELSE 2 END\"}\n+        )\n         self.validate_all(\n             \"IF(BAR(col), sign > 0 ? FOO() : 0, 1)\",\n             write={\n@@ -715,7 +787,9 @@ class TestClickhouse(Validator):\n         )\n         self.validate_all(\n             \"x ? (y ? 1 : 2) : 3\",\n-            write={\"clickhouse\": \"CASE WHEN x THEN (CASE WHEN y THEN 1 ELSE 2 END) ELSE 3 END\"},\n+            write={\n+                \"clickhouse\": \"CASE WHEN x THEN (CASE WHEN y THEN 1 ELSE 2 END) ELSE 3 END\"\n+            },\n         )\n         self.validate_all(\n             \"x AND (foo() ? FALSE : TRUE) ? (y ? 1 : 2) : 3\",\n@@ -737,7 +811,9 @@ class TestClickhouse(Validator):\n         self.assertIsInstance(nested_ternary.args[\"true\"], exp.Literal)\n         self.assertIsInstance(nested_ternary.args[\"false\"], exp.Literal)\n \n-        parse_one(\"a and b ? 1 : 2\", read=\"clickhouse\").assert_is(exp.If).this.assert_is(exp.And)\n+        parse_one(\"a and b ? 1 : 2\", read=\"clickhouse\").assert_is(\n+            exp.If\n+        ).this.assert_is(exp.And)\n \n     def test_parameterization(self):\n         self.validate_all(\n@@ -774,7 +850,14 @@ class TestClickhouse(Validator):\n             )\n \n     def test_geom_types(self):\n-        data_types = [\"Point\", \"Ring\", \"LineString\", \"MultiLineString\", \"Polygon\", \"MultiPolygon\"]\n+        data_types = [\n+            \"Point\",\n+            \"Ring\",\n+            \"LineString\",\n+            \"MultiLineString\",\n+            \"Polygon\",\n+            \"MultiPolygon\",\n+        ]\n         for data_type in data_types:\n             with self.subTest(f\"Casting to ClickHouse {data_type}\"):\n                 self.validate_identity(f\"SELECT CAST(val AS {data_type})\")\n@@ -828,17 +911,25 @@ ORDER BY (\n         create_with_cluster = exp.Create(\n             this=db_table_expr,\n             kind=\"DATABASE\",\n-            properties=exp.Properties(expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]),\n+            properties=exp.Properties(\n+                expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]\n+            ),\n+        )\n+        self.assertEqual(\n+            create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\"\n         )\n-        self.assertEqual(create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\")\n \n         # Transpiled CREATE SCHEMA may have OnCluster property set\n         create_with_cluster = exp.Create(\n             this=db_table_expr,\n             kind=\"SCHEMA\",\n-            properties=exp.Properties(expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]),\n+            properties=exp.Properties(\n+                expressions=[exp.OnCluster(this=exp.to_identifier(\"c\"))]\n+            ),\n+        )\n+        self.assertEqual(\n+            create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\"\n         )\n-        self.assertEqual(create_with_cluster.sql(\"clickhouse\"), \"CREATE DATABASE foo ON CLUSTER c\")\n \n         ctas_with_comment = exp.Create(\n             this=exp.table_(\"foo\"),\n@@ -856,12 +947,22 @@ ORDER BY (\n             \"CREATE TABLE foo ENGINE=Memory AS (SELECT * FROM db.other_table) COMMENT 'foo'\",\n         )\n \n-        self.validate_identity(\"CREATE FUNCTION linear_equation AS (x, k, b) -> k * x + b\")\n-        self.validate_identity(\"CREATE MATERIALIZED VIEW a.b TO a.c (c Int32) AS SELECT * FROM a.d\")\n-        self.validate_identity(\"\"\"CREATE TABLE ip_data (ip4 IPv4, ip6 IPv6) ENGINE=TinyLog()\"\"\")\n+        self.validate_identity(\n+            \"CREATE FUNCTION linear_equation AS (x, k, b) -> k * x + b\"\n+        )\n+        self.validate_identity(\n+            \"CREATE MATERIALIZED VIEW a.b TO a.c (c Int32) AS SELECT * FROM a.d\"\n+        )\n+        self.validate_identity(\n+            \"\"\"CREATE TABLE ip_data (ip4 IPv4, ip6 IPv6) ENGINE=TinyLog()\"\"\"\n+        )\n         self.validate_identity(\"\"\"CREATE TABLE dates (dt1 Date32) ENGINE=TinyLog()\"\"\")\n-        self.validate_identity(\"CREATE TABLE named_tuples (a Tuple(select String, i Int64))\")\n-        self.validate_identity(\"\"\"CREATE TABLE t (a String) EMPTY AS SELECT * FROM dummy\"\"\")\n+        self.validate_identity(\n+            \"CREATE TABLE named_tuples (a Tuple(select String, i Int64))\"\n+        )\n+        self.validate_identity(\n+            \"\"\"CREATE TABLE t (a String) EMPTY AS SELECT * FROM dummy\"\"\"\n+        )\n         self.validate_identity(\n             \"CREATE TABLE t1 (a String EPHEMERAL, b String EPHEMERAL func(), c String MATERIALIZED func(), d String ALIAS func()) ENGINE=TinyLog()\"\n         )\n@@ -1217,9 +1318,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         )\n \n         self.assertIsNotNone(\n-            self.validate_identity(\"CREATE TABLE t1 (a String MATERIALIZED func())\").find(\n-                exp.ColumnConstraint\n-            )\n+            self.validate_identity(\n+                \"CREATE TABLE t1 (a String MATERIALIZED func())\"\n+            ).find(exp.ColumnConstraint)\n         )\n \n     def test_agg_functions(self):\n@@ -1227,7 +1328,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n             return parse_one(query, read=\"clickhouse\").selects[0].this\n \n         self.assertIsInstance(\n-            extract_agg_func(\"select quantileGK(100, 0.95) OVER (PARTITION BY id) FROM table\"),\n+            extract_agg_func(\n+                \"select quantileGK(100, 0.95) OVER (PARTITION BY id) FROM table\"\n+            ),\n             exp.AnonymousAggFunc,\n         )\n         self.assertIsInstance(\n@@ -1237,7 +1340,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n             exp.ParameterizedAgg,\n         )\n         self.assertIsInstance(\n-            extract_agg_func(\"select quantileGKIf(100, 0.95) OVER (PARTITION BY id) FROM table\"),\n+            extract_agg_func(\n+                \"select quantileGKIf(100, 0.95) OVER (PARTITION BY id) FROM table\"\n+            ),\n             exp.CombinedAggFunc,\n         )\n         self.assertIsInstance(\n@@ -1253,7 +1358,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         for creatable in (\"DATABASE\", \"TABLE\", \"VIEW\", \"DICTIONARY\", \"FUNCTION\"):\n             with self.subTest(f\"Test DROP {creatable} ON CLUSTER\"):\n                 self.validate_identity(f\"DROP {creatable} test ON CLUSTER test_cluster\")\n-                self.validate_identity(f\"DROP {creatable} test ON CLUSTER '{{cluster}}'\")\n+                self.validate_identity(\n+                    f\"DROP {creatable} test ON CLUSTER '{{cluster}}'\"\n+                )\n \n     def test_datetime_funcs(self):\n         # Each datetime func has an alias that is roundtripped to the original name e.g. (DATE_SUB, DATESUB) -> DATE_SUB\n@@ -1269,7 +1376,11 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n                 )\n \n         # 3-arg functions of type <func>(unit, value, date)\n-        for func in (*datetime_funcs, (\"DATE_DIFF\", \"DATEDIFF\"), (\"TIMESTAMP_SUB\", \"TIMESTAMPSUB\")):\n+        for func in (\n+            *datetime_funcs,\n+            (\"DATE_DIFF\", \"DATEDIFF\"),\n+            (\"TIMESTAMP_SUB\", \"TIMESTAMPSUB\"),\n+        ):\n             func_name = func[0]\n             for func_alias in func:\n                 with self.subTest(f\"Test 3-arg date-time function {func_alias}\"):\n@@ -1298,7 +1409,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n             \"CAST('2020-01-01 00:00:01' AS DateTime64(6))\",\n         )\n         self.assertEqual(\n-            convert(datetime(2020, 1, 1, 0, 0, 1, tzinfo=timezone.utc)).sql(dialect=self.dialect),\n+            convert(datetime(2020, 1, 1, 0, 0, 1, tzinfo=timezone.utc)).sql(\n+                dialect=self.dialect\n+            ),\n             \"CAST('2020-01-01 00:00:01' AS DateTime64(6, 'UTC'))\",\n         )\n \n@@ -1379,7 +1492,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n                 )\n \n     def test_grant(self):\n-        self.validate_identity(\"GRANT SELECT(x, y) ON db.table TO john WITH GRANT OPTION\")\n+        self.validate_identity(\n+            \"GRANT SELECT(x, y) ON db.table TO john WITH GRANT OPTION\"\n+        )\n         self.validate_identity(\"GRANT INSERT(x, y) ON db.table TO john\")\n \n     def test_array_join(self):\n@@ -1401,7 +1516,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         self.assertIsInstance(join.expressions[1].this, exp.Array)\n \n         self.validate_identity(\"SELECT s, arr FROM arrays_test ARRAY JOIN arr\")\n-        self.validate_identity(\"SELECT s, arr, a FROM arrays_test LEFT ARRAY JOIN arr AS a\")\n+        self.validate_identity(\n+            \"SELECT s, arr, a FROM arrays_test LEFT ARRAY JOIN arr AS a\"\n+        )\n         self.validate_identity(\n             \"SELECT s, arr_external FROM arrays_test ARRAY JOIN [1, 2, 3] AS arr_external\"\n         )\n@@ -1424,7 +1541,9 @@ LIFETIME(MIN 0 MAX 0)\"\"\",\n         )\n \n     def test_functions(self):\n-        self.validate_identity(\"SELECT TRANSFORM(foo, [1, 2], ['first', 'second']) FROM table\")\n+        self.validate_identity(\n+            \"SELECT TRANSFORM(foo, [1, 2], ['first', 'second']) FROM table\"\n+        )\n         self.validate_identity(\n             \"SELECT TRANSFORM(foo, [1, 2], ['first', 'second'], 'default') FROM table\"\n         )\ndiff --git a/tests/dialects/test_dialect.py b/tests/dialects/test_dialect.py\nindex 892fcdc..93d3338 100644\n--- a/tests/dialects/test_dialect.py\n+++ b/tests/dialects/test_dialect.py\n@@ -22,7 +22,12 @@ class Validator(unittest.TestCase):\n         return parse_one(sql, read=self.dialect, **kwargs)\n \n     def validate_identity(\n-        self, sql, write_sql=None, pretty=False, check_command_warning=False, identify=False\n+        self,\n+        sql,\n+        write_sql=None,\n+        pretty=False,\n+        check_command_warning=False,\n+        identify=False,\n     ):\n         if check_command_warning:\n             with self.assertLogs(parser_logger) as cm:\n@@ -32,7 +37,8 @@ class Validator(unittest.TestCase):\n             expression = self.parse_one(sql)\n \n         self.assertEqual(\n-            write_sql or sql, expression.sql(dialect=self.dialect, pretty=pretty, identify=identify)\n+            write_sql or sql,\n+            expression.sql(dialect=self.dialect, pretty=pretty, identify=identify),\n         )\n         return expression\n \n@@ -67,7 +73,9 @@ class Validator(unittest.TestCase):\n             with self.subTest(f\"{sql} -> {write_dialect}\"):\n                 if write_sql is UnsupportedError:\n                     with self.assertRaises(UnsupportedError):\n-                        expression.sql(write_dialect, unsupported_level=ErrorLevel.RAISE)\n+                        expression.sql(\n+                            write_dialect, unsupported_level=ErrorLevel.RAISE\n+                        )\n                 else:\n                     self.assertEqual(\n                         expression.sql(\n@@ -113,7 +121,9 @@ class TestDialect(Validator):\n         lowercase_mysql = Dialect.get_or_raise(\"mysql,normalization_strategy=lowercase\")\n         self.assertEqual(lowercase_mysql.normalization_strategy, \"LOWERCASE\")\n \n-        lowercase_mysql = Dialect.get_or_raise(\"mysql, normalization_strategy = lowercase\")\n+        lowercase_mysql = Dialect.get_or_raise(\n+            \"mysql, normalization_strategy = lowercase\"\n+        )\n         self.assertEqual(lowercase_mysql.normalization_strategy.value, \"LOWERCASE\")\n \n         with self.assertRaises(AttributeError) as cm:\n@@ -124,7 +134,9 @@ class TestDialect(Validator):\n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"myqsl\")\n \n-        self.assertEqual(str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\")\n+        self.assertEqual(\n+            str(cm.exception), \"Unknown dialect 'myqsl'. Did you mean mysql?\"\n+        )\n \n         with self.assertRaises(ValueError) as cm:\n             Dialect.get_or_raise(\"asdfjasodiufjsd\")\n@@ -140,7 +152,9 @@ class TestDialect(Validator):\n         class MyDialect(Dialect):\n             SUPPORTED_SETTINGS = {\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"}\n \n-        bool_settings = Dialect.get_or_raise(\"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\")\n+        bool_settings = Dialect.get_or_raise(\n+            \"mydialect, s1=TruE, s2=1, s3=FaLse, s4=0, s5=nonbool\"\n+        )\n         self.assertEqual(\n             bool_settings.settings,\n             {\"s1\": True, \"s2\": True, \"s3\": False, \"s4\": False, \"s5\": \"nonbool\"},\n@@ -450,7 +464,9 @@ class TestDialect(Validator):\n             },\n         )\n         self.validate_all(\"CAST(a AS TINYINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n-        self.validate_all(\"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"})\n+        self.validate_all(\n+            \"CAST(a AS SMALLINT)\", write={\"oracle\": \"CAST(a AS SMALLINT)\"}\n+        )\n         self.validate_all(\"CAST(a AS BIGINT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\"CAST(a AS INT)\", write={\"oracle\": \"CAST(a AS INT)\"})\n         self.validate_all(\n@@ -1411,11 +1427,15 @@ class TestDialect(Validator):\n         )\n \n         order_by_all_sql = \"SELECT * FROM t ORDER BY ALL\"\n-        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(exp.Column)\n+        self.validate_identity(order_by_all_sql).find(exp.Ordered).this.assert_is(\n+            exp.Column\n+        )\n \n         for dialect in (\"duckdb\", \"spark\", \"databricks\"):\n             with self.subTest(f\"Testing ORDER BY ALL in {dialect}\"):\n-                parse_one(order_by_all_sql, read=dialect).find(exp.Ordered).this.assert_is(exp.Var)\n+                parse_one(order_by_all_sql, read=dialect).find(\n+                    exp.Ordered\n+                ).this.assert_is(exp.Var)\n \n     def test_json(self):\n         self.validate_all(\n@@ -1590,9 +1610,13 @@ class TestDialect(Validator):\n         )\n \n         for dialect in (\"duckdb\", \"starrocks\"):\n-            with self.subTest(f\"Generating json extraction with digit-prefixed key ({dialect})\"):\n+            with self.subTest(\n+                f\"Generating json extraction with digit-prefixed key ({dialect})\"\n+            ):\n                 self.assertEqual(\n-                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(dialect=dialect),\n+                    parse_one(\"\"\"select '{\"0\": \"v\"}' -> '0'\"\"\", read=dialect).sql(\n+                        dialect=dialect\n+                    ),\n                     \"\"\"SELECT '{\"0\": \"v\"}' -> '0'\"\"\",\n                 )\n \n@@ -1809,7 +1833,9 @@ class TestDialect(Validator):\n         )\n \n     def test_operators(self):\n-        self.validate_identity(\"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\")\n+        self.validate_identity(\n+            \"some.column LIKE 'foo' || another.column || 'bar' || LOWER(x)\"\n+        )\n         self.validate_identity(\"some.column LIKE 'foo' + another.column + 'bar'\")\n \n         self.validate_all(\"LIKE(x, 'z')\", write={\"\": \"'z' LIKE x\"})\n@@ -2299,7 +2325,9 @@ class TestDialect(Validator):\n         )\n \n     def test_typeddiv(self):\n-        typed_div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True)\n+        typed_div = exp.Div(\n+            this=exp.column(\"a\"), expression=exp.column(\"b\"), typed=True\n+        )\n         div = exp.Div(this=exp.column(\"a\"), expression=exp.column(\"b\"))\n         typed_div_dialect = \"presto\"\n         div_dialect = \"hive\"\n@@ -2324,7 +2352,9 @@ class TestDialect(Validator):\n             (div, (INT, FLOAT), typed_div_dialect, \"a / b\"),\n             (div, (INT, FLOAT), div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {types} {dialect} -> {expected}\"\n+            ):\n                 expression = expression.copy()\n                 expression.left.type = types[0]\n                 expression.right.type = types[1]\n@@ -2342,7 +2372,9 @@ class TestDialect(Validator):\n             (div, safe_div_dialect, \"a / b\"),\n             (div, div_dialect, \"a / b\"),\n         ]:\n-            with self.subTest(f\"{expression.__class__.__name__} {dialect} -> {expected}\"):\n+            with self.subTest(\n+                f\"{expression.__class__.__name__} {dialect} -> {expected}\"\n+            ):\n                 self.assertEqual(expected, expression.sql(dialect=dialect))\n \n         self.assertEqual(\n@@ -2814,7 +2846,8 @@ SELECT\n         self.validate_identity(\"COUNT_IF(DISTINCT cond)\")\n \n         self.validate_all(\n-            \"SELECT COUNT_IF(cond) FILTER\", write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"}\n+            \"SELECT COUNT_IF(cond) FILTER\",\n+            write={\"\": \"SELECT COUNT_IF(cond) AS FILTER\"},\n         )\n         self.validate_all(\n             \"SELECT COUNT_IF(col % 2 = 0) FROM foo\",\n@@ -3038,8 +3071,12 @@ FROM subquery2\"\"\",\n         with_large_nulls = \"postgres\"\n \n         sql = \"SELECT * FROM t ORDER BY c\"\n-        sql_nulls_last = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n-        sql_nulls_first = \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        sql_nulls_last = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END, c\"\n+        )\n+        sql_nulls_first = (\n+            \"SELECT * FROM t ORDER BY CASE WHEN c IS NULL THEN 1 ELSE 0 END DESC, c\"\n+        )\n \n         for read_dialect, desc, nulls_first, expected_sql in (\n             (with_last_nulls, False, None, sql_nulls_last),\n@@ -3072,7 +3109,9 @@ FROM subquery2\"\"\",\n                 )\n \n                 expected_sql = f\"{expected_sql}{sort_order}\"\n-                expression = parse_one(f\"{sql}{sort_order}{null_order}\", read=read_dialect)\n+                expression = parse_one(\n+                    f\"{sql}{sort_order}{null_order}\", read=read_dialect\n+                )\n \n                 self.assertEqual(expression.sql(dialect=\"mysql\"), expected_sql)\n                 self.assertEqual(expression.sql(dialect=\"tsql\"), expected_sql)\n@@ -3157,7 +3196,9 @@ FROM subquery2\"\"\",\n         self.validate_identity(\n             \"CREATE SEQUENCE seq START WITH 1 NO MINVALUE NO MAXVALUE CYCLE NO CACHE\"\n         )\n-        self.validate_identity(\"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\")\n+        self.validate_identity(\n+            \"CREATE OR REPLACE TEMPORARY SEQUENCE seq INCREMENT BY 1 NO CYCLE\"\n+        )\n         self.validate_identity(\n             \"CREATE OR REPLACE SEQUENCE IF NOT EXISTS seq COMMENT='test comment' ORDER\"\n         )\n@@ -3318,7 +3359,9 @@ FROM subquery2\"\"\",\n                     },\n                 )\n \n-        self.assertIsInstance(parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var)\n+        self.assertIsInstance(\n+            parse_one(\"NORMALIZE('str', NFD)\").args.get(\"form\"), exp.Var\n+        )\n \n     def test_coalesce(self):\n         \"\"\"\n@@ -3425,7 +3468,9 @@ FROM subquery2\"\"\",\n \n     def test_escaped_identifier_delimiter(self):\n         for dialect in (\"databricks\", \"hive\", \"mysql\", \"spark2\", \"spark\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3447,7 +3492,9 @@ FROM subquery2\"\"\",\n             \"snowflake\",\n             \"sqlite\",\n         ):\n-            with self.subTest(f\"Testing escaped double-quote in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped double-quote in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x\"\"\"',\n                     read={\n@@ -3459,7 +3506,9 @@ FROM subquery2\"\"\",\n                 )\n \n         for dialect in (\"clickhouse\", \"sqlite\"):\n-            with self.subTest(f\"Testing escaped backtick in identifier name for {dialect}\"):\n+            with self.subTest(\n+                f\"Testing escaped backtick in identifier name for {dialect}\"\n+            ):\n                 self.validate_all(\n                     'SELECT 1 AS \"x`\"',\n                     read={\n@@ -3569,14 +3618,19 @@ FROM subquery2\"\"\",\n                 \"spark\",\n                 \"redshift\",\n             ):\n-                with self.subTest(f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"):\n+                with self.subTest(\n+                    f\"Testing hex string -> INTEGER evaluation for {read_dialect}\"\n+                ):\n                     self.assertEqual(\n-                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect), \"SELECT 204\"\n+                        parse_one(\"SELECT 0xCC\", read=read_dialect).sql(write_dialect),\n+                        \"SELECT 204\",\n                     )\n \n             for other_integer_dialects in integer_dialects:\n                 self.assertEqual(\n-                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(other_integer_dialects),\n+                    parse_one(\"SELECT 0xCC\", read=read_dialect).sql(\n+                        other_integer_dialects\n+                    ),\n                     \"SELECT 0xCC\",\n                 )\n \n"
    }
]