{
    "sha_fail": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
    "changed_files": [
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/__init__.py",
            "diff": "diff --git a/cookbook/models/vllm/__init__.py b/cookbook/models/vllm/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/async_basic.py",
            "diff": "diff --git a/cookbook/models/vllm/async_basic.py b/cookbook/models/vllm/async_basic.py\nnew file mode 100644\nindex 000000000..15f8292ac\n--- /dev/null\n+++ b/cookbook/models/vllm/async_basic.py\n@@ -0,0 +1,7 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+\n+agent = Agent(model=vLLM(id=\"Qwen/Qwen2.5-7B-Instruct\"), markdown=True)\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\"))\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/vllm/async_basic_stream.py b/cookbook/models/vllm/async_basic_stream.py\nnew file mode 100644\nindex 000000000..cec43d9a9\n--- /dev/null\n+++ b/cookbook/models/vllm/async_basic_stream.py\n@@ -0,0 +1,7 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+\n+agent = Agent(model=vLLM(id=\"Qwen/Qwen2.5-7B-Instruct\"), markdown=True)\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/vllm/async_tool_use.py b/cookbook/models/vllm/async_tool_use.py\nnew file mode 100644\nindex 000000000..939584d63\n--- /dev/null\n+++ b/cookbook/models/vllm/async_tool_use.py\n@@ -0,0 +1,15 @@\n+\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=vLLM(id=\"Qwen/Qwen2.5-7B-Instruct\", top_k=20, enable_thinking=False),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\", stream=True))\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/basic.py",
            "diff": "diff --git a/cookbook/models/vllm/basic.py b/cookbook/models/vllm/basic.py\nnew file mode 100644\nindex 000000000..455f03a33\n--- /dev/null\n+++ b/cookbook/models/vllm/basic.py\n@@ -0,0 +1,9 @@\n+from agno.agent import Agent, RunResponse\n+from agno.models.vllm import vLLM\n+\n+agent = Agent(\n+    model=vLLM(id=\"Qwen/Qwen2.5-7B-Instruct\", top_k=20, enable_thinking=False),\n+    markdown=True,\n+)\n+\n+agent.print_response(\"Share a 2 sentence horror story\")\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/basic_stream.py",
            "diff": "diff --git a/cookbook/models/vllm/basic_stream.py b/cookbook/models/vllm/basic_stream.py\nnew file mode 100644\nindex 000000000..b530bba22\n--- /dev/null\n+++ b/cookbook/models/vllm/basic_stream.py\n@@ -0,0 +1,8 @@\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+\n+agent = Agent(\n+    model=vLLM(id=\"Qwen/Qwen2.5-7B-Instruct\", top_k=20, enable_thinking=False),\n+    markdown=True,\n+)\n+agent.print_response(\"Share a 2 sentence horror story\", stream=True)\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/code_generation.py",
            "diff": "diff --git a/cookbook/models/vllm/code_generation.py b/cookbook/models/vllm/code_generation.py\nnew file mode 100644\nindex 000000000..11a9611d8\n--- /dev/null\n+++ b/cookbook/models/vllm/code_generation.py\n@@ -0,0 +1,18 @@\n+\"\"\"Code generation example with DeepSeek-Coder.\n+Run vLLM model: vllm serve deepseek-ai/deepseek-coder-6.7b-instruct \\\n+        --dtype float32 \\\n+        --tool-call-parser pythonic\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+\n+agent = Agent(\n+    model=vLLM(id=\"deepseek-ai/deepseek-coder-6.7b-instruct\"),\n+    description=\"You are an expert Python developer.\",\n+    markdown=True,\n+)\n+\n+agent.print_response(\n+    \"Write a Python function that returns the nth Fibonacci number using dynamic programming.\"\n+)\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/memory.py",
            "diff": "diff --git a/cookbook/models/vllm/memory.py b/cookbook/models/vllm/memory.py\nnew file mode 100644\nindex 000000000..0acf19532\n--- /dev/null\n+++ b/cookbook/models/vllm/memory.py\n@@ -0,0 +1,58 @@\n+\"\"\"\n+Personalized memory and session summaries with vLLM.\n+Prerequisites:\n+1. Start a Postgres + pgvector container (helper script is provided):\n+       ./cookbook/scripts/run_pgvector.sh\n+2. Install dependencies:\n+       pip install sqlalchemy 'psycopg[binary]' pgvector\n+3. Run a vLLM server (any open model).  Example with Phi-3:\n+       vllm serve microsoft/Phi-3-mini-128k-instruct \\\n+         --dtype float32 \\\n+         --enable-auto-tool-choice \\\n+         --tool-call-parser pythonic\n+Then execute this script \u2013 it will remember facts you tell it and generate a\n+summary.\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.memory.v2.db.postgres import PostgresMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.vllm import vLLM\n+from agno.storage.postgres import PostgresStorage\n+\n+# Change this if your Postgres container is running elsewhere\n+DB_URL = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent = Agent(\n+    model=vLLM(id=\"microsoft/Phi-3-mini-128k-instruct\"),\n+    memory=Memory(\n+        db=PostgresMemoryDb(table_name=\"agent_memory\", db_url=DB_URL),\n+    ),\n+    enable_user_memories=True,\n+    enable_session_summaries=True,\n+    storage=PostgresStorage(table_name=\"personalized_agent_sessions\", db_url=DB_URL),\n+)\n+\n+# Share personal details; the agent should remember them.\n+agent.print_response(\"My name is John Billings.\", stream=True)\n+print(\"Current memories \u2192\")\n+pprint(agent.memory.memories)\n+print(\"Current summary \u2192\")\n+pprint(agent.memory.summaries)\n+\n+agent.print_response(\"I live in NYC.\", stream=True)\n+print(\"Memories \u2192\")\n+pprint(agent.memory.memories)\n+print(\"Summary \u2192\")\n+pprint(agent.memory.summaries)\n+\n+agent.print_response(\"I'm going to a concert tomorrow.\", stream=True)\n+print(\"Memories \u2192\")\n+pprint(agent.memory.memories)\n+print(\"Summary \u2192\")\n+pprint(agent.memory.summaries)\n+\n+# Ask the agent to recall\n+agent.print_response(\n+    \"What have we been talking about, do you know my name?\", stream=True\n+)\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/storage.py",
            "diff": "diff --git a/cookbook/models/vllm/storage.py b/cookbook/models/vllm/storage.py\nnew file mode 100644\nindex 000000000..c1ce1ae98\n--- /dev/null\n+++ b/cookbook/models/vllm/storage.py\n@@ -0,0 +1,18 @@\n+\"\"\"Run `pip install sqlalchemy` and ensure Postgres is running (`./cookbook/scripts/run_pgvector.sh`).\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+from agno.storage.postgres import PostgresStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+DB_URL = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent = Agent(\n+    model=vLLM(id=\"Qwen/Qwen2.5-7B-Instruct\"),\n+    storage=PostgresStorage(table_name=\"agent_sessions\", db_url=DB_URL),\n+    tools=[DuckDuckGoTools()],\n+    add_history_to_messages=True,\n+)\n+\n+agent.print_response(\"How many people live in Canada?\")\n+agent.print_response(\"What is their national anthem called?\")\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/structured_output.py",
            "diff": "diff --git a/cookbook/models/vllm/structured_output.py b/cookbook/models/vllm/structured_output.py\nnew file mode 100644\nindex 000000000..1b092ed0b\n--- /dev/null\n+++ b/cookbook/models/vllm/structured_output.py\n@@ -0,0 +1,35 @@\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+from pydantic import BaseModel, Field\n+\n+\n+class MovieScript(BaseModel):\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+\n+\n+agent = Agent(\n+    model=vLLM(\n+        id=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\", top_k=20, enable_thinking=False\n+    ),\n+    description=\"You write movie scripts.\",\n+    response_model=MovieScript,\n+)\n+\n+agent.print_response(\"Llamas ruling the world\")\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "cookbook/models/vllm/tool_use.py",
            "diff": "diff --git a/cookbook/models/vllm/tool_use.py b/cookbook/models/vllm/tool_use.py\nnew file mode 100644\nindex 000000000..5549c741f\n--- /dev/null\n+++ b/cookbook/models/vllm/tool_use.py\n@@ -0,0 +1,15 @@\n+\"\"\"Build a Web Search Agent using xAI.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.vllm import vLLM\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=vLLM(\n+        id=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\", top_k=20, enable_thinking=False\n+    ),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+agent.print_response(\"Whats happening in France?\", stream=True)\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "libs/agno/agno/models/vllm/__init__.py",
            "diff": "diff --git a/libs/agno/agno/models/vllm/__init__.py b/libs/agno/agno/models/vllm/__init__.py\nnew file mode 100644\nindex 000000000..8884df38c\n--- /dev/null\n+++ b/libs/agno/agno/models/vllm/__init__.py\n@@ -0,0 +1,3 @@\n+from agno.models.vllm.vllm import vLLM\n+\n+__all__ = [\"vLLM\"]\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "libs/agno/agno/models/vllm/vllm.py",
            "diff": "diff --git a/libs/agno/agno/models/vllm/vllm.py b/libs/agno/agno/models/vllm/vllm.py\nnew file mode 100644\nindex 000000000..040df1d05\n--- /dev/null\n+++ b/libs/agno/agno/models/vllm/vllm.py\n@@ -0,0 +1,74 @@\n+from dataclasses import dataclass\n+from os import getenv\n+from typing import Any, Dict, List, Optional, Type, Union\n+\n+from pydantic import BaseModel\n+\n+from agno.models.openai.like import OpenAILike\n+\n+\n+@dataclass\n+class vLLM(OpenAILike):\n+    \"\"\"\n+    Class for interacting with vLLM models via OpenAI-compatible API.\n+\n+    Attributes:\n+        id: Model identifier\n+        name: API name\n+        provider: API provider\n+        base_url: vLLM server URL\n+        temperature: Sampling temperature\n+        top_p: Nucleus sampling probability\n+        presence_penalty: Repetition penalty\n+        top_k: Top-k sampling\n+        enable_thinking: Special mode flag\n+    \"\"\"\n+\n+    id: str = \"not-set\"\n+    name: str = \"vLLM\"\n+    provider: str = \"vLLM\"\n+\n+    api_key: Optional[str] = getenv(\"VLLM_API_KEY\") or \"EMPTY\"\n+    base_url: Optional[str] = getenv(\"VLLM_BASE_URL\", \"http://localhost:8000/v1/\")\n+\n+    temperature: float = 0.7\n+    top_p: float = 0.8\n+    presence_penalty: float = 1.5\n+    top_k: Optional[int] = None\n+    enable_thinking: Optional[bool] = None\n+\n+    def __post_init__(self):\n+        \"\"\"Validate required configuration\"\"\"\n+        if not self.base_url:\n+            raise ValueError(\"VLLM_BASE_URL must be set via environment variable or explicit initialization\")\n+        if self.id == \"not-set\":\n+            raise ValueError(\"Model ID must be set via environment variable or explicit initialization\")\n+\n+        body: Dict[str, Any] = {}\n+        if self.top_k is not None:\n+            body[\"top_k\"] = self.top_k\n+        if self.enable_thinking is not None:\n+            body[\"chat_template_kwargs\"] = {\"enable_thinking\": self.enable_thinking}\n+        self.extra_body = body or None\n+\n+    def get_request_kwargs(\n+        self,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        tools: Optional[List[Dict[str, Any]]] = None,\n+        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n+    ) -> Dict[str, Any]:\n+        request_kwargs = super().get_request_kwargs(\n+            response_format=response_format, tools=tools, tool_choice=tool_choice\n+        )\n+\n+        vllm_body: Dict[str, Any] = {}\n+        if self.top_k is not None:\n+            vllm_body[\"top_k\"] = self.top_k\n+        if self.enable_thinking is not None:\n+            vllm_body.setdefault(\"chat_template_kwargs\", {})[\"enable_thinking\"] = self.enable_thinking\n+\n+        if vllm_body:\n+            existing_body = request_kwargs.get(\"extra_body\") or {}\n+            request_kwargs[\"extra_body\"] = {**existing_body, **vllm_body}\n+\n+        return request_kwargs\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "libs/agno/tests/integration/models/vllm/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/vllm/test_basic.py b/libs/agno/tests/integration/models/vllm/test_basic.py\nnew file mode 100644\nindex 000000000..f56535acf\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/vllm/test_basic.py\n@@ -0,0 +1,172 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.exceptions import ModelProviderError\n+from agno.models.vllm import vLLM\n+from agno.storage.sqlite import SqliteStorage\n+\n+# Use default model id or override via env var\n+VLLM_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(model=vLLM(id=VLLM_MODEL_ID), markdown=True, telemetry=False, monitoring=False)\n+\n+    # Print the response in the terminal\n+    response: RunResponse = agent.run(\"Share a 2 sentence comedy story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(model=vLLM(id=VLLM_MODEL_ID), markdown=True, telemetry=False, monitoring=False)\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(model=vLLM(id=VLLM_MODEL_ID), markdown=True, telemetry=False, monitoring=False)\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(model=vLLM(id=VLLM_MODEL_ID), markdown=True, telemetry=False, monitoring=False)\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        add_history_to_messages=True,\n+        num_history_responses=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_response_model():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_json_response_mode():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        response_model=MovieScript,\n+        use_json_mode=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_history():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        storage=SqliteStorage(table_name=\"agent_sessions\", db_file=\"tmp/agent_storage.db\"),\n+        add_history_to_messages=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    agent.run(\"Hello\")\n+    assert len(agent.run_response.messages) == 2\n+    agent.run(\"Hello 2\")\n+    assert len(agent.run_response.messages) == 4\n+    agent.run(\"Hello 3\")\n+    assert len(agent.run_response.messages) == 6\n+    agent.run(\"Hello 4\")\n+    assert len(agent.run_response.messages) == 8\n+\n+\n+def test_exception():\n+    agent = Agent(model=vLLM(id=\"invalid-model-id\"), markdown=True, telemetry=False, monitoring=False)\n+    with pytest.raises(ModelProviderError):\n+        agent.run(\"Test vLLM exception\")\n"
        },
        {
            "commit": "8c8e822266755d103faa9166aaa4ab3f2aa3e578",
            "file_path": "libs/agno/tests/integration/models/vllm/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/vllm/test_tool_use.py b/libs/agno/tests/integration/models/vllm/test_tool_use.py\nnew file mode 100644\nindex 000000000..06dc7e79f\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/vllm/test_tool_use.py\n@@ -0,0 +1,221 @@\n+from typing import Optional\n+\n+import pytest\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.vllm import vLLM\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+VLLM_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[YFinanceTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[YFinanceTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What is the current price of TSLA?\", stream=True, stream_intermediate_steps=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.tool_name for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"TSLA\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[YFinanceTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What is the current price of TSLA?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[YFinanceTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\n+        \"What is the current price of TSLA?\", stream=True, stream_intermediate_steps=True\n+    )\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.tool_name for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"TSLA\" in r.content for r in responses if r.content)\n+\n+\n+def test_parallel_tool_calls():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[YFinanceTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA and AAPL?\")\n+\n+    # Verify tool usage\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    assert len([call for call in tool_calls if call.get(\"type\", \"\") == \"function\"]) == 2  # Total of 2 tool calls made\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content and \"AAPL\" in response.content\n+\n+\n+def test_multiple_tool_calls():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[YFinanceTools(cache_results=True), DuckDuckGoTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA and what is the latest news about it?\")\n+\n+    # Verify tool usage\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    assert len([call for call in tool_calls if call.get(\"type\", \"\") == \"function\"]) == 2  # Total of 2 tool calls made\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content and \"latest news\" in response.content.lower()\n+\n+\n+def test_tool_call_custom_tool_no_parameters():\n+    def get_the_weather_in_tokyo():\n+        \"\"\"\n+        Get the weather in Tokyo\n+        \"\"\"\n+        return \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[get_the_weather_in_tokyo],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"70\" in response.content\n+\n+\n+def test_tool_call_custom_tool_optional_parameters():\n+    def get_the_weather(city: Optional[str] = None):\n+        \"\"\"\n+        Get the weather in a city\n+\n+        Args:\n+            city: The city to get the weather for\n+        \"\"\"\n+        if city is None:\n+            return \"It is currently 70 degrees and cloudy in Tokyo\"\n+        else:\n+            return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[get_the_weather],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Paris?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"70\" in response.content\n+\n+\n+def test_tool_call_list_parameters():\n+    agent = Agent(\n+        model=vLLM(id=VLLM_MODEL_ID),\n+        tools=[ExaTools()],\n+        instructions=\"Use a single tool call if possible\",\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"What are the papers at https://arxiv.org/pdf/2307.06435 and https://arxiv.org/pdf/2502.09601 about?\"\n+    )\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    for call in tool_calls:\n+        if call.get(\"type\", \"\") == \"function\":\n+            assert call[\"function\"][\"name\"] in [\"get_contents\", \"exa_answer\", \"search_exa\"]\n+    assert response.content is not None\n"
        }
    ]
}