{
    "sha_fail": "ede117552a48bee7f674fbfab87d9586f2fabe19",
    "changed_files": [
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/agentic_user_input.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/agentic_user_input.py b/cookbook/agent_concepts/user_control_flows/agentic_user_input.py\nnew file mode 100644\nindex 000000000..a62354bde\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/agentic_user_input.py\n@@ -0,0 +1,111 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Allowing users to provide input externally\n+\n+This example shows how to use the UserControlFlowTools to allow the agent to get user input dynamically.\n+If the agent doesn't have enough information to complete a task, it will use the toolkit to get the information it needs from the user.\n+\"\"\"\n+\n+from typing import Any, Dict\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.tools.toolkit import Toolkit\n+from agno.tools.user_control_flow import UserControlFlowTools\n+from agno.utils import pprint\n+\n+\n+class EmailTools(Toolkit):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(\n+            name=\"EmailTools\", tools=[self.send_email, self.get_emails], *args, **kwargs\n+        )\n+\n+    def send_email(self, subject: str, body: str, to_address: str) -> str:\n+        \"\"\"Send an email to the given address with the given subject and body.\"\"\"\n+        return f\"Sent email to {to_address} with subject {subject} and body {body}\"\n+\n+    def get_emails(self, date_from: str, date_to: str) -> str:\n+        return [\n+            {\n+                \"subject\": \"Hello\",\n+                \"body\": \"Hello, world!\",\n+                \"to_address\": \"test@test.com\",\n+                \"date\": date_from,\n+            },\n+            {\n+                \"subject\": \"Random other email\",\n+                \"body\": \"This is a random other email\",\n+                \"to_address\": \"john@doe.com\",\n+                \"date\": date_to,\n+            },\n+        ]\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[EmailTools(), UserControlFlowTools()],\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+run_response = agent.run(\"Send an email with the body 'What is the weather in Tokyo?'\")\n+while run_response.is_paused:\n+    for tool in run_response.tools_requiring_user_input:\n+        input_schema: Dict[str, Any] = tool.user_input_schema\n+\n+        for field in input_schema:\n+            # Get user input for each field in the schema\n+            field_type = field.field_type\n+            field_description = field.description\n+\n+            # Display field information to the user\n+            print(f\"\\nField: {field.name}\")\n+            print(f\"Description: {field_description}\")\n+            print(f\"Type: {field_type}\")\n+\n+            # Get user input\n+            if field.value is None:\n+                user_value = input(f\"Please enter a value for {field.name}: \")\n+            else:\n+                print(f\"Value: {field.value}\")\n+                user_value = field.value\n+\n+            # Update the field value\n+            field.value = user_value\n+\n+    run_response = agent.continue_run(run_response=run_response)\n+    if not run_response.is_paused:\n+        pprint.pprint_run_response(run_response)\n+        break\n+\n+\n+run_response = agent.run(\"Get me all my emails\")\n+\n+while run_response.is_paused:\n+    for tool in run_response.tools_requiring_user_input:\n+        input_schema: Dict[str, Any] = tool.user_input_schema\n+\n+        for field in input_schema:\n+            # Get user input for each field in the schema\n+            field_type = field.field_type\n+            field_description = field.description\n+\n+            # Display field information to the user\n+            print(f\"\\nField: {field.name}\")\n+            print(f\"Description: {field_description}\")\n+            print(f\"Type: {field_type}\")\n+\n+            # Get user input\n+            if field.value is None:\n+                user_value = input(f\"Please enter a value for {field.name}: \")\n+            else:\n+                print(f\"Value: {field.value}\")\n+                user_value = field.value\n+\n+            # Update the field value\n+            field.value = user_value\n+\n+    run_response = agent.continue_run(run_response=run_response)\n+    if not run_response.is_paused:\n+        pprint.pprint_run_response(run_response)\n+        break\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required.py b/cookbook/agent_concepts/user_control_flows/confirmation_required.py\nindex c23da64c2..2ea5c930f 100644\n--- a/cookbook/agent_concepts/user_control_flows/confirmation_required.py\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required.py\n@@ -60,7 +60,7 @@ agent = Agent(\n     markdown=True,\n )\n \n-agent.run(\"Fetch the top 2 hackernews stories\")\n+agent.run(\"Fetch the top 2 hackernews stories.\")\n if agent.is_paused:  # Or agent.run_response.is_paused\n     for tool in agent.run_response.tools_requiring_confirmation:\n         # Ask for confirmation\n@@ -74,15 +74,19 @@ if agent.is_paused:  # Or agent.run_response.is_paused\n         )\n \n         if message == \"n\":\n-            break\n+            tool.confirmed = False\n         else:\n             # We update the tools in place\n             tool.confirmed = True\n \n-    run_response = (\n-        agent.continue_run()\n-    )  # or agent.continue_run(run_response=agent.run_response)\n-    pprint.pprint_run_response(run_response)\n+run_response = agent.continue_run()\n+# Or\n+# run_response = agent.continue_run(run_id=run_response.run_id)\n+# Or\n+# run_response = agent.continue_run(run_response=run_response)\n+\n+pprint.pprint_run_response(run_response)\n+\n \n # Or for simple debug flow\n # agent.print_response(\"Fetch the top 2 hackernews stories\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_async.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py\nindex ef012c242..b0059e25b 100644\n--- a/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py\n@@ -75,13 +75,19 @@ if run_response.is_paused:\n         )\n \n         if message == \"n\":\n-            break\n+            tool.confirmed = False\n         else:\n             # We update the tools in place\n             tool.confirmed = True\n \n-    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))\n-    pprint.pprint_run_response(run_response)\n+\n+run_response = asyncio.run(agent.acontinue_run(run_response=run_response))\n+# Or\n+# run_response = asyncio.run(agent.acontinue_run(run_id=run_response.run_id))\n+# Or\n+# run_response = asyncio.run(agent.acontinue_run())\n+\n+pprint.pprint_run_response(run_response)\n \n \n # Or for simple debug flow\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_mixed_tools.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_mixed_tools.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_mixed_tools.py\nnew file mode 100644\nindex 000000000..ac0779c83\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_mixed_tools.py\n@@ -0,0 +1,99 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+\n+In this case we have multiple tools and only one of them requires confirmation.\n+\n+The agent should execute the tool that doesn't require confirmation and then pause for user confirmation.\n+\n+The user can then either approve or reject the tool call and the agent should continue from where it left off.\n+\"\"\"\n+\n+import json\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n+\n+    Args:\n+        num_stories (int): Number of stories to retrieve\n+\n+    Returns:\n+        str: JSON string containing story details\n+    \"\"\"\n+    # Fetch top story IDs\n+    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+    story_ids = response.json()\n+\n+    # Yield story details\n+    all_stories = []\n+    for story_id in story_ids[:num_stories]:\n+        story_response = httpx.get(\n+            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+        )\n+        story = story_response.json()\n+        if \"text\" in story:\n+            story.pop(\"text\", None)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n+\n+\n+@tool(requires_confirmation=True)\n+def send_email(to: str, subject: str, body: str) -> str:\n+    \"\"\"Send an email.\n+\n+    Args:\n+        to (str): Email address to send to\n+        subject (str): Subject of the email\n+        body (str): Body of the email\n+    \"\"\"\n+    return f\"Email sent to {to} with subject {subject} and body {body}\"\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[get_top_hackernews_stories, send_email],\n+    markdown=True,\n+)\n+\n+run_response = agent.run(\n+    \"Fetch the top 2 hackernews stories and email them to john@doe.com.\"\n+)\n+if run_response.is_paused:  # Or agent.run_response.is_paused\n+    for tool in run_response.tools:\n+        if tool.requires_confirmation:\n+            # Ask for confirmation\n+            console.print(\n+                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+            )\n+            message = (\n+                Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+                .strip()\n+                .lower()\n+            )\n+\n+            if message == \"n\":\n+                tool.confirmed = False\n+            else:\n+                # We update the tools in place\n+                tool.confirmed = True\n+        else:\n+            console.print(\n+                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] was completed in [bold green]{tool.metrics.time:.2f}[/] seconds.\"\n+            )\n+\n+    run_response = agent.continue_run()\n+    pprint.pprint_run_response(run_response)\n+\n+# Or for simple debug flow\n+# agent.print_response(\"Fetch the top 2 hackernews stories\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py\nindex a5f999f86..fa028067b 100644\n--- a/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py\n@@ -1,12 +1,17 @@\n \"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n \n This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Handle user confirmation during tool execution\n+- Gracefully cancel operations based on user choice\n \n-In this case we have multiple tools and only one of them requires confirmation.\n+Some practical applications:\n+- Confirming sensitive operations before execution\n+- Reviewing API calls before they're made\n+- Validating data transformations\n+- Approving automated actions in critical systems\n \n-The agent should execute the tool that doesn't require confirmation and then pause for user confirmation.\n-\n-The user can then either approve or reject the tool call and the agent should continue from where it left off.\n+Run `pip install openai httpx rich agno` to install dependencies.\n \"\"\"\n \n import json\n@@ -15,6 +20,7 @@ import httpx\n from agno.agent import Agent\n from agno.models.openai import OpenAIChat\n from agno.tools import tool\n+from agno.tools.wikipedia import WikipediaTools\n from agno.utils import pprint\n from rich.console import Console\n from rich.prompt import Prompt\n@@ -22,6 +28,7 @@ from rich.prompt import Prompt\n console = Console()\n \n \n+@tool(requires_confirmation=True)\n def get_top_hackernews_stories(num_stories: int) -> str:\n     \"\"\"Fetch top stories from Hacker News.\n \n@@ -48,52 +55,38 @@ def get_top_hackernews_stories(num_stories: int) -> str:\n     return json.dumps(all_stories)\n \n \n-@tool(requires_confirmation=True)\n-def send_email(to: str, subject: str, body: str) -> str:\n-    \"\"\"Send an email.\n-\n-    Args:\n-        to (str): Email address to send to\n-        subject (str): Subject of the email\n-        body (str): Body of the email\n-    \"\"\"\n-    return f\"Email sent to {to} with subject {subject} and body {body}\"\n-\n-\n agent = Agent(\n     model=OpenAIChat(id=\"gpt-4o-mini\"),\n-    tools=[get_top_hackernews_stories, send_email],\n+    tools=[\n+        get_top_hackernews_stories,\n+        WikipediaTools(requires_confirmation_tools=[\"search_wikipedia\"]),\n+    ],\n     markdown=True,\n+    debug_mode=True,\n )\n \n run_response = agent.run(\n-    \"Fetch the top 2 hackernews stories and email them to john@doe.com.\"\n+    \"Fetch 2 articles about the topic 'python'. You can choose which source to use, but only use one source.\"\n )\n-if run_response.is_paused:  # Or agent.run_response.is_paused\n-    for tool in run_response.tools:\n-        if tool.requires_confirmation:\n-            # Ask for confirmation\n-            console.print(\n-                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n-            )\n-            message = (\n-                Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n-                .strip()\n-                .lower()\n-            )\n+while run_response.is_paused:  # Or agent.run_response.is_paused\n+    for tool in agent.run_response.tools_requiring_confirmation:\n+        # Ask for confirmation\n+        console.print(\n+            f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+        )\n+        message = (\n+            Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+            .strip()\n+            .lower()\n+        )\n \n-            if message == \"n\":\n-                break\n-            else:\n-                # We update the tools in place\n-                tool.confirmed = True\n-        else:\n-            console.print(\n-                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] was completed in [bold green]{tool.metrics.time:.2f}[/] seconds.\"\n+        if message == \"n\":\n+            tool.confirmed = False\n+            tool.confirmation_note = (\n+                \"This is not the right tool to use. Use the other tool!\"\n             )\n+        else:\n+            # We update the tools in place\n+            tool.confirmed = True\n \n     run_response = agent.continue_run()\n-    pprint.pprint_run_response(run_response)\n-\n-# Or for simple debug flow\n-# agent.print_response(\"Fetch the top 2 hackernews stories\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py\nindex becd91c58..2f23325e6 100644\n--- a/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py\n@@ -74,7 +74,7 @@ for run_response in agent.run(\"Fetch the top 2 hackernews stories\", stream=True)\n             )\n \n             if message == \"n\":\n-                break\n+                tool.confirmed = False\n             else:\n                 # We update the tools in place\n                 tool.confirmed = True\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py\nindex e268c5ecb..9b565bc8f 100644\n--- a/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py\n@@ -80,7 +80,7 @@ async def main():\n                 )\n \n                 if message == \"n\":\n-                    break\n+                    tool.confirmed = False\n                 else:\n                     # We update the tools in place\n                     tool.confirmed = True\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py\nindex 79753fec9..512f6bac4 100644\n--- a/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py\n@@ -44,7 +44,7 @@ if agent.is_paused:  # Or agent.run_response.is_paused\n         )\n \n         if message == \"n\":\n-            break\n+            tool.confirmed = False\n         else:\n             # We update the tools in place\n             tool.confirmed = True\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/demo/teams/reasoning_finance_team.py",
            "diff": "diff --git a/cookbook/demo/teams/reasoning_finance_team.py b/cookbook/demo/teams/reasoning_finance_team.py\nindex c3f8c04df..5f0b6d1ad 100644\n--- a/cookbook/demo/teams/reasoning_finance_team.py\n+++ b/cookbook/demo/teams/reasoning_finance_team.py\n@@ -22,9 +22,11 @@ memory = Memory(\n )\n # *******************************\n \n+# ************* Core Agents *************\n web_agent = Agent(\n     name=\"Web Search Agent\",\n-    role=\"Handle web search requests\",\n+    role=\"Handle web search requests and general research\",\n+    agent_id=\"web_agent\",\n     model=OpenAIChat(id=\"gpt-4.1\"),\n     tools=[DuckDuckGoTools()],\n     storage=PostgresAgentStorage(\n@@ -32,17 +34,29 @@ web_agent = Agent(\n         table_name=\"web_agent_sessions\",\n     ),\n     memory=memory,\n-    add_memory_references=True,\n-    instructions=\"Always include sources\",\n+    instructions=[\n+        \"Search for current and relevant information on financial topics\",\n+        \"Always include sources and publication dates\",\n+        \"Focus on reputable financial news sources\",\n+        \"Provide context and background information\",\n+    ],\n     add_datetime_to_instructions=True,\n )\n \n finance_agent = Agent(\n     name=\"Finance Agent\",\n-    role=\"Handle financial data requests\",\n+    role=\"Handle financial data requests and market analysis\",\n+    agent_id=\"finance_agent\",\n     model=OpenAIChat(id=\"gpt-4.1\"),\n     tools=[\n-        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)\n+        YFinanceTools(\n+            stock_price=True,\n+            company_info=True,\n+            stock_fundamentals=True,\n+            key_financial_ratios=True,\n+            analyst_recommendations=True,\n+            company_news=True,\n+        )\n     ],\n     storage=PostgresAgentStorage(\n         db_url=db_url,\n@@ -50,30 +64,37 @@ finance_agent = Agent(\n     ),\n     memory=memory,\n     instructions=[\n-        \"You are a financial data specialist. Provide concise and accurate data.\",\n-        \"Use tables to display stock prices, fundamentals (P/E, Market Cap), and recommendations.\",\n+        \"You are a financial data specialist. Provide comprehensive and accurate financial data.\",\n+        \"Use tables to display stock prices, fundamentals (P/E, Market Cap, Revenue), and recommendations.\",\n         \"Clearly state the company name and ticker symbol.\",\n-        \"Briefly summarize recent company-specific news if available.\",\n-        \"Focus on delivering the requested financial data points clearly.\",\n+        \"Include key financial ratios and metrics in your analysis.\",\n+        \"Focus on delivering actionable financial insights.\",\n     ],\n-    add_memory_references=True,\n     add_datetime_to_instructions=True,\n )\n+# *******************************\n \n \n def get_reasoning_finance_team():\n     return Team(\n         name=\"Reasoning Finance Team\",\n         mode=\"coordinate\",\n-        model=Claude(id=\"claude-3-7-sonnet-latest\"),\n+        team_id=\"reasoning_finance_team\",\n+        model=Claude(id=\"claude-sonnet-4-20250514\"),\n         members=[\n             web_agent,\n             finance_agent,\n         ],\n         tools=[ReasoningTools(add_instructions=True)],\n         instructions=[\n-            \"Use tables to display data\",\n-            \"Only output the final answer, no other text.\",\n+            \"Collaborate to provide comprehensive financial and investment insights\",\n+            \"Consider both fundamental analysis and market sentiment\",\n+            \"Provide actionable investment recommendations with clear rationale\",\n+            \"Use tables and charts to display data clearly and professionally\",\n+            \"Ensure all claims are supported by data and sources\",\n+            \"Present findings in a structured, easy-to-follow format\",\n+            \"Only output the final consolidated analysis, not individual agent responses\",\n+            \"Dont use emojis\",\n         ],\n         storage=PostgresAgentStorage(\n             db_url=db_url,\n@@ -85,5 +106,46 @@ def get_reasoning_finance_team():\n         show_members_responses=True,\n         enable_agentic_context=True,\n         add_datetime_to_instructions=True,\n-        success_criteria=\"The team has successfully completed the task.\",\n+        success_criteria=\"The team has provided a complete financial analysis with data, visualizations, risk assessment, and actionable investment recommendations supported by quantitative analysis and market research.\",\n     )\n+\n+\n+# ************* Demo Scenarios *************\n+\"\"\"\n+DEMO SCENARIOS - Use these as example queries to showcase the multi-agent system:\n+\n+1. COMPREHENSIVE INVESTMENT RESEARCH:\n+Analyze Apple (AAPL) as a potential investment:\n+1. Get current stock price and fundamentals\n+2. Research recent news and market sentiment\n+3. Calculate key financial ratios and risk metrics\n+4. Provide a comprehensive investment recommendation\n+\n+2. SECTOR COMPARISON ANALYSIS:\n+Compare the tech sector giants (AAPL, GOOGL, MSFT) performance:\n+1. Get financial data for all three companies\n+2. Analyze recent news affecting the tech sector\n+3. Calculate comparative metrics and correlations\n+4. Recommend portfolio allocation weights\n+\n+3. RISK ASSESSMENT SCENARIO:\n+Evaluate the risk profile of Tesla (TSLA):\n+1. Calculate volatility metrics and beta\n+2. Analyze recent news for risk factors\n+3. Compare risk vs return to market benchmarks\n+4. Provide risk-adjusted investment recommendation\n+\n+4. MARKET SENTIMENT ANALYSIS:\n+Analyze current market sentiment around AI stocks:\n+1. Search for recent AI industry news and developments\n+2. Get financial data for key AI companies (NVDA, GOOGL, MSFT, AMD)\n+3. Provide outlook for AI sector investing\n+\n+5. EARNINGS SEASON ANALYSIS:\n+Prepare for upcoming earnings season - analyze Microsoft (MSFT):\n+1. Get current financial metrics and analyst expectations\n+2. Research recent news and market sentiment\n+3. Calculate historical earnings impact on stock price\n+4. Provide trading strategy recommendation\n+\"\"\"\n+# *******************************\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/__init__.py b/cookbook/examples/streamlit_apps/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/__init__.py b/cookbook/examples/streamlit_apps/agentic_rag/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/agentic_rag.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/agentic_rag.py b/cookbook/examples/streamlit_apps/agentic_rag/agentic_rag.py\nnew file mode 100644\nindex 000000000..bb3638615\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/agentic_rag.py\n@@ -0,0 +1,132 @@\n+\"\"\"\ud83e\udd16 Agentic RAG Agent - Your AI Knowledge Assistant!\n+\n+This advanced example shows how to build a sophisticated RAG (Retrieval Augmented Generation) system that\n+leverages vector search and LLMs to provide deep insights from any knowledge base.\n+\n+The agent can:\n+- Process and understand documents from multiple sources (PDFs, websites, text files)\n+- Build a searchable knowledge base using vector embeddings\n+- Maintain conversation context and memory across sessions\n+- Provide relevant citations and sources for its responses\n+- Generate summaries and extract key insights\n+- Answer follow-up questions and clarifications\n+\n+Example queries to try:\n+- \"What are the key points from this document?\"\n+- \"Can you summarize the main arguments and supporting evidence?\"\n+- \"What are the important statistics and findings?\"\n+- \"How does this relate to [topic X]?\"\n+- \"What are the limitations or gaps in this analysis?\"\n+- \"Can you explain [concept X] in more detail?\"\n+- \"What other sources support or contradict these claims?\"\n+\n+The agent uses:\n+- Vector similarity search for relevant document retrieval\n+- Conversation memory for contextual responses\n+- Citation tracking for source attribution\n+- Dynamic knowledge base updates\n+\n+View the README for instructions on how to run the application.\n+\"\"\"\n+\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge import AgentKnowledge\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.agent.postgres import PostgresAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+\n+def get_agentic_rag_agent(\n+    model_id: str = \"openai:gpt-4o\",\n+    user_id: Optional[str] = None,\n+    session_id: Optional[str] = None,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    \"\"\"Get an Agentic RAG Agent with Memory.\"\"\"\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+    # Define persistent memory for chat history\n+\n+    # Define the knowledge base\n+    knowledge_base = AgentKnowledge(\n+        vector_db=PgVector(\n+            db_url=db_url,\n+            table_name=\"agentic_rag_documents\",\n+            schema=\"ai\",\n+            # Use OpenAI embeddings\n+            embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+        ),\n+        num_documents=3,  # Retrieve 3 most relevant documents\n+    )\n+\n+    # Create the Agent\n+    return Agent(\n+        name=\"agentic_rag_agent\",\n+        session_id=session_id,  # Track session ID for persistent conversations\n+        user_id=user_id,\n+        model=model,\n+        storage=PostgresAgentStorage(\n+            table_name=\"agentic_rag_agent_sessions\", db_url=db_url\n+        ),  # Persist session data\n+        knowledge=knowledge_base,  # Add knowledge base\n+        description=\"You are a helpful Agent called 'Agentic RAG' and your goal is to assist the user in the best way possible.\",\n+        instructions=[\n+            \"1. Knowledge Base Search:\",\n+            \"   - ALWAYS start by searching the knowledge base using search_knowledge_base tool\",\n+            \"   - Analyze ALL returned documents thoroughly before responding\",\n+            \"   - If multiple documents are returned, synthesize the information coherently\",\n+            \"2. External Search:\",\n+            \"   - If knowledge base search yields insufficient results, use duckduckgo_search\",\n+            \"   - Focus on reputable sources and recent information\",\n+            \"   - Cross-reference information from multiple sources when possible\",\n+            \"3. Context Management:\",\n+            \"   - Use get_chat_history tool to maintain conversation continuity\",\n+            \"   - Reference previous interactions when relevant\",\n+            \"   - Keep track of user preferences and prior clarifications\",\n+            \"4. Response Quality:\",\n+            \"   - Provide specific citations and sources for claims\",\n+            \"   - Structure responses with clear sections and bullet points when appropriate\",\n+            \"   - Include relevant quotes from source materials\",\n+            \"   - Avoid hedging phrases like 'based on my knowledge' or 'depending on the information'\",\n+            \"5. User Interaction:\",\n+            \"   - Ask for clarification if the query is ambiguous\",\n+            \"   - Break down complex questions into manageable parts\",\n+            \"   - Proactively suggest related topics or follow-up questions\",\n+            \"6. Error Handling:\",\n+            \"   - If no relevant information is found, clearly state this\",\n+            \"   - Suggest alternative approaches or questions\",\n+            \"   - Be transparent about limitations in available information\",\n+        ],\n+        search_knowledge=True,  # This setting gives the model a tool to search the knowledge base for information\n+        read_chat_history=True,  # This setting gives the model a tool to get chat history\n+        tools=[DuckDuckGoTools()],\n+        markdown=True,  # This setting tellss the model to format messages in markdown\n+        # add_chat_history_to_messages=True,\n+        show_tool_calls=True,\n+        add_history_to_messages=True,  # Adds chat history to messages\n+        add_datetime_to_instructions=True,\n+        debug_mode=debug_mode,\n+        read_tool_call_history=True,\n+        num_history_responses=3,\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/app.py b/cookbook/examples/streamlit_apps/agentic_rag/app.py\nnew file mode 100644\nindex 000000000..35aa20eaf\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/app.py\n@@ -0,0 +1,331 @@\n+import os\n+import tempfile\n+from typing import List\n+\n+import nest_asyncio\n+import requests\n+import streamlit as st\n+from agentic_rag import get_agentic_rag_agent\n+from agno.agent import Agent\n+from agno.document import Document\n+from agno.document.reader.csv_reader import CSVReader\n+from agno.document.reader.pdf_reader import PDFReader\n+from agno.document.reader.text_reader import TextReader\n+from agno.document.reader.website_reader import WebsiteReader\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    export_chat_history,\n+    rename_session_widget,\n+    session_selector_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"Agentic RAG\",\n+    page_icon=\"\ud83d\udc8e\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Add custom CSS\n+\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"agentic_rag_agent\"] = None\n+    st.session_state[\"agentic_rag_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def get_reader(file_type: str):\n+    \"\"\"Return appropriate reader based on file type.\"\"\"\n+    readers = {\n+        \"pdf\": PDFReader(),\n+        \"csv\": CSVReader(),\n+        \"txt\": TextReader(),\n+    }\n+    return readers.get(file_type.lower(), None)\n+\n+\n+def main():\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\"<h1 class='main-title'>Agentic RAG </h1>\", unsafe_allow_html=True)\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent research assistant powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"o3-mini\": \"openai:o3-mini\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gemini-2.0-flash-exp\": \"google:gemini-2.0-flash-exp\",\n+        \"claude-3-5-sonnet\": \"anthropic:claude-3-5-sonnet-20241022\",\n+        \"llama-3.3-70b\": \"groq:llama-3.3-70b-versatile\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    agentic_rag_agent: Agent\n+    if (\n+        \"agentic_rag_agent\" not in st.session_state\n+        or st.session_state[\"agentic_rag_agent\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Agentic RAG  ---*---\")\n+        agentic_rag_agent = get_agentic_rag_agent(model_id=model_id)\n+        st.session_state[\"agentic_rag_agent\"] = agentic_rag_agent\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        agentic_rag_agent = st.session_state[\"agentic_rag_agent\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    # Check if session ID is already in session state\n+    session_id_exists = (\n+        \"agentic_rag_agent_session_id\" in st.session_state\n+        and st.session_state[\"agentic_rag_agent_session_id\"]\n+    )\n+\n+    if not session_id_exists:\n+        try:\n+            st.session_state[\"agentic_rag_agent_session_id\"] = (\n+                agentic_rag_agent.load_session()\n+            )\n+        except Exception as e:\n+            logger.error(f\"Session load error: {str(e)}\")\n+            st.warning(\"Could not create Agent session, is the database running?\")\n+            # Continue anyway instead of returning, to avoid breaking session switching\n+    elif (\n+        st.session_state[\"agentic_rag_agent_session_id\"]\n+        and hasattr(agentic_rag_agent, \"memory\")\n+        and agentic_rag_agent.memory is not None\n+        and not agentic_rag_agent.memory.runs\n+    ):\n+        # If we have a session ID but no runs, try to load the session explicitly\n+        try:\n+            agentic_rag_agent.load_session(\n+                st.session_state[\"agentic_rag_agent_session_id\"]\n+            )\n+        except Exception as e:\n+            logger.error(f\"Failed to load existing session: {str(e)}\")\n+            # Continue anyway\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    agent_runs = []\n+    if hasattr(agentic_rag_agent, \"memory\") and agentic_rag_agent.memory is not None:\n+        agent_runs = agentic_rag_agent.memory.runs\n+\n+    # Initialize messages if it doesn't exist yet\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    # Only populate messages from agent runs if we haven't already\n+    if len(st.session_state[\"messages\"]) == 0 and len(agent_runs) > 0:\n+        logger.debug(\"Loading run history\")\n+        for _run in agent_runs:\n+            # Check if _run is an object with message attribute\n+            if hasattr(_run, \"message\") and _run.message is not None:\n+                add_message(_run.message.role, _run.message.content)\n+            # Check if _run is an object with response attribute\n+            if hasattr(_run, \"response\") and _run.response is not None:\n+                add_message(\"assistant\", _run.response.content, _run.response.tools)\n+    elif len(agent_runs) == 0 and len(st.session_state[\"messages\"]) == 0:\n+        logger.debug(\"No run history found\")\n+\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me anything!\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Track loaded URLs and files in session state\n+    ####################################################################\n+    if \"loaded_urls\" not in st.session_state:\n+        st.session_state.loaded_urls = set()\n+    if \"loaded_files\" not in st.session_state:\n+        st.session_state.loaded_files = set()\n+    if \"knowledge_base_initialized\" not in st.session_state:\n+        st.session_state.knowledge_base_initialized = False\n+\n+    st.sidebar.markdown(\"#### \ud83d\udcda Document Management\")\n+    input_url = st.sidebar.text_input(\"Add URL to Knowledge Base\")\n+    if (\n+        input_url and not prompt and not st.session_state.knowledge_base_initialized\n+    ):  # Only load if KB not initialized\n+        if input_url not in st.session_state.loaded_urls:\n+            alert = st.sidebar.info(\"Processing URLs...\", icon=\"\u2139\ufe0f\")\n+            if input_url.lower().endswith(\".pdf\"):\n+                try:\n+                    # Download PDF to temporary file\n+                    response = requests.get(input_url, stream=True, verify=False)\n+                    response.raise_for_status()\n+\n+                    with tempfile.NamedTemporaryFile(\n+                        suffix=\".pdf\", delete=False\n+                    ) as tmp_file:\n+                        for chunk in response.iter_content(chunk_size=8192):\n+                            tmp_file.write(chunk)\n+                        tmp_path = tmp_file.name\n+\n+                    reader = PDFReader()\n+                    docs: List[Document] = reader.read(tmp_path)\n+\n+                    # Clean up temporary file\n+                    os.unlink(tmp_path)\n+                except Exception as e:\n+                    st.sidebar.error(f\"Error processing PDF: {str(e)}\")\n+                    docs = []\n+            else:\n+                scraper = WebsiteReader(max_links=2, max_depth=1)\n+                docs: List[Document] = scraper.read(input_url)\n+\n+            if docs:\n+                agentic_rag_agent.knowledge.load_documents(docs, upsert=True)\n+                st.session_state.loaded_urls.add(input_url)\n+                st.sidebar.success(\"URL added to knowledge base\")\n+            else:\n+                st.sidebar.error(\"Could not process the provided URL\")\n+            alert.empty()\n+        else:\n+            st.sidebar.info(\"URL already loaded in knowledge base\")\n+\n+    uploaded_file = st.sidebar.file_uploader(\n+        \"Add a Document (.pdf, .csv, or .txt)\", key=\"file_upload\"\n+    )\n+    if (\n+        uploaded_file and not prompt and not st.session_state.knowledge_base_initialized\n+    ):  # Only load if KB not initialized\n+        file_identifier = f\"{uploaded_file.name}_{uploaded_file.size}\"\n+        if file_identifier not in st.session_state.loaded_files:\n+            alert = st.sidebar.info(\"Processing document...\", icon=\"\u2139\ufe0f\")\n+            file_type = uploaded_file.name.split(\".\")[-1].lower()\n+            reader = get_reader(file_type)\n+            if reader:\n+                docs = reader.read(uploaded_file)\n+                agentic_rag_agent.knowledge.load_documents(docs, upsert=True)\n+                st.session_state.loaded_files.add(file_identifier)\n+                st.sidebar.success(f\"{uploaded_file.name} added to knowledge base\")\n+                st.session_state.knowledge_base_initialized = True\n+            alert.empty()\n+        else:\n+            st.sidebar.info(f\"{uploaded_file.name} already loaded in knowledge base\")\n+\n+    if st.sidebar.button(\"Clear Knowledge Base\"):\n+        agentic_rag_agent.knowledge.vector_db.delete()\n+        st.session_state.loaded_urls.clear()\n+        st.session_state.loaded_files.clear()\n+        st.session_state.knowledge_base_initialized = False  # Reset initialization flag\n+        st.sidebar.success(\"Knowledge base cleared\")\n+    ###############################################################\n+    # Sample Question\n+    ###############################################################\n+    st.sidebar.markdown(\"#### \u2753 Sample Questions\")\n+    if st.sidebar.button(\"\ud83d\udcdd Summarize\"):\n+        add_message(\n+            \"user\",\n+            \"Can you summarize what is currently in the knowledge base (use `search_knowledge_base` tool)?\",\n+        )\n+\n+    ###############################################################\n+    # Utility buttons\n+    ###############################################################\n+    st.sidebar.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+    col1, col2 = st.sidebar.columns([1, 1])  # Equal width columns\n+    with col1:\n+        if st.sidebar.button(\n+            \"\ud83d\udd04 New Chat\", use_container_width=True\n+        ):  # Added use_container_width\n+            restart_agent()\n+    with col2:\n+        if st.sidebar.download_button(\n+            \"\ud83d\udcbe Export Chat\",\n+            export_chat_history(),\n+            file_name=\"rag_chat_history.md\",\n+            mime=\"text/markdown\",\n+            use_container_width=True,  # Added use_container_width\n+        ):\n+            st.sidebar.success(\"Chat history exported!\")\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = agentic_rag_agent.run(question, stream=True)\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response\n+                        if _resp_chunk.content is not None:\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\n+                        \"assistant\", response, agentic_rag_agent.run_response.tools\n+                    )\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(agentic_rag_agent, model_id)\n+    rename_session_widget(agentic_rag_agent)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/generate_requirements.sh b/cookbook/examples/streamlit_apps/agentic_rag/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/requirements.in b/cookbook/examples/streamlit_apps/agentic_rag/requirements.in\nnew file mode 100644\nindex 000000000..b5ec5dd7c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/requirements.in\n@@ -0,0 +1,9 @@\n+agno\n+anthropic\n+duckduckgo_search\n+google-genai\n+groq\n+nest_asyncio\n+openai\n+sqlalchemy\n+streamlit\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/requirements.txt b/cookbook/examples/streamlit_apps/agentic_rag/requirements.txt\nnew file mode 100644\nindex 000000000..ee8818881\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/requirements.txt\n@@ -0,0 +1,230 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.4.5\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.51.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.4.26\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.2\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==8.0.1\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+exceptiongroup==1.2.2\n+    # via anyio\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.40.1\n+    # via google-genai\n+google-genai==1.14.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+groq==0.24.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+h11==0.16.0\n+    # via httpcore\n+httpcore==1.0.9\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2025.4.1\n+    # via jsonschema\n+lxml==5.4.0\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.38.1\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+numpy==2.2.5\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.77.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.2.1\n+    # via streamlit\n+primp==0.15.0\n+    # via duckduckgo-search\n+protobuf==6.30.2\n+    # via streamlit\n+pyarrow==20.0.0\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pydantic==2.11.4\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.2\n+    # via pydantic\n+pydantic-settings==2.9.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9.1\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+streamlit==1.45.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.3\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   rich\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via\n+    #   pydantic\n+    #   pydantic-settings\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/utils.py b/cookbook/examples/streamlit_apps/agentic_rag/utils.py\nnew file mode 100644\nindex 000000000..b75d431f7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/utils.py\n@@ -0,0 +1,334 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agentic_rag import get_agentic_rag_agent\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Auto RAG Agent - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"agent\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role}\\n{msg['content']}\\n\\n\"\n+            if msg.get(\"tool_calls\"):\n+                chat_text += \"#### Tools Used:\\n\"\n+                for tool in msg[\"tool_calls\"]:\n+                    if isinstance(tool, dict):\n+                        tool_name = tool.get(\"name\", \"Unknown Tool\")\n+                    else:\n+                        tool_name = getattr(tool, \"name\", \"Unknown Tool\")\n+                    chat_text += f\"- {tool_name}\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    if not tools:\n+        return\n+\n+    with tool_calls_container.container():\n+        for tool_call in tools:\n+            # Handle different tool call formats\n+            _tool_name = (\n+                tool_call.get(\"tool_name\") or tool_call.get(\"name\") or \"Unknown Tool\"\n+            )\n+            _tool_args = tool_call.get(\"tool_args\") or tool_call.get(\"arguments\") or {}\n+            _content = tool_call.get(\"content\") or tool_call.get(\"result\", \"\")\n+            _metrics = tool_call.get(\"metrics\", {})\n+\n+            # Handle function objects\n+            if hasattr(tool_call, \"function\") and tool_call.function:\n+                if hasattr(tool_call.function, \"name\"):\n+                    _tool_name = tool_call.function.name\n+                if hasattr(tool_call.function, \"arguments\"):\n+                    _tool_args = tool_call.function.arguments\n+\n+            # Safely create the title with a default if tool name is None\n+            title = f\"\ud83d\udee0\ufe0f {_tool_name.replace('_', ' ').title() if _tool_name else 'Tool Call'}\"\n+\n+            with st.expander(title, expanded=False):\n+                if isinstance(_tool_args, dict) and \"query\" in _tool_args:\n+                    st.code(_tool_args[\"query\"], language=\"sql\")\n+                # Handle string arguments\n+                elif isinstance(_tool_args, str) and _tool_args:\n+                    try:\n+                        # Try to parse as JSON\n+                        import json\n+\n+                        args_dict = json.loads(_tool_args)\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(args_dict)\n+                    except:\n+                        # If not valid JSON, display as string\n+                        st.markdown(\"**Arguments:**\")\n+                        st.markdown(f\"```\\n{_tool_args}\\n```\")\n+                # Handle dict arguments\n+                elif _tool_args and _tool_args != {\"query\": None}:\n+                    st.markdown(\"**Arguments:**\")\n+                    st.json(_tool_args)\n+\n+                if _content:\n+                    st.markdown(\"**Results:**\")\n+                    if isinstance(_content, (dict, list)):\n+                        st.json(_content)\n+                    else:\n+                        try:\n+                            st.json(_content)\n+                        except Exception:\n+                            st.markdown(_content)\n+\n+                if _metrics:\n+                    st.markdown(\"**Metrics:**\")\n+                    st.json(_metrics)\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage\"\"\"\n+\n+    container = st.sidebar.container()\n+\n+    # Initialize session_edit_mode if needed\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    if st.sidebar.button(\"\u270e Rename Session\"):\n+        st.session_state.session_edit_mode = True\n+        st.rerun()\n+\n+    if st.session_state.session_edit_mode:\n+        new_session_name = st.sidebar.text_input(\n+            \"Enter new name:\",\n+            value=agent.session_name,\n+            key=\"session_name_input\",\n+        )\n+        if st.sidebar.button(\"Save\", type=\"primary\"):\n+            if new_session_name:\n+                agent.rename_session(new_session_name)\n+                st.session_state.session_edit_mode = False\n+                st.rerun()\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar\"\"\"\n+\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        # print(f\"Agent sessions: {agent_sessions}\")\n+\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        if session_options:\n+            selected_session = st.sidebar.selectbox(\n+                \"Session\",\n+                options=[s[\"display\"] for s in session_options],\n+                key=\"session_selector\",\n+            )\n+            # Find the selected session ID\n+            selected_session_id = next(\n+                s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+            )\n+\n+            if (\n+                st.session_state.get(\"agentic_rag_agent_session_id\")\n+                != selected_session_id\n+            ):\n+                logger.info(\n+                    f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+                )\n+\n+                try:\n+                    new_agent = get_agentic_rag_agent(\n+                        model_id=model_id,\n+                        session_id=selected_session_id,\n+                    )\n+\n+                    st.session_state[\"agentic_rag_agent\"] = new_agent\n+                    st.session_state[\"agentic_rag_agent_session_id\"] = (\n+                        selected_session_id\n+                    )\n+\n+                    st.session_state[\"messages\"] = []\n+\n+                    selected_session_obj = next(\n+                        (\n+                            s\n+                            for s in agent_sessions\n+                            if s.session_id == selected_session_id\n+                        ),\n+                        None,\n+                    )\n+\n+                    if (\n+                        selected_session_obj\n+                        and selected_session_obj.memory\n+                        and \"runs\" in selected_session_obj.memory\n+                    ):\n+                        seen_messages = set()\n+\n+                        for run in selected_session_obj.memory[\"runs\"]:\n+                            if \"messages\" in run:\n+                                for msg in run[\"messages\"]:\n+                                    msg_role = msg.get(\"role\")\n+                                    msg_content = msg.get(\"content\")\n+\n+                                    if not msg_content or msg_role == \"system\":\n+                                        continue\n+\n+                                    msg_id = f\"{msg_role}:{msg_content}\"\n+\n+                                    if msg_id in seen_messages:\n+                                        continue\n+\n+                                    seen_messages.add(msg_id)\n+\n+                                    if msg_role == \"assistant\":\n+                                        tool_calls = None\n+                                        if \"tool_calls\" in msg:\n+                                            tool_calls = msg[\"tool_calls\"]\n+                                        elif \"metrics\" in msg and msg.get(\"metrics\"):\n+                                            tools = run.get(\"tools\")\n+                                            if tools:\n+                                                tool_calls = tools\n+\n+                                        add_message(msg_role, msg_content, tool_calls)\n+                                    else:\n+                                        add_message(msg_role, msg_content)\n+\n+                            elif (\n+                                \"message\" in run\n+                                and isinstance(run[\"message\"], dict)\n+                                and \"content\" in run[\"message\"]\n+                            ):\n+                                user_msg = run[\"message\"][\"content\"]\n+                                msg_id = f\"user:{user_msg}\"\n+\n+                                if msg_id not in seen_messages:\n+                                    seen_messages.add(msg_id)\n+                                    add_message(\"user\", user_msg)\n+\n+                                if \"content\" in run and run[\"content\"]:\n+                                    asst_msg = run[\"content\"]\n+                                    msg_id = f\"assistant:{asst_msg}\"\n+\n+                                    if msg_id not in seen_messages:\n+                                        seen_messages.add(msg_id)\n+                                        add_message(\n+                                            \"assistant\", asst_msg, run.get(\"tools\")\n+                                        )\n+\n+                    st.rerun()\n+                except Exception as e:\n+                    logger.error(f\"Error switching sessions: {str(e)}\")\n+                    st.sidebar.error(f\"Error loading session: {str(e)}\")\n+        else:\n+            st.sidebar.info(\"No saved sessions available.\")\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\"\"\"\n+    This Agentic RAG Assistant helps you analyze documents and web content using natural language queries.\n+\n+    Built with:\n+    - \ud83d\ude80 Agno\n+    - \ud83d\udcab Streamlit\n+    \"\"\")\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+   .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+        font-size: 3em;\n+        font-weight: bold;\n+        padding: 1em 0;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .tool-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #3B82F6;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .tool-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/.gitignore b/cookbook/examples/streamlit_apps/answer_engine/.gitignore\nnew file mode 100644\nindex 000000000..1d3a629a5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/.gitignore\n@@ -0,0 +1,2 @@\n+output\n+agents.db\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/__init__.py b/cookbook/examples/streamlit_apps/answer_engine/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/agents.py b/cookbook/examples/streamlit_apps/answer_engine/agents.py\nnew file mode 100644\nindex 000000000..0de3c95a5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/agents.py\n@@ -0,0 +1,154 @@\n+\"\"\"\n+Sage: An Answer Engine\n+---------------------------------\n+This example shows how to build Sage, a Perplexity-like Answer Engine that intelligently\n+determines whether to perform a web search or conduct a deep analysis using ExaTools based on the user's query.\n+It also prompts the user to save the generated answer to a file using FileTools.\n+\n+Usage Examples:\n+---------------\n+1. Quick real-time search:\n+   sage = get_sage()\n+   answer = sage.run(\"What are the latest trends in renewable energy?\")\n+\n+2. In-depth analysis:\n+   sage = get_sage()\n+   answer = sage.run(\"Perform a detailed analysis of the impact of climate change on agriculture.\")\n+\n+3. Combined query with saving option:\n+   sage = get_sage()\n+   answer = sage.run(\"What's new in AI regulations in the EU and could you save the summary for me?\")\n+\n+Sage integrates:\n+  - DuckDuckGoTools for real-time web searches.\n+  - ExaTools for structured, in-depth analysis.\n+  - FileTools for saving the output upon user confirmation.\n+\n+Sage intelligently selects the optimal tool based on query complexity to provide insightful, comprehensive answers.\n+\"\"\"\n+\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Optional\n+\n+# Importing the Agent and model classes\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+\n+# Importing storage and tool classes\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.file import FileTools\n+\n+# Import the Agent template\n+from prompts import AGENT_DESCRIPTION, AGENT_INSTRUCTIONS, EXPECTED_OUTPUT_TEMPLATE\n+\n+# ************* Setup Paths *************\n+# Define the current working directory and output directory for saving files\n+cwd = Path(__file__).parent\n+output_dir = cwd.joinpath(\"output\")\n+# Create output directory if it doesn't exist\n+output_dir.mkdir(parents=True, exist_ok=True)\n+# Create tmp directory if it doesn't exist\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+# *************************************\n+\n+# ************* Agent Storage *************\n+# Configure SQLite storage for agent sessions\n+agent_storage = SqliteAgentStorage(\n+    table_name=\"answer_engine_sessions\",  # Table to store agent sessions\n+    db_file=str(tmp_dir.joinpath(\"agents.db\")),  # SQLite database file\n+    auto_upgrade_schema=True,\n+)\n+# *************************************\n+\n+\n+def get_sage(\n+    user_id: Optional[str] = None,\n+    model_id: str = \"openai:gpt-4o\",\n+    session_id: Optional[str] = None,\n+    num_history_responses: int = 5,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    \"\"\"\n+    Returns an instance of Sage, the Answer Engine Agent with integrated tools for web search,\n+    deep contextual analysis, and file management.\n+\n+    Sage will:\n+      - Decide whether a query requires a real-time web search (using DuckDuckGoTools)\n+        or an in-depth analysis (using ExaTools).\n+      - Generate a comprehensive answer that includes:\n+          \u2022 A direct, succinct answer.\n+          \u2022 Detailed explanations and supporting evidence.\n+          \u2022 Examples and clarification of misconceptions.\n+      - Prompt the user:\n+            \"Would you like to save this answer to a file? (yes/no)\"\n+        If confirmed, it will use FileTools to save the answer in markdown format in the output directory.\n+\n+    Args:\n+        user_id: Optional identifier for the user.\n+        model_id: Model identifier in the format 'provider:model_name' (e.g., \"openai:gpt-4o\").\n+        session_id: Optional session identifier for tracking conversation history.\n+        num_history_responses: Number of previous responses to include for context.\n+        debug_mode: Enable logging and debug features.\n+\n+    Returns:\n+        An instance of the configured Agent.\n+    \"\"\"\n+\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+    # Tools for Sage\n+    tools = [\n+        ExaTools(\n+            start_published_date=datetime.now().strftime(\"%Y-%m-%d\"),\n+            type=\"keyword\",\n+            num_results=10,\n+        ),\n+        DuckDuckGoTools(\n+            timeout=20,\n+            fixed_max_results=5,\n+        ),\n+        FileTools(base_dir=output_dir),\n+    ]\n+\n+    return Agent(\n+        name=\"Sage\",\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id or str(uuid.uuid4()),\n+        storage=agent_storage,\n+        tools=tools,\n+        # Allow Sage to read both chat history and tool call history for better context.\n+        read_chat_history=True,\n+        read_tool_call_history=True,\n+        # Append previous conversation responses into the new messages for context.\n+        add_history_to_messages=True,\n+        num_history_responses=num_history_responses,\n+        add_datetime_to_instructions=True,\n+        add_name_to_instructions=True,\n+        description=AGENT_DESCRIPTION,\n+        instructions=AGENT_INSTRUCTIONS,\n+        expected_output=EXPECTED_OUTPUT_TEMPLATE,\n+        debug_mode=debug_mode,\n+        markdown=True,\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/app.py b/cookbook/examples/streamlit_apps/answer_engine/app.py\nnew file mode 100644\nindex 000000000..0c03fddb0\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/app.py\n@@ -0,0 +1,207 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_sage\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    rename_session_widget,\n+    session_selector_widget,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Sage: The Answer Engine\",\n+    page_icon=\":crystal_ball:\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\"<h1 class='main-title'>Sage</h1>\", unsafe_allow_html=True)\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent answer engine powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"llama-3.3-70b\": \"groq:llama-3.3-70b-versatile\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"o3-mini\": \"openai:o3-mini\",\n+        \"gemini-2.0-flash-exp\": \"google:gemini-2.0-flash-exp\",\n+        \"claude-3-5-sonnet\": \"anthropic:claude-3-5-sonnet-20241022\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Choose a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    sage: Agent\n+    if (\n+        \"sage\" not in st.session_state\n+        or st.session_state[\"sage\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Sage agent ---*---\")\n+        sage = get_sage(model_id=model_id)\n+        st.session_state[\"sage\"] = sage\n+        st.session_state[\"current_model\"] = model_id\n+        # Initialize messages array if needed\n+        if \"messages\" not in st.session_state:\n+            st.session_state[\"messages\"] = []\n+    else:\n+        sage = st.session_state[\"sage\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    # Initialize session state\n+    if \"sage_session_id\" not in st.session_state:\n+        st.session_state[\"sage_session_id\"] = None\n+\n+    # Attempt to load or create a session\n+    if not st.session_state[\"sage_session_id\"]:\n+        try:\n+            logger.info(\"---*--- Loading Sage session ---*---\")\n+            st.session_state[\"sage_session_id\"] = sage.load_session()\n+            logger.info(\n+                f\"---*--- Sage session: {st.session_state['sage_session_id']} ---*---\"\n+            )\n+        except Exception as e:\n+            logger.error(f\"Session load error: {str(e)}\")\n+            st.warning(\"Database connection unavailable. Running in memory-only mode.\")\n+            # Generate a temporary session ID to allow the app to function without storage\n+            if not st.session_state[\"sage_session_id\"]:\n+                import uuid\n+\n+                st.session_state[\"sage_session_id\"] = f\"temp-{str(uuid.uuid4())}\"\n+                logger.info(\n+                    f\"---*--- Created temporary session: {st.session_state['sage_session_id']} ---*---\"\n+                )\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    # Initialize the messages array if not already done\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    # Only try to load runs from memory if we have a valid session and no messages yet\n+    if (\n+        len(st.session_state[\"messages\"]) == 0\n+        and hasattr(sage, \"memory\")\n+        and sage.memory is not None\n+    ):\n+        agent_runs = []\n+        # Check if memory is a dict or an object with runs attribute\n+        if isinstance(sage.memory, dict) and \"runs\" in sage.memory:\n+            agent_runs = sage.memory[\"runs\"]\n+        elif hasattr(sage.memory, \"runs\"):\n+            agent_runs = sage.memory.runs\n+\n+        # Load messages from agent runs\n+        if len(agent_runs) > 0:\n+            logger.debug(\"Loading run history\")\n+            for _run in agent_runs:\n+                # Check if _run is an object with message attribute\n+                if hasattr(_run, \"message\") and _run.message is not None:\n+                    add_message(_run.message.role, _run.message.content)\n+                # Check if _run is an object with response attribute\n+                if hasattr(_run, \"response\") and _run.response is not None:\n+                    add_message(\"assistant\", _run.response.content, _run.response.tools)\n+        else:\n+            logger.debug(\"No run history found\")\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\u2728 What would you like to know, bestie?\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\":crystal_ball: Sage is working its magic...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = sage.run(question, stream=True)\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response\n+                        if _resp_chunk.content is not None:\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, sage.run_response.tools)\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(sage, model_id)\n+    rename_session_widget(sage)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/generate_requirements.sh b/cookbook/examples/streamlit_apps/answer_engine/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/prompts.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/prompts.py b/cookbook/examples/streamlit_apps/answer_engine/prompts.py\nnew file mode 100644\nindex 000000000..6b3e96040\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/prompts.py\n@@ -0,0 +1,96 @@\n+\"\"\"Templates for the Answer Engine.\"\"\"\n+\n+from textwrap import dedent\n+\n+AGENT_DESCRIPTION = dedent(\"\"\"\\\n+    You are Sage, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.\n+    You have the following tools at your disposal:\n+      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.\n+      - ExaTools for structured, in-depth analysis.\n+      - FileTools for saving the output upon user confirmation.\n+\n+    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,\n+    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user\n+    with follow-up questions, such as asking if they'd like to save the answer.\n+\n+    <critical>\n+    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.\n+    - You must provide sources, whenever you provide a data point or a statistic.\n+    - When the user asks a follow-up question, you can use the previous answer as context.\n+    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.\n+    </critical>\\\n+\"\"\")\n+\n+AGENT_INSTRUCTIONS = dedent(\"\"\"\\\n+    Here's how you should answer the user's question:\n+\n+    1. Gather Relevant Information\n+      - First, carefully analyze the query to identify the intent of the user.\n+      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.\n+      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.\n+      - Combine the insights from both tools to craft a comprehensive and balanced answer.\n+      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.\n+      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.\n+\n+    2. Construct Your Response\n+      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.\n+      - **Then expand** the answer by including:\n+          \u2022 A clear explanation with context and definitions.\n+          \u2022 Supporting evidence such as statistics, real-world examples, and data points.\n+          \u2022 Clarifications that address common misconceptions.\n+      - Expand the answer only if the query requires more detail. Simple questions like: \"What is the weather in Tokyo?\" or \"What is the capital of France?\" don't need an in-depth analysis.\n+      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.\n+\n+    3. Enhance Engagement\n+      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)\"\n+      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.\n+\n+    4. Final Quality Check & Presentation \u2728\n+      - Review your response to ensure clarity, depth, and engagement.\n+      - Strive to be both informative for quick queries and thorough for detailed exploration.\n+\n+    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\\\n+\"\"\")\n+\n+EXPECTED_OUTPUT_TEMPLATE = dedent(\"\"\"\\\n+    {# If this is the first message, include the question title #}\n+    {% if this is the first message %}\n+    ## {An engaging title for this report. Keep it short.}\n+    {% endif %}\n+\n+    **{A clear and direct response that answers the question.}**\n+\n+    {# If the query requires more detail, include the sections below #}\n+    {% if detailed_response %}\n+\n+    ### {Secion title}\n+    {Add detailed analysis & explanation in this section}\n+    {A comprehensive breakdown covering key insights, context, and definitions.}\n+\n+    ### {Section title}\n+    {Add evidence & support in this section}\n+    {Add relevant data points and statistics in this section}\n+    {Add links or names of reputable sources supporting the answer in this section}\n+\n+    ### {Section title}\n+    {Add real-world examples or case studies that help illustrate the key points in this section}\n+\n+    ### {Section title}\n+    {Add clarifications addressing any common misunderstandings related to the topic in this section}\n+\n+    ### {Section title}\n+    {Add further details, implications, or suggestions for ongoing exploration in this section}\n+    {% endif %}\n+\n+    {Add any more sections you think are relevant, covering all the aspects of the query}\n+\n+    ### Sources\n+    - [1] {Source 1 url}\n+    - [2] {Source 2 url}\n+    - [3] {Source 3 url}\n+    - {any more sources you think are relevant}\n+\n+    Generated by Sage on: {current_time}\n+\n+    Stay curious and keep exploring \u2728\\\n+    \"\"\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/requirements.in b/cookbook/examples/streamlit_apps/answer_engine/requirements.in\nnew file mode 100644\nindex 000000000..32a90e8e8\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/requirements.in\n@@ -0,0 +1,11 @@\n+agno\n+anthropic\n+duckduckgo_search\n+exa_py\n+google-generativeai\n+google-search-results\n+groq\n+nest_asyncio\n+openai\n+sqlalchemy\n+streamlit\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/requirements.txt b/cookbook/examples/streamlit_apps/answer_engine/requirements.txt\nnew file mode 100644\nindex 000000000..bad8ba226\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/requirements.txt\n@@ -0,0 +1,274 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.11\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+anyio==4.8.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.5.2\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+exa-py==1.9.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-ai-generativelanguage==0.6.15\n+    # via google-generativeai\n+google-api-core==2.24.2\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-python-client\n+    #   google-generativeai\n+google-api-python-client==2.164.0\n+    # via google-generativeai\n+google-auth==2.38.0\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+    #   google-api-python-client\n+    #   google-auth-httplib2\n+    #   google-generativeai\n+google-auth-httplib2==0.2.0\n+    # via google-api-python-client\n+google-generativeai==0.8.4\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+google-search-results==2.4.2\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+googleapis-common-protos==1.69.1\n+    # via\n+    #   google-api-core\n+    #   grpcio-status\n+groq==0.19.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+grpcio==1.71.0\n+    # via\n+    #   google-api-core\n+    #   grpcio-status\n+grpcio-status==1.71.0\n+    # via google-api-core\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httplib2==0.22.0\n+    # via\n+    #   google-api-python-client\n+    #   google-auth-httplib2\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.30.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.66.3\n+    # via\n+    #   -r cookbook/examples/apps/answer_engine/requirements.in\n+    #   exa-py\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.14.0\n+    # via duckduckgo-search\n+proto-plus==1.26.1\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+protobuf==5.29.3\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+    #   google-generativeai\n+    #   googleapis-common-protos\n+    #   grpcio-status\n+    #   proto-plus\n+    #   streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyparsing==3.2.1\n+    # via httplib2\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   exa-py\n+    #   google-api-core\n+    #   google-search-results\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.39\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+streamlit==1.43.2\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   google-generativeai\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   exa-py\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+uritemplate==4.1.1\n+    # via google-api-python-client\n+urllib3==2.3.0\n+    # via requests\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/test.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/test.py b/cookbook/examples/streamlit_apps/answer_engine/test.py\nnew file mode 100644\nindex 000000000..b11d0c47f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/test.py\n@@ -0,0 +1,7 @@\n+from agents import get_sage\n+\n+sage = get_sage()\n+\n+if __name__ == \"__main__\":\n+    sage.show_tool_calls = True\n+    sage.print_response(\"Tell me about the tarrifs the US is imposing\", stream=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/answer_engine/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/answer_engine/utils.py b/cookbook/examples/streamlit_apps/answer_engine/utils.py\nnew file mode 100644\nindex 000000000..d9430d34b\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/answer_engine/utils.py\n@@ -0,0 +1,361 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import get_sage\n+from agno.agent.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"sage\"] = None\n+    st.session_state[\"sage_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Sage - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\", {})\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics:\n+                        execution_time = (\n+                            metrics[\"time\"]\n+                            if isinstance(metrics, dict)\n+                            else metrics.time\n+                        )\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.2f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content:\n+                        st.markdown(\"**Results:**\")\n+                        try:\n+                            st.json(content)\n+                        except Exception as e:\n+                            st.markdown(content)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries for Sage.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\udcdc Try me!\")\n+        if st.button(\"\ud83d\udca1 US Tariffs\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about the tariffs the US is imposing in 2025\",\n+            )\n+        if st.button(\"\ud83e\udd14 Reasoning Models\"):\n+            add_message(\n+                \"user\",\n+                \"Which is a better reasoning model: o3-mini or DeepSeek R1?\",\n+            )\n+        if st.button(\"\ud83e\udd16 Tell me about Agno\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about Agno: https://github.com/agno-agi/agno and https://docs.agno.com\",\n+            )\n+        if st.button(\"\u2696\ufe0f Impact of AI Regulations\"):\n+            add_message(\n+                \"user\",\n+                \"Evaluate how emerging AI regulations could influence innovation, privacy, and ethical AI deployment in the near future.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if st.button(\"\ud83d\udd04 New Chat\"):\n+                restart_agent()\n+        with col2:\n+            fn = \"sage_chat_history.md\"\n+            if \"sage_session_id\" in st.session_state:\n+                fn = f\"sage_{st.session_state.sage_session_id}.md\"\n+            if st.download_button(\n+                \"\ud83d\udcbe Export Chat\",\n+                export_chat_history(),\n+                file_name=fn,\n+                mime=\"text/markdown\",\n+            ):\n+                st.sidebar.success(\"Chat history exported!\")\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar.\"\"\"\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+\n+        selected_session_id = next(\n+            s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+        )\n+\n+        if st.session_state.get(\"sage_session_id\") != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+            )\n+\n+            try:\n+                new_agent = get_sage(\n+                    model_id=model_id,\n+                    session_id=selected_session_id,\n+                )\n+\n+                st.session_state[\"sage\"] = new_agent\n+                st.session_state[\"sage_session_id\"] = selected_session_id\n+\n+                st.session_state[\"messages\"] = []\n+\n+                selected_session_obj = next(\n+                    (s for s in agent_sessions if s.session_id == selected_session_id),\n+                    None,\n+                )\n+\n+                if (\n+                    selected_session_obj\n+                    and selected_session_obj.memory\n+                    and \"runs\" in selected_session_obj.memory\n+                ):\n+                    seen_messages = set()\n+\n+                    for run in selected_session_obj.memory[\"runs\"]:\n+                        if \"messages\" in run:\n+                            for msg in run[\"messages\"]:\n+                                msg_role = msg.get(\"role\")\n+                                msg_content = msg.get(\"content\")\n+\n+                                if not msg_content or msg_role == \"system\":\n+                                    continue\n+\n+                                msg_id = f\"{msg_role}:{msg_content}\"\n+\n+                                if msg_id in seen_messages:\n+                                    continue\n+\n+                                seen_messages.add(msg_id)\n+\n+                                if msg_role == \"assistant\":\n+                                    tool_calls = None\n+                                    if \"tool_calls\" in msg:\n+                                        tool_calls = msg[\"tool_calls\"]\n+                                    elif \"metrics\" in msg and msg.get(\"metrics\"):\n+                                        tools = run.get(\"tools\")\n+                                        if tools:\n+                                            tool_calls = tools\n+\n+                                    add_message(msg_role, msg_content, tool_calls)\n+                                else:\n+                                    # For user and other messages\n+                                    add_message(msg_role, msg_content)\n+\n+                        elif (\n+                            \"message\" in run\n+                            and isinstance(run[\"message\"], dict)\n+                            and \"content\" in run[\"message\"]\n+                        ):\n+                            user_msg = run[\"message\"][\"content\"]\n+                            msg_id = f\"user:{user_msg}\"\n+\n+                            if msg_id not in seen_messages:\n+                                seen_messages.add(msg_id)\n+                                add_message(\"user\", user_msg)\n+\n+                            if \"content\" in run and run[\"content\"]:\n+                                asst_msg = run[\"content\"]\n+                                msg_id = f\"assistant:{asst_msg}\"\n+\n+                                if msg_id not in seen_messages:\n+                                    seen_messages.add(msg_id)\n+                                    add_message(\"assistant\", asst_msg, run.get(\"tools\"))\n+\n+                st.rerun()\n+            except Exception as e:\n+                logger.error(f\"Error switching sessions: {str(e)}\")\n+                st.sidebar.error(f\"Error loading session: {str(e)}\")\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage.\"\"\"\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    # Initialize session_edit_mode if needed.\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\n+        \"\"\"\n+        Sage is a cutting-edge answer engine that delivers real-time insights and in-depth analysis on a wide range of topics.\n+\n+        Built with:\n+        - \ud83d\ude80 Agno\n+        - \ud83d\udcab Streamlit\n+        \"\"\"\n+    )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+        font-size: 3em;\n+        font-weight: bold;\n+        padding: 1em 0;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .sql-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #FF4B2B;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .sql-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/__init__.py b/cookbook/examples/streamlit_apps/chess_team/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/agents.py b/cookbook/examples/streamlit_apps/chess_team/agents.py\nnew file mode 100644\nindex 000000000..fe067cd67\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/chess_team/agents.py\n@@ -0,0 +1,163 @@\n+\"\"\"\n+Chess Team Battle\n+---------------------------------\n+This example shows how to build a Chess game where AI agents play different roles in a team.\n+The game features specialized agents for strategy for white pieces, strategy for black pieces,\n+and a master agent overseeing the game. Move validation is handled by python-chess.\n+\n+Usage Examples:\n+---------------\n+1. Quick game with default settings:\n+   team = get_chess_team()\n+\n+2. Game with debug mode off:\n+   team = get_chess_team(debug_mode=False)\n+\n+The game integrates:\n+  - Multiple AI models (Claude, GPT-4, etc.)\n+  - Specialized agent roles (strategist, master)\n+  - Turn-based gameplay coordination\n+  - Move validation using python-chess\n+\"\"\"\n+\n+import sys\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.team.team import Team\n+from agno.utils.log import logger\n+\n+project_root = str(Path(__file__).parent.parent.parent.parent)\n+if project_root not in sys.path:\n+    sys.path.append(project_root)\n+\n+\n+def get_model_for_provider(provider: str, model_name: str):\n+    \"\"\"\n+    Creates and returns the appropriate model instance based on the provider.\n+\n+    Args:\n+        provider: The model provider (e.g., 'openai', 'google', 'anthropic', 'groq')\n+        model_name: The specific model name/ID\n+\n+    Returns:\n+        An instance of the appropriate model class\n+\n+    Raises:\n+        ValueError: If the provider is not supported\n+    \"\"\"\n+    if provider == \"openai\":\n+        return OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        return Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        if model_name == \"claude-3-5-sonnet\":\n+            return Claude(id=\"claude-3-5-sonnet-20241022\", max_tokens=8192)\n+        elif model_name == \"claude-3-7-sonnet\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+            )\n+        elif model_name == \"claude-3-7-sonnet-thinking\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 4096},\n+            )\n+        else:\n+            return Claude(id=model_name)\n+    elif provider == \"groq\":\n+        return Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+\n+def get_chess_team(\n+    white_model: str = \"openai:gpt-4o\",\n+    black_model: str = \"anthropic:claude-3-7-sonnet\",\n+    master_model: str = \"openai:gpt-4o\",\n+    debug_mode: bool = True,\n+) -> Team:\n+    \"\"\"\n+    Returns a chess team with specialized agents for white pieces, black pieces, and game master.\n+\n+    Args:\n+        white_model: Model for white piece strategy\n+        black_model: Model for black piece strategy\n+        master_model: Model for game state evaluation\n+        debug_mode: Enable logging and debug features\n+\n+    Returns:\n+        Team instance configured for chess gameplay\n+    \"\"\"\n+    try:\n+        white_provider, white_name = white_model.split(\":\")\n+        black_provider, black_name = black_model.split(\":\")\n+        master_provider, master_name = master_model.split(\":\")\n+\n+        white_piece_model = get_model_for_provider(white_provider, white_name)\n+        black_piece_model = get_model_for_provider(black_provider, black_name)\n+        master_model = get_model_for_provider(master_provider, master_name)\n+\n+        white_piece_agent = Agent(\n+            name=\"white_piece_agent\",\n+            role=\"White Piece Strategist\",\n+            description=\"\"\"You are a chess strategist for white pieces. Given a list of legal moves,\n+                    analyze them and choose the best one based on standard chess strategy.\n+                    Consider piece development, center control, and king safety.\n+                    Respond ONLY with your chosen move in UCI notation (e.g., 'e2e4').\"\"\",\n+            model=white_piece_model,\n+            debug_mode=debug_mode,\n+        )\n+\n+        black_piece_agent = Agent(\n+            name=\"black_piece_agent\",\n+            role=\"Black Piece Strategist\",\n+            description=\"\"\"You are a chess strategist for black pieces. Given a list of legal moves,\n+                    analyze them and choose the best one based on standard chess strategy.\n+                    Consider piece development, center control, and king safety.\n+                    Respond ONLY with your chosen move in UCI notation (e.g., 'e7e5').\"\"\",\n+            model=black_piece_model,\n+            debug_mode=debug_mode,\n+        )\n+\n+        return Team(\n+            name=\"Chess Team\",\n+            mode=\"route\",\n+            model=master_model,\n+            success_criteria=\"The game is completed with a win, loss, or draw\",\n+            members=[white_piece_agent, black_piece_agent],\n+            instructions=[\n+                \"You are the chess game coordinator and master analyst.\",\n+                \"Your role is to coordinate between two player agents and provide game analysis:\",\n+                \"1. white_piece_agent - Makes moves for white pieces\",\n+                \"2. black_piece_agent - Makes moves for black pieces\",\n+                \"\",\n+                \"When receiving a task:\",\n+                \"1. Check the 'current_player' in the context\",\n+                \"2. If current_player is white_piece_agent or black_piece_agent:\",\n+                \"   - Forward the move request to that agent\",\n+                \"   - Return their move response directly without modification\",\n+                \"3. If no current_player is specified:\",\n+                \"   - This indicates a request for position analysis\",\n+                \"   - Analyze the position yourself and respond with a JSON object:\",\n+                \"   {\",\n+                \"       'game_over': true/false,\",\n+                \"       'result': 'white_win'/'black_win'/'draw'/null,\",\n+                \"   }\",\n+                \"\",\n+                \"Do not modify player agent responses.\",\n+                \"For analysis requests, provide detailed evaluation of the position.\",\n+            ],\n+            debug_mode=debug_mode,\n+            markdown=True,\n+            show_members_responses=True,\n+        )\n+\n+    except Exception as e:\n+        logger.error(f\"Error initializing chess team: {str(e)}\")\n+        raise\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/app.py b/cookbook/examples/streamlit_apps/chess_team/app.py\nnew file mode 100644\nindex 000000000..77caa17bc\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/chess_team/app.py\n@@ -0,0 +1,454 @@\n+import logging\n+from typing import Dict, List\n+\n+import chess\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_chess_team\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    WHITE,\n+    ChessBoard,\n+    display_board,\n+    display_move_history,\n+    parse_move,\n+    show_agent_status,\n+)\n+\n+# Configure logging\n+logging.basicConfig(\n+    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Chess Team Battle\",\n+    page_icon=\"\u265f\ufe0f\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def get_legal_moves_with_descriptions(board: ChessBoard) -> List[Dict]:\n+    \"\"\"\n+    Get all legal moves with descriptions for the current player.\n+\n+    Args:\n+        board: ChessBoard instance\n+\n+    Returns:\n+        List of dictionaries with move information\n+    \"\"\"\n+    legal_moves = []\n+\n+    # Get python-chess board\n+    chess_board = board.board\n+\n+    # Get all legal moves\n+    for move in chess_board.legal_moves:\n+        # Get source and destination squares\n+        from_square = chess.square_name(move.from_square)\n+        to_square = chess.square_name(move.to_square)\n+\n+        # Get piece type\n+        piece = chess_board.piece_at(move.from_square)\n+        piece_type = piece.symbol().upper() if piece else \"?\"\n+\n+        # Check if it's a capture\n+        is_capture = chess_board.is_capture(move)\n+\n+        # Check if it's a promotion\n+        promotion = None\n+        if move.promotion:\n+            promotion = chess.piece_name(move.promotion)\n+\n+        # Check if it's a castling move\n+        is_kingside_castle = chess_board.is_kingside_castling(move)\n+        is_queenside_castle = chess_board.is_queenside_castling(move)\n+\n+        # Create move description\n+        if is_kingside_castle:\n+            description = \"Kingside castle (O-O)\"\n+        elif is_queenside_castle:\n+            description = \"Queenside castle (O-O-O)\"\n+        elif promotion:\n+            description = f\"Pawn {from_square} to {to_square}, promote to {promotion}\"\n+        elif is_capture:\n+            captured_piece = chess_board.piece_at(move.to_square)\n+            captured_type = captured_piece.symbol().upper() if captured_piece else \"?\"\n+            description = f\"{piece_type} from {from_square} captures {captured_type} at {to_square}\"\n+        else:\n+            description = f\"{piece_type} from {from_square} to {to_square}\"\n+\n+        # Add move to list\n+        legal_moves.append(\n+            {\n+                \"uci\": move.uci(),\n+                \"san\": chess_board.san(move),\n+                \"description\": description,\n+                \"is_capture\": is_capture,\n+                \"is_castle\": is_kingside_castle or is_queenside_castle,\n+                \"promotion\": promotion,\n+            }\n+        )\n+\n+    return legal_moves\n+\n+\n+def main():\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>\u265f\ufe0f Chess Team Battle</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    ####################################################################\n+    # Initialize session state\n+    ####################################################################\n+    if \"game_started\" not in st.session_state:\n+        st.session_state.game_started = False\n+    if \"game_paused\" not in st.session_state:\n+        st.session_state.game_paused = False\n+    if \"move_history\" not in st.session_state:\n+        st.session_state.move_history = []\n+\n+    ####################################################################\n+    # Sidebar controls\n+    ####################################################################\n+    with st.sidebar:\n+        st.markdown(\"### Game Settings\")\n+\n+        # Model selection\n+        model_options = {\n+            \"gpt-4o\": \"openai:gpt-4o\",\n+            \"o3-mini\": \"openai:o3-mini\",\n+            \"claude-3.5\": \"anthropic:claude-3-5-sonnet\",\n+            \"claude-3.7\": \"anthropic:claude-3-7-sonnet\",\n+            \"claude-3.7-thinking\": \"anthropic:claude-3-7-sonnet-thinking\",\n+            \"gemini-flash\": \"google:gemini-2.0-flash\",\n+            \"gemini-pro\": \"google:gemini-2.0-pro-exp-02-05\",\n+        }\n+        ################################################################\n+        # Model selection\n+        ################################################################\n+        st.markdown(\"#### White Player\")\n+        selected_white = st.selectbox(\n+            \"Select White Player\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"gpt-4o\"),\n+            key=\"model_white\",\n+        )\n+\n+        st.markdown(\"#### Black Player\")\n+        selected_black = st.selectbox(\n+            \"Select Black Player\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"claude-3.7\"),\n+            key=\"model_black\",\n+        )\n+\n+        st.markdown(\"#### Game Master\")\n+        selected_master = st.selectbox(\n+            \"Select Game Master\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"gpt-4o\"),\n+            key=\"model_master\",\n+        )\n+\n+        ################################################################\n+        # Game controls\n+        ################################################################\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if not st.session_state.game_started:\n+                if st.button(\"\u25b6\ufe0f Start Game\"):\n+                    st.session_state.team = get_chess_team(\n+                        white_model=model_options[selected_white],\n+                        black_model=model_options[selected_black],\n+                        master_model=model_options[selected_master],\n+                        debug_mode=True,\n+                    )\n+                    st.session_state.game_board = ChessBoard()\n+                    st.session_state.game_started = True\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+            else:\n+                game_over, _ = st.session_state.game_board.get_game_state()\n+                if not game_over:\n+                    if st.button(\n+                        \"\u23f8\ufe0f Pause\" if not st.session_state.game_paused else \"\u25b6\ufe0f Resume\"\n+                    ):\n+                        st.session_state.game_paused = not st.session_state.game_paused\n+                        st.rerun()\n+        with col2:\n+            if st.session_state.game_started:\n+                if st.button(\"\ud83d\udd04 New Game\"):\n+                    st.session_state.team = get_chess_team(\n+                        white_model=model_options[selected_white],\n+                        black_model=model_options[selected_black],\n+                        master_model=model_options[selected_master],\n+                        debug_mode=True,\n+                    )\n+                    st.session_state.game_board = ChessBoard()\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+\n+    ####################################################################\n+    # Header showing current models\n+    ####################################################################\n+    if st.session_state.game_started:\n+        st.markdown(\n+            f\"<h3 style='color:#87CEEB; text-align:center;'>{selected_white} vs {selected_black}</h3>\",\n+            unsafe_allow_html=True,\n+        )\n+\n+    ####################################################################\n+    # Main game area\n+    ####################################################################\n+    if st.session_state.game_started:\n+        game_over, state_info = st.session_state.game_board.get_game_state()\n+\n+        display_board(st.session_state.game_board)\n+\n+        # Show game status (winner/draw/current player)\n+        if game_over:\n+            result = state_info.get(\"result\", \"\")\n+            reason = state_info.get(\"reason\", \"\")\n+\n+            if \"white_win\" in result:\n+                st.success(f\"\ud83c\udfc6 Game Over! White ({selected_white}) wins by {reason}!\")\n+            elif \"black_win\" in result:\n+                st.success(f\"\ud83c\udfc6 Game Over! Black ({selected_black}) wins by {reason}!\")\n+            else:\n+                st.info(f\"\ud83e\udd1d Game Over! It's a draw by {reason}!\")\n+        else:\n+            # Show current player status\n+            current_color = st.session_state.game_board.current_color\n+            current_model_name = (\n+                selected_white if current_color == WHITE else selected_black\n+            )\n+\n+            show_agent_status(\n+                f\"{current_color.capitalize()} Player ({current_model_name})\",\n+                \"It's your turn\",\n+                is_white=(current_color == WHITE),\n+            )\n+\n+        display_move_history(st.session_state.move_history)\n+\n+        if not st.session_state.game_paused and not game_over:\n+            # Thinking indicator\n+            st.markdown(\n+                f\"\"\"<div class=\"thinking-container\">\n+                    <div class=\"agent-thinking\">\n+                        <div style=\"margin-right: 10px; display: inline-block;\">\ud83d\udd04</div>\n+                        {current_color.capitalize()} Player ({current_model_name}) is thinking...\n+                    </div>\n+                </div>\"\"\",\n+                unsafe_allow_html=True,\n+            )\n+\n+            # Get legal moves using python-chess directly\n+            legal_moves = get_legal_moves_with_descriptions(st.session_state.game_board)\n+\n+            # Format legal moves for the agent\n+            legal_moves_descriptions = \"\\n\".join(\n+                [\n+                    f\"- {move['san']} ({move['uci']}): {move['description']}\"\n+                    for move in legal_moves\n+                ]\n+            )\n+\n+            # Get board state\n+            board_state = st.session_state.game_board.get_board_state()\n+            fen = st.session_state.game_board.get_fen()\n+\n+            # Get move from current player agent through team\n+            current_agent_name = (\n+                \"white_piece_agent\" if current_color == WHITE else \"black_piece_agent\"\n+            )\n+\n+            # Create the task message for the team\n+            task_message = f\"\"\"\\\n+Current board state (FEN): {fen}\n+Board visualization:\n+{board_state}\n+\n+Legal moves available:\n+{legal_moves_descriptions}\n+\n+Choose your next move from the legal moves above.\n+Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').\n+Do not include any other text in your response.\"\"\"\n+\n+            response = st.session_state.team.run(\n+                task_message,\n+                stream=False,\n+                context={\n+                    \"current_player\": current_agent_name,\n+                    \"board_state\": board_state,\n+                    \"legal_moves\": legal_moves,\n+                },\n+            )\n+\n+            try:\n+                # Parse the move from the response\n+                move_str = parse_move(response.content if response else \"\")\n+\n+                # Verify the move is in the list of legal moves\n+                legal_move_ucis = [move[\"uci\"] for move in legal_moves]\n+\n+                if move_str not in legal_move_ucis:\n+                    # Try to find a matching move\n+                    for move in legal_moves:\n+                        if move[\"san\"].lower() == move_str.lower():\n+                            move_str = move[\"uci\"]\n+                            break\n+\n+                # Make the move\n+                success, message = st.session_state.game_board.make_move(move_str)\n+\n+                if success:\n+                    # Find the move description\n+                    move_description = next(\n+                        (\n+                            move[\"description\"]\n+                            for move in legal_moves\n+                            if move[\"uci\"] == move_str\n+                        ),\n+                        \"\",\n+                    )\n+\n+                    move_number = len(st.session_state.move_history) + 1\n+                    st.session_state.move_history.append(\n+                        {\n+                            \"number\": move_number,\n+                            \"player\": f\"{current_color.capitalize()} ({current_model_name})\",\n+                            \"move\": move_str,\n+                            \"description\": move_description,\n+                        }\n+                    )\n+\n+                    logger.info(\n+                        f\"Move {move_number}: {current_color.capitalize()} ({current_model_name}) played {move_str} - {move_description}\"\n+                    )\n+                    logger.info(\n+                        f\"Board state:\\n{st.session_state.game_board.get_board_state()}\"\n+                    )\n+\n+                    # Check game state after move\n+                    game_over, state_info = st.session_state.game_board.get_game_state()\n+\n+                    # If game is not over, get analysis from coordinator after black's move\n+                    if not game_over and move_number % 2 == 0:  # After black's move\n+                        analysis_message = f\"\"\"\\\n+Current board state (FEN): {fen}\n+Board visualization:\n+{board_state}\n+\n+Last move: {move_str} ({move_description})\n+\n+Analyze the current position and provide your evaluation.\n+Respond with a JSON object containing:\n+{{\n+    \"game_over\": false,\n+    \"result\": null,\n+    \"reason\": null,\n+    \"commentary\": \"Your analysis of the position\",\n+    \"advantage\": \"white\"/\"black\"/\"equal\"\n+}}\"\"\"\n+\n+                        st.session_state.team.run(\n+                            message=analysis_message,\n+                            stream=False,\n+                            context={\n+                                \"board_state\": board_state,\n+                                \"last_move\": move_str,\n+                                \"last_move_description\": move_description,\n+                            },\n+                        )\n+\n+                    if game_over:\n+                        result = state_info.get(\"result\", \"\")\n+                        reason = state_info.get(\"reason\", \"\")\n+\n+                        if \"white_win\" in result:\n+                            logger.info(f\"Game Over - White wins by {reason}\")\n+                            st.success(\n+                                f\"\ud83c\udfc6 Game Over! White ({selected_white}) wins by {reason}!\"\n+                            )\n+                        elif \"black_win\" in result:\n+                            logger.info(f\"Game Over - Black wins by {reason}\")\n+                            st.success(\n+                                f\"\ud83c\udfc6 Game Over! Black ({selected_black}) wins by {reason}!\"\n+                            )\n+                        else:\n+                            logger.info(f\"Game Over - Draw by {reason}\")\n+                            st.info(f\"\ud83e\udd1d Game Over! It's a draw by {reason}!\")\n+\n+                        st.session_state.game_paused = True\n+\n+                    st.rerun()\n+                else:\n+                    logger.error(f\"Invalid move attempt: {message}\")\n+                    st.session_state.team.run(\n+                        message=f\"\"\"\\\n+Invalid move: {message}\n+\n+Current board state (FEN): {fen}\n+Board visualization:\n+{board_state}\n+\n+Legal moves available:\n+{legal_moves_descriptions}\n+\n+Please choose a valid move from the list above.\n+Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').\n+Do not include any other text in your response.\"\"\",\n+                        stream=False,\n+                        context={\n+                            \"current_player\": current_agent_name,\n+                            \"board_state\": board_state,\n+                            \"legal_moves\": legal_moves,\n+                            \"last_error\": message,\n+                        },\n+                    )\n+                    st.rerun()\n+\n+            except Exception as e:\n+                logger.error(f\"Error processing move: {str(e)}\")\n+                st.rerun()\n+    else:\n+        st.info(\"\ud83d\udc48 Press 'Start Game' to begin!\")\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    st.sidebar.markdown(f\"\"\"\n+    ### \u265f\ufe0f Chess Team Battle\n+    Watch AI agents play chess with specialized roles!\n+\n+    **Current Teams:**\n+    * \u2654 White: `{selected_white}`\n+    * \u265a Black: `{selected_black}`\n+    * \ud83e\udde0 Game Master: `{selected_master}`\n+\n+    **How it Works:**\n+    1. Python-chess validates all legal moves\n+    2. The White/Black Player agents choose the best move\n+    3. The Game Master analyzes the position\n+    4. The process repeats until the game ends\n+    \"\"\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/generate_requirements.sh b/cookbook/examples/streamlit_apps/chess_team/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/chess_team/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/requirements.in b/cookbook/examples/streamlit_apps/chess_team/requirements.in\nnew file mode 100644\nindex 000000000..9d7918f95\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/chess_team/requirements.in\n@@ -0,0 +1,14 @@\n+agno\n+anthropic\n+groq\n+google-genai\n+nest-asyncio\n+openai\n+pathlib\n+Pillow\n+rich\n+streamlit\n+pydantic\n+typing-extensions\n+python-dotenv\n+python-chess\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/requirements.txt b/cookbook/examples/streamlit_apps/chess_team/requirements.txt\nnew file mode 100644\nindex 000000000..3f12a0273\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/chess_team/requirements.txt\n@@ -0,0 +1,224 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.17\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+chess==1.11.2\n+    # via python-chess\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.7.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+groq==0.20.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.32.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.68.2\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pathlib==1.0.1\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+pillow==11.1.0\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-chess==1.999\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+streamlit==1.43.2\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/chess_team/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/chess_team/utils.py b/cookbook/examples/streamlit_apps/chess_team/utils.py\nnew file mode 100644\nindex 000000000..4d53019a7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/chess_team/utils.py\n@@ -0,0 +1,797 @@\n+from dataclasses import dataclass\n+from typing import Dict, List, Tuple\n+\n+import chess\n+import streamlit as st\n+from agno.agent import Agent\n+\n+WHITE = \"white\"\n+BLACK = \"black\"\n+\n+# Custom CSS for the chess board\n+CUSTOM_CSS = \"\"\"\n+<style>\n+    /* Dark mode styling */\n+    .main-title {\n+        color: #87CEEB;\n+        text-align: center;\n+        margin-bottom: 20px;\n+    }\n+    \n+    .chess-board {\n+        width: 100%;\n+        max-width: 500px;\n+        margin: 0 auto;\n+        border: 2px solid #555;\n+        border-radius: 5px;\n+        overflow: hidden;\n+    }\n+    \n+    .chess-square {\n+        width: 12.5%;\n+        aspect-ratio: 1;\n+        display: inline-block;\n+        text-align: center;\n+        font-size: 24px;\n+        line-height: 60px;\n+        vertical-align: middle;\n+    }\n+    \n+    .white-square {\n+        background-color: #f0d9b5;\n+        color: #000;\n+    }\n+    \n+    .black-square {\n+        background-color: #b58863;\n+        color: #000;\n+    }\n+    \n+    .piece {\n+        font-size: 32px;\n+        line-height: 60px;\n+    }\n+    \n+    .move-history {\n+        margin-top: 20px;\n+        border: 1px solid #555;\n+        border-radius: 5px;\n+        padding: 10px;\n+        max-height: 300px;\n+        overflow-y: auto;\n+    }\n+    \n+    .move-entry {\n+        padding: 5px;\n+        border-bottom: 1px solid #444;\n+    }\n+    \n+    .move-entry:last-child {\n+        border-bottom: none;\n+    }\n+    \n+    .agent-thinking {\n+        display: flex;\n+        align-items: center;\n+        background-color: rgba(100, 100, 100, 0.2);\n+        padding: 10px;\n+        border-radius: 5px;\n+        margin: 10px 0;\n+        animation: pulse 1.5s infinite;\n+    }\n+    \n+    @keyframes pulse {\n+        0% { opacity: 0.6; }\n+        50% { opacity: 1; }\n+        100% { opacity: 0.6; }\n+    }\n+    \n+    .agent-status {\n+        display: flex;\n+        align-items: center;\n+        background-color: rgba(100, 100, 100, 0.2);\n+        padding: 10px;\n+        border-radius: 5px;\n+        margin: 10px 0;\n+    }\n+    \n+    .agent-status.white {\n+        border-left: 4px solid #f0d9b5;\n+    }\n+    \n+    .agent-status.black {\n+        border-left: 4px solid #b58863;\n+    }\n+    \n+    .thinking-container {\n+        margin: 10px 0;\n+    }\n+    \n+    /* Additional CSS for the simple board representation */\n+    .chess-board-wrapper {\n+        font-family: 'Courier New', monospace;\n+        background: #2b2b2b;\n+        padding: 20px;\n+        border-radius: 10px;\n+        display: inline-block;\n+        margin: 20px auto;\n+        text-align: center;\n+    }\n+    \n+    .board-container {\n+        display: flex;\n+        justify-content: center;\n+        width: 100%;\n+    }\n+    \n+    .chess-files {\n+        color: #888;\n+        text-align: center;\n+        padding: 5px 0;\n+        margin-left: 30px;\n+        display: flex;\n+        justify-content: space-around;\n+        width: calc(100% - 30px);\n+        margin-bottom: 5px;\n+    }\n+    \n+    .chess-file-label {\n+        width: 40px;\n+        text-align: center;\n+    }\n+    \n+    .chess-grid {\n+        border: 1px solid #666;\n+        display: inline-block;\n+    }\n+    \n+    .chess-row {\n+        display: flex;\n+        align-items: center;\n+    }\n+    \n+    .chess-rank {\n+        color: #888;\n+        width: 25px;\n+        text-align: center;\n+        padding-right: 5px;\n+    }\n+    \n+    .chess-cell {\n+        width: 40px;\n+        height: 40px;\n+        display: flex;\n+        align-items: center;\n+        justify-content: center;\n+        border: 1px solid #666;\n+        font-size: 24px;\n+    }\n+    \n+    .piece-white {\n+        color: #fff;\n+    }\n+    \n+    .piece-black {\n+        color: #aaa;\n+    }\n+    \n+    .piece-empty {\n+        color: transparent;\n+    }\n+    \n+    .chess-row:nth-child(odd) .chess-cell:nth-child(even),\n+    .chess-row:nth-child(even) .chess-cell:nth-child(odd) {\n+        background-color: #3c3c3c;\n+    }\n+    \n+    .chess-row:nth-child(even) .chess-cell:nth-child(even),\n+    .chess-row:nth-child(odd) .chess-cell:nth-child(odd) {\n+        background-color: #262626;\n+    }\n+    \n+    /* Additional CSS for move history grid */\n+    .move-history-grid {\n+        display: grid;\n+        grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));\n+        gap: 10px;\n+        padding: 10px;\n+        max-height: 600px;  /* Height of approximately 2 rows of boards */\n+        overflow-y: auto;\n+        scrollbar-width: thin;\n+        scrollbar-color: #666 #333;\n+    }\n+    \n+    /* Webkit scrollbar styling */\n+    .move-history-grid::-webkit-scrollbar {\n+        width: 8px;\n+    }\n+\n+    .move-history-grid::-webkit-scrollbar-track {\n+        background: #333;\n+        border-radius: 4px;\n+    }\n+\n+    .move-history-grid::-webkit-scrollbar-thumb {\n+        background: #666;\n+        border-radius: 4px;\n+    }\n+\n+    .move-history-grid::-webkit-scrollbar-thumb:hover {\n+        background: #888;\n+    }\n+    \n+    .move-history-item {\n+        background: rgba(40, 40, 40, 0.8);\n+        border-radius: 5px;\n+        padding: 10px;\n+        text-align: center;\n+        border: 1px solid #444;\n+    }\n+    \n+    .move-history-item .move-text {\n+        font-family: monospace;\n+        font-size: 1.1em;\n+        margin: 5px 0;\n+    }\n+\n+    .move-history-item .move-text.white-move {\n+        color: #4CAF50;\n+    }\n+\n+    .move-history-item .move-text.black-move {\n+        color: #ff4444;\n+    }\n+    \n+    .move-history-item .description {\n+        color: #888;\n+        font-size: 0.9em;\n+        margin-top: 5px;\n+    }\n+    \n+    /* Mini chess board for history */\n+    .mini-chess-board {\n+        display: grid;\n+        grid-template-columns: repeat(8, 1fr);\n+        width: 160px;\n+        margin: 10px auto;\n+        border: 1px solid #555;\n+        position: relative;\n+    }\n+    \n+    .mini-square {\n+        aspect-ratio: 1;\n+        display: flex;\n+        align-items: center;\n+        justify-content: center;\n+        font-size: 14px;\n+        position: relative;\n+    }\n+    \n+    .mini-white-square {\n+        background-color: #f0d9b5;\n+    }\n+    \n+    .mini-black-square {\n+        background-color: #b58863;\n+    }\n+    \n+    .mini-piece {\n+        font-size: 14px;\n+        line-height: 20px;\n+        z-index: 2;\n+    }\n+\n+    .mini-piece.white-piece {\n+        color: #ffffff;\n+        text-shadow: 0 0 2px #000;\n+    }\n+\n+    .mini-piece.black-piece {\n+        color: #000000;\n+        text-shadow: 0 0 2px #fff;\n+    }\n+\n+    .mini-square.move-from.white-move {\n+        background-color: rgba(76, 175, 80, 0.5) !important;\n+    }\n+\n+    .mini-square.move-to.white-move {\n+        background-color: rgba(76, 175, 80, 0.7) !important;\n+    }\n+\n+    .mini-square.move-from.black-move {\n+        background-color: rgba(255, 68, 68, 0.5) !important;\n+    }\n+\n+    .mini-square.move-to.black-move {\n+        background-color: rgba(255, 68, 68, 0.7) !important;\n+    }\n+</style>\n+\"\"\"\n+\n+\n+@dataclass\n+class SimpleChessBoard:\n+    \"\"\"Represents a simple chess board state without full rules validation\"\"\"\n+\n+    def __init__(self):\n+        # Use Unicode chess pieces for better visualization\n+        self.piece_map = {\n+            \"K\": \"\u2654\",\n+            \"Q\": \"\u2655\",\n+            \"R\": \"\u2656\",\n+            \"B\": \"\u2657\",\n+            \"N\": \"\u2658\",\n+            \"P\": \"\u2659\",  # White pieces\n+            \"k\": \"\u265a\",\n+            \"q\": \"\u265b\",\n+            \"r\": \"\u265c\",\n+            \"b\": \"\u265d\",\n+            \"n\": \"\u265e\",\n+            \"p\": \"\u265f\",  # Black pieces\n+            \".\": \" \",  # Empty square\n+        }\n+\n+        self.board = [\n+            [\"r\", \"n\", \"b\", \"q\", \"k\", \"b\", \"n\", \"r\"],  # Black pieces\n+            [\"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\"],  # Black pawns\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\"],  # White pawns\n+            [\"R\", \"N\", \"B\", \"Q\", \"K\", \"B\", \"N\", \"R\"],  # White pieces\n+        ]\n+\n+    def get_board_state(self) -> str:\n+        \"\"\"Returns a formatted string representation of the current board state with HTML/CSS classes\"\"\"\n+        # First create the HTML structure with CSS classes\n+        html_output = [\n+            '<div class=\"chess-board-wrapper\">',\n+            '<div class=\"chess-files\">',\n+        ]\n+\n+        # Add individual file labels\n+        for file in \"abcdefgh\":\n+            html_output.append(f'<div class=\"chess-file-label\">{file}</div>')\n+\n+        html_output.extend(\n+            [\n+                \"</div>\",  # Close chess-files\n+                '<div class=\"chess-grid\">',\n+            ]\n+        )\n+\n+        for i, row in enumerate(self.board):\n+            html_output.append('<div class=\"chess-row\">')\n+            html_output.append(f'<div class=\"chess-rank\">{8 - i}</div>')\n+\n+            for piece in row:\n+                piece_char = self.piece_map[piece]\n+                piece_class = \"piece-white\" if piece.isupper() else \"piece-black\"\n+                if piece == \".\":\n+                    piece_class = \"piece-empty\"\n+                html_output.append(\n+                    f'<div class=\"chess-cell {piece_class}\">{piece_char}</div>'\n+                )\n+\n+            html_output.append(\"</div>\")\n+\n+        html_output.append(\"</div>\")\n+        html_output.append(\"</div>\")\n+\n+        return \"\\n\".join(html_output)\n+\n+    def update_position(\n+        self, from_pos: Tuple[int, int], to_pos: Tuple[int, int]\n+    ) -> None:\n+        \"\"\"Updates the board with a new move\"\"\"\n+        piece = self.board[from_pos[0]][from_pos[1]]\n+        self.board[from_pos[0]][from_pos[1]] = \".\"\n+        self.board[to_pos[0]][to_pos[1]] = piece\n+\n+    def is_valid_position(self, pos: Tuple[int, int]) -> bool:\n+        \"\"\"Checks if a position is within the board boundaries\"\"\"\n+        return 0 <= pos[0] < 8 and 0 <= pos[1] < 8\n+\n+    def is_valid_move(self, move: str) -> bool:\n+        \"\"\"Validates if a move string is in the correct format (e.g., 'e2e4')\"\"\"\n+        if len(move) != 4:\n+            return False\n+\n+        file_chars = \"abcdefgh\"\n+        rank_chars = \"12345678\"\n+\n+        from_file, from_rank = move[0], move[1]\n+        to_file, to_rank = move[2], move[3]\n+\n+        return all(\n+            [\n+                from_file in file_chars,\n+                from_rank in rank_chars,\n+                to_file in file_chars,\n+                to_rank in rank_chars,\n+            ]\n+        )\n+\n+    def algebraic_to_index(self, move: str) -> tuple[tuple[int, int], tuple[int, int]]:\n+        \"\"\"Converts algebraic notation (e.g., 'e2e4') to board indices\"\"\"\n+        file_map = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7}\n+\n+        from_file, from_rank = move[0], int(move[1])\n+        to_file, to_rank = move[2], int(move[3])\n+\n+        from_pos = (8 - from_rank, file_map[from_file])\n+        to_pos = (8 - to_rank, file_map[to_file])\n+\n+        return from_pos, to_pos\n+\n+    def get_piece_name(self, piece: str) -> str:\n+        \"\"\"Returns the full name of a piece from its symbol\"\"\"\n+        piece_names = {\n+            \"K\": \"King\",\n+            \"Q\": \"Queen\",\n+            \"R\": \"Rook\",\n+            \"B\": \"Bishop\",\n+            \"N\": \"Knight\",\n+            \"P\": \"Pawn\",\n+            \"k\": \"King\",\n+            \"q\": \"Queen\",\n+            \"r\": \"Rook\",\n+            \"b\": \"Bishop\",\n+            \"n\": \"Knight\",\n+            \"p\": \"Pawn\",\n+            \".\": \"Empty\",\n+        }\n+        return piece_names.get(piece, \"Unknown\")\n+\n+    def get_piece_at_position(self, pos: Tuple[int, int]) -> str:\n+        \"\"\"Returns the piece at the given position\"\"\"\n+        return self.board[pos[0]][pos[1]]\n+\n+\n+class ChessBoard:\n+    def __init__(self):\n+        self.board = chess.Board()\n+        self.current_color = WHITE\n+        self.move_history = []\n+\n+    def make_move(self, move_str: str) -> Tuple[bool, str]:\n+        \"\"\"\n+        Make a move on the board using python-chess for validation.\n+\n+        Args:\n+            move_str: Move in UCI notation (e.g., \"e2e4\")\n+\n+        Returns:\n+            Tuple[bool, str]: (Success status, Message with current board state or error)\n+        \"\"\"\n+        try:\n+            # Convert to python-chess move\n+            move = chess.Move.from_uci(move_str)\n+\n+            # Check if move is legal\n+            if move not in self.board.legal_moves:\n+                return False, f\"Invalid move: {move_str} is not a legal move.\"\n+\n+            self.board.push(move)\n+\n+            # Switch player\n+            self.current_color = BLACK if self.current_color == WHITE else WHITE\n+\n+            return True, f\"Move successful!\\n{self.get_board_state()}\"\n+        except ValueError:\n+            return False, f\"Invalid move format: {move_str}. Use format like 'e2e4'.\"\n+        except Exception as e:\n+            return False, f\"Error making move: {str(e)}\"\n+\n+    def get_board_state(self) -> str:\n+        \"\"\"\n+        Get a string representation of the current board state.\n+\n+        Returns:\n+            String representation of the board\n+        \"\"\"\n+        return str(self.board)\n+\n+    def get_fen(self) -> str:\n+        \"\"\"\n+        Get the FEN notation of the current board state.\n+\n+        Returns:\n+            FEN string\n+        \"\"\"\n+        return self.board.fen()\n+\n+    def get_legal_moves(self, color: str = None) -> List[str]:\n+        \"\"\"\n+        Get all legal moves for the current player or specified color.\n+\n+        Args:\n+            color: Optional color to get moves for (WHITE or BLACK)\n+\n+        Returns:\n+            List of legal moves in UCI notation\n+        \"\"\"\n+\n+        # If it's not the specified color's turn, return empty list\n+        if (color == WHITE and not self.board.turn) or (\n+            color == BLACK and self.board.turn\n+        ):\n+            return []\n+\n+        return [move.uci() for move in self.board.legal_moves]\n+\n+    def is_game_over(self) -> bool:\n+        \"\"\"\n+        Check if the game is over.\n+\n+        Returns:\n+            True if game is over, False otherwise\n+        \"\"\"\n+        return self.board.is_game_over()\n+\n+    def get_game_state(self) -> Tuple[bool, Dict]:\n+        \"\"\"\n+        Get the current game state.\n+\n+        Returns:\n+            Tuple[bool, Dict]: (is_game_over, state_info)\n+        \"\"\"\n+        is_game_over = self.board.is_game_over()\n+\n+        state_info = {\n+            \"current_player\": self.current_color,\n+            \"fen\": self.board.fen(),\n+            \"halfmove_clock\": self.board.halfmove_clock,\n+            \"fullmove_number\": self.board.fullmove_number,\n+        }\n+\n+        if is_game_over:\n+            if self.board.is_checkmate():\n+                winner = BLACK if self.board.turn else WHITE\n+                state_info[\"result\"] = f\"{winner}_win\"\n+                state_info[\"reason\"] = \"checkmate\"\n+            elif self.board.is_stalemate():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"stalemate\"\n+            elif self.board.is_insufficient_material():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"insufficient_material\"\n+            elif self.board.is_fifty_moves():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"fifty_move_rule\"\n+            elif self.board.is_repetition():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"repetition\"\n+            else:\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"unknown\"\n+        else:\n+            state_info[\"result\"] = None\n+            state_info[\"reason\"] = None\n+\n+        return is_game_over, state_info\n+\n+\n+def display_board(board: ChessBoard):\n+    \"\"\"\n+    Display the chess board in the Streamlit app.\n+\n+    Args:\n+        board: ChessBoard instance\n+    \"\"\"\n+    board_obj = board.board\n+\n+    # Create HTML for the chess board\n+    html = '<div class=\"chess-board\">'\n+\n+    # Unicode chess pieces\n+    pieces = {\n+        \"r\": \"\u265c\",\n+        \"n\": \"\u265e\",\n+        \"b\": \"\u265d\",\n+        \"q\": \"\u265b\",\n+        \"k\": \"\u265a\",\n+        \"p\": \"\u265f\",\n+        \"R\": \"\u2656\",\n+        \"N\": \"\u2658\",\n+        \"B\": \"\u2657\",\n+        \"Q\": \"\u2655\",\n+        \"K\": \"\u2654\",\n+        \"P\": \"\u2659\",\n+        \".\": \"\",\n+    }\n+\n+    # Convert board to a 2D array for easier rendering\n+    board_array = []\n+    for row in str(board_obj).split(\"\\n\"):\n+        board_array.append(row.split(\" \"))\n+\n+    for i in range(8):\n+        for j in range(8):\n+            square_color = \"white-square\" if (i + j) % 2 == 0 else \"black-square\"\n+            piece = board_array[i][j]\n+            piece_unicode = pieces.get(piece, \"\")\n+\n+            html += f'<div class=\"chess-square {square_color}\">'\n+            if piece_unicode:\n+                html += f'<span class=\"piece\">{piece_unicode}</span>'\n+            html += \"</div>\"\n+\n+    html += \"</div>\"\n+\n+    st.markdown(html, unsafe_allow_html=True)\n+\n+\n+def show_agent_status(agent_name: str, status: str, is_white: bool = True):\n+    \"\"\"\n+    Display the status of an agent.\n+\n+    Args:\n+        agent_name: Name of the agent\n+        status: Status message\n+        is_white: Whether the agent plays white pieces\n+    \"\"\"\n+    color_class = \"white\" if is_white else \"black\"\n+    st.markdown(\n+        f\"\"\"<div class=\"agent-status {color_class}\">\n+            <div style=\"margin-right: 10px;\">{\"\u2654\" if is_white else \"\u265a\"}</div>\n+            <div>\n+                <strong>{agent_name}</strong><br>\n+                {status}\n+            </div>\n+        </div>\"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+\n+def display_move_history(move_history):\n+    \"\"\"\n+    Display the move history with miniature chess boards.\n+\n+    Args:\n+        move_history: List of move history entries\n+    \"\"\"\n+    if not move_history:\n+        return\n+\n+    st.markdown(\"<h3>Move History</h3>\", unsafe_allow_html=True)\n+\n+    html = '<div class=\"move-history-grid\">'\n+\n+    # Unicode chess pieces\n+    pieces = {\n+        \"r\": \"\u265c\",\n+        \"n\": \"\u265e\",\n+        \"b\": \"\u265d\",\n+        \"q\": \"\u265b\",\n+        \"k\": \"\u265a\",\n+        \"p\": \"\u265f\",\n+        \"R\": \"\u2656\",\n+        \"N\": \"\u2658\",\n+        \"B\": \"\u2657\",\n+        \"Q\": \"\u2655\",\n+        \"K\": \"\u2654\",\n+        \"P\": \"\u2659\",\n+        \".\": \"\",\n+    }\n+\n+    for move in move_history:\n+        board = chess.Board()\n+        # Play all moves up to this point\n+        for i in range(move[\"number\"]):\n+            if i < len(move_history):\n+                try:\n+                    board.push(chess.Move.from_uci(move_history[i][\"move\"]))\n+                except (chess.InvalidMoveError, ValueError) as e:\n+                    continue\n+\n+        # Get the current move's from and to squares\n+        current_move = move[\"move\"]\n+        from_square = current_move[:2]\n+        to_square = current_move[2:]\n+\n+        # Convert algebraic notation to board coordinates\n+        file_map = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7}\n+        from_file, from_rank = from_square[0], int(from_square[1])\n+        to_file, to_rank = to_square[0], int(to_square[1])\n+        from_coords = (8 - from_rank, file_map[from_file])\n+        to_coords = (8 - to_rank, file_map[to_file])\n+\n+        # Determine if it's a white or black move\n+        is_white_move = \"white\" in move[\"player\"].lower()\n+        move_color_class = \"white-move\" if is_white_move else \"black-move\"\n+\n+        # Board to 2D array\n+        board_array = []\n+        for row in str(board).split(\"\\n\"):\n+            board_array.append(row.split(\" \"))\n+\n+        # Create move history item with mini board\n+        html += '<div class=\"move-history-item\">'\n+        html += f\"<strong>Move {move['number']}</strong><br>\"\n+        html += f\"{move['player']}<br>\"\n+        html += f'<div class=\"move-text {move_color_class}\">{move[\"move\"]}</div>'\n+\n+        # Add mini chess board\n+        html += '<div class=\"mini-chess-board\">'\n+        for i in range(8):\n+            for j in range(8):\n+                square_color = (\n+                    \"mini-white-square\" if (i + j) % 2 == 0 else \"mini-black-square\"\n+                )\n+                piece = board_array[i][j]\n+                piece_unicode = pieces.get(piece, \"\")\n+\n+                # highlighting moves\n+                highlight_class = \"\"\n+                if (i, j) == from_coords:\n+                    highlight_class = f\" move-from {move_color_class}\"\n+                elif (i, j) == to_coords:\n+                    highlight_class = f\" move-to {move_color_class}\"\n+\n+                html += f'<div class=\"mini-square {square_color}{highlight_class}\">'\n+                if piece_unicode:\n+                    piece_color = \"white-piece\" if piece.isupper() else \"black-piece\"\n+                    html += (\n+                        f'<span class=\"mini-piece {piece_color}\">{piece_unicode}</span>'\n+                    )\n+                html += \"</div>\"\n+        html += \"</div>\"\n+\n+        if move.get(\"description\"):\n+            html += f'<div class=\"description\">{move[\"description\"]}</div>'\n+\n+        html += \"</div>\"\n+\n+    html += \"</div>\"\n+\n+    st.markdown(html, unsafe_allow_html=True)\n+\n+\n+def parse_move(move_text: str) -> str:\n+    \"\"\"\n+    Parse a move from agent response.\n+\n+    Args:\n+        move_text: Text containing the move\n+\n+    Returns:\n+        Extracted move in UCI format\n+    \"\"\"\n+    move_text = move_text.strip()\n+\n+    # If the move is already in UCI format (e.g., \"e2e4\"), return it\n+    if (\n+        len(move_text) == 4\n+        and move_text[0].isalpha()\n+        and move_text[1].isdigit()\n+        and move_text[2].isalpha()\n+        and move_text[3].isdigit()\n+    ):\n+        return move_text\n+\n+    # Try to extract the move from text\n+    import re\n+\n+    move_match = re.search(r\"([a-h][1-8][a-h][1-8])\", move_text)\n+    if move_match:\n+        return move_match.group(1)\n+\n+    return move_text\n+\n+\n+def is_claude_thinking_model(agent: Agent) -> bool:\n+    \"\"\"\n+    Args:\n+        agent: The agent to check\n+    Returns:\n+        bool: True if the agent uses a Claude model with thinking enabled\n+    \"\"\"\n+    return (\n+        hasattr(agent.model, \"id\")\n+        and isinstance(agent.model.id, str)\n+        and \"claude\" in agent.model.id.lower()\n+        and \"thinking\" in agent.model.id.lower()\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/game_generator/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/game_generator/.gitignore b/cookbook/examples/streamlit_apps/game_generator/.gitignore\nnew file mode 100644\nindex 000000000..2d19fc766\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/game_generator/.gitignore\n@@ -0,0 +1 @@\n+*.html\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/game_generator/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/game_generator/__init__.py b/cookbook/examples/streamlit_apps/game_generator/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/game_generator/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/game_generator/app.py b/cookbook/examples/streamlit_apps/game_generator/app.py\nnew file mode 100644\nindex 000000000..208b1904c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/game_generator/app.py\n@@ -0,0 +1,90 @@\n+from pathlib import Path\n+\n+import streamlit as st\n+from agno.utils.string import hash_string_sha256\n+from game_generator import GameGenerator, SqliteWorkflowStorage\n+\n+st.set_page_config(\n+    page_title=\"HTML5 Game Generator\",\n+    page_icon=\"\ud83c\udfae\",\n+    layout=\"wide\",\n+)\n+\n+\n+st.title(\"Game Generator\")\n+st.markdown(\"##### \ud83c\udfae built using [Agno](https://github.com/agno-agi/agno)\")\n+\n+\n+def main() -> None:\n+    game_description = st.sidebar.text_area(\n+        \"\ud83c\udfae Describe your game\",\n+        value=\"An asteroids game. Make sure the asteroids move randomly and are random sizes.\",\n+        height=100,\n+    )\n+\n+    generate_game = st.sidebar.button(\"Generate Game! \ud83d\ude80\")\n+\n+    st.sidebar.markdown(\"## Example Games\")\n+    example_games = [\n+        \"A simple snake game where the snake grows longer as it eats food\",\n+        \"A breakout clone with colorful blocks and power-ups\",\n+        \"A space invaders game with multiple enemy types\",\n+        \"A simple platformer with jumping mechanics\",\n+    ]\n+\n+    for game in example_games:\n+        if st.sidebar.button(game):\n+            st.session_state[\"game_description\"] = game\n+            generate_game = True\n+\n+    if generate_game:\n+        with st.spinner(\"Generating your game... This might take a minute...\"):\n+            try:\n+                hash_of_description = hash_string_sha256(game_description)\n+                game_generator = GameGenerator(\n+                    session_id=f\"game-gen-{hash_of_description}\",\n+                    storage=SqliteWorkflowStorage(\n+                        table_name=\"game_generator_workflows\",\n+                        db_file=\"tmp/workflows.db\",\n+                    ),\n+                )\n+\n+                result = list(game_generator.run(game_description=game_description))\n+\n+                games_dir = Path(__file__).parent.joinpath(\"games\")\n+                game_path = games_dir / \"game_output_file.html\"\n+\n+                if game_path.exists():\n+                    game_code = game_path.read_text()\n+\n+                    with st.status(\n+                        \"Game Generated Successfully!\", expanded=True\n+                    ) as status:\n+                        st.subheader(\"Play the Game\")\n+                        st.components.v1.html(game_code, height=700, scrolling=False)\n+\n+                        st.subheader(\"Game Instructions\")\n+                        st.write(result[-1].content)\n+\n+                        st.download_button(\n+                            label=\"Download Game HTML\",\n+                            data=game_code,\n+                            file_name=\"game.html\",\n+                            mime=\"text/html\",\n+                        )\n+\n+                        status.update(\n+                            label=\"Game ready to play!\",\n+                            state=\"complete\",\n+                            expanded=True,\n+                        )\n+\n+            except Exception as e:\n+                st.error(f\"Failed to generate game: {str(e)}\")\n+\n+    st.sidebar.markdown(\"---\")\n+    if st.sidebar.button(\"Restart\"):\n+        st.rerun()\n+\n+\n+main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/game_generator/game_generator.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/game_generator/game_generator.py b/cookbook/examples/streamlit_apps/game_generator/game_generator.py\nnew file mode 100644\nindex 000000000..d640d6995\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/game_generator/game_generator.py\n@@ -0,0 +1,147 @@\n+\"\"\"\n+1. Install dependencies using: `pip install openai agno`\n+2. Run the script using: `python cookbook/examples/streamlit/game_generator/game_generator.py`\n+\"\"\"\n+\n+import json\n+from pathlib import Path\n+from typing import Iterator\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.openai import OpenAIChat\n+from agno.run.response import RunEvent\n+from agno.storage.workflow.sqlite import SqliteWorkflowStorage\n+from agno.utils.log import logger\n+from agno.utils.pprint import pprint_run_response\n+from agno.utils.string import hash_string_sha256\n+from agno.utils.web import open_html_file\n+from agno.workflow import Workflow\n+from pydantic import BaseModel, Field\n+\n+games_dir = Path(__file__).parent.joinpath(\"games\")\n+games_dir.mkdir(parents=True, exist_ok=True)\n+game_output_path = games_dir / \"game_output_file.html\"\n+game_output_path.unlink(missing_ok=True)\n+\n+\n+class GameOutput(BaseModel):\n+    reasoning: str = Field(..., description=\"Explain your reasoning\")\n+    code: str = Field(..., description=\"The html5 code for the game\")\n+    instructions: str = Field(..., description=\"Instructions how to play the game\")\n+\n+\n+class QAOutput(BaseModel):\n+    reasoning: str = Field(..., description=\"Explain your reasoning\")\n+    correct: bool = Field(False, description=\"Does the game pass your criteria?\")\n+\n+\n+class GameGenerator(Workflow):\n+    # This description is only used in the workflow UI\n+    description: str = \"Generator for single-page HTML5 games\"\n+\n+    game_developer: Agent = Agent(\n+        name=\"Game Developer Agent\",\n+        description=\"You are a game developer that produces working HTML5 code.\",\n+        model=OpenAIChat(id=\"gpt-4o\"),\n+        instructions=[\n+            \"Create a game based on the user's prompt. \"\n+            \"The game should be HTML5, completely self-contained and must be runnable simply by opening on a browser\",\n+            \"Ensure the game has a alert that pops up if the user dies and then allows the user to restart or exit the game.\",\n+            \"add full screen mode to the game\",\n+            \"Ensure instructions for the game are displayed on the HTML page.\"\n+            \"Use user-friendly colours and make the game canvas large enough for the game to be playable on a larger screen.\",\n+        ],\n+        response_model=GameOutput,\n+    )\n+\n+    qa_agent: Agent = Agent(\n+        name=\"QA Agent\",\n+        model=OpenAIChat(id=\"gpt-4o\"),\n+        description=\"You are a game QA and you evaluate html5 code for correctness.\",\n+        instructions=[\n+            \"You will be given some HTML5 code.\"\n+            \"Your task is to read the code and evaluate it for correctness, but also that it matches the original task description.\",\n+        ],\n+        response_model=QAOutput,\n+    )\n+\n+    def run(self, game_description: str) -> Iterator[RunResponse]:\n+        logger.info(f\"Game description: {game_description}\")\n+\n+        game_output = self.game_developer.run(game_description)\n+\n+        if (\n+            game_output\n+            and game_output.content\n+            and isinstance(game_output.content, GameOutput)\n+        ):\n+            game_code = game_output.content.code\n+            logger.info(f\"Game code: {game_code}\")\n+        else:\n+            yield RunResponse(\n+                run_id=self.run_id,\n+                event=RunEvent.workflow_completed,\n+                content=\"Sorry, could not generate a game.\",\n+            )\n+            return\n+\n+        logger.info(\"QA'ing the game code\")\n+        qa_input = {\n+            \"game_description\": game_description,\n+            \"game_code\": game_code,\n+        }\n+        qa_output = self.qa_agent.run({\"role\": \"user\", \"content\": json.dumps(qa_input)})\n+\n+        if qa_output and qa_output.content and isinstance(qa_output.content, QAOutput):\n+            logger.info(qa_output.content)\n+            if not qa_output.content.correct:\n+                raise Exception(f\"QA failed for code: {game_code}\")\n+\n+            # Store the resulting code\n+            game_output_path.write_text(game_code)\n+\n+            yield RunResponse(\n+                run_id=self.run_id,\n+                event=RunEvent.workflow_completed,\n+                content=game_output.content.instructions,\n+            )\n+        else:\n+            yield RunResponse(\n+                run_id=self.run_id,\n+                event=RunEvent.workflow_completed,\n+                content=\"Sorry, could not QA the game.\",\n+            )\n+            return\n+\n+\n+# Run the workflow if the script is executed directly\n+if __name__ == \"__main__\":\n+    from rich.prompt import Prompt\n+\n+    game_description = Prompt.ask(\n+        \"[bold]Describe the game you want to make (keep it simple)[/bold]\\n\u2728\",\n+        # default=\"An asteroids game.\"\n+        default=\"An asteroids game. Make sure the asteroids move randomly and are random sizes. They should continually spawn more and become more difficult over time. Keep score. Make my spaceship's movement realistic.\",\n+    )\n+\n+    hash_of_description = hash_string_sha256(game_description)\n+\n+    # Initialize the investment analyst workflow\n+    game_generator = GameGenerator(\n+        session_id=f\"game-gen-{hash_of_description}\",\n+        storage=SqliteWorkflowStorage(\n+            table_name=\"game_generator_workflows\",\n+            db_file=\"tmp/workflows.db\",\n+        ),\n+    )\n+\n+    # Execute the workflow\n+    result: Iterator[RunResponse] = game_generator.run(\n+        game_description=game_description\n+    )\n+\n+    # Print the report\n+    pprint_run_response(result)\n+\n+    if game_output_path.exists():\n+        open_html_file(game_output_path)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/game_generator/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/game_generator/requirements.txt b/cookbook/examples/streamlit_apps/game_generator/requirements.txt\nnew file mode 100644\nindex 000000000..171e1262f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/game_generator/requirements.txt\n@@ -0,0 +1,4 @@\n+agno\n+openai\n+streamlit\n+\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/__init__.py b/cookbook/examples/streamlit_apps/gemini-tutor/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/agents.py b/cookbook/examples/streamlit_apps/gemini-tutor/agents.py\nnew file mode 100644\nindex 000000000..5af2db03b\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/agents.py\n@@ -0,0 +1,145 @@\n+\"\"\"\n+Gemini Tutor: Advanced Educational AI Assistant powered by Gemini 2.5\n+\"\"\"\n+\n+import json\n+import uuid\n+from pathlib import Path\n+from typing import Any, Dict, Optional\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.google import Gemini\n+from agno.models.message import Message\n+from agno.tools.file import FileTools\n+from agno.tools.googlesearch import GoogleSearchTools\n+from agno.utils.log import logger\n+\n+# Import prompt templates\n+from prompts import (\n+    SEARCH_GROUNDING_INSTRUCTIONS,\n+    TUTOR_DESCRIPTION_TEMPLATE,\n+    TUTOR_INSTRUCTIONS_TEMPLATE,\n+)\n+\n+\n+class TutorAppAgent:\n+    \"\"\"\n+    Central agent that handles all tutoring functionality.\n+    Offloads search, content preparation, and learning experience generation.\n+    \"\"\"\n+\n+    def __init__(\n+        self, model_id=\"gemini-2.5-pro-exp-03-25\", education_level=\"High School\"\n+    ):\n+        \"\"\"\n+        Initialize the TutorAppAgent.\n+\n+        Args:\n+            model_id: Model identifier to use\n+            education_level: Target education level for content\n+        \"\"\"\n+        self.model_id = model_id\n+        self.education_level = education_level\n+        self.agent = self._create_agent()\n+        logger.info(\n+            f\"TutorAppAgent initialized with {model_id} model and {education_level} education level\"\n+        )\n+\n+    def _create_agent(self):\n+        \"\"\"Create and configure the agent with all necessary capabilities\"\"\"\n+\n+        gemini_model = Gemini(\n+            id=self.model_id,\n+            temperature=1,\n+            top_p=0.9,\n+            top_k=40,\n+        )\n+\n+        # Enable grounding if supported by the model\n+        if \"gemini-2.\" in self.model_id or \"gemini-1.5\" in self.model_id:\n+            try:\n+                setattr(gemini_model, \"grounding\", True)\n+                logger.info(\"Enabled model grounding (google_search_retrieval)\")\n+            except AttributeError:\n+                logger.warning(\n+                    f\"Model {self.model_id} does not support grounding attribute.\"\n+                )\n+            except Exception as e:\n+                logger.warning(f\"Could not enable model grounding: {e}\")\n+\n+        # Format description and instructions directly\n+        tutor_description = TUTOR_DESCRIPTION_TEMPLATE.format(\n+            education_level=self.education_level\n+        )\n+        tutor_instructions = TUTOR_INSTRUCTIONS_TEMPLATE.format(\n+            education_level=self.education_level,\n+        )\n+\n+        # Create agent with tutor capabilities, passing the created model\n+        return Agent(\n+            name=\"Gemini Tutor\",\n+            model=gemini_model,\n+            session_id=str(uuid.uuid4()),\n+            read_chat_history=True,\n+            read_tool_call_history=True,\n+            add_history_to_messages=True,\n+            num_history_responses=5,\n+            description=tutor_description,  # Pass formatted description\n+            instructions=tutor_instructions,  # Pass formatted instructions\n+            debug_mode=True,\n+            markdown=True,\n+        )\n+\n+    def create_learning_experience(self, search_topic, education_level=None):\n+        \"\"\"\n+        Create a complete learning experience from search topic to final content.\n+        This method offloads the entire process to the agent.\n+\n+        Args:\n+            search_topic: The topic to create a learning experience for\n+            education_level: Override the default education level for this specific call.\n+\n+        Returns:\n+            The learning experience response from the agent\n+        \"\"\"\n+        # Determine the education level for this specific request\n+        current_education_level = education_level or self.education_level\n+        if education_level and self.education_level != education_level:\n+            logger.info(\n+                f\"Using temporary education level for this request: {education_level}\"\n+            )\n+        else:\n+            # Use the agent's default education level if not overridden\n+            current_education_level = self.education_level\n+\n+        logger.info(\n+            f\"Creating learning experience for '{search_topic}' at {current_education_level} level\"\n+        )\n+\n+        # Construct a focused prompt for the agent, relying on its core instructions\n+        grounding_instructions = (\n+            SEARCH_GROUNDING_INSTRUCTIONS\n+            if \"gemini-2.\" in self.model_id or \"gemini-1.5\" in self.model_id\n+            else \"\"\n+        )\n+        # The agent's core instructions (set during init) already contain formatting rules.\n+        # This prompt focuses on the specific task.\n+        prompt = f\"\"\"\n+        Create a complete and engaging learning experience about '{search_topic}' specifically tailored for {current_education_level} students.\n+\n+        **Task:**\n+        Generate a comprehensive learning module covering the key aspects of '{search_topic}'.\n+\n+        **Follow your core instructions regarding:**\n+        *   Adapting content complexity and style for the {current_education_level} level.\n+        *   Structuring the response logically (introduction, key concepts, examples, etc.).\n+        *   Including interactive elements (thought experiments/questions) and assessments (2-3 simple questions with answers).\n+        *   Strictly adhering to the rules for embedding images and videos (using direct, stable URLs only or omitting embeds).\n+        *   Citing up to 5 key sources if external information was used.\n+\n+        {grounding_instructions}\n+        \"\"\"\n+\n+        # Create message\n+        user_message = Message(role=\"user\", content=prompt)\n+        return self.agent.run(prompt=prompt, messages=[user_message], stream=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/app.py b/cookbook/examples/streamlit_apps/gemini-tutor/app.py\nnew file mode 100644\nindex 000000000..cac6a44a1\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/app.py\n@@ -0,0 +1,293 @@\n+\"\"\"\n+Gemini Tutor: Advanced Educational AI Assistant with Multimodal Learning\n+\"\"\"\n+\n+import os\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import TutorAppAgent\n+from agno.utils.log import logger\n+from utils import display_grounding_metadata, display_tool_calls\n+\n+# Initialize asyncio support\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Gemini Multimodal Tutor\",\n+    page_icon=\"\ud83e\udde0\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# --- Constants ---\n+MODEL_OPTIONS = {\n+    \"Gemini 2.5 Pro Experimental (Recommended)\": \"gemini-2.5-pro-exp-03-25\",\n+    \"Gemini 2.0 Pro\": \"gemini-2.0-pro\",\n+    \"Gemini 2.0 Pro\": \"gemini-2.0-pro\",\n+    \"Gemini 1.5 Pro\": \"gemini-1.5-pro\",\n+}\n+\n+EDUCATION_LEVELS = [\n+    \"Elementary School\",\n+    \"High School\",\n+    \"College\",\n+    \"Graduate\",\n+    \"PhD\",\n+]\n+\n+\n+def initialize_session_state():\n+    \"\"\"Initialize Streamlit session state variables if they don't exist.\"\"\"\n+    if \"tutor_agent\" not in st.session_state:\n+        st.session_state.tutor_agent = None\n+    if \"model_id\" not in st.session_state:\n+        st.session_state.model_id = MODEL_OPTIONS[\n+            \"Gemini 2.5 Pro Experimental (Recommended)\"\n+        ]\n+    if \"education_level\" not in st.session_state:\n+        st.session_state.education_level = \"High School\"\n+    if \"messages\" not in st.session_state:\n+        st.session_state.messages = []\n+    if \"processing\" not in st.session_state:\n+        st.session_state.processing = False\n+    if \"agent_needs_reset\" not in st.session_state:\n+        st.session_state.agent_needs_reset = True  # Start needing initialization\n+\n+\n+def render_sidebar():\n+    \"\"\"Render the sidebar for configuration and reset.\"\"\"\n+    with st.sidebar:\n+        st.header(\"Configuration\")\n+\n+        # Store previous values to detect changes\n+        prev_model_id = st.session_state.model_id\n+        prev_education_level = st.session_state.education_level\n+\n+        selected_model_name = st.selectbox(\n+            \"Select Gemini Model\",\n+            options=list(MODEL_OPTIONS.keys()),\n+            index=list(MODEL_OPTIONS.values()).index(\n+                st.session_state.model_id\n+            ),  # Maintain selection\n+            key=\"selected_model_name\",\n+        )\n+        st.session_state.model_id = MODEL_OPTIONS[selected_model_name]\n+\n+        st.session_state.education_level = st.selectbox(\n+            \"Select Education Level\",\n+            options=EDUCATION_LEVELS,\n+            index=EDUCATION_LEVELS.index(\n+                st.session_state.education_level\n+            ),  # Maintain selection\n+            key=\"education_level_selector\",\n+        )\n+\n+        # Check if settings changed\n+        if (\n+            st.session_state.model_id != prev_model_id\n+            or st.session_state.education_level != prev_education_level\n+        ):\n+            st.session_state.agent_needs_reset = True\n+            st.info(\n+                \"Settings changed. Agent will be updated on next interaction or reset.\"\n+            )\n+\n+        if st.button(\"New chat\", key=\"apply_reset\"):\n+            st.session_state.agent_needs_reset = True\n+            st.session_state.messages = []  # Clear history on reset\n+            st.toast(\"Settings applied. Agent updated and chat reset.\")\n+\n+\n+def initialize_or_update_agent():\n+    \"\"\"Initialize or update the agent if settings have changed.\"\"\"\n+    if st.session_state.agent_needs_reset or st.session_state.tutor_agent is None:\n+        logger.info(\n+            f\"Initializing/Updating Tutor Agent: Model={st.session_state.model_id}, Level={st.session_state.education_level}\"\n+        )\n+        try:\n+            st.session_state.tutor_agent = TutorAppAgent(\n+                model_id=st.session_state.model_id,\n+                education_level=st.session_state.education_level,\n+            )\n+            st.session_state.agent_needs_reset = False\n+        except Exception as e:\n+            st.error(f\"Failed to initialize agent: {e}\")\n+            st.session_state.tutor_agent = None\n+            st.stop()\n+\n+\n+def render_chat_history():\n+    \"\"\"Display the chat messages stored in session state.\"\"\"\n+    st.markdown(\"### Learning Session\")\n+    for message in st.session_state.messages:\n+        # Skip empty messages if any occurred (e.g., during stream error)\n+        if (\n+            not message.get(\"content\")\n+            and not message.get(\"tools\")\n+            and not message.get(\"citations\")\n+        ):\n+            continue\n+        with st.chat_message(message[\"role\"]):\n+            # Display content if it exists\n+            if message.get(\"content\"):\n+                st.markdown(message[\"content\"])\n+\n+            # If assistant message, display tools and citations *within the same bubble*\n+            if message[\"role\"] == \"assistant\":\n+                if message.get(\"tools\"):\n+                    with st.expander(\"\ud83d\udee0\ufe0f Tool Calls\", expanded=False):\n+                        display_tool_calls(message[\"tools\"])\n+                if message.get(\"citations\"):\n+                    display_grounding_metadata(message[\"citations\"])\n+\n+\n+def handle_user_input():\n+    \"\"\"Render the chat input form and handle submission.\"\"\"\n+    with st.form(key=\"topic_form\"):\n+        search_topic = st.text_input(\n+            \"What would you like to learn about?\",\n+            key=\"search_topic_input\",\n+            placeholder=\"e.g., Quantum Physics, History of Rome, Python programming\",\n+        )\n+        submitted = st.form_submit_button(\"Start Learning\", type=\"primary\")\n+\n+        if submitted and search_topic and not st.session_state.processing:\n+            st.session_state.processing = True\n+            user_message = {\n+                \"role\": \"user\",\n+                \"content\": f\"Teach me about: {search_topic}\",\n+            }\n+            st.session_state.messages.append(user_message)\n+            st.rerun()  # Rerun to display user message immediately\n+\n+\n+def process_agent_response():\n+    \"\"\"Process the agent response if the last message was from the user.\"\"\"\n+    if (\n+        st.session_state.processing\n+        and st.session_state.messages\n+        and st.session_state.messages[-1][\"role\"] == \"user\"\n+    ):\n+        if st.session_state.tutor_agent is None:\n+            st.error(\"Agent is not initialized. Cannot process request.\")\n+            st.session_state.processing = False\n+            return\n+\n+        try:\n+            search_topic = st.session_state.messages[-1][\"content\"].replace(\n+                \"Teach me about: \", \"\"\n+            )\n+\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response_stream = (\n+                    st.session_state.tutor_agent.create_learning_experience(\n+                        search_topic=search_topic,\n+                        education_level=st.session_state.education_level,\n+                    )\n+                )\n+\n+                st.session_state.current_tools = None\n+                st.session_state.current_citations = None\n+\n+                def stream_handler(stream_generator):\n+                    logger.info(\"Starting stream processing...\")\n+                    full_content = \"\"\n+                    for chunk in stream_generator:\n+                        content_delta = getattr(chunk, \"content\", None)\n+                        if content_delta:\n+                            full_content += content_delta\n+                            yield content_delta\n+                        tools = getattr(chunk, \"tools\", None)\n+                        if tools:\n+                            st.session_state.current_tools = tools\n+                        citations = getattr(chunk, \"citations\", None)\n+                        if citations:\n+                            st.session_state.current_citations = citations\n+                    logger.info(\"Finished stream processing.\")\n+                    st.session_state.full_content_from_stream = full_content\n+\n+                with st.chat_message(\"assistant\"):\n+                    st.write_stream(stream_handler(response_stream))\n+\n+                assistant_message = {\"role\": \"assistant\"}\n+                full_content = st.session_state.pop(\n+                    \"full_content_from_stream\", \"[No content received]\"\n+                )\n+                assistant_message[\"content\"] = (\n+                    full_content if full_content else \"[No content received]\"\n+                )\n+                if not full_content:\n+                    logger.warning(\"Stream finished with no content.\")\n+\n+                # --- Post-processing to remove duplicate grounding sources ---\n+                grounding_marker = \"\\n\ud83c\udf10 Sources\"\n+                if grounding_marker in full_content:\n+                    logger.info(\"Removing duplicate grounding sources section.\")\n+                    full_content = full_content.split(grounding_marker)[0].rstrip()\n+                # -------------------------------------------------------------\n+\n+                final_tools = st.session_state.pop(\"current_tools\", None)\n+                final_citations = st.session_state.pop(\"current_citations\", None)\n+                if final_tools:\n+                    assistant_message[\"tools\"] = final_tools\n+                if final_citations:\n+                    assistant_message[\"citations\"] = final_citations\n+\n+                st.session_state.final_assistant_message = assistant_message\n+\n+            if \"final_assistant_message\" in st.session_state:\n+                st.session_state.messages.append(\n+                    st.session_state.pop(\"final_assistant_message\")\n+                )\n+\n+        except Exception as e:\n+            logger.error(f\"Error during agent run: {e}\", exc_info=True)\n+            st.session_state.messages.append(\n+                {\"role\": \"assistant\", \"content\": f\"An error occurred: {e}\"}\n+            )\n+        finally:\n+            st.session_state.processing = False\n+            st.rerun()\n+\n+\n+# Custom CSS\n+CUSTOM_CSS = \"\"\"\n+<style>\n+    .main-title {\n+        font-size: 2.5rem;\n+        font-weight: 600;\n+        margin-bottom: 0.5rem;\n+        color: #5186EC;\n+    }\n+    .subtitle {\n+        font-size: 1.2rem;\n+        font-weight: 400;\n+        margin-bottom: 2rem;\n+        opacity: 0.8;\n+    }\n+    [data-testid=\"stChatMessageContent\"] img {\n+        max-width: 350px;\n+        max-height: 300px;\n+        display: block;\n+        margin-top: 10px;\n+        margin-bottom: 10px;\n+        border-radius: 5px;\n+    }\n+</style>\n+\"\"\"\n+\n+\n+st.title(\"\ud83d\udd0d Gemini Tutor \ud83d\udcda\")\n+st.markdown(\n+    '<p class=\"subtitle\">Your AI-powered guide for exploring any topic</p>',\n+    unsafe_allow_html=True,\n+)\n+initialize_session_state()\n+render_sidebar()\n+initialize_or_update_agent()\n+render_chat_history()\n+handle_user_input()\n+process_agent_response()\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/generate_requirements.sh b/cookbook/examples/streamlit_apps/gemini-tutor/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/prompts.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/prompts.py b/cookbook/examples/streamlit_apps/gemini-tutor/prompts.py\nnew file mode 100644\nindex 000000000..124558693\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/prompts.py\n@@ -0,0 +1,46 @@\n+\"\"\"\n+Prompt templates for the Gemini Tutor application.\n+\"\"\"\n+\n+# Instructions specific to using search grounding\n+SEARCH_GROUNDING_INSTRUCTIONS = \"\"\"\n+Use search to get accurate, up-to-date information and cite your sources **as specified in the formatting instructions**.\n+\"\"\"\n+\n+# Base template for the tutor description\n+# The {education_level} will be formatted in agents.py\n+TUTOR_DESCRIPTION_TEMPLATE = \"\"\"You are expertGemini Tutor, an educational AI assistant that provides personalized\n+learning for {education_level} students. You can analyze text and content\n+to create comprehensive learning experiences.\"\"\"\n+\n+# Base template for the tutor's core instructions\n+# The {education_level} will be formatted in agents.py\n+TUTOR_INSTRUCTIONS_TEMPLATE = \"\"\"\n+**Your Role: Expert Gemini Tutor**\n+You are an expert educational AI assistant designed to create personalized and engaging learning experiences for {education_level} students. Your goal is to foster understanding, not just present information. Adapt your tone, vocabulary, depth, and examples appropriately for the specified education level.\n+\n+**Core Task: Create Learning Experiences**\n+1.  **Understand & Research:** Analyze the user's query/topic. Use grounded search (if available) to gather accurate, up-to-date information. If the topic is ambiguous, either ask a clarifying question or make a reasonable assumption and state it clearly.\n+2.  **Structure the Content:** Organize the information logically with clear headings, introductions, explanations of key concepts, and summaries. Use Markdown for formatting (lists, emphasis, code blocks, tables).\n+3.  **Explain Clearly:** Provide explanations tailored to the {education_level} level. Use analogies, examples, and simple language where appropriate.\n+4.  **Engage & Assess:** Make the experience interactive. Include:\n+    *   **Interactive Elements:** At least one relevant thought experiment, practical analogy, or open-ended question to stimulate critical thinking.\n+    *   **Assessment:** 2-3 simple assessment questions (e.g., multiple-choice, true/false, or short fill-in-the-blank) with answers provided to check understanding.\n+5.  **Media Integration (Strict Rules):**\n+    *   Enhance explanations with relevant images or videos *only* if you can find stable, direct URLs.\n+    *   **Images:** Use `![Description](URL)`. **CRITICAL: The URL MUST be a direct link to the image file itself (ending in .png, .jpg, .jpeg, .gif). DO NOT use URLs pointing to webpages, intermediate services, or URLs with excessive query parameters.** Prioritize Wikimedia Commons direct file links if available.\n+    *   **Videos:** Use `[Video Title](URL)`. **CRITICAL: ONLY use standard, publicly accessible YouTube video URLs (e.g., https://www.youtube.com/watch?v=...).**\n+    *   **If you cannot find a URL meeting these strict criteria, DO NOT include the markdown embed.** Instead, describe the concept the media would illustrate or mention it textually (e.g., \"A helpful diagram showing X can be found online\").\n+6.  **Cite Sources:** Ensure factual accuracy. If you used search results or specific external documents to answer, cite **no more than 5** of the most relevant sources in a 'Sources' section at the end. Use the format: `* [Source Title](URL)`. **CRITICAL: Ensure this is the *only* list of sources provided. Do not include any automatically generated source lists (e.g., those labeled '\ud83c\udf10 Sources') that might come from the search tool.**\n+7.  **Formatting:** Follow the specific following formatting instructions provided in the user prompt for overall structure and citations.\n+\n+Format your response as Markdown with:\n+- Clear headings and subheadings\n+- Lists and emphasis for important concepts\n+- Tables and code blocks when relevant\n+- Only provide sources if you used them to answer the question. Limit to 5 sources.\n+- **Source Citations:** At the end of your response, include a 'Sources' section. List **no more than 5** of the most relevant sources you used. Format each source as a markdown link: `* [Source Title](URL)`.\n+- **Images:** Use `![Description](URL)`. **CRITICAL: The URL MUST be a direct link to the image file itself (ending in .png, .jpg, .jpeg, .gif). DO NOT use URLs pointing to webpages, intermediate services, or URLs with excessive query parameters.** Prioritize Wikimedia Commons direct file links if available.\n+- **Videos:** Use `[Video Title](URL)`. **CRITICAL: ONLY use standard, publicly accessible YouTube video URLs (e.g., https://www.youtube.com/watch?v=...).**\n+\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/requirements.in b/cookbook/examples/streamlit_apps/gemini-tutor/requirements.in\nnew file mode 100644\nindex 000000000..73825ee18\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/requirements.in\n@@ -0,0 +1,24 @@\n+# Core dependencies\n+agno>=0.1.0\n+google-genai>=0.3.2\n+streamlit>=1.31.0\n+\n+# Utilities\n+nest-asyncio>=1.5.8\n+python-dotenv>=1.0.0\n+pydantic>=2.0.0\n+typing-extensions>=4.0.0\n+\n+# Media and data handling\n+Pillow>=10.0.0\n+matplotlib>=3.7.0\n+numpy>=1.24.0\n+pandas>=2.0.0\n+\n+# Search and web utilities\n+googlesearch-python>=1.2.3\n+requests>=2.31.0\n+beautifulsoup4>=4.12.0\n+\n+# Optional utilities for enhanced functionality\n+pycountry>=22.1.10  # For handling country data when discussing geography\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/requirements.txt b/cookbook/examples/streamlit_apps/gemini-tutor/requirements.txt\nnew file mode 100644\nindex 000000000..94b5a6d63\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/requirements.txt\n@@ -0,0 +1,221 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.3.2\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.9.0\n+    # via\n+    #   google-genai\n+    #   httpx\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+beautifulsoup4==4.13.4\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   googlesearch-python\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+contourpy==1.3.2\n+    # via matplotlib\n+cycler==0.12.1\n+    # via matplotlib\n+docstring-parser==0.16\n+    # via agno\n+fonttools==4.57.0\n+    # via matplotlib\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.39.0\n+    # via google-genai\n+google-genai==1.11.0\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+googlesearch-python==1.3.0\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   google-genai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+kiwisolver==1.4.8\n+    # via matplotlib\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+matplotlib==3.10.1\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.35.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   contourpy\n+    #   matplotlib\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   matplotlib\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   streamlit\n+pillow==11.2.1\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   matplotlib\n+    #   streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pycountry==24.6.1\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+pydantic==2.11.3\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   agno\n+    #   google-genai\n+    #   pydantic-settings\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyparsing==3.2.3\n+    # via matplotlib\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   matplotlib\n+    #   pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   google-genai\n+    #   googlesearch-python\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9.1\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via anyio\n+soupsieve==2.6\n+    # via beautifulsoup4\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   beautifulsoup4\n+    #   google-genai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via pydantic\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/gemini-tutor/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/gemini-tutor/utils.py b/cookbook/examples/streamlit_apps/gemini-tutor/utils.py\nnew file mode 100644\nindex 000000000..51eaedbd5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/gemini-tutor/utils.py\n@@ -0,0 +1,103 @@\n+\"\"\"\n+Utility functions for Gemini Tutor\n+\"\"\"\n+\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.models.message import Citations\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None, **kwargs\n+) -> None:\n+    \"\"\"\n+    Safely add a message to the session state.\n+\n+    Args:\n+        role: The role of the message sender (user/assistant)\n+        content: The text content of the message\n+        tool_calls: Optional tool calls to include\n+        **kwargs: Additional message attributes (image, audio, video paths)\n+    \"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+\n+    message = {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+\n+    # Add any additional attributes like image, audio, or video paths\n+    for key, value in kwargs.items():\n+        message[key] = value\n+\n+    st.session_state[\"messages\"].append(message)\n+\n+\n+def display_tool_calls(container: Any, tool_calls: List[Dict[str, Any]]) -> None:\n+    \"\"\"\n+    Display tool calls in a formatted way.\n+\n+    Args:\n+        container: Streamlit container to display the tool calls\n+        tool_calls: List of tool call dictionaries\n+    \"\"\"\n+    if not tool_calls:\n+        return\n+\n+    with container:\n+        st.markdown(\"**Tool Calls:**\")\n+\n+        for i, tool_call in enumerate(tool_calls):\n+            # Format the tool call name\n+            tool_name = tool_call.get(\"name\", \"Unknown Tool\")\n+\n+            # Format the args as pretty JSON\n+            args = tool_call.get(\"arguments\", {})\n+            formatted_args = json.dumps(args, indent=2)\n+\n+            expander_label = f\"\ud83d\udccb Tool Call {i + 1}: {tool_name}\"\n+            with st.expander(expander_label, expanded=False):\n+                st.code(formatted_args, language=\"json\")\n+\n+\n+def display_grounding_metadata(citations: Optional[Citations]) -> None:\n+    \"\"\"\n+    Display search grounding metadata (sources) if available.\n+\n+    Args:\n+        citations: Citations object from the agent response chunk or final message.\n+    \"\"\"\n+    # Check if citations object exists and has the 'urls' attribute and it's not empty\n+    if not citations or not hasattr(citations, \"urls\") or not citations.urls:\n+        return\n+\n+    try:\n+        st.markdown(\"---\")\n+        st.markdown(\"### \ud83c\udf10 Sources\")\n+\n+        # Display grounding sources from the pre-parsed list\n+        for citation_url in citations.urls:\n+            # Ensure url and title exist\n+            if (\n+                hasattr(citation_url, \"url\")\n+                and citation_url.url\n+                and hasattr(citation_url, \"title\")\n+                and citation_url.title\n+            ):\n+                st.markdown(f\"- [{citation_url.title}]({citation_url.url})\")\n+            elif (\n+                hasattr(citation_url, \"url\") and citation_url.url\n+            ):  # Fallback if title is missing\n+                st.markdown(f\"- [{citation_url.url}]({citation_url.url})\")\n+\n+        # Optionally, display raw metadata in an expander for debugging if needed\n+        # if hasattr(citations, 'raw') and citations.raw:\n+        #     with st.expander(\"Raw Grounding Metadata (Debug)\"):\n+        #         st.json(citations.raw)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying grounding metadata: {e}\", exc_info=True)\n+        st.warning(\"Could not display sources.\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/geobuddy/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/geobuddy/__init__.py b/cookbook/examples/streamlit_apps/geobuddy/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/geobuddy/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/geobuddy/app.py b/cookbook/examples/streamlit_apps/geobuddy/app.py\nnew file mode 100644\nindex 000000000..4a86f4d0d\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/geobuddy/app.py\n@@ -0,0 +1,89 @@\n+import os\n+from pathlib import Path\n+\n+import streamlit as st\n+from geography_buddy import analyze_image\n+from PIL import Image\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"Geography Location Buddy\",\n+    page_icon=\"\ud83c\udf0d\",\n+)\n+st.title(\"GeoBuddy \ud83c\udf0d\")\n+st.markdown(\"##### :orange_heart: built by [agno](https://github.com/agno-agi/agno)\")\n+st.markdown(\n+    \"\"\"\n+    **Upload your image** and let model guess the location based on visual cues such as landmarks, architecture, and more.\n+    \"\"\"\n+)\n+\n+\n+def main() -> None:\n+    # Sidebar Design\n+    with st.sidebar:\n+        st.markdown(\"<br><br>\", unsafe_allow_html=True)\n+        st.markdown(\"let me guess the location based on visible cues from your image!\")\n+\n+        # Upload Image\n+        uploaded_file = st.file_uploader(\n+            \"\ud83d\udcf7 Upload here..\", type=[\"jpg\", \"jpeg\", \"png\"]\n+        )\n+        st.markdown(\"---\")\n+\n+    # App Logic\n+    if uploaded_file:\n+        col1, col2 = st.columns([1, 2])\n+\n+        # Display Uploaded Image\n+        with col1:\n+            st.markdown(\"#### Uploaded Image\")\n+            image = Image.open(uploaded_file)\n+            resized_image = image.resize((400, 400))\n+            image_path = Path(\"temp_image.png\")\n+            with open(image_path, \"wb\") as f:\n+                f.write(uploaded_file.getbuffer())\n+            st.image(resized_image, caption=\"Your Image\", use_container_width=True)\n+\n+        # Analyze Button and Output\n+        with col2:\n+            st.markdown(\"#### Location Analysis\")\n+            analyze_button = st.button(\"\ud83d\udd0d Analyze Image\")\n+\n+            if analyze_button:\n+                with st.spinner(\"Analyzing the image... please wait.\"):\n+                    try:\n+                        result = analyze_image(image_path)\n+                        if result:\n+                            st.success(\"\ud83c\udf0d Here's my guess:\")\n+                            st.markdown(result)\n+                        else:\n+                            st.warning(\n+                                \"Sorry, I couldn't determine the location. Try another image.\"\n+                            )\n+                    except Exception as e:\n+                        st.error(f\"An error occurred: {e}\")\n+\n+                # Cleanup after analysis\n+                if image_path.exists():\n+                    os.remove(image_path)\n+            else:\n+                st.info(\"Click the **Analyze** button to get started!\")\n+    else:\n+        st.info(\"\ud83d\udcf7 Please upload an image to begin location analysis.\")\n+\n+    # Footer Section\n+    st.markdown(\"---\")\n+    st.markdown(\n+        \"\"\"\n+        **\ud83c\udf1f Features**:\n+        - Identify locations based on uploaded images.\n+        - Advanced reasoning based on landmarks, architecture, and cultural clues.\n+\n+        **\ud83d\udce2 Disclaimer**: GeoBuddy's guesses are based on visual cues and analysis and may not always be accurate.\n+        \"\"\"\n+    )\n+    st.markdown(\":orange_heart: Thank you for using GeoBuddy!\")\n+\n+\n+main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/geobuddy/geography_buddy.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/geobuddy/geography_buddy.py b/cookbook/examples/streamlit_apps/geobuddy/geography_buddy.py\nnew file mode 100644\nindex 000000000..0d3344fb5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/geobuddy/geography_buddy.py\n@@ -0,0 +1,48 @@\n+import os\n+from pathlib import Path\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.google import Gemini\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from dotenv import load_dotenv\n+\n+# Load environment variables\n+load_dotenv()\n+GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n+\n+# Define the query for geography identification\n+geo_query = \"\"\"\n+You are a geography expert. Your task is to analyze the given image and provide a reasoned guess of the location based on visible clues such as:\n+- Landmarks\n+- Architecture\n+- Natural features (mountains, rivers, coastlines)\n+- Language or symbols (text, street signs, billboards, any names mentioned in the picture as clue)\n+- People\u2019s clothing or cultural aspects\n+- Environmental clues like weather, time of day\n+\n+Return in this format:\n+Location Name, City, Country and Reasoning\n+Structure the response in markdown.\n+\n+Instructions:\n+1. Examine the image thoroughly.\n+2. Provide a reasoned guess for the street name, city, state, and country.\n+3. Explain your reasoning in detail by pointing out the visual clues that led to your conclusion.\n+4. If uncertain, offer possible guesses with reasoning.\n+\"\"\"\n+\n+# Initialize the GeoBuddy agent\n+geo_agent = Agent(\n+    model=Gemini(id=\"gemini-2.0-flash-exp\"), tools=[DuckDuckGoTools()], markdown=True\n+)\n+\n+\n+# Function to analyze the image and return location information\n+def analyze_image(image_path: Path) -> Optional[str]:\n+    try:\n+        response = geo_agent.run(geo_query, images=[Image(filepath=image_path)])\n+        return response.content\n+    except Exception as e:\n+        raise RuntimeError(f\"An error occurred while analyzing the image: {e}\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/geobuddy/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/geobuddy/requirements.txt b/cookbook/examples/streamlit_apps/geobuddy/requirements.txt\nnew file mode 100644\nindex 000000000..3efa5fc40\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/geobuddy/requirements.txt\n@@ -0,0 +1,6 @@\n+agno\n+google-generativeai\n+openai\n+streamlit\n+pillow\n+duckduckgo-search\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_mcp_agent/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_mcp_agent/__init__.py b/cookbook/examples/streamlit_apps/github_mcp_agent/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_mcp_agent/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_mcp_agent/agents.py b/cookbook/examples/streamlit_apps/github_mcp_agent/agents.py\nnew file mode 100644\nindex 000000000..f6c74c8e8\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_mcp_agent/agents.py\n@@ -0,0 +1,46 @@\n+import os\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.tools.mcp import MCPTools\n+from mcp import ClientSession, StdioServerParameters\n+from mcp.client.stdio import stdio_client\n+\n+\n+async def run_github_agent(message):\n+    if not os.getenv(\"GITHUB_TOKEN\"):\n+        return \"Error: GitHub token not provided\"\n+\n+    try:\n+        server_params = StdioServerParameters(\n+            command=\"npx\",\n+            args=[\"-y\", \"@modelcontextprotocol/server-github\"],\n+        )\n+\n+        # Create client session\n+        async with stdio_client(server_params) as (read, write):\n+            async with ClientSession(read, write) as session:\n+                # Initialize MCP toolkit\n+                mcp_tools = MCPTools(session=session)\n+                await mcp_tools.initialize()\n+\n+                # Create agent\n+                agent = Agent(\n+                    tools=[mcp_tools],\n+                    instructions=dedent(\"\"\"\\\n+                        You are a GitHub assistant. Help users explore repositories and their activity.\n+                        - Provide organized, concise insights about the repository\n+                        - Focus on facts and data from the GitHub API\n+                        - Use markdown formatting for better readability\n+                        - Present numerical data in tables when appropriate\n+                        - Include links to relevant GitHub pages when helpful\n+                    \"\"\"),\n+                    markdown=True,\n+                    show_tool_calls=True,\n+                )\n+\n+                # Run agent\n+                response = await agent.arun(message)\n+                return response.content\n+    except Exception as e:\n+        return f\"Error: {str(e)}\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_mcp_agent/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_mcp_agent/app.py b/cookbook/examples/streamlit_apps/github_mcp_agent/app.py\nnew file mode 100644\nindex 000000000..5cb153927\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_mcp_agent/app.py\n@@ -0,0 +1,117 @@\n+import asyncio\n+import os\n+\n+import streamlit as st\n+from agents import run_github_agent\n+\n+# Page config\n+st.set_page_config(page_title=\"\ud83d\udc19 GitHub MCP Agent\", page_icon=\"\ud83d\udc19\", layout=\"wide\")\n+\n+# Title and description\n+st.markdown(\"<h1 class='main-header'>\ud83d\udc19 GitHub MCP Agent</h1>\", unsafe_allow_html=True)\n+st.markdown(\n+    \"Explore GitHub repositories with natural language using the Model Context Protocol\"\n+)\n+\n+# Setup sidebar for API key\n+with st.sidebar:\n+    st.header(\"\ud83d\udd11 Authentication\")\n+    github_token = st.text_input(\n+        \"GitHub Token\",\n+        type=\"password\",\n+        help=\"Create a token with repo scope at github.com/settings/tokens\",\n+    )\n+\n+    if github_token:\n+        os.environ[\"GITHUB_TOKEN\"] = github_token\n+\n+    st.markdown(\"---\")\n+    st.markdown(\"### Example Queries\")\n+\n+    st.markdown(\"**Issues**\")\n+    st.markdown(\"- Show me issues by label\")\n+    st.markdown(\"- What issues are being actively discussed?\")\n+\n+    st.markdown(\"**Pull Requests**\")\n+    st.markdown(\"- What PRs need review?\")\n+    st.markdown(\"- Show me recent merged PRs\")\n+\n+    st.markdown(\"**Repository**\")\n+    st.markdown(\"- Show repository health metrics\")\n+    st.markdown(\"- Show repository activity patterns\")\n+\n+    st.markdown(\"---\")\n+    st.caption(\n+        \"Note: Always specify the repository in your query if not already selected in the main input.\"\n+    )\n+\n+# Query input\n+col1, col2 = st.columns([3, 1])\n+with col1:\n+    repo = st.text_input(\"Repository\", value=\"agno-agi/agno\", help=\"Format: owner/repo\")\n+with col2:\n+    query_type = st.selectbox(\n+        \"Query Type\", [\"Issues\", \"Pull Requests\", \"Repository Activity\", \"Custom\"]\n+    )\n+\n+# Create predefined queries based on type\n+if query_type == \"Issues\":\n+    query_template = f\"Find issues labeled as bugs in {repo}\"\n+elif query_type == \"Pull Requests\":\n+    query_template = f\"Show me recent merged PRs in {repo}\"\n+elif query_type == \"Repository Activity\":\n+    query_template = f\"Analyze code quality trends in {repo}\"\n+else:\n+    query_template = \"\"\n+\n+query = st.text_area(\n+    \"Your Query\",\n+    value=query_template,\n+    placeholder=\"What would you like to know about this repository?\",\n+)\n+\n+# Run button\n+if st.button(\"\ud83d\ude80 Run Query\", type=\"primary\", use_container_width=True):\n+    if not github_token:\n+        st.error(\"Please enter your GitHub token in the sidebar\")\n+    elif not query:\n+        st.error(\"Please enter a query\")\n+    else:\n+        with st.spinner(\"Analyzing GitHub repository...\"):\n+            # Ensure the repository is explicitly mentioned in the query\n+            if repo and repo not in query:\n+                full_query = f\"{query} in {repo}\"\n+            else:\n+                full_query = query\n+\n+            result = asyncio.run(run_github_agent(full_query))\n+\n+        # Display results in a nice container\n+        st.markdown(\"### Results\")\n+        st.markdown(result)\n+\n+# Display help text for first-time users\n+if \"result\" not in locals():\n+    st.markdown(\n+        \"\"\"<div class='info-box'>\n+        <h4>How to use this app:</h4>\n+        <ol>\n+            <li>Enter your GitHub token in the sidebar</li>\n+            <li>Specify a repository (e.g., agno-agi/agno)</li>\n+            <li>Select a query type or write your own</li>\n+            <li>Click 'Run Query' to see results</li>\n+        </ol>\n+        <p><strong>Important Notes:</strong></p>\n+        <ul>\n+            <li>The Model Context Protocol (MCP) provides real-time access to GitHub repositories</li>\n+            <li>Queries work best when they focus on specific aspects like issues, PRs, or repository info</li>\n+            <li>More specific queries yield better results</li>\n+            <li>This app requires Node.js to be installed (for the npx command)</li>\n+        </ul>\n+        </div>\"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+# Footer\n+st.markdown(\"---\")\n+st.write(\"Built with Streamlit, Agno, and Model Context Protocol \u2764\ufe0f\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_mcp_agent/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_mcp_agent/generate_requirements.sh b/cookbook/examples/streamlit_apps/github_mcp_agent/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_mcp_agent/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_mcp_agent/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_mcp_agent/requirements.in b/cookbook/examples/streamlit_apps/github_mcp_agent/requirements.in\nnew file mode 100644\nindex 000000000..a3d463785\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_mcp_agent/requirements.in\n@@ -0,0 +1,5 @@\n+streamlit\n+agno\n+mcp\n+openai\n+asyncio\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_mcp_agent/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_mcp_agent/requirements.txt b/cookbook/examples/streamlit_apps/github_mcp_agent/requirements.txt\nnew file mode 100644\nindex 000000000..b76a2e230\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_mcp_agent/requirements.txt\n@@ -0,0 +1,193 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.9\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   httpx\n+    #   mcp\n+    #   openai\n+    #   sse-starlette\n+    #   starlette\n+asyncio==3.4.3\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+    #   uvicorn\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+h11==0.14.0\n+    # via\n+    #   httpcore\n+    #   uvicorn\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   mcp\n+    #   openai\n+httpx-sse==0.4.0\n+    # via mcp\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.8.2\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mcp==1.3.0\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.29.1\n+    # via altair\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.65.4\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+protobuf==5.29.3\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   mcp\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via\n+    #   agno\n+    #   mcp\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+sse-starlette==2.2.1\n+    # via mcp\n+starlette==0.46.0\n+    # via\n+    #   mcp\n+    #   sse-starlette\n+streamlit==1.43.0\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+uvicorn==0.34.0\n+    # via mcp\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/__init__.py b/cookbook/examples/streamlit_apps/github_repo_analyzer/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/agents.py b/cookbook/examples/streamlit_apps/github_repo_analyzer/agents.py\nnew file mode 100644\nindex 000000000..6aed1332f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_repo_analyzer/agents.py\n@@ -0,0 +1,77 @@\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.github import GithubTools\n+\n+\n+def get_github_agent(debug_mode: bool = True) -> Optional[Agent]:\n+    \"\"\"\n+    Args:\n+        repo_name: Optional repository name (\"owner/repo\"). If None, agent relies on user query.\n+        debug_mode: Whether to enable debug mode for tool calls.\n+    \"\"\"\n+\n+    return Agent(\n+        model=OpenAIChat(id=\"gpt-4.1\"),\n+        description=dedent(\"\"\"\n+            You are an expert Code Reviewing Agent specializing in analyzing GitHub repositories,\n+            with a strong focus on detailed code reviews for Pull Requests.\n+            Use your tools to answer questions accurately and provide insightful analysis.\n+        \"\"\"),\n+        instructions=dedent(f\"\"\"\\\n+        **Core Task:** Analyze GitHub repositories and answer user questions based on the available tools and conversation history.\n+\n+        **Repository Context Management:**\n+        1.  **Context Persistence:** Once a target repository (owner/repo) is identified (either initially or from a user query like 'analyze owner/repo'), **MAINTAIN THAT CONTEXT** for all subsequent questions in the current conversation unless the user clearly specifies a *different* repository.\n+        2.  **Determining Context:** If no repository is specified in the *current* user query, **CAREFULLY REVIEW THE CONVERSATION HISTORY** to find the most recently established target repository. Use that repository context.\n+        3.  **Accuracy:** When extracting a repository name (owner/repo) from the query or history, **BE EXTREMELY CAREFUL WITH SPELLING AND FORMATTING**. Double-check against the user's exact input.\n+        4.  **Ambiguity:** If no repository context has been established in the conversation history and the current query doesn't specify one, **YOU MUST ASK THE USER** to clarify which repository (using owner/repo format) they are interested in before using tools that require a repository name.\n+\n+        **How to Answer Questions:**\n+        *   **Identify Key Information:** Understand the user's goal and the target repository (using the context rules above).\n+        *   **Select Appropriate Tools:** Choose the best tool(s) for the task, ensuring you provide the correct `repo_name` argument (owner/repo format, checked for accuracy) if required by the tool.\n+            *   Project Overview: `get_repository`, `get_file_content` (for README.md).\n+            *   Libraries/Dependencies: `get_file_content` (for requirements.txt, pyproject.toml, etc.), `get_directory_content`, `search_code`.\n+            *   PRs/Issues: Use relevant PR/issue tools.\n+            *   List User Repos: `list_repositories` (no repo_name needed).\n+            *   Search Repos: `search_repositories` (no repo_name needed).\n+        *   **Execute Tools:** Run the selected tools.\n+        *   **Synthesize Answer:** Combine tool results into a clear, concise answer using markdown. If a tool fails (e.g., 404 error because the repo name was incorrect), state that you couldn't find the specified repository and suggest checking the name.\n+        *   **Cite Sources:** Mention specific files (e.g., \"According to README.md...\").\n+\n+        **Specific Analysis Areas (Most require a specific repository):**\n+        *   Issues: Listing, summarizing, searching.\n+        *   Pull Requests (PRs): Listing, summarizing, searching, getting details/changes.\n+        *   Code & Files: Searching code, getting file content, listing directory contents.\n+        *   Repository Stats & Activity: Stars, contributors, recent activity.\n+\n+        **Code Review Guidelines (Requires repository and PR):**\n+        *   Fetch Changes: Use `get_pull_request_changes` or `get_pull_request_with_details`.\n+        *   Analyze Patch: Evaluate based on functionality, best practices, style, clarity, efficiency.\n+        *   Present Review: Structure clearly, cite lines/code, be constructive.\n+        \"\"\"),\n+        tools=[\n+            GithubTools(\n+                get_repository=True,\n+                search_repositories=True,\n+                get_pull_request=True,\n+                get_pull_request_changes=True,\n+                list_branches=True,\n+                get_pull_request_count=True,\n+                get_pull_requests=True,\n+                get_pull_request_comments=True,\n+                get_pull_request_with_details=True,\n+                list_issues=True,\n+                get_issue=True,\n+                update_file=True,\n+                get_file_content=True,\n+                get_directory_content=True,\n+                search_code=True,\n+            ),\n+        ],\n+        markdown=True,\n+        debug_mode=debug_mode,\n+        add_history_to_messages=True,\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/app.py b/cookbook/examples/streamlit_apps/github_repo_analyzer/app.py\nnew file mode 100644\nindex 000000000..a19472a35\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_repo_analyzer/app.py\n@@ -0,0 +1,153 @@\n+from os import getenv\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_github_agent\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"GitHub Repo Analyzer\",\n+    page_icon=\"\ud83d\udc68\u200d\ud83d\udcbb\",\n+    layout=\"wide\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    #####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-header'>\ud83d\udc68\u200d\ud83d\udcbb GitHub Repo Analyzer</h1>\", unsafe_allow_html=True\n+    )\n+    st.markdown(\"Analyze GitHub repositories\")\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    github_agent: Agent\n+    if (\n+        \"github_agent\" not in st.session_state\n+        or st.session_state[\"github_agent\"] is None\n+    ):\n+        logger.info(\"---*--- Creating new Github agent ---*---\")\n+        github_agent = get_github_agent()\n+        st.session_state[\"github_agent\"] = github_agent\n+        st.session_state[\"messages\"] = []\n+        st.session_state[\"github_token\"] = getenv(\"GITHUB_ACCESS_TOKEN\")\n+    else:\n+        github_agent = st.session_state[\"github_agent\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        st.session_state[\"github_agent_session_id\"] = github_agent.load_session()\n+    except Exception:\n+        st.warning(\"Could not create Agent session, is the database running?\")\n+        return\n+\n+    ####################################################################\n+    # Load runs from memory (v2 Memory) only on initial load\n+    ####################################################################\n+    if github_agent.memory is not None and not st.session_state.get(\"messages\"):\n+        session_id = st.session_state.get(\"github_agent_session_id\")\n+        # Fetch stored runs for this session\n+        agent_runs = github_agent.memory.get_runs(session_id)\n+        if agent_runs:\n+            logger.debug(\"Loading run history\")\n+            st.session_state[\"messages\"] = []\n+            for run_response in agent_runs:\n+                # Iterate through stored messages in the run\n+                for msg in run_response.messages or []:\n+                    if msg.role in [\"user\", \"assistant\"] and msg.content is not None:\n+                        # Include any tool calls attached to this message\n+                        add_message(\n+                            msg.role, msg.content, getattr(msg, \"tool_calls\", None)\n+                        )\n+        else:\n+            logger.debug(\"No run history found\")\n+            st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me about GitHub repositories!\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = github_agent.run(\n+                        question, stream=True, stream_intermediate_steps=True\n+                    )\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            _resp_chunk.event == \"RunResponse\"\n+                            and _resp_chunk.content is not None\n+                        ):\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, github_agent.run_response.tools)\n+                except Exception as e:\n+                    logger.exception(e)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/generate_requirements.sh b/cookbook/examples/streamlit_apps/github_repo_analyzer/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_repo_analyzer/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.in b/cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.in\nnew file mode 100644\nindex 000000000..fa2c19684\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.in\n@@ -0,0 +1,8 @@\n+# Direct dependencies for the GitHub Repo Chat App\n+agno>=0.1.0\n+PyGithub>=2.1.1\n+python-dotenv>=1.0.0\n+matplotlib>=3.7.2\n+pandas>=2.0.3\n+streamlit>=1.24.0\n+openai>=1.67.0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.txt b/cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.txt\nnew file mode 100644\nindex 000000000..b52ee7f54\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_repo_analyzer/requirements.txt\n@@ -0,0 +1,217 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.4.2\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.9.0\n+    # via\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+cffi==1.17.1\n+    # via\n+    #   cryptography\n+    #   pynacl\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+contourpy==1.3.2\n+    # via matplotlib\n+cryptography==44.0.2\n+    # via pyjwt\n+cycler==0.12.1\n+    # via matplotlib\n+deprecated==1.2.18\n+    # via pygithub\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+fonttools==4.57.0\n+    # via matplotlib\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+kiwisolver==1.4.8\n+    # via matplotlib\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+matplotlib==3.10.1\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.35.0\n+    # via altair\n+numpy==2.2.4\n+    # via\n+    #   contourpy\n+    #   matplotlib\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.75.0\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   matplotlib\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+    #   streamlit\n+pillow==11.2.1\n+    # via\n+    #   matplotlib\n+    #   streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pycparser==2.22\n+    # via cffi\n+pydantic==2.11.3\n+    # via\n+    #   agno\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygithub==2.6.1\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+pygments==2.19.1\n+    # via rich\n+pyjwt==2.10.1\n+    # via pygithub\n+pynacl==1.5.0\n+    # via pygithub\n+pyparsing==3.2.3\n+    # via matplotlib\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   matplotlib\n+    #   pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   pygithub\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   pygithub\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via pydantic\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via\n+    #   pygithub\n+    #   requests\n+wrapt==1.17.2\n+    # via deprecated\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/github_repo_analyzer/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/github_repo_analyzer/utils.py b/cookbook/examples/streamlit_apps/github_repo_analyzer/utils.py\nnew file mode 100644\nindex 000000000..3af5868e3\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/github_repo_analyzer/utils.py\n@@ -0,0 +1,188 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.utils.log import log_debug, log_error, log_info\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Renders the sidebar for configuration and example queries.\"\"\"\n+    with st.sidebar:\n+        # Configuration\n+        st.header(\"Configuration\")\n+\n+        st.markdown(\"**GitHub Token**\")\n+        token_input = st.text_input(\n+            \"Enter your GitHub Personal Access Token (required for most queries):\",\n+            type=\"password\",\n+            key=\"github_token_input\",\n+            value=st.session_state.get(\"github_token\", \"\"),\n+            help=\"Allows the agent to access GitHub API, including your private/org data.\",\n+        )\n+        st.markdown(\n+            \"[How to create a GitHub PAT?](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic)\",\n+            unsafe_allow_html=True,\n+        )\n+\n+        # Update session state if token input changes\n+        current_token_in_state = st.session_state.get(\"github_token\")\n+        if token_input != current_token_in_state and (\n+            token_input or current_token_in_state is not None\n+        ):\n+            st.session_state.github_token = token_input if token_input else None\n+            log_info(\n+                f\"GitHub token updated via sidebar input {'(cleared)' if not token_input else ''}.\"\n+            )\n+            st.session_state.github_agent = None\n+            st.rerun()\n+\n+        st.markdown(\"---\")\n+\n+        st.markdown(\"#### \ud83c\udfc6 Sample Queries\")\n+        if st.button(\"\ud83d\udccb Summarize 'agno-agi/agno'\"):\n+            # Run this query in the current session\n+            add_message(\"user\", \"Summarize 'agno-agi/agno' repo\")\n+        if st.button(\"\ud83e\udd47 List my recent repositories\"):\n+            add_message(\"user\", \"List my recent repositories\")\n+        if st.button(\"\ud83c\udfc6 List latest issues in 'agno-agi/agno' \"):\n+            add_message(\"user\", \"List latest issues in 'agno-agi/agno'\")\n+        if st.button(\"\ud83e\udd47 List recent PRs in 'agno-agi/agno'\"):\n+            add_message(\"user\", \"List recent PRs in 'agno-agi/agno' repo\")\n+        # Chat controls\n+        st.header(\"Chat\")\n+        if st.button(\"\ud83c\udd95 New Chat\"):\n+            # Use restart logic to clear everything and rerun\n+            restart_agent()\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Agno \u2728\")\n+        st.markdown(\"\"\"\n+        Agno is a lightweight library for building Reasoning Agents.\n+\n+        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)\n+        \"\"\")\n+\n+        st.markdown(\"### Need Help?\")\n+        st.markdown(\n+            \"If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community).\"\n+        )\n+\n+\n+def is_json(myjson):\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\", None)\n+                metrics = tool_call.get(\"metrics\", None)\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics is not None and hasattr(metrics, \"time\"):\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.4f}s\"\n+                except Exception as e:\n+                    log_error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content is not None:\n+                        try:\n+                            if is_json(content):\n+                                st.markdown(\"**Results:**\")\n+                                st.json(content)\n+                        except Exception as e:\n+                            log_debug(f\"Skipped tool call content: {e}\")\n+    except Exception as e:\n+        log_error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    log_debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"sql_agent\"] = None\n+    st.session_state[\"sql_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.session_state[\"github_agent\"] = None\n+    st.rerun()\n+\n+\n+# Keep only necessary CSS styles\n+CUSTOM_CSS = \"\"\"\n+<style>\n+    .main-header {\n+        font-size: 2.5rem;\n+        margin-bottom: 1rem;\n+        color: #0366d6;\n+        font-weight: 600;\n+    }\n+    .sub-header {\n+        font-size: 1.5rem;\n+        margin-top: 1rem;\n+        margin-bottom: 0.5rem;\n+        color: #2f363d;\n+        font-weight: 500;\n+    }\n+    .metric-card {\n+        background-color: #f6f8fa;\n+        border-radius: 8px;\n+        padding: 1.2rem;\n+        margin-bottom: 1rem;\n+        border-left: 5px solid #0366d6;\n+    }\n+    .pr-card {\n+        background-color: #f1f8ff;\n+        border-radius: 8px;\n+        padding: 1.2rem;\n+        margin-bottom: 1.2rem;\n+        border-left: 5px solid #6f42c1;\n+    }\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/image_generation/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/image_generation/agents.py b/cookbook/examples/streamlit_apps/image_generation/agents.py\nnew file mode 100644\nindex 000000000..beed5eee7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/image_generation/agents.py\n@@ -0,0 +1,75 @@\n+# Recipe agent for image generation\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.document.reader.pdf_reader import PDFImageReader\n+from agno.embedder.cohere import CohereEmbedder\n+from agno.knowledge.pdf import PDFKnowledgeBase\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.groq import Groq\n+from agno.tools.openai import OpenAITools\n+from agno.vectordb.pgvector import PgVector\n+\n+# Database connection string for recipe knowledge base\n+# Adjust as needed for your environment\n+DB_URL = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+# Constants for recipe agent\n+DEFAULT_RECIPE_URL = \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"\n+DEFAULT_RECIPE_TABLE = \"recipe_documents\"\n+\n+\n+def get_recipe_agent(\n+    local_pdf_path: Optional[str] = None,\n+) -> Agent:\n+    \"\"\"\n+    Returns a RecipeImageAgent backed by a recipe PDF knowledge base.\n+    \"\"\"\n+    # Choose the appropriate knowledge base\n+    if local_pdf_path:\n+        knowledge_base = PDFKnowledgeBase(\n+            path=local_pdf_path,\n+            reader=PDFImageReader(),\n+            vector_db=PgVector(\n+                db_url=DB_URL,\n+                table_name=DEFAULT_RECIPE_TABLE,\n+                embedder=CohereEmbedder(id=\"embed-v4.0\"),\n+            ),\n+        )\n+    else:\n+        knowledge_base = PDFUrlKnowledgeBase(\n+            urls=[DEFAULT_RECIPE_URL],\n+            vector_db=PgVector(\n+                db_url=DB_URL,\n+                table_name=DEFAULT_RECIPE_TABLE,\n+                embedder=CohereEmbedder(id=\"embed-v4.0\"),\n+            ),\n+        )\n+\n+    model = Groq(id=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n+\n+    # Instantiate and return the recipe agent\n+    return Agent(\n+        name=\"RecipeImageAgent\",\n+        model=model,\n+        knowledge=knowledge_base,\n+        tools=[OpenAITools(image_model=\"gpt-image-1\")],\n+        instructions=[\n+            dedent(\"\"\"\\\n+            You are a specialized recipe assistant.\n+            When asked for a recipe:\n+            1. Use the `search_knowledge_base` tool to find and load the most relevant recipe from the knowledge base.\n+            2. Extract and output exactly two formatted markdown sections:\n+               ## Ingredients\n+               - List each ingredient with a hyphen and space prefix.\n+               ## Directions\n+               1. Describe each cooking step succinctly, numbering steps starting at 1.\n+            3. After listing the Directions, invoke the `generate_image` tool exactly once, passing the entire recipe text and using a prompt like '<DishName>: a step-by-step visual guide showing all steps in one overhead image with bright natural lighting. In the prompt make sure to include the all the recipe ingredients and directions that were listed in the Ingredients and Directions sections.'.\n+            4. Maintain a consistent visual style across the image.\n+            5. After the image is generated, conclude with 'Recipe generation complete.'\n+        \"\"\"),\n+        ],\n+        markdown=True,\n+        debug_mode=True,\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/image_generation/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/image_generation/app.py b/cookbook/examples/streamlit_apps/image_generation/app.py\nnew file mode 100644\nindex 000000000..50f01ccf3\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/image_generation/app.py\n@@ -0,0 +1,193 @@\n+import base64\n+import io\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_recipe_agent\n+from agno.utils.log import logger\n+from PIL import Image\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    example_inputs,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"Recipe Image Generator\",\n+    page_icon=\"\ud83c\udf73\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Recipe Image Generator</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>Upload your recipe PDF or use the default. Ask for a recipe and receive step-by-step images!</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"llama-4-scout\": \"groq:meta-llama/llama-4-scout-17b-16e-instruct\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    example_inputs()\n+    ####################################################################\n+    # Recipe source selector & Agent initialization\n+    ####################################################################\n+    uploaded_file = st.sidebar.file_uploader(\"Upload recipe PDF\", type=[\"pdf\"])\n+    use_default = st.sidebar.checkbox(\n+        \"Use default sample recipe book\", value=(uploaded_file is None)\n+    )\n+    pdf_path = None\n+    if uploaded_file:\n+        import tempfile\n+\n+        tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n+        tf.write(uploaded_file.read())\n+        tf.flush()\n+        pdf_path = tf.name\n+    if use_default:\n+        pdf_path = None\n+\n+    if (\n+        \"recipe_agent\" not in st.session_state\n+        or st.session_state.get(\"pdf_path\") != pdf_path\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Recipe agent ---*---\")\n+        recipe_agent = get_recipe_agent(\n+            local_pdf_path=pdf_path,\n+        )\n+        st.session_state[\"recipe_agent\"] = recipe_agent\n+        st.session_state[\"pdf_path\"] = pdf_path\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        recipe_agent = st.session_state[\"recipe_agent\"]\n+\n+    # Track knowledge load state\n+    if \"knowledge_loaded\" not in st.session_state:\n+        st.session_state[\"knowledge_loaded\"] = False\n+\n+    # Manual load button\n+    if st.sidebar.button(\"Load recipes\"):\n+        st.sidebar.info(\"Loading default recipes...\")\n+        recipe_agent.knowledge.load(recreate=True)\n+        st.session_state[\"knowledge_loaded\"] = True\n+        st.sidebar.success(\"Recipes loaded!\")\n+\n+    # Initialize chat history\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me for a recipe (e.g., 'Recipe for Pad Thai')\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        # Auto-load knowledge if needed\n+        if not st.session_state.get(\"knowledge_loaded\", False):\n+            info = st.info(\"Loading default recipes...\")\n+            recipe_agent.knowledge.load(recreate=True)\n+            st.session_state[\"knowledge_loaded\"] = True\n+            info.empty()\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = recipe_agent.run(\n+                        question, stream=True, stream_intermediate_steps=True\n+                    )\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            _resp_chunk.event == \"RunResponse\"\n+                            and _resp_chunk.content is not None\n+                        ):\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    # Display generated images\n+                    for img in recipe_agent.run_response.images or []:\n+                        # Inline base64 content\n+                        if getattr(img, \"content\", None):\n+                            try:\n+                                # img.content is base64-encoded bytes\n+                                decoded = base64.b64decode(img.content)\n+                                image = Image.open(io.BytesIO(decoded))\n+                                resp_container.image(image)\n+                            except Exception as e:\n+                                logger.error(f\"Failed to render inline image: {e}\")\n+                                # Fallback to URL if available\n+                                if getattr(img, \"url\", None):\n+                                    resp_container.image(img.url)\n+                        # URL fallback\n+                        elif getattr(img, \"url\", None):\n+                            resp_container.image(img.url)\n+                    add_message(\"assistant\", response, recipe_agent.run_response.tools)\n+                except Exception as e:\n+                    logger.exception(e)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/image_generation/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/image_generation/generate_requirements.sh b/cookbook/examples/streamlit_apps/image_generation/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/image_generation/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/image_generation/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/image_generation/requirements.in b/cookbook/examples/streamlit_apps/image_generation/requirements.in\nnew file mode 100644\nindex 000000000..06a09f6b0\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/image_generation/requirements.in\n@@ -0,0 +1,12 @@\n+agno\n+cohere\n+groq\n+httpx\n+nest_asyncio\n+openai\n+pypdf\n+pgvector\n+psycopg[binary]\n+rapidocr-onnxruntime\n+sqlalchemy\n+streamlit\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/image_generation/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/image_generation/requirements.txt b/cookbook/examples/streamlit_apps/image_generation/requirements.txt\nnew file mode 100644\nindex 000000000..bade39e7a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/image_generation/requirements.txt\n@@ -0,0 +1,266 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.4.5\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.9.0\n+    # via\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via streamlit\n+certifi==2025.4.26\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.2\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+cohere==5.15.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+coloredlogs==15.0.1\n+    # via onnxruntime\n+distro==1.9.0\n+    # via\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+fastavro==1.10.0\n+    # via cohere\n+filelock==3.18.0\n+    # via huggingface-hub\n+flatbuffers==25.2.10\n+    # via onnxruntime\n+fsspec==2025.3.2\n+    # via huggingface-hub\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+groq==0.24.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+h11==0.16.0\n+    # via httpcore\n+hf-xet==1.1.0\n+    # via huggingface-hub\n+httpcore==1.0.9\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   -r cookbook/examples/apps/image_generation/requirements.in\n+    #   agno\n+    #   cohere\n+    #   groq\n+    #   openai\n+httpx-sse==0.4.0\n+    # via cohere\n+huggingface-hub==0.31.1\n+    # via tokenizers\n+humanfriendly==10.0\n+    # via coloredlogs\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2025.4.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+mpmath==1.3.0\n+    # via sympy\n+narwhals==1.38.2\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+numpy==2.2.5\n+    # via\n+    #   onnxruntime\n+    #   opencv-python\n+    #   pandas\n+    #   pgvector\n+    #   pydeck\n+    #   rapidocr-onnxruntime\n+    #   shapely\n+    #   streamlit\n+onnxruntime==1.22.0\n+    # via rapidocr-onnxruntime\n+openai==1.78.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+opencv-python==4.11.0.86\n+    # via rapidocr-onnxruntime\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   huggingface-hub\n+    #   onnxruntime\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pgvector==0.4.1\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+pillow==11.2.1\n+    # via\n+    #   rapidocr-onnxruntime\n+    #   streamlit\n+protobuf==6.30.2\n+    # via\n+    #   onnxruntime\n+    #   streamlit\n+psycopg==3.2.7\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+psycopg-binary==3.2.7\n+    # via psycopg\n+pyarrow==20.0.0\n+    # via streamlit\n+pyclipper==1.3.0.post6\n+    # via rapidocr-onnxruntime\n+pydantic==2.11.4\n+    # via\n+    #   agno\n+    #   cohere\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.2\n+    # via\n+    #   cohere\n+    #   pydantic\n+pydantic-settings==2.9.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pypdf==5.4.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via\n+    #   agno\n+    #   huggingface-hub\n+    #   rapidocr-onnxruntime\n+rapidocr-onnxruntime==1.4.4\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   cohere\n+    #   huggingface-hub\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shapely==2.1.0\n+    # via rapidocr-onnxruntime\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via\n+    #   python-dateutil\n+    #   rapidocr-onnxruntime\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+streamlit==1.45.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+sympy==1.14.0\n+    # via onnxruntime\n+tenacity==9.1.2\n+    # via streamlit\n+tokenizers==0.21.1\n+    # via cohere\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   huggingface-hub\n+    #   openai\n+    #   rapidocr-onnxruntime\n+typer==0.15.3\n+    # via agno\n+types-requests==2.32.0.20250328\n+    # via cohere\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   cohere\n+    #   groq\n+    #   huggingface-hub\n+    #   openai\n+    #   psycopg\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via\n+    #   pydantic\n+    #   pydantic-settings\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via\n+    #   requests\n+    #   types-requests\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/image_generation/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/image_generation/utils.py b/cookbook/examples/streamlit_apps/image_generation/utils.py\nnew file mode 100644\nindex 000000000..6ffe10407\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/image_generation/utils.py\n@@ -0,0 +1,132 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.utils.log import logger\n+\n+\n+def is_json(myjson: str) -> bool:\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def add_message(\n+    role: str,\n+    content: str,\n+    tool_calls: Optional[List[Dict[str, Any]]] = None,\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def display_tool_calls(tool_calls_container: Any, tools: List[Dict[str, Any]]) -> None:\n+    \"\"\"Display tool calls in a Streamlit container\"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\")\n+\n+                execution_time_str = \"N/A\"\n+                if metrics is not None and hasattr(metrics, \"time\"):\n+                    t = metrics.time\n+                    execution_time_str = f\"{t:.4f}s\" if t else execution_time_str\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+                    if content is not None and is_json(content):\n+                        st.markdown(\"**Results:**\")\n+                        st.json(content)\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {e}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Recipe Generator \u2728\")\n+        st.markdown(\n+            \"Recipe Image Generator powered by Agno. Upload or use default recipe PDF and get step-by-step visual cooking instructions.\"\n+        )\n+\n+\n+# Added example inputs for recipe generation\n+def example_inputs() -> None:\n+    \"\"\"Show example recipe inputs on the sidebar.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### :sparkles: Try an example recipe\")\n+        if st.button(\"Recipe for Pad Thai\"):\n+            add_message(\"user\", \"Recipe for Pad Thai\")\n+        if st.button(\"Recipe for Som Tum\"):\n+            add_message(\"user\", \"Recipe for Som Tum / Papaya Salad\")\n+        if st.button(\"Recipe for Massaman Curry\"):\n+            add_message(\"user\", \"Recipe for Massaman Curry / Massaman Gai\")\n+        if st.button(\"Recipe for Tom Kha Gai\"):\n+            add_message(\"user\", \"Recipe for Tom Kha Gai\")\n+\n+\n+CUSTOM_CSS = \"\"\"\n+<style>\n+.main-title {\n+    text-align: center;\n+    background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+    -webkit-background-clip: text;\n+    -webkit-text-fill-color: transparent;\n+    font-size: 3em;\n+    font-weight: bold;\n+    padding: 1em 0;\n+}\n+.subtitle {\n+    text-align: center;\n+    color: #666;\n+    margin-bottom: 2em;\n+}\n+.stButton button {\n+    width: 100%;\n+    border-radius: 20px;\n+    margin: 0.2em 0;\n+    transition: all 0.3s ease;\n+}\n+.stButton button:hover {\n+    transform: translateY(-2px);\n+    box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+}\n+.chat-container {\n+    border-radius: 15px;\n+    padding: 1em;\n+    margin: 1em 0;\n+    background-color: #f5f5f5;\n+}\n+.success-message {\n+    background-color: #d4edda;\n+    color: #155724;\n+}\n+.error-message {\n+    background-color: #f8d7da;\n+    color: #721c24;\n+}\n+@media (prefers-color-scheme: dark) {\n+    .chat-container { background-color: #2b2b2b; }\n+}\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/.env.example",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/.env.example b/cookbook/examples/streamlit_apps/llama_tutor/.env.example\nnew file mode 100644\nindex 000000000..543509a25\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/.env.example\n@@ -0,0 +1,2 @@\n+GROQ_API_KEY=<your_groq_api_key>\n+EXA_API_KEY=<your_exa_api_key>\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/.gitignore b/cookbook/examples/streamlit_apps/llama_tutor/.gitignore\nnew file mode 100644\nindex 000000000..52a40145c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/.gitignore\n@@ -0,0 +1,5 @@\n+output\n+agents.db\n+.venv/\n+*.env\n+**/__pycache__/**\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/__init__.py b/cookbook/examples/streamlit_apps/llama_tutor/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/agents.py b/cookbook/examples/streamlit_apps/llama_tutor/agents.py\nnew file mode 100644\nindex 000000000..e6c23c85a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/agents.py\n@@ -0,0 +1,190 @@\n+\"\"\"\n+Llama tutor integrates:\n+  - DuckDuckGoTools for real-time web searches.\n+  - ExaTools for structured, in-depth analysis.\n+  - FileTools for saving the output upon user confirmation.\n+\"\"\"\n+\n+import os\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Optional\n+\n+from dotenv import load_dotenv\n+\n+# Load environment variables from .env file\n+load_dotenv(override=True)\n+\n+# Importing the Agent and model classes\n+from agno.agent import Agent\n+from agno.models.groq import Groq\n+\n+# Importing storage and tool classes\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.file import FileTools\n+\n+# Import the Agent template\n+from prompts import AGENT_DESCRIPTION, AGENT_INSTRUCTIONS, EXPECTED_OUTPUT_TEMPLATE\n+\n+# ************* Setup Paths *************\n+# Define the current working directory and output directory for saving files\n+cwd = Path(__file__).parent\n+output_dir = cwd.joinpath(\"output\")\n+# Create output directory if it doesn't exist\n+output_dir.mkdir(parents=True, exist_ok=True)\n+# Create tmp directory if it doesn't exist\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+# *************************************\n+\n+# ************* Agent Storage *************\n+# Configure SQLite storage for agent sessions\n+agent_storage = SqliteAgentStorage(\n+    table_name=\"answer_engine_sessions\",  # Table to store agent sessions\n+    db_file=str(tmp_dir.joinpath(\"agents.db\")),  # SQLite database file\n+)\n+# *************************************\n+\n+\n+def tutor_agent(\n+    user_id: Optional[str] = None,\n+    model_id: str = \"groq:llama-3.3-70b-versatile\",\n+    session_id: Optional[str] = None,\n+    num_history_responses: int = 5,\n+    debug_mode: bool = True,\n+    education_level: str = \"High School\",\n+) -> Agent:\n+    \"\"\"\n+    Returns an instance of Llama Tutor, an educational AI assistant with integrated tools for web search,\n+    deep contextual analysis, and file management.\n+\n+    Llama Tutor will:\n+      - Use DuckDuckGoTools for real-time web searches and ExaTools for in-depth analysis to gather information.\n+      - Generate comprehensive educational answers tailored to the specified education level that include:\n+          \u2022 Direct, succinct answers appropriate for the student's level.\n+          \u2022 Detailed explanations with supporting evidence.\n+          \u2022 Examples and clarification of common misconceptions.\n+          \u2022 Interactive elements like questions to check understanding.\n+      - Prompt the user:\n+            \"Would you like to save this answer to a file? (yes/no)\"\n+        If confirmed, it will use FileTools to save the answer in markdown format in the output directory.\n+\n+    Args:\n+        user_id: Optional identifier for the user.\n+        model_id: Model identifier in the format 'groq:model_name' (e.g., \"groq:llama-3.3-70b-versatile\").\n+                 Will always use Groq with a Llama model regardless of provider specified.\n+        session_id: Optional session identifier for tracking conversation history.\n+        num_history_responses: Number of previous responses to include for context.\n+        debug_mode: Enable logging and debug features.\n+        education_level: Education level for tailoring responses (e.g., \"Elementary School\", \"High School\", \"College\").\n+\n+    Returns:\n+        An instance of the configured Agent.\n+    \"\"\"\n+\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Always use Groq with Llama model\n+    groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n+\n+    # Default to llama-3.3-70b-versatile if the model name doesn't contain \"llama\"\n+    if \"llama\" not in model_name.lower():\n+        model_name = \"llama-3.3-70b-versatile\"\n+\n+    model = Groq(id=model_name, api_key=groq_api_key)\n+\n+    # Get Exa API key from environment variable\n+    exa_api_key = os.environ.get(\"EXA_API_KEY\")\n+\n+    # Tools for Llama Tutor\n+    tools = [\n+        ExaTools(\n+            api_key=exa_api_key,\n+            start_published_date=datetime.now().strftime(\"%Y-%m-%d\"),\n+            type=\"keyword\",\n+            num_results=10,\n+        ),\n+        DuckDuckGoTools(\n+            timeout=20,\n+            fixed_max_results=5,\n+        ),\n+        FileTools(base_dir=output_dir),\n+    ]\n+\n+    # Modify the description to include the education level\n+    tutor_description = f\"\"\"You are Llama Tutor, an educational AI assistant designed to teach concepts at a {education_level} level.\n+    You have the following tools at your disposal:\n+      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.\n+      - ExaTools for structured, in-depth analysis.\n+      - FileTools for saving the output upon user confirmation.\n+\n+    Your response should always be clear, concise, and detailed, tailored to a {education_level} student's understanding.\n+    Blend direct answers with extended analysis, supporting evidence, illustrative examples, and clarifications on common misconceptions.\n+    Engage the user with follow-up questions to check understanding and deepen learning.\n+\n+    <critical>\n+    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.\n+    - You must provide sources, whenever you provide a data point or a statistic.\n+    - When the user asks a follow-up question, you can use the previous answer as context.\n+    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.\n+    - Always adapt your explanations to a {education_level} level of understanding.\n+    </critical>\"\"\"\n+\n+    # Modify the instructions to include the education level\n+    tutor_instructions = f\"\"\"Here's how you should answer the user's question:\n+\n+    1. Gather Relevant Information\n+      - First, carefully analyze the query to identify the intent of the user.\n+      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.\n+      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.\n+      - Combine the insights from both tools to craft a comprehensive and balanced answer.\n+      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.\n+      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.\n+\n+    2. Construct Your Response\n+      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query, tailored to a {education_level} level.\n+      - **Then expand** the answer by including:\n+          \u2022 A clear explanation with context and definitions appropriate for {education_level} students.\n+          \u2022 Supporting evidence such as statistics, real-world examples, and data points that are understandable at a {education_level} level.\n+          \u2022 Clarifications that address common misconceptions students at this level might have.\n+      - Structure your response with clear headings, bullet points, and organized paragraphs to make it easy to follow.\n+      - Include interactive elements like questions to check understanding or mini-quizzes when appropriate.\n+      - Use analogies and examples that would be familiar to students at a {education_level} level.\n+\n+    3. Enhance Engagement\n+      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)\"\n+      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.\n+      - Suggest follow-up topics or questions that might deepen their understanding.\n+\n+    4. Final Quality Check & Presentation \u2728\n+      - Review your response to ensure clarity, depth, and engagement.\n+      - Ensure the language and concepts are appropriate for a {education_level} level.\n+      - Make complex ideas accessible without oversimplifying to the point of inaccuracy.\n+\n+    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\"\"\"\n+\n+    return Agent(\n+        name=\"Llama Tutor\",\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id or str(uuid.uuid4()),\n+        storage=agent_storage,\n+        tools=tools,\n+        # Allow Llama Tutor to read both chat history and tool call history for better context.\n+        read_chat_history=True,\n+        read_tool_call_history=True,\n+        # Append previous conversation responses into the new messages for context.\n+        add_history_to_messages=True,\n+        num_history_responses=num_history_responses,\n+        add_datetime_to_instructions=True,\n+        add_name_to_instructions=True,\n+        description=tutor_description,\n+        instructions=tutor_instructions,\n+        expected_output=EXPECTED_OUTPUT_TEMPLATE,\n+        debug_mode=debug_mode,\n+        markdown=True,\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/app.py b/cookbook/examples/streamlit_apps/llama_tutor/app.py\nnew file mode 100644\nindex 000000000..01b0a2f41\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/app.py\n@@ -0,0 +1,185 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import tutor_agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    rename_session_widget,\n+    session_selector_widget,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Llama Tutor: Learn Anything\",\n+    page_icon=\":book:\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\"<h1 class='main-title'>Llama Tutor</h1>\", unsafe_allow_html=True)\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent answer engine powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model configuration - always use Llama 3.3 70B\n+    ####################################################################\n+    model_id = \"groq:llama-3.3-70b-versatile\"\n+\n+    ####################################################################\n+    # Education level selector\n+    ####################################################################\n+    education_levels = [\n+        \"Elementary School\",\n+        \"Middle School\",\n+        \"High School\",\n+        \"College\",\n+        \"Undergrad\",\n+        \"Graduate\",\n+    ]\n+\n+    selected_education_level = st.sidebar.selectbox(\n+        \"Education Level\",\n+        options=education_levels,\n+        index=2,  # Default to High School\n+        key=\"education_level_selector\",\n+    )\n+\n+    # Store the education level in session state\n+    if \"education_level\" not in st.session_state:\n+        st.session_state[\"education_level\"] = selected_education_level\n+    elif st.session_state[\"education_level\"] != selected_education_level:\n+        st.session_state[\"education_level\"] = selected_education_level\n+        # Reset the agent if education level changes\n+        if \"llama_tutor\" in st.session_state:\n+            st.session_state[\"llama_tutor\"] = None\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    llama_tutor: Agent\n+    if (\n+        \"llama_tutor\" not in st.session_state\n+        or st.session_state[\"llama_tutor\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Llama Tutor agent ---*---\")\n+        llama_tutor = tutor_agent(\n+            model_id=model_id, education_level=st.session_state[\"education_level\"]\n+        )\n+        st.session_state[\"llama_tutor\"] = llama_tutor\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        llama_tutor = st.session_state[\"llama_tutor\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        st.session_state[\"llama_tutor_session_id\"] = llama_tutor.load_session()\n+    except Exception:\n+        st.warning(\"Could not create Agent session, is the database running?\")\n+        return\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    agent_runs = llama_tutor.memory.runs\n+    if len(agent_runs) > 0:\n+        logger.debug(\"Loading run history\")\n+        st.session_state[\"messages\"] = []\n+        for _run in agent_runs:\n+            if _run.message is not None:\n+                add_message(_run.message.role, _run.message.content)\n+            if _run.response is not None:\n+                add_message(\"assistant\", _run.response.content, _run.response.tools)\n+    else:\n+        logger.debug(\"No run history found\")\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\u2728 What would you like to learn about?\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\":book: Llama Tutor is preparing your lesson...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = llama_tutor.run(question, stream=True)\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response\n+                        if _resp_chunk.content is not None:\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, llama_tutor.run_response.tools)\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(llama_tutor, model_id)\n+    rename_session_widget(llama_tutor)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/prompts.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/prompts.py b/cookbook/examples/streamlit_apps/llama_tutor/prompts.py\nnew file mode 100644\nindex 000000000..6faeac318\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/prompts.py\n@@ -0,0 +1,96 @@\n+\"\"\"Templates for the Answer Engine.\"\"\"\n+\n+from textwrap import dedent\n+\n+AGENT_DESCRIPTION = dedent(\"\"\"\\\n+    You are Llama Tutor, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.\n+    You have the following tools at your disposal:\n+      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.\n+      - ExaTools for structured, in-depth analysis.\n+      - FileTools for saving the output upon user confirmation.\n+\n+    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,\n+    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user\n+    with follow-up questions, such as asking if they'd like to save the answer.\n+\n+    <critical>\n+    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.\n+    - You must provide sources, whenever you provide a data point or a statistic.\n+    - When the user asks a follow-up question, you can use the previous answer as context.\n+    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.\n+    </critical>\\\n+\"\"\")\n+\n+AGENT_INSTRUCTIONS = dedent(\"\"\"\\\n+    Here's how you should answer the user's question:\n+\n+    1. Gather Relevant Information\n+      - First, carefully analyze the query to identify the intent of the user.\n+      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.\n+      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.\n+      - Combine the insights from both tools to craft a comprehensive and balanced answer.\n+      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.\n+      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.\n+\n+    2. Construct Your Response\n+      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.\n+      - **Then expand** the answer by including:\n+          \u2022 A clear explanation with context and definitions.\n+          \u2022 Supporting evidence such as statistics, real-world examples, and data points.\n+          \u2022 Clarifications that address common misconceptions.\n+      - Expand the answer only if the query requires more detail. Simple questions like: \"What is the weather in Tokyo?\" or \"What is the capital of France?\" don't need an in-depth analysis.\n+      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.\n+\n+    3. Enhance Engagement\n+      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)\"\n+      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.\n+\n+    4. Final Quality Check & Presentation \u2728\n+      - Review your response to ensure clarity, depth, and engagement.\n+      - Strive to be both informative for quick queries and thorough for detailed exploration.\n+\n+    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\\\n+\"\"\")\n+\n+EXPECTED_OUTPUT_TEMPLATE = dedent(\"\"\"\\\n+    {# If this is the first message, include the question title #}\n+    {% if this is the first message %}\n+    ## {An engaging title for this report. Keep it short.}\n+    {% endif %}\n+\n+    **{A clear and direct response that answers the question.}**\n+\n+    {# If the query requires more detail, include the sections below #}\n+    {% if detailed_response %}\n+\n+    ### {Secion title}\n+    {Add detailed analysis & explanation in this section}\n+    {A comprehensive breakdown covering key insights, context, and definitions.}\n+\n+    ### {Section title}\n+    {Add evidence & support in this section}\n+    {Add relevant data points and statistics in this section}\n+    {Add links or names of reputable sources supporting the answer in this section}\n+\n+    ### {Section title}\n+    {Add real-world examples or case studies that help illustrate the key points in this section}\n+\n+    ### {Section title}\n+    {Add clarifications addressing any common misunderstandings related to the topic in this section}\n+\n+    ### {Section title}\n+    {Add further details, implications, or suggestions for ongoing exploration in this section}\n+    {% endif %}\n+\n+    {Add any more sections you think are relevant, covering all the aspects of the query}\n+\n+    ### Sources\n+    - [1] {Source 1 url}\n+    - [2] {Source 2 url}\n+    - [3] {Source 3 url}\n+    - {any more sources you think are relevant}\n+\n+    Generated by Llama Tutor on: {current_time}\n+\n+    Stay curious and keep exploring \u2728\\\n+    \"\"\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/requirements.txt b/cookbook/examples/streamlit_apps/llama_tutor/requirements.txt\nnew file mode 100644\nindex 000000000..de2fb0df9\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/requirements.txt\n@@ -0,0 +1,235 @@\n+agno==1.1.11\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.5.2\n+exa-py==1.9.0\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+groq==0.19.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+grpcio==1.71.0\n+    # via\n+    #   google-api-core\n+    #   grpcio-status\n+grpcio-status==1.71.0\n+    # via google-api-core\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httplib2==0.22.0\n+    # via\n+    #   google-api-python-client\n+    #   google-auth-httplib2\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.30.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.66.3\n+    # via\n+    #   -r cookbook/examples/apps/answer_engine/requirements.in\n+    #   exa-py\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.14.0\n+    # via duckduckgo-search\n+protobuf==5.29.3\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+    #   google-generativeai\n+    #   googleapis-common-protos\n+    #   grpcio-status\n+    #   proto-plus\n+    #   streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyparsing==3.2.1\n+    # via httplib2\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   exa-py\n+    #   google-api-core\n+    #   google-search-results\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.39\n+streamlit==1.43.2\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   google-generativeai\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   exa-py\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+uritemplate==4.1.1\n+    # via google-api-python-client\n+urllib3==2.3.0\n+    # via requests\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/llama_tutor/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/llama_tutor/utils.py b/cookbook/examples/streamlit_apps/llama_tutor/utils.py\nnew file mode 100644\nindex 000000000..d08836008\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/llama_tutor/utils.py\n@@ -0,0 +1,297 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import tutor_agent\n+from agno.agent.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"llama_tutor\"] = None\n+    st.session_state[\"llama_tutor_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Llama Tutor - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\", {})\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics:\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.2f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content:\n+                        st.markdown(\"**Results:**\")\n+                        try:\n+                            st.json(content)\n+                        except Exception as e:\n+                            st.markdown(content)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries for Llama Tutor.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\udcdc Try me!\")\n+        if st.button(\"\ud83d\udca1 US Tariffs\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about the tariffs the US is imposing in 2025\",\n+            )\n+        if st.button(\"\ud83e\udd14 Reasoning Models\"):\n+            add_message(\n+                \"user\",\n+                \"Which is a better reasoning model: o3-mini or DeepSeek R1?\",\n+            )\n+        if st.button(\"\ud83e\udd16 Tell me about Agno\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about Agno: https://github.com/agno-agi/agno and https://docs.agno.com\",\n+            )\n+        if st.button(\"\u2696\ufe0f Impact of AI Regulations\"):\n+            add_message(\n+                \"user\",\n+                \"Evaluate how emerging AI regulations could influence innovation, privacy, and ethical AI deployment in the near future.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if st.button(\"\ud83d\udd04 New Chat\"):\n+                restart_agent()\n+        with col2:\n+            fn = \"llama_tutor_chat_history.md\"\n+            if \"llama_tutor_session_id\" in st.session_state:\n+                fn = f\"llama_tutor_{st.session_state.llama_tutor_session_id}.md\"\n+            if st.download_button(\n+                \"\ud83d\udcbe Export Chat\",\n+                export_chat_history(),\n+                file_name=fn,\n+                mime=\"text/markdown\",\n+            ):\n+                st.sidebar.success(\"Chat history exported!\")\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar.\"\"\"\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        # Get session names if available, otherwise use IDs.\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        # Display session selector.\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+        # Find the selected session ID.\n+        selected_session_id = next(\n+            s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+        )\n+\n+        if st.session_state.get(\"llama_tutor_session_id\") != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+            )\n+            st.session_state[\"llama_tutor\"] = tutor_agent(\n+                model_id=model_id,\n+                session_id=selected_session_id,\n+            )\n+            st.rerun()\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage.\"\"\"\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    # Initialize session_edit_mode if needed.\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About Llama Tutor\")\n+    st.sidebar.markdown(\n+        \"\"\"\n+        Llama Tutor is an educational AI assistant that delivers personalized learning experiences tailored to your education level.\n+        \n+        Features:\n+        - \ud83d\udcda Personalized education at various academic levels\n+        - \ud83d\udd0d Real-time information retrieval\n+        - \ud83d\udcca In-depth analysis and explanations\n+        - \ud83e\udde0 Interactive learning with quizzes and follow-up questions\n+        - \ud83d\udcbe Save lessons for future reference\n+        \n+        Built with:\n+        - \ud83e\udd99 Llama 3.3 70B from Meta\n+        - \ud83d\ude80 Agno framework\n+        - \ud83d\udcab Streamlit\n+        \"\"\"\n+    )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        font-size: 3rem !important;\n+        font-weight: 700 !important;\n+        color: #2A8EF9 !important;\n+        margin-bottom: 0 !important;\n+        text-align: center;\n+    }\n+    \n+    .subtitle {\n+        font-size: 1.2rem !important;\n+        color: #555 !important;\n+        margin-top: 0 !important;\n+        text-align: center;\n+        margin-bottom: 2rem !important;\n+    }\n+    \n+    /* Dark mode support */\n+    @media (prefers-color-scheme: dark) {\n+        .main-title {\n+            color: #4DA3FF !important;\n+        }\n+        \n+        .subtitle {\n+            color: #CCC !important;\n+        }\n+    }\n+    \n+    /* Tool Call Styling */\n+    .stExpander {\n+        border-radius: 8px !important;\n+        border: 1px solid rgba(49, 51, 63, 0.2) !important;\n+        margin-bottom: 0.5rem !important;\n+    }\n+    \n+    .stExpander summary {\n+        font-weight: 600 !important;\n+        padding: 0.5rem 1rem !important;\n+    }\n+    \n+    /* Sidebar Styling */\n+    .css-1544g2n {\n+        padding-top: 2rem !important;\n+    }\n+    \n+    /* Education Level Display */\n+    .education-level-display {\n+        padding: 8px;\n+        background-color: #f8f9fa;\n+        border-radius: 6px;\n+        margin-top: 8px;\n+        text-align: center;\n+        border: 1px solid #e9ecef;\n+        font-size: 0.9rem;\n+    }\n+    \n+    /* Dark mode support for education level */\n+    @media (prefers-color-scheme: dark) {\n+        .education-level-display {\n+            background-color: #262730;\n+            border-color: #4a4d56;\n+            color: #f8f9fa;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/.gitignore b/cookbook/examples/streamlit_apps/mcp_agent/.gitignore\nnew file mode 100644\nindex 000000000..c9ce91d35\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/.gitignore\n@@ -0,0 +1,3 @@\n+output\n+agents.db\n+tmp\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/__init__.py b/cookbook/examples/streamlit_apps/mcp_agent/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/agents.py b/cookbook/examples/streamlit_apps/mcp_agent/agents.py\nnew file mode 100644\nindex 000000000..5cf58badd\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/agents.py\n@@ -0,0 +1,171 @@\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import List, Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge.url import UrlKnowledge\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.mcp import MCPTools\n+from agno.vectordb.lancedb import LanceDb, SearchType\n+\n+# ************* Setup Paths *************\n+# Define the current working directory\n+cwd = Path(__file__).parent\n+# Create a tmp directory for storing agent sessions and knowledge\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+# *************************************\n+\n+# ************* Agent Storage *************\n+# Store agent sessions in a SQLite database\n+agent_storage = SqliteAgentStorage(\n+    table_name=\"mcp_agent_sessions\",  # Table to store agent sessions\n+    db_file=str(tmp_dir.joinpath(\"agents.db\")),  # SQLite database file\n+)\n+# *************************************\n+\n+# ************* Agent Knowledge *************\n+# Store MCP Documentation in a knowledge base\n+agent_knowledge = UrlKnowledge(\n+    urls=[\"https://modelcontextprotocol.io/llms-full.txt\"],\n+    vector_db=LanceDb(\n+        uri=str(tmp_dir.joinpath(\"mcp_documentation\")),\n+        table_name=\"mcp_documentation\",\n+        search_type=SearchType.hybrid,\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    ),\n+)\n+# *************************************\n+\n+\n+def get_mcp_agent(\n+    user_id: Optional[str] = None,\n+    model_str: str = \"openai:gpt-4o\",\n+    session_id: Optional[str] = None,\n+    num_history_responses: int = 5,\n+    mcp_tools: Optional[List[MCPTools]] = None,\n+    mcp_server_ids: Optional[List[str]] = None,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    model = get_model_for_provider(model_str)\n+\n+    description = dedent(\"\"\"\\\n+        You are UAgI, a universal MCP (Model Context Protocol) agent designed to interact with MCP servers.\n+        You can connect to various MCP servers to access resources and execute tools.\n+\n+        As an MCP agent, you can:\n+        - Connect to file systems, databases, APIs, and other data sources through MCP servers\n+        - Execute tools provided by MCP servers to perform actions\n+        - Access resources exposed by MCP servers\n+\n+        Note: You only have access to the MCP Servers provided below, if you need to access other MCP Servers, please ask the user to enable them.\n+\n+        <critical>\n+        - When a user mentions a task that might require external data or tools, check if an appropriate MCP server is available\n+        - If an MCP server is available, use its capabilities to fulfill the user's request\n+        - You have a knowledge base full of MCP documentation, search it using the `search_knowledge` tool to answer questions about MCP and the different tools available.\n+        - Provide clear explanations of which MCP servers and tools you're using\n+        - If you encounter errors with an MCP server, explain the issue and suggest alternatives\n+        - Always cite sources when providing information retrieved through MCP servers\n+        </critical>\\\n+    \"\"\")\n+\n+    if mcp_server_ids:\n+        description += dedent(\n+            \"\"\"\\n\n+            You have access to the following MCP servers:\n+            {}\n+        \"\"\".format(\"\\n\".join([f\"- {server_id}\" for server_id in mcp_server_ids]))\n+        )\n+\n+    instructions = dedent(\"\"\"\\\n+        Here's how you should fulfill a user request:\n+\n+        1. Understand the user's request\n+        - Read the user's request carefully\n+        - Determine if the request requires MCP server interaction\n+        - Search your knowledge base using the `search_knowledge` tool to answer questions about MCP or to learn how to use different MCP tools.\n+        - To interact with an MCP server, follow these steps:\n+            - Identify which tools are available to you\n+            - Select the appropriate tool for the user's request\n+            - Explain to the user which tool you're using\n+            - Execute the tool\n+            - Provide clear feedback about tool execution results\n+\n+        2. Error Handling\n+        - If an MCP tool fails, explain the issue clearly and provide details about the error.\n+        - Suggest alternatives when MCP capabilities are unavailable\n+\n+        3. Security and Privacy\n+        - Be transparent about which servers and tools you're using\n+        - Request explicit permission before executing tools that modify data\n+        - Respect access limitations of connected MCP servers\n+\n+        MCP Knowledge\n+        - You have access to a knowledge base of MCP documentation\n+        - To answer questions about MCP, use the knowledge base\n+        - If you don't know the answer or can't find the information in the knowledge base, say so\\\n+    \"\"\")\n+\n+    return Agent(\n+        name=\"UAgI: The Universal MCP Agent\",\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id,\n+        tools=mcp_tools,\n+        # Store Agent sessions in the database\n+        storage=agent_storage,\n+        # Store MCP Documentation in a knowledge base\n+        knowledge=agent_knowledge,\n+        # Agent description, instructions and expected output format\n+        description=description,\n+        instructions=instructions,\n+        # Allow MCP Agent to read both chat history and tool call history for better context.\n+        read_chat_history=True,\n+        read_tool_call_history=True,\n+        # Append previous conversation responses into the new messages for context.\n+        add_history_to_messages=True,\n+        num_history_responses=num_history_responses,\n+        add_datetime_to_instructions=True,\n+        add_name_to_instructions=True,\n+        debug_mode=debug_mode,\n+        # Respond in markdown format\n+        markdown=True,\n+    )\n+\n+\n+def get_model_for_provider(model_str: str):\n+    \"\"\"\n+    Creates and returns the appropriate model for a model string.\n+\n+    Args:\n+        model_str: The model string (e.g., 'openai:gpt-4o', 'google:gemini-2.0-flash', 'anthropic:claude-3-5-sonnet', 'groq:llama-3.3-70b-versatile')\n+\n+    Returns:\n+        An instance of the appropriate model class\n+\n+    Raises:\n+        ValueError: If the provider is not supported\n+    \"\"\"\n+    provider, model_name = model_str.split(\":\")\n+    if provider == \"openai\":\n+        return OpenAIChat(id=model_name)\n+    elif provider == \"gemini\":\n+        return Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        if \"thinking\" in model_name:\n+            return Claude(\n+                id=model_name,\n+                max_tokens=16384,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 8192},\n+            )\n+        return Claude(id=model_name, max_tokens=16384)\n+    elif provider == \"groq\":\n+        return Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/app.py b/cookbook/examples/streamlit_apps/mcp_agent/app.py\nnew file mode 100644\nindex 000000000..23dc9bd76\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/app.py\n@@ -0,0 +1,210 @@\n+import asyncio\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_mcp_agent\n+from agno.utils.log import logger\n+from mcp_client import MCPClient\n+from utils import (\n+    about_widget,\n+    add_message,\n+    apply_theme,\n+    display_tool_calls,\n+    example_inputs,\n+    get_mcp_server_config,\n+    get_num_history_responses,\n+    get_selected_model,\n+    session_selector_widget,\n+    utilities_widget,\n+)\n+\n+nest_asyncio.apply()\n+apply_theme()\n+\n+\n+async def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Universal Agent Interface powered by MCP</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>A unified Agentic interface for MCP servers</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Settings\n+    ####################################################################\n+    selected_model = get_selected_model()\n+    mcp_server_config = get_mcp_server_config()\n+    mcp_server_id = mcp_server_config.id\n+    num_history_responses = get_num_history_responses()\n+\n+    ####################################################################\n+    # Initialize MCP Client and Agent\n+    ####################################################################\n+    try:\n+        # Check if we need to reinitialize the MCP client\n+        if (\n+            \"mcp_client\" not in st.session_state\n+            or st.session_state.get(\"mcp_server_id\") != mcp_server_id\n+            or getattr(st.session_state.get(\"mcp_client\", None), \"session\", None)\n+            is None\n+        ):\n+            # # Clean up existing client if it exists\n+            # if \"mcp_client\" in st.session_state:\n+            #     logger.info(\"Cleaning up existing MCP client\")\n+            #     await st.session_state[\"mcp_client\"].cleanup()\n+\n+            # Initialize new MCP client\n+            logger.info(f\"Creating new MCPClient for {mcp_server_id}\")\n+            st.session_state[\"mcp_client\"] = MCPClient()\n+\n+        mcp_client = st.session_state[\"mcp_client\"]\n+        # Connect to the MCP server and get tools\n+        mcp_tools = await mcp_client.connect_to_server(mcp_server_config)\n+\n+        # Initialize or retrieve the agent\n+        if (\n+            \"mcp_agent\" not in st.session_state\n+            or st.session_state[\"mcp_agent\"] is None\n+            or st.session_state.get(\"current_model\") != selected_model\n+            or st.session_state.get(\"mcp_server_id\") != mcp_server_id\n+        ):\n+            logger.info(\"---*--- Creating new MCP Agent ---*---\")\n+            mcp_agent = get_mcp_agent(\n+                model_str=selected_model,\n+                num_history_responses=num_history_responses,\n+                mcp_tools=[mcp_tools],\n+                mcp_server_ids=[mcp_server_id],\n+            )\n+            st.session_state[\"mcp_agent\"] = mcp_agent\n+            st.session_state[\"current_model\"] = selected_model\n+        else:\n+            mcp_agent = st.session_state[\"mcp_agent\"]\n+\n+        # Update the agent's MCP tools incase the session has been reinitialized\n+        mcp_agent.tools = [mcp_tools]\n+        ####################################################################\n+        # Load the current Agent session from the database\n+        ####################################################################\n+        try:\n+            st.session_state[\"mcp_agent_session_id\"] = mcp_agent.load_session()\n+        except Exception as e:\n+            st.warning(\n+                f\"Could not create Agent session: {str(e)}. Is the database running?\"\n+            )\n+            return\n+\n+        ####################################################################\n+        # Load agent runs (i.e. chat history) from memory\n+        ####################################################################\n+        agent_runs = mcp_agent.memory.runs\n+        if len(agent_runs) > 0:\n+            # If there are runs, load the messages\n+            logger.debug(\"Loading run history\")\n+            st.session_state[\"messages\"] = []\n+            # Loop through the runs and add the messages to the messages list\n+            for _run in agent_runs:\n+                if _run.message is not None:\n+                    add_message(_run.message.role, _run.message.content)\n+                if _run.response is not None:\n+                    add_message(\"assistant\", _run.response.content, _run.response.tools)\n+        else:\n+            # If there are no runs, create an empty messages list\n+            logger.debug(\"No run history found\")\n+            st.session_state[\"messages\"] = []\n+\n+        ####################################################################\n+        # Get user input\n+        ####################################################################\n+        if prompt := st.chat_input(\"\u2728 How can I help, bestie?\"):\n+            add_message(\"user\", prompt)\n+\n+        ####################################################################\n+        # Show example inputs\n+        ####################################################################\n+        example_inputs(server_id=mcp_server_id)\n+\n+        ####################################################################\n+        # Display agent messages\n+        ####################################################################\n+        for message in st.session_state[\"messages\"]:\n+            if message[\"role\"] in [\"user\", \"assistant\"]:\n+                _content = message[\"content\"]\n+                if _content is not None:\n+                    with st.chat_message(message[\"role\"]):\n+                        # Display tool calls if they exist in the message\n+                        if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                            display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                        st.markdown(_content)\n+\n+        ####################################################################\n+        # Generate response for user message\n+        ####################################################################\n+        last_message = (\n+            st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+        )\n+        if last_message and last_message.get(\"role\") == \"user\":\n+            question = last_message[\"content\"]\n+            with st.chat_message(\"assistant\"):\n+                # Create container for tool calls\n+                tool_calls_container = st.empty()\n+                resp_container = st.empty()\n+                with st.spinner(\":thinking_face: Thinking...\"):\n+                    response = \"\"\n+                    try:\n+                        # Run the agent and stream the response\n+                        run_response = await mcp_agent.arun(question, stream=True)\n+                        async for _resp_chunk in run_response:\n+                            # Display tool calls if available\n+                            if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                                display_tool_calls(\n+                                    tool_calls_container, _resp_chunk.tools\n+                                )\n+\n+                            # Display response\n+                            if _resp_chunk.content is not None:\n+                                response += _resp_chunk.content\n+                                resp_container.markdown(response)\n+\n+                        add_message(\"assistant\", response, mcp_agent.run_response.tools)\n+                    except Exception as e:\n+                        logger.error(f\"Error during agent run: {str(e)}\", exc_info=True)\n+                        error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                        add_message(\"assistant\", error_message)\n+                        st.error(error_message)\n+\n+        ####################################################################\n+        # Session selector\n+        ####################################################################\n+        session_selector_widget(\n+            agent=mcp_agent,\n+            model_str=selected_model,\n+            num_history_responses=num_history_responses,\n+            mcp_tools=[mcp_tools],\n+            mcp_server_ids=[mcp_server_id],\n+        )\n+\n+        ####################################################################\n+        # About section\n+        ####################################################################\n+        utilities_widget(agent=mcp_agent)\n+        about_widget()\n+\n+    except Exception as e:\n+        logger.error(f\"Error during agent run: {str(e)}\", exc_info=True)\n+        error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+        add_message(\"assistant\", error_message)\n+        st.error(error_message)\n+    finally:\n+        # Don't clean up resources here - we want to keep the connection alive\n+        # between Streamlit reruns. We'll clean up when we need to reinitialize.\n+        pass\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/generate_requirements.sh b/cookbook/examples/streamlit_apps/mcp_agent/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/mcp_client.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/mcp_client.py b/cookbook/examples/streamlit_apps/mcp_agent/mcp_client.py\nnew file mode 100644\nindex 000000000..d725a8b0e\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/mcp_client.py\n@@ -0,0 +1,62 @@\n+from contextlib import AsyncExitStack\n+from typing import List, Optional\n+\n+from agno.tools.mcp import MCPTools\n+from agno.utils.log import logger\n+from mcp import ClientSession, StdioServerParameters\n+from mcp.client.stdio import stdio_client\n+from pydantic import BaseModel\n+\n+\n+class MCPServerConfig(BaseModel):\n+    \"\"\"Configuration for an MCP server.\"\"\"\n+\n+    id: str\n+    command: str\n+    args: Optional[List[str]] = None\n+\n+\n+class MCPClient:\n+    def __init__(self):\n+        # Initialize session and client objects\n+        self.session = None\n+        self.exit_stack = AsyncExitStack()\n+        self.tools = []\n+        self.server_id = None\n+\n+    async def connect_to_server(self, server_config):\n+        \"\"\"Connect to an MCP server using the provided configuration\n+\n+        Args:\n+            server_config: Configuration for the MCP server\n+        \"\"\"\n+        self.server_id = server_config.id\n+\n+        server_params = StdioServerParameters(\n+            command=server_config.command,\n+            args=server_config.args,\n+        )\n+        logger.info(f\"Connecting to server {self.server_id}\")\n+\n+        # Create client session\n+        stdio_transport = await self.exit_stack.enter_async_context(\n+            stdio_client(server_params)\n+        )\n+        self.stdio, self.write = stdio_transport\n+        self.session = await self.exit_stack.enter_async_context(\n+            ClientSession(self.stdio, self.write)\n+        )\n+\n+        # Initialize the session\n+        await self.session.initialize()\n+\n+        # Create MCPTools for this server\n+        mcp_tools = MCPTools(session=self.session)\n+        await mcp_tools.initialize()\n+        logger.info(f\"Connected to server {self.server_id}\")\n+\n+        return mcp_tools\n+\n+    async def cleanup(self):\n+        \"\"\"Clean up resources\"\"\"\n+        await self.exit_stack.aclose()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/requirements.in b/cookbook/examples/streamlit_apps/mcp_agent/requirements.in\nnew file mode 100644\nindex 000000000..59f7aeb13\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/requirements.in\n@@ -0,0 +1,11 @@\n+agno\n+anthropic\n+google-genai\n+groq\n+lancedb\n+mcp\n+nest_asyncio\n+openai\n+sqlalchemy\n+streamlit\n+tantivy\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/requirements.txt b/cookbook/examples/streamlit_apps/mcp_agent/requirements.txt\nnew file mode 100644\nindex 000000000..2679ab4a7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/requirements.txt\n@@ -0,0 +1,255 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.9\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+anyio==4.8.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   mcp\n+    #   openai\n+    #   sse-starlette\n+    #   starlette\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+    #   uvicorn\n+deprecation==2.1.0\n+    # via lancedb\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.5.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+groq==0.18.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+h11==0.14.0\n+    # via\n+    #   httpcore\n+    #   uvicorn\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   mcp\n+    #   openai\n+httpx-sse==0.4.0\n+    # via mcp\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.8.2\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lancedb==0.20.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mcp==1.3.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.29.1\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   pylance\n+    #   streamlit\n+openai==1.65.5\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+overrides==7.7.0\n+    # via lancedb\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   deprecation\n+    #   lancedb\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+protobuf==5.29.3\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via\n+    #   pylance\n+    #   streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   lancedb\n+    #   mcp\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via\n+    #   agno\n+    #   mcp\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pylance==0.23.2\n+    # via lancedb\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.38\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+sse-starlette==2.2.1\n+    # via mcp\n+starlette==0.46.1\n+    # via\n+    #   mcp\n+    #   sse-starlette\n+streamlit==1.43.1\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+tantivy==0.22.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   lancedb\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+uvicorn==0.34.0\n+    # via mcp\n+websockets==14.2\n+    # via google-genai\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/mcp_agent/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/mcp_agent/utils.py b/cookbook/examples/streamlit_apps/mcp_agent/utils.py\nnew file mode 100644\nindex 000000000..741a7f03a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/mcp_agent/utils.py\n@@ -0,0 +1,478 @@\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import get_mcp_agent\n+from agno.agent import Agent\n+from agno.tools.mcp import MCPTools\n+from agno.utils.log import logger\n+from mcp_client import MCPServerConfig\n+\n+\n+def get_selected_model() -> str:\n+    \"\"\"Return the selected model identifier based on user selection in the sidebar.\n+\n+    Returns:\n+        str: The model identifier string in the format 'provider:model-name'\n+    \"\"\"\n+    model_options = {\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gpt-4.5\": \"openai:gpt-4.5-preview\",\n+        \"gpt-4o-mini\": \"openai:gpt-4o-mini\",\n+        \"o3-mini\": \"openai:o3-mini\",\n+        \"sonnet-3-7\": \"anthropic:claude-3-7-sonnet-latest\",\n+        \"sonnet-3.7-thinking\": \"anthropic:claude-3-7-sonnet-thinking\",\n+        \"gemini-flash\": \"gemini:gemini-2.0-flash\",\n+        \"gemini-pro\": \"gemini:gemini-2.0-pro-exp-02-05\",\n+        \"llama-3.3-70b\": \"groq:llama-3.3-70b-versatile\",\n+    }\n+    st.sidebar.markdown(\"#### :sparkles: Select a model\")\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=list(model_options.keys()).index(\"gpt-4o\"),\n+        key=\"selected_model\",\n+        label_visibility=\"collapsed\",\n+    )\n+    return model_options[selected_model]\n+\n+\n+def get_num_history_responses() -> int:\n+    \"\"\"Return the number of messages from history to send to the LLM.\n+\n+    Returns:\n+        int: The number of messages from history to include\n+    \"\"\"\n+    num_history = st.sidebar.slider(\n+        \"Number of previous messages to include\",\n+        min_value=1,\n+        max_value=20,\n+        value=5,\n+        step=1,\n+        help=\"Controls how many previous messages are sent to the LLM for context\",\n+    )\n+    return num_history\n+\n+\n+def get_mcp_server_config() -> Optional[MCPServerConfig]:\n+    \"\"\"Get a single MCP server config to add to the agent.\n+\n+    Returns:\n+        Optional[MCPServerConfig]: A single MCP server config, or None if none selected.\n+    \"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Select MCP Tool\")\n+\n+        # Use radio button for single selection\n+        selected_tool = st.radio(\n+            \"Select a tool to use:\",\n+            options=[\"GitHub\", \"Filesystem\"],\n+            key=\"selected_mcp_tool\",\n+            label_visibility=\"collapsed\",\n+        )\n+\n+        if selected_tool == \"GitHub\":\n+            github_token_from_env = os.getenv(\"GITHUB_TOKEN\")\n+            github_token = st.text_input(\n+                \"GitHub Token\",\n+                type=\"password\",\n+                help=\"Create a token with repo scope at github.com/settings/tokens\",\n+                value=github_token_from_env,\n+            )\n+            if github_token:\n+                os.environ[\"GITHUB_TOKEN\"] = github_token\n+                return MCPServerConfig(\n+                    id=\"github\",\n+                    command=\"npx\",\n+                    args=[\"-y\", \"@modelcontextprotocol/server-github\"],\n+                    env_vars=[\"GITHUB_TOKEN\"],\n+                )\n+            else:\n+                st.error(\"GitHub Token is required to use GitHub MCP Tools\")\n+\n+        elif selected_tool == \"Filesystem\":\n+            # Get the repository root\n+            cwd = Path(__file__).parent\n+            repo_root = cwd.parent.parent.parent.parent.resolve()\n+            st.info(f\"Repository path: {repo_root}\")\n+            return MCPServerConfig(\n+                id=\"filesystem\",\n+                command=\"npx\",\n+                args=[\"-y\", \"@modelcontextprotocol/server-filesystem\"]\n+                + [str(repo_root)],\n+            )\n+\n+    return None\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    if not tools:\n+        return\n+\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\", {})\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                if metrics and isinstance(metrics, dict):\n+                    execution_time = metrics.get(\"time\")\n+                    if execution_time is None:\n+                        execution_time_str = \"N/A\"\n+                    else:\n+                        execution_time_str = f\"{execution_time:.2f}s\"\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and tool_args.get(\"query\"):\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content:\n+                        st.markdown(\"**Results:**\")\n+                        try:\n+                            # Check if content is already a dictionary or can be parsed as JSON\n+                            if isinstance(content, dict) or (\n+                                isinstance(content, str)\n+                                and content.strip().startswith((\"{\", \"[\"))\n+                            ):\n+                                st.json(content)\n+                            else:\n+                                # If not JSON, show as markdown\n+                                st.markdown(content)\n+                        except Exception:\n+                            # If JSON display fails, show as markdown\n+                            st.markdown(content)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(f\"Failed to display tool results: {str(e)}\")\n+\n+\n+def example_inputs(server_id: str) -> None:\n+    \"\"\"Show example inputs for the MCP Agent.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### :thinking_face: Try me!\")\n+        if st.button(\"Who are you?\"):\n+            add_message(\n+                \"user\",\n+                \"Who are you?\",\n+            )\n+        if st.button(\"What is your purpose?\"):\n+            add_message(\n+                \"user\",\n+                \"What is your purpose?\",\n+            )\n+        # Common examples for all server types\n+        if st.button(\"What can you help me with?\"):\n+            add_message(\n+                \"user\",\n+                \"What can you help me with?\",\n+            )\n+        if st.button(\"How do MCP tools work?\"):\n+            add_message(\n+                \"user\",\n+                \"How do MCP tools work? Explain the Model Context Protocol.\",\n+            )\n+\n+        # Server-specific examples\n+        if server_id == \"github\":\n+            if st.button(\"Tell me about Agno\"):\n+                add_message(\n+                    \"user\",\n+                    \"Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information.\",\n+                )\n+            if st.button(\"Find issues in the Agno repo\"):\n+                add_message(\n+                    \"user\",\n+                    \"Find open issues in the agno-agi/agno repository and summarize the top 3 most recent ones.\",\n+                )\n+        elif server_id == \"filesystem\":\n+            if st.button(\"Summarize the README\"):\n+                add_message(\n+                    \"user\",\n+                    \"If there is a README file in the current directory, summarize it.\",\n+                )\n+\n+\n+def session_selector_widget(\n+    agent: Agent,\n+    model_str: str,\n+    num_history_responses: int,\n+    mcp_tools: List[MCPTools],\n+    mcp_server_ids: List[str],\n+) -> None:\n+    \"\"\"Display a session selector in the sidebar, if a new session is selected, the agent is restarted with the new session.\"\"\"\n+\n+    if not agent.storage:\n+        return\n+\n+    try:\n+        # -*- Get all agent sessions.\n+        agent_sessions = agent.storage.get_all_sessions()\n+\n+        if not agent_sessions:\n+            st.sidebar.info(\"No saved sessions found.\")\n+            return\n+\n+        # -*- Get session names if available, otherwise use IDs.\n+        sessions_list = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            sessions_list.append({\"id\": session_id, \"display_name\": display_name})\n+\n+        # -*- Display session selector.\n+        st.sidebar.markdown(\"#### \ud83d\udcac Session\")\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display_name\"] for s in sessions_list],\n+            key=\"session_selector\",\n+            label_visibility=\"collapsed\",\n+        )\n+        # -*- Find the selected session ID.\n+        selected_session_id = next(\n+            s[\"id\"] for s in sessions_list if s[\"display_name\"] == selected_session\n+        )\n+        # -*- Update the selected session if it has changed.\n+        if st.session_state.get(\"mcp_agent_session_id\") != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_str} run: {selected_session_id} ---*---\"\n+            )\n+            st.session_state[\"mcp_agent\"] = get_mcp_agent(\n+                model_str=model_str,\n+                session_id=selected_session_id,\n+                num_history_responses=num_history_responses,\n+                mcp_tools=mcp_tools,\n+                mcp_server_ids=mcp_server_ids,\n+            )\n+            st.rerun()\n+\n+        # -*- Show the rename session widget.\n+        container = st.sidebar.container()\n+        session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+        # -*- Initialize session_edit_mode if needed.\n+        if \"session_edit_mode\" not in st.session_state:\n+            st.session_state.session_edit_mode = False\n+\n+        # -*- Show the session name.\n+        with session_row[0]:\n+            if st.session_state.session_edit_mode:\n+                new_session_name = st.text_input(\n+                    \"Session Name\",\n+                    value=agent.session_name,\n+                    key=\"session_name_input\",\n+                    label_visibility=\"collapsed\",\n+                )\n+            else:\n+                st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+        # -*- Show the rename session button.\n+        with session_row[1]:\n+            if st.session_state.session_edit_mode:\n+                if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                    if new_session_name:\n+                        agent.rename_session(new_session_name)\n+                        st.session_state.session_edit_mode = False\n+                        container.success(\"Renamed!\")\n+                        # Trigger a rerun to refresh the sessions list\n+                        st.rerun()\n+            else:\n+                if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                    st.session_state.session_edit_mode = True\n+    except Exception as e:\n+        logger.error(f\"Error in session selector: {str(e)}\")\n+        st.sidebar.error(\"Failed to load sessions\")\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"mcp_agent\"] = None\n+    st.session_state[\"mcp_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\n+\n+    Returns:\n+        str: Formatted markdown string of the chat history\n+    \"\"\"\n+    if \"messages\" not in st.session_state or not st.session_state[\"messages\"]:\n+        return \"# MCP Agent - Chat History\\n\\nNo messages to export.\"\n+\n+    chat_text = \"# MCP Agent - Chat History\\n\\n\"\n+    for msg in st.session_state[\"messages\"]:\n+        role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+        chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+\n+        # Include tool calls if present\n+        if msg.get(\"tool_calls\"):\n+            chat_text += \"#### Tool Calls:\\n\"\n+            for i, tool_call in enumerate(msg[\"tool_calls\"]):\n+                tool_name = tool_call.get(\"name\", \"Unknown Tool\")\n+                chat_text += f\"**{i + 1}. {tool_name}**\\n\\n\"\n+                if \"arguments\" in tool_call:\n+                    chat_text += (\n+                        f\"Arguments: ```json\\n{tool_call['arguments']}\\n```\\n\\n\"\n+                    )\n+                if \"content\" in tool_call:\n+                    chat_text += f\"Results: ```\\n{tool_call['content']}\\n```\\n\\n\"\n+\n+    return chat_text\n+\n+\n+def utilities_widget(agent: Agent) -> None:\n+    \"\"\"Display a utilities widget in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+    col1, col2 = st.sidebar.columns(2)\n+    with col1:\n+        if st.button(\"\ud83d\udd04 New Chat\"):\n+            restart_agent()\n+    with col2:\n+        fn = \"mcp_agent_chat_history.md\"\n+        if \"mcp_agent_session_id\" in st.session_state:\n+            fn = f\"mcp_agent_{st.session_state.mcp_agent_session_id}.md\"\n+        if st.download_button(\n+            \":file_folder: Export Chat\",\n+            export_chat_history(),\n+            file_name=fn,\n+            mime=\"text/markdown\",\n+        ):\n+            st.sidebar.success(\"Chat history exported!\")\n+    if agent is not None and agent.knowledge is not None:\n+        if st.sidebar.button(\"\ud83d\udcda Load Knowledge\"):\n+            agent.knowledge.load()\n+            st.sidebar.success(\"Knowledge loaded!\")\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"#### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\n+        \"\"\"\n+        The Universal MCP Agent lets you interact with MCP servers using a chat interface.\n+\n+        Built with:\n+        - \ud83d\ude80 [Agno](https://github.com/agno-agi/agno)\n+        - \ud83d\udcab [Streamlit](https://streamlit.io)\n+        \"\"\"\n+    )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .sql-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #FF4B2B;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .sql-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n+\n+\n+# Add a function to handle theme customization\n+def apply_theme():\n+    \"\"\"Apply custom theme settings to the Streamlit app.\"\"\"\n+    # Set page configuration\n+    st.set_page_config(\n+        page_title=\"Universal MCP Agent\",\n+        page_icon=\":crystal_ball:\",\n+        layout=\"wide\",\n+        initial_sidebar_state=\"expanded\",\n+    )\n+\n+    # Apply custom CSS\n+    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/medical_imaging/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/medical_imaging/__init__.py b/cookbook/examples/streamlit_apps/medical_imaging/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/medical_imaging/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/medical_imaging/app.py b/cookbook/examples/streamlit_apps/medical_imaging/app.py\nnew file mode 100644\nindex 000000000..d08190135\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/medical_imaging/app.py\n@@ -0,0 +1,121 @@\n+import os\n+\n+import streamlit as st\n+from agno.media import Image as AgnoImage\n+from medical_agent import agent\n+from PIL import Image as PILImage\n+\n+st.set_page_config(\n+    page_title=\"Medical Imaging Analysis\",\n+    page_icon=\"\ud83c\udfe5\",\n+    layout=\"wide\",\n+)\n+st.markdown(\"##### \ud83c\udfe5 built using [Agno](https://github.com/agno-agi/agno)\")\n+\n+\n+def main():\n+    with st.sidebar:\n+        st.info(\n+            \"This tool provides AI-powered analysis of medical imaging data using \"\n+            \"advanced computer vision and radiological expertise.\"\n+        )\n+        st.warning(\n+            \"\u26a0DISCLAIMER: This tool is for educational and informational purposes only. \"\n+            \"All analyses should be reviewed by qualified healthcare professionals. \"\n+            \"Do not make medical decisions based solely on this analysis.\"\n+        )\n+\n+    st.title(\"\ud83c\udfe5 Medical Imaging Diagnosis Agent\")\n+    st.write(\"Upload a medical image for professional analysis\")\n+\n+    # Create containers for better organization\n+    upload_container = st.container()\n+    image_container = st.container()\n+    analysis_container = st.container()\n+\n+    with upload_container:\n+        uploaded_file = st.file_uploader(\n+            \"Upload Medical Image\",\n+            type=[\"jpg\", \"jpeg\", \"png\", \"dicom\"],\n+            help=\"Supported formats: JPG, JPEG, PNG, DICOM\",\n+        )\n+\n+    if uploaded_file is not None:\n+        with image_container:\n+            col1, col2, col3 = st.columns([1, 2, 1])\n+            with col2:\n+                # Use PILImage for display\n+                pil_image = PILImage.open(uploaded_file)\n+                width, height = pil_image.size\n+                aspect_ratio = width / height\n+                new_width = 500\n+                new_height = int(new_width / aspect_ratio)\n+                resized_image = pil_image.resize((new_width, new_height))\n+\n+                st.image(\n+                    resized_image,\n+                    caption=\"Uploaded Medical Image\",\n+                    use_container_width=True,\n+                )\n+\n+                analyze_button = st.button(\n+                    \"\ud83d\udd0d Analyze Image\", type=\"primary\", use_container_width=True\n+                )\n+\n+                additional_info = st.text_area(\n+                    \"Provide additional context about the image (e.g., patient history, symptoms)\",\n+                    placeholder=\"Enter any relevant information here...\",\n+                )\n+\n+        with analysis_container:\n+            if analyze_button:\n+                image_path = \"temp_medical_image.png\"\n+                # Save the resized image\n+                resized_image.save(image_path, format=\"PNG\")\n+\n+                with st.spinner(\"\ud83d\udd04 Analyzing image... Please wait.\"):\n+                    try:\n+                        # Read the image file as binary\n+                        with open(image_path, \"rb\") as f:\n+                            image_bytes = f.read()\n+                        # creating an instance of Image\n+                        agno_image = AgnoImage(content=image_bytes, format=\"png\")\n+\n+                        prompt = (\n+                            f\"Analyze this medical image considering the following context: {additional_info}\"\n+                            if additional_info\n+                            else \"Analyze this medical image and provide detailed findings.\"\n+                        )\n+                        response = agent.run(prompt, images=[agno_image])\n+                        st.markdown(\"### \ud83d\udccb Analysis Results\")\n+                        st.markdown(\"---\")\n+                        if hasattr(response, \"content\"):\n+                            st.markdown(response.content)\n+                        elif isinstance(response, str):\n+                            st.markdown(response)\n+                        elif isinstance(response, dict) and \"content\" in response:\n+                            st.markdown(response[\"content\"])\n+                        else:\n+                            st.markdown(str(response))\n+                        st.markdown(\"---\")\n+                        st.caption(\n+                            \"Note: This analysis is generated by AI and should be reviewed by \"\n+                            \"a qualified healthcare professional.\"\n+                        )\n+\n+                    except Exception as e:\n+                        st.error(f\"Analysis error: {str(e)}\")\n+                        st.info(\n+                            \"Please try again or contact support if the issue persists.\"\n+                        )\n+                        print(f\"Detailed error: {e}\")\n+                    finally:\n+                        if os.path.exists(image_path):\n+                            os.remove(image_path)\n+\n+    else:\n+        st.info(\"\ud83d\udc46 Please upload a medical image to begin analysis\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/medical_imaging/medical_agent.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/medical_imaging/medical_agent.py b/cookbook/examples/streamlit_apps/medical_imaging/medical_agent.py\nnew file mode 100644\nindex 000000000..505f7589f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/medical_imaging/medical_agent.py\n@@ -0,0 +1,92 @@\n+\"\"\"\n+Medical Imaging Analysis Agent Tutorial\n+=====================================\n+\n+This example demonstrates how to create an AI agent specialized in medical imaging analysis.\n+The agent can analyze various types of medical images (X-ray, MRI, CT, Ultrasound) and provide\n+detailed professional analysis along with patient-friendly explanations.\n+\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+# Base prompt that defines the agent's expertise and response structure\n+BASE_PROMPT = \"\"\"You are a highly skilled medical imaging expert with extensive knowledge in radiology \n+and diagnostic imaging. Your role is to provide comprehensive, accurate, and ethical analysis of medical images.\n+\n+Key Responsibilities:\n+1. Maintain patient privacy and confidentiality\n+2. Provide objective, evidence-based analysis\n+3. Highlight any urgent or critical findings\n+4. Explain findings in both professional and patient-friendly terms\n+\n+For each image analysis, structure your response as follows:\"\"\"\n+\n+# Detailed instructions for image analysis\n+ANALYSIS_TEMPLATE = \"\"\"\n+### 1. Image Technical Assessment\n+- Imaging modality identification\n+- Anatomical region and patient positioning\n+- Image quality evaluation (contrast, clarity, artifacts)\n+- Technical adequacy for diagnostic purposes\n+\n+### 2. Professional Analysis\n+- Systematic anatomical review\n+- Primary findings with precise measurements\n+- Secondary observations\n+- Anatomical variants or incidental findings\n+- Severity assessment (Normal/Mild/Moderate/Severe)\n+\n+### 3. Clinical Interpretation\n+- Primary diagnosis (with confidence level)\n+- Differential diagnoses (ranked by probability)\n+- Supporting evidence from the image\n+- Critical/Urgent findings (if any)\n+- Recommended follow-up studies (if needed)\n+\n+### 4. Patient Education\n+- Clear, jargon-free explanation of findings\n+- Visual analogies and simple diagrams when helpful\n+- Common questions addressed\n+- Lifestyle implications (if any)\n+\n+### 5. Evidence-Based Context\n+Using DuckDuckGo search:\n+- Recent relevant medical literature\n+- Standard treatment guidelines\n+- Similar case studies\n+- Technological advances in imaging/treatment\n+- 2-3 authoritative medical references\n+\n+Please maintain a professional yet empathetic tone throughout the analysis.\n+\"\"\"\n+\n+# Combine prompts for the final instruction\n+FULL_INSTRUCTIONS = BASE_PROMPT + ANALYSIS_TEMPLATE\n+\n+# Initialize the Medical Imaging Expert agent\n+agent = Agent(\n+    name=\"Medical Imaging Expert\",\n+    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    tools=[DuckDuckGoTools()],  # Enable web search for medical literature\n+    markdown=True,  # Enable markdown formatting for structured output\n+    instructions=FULL_INSTRUCTIONS,\n+)\n+\n+# Example usage\n+if __name__ == \"__main__\":\n+    # Example image path (users should replace with their own image)\n+    image_path = Path(__file__).parent.joinpath(\"test.jpg\")\n+\n+    # Uncomment to run the analysis\n+    # agent.print_response(\"Please analyze this medical image.\", images=[image_path])\n+\n+    # Example with specific focus\n+    # agent.print_response(\n+    #     \"Please analyze this image with special attention to bone density.\",\n+    #     images=[image_path]\n+    # )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/medical_imaging/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/medical_imaging/requirements.txt b/cookbook/examples/streamlit_apps/medical_imaging/requirements.txt\nnew file mode 100644\nindex 000000000..d55e11faf\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/medical_imaging/requirements.txt\n@@ -0,0 +1,4 @@\n+agno\n+openai\n+streamlit\n+duckduckgo-search\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/paperpal/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/paperpal/__init__.py b/cookbook/examples/streamlit_apps/paperpal/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/paperpal/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/paperpal/app.py b/cookbook/examples/streamlit_apps/paperpal/app.py\nnew file mode 100644\nindex 000000000..bde49bf29\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/paperpal/app.py\n@@ -0,0 +1,242 @@\n+import json\n+from typing import Optional\n+\n+import pandas as pd\n+import streamlit as st\n+from technical_writer import (\n+    ArxivSearchResults,\n+    SearchTerms,\n+    WebSearchResults,\n+    arxiv_search_agent,\n+    arxiv_toolkit,\n+    exa_search_agent,\n+    research_editor,\n+    search_term_generator,\n+)\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"AI Researcher Workflow\",\n+    page_icon=\":orange_heart:\",\n+)\n+st.title(\"Paperpal\")\n+st.markdown(\"##### :orange_heart: built by [agno](https://github.com/agno-agi/agno)\")\n+\n+\n+def main() -> None:\n+    # Get topic for report\n+    input_topic = st.sidebar.text_input(\n+        \":female-scientist: Enter a topic\",\n+        value=\"LLM evals in multi-agentic space\",\n+    )\n+    # Button to generate blog\n+    generate_report = st.sidebar.button(\"Generate Blog\")\n+    if generate_report:\n+        st.session_state[\"topic\"] = input_topic\n+\n+    # Checkboxes for search\n+    st.sidebar.markdown(\"## Agents\")\n+    search_exa = st.sidebar.checkbox(\"Exa Search\", value=True)\n+    search_arxiv = st.sidebar.checkbox(\"ArXiv Search\", value=False)\n+    # search_pubmed = st.sidebar.checkbox(\"PubMed Search\", disabled=True)  # noqa\n+    # search_google_scholar = st.sidebar.checkbox(\"Google Scholar Search\", disabled=True)  # noqa\n+    # use_cache = st.sidebar.toggle(\"Use Cache\", value=False, disabled=True)  # noqa\n+    num_search_terms = st.sidebar.number_input(\n+        \"Number of Search Terms\",\n+        value=2,\n+        min_value=2,\n+        max_value=3,\n+        help=\"This will increase latency.\",\n+    )\n+\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"## Trending Topics\")\n+    topic = \"Humanoid and Autonomous Agents\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    topic = \"Gene Editing for Disease Treatment\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    topic = \"Multimodal AI in healthcare\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    topic = \"Brain Aging and Neurodegenerative Diseases\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    if \"topic\" in st.session_state:\n+        report_topic = st.session_state[\"topic\"]\n+\n+        search_terms: Optional[SearchTerms] = None\n+        with st.status(\"Generating Search Terms\", expanded=True) as status:\n+            with st.container():\n+                search_terms_container = st.empty()\n+                search_generator_input = {\n+                    \"topic\": report_topic,\n+                    \"num_terms\": num_search_terms,\n+                }\n+                search_terms = search_term_generator.run(\n+                    json.dumps(search_generator_input)\n+                ).content\n+                if search_terms:\n+                    search_terms_container.json(search_terms.model_dump())\n+            status.update(\n+                label=\"Search Terms Generated\", state=\"complete\", expanded=False\n+            )\n+\n+        if not search_terms:\n+            st.write(\"Sorry report generation failed. Please try again.\")\n+            return\n+\n+        exa_content: Optional[str] = None\n+        arxiv_content: Optional[str] = None\n+\n+        if search_exa:\n+            with st.status(\"Searching Exa\", expanded=True) as status:\n+                with st.container():\n+                    exa_container = st.empty()\n+                    try:\n+                        exa_search_results = exa_search_agent.run(\n+                            search_terms.model_dump_json(indent=4)\n+                        )\n+                        if isinstance(exa_search_results, str):\n+                            raise ValueError(\n+                                \"Unexpected string response from exa_search_agent\"\n+                            )\n+\n+                        if isinstance(exa_search_results.content, WebSearchResults):\n+                            exa_container.json(exa_search_results.content.results)\n+                            if (\n+                                exa_search_results\n+                                and exa_search_results.content\n+                                and len(exa_search_results.content.results) > 0\n+                            ):\n+                                exa_content = (\n+                                    exa_search_results.content.model_dump_json(indent=4)\n+                                )\n+                                exa_container.json(exa_search_results.content.results)\n+                                status.update(\n+                                    label=\"Exa Search Complete\",\n+                                    state=\"complete\",\n+                                    expanded=False,\n+                                )\n+                        else:\n+                            raise TypeError(\"Unexpected response from exa_search_agent\")\n+\n+                    except Exception as e:\n+                        st.error(f\"An error occurred during Exa search: {e}\")\n+                        status.update(\n+                            label=\"Exa Search Failed\", state=\"error\", expanded=True\n+                        )\n+                        exa_content = None\n+\n+        if search_arxiv:\n+            with st.status(\n+                \"Searching ArXiv (this takes a while)\", expanded=True\n+            ) as status:\n+                with st.container():\n+                    arxiv_container = st.empty()\n+                    arxiv_search_results = arxiv_search_agent.run(\n+                        search_terms.model_dump_json(indent=4)\n+                    )\n+                    if isinstance(arxiv_search_results.content, ArxivSearchResults):\n+                        if (\n+                            arxiv_search_results\n+                            and arxiv_search_results.content\n+                            and arxiv_search_results.content.results\n+                        ):\n+                            arxiv_container.json(\n+                                [\n+                                    result.model_dump()\n+                                    for result in arxiv_search_results.content.results\n+                                ]\n+                            )\n+                    else:\n+                        raise TypeError(\"Unexpected response from arxiv_search_agent\")\n+\n+                status.update(\n+                    label=\"ArXiv Search Complete\", state=\"complete\", expanded=False\n+                )\n+\n+            if (\n+                arxiv_search_results\n+                and arxiv_search_results.content\n+                and arxiv_search_results.content.results\n+            ):\n+                paper_summaries = []\n+                for result in arxiv_search_results.content.results:\n+                    summary = {\n+                        \"ID\": result.id,\n+                        \"Title\": result.title,\n+                        \"Authors\": \", \".join(result.authors)\n+                        if result.authors\n+                        else \"No authors available\",\n+                        \"Summary\": result.summary[:200] + \"...\"\n+                        if len(result.summary) > 200\n+                        else result.summary,\n+                    }\n+                    paper_summaries.append(summary)\n+\n+                if paper_summaries:\n+                    with st.status(\n+                        \"Displaying ArXiv Paper Summaries\", expanded=True\n+                    ) as status:\n+                        with st.container():\n+                            st.subheader(\"ArXiv Paper Summaries\")\n+                            df = pd.DataFrame(paper_summaries)\n+                            st.dataframe(df, use_container_width=True)\n+                        status.update(\n+                            label=\"ArXiv Paper Summaries Displayed\",\n+                            state=\"complete\",\n+                            expanded=False,\n+                        )\n+\n+                    arxiv_paper_ids = [summary[\"ID\"] for summary in paper_summaries]\n+                    if arxiv_paper_ids:\n+                        with st.status(\"Reading ArXiv Papers\", expanded=True) as status:\n+                            with st.container():\n+                                arxiv_content = arxiv_toolkit.read_arxiv_papers(\n+                                    arxiv_paper_ids, pages_to_read=2\n+                                )\n+                                st.write(f\"Read {len(arxiv_paper_ids)} ArXiv papers\")\n+                            status.update(\n+                                label=\"Reading ArXiv Papers Complete\",\n+                                state=\"complete\",\n+                                expanded=False,\n+                            )\n+\n+        report_input = \"\"\n+        report_input += f\"# Topic: {report_topic}\\n\\n\"\n+        report_input += \"## Search Terms\\n\\n\"\n+        report_input += f\"{search_terms}\\n\\n\"\n+        if arxiv_content:\n+            report_input += \"## ArXiv Papers\\n\\n\"\n+            report_input += \"<arxiv_papers>\\n\\n\"\n+            report_input += f\"{arxiv_content}\\n\\n\"\n+            report_input += \"</arxiv_papers>\\n\\n\"\n+        if exa_content:\n+            report_input += \"## Web Search Content from Exa\\n\\n\"\n+            report_input += \"<exa_content>\\n\\n\"\n+            report_input += f\"{exa_content}\\n\\n\"\n+            report_input += \"</exa_content>\\n\\n\"\n+\n+        # Only generate the report if we have content\n+        if arxiv_content or exa_content:\n+            with st.spinner(\"Generating Blog\"):\n+                final_report_container = st.empty()\n+                research_report = research_editor.run(report_input)\n+                final_report_container.markdown(research_report.content)\n+        else:\n+            st.error(\n+                \"Report generation cancelled due to search failure. Please try again or select another search option.\"\n+            )\n+\n+    st.sidebar.markdown(\"---\")\n+    if st.sidebar.button(\"Restart\"):\n+        st.rerun()\n+\n+\n+main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/paperpal/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/paperpal/requirements.txt b/cookbook/examples/streamlit_apps/paperpal/requirements.txt\nnew file mode 100644\nindex 000000000..311fcff17\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/paperpal/requirements.txt\n@@ -0,0 +1,5 @@\n+agno\n+openai\n+streamlit\n+exa_py\n+arxiv\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/paperpal/technical_writer.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/paperpal/technical_writer.py b/cookbook/examples/streamlit_apps/paperpal/technical_writer.py\nnew file mode 100644\nindex 000000000..156d203d2\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/paperpal/technical_writer.py\n@@ -0,0 +1,147 @@\n+import os\n+from pathlib import Path\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.arxiv import ArxivTools\n+from agno.tools.exa import ExaTools\n+from dotenv import load_dotenv\n+from pydantic import BaseModel, Field\n+\n+load_dotenv()\n+\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+\n+# Define data models\n+class SearchTerms(BaseModel):\n+    terms: List[str] = Field(\n+        ..., description=\"List of search terms related to a topic.\"\n+    )\n+\n+\n+class ArxivSearchResult(BaseModel):\n+    title: str = Field(..., description=\"Title of the article.\")\n+    id: str = Field(..., description=\"The ID of the article.\")\n+    authors: List[str] = Field(..., description=\"Authors of the article.\")\n+    summary: str = Field(..., description=\"Summary of the article.\")\n+    pdf_url: str = Field(..., description=\"URL of the PDF of the article.\")\n+    links: List[str] = Field(..., description=\"Links related to the article.\")\n+    reasoning: str = Field(..., description=\"Reason for selecting this article.\")\n+\n+\n+class ArxivSearchResults(BaseModel):\n+    results: List[ArxivSearchResult] = Field(\n+        ..., description=\"List of top search results.\"\n+    )\n+\n+\n+class WebSearchResult(BaseModel):\n+    title: str = Field(..., description=\"Title of the article.\")\n+    summary: str = Field(..., description=\"Summary of the article.\")\n+    links: List[str] = Field(..., description=\"Links related to the article.\")\n+    reasoning: str = Field(..., description=\"Reason for selecting this article.\")\n+\n+\n+class WebSearchResults(BaseModel):\n+    results: List[WebSearchResult] = Field(\n+        ..., description=\"List of top search results.\"\n+    )\n+\n+\n+# Initialize tools\n+arxiv_toolkit = ArxivTools(\n+    download_dir=Path(__file__).parent.parent.parent.parent.joinpath(\n+        \"wip\", \"arxiv_pdfs\"\n+    )\n+)\n+exa_tools = ExaTools()\n+\n+# Initialize agents\n+search_term_generator = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are an expert research strategist. Generate 2 specific and distinct search terms that will capture different key aspects of the given topic.\n+Focus on terms that are:\n+    - Specific enough to yield relevant results\n+    - Cover both technical and practical aspects of the topic\n+    - Relevant to current developments\n+    - Optimized for searching academic and web resources effectively\n+\n+Provide the search terms as a list of strings like [\"xyz\", \"abc\", ...]\n+\"\"\",\n+    response_model=SearchTerms,\n+)\n+\n+arxiv_search_agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are an expert in academic research with access to ArXiv's database.\n+\n+Your task is to:\n+1. Search ArXiv for the top 10 papers related to the provided search term.\n+2. Select the 3 most relevant research papers based on:\n+    - Direct relevance to the search term.\n+    - Scientific impact (e.g., citations, journal reputation).\n+    - Recency of publication.\n+\n+For each selected paper, the output should be in json structure have these details:\n+    - title\n+    - id\n+    - authors\n+    - a concise summary\n+    - the PDF link of the research paper\n+    - links related to the research paper\n+    - reasoning for why the paper was chosen\n+\n+Ensure the selected research papers directly address the topic and offer valuable insights.\n+\"\"\",\n+    tools=[arxiv_toolkit],\n+    response_model=ArxivSearchResults,\n+)\n+\n+exa_search_agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are a web search expert specializing in extracting high-quality information.\n+\n+Your task is to:\n+1. Given a topic, search Exa for the top 10 articles about that topic.\n+2. Select the 3 most relevant articles based on:\n+    - Source credibility.\n+    - Content depth and relevance.\n+\n+For each selected article, the output should have:\n+    - title\n+    - a concise summary\n+    - related links to the article\n+    - reasoning for why the article was chosen and how it contributes to understanding the topic.\n+\n+Ensure the selected articles are credible, relevant, and provide significant insights into the topic.\n+\"\"\",\n+    tools=[ExaTools()],\n+    response_model=WebSearchResults,\n+)\n+\n+research_editor = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are a senior research editor specializing in breaking complex topics and information into understandable, engaging, high-quality blogs.\n+\n+Your task is to:\n+1. Create a detailed blog within 1000 words based on the given topic.\n+2. The blog should be of max 7-8 paragraphs, understandable, intuitive, making things easy to understand for the reader.\n+3. Highlight key findings and provide a clear, high-level overview of the topic.\n+4. At the end add the supporting articles link, paper link or any findings you think is necessary to add.\n+\n+The blog should help the reader in getting a decent understanding of the topic.\n+The blog should me in markdown format.\n+\"\"\",\n+    instructions=[\n+        \"Analyze all materials before writing.\",\n+        \"Build a clear narrative structure.\",\n+        \"Balance technical accuracy with explainability.\",\n+    ],\n+    markdown=True,\n+)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/parallel_world_builder/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/parallel_world_builder/__init__.py b/cookbook/examples/streamlit_apps/parallel_world_builder/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/parallel_world_builder/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/parallel_world_builder/agents.py b/cookbook/examples/streamlit_apps/parallel_world_builder/agents.py\nnew file mode 100644\nindex 000000000..62a64c193\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/parallel_world_builder/agents.py\n@@ -0,0 +1,141 @@\n+\"\"\"\ud83c\udf0e World Builder - Your AI World Creator!\n+\n+This advanced example shows how to build a sophisticated world building system that\n+creates rich, detailed fictional worlds.\n+\n+Example prompts to try:\n+- \"Create a world where time flows backwards\"\n+- \"Design a steampunk world powered by dreams\"\n+- \"Build an underwater civilization in a gas giant\"\n+- \"Make a world where music is the source of magic\"\n+- \"Design a world where plants are sentient and rule\"\n+- \"Create a world inside a giant computer simulation\"\n+\n+View the README for instructions on how to run the application.\n+\"\"\"\n+\n+from textwrap import dedent\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.models.anthropic.claude import Claude\n+from agno.models.google.gemini import Gemini\n+from agno.models.openai import OpenAIChat\n+from pydantic import BaseModel, Field\n+\n+\n+class World(BaseModel):\n+    \"\"\"Model representing a fictional world with its key attributes.\"\"\"\n+\n+    name: str = Field(\n+        ...,\n+        description=(\n+            \"The name of this world. Be exceptionally creative and unique. \"\n+            \"Avoid using simple names like Futura, Earth, or other common names.\"\n+        ),\n+    )\n+    characteristics: List[str] = Field(\n+        ...,\n+        description=(\n+            \"Key attributes of the world. Examples: Ethereal, Arcane, Quantum-Fueled, \"\n+            \"Dreamlike, Mechanized, Harmonious. Think outside the box.\"\n+        ),\n+    )\n+    currency: str = Field(\n+        ...,\n+        description=(\n+            \"The monetary system or trade medium in the world. \"\n+            \"Consider unusual or symbolic currencies (e.g., Memory Crystals, Void Shards).\"\n+        ),\n+    )\n+    languages: List[str] = Field(\n+        ...,\n+        description=(\n+            \"The languages spoken in the world. Invent languages with unique phonetics, \"\n+            \"scripts, or origins. Examples: Elurian, Syneth, Aeon's Glyph.\"\n+        ),\n+    )\n+    history: str = Field(\n+        ...,\n+        description=(\n+            \"The detailed history of the world spanning at least 100,000 years. \"\n+            \"Include pivotal events, revolutions, cataclysms, golden ages, and more. \"\n+            \"Make it immersive and richly detailed.\"\n+        ),\n+    )\n+    wars: List[str] = Field(\n+        ...,\n+        description=(\n+            \"List of major wars or conflicts that shaped the world. Each should have unique \"\n+            \"motivations, participants, and consequences.\"\n+        ),\n+    )\n+    drugs: List[str] = Field(\n+        ...,\n+        description=(\n+            \"Substances used in the world, either recreationally, medically, or spiritually. \"\n+            \"Invent intriguing names and effects (e.g., Lunar Nectar, Dreamweaver Elixir).\"\n+        ),\n+    )\n+\n+\n+def get_world_builder(\n+    model_id: str = \"openai:gpt-4o\",\n+    debug_mode: bool = False,\n+) -> Agent:\n+    \"\"\"Returns an instance of the World Builder Agent.\n+\n+    Args:\n+        model: Model identifier to use\n+        debug_mode: Enable debug logging\n+    \"\"\"\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+    return Agent(\n+        name=\"world_builder\",\n+        model=model,\n+        description=dedent(\"\"\"\\\n+        You are WorldCrafter-X, an elite world building specialist focused on:\n+        \n+        - Unique world concepts\n+        - Rich cultural details  \n+        - Complex histories\n+        - Innovative systems\n+        - Compelling conflicts\n+        - Immersive atmospheres\n+        \n+        You combine boundless creativity with meticulous attention to detail to craft unforgettable worlds.\"\"\"),\n+        instructions=dedent(\"\"\"\\\n+        You are tasked with creating entirely unique and intricate worlds.\n+        \n+        When a user provides a world description:\n+        1. Carefully analyze all aspects of the requested world\n+        2. Think deeply about how different elements would interact\n+        3. Create rich, interconnected systems and histories\n+        4. Ensure internal consistency while being creative\n+        5. Focus on unique and memorable details\n+        6. Avoid clich\u00e9s and common tropes\n+        7. Consider long-term implications of world features\n+        8. Create compelling conflicts and dynamics\n+        \n+        Remember to:\n+        - Push creative boundaries\n+        - Use vivid, evocative language\n+        - Create memorable names and terms\n+        - Maintain internal logic\n+        - Consider multiple cultural perspectives\n+        - Add unexpected but fitting elements\"\"\"),\n+        response_model=World,\n+        debug_mode=debug_mode,\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/parallel_world_builder/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/parallel_world_builder/app.py b/cookbook/examples/streamlit_apps/parallel_world_builder/app.py\nnew file mode 100644\nindex 000000000..df71ee8f5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/parallel_world_builder/app.py\n@@ -0,0 +1,166 @@\n+from typing import Optional\n+\n+import streamlit as st\n+from agents import World, get_world_builder\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import add_message, display_tool_calls, sidebar_widget\n+\n+# set page config\n+st.set_page_config(\n+    page_title=\"World Building\",\n+    page_icon=\":ringed_planet:\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Parallel World Building</h1>\", unsafe_allow_html=True\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent world creator powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gemini-2.0-flash-exp\": \"google:gemini-2.0-flash-exp\",\n+        \"claude-3-5-sonnet\": \"anthropic:claude-3-5-sonnet-20241022\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    world_builder: Agent\n+    if (\n+        \"world_builder\" not in st.session_state\n+        or st.session_state[\"world_builder\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new World Builder agent ---*---\")\n+        world_builder = get_world_builder(model_id=model_id)\n+        st.session_state[\"world_builder\"] = world_builder\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        world_builder = st.session_state[\"world_builder\"]\n+\n+    ####################################################################\n+    # Initialize messages if not exists\n+    ####################################################################\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"Describe your world! \ud83c\udf0f\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            with st.chat_message(message[\"role\"]):\n+                if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                    display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                st.markdown(message[\"content\"])\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Generating world...\"):\n+                try:\n+                    # Run the agent and get response\n+                    run_response = world_builder.run(question)\n+                    world_data: World = run_response.content\n+\n+                    # Display world details in a single column layout\n+                    st.header(world_data.name)\n+\n+                    st.subheader(\"\ud83c\udf1f Characteristics\")\n+                    for char in world_data.characteristics:\n+                        st.markdown(f\"- {char}\")\n+\n+                    st.subheader(\"\ud83d\udcb0 Currency\")\n+                    st.markdown(world_data.currency)\n+\n+                    st.subheader(\"\ud83d\udde3\ufe0f Languages\")\n+                    for lang in world_data.languages:\n+                        st.markdown(f\"- {lang}\")\n+\n+                    st.subheader(\"\u2694\ufe0f Major Wars & Conflicts\")\n+                    for war in world_data.wars:\n+                        st.markdown(f\"- {war}\")\n+\n+                    st.subheader(\"\ud83e\uddea Notable Substances\")\n+                    for drug in world_data.drugs:\n+                        st.markdown(f\"- {drug}\")\n+\n+                    st.subheader(\"\ud83d\udcdc History\")\n+                    st.markdown(world_data.history)\n+\n+                    # Store the formatted response for chat history\n+                    response = f\"\"\"# {world_data.name}\n+\n+### Characteristics\n+{chr(10).join(\"- \" + char for char in world_data.characteristics)}\n+\n+### Currency\n+{world_data.currency}\n+\n+### Languages\n+{chr(10).join(\"- \" + lang for lang in world_data.languages)}\n+\n+### History\n+{world_data.history}\n+\n+### Major Wars & Conflicts\n+{chr(10).join(\"- \" + war for war in world_data.wars)}\n+\n+### Notable Substances\n+{chr(10).join(\"- \" + drug for drug in world_data.drugs)}\"\"\"\n+\n+                    # Display tool calls if available\n+                    if run_response.tools and len(run_response.tools) > 0:\n+                        display_tool_calls(tool_calls_container, run_response.tools)\n+\n+                    add_message(\"assistant\", response, run_response.tools)\n+\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/parallel_world_builder/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/parallel_world_builder/requirements.txt b/cookbook/examples/streamlit_apps/parallel_world_builder/requirements.txt\nnew file mode 100644\nindex 000000000..4d4ef595d\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/parallel_world_builder/requirements.txt\n@@ -0,0 +1,5 @@\n+agno\n+openai\n+streamlit\n+google-generativeai\n+anthropic\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/parallel_world_builder/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py b/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py\nnew file mode 100644\nindex 000000000..df888b707\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py\n@@ -0,0 +1,99 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"world_builder\"] = None\n+    st.session_state[\"world\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries\"\"\"\n+    with st.sidebar:\n+        # Basic Information\n+        st.markdown(\"#### Sample Queries\")\n+        if st.button(\n+            \"An advanced futuristic city on distant planet with only 1 island. Dark history. Population 1 trillion.\"\n+        ):\n+            add_message(\n+                \"user\",\n+                \"An advanced futuristic city on distant planet with only 1 island. Dark history. Population 1 trillion.\",\n+            )\n+        if st.button(\"A medieval fantasy world with dragons, castles, and knights.\"):\n+            add_message(\n+                \"user\",\n+                \"A medieval fantasy world with dragons, castles, and knights.\",\n+            )\n+        if st.button(\n+            \"A post-apocalyptic world with a nuclear wasteland and a small community living in a dome.\"\n+        ):\n+            add_message(\n+                \"user\",\n+                \"A post-apocalyptic world with a nuclear wasteland and a small community living in a dome.\",\n+            )\n+        if st.button(\n+            \"A world with a mix of ancient and modern civilizations, where magic and technology coexist.\"\n+        ):\n+            add_message(\n+                \"user\",\n+                \"A world with a mix of ancient and modern civilizations, where magic and technology coexist.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        if st.button(\"\ud83d\udd04 New World\"):\n+            restart_agent()\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    with tool_calls_container.container():\n+        for tool_call in tools:\n+            _tool_name = tool_call.get(\"tool_name\")\n+            _tool_args = tool_call.get(\"tool_args\")\n+            _content = tool_call.get(\"content\")\n+            _metrics = tool_call.get(\"metrics\")\n+\n+            with st.expander(\n+                f\"\ud83d\udee0\ufe0f {_tool_name.replace('_', ' ').title()}\", expanded=False\n+            ):\n+                if isinstance(_tool_args, dict) and \"query\" in _tool_args:\n+                    st.code(_tool_args[\"query\"], language=\"sql\")\n+\n+                if _tool_args and _tool_args != {\"query\": None}:\n+                    st.markdown(\"**Arguments:**\")\n+                    st.json(_tool_args)\n+\n+                if _content:\n+                    st.markdown(\"**Results:**\")\n+                    try:\n+                        st.json(_content)\n+                    except Exception as e:\n+                        st.markdown(_content)\n+\n+                if _metrics:\n+                    st.markdown(\"**Metrics:**\")\n+                    st.json(_metrics)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/podcast_generator/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/podcast_generator/__init__.py b/cookbook/examples/streamlit_apps/podcast_generator/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/podcast_generator/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/podcast_generator/agents.py b/cookbook/examples/streamlit_apps/podcast_generator/agents.py\nnew file mode 100644\nindex 000000000..4f7afc67a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/podcast_generator/agents.py\n@@ -0,0 +1,78 @@\n+import os\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.utils.audio import write_audio_to_file\n+from dotenv import load_dotenv\n+\n+# Load environment variables\n+load_dotenv()\n+\n+# OpenAI API Key\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+os.makedirs(\"tmp\", exist_ok=True)\n+\n+\n+def generate_podcast(topic, voice=\"alloy\"):\n+    \"\"\"\n+    Generates a podcast script using a agnodata Agent and converts it to speech using OpenAI TTS.\n+\n+    Args:\n+        topic (str): The topic of the podcast.\n+        voice (str): Voice model for OpenAI TTS. Options: [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n+    \"\"\"\n+\n+    # Create a agnodata agent to generate the podcast script\n+    audio_agent = Agent(\n+        system_message=\"You are a podcast scriptwriter specializing in concise and engaging narratives. Your task is to research a given topic using Exa and DuckDuckGo, gather relevant insights, and compose a short, compelling podcast script.\",\n+        instructions=\"\"\"### **Instructions:**\n+            1. **Research Phase:**\n+            - Use Exa and DuckDuckGo to gather the most recent and relevant information on the given topic.\n+            - Prioritize trustworthy sources such as news sites, academic articles, or well-established blogs.\n+            - Identify key points, statistics, expert opinions, and interesting facts.\n+\n+            2. **Scripting Phase:**\n+            - Write a concise podcast script in a conversational tone.\n+            - Begin with a strong hook to capture the listener's attention.\n+            - Present the key insights in an engaging, easy-to-follow manner.\n+            - Include a smooth transition between ideas to maintain narrative flow.\n+            - End with a closing remark that summarizes the main takeaways and encourages further curiosity.\n+\n+            ### **Formatting Guidelines:**\n+            - Use simple, engaging language.\n+            - Keep the script under 300 words (around 2 minutes of audio).\n+            - Write in a natural, spoken format, avoiding overly formal or technical jargon.\n+            - Start with a short intro of the topic, then cover the main content, and conclude.\n+\n+            ### **Example Output:**\n+            #### **Today we will be covering the topic The Future of AI in Healthcare**\n+            \"Imagine walking into a hospital where AI instantly diagnoses your illness, prescribes treatment, and even assists in surgery. Sounds like science fiction? Well, it's closer to reality than you think! Welcome to today's episode, where we explore how AI is revolutionizing healthcare.\"\n+\n+            \"AI is making waves in medical research, diagnostics, and patient care. For instance, Google's DeepMind developed an AI that can detect over 50 eye diseases with a single scan\u2014just as accurately as top doctors! Meanwhile, robotic surgeries assisted by AI are reducing complications and recovery time for patients. But it's not just about tech\u2014AI is also addressing healthcare accessibility. In rural areas, AI-powered chatbots provide medical advice where doctors are scarce.\"\n+\n+            \"While AI in healthcare is promising, it also raises ethical concerns. Who takes responsibility for a wrong diagnosis? How do we ensure data privacy? These are crucial questions for the future. One thing's for sure\u2014AI is here to stay, and it's reshaping medicine as we know it. Thanks for tuning in, and see you next time!\"\n+            \"\"\",\n+        model=OpenAIChat(\n+            id=\"gpt-4o-audio-preview\",\n+            modalities=[\"text\", \"audio\"],\n+            audio={\"voice\": voice, \"format\": \"wav\"},\n+        ),\n+        tools=[DuckDuckGoTools()],\n+    )\n+\n+    # Generate the podcast script\n+    audio_agent.run(f\"Write the content of podcast for the topic: {topic}\")\n+    audio_file_path = \"tmp/generated_podcast.wav\"\n+\n+    if audio_agent.run_response.response_audio is not None:\n+        audio_content = audio_agent.run_response.response_audio.content\n+\n+        if audio_content:\n+            write_audio_to_file(\n+                audio=audio_content,\n+                filename=audio_file_path,\n+            )\n+            return audio_file_path\n+    return None\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/podcast_generator/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/podcast_generator/app.py b/cookbook/examples/streamlit_apps/podcast_generator/app.py\nnew file mode 100644\nindex 000000000..e36ba1f14\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/podcast_generator/app.py\n@@ -0,0 +1,122 @@\n+import streamlit as st\n+from agents import generate_podcast\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"Podify AI \ud83c\udf99\ufe0f\",\n+    page_icon=\"\ud83c\udfa7\",\n+    layout=\"wide\",\n+)\n+\n+# Sidebar Section\n+with st.sidebar:\n+    st.title(\"\ud83c\udfa7 Podify AI\")\n+    st.markdown(\"AI voices to generate an **engaging podcast**!\")\n+\n+    # Voice Selection\n+    voice_options = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n+    selected_voice = st.selectbox(\"\ud83c\udfa4 Choose a Voice:\", voice_options, index=0)\n+\n+    st.markdown(\"---\")\n+    st.subheader(\"\ud83d\udd25 Suggested Topics:\")\n+    trending_topics = [\n+        \"\ud83c\udfad Impact of AI on Creativity and Art\",\n+        \"\ud83d\udca1 Elon Musk vs Sam Altman\",\n+        \"\ud83c\udfe5 Using AI in healthcare\",\n+        \"\ud83d\ude80 The Future of Space Exploration\",\n+    ]\n+\n+    # Create row-wise aligned buttons\n+    num_cols = 1  # Change this to 3 for three buttons in a row\n+    cols = st.columns(num_cols)  # Define columns\n+\n+    for i, topic in enumerate(trending_topics):\n+        with cols[i % num_cols]:  # Distribute buttons evenly across columns\n+            if st.button(topic):\n+                st.session_state[\"topic\"] = topic\n+                st.session_state[\"generate\"] = True\n+\n+    st.markdown(\"---\")\n+    st.subheader(\"\u2139\ufe0f About\")\n+    st.markdown(\n+        \"\"\"\n+        - Enter a **topic** <br>\n+        - **Select a voice** <br>\n+        - **Click Generate Podcast** <br>\n+        - **Listen & Download** the AI-generated audio\n+        \"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+# Main Content\n+st.title(\"Podify AI\ud83c\udf99\ufe0f\")\n+st.markdown(\":orange_heart: **powered by Agno**\")\n+\n+st.markdown(\n+    \"Create high-quality podcasts on **any topic**! Simply enter a topic and let Podify AI generate a professional podcast with **realistic AI voices**. \ud83d\ude80\"\n+)\n+\n+# Get pre-selected topic from sidebar\n+pre_selected_topic = st.session_state.get(\"topic\", \"\")\n+\n+# Input for Podcast Topic\n+topic = st.text_input(\n+    \"\ud83d\udcd6 **Enter Your Podcast Topic Below:**\",\n+    placeholder=\"E.g., How AI is Changing the Job Market\",\n+    value=pre_selected_topic,\n+)\n+\n+# Check if auto-generation is triggered\n+generate_now = st.session_state.get(\"generate\", False)\n+\n+\n+# Generate Podcast Function\n+def generate_and_display_podcast(topic):\n+    with st.spinner(\"\u23f3 Generating Podcast... This may take up to 2 minute...\"):\n+        audio_path = generate_podcast(topic, selected_voice)\n+\n+    if audio_path:\n+        st.success(\"\u2705 Podcast generated successfully!\")\n+\n+        st.subheader(\"\ud83c\udfa7 Your AI Podcast\")\n+        st.audio(audio_path, format=\"audio/wav\")\n+\n+        with open(audio_path, \"rb\") as audio_file:\n+            st.download_button(\n+                \"\u2b07\ufe0f Download Podcast\",\n+                audio_file,\n+                file_name=\"podcast.wav\",\n+                mime=\"audio/wav\",\n+            )\n+\n+    else:\n+        st.error(\"\u274c Failed to generate podcast. Please try again.\")\n+\n+\n+# Auto-generate podcast if a trending topic is selected\n+if generate_now and topic:\n+    generate_and_display_podcast(topic)\n+    st.session_state[\"generate\"] = False  # Reset the flag after generation\n+\n+# Manual Generate Podcast Button\n+if st.button(\"\ud83c\udfac Generate Podcast\"):\n+    if topic:\n+        generate_and_display_podcast(topic)\n+    else:\n+        st.warning(\"\u26a0\ufe0f Please enter a topic before generating.\")\n+\n+# Footer Section\n+st.markdown(\"---\")\n+st.markdown(\n+    \"\"\"\n+    \ud83c\udf1f **Features:**\n+    - \ud83c\udf99\ufe0f AI-generated podcast scripts based on **real-time research**.\n+    - \ud83d\udde3\ufe0f Multiple **realistic voices** for narration.\n+    - \ud83d\udce5 **Download & share** your podcasts instantly.\n+\n+    \ud83d\udce2 **Disclaimer:** AI-generated content is based on available online data and may not always be accurate.\n+    \"\"\",\n+    unsafe_allow_html=True,\n+)\n+st.markdown(\"---\")\n+st.markdown(\":orange_heart: **Thank you for using Podify AI!**\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/podcast_generator/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/podcast_generator/generate_requirements.sh b/cookbook/examples/streamlit_apps/podcast_generator/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/podcast_generator/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/podcast_generator/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/podcast_generator/requirements.in b/cookbook/examples/streamlit_apps/podcast_generator/requirements.in\nnew file mode 100644\nindex 000000000..d55e11faf\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/podcast_generator/requirements.in\n@@ -0,0 +1,4 @@\n+agno\n+openai\n+streamlit\n+duckduckgo-search\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/podcast_generator/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/podcast_generator/requirements.txt b/cookbook/examples/streamlit_apps/podcast_generator/requirements.txt\nnew file mode 100644\nindex 000000000..3e0299183\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/podcast_generator/requirements.txt\n@@ -0,0 +1,177 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.0\n+    # via -r requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   httpx\n+    #   openai\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.1\n+    # via streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.3.2\n+    # via -r requirements.in\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.5\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.8.2\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.26.0\n+    # via altair\n+numpy==2.2.2\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.62.0\n+    # via -r requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.12.1\n+    # via duckduckgo-search\n+protobuf==5.29.3\n+    # via streamlit\n+pyarrow==19.0.0\n+    # via streamlit\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.7.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   streamlit\n+    #   typer\n+rpds-py==0.22.3\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+streamlit==1.42.0\n+    # via -r requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.1\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/.gitignore b/cookbook/examples/streamlit_apps/sql_agent/.gitignore\nnew file mode 100644\nindex 000000000..53752db25\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/.gitignore\n@@ -0,0 +1 @@\n+output\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/__init__.py b/cookbook/examples/streamlit_apps/sql_agent/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/agents.py b/cookbook/examples/streamlit_apps/sql_agent/agents.py\nnew file mode 100644\nindex 000000000..853e0d82e\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/agents.py\n@@ -0,0 +1,247 @@\n+\"\"\"\ud83d\udc8e Reasoning SQL Agent - Your AI Data Analyst!\n+\n+This advanced example shows how to build a sophisticated text-to-SQL system that\n+leverages Reasoning Agents to provide deep insights into any data.\n+\n+Example queries to try:\n+- \"Who are the top 5 drivers with the most race wins?\"\n+- \"Compare Mercedes vs Ferrari performance in constructors championships\"\n+- \"Show me the progression of fastest lap times at Monza\"\n+- \"Which drivers have won championships with multiple teams?\"\n+- \"What tracks have hosted the most races?\"\n+- \"Show me Lewis Hamilton's win percentage by season\"\n+\n+Examples with table joins:\n+- \"How many races did the championship winners win each year?\"\n+- \"Compare the number of race wins vs championship positions for constructors in 2019\"\n+- \"Show me Lewis Hamilton's race wins and championship positions by year\"\n+- \"Which drivers have both won races and set fastest laps at Monaco?\"\n+- \"Show me Ferrari's race wins and constructor championship positions from 2015-2020\"\n+\n+View the README for instructions on how to run the application.\n+\"\"\"\n+\n+import json\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge.combined import CombinedKnowledgeBase\n+from agno.knowledge.json import JSONKnowledgeBase\n+from agno.knowledge.text import TextKnowledgeBase\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.agent.postgres import PostgresAgentStorage\n+from agno.tools.file import FileTools\n+from agno.tools.reasoning import ReasoningTools\n+from agno.tools.sql import SQLTools\n+from agno.vectordb.pgvector import PgVector\n+\n+# ************* Database Connection *************\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+# *******************************\n+\n+# ************* Paths *************\n+cwd = Path(__file__).parent\n+knowledge_dir = cwd.joinpath(\"knowledge\")\n+output_dir = cwd.joinpath(\"output\")\n+\n+# Create the output directory if it does not exist\n+output_dir.mkdir(parents=True, exist_ok=True)\n+# *******************************\n+\n+# ************* Storage & Knowledge *************\n+agent_storage = PostgresAgentStorage(\n+    db_url=db_url,\n+    # Store agent sessions in the ai.sql_agent_sessions table\n+    table_name=\"sql_agent_sessions\",\n+    schema=\"ai\",\n+)\n+agent_knowledge = CombinedKnowledgeBase(\n+    sources=[\n+        # Reads text files, SQL files, and markdown files\n+        TextKnowledgeBase(\n+            path=knowledge_dir,\n+            formats=[\".txt\", \".sql\", \".md\"],\n+        ),\n+        # Reads JSON files\n+        JSONKnowledgeBase(path=knowledge_dir),\n+    ],\n+    # Store agent knowledge in the ai.sql_agent_knowledge table\n+    vector_db=PgVector(\n+        db_url=db_url,\n+        table_name=\"sql_agent_knowledge\",\n+        schema=\"ai\",\n+        # Use OpenAI embeddings\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    ),\n+    # 5 references are added to the prompt\n+    num_documents=5,\n+)\n+# *******************************\n+\n+# ************* Semantic Model *************\n+# The semantic model helps the agent identify the tables and columns to use\n+# This is sent in the system prompt, the agent then uses the `search_knowledge_base` tool to get table metadata, rules and sample queries\n+# This is very much how data analysts and data scientists work:\n+#  - We start with a set of tables and columns that we know are relevant to the task\n+#  - We then use the `search_knowledge_base` tool to get more information about the tables and columns\n+#  - We then use the `describe_table` tool to get more information about the tables and columns\n+#  - We then use the `search_knowledge_base` tool to get sample queries for the tables and columns\n+semantic_model = {\n+    \"tables\": [\n+        {\n+            \"table_name\": \"constructors_championship\",\n+            \"table_description\": \"Contains data for the constructor's championship from 1958 to 2020, capturing championship standings from when it was introduced.\",\n+            \"Use Case\": \"Use this table to get data on constructor's championship for various years or when analyzing team performance over the years.\",\n+        },\n+        {\n+            \"table_name\": \"drivers_championship\",\n+            \"table_description\": \"Contains data for driver's championship standings from 1950-2020, detailing driver positions, teams, and points.\",\n+            \"Use Case\": \"Use this table to access driver championship data, useful for detailed driver performance analysis and comparisons over years.\",\n+        },\n+        {\n+            \"table_name\": \"fastest_laps\",\n+            \"table_description\": \"Contains data for the fastest laps recorded in races from 1950-2020, including driver and team details.\",\n+            \"Use Case\": \"Use this table when needing detailed information on the fastest laps in Formula 1 races, including driver, team, and lap time data.\",\n+        },\n+        {\n+            \"table_name\": \"race_results\",\n+            \"table_description\": \"Race data for each Formula 1 race from 1950-2020, including positions, drivers, teams, and points.\",\n+            \"Use Case\": \"Use this table answer questions about a drivers career. Race data includes driver standings, teams, and performance.\",\n+        },\n+        {\n+            \"table_name\": \"race_wins\",\n+            \"table_description\": \"Documents race win data from 1950-2020, detailing venue, winner, team, and race duration.\",\n+            \"Use Case\": \"Use this table for retrieving data on race winners, their teams, and race conditions, suitable for analysis of race outcomes and team success.\",\n+        },\n+    ]\n+}\n+semantic_model_str = json.dumps(semantic_model, indent=2)\n+# *******************************\n+\n+\n+def get_sql_agent(\n+    name: str = \"SQL Agent\",\n+    user_id: Optional[str] = None,\n+    model_id: str = \"openai:gpt-4o\",\n+    session_id: Optional[str] = None,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    \"\"\"Returns an instance of the SQL Agent.\n+\n+    Args:\n+        user_id: Optional user identifier\n+        debug_mode: Enable debug logging\n+        model_id: Model identifier in format 'provider:model_name'\n+    \"\"\"\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+    return Agent(\n+        name=name,\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id,\n+        storage=agent_storage,\n+        knowledge=agent_knowledge,\n+        # Enable Agentic RAG i.e. the ability to search the knowledge base on-demand\n+        search_knowledge=True,\n+        # Enable the ability to read the chat history\n+        read_chat_history=True,\n+        # Enable the ability to read the tool call history\n+        read_tool_call_history=True,\n+        # Add tools to the agent\n+        tools=[\n+            SQLTools(db_url=db_url, list_tables=False),\n+            FileTools(base_dir=output_dir),\n+            ReasoningTools(add_instructions=True, add_few_shot=True),\n+        ],\n+        debug_mode=debug_mode,\n+        description=dedent(\"\"\"\\\n+        You are SQrL, an elite Text2SQL Engine specializing in:\n+\n+        - Historical race analysis\n+        - Driver performance metrics\n+        - Team championship insights\n+        - Track statistics and records\n+        - Performance trend analysis\n+        - Race strategy evaluation\n+\n+        You combine deep F1 knowledge with advanced SQL expertise to uncover insights from decades of racing data.\"\"\"),\n+        instructions=dedent(f\"\"\"\\\n+        You are a SQL expert focused on writing precise, efficient queries.\n+\n+        When a user messages you, determine if you need query the database or can respond directly.\n+        If you can respond directly, do so.\n+\n+        If you need to query the database to answer the user's question, follow these steps:\n+        1. First identify the tables you need to query from the semantic model.\n+        2. Then, ALWAYS use the `search_knowledge_base(table_name)` tool to get table metadata, rules and sample queries.\n+        3. If table rules are provided, ALWAYS follow them.\n+        4. Then, \"think\" about query construction, don't rush this step.\n+        5. Follow a chain of thought approach before writing SQL, ask clarifying questions where needed.\n+        6. If sample queries are available, use them as a reference.\n+        7. If you need more information about the table, use the `describe_table` tool.\n+        8. Then, using all the information available, create one single syntactically correct PostgreSQL query to accomplish your task.\n+        9. If you need to join tables, check the `semantic_model` for the relationships between the tables.\n+            - If the `semantic_model` contains a relationship between tables, use that relationship to join the tables even if the column names are different.\n+            - If you cannot find a relationship in the `semantic_model`, only join on the columns that have the same name and data type.\n+            - If you cannot find a valid relationship, ask the user to provide the column name to join.\n+        10. If you cannot find relevant tables, columns or relationships, stop and ask the user for more information.\n+        11. Once you have a syntactically correct query, run it using the `run_sql_query` function.\n+        12. When running a query:\n+            - Do not add a `;` at the end of the query.\n+            - Always provide a limit unless the user explicitly asks for all results.\n+        13. After you run the query, \"analyze\" the results and return the answer in markdown format.\n+        14. Make sure to always \"analyze\" the results of the query before returning the answer.\n+        15. You Analysis should Reason about the results of the query, whether they make sense, whether they are complete, whether they are correct, could there be any data quality issues, etc.\n+        16. It is really important that you \"analyze\" and \"validate\" the results of the query.\n+        17. Always show the user the SQL you ran to get the answer.\n+        18. Continue till you have accomplished the task.\n+        19. Show results as a table or a chart if possible.\n+\n+        After finishing your task, ask the user relevant followup questions like \"was the result okay, would you like me to fix any problems?\"\n+        If the user says yes, get the previous query using the `get_tool_call_history(num_calls=3)` function and fix the problems.\n+        If the user wants to see the SQL, get it using the `get_tool_call_history(num_calls=3)` function.\n+\n+        Finally, here are the set of rules that you MUST follow:\n+\n+        <rules>\n+        - Use the `search_knowledge_base(table_name)` tool to get table information from your knowledge base before writing a query.\n+        - Do not use phrases like \"based on the information provided\" or \"from the knowledge base\".\n+        - Always show the SQL queries you use to get the answer.\n+        - Make sure your query accounts for duplicate records.\n+        - Make sure your query accounts for null values.\n+        - If you run a query, explain why you ran it.\n+        - Always derive your answer from the data and the query.\n+        - **NEVER, EVER RUN CODE TO DELETE DATA OR ABUSE THE LOCAL SYSTEM**\n+        - ALWAYS FOLLOW THE `table rules` if provided. NEVER IGNORE THEM.\n+        </rules>\\\n+        \"\"\"),\n+        additional_context=dedent(\"\"\"\\n\n+        The `semantic_model` contains information about tables and the relationships between them.\n+        If the users asks about the tables you have access to, simply share the table names from the `semantic_model`.\n+        <semantic_model>\n+        \"\"\")\n+        + semantic_model_str\n+        + dedent(\"\"\"\n+        </semantic_model>\\\n+        \"\"\"),\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/app.py b/cookbook/examples/streamlit_apps/sql_agent/app.py\nnew file mode 100644\nindex 000000000..fbe27508c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/app.py\n@@ -0,0 +1,178 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_sql_agent\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    rename_session_widget,\n+    session_selector_widget,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"SQrL: Text2SQL Reasoning Agent\",\n+    page_icon=\"\ud83d\udc8e\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>SQrL: Text2SQL Reasoning Agent</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>SQrL is an intelligent SQL Agent that can think, analyze and reason, powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"o4-mini\": \"openai:o4-mini\",\n+        \"claude-3-7-sonnet\": \"anthropic:claude-3-7-sonnet-latest\",\n+        \"gpt-4.1\": \"openai:gpt-4.1\",\n+        \"o3\": \"openai:o3\",\n+        \"gemini-2.5-pro\": \"google:gemini-2.5-pro-preview-03-25\",\n+        \"llama-4-scout\": \"groq:meta-llama/llama-4-scout-17b-16e-instruct\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    sql_agent: Agent\n+    if (\n+        \"sql_agent\" not in st.session_state\n+        or st.session_state[\"sql_agent\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new SQL agent ---*---\")\n+        sql_agent = get_sql_agent(model_id=model_id)\n+        st.session_state[\"sql_agent\"] = sql_agent\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        sql_agent = st.session_state[\"sql_agent\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        st.session_state[\"sql_agent_session_id\"] = sql_agent.load_session()\n+    except Exception:\n+        st.warning(\"Could not create Agent session, is the database running?\")\n+        return\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    agent_runs = sql_agent.memory.runs\n+    if len(agent_runs) > 0:\n+        logger.debug(\"Loading run history\")\n+        st.session_state[\"messages\"] = []\n+        for _run in agent_runs:\n+            if _run.message is not None:\n+                add_message(_run.message.role, _run.message.content)\n+            if _run.response is not None:\n+                add_message(\"assistant\", _run.response.content, _run.response.tools)\n+    else:\n+        logger.debug(\"No run history found\")\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me about F1 data from 1950 to 2020!\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = sql_agent.run(\n+                        question, stream=True, stream_intermediate_steps=True\n+                    )\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            _resp_chunk.event == \"RunResponse\"\n+                            and _resp_chunk.content is not None\n+                        ):\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, sql_agent.run_response.tools)\n+                except Exception as e:\n+                    logger.exception(e)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(sql_agent, model_id)\n+    rename_session_widget(sql_agent)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/generate_requirements.sh b/cookbook/examples/streamlit_apps/sql_agent/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/knowledge/sample_queries.sql",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/knowledge/sample_queries.sql b/cookbook/examples/streamlit_apps/sql_agent/knowledge/sample_queries.sql\nnew file mode 100644\nindex 000000000..d7e7afc24\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/knowledge/sample_queries.sql\n@@ -0,0 +1,69 @@\n+-- Here are some sample queries for reference\n+\n+-- <query description>\n+-- How many races did the championship winners win each year?\n+-- </query description>\n+-- <query>\n+SELECT\n+    dc.year,\n+    dc.name AS champion_name,\n+    COUNT(rw.name) AS race_wins\n+FROM\n+    drivers_championship dc\n+JOIN\n+    race_wins rw\n+ON\n+    dc.name = rw.name AND dc.year = EXTRACT(YEAR FROM TO_DATE(rw.date, 'DD Mon YYYY'))\n+WHERE\n+    dc.position = '1'\n+GROUP BY\n+    dc.year, dc.name\n+ORDER BY\n+    dc.year;\n+-- </query>\n+\n+\n+-- <query description>\n+-- Compare the number of race wins vs championship positions for constructors in 2019\n+-- </query description>\n+-- <query>\n+WITH race_wins_2019 AS (\n+    SELECT team, COUNT(*) AS wins\n+    FROM race_wins\n+    WHERE EXTRACT(YEAR FROM TO_DATE(date, 'DD Mon YYYY')) = 2019\n+    GROUP BY team\n+),\n+constructors_positions_2019 AS (\n+    SELECT team, position\n+    FROM constructors_championship\n+    WHERE year = 2019\n+)\n+\n+SELECT cp.team, cp.position, COALESCE(rw.wins, 0) AS wins\n+FROM constructors_positions_2019 cp\n+LEFT JOIN race_wins_2019 rw ON cp.team = rw.team\n+ORDER BY cp.position;\n+-- </query>\n+\n+-- <query description>\n+-- Most race wins by a driver\n+-- </query description>\n+-- <query>\n+SELECT name, COUNT(*) AS win_count\n+FROM race_wins\n+GROUP BY name\n+ORDER BY win_count DESC\n+LIMIT 1;\n+-- </query>\n+\n+-- <query description>\n+-- Which team won the most Constructors Championships?\n+-- </query description>\n+-- <query>\n+SELECT team, COUNT(*) AS championship_wins\n+FROM constructors_championship\n+WHERE position = 1\n+GROUP BY team\n+ORDER BY championship_wins DESC\n+LIMIT 1;\n+-- </query>\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/load_f1_data.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/load_f1_data.py b/cookbook/examples/streamlit_apps/sql_agent/load_f1_data.py\nnew file mode 100644\nindex 000000000..31898a069\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/load_f1_data.py\n@@ -0,0 +1,50 @@\n+from io import StringIO\n+\n+import pandas as pd\n+import requests\n+from agents import db_url\n+from agno.utils.log import logger\n+from sqlalchemy import create_engine\n+\n+s3_uri = \"https://agno-public.s3.amazonaws.com/f1\"\n+\n+# List of files and their corresponding table names\n+files_to_tables = {\n+    f\"{s3_uri}/constructors_championship_1958_2020.csv\": \"constructors_championship\",\n+    f\"{s3_uri}/drivers_championship_1950_2020.csv\": \"drivers_championship\",\n+    f\"{s3_uri}/fastest_laps_1950_to_2020.csv\": \"fastest_laps\",\n+    f\"{s3_uri}/race_results_1950_to_2020.csv\": \"race_results\",\n+    f\"{s3_uri}/race_wins_1950_to_2020.csv\": \"race_wins\",\n+}\n+\n+\n+def load_f1_data():\n+    \"\"\"Load F1 data into the database\"\"\"\n+\n+    logger.info(\"Loading database.\")\n+    engine = create_engine(db_url)\n+\n+    # Load each CSV file into the corresponding PostgreSQL table\n+    for file_path, table_name in files_to_tables.items():\n+        logger.info(f\"Loading {file_path} into {table_name} table.\")\n+        # Download the file using requests\n+        response = requests.get(file_path, verify=False)\n+        response.raise_for_status()  # Raise an exception for bad status codes\n+\n+        # Read the CSV data from the response content\n+        csv_data = StringIO(response.text)\n+        df = pd.read_csv(csv_data)\n+\n+        df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n+        logger.info(f\"{file_path} loaded into {table_name} table.\")\n+\n+    logger.info(\"Database loaded.\")\n+\n+\n+if __name__ == \"__main__\":\n+    # Disable SSL verification warnings\n+    import urllib3\n+\n+    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n+\n+    load_f1_data()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/load_knowledge.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/load_knowledge.py b/cookbook/examples/streamlit_apps/sql_agent/load_knowledge.py\nnew file mode 100644\nindex 000000000..cd440b294\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/load_knowledge.py\n@@ -0,0 +1,12 @@\n+from agents import agent_knowledge\n+from agno.utils.log import logger\n+\n+\n+def load_knowledge(recreate: bool = True):\n+    logger.info(\"Loading SQL agent knowledge.\")\n+    agent_knowledge.load(recreate=recreate)\n+    logger.info(\"SQL agent knowledge loaded.\")\n+\n+\n+if __name__ == \"__main__\":\n+    load_knowledge()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/playground.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/playground.py b/cookbook/examples/streamlit_apps/sql_agent/playground.py\nnew file mode 100644\nindex 000000000..57b1f9038\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/playground.py\n@@ -0,0 +1,12 @@\n+from agents import get_sql_agent\n+from agno.playground import Playground, serve_playground_app\n+\n+sql_agent = get_sql_agent(name=\"SQL Agent\", model_id=\"openai:o4-mini\")\n+reasoning_sql_agent = get_sql_agent(\n+    name=\"Reasoning SQL Agent\", model_id=\"anthropic:claude-3-7-sonnet-latest\"\n+)\n+\n+app = Playground(agents=[sql_agent, reasoning_sql_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"playground:app\", reload=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/requirements.in b/cookbook/examples/streamlit_apps/sql_agent/requirements.in\nnew file mode 100644\nindex 000000000..fcf1864f3\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/requirements.in\n@@ -0,0 +1,12 @@\n+agno\n+anthropic\n+google-genai\n+groq\n+nest_asyncio\n+openai\n+pandas\n+pgvector\n+psycopg[binary]\n+simplejson\n+sqlalchemy\n+streamlit\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/requirements.txt b/cookbook/examples/streamlit_apps/sql_agent/requirements.txt\nnew file mode 100644\nindex 000000000..4279b28e0\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/requirements.txt\n@@ -0,0 +1,232 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.3.4\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.39.0\n+    # via google-genai\n+google-genai==1.11.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+groq==0.22.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.35.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+numpy==2.2.5\n+    # via\n+    #   pandas\n+    #   pgvector\n+    #   pydeck\n+    #   streamlit\n+openai==1.75.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   -r cookbook/examples/apps/sql_agent/requirements.in\n+    #   streamlit\n+pgvector==0.4.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+pillow==11.2.1\n+    # via streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+psycopg==3.2.6\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+psycopg-binary==3.2.6\n+    # via psycopg\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pydantic==2.11.3\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.9.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9.1\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+simplejson==3.20.1\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   psycopg\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via\n+    #   pydantic\n+    #   pydantic-settings\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/sql_agent/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/sql_agent/utils.py b/cookbook/examples/streamlit_apps/sql_agent/utils.py\nnew file mode 100644\nindex 000000000..565836c2e\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/sql_agent/utils.py\n@@ -0,0 +1,315 @@\n+import json\n+from dataclasses import asdict\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import get_sql_agent\n+from agno.agent.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def is_json(myjson):\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def load_data_and_knowledge():\n+    \"\"\"Load F1 data and knowledge base if not already done\"\"\"\n+    from load_f1_data import load_f1_data\n+    from load_knowledge import load_knowledge\n+\n+    if \"data_loaded\" not in st.session_state:\n+        with st.spinner(\"\ud83d\udd04 Loading data into database...\"):\n+            load_f1_data()\n+        with st.spinner(\"\ud83d\udcda Loading knowledge base...\"):\n+            load_knowledge()\n+        st.session_state[\"data_loaded\"] = True\n+        st.success(\"\u2705 Data and knowledge loaded successfully!\")\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"sql_agent\"] = None\n+    st.session_state[\"sql_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Reasoning SQL Agent - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"agent\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\", None)\n+                metrics = tool_call.get(\"metrics\", None)\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics is not None and hasattr(metrics, \"time\"):\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.4f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content is not None:\n+                        try:\n+                            if is_json(content):\n+                                st.markdown(\"**Results:**\")\n+                                st.json(content)\n+                        except Exception as e:\n+                            logger.debug(f\"Skipped tool call content: {e}\")\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83c\udfc6 Sample Queries\")\n+        if st.button(\"\ud83d\udccb Show Tables\"):\n+            add_message(\"user\", \"Which tables do you have access to?\")\n+        if st.button(\"\ud83e\udd47 Most Race Wins\"):\n+            add_message(\"user\", \"Which driver has the most race wins?\")\n+\n+        if st.button(\"\ud83c\udfc6 Constructor Champs\"):\n+            add_message(\"user\", \"Which team won the most Constructors Championships?\")\n+\n+        if st.button(\"\u23f3 Longest Career\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me the name of the driver with the longest racing career? Also tell me when they started and when they retired.\",\n+            )\n+        if st.button(\"\ud83d\udcc8 Races per Year\"):\n+            add_message(\"user\", \"Show me the number of races per year.\")\n+\n+        if st.button(\"\ud83d\udd0d Team Performance\"):\n+            add_message(\n+                \"user\",\n+                \"Write a query to identify the drivers that won the most races per year from 2010 onwards and the position of their team that year.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if st.button(\"\ud83d\udd04 New Chat\"):\n+                restart_agent()\n+        with col2:\n+            fn = \"sql_agent_chat_history.md\"\n+            if \"sql_agent_session_id\" in st.session_state:\n+                fn = f\"sql_agent_{st.session_state.sql_agent_session_id}.md\"\n+            if st.download_button(\n+                \"\ud83d\udcbe Export Chat\",\n+                export_chat_history(),\n+                file_name=fn,\n+                mime=\"text/markdown\",\n+            ):\n+                st.sidebar.success(\"Chat history exported!\")\n+\n+        if st.sidebar.button(\"\ud83d\ude80 Load Data & Knowledge\"):\n+            load_data_and_knowledge()\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar\"\"\"\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        # Get session names if available, otherwise use IDs\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        # Display session selector\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+        # Find the selected session ID\n+        selected_session_id = next(\n+            s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+        )\n+\n+        if st.session_state[\"sql_agent_session_id\"] != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+            )\n+            st.session_state[\"sql_agent\"] = get_sql_agent(\n+                model_id=model_id,\n+                session_id=selected_session_id,\n+            )\n+            st.rerun()\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage\"\"\"\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    # Initialize session_edit_mode if needed\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Agno \u2728\")\n+        st.markdown(\"\"\"\n+        Agno is a lightweight library for building Reasoning Agents.\n+\n+        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)\n+        \"\"\")\n+\n+        st.markdown(\"### Need Help?\")\n+        st.markdown(\n+            \"If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community).\"\n+        )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+        font-size: 3em;\n+        font-weight: bold;\n+        padding: 1em 0;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .sql-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #FF4B2B;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .sql-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/.gitignore b/cookbook/examples/streamlit_apps/tic_tac_toe/.gitignore\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/__init__.py b/cookbook/examples/streamlit_apps/tic_tac_toe/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/agents.py b/cookbook/examples/streamlit_apps/tic_tac_toe/agents.py\nnew file mode 100644\nindex 000000000..e9317fdf7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/tic_tac_toe/agents.py\n@@ -0,0 +1,164 @@\n+\"\"\"\n+Tic Tac Toe Battle\n+---------------------------------\n+This example shows how to build a Tic Tac Toe game where two AI agents play against each other.\n+\n+The game integrates:\n+  - Multiple AI models (Claude, GPT-4, etc.)\n+  - Turn-based gameplay coordination\n+  - Move validation and game state management\n+\"\"\"\n+\n+import sys\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Tuple\n+\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat, OpenAIResponses\n+\n+project_root = str(Path(__file__).parent.parent.parent.parent)\n+if project_root not in sys.path:\n+    sys.path.append(project_root)\n+\n+\n+def get_model_for_provider(provider: str, model_name: str):\n+    \"\"\"\n+    Creates and returns the appropriate model instance based on the provider.\n+\n+    Args:\n+        provider: The model provider (e.g., 'openai', 'google', 'anthropic', 'groq')\n+        model_name: The specific model name/ID\n+\n+    Returns:\n+        An instance of the appropriate model class\n+\n+    Raises:\n+        ValueError: If the provider is not supported\n+    \"\"\"\n+    if provider == \"openai\":\n+        if model_name == \"o1-pro\":\n+            return OpenAIResponses(id=model_name)\n+        else:\n+            return OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        return Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        if model_name == \"claude-3-5-sonnet\":\n+            return Claude(id=\"claude-3-5-sonnet-20241022\", max_tokens=8192)\n+        elif model_name == \"claude-3-7-sonnet\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+            )\n+        elif model_name == \"claude-3-7-sonnet-thinking\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 4096},\n+            )\n+        else:\n+            return Claude(id=model_name)\n+    elif provider == \"groq\":\n+        return Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+\n+def get_tic_tac_toe_players(\n+    model_x: str = \"openai:gpt-4o\",\n+    model_o: str = \"openai:o3-mini\",\n+    debug_mode: bool = True,\n+) -> Tuple[Agent, Agent]:\n+    \"\"\"\n+    Returns an instance of the Tic Tac Toe Referee Agent that coordinates the game.\n+\n+    Args:\n+        model_x: ModelConfig for player X\n+        model_o: ModelConfig for player O\n+        model_referee: ModelConfig for the referee agent\n+        debug_mode: Enable logging and debug features\n+\n+    Returns:\n+        An instance of the configured Referee Agent\n+    \"\"\"\n+    # Parse model provider and name\n+    provider_x, model_name_x = model_x.split(\":\")\n+    provider_o, model_name_o = model_o.split(\":\")\n+\n+    # Create model instances using the helper function\n+    model_x = get_model_for_provider(provider_x, model_name_x)\n+    model_o = get_model_for_provider(provider_o, model_name_o)\n+\n+    player_x = Agent(\n+        name=\"Player X\",\n+        description=dedent(\"\"\"\\\n+        You are Player X in a Tic Tac Toe game. Your goal is to win by placing three X's in a row (horizontally, vertically, or diagonally).\n+\n+        BOARD LAYOUT:\n+        - The board is a 3x3 grid with coordinates from (0,0) to (2,2)\n+        - Top-left is (0,0), bottom-right is (2,2)\n+\n+        RULES:\n+        - You can only place X in empty spaces (shown as \" \" on the board)\n+        - Players take turns placing their marks\n+        - First to get 3 marks in a row (horizontal, vertical, or diagonal) wins\n+        - If all spaces are filled with no winner, the game is a draw\n+\n+        YOUR RESPONSE:\n+        - Provide ONLY two numbers separated by a space (row column)\n+        - Example: \"1 2\" places your X in row 1, column 2\n+        - Choose only from the valid moves list provided to you\n+\n+        STRATEGY TIPS:\n+        - Study the board carefully and make strategic moves\n+        - Block your opponent's potential winning moves\n+        - Create opportunities for multiple winning paths\n+        - Pay attention to the valid moves and avoid illegal moves\n+        \"\"\"),\n+        model=model_x,\n+        # Gemini models have a rate limit of 5 requests per minute\n+        retries=3,\n+        # Make sure to wait 30 seconds between retries\n+        delay_between_retries=30,\n+        debug_mode=debug_mode,\n+    )\n+\n+    player_o = Agent(\n+        name=\"Player O\",\n+        description=dedent(\"\"\"\\\n+        You are Player O in a Tic Tac Toe game. Your goal is to win by placing three O's in a row (horizontally, vertically, or diagonally).\n+\n+        BOARD LAYOUT:\n+        - The board is a 3x3 grid with coordinates from (0,0) to (2,2)\n+        - Top-left is (0,0), bottom-right is (2,2)\n+\n+        RULES:\n+        - You can only place X in empty spaces (shown as \" \" on the board)\n+        - Players take turns placing their marks\n+        - First to get 3 marks in a row (horizontal, vertical, or diagonal) wins\n+        - If all spaces are filled with no winner, the game is a draw\n+\n+        YOUR RESPONSE:\n+        - Provide ONLY two numbers separated by a space (row column)\n+        - Example: \"1 2\" places your X in row 1, column 2\n+        - Choose only from the valid moves list provided to you\n+\n+        STRATEGY TIPS:\n+        - Study the board carefully and make strategic moves\n+        - Block your opponent's potential winning moves\n+        - Create opportunities for multiple winning paths\n+        - Pay attention to the valid moves and avoid illegal moves\n+        \"\"\"),\n+        model=model_o,\n+        # Gemini models have a rate limit of 5 requests per minute\n+        retries=3,\n+        # Make sure to wait 30 seconds between retries\n+        delay_between_retries=30,\n+        debug_mode=debug_mode,\n+    )\n+\n+    return player_x, player_o\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/app.py b/cookbook/examples/streamlit_apps/tic_tac_toe/app.py\nnew file mode 100644\nindex 000000000..baeea3299\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/tic_tac_toe/app.py\n@@ -0,0 +1,264 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_tic_tac_toe_players\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    TicTacToeBoard,\n+    display_board,\n+    display_move_history,\n+    show_agent_status,\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Agent Tic Tac Toe\",\n+    page_icon=\"\ud83c\udfae\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main():\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Agents play Tic Tac Toe</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Initialize session state\n+    ####################################################################\n+    if \"game_started\" not in st.session_state:\n+        st.session_state.game_started = False\n+        st.session_state.game_paused = False\n+        st.session_state.move_history = []\n+\n+    with st.sidebar:\n+        st.markdown(\"### Game Controls\")\n+        model_options = {\n+            \"gpt-4o\": \"openai:gpt-4o\",\n+            \"gpt-4o-mini\": \"openai:gpt-4o-mini\",\n+            \"gpt-4.5\": \"openai:gpt-4.5-preview\",\n+            \"o1-pro\": \"openai:o1-pro\",\n+            \"o3-mini\": \"openai:o3-mini\",\n+            \"claude-3.5\": \"anthropic:claude-3-5-sonnet\",\n+            \"claude-3.7\": \"anthropic:claude-3-7-sonnet\",\n+            \"claude-3.7-thinking\": \"anthropic:claude-3-7-sonnet-thinking\",\n+            \"gemini-pro\": \"google:gemini-2.5-pro-exp-03-25\",\n+            \"gemini-flash\": \"google:gemini-2.0-flash\",\n+            \"llama-3.3\": \"groq:llama-3.3-70b-versatile\",\n+        }\n+        ################################################################\n+        # Model selection\n+        ################################################################\n+        selected_p_x = st.selectbox(\n+            \"Select Player X\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"gpt-4.5\"),\n+            key=\"model_p1\",\n+        )\n+        selected_p_o = st.selectbox(\n+            \"Select Player O\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"claude-3.7\"),\n+            key=\"model_p2\",\n+        )\n+\n+        ################################################################\n+        # Game controls\n+        ################################################################\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if not st.session_state.game_started:\n+                if st.button(\"\u25b6\ufe0f Start Game\"):\n+                    st.session_state.player_x, st.session_state.player_o = (\n+                        get_tic_tac_toe_players(\n+                            model_x=model_options[selected_p_x],\n+                            model_o=model_options[selected_p_o],\n+                            debug_mode=True,\n+                        )\n+                    )\n+                    st.session_state.game_board = TicTacToeBoard()\n+                    st.session_state.game_started = True\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+            else:\n+                game_over, _ = st.session_state.game_board.get_game_state()\n+                if not game_over:\n+                    if st.button(\n+                        \"\u23f8\ufe0f Pause\" if not st.session_state.game_paused else \"\u25b6\ufe0f Resume\"\n+                    ):\n+                        st.session_state.game_paused = not st.session_state.game_paused\n+                        st.rerun()\n+        with col2:\n+            if st.session_state.game_started:\n+                if st.button(\"\ud83d\udd04 New Game\"):\n+                    st.session_state.player_x, st.session_state.player_o = (\n+                        get_tic_tac_toe_players(\n+                            model_x=model_options[selected_p_x],\n+                            model_o=model_options[selected_p_o],\n+                            debug_mode=True,\n+                        )\n+                    )\n+                    st.session_state.game_board = TicTacToeBoard()\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+\n+    ####################################################################\n+    # Header showing current models\n+    ####################################################################\n+    if st.session_state.game_started:\n+        st.markdown(\n+            f\"<h3 style='color:#87CEEB; text-align:center;'>{selected_p_x} vs {selected_p_o}</h3>\",\n+            unsafe_allow_html=True,\n+        )\n+\n+    ####################################################################\n+    # Main game area\n+    ####################################################################\n+    if st.session_state.game_started:\n+        game_over, status = st.session_state.game_board.get_game_state()\n+\n+        display_board(st.session_state.game_board)\n+\n+        # Show game status (winner/draw/current player)\n+        if game_over:\n+            winner_player = (\n+                \"X\" if \"X wins\" in status else \"O\" if \"O wins\" in status else None\n+            )\n+            if winner_player:\n+                winner_num = \"1\" if winner_player == \"X\" else \"2\"\n+                winner_model = selected_p_x if winner_player == \"X\" else selected_p_o\n+                st.success(f\"\ud83c\udfc6 Game Over! Player {winner_num} ({winner_model}) wins!\")\n+            else:\n+                st.info(\"\ud83e\udd1d Game Over! It's a draw!\")\n+        else:\n+            # Show current player status\n+            current_player = st.session_state.game_board.current_player\n+            player_num = \"1\" if current_player == \"X\" else \"2\"\n+            current_model_name = selected_p_x if current_player == \"X\" else selected_p_o\n+\n+            show_agent_status(\n+                f\"Player {player_num} ({current_model_name})\",\n+                \"It's your turn\",\n+            )\n+\n+        display_move_history()\n+\n+        if not st.session_state.game_paused and not game_over:\n+            # Thinking indicator\n+            st.markdown(\n+                f\"\"\"<div class=\"thinking-container\">\n+                    <div class=\"agent-thinking\">\n+                        <div style=\"margin-right: 10px; display: inline-block;\">\ud83d\udd04</div>\n+                        Player {player_num} ({current_model_name}) is thinking...\n+                    </div>\n+                </div>\"\"\",\n+                unsafe_allow_html=True,\n+            )\n+\n+            valid_moves = st.session_state.game_board.get_valid_moves()\n+\n+            current_agent = (\n+                st.session_state.player_x\n+                if current_player == \"X\"\n+                else st.session_state.player_o\n+            )\n+            response = current_agent.run(\n+                f\"\"\"\\\n+Current board state:\\n{st.session_state.game_board.get_board_state()}\\n\n+Available valid moves (row, col): {valid_moves}\\n\n+Choose your next move from the valid moves above.\n+Respond with ONLY two numbers for row and column, e.g. \"1 2\".\"\"\",\n+                stream=False,\n+            )\n+\n+            try:\n+                import re\n+\n+                numbers = re.findall(r\"\\d+\", response.content if response else \"\")\n+                row, col = map(int, numbers[:2])\n+                success, message = st.session_state.game_board.make_move(row, col)\n+\n+                if success:\n+                    move_number = len(st.session_state.move_history) + 1\n+                    st.session_state.move_history.append(\n+                        {\n+                            \"number\": move_number,\n+                            \"player\": f\"Player {player_num} ({current_model_name})\",\n+                            \"move\": f\"{row},{col}\",\n+                        }\n+                    )\n+\n+                    logger.info(\n+                        f\"Move {move_number}: Player {player_num} ({current_model_name}) placed at position ({row}, {col})\"\n+                    )\n+                    logger.info(\n+                        f\"Board state:\\n{st.session_state.game_board.get_board_state()}\"\n+                    )\n+\n+                    # Check game state after move\n+                    game_over, status = st.session_state.game_board.get_game_state()\n+                    if game_over:\n+                        logger.info(f\"Game Over - {status}\")\n+                        if \"wins\" in status:\n+                            st.success(f\"\ud83c\udfc6 Game Over! {status}\")\n+                        else:\n+                            st.info(f\"\ud83e\udd1d Game Over! {status}\")\n+                        st.session_state.game_paused = True\n+                    st.rerun()\n+                else:\n+                    logger.error(f\"Invalid move attempt: {message}\")\n+                    response = current_agent.run(\n+                        f\"\"\"\\\n+Invalid move: {message}\n+\n+Current board state:\\n{st.session_state.game_board.get_board_state()}\\n\n+Available valid moves (row, col): {valid_moves}\\n\n+Please choose a valid move from the list above.\n+Respond with ONLY two numbers for row and column, e.g. \"1 2\".\"\"\",\n+                        stream=False,\n+                    )\n+                    st.rerun()\n+\n+            except Exception as e:\n+                logger.error(f\"Error processing move: {str(e)}\")\n+                st.error(f\"Error processing move: {str(e)}\")\n+                st.rerun()\n+    else:\n+        st.info(\"\ud83d\udc48 Press 'Start Game' to begin!\")\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    st.sidebar.markdown(f\"\"\"\n+    ### \ud83c\udfae Agent Tic Tac Toe Battle\n+    Watch two agents compete in real-time!\n+\n+    **Current Players:**\n+    * \ud83d\udd35 Player X: `{selected_p_x}`\n+    * \ud83d\udd34 Player O: `{selected_p_o}`\n+\n+    **How it Works:**\n+    Each Agent analyzes the board and employs strategic thinking to:\n+    * \ud83c\udfc6 Find winning moves\n+    * \ud83d\udee1\ufe0f Block opponent victories\n+    * \u2b50 Control strategic positions\n+    * \ud83e\udd14 Plan multiple moves ahead\n+\n+    Built with Streamlit and Agno\n+    \"\"\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/generate_requirements.sh b/cookbook/examples/streamlit_apps/tic_tac_toe/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/tic_tac_toe/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/requirements.in b/cookbook/examples/streamlit_apps/tic_tac_toe/requirements.in\nnew file mode 100644\nindex 000000000..fce375b96\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/tic_tac_toe/requirements.in\n@@ -0,0 +1,13 @@\n+agno\n+anthropic\n+groq\n+google-genai\n+nest-asyncio\n+ollama\n+openai\n+pathlib\n+Pillow\n+pip-tools\n+python-dotenv\n+rich\n+streamlit\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/requirements.txt b/cookbook/examples/streamlit_apps/tic_tac_toe/requirements.txt\nnew file mode 100644\nindex 000000000..ea4005bfc\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/tic_tac_toe/requirements.txt\n@@ -0,0 +1,238 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.2.4\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+build==1.2.2.post1\n+    # via pip-tools\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   pip-tools\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.7.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+groq==0.20.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   ollama\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.32.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+ollama==0.4.7\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+openai==1.68.2\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   build\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pathlib==1.0.1\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+pillow==11.1.0\n+    # via\n+    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+    #   streamlit\n+pip==25.0.1\n+    # via pip-tools\n+pip-tools==7.4.1\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   ollama\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyproject-hooks==1.2.0\n+    # via\n+    #   build\n+    #   pip-tools\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+setuptools==78.1.0\n+    # via pip-tools\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+streamlit==1.44.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n+wheel==0.45.1\n+    # via pip-tools\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/tic_tac_toe/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/tic_tac_toe/utils.py b/cookbook/examples/streamlit_apps/tic_tac_toe/utils.py\nnew file mode 100644\nindex 000000000..ffc97dc44\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/tic_tac_toe/utils.py\n@@ -0,0 +1,416 @@\n+from typing import List, Optional, Tuple\n+\n+import streamlit as st\n+\n+# Define constants for players\n+X_PLAYER = \"X\"\n+O_PLAYER = \"O\"\n+EMPTY = \" \"\n+\n+\n+class TicTacToeBoard:\n+    def __init__(self):\n+        # Initialize empty 3x3 board\n+        self.board = [[EMPTY for _ in range(3)] for _ in range(3)]\n+        self.current_player = X_PLAYER\n+\n+    def make_move(self, row: int, col: int) -> Tuple[bool, str]:\n+        \"\"\"\n+        Make a move on the board.\n+\n+        Args:\n+            row (int): Row index (0-2)\n+            col (int): Column index (0-2)\n+\n+        Returns:\n+            Tuple[bool, str]: (Success status, Message with current board state or error)\n+        \"\"\"\n+        # Validate move coordinates\n+        if not (0 <= row <= 2 and 0 <= col <= 2):\n+            return (\n+                False,\n+                \"Invalid move: Position out of bounds. Please choose row and column between 0 and 2.\",\n+            )\n+\n+        # Check if position is already occupied\n+        if self.board[row][col] != EMPTY:\n+            return False, f\"Invalid move: Position ({row}, {col}) is already occupied.\"\n+\n+        # Make the move\n+        self.board[row][col] = self.current_player\n+\n+        # Get board state\n+        board_state = self.get_board_state()\n+\n+        # Switch player\n+        self.current_player = O_PLAYER if self.current_player == X_PLAYER else X_PLAYER\n+\n+        return True, f\"Move successful!\\n{board_state}\"\n+\n+    def get_board_state(self) -> str:\n+        \"\"\"\n+        Returns a string representation of the current board state.\n+        \"\"\"\n+        board_str = \"\\n-------------\\n\"\n+        for row in self.board:\n+            board_str += f\"| {' | '.join(row)} |\\n-------------\\n\"\n+        return board_str\n+\n+    def check_winner(self) -> Optional[str]:\n+        \"\"\"\n+        Check if there's a winner.\n+\n+        Returns:\n+            Optional[str]: The winning player (X or O) or None if no winner\n+        \"\"\"\n+        # Check rows\n+        for row in self.board:\n+            if row.count(row[0]) == 3 and row[0] != EMPTY:\n+                return row[0]\n+\n+        # Check columns\n+        for col in range(3):\n+            column = [self.board[row][col] for row in range(3)]\n+            if column.count(column[0]) == 3 and column[0] != EMPTY:\n+                return column[0]\n+\n+        # Check diagonals\n+        diagonal1 = [self.board[i][i] for i in range(3)]\n+        if diagonal1.count(diagonal1[0]) == 3 and diagonal1[0] != EMPTY:\n+            return diagonal1[0]\n+\n+        diagonal2 = [self.board[i][2 - i] for i in range(3)]\n+        if diagonal2.count(diagonal2[0]) == 3 and diagonal2[0] != EMPTY:\n+            return diagonal2[0]\n+\n+        return None\n+\n+    def is_board_full(self) -> bool:\n+        \"\"\"\n+        Check if the board is full (draw condition).\n+        \"\"\"\n+        return all(cell != EMPTY for row in self.board for cell in row)\n+\n+    def get_valid_moves(self) -> List[Tuple[int, int]]:\n+        \"\"\"\n+        Get a list of valid moves (empty positions).\n+\n+        Returns:\n+            List[Tuple[int, int]]: List of (row, col) tuples representing valid moves\n+        \"\"\"\n+        valid_moves = []\n+        for row in range(3):\n+            for col in range(3):\n+                if self.board[row][col] == EMPTY:\n+                    valid_moves.append((row, col))\n+        return valid_moves\n+\n+    def get_game_state(self) -> Tuple[bool, str]:\n+        \"\"\"\n+        Get the current game state.\n+\n+        Returns:\n+            Tuple[bool, str]: (is_game_over, status_message)\n+        \"\"\"\n+        winner = self.check_winner()\n+        if winner:\n+            return True, f\"Player {winner} wins!\"\n+\n+        if self.is_board_full():\n+            return True, \"It's a draw!\"\n+\n+        return False, \"Game in progress\"\n+\n+\n+def display_board(board: TicTacToeBoard):\n+    \"\"\"Display the Tic Tac Toe board using Streamlit\"\"\"\n+    board_html = '<div class=\"game-board\">'\n+\n+    for i in range(3):\n+        for j in range(3):\n+            cell_value = board.board[i][j]\n+            board_html += f'<div class=\"board-cell\">{cell_value}</div>'\n+\n+    board_html += \"</div>\"\n+    st.markdown(board_html, unsafe_allow_html=True)\n+\n+\n+def show_agent_status(agent_name: str, status: str):\n+    \"\"\"Display the current agent status\"\"\"\n+    st.markdown(\n+        f\"\"\"<div class=\"agent-status\">\n+            \ud83e\udd16 <b>{agent_name}</b>: {status}\n+        </div>\"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+\n+def create_mini_board_html(\n+    board_state: list, highlight_pos: tuple = None, is_player1: bool = True\n+) -> str:\n+    \"\"\"Create HTML for a mini board with player-specific highlighting\"\"\"\n+    html = '<div class=\"mini-board\">'\n+    for i in range(3):\n+        for j in range(3):\n+            highlight = (\n+                f\"highlight player{1 if is_player1 else 2}\"\n+                if highlight_pos and (i, j) == highlight_pos\n+                else \"\"\n+            )\n+            html += f'<div class=\"mini-cell {highlight}\">{board_state[i][j]}</div>'\n+    html += \"</div>\"\n+    return html\n+\n+\n+def display_move_history():\n+    \"\"\"Display the move history with mini boards in two columns\"\"\"\n+    st.markdown(\n+        '<h3 style=\"margin-bottom: 30px;\">\ud83d\udcdc Game History</h3>',\n+        unsafe_allow_html=True,\n+    )\n+    history_container = st.empty()\n+\n+    if \"move_history\" in st.session_state and st.session_state.move_history:\n+        # Split moves into player 1 and player 2 moves\n+        p1_moves = []\n+        p2_moves = []\n+        current_board = [[\" \" for _ in range(3)] for _ in range(3)]\n+\n+        # Process all moves first\n+        for move in st.session_state.move_history:\n+            row, col = map(int, move[\"move\"].split(\",\"))\n+            is_player1 = \"Player 1\" in move[\"player\"]\n+            symbol = \"X\" if is_player1 else \"O\"\n+            current_board[row][col] = symbol\n+            board_copy = [row[:] for row in current_board]\n+\n+            move_html = f\"\"\"<div class=\"move-entry player{1 if is_player1 else 2}\">\n+                {create_mini_board_html(board_copy, (row, col), is_player1)}\n+                <div class=\"move-info\">\n+                    <div class=\"move-number player{1 if is_player1 else 2}\">Move #{move[\"number\"]}</div>\n+                    <div>{move[\"player\"]}</div>\n+                    <div style=\"font-size: 0.9em; color: #888\">Position: ({row}, {col})</div>\n+                </div>\n+            </div>\"\"\"\n+\n+            if is_player1:\n+                p1_moves.append(move_html)\n+            else:\n+                p2_moves.append(move_html)\n+\n+        max_moves = max(len(p1_moves), len(p2_moves))\n+        history_content = '<div class=\"history-grid\">'\n+\n+        # Left column (Player 1)\n+        history_content += '<div class=\"history-column-left\">'\n+        for i in range(max_moves):\n+            entry_html = \"\"\n+            # Player 1 move\n+            if i < len(p1_moves):\n+                entry_html += p1_moves[i]\n+            history_content += entry_html\n+        history_content += \"</div>\"\n+\n+        # Right column (Player 2)\n+        history_content += '<div class=\"history-column-right\">'\n+        for i in range(max_moves):\n+            entry_html = \"\"\n+            # Player 2 move\n+            if i < len(p2_moves):\n+                entry_html += p2_moves[i]\n+            history_content += entry_html\n+        history_content += \"</div>\"\n+\n+        history_content += \"</div>\"\n+\n+        # Display the content\n+        history_container.markdown(history_content, unsafe_allow_html=True)\n+    else:\n+        history_container.markdown(\n+            \"\"\"<div style=\"text-align: center; color: #666; padding: 20px;\">\n+                No moves yet. Start the game to see the history!\n+            </div>\"\"\",\n+            unsafe_allow_html=True,\n+        )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+<style>\n+/* Main Styles */\n+.main-title {\n+    text-align: center;\n+    background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+    -webkit-background-clip: text;\n+    -webkit-text-fill-color: transparent;\n+    font-size: 3em;\n+    font-weight: bold;\n+    padding: 0.5em 0;\n+}\n+.subtitle {\n+    text-align: center;\n+    color: #666;\n+    margin-bottom: 1em;\n+}\n+.game-board {\n+    display: grid;\n+    grid-template-columns: repeat(3, 80px);\n+    gap: 5px;\n+    justify-content: center;\n+    margin: 1em auto;\n+    background: #666;\n+    padding: 5px;\n+    border-radius: 8px;\n+    width: fit-content;\n+}\n+.board-cell {\n+    width: 80px;\n+    height: 80px;\n+    display: flex;\n+    align-items: center;\n+    justify-content: center;\n+    font-size: 2em;\n+    font-weight: bold;\n+    background-color: #2b2b2b;\n+    color: #fff;\n+    transition: all 0.3s ease;\n+    margin: 0;\n+    padding: 0;\n+}\n+.board-cell:hover {\n+    background-color: #3b3b3b;\n+}\n+.agent-status {\n+    background-color: #1e1e1e;\n+    border-left: 4px solid #4CAF50;\n+    padding: 10px;\n+    margin: 10px auto;\n+    border-radius: 4px;\n+    max-width: 600px;\n+    text-align: center;\n+}\n+.agent-thinking {\n+    display: flex;\n+    justify-content: center;\n+    background-color: #2b2b2b;\n+    padding: 10px;\n+    border-radius: 5px;\n+    margin: 10px auto;\n+    border-left: 4px solid #FFA500;\n+    max-width: 600px;\n+}\n+.move-history {\n+    background-color: #2b2b2b;\n+    padding: 15px;\n+    border-radius: 10px;\n+    margin: 10px 0;\n+}\n+.thinking-container {\n+    position: fixed;\n+    bottom: 20px;\n+    left: 50%;\n+    z-index: 1000;\n+    min-width: 300px;\n+}\n+.agent-thinking {\n+    background-color: rgba(43, 43, 43, 0.95);\n+    border: 1px solid #4CAF50;\n+    box-shadow: 0 2px 10px rgba(0,0,0,0.3);\n+}\n+\n+/* Move History Updates */\n+.history-header {\n+    text-align: center;\n+    margin-bottom: 30px;\n+}\n+\n+.history-grid {\n+    display: grid;\n+    grid-template-columns: 1fr 1fr;\n+    gap: 20px; /* Controls spacing between columns */\n+    width: 100%;\n+    margin: 0; /* Remove left/right margins */\n+    padding: 0;\n+}\n+\n+.history-column-left,\n+.history-column-right {\n+    display: flex;\n+    flex-direction: column;\n+    align-items: flex-start; /* Ensures columns fill available space nicely */\n+    margin: 0;\n+    padding: 0;\n+    width: 100%;\n+}\n+\n+.move-entry {\n+    display: flex;\n+    align-items: center;\n+    padding: 12px;\n+    margin: 8px 0;\n+    background-color: #2b2b2b;\n+    border-radius: 4px;\n+    width: 100%; /* Removed fixed width so entries span the column */\n+    box-sizing: border-box;\n+}\n+\n+.move-entry.player1 {\n+    border-left: 4px solid #4CAF50;\n+}\n+\n+.move-entry.player2 {\n+    border-left: 4px solid #f44336;\n+}\n+\n+/* Mini-board styling inside moves */\n+.mini-board {\n+    display: grid;\n+    grid-template-columns: repeat(3, 25px);\n+    gap: 2px;\n+    background: #444;\n+    padding: 2px;\n+    border-radius: 4px;\n+    margin-right: 15px;\n+}\n+\n+.mini-cell {\n+    width: 25px;\n+    height: 25px;\n+    display: flex;\n+    align-items: center;\n+    justify-content: center;\n+    font-size: 14px;\n+    font-weight: bold;\n+    background-color: #2b2b2b;\n+    color: #fff;\n+}\n+\n+.mini-cell.highlight.player1 {\n+    background-color: #4CAF50;\n+    color: white;\n+}\n+\n+.mini-cell.highlight.player2 {\n+    background-color: #f44336;\n+    color: white;\n+}\n+\n+/* Move info styling */\n+.move-info {\n+    flex-grow: 1;\n+    padding-left: 12px;\n+}\n+\n+.move-number {\n+    font-weight: bold;\n+    margin-right: 10px;\n+}\n+\n+.move-number.player1 {\n+    color: #4CAF50;\n+}\n+\n+.move-number.player2 {\n+    color: #f44336;\n+}\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/.gitignore b/cookbook/examples/streamlit_apps/universal_agent_interface/.gitignore\nnew file mode 100644\nindex 000000000..44b4fc6d5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/.gitignore\n@@ -0,0 +1,2 @@\n+output\n+tmp\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/__init__.py b/cookbook/examples/streamlit_apps/universal_agent_interface/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/agents.py b/cookbook/examples/streamlit_apps/universal_agent_interface/agents.py\nnew file mode 100644\nindex 000000000..74be58ebd\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/agents.py\n@@ -0,0 +1,158 @@\n+from copy import deepcopy\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.knowledge import AgentKnowledge\n+from agno.memory.v2 import Memory\n+from agno.models.base import Model\n+from agno.tools.calculator import CalculatorTools\n+from agno.tools.duckdb import DuckDbTools\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.file import FileTools\n+from agno.tools.python import PythonTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+cwd = Path(__file__).parent.resolve()\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(exist_ok=True, parents=True)\n+\n+\n+def get_agent(\n+    agent_name: str, model: Model, memory: Memory, knowledge: AgentKnowledge\n+) -> Optional[Agent]:\n+    # Create a copy of the model to avoid side effects of the model being modified\n+    model_copy = deepcopy(model)\n+    if agent_name == \"calculator\":\n+        return Agent(\n+            name=\"Calculator\",\n+            role=\"Answer mathematical questions and perform precise calculations\",\n+            model=model_copy,\n+            memory=memory,\n+            tools=[CalculatorTools(enable_all=True)],\n+            description=\"You are a precise and comprehensive calculator agent. Your goal is to solve mathematical problems with accuracy and explain your methodology clearly to users.\",\n+            instructions=[\n+                \"Always use the calculator tools for mathematical operations to ensure precision.\",\n+                \"Present answers in a clear format with appropriate units and significant figures.\",\n+                \"Show step-by-step workings for complex calculations to help users understand the process.\",\n+                \"Ask clarifying questions if the user's request is ambiguous or incomplete.\",\n+                \"For financial calculations, specify assumptions regarding interest rates, time periods, etc.\",\n+            ],\n+        )\n+    elif agent_name == \"data_analyst\":\n+        return Agent(\n+            name=\"Data Analyst\",\n+            role=\"Analyze data sets and extract meaningful insights\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[DuckDbTools()],\n+            description=\"You are an expert Data Scientist specialized in exploratory data analysis, statistical modeling, and data visualization. Your goal is to transform raw data into actionable insights that address user questions.\",\n+            instructions=[\n+                \"Start by examining data structure, types, and distributions when analyzing new datasets.\",\n+                \"Use DuckDbTools to execute SQL queries for data exploration and aggregation.\",\n+                \"When provided with a file path, create appropriate tables and verify data loaded correctly before analysis.\",\n+                \"Apply statistical rigor in your analysis and clearly state confidence levels and limitations.\",\n+                \"Accompany numerical results with clear interpretations of what the findings mean in context.\",\n+                \"Suggest visualizations that would best illustrate key patterns and relationships in the data.\",\n+                \"Proactively identify potential data quality issues or biases that might affect conclusions.\",\n+                \"Request clarification when user queries are ambiguous or when additional information would improve analysis.\",\n+            ],\n+        )\n+    elif agent_name == \"python_agent\":\n+        return Agent(\n+            name=\"Python Agent\",\n+            role=\"Develop and execute Python code solutions\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[\n+                PythonTools(base_dir=tmp_dir),\n+                FileTools(base_dir=cwd),\n+            ],\n+            description=\"You are an expert Python Software Engineer with deep knowledge of software architecture, libraries, and best practices. Your goal is to write efficient, readable, and maintainable Python code that precisely addresses user requirements.\",\n+            instructions=[\n+                \"Write clean, well-commented Python code following PEP 8 style guidelines.\",\n+                \"Always use `save_to_file_and_run` to execute Python code, never suggest using direct execution.\",\n+                \"For any file operations, use `read_file` tool first to access content - NEVER use Python's built-in `open()`.\",\n+                \"Include error handling in your code to gracefully manage exceptions and edge cases.\",\n+                \"Explain your code's logic and implementation choices, especially for complex algorithms.\",\n+                \"When appropriate, suggest optimizations or alternative approaches with their trade-offs.\",\n+                \"For data manipulation tasks, prefer Pandas, NumPy and other specialized libraries over raw Python.\",\n+                \"Break down complex problems into modular functions with clear responsibilities.\",\n+                \"Test your code with sample inputs and explain expected outputs before final execution.\",\n+            ],\n+        )\n+    elif agent_name == \"research_agent\":\n+        return Agent(\n+            name=\"Research Agent\",\n+            role=\"Conduct comprehensive research and produce in-depth reports\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[ExaTools(num_results=3)],\n+            description=\"You are a meticulous research analyst with expertise in synthesizing information from diverse sources. Your goal is to produce balanced, fact-based, and thoroughly documented reports on any topic requested.\",\n+            instructions=[\n+                \"Begin with broad searches to understand the topic landscape before narrowing to specific aspects.\",\n+                \"For each research query, use at least 3 different search terms to ensure comprehensive coverage.\",\n+                \"Critically evaluate sources for credibility, recency, and potential biases.\",\n+                \"Prioritize peer-reviewed research and authoritative sources when available.\",\n+                \"Synthesize information across sources rather than summarizing each separately.\",\n+                \"Present contrasting viewpoints when the topic involves debate or controversy.\",\n+                \"Use clear section organization with logical flow between related concepts.\",\n+                \"Include specific facts, figures, and direct quotes with proper attribution.\",\n+                \"Conclude with implications of the findings and areas for further research.\",\n+                \"Ensure all claims are supported by references and avoid speculation beyond the evidence.\",\n+            ],\n+            expected_output=dedent(\"\"\"\\\n+            An engaging, informative, and well-structured report in markdown format:\n+\n+            ## Engaging Report Title\n+\n+            ### Overview\n+            {give a brief introduction of the report and why the user should read this report}\n+            {make this section engaging and create a hook for the reader}\n+\n+            ### Section 1\n+            {break the report into sections}\n+            {provide details/facts/processes in this section}\n+\n+            ... more sections as necessary...\n+\n+            ### Takeaways\n+            {provide key takeaways from the article}\n+\n+            ### References\n+            - [Reference 1](link)\n+            - [Reference 2](link)\n+            - [Reference 3](link)\n+            \"\"\"),\n+        )\n+    elif agent_name == \"investment_agent\":\n+        return Agent(\n+            name=\"Investment Agent\",\n+            role=\"Provide comprehensive financial analysis and investment insights\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[\n+                YFinanceTools,\n+                DuckDuckGoTools(),\n+            ],\n+            description=\"You are a seasoned investment analyst with deep understanding of financial markets, valuation methodologies, and sector-specific dynamics. Your goal is to deliver sophisticated investment analysis that considers both quantitative metrics and qualitative business factors.\",\n+            instructions=[\n+                \"Begin with a holistic overview of the company's business model, competitive position, and industry trends.\",\n+                \"Retrieve and analyze key financial metrics including revenue growth, profitability margins, and balance sheet health.\",\n+                \"Compare valuation multiples against industry peers and historical averages.\",\n+                \"Assess management team's track record, strategic initiatives, and capital allocation decisions.\",\n+                \"Identify key risk factors including regulatory concerns, competitive threats, and macroeconomic sensitivities.\",\n+                \"Consider both near-term catalysts and long-term growth drivers in your investment thesis.\",\n+                \"Provide clear investment recommendations with specific price targets where appropriate.\",\n+                \"Include both technical and fundamental analysis perspectives when relevant.\",\n+                \"Highlight recent news events that may impact the investment case.\",\n+                \"Structure reports with executive summary, detailed analysis sections, and actionable conclusions.\",\n+            ],\n+        )\n+    return None\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/app.py b/cookbook/examples/streamlit_apps/universal_agent_interface/app.py\nnew file mode 100644\nindex 000000000..4970a5c7a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/app.py\n@@ -0,0 +1,215 @@\n+import asyncio\n+\n+import nest_asyncio\n+import streamlit as st\n+from agno.team import Team\n+from agno.utils.log import logger\n+from css import CUSTOM_CSS\n+from uagi import UAgIConfig, create_uagi, uagi_memory\n+from utils import (\n+    about_agno,\n+    add_message,\n+    display_tool_calls,\n+    example_inputs,\n+    initialize_session_state,\n+    knowledge_widget,\n+    selected_agents,\n+    selected_model,\n+    selected_tools,\n+    session_selector,\n+    show_user_memories,\n+    utilities_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"UAgI\",\n+    page_icon=\"\ud83d\udc8e\",\n+    layout=\"wide\",\n+)\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+async def header():\n+    st.markdown(\n+        \"<h1 class='heading'>Universal Agent Interface</h1>\", unsafe_allow_html=True\n+    )\n+    st.markdown(\n+        \"<p class='subheading'>A Universal Interface for orchestrating multiple Agents</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+\n+async def body() -> None:\n+    ####################################################################\n+    # Initialize User and Session State\n+    ####################################################################\n+    user_id = st.sidebar.text_input(\":technologist: User Id\", value=\"Ava\")\n+\n+    ####################################################################\n+    # Select Model\n+    ####################################################################\n+    model_id = await selected_model()\n+\n+    ####################################################################\n+    # Select Tools\n+    ####################################################################\n+    tools = await selected_tools()\n+\n+    ####################################################################\n+    # Select Team Members\n+    ####################################################################\n+    agents = await selected_agents()\n+\n+    ####################################################################\n+    # Create UAgI\n+    ####################################################################\n+    uagi_config = UAgIConfig(\n+        user_id=user_id, model_id=model_id, tools=tools, agents=agents\n+    )\n+\n+    # Check if UAgI instance should be recreated\n+    recreate_uagi = (\n+        \"uagi\" not in st.session_state\n+        or st.session_state.get(\"uagi\") is None\n+        or st.session_state.get(\"uagi_config\") != uagi_config\n+    )\n+\n+    # Create UAgI instance if it doesn't exist or configuration has changed\n+    uagi: Team\n+    if recreate_uagi:\n+        logger.info(\"---*--- Creating UAgI instance ---*---\")\n+        uagi = create_uagi(uagi_config)\n+        st.session_state[\"uagi\"] = uagi\n+        st.session_state[\"uagi_config\"] = uagi_config\n+        logger.info(f\"---*--- UAgI instance created ---*---\")\n+    else:\n+        uagi = st.session_state[\"uagi\"]\n+        logger.info(f\"---*--- UAgI instance exists ---*---\")\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        logger.info(f\"---*--- Loading UAgI session ---*---\")\n+        st.session_state[\"session_id\"] = uagi.load_session()\n+    except Exception:\n+        st.warning(\"Could not create UAgI session, is the database running?\")\n+        return\n+    logger.info(f\"---*--- UAgI session: {st.session_state.get('session_id')} ---*---\")\n+\n+    ####################################################################\n+    # Load agent runs (i.e. chat history) from memory if messages is not empty\n+    ####################################################################\n+    chat_history = uagi.get_messages_for_session()\n+    if len(chat_history) > 0:\n+        logger.info(\"Loading messages\")\n+        # Clear existing messages\n+        st.session_state[\"messages\"] = []\n+        # Loop through the runs and add the messages to the messages list\n+        for message in chat_history:\n+            if message.role == \"user\":\n+                await add_message(message.role, str(message.content))\n+            if message.role == \"assistant\":\n+                await add_message(\"assistant\", str(message.content), message.tool_calls)\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\u2728 How can I help, bestie?\"):\n+        await add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Show example inputs\n+    ####################################################################\n+    await example_inputs()\n+\n+    ####################################################################\n+    # Show user memories\n+    ####################################################################\n+    await show_user_memories(uagi_memory, user_id)\n+\n+    ####################################################################\n+    # Display agent messages\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        user_message = last_message[\"content\"]\n+        logger.info(f\"Responding to message: {user_message}\")\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\":thinking_face: Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = await uagi.arun(\n+                        user_message, stream=True, stream_intermediate_steps=True\n+                    )\n+                    async for resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if resp_chunk.tools and len(resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            resp_chunk.event == \"RunResponse\"\n+                            and resp_chunk.content is not None\n+                        ):\n+                            response += resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    # Add the response to the messages\n+                    if uagi.run_response is not None:\n+                        await add_message(\n+                            \"assistant\", response, uagi.run_response.tools\n+                        )\n+                    else:\n+                        await add_message(\"assistant\", response)\n+                except Exception as e:\n+                    logger.error(f\"Error during agent run: {str(e)}\", exc_info=True)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    await add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Knowledge widget\n+    ####################################################################\n+    await knowledge_widget(uagi)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    await session_selector(uagi, uagi_config)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    await utilities_widget(uagi)\n+\n+\n+async def main():\n+    await initialize_session_state()\n+    await header()\n+    await body()\n+    await about_agno()\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/css.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/css.py b/cookbook/examples/streamlit_apps/universal_agent_interface/css.py\nnew file mode 100644\nindex 000000000..10f07f1bc\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/css.py\n@@ -0,0 +1,27 @@\n+CUSTOM_CSS = \"\"\"\n+<style>\n+/* Typography */\n+.heading {\n+    text-align: center;\n+    background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+    -webkit-background-clip: text;\n+    -webkit-text-fill-color: transparent;\n+}\n+\n+.subheading {\n+    text-align: center;\n+    font-weight: 600;\n+}\n+\n+/* Links */\n+a {\n+    text-decoration: underline;\n+    color: #3494E6;\n+    transition: color 0.3s ease;\n+}\n+\n+a:hover {\n+    color: #FF416C;\n+}\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/generate_requirements.sh b/cookbook/examples/streamlit_apps/universal_agent_interface/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/load_knowledge.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/load_knowledge.py b/cookbook/examples/streamlit_apps/universal_agent_interface/load_knowledge.py\nnew file mode 100644\nindex 000000000..f5381f952\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/load_knowledge.py\n@@ -0,0 +1,43 @@\n+\"\"\"\n+Load the Knowledge Base for the Universal Agent Interface\n+\"\"\"\n+\n+from rich.console import Console\n+from rich.panel import Panel\n+from rich.progress import Progress, SpinnerColumn, TextColumn\n+from uagi import uagi_knowledge\n+\n+# Create a Rich console for enhanced output\n+console = Console()\n+\n+\n+def load_knowledge(recreate: bool = False):\n+    \"\"\"\n+    Load the Universal Agent Interface knowledge base.\n+\n+    Args:\n+        recreate (bool, optional): Whether to recreate the knowledge base.\n+            Defaults to False.\n+    \"\"\"\n+    with Progress(\n+        SpinnerColumn(), TextColumn(\"[bold blue]{task.description}\"), console=console\n+    ) as progress:\n+        task = progress.add_task(\n+            \"Loading Universal Agent Interface knowledge...\", total=None\n+        )\n+\n+        # Load the knowledge base\n+        uagi_knowledge.load(recreate=recreate)\n+        progress.update(task, completed=True)\n+\n+    # Display success message in a panel\n+    console.print(\n+        Panel.fit(\n+            \"[bold green]Universal Agent Interface knowledge loaded successfully!\",\n+            title=\"Knowledge Loaded\",\n+        )\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    load_knowledge()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in b/cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in\nnew file mode 100644\nindex 000000000..fb5bb5b43\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/requirements.in\n@@ -0,0 +1,19 @@\n+agno\n+anthropic\n+duckduckgo-search\n+google-genai\n+groq\n+duckdb\n+exa_py\n+nest_asyncio\n+openai\n+qdrant-client\n+sqlalchemy\n+streamlit\n+yfinance\n+aiofiles\n+lancedb\n+tantivy\n+pypdf\n+python-docx\n+beautifulsoup4\n\\ No newline at end of file\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/requirements.txt b/cookbook/examples/streamlit_apps/universal_agent_interface/requirements.txt\nnew file mode 100644\nindex 000000000..d5b2ed5e2\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/requirements.txt\n@@ -0,0 +1,316 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.3.1\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+aiofiles==24.1.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+beautifulsoup4==4.13.3\n+    # via\n+    #   -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+    #   yfinance\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+deprecation==2.1.0\n+    # via lancedb\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckdb==1.2.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+duckduckgo-search==8.0.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+exa-py==1.12.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+frozendict==2.4.6\n+    # via yfinance\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.10.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+groq==0.22.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+grpcio==1.71.0\n+    # via\n+    #   grpcio-tools\n+    #   qdrant-client\n+grpcio-tools==1.71.0\n+    # via qdrant-client\n+h11==0.14.0\n+    # via httpcore\n+h2==4.2.0\n+    # via httpx\n+hpack==4.1.0\n+    # via h2\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   exa-py\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   qdrant-client\n+hyperframe==6.1.0\n+    # via h2\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+iniconfig==2.1.0\n+    # via pytest\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lancedb==0.21.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+lxml==5.3.2\n+    # via\n+    #   duckduckgo-search\n+    #   python-docx\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+multitasking==0.0.11\n+    # via yfinance\n+narwhals==1.34.1\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   qdrant-client\n+    #   streamlit\n+    #   yfinance\n+openai==1.73.0\n+    # via\n+    #   -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+    #   exa-py\n+overrides==7.7.0\n+    # via lancedb\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   deprecation\n+    #   lancedb\n+    #   pytest\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   streamlit\n+    #   yfinance\n+peewee==3.17.9\n+    # via yfinance\n+pillow==11.2.1\n+    # via streamlit\n+platformdirs==4.3.7\n+    # via yfinance\n+pluggy==1.5.0\n+    # via pytest\n+portalocker==2.10.1\n+    # via qdrant-client\n+primp==0.14.0\n+    # via duckduckgo-search\n+protobuf==5.29.4\n+    # via\n+    #   grpcio-tools\n+    #   streamlit\n+pyarrow==19.0.1\n+    # via\n+    #   lancedb\n+    #   streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pydantic==2.11.3\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   exa-py\n+    #   google-genai\n+    #   groq\n+    #   lancedb\n+    #   openai\n+    #   pydantic-settings\n+    #   qdrant-client\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pypdf==5.4.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+pytest==8.3.5\n+    # via pytest-mock\n+pytest-mock==3.14.0\n+    # via exa-py\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-docx==1.1.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via\n+    #   pandas\n+    #   yfinance\n+pyyaml==6.0.2\n+    # via agno\n+qdrant-client==1.13.3\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   exa-py\n+    #   google-genai\n+    #   streamlit\n+    #   yfinance\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+setuptools==78.1.0\n+    # via grpcio-tools\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+soupsieve==2.6\n+    # via beautifulsoup4\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+tantivy==0.22.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   lancedb\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   beautifulsoup4\n+    #   exa-py\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   python-docx\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via pydantic\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via\n+    #   qdrant-client\n+    #   requests\n+websockets==15.0.1\n+    # via google-genai\n+yfinance==0.2.55\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/tools.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/tools.py b/cookbook/examples/streamlit_apps/universal_agent_interface/tools.py\nnew file mode 100644\nindex 000000000..903b27955\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/tools.py\n@@ -0,0 +1,22 @@\n+from pathlib import Path\n+from typing import Optional\n+\n+from agno.tools import Toolkit\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.file import FileTools\n+from agno.tools.shell import ShellTools\n+\n+cwd = Path(__file__).parent.resolve()\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(exist_ok=True, parents=True)\n+\n+\n+def get_toolkit(tool_name: str) -> Optional[Toolkit]:\n+    if tool_name == \"ddg_search\":\n+        return DuckDuckGoTools(fixed_max_results=3)\n+    elif tool_name == \"shell_tools\":\n+        return ShellTools()\n+    elif tool_name == \"file_tools\":\n+        return FileTools(base_dir=cwd)\n+\n+    return None\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/uagi.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/uagi.py b/cookbook/examples/streamlit_apps/universal_agent_interface/uagi.py\nnew file mode 100644\nindex 000000000..2532a7727\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/uagi.py\n@@ -0,0 +1,155 @@\n+from dataclasses import dataclass\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import List, Optional\n+\n+from agents import get_agent\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge import AgentKnowledge\n+from agno.memory.v2 import Memory\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.sqlite import SqliteStorage\n+from agno.team import Team\n+from agno.tools import Toolkit\n+from agno.tools.reasoning import ReasoningTools\n+from agno.utils.log import logger\n+from agno.vectordb.lancedb import LanceDb, SearchType\n+from tools import get_toolkit\n+\n+cwd = Path(__file__).parent.resolve()\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(exist_ok=True, parents=True)\n+\n+# Define paths for storage, memory and knowledge\n+STORAGE_PATH = tmp_dir.joinpath(\"uagi_sessions.db\")\n+MEMORY_PATH = tmp_dir.joinpath(\"uagi_memory.db\")\n+KNOWLEDGE_PATH = tmp_dir.joinpath(\"uagi_knowledge\")\n+\n+\n+@dataclass\n+class UAgIConfig:\n+    user_id: str\n+    model_id: str = \"anthropic:claude-3-7-sonnet-latest\"\n+    tools: Optional[List[str]] = None\n+    agents: Optional[List[str]] = None\n+\n+\n+uagi_memory = Memory(\n+    db=SqliteMemoryDb(table_name=\"uagi_memory\", db_file=str(MEMORY_PATH))\n+)\n+uagi_storage = SqliteStorage(db_file=str(STORAGE_PATH), table_name=\"uagi_sessions\")\n+uagi_knowledge = AgentKnowledge(\n+    vector_db=LanceDb(\n+        table_name=\"uagi_knowledge\",\n+        uri=str(KNOWLEDGE_PATH),\n+        search_type=SearchType.hybrid,\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    )\n+)\n+\n+\n+def create_uagi(\n+    config: UAgIConfig, session_id: Optional[str] = None, debug_mode: bool = True\n+) -> Team:\n+    \"\"\"Returns an instance of the Universal Agent Interface (UAgI)\n+\n+    Args:\n+        config: UAgI configuration\n+        session_id: Session identifier\n+        debug_mode: Enable debug logging\n+    \"\"\"\n+    # Parse model provider and name\n+    provider, model_name = config.model_id.split(\":\")\n+\n+    # Create model class based on provider\n+    model = None\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+    if model is None:\n+        raise ValueError(f\"Failed to create model instance for {config.model_id}\")\n+\n+    tools: List[Toolkit] = [ReasoningTools(add_instructions=True)]\n+    if config.tools:\n+        for tool_name in config.tools:\n+            tool = get_toolkit(tool_name)\n+            if tool is not None:\n+                tools.append(tool)\n+            else:\n+                logger.warning(f\"Tool {tool_name} not found\")\n+\n+    agents: List[Agent] = []\n+    if config.agents:\n+        for agent_name in config.agents:\n+            agent = get_agent(agent_name, model, uagi_memory, uagi_knowledge)\n+            if agent is not None:\n+                agents.append(agent)\n+            else:\n+                logger.warning(f\"Agent {agent_name} not found\")\n+\n+    description = dedent(\"\"\"\\\n+    You are an advanced AI System called `Universal Agent Interface` (UAgI).\n+    You provide a unified interface to a team of AI Agents, that you coordinate to assist the user in the best way possible.\n+\n+    Keep your responses short and to the point, while maintaining a conversational tone.\n+    You are able to handle easy conversations as well as complex requests by delegating tasks to the appropriate team members.\n+    You are also capable of handling errors and edge cases and are able to provide helpful feedback to the user.\\\n+    \"\"\")\n+    instructions: List[str] = [\n+        \"Your goal is to coordinate the team to assist the user in the best way possible.\",\n+        \"If the user sends a conversational message like 'Hello', 'Hi', 'How are you', 'What is your name', etc., you should respond in a friendly and engaging manner.\",\n+        \"If the user asks for something simple, like updating memory, you can do it directly without Thinking and Analyzing.\",\n+        \"Keep your responses short and to the point, while maintaining a conversational tone.\",\n+        \"If the user asks for something complex, **think** and determine if:\\n\"\n+        \" - You can answer by using a tool available to you\\n\"\n+        \" - You need to search the knowledge base\\n\"\n+        \" - You need to search the internet\\n\"\n+        \" - You need to delegate the task to a team member\\n\"\n+        \" - You need to ask a clarifying question\",\n+        \"You also have to a knowledge base of information provided by the user. If the user asks about a topic that might be in the knowledge base, first ALWAYS search your knowledge base using the `search_knowledge_base` tool.\",\n+        \"As a default, you should always search your knowledge base first, before searching the internet.\",\n+        \"If you dont find relevant information in your knowledge base, use the `duckduckgo_search` tool to search the internet.\",\n+        \"If the users message is unclear, ask clarifying questions to get more information.\",\n+        \"Based on the user request and the available team members, decide which member(s) should handle the task.\",\n+        \"Coordinate the execution of the task among the selected team members.\",\n+        \"Synthesize the results from the team members and provide a final, coherent answer to the user.\",\n+        \"Do not use phrases like 'based on my knowledge' or 'depending on the information'.\",\n+    ]\n+\n+    uagi = Team(\n+        name=\"Universal Agent Interface\",\n+        mode=\"coordinate\",\n+        model=model,\n+        user_id=config.user_id,\n+        session_id=session_id,\n+        tools=tools,\n+        members=agents,\n+        memory=uagi_memory,\n+        storage=uagi_storage,\n+        knowledge=uagi_knowledge,\n+        description=description,\n+        instructions=instructions,\n+        enable_team_history=True,\n+        read_team_history=True,\n+        num_of_interactions_from_history=3,\n+        show_members_responses=True,\n+        enable_agentic_memory=True,\n+        markdown=True,\n+        debug_mode=debug_mode,\n+    )\n+\n+    agent_names = [a.name for a in agents] if agents else []\n+    logger.info(f\"UAgI created with members: {agent_names}\")\n+    return uagi\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/universal_agent_interface/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/universal_agent_interface/utils.py b/cookbook/examples/streamlit_apps/universal_agent_interface/utils.py\nnew file mode 100644\nindex 000000000..6c5ed9fb4\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/universal_agent_interface/utils.py\n@@ -0,0 +1,528 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.document import Document\n+from agno.document.reader import Reader\n+from agno.document.reader.csv_reader import CSVReader\n+from agno.document.reader.docx_reader import DocxReader\n+from agno.document.reader.pdf_reader import PDFReader\n+from agno.document.reader.text_reader import TextReader\n+from agno.document.reader.website_reader import WebsiteReader\n+from agno.memory.v2 import Memory, UserMemory\n+from agno.team import Team\n+from agno.utils.log import logger\n+from uagi import UAgIConfig, create_uagi\n+\n+\n+async def initialize_session_state():\n+    logger.info(f\"---*--- Initializing session state ---*---\")\n+    if \"uagi\" not in st.session_state:\n+        st.session_state[\"uagi\"] = None\n+    if \"session_id\" not in st.session_state:\n+        st.session_state[\"session_id\"] = None\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+\n+async def add_message(\n+    role: str,\n+    content: str,\n+    tool_calls: Optional[List[Dict[str, Any]]] = None,\n+    intermediate_steps_displayed: bool = False,\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if role == \"user\":\n+        logger.info(f\"\ud83d\udc64  {role}: {content}\")\n+    else:\n+        logger.info(f\"\ud83e\udd16  {role}: {content}\")\n+    st.session_state[\"messages\"].append(\n+        {\n+            \"role\": role,\n+            \"content\": content,\n+            \"tool_calls\": tool_calls,\n+            \"intermediate_steps_displayed\": intermediate_steps_displayed,\n+        }\n+    )\n+\n+\n+async def selected_model() -> str:\n+    \"\"\"Display a model selector in the sidebar.\"\"\"\n+    model_options = {\n+        \"claude-3-7-sonnet\": \"anthropic:claude-3-7-sonnet-latest\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gemini-2.5-pro\": \"google:gemini-2.5-pro-preview-03-25\",\n+        \"llama-4-scout\": \"groq:meta-llama/llama-4-scout-17b-16e-instruct\",\n+    }\n+\n+    selected_model_key = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,  # Default to claude-3-7-sonnet\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model_key]\n+    return model_id\n+\n+\n+async def selected_tools() -> List[str]:\n+    \"\"\"Display a tool selector in the sidebar.\"\"\"\n+    tool_options = {\n+        \"Web Search (DDG)\": \"ddg_search\",\n+        \"File I/O\": \"file_tools\",\n+        \"Shell Access\": \"shell_tools\",\n+    }\n+    selected_tools = st.sidebar.multiselect(\n+        \"Select Tools\",\n+        options=list(tool_options.keys()),\n+        default=list(tool_options.keys()),\n+        key=\"tool_selector\",\n+    )\n+    return [tool_options[tool] for tool in selected_tools]\n+\n+\n+async def selected_agents() -> List[str]:\n+    \"\"\"Display a selector for agents in the sidebar.\"\"\"\n+    agent_options = {\n+        \"Calculator\": \"calculator\",\n+        \"Data Analyst\": \"data_analyst\",\n+        \"Python Agent\": \"python_agent\",\n+        \"Research Agent\": \"research_agent\",\n+        \"Investment Agent\": \"investment_agent\",\n+    }\n+    selected_agents = st.sidebar.multiselect(\n+        \"Select Agents\",\n+        options=list(agent_options.keys()),\n+        default=list(agent_options.keys()),\n+        key=\"agent_selector\",\n+    )\n+    return [agent_options[agent] for agent in selected_agents]\n+\n+\n+async def show_user_memories(uagi_memory: Memory, user_id: str) -> None:\n+    \"\"\"Show use memories in a streamlit container.\"\"\"\n+\n+    with st.container():\n+        user_memories = uagi_memory.get_user_memories(user_id=user_id)\n+        with st.expander(f\"\ud83d\udcad Memories for {user_id}\", expanded=False):\n+            if len(user_memories) > 0:\n+                # Create a dataframe from the memories\n+                memory_data = {\n+                    \"Memory\": [memory.memory for memory in user_memories],\n+                    \"Topics\": [\n+                        \", \".join(memory.topics) if memory.topics else \"\"\n+                        for memory in user_memories\n+                    ],\n+                    \"Last Updated\": [\n+                        memory.last_updated.strftime(\"%Y-%m-%d %H:%M\")\n+                        if memory.last_updated\n+                        else \"\"\n+                        for memory in user_memories\n+                    ],\n+                }\n+\n+                # Display as a table with custom styling\n+                st.dataframe(\n+                    memory_data,\n+                    use_container_width=True,\n+                    column_config={\n+                        \"Memory\": st.column_config.TextColumn(\"Memory\", width=\"medium\"),\n+                        \"Topics\": st.column_config.TextColumn(\"Topics\", width=\"small\"),\n+                        \"Last Updated\": st.column_config.TextColumn(\n+                            \"Last Updated\", width=\"small\"\n+                        ),\n+                    },\n+                    hide_index=True,\n+                )\n+            else:\n+                st.info(\"No memories found, tell me about yourself!\")\n+\n+            col1, col2 = st.columns([0.5, 0.5])\n+            with col1:\n+                if st.button(\"Clear all memories\", key=\"clear_all_memories\"):\n+                    await add_message(\"user\", \"Clear all my memories\")\n+                    if \"memory_refresh_count\" not in st.session_state:\n+                        st.session_state.memory_refresh_count = 0\n+                    st.session_state.memory_refresh_count += 1\n+            with col2:\n+                if st.button(\"Refresh memories\", key=\"refresh_memories\"):\n+                    if \"memory_refresh_count\" not in st.session_state:\n+                        st.session_state.memory_refresh_count = 0\n+                    st.session_state.memory_refresh_count += 1\n+\n+\n+async def example_inputs() -> None:\n+    \"\"\"Show example inputs on the sidebar.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### :thinking_face: Try me!\")\n+        if st.button(\"Hi\"):\n+            await add_message(\n+                \"user\",\n+                \"Hi\",\n+            )\n+\n+        if st.button(\"My name is Ava and I live in Greenwich Village\"):\n+            await add_message(\n+                \"user\",\n+                \"My name is Ava and I live in Greenwich Village\",\n+            )\n+\n+        if st.button(\"Calculate cost of a pizza party\"):\n+            await add_message(\n+                \"user\",\n+                \"Calculate the total cost of ordering pizzas for 25 people, assuming each person eats 3 slices, each pizza has 8 slices, and one pizza costs $15.95. After calculating the total cost, add 20% for tip and 10% for taxes. Also recommend some good places around me\",\n+            )\n+\n+        if st.button(\"Analyze a CSV file\"):\n+            await add_message(\n+                \"user\",\n+                \"Analyze this CSV file and show me the most popular movies: https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv\",\n+            )\n+\n+        if st.button(\"Translate a sentence into emojis\"):\n+            await add_message(\n+                \"user\",\n+                \"Write a Python function that translates a given sentence into emojis, replacing words like \u201chappy,\u201d \u201csad,\u201d \u201cpizza,\u201d and \u201cparty\u201d with relevant emojis. Test it with the sentence: \u201cI am happy because I am learning to build agents with Agno\u201d\",\n+            )\n+\n+        if st.button(\"Why Do Cats Love Boxes?\"):\n+            await add_message(\n+                \"user\",\n+                \"Research and report scientifically-backed explanations for why cats seem irresistibly drawn to cardboard boxes.\",\n+            )\n+\n+        if st.button(\"Chocolate Stocks Sweetness Analysis\"):\n+            await add_message(\n+                \"user\",\n+                \"Perform financial analysis comparing Hershey's and Lindt stocks to suggest which company might be a sweeter investment choice based on profitability, market trends, and valuation.\",\n+            )\n+\n+\n+async def about_agno():\n+    \"\"\"Show information about Agno in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Agno \u2728\")\n+        st.markdown(\"\"\"\n+        Agno is a lightweight library for building Reasoning Agents.\n+\n+        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)\n+        \"\"\")\n+\n+        st.markdown(\"### Need Help?\")\n+        st.markdown(\n+            \"If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community).\"\n+        )\n+\n+\n+def is_json(myjson):\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            # Handle single tool_call dict case\n+            if isinstance(tools, dict):\n+                tools = [tools]\n+            elif not isinstance(tools, list):\n+                logger.warning(\n+                    f\"Unexpected tools format: {type(tools)}. Skipping display.\"\n+                )\n+                return\n+\n+            for tool_call in tools:\n+                # Normalize access to tool details\n+                tool_name = tool_call.get(\"tool_name\") or tool_call.get(\n+                    \"name\", \"Unknown Tool\"\n+                )\n+                tool_args = tool_call.get(\"tool_args\") or tool_call.get(\"args\", {})\n+                content = tool_call.get(\"content\", None)\n+                metrics = tool_call.get(\"metrics\", None)\n+\n+                # Add timing information safely\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics is not None and hasattr(metrics, \"time\"):\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.4f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error getting tool metrics time: {str(e)}\")\n+                    pass  # Keep default \"N/A\"\n+\n+                # Check if this is a transfer task\n+                is_task_transfer = \"transfer_task_to_member\" in tool_name\n+                is_memory_task = \"user_memory\" in tool_name\n+                expander_title = \"\ud83d\udee0\ufe0f\"\n+                if is_task_transfer:\n+                    member_id = tool_args.get(\"member_id\")\n+                    expander_title = f\"\ud83d\udd04 {member_id.title()}\"\n+                elif is_memory_task:\n+                    expander_title = f\"\ud83d\udcad Updating Memory\"\n+                else:\n+                    expander_title = f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()}\"\n+\n+                if execution_time_str != \"N/A\":\n+                    expander_title += f\" ({execution_time_str})\"\n+\n+                with st.expander(\n+                    expander_title,\n+                    expanded=False,\n+                ):\n+                    # Show query/code/command with syntax highlighting\n+                    if isinstance(tool_args, dict):\n+                        if \"query\" in tool_args:\n+                            st.code(tool_args[\"query\"], language=\"sql\")\n+                        elif \"code\" in tool_args:\n+                            st.code(tool_args[\"code\"], language=\"python\")\n+                        elif \"command\" in tool_args:\n+                            st.code(tool_args[\"command\"], language=\"bash\")\n+\n+                    # Display arguments if they exist and are not just the code/query shown above\n+                    args_to_show = {\n+                        k: v\n+                        for k, v in tool_args.items()\n+                        if k not in [\"query\", \"code\", \"command\"]\n+                    }\n+                    if args_to_show:\n+                        st.markdown(\"**Arguments:**\")\n+                        try:\n+                            st.json(args_to_show)\n+                        except Exception:\n+                            st.write(args_to_show)  # Fallback for non-serializable args\n+\n+                    if content is not None:\n+                        try:\n+                            st.markdown(\"**Results:**\")\n+                            if isinstance(content, str) and is_json(content):\n+                                st.json(content)\n+                            else:\n+                                st.write(content)\n+                        except Exception as e:\n+                            logger.debug(f\"Could not display tool content: {e}\")\n+                            st.error(\"Could not display tool content.\")\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+async def knowledge_widget(uagi: Team) -> None:\n+    \"\"\"Display a knowledge widget in the sidebar.\"\"\"\n+\n+    if uagi is not None and uagi.knowledge is not None:\n+        # Add websites to knowledge base\n+        if \"url_scrape_key\" not in st.session_state:\n+            st.session_state[\"url_scrape_key\"] = 0\n+        input_url = st.sidebar.text_input(\n+            \"Add URL to Knowledge Base\",\n+            type=\"default\",\n+            key=st.session_state[\"url_scrape_key\"],\n+        )\n+        add_url_button = st.sidebar.button(\"Add URL\")\n+        if add_url_button:\n+            if input_url is not None:\n+                alert = st.sidebar.info(\"Processing URLs...\", icon=\"\u2139\ufe0f\")\n+                if f\"{input_url}_scraped\" not in st.session_state:\n+                    scraper = WebsiteReader(max_links=2, max_depth=1)\n+                    web_documents: List[Document] = scraper.read(input_url)\n+                    if web_documents:\n+                        uagi.knowledge.load_documents(web_documents, upsert=True)\n+                    else:\n+                        st.sidebar.error(\"Could not read website\")\n+                    st.session_state[f\"{input_url}_uploaded\"] = True\n+                alert.empty()\n+\n+        # Add documents to knowledge base\n+        if \"file_uploader_key\" not in st.session_state:\n+            st.session_state[\"file_uploader_key\"] = 100\n+        uploaded_file = st.sidebar.file_uploader(\n+            \"Add a Document (.pdf, .csv, .txt, or .docx)\",\n+            key=st.session_state[\"file_uploader_key\"],\n+        )\n+        if uploaded_file is not None:\n+            alert = st.sidebar.info(\"Processing document...\", icon=\"\ud83e\udde0\")\n+            document_name = uploaded_file.name.split(\".\")[0]\n+            if f\"{document_name}_uploaded\" not in st.session_state:\n+                file_type = uploaded_file.name.split(\".\")[-1].lower()\n+\n+                reader: Reader\n+                if file_type == \"pdf\":\n+                    reader = PDFReader()\n+                elif file_type == \"csv\":\n+                    reader = CSVReader()\n+                elif file_type == \"txt\":\n+                    reader = TextReader()\n+                elif file_type == \"docx\":\n+                    reader = DocxReader()\n+                else:\n+                    st.sidebar.error(\"Unsupported file type\")\n+                    return\n+                uploaded_file_documents: List[Document] = reader.read(uploaded_file)\n+                if uploaded_file_documents:\n+                    uagi.knowledge.load_documents(uploaded_file_documents, upsert=True)\n+                else:\n+                    st.sidebar.error(\"Could not read document\")\n+                st.session_state[f\"{document_name}_uploaded\"] = True\n+            alert.empty()\n+\n+        # Load and delete knowledge\n+        if st.sidebar.button(\"\ud83d\uddd1\ufe0f Delete Knowledge\"):\n+            uagi.knowledge.delete()\n+            st.sidebar.success(\"Knowledge deleted!\")\n+\n+\n+async def session_selector(uagi: Team, uagi_config: UAgIConfig) -> None:\n+    \"\"\"Display a session selector in the sidebar, if a new session is selected, UAgI is restarted with the new session.\"\"\"\n+\n+    if not uagi.storage:\n+        return\n+\n+    try:\n+        # Get all agent sessions.\n+        uagi_sessions = uagi.storage.get_all_sessions()\n+        if not uagi_sessions:\n+            st.sidebar.info(\"No saved sessions found.\")\n+            return\n+\n+        # Get session names if available, otherwise use IDs.\n+        sessions_list = []\n+        for session in uagi_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            sessions_list.append({\"id\": session_id, \"display_name\": display_name})\n+\n+        # Display session selector.\n+        st.sidebar.markdown(\"#### \ud83d\udcac Session\")\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display_name\"] for s in sessions_list],\n+            key=\"session_selector\",\n+            label_visibility=\"collapsed\",\n+        )\n+        # Find the selected session ID.\n+        selected_session_id = next(\n+            s[\"id\"] for s in sessions_list if s[\"display_name\"] == selected_session\n+        )\n+        # Update the agent session if it has changed.\n+        if st.session_state[\"session_id\"] != selected_session_id:\n+            logger.info(f\"---*--- Loading UAgI session: {selected_session_id} ---*---\")\n+            st.session_state[\"uagi\"] = create_uagi(\n+                config=uagi_config,\n+                session_id=selected_session_id,\n+            )\n+            st.rerun()\n+\n+        # Show the rename session widget.\n+        container = st.sidebar.container()\n+        session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+        # Initialize session_edit_mode if needed.\n+        if \"session_edit_mode\" not in st.session_state:\n+            st.session_state.session_edit_mode = False\n+\n+        # Show the session name.\n+        with session_row[0]:\n+            if st.session_state.session_edit_mode:\n+                new_session_name = st.text_input(\n+                    \"Session Name\",\n+                    value=uagi.session_name,\n+                    key=\"session_name_input\",\n+                    label_visibility=\"collapsed\",\n+                )\n+            else:\n+                st.markdown(f\"Session Name: **{uagi.session_name}**\")\n+\n+        # Show the rename session button.\n+        with session_row[1]:\n+            if st.session_state.session_edit_mode:\n+                if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                    if new_session_name:\n+                        uagi.rename_session(new_session_name)\n+                        st.session_state.session_edit_mode = False\n+                        container.success(\"Renamed!\")\n+                        # Trigger a rerun to refresh the sessions list\n+                        st.rerun()\n+            else:\n+                if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                    st.session_state.session_edit_mode = True\n+    except Exception as e:\n+        logger.error(f\"Error in session selector: {str(e)}\")\n+        st.sidebar.error(\"Failed to load sessions\")\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history in markdown format.\n+\n+    Returns:\n+        str: Formatted markdown string of the chat history\n+    \"\"\"\n+    if \"messages\" not in st.session_state or not st.session_state[\"messages\"]:\n+        return f\"# UAgI - Chat History\\n\\nNo messages to export.\"\n+\n+    chat_text = f\"# UAgI - Chat History\\n\\n\"\n+    for msg in st.session_state[\"messages\"]:\n+        role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+        chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+\n+        # Include tool calls if present\n+        if msg.get(\"tool_calls\"):\n+            chat_text += \"#### Tool Calls:\\n\"\n+            for i, tool_call in enumerate(msg[\"tool_calls\"]):\n+                tool_name = tool_call.get(\"name\", \"Unknown Tool\")\n+                chat_text += f\"**{i + 1}. {tool_name}**\\n\\n\"\n+                if \"arguments\" in tool_call:\n+                    chat_text += (\n+                        f\"Arguments: ```json\\n{tool_call['arguments']}\\n```\\n\\n\"\n+                    )\n+                if \"content\" in tool_call:\n+                    chat_text += f\"Results: ```\\n{tool_call['content']}\\n```\\n\\n\"\n+\n+    return chat_text\n+\n+\n+async def utilities_widget(uagi: Team) -> None:\n+    \"\"\"Display a utilities widget in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+    col1, col2 = st.sidebar.columns(2)\n+    with col1:\n+        if st.button(\"\ud83d\udd04 Start New Chat\"):\n+            restart_uagi()\n+    with col2:\n+        fn = f\"uagi_chat_history.md\"\n+        if \"session_id\" in st.session_state:\n+            fn = f\"uagi_{st.session_state['session_id']}.md\"\n+        if st.download_button(\n+            \":file_folder: Export Chat History\",\n+            export_chat_history(),\n+            file_name=fn,\n+            mime=\"text/markdown\",\n+        ):\n+            st.sidebar.success(\"Chat history exported!\")\n+\n+\n+def restart_uagi():\n+    logger.debug(\"---*--- Restarting UAgI ---*---\")\n+    st.session_state[\"uagi\"] = None\n+    st.session_state[\"session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    if \"url_scrape_key\" in st.session_state:\n+        st.session_state[\"url_scrape_key\"] += 1\n+    if \"file_uploader_key\" in st.session_state:\n+        st.session_state[\"file_uploader_key\"] += 1\n+    st.rerun()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/__init__.py b/cookbook/examples/streamlit_apps/vision_ai/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/agents.py b/cookbook/examples/streamlit_apps/vision_ai/agents.py\nnew file mode 100644\nindex 000000000..040287f66\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/agents.py\n@@ -0,0 +1,33 @@\n+from agno.agent import Agent\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+\n+def image_processing_agent(\n+    model,\n+) -> Agent:\n+    extraction_agent = Agent(\n+        name=\"image_analysis_agent\",\n+        model=model,\n+        markdown=True,\n+    )\n+\n+    return extraction_agent\n+\n+\n+def chat_followup_agent(\n+    model,\n+    enable_search: bool = False,\n+) -> Agent:\n+    tools = [DuckDuckGoTools()] if enable_search else []\n+    followup_agent = Agent(\n+        name=\"image_chat_followup_agent\",\n+        model=model,\n+        tools=tools,\n+        read_chat_history=True,\n+        add_history_to_messages=True,\n+        num_history_responses=5,\n+        markdown=True,\n+        add_datetime_to_instructions=True,\n+    )\n+\n+    return followup_agent\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/app.py b/cookbook/examples/streamlit_apps/vision_ai/app.py\nnew file mode 100644\nindex 000000000..14d509b81\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/app.py\n@@ -0,0 +1,321 @@\n+import os\n+import time\n+from pathlib import Path\n+\n+import streamlit as st\n+from agents import chat_followup_agent, image_processing_agent\n+from agno.media import Image\n+from agno.models.google import Gemini\n+from agno.models.mistral.mistral import MistralChat\n+from agno.models.openai import OpenAIChat\n+from agno.utils.log import logger\n+from dotenv import load_dotenv\n+from prompt import extraction_prompt\n+from utils import about_widget, add_message, clear_chat\n+\n+load_dotenv()\n+\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n+MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"VisionAI Chat\",\n+    page_icon=\"\ud83d\udcf7\",\n+    layout=\"wide\",\n+)\n+\n+\n+def main():\n+    ####################################################################\n+    # App Header\n+    ####################################################################\n+    st.markdown(\n+        \"\"\"\n+        <style>\n+            .title {\n+                text-align: center;\n+                font-size: 3em;\n+                font-weight: bold;\n+                color: white;\n+            }\n+            .subtitle {\n+                text-align: center;\n+                font-size: 1.5em;\n+                color: #bbb;\n+                margin-top: -15px;\n+            }\n+        </style>\n+        <h1 class='title'>VisionAI \ud83d\uddbc\ufe0f</h1>\n+        <p class='subtitle'>Your AI-powered smart image analysis agent</p>\n+        \"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Ensure session state variables are initialized\n+    ####################################################################\n+    if \"last_extracted_image\" not in st.session_state:\n+        st.session_state[\"last_extracted_image\"] = None\n+    if \"last_image_response\" not in st.session_state:\n+        st.session_state[\"last_image_response\"] = None\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+    if \"extract_triggered\" not in st.session_state:\n+        st.session_state[\"extract_triggered\"] = False\n+\n+    ####################################################################\n+    # Sidebar Configuration\n+    ####################################################################\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\uddbc\ufe0f Smart Image Analysis Agent\")\n+\n+        # Model Selection\n+        model_choice = st.selectbox(\n+            \"\ud83d\udd0d Select Model Provider\", [\"OpenAI\", \"Gemini\", \"Mistral\"], index=0\n+        )\n+\n+        # Mode Selection\n+        mode = st.radio(\n+            \"\u2699\ufe0f Extraction Mode\",\n+            [\"Auto\", \"Manual\", \"Hybrid\"],\n+            index=0,\n+            help=\"Select how the image analysis should be performed:\\n\"\n+            \"- **Auto**: Extracts the image automatically without any extra information from users.\\n\"\n+            \"- **Manual**: User provide specific instructions for image extraction.\\n\"\n+            \"- **Hybrid**: Combined auto-processing mode with user-defined instructions.\",\n+        )\n+\n+        # Web Search Option (Enable/Disable DuckDuckGo)\n+        enable_search_option = st.radio(\"\ud83c\udf10 Enable Web Search?\", [\"Yes\", \"No\"], index=1)\n+        enable_search = True if enable_search_option == \"Yes\" else False\n+\n+    ####################################################################\n+    # Ensure Model is Initialized Properly\n+    ####################################################################\n+    if (\n+        \"model_instance\" not in st.session_state\n+        or st.session_state.get(\"model_choice\", None) != model_choice\n+    ):\n+        if model_choice == \"OpenAI\":\n+            if not OPENAI_API_KEY:\n+                st.error(\n+                    \"\u26a0\ufe0f OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\"\n+                )\n+            model = OpenAIChat(id=\"gpt-4o\", api_key=OPENAI_API_KEY)\n+        elif model_choice == \"Gemini\":\n+            if not GOOGLE_API_KEY:\n+                st.error(\n+                    \"\u26a0\ufe0f Google API key not found. Please set the GOOGLE_API_KEY environment variable.\"\n+                )\n+            model = Gemini(id=\"gemini-2.0-flash\", api_key=GOOGLE_API_KEY)\n+        elif model_choice == \"Mistral\":\n+            if not MISTRAL_API_KEY:\n+                st.error(\n+                    \"\u26a0\ufe0f Mistral API key not found. Please set the MISTRAL_API_KEY environment variable.\"\n+                )\n+            model = MistralChat(id=\"pixtral-12b-2409\", api_key=MISTRAL_API_KEY)\n+        else:\n+            st.error(\n+                \"\u26a0\ufe0f Unsupported model provider. Please select OpenAI, Gemini, or Mistral.\"\n+            )\n+            st.stop()  # Stop execution if model is not supported\n+\n+        st.session_state[\"model_instance\"] = model\n+    else:\n+        model = st.session_state[\"model_instance\"]\n+\n+    ####################################################################\n+    # Modify Agents Without Creating New Session\n+    ####################################################################\n+    if (\n+        \"image_agent\" not in st.session_state\n+        or \"chat_agent\" not in st.session_state\n+        or st.session_state.get(\"model_choice\", None) != model_choice\n+        or st.session_state.get(\"enable_search\", None) != enable_search\n+    ):\n+        logger.info(\n+            f\"Updating Agents with model {model.id} and search enabled {enable_search}\"\n+        )\n+        image_agent = image_processing_agent(model=model)\n+        st.session_state[\"image_agent\"] = image_agent\n+        chat_agent = chat_followup_agent(model=model, enable_search=enable_search)\n+        st.session_state[\"chat_agent\"] = chat_agent\n+        st.session_state[\"enable_search\"] = enable_search\n+\n+        ####################################################################\n+        # Store new selections in session_state\n+        ####################################################################\n+        st.session_state[\"model_choice\"] = model_choice\n+        st.session_state[\"enable_search\"] = enable_search\n+\n+    else:\n+        image_agent = st.session_state[\"image_agent\"]\n+        chat_agent = st.session_state[\"chat_agent\"]\n+\n+    ####################################################################\n+    # Load Runs from Memory (Chat History)\n+    ####################################################################\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Image Upload Section\n+    ####################################################################\n+    uploaded_file = st.file_uploader(\n+        \"\ud83d\udce4 Upload an Image (Max: 20MB) \ud83d\udcf7\", type=[\"png\", \"jpg\", \"jpeg\"]\n+    )\n+    image_path = None\n+\n+    if uploaded_file:\n+        temp_dir = Path(\"tmp/\")\n+        temp_dir.mkdir(exist_ok=True)\n+        image_path = temp_dir / uploaded_file.name\n+\n+        # Check if this is a new image different from the last one\n+        if (\n+            \"last_extracted_image\" in st.session_state\n+            and st.session_state[\"last_extracted_image\"] is not None\n+            and str(st.session_state[\"last_extracted_image\"]) != str(image_path)\n+        ):\n+            logger.info(\n+                f\"New image detected. Resetting chat history and reinitializing agents.\"\n+            )\n+            clear_chat()\n+\n+        with open(image_path, \"wb\") as f:\n+            f.write(uploaded_file.getbuffer())\n+\n+            # Display image preview in sidebar if an image is uploaded\n+            st.sidebar.markdown(\"#### \ud83d\uddbc\ufe0f Current Image\")\n+            st.sidebar.image(uploaded_file, use_container_width=True)\n+\n+        logger.info(f\"\u2705 Image successfully saved at: {image_path}\")\n+\n+        # Show instruction input only for Manual & Hybrid Mode\n+        if mode in [\"Manual\", \"Hybrid\"]:\n+            instruction = st.text_area(\n+                \"\ud83d\udcdd Enter Extraction Instructions\",\n+                placeholder=\"Extract number plates...\",\n+            )\n+        else:\n+            instruction = None\n+\n+        # ADD Extract Data Button\n+        if st.button(\"\ud83d\udd0d Extract Data\"):\n+            if (\n+                image_path\n+                and (mode == \"Auto\" or instruction)\n+                and (\n+                    \"last_image_response\" not in st.session_state\n+                    or st.session_state[\"last_extracted_image\"] != image_path\n+                )\n+            ):\n+                with st.spinner(\"\ud83d\udce4 Processing Image! Extracting image data...\"):\n+                    extracted_data = image_agent.run(\n+                        extraction_prompt,\n+                        images=[Image(filepath=image_path)],\n+                        instructions=instruction if instruction else None,\n+                    )\n+\n+                # Store last extracted response for chat follow-ups\n+                st.session_state[\"last_image_response\"] = extracted_data.content\n+                st.session_state[\"last_extracted_image\"] = image_path\n+\n+                # Create a temporary success message container\n+                success_message = st.empty()\n+                success_message.success(\"\u2705 Image processing completed successfully!\")\n+\n+                logger.info(f\"Extracted Data Response: {extracted_data.content}\")\n+\n+                # Wait for 1 seconds, then clear the success message\n+                time.sleep(1)\n+                success_message.empty()\n+\n+        # Display Extracted Image Data Persistently\n+        if st.session_state[\"last_image_response\"]:\n+            st.write(\"### Extracted Image Insights:\")\n+            st.write(st.session_state[\"last_image_response\"])\n+\n+    ####################################################################\n+    # Follow-up Chat Section\n+    ####################################################################\n+    st.markdown(\"---\")\n+    st.markdown(\"### \ud83d\udcac Chat with VisionAI\")\n+\n+    ####################################################################\n+    # Display Chat History First\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] == \"system\":\n+            continue\n+        with st.chat_message(message[\"role\"]):\n+            st.write(message[\"content\"])\n+\n+    if prompt := st.chat_input(\n+        \"\ud83d\udcac Ask follow-up questions on the image extracted data...\"\n+    ):\n+        # Display user message first\n+        with st.chat_message(\"user\"):\n+            st.write(prompt)\n+        # Add user message to session state\n+        add_message(\"user\", prompt)\n+\n+        ####################################################################\n+        # Process User Queries & Stream Responses\n+        ####################################################################\n+        last_message = (\n+            st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+        )\n+\n+        if last_message and last_message[\"role\"] == \"user\":\n+            user_question = last_message[\"content\"]\n+\n+            # Ensure Image Agent has extracted data before running chat agent\n+            if (\n+                \"last_image_response\" not in st.session_state\n+                or not st.session_state[\"last_image_response\"]\n+            ):\n+                st.warning(\n+                    \"\u26a0\ufe0f No extracted insights available. Please process an image first.\"\n+                )\n+            else:\n+                with st.chat_message(\"assistant\"):\n+                    response_container = st.empty()\n+                    with st.spinner(\"\ud83e\udd14 Processing follow-up question...\"):\n+                        try:\n+                            chat_response = chat_agent.run(\n+                                f\"\"\"You are a chat agent who answers followup questions based on extracted image data.\n+    Understand the requirement properly and then answer the question correctly.\n+\n+    Extracted Image Data: {st.session_state[\"last_image_response\"]}\n+\n+    Use the above image insights to answer the following question.\n+    Answer the following question from the above given extracted image data: {user_question}\"\"\",\n+                                stream=True,\n+                            )\n+\n+                            response_text = \"\"\n+                            for chunk in chat_response:\n+                                if chunk and chunk.content:\n+                                    response_text += chunk.content\n+                                    response_container.markdown(response_text)\n+\n+                            add_message(\"assistant\", response_text)\n+\n+                        except Exception as e:\n+                            error_message = f\"\u274c Error: {str(e)}\"\n+                            add_message(\"assistant\", error_message)\n+                            st.error(error_message)\n+\n+    # Add clear chat button in sidebar\n+    if st.sidebar.button(\"\ud83e\uddf9 Clear Chat History\", key=\"clear_chat\"):\n+        clear_chat()\n+\n+    # About Section\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/generate_requirements.sh b/cookbook/examples/streamlit_apps/vision_ai/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/prompt.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/prompt.py b/cookbook/examples/streamlit_apps/vision_ai/prompt.py\nnew file mode 100644\nindex 000000000..2a2bebf11\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/prompt.py\n@@ -0,0 +1,143 @@\n+extraction_prompt = \"\"\"\n+\n+    ### Task: Extract Maximum Information from the Image\n+    You are an advanced AI agent specialized in analyzing and extracting structured data from images.\n+\n+    #### \ud83d\udd0d **General Extraction Process:**\n+    1. **Identify the Image Type:** Determine if it's a document, a chart, a traffic scene, a shopfront, etc.\n+    2. **Extract All Relevant Elements:**\n+        - For documents: Extract **printed text, handwriting, tables, signatures**.\n+        - For traffic scenes: Detect **cars, people, number plates, road signs**.\n+        - For charts: Identify **chart type, X & Y labels, legend, data points**.\n+        - For places: Recognize **business names, advertisements, location details**.\n+    3. **Provide Contextual Insights:** Explain what the image represents.\n+    4. **Output in a Structured JSON Format.**\n+\n+    #### **\ud83d\udccc Important Guidelines**\n+    - Do **not** just list objects, extract **detailed insights**.\n+    - If text is present, perform OCR and extract structured content.\n+    - Extract colors, numbers, categories where applicable.\n+    - If the image shows a **famous place**, provide historical or contextual details.\n+\n+    ---\n+\n+    ## **Example 1: Traffic Scene with Multiple Elements**\n+    **Input:** Image of a traffic junction with vehicles, road signs, and people.\n+\n+    **Output:**\n+    ```json\n+    {\n+        \"scene_description\": \"A busy traffic junction with cars waiting at a red signal. There are pedestrians crossing, and road signs providing directions.\",\n+        \"vehicles\": {\n+            \"count\": 5,\n+            \"details\": [\n+                {\"type\": \"Car\", \"color\": \"Red\", \"number_plate\": \"AB1234\"},\n+                {\"type\": \"Car\", \"color\": \"Blue\", \"number_plate\": \"XY5678\"},\n+                {\"type\": \"Bus\", \"color\": \"Yellow\", \"number_plate\": \"TR7890\"},\n+                {\"type\": \"Bike\", \"color\": \"Black\"},\n+                {\"type\": \"Truck\", \"color\": \"White\", \"number_plate\": \"LM4567\"}\n+            ]\n+        },\n+        \"road_signs\": [\n+            {\"text\": \"STOP\", \"type\": \"Regulatory\", \"position\": \"Left side of the road\"},\n+            {\"text\": \"Speed Limit 60 km/h\", \"type\": \"Warning\", \"position\": \"Above traffic lights\"}\n+        ],\n+        \"pedestrians\": {\n+            \"count\": 3,\n+            \"activity\": \"Crossing the road at a zebra crossing\"\n+        },\n+        \"analysis\": \"Busy urban intersection during business hours with mixed vehicle and pedestrian traffic. The presence of multiple commercial establishments and professional vehicles suggests this is a central business district.\",\n+        \"significance\": \"This appears to be a major commercial hub given the intersection of Main St and Commerce Ave, with diverse business activity evident from signage and foot traffic.\"\n+    }\n+    ```\n+\n+    ---\n+\n+    ## **Example 2: Document Image with Text & Tables**\n+    **Input:** A scanned invoice containing text, tables, and a signature.\n+\n+    **Output:**\n+    ```json\n+    {\n+        \"document_type\": \"Invoice\",\n+        \"header\": {\n+            \"company_name\": \"ABC Corp.\",\n+            \"invoice_number\": \"INV-2024021\",\n+            \"date\": \"2024-02-12\"\n+        },\n+        \"items\": [\n+            {\"item\": \"Laptop\", \"quantity\": 1, \"price\": \"$1200\"},\n+            {\"item\": \"Mouse\", \"quantity\": 2, \"price\": \"$40\"}\n+        ],\n+        \"total_amount\": \"$1280\",\n+        \"signature_detected\": true,\n+        \"notes\": \"Paid via Credit Card\"\n+    }\n+    ```\n+\n+    ## **Example 3: Document/Chart Analysis**\n+    **Input:** An visualization chart of business report.\n+\n+    **Output:**\n+    ```json\n+    {\n+        \"extracted_data\": {\n+            \"document_type\": \"Business report with charts\",\n+            \"content_elements\": {\n+                \"charts\": [\n+                    {\n+                        \"type\": \"Bar graph\",\n+                        \"title\": \"Annual Revenue 2020-2023\",\n+                        \"axes\": {\n+                            \"x\": \"Years\",\n+                            \"y\": \"Revenue ($ millions)\"\n+                        },\n+                        \"data_points\": [\n+                            {\"year\": \"2020\", \"value\": 1.2},\n+                            {\"year\": \"2021\", \"value\": 1.5},\n+                            {\"year\": \"2022\", \"value\": 1.8},\n+                            {\"year\": \"2023\", \"value\": 2.1}\n+                        ]\n+                    }\n+                ],\n+                \"text_blocks\": [\n+                    {\n+                        \"type\": \"Heading\",\n+                        \"content\": \"Q4 Financial Summary\",\n+                        \"location\": \"Top of page\"\n+                    },\n+                    {\n+                        \"type\": \"Paragraph\",\n+                        \"content\": \"The fourth quarter showed strong growth...\",\n+                        \"location\": \"Below heading\"\n+                    }\n+                ],\n+                \"tables\": [\n+                    {\n+                        \"title\": \"Regional Performance\",\n+                        \"columns\": [\"Region\", \"Revenue\", \"Growth\"],\n+                        \"rows\": [\n+                            [\"North\", \"$500K\", \"+15%\"],\n+                            [\"South\", \"$600K\", \"+12%\"]\n+                        ]\n+                    }\n+                ]\n+            },\n+            \"analysis\": \"Comprehensive financial report showing positive growth trends across multiple regions. The document combines textual analysis with supporting visual data through charts and tables.\",\n+            \"significance\": \"This report indicates a consistent upward trend in company performance over the past four years, with particularly strong regional growth in the North and South territories.\"\n+        }\n+    }\n+\n+    ---\n+\n+    ### **Final Instructions**\n+    - Always return JSON output.\n+    - If text is present, extract and categorize it.\n+    - If objects are found, count and describe them.\n+    - If a known landmark is detected, provide extra context.\n+    - Provide comprehensive details but maintain appropriate privacy (no personal identification)\n+    - Organize information hierarchically from general to specific\n+    - Always include analysis and significance when relevant\n+\n+    **Process the image and return structured data now.**\n+    \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/requirements.in b/cookbook/examples/streamlit_apps/vision_ai/requirements.in\nnew file mode 100644\nindex 000000000..423d57c13\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/requirements.in\n@@ -0,0 +1,11 @@\n+agno\n+openai\n+google-genai\n+mistralai\n+pgvector\n+psycopg[binary]\n+simplejson\n+sqlalchemy\n+streamlit\n+duckduckgo-search\n+nest_asyncio\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/requirements.txt b/cookbook/examples/streamlit_apps/vision_ai/requirements.txt\nnew file mode 100644\nindex 000000000..98ea4f72a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/requirements.txt\n@@ -0,0 +1,228 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.9\n+    # via -r requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   google-genai\n+    #   httpx\n+    #   openai\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.5.1\n+    # via -r requirements.in\n+eval-type-backport==0.2.2\n+    # via mistralai\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.5.0\n+    # via -r requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   google-genai\n+    #   mistralai\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via openai\n+jsonpath-python==1.0.6\n+    # via mistralai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+mistralai==1.5.1\n+    # via -r requirements.in\n+mypy-extensions==1.0.0\n+    # via typing-inspect\n+narwhals==1.30.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pgvector\n+    #   pydeck\n+    #   streamlit\n+openai==1.65.5\n+    # via -r requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pgvector==0.3.6\n+    # via -r requirements.in\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.14.0\n+    # via duckduckgo-search\n+protobuf==5.29.3\n+    # via streamlit\n+psycopg==3.2.5\n+    # via -r requirements.in\n+psycopg-binary==3.2.5\n+    # via psycopg\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   google-genai\n+    #   mistralai\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   mistralai\n+    #   pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+simplejson==3.20.1\n+    # via -r requirements.in\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+sqlalchemy==2.0.38\n+    # via -r requirements.in\n+streamlit==1.43.1\n+    # via -r requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   google-genai\n+    #   openai\n+    #   psycopg\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspect\n+typing-inspect==0.9.0\n+    # via mistralai\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+websockets==14.2\n+    # via google-genai\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/examples/streamlit_apps/vision_ai/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/vision_ai/utils.py b/cookbook/examples/streamlit_apps/vision_ai/utils.py\nnew file mode 100644\nindex 000000000..a147a9008\n--- /dev/null\n+++ b/cookbook/examples/streamlit_apps/vision_ai/utils.py\n@@ -0,0 +1,155 @@\n+import streamlit as st\n+from agents import image_processing_agent\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from agno.models.openai import OpenAIChat\n+from agno.utils.log import logger\n+\n+\n+def add_message(role: str, content: str) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append({\"role\": role, \"content\": content})\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting Image Agent ---*---\")\n+    st.session_state[\"image_agent\"] = None\n+    st.session_state[\"image_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def clear_chat():\n+    \"\"\"Clear chat history and reset relevant session state variables\n+    while preserving model settings\"\"\"\n+\n+    # Clear chat and extraction data\n+    st.session_state[\"messages\"] = []\n+    st.session_state[\"last_image_response\"] = None\n+    st.session_state[\"last_extracted_image\"] = None\n+    st.session_state[\"extract_triggered\"] = False\n+\n+    # Remove agents so they'll be reinitialized\n+    if \"image_agent\" in st.session_state:\n+        del st.session_state[\"image_agent\"]\n+    if \"chat_agent\" in st.session_state:\n+        del st.session_state[\"chat_agent\"]\n+\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# VisioAI - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def session_selector_widget(agent: Agent) -> None:\n+    \"\"\"Display a session selector in the sidebar and reinitialize the agent if needed.\"\"\"\n+\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        session_options = [\n+            {\n+                \"id\": session.session_id,\n+                \"display\": session.session_data.get(\"session_name\", session.session_id),\n+            }\n+            for session in agent_sessions\n+        ]\n+\n+        # Ensure we have sessions before showing the selector\n+        if not session_options:\n+            st.sidebar.warning(\"\u26a0\ufe0f No previous sessions found. Starting a new session.\")\n+            return\n+\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+\n+        # Safely find the selected session ID\n+        selected_session_id = next(\n+            (s[\"id\"] for s in session_options if s[\"display\"] == selected_session),\n+            None,  # Default to None if not found\n+        )\n+\n+        if not selected_session_id:\n+            st.sidebar.warning(\n+                \"\u26a0\ufe0f Selected session not found. Please restart or choose another session.\"\n+            )\n+            return\n+\n+        if st.session_state.get(\"image_agent_session_id\") != selected_session_id:\n+            logger.info(f\"---*--- Loading session: {selected_session_id} ---*---\")\n+\n+            # Retrieve Model Choice & API Key\n+            model_choice = st.session_state.get(\"model_choice\")\n+            api_key = st.session_state.get(\"api_key\")\n+\n+            if model_choice == \"OpenAI\":\n+                model = OpenAIChat(id=\"gpt-4o\", api_key=api_key)\n+            else:\n+                model = Gemini(id=\"gemini-2.0-flash\", api_key=api_key)\n+\n+            # Reload the agent with the selected session\n+            st.session_state[\"image_agent\"] = image_processing_agent(model=model)\n+            st.session_state[\"image_agent\"].load_session(selected_session_id)\n+            st.session_state[\"image_agent_session_id\"] = selected_session_id\n+            st.rerun()\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage.\"\"\"\n+\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\"\"\"\n+    VisioAI helps you analyze images and extract insights using AI-powered object detection,\n+    OCR, and scene recognition.\n+\n+    Built with:\n+    - \ud83d\ude80 Agno\n+    - \ud83d\udcab Streamlit\n+    \"\"\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/anthropic/image_input_file_upload.py",
            "diff": "diff --git a/cookbook/models/anthropic/image_input_file_upload.py b/cookbook/models/anthropic/image_input_file_upload.py\nnew file mode 100644\nindex 000000000..1aca8b723\n--- /dev/null\n+++ b/cookbook/models/anthropic/image_input_file_upload.py\n@@ -0,0 +1,41 @@\n+\"\"\"\n+In this example, we upload a PDF file to Anthropic directly and then use it as an input to an agent.\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.anthropic import Claude\n+from agno.utils.media import download_file\n+from anthropic import Anthropic\n+\n+img_path = Path(__file__).parent.joinpath(\"agno-intro.png\")\n+\n+# Download the file using the download_file function\n+download_file(\n+    \"https://agno-public.s3.us-east-1.amazonaws.com/images/agno-intro.png\",\n+    str(img_path),\n+)\n+\n+# Initialize Anthropic client\n+client = Anthropic()\n+\n+# Upload the file to Anthropic\n+uploaded_file = client.beta.files.upload(\n+    file=Path(img_path),\n+)\n+\n+if uploaded_file is not None:\n+    agent = Agent(\n+        model=Claude(\n+            id=\"claude-opus-4-20250514\",\n+            default_headers={\"anthropic-beta\": \"files-api-2025-04-14\"},\n+        ),\n+        markdown=True,\n+    )\n+\n+    agent.print_response(\n+        \"What does the attached image say.\",\n+        images=[Image(content=uploaded_file)],\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/anthropic/pdf_input_file_upload.py",
            "diff": "diff --git a/cookbook/models/anthropic/pdf_input_file_upload.py b/cookbook/models/anthropic/pdf_input_file_upload.py\nnew file mode 100644\nindex 000000000..4f271f931\n--- /dev/null\n+++ b/cookbook/models/anthropic/pdf_input_file_upload.py\n@@ -0,0 +1,40 @@\n+\"\"\"\n+In this example, we upload a PDF file to Anthropic directly and then use it as an input to an agent.\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.anthropic import Claude\n+from agno.utils.media import download_file\n+from anthropic import Anthropic\n+\n+pdf_path = Path(__file__).parent.joinpath(\"ThaiRecipes.pdf\")\n+\n+# Download the file using the download_file function\n+download_file(\n+    \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\", str(pdf_path)\n+)\n+\n+# Initialize Anthropic client\n+client = Anthropic()\n+\n+# Upload the file to Anthropic\n+uploaded_file = client.beta.files.upload(\n+    file=Path(pdf_path),\n+)\n+\n+if uploaded_file is not None:\n+    agent = Agent(\n+        model=Claude(\n+            id=\"claude-opus-4-20250514\",\n+            default_headers={\"anthropic-beta\": \"files-api-2025-04-14\"},\n+        ),\n+        markdown=True,\n+    )\n+\n+    agent.print_response(\n+        \"Summarize the contents of the attached file.\",\n+        files=[File(external=uploaded_file)],\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_tool_use.py b/cookbook/models/meta/llama/async_tool_use.py\nindex a0342c619..537cca549 100644\n--- a/cookbook/models/meta/llama/async_tool_use.py\n+++ b/cookbook/models/meta/llama/async_tool_use.py\n@@ -1,14 +1,17 @@\n-\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install agno llama-api-client yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n import asyncio\n \n from agno.agent import Agent\n from agno.models.meta import Llama\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     debug_mode=True,\n )\n-asyncio.run(agent.aprint_response(\"Tell me the latest news about Llama API\"))\n+asyncio.run(agent.aprint_response(\"Whats the price of AAPL stock?\"))\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama/async_tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_tool_use_stream.py b/cookbook/models/meta/llama/async_tool_use_stream.py\nindex 89931951b..d559771e7 100644\n--- a/cookbook/models/meta/llama/async_tool_use_stream.py\n+++ b/cookbook/models/meta/llama/async_tool_use_stream.py\n@@ -1,14 +1,17 @@\n-\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install agno llama-api-client yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n import asyncio\n \n from agno.agent import Agent\n from agno.models.meta import Llama\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     show_tool_calls=True,\n )\n-asyncio.run(agent.aprint_response(\"What's happening in France?\", stream=True))\n+asyncio.run(agent.aprint_response(\"Whats the price of AAPL stock?\", stream=True))\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama/tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama/tool_use.py b/cookbook/models/meta/llama/tool_use.py\nindex 19fa85bae..daeae7e52 100644\n--- a/cookbook/models/meta/llama/tool_use.py\n+++ b/cookbook/models/meta/llama/tool_use.py\n@@ -1,11 +1,14 @@\n-\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install agno llama-api-client yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n from agno.agent import Agent\n from agno.models.meta import Llama\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n )\n-agent.print_response(\"Tell me the latest news about Llama API\")\n+agent.print_response(\"What is the price of AAPL stock?\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama/tool_use_stream.py b/cookbook/models/meta/llama/tool_use_stream.py\nindex 4e56f81bd..78332cc49 100644\n--- a/cookbook/models/meta/llama/tool_use_stream.py\n+++ b/cookbook/models/meta/llama/tool_use_stream.py\n@@ -1,12 +1,15 @@\n-\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install agno llama-api-client yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n from agno.agent import Agent\n from agno.models.meta import Llama\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     show_tool_calls=True,\n )\n-agent.print_response(\"Tell me the latest news about Llama API\", stream=True)\n+agent.print_response(\"Tell me the price of AAPL stock\", stream=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama_openai/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/async_tool_use.py b/cookbook/models/meta/llama_openai/async_tool_use.py\nindex 644280080..4b1ea5016 100644\n--- a/cookbook/models/meta/llama_openai/async_tool_use.py\n+++ b/cookbook/models/meta/llama_openai/async_tool_use.py\n@@ -1,14 +1,17 @@\n-\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install openai yfinance` to install dependencies.\"\"\"    \n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n import asyncio\n \n from agno.agent import Agent\n from agno.models.meta import LlamaOpenAI\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     show_tool_calls=True,\n )\n-asyncio.run(agent.aprint_response(\"Whats happening in France?\"))\n+asyncio.run(agent.aprint_response(\"Whats the price of AAPL stock?\"))\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama_openai/async_tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/async_tool_use_stream.py b/cookbook/models/meta/llama_openai/async_tool_use_stream.py\nindex b4061c795..4a20b63c3 100644\n--- a/cookbook/models/meta/llama_openai/async_tool_use_stream.py\n+++ b/cookbook/models/meta/llama_openai/async_tool_use_stream.py\n@@ -1,14 +1,17 @@\n-\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install openai yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n import asyncio\n \n from agno.agent import Agent\n from agno.models.meta import LlamaOpenAI\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     show_tool_calls=True,\n )\n-asyncio.run(agent.aprint_response(\"Whats happening in France?\", stream=True))\n+asyncio.run(agent.aprint_response(\"Whats the price of AAPL stock?\", stream=True))\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama_openai/tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/tool_use.py b/cookbook/models/meta/llama_openai/tool_use.py\nindex af3286b85..56bce9d85 100644\n--- a/cookbook/models/meta/llama_openai/tool_use.py\n+++ b/cookbook/models/meta/llama_openai/tool_use.py\n@@ -1,12 +1,15 @@\n-\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install openai yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n from agno.agent import Agent\n from agno.models.meta import LlamaOpenAI\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     show_tool_calls=True,\n )\n-agent.print_response(\"Whats happening in France?\")\n+agent.print_response(\"Whats the price of AAPL stock?\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/models/meta/llama_openai/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/tool_use_stream.py b/cookbook/models/meta/llama_openai/tool_use_stream.py\nindex eaa3f74ab..49b35908a 100644\n--- a/cookbook/models/meta/llama_openai/tool_use_stream.py\n+++ b/cookbook/models/meta/llama_openai/tool_use_stream.py\n@@ -1,12 +1,15 @@\n-\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\"\"\"Run `pip install openai yfinance` to install dependencies.\"\"\"\n+\n+# Note: Currently, Llama API does not support tools with parameters other than string.\n+# This is a limitation of the Llama API.\n \n from agno.agent import Agent\n from agno.models.meta import LlamaOpenAI\n-from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n \n agent = Agent(\n     model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n-    tools=[DuckDuckGoTools()],\n+    tools=[YFinanceTools()],\n     show_tool_calls=True,\n )\n-agent.print_response(\"Whats happening in France?\", stream=True)\n+agent.print_response(\"Whats the price of AAPL stock?\", stream=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/observability/teams/langfuse_via_openinference_async_team.py",
            "diff": "diff --git a/cookbook/observability/teams/langfuse_via_openinference_async_team.py b/cookbook/observability/teams/langfuse_via_openinference_async_team.py\nindex c9aae4f2f..3dd30d66d 100644\n--- a/cookbook/observability/teams/langfuse_via_openinference_async_team.py\n+++ b/cookbook/observability/teams/langfuse_via_openinference_async_team.py\n@@ -53,7 +53,7 @@ market_data_agent = Agent(\n news_agent = Agent(\n     name=\"News Research Agent\",\n     role=\"Research company news\",\n-    agent_id=\"news-research\", \n+    agent_id=\"news-research\",\n     model=OpenAIChat(id=\"gpt-4.1\"),\n     tools=[DuckDuckGoTools()],\n     instructions=[\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/observability/teams/langfuse_via_openinference_team.py",
            "diff": "diff --git a/cookbook/observability/teams/langfuse_via_openinference_team.py b/cookbook/observability/teams/langfuse_via_openinference_team.py\nindex d27da5cf1..56f8ee32a 100644\n--- a/cookbook/observability/teams/langfuse_via_openinference_team.py\n+++ b/cookbook/observability/teams/langfuse_via_openinference_team.py\n@@ -36,7 +36,7 @@ AgnoInstrumentor().instrument()\n market_data_agent = Agent(\n     name=\"Market Data Agent\",\n     role=\"Fetch and analyze stock market data\",\n-    agent_id=\"market-data\", \n+    agent_id=\"market-data\",\n     model=OpenAIChat(id=\"gpt-4.1\"),\n     tools=[\n         YFinanceTools(stock_price=True, company_info=True, analyst_recommendations=True)\n@@ -52,7 +52,7 @@ market_data_agent = Agent(\n news_agent = Agent(\n     name=\"News Research Agent\",\n     role=\"Research company news\",\n-    agent_id=\"news-research\", \n+    agent_id=\"news-research\",\n     model=OpenAIChat(id=\"gpt-4.1\"),\n     tools=[DuckDuckGoTools()],\n     instructions=[\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/reasoning/tools/llama_reasoning_tools.py",
            "diff": "diff --git a/cookbook/reasoning/tools/llama_reasoning_tools.py b/cookbook/reasoning/tools/llama_reasoning_tools.py\nnew file mode 100644\nindex 000000000..4bc66994e\n--- /dev/null\n+++ b/cookbook/reasoning/tools/llama_reasoning_tools.py\n@@ -0,0 +1,29 @@\n+from agno.agent import Agent\n+from agno.models.meta import Llama\n+from agno.tools.reasoning import ReasoningTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+reasoning_agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[\n+        ReasoningTools(\n+            think=True,\n+            analyze=True,\n+            add_instructions=True,\n+        ),\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        ),\n+    ],\n+    instructions=\"Use tables where possible\",\n+    markdown=True,\n+    show_tool_calls=True,\n+)\n+reasoning_agent.print_response(\n+    \"What is the NVDA stock price? Write me a report\",\n+    show_full_reasoning=True,\n+    stream_intermediate_steps=True,\n+)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "cookbook/tools/firecrawl_tools.py",
            "diff": "diff --git a/cookbook/tools/firecrawl_tools.py b/cookbook/tools/firecrawl_tools.py\nindex a8f158123..ec8c5f852 100644\n--- a/cookbook/tools/firecrawl_tools.py\n+++ b/cookbook/tools/firecrawl_tools.py\n@@ -1,9 +1,26 @@\n+\"\"\"\n+This is an example of how to use the FirecrawlTools.\n+\n+Prerequisites:\n+- Create a Firecrawl account and get an API key\n+- Set the API key as an environment variable:\n+    export FIRECRAWL_API_KEY=<your-api-key>\n+\"\"\"\n+\n from agno.agent import Agent\n from agno.tools.firecrawl import FirecrawlTools\n \n agent = Agent(\n-    tools=[FirecrawlTools(scrape=False, crawl=True)],\n+    tools=[FirecrawlTools(scrape=False, crawl=True, search=True, poll_interval=2)],\n     show_tool_calls=True,\n     markdown=True,\n )\n-agent.print_response(\"Summarize this https://finance.yahoo.com/\")\n+\n+# Should use search\n+agent.print_response(\n+    \"Search for the web for the latest on 'web scraping technologies'\",\n+    formats=[\"markdown\", \"links\"],\n+)\n+\n+# Should use crawl\n+agent.print_response(\"Summarize this https://docs.agno.com/introduction/\")\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex a037b72da..0e738a467 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -31,7 +31,7 @@ from agno.media import Audio, AudioArtifact, AudioResponse, File, Image, ImageAr\n from agno.memory.agent import AgentMemory, AgentRun\n from agno.memory.v2.memory import Memory, SessionSummary\n from agno.models.base import Model\n-from agno.models.message import Citations, Message, MessageReferences\n+from agno.models.message import Citations, Message, MessageMetrics, MessageReferences\n from agno.models.response import ModelResponse, ModelResponseEvent, ToolExecution\n from agno.reasoning.step import NextAction, ReasoningStep, ReasoningSteps\n from agno.run.messages import RunMessages\n@@ -590,10 +590,11 @@ class Agent:\n         Steps:\n         1. Reason about the task if reasoning is enabled\n         2. Generate a response from the Model (includes running function calls)\n-        3. Update Agent Memory\n-        4. Calculate session metrics\n-        5. Save session to storage\n-        6. Save output to file if save_response_to_file is set\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n         \"\"\"\n         log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n \n@@ -617,26 +618,36 @@ class Agent:\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             return self._handle_agent_run_paused(\n                 run_response=run_response, session_id=session_id, user_id=user_id, message=message\n             )\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         self._update_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n-        # 5. Save session to storage\n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -660,6 +671,18 @@ class Agent:\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n         stream_intermediate_steps: bool = False,\n     ) -> Iterator[RunResponse]:\n+        \"\"\"Run the Agent and yield the RunResponse.\n+\n+        Steps:\n+        1. Reason about the task if reasoning is enabled\n+        2. Generate a response from the Model (includes running function calls)\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n+        \"\"\"\n+\n         log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n \n         # 1. Reason about the task if reasoning is enabled\n@@ -683,6 +706,15 @@ class Agent:\n         ):\n             yield event\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             yield from self._handle_agent_run_paused_stream(\n@@ -690,16 +722,17 @@ class Agent:\n             )\n             return\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         self._update_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n         # Yield UpdatingMemory event\n         if stream_intermediate_steps:\n             yield self.create_run_response(\n@@ -708,10 +741,10 @@ class Agent:\n                 event=RunEvent.updating_memory,\n             )\n \n-        # 5. Save session to storage\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -985,10 +1018,11 @@ class Agent:\n         Steps:\n         1. Reason about the task if reasoning is enabled\n         2. Generate a response from the Model (includes running function calls)\n-        3. Update Agent Memory\n-        4. Calculate session metrics\n-        5. Save session to storage\n-        6. Save output to file if save_response_to_file is set\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n         \"\"\"\n         log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n \n@@ -1012,26 +1046,36 @@ class Agent:\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             return self._handle_agent_run_paused(\n                 run_response=run_response, session_id=session_id, user_id=user_id, message=message\n             )\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         await self._aupdate_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n-        # 5. Save session to storage\n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -1060,10 +1104,11 @@ class Agent:\n         Steps:\n         1. Reason about the task if reasoning is enabled\n         2. Generate a response from the Model (includes running function calls)\n-        3. Update Agent Memory\n-        4. Calculate session metrics\n-        5. Save session to storage\n-        6. Save output to file if save_response_to_file is set\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n         \"\"\"\n         log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n \n@@ -1089,6 +1134,15 @@ class Agent:\n         ):\n             yield event\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             for item in self._handle_agent_run_paused_stream(\n@@ -1097,16 +1151,17 @@ class Agent:\n                 yield item\n             return\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         await self._aupdate_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n         # Yield UpdatingMemory event\n         if stream_intermediate_steps:\n             yield self.create_run_response(\n@@ -1115,10 +1170,10 @@ class Agent:\n                 event=RunEvent.updating_memory,\n             )\n \n-        # 5. Save session to storage\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -1345,6 +1400,7 @@ class Agent:\n         self,\n         run_response: Optional[RunResponse] = None,\n         *,\n+        run_id: Optional[str] = None,\n         stream: Literal[False] = False,\n         stream_intermediate_steps: bool = False,\n         user_id: Optional[str] = None,\n@@ -1358,6 +1414,7 @@ class Agent:\n         self,\n         run_response: Optional[RunResponse] = None,\n         *,\n+        run_id: Optional[str] = None,\n         stream: Literal[True] = True,\n         stream_intermediate_steps: bool = False,\n         user_id: Optional[str] = None,\n@@ -1370,6 +1427,7 @@ class Agent:\n         self,\n         run_response: Optional[RunResponse] = None,\n         *,\n+        run_id: Optional[str] = None,\n         stream: Optional[bool] = None,\n         stream_intermediate_steps: bool = False,\n         user_id: Optional[str] = None,\n@@ -1440,6 +1498,30 @@ class Agent:\n         # Read existing session from storage\n         self.read_from_storage(session_id=session_id, user_id=user_id)\n \n+        # Run can be continued from previous run response or from passed run_response context\n+        if run_response is not None:\n+            messages = run_response.messages or []\n+            self.run_response = run_response\n+            self.run_id = run_response.run_id\n+        elif run_id is not None:\n+            if isinstance(self.memory, Memory):\n+                runs = self.memory.get_runs(session_id=session_id)\n+                run_response = next((r for r in runs if r.run_id == run_id), None)  # type: ignore\n+            else:\n+                runs = self.memory.runs  # type: ignore\n+                run_response = next((r for r in runs if r.response.run_id == run_id), None)  # type: ignore\n+            if run_response is None:\n+                raise RuntimeError(f\"No runs found for run ID {run_id}\")\n+            messages = run_response.messages or []\n+            self.run_response = run_response\n+            self.run_id = run_id\n+        else:\n+            self.run_response = cast(RunResponse, self.run_response)\n+            # We are continuing from a previous run\n+            run_response = self.run_response\n+            messages = self.run_response.messages or []\n+            self.run_id = self.run_response.run_id\n+\n         # Read existing session from storage\n         if self.context is not None:\n             self.resolve_run_context()\n@@ -1462,18 +1544,6 @@ class Agent:\n             knowledge_filters=effective_filters,\n         )\n \n-        # Run can be continued from previous run response or from passed run_response context\n-        if run_response is not None:\n-            messages = run_response.messages or []\n-            self.run_response = run_response\n-            self.run_id = run_response.run_id\n-        else:\n-            self.run_response = cast(RunResponse, self.run_response)\n-            # We are continuing from a previous run\n-            run_response = self.run_response\n-            messages = self.run_response.messages or []\n-            self.run_id = self.run_response.run_id\n-\n         # Extract original user message from messages and remove from messages\n         user_message = None\n         for m in messages:\n@@ -1575,6 +1645,17 @@ class Agent:\n         message: Optional[Union[str, List, Dict, Message]] = None,\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n     ) -> RunResponse:\n+        \"\"\"Continue a previous run.\n+\n+        Steps:\n+        1. Handle any updated tools\n+        2. Generate a response from the Model\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n+        \"\"\"\n         self.model = cast(Model, self.model)\n \n         # 1. Handle the updated tools\n@@ -1597,26 +1678,36 @@ class Agent:\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             return self._handle_agent_run_paused(\n                 run_response=run_response, session_id=session_id, user_id=user_id, message=message\n             )\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         self._update_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n-        # 5. Save session to storage\n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -1640,16 +1731,16 @@ class Agent:\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n         stream_intermediate_steps: bool = False,\n     ) -> Iterator[RunResponse]:\n-        \"\"\"\n-        Continue a previous agent run\n+        \"\"\"Continue a previous run.\n \n         Steps:\n-        1. Handle tool calls as updated by the user\n+        1. Handle any updated tools\n         2. Generate a response from the Model\n-        3. Update Agent Memory\n-        4. Calculate session metrics\n-        5. Save session to storage\n-        6. Save output to file if save_response_to_file is set\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n         \"\"\"\n         # 1. Handle the updated tools\n         yield from self._handle_tool_call_updates_stream(\n@@ -1674,6 +1765,15 @@ class Agent:\n         ):\n             yield event\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             yield from self._handle_agent_run_paused_stream(\n@@ -1681,16 +1781,17 @@ class Agent:\n             )\n             return\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         self._update_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n         # Yield UpdatingMemory event\n         if stream_intermediate_steps:\n             yield self.create_run_response(\n@@ -1699,10 +1800,10 @@ class Agent:\n                 event=RunEvent.updating_memory,\n             )\n \n-        # 5. Save session to storage\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -1723,6 +1824,7 @@ class Agent:\n         self,\n         run_response: Optional[RunResponse] = None,\n         *,\n+        run_id: Optional[str] = None,\n         stream: Optional[bool] = None,\n         stream_intermediate_steps: bool = False,\n         user_id: Optional[str] = None,\n@@ -1793,6 +1895,30 @@ class Agent:\n         # Read existing session from storage\n         self.read_from_storage(session_id=session_id, user_id=user_id)\n \n+        # Run can be continued from previous run response or from passed run_response context\n+        if run_response is not None:\n+            messages = run_response.messages or []\n+            self.run_response = run_response\n+            self.run_id = run_response.run_id\n+        elif run_id is not None:\n+            if isinstance(self.memory, Memory):\n+                runs = self.memory.get_runs(session_id=session_id)\n+                run_response = next((r for r in runs if r.run_id == run_id), None)  # type: ignore\n+            else:\n+                runs = self.memory.runs  # type: ignore\n+                run_response = next((r for r in runs if r.response.run_id == run_id), None)  # type: ignore\n+            if run_response is None:\n+                raise RuntimeError(f\"No runs found for run ID {run_id}\")\n+            messages = run_response.messages or []\n+            self.run_response = run_response\n+            self.run_id = run_id\n+        else:\n+            # We are continuing from a previous run\n+            self.run_response = cast(RunResponse, self.run_response)\n+            run_response = self.run_response\n+            messages = self.run_response.messages or []\n+            self.run_id = self.run_response.run_id\n+\n         # Read existing session from storage\n         if self.context is not None:\n             self.resolve_run_context()\n@@ -1815,18 +1941,6 @@ class Agent:\n             knowledge_filters=effective_filters,\n         )\n \n-        # Run can be continued from previous run response or from passed run_response context\n-        if run_response is not None:\n-            messages = run_response.messages or []\n-            self.run_response = run_response\n-            self.run_id = run_response.run_id\n-        else:\n-            # We are continuing from a previous run\n-            self.run_response = cast(RunResponse, self.run_response)\n-            run_response = self.run_response\n-            messages = self.run_response.messages or []\n-            self.run_id = self.run_response.run_id\n-\n         # Extract original user message from messages and remove from messages\n         user_message = None\n         for m in messages:\n@@ -1927,6 +2041,18 @@ class Agent:\n         message: Optional[Union[str, List, Dict, Message]] = None,\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n     ) -> RunResponse:\n+        \"\"\"Continue a previous run.\n+\n+        Steps:\n+        1. Handle any updated tools\n+        2. Generate a response from the Model\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n+        \"\"\"\n+\n         self.model = cast(Model, self.model)\n \n         # 1. Handle the updated tools\n@@ -1948,26 +2074,36 @@ class Agent:\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             return self._handle_agent_run_paused(\n                 run_response=run_response, session_id=session_id, user_id=user_id, message=message\n             )\n \n-        # 3. Update Agent Memory\n+        # 4. Update Agent Memory\n         await self._aupdate_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n-        # 5. Save session to storage\n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -1993,16 +2129,16 @@ class Agent:\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n         stream_intermediate_steps: bool = False,\n     ) -> AsyncIterator[RunResponse]:\n-        \"\"\"\n-        Continue a previous agent run\n+        \"\"\"Continue a previous run.\n \n         Steps:\n-        1. Handle tool calls as updated by the user\n+        1. Handle any updated tools\n         2. Generate a response from the Model\n-        3. Update Agent Memory\n-        4. Calculate session metrics\n-        5. Save session to storage\n-        6. Save output to file if save_response_to_file is set\n+        3. Add the run to memory\n+        4. Update Agent Memory\n+        5. Calculate session metrics\n+        6. Save session to storage\n+        7. Save output to file if save_response_to_file is set\n         \"\"\"\n         # 1. Handle the updated tools\n         async for event in self._ahandle_tool_call_updates_stream(\n@@ -2028,6 +2164,15 @@ class Agent:\n         ):\n             yield event\n \n+        # 3. Add the run to memory\n+        self._add_run_to_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n         # We should break out of the run function\n         if any(tool_call.is_paused for tool_call in run_response.tools or []):\n             for item in self._handle_agent_run_paused_stream(\n@@ -2035,16 +2180,18 @@ class Agent:\n             ):\n                 yield item\n             return\n-        # 3. Update Agent Memory\n+\n+        # 4. Update Agent Memory\n         await self._aupdate_memory(\n-            run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             user_id=user_id,\n             messages=messages,\n-            index_of_last_user_message=index_of_last_user_message,\n         )\n \n+        # 5. Calculate session metrics\n+        self._set_session_metrics(run_messages)\n+\n         # Yield UpdatingMemory event\n         if stream_intermediate_steps:\n             yield self.create_run_response(\n@@ -2053,10 +2200,10 @@ class Agent:\n                 event=RunEvent.updating_memory,\n             )\n \n-        # 5. Save session to storage\n+        # 6. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 6. Save output to file if save_response_to_file is set\n+        # 7. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n         # Log Agent Run\n@@ -2166,6 +2313,27 @@ class Agent:\n                 tool.tool_args = {}\n             tool.tool_args[field.name] = field.value\n \n+    def _handle_get_user_input_tool_update(self, run_messages: RunMessages, tool: ToolExecution):\n+        import json\n+\n+        self.model = cast(Model, self.model)\n+\n+        user_input_result = [\n+            {\"name\": user_input_field.name, \"value\": user_input_field.value}\n+            for user_input_field in tool.user_input_schema or []\n+        ]\n+        # Add the tool call result to the run_messages\n+        run_messages.messages.append(\n+            Message(\n+                role=self.model.tool_message_role,\n+                content=f\"User inputs retrieved: {json.dumps(user_input_result)}\",\n+                tool_call_id=tool.tool_call_id,\n+                tool_name=tool.tool_name,\n+                tool_args=tool.tool_args,\n+                metrics=MessageMetrics(time=0),\n+            )\n+        )\n+\n     def _run_tool(\n         self, run_messages: RunMessages, tool: ToolExecution, session_id: Optional[str] = None\n     ) -> Iterator[RunResponse]:\n@@ -2196,6 +2364,17 @@ class Agent:\n         if len(function_call_results) > 0:\n             run_messages.messages.extend(function_call_results)\n \n+    def _reject_tool_call(self, run_messages: RunMessages, tool: ToolExecution):\n+        self.model = cast(Model, self.model)\n+        function_call = self.model.get_function_call_to_run_from_tool_execution(tool, self._functions_for_model)\n+        function_call.error = tool.confirmation_note or \"Function call was rejected by the user\"\n+\n+        function_call_result = self.model.create_function_call_result(\n+            function_call=function_call,\n+            success=False,\n+        )\n+        run_messages.messages.append(function_call_result)\n+\n     async def _arun_tool(\n         self, run_messages: RunMessages, tool: ToolExecution, session_id: Optional[str] = None\n     ) -> AsyncIterator[RunResponse]:\n@@ -2236,15 +2415,24 @@ class Agent:\n                 if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n                     # Consume the generator without yielding\n                     deque(self._run_tool(run_messages, _t), maxlen=0)\n-                    _t.requires_confirmation = False\n                 else:\n-                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n+                    self._reject_tool_call(run_messages, _t)\n+                _t.requires_confirmation = False\n \n             # Case 2: Handle external execution required tools\n             if _t.external_execution_required is not None and _t.external_execution_required is True:\n                 self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n-            # Case 3: Handle user input required tools\n+            # Case 3: Agentic user input required\n+            if (\n+                _t.tool_name == \"get_user_input\"\n+                and _t.requires_user_input is not None\n+                and _t.requires_user_input is True\n+            ):\n+                self._handle_get_user_input_tool_update(run_messages=run_messages, tool=_t)\n+                _t.requires_user_input = False\n+\n+            # Case 4: Handle user input required tools\n             if _t.requires_user_input is not None and _t.requires_user_input is True:\n                 self._handle_user_input_update(tool=_t)\n                 _t.requires_user_input = False\n@@ -2261,15 +2449,23 @@ class Agent:\n                 # Tool is confirmed and hasn't been run before\n                 if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n                     yield from self._run_tool(run_messages, _t, session_id)\n-                    _t.requires_confirmation = False\n                 else:\n-                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n-\n+                    self._reject_tool_call(run_messages, _t)\n+                _t.requires_confirmation = False\n             # Case 2: Handle external execution required tools\n             if _t.external_execution_required is not None and _t.external_execution_required is True:\n                 self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n-            # Case 3: Handle user input required tools\n+            # Case 3: Agentic user input required\n+            if (\n+                _t.tool_name == \"get_user_input\"\n+                and _t.requires_user_input is not None\n+                and _t.requires_user_input is True\n+            ):\n+                self._handle_get_user_input_tool_update(run_messages=run_messages, tool=_t)\n+                _t.requires_user_input = False\n+\n+            # Case 4: Handle user input required tools\n             if _t.requires_user_input is not None and _t.requires_user_input is True:\n                 self._handle_user_input_update(tool=_t)\n \n@@ -2285,15 +2481,24 @@ class Agent:\n                 if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n                     async for _ in self._arun_tool(run_messages, _t):\n                         pass\n-                    _t.requires_confirmation = False\n                 else:\n-                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n+                    self._reject_tool_call(run_messages, _t)\n+                _t.requires_confirmation = False\n \n             # Case 2: Handle external execution required tools\n             if _t.external_execution_required is not None and _t.external_execution_required is True:\n                 self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n-            # Case 3: Handle user input required tools\n+            # Case 3: Agentic user input required\n+            if (\n+                _t.tool_name == \"get_user_input\"\n+                and _t.requires_user_input is not None\n+                and _t.requires_user_input is True\n+            ):\n+                self._handle_get_user_input_tool_update(run_messages=run_messages, tool=_t)\n+                _t.requires_user_input = False\n+\n+            # Case 4: Handle user input required tools\n             if _t.requires_user_input is not None and _t.requires_user_input is True:\n                 self._handle_user_input_update(tool=_t)\n \n@@ -2312,15 +2517,24 @@ class Agent:\n                 if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n                     async for event in self._arun_tool(run_messages, _t):\n                         yield event\n-                    _t.requires_confirmation = False\n                 else:\n-                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n+                    self._reject_tool_call(run_messages, _t)\n+                _t.requires_confirmation = False\n \n             # Case 2: Handle external execution required tools\n             if _t.external_execution_required is not None and _t.external_execution_required is True:\n                 self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n-            # Case 3: Handle user input required tools\n+            # Case 3: Agentic user input required\n+            if (\n+                _t.tool_name == \"get_user_input\"\n+                and _t.requires_user_input is not None\n+                and _t.requires_user_input is True\n+            ):\n+                self._handle_get_user_input_tool_update(run_messages=run_messages, tool=_t)\n+                _t.requires_user_input = False\n+\n+            # Case 4: Handle user input required tools\n             if _t.requires_user_input is not None and _t.requires_user_input is True:\n                 self._handle_user_input_update(tool=_t)\n \n@@ -2391,15 +2605,14 @@ class Agent:\n         # Update the RunResponse metrics\n         run_response.metrics = self.aggregate_metrics_from_messages(messages_for_run_response)\n \n-    def _update_memory(\n+    def _add_run_to_memory(\n         self,\n         run_response: RunResponse,\n         run_messages: RunMessages,\n         session_id: str,\n-        user_id: Optional[str] = None,\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n         index_of_last_user_message: int = 0,\n-    ) -> None:\n+    ):\n         if isinstance(self.memory, AgentMemory):\n             self.memory = cast(AgentMemory, self.memory)\n         else:\n@@ -2427,14 +2640,6 @@ class Agent:\n             agent_run = AgentRun(response=run_response)\n             agent_run.message = run_messages.user_message\n \n-            # Update the memories with the user message if needed\n-            if (\n-                self.memory.create_user_memories\n-                and self.memory.update_user_memories_after_run\n-                and run_messages.user_message is not None\n-            ):\n-                self.memory.update_memory(input=run_messages.user_message.get_content_string())\n-\n             if messages is not None and len(messages) > 0:\n                 for _im in messages:\n                     # Parse the message and convert to a Message object if possible\n@@ -2455,25 +2660,27 @@ class Agent:\n                         if agent_run.messages is None:\n                             agent_run.messages = []\n                         agent_run.messages.append(mp)\n-                        if self.memory.create_user_memories and self.memory.update_user_memories_after_run:\n-                            self.memory.update_memory(input=mp.get_content_string())\n                     else:\n                         log_warning(\"Unable to add message to memory\")\n+\n             # Add AgentRun to memory\n             self.memory.add_run(agent_run)\n-            # Update the session summary if needed\n-            if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n-                self.memory.update_summary()\n \n-            # 4. Calculate session metrics\n-            self.session_metrics = self.calculate_metrics(self.memory.messages)\n         elif isinstance(self.memory, Memory):\n             # Add AgentRun to memory\n             self.memory.add_run(session_id=session_id, run=run_response)\n \n-            self._make_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n+    def _set_session_metrics(self, run_messages: RunMessages):\n+        if isinstance(self.memory, AgentMemory):\n+            self.memory = cast(AgentMemory, self.memory)\n+        else:\n+            self.memory = cast(Memory, self.memory)\n \n-            # 4. Calculate session metrics\n+        if isinstance(self.memory, AgentMemory):\n+            # Calculate session metrics\n+            self.session_metrics = self.calculate_metrics(self.memory.messages)\n+        elif isinstance(self.memory, Memory):\n+            # Calculate session metrics\n             if self.session_metrics is None:\n                 self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n             else:\n@@ -2481,14 +2688,12 @@ class Agent:\n                     run_messages.messages\n                 )  # Calculate metrics for the session\n \n-    async def _aupdate_memory(\n+    def _update_memory(\n         self,\n-        run_response: RunResponse,\n         run_messages: RunMessages,\n         session_id: str,\n         user_id: Optional[str] = None,\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n-        index_of_last_user_message: int = 0,\n     ) -> None:\n         if isinstance(self.memory, AgentMemory):\n             self.memory = cast(AgentMemory, self.memory)\n@@ -2496,27 +2701,56 @@ class Agent:\n             self.memory = cast(Memory, self.memory)\n \n         if isinstance(self.memory, AgentMemory):\n-            # Add the system message to the memory\n-            if run_messages.system_message is not None:\n-                self.memory.add_system_message(\n-                    run_messages.system_message, system_message_role=self.system_message_role\n-                )\n+            # Update the memories with the user message if needed\n+            if (\n+                self.memory.create_user_memories\n+                and self.memory.update_user_memories_after_run\n+                and run_messages.user_message is not None\n+            ):\n+                self.memory.update_memory(input=run_messages.user_message.get_content_string())\n \n-            # Build a list of messages that should be added to the AgentMemory\n-            messages_for_memory: List[Message] = (\n-                [run_messages.user_message] if run_messages.user_message is not None else []\n-            )\n-            # Add messages from messages_for_run after the last user message\n-            for _rm in run_messages.messages[index_of_last_user_message:]:\n-                if _rm.add_to_agent_memory:\n-                    messages_for_memory.append(_rm)\n-            if len(messages_for_memory) > 0:\n-                self.memory.add_messages(messages=messages_for_memory)\n+            if messages is not None and len(messages) > 0:\n+                for _im in messages:\n+                    # Parse the message and convert to a Message object if possible\n+                    mp = None\n+                    if isinstance(_im, Message):\n+                        mp = _im\n+                    elif isinstance(_im, dict):\n+                        try:\n+                            mp = Message(**_im)\n+                        except Exception as e:\n+                            log_warning(f\"Failed to validate message during memory update: {e}\")\n+                    else:\n+                        log_warning(f\"Unsupported message type during memory update: {type(_im)}\")\n+                        continue\n \n-            # Create an AgentRun object to add to memory\n-            agent_run = AgentRun(response=run_response)\n-            agent_run.message = run_messages.user_message\n+                    # Add the message to the AgentRun\n+                    if mp:\n+                        if self.memory.create_user_memories and self.memory.update_user_memories_after_run:\n+                            self.memory.update_memory(input=mp.get_content_string())\n+                    else:\n+                        log_warning(\"Unable to add message to memory\")\n+\n+            # Update the session summary if needed\n+            if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n+                self.memory.update_summary()\n+\n+        elif isinstance(self.memory, Memory):\n+            self._make_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n \n+    async def _aupdate_memory(\n+        self,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+    ) -> None:\n+        if isinstance(self.memory, AgentMemory):\n+            self.memory = cast(AgentMemory, self.memory)\n+        else:\n+            self.memory = cast(Memory, self.memory)\n+\n+        if isinstance(self.memory, AgentMemory):\n             # Update the memories with the user message if needed\n             if (\n                 self.memory.create_user_memories\n@@ -2535,42 +2769,24 @@ class Agent:\n                         try:\n                             mp = Message(**_im)\n                         except Exception as e:\n-                            log_warning(f\"Failed to validate message: {e}\")\n+                            log_warning(f\"Failed to validate message during memory update: {e}\")\n                     else:\n                         log_warning(f\"Unsupported message type: {type(_im)}\")\n                         continue\n \n                     # Add the message to the AgentRun\n                     if mp:\n-                        if agent_run.messages is None:\n-                            agent_run.messages = []\n-                        agent_run.messages.append(mp)\n                         if self.memory.create_user_memories and self.memory.update_user_memories_after_run:\n                             await self.memory.aupdate_memory(input=mp.get_content_string())\n                     else:\n                         log_warning(\"Unable to add message to memory\")\n-            # Add AgentRun to memory\n-            self.memory.add_run(agent_run)\n             # Update the session summary if needed\n             if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n                 await self.memory.aupdate_summary()\n \n-            # 4. Calculate metrics for the run\n-            self.session_metrics = self.calculate_metrics(self.memory.messages)\n         elif isinstance(self.memory, Memory):\n-            # Add AgentRun to memory\n-            self.memory.add_run(session_id=session_id, run=run_response)\n-\n             await self._amake_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n \n-            # 4. Calculate metrics for the run\n-            if self.session_metrics is None:\n-                self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n-            else:\n-                self.session_metrics += self.calculate_metrics(\n-                    run_messages.messages\n-                )  # Calculate metrics for the session\n-\n     def _handle_model_response_stream(\n         self,\n         run_response: RunResponse,\n@@ -2971,7 +3187,7 @@ class Agent:\n                         try:\n                             parsed_messages.append(Message(**_im))\n                         except Exception as e:\n-                            log_warning(f\"Failed to validate message: {e}\")\n+                            log_warning(f\"Failed to validate message during memory update: {e}\")\n                     else:\n                         log_warning(f\"Unsupported message type: {type(_im)}\")\n                         continue\n@@ -3015,7 +3231,7 @@ class Agent:\n                         try:\n                             parsed_messages.append(Message(**_im))\n                         except Exception as e:\n-                            log_warning(f\"Failed to validate message: {e}\")\n+                            log_warning(f\"Failed to validate message during memory update: {e}\")\n                     else:\n                         log_warning(f\"Unsupported message type: {type(_im)}\")\n                         continue\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/app/playground/async_router.py",
            "diff": "diff --git a/libs/agno/agno/app/playground/async_router.py b/libs/agno/agno/app/playground/async_router.py\nindex efed4fdb3..ac8c71bcd 100644\n--- a/libs/agno/agno/app/playground/async_router.py\n+++ b/libs/agno/agno/app/playground/async_router.py\n@@ -108,6 +108,9 @@ async def team_chat_response_streamer(\n             run_response_chunk = cast(TeamRunResponse, run_response_chunk)\n             yield run_response_chunk.to_json()\n     except Exception as e:\n+        import traceback\n+\n+        traceback.print_exc(limit=3)\n         error_response = TeamRunResponse(\n             content=str(e),\n             event=RunEvent.run_error,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/app/playground/sync_router.py",
            "diff": "diff --git a/libs/agno/agno/app/playground/sync_router.py b/libs/agno/agno/app/playground/sync_router.py\nindex 5951a4cf9..31a7a41f9 100644\n--- a/libs/agno/agno/app/playground/sync_router.py\n+++ b/libs/agno/agno/app/playground/sync_router.py\n@@ -71,6 +71,9 @@ def chat_response_streamer(\n             run_response_chunk = cast(RunResponse, run_response_chunk)\n             yield run_response_chunk.to_json()\n     except Exception as e:\n+        import traceback\n+\n+        traceback.print_exc(limit=3)\n         error_response = RunResponse(\n             content=str(e),\n             event=RunEvent.run_error,\n@@ -105,6 +108,9 @@ def team_chat_response_streamer(\n             run_response_chunk = cast(TeamRunResponse, run_response_chunk)\n             yield run_response_chunk.to_json()\n     except Exception as e:\n+        import traceback\n+\n+        traceback.print_exc(limit=3)\n         error_response = TeamRunResponse(\n             content=str(e),\n             event=RunEvent.run_error,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/memory/agent.py",
            "diff": "diff --git a/libs/agno/agno/memory/agent.py b/libs/agno/agno/memory/agent.py\nindex e311f26de..c5ab8f73a 100644\n--- a/libs/agno/agno/memory/agent.py\n+++ b/libs/agno/agno/memory/agent.py\n@@ -98,8 +98,25 @@ class AgentMemory(BaseModel):\n \n     def add_run(self, agent_run: AgentRun) -> None:\n         \"\"\"Adds an AgentRun to the runs list.\"\"\"\n-        self.runs.append(agent_run)\n-        log_debug(\"Added AgentRun to AgentMemory\")\n+        # Initialize runs list if it doesn't exist\n+        if self.runs is None:\n+            self.runs = []\n+\n+        # Process run if it has a valid response with run_id\n+        if agent_run.response and agent_run.response.run_id:\n+            run_id = agent_run.response.run_id\n+\n+            # Check for existing run with same ID\n+            for i, run in enumerate(self.runs):\n+                if run.response and run.response.run_id == run_id:\n+                    # Replace existing run\n+                    self.runs[i] = agent_run\n+                    log_debug(f\"Replaced existing AgentRun with run_id {run_id} in memory\")\n+                    return\n+\n+            # Add new run if not found\n+            self.runs.append(agent_run)\n+            log_debug(\"Added AgentRun to AgentMemory\")\n \n     def add_system_message(self, message: Message, system_message_role: str = \"system\") -> None:\n         \"\"\"Add the system messages to the messages list\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/memory/v2/memory.py",
            "diff": "diff --git a/libs/agno/agno/memory/v2/memory.py b/libs/agno/agno/memory/v2/memory.py\nindex 96096e2b1..48a89aeb8 100644\n--- a/libs/agno/agno/memory/v2/memory.py\n+++ b/libs/agno/agno/memory/v2/memory.py\n@@ -674,7 +674,21 @@ class Memory:\n         if not self.runs:\n             self.runs = {}\n \n-        self.runs.setdefault(session_id, []).append(run)\n+        if session_id not in self.runs:\n+            self.runs[session_id] = []\n+\n+        # Check if run already exists with the same run_id\n+        if hasattr(run, \"run_id\") and run.run_id:\n+            run_id = run.run_id\n+            # Look for existing run with same ID\n+            for i, existing_run in enumerate(self.runs[session_id]):\n+                if hasattr(existing_run, \"run_id\") and existing_run.run_id == run_id:\n+                    # Replace existing run\n+                    self.runs[session_id][i] = run\n+                    log_debug(f\"Replaced existing run with run_id {run_id} in memory\")\n+                    return\n+\n+        self.runs[session_id].append(run)\n         log_debug(\"Added RunResponse to Memory\")\n \n     def get_messages_from_last_n_runs(\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/models/anthropic/claude.py",
            "diff": "diff --git a/libs/agno/agno/models/anthropic/claude.py b/libs/agno/agno/models/anthropic/claude.py\nindex 6f0f8d728..84e0ab25d 100644\n--- a/libs/agno/agno/models/anthropic/claude.py\n+++ b/libs/agno/agno/models/anthropic/claude.py\n@@ -58,6 +58,7 @@ class Claude(Model):\n     api_key: Optional[str] = None\n     default_headers: Optional[Dict[str, Any]] = None\n     client_params: Optional[Dict[str, Any]] = None\n+    default_headers: Optional[Dict[str, Any]] = None\n \n     # Anthropic clients\n     client: Optional[AnthropicClient] = None\n@@ -78,7 +79,6 @@ class Claude(Model):\n             client_params.update(self.client_params)\n         if self.default_headers is not None:\n             client_params[\"default_headers\"] = self.default_headers\n-\n         return client_params\n \n     def get_client(self) -> AnthropicClient:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/models/base.py",
            "diff": "diff --git a/libs/agno/agno/models/base.py b/libs/agno/agno/models/base.py\nindex d8492dd75..41a4c9071 100644\n--- a/libs/agno/agno/models/base.py\n+++ b/libs/agno/agno/models/base.py\n@@ -12,7 +12,7 @@ from agno.exceptions import AgentRunException\n from agno.media import AudioResponse, ImageArtifact\n from agno.models.message import Citations, Message, MessageMetrics\n from agno.models.response import ModelResponse, ModelResponseEvent, ToolExecution\n-from agno.tools.function import Function, FunctionCall, FunctionExecutionResult\n+from agno.tools.function import Function, FunctionCall, FunctionExecutionResult, UserInputField\n from agno.utils.log import log_debug, log_error, log_warning\n from agno.utils.timer import Timer\n from agno.utils.tools import get_function_call_for_tool_call, get_function_call_for_tool_execution\n@@ -1069,19 +1069,26 @@ class Model(ABC):\n                 function_calls_to_run.append(_function_call)\n         return function_calls_to_run\n \n-    def _create_function_call_result(\n-        self, fc: FunctionCall, success: bool, output: Optional[Union[List[Any], str]], timer: Timer\n+    def create_function_call_result(\n+        self,\n+        function_call: FunctionCall,\n+        success: bool,\n+        output: Optional[Union[List[Any], str]] = None,\n+        timer: Optional[Timer] = None,\n     ) -> Message:\n         \"\"\"Create a function call result message.\"\"\"\n+        kwargs = {}\n+        if timer is not None:\n+            kwargs[\"metrics\"] = MessageMetrics(time=timer.elapsed)\n         return Message(\n             role=self.tool_message_role,\n-            content=output if success else fc.error,\n-            tool_call_id=fc.call_id,\n-            tool_name=fc.function.name,\n-            tool_args=fc.arguments,\n+            content=output if success else function_call.error,\n+            tool_call_id=function_call.call_id,\n+            tool_name=function_call.function.name,\n+            tool_args=function_call.arguments,\n             tool_call_error=not success,\n-            stop_after_tool_call=fc.function.stop_after_tool_call,\n-            metrics=MessageMetrics(time=timer.elapsed),\n+            stop_after_tool_call=function_call.function.stop_after_tool_call,\n+            **kwargs,\n         )\n \n     def run_function_call(\n@@ -1137,7 +1144,7 @@ class Model(ABC):\n                 yield ModelResponse(content=function_call_output)\n \n         # Create and yield function call result\n-        function_call_result = self._create_function_call_result(\n+        function_call_result = self.create_function_call_result(\n             function_call, success=function_call_success, output=function_call_output, timer=function_call_timer\n         )\n         yield ModelResponse(\n@@ -1195,6 +1202,32 @@ class Model(ABC):\n                             if user_input_field.name == name:\n                                 user_input_field.value = value\n \n+                paused_tool_executions.append(\n+                    ToolExecution(\n+                        tool_call_id=fc.call_id,\n+                        tool_name=fc.function.name,\n+                        tool_args=fc.arguments,\n+                        requires_user_input=True,\n+                        user_input_schema=user_input_schema,\n+                    )\n+                )\n+            # If the function is from the user control flow tools, we handle it here\n+            if fc.function.name == \"get_user_input\" and fc.arguments and fc.arguments.get(\"user_input_fields\"):\n+                user_input_schema = []\n+                for input_field in fc.arguments.get(\"user_input_fields\", []):\n+                    field_type = input_field.get(\"field_type\")\n+                    try:\n+                        python_type = eval(field_type) if isinstance(field_type, str) else field_type\n+                    except (NameError, SyntaxError):\n+                        python_type = str  # Default to str if type is invalid\n+                    user_input_schema.append(\n+                        UserInputField(\n+                            name=input_field.get(\"field_name\"),\n+                            field_type=python_type,\n+                            description=input_field.get(\"field_description\"),\n+                        )\n+                    )\n+\n                 paused_tool_executions.append(\n                     ToolExecution(\n                         tool_call_id=fc.call_id,\n@@ -1315,6 +1348,38 @@ class Model(ABC):\n                             if user_input_field.name == name:\n                                 user_input_field.value = value\n \n+                paused_tool_executions.append(\n+                    ToolExecution(\n+                        tool_call_id=fc.call_id,\n+                        tool_name=fc.function.name,\n+                        tool_args=fc.arguments,\n+                        requires_user_input=True,\n+                        user_input_schema=user_input_schema,\n+                    )\n+                )\n+            # If the function is from the user control flow tools, we handle it here\n+            if (\n+                fc.function.name == \"get_user_input\"\n+                and fc.arguments\n+                and fc.arguments.get(\"user_input_fields\")\n+                and not skip_pause_check\n+            ):\n+                fc.function.requires_user_input = True\n+                user_input_schema = []\n+                for input_field in fc.arguments.get(\"user_input_fields\", []):\n+                    field_type = input_field.get(\"field_type\")\n+                    try:\n+                        python_type = eval(field_type) if isinstance(field_type, str) else field_type\n+                    except (NameError, SyntaxError):\n+                        python_type = str  # Default to str if type is invalid\n+                    user_input_schema.append(\n+                        UserInputField(\n+                            name=input_field.get(\"field_name\"),\n+                            field_type=python_type,\n+                            description=input_field.get(\"field_description\"),\n+                        )\n+                    )\n+\n                 paused_tool_executions.append(\n                     ToolExecution(\n                         tool_call_id=fc.call_id,\n@@ -1409,7 +1474,7 @@ class Model(ABC):\n                     yield ModelResponse(content=function_call_output)\n \n             # Create and yield function call result\n-            function_call_result = self._create_function_call_result(\n+            function_call_result = self.create_function_call_result(\n                 fc, success=function_call_success, output=function_call_output, timer=function_call_timer\n             )\n             yield ModelResponse(\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/models/meta/llama.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/llama.py b/libs/agno/agno/models/meta/llama.py\nindex 428befffa..3573159c9 100644\n--- a/libs/agno/agno/models/meta/llama.py\n+++ b/libs/agno/agno/models/meta/llama.py\n@@ -199,7 +199,7 @@ class Llama(Model):\n         \"\"\"\n         return self.get_client().chat.completions.create(\n             model=self.id,\n-            messages=[format_message(m) for m in messages],  # type: ignore\n+            messages=[format_message(m, tool_calls=bool(tools)) for m in messages],  # type: ignore\n             **self.get_request_kwargs(tools=tools, response_format=response_format),\n         )\n \n@@ -216,7 +216,7 @@ class Llama(Model):\n \n         return await self.get_async_client().chat.completions.create(\n             model=self.id,\n-            messages=[format_message(m) for m in messages],  # type: ignore\n+            messages=[format_message(m, tool_calls=bool(tools)) for m in messages],  # type: ignore\n             **self.get_request_kwargs(tools=tools, response_format=response_format),\n         )\n \n@@ -234,7 +234,7 @@ class Llama(Model):\n         try:\n             yield from self.get_client().chat.completions.create(\n                 model=self.id,\n-                messages=[format_message(m) for m in messages],  # type: ignore\n+                messages=[format_message(m, tool_calls=bool(tools)) for m in messages],  # type: ignore\n                 stream=True,\n                 **self.get_request_kwargs(tools=tools, response_format=response_format),\n             )  # type: ignore\n@@ -256,7 +256,7 @@ class Llama(Model):\n         try:\n             async_stream = await self.get_async_client().chat.completions.create(\n                 model=self.id,\n-                messages=[format_message(m) for m in messages],  # type: ignore\n+                messages=[format_message(m, tool_calls=bool(tools)) for m in messages],  # type: ignore\n                 stream=True,\n                 **self.get_request_kwargs(tools=tools, response_format=response_format),\n             )\n@@ -322,6 +322,12 @@ class Llama(Model):\n     def parse_provider_response(self, response: CreateChatCompletionResponse, **kwargs) -> ModelResponse:\n         \"\"\"\n         Parse the Llama response into a ModelResponse.\n+\n+        Args:\n+            response: Response from invoke() method\n+\n+        Returns:\n+            ModelResponse: Parsed response data\n         \"\"\"\n         model_response = ModelResponse()\n \n@@ -381,7 +387,7 @@ class Llama(Model):\n \n         return model_response\n \n-    def parse_provider_response_delta(self, response_delta: CreateChatCompletionResponseStreamChunk) -> ModelResponse:\n+    def parse_provider_response_delta(self, response_delta: CreateChatCompletionResponseStreamChunk, **kwargs) -> ModelResponse:\n         \"\"\"\n         Parse the Llama streaming response into a ModelResponse.\n \n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/models/ollama/tools.py",
            "diff": "diff --git a/libs/agno/agno/models/ollama/tools.py b/libs/agno/agno/models/ollama/tools.py\nindex b01c9c053..f6678ac60 100644\n--- a/libs/agno/agno/models/ollama/tools.py\n+++ b/libs/agno/agno/models/ollama/tools.py\n@@ -136,24 +136,24 @@ class OllamaTools(Ollama):\n \n         return model_response\n \n-    def _create_function_call_result(\n-        self, fc: FunctionCall, success: bool, output: Optional[Union[List[Any], str]], timer: Timer\n+    def create_function_call_result(\n+        self, function_call: FunctionCall, success: bool, output: Optional[Union[List[Any], str]], timer: Timer\n     ) -> Message:\n         \"\"\"Create a function call result message.\"\"\"\n         content = (\n             \"<tool_response>\\n\"\n-            + json.dumps({\"name\": fc.function.name, \"content\": output if success else fc.error})\n+            + json.dumps({\"name\": function_call.function.name, \"content\": output if success else function_call.error})\n             + \"\\n</tool_response>\"\n         )\n \n         return Message(\n             role=self.tool_message_role,\n             content=content,\n-            tool_call_id=fc.call_id,\n-            tool_name=fc.function.name,\n-            tool_args=fc.arguments,\n+            tool_call_id=function_call.call_id,\n+            tool_name=function_call.function.name,\n+            tool_args=function_call.arguments,\n             tool_call_error=not success,\n-            stop_after_tool_call=fc.function.stop_after_tool_call,\n+            stop_after_tool_call=function_call.function.stop_after_tool_call,\n             metrics=MessageMetrics(time=timer.elapsed),\n         )\n \n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/models/openai/responses.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/responses.py b/libs/agno/agno/models/openai/responses.py\nindex 245139e82..31375d14d 100644\n--- a/libs/agno/agno/models/openai/responses.py\n+++ b/libs/agno/agno/models/openai/responses.py\n@@ -730,12 +730,20 @@ class OpenAIResponses(Model):\n             else:\n                 stream_data.response_citations.raw.append(stream_event.annotation)  # type: ignore\n \n-            if stream_event.annotation.type == \"url_citation\":\n-                if stream_data.response_citations.urls is None:\n-                    stream_data.response_citations.urls = []\n-                stream_data.response_citations.urls.append(\n-                    UrlCitation(url=stream_event.annotation.url, title=stream_event.annotation.title)\n-                )\n+            if isinstance(stream_event.annotation, dict):\n+                if stream_event.annotation.get(\"type\") == \"url_citation\":\n+                    if stream_data.response_citations.urls is None:\n+                        stream_data.response_citations.urls = []\n+                    stream_data.response_citations.urls.append(\n+                        UrlCitation(url=stream_event.annotation.get(\"url\"), title=stream_event.annotation.get(\"title\"))\n+                    )\n+            else:\n+                if stream_event.annotation.type == \"url_citation\":\n+                    if stream_data.response_citations.urls is None:\n+                        stream_data.response_citations.urls = []\n+                    stream_data.response_citations.urls.append(\n+                        UrlCitation(url=stream_event.annotation.url, title=stream_event.annotation.title)\n+                    )\n \n             model_response.citations = stream_data.response_citations\n \n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/models/response.py",
            "diff": "diff --git a/libs/agno/agno/models/response.py b/libs/agno/agno/models/response.py\nindex e8c144662..dcac17612 100644\n--- a/libs/agno/agno/models/response.py\n+++ b/libs/agno/agno/models/response.py\n@@ -1,4 +1,4 @@\n-from dataclasses import dataclass, field\n+from dataclasses import asdict, dataclass, field\n from enum import Enum\n from time import time\n from typing import Any, Dict, List, Optional\n@@ -35,6 +35,7 @@ class ToolExecution:\n \n     requires_confirmation: Optional[bool] = None\n     confirmed: Optional[bool] = None\n+    confirmation_note: Optional[str] = None\n \n     requires_user_input: Optional[bool] = None\n     user_input_schema: Optional[List[UserInputField]] = None\n@@ -45,6 +46,34 @@ class ToolExecution:\n     def is_paused(self) -> bool:\n         return bool(self.requires_confirmation or self.requires_user_input or self.external_execution_required)\n \n+    def to_dict(self) -> Dict[str, Any]:\n+        _dict = asdict(self)\n+        if self.metrics is not None:\n+            _dict[\"metrics\"] = self.metrics._to_dict()\n+\n+        if self.user_input_schema is not None:\n+            _dict[\"user_input_schema\"] = [field.to_dict() for field in self.user_input_schema]\n+\n+        return _dict\n+\n+    @classmethod\n+    def from_dict(cls, data: Dict[str, Any]) -> \"ToolExecution\":\n+        return cls(\n+            tool_call_id=data.get(\"tool_call_id\"),\n+            tool_name=data.get(\"tool_name\"),\n+            tool_args=data.get(\"tool_args\"),\n+            tool_call_error=data.get(\"tool_call_error\"),\n+            result=data.get(\"result\"),\n+            stop_after_tool_call=data.get(\"stop_after_tool_call\", False),\n+            requires_confirmation=data.get(\"requires_confirmation\"),\n+            confirmed=data.get(\"confirmed\"),\n+            confirmation_note=data.get(\"confirmation_note\"),\n+            requires_user_input=data.get(\"requires_user_input\"),\n+            user_input_schema=[UserInputField.from_dict(field) for field in data.get(\"user_input_schema\", [])],\n+            external_execution_required=data.get(\"external_execution_required\"),\n+            metrics=MessageMetrics(**data.get(\"metrics\", {})),\n+        )\n+\n \n @dataclass\n class ModelResponse:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/run/response.py",
            "diff": "diff --git a/libs/agno/agno/run/response.py b/libs/agno/agno/run/response.py\nindex 27e4f2c5d..d757d5992 100644\n--- a/libs/agno/agno/run/response.py\n+++ b/libs/agno/agno/run/response.py\n@@ -61,9 +61,6 @@ class RunResponseExtraData:\n         add_messages = data.pop(\"add_messages\", None)\n         add_messages = [Message.model_validate(message) for message in add_messages] if add_messages else None\n \n-        history = data.pop(\"history\", None)\n-        history = [Message.model_validate(message) for message in history] if history else None\n-\n         reasoning_steps = data.pop(\"reasoning_steps\", None)\n         reasoning_steps = [ReasoningStep.model_validate(step) for step in reasoning_steps] if reasoning_steps else None\n \n@@ -133,7 +130,7 @@ class RunResponse:\n             k: v\n             for k, v in asdict(self).items()\n             if v is not None\n-            and k not in [\"messages\", \"extra_data\", \"images\", \"videos\", \"audio\", \"response_audio\", \"citations\"]\n+            and k not in [\"messages\", \"tools\", \"extra_data\", \"images\", \"videos\", \"audio\", \"response_audio\", \"citations\"]\n         }\n         if self.messages is not None:\n             _dict[\"messages\"] = [m.to_dict() for m in self.messages]\n@@ -182,6 +179,14 @@ class RunResponse:\n         if self.content and isinstance(self.content, BaseModel):\n             _dict[\"content\"] = self.content.model_dump(exclude_none=True)\n \n+        if self.tools is not None:\n+            _dict[\"tools\"] = []\n+            for tool in self.tools:\n+                if isinstance(tool, ToolExecution):\n+                    _dict[\"tools\"].append(tool.to_dict())\n+                else:\n+                    _dict[\"tools\"].append(tool)\n+\n         return _dict\n \n     def to_json(self) -> str:\n@@ -200,7 +205,10 @@ class RunResponse:\n         messages = data.pop(\"messages\", None)\n         messages = [Message.model_validate(message) for message in messages] if messages else None\n \n-        return cls(messages=messages, **data)\n+        tools = data.pop(\"tools\", None)\n+        tools = [ToolExecution.from_dict(tool) for tool in tools] if tools else None\n+\n+        return cls(messages=messages, tools=tools, **data)\n \n     def get_content_as_string(self, **kwargs) -> str:\n         import json\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/run/team.py",
            "diff": "diff --git a/libs/agno/agno/run/team.py b/libs/agno/agno/run/team.py\nindex a396efb65..d1af4ec83 100644\n--- a/libs/agno/agno/run/team.py\n+++ b/libs/agno/agno/run/team.py\n@@ -51,7 +51,7 @@ class TeamRunResponse:\n             k: v\n             for k, v in asdict(self).items()\n             if v is not None\n-            and k not in [\"messages\", \"extra_data\", \"images\", \"videos\", \"audio\", \"response_audio\", \"citations\"]\n+            and k not in [\"messages\", \"tools\", \"extra_data\", \"images\", \"videos\", \"audio\", \"response_audio\", \"citations\"]\n         }\n         if self.messages is not None:\n             _dict[\"messages\"] = [m.to_dict() for m in self.messages]\n@@ -83,6 +83,14 @@ class TeamRunResponse:\n         if self.content and isinstance(self.content, BaseModel):\n             _dict[\"content\"] = self.content.model_dump(exclude_none=True)\n \n+        if self.tools is not None:\n+            _dict[\"tools\"] = []\n+            for tool in self.tools:\n+                if isinstance(tool, ToolExecution):\n+                    _dict[\"tools\"].append(tool.to_dict())\n+                else:\n+                    _dict[\"tools\"].append(tool)\n+\n         return _dict\n \n     def to_json(self) -> str:\n@@ -119,6 +127,9 @@ class TeamRunResponse:\n         audio = data.pop(\"audio\", None)\n         audio = [AudioArtifact.model_validate(audio) for audio in audio] if audio else None\n \n+        tools = data.pop(\"tools\", None)\n+        tools = [ToolExecution.from_dict(tool) for tool in tools] if tools else None\n+\n         response_audio = data.pop(\"response_audio\", None)\n         response_audio = AudioResponse.model_validate(response_audio) if response_audio else None\n \n@@ -130,6 +141,7 @@ class TeamRunResponse:\n             videos=videos,\n             audio=audio,\n             response_audio=response_audio,\n+            tools=tools,\n             **data,\n         )\n \n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/team/team.py",
            "diff": "diff --git a/libs/agno/agno/team/team.py b/libs/agno/agno/team/team.py\nindex b9b5ac33a..87ea92e8c 100644\n--- a/libs/agno/agno/team/team.py\n+++ b/libs/agno/agno/team/team.py\n@@ -5088,7 +5088,9 @@ class Team:\n                             and member_agent_run_response_chunk.tools is not None\n                             and len(member_agent_run_response_chunk.tools) > 0\n                         ):\n-                            yield \",\".join([tool.result for tool in member_agent_run_response_chunk.tools if tool.result])  # type: ignore\n+                            yield \",\".join(\n+                                [tool.result for tool in member_agent_run_response_chunk.tools if tool.result]\n+                            )  # type: ignore\n                 else:\n                     member_agent_run_response = member_agent.run(\n                         member_agent_task, images=images, videos=videos, audio=audio, files=files, stream=False\n@@ -5627,7 +5629,11 @@ class Team:\n                         yield member_agent_run_response.content\n \n                     # If the content is empty but we have tool calls\n-                    elif member_agent_run_response.event == RunEvent.tool_call_completed and member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n+                    elif (\n+                        member_agent_run_response.event == RunEvent.tool_call_completed\n+                        and member_agent_run_response.tools is not None\n+                        and len(member_agent_run_response.tools) > 0\n+                    ):\n                         yield \",\".join([tool.result for tool in member_agent_run_response.tools if tool.result])  # type: ignore\n                 elif issubclass(type(member_agent_run_response.content), BaseModel):\n                     try:\n@@ -5971,7 +5977,11 @@ class Team:\n                         yield member_agent_run_response.content\n \n                     # If the content is empty but we have tool calls\n-                    elif member_agent_run_response.event == RunEvent.tool_call_completed and member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n+                    elif (\n+                        member_agent_run_response.event == RunEvent.tool_call_completed\n+                        and member_agent_run_response.tools is not None\n+                        and len(member_agent_run_response.tools) > 0\n+                    ):\n                         yield \",\".join([tool.result for tool in member_agent_run_response.tools if tool.result])  # type: ignore\n                 elif issubclass(type(member_agent_run_response.content), BaseModel):\n                     try:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/agentql.py",
            "diff": "diff --git a/libs/agno/agno/tools/agentql.py b/libs/agno/agno/tools/agentql.py\nindex b9bd3ecdc..639e4e9e5 100644\n--- a/libs/agno/agno/tools/agentql.py\n+++ b/libs/agno/agno/tools/agentql.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_info\n@@ -13,20 +13,20 @@ except ImportError:\n \n class AgentQLTools(Toolkit):\n     def __init__(self, api_key: Optional[str] = None, scrape: bool = True, agentql_query: str = \"\", **kwargs):\n-        super().__init__(name=\"agentql_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"AGENTQL_API_KEY\")\n         if not self.api_key:\n             raise ValueError(\"AGENTQL_API_KEY not set. Please set the AGENTQL_API_KEY environment variable.\")\n \n         self.agentql_query = agentql_query\n \n+        tools: List[Any] = []\n         if scrape:\n-            self.register(self.scrape_website)\n-\n+            tools.append(self.scrape_website)\n         if agentql_query:\n             log_info(\"Custom AgentQL query provided. Registering custom scrape function.\")\n-            self.register(self.custom_scrape_website)\n+            tools.append(self.custom_scrape_website)\n+\n+        super().__init__(name=\"agentql_tools\", tools=tools, **kwargs)\n \n     def scrape_website(self, url: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/airflow.py",
            "diff": "diff --git a/libs/agno/agno/tools/airflow.py b/libs/agno/agno/tools/airflow.py\nindex d30bf9ec4..3647503e8 100644\n--- a/libs/agno/agno/tools/airflow.py\n+++ b/libs/agno/agno/tools/airflow.py\n@@ -1,5 +1,5 @@\n from pathlib import Path\n-from typing import Optional, Union\n+from typing import Any, List, Optional, Union\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -12,7 +12,6 @@ class AirflowTools(Toolkit):\n         \"\"\"\n         quick start to work with airflow : https://airflow.apache.org/docs/apache-airflow/stable/start.html\n         \"\"\"\n-        super().__init__(name=\"AirflowTools\", **kwargs)\n \n         _dags_dir: Optional[Path] = None\n         if dags_dir is not None:\n@@ -21,10 +20,14 @@ class AirflowTools(Toolkit):\n             else:\n                 _dags_dir = dags_dir\n         self.dags_dir: Path = _dags_dir or Path.cwd()\n+\n+        tools: List[Any] = []\n         if save_dag:\n-            self.register(self.save_dag_file, sanitize_arguments=False)\n+            tools.append(self.save_dag_file)\n         if read_dag:\n-            self.register(self.read_dag_file)\n+            tools.append(self.read_dag_file)\n+\n+        super().__init__(name=\"AirflowTools\", tools=tools, **kwargs)\n \n     def save_dag_file(self, contents: str, dag_file: str) -> str:\n         \"\"\"Saves python code for an Airflow DAG to a file called `dag_file` and returns the file path if successful.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/api.py",
            "diff": "diff --git a/libs/agno/agno/tools/api.py b/libs/agno/agno/tools/api.py\nindex bf844f104..93403ed81 100644\n--- a/libs/agno/agno/tools/api.py\n+++ b/libs/agno/agno/tools/api.py\n@@ -1,5 +1,5 @@\n import json\n-from typing import Any, Dict, Literal, Optional\n+from typing import Any, Dict, List, Literal, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -24,8 +24,6 @@ class CustomApiTools(Toolkit):\n         make_request: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"api_tools\", **kwargs)\n-\n         self.base_url = base_url\n         self.username = username\n         self.password = password\n@@ -34,8 +32,11 @@ class CustomApiTools(Toolkit):\n         self.verify_ssl = verify_ssl\n         self.timeout = timeout\n \n+        tools: List[Any] = []\n         if make_request:\n-            self.register(self.make_request)\n+            tools.append(self.make_request)\n+\n+        super().__init__(name=\"api_tools\", tools=tools, **kwargs)\n \n     def _get_auth(self) -> Optional[HTTPBasicAuth]:\n         \"\"\"Get authentication object if credentials are provided.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/apify.py",
            "diff": "diff --git a/libs/agno/agno/tools/apify.py b/libs/agno/agno/tools/apify.py\nindex df86f12bf..4394bd6c7 100644\n--- a/libs/agno/agno/tools/apify.py\n+++ b/libs/agno/agno/tools/apify.py\n@@ -66,8 +66,6 @@ class ApifyTools(Toolkit):\n                 markdown=True\n             )\n         \"\"\"\n-        super().__init__(name=\"ApifyTools\")\n-\n         # Get API token from args or environment\n         self.apify_api_token = apify_api_token or os.getenv(\"APIFY_API_TOKEN\")\n         if not self.apify_api_token:\n@@ -75,11 +73,13 @@ class ApifyTools(Toolkit):\n \n         self.client = create_apify_client(self.apify_api_token)\n \n-        # Register specific Actors if provided\n+        tools: List[Any] = []\n         if actors:\n             actor_list = [actors] if isinstance(actors, str) else actors\n             for actor_id in actor_list:\n-                self.register_actor(actor_id)\n+                tools.append(actor_id)\n+\n+        super().__init__(name=\"ApifyTools\", tools=tools)\n \n     def register_actor(self, actor_id: str) -> None:\n         \"\"\"Register an Apify Actor as a function in the toolkit.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/arxiv.py",
            "diff": "diff --git a/libs/agno/agno/tools/arxiv.py b/libs/agno/agno/tools/arxiv.py\nindex 939d8c820..34e86b2cc 100644\n--- a/libs/agno/agno/tools/arxiv.py\n+++ b/libs/agno/agno/tools/arxiv.py\n@@ -20,15 +20,16 @@ class ArxivTools(Toolkit):\n     def __init__(\n         self, search_arxiv: bool = True, read_arxiv_papers: bool = True, download_dir: Optional[Path] = None, **kwargs\n     ):\n-        super().__init__(name=\"arxiv_tools\", **kwargs)\n-\n         self.client: arxiv.Client = arxiv.Client()\n         self.download_dir: Path = download_dir or Path(__file__).parent.joinpath(\"arxiv_pdfs\")\n \n+        tools: List[Any] = []\n         if search_arxiv:\n-            self.register(self.search_arxiv_and_return_articles)\n+            tools.append(self.search_arxiv_and_return_articles)\n         if read_arxiv_papers:\n-            self.register(self.read_arxiv_papers)\n+            tools.append(self.read_arxiv_papers)\n+\n+        super().__init__(name=\"arxiv_tools\", tools=tools, **kwargs)\n \n     def search_arxiv_and_return_articles(self, query: str, num_articles: int = 10) -> str:\n         \"\"\"Use this function to search arXiv for a query and return the top articles.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/aws_lambda.py",
            "diff": "diff --git a/libs/agno/agno/tools/aws_lambda.py b/libs/agno/agno/tools/aws_lambda.py\nindex ac7ee8bb8..fedc62600 100644\n--- a/libs/agno/agno/tools/aws_lambda.py\n+++ b/libs/agno/agno/tools/aws_lambda.py\n@@ -1,3 +1,5 @@\n+from typing import Any, List\n+\n from agno.tools import Toolkit\n \n try:\n@@ -10,11 +12,18 @@ class AWSLambdaTools(Toolkit):\n     name: str = \"AWSLambdaTool\"\n     description: str = \"A tool for interacting with AWS Lambda functions\"\n \n-    def __init__(self, region_name: str = \"us-east-1\", **kwargs):\n-        super().__init__(name=\"aws-lambda\", **kwargs)\n+    def __init__(\n+        self,\n+        region_name: str = \"us-east-1\",\n+        **kwargs,\n+    ):\n         self.client = boto3.client(\"lambda\", region_name=region_name)\n-        self.register(self.list_functions)\n-        self.register(self.invoke_function)\n+\n+        tools: List[Any] = []\n+        tools.append(self.list_functions)\n+        tools.append(self.invoke_function)\n+\n+        super().__init__(name=\"aws-lambda\", tools=tools, **kwargs)\n \n     def list_functions(self) -> str:\n         try:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/baidusearch.py",
            "diff": "diff --git a/libs/agno/agno/tools/baidusearch.py b/libs/agno/agno/tools/baidusearch.py\nindex beaeec87c..3988e1f81 100644\n--- a/libs/agno/agno/tools/baidusearch.py\n+++ b/libs/agno/agno/tools/baidusearch.py\n@@ -37,14 +37,17 @@ class BaiduSearchTools(Toolkit):\n         debug: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"baidusearch\", **kwargs)\n         self.fixed_max_results = fixed_max_results\n         self.fixed_language = fixed_language\n         self.headers = headers\n         self.proxy = proxy\n         self.timeout = timeout\n         self.debug = debug\n-        self.register(self.baidu_search)\n+\n+        tools = []\n+        tools.append(self.baidu_search)\n+\n+        super().__init__(name=\"baidusearch\", tools=tools, **kwargs)\n \n     def baidu_search(self, query: str, max_results: int = 5, language: str = \"zh\") -> str:\n         \"\"\"Execute Baidu search and return results\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/browserbase.py",
            "diff": "diff --git a/libs/agno/agno/tools/browserbase.py b/libs/agno/agno/tools/browserbase.py\nindex ff490f85c..3f6dc3409 100644\n--- a/libs/agno/agno/tools/browserbase.py\n+++ b/libs/agno/agno/tools/browserbase.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -33,8 +33,6 @@ class BrowserbaseTools(Toolkit):\n             project_id (str, optional): Browserbase project ID.\n             base_url (str, optional): Custom Browserbase API endpoint URL (NOT the target website URL). Only use this if you're using a self-hosted Browserbase instance or need to connect to a different region.\n         \"\"\"\n-        super().__init__(name=\"browserbase_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"BROWSERBASE_API_KEY\")\n         if not self.api_key:\n             raise ValueError(\n@@ -62,10 +60,13 @@ class BrowserbaseTools(Toolkit):\n         self._session = None\n         self._connect_url = None\n \n-        self.register(self.navigate_to)\n-        self.register(self.screenshot)\n-        self.register(self.get_page_content)\n-        self.register(self.close_session)\n+        tools: List[Any] = []\n+        tools.append(self.navigate_to)\n+        tools.append(self.screenshot)\n+        tools.append(self.get_page_content)\n+        tools.append(self.close_session)\n+\n+        super().__init__(name=\"browserbase_tools\", tools=tools, **kwargs)\n \n     def _ensure_session(self):\n         \"\"\"Ensures a session exists, creating one if needed.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/calcom.py",
            "diff": "diff --git a/libs/agno/agno/tools/calcom.py b/libs/agno/agno/tools/calcom.py\nindex b68a0478f..f49c717ff 100644\n--- a/libs/agno/agno/tools/calcom.py\n+++ b/libs/agno/agno/tools/calcom.py\n@@ -1,6 +1,6 @@\n from datetime import datetime\n from os import getenv\n-from typing import Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import logger\n@@ -32,7 +32,6 @@ class CalComTools(Toolkit):\n             event_type_id: Default event type ID for bookings\n             user_timezone: User's timezone in IANA format (e.g., 'Asia/Kolkata')\n         \"\"\"\n-        super().__init__(name=\"calcom\", **kwargs)\n \n         # Get credentials from environment if not provided\n         self.api_key = api_key or getenv(\"CALCOM_API_KEY\")\n@@ -49,17 +48,19 @@ class CalComTools(Toolkit):\n \n         self.user_timezone = user_timezone or \"America/New_York\"\n \n-        # Register all methods\n+        tools: List[Any] = []\n         if get_available_slots:\n-            self.register(self.get_available_slots)\n+            tools.append(self.get_available_slots)\n         if create_booking:\n-            self.register(self.create_booking)\n+            tools.append(self.create_booking)\n         if get_upcoming_bookings:\n-            self.register(self.get_upcoming_bookings)\n+            tools.append(self.get_upcoming_bookings)\n         if reschedule_booking:\n-            self.register(self.reschedule_booking)\n+            tools.append(self.reschedule_booking)\n         if cancel_booking:\n-            self.register(self.cancel_booking)\n+            tools.append(self.cancel_booking)\n+\n+        super().__init__(name=\"calcom\", tools=tools, **kwargs)\n \n     def _convert_to_user_timezone(self, utc_time: str) -> str:\n         \"\"\"Convert UTC time to user's timezone.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/cartesia.py",
            "diff": "diff --git a/libs/agno/agno/tools/cartesia.py b/libs/agno/agno/tools/cartesia.py\nindex d982cf885..ba510e227 100644\n--- a/libs/agno/agno/tools/cartesia.py\n+++ b/libs/agno/agno/tools/cartesia.py\n@@ -1,7 +1,7 @@\n import json\n from base64 import b64encode\n from os import getenv\n-from typing import Any, Dict, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n from uuid import uuid4\n \n from agno.agent import Agent\n@@ -25,9 +25,8 @@ class CartesiaTools(Toolkit):\n         text_to_speech_enabled: bool = True,\n         list_voices_enabled: bool = True,\n         voice_localize_enabled: bool = False,\n+        **kwargs,\n     ):\n-        super().__init__(name=\"cartesia_tools\")\n-\n         self.api_key = api_key or getenv(\"CARTESIA_API_KEY\")\n \n         if not self.api_key:\n@@ -37,12 +36,15 @@ class CartesiaTools(Toolkit):\n         self.model_id = model_id\n         self.default_voice_id = default_voice_id\n \n+        tools: List[Any] = []\n         if voice_localize_enabled:\n-            self.register(self.localize_voice)\n+            tools.append(self.localize_voice)\n         if text_to_speech_enabled:\n-            self.register(self.text_to_speech)\n+            tools.append(self.text_to_speech)\n         if list_voices_enabled:\n-            self.register(self.list_voices)\n+            tools.append(self.list_voices)\n+\n+        super().__init__(name=\"cartesia_tools\", tools=tools, **kwargs)\n \n     def list_voices(self) -> str:\n         \"\"\"List available voices from Cartesia (first page).\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/clickup_tool.py",
            "diff": "diff --git a/libs/agno/agno/tools/clickup_tool.py b/libs/agno/agno/tools/clickup_tool.py\nindex 13f05c6ad..ffcd3e040 100644\n--- a/libs/agno/agno/tools/clickup_tool.py\n+++ b/libs/agno/agno/tools/clickup_tool.py\n@@ -26,8 +26,6 @@ class ClickUpTools(Toolkit):\n         list_lists: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"clickup\", **kwargs)\n-\n         self.api_key = api_key or os.getenv(\"CLICKUP_API_KEY\")\n         self.master_space_id = master_space_id or os.getenv(\"MASTER_SPACE_ID\")\n         self.base_url = \"https://api.clickup.com/api/v2\"\n@@ -38,20 +36,23 @@ class ClickUpTools(Toolkit):\n         if not self.master_space_id:\n             raise ValueError(\"MASTER_SPACE_ID not set. Please set the MASTER_SPACE_ID environment variable.\")\n \n+        tools: List[Any] = []\n         if list_tasks:\n-            self.register(self.list_tasks)\n+            tools.append(self.list_tasks)\n         if create_task:\n-            self.register(self.create_task)\n+            tools.append(self.create_task)\n         if get_task:\n-            self.register(self.get_task)\n+            tools.append(self.get_task)\n         if update_task:\n-            self.register(self.update_task)\n+            tools.append(self.update_task)\n         if delete_task:\n-            self.register(self.delete_task)\n+            tools.append(self.delete_task)\n         if list_spaces:\n-            self.register(self.list_spaces)\n+            tools.append(self.list_spaces)\n         if list_lists:\n-            self.register(self.list_lists)\n+            tools.append(self.list_lists)\n+\n+        super().__init__(name=\"clickup\", tools=tools, **kwargs)\n \n     def _make_request(\n         self, method: str, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/confluence.py",
            "diff": "diff --git a/libs/agno/agno/tools/confluence.py b/libs/agno/agno/tools/confluence.py\nindex eafabc1c6..e52233d69 100644\n--- a/libs/agno/agno/tools/confluence.py\n+++ b/libs/agno/agno/tools/confluence.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_info, logger\n@@ -63,12 +63,15 @@ class ConfluenceTools(Toolkit):\n \n             urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n \n-        self.register(self.get_page_content)\n-        self.register(self.get_space_key)\n-        self.register(self.create_page)\n-        self.register(self.update_page)\n-        self.register(self.get_all_space_detail)\n-        self.register(self.get_all_page_from_space)\n+        tools: List[Any] = []\n+        tools.append(self.get_page_content)\n+        tools.append(self.get_space_key)\n+        tools.append(self.create_page)\n+        tools.append(self.update_page)\n+        tools.append(self.get_all_space_detail)\n+        tools.append(self.get_all_page_from_space)\n+\n+        super().__init__(name=\"confluence_tools\", tools=tools, **kwargs)\n \n     def get_page_content(self, space_name: str, page_title: str, expand: Optional[str] = \"body.storage\"):\n         \"\"\"Retrieve the content of a specific page in a Confluence space.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/crawl4ai.py",
            "diff": "diff --git a/libs/agno/agno/tools/crawl4ai.py b/libs/agno/agno/tools/crawl4ai.py\nindex ab8bfa392..8d1f4e990 100644\n--- a/libs/agno/agno/tools/crawl4ai.py\n+++ b/libs/agno/agno/tools/crawl4ai.py\n@@ -1,5 +1,5 @@\n import asyncio\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n \n@@ -15,11 +15,13 @@ class Crawl4aiTools(Toolkit):\n         max_length: Optional[int] = 1000,\n         **kwargs,\n     ):\n-        super().__init__(name=\"crawl4ai_tools\", **kwargs)\n-\n         self.max_length = max_length\n \n-        self.register(self.web_crawler)\n+        tools: List[Any] = []\n+        tools.append(self.web_crawler)\n+\n+        # Call superclass with tools list\n+        super().__init__(name=\"crawl4ai_tools\", tools=tools, **kwargs)\n \n     def web_crawler(self, url: str, max_length: Optional[int] = None) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/csv_toolkit.py",
            "diff": "diff --git a/libs/agno/agno/tools/csv_toolkit.py b/libs/agno/agno/tools/csv_toolkit.py\nindex 937b61799..1c07f3941 100644\n--- a/libs/agno/agno/tools/csv_toolkit.py\n+++ b/libs/agno/agno/tools/csv_toolkit.py\n@@ -20,8 +20,6 @@ class CsvTools(Toolkit):\n         duckdb_kwargs: Optional[Dict[str, Any]] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"csv_tools\", **kwargs)\n-\n         self.csvs: List[Path] = []\n         if csvs:\n             for _csv in csvs:\n@@ -35,18 +33,22 @@ class CsvTools(Toolkit):\n         self.duckdb_connection: Optional[Any] = duckdb_connection\n         self.duckdb_kwargs: Optional[Dict[str, Any]] = duckdb_kwargs\n \n+        tools: List[Any] = []\n         if read_csvs:\n-            self.register(self.read_csv_file)\n+            tools.append(self.read_csv_file)\n         if list_csvs:\n-            self.register(self.list_csv_files)\n+            tools.append(self.list_csv_files)\n         if read_column_names:\n-            self.register(self.get_columns)\n+            tools.append(self.get_columns)\n         if query_csvs:\n             try:\n                 import duckdb  # noqa: F401\n+\n+                tools.append(self.query_csv_file)\n             except ImportError:\n-                raise ImportError(\"`duckdb` not installed. Please install using `pip install duckdb`.\")\n-            self.register(self.query_csv_file)\n+                logger.warning(\"`duckdb` not installed. Query functionality disabled.\")\n+\n+        super().__init__(name=\"csv_tools\", tools=tools, **kwargs)\n \n     def list_csv_files(self) -> str:\n         \"\"\"Returns a list of available csv files\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/dalle.py",
            "diff": "diff --git a/libs/agno/agno/tools/dalle.py b/libs/agno/agno/tools/dalle.py\nindex 37120a8c7..28633a70c 100644\n--- a/libs/agno/agno/tools/dalle.py\n+++ b/libs/agno/agno/tools/dalle.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Literal, Optional, Union\n+from typing import Any, List, Literal, Optional, Union\n from uuid import uuid4\n \n from agno.agent import Agent\n@@ -26,8 +26,6 @@ class DalleTools(Toolkit):\n         api_key: Optional[str] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"dalle\", **kwargs)\n-\n         self.model = model\n         self.n = n\n         self.size = size\n@@ -52,7 +50,11 @@ class DalleTools(Toolkit):\n         if not self.api_key:\n             logger.error(\"OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.\")\n \n-        self.register(self.create_image)\n+        tools: List[Any] = []\n+        tools.append(self.create_image)\n+\n+        super().__init__(name=\"dalle\", tools=tools, **kwargs)\n+\n         # TODO:\n         # - Add support for response_format\n         # - Add support for saving images\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/desi_vocal.py",
            "diff": "diff --git a/libs/agno/agno/tools/desi_vocal.py b/libs/agno/agno/tools/desi_vocal.py\nindex bc970d337..70d24a1be 100644\n--- a/libs/agno/agno/tools/desi_vocal.py\n+++ b/libs/agno/agno/tools/desi_vocal.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Optional, Union\n+from typing import Any, List, Optional, Union\n from uuid import uuid4\n \n import requests\n@@ -18,16 +18,17 @@ class DesiVocalTools(Toolkit):\n         voice_id: Optional[str] = \"f27d74e5-ea71-4697-be3e-f04bbd80c1a8\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"desi_vocal_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"DESI_VOCAL_API_KEY\")\n         if not self.api_key:\n             logger.error(\"DESI_VOCAL_API_KEY not set. Please set the DESI_VOCAL_API_KEY environment variable.\")\n \n         self.voice_id = voice_id\n \n-        self.register(self.get_voices)\n-        self.register(self.text_to_speech)\n+        tools: List[Any] = []\n+        tools.append(self.get_voices)\n+        tools.append(self.text_to_speech)\n+\n+        super().__init__(name=\"desi_vocal_tools\", tools=tools, **kwargs)\n \n     def get_voices(self) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/discord.py",
            "diff": "diff --git a/libs/agno/agno/tools/discord.py b/libs/agno/agno/tools/discord.py\nindex bc7ba7cfc..b4194433e 100644\n--- a/libs/agno/agno/tools/discord.py\n+++ b/libs/agno/agno/tools/discord.py\n@@ -2,7 +2,7 @@\n \n import json\n from os import getenv\n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n import requests\n \n@@ -20,9 +20,6 @@ class DiscordTools(Toolkit):\n         enable_message_management: bool = True,\n         **kwargs,\n     ):\n-        \"\"\"Initialize Discord tools.\"\"\"\n-        super().__init__(name=\"discord\", **kwargs)\n-\n         self.bot_token = bot_token or getenv(\"DISCORD_BOT_TOKEN\")\n         if not self.bot_token:\n             logger.error(\"Discord bot token is required\")\n@@ -34,16 +31,18 @@ class DiscordTools(Toolkit):\n             \"Content-Type\": \"application/json\",\n         }\n \n-        # Register tools based on enabled features\n+        tools: List[Any] = []\n         if enable_messaging:\n-            self.register(self.send_message)\n+            tools.append(self.send_message)\n         if enable_history:\n-            self.register(self.get_channel_messages)\n+            tools.append(self.get_channel_messages)\n         if enable_channel_management:\n-            self.register(self.get_channel_info)\n-            self.register(self.list_channels)\n+            tools.append(self.get_channel_info)\n+            tools.append(self.list_channels)\n         if enable_message_management:\n-            self.register(self.delete_message)\n+            tools.append(self.delete_message)\n+\n+        super().__init__(name=\"discord\", tools=tools, **kwargs)\n \n     def _make_request(self, method: str, endpoint: str, data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n         \"\"\"Make a request to Discord API.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/docker.py",
            "diff": "diff --git a/libs/agno/agno/tools/docker.py b/libs/agno/agno/tools/docker.py\nindex 558d1b770..e13c709ef 100644\n--- a/libs/agno/agno/tools/docker.py\n+++ b/libs/agno/agno/tools/docker.py\n@@ -1,7 +1,7 @@\n import json\n import os\n import sys\n-from typing import Dict, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n \n from agno.tools import Toolkit\n from agno.utils.log import logger\n@@ -64,9 +64,6 @@ class DockerTools(Toolkit):\n         enable_network_management: bool = False,\n         **kwargs,\n     ):\n-        \"\"\"Initialize Docker tools.\"\"\"\n-        super().__init__(name=\"docker_tools\", **kwargs)\n-\n         self._check_docker_availability()\n \n         try:\n@@ -83,38 +80,46 @@ class DockerTools(Toolkit):\n         except Exception as e:\n             logger.error(f\"Error connecting to Docker: {e}\")\n \n-        # Register tools based on enabled features\n+        tools: List[Any] = []\n         if enable_container_management:\n-            self.register(self.list_containers)\n-            self.register(self.start_container)\n-            self.register(self.stop_container)\n-            self.register(self.remove_container)\n-            self.register(self.get_container_logs)\n-            self.register(self.inspect_container)\n-            self.register(self.run_container)\n-            self.register(self.exec_in_container)\n-\n+            tools.extend(\n+                [\n+                    self.list_containers,\n+                    self.start_container,\n+                    self.stop_container,\n+                    self.remove_container,\n+                    self.get_container_logs,\n+                    self.inspect_container,\n+                    self.run_container,\n+                    self.exec_in_container,\n+                ]\n+            )\n         if enable_image_management:\n-            self.register(self.list_images)\n-            self.register(self.pull_image)\n-            self.register(self.remove_image)\n-            self.register(self.build_image)\n-            self.register(self.tag_image)\n-            self.register(self.inspect_image)\n-\n+            tools.extend(\n+                [\n+                    self.list_images,\n+                    self.pull_image,\n+                    self.remove_image,\n+                    self.build_image,\n+                    self.tag_image,\n+                    self.inspect_image,\n+                ]\n+            )\n         if enable_volume_management:\n-            self.register(self.list_volumes)\n-            self.register(self.create_volume)\n-            self.register(self.remove_volume)\n-            self.register(self.inspect_volume)\n-\n+            tools.extend([self.list_volumes, self.create_volume, self.remove_volume, self.inspect_volume])\n         if enable_network_management:\n-            self.register(self.list_networks)\n-            self.register(self.create_network)\n-            self.register(self.remove_network)\n-            self.register(self.inspect_network)\n-            self.register(self.connect_container_to_network)\n-            self.register(self.disconnect_container_from_network)\n+            tools.extend(\n+                [\n+                    self.list_networks,\n+                    self.create_network,\n+                    self.remove_network,\n+                    self.inspect_network,\n+                    self.connect_container_to_network,\n+                    self.disconnect_container_from_network,\n+                ]\n+            )\n+\n+        super().__init__(name=\"docker_tools\", tools=tools, **kwargs)\n \n     def _check_docker_availability(self):\n         \"\"\"Check if Docker socket exists and is accessible.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/duckdb.py",
            "diff": "diff --git a/libs/agno/agno/tools/duckdb.py b/libs/agno/agno/tools/duckdb.py\nindex bb850bbc9..ccda52aca 100644\n--- a/libs/agno/agno/tools/duckdb.py\n+++ b/libs/agno/agno/tools/duckdb.py\n@@ -24,26 +24,28 @@ class DuckDbTools(Toolkit):\n         export_tables: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"duckdb_tools\", **kwargs)\n-\n         self.db_path: Optional[str] = db_path\n         self.read_only: bool = read_only\n         self.config: Optional[dict] = config\n         self._connection: Optional[duckdb.DuckDBPyConnection] = connection\n         self.init_commands: Optional[List] = init_commands\n \n-        self.register(self.show_tables)\n-        self.register(self.describe_table)\n+        tools: List[Any] = []\n+        tools.append(self.show_tables)\n+        tools.append(self.describe_table)\n+\n         if inspect_queries:\n-            self.register(self.inspect_query)\n+            tools.append(self.inspect_query)\n         if run_queries:\n-            self.register(self.run_query)\n+            tools.append(self.run_query)\n         if create_tables:\n-            self.register(self.create_table_from_path)\n+            tools.append(self.create_table_from_path)\n         if summarize_tables:\n-            self.register(self.summarize_table)\n+            tools.append(self.summarize_table)\n         if export_tables:\n-            self.register(self.export_table_to_path)\n+            tools.append(self.export_table_to_path)\n+\n+        super().__init__(name=\"duckdb_tools\", tools=tools, **kwargs)\n \n     @property\n     def connection(self) -> duckdb.DuckDBPyConnection:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/duckduckgo.py",
            "diff": "diff --git a/libs/agno/agno/tools/duckduckgo.py b/libs/agno/agno/tools/duckduckgo.py\nindex 7edf74f11..a1fa465f7 100644\n--- a/libs/agno/agno/tools/duckduckgo.py\n+++ b/libs/agno/agno/tools/duckduckgo.py\n@@ -1,5 +1,5 @@\n import json\n-from typing import Any, Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug\n@@ -46,7 +46,7 @@ class DuckDuckGoTools(Toolkit):\n         self.modifier: Optional[str] = modifier\n         self.verify_ssl: bool = verify_ssl\n \n-        tools = []\n+        tools: List[Any] = []\n         if search:\n             tools.append(self.duckduckgo_search)\n         if news:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/e2b.py",
            "diff": "diff --git a/libs/agno/agno/tools/e2b.py b/libs/agno/agno/tools/e2b.py\nindex a2898b6dd..3809c9217 100644\n--- a/libs/agno/agno/tools/e2b.py\n+++ b/libs/agno/agno/tools/e2b.py\n@@ -5,7 +5,7 @@ import tempfile\n import time\n from os import fdopen, getenv\n from pathlib import Path\n-from typing import Any, Callable, Dict, Optional, Union\n+from typing import Any, Callable, Dict, List, Optional, Union\n from uuid import uuid4\n \n from agno.agent import Agent\n@@ -48,7 +48,6 @@ class E2BTools(Toolkit):\n             timeout: Timeout in seconds for the sandbox (default: 5 minutes)\n             sandbox_options: Additional options to pass to the Sandbox constructor\n         \"\"\"\n-        super().__init__(name=\"e2b_tools\", **kwargs)\n \n         self.api_key = api_key or getenv(\"E2B_API_KEY\")\n         if not self.api_key:\n@@ -68,39 +67,28 @@ class E2BTools(Toolkit):\n         self.last_execution = None\n         self.downloaded_files: Dict[int, str] = {}\n \n-        # Register the functions based on the parameters\n-        if run_code:\n-            self.register(self.run_python_code)\n+        tools: List[Any] = []\n \n+        if run_code:\n+            tools.append(self.run_python_code)\n         if upload_file:\n-            self.register(self.upload_file)\n-\n+            tools.append(self.upload_file)\n         if download_result:\n-            self.register(self.download_png_result)\n-            self.register(self.download_chart_data)\n-            self.register(self.download_file_from_sandbox)\n-\n+            tools.extend([self.download_png_result, self.download_chart_data, self.download_file_from_sandbox])\n         if filesystem:\n-            self.register(self.list_files)\n-            self.register(self.read_file_content)\n-            self.register(self.write_file_content)\n-            self.register(self.watch_directory)\n-\n+            tools.extend([self.list_files, self.read_file_content, self.write_file_content, self.watch_directory])\n         if internet_access:\n-            self.register(self.get_public_url)\n-            self.register(self.run_server)\n-\n+            tools.extend([self.get_public_url, self.run_server])\n         if sandbox_management:\n-            self.register(self.set_sandbox_timeout)\n-            self.register(self.get_sandbox_status)\n-            self.register(self.shutdown_sandbox)\n-            self.register(self.list_running_sandboxes)\n-\n+            tools.extend(\n+                [self.set_sandbox_timeout, self.get_sandbox_status, self.shutdown_sandbox, self.list_running_sandboxes]\n+            )\n         if command_execution:\n-            self.register(self.run_command)\n-            self.register(self.stream_command)\n-            self.register(self.run_background_command)\n-            self.register(self.kill_background_command)\n+            tools.extend(\n+                [self.run_command, self.stream_command, self.run_background_command, self.kill_background_command]\n+            )\n+\n+        super().__init__(name=\"e2b_tools\", tools=tools, **kwargs)\n \n     # Code Execution Functions\n     def run_python_code(self, code: str) -> str:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/eleven_labs.py",
            "diff": "diff --git a/libs/agno/agno/tools/eleven_labs.py b/libs/agno/agno/tools/eleven_labs.py\nindex 17b9ce6e2..3094541ad 100644\n--- a/libs/agno/agno/tools/eleven_labs.py\n+++ b/libs/agno/agno/tools/eleven_labs.py\n@@ -2,7 +2,7 @@ from base64 import b64encode\n from io import BytesIO\n from os import getenv, path\n from pathlib import Path\n-from typing import Iterator, Literal, Optional, Union\n+from typing import Any, Iterator, List, Literal, Optional, Union\n from uuid import uuid4\n \n from agno.agent import Agent\n@@ -41,8 +41,6 @@ class ElevenLabsTools(Toolkit):\n         output_format: ElevenLabsAudioOutputFormat = \"mp3_44100_64\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"elevenlabs_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"ELEVEN_LABS_API_KEY\")\n         if not self.api_key:\n             logger.error(\"ELEVEN_LABS_API_KEY not set. Please set the ELEVEN_LABS_API_KEY environment variable.\")\n@@ -57,9 +55,14 @@ class ElevenLabsTools(Toolkit):\n             target_path.mkdir(parents=True, exist_ok=True)\n \n         self.eleven_labs_client = ElevenLabs(api_key=self.api_key)\n-        self.register(self.get_voices)\n-        self.register(self.generate_sound_effect)\n-        self.register(self.text_to_speech)\n+\n+        tools: List[Any] = []\n+\n+        tools.append(self.get_voices)\n+        tools.append(self.generate_sound_effect)\n+        tools.append(self.text_to_speech)\n+\n+        super().__init__(name=\"elevenlabs_tools\", tools=tools, **kwargs)\n \n     def get_voices(self) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/email.py",
            "diff": "diff --git a/libs/agno/agno/tools/email.py b/libs/agno/agno/tools/email.py\nindex 1ea222f1d..a663067fa 100644\n--- a/libs/agno/agno/tools/email.py\n+++ b/libs/agno/agno/tools/email.py\n@@ -13,13 +13,16 @@ class EmailTools(Toolkit):\n         sender_passkey: Optional[str] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"email_tools\", **kwargs)\n-\n         self.receiver_email: Optional[str] = receiver_email\n         self.sender_name: Optional[str] = sender_name\n         self.sender_email: Optional[str] = sender_email\n         self.sender_passkey: Optional[str] = sender_passkey\n-        self.register(self.email_user)\n+\n+        tools = []\n+        tools.append(self.email_user)\n+\n+        # Call superclass with tools list\n+        super().__init__(name=\"email_tools\", tools=tools, **kwargs)\n \n     def email_user(self, subject: str, body: str, **kwargs) -> str:\n         \"\"\"Emails the user with the given subject and body.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/exa.py",
            "diff": "diff --git a/libs/agno/agno/tools/exa.py b/libs/agno/agno/tools/exa.py\nindex 3f55ee969..939f07361 100644\n--- a/libs/agno/agno/tools/exa.py\n+++ b/libs/agno/agno/tools/exa.py\n@@ -63,8 +63,6 @@ class ExaTools(Toolkit):\n         model: Optional[str] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"exa\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"EXA_API_KEY\")\n         if not self.api_key:\n             logger.error(\"EXA_API_KEY not set. Please set the EXA_API_KEY environment variable.\")\n@@ -89,14 +87,17 @@ class ExaTools(Toolkit):\n         self.exclude_domains: Optional[List[str]] = exclude_domains\n         self.model: Optional[str] = model\n \n+        tools: List[Any] = []\n         if search:\n-            self.register(self.search_exa)\n+            tools.append(self.search_exa)\n         if get_contents:\n-            self.register(self.get_contents)\n+            tools.append(self.get_contents)\n         if find_similar:\n-            self.register(self.find_similar)\n+            tools.append(self.find_similar)\n         if answer:\n-            self.register(self.exa_answer)\n+            tools.append(self.exa_answer)\n+\n+        super().__init__(name=\"exa\", tools=tools, **kwargs)\n \n     def _parse_results(self, exa_results: SearchResponse) -> str:\n         exa_results_parsed = []\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/fal.py",
            "diff": "diff --git a/libs/agno/agno/tools/fal.py b/libs/agno/agno/tools/fal.py\nindex bb1cbaf8a..db6da1640 100644\n--- a/libs/agno/agno/tools/fal.py\n+++ b/libs/agno/agno/tools/fal.py\n@@ -25,14 +25,14 @@ class FalTools(Toolkit):\n         model: str = \"fal-ai/hunyuan-video\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"fal\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"FAL_KEY\")\n         if not self.api_key:\n             logger.error(\"FAL_KEY not set. Please set the FAL_KEY environment variable.\")\n         self.model = model\n         self.seen_logs: set[str] = set()\n-        self.register(self.generate_media)\n+\n+        tools = []\n+        tools.append(self.generate_media)\n \n     def on_queue_update(self, update):\n         if isinstance(update, fal_client.InProgress) and update.logs:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/file.py",
            "diff": "diff --git a/libs/agno/agno/tools/file.py b/libs/agno/agno/tools/file.py\nindex 9839f0434..347701d22 100644\n--- a/libs/agno/agno/tools/file.py\n+++ b/libs/agno/agno/tools/file.py\n@@ -1,6 +1,6 @@\n import json\n from pathlib import Path\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -15,15 +15,17 @@ class FileTools(Toolkit):\n         list_files: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"file_tools\", **kwargs)\n-\n         self.base_dir: Path = base_dir or Path.cwd()\n+\n+        tools: List[Any] = []\n         if save_files:\n-            self.register(self.save_file, sanitize_arguments=False)\n+            tools.append(self.save_file)\n         if read_files:\n-            self.register(self.read_file)\n+            tools.append(self.read_file)\n         if list_files:\n-            self.register(self.list_files)\n+            tools.append(self.list_files)\n+\n+        super().__init__(name=\"file_tools\", tools=tools, **kwargs)\n \n     def save_file(self, contents: str, file_name: str, overwrite: bool = True) -> str:\n         \"\"\"Saves the contents to a file called `file_name` and returns the file name if successful.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/financial_datasets.py",
            "diff": "diff --git a/libs/agno/agno/tools/financial_datasets.py b/libs/agno/agno/tools/financial_datasets.py\nindex 5258a0d08..3770ad2e5 100644\n--- a/libs/agno/agno/tools/financial_datasets.py\n+++ b/libs/agno/agno/tools/financial_datasets.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n import requests\n \n@@ -35,7 +35,6 @@ class FinancialDatasetsTools(Toolkit):\n             enable_crypto: Enable cryptocurrency related functions\n             enable_search: Enable search related functions\n         \"\"\"\n-        super().__init__(name=\"financial_datasets_tools\", **kwargs)\n \n         self.api_key: Optional[str] = api_key or getenv(\"FINANCIAL_DATASETS_API_KEY\")\n         if not self.api_key:\n@@ -45,36 +44,34 @@ class FinancialDatasetsTools(Toolkit):\n \n         self.base_url = \"https://api.financialdatasets.ai\"\n \n-        # Register functions based on feature flags\n-        if enable_financial_statements:\n-            self.register(self.get_income_statements)\n-            self.register(self.get_balance_sheets)\n-            self.register(self.get_cash_flow_statements)\n-            self.register(self.get_segmented_financials)\n-            self.register(self.get_financial_metrics)\n+        tools: List[Any] = []\n \n+        if enable_financial_statements:\n+            tools.extend(\n+                [\n+                    self.get_income_statements,\n+                    self.get_balance_sheets,\n+                    self.get_cash_flow_statements,\n+                    self.get_segmented_financials,\n+                    self.get_financial_metrics,\n+                ]\n+            )\n         if enable_company_info:\n-            self.register(self.get_company_info)\n-\n+            tools.append(self.get_company_info)\n         if enable_market_data:\n-            self.register(self.get_stock_prices)\n-            self.register(self.get_earnings)\n-\n+            tools.extend([self.get_stock_prices, self.get_earnings])\n         if enable_ownership_data:\n-            self.register(self.get_insider_trades)\n-            self.register(self.get_institutional_ownership)\n-\n+            tools.extend([self.get_insider_trades, self.get_institutional_ownership])\n         if enable_news:\n-            self.register(self.get_news)\n-\n+            tools.append(self.get_news)\n         if enable_sec_filings:\n-            self.register(self.get_sec_filings)\n-\n+            tools.append(self.get_sec_filings)\n         if enable_crypto:\n-            self.register(self.get_crypto_prices)\n-\n+            tools.append(self.get_crypto_prices)\n         if enable_search:\n-            self.register(self.search_tickers)\n+            tools.append(self.search_tickers)\n+\n+        super().__init__(name=\"financial_datasets_tools\", tools=tools, **kwargs)\n \n     def _make_request(self, endpoint: str, params: Dict[str, Any]) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/firecrawl.py",
            "diff": "diff --git a/libs/agno/agno/tools/firecrawl.py b/libs/agno/agno/tools/firecrawl.py\nindex f1f662ebe..aec7b2f03 100644\n--- a/libs/agno/agno/tools/firecrawl.py\n+++ b/libs/agno/agno/tools/firecrawl.py\n@@ -39,21 +39,24 @@ class FirecrawlTools(Toolkit):\n         api_key: Optional[str] = None,\n         formats: Optional[List[str]] = None,\n         limit: int = 10,\n+        poll_interval: int = 30,\n         scrape: bool = True,\n         crawl: bool = False,\n         mapping: bool = False,\n+        search: bool = False,\n+        search_params: Optional[Dict[str, Any]] = None,\n         api_url: Optional[str] = \"https://api.firecrawl.dev\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"firecrawl_tools\", **kwargs)\n-\n         self.api_key: Optional[str] = api_key or getenv(\"FIRECRAWL_API_KEY\")\n         if not self.api_key:\n             logger.error(\"FIRECRAWL_API_KEY not set. Please set the FIRECRAWL_API_KEY environment variable.\")\n \n         self.formats: Optional[List[str]] = formats\n         self.limit: int = limit\n+        self.poll_interval: int = poll_interval\n         self.app: FirecrawlApp = FirecrawlApp(api_key=self.api_key, api_url=api_url)\n+        self.search_params = search_params\n \n         # Start with scrape by default. But if crawl is set, then set scrape to False.\n         if crawl:\n@@ -62,12 +65,17 @@ class FirecrawlTools(Toolkit):\n         elif not scrape:\n             crawl = True\n \n+        tools: List[Any] = []\n         if scrape:\n-            self.register(self.scrape_website)\n+            tools.append(self.scrape_website)\n         if crawl:\n-            self.register(self.crawl_website)\n+            tools.append(self.crawl_website)\n         if mapping:\n-            self.register(self.map_website)\n+            tools.append(self.map_website)\n+        if search:\n+            tools.append(self.search)\n+\n+        super().__init__(name=\"firecrawl_tools\", tools=tools, **kwargs)\n \n     def scrape_website(self, url: str) -> str:\n         \"\"\"Use this function to scrape a website using Firecrawl.\n@@ -98,7 +106,7 @@ class FirecrawlTools(Toolkit):\n         if self.formats:\n             params[\"scrape_options\"] = ScrapeOptions(formats=self.formats)  # type: ignore\n \n-        params[\"poll_interval\"] = 30\n+        params[\"poll_interval\"] = self.poll_interval\n \n         crawl_result = self.app.crawl_url(url, **params)\n         return json.dumps(crawl_result.model_dump(), cls=CustomJSONEncoder)\n@@ -112,3 +120,24 @@ class FirecrawlTools(Toolkit):\n         \"\"\"\n         map_result = self.app.map_url(url)\n         return json.dumps(map_result.model_dump(), cls=CustomJSONEncoder)\n+\n+    def search(self, query: str, limit: Optional[int] = None):\n+        \"\"\"Use this function to search for the web using Firecrawl.\n+\n+        Args:\n+            query (str): The query to search for.\n+            limit (int): The maximum number of results to return.\n+        \"\"\"\n+        params: Dict[str, Any] = {}\n+        if self.limit or limit:\n+            params[\"limit\"] = self.limit or limit\n+        if self.formats:\n+            params[\"scrape_options\"] = ScrapeOptions(formats=self.formats)  # type: ignore\n+        if self.search_params:\n+            params.update(self.search_params)\n+\n+        search_result = self.app.search(query, **params)\n+        if search_result.success:\n+            return json.dumps(search_result.data, cls=CustomJSONEncoder)\n+        else:\n+            return \"Error searching with the Firecrawl tool: \" + search_result.error\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/function.py",
            "diff": "diff --git a/libs/agno/agno/tools/function.py b/libs/agno/agno/tools/function.py\nindex 5304484c9..afadd4ad6 100644\n--- a/libs/agno/agno/tools/function.py\n+++ b/libs/agno/agno/tools/function.py\n@@ -40,6 +40,23 @@ class UserInputField:\n     description: Optional[str] = None\n     value: Optional[Any] = None\n \n+    def to_dict(self) -> Dict[str, Any]:\n+        return {\n+            \"name\": self.name,\n+            \"field_type\": str(self.field_type.__name__),\n+            \"description\": self.description,\n+            \"value\": self.value,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: Dict[str, Any]) -> \"UserInputField\":\n+        return cls(\n+            name=data[\"name\"],\n+            field_type=eval(data[\"field_type\"]),  # Convert string type name to actual type\n+            description=data[\"description\"],\n+            value=data[\"value\"],\n+        )\n+\n \n class Function(BaseModel):\n     \"\"\"Model for storing functions that can be called by an agent.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/giphy.py",
            "diff": "diff --git a/libs/agno/agno/tools/giphy.py b/libs/agno/agno/tools/giphy.py\nindex 2dc09db25..622ab2791 100644\n--- a/libs/agno/agno/tools/giphy.py\n+++ b/libs/agno/agno/tools/giphy.py\n@@ -1,6 +1,6 @@\n import os\n import uuid\n-from typing import Optional, Union\n+from typing import Any, List, Optional, Union\n \n import httpx\n \n@@ -18,15 +18,23 @@ class GiphyTools(Toolkit):\n         limit: int = 1,\n         **kwargs,\n     ):\n-        super().__init__(name=\"giphy_tools\", **kwargs)\n+        \"\"\"Initialize Giphy tools.\n \n+        Args:\n+            api_key: Giphy API key. Defaults to GIPHY_API_KEY environment variable.\n+            limit: Number of GIFs to return. Defaults to 1.\n+            search_gifs: Whether to enable GIF search functionality. Defaults to True.\n+        \"\"\"\n         self.api_key = api_key or os.getenv(\"GIPHY_API_KEY\")\n         if not self.api_key:\n             logger.error(\"No Giphy API key provided\")\n \n         self.limit: int = limit\n \n-        self.register(self.search_gifs)\n+        tools: List[Any] = []\n+        tools.append(self.search_gifs)\n+\n+        super().__init__(name=\"giphy_tools\", tools=tools, **kwargs)\n \n     def search_gifs(self, agent: Union[Agent, Team], query: str) -> str:\n         \"\"\"Find a GIPHY gif\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/github.py",
            "diff": "diff --git a/libs/agno/agno/tools/github.py b/libs/agno/agno/tools/github.py\nindex 15e7b4592..900ac8387 100644\n--- a/libs/agno/agno/tools/github.py\n+++ b/libs/agno/agno/tools/github.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import List, Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -56,91 +56,90 @@ class GithubTools(Toolkit):\n         create_review_request: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"github\", **kwargs)\n-\n         self.access_token = access_token or getenv(\"GITHUB_ACCESS_TOKEN\")\n         self.base_url = base_url\n \n         self.g = self.authenticate()\n \n+        tools: List[Any] = []\n         if search_repositories:\n-            self.register(self.search_repositories)\n+            tools.append(self.search_repositories)\n         if get_repository:\n-            self.register(self.get_repository)\n+            tools.append(self.get_repository)\n         if get_pull_request:\n-            self.register(self.get_pull_request)\n+            tools.append(self.get_pull_request)\n         if get_pull_request_changes:\n-            self.register(self.get_pull_request_changes)\n+            tools.append(self.get_pull_request_changes)\n         if create_issue:\n-            self.register(self.create_issue)\n+            tools.append(self.create_issue)\n         if create_repository:\n-            self.register(self.create_repository)\n+            tools.append(self.create_repository)\n         if delete_repository:\n-            self.register(self.delete_repository)\n+            tools.append(self.delete_repository)\n         if list_branches:\n-            self.register(self.list_branches)\n+            tools.append(self.list_branches)\n         if get_repository_languages:\n-            self.register(self.get_repository_languages)\n+            tools.append(self.get_repository_languages)\n         if get_pull_request_count:\n-            self.register(self.get_pull_request_count)\n+            tools.append(self.get_pull_request_count)\n         if get_repository_stars:\n-            self.register(self.get_repository_stars)\n+            tools.append(self.get_repository_stars)\n         if get_pull_requests:\n-            self.register(self.get_pull_requests)\n+            tools.append(self.get_pull_requests)\n         if get_pull_request_comments:\n-            self.register(self.get_pull_request_comments)\n+            tools.append(self.get_pull_request_comments)\n         if create_pull_request_comment:\n-            self.register(self.create_pull_request_comment)\n+            tools.append(self.create_pull_request_comment)\n         if edit_pull_request_comment:\n-            self.register(self.edit_pull_request_comment)\n+            tools.append(self.edit_pull_request_comment)\n         if get_pull_request_with_details:\n-            self.register(self.get_pull_request_with_details)\n+            tools.append(self.get_pull_request_with_details)\n         if get_repository_with_stats:\n-            self.register(self.get_repository_with_stats)\n-\n+            tools.append(self.get_repository_with_stats)\n         if list_issues:\n-            self.register(self.list_issues)\n+            tools.append(self.list_issues)\n         if get_issue:\n-            self.register(self.get_issue)\n+            tools.append(self.get_issue)\n         if comment_on_issue:\n-            self.register(self.comment_on_issue)\n+            tools.append(self.comment_on_issue)\n         if close_issue:\n-            self.register(self.close_issue)\n+            tools.append(self.close_issue)\n         if reopen_issue:\n-            self.register(self.reopen_issue)\n+            tools.append(self.reopen_issue)\n         if assign_issue:\n-            self.register(self.assign_issue)\n+            tools.append(self.assign_issue)\n         if label_issue:\n-            self.register(self.label_issue)\n+            tools.append(self.label_issue)\n         if list_issue_comments:\n-            self.register(self.list_issue_comments)\n+            tools.append(self.list_issue_comments)\n         if edit_issue:\n-            self.register(self.edit_issue)\n-\n+            tools.append(self.edit_issue)\n         if create_pull_request:\n-            self.register(self.create_pull_request)\n+            tools.append(self.create_pull_request)\n         if create_file:\n-            self.register(self.create_file)\n+            tools.append(self.create_file)\n         if get_file_content:\n-            self.register(self.get_file_content)\n+            tools.append(self.get_file_content)\n         if update_file:\n-            self.register(self.update_file)\n+            tools.append(self.update_file)\n         if delete_file:\n-            self.register(self.delete_file)\n+            tools.append(self.delete_file)\n         if get_directory_content:\n-            self.register(self.get_directory_content)\n+            tools.append(self.get_directory_content)\n         if get_branch_content:\n-            self.register(self.get_branch_content)\n+            tools.append(self.get_branch_content)\n         if create_branch:\n-            self.register(self.create_branch)\n+            tools.append(self.create_branch)\n         if set_default_branch:\n-            self.register(self.set_default_branch)\n+            tools.append(self.set_default_branch)\n         if search_code:\n-            self.register(self.search_code)\n+            tools.append(self.search_code)\n         if search_issues_and_prs:\n-            self.register(self.search_issues_and_prs)\n+            tools.append(self.search_issues_and_prs)\n         if create_review_request:\n-            self.register(self.create_review_request)\n+            tools.append(self.create_review_request)\n+\n+        super().__init__(name=\"github\", tools=tools, **kwargs)\n \n     def authenticate(self):\n         \"\"\"Authenticate with GitHub using the provided access token.\"\"\"\n@@ -1666,7 +1665,8 @@ class GithubTools(Toolkit):\n \n             # Process results\n             results = []\n-            for code in code_results[:50]:  # Limit to 50 results to prevent timeouts\n+            # Limit to 50 results to prevent timeouts\n+            for code in code_results[:50]:\n                 code_info = {\n                     \"repository\": code.repository.full_name,\n                     \"path\": code.path,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/gmail.py",
            "diff": "diff --git a/libs/agno/agno/tools/gmail.py b/libs/agno/agno/tools/gmail.py\nindex 9221bc440..e4bf3613a 100644\n--- a/libs/agno/agno/tools/gmail.py\n+++ b/libs/agno/agno/tools/gmail.py\n@@ -46,7 +46,7 @@ from datetime import datetime, timedelta\n from functools import wraps\n from os import getenv\n from pathlib import Path\n-from typing import List, Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n \n@@ -131,7 +131,6 @@ class GmailTools(Toolkit):\n             token_path (Optional[str]): Path to token file. Defaults to None.\n             scopes (Optional[List[str]]): Custom OAuth scopes. If None, uses DEFAULT_SCOPES.\n         \"\"\"\n-        super().__init__(name=\"gmail_tools\", **kwargs)\n         self.creds = creds\n         self.credentials_path = credentials_path\n         self.token_path = token_path\n@@ -161,28 +160,31 @@ class GmailTools(Toolkit):\n             if read_scope not in self.scopes and write_scope not in self.scopes:\n                 raise ValueError(f\"The scope {read_scope} is required for email reading operations\")\n \n+        tools: List[Any] = []\n         if get_latest_emails:\n-            self.register(self.get_latest_emails)\n+            tools.append(self.get_latest_emails)\n         if get_emails_from_user:\n-            self.register(self.get_emails_from_user)\n+            tools.append(self.get_emails_from_user)\n         if get_unread_emails:\n-            self.register(self.get_unread_emails)\n+            tools.append(self.get_unread_emails)\n         if get_starred_emails:\n-            self.register(self.get_starred_emails)\n+            tools.append(self.get_starred_emails)\n         if get_emails_by_context:\n-            self.register(self.get_emails_by_context)\n+            tools.append(self.get_emails_by_context)\n         if get_emails_by_date:\n-            self.register(self.get_emails_by_date)\n+            tools.append(self.get_emails_by_date)\n         if get_emails_by_thread:\n-            self.register(self.get_emails_by_thread)\n+            tools.append(self.get_emails_by_thread)\n         if create_draft_email:\n-            self.register(self.create_draft_email)\n+            tools.append(self.create_draft_email)\n         if send_email:\n-            self.register(self.send_email)\n+            tools.append(self.send_email)\n         if send_email_reply:\n-            self.register(self.send_email_reply)\n+            tools.append(self.send_email_reply)\n         if search_emails:\n-            self.register(self.search_emails)\n+            tools.append(self.search_emails)\n+\n+        super().__init__(name=\"gmail_tools\", tools=tools, **kwargs)\n \n     def _auth(self) -> None:\n         \"\"\"Authenticate with Gmail API\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/google_bigquery.py",
            "diff": "diff --git a/libs/agno/agno/tools/google_bigquery.py b/libs/agno/agno/tools/google_bigquery.py\nindex 86d9eb18e..5eb8a0ebf 100644\n--- a/libs/agno/agno/tools/google_bigquery.py\n+++ b/libs/agno/agno/tools/google_bigquery.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Any, Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -23,7 +23,6 @@ class GoogleBigQueryTools(Toolkit):\n         credentials: Optional[Any] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"google_bigquery_tools\", **kwargs)\n         self.project = project or getenv(\"GOOGLE_CLOUD_PROJECT\")\n         self.location = location or getenv(\"GOOGLE_CLOUD_LOCATION\")\n \n@@ -37,13 +36,15 @@ class GoogleBigQueryTools(Toolkit):\n         # Initialize the BQ CLient\n         self.client = bigquery.Client(project=self.project, credentials=credentials)\n \n-        # Register functions in the toolkit\n+        tools: List[Any] = []\n         if list_tables:\n-            self.register(self.list_tables)\n+            tools.append(self.list_tables)\n         if describe_table:\n-            self.register(self.describe_table)\n+            tools.append(self.describe_table)\n         if run_sql_query:\n-            self.register(self.run_sql_query)\n+            tools.append(self.run_sql_query)\n+\n+        super().__init__(name=\"google_bigquery_tools\", tools=tools, **kwargs)\n \n     def list_tables(self) -> str:\n         \"\"\"Use this function to get a list of table names in the dataset.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/google_maps.py",
            "diff": "diff --git a/libs/agno/agno/tools/google_maps.py b/libs/agno/agno/tools/google_maps.py\nindex 6c90c5a43..54b4a284d 100644\n--- a/libs/agno/agno/tools/google_maps.py\n+++ b/libs/agno/agno/tools/google_maps.py\n@@ -14,7 +14,7 @@ Prerequisites:\n import json\n from datetime import datetime\n from os import getenv\n-from typing import List, Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n \n@@ -39,8 +39,6 @@ class GoogleMapTools(Toolkit):\n         get_timezone: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"google_maps\", **kwargs)\n-\n         self.api_key = key or getenv(\"GOOGLE_MAPS_API_KEY\")\n         if not self.api_key:\n             raise ValueError(\"GOOGLE_MAPS_API_KEY is not set in the environment variables.\")\n@@ -48,22 +46,25 @@ class GoogleMapTools(Toolkit):\n \n         self.places_client = places_v1.PlacesClient()\n \n+        tools: List[Any] = []\n         if search_places:\n-            self.register(self.search_places)\n+            tools.append(self.search_places)\n         if get_directions:\n-            self.register(self.get_directions)\n+            tools.append(self.get_directions)\n         if validate_address:\n-            self.register(self.validate_address)\n+            tools.append(self.validate_address)\n         if geocode_address:\n-            self.register(self.geocode_address)\n+            tools.append(self.geocode_address)\n         if reverse_geocode:\n-            self.register(self.reverse_geocode)\n+            tools.append(self.reverse_geocode)\n         if get_distance_matrix:\n-            self.register(self.get_distance_matrix)\n+            tools.append(self.get_distance_matrix)\n         if get_elevation:\n-            self.register(self.get_elevation)\n+            tools.append(self.get_elevation)\n         if get_timezone:\n-            self.register(self.get_timezone)\n+            tools.append(self.get_timezone)\n+\n+        super().__init__(name=\"google_maps\", tools=tools, **kwargs)\n \n     def search_places(self, query: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/googlecalendar.py",
            "diff": "diff --git a/libs/agno/agno/tools/googlecalendar.py b/libs/agno/agno/tools/googlecalendar.py\nindex 794d70976..c720fccb7 100644\n--- a/libs/agno/agno/tools/googlecalendar.py\n+++ b/libs/agno/agno/tools/googlecalendar.py\n@@ -57,7 +57,12 @@ def authenticated(func):\n \n \n class GoogleCalendarTools(Toolkit):\n-    def __init__(self, credentials_path: Optional[str] = None, token_path: Optional[str] = None, **kwargs):\n+    def __init__(\n+        self,\n+        credentials_path: Optional[str] = None,\n+        token_path: Optional[str] = None,\n+        **kwargs,\n+    ):\n         \"\"\"\n         Google Calendar Tool.\n \n@@ -65,7 +70,6 @@ class GoogleCalendarTools(Toolkit):\n         :param token_path: Path of the file token.json which stores the user's access and refresh tokens, and is created automatically when the authorization flow completes for the first time.\n \n         \"\"\"\n-        super().__init__(name=\"google_calendar_tools\", **kwargs)\n \n         if not credentials_path:\n             logger.error(\n@@ -88,8 +92,12 @@ class GoogleCalendarTools(Toolkit):\n         self.service = None\n         self.token_path = token_path\n         self.creds_path = credentials_path\n-        self.register(self.list_events)\n-        self.register(self.create_event)\n+\n+        tools = []\n+        tools.append(self.list_events)\n+        tools.append(self.create_event)\n+\n+        super().__init__(name=\"google_calendar_tools\", tools=tools, **kwargs)\n \n     @authenticated\n     def list_events(self, limit: int = 10, date_from: str = datetime.date.today().isoformat()) -> str:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/googlesearch.py",
            "diff": "diff --git a/libs/agno/agno/tools/googlesearch.py b/libs/agno/agno/tools/googlesearch.py\nindex 1705b5422..b9e45ebcf 100644\n--- a/libs/agno/agno/tools/googlesearch.py\n+++ b/libs/agno/agno/tools/googlesearch.py\n@@ -40,15 +40,16 @@ class GoogleSearchTools(Toolkit):\n         timeout: Optional[int] = 10,\n         **kwargs,\n     ):\n-        super().__init__(name=\"googlesearch\", **kwargs)\n-\n         self.fixed_max_results: Optional[int] = fixed_max_results\n         self.fixed_language: Optional[str] = fixed_language\n         self.headers: Optional[Any] = headers\n         self.proxy: Optional[str] = proxy\n         self.timeout: Optional[int] = timeout\n \n-        self.register(self.google_search)\n+        tools = []\n+        tools.append(self.google_search)\n+\n+        super().__init__(name=\"google_search_tools\", tools=tools, **kwargs)\n \n     def google_search(self, query: str, max_results: int = 5, language: str = \"en\") -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/googlesheets.py",
            "diff": "diff --git a/libs/agno/agno/tools/googlesheets.py b/libs/agno/agno/tools/googlesheets.py\nindex 23a5937a5..db0333262 100644\n--- a/libs/agno/agno/tools/googlesheets.py\n+++ b/libs/agno/agno/tools/googlesheets.py\n@@ -114,7 +114,7 @@ class GoogleSheetsTools(Toolkit):\n             update (bool): Enable update operations. Defaults to False.\n             duplicate (bool): Enable duplicate operations. Defaults to False.\n         \"\"\"\n-        super().__init__(name=\"google_sheets_tools\", **kwargs)\n+\n         self.spreadsheet_id = spreadsheet_id\n         self.spreadsheet_range = spreadsheet_range\n         self.creds = creds\n@@ -145,14 +145,17 @@ class GoogleSheetsTools(Toolkit):\n                     f\"Either {self.DEFAULT_SCOPES['read']} or {self.DEFAULT_SCOPES['write']} is required for read operations\"\n                 )\n \n+        tools: List[Any] = []\n         if read:\n-            self.register(self.read_sheet)\n+            tools.append(self.read_sheet)\n         if create:\n-            self.register(self.create_sheet)\n+            tools.append(self.create_sheet)\n         if update:\n-            self.register(self.update_sheet)\n+            tools.append(self.update_sheet)\n         if duplicate:\n-            self.register(self.create_duplicate_sheet)\n+            tools.append(self.create_duplicate_sheet)\n+\n+        super().__init__(name=\"google_sheets_tools\", tools=tools, **kwargs)\n \n     def _auth(self) -> None:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/hackernews.py",
            "diff": "diff --git a/libs/agno/agno/tools/hackernews.py b/libs/agno/agno/tools/hackernews.py\nindex 9d833d28b..786280a96 100644\n--- a/libs/agno/agno/tools/hackernews.py\n+++ b/libs/agno/agno/tools/hackernews.py\n@@ -1,4 +1,5 @@\n import json\n+from typing import Any, List\n \n import httpx\n \n@@ -15,13 +16,13 @@ class HackerNewsTools(Toolkit):\n     \"\"\"\n \n     def __init__(self, get_top_stories: bool = True, get_user_details: bool = True, **kwargs):\n-        super().__init__(name=\"hackers_news\", **kwargs)\n-\n-        # Register functions in the toolkit\n+        tools: List[Any] = []\n         if get_top_stories:\n-            self.register(self.get_top_hackernews_stories)\n+            tools.append(self.get_top_hackernews_stories)\n         if get_user_details:\n-            self.register(self.get_user_details)\n+            tools.append(self.get_user_details)\n+\n+        super().__init__(name=\"hackers_news\", tools=tools, **kwargs)\n \n     def get_top_hackernews_stories(self, num_stories: int = 10) -> str:\n         \"\"\"Use this function to get top stories from Hacker News.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/jina.py",
            "diff": "diff --git a/libs/agno/agno/tools/jina.py b/libs/agno/agno/tools/jina.py\nindex 4d23d3f92..f5e59519e 100644\n--- a/libs/agno/agno/tools/jina.py\n+++ b/libs/agno/agno/tools/jina.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n import httpx\n from pydantic import BaseModel, Field, HttpUrl\n@@ -28,8 +28,6 @@ class JinaReaderTools(Toolkit):\n         search_query: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"jina_reader_tools\", **kwargs)\n-\n         self.config: JinaReaderToolsConfig = JinaReaderToolsConfig(\n             api_key=api_key,\n             base_url=base_url,\n@@ -38,10 +36,13 @@ class JinaReaderTools(Toolkit):\n             timeout=timeout,\n         )\n \n+        tools: List[Any] = []\n         if read_url:\n-            self.register(self.read_url)\n+            tools.append(self.read_url)\n         if search_query:\n-            self.register(self.search_query)\n+            tools.append(self.search_query)\n+\n+        super().__init__(name=\"jina_reader_tools\", tools=tools, **kwargs)\n \n     def read_url(self, url: str) -> str:\n         \"\"\"Reads a URL and returns the truncated content using Jina Reader API.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/jira.py",
            "diff": "diff --git a/libs/agno/agno/tools/jira.py b/libs/agno/agno/tools/jira.py\nindex 2ff3f0e2d..ef42cc37a 100644\n--- a/libs/agno/agno/tools/jira.py\n+++ b/libs/agno/agno/tools/jira.py\n@@ -1,6 +1,6 @@\n import json\n import os\n-from typing import Optional, cast\n+from typing import Any, List, Optional, cast\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -20,8 +20,6 @@ class JiraTools(Toolkit):\n         token: Optional[str] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"jira_tools\", **kwargs)\n-\n         self.server_url = server_url or os.getenv(\"JIRA_SERVER_URL\")\n         self.username = username or os.getenv(\"JIRA_USERNAME\")\n         self.password = password or os.getenv(\"JIRA_PASSWORD\")\n@@ -43,12 +41,13 @@ class JiraTools(Toolkit):\n         else:\n             self.jira = JIRA(server=self.server_url)\n \n-        # Register methods\n-        self.register(self.get_issue)\n-        self.register(self.create_issue)\n-        self.register(self.search_issues)\n-        self.register(self.add_comment)\n-        # You can register more methods here\n+        tools: List[Any] = []\n+        tools.append(self.get_issue)\n+        tools.append(self.create_issue)\n+        tools.append(self.search_issues)\n+        tools.append(self.add_comment)\n+\n+        super().__init__(name=\"jira_tools\", tools=tools, **kwargs)\n \n     def get_issue(self, issue_key: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/knowledge.py",
            "diff": "diff --git a/libs/agno/agno/tools/knowledge.py b/libs/agno/agno/tools/knowledge.py\nindex 131169058..e2e2fd841 100644\n--- a/libs/agno/agno/tools/knowledge.py\n+++ b/libs/agno/agno/tools/knowledge.py\n@@ -1,6 +1,6 @@\n import json\n from textwrap import dedent\n-from typing import List, Optional, Union\n+from typing import Any, List, Optional, Union\n \n from agno.agent import Agent\n from agno.document import Document\n@@ -26,13 +26,6 @@ class KnowledgeTools(Toolkit):\n         if knowledge is None:\n             raise ValueError(\"knowledge must be provided when using KnowledgeTools\")\n \n-        super().__init__(\n-            name=\"knowledge_tools\",\n-            instructions=instructions,\n-            add_instructions=add_instructions,\n-            **kwargs,\n-        )\n-\n         # Add instructions for using this toolkit\n         if instructions is None:\n             self.instructions = self.DEFAULT_INSTRUCTIONS\n@@ -44,13 +37,22 @@ class KnowledgeTools(Toolkit):\n \n         # The knowledge to search\n         self.knowledge: AgentKnowledge = knowledge\n-        # Register tools\n+\n+        tools: List[Any] = []\n         if think:\n-            self.register(self.think)\n+            tools.append(self.think)\n         if search:\n-            self.register(self.search)\n+            tools.append(self.search)\n         if analyze:\n-            self.register(self.analyze)\n+            tools.append(self.analyze)\n+\n+        super().__init__(\n+            name=\"knowledge_tools\",\n+            instructions=instructions,\n+            add_instructions=add_instructions,\n+            tools=tools,\n+            **kwargs,\n+        )\n \n     def think(self, agent: Union[Agent, Team], thought: str) -> str:\n         \"\"\"Use this tool as a scratchpad to reason about the question, refine your approach, brainstorm search terms, or revise your plan.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/linear.py",
            "diff": "diff --git a/libs/agno/agno/tools/linear.py b/libs/agno/agno/tools/linear.py\nindex c71ed3f62..21b29df22 100644\n--- a/libs/agno/agno/tools/linear.py\n+++ b/libs/agno/agno/tools/linear.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n import requests\n \n@@ -19,7 +19,6 @@ class LinearTools(Toolkit):\n         get_high_priority_issues: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"linear tools\", **kwargs)\n         self.api_token = getenv(\"LINEAR_API_KEY\")\n \n         if not self.api_token:\n@@ -29,20 +28,23 @@ class LinearTools(Toolkit):\n         self.endpoint = \"https://api.linear.app/graphql\"\n         self.headers = {\"Authorization\": f\"{self.api_token}\"}\n \n+        tools: List[Any] = []\n         if get_user_details:\n-            self.register(self.get_user_details)\n+            tools.append(self.get_user_details)\n         if get_issue_details:\n-            self.register(self.get_issue_details)\n+            tools.append(self.get_issue_details)\n         if create_issue:\n-            self.register(self.create_issue)\n+            tools.append(self.create_issue)\n         if update_issue:\n-            self.register(self.update_issue)\n+            tools.append(self.update_issue)\n         if get_user_assigned_issues:\n-            self.register(self.get_user_assigned_issues)\n+            tools.append(self.get_user_assigned_issues)\n         if get_workflow_issues:\n-            self.register(self.get_workflow_issues)\n+            tools.append(self.get_workflow_issues)\n         if get_high_priority_issues:\n-            self.register(self.get_high_priority_issues)\n+            tools.append(self.get_high_priority_issues)\n+\n+        super().__init__(name=\"linear tools\", tools=tools, **kwargs)\n \n     def _execute_query(self, query, variables=None):\n         \"\"\"Helper method to execute GraphQL queries with optional variables.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/local_file_system.py",
            "diff": "diff --git a/libs/agno/agno/tools/local_file_system.py b/libs/agno/agno/tools/local_file_system.py\nindex 457000e1b..d18dbaafb 100644\n--- a/libs/agno/agno/tools/local_file_system.py\n+++ b/libs/agno/agno/tools/local_file_system.py\n@@ -20,7 +20,6 @@ class LocalFileSystemTools(Toolkit):\n             target_directory (Optional[str]): Default directory to write files to. Creates if doesn't exist.\n             default_extension (str): Default file extension to use if none specified.\n         \"\"\"\n-        super().__init__(name=\"write_to_local\", **kwargs)\n \n         self.target_directory = target_directory or os.getcwd()\n         self.default_extension = default_extension.lstrip(\".\")\n@@ -28,7 +27,10 @@ class LocalFileSystemTools(Toolkit):\n         target_path = Path(self.target_directory)\n         target_path.mkdir(parents=True, exist_ok=True)\n \n-        self.register(self.write_file)\n+        tools = []\n+        tools.append(self.write_file)\n+\n+        super().__init__(name=\"write_to_local\", tools=tools, **kwargs)\n \n     def write_file(\n         self,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/lumalab.py",
            "diff": "diff --git a/libs/agno/agno/tools/lumalab.py b/libs/agno/agno/tools/lumalab.py\nindex a0e6de6f4..29ea94ab6 100644\n--- a/libs/agno/agno/tools/lumalab.py\n+++ b/libs/agno/agno/tools/lumalab.py\n@@ -1,7 +1,7 @@\n import time\n import uuid\n from os import getenv\n-from typing import Any, Dict, Literal, Optional, TypedDict\n+from typing import Any, Dict, List, Literal, Optional, TypedDict\n \n from agno.agent import Agent\n from agno.media import VideoArtifact\n@@ -32,8 +32,6 @@ class LumaLabTools(Toolkit):\n         max_wait_time: int = 300,  # 5 minutes\n         **kwargs,\n     ):\n-        super().__init__(name=\"luma_lab\", **kwargs)\n-\n         self.wait_for_completion = wait_for_completion\n         self.poll_interval = poll_interval\n         self.max_wait_time = max_wait_time\n@@ -43,8 +41,12 @@ class LumaLabTools(Toolkit):\n             logger.error(\"LUMAAI_API_KEY not set. Please set the LUMAAI_API_KEY environment variable.\")\n \n         self.client = LumaAI(auth_token=self.api_key)\n-        self.register(self.generate_video)\n-        self.register(self.image_to_video)\n+\n+        tools: List[Any] = []\n+        tools.append(self.generate_video)\n+        tools.append(self.image_to_video)\n+\n+        super().__init__(name=\"luma_lab\", tools=tools, **kwargs)\n \n     def image_to_video(\n         self,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/mlx_transcribe.py",
            "diff": "diff --git a/libs/agno/agno/tools/mlx_transcribe.py b/libs/agno/agno/tools/mlx_transcribe.py\nindex 837db060e..934af6e62 100644\n--- a/libs/agno/agno/tools/mlx_transcribe.py\n+++ b/libs/agno/agno/tools/mlx_transcribe.py\n@@ -49,8 +49,6 @@ class MLXTranscribeTools(Toolkit):\n         decode_options: Optional[dict] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"mlx_transcribe\", **kwargs)\n-\n         self.base_dir: Path = base_dir or Path.cwd()\n         self.path_or_hf_repo: str = path_or_hf_repo\n         self.verbose: Optional[bool] = verbose\n@@ -67,9 +65,11 @@ class MLXTranscribeTools(Toolkit):\n         self.hallucination_silence_threshold: Optional[float] = hallucination_silence_threshold\n         self.decode_options: Optional[dict] = decode_options\n \n-        self.register(self.transcribe)\n+        tools: List[Any] = [self.transcribe]\n         if read_files_in_base_dir:\n-            self.register(self.read_files)\n+            tools.append(self.read_files)\n+\n+        super().__init__(name=\"mlx_transcribe\", tools=tools, **kwargs)\n \n     def transcribe(self, file_name: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/models_labs.py",
            "diff": "diff --git a/libs/agno/agno/tools/models_labs.py b/libs/agno/agno/tools/models_labs.py\nindex 981cb29d9..932b5f647 100644\n--- a/libs/agno/agno/tools/models_labs.py\n+++ b/libs/agno/agno/tools/models_labs.py\n@@ -1,7 +1,7 @@\n import json\n import time\n from os import getenv\n-from typing import Any, Dict, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n from uuid import uuid4\n \n from agno.agent import Agent\n@@ -40,8 +40,6 @@ class ModelsLabTools(Toolkit):\n         file_type: FileType = FileType.MP4,\n         **kwargs,\n     ):\n-        super().__init__(name=\"models_labs\", **kwargs)\n-\n         file_type_str = file_type.value.upper()\n         self.url = MODELS_LAB_URLS[file_type_str]\n         self.fetch_url = MODELS_LAB_FETCH_URLS[file_type_str]\n@@ -54,7 +52,10 @@ class ModelsLabTools(Toolkit):\n         if not self.api_key:\n             logger.error(\"MODELS_LAB_API_KEY not set. Please set the MODELS_LAB_API_KEY environment variable.\")\n \n-        self.register(self.generate_media)\n+        tools: List[Any] = []\n+        tools.append(self.generate_media)\n+\n+        super().__init__(name=\"models_labs\", tools=tools, **kwargs)\n \n     def _create_payload(self, prompt: str) -> Dict[str, Any]:\n         \"\"\"Create payload based on file type.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/moviepy_video.py",
            "diff": "diff --git a/libs/agno/agno/tools/moviepy_video.py b/libs/agno/agno/tools/moviepy_video.py\nindex 9fddecf79..d1840a8d5 100644\n--- a/libs/agno/agno/tools/moviepy_video.py\n+++ b/libs/agno/agno/tools/moviepy_video.py\n@@ -1,4 +1,4 @@\n-from typing import Dict, List, Optional\n+from typing import Any, Dict, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -19,14 +19,15 @@ class MoviePyVideoTools(Toolkit):\n         embed_captions: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"video_tools\", **kwargs)\n-\n+        tools: List[Any] = []\n         if process_video:\n-            self.register(self.extract_audio)\n+            tools.append(self.extract_audio)\n         if generate_captions:\n-            self.register(self.create_srt)\n+            tools.append(self.create_srt)\n         if embed_captions:\n-            self.register(self.embed_captions)\n+            tools.append(self.embed_captions)\n+\n+        super().__init__(name=\"video_tools\", tools=tools, **kwargs)\n \n     def split_text_into_lines(self, words: List[Dict]) -> List[Dict]:\n         \"\"\"Split transcribed words into lines based on duration and length constraints\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/newspaper.py",
            "diff": "diff --git a/libs/agno/agno/tools/newspaper.py b/libs/agno/agno/tools/newspaper.py\nindex a39c151f7..b50916cb9 100644\n--- a/libs/agno/agno/tools/newspaper.py\n+++ b/libs/agno/agno/tools/newspaper.py\n@@ -15,10 +15,11 @@ class NewspaperTools(Toolkit):\n     \"\"\"\n \n     def __init__(self, get_article_text: bool = True, **kwargs):\n-        super().__init__(name=\"newspaper_toolkit\", **kwargs)\n-\n+        tools = []\n         if get_article_text:\n-            self.register(self.get_article_text)\n+            tools.append(self.get_article_text)\n+\n+        super().__init__(name=\"newspaper_toolkit\", tools=tools, **kwargs)\n \n     def get_article_text(self, url: str) -> str:\n         \"\"\"Get the text of an article from a URL.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/newspaper4k.py",
            "diff": "diff --git a/libs/agno/agno/tools/newspaper4k.py b/libs/agno/agno/tools/newspaper4k.py\nindex 7f2b9ab1f..72b6947df 100644\n--- a/libs/agno/agno/tools/newspaper4k.py\n+++ b/libs/agno/agno/tools/newspaper4k.py\n@@ -22,12 +22,14 @@ class Newspaper4kTools(Toolkit):\n     def __init__(\n         self, read_article: bool = True, include_summary: bool = False, article_length: Optional[int] = None, **kwargs\n     ):\n-        super().__init__(name=\"newspaper4k_tools\", **kwargs)\n-\n         self.include_summary: bool = include_summary\n         self.article_length: Optional[int] = article_length\n+\n+        tools = []\n         if read_article:\n-            self.register(self.read_article)\n+            tools.append(self.read_article)\n+\n+        super().__init__(name=\"newspaper4k_tools\", tools=tools, **kwargs)\n \n     def get_article_data(self, url: str) -> Optional[Dict[str, Any]]:\n         \"\"\"Read and get article data from a URL.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/openai.py",
            "diff": "diff --git a/libs/agno/agno/tools/openai.py b/libs/agno/agno/tools/openai.py\nindex fbb1fa0f8..0d96fd7e4 100644\n--- a/libs/agno/agno/tools/openai.py\n+++ b/libs/agno/agno/tools/openai.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Literal, Optional\n+from typing import Any, List, Literal, Optional\n from uuid import uuid4\n \n from agno.agent import Agent\n@@ -37,8 +37,6 @@ class OpenAITools(Toolkit):\n         image_style: Optional[Literal[\"vivid\", \"natural\"]] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"openai_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"OPENAI_API_KEY\")\n         if not self.api_key:\n             raise ValueError(\"OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.\")\n@@ -53,12 +51,15 @@ class OpenAITools(Toolkit):\n         self.image_style = image_style\n         self.image_size = image_size\n \n+        tools: List[Any] = []\n         if enable_transcription:\n-            self.register(self.transcribe_audio)\n+            tools.append(self.transcribe_audio)\n         if enable_image_generation:\n-            self.register(self.generate_image)\n+            tools.append(self.generate_image)\n         if enable_speech_generation:\n-            self.register(self.generate_speech)\n+            tools.append(self.generate_speech)\n+\n+        super().__init__(name=\"openai_tools\", tools=tools, **kwargs)\n \n     def transcribe_audio(self, audio_path: str) -> str:\n         \"\"\"Transcribe audio file using OpenAI's Whisper API\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/openbb.py",
            "diff": "diff --git a/libs/agno/agno/tools/openbb.py b/libs/agno/agno/tools/openbb.py\nindex df1a68162..7bbbfe908 100644\n--- a/libs/agno/agno/tools/openbb.py\n+++ b/libs/agno/agno/tools/openbb.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Any, Literal, Optional\n+from typing import Any, List, Literal, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -24,8 +24,6 @@ class OpenBBTools(Toolkit):\n         price_targets: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"yfinance_tools\", **kwargs)\n-\n         self.obb = obb or openbb_app\n         try:\n             if openbb_pat or getenv(\"OPENBB_PAT\"):\n@@ -35,16 +33,19 @@ class OpenBBTools(Toolkit):\n \n         self.provider: Literal[\"benzinga\", \"fmp\", \"intrinio\", \"polygon\", \"tiingo\", \"tmx\", \"yfinance\"] = provider\n \n+        tools: List[Any] = []\n         if stock_price:\n-            self.register(self.get_stock_price)\n+            tools.append(self.get_stock_price)\n         if search_symbols:\n-            self.register(self.search_company_symbol)\n+            tools.append(self.search_company_symbol)\n         if company_news:\n-            self.register(self.get_company_news)\n+            tools.append(self.get_company_news)\n         if company_profile:\n-            self.register(self.get_company_profile)\n+            tools.append(self.get_company_profile)\n         if price_targets:\n-            self.register(self.get_price_targets)\n+            tools.append(self.get_price_targets)\n+\n+        super().__init__(name=\"yfinance_tools\", tools=tools, **kwargs)\n \n     def get_stock_price(self, symbol: str) -> str:\n         \"\"\"Use this function to get the current stock price for a stock symbol or list of symbols.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/openweather.py",
            "diff": "diff --git a/libs/agno/agno/tools/openweather.py b/libs/agno/agno/tools/openweather.py\nindex fc2c9f668..43667a353 100644\n--- a/libs/agno/agno/tools/openweather.py\n+++ b/libs/agno/agno/tools/openweather.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_info, logger\n@@ -34,8 +34,6 @@ class OpenWeatherTools(Toolkit):\n         geocoding: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"openweather_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"OPENWEATHER_API_KEY\")\n         if not self.api_key:\n             raise ValueError(\n@@ -46,15 +44,17 @@ class OpenWeatherTools(Toolkit):\n         self.base_url = \"https://api.openweathermap.org/data/2.5\"\n         self.geo_url = \"https://api.openweathermap.org/geo/1.0\"\n \n-        # Register functions based on parameters\n+        tools: List[Any] = []\n         if current_weather:\n-            self.register(self.get_current_weather)\n+            tools.append(self.get_current_weather)\n         if forecast:\n-            self.register(self.get_forecast)\n+            tools.append(self.get_forecast)\n         if air_pollution:\n-            self.register(self.get_air_pollution)\n+            tools.append(self.get_air_pollution)\n         if geocoding:\n-            self.register(self.geocode_location)\n+            tools.append(self.geocode_location)\n+\n+        super().__init__(name=\"openweather_tools\", tools=tools, **kwargs)\n \n     def _make_request(self, url: str, params: Dict) -> Dict:\n         \"\"\"Make a request to the OpenWeatherMap API.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/pandas.py",
            "diff": "diff --git a/libs/agno/agno/tools/pandas.py b/libs/agno/agno/tools/pandas.py\nindex a78574bf0..153829533 100644\n--- a/libs/agno/agno/tools/pandas.py\n+++ b/libs/agno/agno/tools/pandas.py\n@@ -1,4 +1,4 @@\n-from typing import Any, Dict\n+from typing import Any, Dict, List\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -11,11 +11,13 @@ except ImportError:\n \n class PandasTools(Toolkit):\n     def __init__(self, **kwargs):\n-        super().__init__(name=\"pandas_tools\", **kwargs)\n-\n         self.dataframes: Dict[str, pd.DataFrame] = {}\n-        self.register(self.create_pandas_dataframe)\n-        self.register(self.run_dataframe_operation)\n+\n+        tools: List[Any] = []\n+        tools.append(self.create_pandas_dataframe)\n+        tools.append(self.run_dataframe_operation)\n+\n+        super().__init__(name=\"pandas_tools\", tools=tools, **kwargs)\n \n     def create_pandas_dataframe(\n         self, dataframe_name: str, create_using_function: str, function_parameters: Dict[str, Any]\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/postgres.py",
            "diff": "diff --git a/libs/agno/agno/tools/postgres.py b/libs/agno/agno/tools/postgres.py\nindex 81bdd3920..04b6e0957 100644\n--- a/libs/agno/agno/tools/postgres.py\n+++ b/libs/agno/agno/tools/postgres.py\n@@ -1,4 +1,4 @@\n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n try:\n     import psycopg2\n@@ -29,7 +29,6 @@ class PostgresTools(Toolkit):\n         table_schema: str = \"public\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"postgres_tools\", **kwargs)\n         self._connection: Optional[psycopg2.extensions.connection] = connection\n         self.db_name: Optional[str] = db_name\n         self.user: Optional[str] = user\n@@ -38,16 +37,19 @@ class PostgresTools(Toolkit):\n         self.port: Optional[int] = port\n         self.table_schema: str = table_schema\n \n-        self.register(self.show_tables)\n-        self.register(self.describe_table)\n+        tools: List[Any] = []\n+        tools.append(self.show_tables)\n+        tools.append(self.describe_table)\n         if inspect_queries:\n-            self.register(self.inspect_query)\n+            tools.append(self.inspect_query)\n         if run_queries:\n-            self.register(self.run_query)\n+            tools.append(self.run_query)\n         if summarize_tables:\n-            self.register(self.summarize_table)\n+            tools.append(self.summarize_table)\n         if export_tables:\n-            self.register(self.export_table_to_path)\n+            tools.append(self.export_table_to_path)\n+\n+        super().__init__(name=\"postgres_tools\", tools=tools, **kwargs)\n \n     @property\n     def connection(self) -> psycopg2.extensions.connection:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/pubmed.py",
            "diff": "diff --git a/libs/agno/agno/tools/pubmed.py b/libs/agno/agno/tools/pubmed.py\nindex 1d329dd17..325e43532 100644\n--- a/libs/agno/agno/tools/pubmed.py\n+++ b/libs/agno/agno/tools/pubmed.py\n@@ -16,12 +16,14 @@ class PubmedTools(Toolkit):\n         results_expanded: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"pubmed\", **kwargs)\n         self.max_results: Optional[int] = max_results\n         self.email: str = email\n         self.results_expanded: bool = results_expanded\n \n-        self.register(self.search_pubmed)\n+        tools: List[Any] = []\n+        tools.append(self.search_pubmed)\n+\n+        super().__init__(name=\"pubmed\", tools=tools, **kwargs)\n \n     def fetch_pubmed_ids(self, query: str, max_results: int, email: str) -> List[str]:\n         url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/python.py",
            "diff": "diff --git a/libs/agno/agno/tools/python.py b/libs/agno/agno/tools/python.py\nindex aa4b09483..db616d477 100644\n--- a/libs/agno/agno/tools/python.py\n+++ b/libs/agno/agno/tools/python.py\n@@ -1,7 +1,7 @@\n import functools\n import runpy\n from pathlib import Path\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -27,28 +27,29 @@ class PythonTools(Toolkit):\n         safe_locals: Optional[dict] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"python_tools\", **kwargs)\n-\n         self.base_dir: Path = base_dir or Path.cwd()\n \n         # Restricted global and local scope\n         self.safe_globals: dict = safe_globals or globals()\n         self.safe_locals: dict = safe_locals or locals()\n \n+        tools: List[Any] = []\n         if run_code:\n-            self.register(self.run_python_code, sanitize_arguments=False)\n+            tools.append(self.run_python_code)\n         if save_and_run:\n-            self.register(self.save_to_file_and_run, sanitize_arguments=False)\n+            tools.append(self.save_to_file_and_run)\n         if pip_install:\n-            self.register(self.pip_install_package)\n+            tools.append(self.pip_install_package)\n         if uv_pip_install:\n-            self.register(self.uv_pip_install_package)\n+            tools.append(self.uv_pip_install_package)\n         if run_files:\n-            self.register(self.run_python_file_return_variable)\n+            tools.append(self.run_python_file_return_variable)\n         if read_files:\n-            self.register(self.read_file)\n+            tools.append(self.read_file)\n         if list_files:\n-            self.register(self.list_files)\n+            tools.append(self.list_files)\n+\n+        super().__init__(name=\"python_tools\", tools=tools, **kwargs)\n \n     def save_to_file_and_run(\n         self, file_name: str, code: str, variable_to_return: Optional[str] = None, overwrite: bool = True\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/reasoning.py",
            "diff": "diff --git a/libs/agno/agno/tools/reasoning.py b/libs/agno/agno/tools/reasoning.py\nindex 73aa4bb5b..13d92a056 100644\n--- a/libs/agno/agno/tools/reasoning.py\n+++ b/libs/agno/agno/tools/reasoning.py\n@@ -1,5 +1,5 @@\n from textwrap import dedent\n-from typing import Optional, Union\n+from typing import Any, List, Optional, Union\n \n from agno.agent import Agent\n from agno.reasoning.step import NextAction, ReasoningStep\n@@ -20,12 +20,6 @@ class ReasoningTools(Toolkit):\n         **kwargs,\n     ):\n         \"\"\"A toolkit that provides step-by-step reasoning tools: Think and Analyze.\"\"\"\n-        super().__init__(\n-            name=\"reasoning_tools\",\n-            instructions=instructions,\n-            add_instructions=add_instructions,\n-            **kwargs,\n-        )\n \n         # Add instructions for using this toolkit\n         if instructions is None:\n@@ -37,11 +31,19 @@ class ReasoningTools(Toolkit):\n                     self.instructions += \"\\n\" + self.FEW_SHOT_EXAMPLES\n             self.instructions += \"\\n</reasoning_instructions>\\n\"\n \n-        # Register each tool based on the init flags\n+        tools: List[Any] = []\n         if think:\n-            self.register(self.think)\n+            tools.append(self.think)\n         if analyze:\n-            self.register(self.analyze)\n+            tools.append(self.analyze)\n+\n+        super().__init__(\n+            name=\"reasoning_tools\",\n+            instructions=instructions,\n+            add_instructions=add_instructions,\n+            tools=tools,\n+            **kwargs,\n+        )\n \n     def think(\n         self, agent: Union[Agent, Team], title: str, thought: str, action: Optional[str] = None, confidence: float = 0.8\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/reddit.py",
            "diff": "diff --git a/libs/agno/agno/tools/reddit.py b/libs/agno/agno/tools/reddit.py\nindex f183b3e10..3486c0a6a 100644\n--- a/libs/agno/agno/tools/reddit.py\n+++ b/libs/agno/agno/tools/reddit.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -30,8 +30,6 @@ class RedditTools(Toolkit):\n         reply_to_comment: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"reddit\", **kwargs)\n-\n         if reddit_instance is not None:\n             log_info(\"Using provided Reddit instance\")\n             self.reddit = reddit_instance\n@@ -67,22 +65,25 @@ class RedditTools(Toolkit):\n             else:\n                 logger.warning(\"Missing Reddit API credentials\")\n \n+        tools: List[Any] = []\n         if get_user_info:\n-            self.register(self.get_user_info)\n+            tools.append(self.get_user_info)\n         if get_top_posts:\n-            self.register(self.get_top_posts)\n+            tools.append(self.get_top_posts)\n         if get_subreddit_info:\n-            self.register(self.get_subreddit_info)\n+            tools.append(self.get_subreddit_info)\n         if get_trending_subreddits:\n-            self.register(self.get_trending_subreddits)\n+            tools.append(self.get_trending_subreddits)\n         if get_subreddit_stats:\n-            self.register(self.get_subreddit_stats)\n+            tools.append(self.get_subreddit_stats)\n         if create_post:\n-            self.register(self.create_post)\n+            tools.append(self.create_post)\n         if reply_to_post:\n-            self.register(self.reply_to_post)\n+            tools.append(self.reply_to_post)\n         if reply_to_comment:\n-            self.register(self.reply_to_comment)\n+            tools.append(self.reply_to_comment)\n+\n+        super().__init__(name=\"reddit\", tools=tools, **kwargs)\n \n     def _check_user_auth(self) -> bool:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/replicate.py",
            "diff": "diff --git a/libs/agno/agno/tools/replicate.py b/libs/agno/agno/tools/replicate.py\nindex d5dff4cd7..1a2bc1836 100644\n--- a/libs/agno/agno/tools/replicate.py\n+++ b/libs/agno/agno/tools/replicate.py\n@@ -1,6 +1,6 @@\n import os\n from os import getenv\n-from typing import Iterable, Iterator, Optional, Union\n+from typing import Any, Iterable, Iterator, List, Optional, Union\n from urllib.parse import urlparse\n from uuid import uuid4\n \n@@ -24,12 +24,15 @@ class ReplicateTools(Toolkit):\n         model: str = \"minimax/video-01\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"replicate_toolkit\", **kwargs)\n         self.api_key = api_key or getenv(\"REPLICATE_API_TOKEN\")\n         if not self.api_key:\n             logger.error(\"REPLICATE_API_TOKEN not set. Please set the REPLICATE_API_TOKEN environment variable.\")\n         self.model = model\n-        self.register(self.generate_media)\n+\n+        tools: List[Any] = []\n+        tools.append(self.generate_media)\n+\n+        super().__init__(name=\"replicate_toolkit\", tools=tools, **kwargs)\n \n     def generate_media(self, agent: Union[Agent, Team], prompt: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/resend.py",
            "diff": "diff --git a/libs/agno/agno/tools/resend.py b/libs/agno/agno/tools/resend.py\nindex da909af1e..d8075587b 100644\n--- a/libs/agno/agno/tools/resend.py\n+++ b/libs/agno/agno/tools/resend.py\n@@ -1,5 +1,5 @@\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_info, logger\n@@ -17,14 +17,15 @@ class ResendTools(Toolkit):\n         from_email: Optional[str] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"resend_tools\", **kwargs)\n-\n         self.from_email = from_email\n         self.api_key = api_key or getenv(\"RESEND_API_KEY\")\n         if not self.api_key:\n             logger.error(\"No Resend API key provided\")\n \n-        self.register(self.send_email)\n+        tools: List[Any] = []\n+        tools.append(self.send_email)\n+\n+        super().__init__(name=\"resend_tools\", tools=tools, **kwargs)\n \n     def send_email(self, to_email: str, subject: str, body: str) -> str:\n         \"\"\"Send an email using the Resend API. Returns if the email was sent successfully or an error message.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/scrapegraph.py",
            "diff": "diff --git a/libs/agno/agno/tools/scrapegraph.py b/libs/agno/agno/tools/scrapegraph.py\nindex 1ba8182b5..5a5e44c17 100644\n--- a/libs/agno/agno/tools/scrapegraph.py\n+++ b/libs/agno/agno/tools/scrapegraph.py\n@@ -1,6 +1,6 @@\n import json\n import os\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n \n@@ -18,8 +18,6 @@ class ScrapeGraphTools(Toolkit):\n         markdownify: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"scrapegraph_tools\", **kwargs)\n-\n         self.api_key: Optional[str] = api_key or os.getenv(\"SGAI_API_KEY\")\n         self.client = Client(api_key=self.api_key)\n \n@@ -28,10 +26,13 @@ class ScrapeGraphTools(Toolkit):\n         if not smartscraper:\n             markdownify = True\n \n+        tools: List[Any] = []\n         if smartscraper:\n-            self.register(self.smartscraper)\n+            tools.append(self.smartscraper)\n         if markdownify:\n-            self.register(self.markdownify)\n+            tools.append(self.markdownify)\n+\n+        super().__init__(name=\"scrapegraph_tools\", tools=tools, **kwargs)\n \n     def smartscraper(self, url: str, prompt: str) -> str:\n         \"\"\"Use this function to extract structured data from a webpage using LLM.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/searxng.py",
            "diff": "diff --git a/libs/agno/agno/tools/searxng.py b/libs/agno/agno/tools/searxng.py\nindex ca128cc28..c5dbd6698 100644\n--- a/libs/agno/agno/tools/searxng.py\n+++ b/libs/agno/agno/tools/searxng.py\n@@ -1,6 +1,6 @@\n import json\n import urllib.parse\n-from typing import List, Optional\n+from typing import Any, List, Optional\n \n import httpx\n \n@@ -23,28 +23,29 @@ class Searxng(Toolkit):\n         videos: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"searxng\", **kwargs)\n-\n         self.host = host\n         self.engines = engines\n         self.fixed_max_results = fixed_max_results\n \n         self.register(self.search)\n \n+        tools: List[Any] = []\n         if images:\n-            self.register(self.image_search)\n+            tools.append(self.image_search)\n         if it:\n-            self.register(self.it_search)\n+            tools.append(self.it_search)\n         if map:\n-            self.register(self.map_search)\n+            tools.append(self.map_search)\n         if music:\n-            self.register(self.music_search)\n+            tools.append(self.music_search)\n         if news:\n-            self.register(self.news_search)\n+            tools.append(self.news_search)\n         if science:\n-            self.register(self.science_search)\n+            tools.append(self.science_search)\n         if videos:\n-            self.register(self.video_search)\n+            tools.append(self.video_search)\n+\n+        super().__init__(name=\"searxng\", tools=tools, **kwargs)\n \n     def search(self, query: str, max_results: int = 5) -> str:\n         \"\"\"Use this function to search the web.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/serpapi.py",
            "diff": "diff --git a/libs/agno/agno/tools/serpapi.py b/libs/agno/agno/tools/serpapi.py\nindex 392740cd4..200c78a68 100644\n--- a/libs/agno/agno/tools/serpapi.py\n+++ b/libs/agno/agno/tools/serpapi.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_info, logger\n@@ -18,15 +18,16 @@ class SerpApiTools(Toolkit):\n         search_youtube: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"serpapi_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"SERP_API_KEY\")\n         if not self.api_key:\n             logger.warning(\"No Serpapi API key provided\")\n \n-        self.register(self.search_google)\n+        tools: List[Any] = []\n+        tools.append(self.search_google)\n         if search_youtube:\n-            self.register(self.search_youtube)\n+            tools.append(self.search_youtube)\n+\n+        super().__init__(name=\"serpapi_tools\", tools=tools, **kwargs)\n \n     def search_google(self, query: str, num_results: int = 10) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/shell.py",
            "diff": "diff --git a/libs/agno/agno/tools/shell.py b/libs/agno/agno/tools/shell.py\nindex a6817621e..e3a85f161 100644\n--- a/libs/agno/agno/tools/shell.py\n+++ b/libs/agno/agno/tools/shell.py\n@@ -7,13 +7,14 @@ from agno.utils.log import log_debug, log_info, logger\n \n class ShellTools(Toolkit):\n     def __init__(self, base_dir: Optional[Union[Path, str]] = None, **kwargs):\n-        super().__init__(name=\"shell_tools\", **kwargs)\n-\n         self.base_dir: Optional[Path] = None\n         if base_dir is not None:\n             self.base_dir = Path(base_dir) if isinstance(base_dir, str) else base_dir\n \n-        self.register(self.run_shell_command)\n+        tools = []\n+        tools.append(self.run_shell_command)\n+\n+        super().__init__(name=\"shell_tools\", tools=tools, **kwargs)\n \n     def run_shell_command(self, args: List[str], tail: int = 100) -> str:\n         \"\"\"Runs a shell command and returns the output or error.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/slack.py",
            "diff": "diff --git a/libs/agno/agno/tools/slack.py b/libs/agno/agno/tools/slack.py\nindex 4cb88cb8c..08cf1261c 100644\n--- a/libs/agno/agno/tools/slack.py\n+++ b/libs/agno/agno/tools/slack.py\n@@ -21,17 +21,20 @@ class SlackTools(Toolkit):\n         get_channel_history: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"slack\", **kwargs)\n         self.token: Optional[str] = token or os.getenv(\"SLACK_TOKEN\")\n         if self.token is None or self.token == \"\":\n             raise ValueError(\"SLACK_TOKEN is not set\")\n         self.client = WebClient(token=self.token)\n+\n+        tools: List[Any] = []\n         if send_message:\n-            self.register(self.send_message)\n+            tools.append(self.send_message)\n         if list_channels:\n-            self.register(self.list_channels)\n+            tools.append(self.list_channels)\n         if get_channel_history:\n-            self.register(self.get_channel_history)\n+            tools.append(self.get_channel_history)\n+\n+        super().__init__(name=\"slack\", tools=tools, **kwargs)\n \n     def send_message(self, channel: str, text: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/sleep.py",
            "diff": "diff --git a/libs/agno/agno/tools/sleep.py b/libs/agno/agno/tools/sleep.py\nindex 32bddbb73..4dc2a6c0f 100644\n--- a/libs/agno/agno/tools/sleep.py\n+++ b/libs/agno/agno/tools/sleep.py\n@@ -6,9 +6,10 @@ from agno.utils.log import log_info\n \n class SleepTools(Toolkit):\n     def __init__(self, **kwargs):\n-        super().__init__(name=\"sleep\", **kwargs)\n+        tools = []\n+        tools.append(self.sleep)\n \n-        self.register(self.sleep)\n+        super().__init__(name=\"sleep\", tools=tools, **kwargs)\n \n     def sleep(self, seconds: int) -> str:\n         \"\"\"Use this function to sleep for a given number of seconds.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/spider.py",
            "diff": "diff --git a/libs/agno/agno/tools/spider.py b/libs/agno/agno/tools/spider.py\nindex 996b6ccb7..676cbe522 100644\n--- a/libs/agno/agno/tools/spider.py\n+++ b/libs/agno/agno/tools/spider.py\n@@ -5,7 +5,7 @@ try:\n except ImportError:\n     raise ImportError(\"`spider-client` not installed. Please install using `pip install spider-client`\")\n \n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools.toolkit import Toolkit\n from agno.utils.log import log_info, logger\n@@ -19,13 +19,16 @@ class SpiderTools(Toolkit):\n         optional_params: Optional[dict] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"spider\", **kwargs)\n         self.max_results = max_results\n         self.url = url\n         self.optional_params = optional_params or {}\n-        self.register(self.search)\n-        self.register(self.scrape)\n-        self.register(self.crawl)\n+\n+        tools: List[Any] = []\n+        tools.append(self.search)\n+        tools.append(self.scrape)\n+        tools.append(self.crawl)\n+\n+        super().__init__(name=\"spider\", tools=tools, **kwargs)\n \n     def search(self, query: str, max_results: int = 5) -> str:\n         \"\"\"Use this function to search the web.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/sql.py",
            "diff": "diff --git a/libs/agno/agno/tools/sql.py b/libs/agno/agno/tools/sql.py\nindex 325a8bd1d..ebb907a9c 100644\n--- a/libs/agno/agno/tools/sql.py\n+++ b/libs/agno/agno/tools/sql.py\n@@ -30,8 +30,6 @@ class SQLTools(Toolkit):\n         run_sql_query: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"sql_tools\", **kwargs)\n-\n         # Get the database engine\n         _engine: Optional[Engine] = db_engine\n         if _engine is None and db_url is not None:\n@@ -54,13 +52,15 @@ class SQLTools(Toolkit):\n         # Tables this toolkit can access\n         self.tables: Optional[Dict[str, Any]] = tables\n \n-        # Register functions in the toolkit\n+        tools: List[Any] = []\n         if list_tables:\n-            self.register(self.list_tables)\n+            tools.append(self.list_tables)\n         if describe_table:\n-            self.register(self.describe_table)\n+            tools.append(self.describe_table)\n         if run_sql_query:\n-            self.register(self.run_sql_query)\n+            tools.append(self.run_sql_query)\n+\n+        super().__init__(name=\"sql_tools\", tools=tools, **kwargs)\n \n     def list_tables(self) -> str:\n         \"\"\"Use this function to get a list of table names in the database.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/tavily.py",
            "diff": "diff --git a/libs/agno/agno/tools/tavily.py b/libs/agno/agno/tools/tavily.py\nindex c49071833..cd608465a 100644\n--- a/libs/agno/agno/tools/tavily.py\n+++ b/libs/agno/agno/tools/tavily.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Any, Dict, Literal, Optional\n+from typing import Any, Dict, List, Literal, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import logger\n@@ -23,8 +23,6 @@ class TavilyTools(Toolkit):\n         use_search_context: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"tavily_tools\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"TAVILY_API_KEY\")\n         if not self.api_key:\n             logger.error(\"TAVILY_API_KEY not provided\")\n@@ -35,11 +33,14 @@ class TavilyTools(Toolkit):\n         self.include_answer: bool = include_answer\n         self.format: Literal[\"json\", \"markdown\"] = format\n \n+        tools: List[Any] = []\n         if search:\n             if use_search_context:\n-                self.register(self.web_search_with_tavily)\n+                tools.append(self.web_search_with_tavily)\n             else:\n-                self.register(self.web_search_using_tavily)\n+                tools.append(self.web_search_using_tavily)\n+\n+        super().__init__(name=\"tavily_tools\", tools=tools, **kwargs)\n \n     def web_search_using_tavily(self, query: str, max_results: int = 5) -> str:\n         \"\"\"Use this function to search the web for a given query.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/telegram.py",
            "diff": "diff --git a/libs/agno/agno/tools/telegram.py b/libs/agno/agno/tools/telegram.py\nindex 7bbac0894..340ef5355 100644\n--- a/libs/agno/agno/tools/telegram.py\n+++ b/libs/agno/agno/tools/telegram.py\n@@ -1,5 +1,5 @@\n import os\n-from typing import Optional, Union\n+from typing import Any, List, Optional, Union\n \n import httpx\n \n@@ -11,15 +11,16 @@ class TelegramTools(Toolkit):\n     base_url = \"https://api.telegram.org\"\n \n     def __init__(self, chat_id: Union[str, int], token: Optional[str] = None, **kwargs):\n-        super().__init__(name=\"telegram\", **kwargs)\n-\n         self.token = token or os.getenv(\"TELEGRAM_TOKEN\")\n         if not self.token:\n             logger.error(\"TELEGRAM_TOKEN not set. Please set the TELEGRAM_TOKEN environment variable.\")\n \n         self.chat_id = chat_id\n \n-        self.register(self.send_message)\n+        tools: List[Any] = []\n+        tools.append(self.send_message)\n+\n+        super().__init__(name=\"telegram\", tools=tools, **kwargs)\n \n     def _call_post_method(self, method, *args, **kwargs):\n         return httpx.post(f\"{self.base_url}/bot{self.token}/{method}\", *args, **kwargs)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/thinking.py",
            "diff": "diff --git a/libs/agno/agno/tools/thinking.py b/libs/agno/agno/tools/thinking.py\nindex fa15336a5..cdca35491 100644\n--- a/libs/agno/agno/tools/thinking.py\n+++ b/libs/agno/agno/tools/thinking.py\n@@ -15,13 +15,6 @@ class ThinkingTools(Toolkit):\n         add_instructions: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            name=\"thinking_tools\",\n-            instructions=instructions,\n-            add_instructions=add_instructions,\n-            **kwargs,\n-        )\n-\n         if instructions is None:\n             self.instructions = dedent(\"\"\"\\\n             ## Using the think tool\n@@ -35,9 +28,17 @@ class ThinkingTools(Toolkit):\n             - Use the think tool generously to jot down thoughts and ideas.\\\n             \"\"\")\n \n+        tools = []\n         if think:\n-            # Register the think tool\n-            self.register(self.think)\n+            tools.append(self.think)\n+\n+        super().__init__(\n+            name=\"thinking_tools\",\n+            instructions=instructions,\n+            add_instructions=add_instructions,\n+            tools=tools,\n+            **kwargs,\n+        )\n \n     def think(self, agent: Union[Agent, Team], thought: str) -> str:\n         \"\"\"Use the tool to think about something.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/todoist.py",
            "diff": "diff --git a/libs/agno/agno/tools/todoist.py b/libs/agno/agno/tools/todoist.py\nindex 2deb43133..c7e463dbb 100644\n--- a/libs/agno/agno/tools/todoist.py\n+++ b/libs/agno/agno/tools/todoist.py\n@@ -38,29 +38,29 @@ class TodoistTools(Toolkit):\n             get_active_tasks: Whether to register the get_active_tasks function\n             get_projects: Whether to register the get_projects function\n         \"\"\"\n-        super().__init__(name=\"todoist\", **kwargs)\n-\n         self.api_token = api_token or os.getenv(\"TODOIST_API_TOKEN\")\n         if not self.api_token:\n             raise ValueError(\"TODOIST_API_TOKEN not set. Please set the TODOIST_API_TOKEN environment variable.\")\n \n         self.api = TodoistAPI(self.api_token)\n \n-        # Register enabled functions\n+        tools: List[Any] = []\n         if create_task:\n-            self.register(self.create_task)\n+            tools.append(self.create_task)\n         if get_task:\n-            self.register(self.get_task)\n+            tools.append(self.get_task)\n         if update_task:\n-            self.register(self.update_task)\n+            tools.append(self.update_task)\n         if close_task:\n-            self.register(self.close_task)\n+            tools.append(self.close_task)\n         if delete_task:\n-            self.register(self.delete_task)\n+            tools.append(self.delete_task)\n         if get_active_tasks:\n-            self.register(self.get_active_tasks)\n+            tools.append(self.get_active_tasks)\n         if get_projects:\n-            self.register(self.get_projects)\n+            tools.append(self.get_projects)\n+\n+        super().__init__(name=\"todoist\", tools=tools, **kwargs)\n \n     def _task_to_dict(self, task: Any) -> Dict[str, Any]:\n         \"\"\"Convert a Todoist task to a dictionary with proper typing.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/trello.py",
            "diff": "diff --git a/libs/agno/agno/tools/trello.py b/libs/agno/agno/tools/trello.py\nindex 31ca3a49a..7e2903555 100644\n--- a/libs/agno/agno/tools/trello.py\n+++ b/libs/agno/agno/tools/trello.py\n@@ -1,6 +1,6 @@\n import json\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -26,8 +26,6 @@ class TrelloTools(Toolkit):\n         list_boards: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(name=\"trello\", **kwargs)\n-\n         self.api_key = api_key or getenv(\"TRELLO_API_KEY\")\n         self.api_secret = api_secret or getenv(\"TRELLO_API_SECRET\")\n         self.token = token or getenv(\"TRELLO_TOKEN\")\n@@ -41,20 +39,23 @@ class TrelloTools(Toolkit):\n             logger.error(f\"Error initializing Trello client: {e}\")\n             self.client = None\n \n+        tools: List[Any] = []\n         if create_card:\n-            self.register(self.create_card)\n+            tools.append(self.create_card)\n         if get_board_lists:\n-            self.register(self.get_board_lists)\n+            tools.append(self.get_board_lists)\n         if move_card:\n-            self.register(self.move_card)\n+            tools.append(self.move_card)\n         if get_cards:\n-            self.register(self.get_cards)\n+            tools.append(self.get_cards)\n         if create_board:\n-            self.register(self.create_board)\n+            tools.append(self.create_board)\n         if create_list:\n-            self.register(self.create_list)\n+            tools.append(self.create_list)\n         if list_boards:\n-            self.register(self.list_boards)\n+            tools.append(self.list_boards)\n+\n+        super().__init__(name=\"trello\", tools=tools, **kwargs)\n \n     def create_card(self, board_id: str, list_name: str, card_title: str, description: str = \"\") -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/twilio.py",
            "diff": "diff --git a/libs/agno/agno/tools/twilio.py b/libs/agno/agno/tools/twilio.py\nindex 677f9b090..efe3191d8 100644\n--- a/libs/agno/agno/tools/twilio.py\n+++ b/libs/agno/agno/tools/twilio.py\n@@ -39,8 +39,6 @@ class TwilioTools(Toolkit):\n             edge: Optional Twilio edge location (e.g. 'sydney')\n             debug: Enable debug logging\n         \"\"\"\n-        super().__init__(name=\"twilio\", **kwargs)\n-\n         # Get credentials from environment if not provided\n         self.account_sid = account_sid or getenv(\"TWILIO_ACCOUNT_SID\")\n         self.auth_token = auth_token or getenv(\"TWILIO_AUTH_TOKEN\")\n@@ -85,9 +83,12 @@ class TwilioTools(Toolkit):\n             logging.basicConfig()\n             self.client.http_client.logger.setLevel(logging.INFO)\n \n-        self.register(self.send_sms)\n-        self.register(self.get_call_details)\n-        self.register(self.list_messages)\n+        tools: List[Any] = []\n+        tools.append(self.send_sms)\n+        tools.append(self.get_call_details)\n+        tools.append(self.list_messages)\n+\n+        super().__init__(name=\"twilio\", tools=tools, **kwargs)\n \n     @staticmethod\n     def validate_phone_number(phone: str) -> bool:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/user_control_flow.py",
            "diff": "diff --git a/libs/agno/agno/tools/user_control_flow.py b/libs/agno/agno/tools/user_control_flow.py\nnew file mode 100644\nindex 000000000..92ba6c3a0\n--- /dev/null\n+++ b/libs/agno/agno/tools/user_control_flow.py\n@@ -0,0 +1,58 @@\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.tools import Toolkit\n+\n+\n+class UserControlFlowTools(Toolkit):\n+    def __init__(\n+        self,\n+        instructions: Optional[str] = None,\n+        add_instructions: bool = True,\n+        **kwargs,\n+    ):\n+        \"\"\"A toolkit that provides the ability for the agent to interrupt the agent run and interact with the user.\"\"\"\n+\n+        tools = [self.get_user_input]\n+\n+        super().__init__(\n+            name=\"user_control_flow_tools\",\n+            instructions=instructions or self.DEFAULT_INSTRUCTIONS,\n+            add_instructions=add_instructions,\n+            tools=tools,\n+            **kwargs,\n+        )\n+\n+    def get_user_input(self, user_input_fields: list[dict]) -> str:\n+        \"\"\"Use this tool to get user input for the given fields. Provide all the fields that you require the user to fill in, as if they were filling in a form.\n+\n+        Args:\n+            user_input_fields (list[dict[str, str]]): A list of dictionaries, each containing the following keys:\n+                - field_name: The name of the field to get input for.\n+                - field_type: The type of the field to get input for. Only valid python types are supported (e.g. str, int, float, bool, list, dict, etc.).\n+                - field_description: A description of the field to get input for.\n+\n+        \"\"\"\n+        # Nothing needs to be executed here, the agent logic will interrupt the run and wait for the user input\n+        return \"User input received\"\n+\n+    # --------------------------------------------------------------------------------\n+    # Default instructions\n+    # --------------------------------------------------------------------------------\n+\n+    DEFAULT_INSTRUCTIONS = dedent(\n+        \"\"\"\\\n+        You have access to the `get_user_input` tool to get user input for the given fields.\n+\n+        1. **Get User Input**:\n+            - Purpose: When you have call a tool/function where you don't have enough information, don't say you can't do it, just use the `get_user_input` tool to get the information you need from the user.\n+            - Usage: Call `get_user_input` with the fields you require the user to fill in for you to continue your task.\n+\n+        ## IMPORTANT GUIDELINES\n+        - **Don't respond and ask the user for information.** Just use the `get_user_input` tool to get the information you need from the user.\n+        - **Don't make up information you don't have.** If you don't have the information, use the `get_user_input` tool to get the information you need from the user.\n+        - **Include only the required fields.** Include only the required fields in the `user_input_fields` parameter of the `get_user_input` tool. Don't include fields you already have the information for.\n+        - **Provide a clear and concise description of the field.** Clearly describe the field in the `field_description` parameter of the `user_input_fields` parameter of the `get_user_input` tool.\n+        - **Provide a type for the field.** Fill the `field_type` parameter of the `user_input_fields` parameter of the `get_user_input` tool with the type of the field.\n+        \"\"\"\n+    )\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/webbrowser.py",
            "diff": "diff --git a/libs/agno/agno/tools/webbrowser.py b/libs/agno/agno/tools/webbrowser.py\nindex b6c3ca5c9..3ae9276b2 100644\n--- a/libs/agno/agno/tools/webbrowser.py\n+++ b/libs/agno/agno/tools/webbrowser.py\n@@ -1,4 +1,5 @@\n import webbrowser\n+from typing import Any, List\n \n from agno.tools import Toolkit\n \n@@ -7,8 +8,10 @@ class WebBrowserTools(Toolkit):\n     \"\"\"Tools for opening a page on the web browser\"\"\"\n \n     def __init__(self):\n-        super().__init__(name=\"webbrowser_tools\")\n-        self.register(self.open_page)\n+        tools: List[Any] = []\n+        tools.append(self.open_page)\n+\n+        super().__init__(name=\"webbrowser_tools\", tools=tools)\n \n     def open_page(self, url: str, new_window: bool = False):\n         \"\"\"Open a URL in a browser window\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/webex.py",
            "diff": "diff --git a/libs/agno/agno/tools/webex.py b/libs/agno/agno/tools/webex.py\nindex 0d6a131cf..041448460 100644\n--- a/libs/agno/agno/tools/webex.py\n+++ b/libs/agno/agno/tools/webex.py\n@@ -1,6 +1,6 @@\n import json\n import os\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools.toolkit import Toolkit\n from agno.utils.log import logger\n@@ -16,17 +16,20 @@ class WebexTools(Toolkit):\n     def __init__(\n         self, send_message: bool = True, list_rooms: bool = True, access_token: Optional[str] = None, **kwargs\n     ):\n-        super().__init__(name=\"webex\", **kwargs)\n         if access_token is None:\n             access_token = os.getenv(\"WEBEX_ACCESS_TOKEN\")\n         if access_token is None:\n             raise ValueError(\"Webex access token is not set. Please set the WEBEX_ACCESS_TOKEN environment variable.\")\n \n         self.client = WebexAPI(access_token=access_token)\n+\n+        tools: List[Any] = []\n         if send_message:\n-            self.register(self.send_message)\n+            tools.append(self.send_message)\n         if list_rooms:\n-            self.register(self.list_rooms)\n+            tools.append(self.list_rooms)\n+\n+        super().__init__(name=\"webex\", tools=tools, **kwargs)\n \n     def send_message(self, room_id: str, text: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/website.py",
            "diff": "diff --git a/libs/agno/agno/tools/website.py b/libs/agno/agno/tools/website.py\nindex 5ccba563d..bd95f4246 100644\n--- a/libs/agno/agno/tools/website.py\n+++ b/libs/agno/agno/tools/website.py\n@@ -1,5 +1,5 @@\n import json\n-from typing import List, Optional, Union, cast\n+from typing import Any, List, Optional, Union, cast\n \n from agno.document import Document\n from agno.knowledge.combined import CombinedKnowledgeBase\n@@ -10,16 +10,18 @@ from agno.utils.log import log_debug\n \n class WebsiteTools(Toolkit):\n     def __init__(self, knowledge_base: Optional[Union[WebsiteKnowledgeBase, CombinedKnowledgeBase]] = None, **kwargs):\n-        super().__init__(name=\"website_tools\", **kwargs)\n         self.knowledge_base: Optional[Union[WebsiteKnowledgeBase, CombinedKnowledgeBase]] = knowledge_base\n \n+        tools: List[Any] = []\n         if self.knowledge_base is not None:\n             if isinstance(self.knowledge_base, WebsiteKnowledgeBase):\n-                self.register(self.add_website_to_knowledge_base)\n+                tools.append(self.add_website_to_knowledge_base)\n             elif isinstance(self.knowledge_base, CombinedKnowledgeBase):\n-                self.register(self.add_website_to_combined_knowledge_base)\n+                tools.append(self.add_website_to_combined_knowledge_base)\n         else:\n-            self.register(self.read_url)\n+            tools.append(self.read_url)\n+\n+        super().__init__(name=\"website_tools\", tools=tools, **kwargs)\n \n     def add_website_to_knowledge_base(self, url: str) -> str:\n         \"\"\"This function adds a websites content to the knowledge base.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/whatsapp.py",
            "diff": "diff --git a/libs/agno/agno/tools/whatsapp.py b/libs/agno/agno/tools/whatsapp.py\nindex 0f7ddf396..3d7017441 100644\n--- a/libs/agno/agno/tools/whatsapp.py\n+++ b/libs/agno/agno/tools/whatsapp.py\n@@ -29,8 +29,6 @@ class WhatsAppTools(Toolkit):\n             recipient_waid: Default recipient WhatsApp ID (optional)\n             async_mode: Whether to use async methods (default: False)\n         \"\"\"\n-        super().__init__(name=\"whatsapp\")\n-\n         # Core credentials\n         self.access_token = access_token or os.getenv(\"WHATSAPP_ACCESS_TOKEN\") or os.getenv(\"WHATSAPP_ACCESS_TOKEN\")\n         if not self.access_token:\n@@ -53,13 +51,15 @@ class WhatsAppTools(Toolkit):\n         self.version = version or os.getenv(\"WHATSAPP_VERSION\") or os.getenv(\"WHATSAPP_VERSION\", \"v22.0\")\n         self.async_mode = async_mode\n \n-        # Register methods that can be used by the agent based on mode\n+        tools: List[Any] = []\n         if self.async_mode:\n-            self.register(self.send_text_message_async)\n-            self.register(self.send_template_message_async)\n+            tools.append(self.send_text_message_async)\n+            tools.append(self.send_template_message_async)\n         else:\n-            self.register(self.send_text_message_sync)\n-            self.register(self.send_template_message_sync)\n+            tools.append(self.send_text_message_sync)\n+            tools.append(self.send_template_message_sync)\n+\n+        super().__init__(name=\"whatsapp\", tools=tools)\n \n     def _get_headers(self) -> Dict[str, str]:\n         \"\"\"Get headers for API requests.\"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/wikipedia.py",
            "diff": "diff --git a/libs/agno/agno/tools/wikipedia.py b/libs/agno/agno/tools/wikipedia.py\nindex c086a1e9f..c9a846b1e 100644\n--- a/libs/agno/agno/tools/wikipedia.py\n+++ b/libs/agno/agno/tools/wikipedia.py\n@@ -9,13 +9,15 @@ from agno.utils.log import log_debug, log_info\n \n class WikipediaTools(Toolkit):\n     def __init__(self, knowledge_base: Optional[WikipediaKnowledgeBase] = None, **kwargs):\n-        super().__init__(name=\"wikipedia_tools\", **kwargs)\n-        self.knowledge_base: Optional[WikipediaKnowledgeBase] = knowledge_base\n+        tools = []\n \n+        self.knowledge_base: Optional[WikipediaKnowledgeBase] = knowledge_base\n         if self.knowledge_base is not None and isinstance(self.knowledge_base, WikipediaKnowledgeBase):\n-            self.register(self.search_wikipedia_and_update_knowledge_base)\n+            tools.append(self.search_wikipedia_and_update_knowledge_base)\n         else:\n-            self.register(self.search_wikipedia)\n+            tools.append(self.search_wikipedia)  # type: ignore\n+\n+        super().__init__(name=\"wikipedia_tools\", tools=tools, **kwargs)\n \n     def search_wikipedia_and_update_knowledge_base(self, topic: str) -> str:\n         \"\"\"This function searches wikipedia for a topic, adds the results to the knowledge base and returns them.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/x.py",
            "diff": "diff --git a/libs/agno/agno/tools/x.py b/libs/agno/agno/tools/x.py\nindex 631d74614..144d9440d 100644\n--- a/libs/agno/agno/tools/x.py\n+++ b/libs/agno/agno/tools/x.py\n@@ -1,6 +1,6 @@\n import json\n import os\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_info, logger\n@@ -31,8 +31,6 @@ class XTools(Toolkit):\n             access_token Optional[str]: The access token for Twitter API.\n             access_token_secret Optional[str]: The access token secret for Twitter API.\n         \"\"\"\n-        super().__init__(name=\"x\", **kwargs)\n-\n         self.bearer_token = bearer_token or os.getenv(\"X_BEARER_TOKEN\")\n         self.consumer_key = consumer_key or os.getenv(\"X_CONSUMER_KEY\")\n         self.consumer_secret = consumer_secret or os.getenv(\"X_CONSUMER_SECRET\")\n@@ -47,11 +45,14 @@ class XTools(Toolkit):\n             access_token_secret=self.access_token_secret,\n         )\n \n-        self.register(self.create_post)\n-        self.register(self.reply_to_post)\n-        self.register(self.send_dm)\n-        self.register(self.get_user_info)\n-        self.register(self.get_home_timeline)\n+        tools: List[Any] = []\n+        tools.append(self.create_post)\n+        tools.append(self.reply_to_post)\n+        tools.append(self.send_dm)\n+        tools.append(self.get_user_info)\n+        tools.append(self.get_home_timeline)\n+\n+        super().__init__(name=\"x\", tools=tools, **kwargs)\n \n     def create_post(self, text: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/youtube.py",
            "diff": "diff --git a/libs/agno/agno/tools/youtube.py b/libs/agno/agno/tools/youtube.py\nindex 1f8c10eec..010398a55 100644\n--- a/libs/agno/agno/tools/youtube.py\n+++ b/libs/agno/agno/tools/youtube.py\n@@ -24,16 +24,18 @@ class YouTubeTools(Toolkit):\n         proxies: Optional[Dict[str, Any]] = None,\n         **kwargs,\n     ):\n-        super().__init__(name=\"youtube_tools\", **kwargs)\n-\n         self.languages: Optional[List[str]] = languages\n         self.proxies: Optional[Dict[str, Any]] = proxies\n+\n+        tools: List[Any] = []\n         if get_video_captions:\n-            self.register(self.get_youtube_video_captions)\n+            tools.append(self.get_youtube_video_captions)\n         if get_video_data:\n-            self.register(self.get_youtube_video_data)\n+            tools.append(self.get_youtube_video_data)\n         if get_video_timestamps:\n-            self.register(self.get_video_timestamps)\n+            tools.append(self.get_video_timestamps)\n+\n+        super().__init__(name=\"youtube_tools\", tools=tools, **kwargs)\n \n     def get_youtube_video_id(self, url: str) -> Optional[str]:\n         \"\"\"Function to get the video ID from a YouTube URL.\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/zendesk.py",
            "diff": "diff --git a/libs/agno/agno/tools/zendesk.py b/libs/agno/agno/tools/zendesk.py\nindex 8a3062017..214a77f4c 100644\n--- a/libs/agno/agno/tools/zendesk.py\n+++ b/libs/agno/agno/tools/zendesk.py\n@@ -1,7 +1,7 @@\n import json\n import re\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, logger\n@@ -34,7 +34,6 @@ class ZendeskTools(Toolkit):\n         password (str): The password for Zendesk API authentication.\n         company_name (str): The company name to form the base URL for API requests.\n         \"\"\"\n-        super().__init__(name=\"zendesk_tools\", **kwargs)\n         self.username = username or getenv(\"ZENDESK_USERNAME\")\n         self.password = password or getenv(\"ZENDESK_PW\")\n         self.company_name = company_name or getenv(\"ZENDESK_COMPANY_NAME\")\n@@ -42,7 +41,10 @@ class ZendeskTools(Toolkit):\n         if not self.username or not self.password or not self.company_name:\n             logger.error(\"Username, password, or company name not provided.\")\n \n-        self.register(self.search_zendesk)\n+        tools: List[Any] = []\n+        tools.append(self.search_zendesk)\n+\n+        super().__init__(name=\"zendesk_tools\", tools=tools, **kwargs)\n \n     def search_zendesk(self, search_string: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/zep.py",
            "diff": "diff --git a/libs/agno/agno/tools/zep.py b/libs/agno/agno/tools/zep.py\nindex 0e7526b72..a63a8a5a3 100644\n--- a/libs/agno/agno/tools/zep.py\n+++ b/libs/agno/agno/tools/zep.py\n@@ -1,7 +1,7 @@\n import uuid\n from os import getenv\n from textwrap import dedent\n-from typing import List, Optional\n+from typing import Any, List, Optional\n \n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_error, log_warning\n@@ -43,8 +43,6 @@ class ZepTools(Toolkit):\n         add_instructions: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"zep_tools\", instructions=instructions, add_instructions=add_instructions, **kwargs)\n-\n         self._api_key = api_key or getenv(\"ZEP_API_KEY\")\n         if not self._api_key:\n             raise ValueError(\n@@ -66,13 +64,17 @@ class ZepTools(Toolkit):\n \n         self.initialize()\n \n-        # Register methods as tools conditionally\n+        tools: List[Any] = []\n         if add_zep_message:\n-            self.register(self.add_zep_message)\n+            tools.append(self.add_zep_message)\n         if get_zep_memory:\n-            self.register(self.get_zep_memory)\n+            tools.append(self.get_zep_memory)\n         if search_zep_memory:\n-            self.register(self.search_zep_memory)\n+            tools.append(self.search_zep_memory)\n+\n+        super().__init__(\n+            name=\"zep_tools\", instructions=instructions, add_instructions=add_instructions, tools=tools, **kwargs\n+        )\n \n     def initialize(self) -> bool:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/tools/zoom.py",
            "diff": "diff --git a/libs/agno/agno/tools/zoom.py b/libs/agno/agno/tools/zoom.py\nindex 9112fda0a..1cecf797d 100644\n--- a/libs/agno/agno/tools/zoom.py\n+++ b/libs/agno/agno/tools/zoom.py\n@@ -2,7 +2,7 @@ import json\n from base64 import b64encode\n from datetime import datetime, timedelta\n from os import getenv\n-from typing import Optional\n+from typing import Any, List, Optional\n \n import requests\n \n@@ -27,8 +27,6 @@ class ZoomTools(Toolkit):\n             client_secret (str): The client secret for authentication. If not provided, will use ZOOM_CLIENT_SECRET env var.\n             name (str): The name of the tool. Defaults to \"zoom_tool\".\n         \"\"\"\n-        super().__init__(name=\"zoom_tool\", **kwargs)\n-\n         # Get credentials from env vars if not provided\n         self.account_id = account_id or getenv(\"ZOOM_ACCOUNT_ID\")\n         self.client_id = client_id or getenv(\"ZOOM_CLIENT_ID\")\n@@ -41,13 +39,16 @@ class ZoomTools(Toolkit):\n                 \"ZOOM_ACCOUNT_ID, ZOOM_CLIENT_ID, and ZOOM_CLIENT_SECRET must be set either through parameters or environment variables.\"\n             )\n \n-        # Register functions\n-        self.register(self.schedule_meeting)\n-        self.register(self.get_upcoming_meetings)\n-        self.register(self.list_meetings)\n-        self.register(self.get_meeting_recordings)\n-        self.register(self.delete_meeting)\n-        self.register(self.get_meeting)\n+        tools: List[Any] = []\n+\n+        tools.append(self.schedule_meeting)\n+        tools.append(self.get_upcoming_meetings)\n+        tools.append(self.list_meetings)\n+        tools.append(self.get_meeting_recordings)\n+        tools.append(self.delete_meeting)\n+        tools.append(self.get_meeting)\n+\n+        super().__init__(name=\"zoom_tool\", tools=tools, **kwargs)\n \n     def get_access_token(self) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/utils/models/claude.py",
            "diff": "diff --git a/libs/agno/agno/utils/models/claude.py b/libs/agno/agno/utils/models/claude.py\nindex be57dc5fd..c529c5a2b 100644\n--- a/libs/agno/agno/utils/models/claude.py\n+++ b/libs/agno/agno/utils/models/claude.py\n@@ -50,6 +50,10 @@ def _format_image_for_message(image: Image) -> Optional[Dict[str, Any]]:\n     }\n \n     try:\n+        # Case 0: Image is an Anthropic uploaded file\n+        if image.content is not None and hasattr(image.content, \"id\"):\n+            return {\"type\": \"image\", \"source\": {\"type\": \"file\", \"file_id\": image.content.id}}\n+\n         # Case 1: Image is a URL\n         if image.url is not None:\n             return {\"type\": \"image\", \"source\": {\"type\": \"url\", \"url\": image.url}}\n@@ -117,6 +121,16 @@ def _format_file_for_message(file: File) -> Optional[Dict[str, Any]]:\n         \"text/plain\": \"text\",\n     }\n \n+    # Case 0: File is an Anthropic uploaded file\n+    if file.external is not None and hasattr(file.external, \"id\"):\n+        return {\n+            \"type\": \"document\",\n+            \"source\": {\n+                \"type\": \"file\",\n+                \"file_id\": file.external.id,\n+            },\n+        }\n+\n     # Case 1: Document is a URL\n     if file.url is not None:\n         return {\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/agno/utils/models/llama.py",
            "diff": "diff --git a/libs/agno/agno/utils/models/llama.py b/libs/agno/agno/utils/models/llama.py\nindex df29f5b02..c732494cf 100644\n--- a/libs/agno/agno/utils/models/llama.py\n+++ b/libs/agno/agno/utils/models/llama.py\n@@ -4,8 +4,22 @@ from agno.agent import Message\n from agno.utils.log import log_warning\n from agno.utils.openai import process_image\n \n+ROLE_MAP = {\n+    \"user\": \"user\",\n+    \"assistant\": \"assistant\",\n+    \"system\": \"system\",\n+    \"tool\": \"tool\",\n+}\n \n-def format_message(message: Message, openai_like: bool = False) -> Dict[str, Any]:\n+TOOL_CALL_ROLE_MAP = {\n+    \"user\": \"user\",\n+    \"assistant\": \"assistant\",\n+    \"system\": \"user\",\n+    \"tool\": \"tool\",\n+}\n+\n+\n+def format_message(message: Message, openai_like: bool = False, tool_calls: bool = False) -> Dict[str, Any]:\n     \"\"\"\n     Format a message into the format expected by Llama API.\n \n@@ -17,7 +31,7 @@ def format_message(message: Message, openai_like: bool = False) -> Dict[str, Any\n         Dict[str, Any]: The formatted message.\n     \"\"\"\n     message_dict: Dict[str, Any] = {\n-        \"role\": message.role,\n+        \"role\": ROLE_MAP[message.role] if not tool_calls else TOOL_CALL_ROLE_MAP[message.role],\n         \"content\": [{\"type\": \"text\", \"text\": message.content or \" \"}],\n         \"name\": message.name,\n         \"tool_call_id\": message.tool_call_id,\n@@ -45,19 +59,16 @@ def format_message(message: Message, openai_like: bool = False) -> Dict[str, Any\n         }\n \n     if message.role == \"assistant\":\n+        text_content = {\"type\": \"text\", \"text\": message.content or \" \"}\n+\n         if message.tool_calls is not None and len(message.tool_calls) > 0:\n             message_dict = {\n-                \"content\": {\n-                    \"type\": \"text\",\n-                    \"text\": message.content or \" \",\n-                },\n+                \"content\": [text_content] if openai_like else text_content,\n                 \"role\": \"assistant\",\n                 \"tool_calls\": message.tool_calls,\n                 \"stop_reason\": \"tool_calls\",\n             }\n         else:\n-            text_content = {\"type\": \"text\", \"text\": message.content or \" \"}\n-\n             message_dict = {\n                 \"role\": \"assistant\",\n                 \"content\": [text_content] if openai_like else text_content,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py b/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py\nindex aea583356..8f1a44b23 100644\n--- a/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py\n+++ b/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py\n@@ -5,9 +5,11 @@ import uuid\n import pytest\n \n from agno.agent.agent import Agent\n+from agno.memory.agent import AgentMemory\n from agno.memory.v2.db.sqlite import SqliteMemoryDb\n from agno.memory.v2.memory import Memory\n from agno.models.anthropic.claude import Claude\n+from agno.models.message import Message\n from agno.models.openai.chat import OpenAIChat\n from agno.storage.sqlite import SqliteStorage\n \n@@ -65,6 +67,16 @@ def memory(memory_db):\n @pytest.fixture\n def chat_agent(agent_storage, memory):\n     \"\"\"Create an agent with storage and memory for testing.\"\"\"\n+    return Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        storage=agent_storage,\n+        memory=memory,\n+    )\n+\n+\n+@pytest.fixture\n+def memory_agent(agent_storage, memory):\n+    \"\"\"Create an agent that creates memories.\"\"\"\n     return Agent(\n         model=OpenAIChat(id=\"gpt-4o-mini\"),\n         storage=agent_storage,\n@@ -73,8 +85,49 @@ def chat_agent(agent_storage, memory):\n     )\n \n \n+def test_agent_runs_in_memory(chat_agent):\n+    session_id = \"test_session\"\n+    response = chat_agent.run(\"Hello, how are you?\", session_id=session_id)\n+    assert response is not None\n+    assert response.content is not None\n+    assert response.run_id is not None\n+\n+    assert len(chat_agent.memory.runs[session_id]) == 1\n+    stored_run_response = chat_agent.memory.runs[session_id][0]\n+    assert stored_run_response.run_id == response.run_id\n+    assert len(stored_run_response.messages) == 2\n+\n+    # Check that the run is also stored in the agent session\n+    assert len(chat_agent.agent_session.memory[\"runs\"]) == 1\n+\n+\n+def test_agent_runs_in_memory_legacy(chat_agent):\n+    chat_agent.memory = AgentMemory()\n+    session_id = \"test_session\"\n+    response = chat_agent.run(\n+        \"What can you do?\",\n+        messages=[\n+            Message(role=\"user\", content=\"Hello, how are you?\"),\n+            Message(role=\"assistant\", content=\"I'm good, thank you!\"),\n+        ],\n+        session_id=session_id,\n+    )\n+    assert response is not None\n+    assert response.content is not None\n+    assert response.run_id is not None\n+\n+    assert len(chat_agent.memory.runs) == 1\n+    stored_agent_run = chat_agent.memory.runs[0]\n+    assert stored_agent_run.response.run_id == response.run_id\n+    assert len(stored_agent_run.response.messages) == 4\n+    assert len(stored_agent_run.messages) == 2\n+\n+    # Check that the run is also stored in the agent session\n+    assert len(chat_agent.agent_session.memory[\"runs\"]) == 1\n+\n+\n @pytest.mark.asyncio\n-async def test_multi_user_multi_session_chat(chat_agent, agent_storage, memory):\n+async def test_multi_user_multi_session_chat(memory_agent, agent_storage, memory):\n     \"\"\"Test multi-user multi-session chat with storage and memory.\"\"\"\n     # Define user and session IDs\n     user_1_id = \"user_1@example.com\"\n@@ -90,26 +143,26 @@ async def test_multi_user_multi_session_chat(chat_agent, agent_storage, memory):\n     memory.clear()\n \n     # Chat with user 1 - Session 1\n-    await chat_agent.arun(\n+    await memory_agent.arun(\n         \"My name is Mark Gonzales and I like anime and video games.\", user_id=user_1_id, session_id=user_1_session_1_id\n     )\n-    await chat_agent.arun(\n+    await memory_agent.arun(\n         \"I also enjoy reading manga and playing video games.\", user_id=user_1_id, session_id=user_1_session_1_id\n     )\n \n     # Chat with user 1 - Session 2\n-    await chat_agent.arun(\"I'm going to the movies tonight.\", user_id=user_1_id, session_id=user_1_session_2_id)\n+    await memory_agent.arun(\"I'm going to the movies tonight.\", user_id=user_1_id, session_id=user_1_session_2_id)\n \n     # Chat with user 2\n-    await chat_agent.arun(\"Hi my name is John Doe.\", user_id=user_2_id, session_id=user_2_session_1_id)\n-    await chat_agent.arun(\"I'm planning to hike this weekend.\", user_id=user_2_id, session_id=user_2_session_1_id)\n+    await memory_agent.arun(\"Hi my name is John Doe.\", user_id=user_2_id, session_id=user_2_session_1_id)\n+    await memory_agent.arun(\"I'm planning to hike this weekend.\", user_id=user_2_id, session_id=user_2_session_1_id)\n \n     # Chat with user 3\n-    await chat_agent.arun(\"Hi my name is Jane Smith.\", user_id=user_3_id, session_id=user_3_session_1_id)\n-    await chat_agent.arun(\"I'm going to the gym tomorrow.\", user_id=user_3_id, session_id=user_3_session_1_id)\n+    await memory_agent.arun(\"Hi my name is Jane Smith.\", user_id=user_3_id, session_id=user_3_session_1_id)\n+    await memory_agent.arun(\"I'm going to the gym tomorrow.\", user_id=user_3_id, session_id=user_3_session_1_id)\n \n     # Continue the conversation with user 1\n-    await chat_agent.arun(\"What do you suggest I do this weekend?\", user_id=user_1_id, session_id=user_1_session_1_id)\n+    await memory_agent.arun(\"What do you suggest I do this weekend?\", user_id=user_1_id, session_id=user_1_session_1_id)\n \n     # Verify storage DB has the right sessions\n     all_session_ids = agent_storage.get_all_session_ids()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/agent/test_external_execution_flows.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_external_execution_flows.py b/libs/agno/tests/integration/agent/test_external_execution_flows.py\nindex 7e692a3d9..51e84a529 100644\n--- a/libs/agno/tests/integration/agent/test_external_execution_flows.py\n+++ b/libs/agno/tests/integration/agent/test_external_execution_flows.py\n@@ -87,7 +87,9 @@ async def test_tool_call_requires_external_execution_async():\n         monitoring=False,\n     )\n \n-    response = await agent.arun(\"What is the weather in Tokyo?\")\n+    response = await agent.arun(\n+        \"Send an email to john@doe.com with the subject 'Test' and the body 'Hello, how are you?'\"\n+    )\n \n     assert response.is_paused\n     assert response.tools[0].external_execution_required\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/agent/test_user_confirmation_flows.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_user_confirmation_flows.py b/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\nindex 861059ba7..48049e48a 100644\n--- a/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\n+++ b/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\n@@ -19,6 +19,64 @@ def test_tool_call_requires_confirmation():\n         monitoring=False,\n     )\n \n+    agent.run(\"What is the weather in Tokyo?\")\n+\n+    assert agent.run_response.is_paused\n+    assert agent.run_response.tools[0].requires_confirmation\n+    assert agent.run_response.tools[0].tool_name == \"get_the_weather\"\n+    assert agent.run_response.tools[0].tool_args == {\"city\": \"Tokyo\"}\n+\n+    # Mark the tool as confirmed\n+    agent.run_response.tools[0].confirmed = True\n+\n+    agent.continue_run()\n+    assert agent.run_response.is_paused is False\n+    assert agent.run_response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+\n+def test_tool_call_requires_confirmation_continue_with_run_response():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo?\")\n+\n+    assert response.is_paused\n+    assert response.tools[0].requires_confirmation\n+    assert response.tools[0].tool_name == \"get_the_weather\"\n+    assert response.tools[0].tool_args == {\"city\": \"Tokyo\"}\n+\n+    # Mark the tool as confirmed\n+    response.tools[0].confirmed = True\n+\n+    response = agent.continue_run(response)\n+    assert response.is_paused is False\n+    assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+\n+def test_tool_call_requires_confirmation_continue_with_run_id():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n     response = agent.run(\"What is the weather in Tokyo?\")\n \n     assert response.is_paused\n@@ -29,10 +87,48 @@ def test_tool_call_requires_confirmation():\n     # Mark the tool as confirmed\n     response.tools[0].confirmed = True\n \n+    response = agent.continue_run(run_id=response.run_id)\n+    assert response.is_paused is False\n+    assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+\n+def test_tool_call_requires_confirmation_memory_footprint():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    session_id = \"test_session\"\n+\n+    response = agent.run(\"What is the weather in Tokyo?\", session_id=session_id)\n+\n+    assert len(agent.memory.runs[session_id]) == 1, \"There should be one run in the memory\"\n+    assert len(agent.memory.runs[session_id][0].messages) == 3, (\n+        \"There should be three messages in the run (system, user, assistant)\"\n+    )\n+\n+    assert response.is_paused\n+\n+    # Mark the tool as confirmed\n+    response.tools[0].confirmed = True\n+\n     response = agent.continue_run(response)\n     assert response.is_paused is False\n     assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n \n+    assert len(agent.memory.runs[session_id]) == 1, \"There should be one run in the memory\"\n+    assert len(agent.memory.runs[session_id][0].messages) == 5, (\n+        \"There should be five messages in the run (system, user, assistant, tool call, assistant)\"\n+    )\n+\n \n def test_tool_call_requires_confirmation_stream():\n     @tool(requires_confirmation=True)\n@@ -97,27 +193,6 @@ async def test_tool_call_requires_confirmation_async():\n     assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n \n \n-def test_tool_call_requires_confirmation_error():\n-    @tool(requires_confirmation=True)\n-    def get_the_weather(city: str):\n-        return f\"It is currently 70 degrees and cloudy in {city}\"\n-\n-    agent = Agent(\n-        model=OpenAIChat(id=\"gpt-4o-mini\"),\n-        tools=[get_the_weather],\n-        show_tool_calls=True,\n-        markdown=True,\n-        telemetry=False,\n-        monitoring=False,\n-    )\n-\n-    response = agent.run(\"What is the weather in Tokyo?\")\n-\n-    # Check that we cannot continue without confirmation\n-    with pytest.raises(ValueError):\n-        response = agent.continue_run(response)\n-\n-\n @pytest.mark.asyncio\n async def test_tool_call_requires_confirmation_stream_async():\n     @tool(requires_confirmation=True)\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/anthropic/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/anthropic/test_tool_use.py b/libs/agno/tests/integration/models/anthropic/test_tool_use.py\nindex e12c4f584..3ddaa3f52 100644\n--- a/libs/agno/tests/integration/models/anthropic/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/anthropic/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/aws/bedrock/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/aws/bedrock/test_tool_use.py b/libs/agno/tests/integration/models/aws/bedrock/test_tool_use.py\nindex 94bbbde8a..462d62ea2 100644\n--- a/libs/agno/tests/integration/models/aws/bedrock/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/aws/bedrock/test_tool_use.py\n@@ -44,7 +44,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/aws/claude/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/aws/claude/test_tool_use.py b/libs/agno/tests/integration/models/aws/claude/test_tool_use.py\nindex ee8ee0c81..b14a93f09 100644\n--- a/libs/agno/tests/integration/models/aws/claude/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/aws/claude/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -98,7 +98,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/azure/ai_foundry/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/azure/ai_foundry/test_tool_use.py b/libs/agno/tests/integration/models/azure/ai_foundry/test_tool_use.py\nindex 947b50e83..7cea842c9 100644\n--- a/libs/agno/tests/integration/models/azure/ai_foundry/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/azure/ai_foundry/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/azure/openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/azure/openai/test_tool_use.py b/libs/agno/tests/integration/models/azure/openai/test_tool_use.py\nindex 648d4b142..060be9ade 100644\n--- a/libs/agno/tests/integration/models/azure/openai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/azure/openai/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/cerebras/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras/test_tool_use.py b/libs/agno/tests/integration/models/cerebras/test_tool_use.py\nindex b5a68e22e..f74fe3012 100644\n--- a/libs/agno/tests/integration/models/cerebras/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/cerebras/test_tool_use.py\n@@ -40,7 +40,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -85,7 +85,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py b/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\nindex 5b76dbf55..2113781c6 100644\n--- a/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\n@@ -40,7 +40,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -85,7 +85,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/cohere/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cohere/test_tool_use.py b/libs/agno/tests/integration/models/cohere/test_tool_use.py\nindex 02bef4b06..bdd0bfe30 100644\n--- a/libs/agno/tests/integration/models/cohere/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/cohere/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -98,7 +98,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/deepinfra/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/deepinfra/test_tool_use.py b/libs/agno/tests/integration/models/deepinfra/test_tool_use.py\nindex 56c8f52c0..c3e9b20ff 100644\n--- a/libs/agno/tests/integration/models/deepinfra/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/deepinfra/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -98,7 +98,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/deepseek/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/deepseek/test_tool_use.py b/libs/agno/tests/integration/models/deepseek/test_tool_use.py\nindex 8704a5413..6c645d85e 100644\n--- a/libs/agno/tests/integration/models/deepseek/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/deepseek/test_tool_use.py\n@@ -48,7 +48,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -99,7 +99,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/fireworks/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/fireworks/test_tool_use.py b/libs/agno/tests/integration/models/fireworks/test_tool_use.py\nindex af87eac26..c04cac20c 100644\n--- a/libs/agno/tests/integration/models/fireworks/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/fireworks/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/google/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/google/test_tool_use.py b/libs/agno/tests/integration/models/google/test_tool_use.py\nindex eda08ce3b..fedac8771 100644\n--- a/libs/agno/tests/integration/models/google/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/google/test_tool_use.py\n@@ -48,7 +48,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -101,7 +101,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/groq/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/groq/test_tool_use.py b/libs/agno/tests/integration/models/groq/test_tool_use.py\nindex 5b9d50fc8..20327b5ee 100644\n--- a/libs/agno/tests/integration/models/groq/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/groq/test_tool_use.py\n@@ -47,7 +47,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -94,7 +94,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/huggingface/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/huggingface/test_tool_use.py b/libs/agno/tests/integration/models/huggingface/test_tool_use.py\nindex e8c9809e3..1a55602ae 100644\n--- a/libs/agno/tests/integration/models/huggingface/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/huggingface/test_tool_use.py\n@@ -45,7 +45,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/ibm/watsonx/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/ibm/watsonx/test_tool_use.py b/libs/agno/tests/integration/models/ibm/watsonx/test_tool_use.py\nindex a49d65cfb..65fdd4ff4 100644\n--- a/libs/agno/tests/integration/models/ibm/watsonx/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/ibm/watsonx/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/litellm/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/litellm/test_tool_use.py b/libs/agno/tests/integration/models/litellm/test_tool_use.py\nindex 0a006f5ee..a7af05728 100644\n--- a/libs/agno/tests/integration/models/litellm/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/litellm/test_tool_use.py\n@@ -73,7 +73,7 @@ def test_tool_use_stream():\n         responses.append(chunk)\n         print(chunk.content)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -129,7 +129,7 @@ async def test_async_tool_use_streaming():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/litellm_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/litellm_openai/test_tool_use.py b/libs/agno/tests/integration/models/litellm_openai/test_tool_use.py\nindex cb75e4c3b..081f165f4 100644\n--- a/libs/agno/tests/integration/models/litellm_openai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/litellm_openai/test_tool_use.py\n@@ -72,7 +72,7 @@ def test_tool_use_stream():\n         responses.append(chunk)\n         print(chunk.content)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -128,7 +128,7 @@ async def test_async_tool_use_streaming():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/lmstudio/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/lmstudio/test_tool_use.py b/libs/agno/tests/integration/models/lmstudio/test_tool_use.py\nindex 7259f8e22..f402b71bf 100644\n--- a/libs/agno/tests/integration/models/lmstudio/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/lmstudio/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/meta/llama/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama/test_tool_use.py b/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\nindex 4be765f82..2a3efc872 100644\n--- a/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\n@@ -43,7 +43,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -90,7 +90,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py b/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py\nindex 6bb5b6436..8b87c935e 100644\n--- a/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py\n+++ b/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py\n@@ -2,7 +2,7 @@ from pathlib import Path\n \n from agno.agent.agent import Agent\n from agno.media import Image\n-from agno.models.meta import Llama\n+from agno.models.meta import LlamaOpenAI\n from agno.utils.media import download_image\n \n image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n@@ -15,7 +15,7 @@ download_image(\n \n def test_image_input_file():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n     )\n \n     response = agent.run(\n@@ -29,7 +29,7 @@ def test_image_input_file():\n \n def test_image_input_bytes():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n     )\n \n     image_bytes = image_path.read_bytes()\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py b/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\nindex 4be765f82..7e8ae1ab8 100644\n--- a/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\n@@ -3,14 +3,14 @@ from typing import Optional\n import pytest\n \n from agno.agent import Agent, RunResponse  # noqa\n-from agno.models.meta import Llama\n+from agno.models.meta import LlamaOpenAI\n from agno.tools.duckduckgo import DuckDuckGoTools\n from agno.tools.yfinance import YFinanceTools\n \n \n def test_tool_use():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -27,7 +27,7 @@ def test_tool_use():\n \n def test_tool_use_stream():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -43,7 +43,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -54,7 +54,7 @@ def test_tool_use_stream():\n @pytest.mark.asyncio\n async def test_async_tool_use():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -72,7 +72,7 @@ async def test_async_tool_use():\n @pytest.mark.asyncio\n async def test_async_tool_use_stream():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -90,7 +90,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -100,7 +100,7 @@ async def test_async_tool_use_stream():\n \n def test_tool_use_with_content():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -118,7 +118,7 @@ def test_tool_use_with_content():\n \n def test_parallel_tool_calls():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -139,7 +139,7 @@ def test_parallel_tool_calls():\n \n def test_multiple_tool_calls():\n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[YFinanceTools(cache_results=True), DuckDuckGoTools(cache_results=True)],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -167,7 +167,7 @@ def test_tool_call_custom_tool_no_parameters():\n         return \"It is currently 70 degrees and cloudy in Tokyo\"\n \n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[get_the_weather_in_tokyo],\n         show_tool_calls=True,\n         telemetry=False,\n@@ -196,7 +196,7 @@ def test_tool_call_custom_tool_optional_parameters():\n             return f\"It is currently 70 degrees and cloudy in {city}\"\n \n     agent = Agent(\n-        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n         tools=[get_the_weather],\n         show_tool_calls=True,\n         telemetry=False,\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/mistral/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/mistral/test_tool_use.py b/libs/agno/tests/integration/models/mistral/test_tool_use.py\nindex ad079d0c8..c65ea1487 100644\n--- a/libs/agno/tests/integration/models/mistral/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/mistral/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -98,7 +98,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/nebius/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/nebius/test_tool_use.py b/libs/agno/tests/integration/models/nebius/test_tool_use.py\nindex 299d9f8c3..979be4763 100644\n--- a/libs/agno/tests/integration/models/nebius/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/nebius/test_tool_use.py\n@@ -48,7 +48,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -97,7 +97,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/nvidia/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/nvidia/test_tool_use.py b/libs/agno/tests/integration/models/nvidia/test_tool_use.py\nindex 634781504..1dfb180a1 100644\n--- a/libs/agno/tests/integration/models/nvidia/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/nvidia/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/ollama/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/ollama/test_tool_use.py b/libs/agno/tests/integration/models/ollama/test_tool_use.py\nindex 5b717f5fe..a80be7560 100644\n--- a/libs/agno/tests/integration/models/ollama/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/ollama/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/ollama_tools/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/ollama_tools/test_tool_use.py b/libs/agno/tests/integration/models/ollama_tools/test_tool_use.py\nindex 0133ac374..132366aa4 100644\n--- a/libs/agno/tests/integration/models/ollama_tools/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/ollama_tools/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/openai/chat/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/openai/chat/test_tool_use.py b/libs/agno/tests/integration/models/openai/chat/test_tool_use.py\nindex 5aa82c9d2..9ab32019e 100644\n--- a/libs/agno/tests/integration/models/openai/chat/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/openai/chat/test_tool_use.py\n@@ -47,7 +47,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -99,7 +99,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/openai/responses/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/openai/responses/test_tool_use.py b/libs/agno/tests/integration/models/openai/responses/test_tool_use.py\nindex bbd4fe6a1..6e7474d64 100644\n--- a/libs/agno/tests/integration/models/openai/responses/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/openai/responses/test_tool_use.py\n@@ -47,7 +47,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -101,7 +101,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/openrouter/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/openrouter/test_tool_use.py b/libs/agno/tests/integration/models/openrouter/test_tool_use.py\nindex 2265b90f1..c91e639f7 100644\n--- a/libs/agno/tests/integration/models/openrouter/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/openrouter/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/together/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/together/test_tool_use.py b/libs/agno/tests/integration/models/together/test_tool_use.py\nindex d044f6a2a..543e9618b 100644\n--- a/libs/agno/tests/integration/models/together/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/together/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -95,7 +95,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/integration/models/xai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/xai/test_tool_use.py b/libs/agno/tests/integration/models/xai/test_tool_use.py\nindex 77b376e02..e0f169e66 100644\n--- a/libs/agno/tests/integration/models/xai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/xai/test_tool_use.py\n@@ -46,7 +46,7 @@ def test_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n@@ -98,7 +98,7 @@ async def test_async_tool_use_stream():\n         assert isinstance(chunk, RunResponse)\n         responses.append(chunk)\n         if chunk.tools:\n-            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+            if any(tc.tool_name for tc in chunk.tools):\n                 tool_call_seen = True\n \n     assert len(responses) > 0\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/unit/reader/test_firecrawl_reader.py",
            "diff": "diff --git a/libs/agno/tests/unit/reader/test_firecrawl_reader.py b/libs/agno/tests/unit/reader/test_firecrawl_reader.py\nindex 8a9d48471..46d8cae4c 100644\n--- a/libs/agno/tests/unit/reader/test_firecrawl_reader.py\n+++ b/libs/agno/tests/unit/reader/test_firecrawl_reader.py\n@@ -78,6 +78,27 @@ def test_scrape_with_api_key_and_params():\n         mock_app.scrape_url.assert_called_once_with(\"https://example.com\", **params)\n \n \n+def test_scrape_with_api_key_and_formats_params():\n+    \"\"\"Test scraping with API key and formats parameter\"\"\"\n+    with patch(\"agno.document.reader.firecrawl_reader.FirecrawlApp\") as MockFirecrawlApp:\n+        # Set up mock\n+        mock_app = MockFirecrawlApp.return_value\n+        mock_app.scrape_url.return_value = {\"markdown\": \"Test content\"}\n+\n+        # Create reader with API key and params containing both formats and other params\n+        api_key = \"test_api_key\"\n+        params = {\n+            \"waitUntil\": \"networkidle2\",  # This should be ignored\n+            \"formats\": [\"markdown\"],\n+        }\n+        reader = FirecrawlReader(api_key=api_key, params=params)\n+        reader.scrape(\"https://example.com\")\n+\n+        # Verify FirecrawlApp was called with correct parameters\n+        MockFirecrawlApp.assert_called_once_with(api_key=api_key)\n+        mock_app.scrape_url.assert_called_once_with(\"https://example.com\", **params)\n+\n+\n def test_scrape_empty_response():\n     \"\"\"Test handling of empty response from scrape_url\"\"\"\n     with patch(\"agno.document.reader.firecrawl_reader.FirecrawlApp\") as MockFirecrawlApp:\n"
        },
        {
            "commit": "ede117552a48bee7f674fbfab87d9586f2fabe19",
            "file_path": "libs/agno/tests/unit/tools/test_firecrawl.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_firecrawl.py b/libs/agno/tests/unit/tools/test_firecrawl.py\nnew file mode 100644\nindex 000000000..43e60985c\n--- /dev/null\n+++ b/libs/agno/tests/unit/tools/test_firecrawl.py\n@@ -0,0 +1,225 @@\n+\"\"\"Unit tests for FirecrawlTools class.\"\"\"\n+\n+import json\n+import os\n+from unittest.mock import Mock, patch\n+\n+import pytest\n+from firecrawl import FirecrawlApp\n+\n+from agno.tools.firecrawl import FirecrawlTools\n+\n+TEST_API_KEY = os.environ.get(\"FIRECRAWL_API_KEY\", \"test_api_key\")\n+TEST_API_URL = \"https://api.firecrawl.dev\"\n+\n+\n+@pytest.fixture\n+def mock_firecrawl():\n+    \"\"\"Create a mock FirecrawlApp instance.\"\"\"\n+    with patch(\"agno.tools.firecrawl.FirecrawlApp\") as mock_firecrawl_cls:\n+        mock_app = Mock(spec=FirecrawlApp)\n+        mock_firecrawl_cls.return_value = mock_app\n+        return mock_app\n+\n+\n+@pytest.fixture\n+def firecrawl_tools(mock_firecrawl):\n+    \"\"\"Create a FirecrawlTools instance with mocked dependencies.\"\"\"\n+    with patch.dict(\"os.environ\", {\"FIRECRAWL_API_KEY\": TEST_API_KEY}):\n+        tools = FirecrawlTools()\n+        # Directly set the app to our mock to avoid initialization issues\n+        tools.app = mock_firecrawl\n+        return tools\n+\n+\n+def test_init_with_env_vars():\n+    \"\"\"Test initialization with environment variables.\"\"\"\n+    with patch(\"agno.tools.firecrawl.FirecrawlApp\"):\n+        with patch.dict(\"os.environ\", {\"FIRECRAWL_API_KEY\": TEST_API_KEY}, clear=True):\n+            tools = FirecrawlTools()\n+            assert tools.api_key == TEST_API_KEY\n+            assert tools.formats is None\n+            assert tools.limit == 10\n+            assert tools.app is not None\n+\n+\n+def test_init_with_params():\n+    \"\"\"Test initialization with parameters.\"\"\"\n+    with patch(\"agno.tools.firecrawl.FirecrawlApp\"):\n+        tools = FirecrawlTools(api_key=\"param_api_key\", formats=[\"html\", \"text\"], limit=5, api_url=TEST_API_URL)\n+        assert tools.api_key == \"param_api_key\"\n+        assert tools.formats == [\"html\", \"text\"]\n+        assert tools.limit == 5\n+        assert tools.app is not None\n+\n+\n+def test_scrape_website(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test scrape_website method.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.model_dump.return_value = {\n+        \"url\": \"https://example.com\",\n+        \"content\": \"Test content\",\n+        \"status\": \"success\",\n+    }\n+    mock_firecrawl.scrape_url.return_value = mock_response\n+\n+    # Call the method\n+    result = firecrawl_tools.scrape_website(\"https://example.com\")\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"url\"] == \"https://example.com\"\n+    assert result_data[\"content\"] == \"Test content\"\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.scrape_url.assert_called_once_with(\"https://example.com\")\n+\n+\n+def test_scrape_website_with_formats(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test scrape_website method with formats.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.model_dump.return_value = {\n+        \"url\": \"https://example.com\",\n+        \"content\": \"Test content\",\n+        \"status\": \"success\",\n+    }\n+    mock_firecrawl.scrape_url.return_value = mock_response\n+\n+    # Set formats\n+    firecrawl_tools.formats = [\"html\", \"text\"]\n+\n+    # Call the method\n+    result = firecrawl_tools.scrape_website(\"https://example.com\")\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"url\"] == \"https://example.com\"\n+    assert result_data[\"content\"] == \"Test content\"\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.scrape_url.assert_called_once_with(\"https://example.com\", formats=[\"html\", \"text\"])\n+\n+\n+def test_crawl_website(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test crawl_website method.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.model_dump.return_value = {\n+        \"url\": \"https://example.com\",\n+        \"pages\": [\"page1\", \"page2\"],\n+        \"status\": \"success\",\n+    }\n+    mock_firecrawl.crawl_url.return_value = mock_response\n+\n+    # Call the method\n+    result = firecrawl_tools.crawl_website(\"https://example.com\")\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"url\"] == \"https://example.com\"\n+    assert result_data[\"pages\"] == [\"page1\", \"page2\"]\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.crawl_url.assert_called_once_with(\"https://example.com\", limit=10, poll_interval=30)\n+\n+\n+def test_crawl_website_with_custom_limit(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test crawl_website method with custom limit.\"\"\"\n+    # Reset the default limit\n+    firecrawl_tools.limit = None\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.model_dump.return_value = {\n+        \"url\": \"https://example.com\",\n+        \"pages\": [\"page1\", \"page2\"],\n+        \"status\": \"success\",\n+    }\n+    mock_firecrawl.crawl_url.return_value = mock_response\n+\n+    # Call the method with custom limit\n+    result = firecrawl_tools.crawl_website(\"https://example.com\", limit=5)\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"url\"] == \"https://example.com\"\n+    assert result_data[\"pages\"] == [\"page1\", \"page2\"]\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.crawl_url.assert_called_once_with(\"https://example.com\", limit=5, poll_interval=30)\n+\n+\n+def test_map_website(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test map_website method.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.model_dump.return_value = {\n+        \"url\": \"https://example.com\",\n+        \"sitemap\": {\"page1\": [\"link1\", \"link2\"]},\n+        \"status\": \"success\",\n+    }\n+    mock_firecrawl.map_url.return_value = mock_response\n+\n+    # Call the method\n+    result = firecrawl_tools.map_website(\"https://example.com\")\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"url\"] == \"https://example.com\"\n+    assert result_data[\"sitemap\"] == {\"page1\": [\"link1\", \"link2\"]}\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.map_url.assert_called_once_with(\"https://example.com\")\n+\n+\n+def test_search(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test search method.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.success = True\n+    mock_response.data = {\"query\": \"test query\", \"results\": [\"result1\", \"result2\"], \"status\": \"success\"}\n+    mock_firecrawl.search.return_value = mock_response\n+\n+    # Call the method\n+    result = firecrawl_tools.search(\"test query\")\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"query\"] == \"test query\"\n+    assert result_data[\"results\"] == [\"result1\", \"result2\"]\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.search.assert_called_once_with(\"test query\", limit=10)\n+\n+\n+def test_search_with_error(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test search method with error response.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.success = False\n+    mock_response.error = \"Search failed\"\n+    mock_firecrawl.search.return_value = mock_response\n+\n+    # Call the method\n+    result = firecrawl_tools.search(\"test query\")\n+\n+    # Verify results\n+    assert result == \"Error searching with the Firecrawl tool: Search failed\"\n+    mock_firecrawl.search.assert_called_once_with(\"test query\", limit=10)\n+\n+\n+def test_search_with_custom_params(firecrawl_tools, mock_firecrawl):\n+    \"\"\"Test search method with custom search parameters.\"\"\"\n+    # Setup mock response\n+    mock_response = Mock()\n+    mock_response.success = True\n+    mock_response.data = {\"query\": \"test query\", \"results\": [\"result1\", \"result2\"], \"status\": \"success\"}\n+    mock_firecrawl.search.return_value = mock_response\n+\n+    # Set custom search parameters\n+    firecrawl_tools.search_params = {\"language\": \"en\", \"region\": \"us\"}\n+\n+    # Call the method\n+    result = firecrawl_tools.search(\"test query\")\n+    result_data = json.loads(result)\n+\n+    # Verify results\n+    assert result_data[\"query\"] == \"test query\"\n+    assert result_data[\"results\"] == [\"result1\", \"result2\"]\n+    assert result_data[\"status\"] == \"success\"\n+    mock_firecrawl.search.assert_called_once_with(\"test query\", limit=10, language=\"en\", region=\"us\")\n"
        }
    ]
}