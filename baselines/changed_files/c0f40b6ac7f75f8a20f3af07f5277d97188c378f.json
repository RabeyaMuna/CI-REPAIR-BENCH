{
    "sha_fail": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
    "changed_files": [
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/async_basic.py",
            "diff": "diff --git a/cookbook/models/cerebras/async_basic.py b/cookbook/models/cerebras/async_basic.py\nnew file mode 100644\nindex 000000000..a8fe26656\n--- /dev/null\n+++ b/cookbook/models/cerebras/async_basic.py\n@@ -0,0 +1,13 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+asyncio.run(agent.aprint_response(\"write a two sentence horror story\"))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras/async_basic_stream.py b/cookbook/models/cerebras/async_basic_stream.py\nnew file mode 100644\nindex 000000000..a47cb0fbd\n--- /dev/null\n+++ b/cookbook/models/cerebras/async_basic_stream.py\n@@ -0,0 +1,13 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+asyncio.run(agent.aprint_response(\"write a two sentence horror story\", stream=True))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/cerebras/async_tool_use.py b/cookbook/models/cerebras/async_tool_use.py\nnew file mode 100644\nindex 000000000..48ae81f87\n--- /dev/null\n+++ b/cookbook/models/cerebras/async_tool_use.py\n@@ -0,0 +1,15 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\"))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/async_tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras/async_tool_use_stream.py b/cookbook/models/cerebras/async_tool_use_stream.py\nnew file mode 100644\nindex 000000000..f822e2c8b\n--- /dev/null\n+++ b/cookbook/models/cerebras/async_tool_use_stream.py\n@@ -0,0 +1,15 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\", stream=True))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/basic.py",
            "diff": "diff --git a/cookbook/models/cerebras/basic.py b/cookbook/models/cerebras/basic.py\nnew file mode 100644\nindex 000000000..7f4ff0b00\n--- /dev/null\n+++ b/cookbook/models/cerebras/basic.py\n@@ -0,0 +1,12 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"write a two sentence horror story\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/basic_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras/basic_stream.py b/cookbook/models/cerebras/basic_stream.py\nnew file mode 100644\nindex 000000000..e24772d42\n--- /dev/null\n+++ b/cookbook/models/cerebras/basic_stream.py\n@@ -0,0 +1,13 @@\n+from agno.agent import Agent, RunResponse  # noqa\n+import asyncio\n+from agno.models.cerebras import Cerebras\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"write a two sentence horror story\", stream=True)\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/knowledge.py",
            "diff": "diff --git a/cookbook/models/cerebras/knowledge.py b/cookbook/models/cerebras/knowledge.py\nnew file mode 100644\nindex 000000000..7eb4ec464\n--- /dev/null\n+++ b/cookbook/models/cerebras/knowledge.py\n@@ -0,0 +1,21 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.cerebras import Cerebras\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=PgVector(table_name=\"recipes\", db_url=db_url),\n+)\n+knowledge_base.load(recreate=True)  # Comment out after first run\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    knowledge=knowledge_base,\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"How to make Thai curry?\", markdown=True)\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/storage.py",
            "diff": "diff --git a/cookbook/models/cerebras/storage.py b/cookbook/models/cerebras/storage.py\nnew file mode 100644\nindex 000000000..4906311f8\n--- /dev/null\n+++ b/cookbook/models/cerebras/storage.py\n@@ -0,0 +1,17 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy cerebras_cloud_sdk` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+from agno.storage.postgres import PostgresStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    storage=PostgresStorage(table_name=\"agent_sessions\", db_url=db_url),\n+    tools=[DuckDuckGoTools()],\n+    add_history_to_messages=True,\n+)\n+agent.print_response(\"How many people live in Canada?\")\n+agent.print_response(\"What is their national anthem called?\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/structured_output.py",
            "diff": "diff --git a/cookbook/models/cerebras/structured_output.py b/cookbook/models/cerebras/structured_output.py\nnew file mode 100644\nindex 000000000..377297ef2\n--- /dev/null\n+++ b/cookbook/models/cerebras/structured_output.py\n@@ -0,0 +1,37 @@\n+from typing import List\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.cerebras import Cerebras\n+from pydantic import BaseModel, Field\n+from rich.pretty import pprint  # noqa\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+\n+\n+# Agent that uses a JSON schema output\n+json_schema_output_agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    description=\"You are a helpful assistant. Summarize the movie script based on the location in a JSON object.\",\n+    response_model=MovieScript,\n+    show_tool_calls=True,\n+    debug_mode=True,\n+)\n+\n+json_schema_output_agent.print_response(\"New York\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/tool_use.py",
            "diff": "diff --git a/cookbook/models/cerebras/tool_use.py b/cookbook/models/cerebras/tool_use.py\nnew file mode 100644\nindex 000000000..57c899ba0\n--- /dev/null\n+++ b/cookbook/models/cerebras/tool_use.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Whats happening in France?\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras/tool_use_stream.py b/cookbook/models/cerebras/tool_use_stream.py\nnew file mode 100644\nindex 000000000..7abc460d3\n--- /dev/null\n+++ b/cookbook/models/cerebras/tool_use_stream.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import Cerebras\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Cerebras(id=\"llama-3.3-70b\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Whats happening in France?\", stream=True)\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/async_basic.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/async_basic.py b/cookbook/models/cerebras_openai/async_basic.py\nnew file mode 100644\nindex 000000000..34efcd326\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/async_basic.py\n@@ -0,0 +1,14 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+asyncio.run(agent.aprint_response(\"write a two sentence horror story\"))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/async_basic_stream.py b/cookbook/models/cerebras_openai/async_basic_stream.py\nnew file mode 100644\nindex 000000000..25f0bdeaf\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/async_basic_stream.py\n@@ -0,0 +1,14 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+asyncio.run(agent.aprint_response(\"write a two sentence horror story\", stream=True))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/async_tool_use.py b/cookbook/models/cerebras_openai/async_tool_use.py\nnew file mode 100644\nindex 000000000..471bc5f80\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/async_tool_use.py\n@@ -0,0 +1,15 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-3.3-70b\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\"))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/async_tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/async_tool_use_stream.py b/cookbook/models/cerebras_openai/async_tool_use_stream.py\nnew file mode 100644\nindex 000000000..4201c4dd2\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/async_tool_use_stream.py\n@@ -0,0 +1,15 @@\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-3.3-70b\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\", stream=True))\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/basic.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/basic.py b/cookbook/models/cerebras_openai/basic.py\nnew file mode 100644\nindex 000000000..9a68c7328\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/basic.py\n@@ -0,0 +1,12 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"write a two sentence horror story\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/basic_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/basic_stream.py b/cookbook/models/cerebras_openai/basic_stream.py\nnew file mode 100644\nindex 000000000..0bc64b669\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/basic_stream.py\n@@ -0,0 +1,12 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"write a two sentence horror story\", stream=True)\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/knowledge.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/knowledge.py b/cookbook/models/cerebras_openai/knowledge.py\nnew file mode 100644\nindex 000000000..0e8694594\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/knowledge.py\n@@ -0,0 +1,21 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy pgvector pypdf cerebras_cloud_sdk` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=PgVector(table_name=\"recipes\", db_url=db_url),\n+)\n+knowledge_base.load(recreate=True)  # Comment out after first run\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    knowledge=knowledge_base,\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"How to make Thai curry?\", markdown=True)\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/storage.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/storage.py b/cookbook/models/cerebras_openai/storage.py\nnew file mode 100644\nindex 000000000..1f6c98bac\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/storage.py\n@@ -0,0 +1,19 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy cerebras_cloud_sdk` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.storage.postgres import PostgresStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    storage=PostgresStorage(table_name=\"agent_sessions\", db_url=db_url),\n+    tools=[DuckDuckGoTools()],\n+    debug_mode=True,\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+)\n+agent.print_response(\"How many people live in Canada?\")\n+agent.print_response(\"What is their national anthem called?\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/structured_output.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/structured_output.py b/cookbook/models/cerebras_openai/structured_output.py\nnew file mode 100644\nindex 000000000..a1b11f74d\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/structured_output.py\n@@ -0,0 +1,35 @@\n+from typing import List\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.cerebras import CerebrasOpenAI\n+from pydantic import BaseModel, Field\n+from rich.pretty import pprint  # noqa\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+\n+\n+# Agent that uses a structured output\n+structured_output_agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    description=\"You are a helpful assistant. Summarize the movie script based on the location in a JSON object.\",\n+    response_model=MovieScript,\n+)\n+\n+structured_output_agent.print_response(\"New York\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/tool_use.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/tool_use.py b/cookbook/models/cerebras_openai/tool_use.py\nnew file mode 100644\nindex 000000000..63f72b663\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/tool_use.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Whats happening in France?\")\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "cookbook/models/cerebras_openai/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/cerebras_openai/tool_use_stream.py b/cookbook/models/cerebras_openai/tool_use_stream.py\nnew file mode 100644\nindex 000000000..f9813a6cd\n--- /dev/null\n+++ b/cookbook/models/cerebras_openai/tool_use_stream.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Whats happening in France?\", stream=True)\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/agno/models/cerebras/__init__.py",
            "diff": "diff --git a/libs/agno/agno/models/cerebras/__init__.py b/libs/agno/agno/models/cerebras/__init__.py\nnew file mode 100644\nindex 000000000..d0353b409\n--- /dev/null\n+++ b/libs/agno/agno/models/cerebras/__init__.py\n@@ -0,0 +1,12 @@\n+from agno.models.cerebras.cerebras import Cerebras\n+\n+try:\n+    from agno.models.cerebras.cerebras_openai import CerebrasOpenAI\n+except ImportError:\n+\n+    class CerebrasOpenAI:  # type: ignore\n+        def __init__(self, *args, **kwargs):\n+            raise ImportError(\"`openai` not installed. Please install it via `pip install openai`\")\n+\n+\n+__all__ = [\"Cerebras\", \"CerebrasOpenAI\"]\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/agno/models/cerebras/cerebras.py",
            "diff": "diff --git a/libs/agno/agno/models/cerebras/cerebras.py b/libs/agno/agno/models/cerebras/cerebras.py\nnew file mode 100644\nindex 000000000..ef5851b84\n--- /dev/null\n+++ b/libs/agno/agno/models/cerebras/cerebras.py\n@@ -0,0 +1,404 @@\n+import json\n+from collections.abc import AsyncIterator\n+from dataclasses import dataclass\n+from os import getenv\n+from typing import Any, Dict, Iterator, List, Optional, Union\n+\n+import httpx\n+\n+from agno.models.base import Model\n+from agno.models.message import Message\n+from agno.models.response import ModelResponse\n+from agno.utils.log import log_error, log_warning\n+\n+try:\n+    from cerebras.cloud.sdk import AsyncCerebras as AsyncCerebrasClient\n+    from cerebras.cloud.sdk import Cerebras as CerebrasClient\n+    from cerebras.cloud.sdk.types.chat.chat_completion import (\n+        ChatChunkResponse,\n+        ChatChunkResponseChoice,\n+        ChatChunkResponseChoiceDelta,\n+        ChatCompletionResponse,\n+        ChatCompletionResponseChoice,\n+        ChatCompletionResponseChoiceMessage,\n+    )\n+    from cerebras.cloud.sdk.types.completion import CompletionResponse\n+except (ImportError, ModuleNotFoundError):\n+    raise ImportError(\"`cerebras-cloud-sdk` not installed. Please install using `pip install cerebras-cloud-sdk`\")\n+\n+\n+@dataclass\n+class Cerebras(Model):\n+    \"\"\"\n+    A class for interacting with models using the Cerebras API.\n+    \"\"\"\n+\n+    id: str = \"llama-4-scout-17b-16e-instruct\"\n+    name: str = \"Cerebras\"\n+    provider: str = \"Cerebras\"\n+\n+    supports_native_structured_outputs: bool = False\n+    supports_json_schema_outputs: bool = True\n+\n+    # Request parameters\n+    parallel_tool_calls: bool = False\n+    max_completion_tokens: Optional[int] = None\n+    repetition_penalty: Optional[float] = None\n+    temperature: Optional[float] = None\n+    top_p: Optional[float] = None\n+    top_k: Optional[int] = None\n+    extra_headers: Optional[Any] = None\n+    extra_query: Optional[Any] = None\n+    extra_body: Optional[Any] = None\n+    request_params: Optional[Dict[str, Any]] = None\n+\n+    # Client parameters\n+    api_key: Optional[str] = None\n+    base_url: Optional[Union[str, httpx.URL]] = None\n+    timeout: Optional[float] = None\n+    max_retries: Optional[int] = None\n+    default_headers: Optional[Any] = None\n+    default_query: Optional[Any] = None\n+    http_client: Optional[httpx.Client] = None\n+    client_params: Optional[Dict[str, Any]] = None\n+\n+    # Cerebras clients\n+    client: Optional[CerebrasClient] = None\n+    async_client: Optional[AsyncCerebrasClient] = None\n+\n+    def _get_client_params(self) -> Dict[str, Any]:\n+        # Fetch API key from env if not already set\n+        if not self.api_key:\n+            self.api_key = getenv(\"CEREBRAS_API_KEY\")\n+            if not self.api_key:\n+                log_error(\"CEREBRAS_API_KEY not set. Please set the CEREBRAS_API_KEY environment variable.\")\n+\n+        # Define base client params\n+        base_params = {\n+            \"api_key\": self.api_key,\n+            \"base_url\": self.base_url,\n+            \"timeout\": self.timeout,\n+            \"max_retries\": self.max_retries,\n+            \"default_headers\": self.default_headers,\n+            \"default_query\": self.default_query,\n+        }\n+\n+        # Create client_params dict with non-None values\n+        client_params = {k: v for k, v in base_params.items() if v is not None}\n+\n+        # Add additional client params if provided\n+        if self.client_params:\n+            client_params.update(self.client_params)\n+        return client_params\n+\n+    def get_client(self) -> CerebrasClient:\n+        \"\"\"\n+        Returns a Cerebras client.\n+\n+        Returns:\n+            CerebrasClient: An instance of the Cerebras client.\n+        \"\"\"\n+        if self.client:\n+            return self.client\n+\n+        client_params: Dict[str, Any] = self._get_client_params()\n+        if self.http_client is not None:\n+            client_params[\"http_client\"] = self.http_client\n+        self.client = CerebrasClient(**client_params)\n+        return self.client\n+\n+    def get_async_client(self) -> AsyncCerebrasClient:\n+        \"\"\"\n+        Returns an asynchronous Cerebras client.\n+\n+        Returns:\n+            AsyncCerebras: An instance of the asynchronous Cerebras client.\n+        \"\"\"\n+        if self.async_client:\n+            return self.async_client\n+\n+        client_params: Dict[str, Any] = self._get_client_params()\n+        if self.http_client:\n+            client_params[\"http_client\"] = self.http_client\n+        else:\n+            # Create a new async HTTP client with custom limits\n+            client_params[\"http_client\"] = httpx.AsyncClient(\n+                limits=httpx.Limits(max_connections=1000, max_keepalive_connections=100)\n+            )\n+        self.async_client = AsyncCerebrasClient(**client_params)\n+        return self.async_client\n+\n+    @property\n+    def request_kwargs(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Returns keyword arguments for API requests.\n+\n+        Returns:\n+            Dict[str, Any]: A dictionary of keyword arguments for API requests.\n+        \"\"\"\n+        # Define base request parameters\n+        base_params = {\n+            \"max_completion_tokens\": self.max_completion_tokens,\n+            \"repetition_penalty\": self.repetition_penalty,\n+            \"temperature\": self.temperature,\n+            \"top_p\": self.top_p,\n+            \"top_k\": self.top_k,\n+            \"extra_headers\": self.extra_headers,\n+            \"extra_query\": self.extra_query,\n+            \"extra_body\": self.extra_body,\n+            \"request_params\": self.request_params,\n+        }\n+\n+        # Filter out None values\n+        request_params = {k: v for k, v in base_params.items() if v is not None}\n+\n+        # Add tools\n+        if self._tools is not None and len(self._tools) > 0:\n+            request_params[\"tools\"] = [\n+                {\n+                    \"type\": \"function\",\n+                    \"function\": {\n+                        \"name\": tool[\"function\"][\"name\"],\n+                        \"strict\": True,  # Ensure strict adherence to expected outputs\n+                        \"description\": tool[\"function\"][\"description\"],\n+                        \"parameters\": tool[\"function\"][\"parameters\"],\n+                    },\n+                }\n+                for tool in self._tools\n+            ]\n+            # Cerebras requires parallel_tool_calls=False for llama-4-scout-17b-16e-instruct\n+            request_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n+\n+        # Handle response format for structured outputs\n+        if self.response_format is not None:\n+            if (\n+                isinstance(self.response_format, dict)\n+                and self.response_format.get(\"type\") == \"json_schema\"\n+                and isinstance(self.response_format.get(\"json_schema\"), dict)\n+            ):\n+                # Ensure json_schema has strict=True as required by Cerebras API\n+                schema = self.response_format[\"json_schema\"]\n+                if isinstance(schema.get(\"schema\"), dict) and \"strict\" not in schema:\n+                    schema[\"strict\"] = True\n+\n+                request_params[\"response_format\"] = self.response_format\n+\n+        # Add additional request params if provided\n+        if self.request_params:\n+            request_params.update(self.request_params)\n+\n+        return request_params\n+\n+    def invoke(self, messages: List[Message]) -> CompletionResponse:\n+        \"\"\"\n+        Send a chat completion request to the Cerebras API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            CompletionResponse: The chat completion response from the API.\n+        \"\"\"\n+        return self.get_client().chat.completions.create(\n+            model=self.id,\n+            messages=[self._format_message(m) for m in messages],  # type: ignore\n+            **self.request_kwargs,\n+        )\n+\n+    async def ainvoke(self, messages: List[Message]) -> CompletionResponse:\n+        \"\"\"\n+        Sends an asynchronous chat completion request to the Cerebras API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            ChatCompletion: The chat completion response from the API.\n+        \"\"\"\n+        return await self.get_async_client().chat.completions.create(\n+            model=self.id,\n+            messages=[self._format_message(m) for m in messages],  # type: ignore\n+            **self.request_kwargs,\n+        )\n+\n+    def invoke_stream(self, messages: List[Message]) -> Iterator[ChatChunkResponse]:\n+        \"\"\"\n+        Send a streaming chat completion request to the Cerebras API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            Iterator[ChatChunkResponse]: An iterator of chat completion chunks.\n+        \"\"\"\n+        yield from self.get_client().chat.completions.create(\n+            model=self.id,\n+            messages=[self._format_message(m) for m in messages],  # type: ignore\n+            stream=True,\n+            **self.request_kwargs,\n+        )  # type: ignore\n+\n+    async def ainvoke_stream(self, messages: List[Message]) -> AsyncIterator[ChatChunkResponse]:\n+        \"\"\"\n+        Sends an asynchronous streaming chat completion request to the Cerebras API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            AsyncIterator[ChatChunkResponse]: An asynchronous iterator of chat completion chunks.\n+        \"\"\"\n+        async_stream = await self.get_async_client().chat.completions.create(\n+            model=self.id,\n+            messages=[self._format_message(m) for m in messages],  # type: ignore\n+            stream=True,\n+            **self.request_kwargs,\n+        )\n+        async for chunk in async_stream:  # type: ignore\n+            yield chunk  # type: ignore\n+\n+    def _format_message(self, message: Message) -> Dict[str, Any]:\n+        \"\"\"\n+        Format a message into the format expected by the Cerebras API.\n+\n+        Args:\n+            message (Message): The message to format.\n+\n+        Returns:\n+            Dict[str, Any]: The formatted message.\n+        \"\"\"\n+        # Basic message content\n+        message_dict: Dict[str, Any] = {\n+            \"role\": message.role,\n+            \"content\": message.content if message.content is not None else \"\",\n+        }\n+\n+        # Add name if present\n+        if message.name:\n+            message_dict[\"name\"] = message.name\n+\n+        # Handle tool calls\n+        if message.tool_calls:\n+            # Ensure tool_calls is properly formatted\n+            message_dict[\"tool_calls\"] = [\n+                {\n+                    \"id\": tool_call[\"id\"],\n+                    \"type\": tool_call[\"type\"],\n+                    \"function\": {\n+                        \"name\": tool_call[\"function\"][\"name\"],\n+                        \"arguments\": json.dumps(tool_call[\"function\"][\"arguments\"])\n+                        if isinstance(tool_call[\"function\"][\"arguments\"], (dict, list))\n+                        else tool_call[\"function\"][\"arguments\"],\n+                    },\n+                }\n+                for tool_call in message.tool_calls\n+            ]\n+\n+        # Handle tool responses\n+        if message.role == \"tool\" and message.tool_call_id:\n+            message_dict = {\n+                \"role\": \"tool\",\n+                \"tool_call_id\": message.tool_call_id,\n+                \"content\": message.content if message.content is not None else \"\",\n+            }\n+\n+        # Ensure no None values in the message\n+        message_dict = {k: v for k, v in message_dict.items() if v is not None}\n+\n+        return message_dict\n+\n+    def parse_provider_response(self, response: ChatCompletionResponse) -> ModelResponse:\n+        \"\"\"\n+        Parse the Cerebras response into a ModelResponse.\n+\n+        Args:\n+            response (CompletionResponse): The response from the Cerebras API.\n+\n+        Returns:\n+            ModelResponse: The parsed response.\n+        \"\"\"\n+        model_response = ModelResponse()\n+\n+        # Get the first choice (assuming single response)\n+        choice: ChatCompletionResponseChoice = response.choices[0]\n+        message: ChatCompletionResponseChoiceMessage = choice.message\n+\n+        # Add role\n+        if message.role is not None:\n+            model_response.role = message.role\n+\n+        # Add content\n+        if message.content is not None:\n+            model_response.content = message.content\n+\n+        # Add tool calls\n+        if message.tool_calls is not None:\n+            try:\n+                model_response.tool_calls = [\n+                    {\n+                        \"id\": tool_call.id,\n+                        \"type\": tool_call.type,\n+                        \"function\": {\n+                            \"name\": tool_call.function.name,\n+                            \"arguments\": tool_call.function.arguments,\n+                        },\n+                    }\n+                    for tool_call in message.tool_calls\n+                ]\n+            except Exception as e:\n+                log_warning(f\"Error processing tool calls: {e}\")\n+\n+        # Add usage metrics\n+        if response.usage:\n+            model_response.response_usage = {\n+                \"input_tokens\": response.usage.prompt_tokens,\n+                \"output_tokens\": response.usage.completion_tokens,\n+                \"total_tokens\": response.usage.total_tokens,\n+            }\n+\n+        return model_response\n+\n+    def parse_provider_response_delta(self, response_delta: ChatChunkResponse) -> ModelResponse:\n+        \"\"\"\n+        Parse the streaming response from the Cerebras API into a ModelResponse.\n+\n+        Args:\n+            response_delta (ChatChunkResponse): The streaming response chunk.\n+\n+        Returns:\n+            ModelResponse: The parsed response.\n+        \"\"\"\n+        model_response = ModelResponse()\n+\n+        # Get the first choice (assuming single response)\n+        if response_delta.choices is not None:\n+            choice: ChatChunkResponseChoice = response_delta.choices[0]\n+            delta: ChatChunkResponseChoiceDelta = choice.delta\n+\n+            # Add content\n+            if delta.content:\n+                model_response.content = delta.content\n+\n+            # Add tool calls\n+            if delta.tool_calls:\n+                model_response.tool_calls = [\n+                    {\n+                        \"id\": tool_call.id,\n+                        \"type\": tool_call.type,\n+                        \"function\": {\n+                            \"name\": tool_call.function.name,\n+                            \"arguments\": tool_call.function.arguments,\n+                        },\n+                    }\n+                    for tool_call in delta.tool_calls\n+                ]\n+\n+        # Add usage metrics\n+        if response_delta.usage:\n+            model_response.response_usage = {\n+                \"input_tokens\": response_delta.usage.prompt_tokens,\n+                \"output_tokens\": response_delta.usage.completion_tokens,\n+                \"total_tokens\": response_delta.usage.total_tokens,\n+            }\n+\n+        return model_response\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/agno/models/cerebras/cerebras_openai.py",
            "diff": "diff --git a/libs/agno/agno/models/cerebras/cerebras_openai.py b/libs/agno/agno/models/cerebras/cerebras_openai.py\nnew file mode 100644\nindex 000000000..9da4cf4c1\n--- /dev/null\n+++ b/libs/agno/agno/models/cerebras/cerebras_openai.py\n@@ -0,0 +1,98 @@\n+import json\n+from dataclasses import dataclass\n+from os import getenv\n+from typing import Any, Dict, Optional\n+\n+from agno.models.message import Message\n+from agno.models.openai.like import OpenAILike\n+\n+\n+@dataclass\n+class CerebrasOpenAI(OpenAILike):\n+    id: str = \"llama-4-scout-17b-16e-instruct\"\n+    name: str = \"CerebrasOpenAI\"\n+    provider: str = \"CerebrasOpenAI\"\n+\n+    parallel_tool_calls: bool = False\n+    base_url: str = \"https://api.cerebras.ai/v1\"\n+    api_key: Optional[str] = getenv(\"CEREBRAS_API_KEY\", None)\n+\n+    @property\n+    def request_kwargs(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Returns keyword arguments for API requests.\n+\n+        Returns:\n+            Dict[str, Any]: A dictionary of keyword arguments for API requests.\n+        \"\"\"\n+        # Get base request kwargs from the parent class\n+        request_params = super().request_kwargs\n+\n+        # Add tools with proper formatting\n+        if self._tools is not None and len(self._tools) > 0:\n+            request_params[\"tools\"] = [\n+                {\n+                    \"type\": \"function\",\n+                    \"function\": {\n+                        \"name\": tool[\"function\"][\"name\"],\n+                        \"strict\": True,  # Ensure strict adherence to expected outputs\n+                        \"description\": tool[\"function\"][\"description\"],\n+                        \"parameters\": tool[\"function\"][\"parameters\"],\n+                    },\n+                }\n+                for tool in self._tools\n+            ]\n+            # Cerebras requires parallel_tool_calls=False for llama-4-scout-17b-16e-instruct\n+            request_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n+\n+        return request_params\n+\n+    def _format_message(self, message: Message) -> Dict[str, Any]:\n+        \"\"\"\n+        Format a message into the format expected by the Cerebras API.\n+\n+        Args:\n+            message (Message): The message to format.\n+\n+        Returns:\n+            Dict[str, Any]: The formatted message.\n+        \"\"\"\n+        # Basic message content\n+        message_dict: Dict[str, Any] = {\n+            \"role\": message.role,\n+            \"content\": message.content if message.content is not None else \"\",\n+        }\n+\n+        # Add name if present\n+        if message.name:\n+            message_dict[\"name\"] = message.name\n+\n+        # Handle tool calls\n+        if message.tool_calls:\n+            # Ensure tool_calls is properly formatted\n+            message_dict[\"tool_calls\"] = [\n+                {\n+                    \"id\": tool_call[\"id\"],\n+                    \"type\": tool_call[\"type\"],\n+                    \"function\": {\n+                        \"name\": tool_call[\"function\"][\"name\"],\n+                        \"arguments\": json.dumps(tool_call[\"function\"][\"arguments\"])\n+                        if isinstance(tool_call[\"function\"][\"arguments\"], (dict, list))\n+                        else tool_call[\"function\"][\"arguments\"],\n+                    },\n+                }\n+                for tool_call in message.tool_calls\n+            ]\n+\n+        # Handle tool responses\n+        if message.role == \"tool\" and message.tool_call_id:\n+            message_dict = {\n+                \"role\": \"tool\",\n+                \"tool_call_id\": message.tool_call_id,\n+                \"content\": message.content if message.content is not None else \"\",\n+            }\n+\n+        # Ensure no None values in the message\n+        message_dict = {k: v for k, v in message_dict.items() if v is not None}\n+\n+        return message_dict\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/tests/integration/models/cerebras/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras/test_basic.py b/libs/agno/tests/integration/models/cerebras/test_basic.py\nnew file mode 100644\nindex 000000000..fd1491aeb\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras/test_basic.py\n@@ -0,0 +1,138 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.cerebras import Cerebras\n+from agno.storage.sqlite import SqliteStorage\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        add_history_to_messages=True,\n+        num_history_runs=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_structured_output():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_history():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        storage=SqliteStorage(table_name=\"agent_sessions\", db_file=\"tmp/agent_storage.db\"),\n+        add_history_to_messages=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    agent.run(\"Hello\")\n+    assert len(agent.run_response.messages) == 2\n+    agent.run(\"Hello 2\")\n+    assert len(agent.run_response.messages) == 4\n+    agent.run(\"Hello 3\")\n+    assert len(agent.run_response.messages) == 6\n+    agent.run(\"Hello 4\")\n+    assert len(agent.run_response.messages) == 8\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/tests/integration/models/cerebras/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras/test_tool_use.py b/libs/agno/tests/integration/models/cerebras/test_tool_use.py\nnew file mode 100644\nindex 000000000..4afd796a8\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras/test_tool_use.py\n@@ -0,0 +1,110 @@\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.cerebras import Cerebras\n+from agno.tools.googlesearch import GoogleSearchTools\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+def test_tool_use_with_content():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France? Summarize the key events.\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/tests/integration/models/cerebras_openai/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras_openai/test_basic.py b/libs/agno/tests/integration/models/cerebras_openai/test_basic.py\nnew file mode 100644\nindex 000000000..c6dfe92ee\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras_openai/test_basic.py\n@@ -0,0 +1,127 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.cerebras import CerebrasOpenAI\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        add_history_to_messages=True,\n+        num_history_runs=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_structured_output():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n"
        },
        {
            "commit": "c0f40b6ac7f75f8a20f3af07f5277d97188c378f",
            "file_path": "libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py b/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\nnew file mode 100644\nindex 000000000..df225590f\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\n@@ -0,0 +1,110 @@\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.tools.googlesearch import GoogleSearchTools\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+def test_tool_use_with_content():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France? Summarize the key events.\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n"
        }
    ]
}