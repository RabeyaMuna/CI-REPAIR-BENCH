{
    "sha_fail": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
    "changed_files": [
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "cookbook/storage/mongodb_storage/mongodb_storage_for_team.py",
            "diff": "diff --git a/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py b/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py\nindex 985a85c37..ed4c5d9b9 100644\n--- a/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py\n+++ b/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py\n@@ -57,6 +57,7 @@ hn_team = Team(\n     markdown=True,\n     debug_mode=True,\n     show_members_responses=True,\n+    add_member_tools_to_system_message=False,\n )\n \n hn_team.print_response(\"Write an article about the top 2 stories on hackernews\")\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "cookbook/tools/apify_tools.py",
            "diff": "diff --git a/cookbook/tools/apify_tools.py b/cookbook/tools/apify_tools.py\nindex bc247a7cb..f68750607 100644\n--- a/cookbook/tools/apify_tools.py\n+++ b/cookbook/tools/apify_tools.py\n@@ -1,5 +1,82 @@\n from agno.agent import Agent\n from agno.tools.apify import ApifyTools\n \n-agent = Agent(tools=[ApifyTools()], show_tool_calls=True)\n-agent.print_response(\"Tell me about https://docs.agno.com/introduction\", markdown=True)\n+# Apify Tools Demonstration Script\n+\"\"\"\n+This script showcases the power of web scraping and data extraction using Apify's Actors (serverless tools). \n+The Apify ecosystem has 4000+ pre-built Actors for almost any web data extraction need!\n+\n+---\n+Configuration Instructions:\n+1. Install required dependencies:\n+   pip install agno langchain-apify apify-client\n+\n+2. Set the APIFY_API_TOKEN environment variable:\n+   Add a .env file with APIFY_API_TOKEN=your_apify_api_key\n+---\n+\n+Tip: Check out the Apify Store (https://apify.com/store) to find tools for almost any web scraping or data extraction task.\n+\"\"\"\n+\n+# Create an Apify Tools agent with versatile capabilities\n+agent = Agent(\n+    name=\"Web Insights Explorer\",\n+    instructions=[\n+        \"You are a sophisticated web research assistant capable of extracting insights from various online sources. \"\n+        \"Use the available tools for your tasks to gather accurate, well-structured information.\"\n+    ],\n+    tools=[\n+        ApifyTools(\n+            actors=[\n+                \"apify/rag-web-browser\",\n+                \"compass/crawler-google-places\",\n+                \"clockworks/free-tiktok-scraper\",\n+            ]\n+        )\n+    ],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+\n+def demonstrate_tools():\n+    print(\"Apify Tools Exploration \ud83d\udd0d\")\n+\n+    # RAG Web Search Demonstrations\n+    print(\"\\n1.1 \ud83d\udd75\ufe0f RAG Web Search Scenarios:\")\n+    prompt = \"Research the latest AI ethics guidelines from top tech companies. Compile a summary from at least 3 different sources comparing their approaches using RAG Web Browser.\"\n+    agent.print_response(prompt, show_full_reasoning=True)\n+\n+    print(\"\\n1.2 \ud83d\udd75\ufe0f RAG Web Search Scenarios:\")\n+    prompt = \"Carefully extract the key introduction details from https://docs.agno.com/introduction\"  #  Extract content from specific website\n+    agent.print_response(prompt)\n+\n+    # Google Places Demonstration\n+    print(\"\\n2. Google Places Crawler:\")\n+    prompt = \"Find the top 5 highest-rated coffee shops in San Francisco with detailed information about each location\"\n+    agent.print_response(prompt)\n+\n+    # Tiktok Scraper Demonstration\n+    print(\"\\n3. Tiktok Profile Analysis:\")\n+    prompt = \"Analyze two profiles on Tiktok that lately added #AI (hashtag AI), extracting their statistics and recent content trends\"\n+    agent.print_response(prompt)\n+\n+\n+if __name__ == \"__main__\":\n+    demonstrate_tools()\n+\n+\"\"\"\n+Want to add a new tool? It's easy!\n+- Browse Apify Store\n+- Find an Actor that matches your needs\n+- Add a new method to ApifyTools following the existing pattern\n+- Register the method in the __init__\n+\n+Examples of potential tools:\n+- YouTube video info scraper\n+- Twitter/X profile analyzer\n+- Product price trackers\n+- Job board crawlers\n+- News article extractors\n+- And SO MUCH MORE!\n+\"\"\"\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "cookbook/tools/models/__init__.py",
            "diff": "diff --git a/cookbook/tools/models/__init__.py b/cookbook/tools/models/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "cookbook/tools/models/gemini_video_generation.py",
            "diff": "diff --git a/cookbook/tools/models/gemini_video_generation.py b/cookbook/tools/models/gemini_video_generation.py\nnew file mode 100644\nindex 000000000..0ce53bafc\n--- /dev/null\n+++ b/cookbook/tools/models/gemini_video_generation.py\n@@ -0,0 +1,33 @@\n+\"\"\"\ud83d\udd27 Example: Using the GeminiTools Toolkit for Video Generation\n+\n+An Agent using the Gemini video generation tool.\n+\n+Video generation only works with Vertex AI.\n+Make sure you have set the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION environment variables.\n+\n+Example prompts to try:\n+- \"Generate a 5-second video of a kitten playing a piano\"\n+- \"Create a short looping animation of a neon city skyline at dusk\"\n+\n+Run `pip install google-genai agno` to install the necessary dependencies.\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.models.gemini import GeminiTools\n+from agno.utils.media import save_base64_data\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[GeminiTools(vertexai=True)],  # Video Generation only works on VertexAI mode\n+    show_tool_calls=True,\n+    debug_mode=True,\n+)\n+\n+agent.print_response(\n+    \"create a video of a cat driving at top speed\",\n+)\n+response = agent.run_response\n+if response.videos:\n+    for video in response.videos:\n+        save_base64_data(video.content, f\"tmp/cat_driving_{video.id}.mp4\")\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex 3504dcc93..e3638ae1f 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -2031,7 +2031,7 @@ class Agent:\n                             if name not in _functions_for_model:\n                                 func._agent = self\n                                 func.process_entrypoint(strict=strict)\n-                                if strict:\n+                                if strict and func.strict is None:\n                                     func.strict = True\n                                 if self.tool_hooks is not None:\n                                     func.tool_hooks = self.tool_hooks\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/media.py",
            "diff": "diff --git a/libs/agno/agno/media.py b/libs/agno/agno/media.py\nindex 785d5f296..1c97c8509 100644\n--- a/libs/agno/agno/media.py\n+++ b/libs/agno/agno/media.py\n@@ -11,7 +11,9 @@ class Media(BaseModel):\n \n \n class VideoArtifact(Media):\n-    url: str  # Remote location for file\n+    url: Optional[str] = None  # Remote location for file (if no inline content)\n+    content: Optional[Union[str, bytes]] = None  # type: ignore\n+    mime_type: Optional[str] = None  # MIME type of the video content\n     eta: Optional[str] = None\n     length: Optional[str] = None\n \n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/memory/v2/memory.py",
            "diff": "diff --git a/libs/agno/agno/memory/v2/memory.py b/libs/agno/agno/memory/v2/memory.py\nindex 4a5dd5aea..2da4004ad 100644\n--- a/libs/agno/agno/memory/v2/memory.py\n+++ b/libs/agno/agno/memory/v2/memory.py\n@@ -673,6 +673,7 @@ class Memory:\n         \"\"\"Adds a RunResponse to the runs list.\"\"\"\n         if not self.runs:\n             self.runs = {}\n+\n         self.runs.setdefault(session_id, []).append(run)\n         log_debug(\"Added RunResponse to Memory\")\n \n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/models/meta/llama.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/llama.py b/libs/agno/agno/models/meta/llama.py\nindex d1283268d..cd607d5b5 100644\n--- a/libs/agno/agno/models/meta/llama.py\n+++ b/libs/agno/agno/models/meta/llama.py\n@@ -153,6 +153,13 @@ class Llama(Model):\n         if self._tools is not None and len(self._tools) > 0:\n             request_params[\"tools\"] = self._tools\n \n+            # Fix optional parameters where the \"type\" is [<type>, null]\n+            for tool in request_params[\"tools\"]:  # type: ignore\n+                if \"parameters\" in tool[\"function\"] and \"properties\" in tool[\"function\"][\"parameters\"]:  # type: ignore\n+                    for _, obj in tool[\"function\"][\"parameters\"].get(\"properties\", {}).items():  # type: ignore\n+                        if isinstance(obj[\"type\"], list):\n+                            obj[\"type\"] = obj[\"type\"][0]\n+\n         if self.response_format is not None:\n             request_params[\"response_format\"] = self.response_format\n \n@@ -267,7 +274,6 @@ class Llama(Model):\n             log_error(f\"Error from Llama API: {e}\")\n             raise ModelProviderError(message=str(e), model_name=self.name, model_id=self.id) from e\n \n-    # Override base method\n     @staticmethod\n     def parse_tool_calls(tool_calls_data: List[EventDeltaToolCallDeltaFunction]) -> List[Dict[str, Any]]:\n         \"\"\"\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/models/meta/llama_openai.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/llama_openai.py b/libs/agno/agno/models/meta/llama_openai.py\nindex ce593e1f8..3194e5c6d 100644\n--- a/libs/agno/agno/models/meta/llama_openai.py\n+++ b/libs/agno/agno/models/meta/llama_openai.py\n@@ -34,9 +34,64 @@ class LlamaOpenAI(OpenAILike):\n     api_key: Optional[str] = getenv(\"LLAMA_API_KEY\")\n     base_url: Optional[str] = \"https://api.llama.com/compat/v1/\"\n \n+    # Request parameters\n+    max_completion_tokens: Optional[int] = None\n+    repetition_penalty: Optional[float] = None\n+    temperature: Optional[float] = None\n+    top_p: Optional[float] = None\n+    top_k: Optional[int] = None\n+    extra_headers: Optional[Any] = None\n+    extra_query: Optional[Any] = None\n+    extra_body: Optional[Any] = None\n+    request_params: Optional[Dict[str, Any]] = None\n+\n     supports_native_structured_outputs: bool = False\n     supports_json_schema_outputs: bool = True\n \n+    @property\n+    def request_kwargs(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Returns keyword arguments for API requests.\n+\n+        Returns:\n+            Dict[str, Any]: A dictionary of keyword arguments for API requests.\n+        \"\"\"\n+        # Define base request parameters\n+        base_params = {\n+            \"max_completion_tokens\": self.max_completion_tokens,\n+            \"repetition_penalty\": self.repetition_penalty,\n+            \"temperature\": self.temperature,\n+            \"top_p\": self.top_p,\n+            \"top_k\": self.top_k,\n+            \"extra_headers\": self.extra_headers,\n+            \"extra_query\": self.extra_query,\n+            \"extra_body\": self.extra_body,\n+            \"request_params\": self.request_params,\n+        }\n+\n+        # Filter out None values\n+        request_params = {k: v for k, v in base_params.items() if v is not None}\n+\n+        # Add tools\n+        if self._tools is not None and len(self._tools) > 0:\n+            request_params[\"tools\"] = self._tools\n+\n+            # Fix optional parameters where the \"type\" is [<type>, null]\n+            for tool in request_params[\"tools\"]:  # type: ignore\n+                if \"parameters\" in tool[\"function\"] and \"properties\" in tool[\"function\"][\"parameters\"]:  # type: ignore\n+                    for _, obj in tool[\"function\"][\"parameters\"].get(\"properties\", {}).items():  # type: ignore\n+                        if isinstance(obj[\"type\"], list):\n+                            obj[\"type\"] = obj[\"type\"][0]\n+\n+        if self.response_format is not None:\n+            request_params[\"response_format\"] = self.response_format\n+\n+        # Add additional request params if provided\n+        if self.request_params:\n+            request_params.update(self.request_params)\n+\n+        return request_params\n+\n     def _format_message(self, message: Message) -> Dict[str, Any]:\n         \"\"\"\n         Format a message into the format expected by Llama API.\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/models/openai/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/chat.py b/libs/agno/agno/models/openai/chat.py\nindex 7dcddcbdc..740d67059 100644\n--- a/libs/agno/agno/models/openai/chat.py\n+++ b/libs/agno/agno/models/openai/chat.py\n@@ -386,7 +386,6 @@ class OpenAIChat(Model):\n         Returns:\n             ChatCompletion: The chat completion response from the API.\n         \"\"\"\n-\n         try:\n             if self.response_format is not None and self.structured_outputs:\n                 if isinstance(self.response_format, type) and issubclass(self.response_format, BaseModel):\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/team/team.py",
            "diff": "diff --git a/libs/agno/agno/team/team.py b/libs/agno/agno/team/team.py\nindex 3643d1058..34ce10780 100644\n--- a/libs/agno/agno/team/team.py\n+++ b/libs/agno/agno/team/team.py\n@@ -6042,11 +6042,9 @@ class Team:\n             if isinstance(self.memory, dict) and \"create_user_memories\" in self.memory:\n                 # Convert dict to TeamMemory\n                 self.memory = TeamMemory(**self.memory)\n-            elif isinstance(self.memory, dict):\n-                # Convert dict to Memory\n-                self.memory = Memory(**self.memory)\n             else:\n-                raise TypeError(f\"Expected memory to be a dict or TeamMemory, but got {type(self.memory)}\")\n+                # Default to base memory\n+                self.memory = Memory()\n \n         if session.memory is not None:\n             if isinstance(self.memory, TeamMemory):\n@@ -6086,13 +6084,13 @@ class Team:\n                     try:\n                         if self.memory.runs is None:\n                             self.memory.runs = {}\n+                        self.memory.runs[session.session_id] = []\n                         for run in session.memory[\"runs\"]:\n-                            session_id = run[\"session_id\"]\n-                            self.memory.runs[session_id] = []\n+                            run_session_id = run[\"session_id\"]\n                             if \"team_id\" in run:\n-                                self.memory.runs[session_id].append(TeamRunResponse.from_dict(run))\n+                                self.memory.runs[run_session_id].append(TeamRunResponse.from_dict(run))\n                             else:\n-                                self.memory.runs[session_id].append(RunResponse.from_dict(run))\n+                                self.memory.runs[run_session_id].append(RunResponse.from_dict(run))\n                     except Exception as e:\n                         log_warning(f\"Failed to load runs from memory: {e}\")\n                 if \"team_context\" in session.memory:\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/tools/apify.py",
            "diff": "diff --git a/libs/agno/agno/tools/apify.py b/libs/agno/agno/tools/apify.py\nindex ac515e46c..d3e4acae6 100644\n--- a/libs/agno/agno/tools/apify.py\n+++ b/libs/agno/agno/tools/apify.py\n@@ -1,8 +1,12 @@\n-from os import getenv\n-from typing import List, Optional\n+import json\n+import os\n+import string\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import requests\n \n from agno.tools import Toolkit\n-from agno.utils.log import log_debug, logger\n+from agno.utils.log import log_info, logger\n \n try:\n     from apify_client import ApifyClient\n@@ -11,112 +15,341 @@ except ImportError:\n \n \n class ApifyTools(Toolkit):\n-    def __init__(\n-        self,\n-        api_key: Optional[str] = None,\n-        website_content_crawler: bool = True,\n-        web_scraper: bool = False,\n-        **kwargs,\n-    ):\n-        super().__init__(name=\"apify_tools\", **kwargs)\n-\n-        self.api_key = api_key or getenv(\"MY_APIFY_TOKEN\")\n-        if not self.api_key:\n-            logger.error(\"No Apify API key provided\")\n-\n-        if website_content_crawler:\n-            self.register(self.website_content_crawler)\n-        if web_scraper:\n-            self.register(self.web_scrapper)\n-\n-    def website_content_crawler(self, urls: List[str], timeout: Optional[int] = 60) -> str:\n+    def __init__(self, actors: Optional[Union[str, List[str]]] = None, apify_api_token: Optional[str] = None):\n+        \"\"\"Initialize ApifyTools with specific Actors.\n+\n+        Args:\n+            actors (Union[str, List[str]]): Single Actor ID as string or list of Actor IDs to register as individual tools\n+            apify_api_token (Optional[str]): Apify API token (defaults to APIFY_API_TOKEN env variable)\n+\n+        Examples:\n+            Configuration Instructions:\n+            1. Install required dependencies:\n+            pip install agno apify-client\n+\n+            2. Set the APIFY_API_TOKEN environment variable:\n+            Add a .env file with APIFY_API_TOKEN=your_apify_api_key\n+\n+            Import necessary components:\n+\n+            from agno.agent import Agent\n+            from agno.tools.apify import ApifyTools\n+\n+            # Create an agent with ApifyTools\n+            agent = Agent(\n+                tools=[\n+                    ApifyTools(actors=[\"apify/rag-web-browser\"])\n+                ],\n+                markdown=True\n+            )\n+\n+            # Ask the agent to process web content\n+            agent.print_response(\"Summarize the content from https://docs.agno.com/introduction\", markdown=True)\n+\n+            # Using multiple actors with the agent\n+            agent = Agent(\n+                tools=[\n+                    ApifyTools(actors=[\n+                        \"apify/rag-web-browser\",\n+                        \"compass/crawler-google-places\"\n+                    ])\n+                ],\n+                show_tool_calls=True\n+            )\n+            agent.print_response(\n+                '''\n+                I'm traveling to Tokyo next month.\n+                1. Research the best time to visit and major attractions\n+                2. Find one good rated sushi restaurant near Shinjuku\n+                Compile a comprehensive travel guide with this information.\n+                ''',\n+                markdown=True\n+            )\n         \"\"\"\n-        Crawls a website using Apify's website-content-crawler actor.\n+        super().__init__(name=\"ApifyTools\")\n+\n+        # Get API token from args or environment\n+        self.apify_api_token = apify_api_token or os.getenv(\"APIFY_API_TOKEN\")\n+        if not self.apify_api_token:\n+            raise ValueError(\"APIFY_API_TOKEN environment variable or apify_api_token parameter must be set\")\n+\n+        self.client = create_apify_client(self.apify_api_token)\n \n-        :param urls: The URLs to crawl.\n-        :param timeout: The timeout for the crawling.\n+        # Register specific Actors if provided\n+        if actors:\n+            actor_list = [actors] if isinstance(actors, str) else actors\n+            for actor_id in actor_list:\n+                self.register_actor(actor_id)\n \n-        :return: The results of the crawling.\n+    def register_actor(self, actor_id: str) -> None:\n+        \"\"\"Register an Apify Actor as a function in the toolkit.\n+\n+        Args:\n+            actor_id (str): ID of the Apify Actor to register (e.g., 'apify/web-scraper')\n         \"\"\"\n-        if self.api_key is None:\n-            return \"No API key provided\"\n+        try:\n+            # Get actor metadata\n+            build = get_actor_latest_build(self.client, actor_id)\n+            tool_name = actor_id_to_tool_name(actor_id)\n+\n+            # Get actor description\n+            actor_description = build.get(\"actorDefinition\", {}).get(\"description\", \"\")\n+            if len(actor_description) > MAX_DESCRIPTION_LEN:\n+                actor_description = actor_description[:MAX_DESCRIPTION_LEN] + \"...(TRUNCATED, TOO LONG)\"\n+\n+            # Get input schema\n+            actor_input = build.get(\"actorDefinition\", {}).get(\"input\")\n+            if not actor_input:\n+                raise ValueError(f\"Input schema not found in the Actor build for Actor: {actor_id}\")\n+\n+            properties, required = prune_actor_input_schema(actor_input)\n+\n+            # Create a wrapper function that calls the Actor\n+            def actor_function(**kwargs) -> str:\n+                \"\"\"Actor function wrapper.\"\"\"\n+                try:\n+                    # Params are nested under 'kwargs' key\n+                    if len(kwargs) == 1 and \"kwargs\" in kwargs:\n+                        kwargs = kwargs[\"kwargs\"]\n+\n+                    log_info(f\"Running Apify Actor {actor_id} with parameters: {kwargs}\")\n+\n+                    # Run the Actor directly with the client\n+                    details = self.client.actor(actor_id=actor_id).call(run_input=kwargs)\n+                    if details is None:\n+                        error_msg = (\n+                            f\"Actor: {actor_id} was not started properly and details about the run were not returned\"\n+                        )\n+                        raise ValueError(error_msg)\n+\n+                    run_id = details.get(\"id\")\n+                    if run_id is None:\n+                        error_msg = f\"Run ID not found in the run details for Actor: {actor_id}\"\n+                        raise ValueError(error_msg)\n+\n+                    # Get the results\n+                    run = self.client.run(run_id=run_id)\n+                    results = run.dataset().list_items(clean=True).items\n+\n+                    return json.dumps(results)\n+\n+                except Exception as e:\n+                    error_msg = f\"Error running Apify Actor {actor_id}: {str(e)}\"\n+                    logger.error(error_msg)\n+                    return json.dumps([{\"error\": error_msg}])\n+\n+            docstring = f\"{actor_description}\\n\\nArgs:\\n\"\n+\n+            for param_name, param_info in properties.items():\n+                param_type = param_info.get(\"type\", \"any\")\n+                param_desc = param_info.get(\"description\", \"No description available\")\n+                required_marker = \"(required)\" if param_name in required else \"(optional)\"\n+                docstring += f\"    {param_name} ({param_type}): {required_marker} {param_desc}\\n\"\n+\n+            docstring += \"\"\"\n+Returns:\n+    str: JSON string containing the Actor's output dataset\n+\"\"\"\n+\n+            # Update function metadata\n+            actor_function.__name__ = tool_name\n+            actor_function.__doc__ = docstring\n+\n+            # Register the function with the toolkit\n+            self.register(actor_function, sanitize_arguments=False)\n+            # Fix params schema\n+            self.functions[tool_name].parameters = props_to_json_schema(properties, required)\n+            log_info(f\"Registered Apify Actor '{actor_id}' as function '{tool_name}'\")\n+\n+        except Exception as e:\n+            logger.error(f\"Failed to register Apify Actor '{actor_id}': {str(e)}\")\n+\n+\n+# Constants\n+MAX_DESCRIPTION_LEN = 350\n+REQUESTS_TIMEOUT_SECS = 300\n+APIFY_API_ENDPOINT_GET_DEFAULT_BUILD = \"https://api.apify.com/v2/acts/{actor_id}/builds/default\"\n+\n+\n+# Utility functions\n+def props_to_json_schema(input_dict, required_fields=None):\n+    schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": required_fields or []}\n+\n+    def infer_array_item_type(prop):\n+        type_map = {\n+            \"string\": \"string\",\n+            \"int\": \"number\",\n+            \"float\": \"number\",\n+            \"dict\": \"object\",\n+            \"list\": \"array\",\n+            \"bool\": \"boolean\",\n+            \"none\": \"null\",\n+        }\n+        if prop.get(\"items\", {}).get(\"type\"):\n+            return prop[\"items\"][\"type\"]\n+        if \"prefill\" in prop and prop[\"prefill\"] and len(prop[\"prefill\"]) > 0:\n+            return type_map.get(type(prop[\"prefill\"][0]).__name__.lower(), \"string\")\n+        if \"default\" in prop and prop[\"default\"] and len(prop[\"default\"]) > 0:\n+            return type_map.get(type(prop[\"default\"][0]).__name__.lower(), \"string\")\n+        return \"string\"  # Fallback for arrays like searchStringsArray\n+\n+    for key, value in input_dict.items():\n+        prop_schema: Dict[str, Any] = {}\n+        prop_type = value.get(\"type\")\n+\n+        if \"enum\" in value:\n+            prop_schema[\"enum\"] = value[\"enum\"]\n+\n+        if \"default\" in value:\n+            prop_schema[\"default\"] = value[\"default\"]\n+        elif \"prefill\" in value:\n+            prop_schema[\"default\"] = value[\"prefill\"]\n+\n+        if \"description\" in value:\n+            prop_schema[\"description\"] = value[\"description\"]\n+\n+        # Handle Apify special types first\n+        if prop_type == \"object\" and value.get(\"editor\") == \"proxy\":\n+            prop_schema[\"type\"] = \"object\"\n+            prop_schema[\"properties\"] = {\n+                \"useApifyProxy\": {\n+                    \"title\": \"Use Apify Proxy\",\n+                    \"type\": \"boolean\",\n+                    \"description\": \"Whether to use Apify Proxy - ALWAYS SET TO TRUE.\",\n+                    \"default\": True,\n+                    \"examples\": [True],\n+                }\n+            }\n+            prop_schema[\"required\"] = [\"useApifyProxy\"]\n+            if \"default\" in value:\n+                prop_schema[\"default\"] = value[\"default\"]\n+\n+        elif prop_type == \"array\" and value.get(\"editor\") == \"requestListSources\":\n+            prop_schema[\"type\"] = \"array\"\n+            prop_schema[\"items\"] = {\n+                \"type\": \"object\",\n+                \"title\": \"Request list source\",\n+                \"description\": \"Request list source\",\n+                \"properties\": {\n+                    \"url\": {\"title\": \"URL\", \"type\": \"string\", \"description\": \"URL of the request list source\"}\n+                },\n+            }\n \n-        if urls is None:\n-            return \"No URLs provided\"\n+        elif prop_type == \"array\":\n+            prop_schema[\"type\"] = \"array\"\n+            prop_schema[\"items\"] = {\n+                \"type\": infer_array_item_type(value),\n+                \"title\": value.get(\"title\", \"Item\"),\n+                \"description\": \"Item\",\n+            }\n \n-        client = ApifyClient(self.api_key)\n+        elif prop_type == \"object\":\n+            prop_schema[\"type\"] = \"object\"\n+            if \"default\" in value:\n+                prop_schema[\"default\"] = value[\"default\"]\n+                prop_schema[\"properties\"] = {}\n+                for k, v in value.get(\"properties\", value[\"default\"]).items():\n+                    prop_type = v[\"type\"] if isinstance(v, dict) else type(v).__name__.lower()\n+                    if prop_type == \"bool\":\n+                        prop_type = \"boolean\"\n+                    prop_schema[\"properties\"][k] = {\"type\": prop_type}\n \n-        log_debug(f\"Crawling URLs: {urls}\")\n+        else:\n+            prop_schema[\"type\"] = prop_type\n \n-        formatted_urls = [{\"url\": url} for url in urls]\n+        schema[\"properties\"][key] = prop_schema\n \n-        run_input = {\"startUrls\": formatted_urls}\n+    return schema\n \n-        run = client.actor(\"apify/website-content-crawler\").call(run_input=run_input, timeout_secs=timeout)\n \n-        results: str = \"\"\n+def create_apify_client(token: str) -> ApifyClient:\n+    \"\"\"Create an Apify client instance with a custom user-agent.\n \n-        for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n-            results += \"Results for URL: \" + item.get(\"url\") + \"\\n\"\n-            results += item.get(\"text\") + \"\\n\"\n+    Args:\n+        token (str): API token\n \n-        return results\n+    Returns:\n+        ApifyClient: Apify client instance\n \n-    def web_scrapper(self, urls: List[str], timeout: Optional[int] = 60) -> str:\n-        \"\"\"\n-        Scrapes a website using Apify's web-scraper actor.\n+    Raises:\n+        ValueError: If the API token is not provided\n+    \"\"\"\n+    if not token:\n+        raise ValueError(\"API token is required to create an Apify client.\")\n \n-        :param urls: The URLs to scrape.\n-        :param timeout: The timeout for the scraping.\n+    client = ApifyClient(token)\n+    if http_client := getattr(client.http_client, \"httpx_client\", None):\n+        http_client.headers[\"user-agent\"] += \"; Origin/agno\"\n+    return client\n \n-        :return: The results of the scraping.\n-        \"\"\"\n-        if self.api_key is None:\n-            return \"No API key provided\"\n \n-        if urls is None:\n-            return \"No URLs provided\"\n+def actor_id_to_tool_name(actor_id: str) -> str:\n+    \"\"\"Turn actor_id into a valid tool name.\n \n-        client = ApifyClient(self.api_key)\n+    Args:\n+        actor_id (str): Actor ID from Apify store\n \n-        log_debug(f\"Scrapping URLs: {urls}\")\n+    Returns:\n+        str: A valid tool name\n+    \"\"\"\n+    valid_chars = string.ascii_letters + string.digits + \"_-\"\n+    return \"apify_actor_\" + \"\".join(char if char in valid_chars else \"_\" for char in actor_id)\n \n-        formatted_urls = [{\"url\": url} for url in urls]\n \n-        page_function_string = \"\"\"\n-            async function pageFunction(context) {\n-                const $ = context.jQuery;\n-                const pageTitle = $('title').first().text();\n-                const h1 = $('h1').first().text();\n-                const first_h2 = $('h2').first().text();\n-                const random_text_from_the_page = $('p').first().text();\n+def get_actor_latest_build(apify_client: ApifyClient, actor_id: str) -> Dict[str, Any]:\n+    \"\"\"Get the latest build of an Actor from the default build tag.\n \n-                context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n+    Args:\n+        apify_client (ApifyClient): An instance of the ApifyClient class\n+        actor_id (str): Actor name from Apify store to run\n \n-                return {\n-                    url: context.request.url,\n-                    pageTitle,\n-                    h1,\n-                    first_h2,\n-                    random_text_from_the_page\n-                };\n-            }\n-        \"\"\"\n+    Returns:\n+        Dict[str, Any]: The latest build of the Actor\n \n-        run_input = {\n-            \"pageFunction\": page_function_string,\n-            \"startUrls\": formatted_urls,\n-        }\n+    Raises:\n+        ValueError: If the Actor is not found or the build data is not found\n+        TypeError: If the build is not a dictionary\n+    \"\"\"\n+    if not (actor := apify_client.actor(actor_id).get()):\n+        raise ValueError(f\"Actor {actor_id} not found.\")\n+\n+    if not (actor_obj_id := actor.get(\"id\")):\n+        raise ValueError(f\"Failed to get the Actor object ID for {actor_id}.\")\n+\n+    url = APIFY_API_ENDPOINT_GET_DEFAULT_BUILD.format(actor_id=actor_obj_id)\n+    response = requests.request(\"GET\", url, timeout=REQUESTS_TIMEOUT_SECS)\n+\n+    build = response.json()\n+    if not isinstance(build, dict):\n+        raise TypeError(f\"Failed to get the latest build of the Actor {actor_id}.\")\n+\n+    if (data := build.get(\"data\")) is None:\n+        raise ValueError(f\"Failed to get the latest build data of the Actor {actor_id}.\")\n+\n+    return data\n+\n+\n+def prune_actor_input_schema(input_schema: Dict[str, Any]) -> Tuple[Dict[str, Any], List[str]]:\n+    \"\"\"Get the input schema from the Actor build and trim descriptions.\n \n-        run = client.actor(\"apify/web-scraper\").call(run_input=run_input, timeout_secs=timeout)\n+    Args:\n+        input_schema (Dict[str, Any]): The input schema from the Actor build\n \n-        results: str = \"\"\n+    Returns:\n+        Tuple[Dict[str, Any], List[str]]: A tuple containing the pruned properties and required fields\n+    \"\"\"\n+    properties = input_schema.get(\"properties\", {})\n+    required = input_schema.get(\"required\", [])\n \n-        for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n-            results += \"Results for URL: \" + item.get(\"url\") + \"\\n\"\n-            results += item.get(\"pageTitle\") + \"\\n\"\n-            results += item.get(\"h1\") + \"\\n\"\n-            results += item.get(\"first_h2\") + \"\\n\"\n-            results += item.get(\"random_text_from_the_page\") + \"\\n\"\n+    properties_out: Dict[str, Any] = {}\n+    for item, meta in properties.items():\n+        properties_out[item] = {}\n+        if desc := meta.get(\"description\"):\n+            properties_out[item][\"description\"] = (\n+                desc[:MAX_DESCRIPTION_LEN] + \"...\" if len(desc) > MAX_DESCRIPTION_LEN else desc\n+            )\n+        for key_name in (\"type\", \"default\", \"prefill\", \"enum\"):\n+            if value := meta.get(key_name):\n+                properties_out[item][key_name] = value\n \n-        return results\n+    return properties_out, required\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/tools/function.py",
            "diff": "diff --git a/libs/agno/agno/tools/function.py b/libs/agno/agno/tools/function.py\nindex 4407903a9..ec7f6186a 100644\n--- a/libs/agno/agno/tools/function.py\n+++ b/libs/agno/agno/tools/function.py\n@@ -169,6 +169,8 @@ class Function(BaseModel):\n         from agno.utils.json_schema import get_json_schema\n \n         if self.skip_entrypoint_processing:\n+            if strict:\n+                self.process_schema_for_strict()\n             return\n \n         if self.entrypoint is None:\n@@ -260,6 +262,10 @@ class Function(BaseModel):\n         except Exception as e:\n             log_warning(f\"Failed to add validate decorator to entrypoint: {e}\")\n \n+    def process_schema_for_strict(self):\n+        self.parameters[\"additionalProperties\"] = False\n+        self.parameters[\"required\"] = [name for name in self.parameters[\"properties\"] if name not in [\"agent\", \"team\"]]\n+\n     def get_type_name(self, t: Type[T]):\n         name = str(t)\n         if \"list\" in name or \"dict\" in name:\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/tools/mcp.py",
            "diff": "diff --git a/libs/agno/agno/tools/mcp.py b/libs/agno/agno/tools/mcp.py\nindex fdc5b1c7d..d8c1dd6f3 100644\n--- a/libs/agno/agno/tools/mcp.py\n+++ b/libs/agno/agno/tools/mcp.py\n@@ -203,7 +203,6 @@ class MCPTools(Toolkit):\n                 try:\n                     # Get an entrypoint for the tool\n                     entrypoint = get_entrypoint_for_tool(tool, self.session)\n-\n                     # Create a Function for the tool\n                     f = Function(\n                         name=tool.name,\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/agno/tools/models/gemini.py",
            "diff": "diff --git a/libs/agno/agno/tools/models/gemini.py b/libs/agno/agno/tools/models/gemini.py\nindex 0b6016f8b..841a6261b 100644\n--- a/libs/agno/agno/tools/models/gemini.py\n+++ b/libs/agno/agno/tools/models/gemini.py\n@@ -1,44 +1,67 @@\n import base64\n+import time\n from os import getenv\n-from typing import Optional\n+from typing import Any, Optional\n from uuid import uuid4\n \n from agno.agent import Agent\n-from agno.media import ImageArtifact\n+from agno.media import ImageArtifact, VideoArtifact\n from agno.tools import Toolkit\n-from agno.utils.log import log_debug, log_error\n+from agno.utils.log import log_debug, log_error, log_info\n \n try:\n     from google.genai import Client\n+    from google.genai.types import GenerateImagesResponse, GenerateVideosOperation\n except (ModuleNotFoundError, ImportError):\n     raise ImportError(\"`google-genai` not installed. Please install using `pip install google-genai`\")\n \n \n class GeminiTools(Toolkit):\n-    \"\"\"Tools for interacting with Google Gemini API (including Imagen for images)\"\"\"\n+    \"\"\"Tools for interacting with Google Gemini API\"\"\"\n \n     def __init__(\n         self,\n         api_key: Optional[str] = None,\n+        vertexai: bool = False,\n+        project_id: Optional[str] = None,\n+        location: Optional[str] = None,\n         image_generation_model: str = \"imagen-3.0-generate-002\",\n+        video_generation_model: str = \"veo-2.0-generate-001\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"gemini_tools\", tools=[self.generate_image], **kwargs)\n+        super().__init__(name=\"gemini_tools\", tools=[self.generate_image, self.generate_video], **kwargs)\n \n+        # Set mode and credentials: use only provided vertexai parameter\n+        self.vertexai = vertexai\n+        self.project_id = project_id\n+        self.location = location\n+\n+        # Load API key from argument or environment\n         self.api_key = api_key or getenv(\"GOOGLE_API_KEY\")\n-        if not self.api_key:\n-            raise ValueError(\n-                \"GOOGLE_API_KEY not set. Please provide api_key or set the GOOGLE_API_KEY environment variable.\"\n-            )\n+        if not self.vertexai and not self.api_key:\n+            log_error(\"GOOGLE_API_KEY not set. Please set the GOOGLE_API_KEY environment variable.\")\n+            raise ValueError(\"GOOGLE_API_KEY not set. Please provide api_key or set the environment variable.\")\n+\n+        # Prepare client parameters\n+        client_params: dict[str, Any] = {}\n+        if self.vertexai:\n+            log_info(\"Using Vertex AI API\")\n+            client_params[\"vertexai\"] = True\n+            client_params[\"project\"] = self.project_id or getenv(\"GOOGLE_CLOUD_PROJECT\")\n+            client_params[\"location\"] = self.location or getenv(\"GOOGLE_CLOUD_LOCATION\")\n+        else:\n+            log_info(\"Using Gemini API\")\n+            client_params[\"api_key\"] = self.api_key\n \n         try:\n-            self.client = Client(api_key=self.api_key)\n+            self.client = Client(**client_params)\n             log_debug(\"Google GenAI Client created successfully.\")\n         except Exception as e:\n             log_error(f\"Failed to create Google GenAI Client: {e}\", exc_info=True)\n             raise ValueError(f\"Failed to create Google GenAI Client. Error: {e}\")\n \n         self.image_model = image_generation_model\n+        self.video_model = video_generation_model\n \n     def generate_image(\n         self,\n@@ -54,40 +77,89 @@ class GeminiTools(Toolkit):\n         \"\"\"\n \n         try:\n-            response = self.client.models.generate_images(\n+            response: GenerateImagesResponse = self.client.models.generate_images(\n                 model=self.image_model,\n                 prompt=prompt,\n             )\n \n             log_debug(\"DEBUG: Raw Gemini API response\")\n \n-            image_bytes = None\n-            actual_mime_type = \"image/png\"\n+            # Extract image bytes\n+            image_bytes = response.generated_images[0].image.image_bytes\n+            for generated_image in response.generated_images:\n+                image_bytes = generated_image.image.image_bytes\n+                if not image_bytes:\n+                    log_error(\"No valid image data extracted.\")\n+                    return \"Failed to generate image: No valid image data extracted.\"\n+                base64_encoded_image_bytes = base64.b64encode(image_bytes)\n+                actual_mime_type = \"image/png\"\n+\n+                media_id = str(uuid4())\n+                agent.add_image(\n+                    ImageArtifact(\n+                        id=media_id,\n+                        content=base64_encoded_image_bytes,\n+                        original_prompt=prompt,\n+                        mime_type=actual_mime_type,\n+                    )\n+                )\n+                log_debug(f\"Successfully generated image {media_id} with model {self.image_model}\")\n+            return \"Image generated successfully\"\n \n-            if response.generated_images and response.generated_images[0].image.image_bytes:\n-                image_bytes = response.generated_images[0].image.image_bytes\n-            else:\n-                log_error(\"No image data found in the response structure.\")\n-                return \"Failed to generate image: No valid image data extracted.\"\n+        except Exception as e:\n+            log_error(f\"Failed to generate image: Client or method not available ({e})\")\n+            return f\"Failed to generate image: Client or method not available ({e})\"\n \n-            if image_bytes is None:\n-                log_error(\"image_bytes is None after extraction.\")\n-                return \"Failed to generate image: No valid image data extracted.\"\n+    def generate_video(\n+        self,\n+        agent: Agent,\n+        prompt: str,\n+    ) -> str:\n+        \"\"\"Generate a video based on a text prompt.\n+        Args:\n+            prompt (str): The text prompt to generate the video from.\n+        Returns:\n+            str: A message indicating success or failure.\n+        \"\"\"\n+        # Video generation requires Vertex AI mode.\n+        if not self.vertexai:\n+            log_error(\"Video generation requires Vertex AI mode. Please enable Vertex AI mode.\")\n+            return (\n+                \"Video generation requires Vertex AI mode. \"\n+                \"Please set `vertexai=True` or environment variable `GOOGLE_GENAI_USE_VERTEXAI=true`.\"\n+            )\n \n-            base64_encoded_image_bytes = base64.b64encode(image_bytes)\n+        from google.genai.types import GenerateVideosConfig\n \n-            media_id = str(uuid4())\n-            agent.add_image(\n-                ImageArtifact(\n-                    id=media_id,\n-                    content=base64_encoded_image_bytes,\n-                    original_prompt=prompt,\n-                    mime_type=actual_mime_type,\n-                )\n+        try:\n+            operation: GenerateVideosOperation = self.client.models.generate_videos(\n+                model=self.video_model,\n+                prompt=prompt,\n+                config=GenerateVideosConfig(\n+                    enhance_prompt=True,\n+                ),\n             )\n-            log_debug(f\"Successfully generated image {media_id} with model {self.image_model}\")\n-            return f\"Image generated successfully with ID: {media_id}\"\n \n+            while not operation.done:\n+                time.sleep(5)\n+                operation = self.client.operations.get(operation=operation)\n+\n+            for video in operation.result.generated_videos:\n+                generated_video = video.video\n+\n+                media_id = str(uuid4())\n+                encoded_video = base64.b64encode(generated_video.video_bytes).decode(\"utf-8\")\n+\n+                agent.add_video(\n+                    VideoArtifact(\n+                        id=media_id,\n+                        content=encoded_video,\n+                        original_prompt=prompt,\n+                        mime_type=generated_video.mime_type,\n+                    )\n+                )\n+                log_debug(f\"Successfully generated video {media_id} with model {self.video_model}\")\n+            return \"Video generated successfully\"\n         except Exception as e:\n-            log_error(f\"Failed to generate image: Client or method not available ({e})\")\n-            return f\"Failed to generate image: Client or method not available ({e})\"\n+            log_error(f\"Failed to generate video: {e}\")\n+            return f\"Failed to generate video: {e}\"\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py b/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py\nindex 856873fc8..4494d6e2e 100644\n--- a/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py\n+++ b/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py\n@@ -70,16 +70,21 @@ def test_knowledge_tools_non_streaming(knowledge_base):\n     # Run the agent in non-streaming mode\n     response = agent.run(\"What does Paul Graham explain about reading in his essay?\", stream=False)\n \n-    # Print the reasoning_content when received\n-    if hasattr(response, \"reasoning_content\") and response.reasoning_content:\n-        print(\"\\n=== KnowledgeTools (non-streaming) reasoning_content ===\")\n-        print(response.reasoning_content)\n-        print(\"=========================================================\\n\")\n-\n-    # Assert that reasoning_content exists and is populated\n-    assert hasattr(response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n-    assert response.reasoning_content is not None, \"reasoning_content should not be None\"\n-    assert len(response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n+    think_called = False\n+    for tool_call in response.formatted_tool_calls:\n+        if \"think\" in tool_call:\n+            think_called = True\n+    if think_called:\n+        # Print the reasoning_content when received\n+        if hasattr(response, \"reasoning_content\") and response.reasoning_content:\n+            print(\"\\n=== KnowledgeTools (non-streaming) reasoning_content ===\")\n+            print(response.reasoning_content)\n+            print(\"=========================================================\\n\")\n+\n+        # Assert that reasoning_content exists and is populated\n+        assert hasattr(response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n+        assert response.reasoning_content is not None, \"reasoning_content should not be None\"\n+        assert len(response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n \n \n @pytest.mark.integration\n@@ -102,20 +107,25 @@ def test_knowledge_tools_streaming(knowledge_base):\n         agent.run(\"What are Paul Graham's suggestions on what to read?\", stream=True, stream_intermediate_steps=True)\n     )\n \n-    # Print the reasoning_content when received\n-    if (\n-        hasattr(agent, \"run_response\")\n-        and agent.run_response\n-        and hasattr(agent.run_response, \"reasoning_content\")\n-        and agent.run_response.reasoning_content\n-    ):\n-        print(\"\\n=== KnowledgeTools (streaming) reasoning_content ===\")\n-        print(agent.run_response.reasoning_content)\n-        print(\"====================================================\\n\")\n-\n-    # Check the agent's run_response directly after streaming is complete\n-    assert hasattr(agent, \"run_response\"), \"Agent should have run_response after streaming\"\n-    assert agent.run_response is not None, \"Agent's run_response should not be None\"\n-    assert hasattr(agent.run_response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n-    assert agent.run_response.reasoning_content is not None, \"reasoning_content should not be None\"\n-    assert len(agent.run_response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n+    think_called = False\n+    for tool_call in agent.run_response.formatted_tool_calls:\n+        if \"think\" in tool_call:\n+            think_called = True\n+    if think_called:\n+        # Print the reasoning_content when received\n+        if (\n+            hasattr(agent, \"run_response\")\n+            and agent.run_response\n+            and hasattr(agent.run_response, \"reasoning_content\")\n+            and agent.run_response.reasoning_content\n+        ):\n+            print(\"\\n=== KnowledgeTools (streaming) reasoning_content ===\")\n+            print(agent.run_response.reasoning_content)\n+            print(\"====================================================\\n\")\n+\n+        # Check the agent's run_response directly after streaming is complete\n+        assert hasattr(agent, \"run_response\"), \"Agent should have run_response after streaming\"\n+        assert agent.run_response is not None, \"Agent's run_response should not be None\"\n+        assert hasattr(agent.run_response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n+        assert agent.run_response.reasoning_content is not None, \"reasoning_content should not be None\"\n+        assert len(agent.run_response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/tests/integration/models/meta/llama/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama/test_tool_use.py b/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\nindex c81883741..6b657ed6c 100644\n--- a/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\n@@ -180,7 +180,6 @@ def test_tool_call_custom_tool_no_parameters():\n     assert \"Tokyo\" in response.content\n \n \n-@pytest.mark.skip(\"Llama models do not not accept optional parameters\")\n def test_tool_call_custom_tool_optional_parameters():\n     def get_the_weather(city: Optional[str] = None):\n         \"\"\"\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py b/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\nindex c81883741..6b657ed6c 100644\n--- a/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\n@@ -180,7 +180,6 @@ def test_tool_call_custom_tool_no_parameters():\n     assert \"Tokyo\" in response.content\n \n \n-@pytest.mark.skip(\"Llama models do not not accept optional parameters\")\n def test_tool_call_custom_tool_optional_parameters():\n     def get_the_weather(city: Optional[str] = None):\n         \"\"\"\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/tests/integration/models/xai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/xai/test_tool_use.py b/libs/agno/tests/integration/models/xai/test_tool_use.py\nindex 2d4991dd4..2f86ec305 100644\n--- a/libs/agno/tests/integration/models/xai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/xai/test_tool_use.py\n@@ -212,5 +212,5 @@ def test_tool_call_list_parameters():\n             tool_calls.extend(msg.tool_calls)\n     for call in tool_calls:\n         if call.get(\"type\", \"\") == \"function\":\n-            assert call[\"function\"][\"name\"] in [\"get_contents\", \"exa_answer\"]\n+            assert call[\"function\"][\"name\"] in [\"search_exa\", \"get_contents\", \"exa_answer\"]\n     assert response.content is not None\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\nnew file mode 100644\nindex 000000000..312598de6\n--- /dev/null\n+++ b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\n@@ -0,0 +1,178 @@\n+import os\n+import tempfile\n+import uuid\n+\n+import pytest\n+\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.anthropic.claude import Claude\n+from agno.models.openai.chat import OpenAIChat\n+from agno.storage.sqlite import SqliteStorage\n+from agno.team.team import Team\n+\n+\n+@pytest.fixture\n+def temp_storage_db_file():\n+    \"\"\"Create a temporary SQLite database file for team storage testing.\"\"\"\n+    with tempfile.NamedTemporaryFile(suffix=\".db\", delete=False) as temp_file:\n+        db_path = temp_file.name\n+\n+    yield db_path\n+\n+    # Clean up the temporary file after the test\n+    if os.path.exists(db_path):\n+        os.unlink(db_path)\n+\n+\n+@pytest.fixture\n+def temp_memory_db_file():\n+    \"\"\"Create a temporary SQLite database file for memory testing.\"\"\"\n+    with tempfile.NamedTemporaryFile(suffix=\".db\", delete=False) as temp_file:\n+        db_path = temp_file.name\n+\n+    yield db_path\n+\n+    # Clean up the temporary file after the test\n+    if os.path.exists(db_path):\n+        os.unlink(db_path)\n+\n+\n+@pytest.fixture\n+def team_storage(temp_storage_db_file):\n+    \"\"\"Create a SQLite storage for team sessions.\"\"\"\n+    # Use a unique table name for each test run\n+    table_name = f\"team_sessions_{uuid.uuid4().hex[:8]}\"\n+    storage = SqliteStorage(table_name=table_name, db_file=temp_storage_db_file, mode=\"team\")\n+    storage.create()\n+    return storage\n+\n+\n+@pytest.fixture\n+def memory_db(temp_memory_db_file):\n+    \"\"\"Create a SQLite memory database for testing.\"\"\"\n+    db = SqliteMemoryDb(db_file=temp_memory_db_file)\n+    db.create()\n+    return db\n+\n+\n+@pytest.fixture\n+def memory(memory_db):\n+    \"\"\"Create a Memory instance for testing.\"\"\"\n+    return Memory(model=Claude(id=\"claude-3-5-sonnet-20241022\"), db=memory_db)\n+\n+\n+@pytest.fixture\n+def route_team(team_storage, memory):\n+    \"\"\"Create a route team with storage and memory for testing.\"\"\"\n+    return Team(\n+        name=\"Route Team\",\n+        mode=\"route\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        members=[],\n+        storage=team_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_run_history_persistence(route_team, team_storage, memory):\n+    \"\"\"Test that all runs within a session are persisted in storage.\"\"\"\n+    user_id = \"john@example.com\"\n+    session_id = \"session_123\"\n+    num_turns = 5\n+\n+    # Clear memory for this specific test case\n+    memory.clear()\n+\n+    # Perform multiple turns\n+    conversation_messages = [\n+        \"What's the weather like today?\",\n+        \"What about tomorrow?\",\n+        \"Any recommendations for indoor activities?\",\n+        \"Search for nearby museums.\",\n+        \"Which one has the best reviews?\",\n+    ]\n+\n+    assert len(conversation_messages) == num_turns\n+\n+    for msg in conversation_messages:\n+        await route_team.arun(msg, user_id=user_id, session_id=session_id)\n+\n+    # Verify the stored session data after all turns\n+    team_session = team_storage.read(session_id=session_id)\n+\n+    stored_memory_data = team_session.memory\n+    assert stored_memory_data is not None, \"Memory data not found in stored session.\"\n+\n+    stored_runs = stored_memory_data[\"runs\"]\n+    assert isinstance(stored_runs, list), \"Stored runs data is not a list.\"\n+\n+    first_user_message_content = stored_runs[0][\"messages\"][1][\"content\"]\n+    assert first_user_message_content == conversation_messages[0]\n+\n+\n+@pytest.mark.asyncio\n+async def test_multi_user_multi_session_route_team(route_team, team_storage, memory):\n+    \"\"\"Test multi-user multi-session route team with storage and memory.\"\"\"\n+    # Define user and session IDs\n+    user_1_id = \"user_1@example.com\"\n+    user_2_id = \"user_2@example.com\"\n+    user_3_id = \"user_3@example.com\"\n+\n+    user_1_session_1_id = \"user_1_session_1\"\n+    user_1_session_2_id = \"user_1_session_2\"\n+    user_2_session_1_id = \"user_2_session_1\"\n+    user_3_session_1_id = \"user_3_session_1\"\n+\n+    # Clear memory for this test\n+    memory.clear()\n+\n+    # Team interaction with user 1 - Session 1\n+    await route_team.arun(\"What is the current stock price of AAPL?\", user_id=user_1_id, session_id=user_1_session_1_id)\n+    await route_team.arun(\"What are the latest news about Apple?\", user_id=user_1_id, session_id=user_1_session_1_id)\n+\n+    # Team interaction with user 1 - Session 2\n+    await route_team.arun(\n+        \"Compare the stock performance of AAPL with recent tech industry news\",\n+        user_id=user_1_id,\n+        session_id=user_1_session_2_id,\n+    )\n+\n+    # Team interaction with user 2\n+    await route_team.arun(\"What is the current stock price of MSFT?\", user_id=user_2_id, session_id=user_2_session_1_id)\n+    await route_team.arun(\n+        \"What are the latest news about Microsoft?\", user_id=user_2_id, session_id=user_2_session_1_id\n+    )\n+\n+    # Team interaction with user 3\n+    await route_team.arun(\n+        \"What is the current stock price of GOOGL?\", user_id=user_3_id, session_id=user_3_session_1_id\n+    )\n+    await route_team.arun(\"What are the latest news about Google?\", user_id=user_3_id, session_id=user_3_session_1_id)\n+\n+    # Continue the conversation with user 1\n+    await route_team.arun(\n+        \"Based on the information you have, what stock would you recommend investing in?\",\n+        user_id=user_1_id,\n+        session_id=user_1_session_1_id,\n+    )\n+\n+    # Verify storage DB has the right sessions\n+    all_session_ids = team_storage.get_all_session_ids()\n+    assert len(all_session_ids) == 4  # 4 sessions total\n+\n+    # Check that each user has the expected sessions\n+    user_1_sessions = team_storage.get_all_sessions(user_id=user_1_id)\n+    assert len(user_1_sessions) == 2\n+    assert user_1_session_1_id in [session.session_id for session in user_1_sessions]\n+    assert user_1_session_2_id in [session.session_id for session in user_1_sessions]\n+\n+    user_2_sessions = team_storage.get_all_sessions(user_id=user_2_id)\n+    assert len(user_2_sessions) == 1\n+    assert user_2_session_1_id in [session.session_id for session in user_2_sessions]\n+\n+    user_3_sessions = team_storage.get_all_sessions(user_id=user_3_id)\n+    assert len(user_3_sessions) == 1\n+    assert user_3_session_1_id in [session.session_id for session in user_3_sessions]\n"
        },
        {
            "commit": "ff5106ea7e8f83cb7a85bbee6f2299ac736ac860",
            "file_path": "libs/agno/tests/unit/tools/models/test_gemini.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/models/test_gemini.py b/libs/agno/tests/unit/tools/models/test_gemini.py\nindex ade365023..283645245 100644\n--- a/libs/agno/tests/unit/tools/models/test_gemini.py\n+++ b/libs/agno/tests/unit/tools/models/test_gemini.py\n@@ -6,7 +6,7 @@ from uuid import UUID\n import pytest\n \n from agno.agent import Agent\n-from agno.media import ImageArtifact\n+from agno.media import ImageArtifact, VideoArtifact\n from agno.tools.models.gemini import GeminiTools\n \n \n@@ -122,7 +122,7 @@ def test_generate_image_success(mock_gemini_tools, mock_agent, mock_successful_r\n         result = mock_gemini_tools.generate_image(mock_agent, prompt)\n \n         expected_media_id = \"12345678-1234-5678-1234-567812345678\"\n-        assert result == f\"Image generated successfully with ID: {expected_media_id}\"\n+        assert result == \"Image generated successfully\"\n         mock_gemini_tools.client.models.generate_images.assert_called_once_with(model=image_model, prompt=prompt)\n \n         # Verify agent.add_image was called with the correct ImageArtifact\n@@ -173,3 +173,63 @@ def test_generate_image_no_image_bytes(mock_gemini_tools, mock_agent, mock_faile\n         prompt=prompt,\n     )\n     mock_agent.add_image.assert_not_called()\n+\n+\n+# Tests for generate_video method\n+def test_generate_video_requires_vertexai(mock_gemini_tools, mock_agent):\n+    \"\"\"Test video generation when vertexai mode is disabled.\"\"\"\n+    prompt = \"A sample video prompt\"\n+    result = mock_gemini_tools.generate_video(mock_agent, prompt)\n+    expected = (\n+        \"Video generation requires Vertex AI mode. \"\n+        \"Please set `vertexai=True` or environment variable `GOOGLE_GENAI_USE_VERTEXAI=true`.\"\n+    )\n+    assert result == expected\n+    mock_agent.add_video.assert_not_called()\n+\n+\n+@pytest.fixture\n+def mock_video_operation():\n+    \"\"\"Fixture for a completed video generation operation.\"\"\"\n+    op = MagicMock()\n+    op.done = True\n+    video = MagicMock()\n+    video.video_bytes = b\"fake_video_bytes\"\n+    video.mime_type = \"video/mp4\"\n+    op.result = MagicMock(generated_videos=[MagicMock(video=video)])\n+    return op\n+\n+\n+def test_generate_video_success(mock_gemini_tools, mock_agent, mock_video_operation):\n+    \"\"\"Test successful video generation.\"\"\"\n+    mock_gemini_tools.vertexai = True\n+    mock_gemini_tools.client.models.generate_videos.return_value = mock_video_operation\n+    prompt = \"A sample video prompt\"\n+    with patch(\"agno.tools.models.gemini.uuid4\", return_value=UUID(\"87654321-4321-8765-4321-876543214321\")):\n+        result = mock_gemini_tools.generate_video(mock_agent, prompt)\n+        expected_id = \"87654321-4321-8765-4321-876543214321\"\n+        assert result == \"Video generated successfully\"\n+        assert mock_gemini_tools.client.models.generate_videos.called\n+        call_args = mock_gemini_tools.client.models.generate_videos.call_args\n+        assert call_args.kwargs[\"model\"] == mock_gemini_tools.video_model\n+        assert call_args.kwargs[\"prompt\"] == prompt\n+        mock_agent.add_video.assert_called_once()\n+        added = mock_agent.add_video.call_args[0][0]\n+        assert isinstance(added, VideoArtifact)\n+        assert added.id == expected_id\n+        assert added.original_prompt == prompt\n+        assert added.mime_type == \"video/mp4\"\n+        import base64\n+\n+        expected_content = base64.b64encode(b\"fake_video_bytes\").decode(\"utf-8\")\n+        assert added.content == expected_content\n+\n+\n+def test_generate_video_exception(mock_gemini_tools, mock_agent):\n+    \"\"\"Test video generation when API raises exception.\"\"\"\n+    mock_gemini_tools.vertexai = True\n+    mock_gemini_tools.client.models.generate_videos.side_effect = Exception(\"API error\")\n+    prompt = \"A sample video prompt\"\n+    result = mock_gemini_tools.generate_video(mock_agent, prompt)\n+    assert result == \"Failed to generate video: API error\"\n+    mock_agent.add_video.assert_not_called()\n"
        }
    ]
}