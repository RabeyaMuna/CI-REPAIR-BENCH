{
    "sha_fail": "f2436c62292765f014a0dd30a013035abd13c33f",
    "changed_files": [
        {
            "commit": "f2436c62292765f014a0dd30a013035abd13c33f",
            "file_path": "cookbook/evals/accuracy/additional_evaluation_guidelines.py",
            "diff": "diff --git a/cookbook/evals/accuracy/additional_evaluation_guidelines.py b/cookbook/evals/accuracy/additional_evaluation_guidelines.py\nnew file mode 100644\nindex 000000000..89c49bc97\n--- /dev/null\n+++ b/cookbook/evals/accuracy/additional_evaluation_guidelines.py\n@@ -0,0 +1,38 @@\n+\"\"\"This example shows how to provide additional guidelines and context for the evaluator agent.\"\"\"\n+\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.eval.accuracy import AccuracyAgentResponse, AccuracyEval, AccuracyResult\n+from agno.models.openai import OpenAIChat\n+from agno.tools.calculator import CalculatorTools\n+\n+# This is the agent that will be evaluated\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[CalculatorTools(enable_all=True)],\n+)\n+\n+# This is the agent we will use to perform the evaluation\n+evaluator_agent = Agent(\n+    model=OpenAIChat(id=\"o4-mini\"),\n+    tools=[CalculatorTools(enable_all=True)],\n+    response_model=AccuracyAgentResponse,  # Use this response model for the evaluator agent\n+)\n+\n+evaluation = AccuracyEval(\n+    agent=agent,\n+    evaluator_agent=evaluator_agent,\n+    input=\"A company's revenue grew by 15% in Q1, then decreased by 8% in Q2. If the Q2 revenue was $1,840,000, what was the original revenue before Q1?\",\n+    expected_output=\"$1,740,000\",\n+    additional_guidelines=\"\"\"\n+    - Verify the mathematical steps using the calculator tool.\n+    - Final answer must be in USD format with commas for thousands ($1,000,000).\n+    - Accept answers within $1,000 of the expected value due to rounding.\n+    - Ensure the agent worked backwards from Q2 to find the original revenue.\n+    \"\"\",\n+    additional_context=\"This is testing compound percentage calculations in a business context where precision in financial reporting is critical.\",\n+)\n+\n+result: Optional[AccuracyResult] = evaluation.run(print_results=True)\n+assert result is not None and result.avg_score >= 8\n"
        },
        {
            "commit": "f2436c62292765f014a0dd30a013035abd13c33f",
            "file_path": "cookbook/evals/accuracy/custom_evaluator_agent.py",
            "diff": "diff --git a/cookbook/evals/accuracy/custom_evaluator_agent.py b/cookbook/evals/accuracy/custom_evaluator_agent.py\nnew file mode 100644\nindex 000000000..22ae99736\n--- /dev/null\n+++ b/cookbook/evals/accuracy/custom_evaluator_agent.py\n@@ -0,0 +1,35 @@\n+\"\"\"\n+This example shows how to use a custom evaluator agent to evaluate the accuracy of an agent.\n+\n+The evaluator agent is a simple agent that uses the AccuracyAgentResponse response model.\n+\"\"\"\n+\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.eval.accuracy import AccuracyAgentResponse, AccuracyEval, AccuracyResult\n+from agno.models.openai import OpenAIChat\n+from agno.tools.calculator import CalculatorTools\n+\n+# This is the agent that will be evaluated\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[CalculatorTools(enable_all=True)],\n+)\n+\n+# This is the agent we will use to perform the evaluation\n+evaluator_agent = Agent(\n+    model=OpenAIChat(id=\"o4-mini\"),\n+    tools=[CalculatorTools(enable_all=True)],\n+    response_model=AccuracyAgentResponse,  # Use this response model for the evaluator agent\n+)\n+\n+evaluation = AccuracyEval(\n+    agent=agent,\n+    evaluator_agent=evaluator_agent,\n+    input=\"A company's revenue grew by 15% in Q1, then decreased by 8% in Q2. If the Q2 revenue was $1,840,000, what was the original revenue before Q1?\",\n+    expected_output=\"$1,739,130.43\",\n+)\n+\n+result: Optional[AccuracyResult] = evaluation.run(print_results=True)\n+assert result is not None and result.avg_score >= 8\n"
        },
        {
            "commit": "f2436c62292765f014a0dd30a013035abd13c33f",
            "file_path": "cookbook/evals/accuracy/custom_response_model.py",
            "diff": "diff --git a/cookbook/evals/accuracy/custom_response_model.py b/cookbook/evals/accuracy/custom_response_model.py\nnew file mode 100644\nindex 000000000..f12ad6503\n--- /dev/null\n+++ b/cookbook/evals/accuracy/custom_response_model.py\n@@ -0,0 +1,49 @@\n+\"\"\"\n+This example shows how to use a custom response model for your evaluator agent.\n+\n+We recommend using our AccuracyAgentResponse class as response_model for the evaluator agent.\n+That will give you a full Accuracy result containing score averages and other relevant information.\n+\n+However, if you want to use a custom response model or no response model at all, you can do that.\n+This is useful if you want to evaluate something specific and do not care about the full Accuracy result.\n+\"\"\"\n+\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.eval.accuracy import AccuracyEval, AccuracyResult\n+from agno.models.openai import OpenAIChat\n+from agno.tools.calculator import CalculatorTools\n+from pydantic import BaseModel\n+\n+\n+# Simple custom response model\n+class ResponseModel(BaseModel):\n+    answer: str\n+    reason: str\n+\n+    def __str__(self) -> str:\n+        return f\"Answer: {self.answer}\\nReason: {self.reason}\"\n+\n+\n+# This is the agent that will be evaluated\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[CalculatorTools(enable_all=True)],\n+)\n+\n+# This is the agent we will use to perform the evaluation\n+evaluator_agent = Agent(\n+    model=OpenAIChat(id=\"o4-mini\"),\n+    tools=[CalculatorTools(enable_all=True)],\n+    response_model=ResponseModel,\n+)\n+\n+evaluation = AccuracyEval(\n+    agent=agent,\n+    evaluator_agent=evaluator_agent,\n+    input=\"A company's revenue grew by 15% in Q1, then decreased by 8% in Q2. If the Q2 revenue was $1,840,000, what was the original revenue before Q1?\",\n+    expected_output=\"$1,739,130.43\",\n+)\n+\n+result: Optional[AccuracyResult] = evaluation.run(print_results=True)\n"
        },
        {
            "commit": "f2436c62292765f014a0dd30a013035abd13c33f",
            "file_path": "libs/agno/agno/eval/accuracy.py",
            "diff": "diff --git a/libs/agno/agno/eval/accuracy.py b/libs/agno/agno/eval/accuracy.py\nindex bd317de02..dea90b372 100644\n--- a/libs/agno/agno/eval/accuracy.py\n+++ b/libs/agno/agno/eval/accuracy.py\n@@ -167,9 +167,51 @@ class AccuracyEval:\n     # Log the results to the Agno platform. On by default.\n     monitoring: bool = getenv(\"AGNO_MONITOR\", \"true\").lower() == \"true\"\n \n+    def _using_custom_response(self) -> bool:\n+        \"\"\"Check if the evaluator agent is using a custom response model\"\"\"\n+        if not self.evaluator_agent:\n+            return False\n+        return self.evaluator_agent.response_model is not AccuracyAgentResponse\n+\n+    def parse_additional_guidelines(self) -> str:\n+        \"\"\"Parse the user-provided additional guidelines into a prompt-ready string\"\"\"\n+        if not self.additional_guidelines:\n+            return \"\"\n+        additional_guidelines = \"\\n## Additional Guidelines\\n\"\n+        if isinstance(self.additional_guidelines, str):\n+            additional_guidelines += self.additional_guidelines\n+        else:\n+            additional_guidelines += \"\\n- \".join(self.additional_guidelines)\n+        additional_guidelines += \"\\n\"\n+        return additional_guidelines\n+\n+    def parse_additional_context(self) -> str:\n+        \"\"\"Parse the user-provided additional context into a prompt-ready string\"\"\"\n+        if not self.additional_context:\n+            return \"\"\n+        additional_context = \"\\n## Additional Context\\n\"\n+        if isinstance(self.additional_context, str):\n+            additional_context += self.additional_context\n+        else:\n+            additional_context += \"\\n- \".join(self.additional_context)\n+        additional_context += \"\\n\"\n+        return additional_context\n+\n     def get_evaluator_agent(self) -> Agent:\n         \"\"\"Return the evaluator agent. If not provided, build it based on the evaluator fields and default instructions.\"\"\"\n+\n         if self.evaluator_agent is not None:\n+            # Adding user-provided guidelines and context to the evaluator agent description\n+            if self.additional_guidelines is not None or self.additional_context is not None:\n+                self.evaluator_agent.description = (\n+                    (self.evaluator_agent.description or \"\")\n+                    + self.parse_additional_guidelines()\n+                    + self.parse_additional_context()\n+                )\n+            if self.evaluator_agent.response_model is not AccuracyAgentResponse:\n+                logger.warning(\n+                    \"The evaluator agent is not using the AccuracyAgentResponse response model. This means a complete Accuracy result won't be available, stored or monitored.\"\n+                )\n             return self.evaluator_agent\n \n         model = self.model\n@@ -184,20 +226,8 @@ class AccuracyEval:\n                     \"Agno uses `openai` as the default model provider. Please run `pip install openai` to use the default evaluator.\"\n                 )\n \n-        additional_guidelines = \"\"\n-        if self.additional_guidelines is not None:\n-            additional_guidelines = \"\\n## Additional Guidelines\\n\"\n-            if isinstance(self.additional_guidelines, str):\n-                additional_guidelines += self.additional_guidelines\n-            else:\n-                additional_guidelines += \"\\n- \".join(self.additional_guidelines)\n-            additional_guidelines += \"\\n\"\n-\n-        additional_context = \"\"\n-        if self.additional_context is not None and len(self.additional_context) > 0:\n-            additional_context = \"\\n## Additional Context\\n\"\n-            additional_context += self.additional_context\n-            additional_context += \"\\n\"\n+        additional_guidelines = self.parse_additional_guidelines()\n+        additional_context = self.parse_additional_context()\n \n         return Agent(\n             model=model,\n@@ -263,18 +293,20 @@ Remember: You must only compare the agent_output to the expected_output. The exp\n         evaluation_input: str,\n         evaluator_expected_output: str,\n         agent_output: str,\n-    ) -> Optional[AccuracyEvaluation]:\n+    ) -> Optional[Union[AccuracyEvaluation, str]]:\n         \"\"\"Orchestrate the evaluation process.\"\"\"\n         try:\n-            accuracy_agent_response = evaluator_agent.run(evaluation_input).content\n-            if accuracy_agent_response is None or not isinstance(accuracy_agent_response, AccuracyAgentResponse):\n-                raise EvalError(f\"Evaluator Agent returned an invalid response: {accuracy_agent_response}\")\n+            response = evaluator_agent.run(evaluation_input).content\n+            if response is None:\n+                raise EvalError(f\"Evaluator Agent returned an invalid response: {response}\")\n+            if not isinstance(response, AccuracyAgentResponse):\n+                return response\n             return AccuracyEvaluation(\n                 input=input,\n                 output=agent_output,\n                 expected_output=evaluator_expected_output,\n-                score=accuracy_agent_response.accuracy_score,\n-                reason=accuracy_agent_response.accuracy_reason,\n+                score=response.accuracy_score,\n+                reason=response.accuracy_reason,\n             )\n         except Exception as e:\n             logger.exception(f\"Failed to evaluate accuracy: {e}\")\n@@ -338,9 +370,12 @@ Remember: You must only compare the agent_output to the expected_output. The exp\n                 if result is None:\n                     logger.error(f\"Failed to evaluate accuracy on iteration {i + 1}\")\n                     continue\n-\n-                self.result.results.append(result)\n-                self.result.compute_stats()\n+                if self._using_custom_response():\n+                    console.print(f\"Evaluator Agent response on iteration {i + 1}: \\n{result}\")\n+                    continue\n+                else:\n+                    self.result.results.append(result)  # type: ignore\n+                    self.result.compute_stats()\n                 status.update(f\"Eval iteration {i + 1} finished\")\n \n             status.stop()\n@@ -354,26 +389,27 @@ Remember: You must only compare the agent_output to the expected_output. The exp\n                 result=self.result,\n             )\n \n-        # Print results if requested\n-        if self.print_results or print_results:\n-            self.result.print_results(console)\n-        if self.print_summary or print_summary:\n-            self.result.print_summary(console)\n-\n-        # Log results to the Agno platform if requested\n-        if self.monitoring:\n-            log_eval_run(\n-                run_id=self.eval_id,  # type: ignore\n-                run_data=asdict(self.result),\n-                eval_type=EvalType.ACCURACY,\n-                agent_id=self.agent.agent_id if self.agent is not None else None,\n-                model_id=self.agent.model.id if self.agent is not None and self.agent.model is not None else None,\n-                model_provider=self.agent.model.provider\n-                if self.agent is not None and self.agent.model is not None\n-                else None,\n-                name=self.name if self.name is not None else None,\n-                evaluated_entity_name=self.agent.name if self.agent is not None else None,\n-            )\n+        if not self._using_custom_response():\n+            # Print results if requested\n+            if self.print_results or print_results:\n+                self.result.print_results(console)\n+            if self.print_summary or print_summary:\n+                self.result.print_summary(console)\n+\n+            # Log results to the Agno platform if requested\n+            if self.monitoring:\n+                log_eval_run(\n+                    run_id=self.eval_id,  # type: ignore\n+                    run_data=asdict(self.result),\n+                    eval_type=EvalType.ACCURACY,\n+                    agent_id=self.agent.agent_id if self.agent is not None else None,\n+                    model_id=self.agent.model.id if self.agent is not None and self.agent.model is not None else None,\n+                    model_provider=self.agent.model.provider\n+                    if self.agent is not None and self.agent.model is not None\n+                    else None,\n+                    name=self.name if self.name is not None else None,\n+                    evaluated_entity_name=self.agent.name if self.agent is not None else None,\n+                )\n \n         logger.debug(f\"*********** Evaluation {self.eval_id} Finished ***********\")\n         return self.result\n@@ -419,7 +455,11 @@ Remember: You must only compare the agent_output to the expected_output. The exp\n         )\n \n         if result is not None:\n-            self.result.results.append(result)\n+            if self._using_custom_response():\n+                print(f\"Evaluator Agent response: {result}\")\n+                return\n+\n+            self.result.results.append(result)  # type: ignore\n             self.result.compute_stats()\n \n             # Print results if requested\n"
        }
    ]
}