{
    "sha_fail": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
    "changed_files": [
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "CONTRIBUTORS",
            "diff": "diff --git a/CONTRIBUTORS b/CONTRIBUTORS\nindex 6ccd0893..adcc9214 100644\n--- a/CONTRIBUTORS\n+++ b/CONTRIBUTORS\n@@ -2,7 +2,6 @@ pukkandan (owner)\n shirt-dev (collaborator)\n coletdjnz/colethedj (collaborator)\n Ashish0804 (collaborator)\n-nao20010128nao/Lesmiscore (collaborator)\n bashonly (collaborator)\n Grub4K (collaborator)\n h-h-h-h\n@@ -467,3 +466,79 @@ nnoboa\n rdamas\n RfadnjdExt\n urectanc\n+nao20010128nao/Lesmiscore\n+04-pasha-04\n+aaruni96\n+aky-01\n+AmirAflak\n+ApoorvShah111\n+at-wat\n+davinkevin\n+demon071\n+denhotte\n+FinnRG\n+fireattack\n+Frankgoji\n+GD-Slime\n+hatsomatt\n+ifan-t\n+kshitiz305\n+kylegustavo\n+mabdelfattah\n+nathantouze\n+niemands\n+Rajeshwaran2001\n+RedDeffender\n+Rohxn16\n+sb0stn\n+SevenLives\n+simon300000\n+snixon\n+soundchaser128\n+szabyg\n+trainman261\n+trislee\n+wader\n+Yalab7\n+zhallgato\n+zhong-yiyu\n+Zprokkel\n+AS6939\n+drzraf\n+handlerug\n+jiru\n+madewokherd\n+xofe\n+awalgarg\n+midnightveil\n+naginatana\n+Riteo\n+1100101\n+aniolpages\n+bartbroere\n+CrendKing\n+Esokrates\n+HitomaruKonpaku\n+LoserFox\n+peci1\n+saintliao\n+shubhexists\n+SirElderling\n+almx\n+elivinsky\n+starius\n+TravisDupes\n+amir16yp\n+Fymyte\n+Ganesh910\n+hashFactory\n+kclauhk\n+Kyraminol\n+lstrojny\n+middlingphys\n+NickCis\n+nicodato\n+prettykool\n+S-Aarab\n+sonmezberkay\n+TSRBerry\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/make_changelog.py",
            "diff": "diff --git a/devscripts/make_changelog.py b/devscripts/make_changelog.py\nindex 84f72d52..123eebc2 100644\n--- a/devscripts/make_changelog.py\n+++ b/devscripts/make_changelog.py\n@@ -31,59 +31,55 @@ class CommitGroup(enum.Enum):\n     EXTRACTOR = 'Extractor'\n     DOWNLOADER = 'Downloader'\n     POSTPROCESSOR = 'Postprocessor'\n+    NETWORKING = 'Networking'\n     MISC = 'Misc.'\n \n-    @classmethod\n-    @property\n-    def ignorable_prefixes(cls):\n-        return ('core', 'downloader', 'extractor', 'misc', 'postprocessor', 'upstream')\n-\n     @classmethod\n     @lru_cache\n-    def commit_lookup(cls):\n+    def subgroup_lookup(cls):\n         return {\n             name: group\n             for group, names in {\n-                cls.PRIORITY: {'priority'},\n-                cls.CORE: {\n-                    'aes',\n-                    'cache',\n-                    'compat_utils',\n-                    'compat',\n-                    'cookies',\n-                    'core',\n-                    'dependencies',\n-                    'formats',\n-                    'jsinterp',\n-                    'networking',\n-                    'outtmpl',\n-                    'plugins',\n-                    'update',\n-                    'upstream',\n-                    'utils',\n-                },\n                 cls.MISC: {\n                     'build',\n+                    'ci',\n                     'cleanup',\n                     'devscripts',\n                     'docs',\n-                    'misc',\n                     'test',\n                 },\n-                cls.EXTRACTOR: {'extractor', 'ie'},\n-                cls.DOWNLOADER: {'downloader', 'fd'},\n-                cls.POSTPROCESSOR: {'postprocessor', 'pp'},\n+                cls.NETWORKING: {\n+                    'rh',\n+                },\n             }.items()\n             for name in names\n         }\n \n     @classmethod\n-    def get(cls, value):\n-        result = cls.commit_lookup().get(value)\n-        if result:\n-            logger.debug(f'Mapped {value!r} => {result.name}')\n+    @lru_cache\n+    def group_lookup(cls):\n+        result = {\n+            'fd': cls.DOWNLOADER,\n+            'ie': cls.EXTRACTOR,\n+            'pp': cls.POSTPROCESSOR,\n+            'upstream': cls.CORE,\n+        }\n+        result.update({item.name.lower(): item for item in iter(cls)})\n         return result\n \n+    @classmethod\n+    def get(cls, value: str) -> tuple[CommitGroup | None, str | None]:\n+        group, _, subgroup = (group.strip().lower() for group in value.partition('/'))\n+\n+        result = cls.group_lookup().get(group)\n+        if not result:\n+            if subgroup:\n+                return None, value\n+            subgroup = group\n+            result = cls.subgroup_lookup().get(subgroup)\n+\n+        return result, subgroup or None\n+\n \n @dataclass\n class Commit:\n@@ -198,19 +194,23 @@ def _prepare_cleanup_misc_items(self, items):\n         for commit_infos in cleanup_misc_items.values():\n             sorted_items.append(CommitInfo(\n                 'cleanup', ('Miscellaneous',), ', '.join(\n-                    self._format_message_link(None, info.commit.hash).strip()\n+                    self._format_message_link(None, info.commit.hash)\n                     for info in sorted(commit_infos, key=lambda item: item.commit.hash or '')),\n                 [], Commit(None, '', commit_infos[0].commit.authors), []))\n \n         return sorted_items\n \n-    def format_single_change(self, info):\n-        message = self._format_message_link(info.message, info.commit.hash)\n+    def format_single_change(self, info: CommitInfo):\n+        message, sep, rest = info.message.partition('\\n')\n+        if '[' not in message:\n+            # If the message doesn't already contain markdown links, try to add a link to the commit\n+            message = self._format_message_link(message, info.commit.hash)\n+\n         if info.issues:\n-            message = message.replace('\\n', f' ({self._format_issues(info.issues)})\\n', 1)\n+            message = f'{message} ({self._format_issues(info.issues)})'\n \n         if info.commit.authors:\n-            message = message.replace('\\n', f' by {self._format_authors(info.commit.authors)}\\n', 1)\n+            message = f'{message} by {self._format_authors(info.commit.authors)}'\n \n         if info.fixes:\n             fix_message = ', '.join(f'{self._format_message_link(None, fix.hash)}' for fix in info.fixes)\n@@ -219,16 +219,14 @@ def format_single_change(self, info):\n             if authors != info.commit.authors:\n                 fix_message = f'{fix_message} by {self._format_authors(authors)}'\n \n-            message = message.replace('\\n', f' (With fixes in {fix_message})\\n', 1)\n+            message = f'{message} (With fixes in {fix_message})'\n \n-        return message[:-1]\n+        return message if not sep else f'{message}{sep}{rest}'\n \n     def _format_message_link(self, message, hash):\n         assert message or hash, 'Improperly defined commit message or override'\n         message = message if message else hash[:HASH_LENGTH]\n-        if not hash:\n-            return f'{message}\\n'\n-        return f'[{message}\\n'.replace('\\n', f']({self.repo_url}/commit/{hash})\\n', 1)\n+        return f'[{message}]({self.repo_url}/commit/{hash})' if hash else message\n \n     def _format_issues(self, issues):\n         return ', '.join(f'[#{issue}]({self.repo_url}/issues/{issue})' for issue in issues)\n@@ -249,7 +247,7 @@ class CommitRange:\n     AUTHOR_INDICATOR_RE = re.compile(r'Authored by:? ', re.IGNORECASE)\n     MESSAGE_RE = re.compile(r'''\n         (?:\\[(?P<prefix>[^\\]]+)\\]\\ )?\n-        (?:(?P<sub_details>`?[^:`]+`?): )?\n+        (?:(?P<sub_details>`?[\\w.-]+`?): )?\n         (?P<message>.+?)\n         (?:\\ \\((?P<issues>\\#\\d+(?:,\\ \\#\\d+)*)\\))?\n         ''', re.VERBOSE | re.DOTALL)\n@@ -318,7 +316,7 @@ def _get_commits_and_fixes(self, default_author):\n         for commitish, revert_commit in reverts.items():\n             reverted = commits.pop(commitish, None)\n             if reverted:\n-                logger.debug(f'{commit} fully reverted {reverted}')\n+                logger.debug(f'{commitish} fully reverted {reverted}')\n             else:\n                 commits[revert_commit.hash] = revert_commit\n \n@@ -337,7 +335,7 @@ def apply_overrides(self, overrides):\n         for override in overrides:\n             when = override.get('when')\n             if when and when not in self and when != self._start:\n-                logger.debug(f'Ignored {when!r}, not in commits {self._start!r}')\n+                logger.debug(f'Ignored {when!r} override')\n                 continue\n \n             override_hash = override.get('hash') or when\n@@ -365,7 +363,7 @@ def groups(self):\n         for commit in self:\n             upstream_re = self.UPSTREAM_MERGE_RE.search(commit.short)\n             if upstream_re:\n-                commit.short = f'[core/upstream] Merged with youtube-dl {upstream_re.group(1)}'\n+                commit.short = f'[upstream] Merged with youtube-dl {upstream_re.group(1)}'\n \n             match = self.MESSAGE_RE.fullmatch(commit.short)\n             if not match:\n@@ -392,9 +390,9 @@ def groups(self):\n             if not group:\n                 if self.EXTRACTOR_INDICATOR_RE.search(commit.short):\n                     group = CommitGroup.EXTRACTOR\n+                    logger.error(f'Assuming [ie] group for {commit.short!r}')\n                 else:\n-                    group = CommitGroup.POSTPROCESSOR\n-                logger.warning(f'Failed to map {commit.short!r}, selected {group.name.lower()}')\n+                    group = CommitGroup.CORE\n \n             commit_info = CommitInfo(\n                 details, sub_details, message.strip(),\n@@ -410,25 +408,20 @@ def details_from_prefix(prefix):\n         if not prefix:\n             return CommitGroup.CORE, None, ()\n \n-        prefix, _, details = prefix.partition('/')\n-        prefix = prefix.strip()\n-        details = details.strip()\n+        prefix, *sub_details = prefix.split(':')\n \n-        group = CommitGroup.get(prefix.lower())\n-        if group is CommitGroup.PRIORITY:\n-            prefix, _, details = details.partition('/')\n+        group, details = CommitGroup.get(prefix)\n+        if group is CommitGroup.PRIORITY and details:\n+            details = details.partition('/')[2].strip()\n \n-        if not details and prefix and prefix not in CommitGroup.ignorable_prefixes:\n-            logger.debug(f'Replaced details with {prefix!r}')\n-            details = prefix or None\n+        if details and '/' in details:\n+            logger.error(f'Prefix is overnested, using first part: {prefix}')\n+            details = details.partition('/')[0].strip()\n \n         if details == 'common':\n             details = None\n-\n-        if details:\n-            details, *sub_details = details.split(':')\n-        else:\n-            sub_details = []\n+        elif group is CommitGroup.NETWORKING and details == 'rh':\n+            details = 'Request Handler'\n \n         return group, details, sub_details\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/make_issue_template.py",
            "diff": "diff --git a/devscripts/make_issue_template.py b/devscripts/make_issue_template.py\nindex 39b95c8d..a5d59f3c 100644\n--- a/devscripts/make_issue_template.py\n+++ b/devscripts/make_issue_template.py\n@@ -9,12 +9,7 @@\n \n import re\n \n-from devscripts.utils import (\n-    get_filename_args,\n-    read_file,\n-    read_version,\n-    write_file,\n-)\n+from devscripts.utils import get_filename_args, read_file, write_file\n \n VERBOSE_TMPL = '''\n   - type: checkboxes\n@@ -35,19 +30,18 @@\n       description: |\n         It should start like this:\n       placeholder: |\n-        [debug] Command-line config: ['-vU', 'test:youtube']\n-        [debug] Portable config \"yt-dlp.conf\": ['-i']\n+        [debug] Command-line config: ['-vU', 'https://www.youtube.com/watch?v=BaW_jenozKc']\n         [debug] Encodings: locale cp65001, fs utf-8, pref cp65001, out utf-8, error utf-8, screen utf-8\n-        [debug] yt-dlp version %(version)s [9d339c4] (win32_exe)\n+        [debug] yt-dlp version nightly@... from yt-dlp/yt-dlp [b634ba742] (win_exe)\n         [debug] Python 3.8.10 (CPython 64bit) - Windows-10-10.0.22000-SP0\n-        [debug] Checking exe version: ffmpeg -bsfs\n-        [debug] Checking exe version: ffprobe -bsfs\n         [debug] exe versions: ffmpeg N-106550-g072101bd52-20220410 (fdk,setts), ffprobe N-106624-g391ce570c8-20220415, phantomjs 2.1.1\n         [debug] Optional libraries: Cryptodome-3.15.0, brotli-1.0.9, certifi-2022.06.15, mutagen-1.45.1, sqlite3-2.6.0, websockets-10.3\n         [debug] Proxy map: {}\n-        [debug] Fetching release info: https://api.github.com/repos/yt-dlp/yt-dlp/releases/latest\n-        Latest version: %(version)s, Current version: %(version)s\n-        yt-dlp is up to date (%(version)s)\n+        [debug] Request Handlers: urllib, requests\n+        [debug] Loaded 1893 extractors\n+        [debug] Fetching release info: https://api.github.com/repos/yt-dlp/yt-dlp-nightly-builds/releases/latest\n+        yt-dlp is up to date (nightly@... from yt-dlp/yt-dlp-nightly-builds)\n+        [youtube] Extracting URL: https://www.youtube.com/watch?v=BaW_jenozKc\n         <more lines>\n       render: shell\n     validations:\n@@ -66,7 +60,7 @@\n \n \n def main():\n-    fields = {'version': read_version(), 'no_skip': NO_SKIP}\n+    fields = {'no_skip': NO_SKIP}\n     fields['verbose'] = VERBOSE_TMPL % fields\n     fields['verbose_optional'] = re.sub(r'(\\n\\s+validations:)?\\n\\s+required: true', '', fields['verbose'])\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/run_tests.bat",
            "diff": "diff --git a/devscripts/run_tests.bat b/devscripts/run_tests.bat\nindex 190d2391..57b1f4bf 100644\n--- a/devscripts/run_tests.bat\n+++ b/devscripts/run_tests.bat\n@@ -1,17 +1,4 @@\n-@setlocal\n @echo off\n-cd /d %~dp0..\n \n-if [\"%~1\"]==[\"\"] (\n-    set \"test_set=\"test\"\"\n-) else if [\"%~1\"]==[\"core\"] (\n-    set \"test_set=\"-m not download\"\"\n-) else if [\"%~1\"]==[\"download\"] (\n-    set \"test_set=\"-m \"download\"\"\n-) else (\n-    echo.Invalid test type \"%~1\". Use \"core\" ^| \"download\"\n-    exit /b 1\n-)\n-\n-set PYTHONWARNINGS=error\n-pytest %test_set%\n+>&2 echo run_tests.bat is deprecated. Please use `devscripts/run_tests.py` instead\n+python %~dp0run_tests.py %~1\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/run_tests.py",
            "diff": "diff --git a/devscripts/run_tests.py b/devscripts/run_tests.py\nnew file mode 100755\nindex 00000000..6d638a97\n--- /dev/null\n+++ b/devscripts/run_tests.py\n@@ -0,0 +1,71 @@\n+#!/usr/bin/env python3\n+\n+import argparse\n+import functools\n+import os\n+import re\n+import subprocess\n+import sys\n+from pathlib import Path\n+\n+\n+fix_test_name = functools.partial(re.compile(r'IE(_all|_\\d+)?$').sub, r'\\1')\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(description='Run selected yt-dlp tests')\n+    parser.add_argument(\n+        'test', help='a extractor tests, or one of \"core\" or \"download\"', nargs='*')\n+    parser.add_argument(\n+        '-k', help='run a test matching EXPRESSION. Same as \"pytest -k\"', metavar='EXPRESSION')\n+    return parser.parse_args()\n+\n+\n+def run_tests(*tests, pattern=None, ci=False):\n+    run_core = 'core' in tests or (not pattern and not tests)\n+    run_download = 'download' in tests\n+    tests = list(map(fix_test_name, tests))\n+\n+    arguments = ['pytest', '-Werror', '--tb=short']\n+    if ci:\n+        arguments.append('--color=yes')\n+    if run_core:\n+        arguments.extend(['-m', 'not download'])\n+    elif run_download:\n+        arguments.extend(['-m', 'download'])\n+    elif pattern:\n+        arguments.extend(['-k', pattern])\n+    else:\n+        arguments.extend(\n+            f'test/test_download.py::TestDownload::test_{test}' for test in tests)\n+\n+    print(f'Running {arguments}', flush=True)\n+    try:\n+        return subprocess.call(arguments)\n+    except FileNotFoundError:\n+        pass\n+\n+    arguments = [sys.executable, '-Werror', '-m', 'unittest']\n+    if run_core:\n+        print('\"pytest\" needs to be installed to run core tests', file=sys.stderr, flush=True)\n+        return 1\n+    elif run_download:\n+        arguments.append('test.test_download')\n+    elif pattern:\n+        arguments.extend(['-k', pattern])\n+    else:\n+        arguments.extend(\n+            f'test.test_download.TestDownload.test_{test}' for test in tests)\n+\n+    print(f'Running {arguments}', flush=True)\n+    return subprocess.call(arguments)\n+\n+\n+if __name__ == '__main__':\n+    try:\n+        args = parse_args()\n+\n+        os.chdir(Path(__file__).parent.parent)\n+        sys.exit(run_tests(*args.test, pattern=args.k, ci=bool(os.getenv('CI'))))\n+    except KeyboardInterrupt:\n+        pass\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/run_tests.sh",
            "diff": "diff --git a/devscripts/run_tests.sh b/devscripts/run_tests.sh\nindex faa642e9..123ceb1e 100755\n--- a/devscripts/run_tests.sh\n+++ b/devscripts/run_tests.sh\n@@ -1,14 +1,4 @@\n #!/usr/bin/env sh\n \n-if [ -z \"$1\" ]; then\n-    test_set='test'\n-elif [ \"$1\" = 'core' ]; then\n-    test_set=\"-m not download\"\n-elif [ \"$1\" = 'download' ]; then\n-    test_set=\"-m download\"\n-else\n-    echo 'Invalid test type \"'\"$1\"'\". Use \"core\" | \"download\"'\n-    exit 1\n-fi\n-\n-python3 -bb -Werror -m pytest \"$test_set\"\n+>&2 echo 'run_tests.sh is deprecated. Please use `devscripts/run_tests.py` instead'\n+python3 devscripts/run_tests.py \"$1\"\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/update-formulae.py",
            "diff": "diff --git a/devscripts/update-formulae.py b/devscripts/update-formulae.py\ndeleted file mode 100644\nindex e79297f5..00000000\n--- a/devscripts/update-formulae.py\n+++ /dev/null\n@@ -1,39 +0,0 @@\n-#!/usr/bin/env python3\n-\n-\"\"\"\n-Usage: python3 ./devscripts/update-formulae.py <path-to-formulae-rb> <version>\n-version can be either 0-aligned (yt-dlp version) or normalized (PyPi version)\n-\"\"\"\n-\n-# Allow direct execution\n-import os\n-import sys\n-\n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n-\n-\n-import json\n-import re\n-import urllib.request\n-\n-from devscripts.utils import read_file, write_file\n-\n-filename, version = sys.argv[1:]\n-\n-normalized_version = '.'.join(str(int(x)) for x in version.split('.'))\n-\n-pypi_release = json.loads(urllib.request.urlopen(\n-    'https://pypi.org/pypi/yt-dlp/%s/json' % normalized_version\n-).read().decode())\n-\n-tarball_file = next(x for x in pypi_release['urls'] if x['filename'].endswith('.tar.gz'))\n-\n-sha256sum = tarball_file['digests']['sha256']\n-url = tarball_file['url']\n-\n-formulae_text = read_file(filename)\n-\n-formulae_text = re.sub(r'sha256 \"[0-9a-f]*?\"', 'sha256 \"%s\"' % sha256sum, formulae_text, count=1)\n-formulae_text = re.sub(r'url \"[^\"]*?\"', 'url \"%s\"' % url, formulae_text, count=1)\n-\n-write_file(filename, formulae_text)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/update-version.py",
            "diff": "diff --git a/devscripts/update-version.py b/devscripts/update-version.py\nindex c873d10a..da54a6a2 100644\n--- a/devscripts/update-version.py\n+++ b/devscripts/update-version.py\n@@ -10,17 +10,17 @@\n import argparse\n import contextlib\n import sys\n-from datetime import datetime\n+from datetime import datetime, timezone\n \n from devscripts.utils import read_version, run_process, write_file\n \n \n def get_new_version(version, revision):\n     if not version:\n-        version = datetime.utcnow().strftime('%Y.%m.%d')\n+        version = datetime.now(timezone.utc).strftime('%Y.%m.%d')\n \n     if revision:\n-        assert revision.isdigit(), 'Revision must be a number'\n+        assert revision.isdecimal(), 'Revision must be a number'\n     else:\n         old_version = read_version().split('.')\n         if version.split('.') == old_version[:3]:\n@@ -46,6 +46,10 @@ def get_git_head():\n UPDATE_HINT = None\n \n CHANNEL = {channel!r}\n+\n+ORIGIN = {origin!r}\n+\n+_pkg_version = {package_version!r}\n '''\n \n if __name__ == '__main__':\n@@ -53,6 +57,12 @@ def get_git_head():\n     parser.add_argument(\n         '-c', '--channel', default='stable',\n         help='Select update channel (default: %(default)s)')\n+    parser.add_argument(\n+        '-r', '--origin', default='local',\n+        help='Select origin/repository (default: %(default)s)')\n+    parser.add_argument(\n+        '-s', '--suffix', default='',\n+        help='Add an alphanumeric suffix to the package version, e.g. \"dev\"')\n     parser.add_argument(\n         '-o', '--output', default='yt_dlp/version.py',\n         help='The output file to write to (default: %(default)s)')\n@@ -66,6 +76,7 @@ def get_git_head():\n         args.version if args.version and '.' in args.version\n         else get_new_version(None, args.version))\n     write_file(args.output, VERSION_TEMPLATE.format(\n-        version=version, git_head=git_head, channel=args.channel))\n+        version=version, git_head=git_head, channel=args.channel, origin=args.origin,\n+        package_version=f'{version}{args.suffix}'))\n \n     print(f'version={version} ({args.channel}), head={git_head}')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "devscripts/utils.py",
            "diff": "diff --git a/devscripts/utils.py b/devscripts/utils.py\nindex f75a84da..a952c9fa 100644\n--- a/devscripts/utils.py\n+++ b/devscripts/utils.py\n@@ -13,10 +13,11 @@ def write_file(fname, content, mode='w'):\n         return f.write(content)\n \n \n-def read_version(fname='yt_dlp/version.py'):\n+def read_version(fname='yt_dlp/version.py', varname='__version__'):\n     \"\"\"Get the version without importing the package\"\"\"\n-    exec(compile(read_file(fname), fname, 'exec'))\n-    return locals()['__version__']\n+    items = {}\n+    exec(compile(read_file(fname), fname, 'exec'), items)\n+    return items[varname]\n \n \n def get_filename_args(has_infile=False, default_outfile=None):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "requirements.txt",
            "diff": "diff --git a/requirements.txt b/requirements.txt\nindex dde37120..06ff82a8 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,6 +1,8 @@\n mutagen\n pycryptodomex\n-websockets\n-brotli; platform_python_implementation=='CPython'\n-brotlicffi; platform_python_implementation!='CPython'\n+brotli; implementation_name=='cpython'\n+brotlicffi; implementation_name!='cpython'\n certifi\n+requests>=2.31.0,<3\n+urllib3>=1.26.17,<3\n+websockets>=12.0\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "setup.cfg",
            "diff": "diff --git a/setup.cfg b/setup.cfg\nindex 6deaa797..a799f729 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -26,7 +26,7 @@ markers =\n \n [tox:tox]\n skipsdist = true\n-envlist = py{36,37,38,39,310,311},pypy{36,37,38,39}\n+envlist = py{38,39,310,311,312},pypy{38,39,310}\n skip_missing_interpreters = true\n \n [testenv]  # tox\n@@ -39,7 +39,7 @@ setenv =\n \n \n [isort]\n-py_version = 37\n+py_version = 38\n multi_line_output = VERTICAL_HANGING_INDENT\n line_length = 80\n reverse_relative = true\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "setup.py",
            "diff": "diff --git a/setup.py b/setup.py\nindex a2f9f55c..3d9a69d1 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -18,7 +18,7 @@\n \n from devscripts.utils import read_file, read_version\n \n-VERSION = read_version()\n+VERSION = read_version(varname='_pkg_version')\n \n DESCRIPTION = 'A youtube-dl fork with additional features and patches'\n \n@@ -62,7 +62,14 @@ def py2exe_params():\n             'compressed': 1,\n             'optimize': 2,\n             'dist_dir': './dist',\n-            'excludes': ['Crypto', 'Cryptodome'],  # py2exe cannot import Crypto\n+            'excludes': [\n+                # py2exe cannot import Crypto\n+                'Crypto',\n+                'Cryptodome',\n+                # py2exe appears to confuse this with our socks library.\n+                # We don't use pysocks and urllib3.contrib.socks would fail to import if tried.\n+                'urllib3.contrib.socks'\n+            ],\n             'dll_excludes': ['w9xpopen.exe', 'crypt32.dll'],\n             # Modules that are only imported dynamically must be added here\n             'includes': ['yt_dlp.compat._legacy', 'yt_dlp.compat._deprecated',\n@@ -135,7 +142,7 @@ def main():\n         params = build_params()\n \n     setup(\n-        name='yt-dlp',\n+        name='yt-dlp',  # package name (do not change/remove comment)\n         version=VERSION,\n         maintainer='pukkandan',\n         maintainer_email='pukkandan.ytdlp@gmail.com',\n@@ -145,7 +152,7 @@ def main():\n         url='https://github.com/yt-dlp/yt-dlp',\n         packages=packages(),\n         install_requires=REQUIREMENTS,\n-        python_requires='>=3.7',\n+        python_requires='>=3.8',\n         project_urls={\n             'Documentation': 'https://github.com/yt-dlp/yt-dlp#readme',\n             'Source': 'https://github.com/yt-dlp/yt-dlp',\n@@ -157,11 +164,11 @@ def main():\n             'Development Status :: 5 - Production/Stable',\n             'Environment :: Console',\n             'Programming Language :: Python',\n-            'Programming Language :: Python :: 3.7',\n             'Programming Language :: Python :: 3.8',\n             'Programming Language :: Python :: 3.9',\n             'Programming Language :: Python :: 3.10',\n             'Programming Language :: Python :: 3.11',\n+            'Programming Language :: Python :: 3.12',\n             'Programming Language :: Python :: Implementation',\n             'Programming Language :: Python :: Implementation :: CPython',\n             'Programming Language :: Python :: Implementation :: PyPy',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/conftest.py",
            "diff": "diff --git a/test/conftest.py b/test/conftest.py\nnew file mode 100644\nindex 00000000..2fbc269e\n--- /dev/null\n+++ b/test/conftest.py\n@@ -0,0 +1,26 @@\n+import functools\n+import inspect\n+\n+import pytest\n+\n+from yt_dlp.networking import RequestHandler\n+from yt_dlp.networking.common import _REQUEST_HANDLERS\n+from yt_dlp.utils._utils import _YDLLogger as FakeLogger\n+\n+\n+@pytest.fixture\n+def handler(request):\n+    RH_KEY = request.param\n+    if inspect.isclass(RH_KEY) and issubclass(RH_KEY, RequestHandler):\n+        handler = RH_KEY\n+    elif RH_KEY in _REQUEST_HANDLERS:\n+        handler = _REQUEST_HANDLERS[RH_KEY]\n+    else:\n+        pytest.skip(f'{RH_KEY} request handler is not available')\n+\n+    return functools.partial(handler, logger=FakeLogger)\n+\n+\n+def validate_and_send(rh, req):\n+    rh.validate(req)\n+    return rh.send(req)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/helper.py",
            "diff": "diff --git a/test/helper.py b/test/helper.py\nindex 539b2f61..e5ace8fe 100644\n--- a/test/helper.py\n+++ b/test/helper.py\n@@ -10,7 +10,7 @@\n import yt_dlp.extractor\n from yt_dlp import YoutubeDL\n from yt_dlp.compat import compat_os_name\n-from yt_dlp.utils import preferredencoding, write_string\n+from yt_dlp.utils import preferredencoding, try_call, write_string\n \n if 'pytest' in sys.modules:\n     import pytest\n@@ -214,14 +214,19 @@ def sanitize(key, value):\n \n     test_info_dict = {\n         key: sanitize(key, value) for key, value in got_dict.items()\n-        if value is not None and key not in IGNORED_FIELDS and not any(\n-            key.startswith(f'{prefix}_') for prefix in IGNORED_PREFIXES)\n+        if value is not None and key not in IGNORED_FIELDS and (\n+            not any(key.startswith(f'{prefix}_') for prefix in IGNORED_PREFIXES)\n+            or key == '_old_archive_ids')\n     }\n \n     # display_id may be generated from id\n     if test_info_dict.get('display_id') == test_info_dict.get('id'):\n         test_info_dict.pop('display_id')\n \n+    # release_year may be generated from release_date\n+    if try_call(lambda: test_info_dict['release_year'] == int(test_info_dict['release_date'][:4])):\n+        test_info_dict.pop('release_year')\n+\n     # Check url for flat entries\n     if got_dict.get('_type', 'video') != 'video' and got_dict.get('url'):\n         test_info_dict['url'] = got_dict['url']\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_YoutubeDL.py",
            "diff": "diff --git a/test/test_YoutubeDL.py b/test/test_YoutubeDL.py\nindex 3cfb61fb..0087cbc9 100644\n--- a/test/test_YoutubeDL.py\n+++ b/test/test_YoutubeDL.py\n@@ -140,6 +140,8 @@ def test(inp, *expected, multi=False):\n         test('example-with-dashes', 'example-with-dashes')\n         test('all', '2', '47', '45', 'example-with-dashes', '35')\n         test('mergeall', '2+47+45+example-with-dashes+35', multi=True)\n+        # See: https://github.com/yt-dlp/yt-dlp/pulls/8797\n+        test('7_a/worst', '35')\n \n     def test_format_selection_audio(self):\n         formats = [\n@@ -631,7 +633,6 @@ def test_add_extra_info(self):\n         self.assertEqual(test_dict['playlist'], 'funny videos')\n \n     outtmpl_info = {\n-        'id': '1234',\n         'id': '1234',\n         'ext': 'mp4',\n         'width': None,\n@@ -729,7 +730,7 @@ def expect_same_infodict(out):\n                 self.assertEqual(got_dict.get(info_field), expected, info_field)\n             return True\n \n-        test('%()j', (expect_same_infodict, str))\n+        test('%()j', (expect_same_infodict, None))\n \n         # NA placeholder\n         NA_TEST_OUTTMPL = '%(uploader_date)s-%(width)d-%(x|def)s-%(id)s.%(ext)s'\n@@ -785,9 +786,9 @@ def expect_same_infodict(out):\n         test('%(title4)#S', 'foo_bar_test')\n         test('%(title4).10S', ('foo \uff02bar\uff02 ', 'foo \uff02bar\uff02' + ('#' if compat_os_name == 'nt' else ' ')))\n         if compat_os_name == 'nt':\n-            test('%(title4)q', ('\"foo \\\\\"bar\\\\\" test\"', \"\uff02foo \u29f9\uff02bar\u29f9\uff02 test\uff02\"))\n-            test('%(formats.:.id)#q', ('\"id 1\" \"id 2\" \"id 3\"', '\uff02id 1\uff02 \uff02id 2\uff02 \uff02id 3\uff02'))\n-            test('%(formats.0.id)#q', ('\"id 1\"', '\uff02id 1\uff02'))\n+            test('%(title4)q', ('\"foo \"\"bar\"\" test\"', None))\n+            test('%(formats.:.id)#q', ('\"id 1\" \"id 2\" \"id 3\"', None))\n+            test('%(formats.0.id)#q', ('\"id 1\"', None))\n         else:\n             test('%(title4)q', ('\\'foo \"bar\" test\\'', '\\'foo \uff02bar\uff02 test\\''))\n             test('%(formats.:.id)#q', \"'id 1' 'id 2' 'id 3'\")\n@@ -798,6 +799,7 @@ def expect_same_infodict(out):\n         test('%(title|%)s %(title|%%)s', '% %%')\n         test('%(id+1-height+3)05d', '00158')\n         test('%(width+100)05d', 'NA')\n+        test('%(filesize*8)d', '8192')\n         test('%(formats.0) 15s', ('% 15s' % FORMATS[0], None))\n         test('%(formats.0)r', (repr(FORMATS[0]), None))\n         test('%(height.0)03d', '001')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_download.py",
            "diff": "diff --git a/test/test_download.py b/test/test_download.py\nindex 6f00a4de..25307924 100755\n--- a/test/test_download.py\n+++ b/test/test_download.py\n@@ -31,6 +31,7 @@\n     DownloadError,\n     ExtractorError,\n     UnavailableVideoError,\n+    YoutubeDLError,\n     format_bytes,\n     join_nonempty,\n )\n@@ -100,6 +101,8 @@ def print_skipping(reason):\n             print_skipping('IE marked as not _WORKING')\n \n         for tc in test_cases:\n+            if tc.get('expected_exception'):\n+                continue\n             info_dict = tc.get('info_dict', {})\n             params = tc.get('params', {})\n             if not info_dict.get('id'):\n@@ -139,6 +142,17 @@ def get_tc_filename(tc):\n \n         res_dict = None\n \n+        def match_exception(err):\n+            expected_exception = test_case.get('expected_exception')\n+            if not expected_exception:\n+                return False\n+            if err.__class__.__name__ == expected_exception:\n+                return True\n+            for exc in err.exc_info:\n+                if exc.__class__.__name__ == expected_exception:\n+                    return True\n+            return False\n+\n         def try_rm_tcs_files(tcs=None):\n             if tcs is None:\n                 tcs = test_cases\n@@ -161,6 +175,8 @@ def try_rm_tcs_files(tcs=None):\n                 except (DownloadError, ExtractorError) as err:\n                     # Check if the exception is not a network related one\n                     if not isinstance(err.exc_info[1], (TransportError, UnavailableVideoError)) or (isinstance(err.exc_info[1], HTTPError) and err.exc_info[1].status == 503):\n+                        if match_exception(err):\n+                            return\n                         err.msg = f'{getattr(err, \"msg\", err)} ({tname})'\n                         raise\n \n@@ -171,6 +187,10 @@ def try_rm_tcs_files(tcs=None):\n                     print(f'Retrying: {try_num} failed tries\\n\\n##########\\n\\n')\n \n                     try_num += 1\n+                except YoutubeDLError as err:\n+                    if match_exception(err):\n+                        return\n+                    raise\n                 else:\n                     break\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_execution.py",
            "diff": "diff --git a/test/test_execution.py b/test/test_execution.py\nindex 7a9e800b..fb2f6e2e 100644\n--- a/test/test_execution.py\n+++ b/test/test_execution.py\n@@ -45,6 +45,9 @@ def test_lazy_extractors(self):\n             self.assertTrue(os.path.exists(LAZY_EXTRACTORS))\n \n             _, stderr = self.run_yt_dlp(opts=('-s', 'test:'))\n+            # `MIN_RECOMMENDED` emits a deprecated feature warning for deprecated python versions\n+            if stderr and stderr.startswith('Deprecated Feature: Support for Python'):\n+                stderr = ''\n             self.assertFalse(stderr)\n \n             subprocess.check_call([sys.executable, 'test/test_all_urls.py'], cwd=rootDir, stdout=subprocess.DEVNULL)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_networking.py",
            "diff": "diff --git a/test/test_networking.py b/test/test_networking.py\nindex 2622d24d..dc60ca69 100644\n--- a/test/test_networking.py\n+++ b/test/test_networking.py\n@@ -8,12 +8,10 @@\n \n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-import functools\n import gzip\n import http.client\n import http.cookiejar\n import http.server\n-import inspect\n import io\n import pathlib\n import random\n@@ -30,7 +28,7 @@\n \n from test.helper import FakeYDL, http_server_port\n from yt_dlp.cookies import YoutubeDLCookieJar\n-from yt_dlp.dependencies import brotli\n+from yt_dlp.dependencies import brotli, requests, urllib3\n from yt_dlp.networking import (\n     HEADRequest,\n     PUTRequest,\n@@ -40,12 +38,12 @@\n     Response,\n )\n from yt_dlp.networking._urllib import UrllibRH\n-from yt_dlp.networking.common import _REQUEST_HANDLERS\n from yt_dlp.networking.exceptions import (\n     CertificateVerifyError,\n     HTTPError,\n     IncompleteRead,\n     NoSupportingHandlers,\n+    ProxyError,\n     RequestError,\n     SSLError,\n     TransportError,\n@@ -54,6 +52,8 @@\n from yt_dlp.utils._utils import _YDLLogger as FakeLogger\n from yt_dlp.utils.networking import HTTPHeaderDict\n \n+from test.conftest import validate_and_send\n+\n TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n \n \n@@ -277,11 +277,6 @@ def send_header(self, keyword, value):\n         self._headers_buffer.append(f'{keyword}: {value}\\r\\n'.encode())\n \n \n-def validate_and_send(rh, req):\n-    rh.validate(req)\n-    return rh.send(req)\n-\n-\n class TestRequestHandlerBase:\n     @classmethod\n     def setup_class(cls):\n@@ -307,21 +302,8 @@ def setup_class(cls):\n         cls.https_server_thread.start()\n \n \n-@pytest.fixture\n-def handler(request):\n-    RH_KEY = request.param\n-    if inspect.isclass(RH_KEY) and issubclass(RH_KEY, RequestHandler):\n-        handler = RH_KEY\n-    elif RH_KEY in _REQUEST_HANDLERS:\n-        handler = _REQUEST_HANDLERS[RH_KEY]\n-    else:\n-        pytest.skip(f'{RH_KEY} request handler is not available')\n-\n-    return functools.partial(handler, logger=FakeLogger)\n-\n-\n class TestHTTPRequestHandler(TestRequestHandlerBase):\n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_verify_cert(self, handler):\n         with handler() as rh:\n             with pytest.raises(CertificateVerifyError):\n@@ -332,7 +314,7 @@ def test_verify_cert(self, handler):\n             assert r.status == 200\n             r.close()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_ssl_error(self, handler):\n         # HTTPS server with too old TLS version\n         # XXX: is there a better way to test this than to create a new server?\n@@ -346,11 +328,11 @@ def test_ssl_error(self, handler):\n         https_server_thread.start()\n \n         with handler(verify=False) as rh:\n-            with pytest.raises(SSLError, match='sslv3 alert handshake failure') as exc_info:\n+            with pytest.raises(SSLError, match=r'ssl(?:v3|/tls) alert handshake failure') as exc_info:\n                 validate_and_send(rh, Request(f'https://127.0.0.1:{https_port}/headers'))\n             assert not issubclass(exc_info.type, CertificateVerifyError)\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_percent_encode(self, handler):\n         with handler() as rh:\n             # Unicode characters should be encoded with uppercase percent-encoding\n@@ -362,7 +344,7 @@ def test_percent_encode(self, handler):\n             assert res.status == 200\n             res.close()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_remove_dot_segments(self, handler):\n         with handler() as rh:\n             # This isn't a comprehensive test,\n@@ -377,14 +359,14 @@ def test_remove_dot_segments(self, handler):\n             assert res.url == f'http://127.0.0.1:{self.http_port}/headers'\n             res.close()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_unicode_path_redirection(self, handler):\n         with handler() as rh:\n             r = validate_and_send(rh, Request(f'http://127.0.0.1:{self.http_port}/302-non-ascii-redirect'))\n             assert r.url == f'http://127.0.0.1:{self.http_port}/%E4%B8%AD%E6%96%87.html'\n             r.close()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_raise_http_error(self, handler):\n         with handler() as rh:\n             for bad_status in (400, 500, 599, 302):\n@@ -394,7 +376,7 @@ def test_raise_http_error(self, handler):\n             # Should not raise an error\n             validate_and_send(rh, Request('http://127.0.0.1:%d/gen_200' % self.http_port)).close()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_response_url(self, handler):\n         with handler() as rh:\n             # Response url should be that of the last url in redirect chain\n@@ -405,7 +387,7 @@ def test_response_url(self, handler):\n             assert res2.url == f'http://127.0.0.1:{self.http_port}/gen_200'\n             res2.close()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_redirect(self, handler):\n         with handler() as rh:\n             def do_req(redirect_status, method, assert_no_content=False):\n@@ -460,7 +442,7 @@ def do_req(redirect_status, method, assert_no_content=False):\n                 with pytest.raises(HTTPError):\n                     do_req(code, 'GET')\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_request_cookie_header(self, handler):\n         # We should accept a Cookie header being passed as in normal headers and handle it appropriately.\n         with handler() as rh:\n@@ -492,19 +474,19 @@ def test_request_cookie_header(self, handler):\n             assert b'Cookie: test=ytdlp' not in data\n             assert b'Cookie: test=test' in data\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_redirect_loop(self, handler):\n         with handler() as rh:\n             with pytest.raises(HTTPError, match='redirect loop'):\n                 validate_and_send(rh, Request(f'http://127.0.0.1:{self.http_port}/redirect_loop'))\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_incompleteread(self, handler):\n         with handler(timeout=2) as rh:\n             with pytest.raises(IncompleteRead):\n                 validate_and_send(rh, Request('http://127.0.0.1:%d/incompleteread' % self.http_port)).read()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_cookies(self, handler):\n         cookiejar = YoutubeDLCookieJar()\n         cookiejar.set_cookie(http.cookiejar.Cookie(\n@@ -521,7 +503,7 @@ def test_cookies(self, handler):\n                 rh, Request(f'http://127.0.0.1:{self.http_port}/headers', extensions={'cookiejar': cookiejar})).read()\n             assert b'Cookie: test=ytdlp' in data\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_headers(self, handler):\n \n         with handler(headers=HTTPHeaderDict({'test1': 'test', 'test2': 'test2'})) as rh:\n@@ -537,7 +519,7 @@ def test_headers(self, handler):\n             assert b'Test2: test2' not in data\n             assert b'Test3: test3' in data\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_timeout(self, handler):\n         with handler() as rh:\n             # Default timeout is 20 seconds, so this should go through\n@@ -553,7 +535,7 @@ def test_timeout(self, handler):\n             validate_and_send(\n                 rh, Request(f'http://127.0.0.1:{self.http_port}/timeout_1', extensions={'timeout': 4}))\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_source_address(self, handler):\n         source_address = f'127.0.0.{random.randint(5, 255)}'\n         with handler(source_address=source_address) as rh:\n@@ -561,13 +543,13 @@ def test_source_address(self, handler):\n                 rh, Request(f'http://127.0.0.1:{self.http_port}/source_address')).read().decode()\n             assert source_address == data\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_gzip_trailing_garbage(self, handler):\n         with handler() as rh:\n             data = validate_and_send(rh, Request(f'http://localhost:{self.http_port}/trailing_garbage')).read().decode()\n             assert data == '<html><video src=\"/vid.mp4\" /></html>'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     @pytest.mark.skipif(not brotli, reason='brotli support is not installed')\n     def test_brotli(self, handler):\n         with handler() as rh:\n@@ -578,7 +560,7 @@ def test_brotli(self, handler):\n             assert res.headers.get('Content-Encoding') == 'br'\n             assert res.read() == b'<html><video src=\"/vid.mp4\" /></html>'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_deflate(self, handler):\n         with handler() as rh:\n             res = validate_and_send(\n@@ -588,7 +570,7 @@ def test_deflate(self, handler):\n             assert res.headers.get('Content-Encoding') == 'deflate'\n             assert res.read() == b'<html><video src=\"/vid.mp4\" /></html>'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_gzip(self, handler):\n         with handler() as rh:\n             res = validate_and_send(\n@@ -598,7 +580,7 @@ def test_gzip(self, handler):\n             assert res.headers.get('Content-Encoding') == 'gzip'\n             assert res.read() == b'<html><video src=\"/vid.mp4\" /></html>'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_multiple_encodings(self, handler):\n         with handler() as rh:\n             for pair in ('gzip,deflate', 'deflate, gzip', 'gzip, gzip', 'deflate, deflate'):\n@@ -609,7 +591,7 @@ def test_multiple_encodings(self, handler):\n                 assert res.headers.get('Content-Encoding') == pair\n                 assert res.read() == b'<html><video src=\"/vid.mp4\" /></html>'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_unsupported_encoding(self, handler):\n         with handler() as rh:\n             res = validate_and_send(\n@@ -619,7 +601,7 @@ def test_unsupported_encoding(self, handler):\n             assert res.headers.get('Content-Encoding') == 'unsupported'\n             assert res.read() == b'raw'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_read(self, handler):\n         with handler() as rh:\n             res = validate_and_send(\n@@ -649,7 +631,7 @@ def setup_class(cls):\n         cls.geo_proxy_thread.daemon = True\n         cls.geo_proxy_thread.start()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_http_proxy(self, handler):\n         http_proxy = f'http://127.0.0.1:{self.proxy_port}'\n         geo_proxy = f'http://127.0.0.1:{self.geo_port}'\n@@ -675,7 +657,7 @@ def test_http_proxy(self, handler):\n             assert res != f'normal: {real_url}'\n             assert 'Accept' in res\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_noproxy(self, handler):\n         with handler(proxies={'proxy': f'http://127.0.0.1:{self.proxy_port}'}) as rh:\n             # NO_PROXY\n@@ -685,7 +667,7 @@ def test_noproxy(self, handler):\n                     'utf-8')\n                 assert 'Accept' in nop_response\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_allproxy(self, handler):\n         url = 'http://foo.com/bar'\n         with handler() as rh:\n@@ -693,7 +675,7 @@ def test_allproxy(self, handler):\n                 'utf-8')\n             assert response == f'normal: {url}'\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_http_proxy_with_idn(self, handler):\n         with handler(proxies={\n             'http': f'http://127.0.0.1:{self.proxy_port}',\n@@ -731,27 +713,27 @@ def _run_test(self, handler, **handler_kwargs):\n         ) as rh:\n             validate_and_send(rh, Request(f'https://127.0.0.1:{self.port}/video.html')).read().decode()\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_certificate_combined_nopass(self, handler):\n         self._run_test(handler, client_cert={\n             'client_certificate': os.path.join(self.certdir, 'clientwithkey.crt'),\n         })\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_certificate_nocombined_nopass(self, handler):\n         self._run_test(handler, client_cert={\n             'client_certificate': os.path.join(self.certdir, 'client.crt'),\n             'client_certificate_key': os.path.join(self.certdir, 'client.key'),\n         })\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_certificate_combined_pass(self, handler):\n         self._run_test(handler, client_cert={\n             'client_certificate': os.path.join(self.certdir, 'clientwithencryptedkey.crt'),\n             'client_certificate_password': 'foobar',\n         })\n \n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_certificate_nocombined_pass(self, handler):\n         self._run_test(handler, client_cert={\n             'client_certificate': os.path.join(self.certdir, 'client.crt'),\n@@ -835,6 +817,76 @@ def test_httplib_validation_errors(self, handler, req, match, version_check):\n             assert not isinstance(exc_info.value, TransportError)\n \n \n+class TestRequestsRequestHandler(TestRequestHandlerBase):\n+    @pytest.mark.parametrize('raised,expected', [\n+        (lambda: requests.exceptions.ConnectTimeout(), TransportError),\n+        (lambda: requests.exceptions.ReadTimeout(), TransportError),\n+        (lambda: requests.exceptions.Timeout(), TransportError),\n+        (lambda: requests.exceptions.ConnectionError(), TransportError),\n+        (lambda: requests.exceptions.ProxyError(), ProxyError),\n+        (lambda: requests.exceptions.SSLError('12[CERTIFICATE_VERIFY_FAILED]34'), CertificateVerifyError),\n+        (lambda: requests.exceptions.SSLError(), SSLError),\n+        (lambda: requests.exceptions.InvalidURL(), RequestError),\n+        (lambda: requests.exceptions.InvalidHeader(), RequestError),\n+        # catch-all: https://github.com/psf/requests/blob/main/src/requests/adapters.py#L535\n+        (lambda: urllib3.exceptions.HTTPError(), TransportError),\n+        (lambda: requests.exceptions.RequestException(), RequestError)\n+        #  (lambda: requests.exceptions.TooManyRedirects(), HTTPError) - Needs a response object\n+    ])\n+    @pytest.mark.parametrize('handler', ['Requests'], indirect=True)\n+    def test_request_error_mapping(self, handler, monkeypatch, raised, expected):\n+        with handler() as rh:\n+            def mock_get_instance(*args, **kwargs):\n+                class MockSession:\n+                    def request(self, *args, **kwargs):\n+                        raise raised()\n+                return MockSession()\n+\n+            monkeypatch.setattr(rh, '_get_instance', mock_get_instance)\n+\n+            with pytest.raises(expected) as exc_info:\n+                rh.send(Request('http://fake'))\n+\n+            assert exc_info.type is expected\n+\n+    @pytest.mark.parametrize('raised,expected,match', [\n+        (lambda: urllib3.exceptions.SSLError(), SSLError, None),\n+        (lambda: urllib3.exceptions.TimeoutError(), TransportError, None),\n+        (lambda: urllib3.exceptions.ReadTimeoutError(None, None, None), TransportError, None),\n+        (lambda: urllib3.exceptions.ProtocolError(), TransportError, None),\n+        (lambda: urllib3.exceptions.DecodeError(), TransportError, None),\n+        (lambda: urllib3.exceptions.HTTPError(), TransportError, None),  # catch-all\n+        (\n+            lambda: urllib3.exceptions.ProtocolError('error', http.client.IncompleteRead(partial=b'abc', expected=4)),\n+            IncompleteRead,\n+            '3 bytes read, 4 more expected'\n+        ),\n+        (\n+            lambda: urllib3.exceptions.ProtocolError('error', urllib3.exceptions.IncompleteRead(partial=3, expected=5)),\n+            IncompleteRead,\n+            '3 bytes read, 5 more expected'\n+        ),\n+    ])\n+    @pytest.mark.parametrize('handler', ['Requests'], indirect=True)\n+    def test_response_error_mapping(self, handler, monkeypatch, raised, expected, match):\n+        from requests.models import Response as RequestsResponse\n+        from urllib3.response import HTTPResponse as Urllib3Response\n+\n+        from yt_dlp.networking._requests import RequestsResponseAdapter\n+        requests_res = RequestsResponse()\n+        requests_res.raw = Urllib3Response(body=b'', status=200)\n+        res = RequestsResponseAdapter(requests_res)\n+\n+        def mock_read(*args, **kwargs):\n+            raise raised()\n+        monkeypatch.setattr(res.fp, 'read', mock_read)\n+\n+        with pytest.raises(expected, match=match) as exc_info:\n+            res.read()\n+\n+        assert exc_info.type is expected\n+\n+\n def run_validation(handler, error, req, **handler_kwargs):\n     with handler(**handler_kwargs) as rh:\n         if error:\n@@ -871,13 +923,21 @@ class HTTPSupportedRH(ValidationRH):\n             ('file', UnsupportedRequest, {}),\n             ('file', False, {'enable_file_urls': True}),\n         ]),\n+        ('Requests', [\n+            ('http', False, {}),\n+            ('https', False, {}),\n+        ]),\n+        ('Websockets', [\n+            ('ws', False, {}),\n+            ('wss', False, {}),\n+        ]),\n         (NoCheckRH, [('http', False, {})]),\n         (ValidationRH, [('http', UnsupportedRequest, {})])\n     ]\n \n     PROXY_SCHEME_TESTS = [\n         # scheme, expected to fail\n-        ('Urllib', [\n+        ('Urllib', 'http', [\n             ('http', False),\n             ('https', UnsupportedRequest),\n             ('socks4', False),\n@@ -886,8 +946,19 @@ class HTTPSupportedRH(ValidationRH):\n             ('socks5h', False),\n             ('socks', UnsupportedRequest),\n         ]),\n-        (NoCheckRH, [('http', False)]),\n-        (HTTPSupportedRH, [('http', UnsupportedRequest)]),\n+        ('Requests', 'http', [\n+            ('http', False),\n+            ('https', False),\n+            ('socks4', False),\n+            ('socks4a', False),\n+            ('socks5', False),\n+            ('socks5h', False),\n+        ]),\n+        (NoCheckRH, 'http', [('http', False)]),\n+        (HTTPSupportedRH, 'http', [('http', UnsupportedRequest)]),\n+        ('Websockets', 'ws', [('http', UnsupportedRequest)]),\n+        (NoCheckRH, 'http', [('http', False)]),\n+        (HTTPSupportedRH, 'http', [('http', UnsupportedRequest)]),\n     ]\n \n     PROXY_KEY_TESTS = [\n@@ -896,13 +967,17 @@ class HTTPSupportedRH(ValidationRH):\n             ('all', False),\n             ('unrelated', False),\n         ]),\n+        ('Requests', [\n+            ('all', False),\n+            ('unrelated', False),\n+        ]),\n         (NoCheckRH, [('all', False)]),\n         (HTTPSupportedRH, [('all', UnsupportedRequest)]),\n         (HTTPSupportedRH, [('no', UnsupportedRequest)]),\n     ]\n \n     EXTENSION_TESTS = [\n-        ('Urllib', [\n+        ('Urllib', 'http', [\n             ({'cookiejar': 'notacookiejar'}, AssertionError),\n             ({'cookiejar': YoutubeDLCookieJar()}, False),\n             ({'cookiejar': CookieJar()}, AssertionError),\n@@ -910,10 +985,21 @@ class HTTPSupportedRH(ValidationRH):\n             ({'timeout': 'notatimeout'}, AssertionError),\n             ({'unsupported': 'value'}, UnsupportedRequest),\n         ]),\n-        (NoCheckRH, [\n+        ('Requests', 'http', [\n+            ({'cookiejar': 'notacookiejar'}, AssertionError),\n+            ({'cookiejar': YoutubeDLCookieJar()}, False),\n+            ({'timeout': 1}, False),\n+            ({'timeout': 'notatimeout'}, AssertionError),\n+            ({'unsupported': 'value'}, UnsupportedRequest),\n+        ]),\n+        (NoCheckRH, 'http', [\n             ({'cookiejar': 'notacookiejar'}, False),\n             ({'somerandom': 'test'}, False),  # but any extension is allowed through\n         ]),\n+        ('Websockets', 'ws', [\n+            ({'cookiejar': YoutubeDLCookieJar()}, False),\n+            ({'timeout': 2}, False),\n+        ]),\n     ]\n \n     @pytest.mark.parametrize('handler,scheme,fail,handler_kwargs', [\n@@ -925,7 +1011,7 @@ class HTTPSupportedRH(ValidationRH):\n     def test_url_scheme(self, handler, scheme, fail, handler_kwargs):\n         run_validation(handler, fail, Request(f'{scheme}://'), **(handler_kwargs or {}))\n \n-    @pytest.mark.parametrize('handler,fail', [('Urllib', False)], indirect=['handler'])\n+    @pytest.mark.parametrize('handler,fail', [('Urllib', False), ('Requests', False)], indirect=['handler'])\n     def test_no_proxy(self, handler, fail):\n         run_validation(handler, fail, Request('http://', proxies={'no': '127.0.0.1,github.com'}))\n         run_validation(handler, fail, Request('http://'), proxies={'no': '127.0.0.1,github.com'})\n@@ -939,33 +1025,33 @@ def test_proxy_key(self, handler, proxy_key, fail):\n         run_validation(handler, fail, Request('http://', proxies={proxy_key: 'http://example.com'}))\n         run_validation(handler, fail, Request('http://'), proxies={proxy_key: 'http://example.com'})\n \n-    @pytest.mark.parametrize('handler,scheme,fail', [\n-        (handler_tests[0], scheme, fail)\n+    @pytest.mark.parametrize('handler,req_scheme,scheme,fail', [\n+        (handler_tests[0], handler_tests[1], scheme, fail)\n         for handler_tests in PROXY_SCHEME_TESTS\n-        for scheme, fail in handler_tests[1]\n+        for scheme, fail in handler_tests[2]\n     ], indirect=['handler'])\n-    def test_proxy_scheme(self, handler, scheme, fail):\n-        run_validation(handler, fail, Request('http://', proxies={'http': f'{scheme}://example.com'}))\n-        run_validation(handler, fail, Request('http://'), proxies={'http': f'{scheme}://example.com'})\n+    def test_proxy_scheme(self, handler, req_scheme, scheme, fail):\n+        run_validation(handler, fail, Request(f'{req_scheme}://', proxies={req_scheme: f'{scheme}://example.com'}))\n+        run_validation(handler, fail, Request(f'{req_scheme}://'), proxies={req_scheme: f'{scheme}://example.com'})\n \n-    @pytest.mark.parametrize('handler', ['Urllib', HTTPSupportedRH], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', HTTPSupportedRH, 'Requests'], indirect=True)\n     def test_empty_proxy(self, handler):\n         run_validation(handler, False, Request('http://', proxies={'http': None}))\n         run_validation(handler, False, Request('http://'), proxies={'http': None})\n \n     @pytest.mark.parametrize('proxy_url', ['//example.com', 'example.com', '127.0.0.1', '/a/b/c'])\n-    @pytest.mark.parametrize('handler', ['Urllib'], indirect=True)\n+    @pytest.mark.parametrize('handler', ['Urllib', 'Requests'], indirect=True)\n     def test_invalid_proxy_url(self, handler, proxy_url):\n         run_validation(handler, UnsupportedRequest, Request('http://', proxies={'http': proxy_url}))\n \n-    @pytest.mark.parametrize('handler,extensions,fail', [\n-        (handler_tests[0], extensions, fail)\n+    @pytest.mark.parametrize('handler,scheme,extensions,fail', [\n+        (handler_tests[0], handler_tests[1], extensions, fail)\n         for handler_tests in EXTENSION_TESTS\n-        for extensions, fail in handler_tests[1]\n+        for extensions, fail in handler_tests[2]\n     ], indirect=['handler'])\n-    def test_extension(self, handler, extensions, fail):\n+    def test_extension(self, handler, scheme, extensions, fail):\n         run_validation(\n-            handler, fail, Request('http://', extensions=extensions))\n+            handler, fail, Request(f'{scheme}://', extensions=extensions))\n \n     def test_invalid_request_type(self):\n         rh = self.ValidationRH(logger=FakeLogger())\n@@ -998,6 +1084,22 @@ def __init__(self, *args, **kwargs):\n         self._request_director = self.build_request_director([FakeRH])\n \n \n+class AllUnsupportedRHYDL(FakeYDL):\n+\n+    def __init__(self, *args, **kwargs):\n+\n+        class UnsupportedRH(RequestHandler):\n+            def _send(self, request: Request):\n+                pass\n+\n+            _SUPPORTED_FEATURES = ()\n+            _SUPPORTED_PROXY_SCHEMES = ()\n+            _SUPPORTED_URL_SCHEMES = ()\n+\n+        super().__init__(*args, **kwargs)\n+        self._request_director = self.build_request_director([UnsupportedRH])\n+\n+\n class TestRequestDirector:\n \n     def test_handler_operations(self):\n@@ -1157,6 +1259,12 @@ def test_file_urls_error(self):\n             with pytest.raises(RequestError, match=r'file:// URLs are disabled by default'):\n                 ydl.urlopen('file://')\n \n+    @pytest.mark.parametrize('scheme', (['ws', 'wss']))\n+    def test_websocket_unavailable_error(self, scheme):\n+        with AllUnsupportedRHYDL() as ydl:\n+            with pytest.raises(RequestError, match=r'This request requires WebSocket support'):\n+                ydl.urlopen(f'{scheme}://')\n+\n     def test_legacy_server_connect_error(self):\n         with FakeRHYDL() as ydl:\n             for error in ('UNSAFE_LEGACY_RENEGOTIATION_DISABLED', 'SSLV3_ALERT_HANDSHAKE_FAILURE'):\n@@ -1216,6 +1324,10 @@ def test_clean_header(self):\n             assert 'Youtubedl-no-compression' not in rh.headers\n             assert rh.headers.get('Accept-Encoding') == 'identity'\n \n+        with FakeYDL({'http_headers': {'Ytdl-socks-proxy': 'socks://localhost:1080'}}) as ydl:\n+            rh = self.build_handler(ydl)\n+            assert 'Ytdl-socks-proxy' not in rh.headers\n+\n     def test_build_handler_params(self):\n         with FakeYDL({\n             'http_headers': {'test': 'testtest'},\n@@ -1258,6 +1370,13 @@ def test_urllib_file_urls(self):\n             rh = self.build_handler(ydl, UrllibRH)\n             assert rh.enable_file_urls is True\n \n+    def test_compat_opt_prefer_urllib(self):\n+        # This assumes urllib only has a preference when this compat opt is given\n+        with FakeYDL({'compat_opts': ['prefer-legacy-http-handler']}) as ydl:\n+            director = ydl.build_request_director([UrllibRH])\n+            assert len(director.preferences) == 1\n+            assert director.preferences.pop()(UrllibRH, None)\n+\n \n class TestRequest:\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_networking_utils.py",
            "diff": "diff --git a/test/test_networking_utils.py b/test/test_networking_utils.py\nindex dbf65609..419aae1e 100644\n--- a/test/test_networking_utils.py\n+++ b/test/test_networking_utils.py\n@@ -269,14 +269,14 @@ def test_compat_http_error_autoclose(self):\n         assert not response.closed\n \n     def test_incomplete_read_error(self):\n-        error = IncompleteRead(b'test', 3, cause='test')\n+        error = IncompleteRead(4, 3, cause='test')\n         assert isinstance(error, IncompleteRead)\n         assert repr(error) == '<IncompleteRead: 4 bytes read, 3 more expected>'\n         assert str(error) == error.msg == '4 bytes read, 3 more expected'\n-        assert error.partial == b'test'\n+        assert error.partial == 4\n         assert error.expected == 3\n         assert error.cause == 'test'\n \n-        error = IncompleteRead(b'aaa')\n+        error = IncompleteRead(3)\n         assert repr(error) == '<IncompleteRead: 3 bytes read>'\n         assert str(error) == '3 bytes read'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_socks.py",
            "diff": "diff --git a/test/test_socks.py b/test/test_socks.py\nindex 6651290d..71f783e1 100644\n--- a/test/test_socks.py\n+++ b/test/test_socks.py\n@@ -1,113 +1,474 @@\n #!/usr/bin/env python3\n-\n # Allow direct execution\n import os\n import sys\n+import threading\n import unittest\n \n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+import pytest\n \n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n+import abc\n+import contextlib\n+import enum\n+import functools\n+import http.server\n+import json\n import random\n-import subprocess\n-import urllib.request\n+import socket\n+import struct\n+import time\n+from socketserver import (\n+    BaseRequestHandler,\n+    StreamRequestHandler,\n+    ThreadingTCPServer,\n+)\n \n-from test.helper import FakeYDL, get_params, is_download_test\n+from test.helper import http_server_port\n+from yt_dlp.networking import Request\n+from yt_dlp.networking.exceptions import ProxyError, TransportError\n+from yt_dlp.socks import (\n+    SOCKS4_REPLY_VERSION,\n+    SOCKS4_VERSION,\n+    SOCKS5_USER_AUTH_SUCCESS,\n+    SOCKS5_USER_AUTH_VERSION,\n+    SOCKS5_VERSION,\n+    Socks5AddressType,\n+    Socks5Auth,\n+)\n \n+SOCKS5_USER_AUTH_FAILURE = 0x1\n \n-@is_download_test\n-class TestMultipleSocks(unittest.TestCase):\n-    @staticmethod\n-    def _check_params(attrs):\n-        params = get_params()\n-        for attr in attrs:\n-            if attr not in params:\n-                print('Missing %s. Skipping.' % attr)\n-                return\n-        return params\n \n-    def test_proxy_http(self):\n-        params = self._check_params(['primary_proxy', 'primary_server_ip'])\n-        if params is None:\n+class Socks4CD(enum.IntEnum):\n+    REQUEST_GRANTED = 90\n+    REQUEST_REJECTED_OR_FAILED = 91\n+    REQUEST_REJECTED_CANNOT_CONNECT_TO_IDENTD = 92\n+    REQUEST_REJECTED_DIFFERENT_USERID = 93\n+\n+\n+class Socks5Reply(enum.IntEnum):\n+    SUCCEEDED = 0x0\n+    GENERAL_FAILURE = 0x1\n+    CONNECTION_NOT_ALLOWED = 0x2\n+    NETWORK_UNREACHABLE = 0x3\n+    HOST_UNREACHABLE = 0x4\n+    CONNECTION_REFUSED = 0x5\n+    TTL_EXPIRED = 0x6\n+    COMMAND_NOT_SUPPORTED = 0x7\n+    ADDRESS_TYPE_NOT_SUPPORTED = 0x8\n+\n+\n+class SocksTestRequestHandler(BaseRequestHandler):\n+\n+    def __init__(self, *args, socks_info=None, **kwargs):\n+        self.socks_info = socks_info\n+        super().__init__(*args, **kwargs)\n+\n+\n+class SocksProxyHandler(BaseRequestHandler):\n+    def __init__(self, request_handler_class, socks_server_kwargs, *args, **kwargs):\n+        self.socks_kwargs = socks_server_kwargs or {}\n+        self.request_handler_class = request_handler_class\n+        super().__init__(*args, **kwargs)\n+\n+\n+class Socks5ProxyHandler(StreamRequestHandler, SocksProxyHandler):\n+\n+    # SOCKS5 protocol https://tools.ietf.org/html/rfc1928\n+    # SOCKS5 username/password authentication https://tools.ietf.org/html/rfc1929\n+\n+    def handle(self):\n+        sleep = self.socks_kwargs.get('sleep')\n+        if sleep:\n+            time.sleep(sleep)\n+        version, nmethods = self.connection.recv(2)\n+        assert version == SOCKS5_VERSION\n+        methods = list(self.connection.recv(nmethods))\n+\n+        auth = self.socks_kwargs.get('auth')\n+\n+        if auth is not None and Socks5Auth.AUTH_USER_PASS not in methods:\n+            self.connection.sendall(struct.pack('!BB', SOCKS5_VERSION, Socks5Auth.AUTH_NO_ACCEPTABLE))\n+            self.server.close_request(self.request)\n             return\n-        ydl = FakeYDL({\n-            'proxy': params['primary_proxy']\n-        })\n-        self.assertEqual(\n-            ydl.urlopen('http://yt-dl.org/ip').read().decode(),\n-            params['primary_server_ip'])\n-\n-    def test_proxy_https(self):\n-        params = self._check_params(['primary_proxy', 'primary_server_ip'])\n-        if params is None:\n+\n+        elif Socks5Auth.AUTH_USER_PASS in methods:\n+            self.connection.sendall(struct.pack(\"!BB\", SOCKS5_VERSION, Socks5Auth.AUTH_USER_PASS))\n+\n+            _, user_len = struct.unpack('!BB', self.connection.recv(2))\n+            username = self.connection.recv(user_len).decode()\n+            pass_len = ord(self.connection.recv(1))\n+            password = self.connection.recv(pass_len).decode()\n+\n+            if username == auth[0] and password == auth[1]:\n+                self.connection.sendall(struct.pack('!BB', SOCKS5_USER_AUTH_VERSION, SOCKS5_USER_AUTH_SUCCESS))\n+            else:\n+                self.connection.sendall(struct.pack('!BB', SOCKS5_USER_AUTH_VERSION, SOCKS5_USER_AUTH_FAILURE))\n+                self.server.close_request(self.request)\n+                return\n+\n+        elif Socks5Auth.AUTH_NONE in methods:\n+            self.connection.sendall(struct.pack('!BB', SOCKS5_VERSION, Socks5Auth.AUTH_NONE))\n+        else:\n+            self.connection.sendall(struct.pack('!BB', SOCKS5_VERSION, Socks5Auth.AUTH_NO_ACCEPTABLE))\n+            self.server.close_request(self.request)\n             return\n-        ydl = FakeYDL({\n-            'proxy': params['primary_proxy']\n-        })\n-        self.assertEqual(\n-            ydl.urlopen('https://yt-dl.org/ip').read().decode(),\n-            params['primary_server_ip'])\n-\n-    def test_secondary_proxy_http(self):\n-        params = self._check_params(['secondary_proxy', 'secondary_server_ip'])\n-        if params is None:\n+\n+        version, command, _, address_type = struct.unpack('!BBBB', self.connection.recv(4))\n+        socks_info = {\n+            'version': version,\n+            'auth_methods': methods,\n+            'command': command,\n+            'client_address': self.client_address,\n+            'ipv4_address': None,\n+            'domain_address': None,\n+            'ipv6_address': None,\n+        }\n+        if address_type == Socks5AddressType.ATYP_IPV4:\n+            socks_info['ipv4_address'] = socket.inet_ntoa(self.connection.recv(4))\n+        elif address_type == Socks5AddressType.ATYP_DOMAINNAME:\n+            socks_info['domain_address'] = self.connection.recv(ord(self.connection.recv(1))).decode()\n+        elif address_type == Socks5AddressType.ATYP_IPV6:\n+            socks_info['ipv6_address'] = socket.inet_ntop(socket.AF_INET6, self.connection.recv(16))\n+        else:\n+            self.server.close_request(self.request)\n+\n+        socks_info['port'] = struct.unpack('!H', self.connection.recv(2))[0]\n+\n+        # dummy response, the returned IP is just a placeholder\n+        self.connection.sendall(struct.pack(\n+            '!BBBBIH', SOCKS5_VERSION, self.socks_kwargs.get('reply', Socks5Reply.SUCCEEDED), 0x0, 0x1, 0x7f000001, 40000))\n+\n+        self.request_handler_class(self.request, self.client_address, self.server, socks_info=socks_info)\n+\n+\n+class Socks4ProxyHandler(StreamRequestHandler, SocksProxyHandler):\n+\n+    # SOCKS4 protocol http://www.openssh.com/txt/socks4.protocol\n+    # SOCKS4A protocol http://www.openssh.com/txt/socks4a.protocol\n+\n+    def _read_until_null(self):\n+        return b''.join(iter(functools.partial(self.connection.recv, 1), b'\\x00'))\n+\n+    def handle(self):\n+        sleep = self.socks_kwargs.get('sleep')\n+        if sleep:\n+            time.sleep(sleep)\n+        socks_info = {\n+            'version': SOCKS4_VERSION,\n+            'command': None,\n+            'client_address': self.client_address,\n+            'ipv4_address': None,\n+            'port': None,\n+            'domain_address': None,\n+        }\n+        version, command, dest_port, dest_ip = struct.unpack('!BBHI', self.connection.recv(8))\n+        socks_info['port'] = dest_port\n+        socks_info['command'] = command\n+        if version != SOCKS4_VERSION:\n+            self.server.close_request(self.request)\n             return\n-        ydl = FakeYDL()\n-        req = urllib.request.Request('http://yt-dl.org/ip')\n-        req.add_header('Ytdl-request-proxy', params['secondary_proxy'])\n-        self.assertEqual(\n-            ydl.urlopen(req).read().decode(),\n-            params['secondary_server_ip'])\n-\n-    def test_secondary_proxy_https(self):\n-        params = self._check_params(['secondary_proxy', 'secondary_server_ip'])\n-        if params is None:\n+        use_remote_dns = False\n+        if 0x0 < dest_ip <= 0xFF:\n+            use_remote_dns = True\n+        else:\n+            socks_info['ipv4_address'] = socket.inet_ntoa(struct.pack(\"!I\", dest_ip))\n+\n+        user_id = self._read_until_null().decode()\n+        if user_id != (self.socks_kwargs.get('user_id') or ''):\n+            self.connection.sendall(struct.pack(\n+                '!BBHI', SOCKS4_REPLY_VERSION, Socks4CD.REQUEST_REJECTED_DIFFERENT_USERID, 0x00, 0x00000000))\n+            self.server.close_request(self.request)\n             return\n-        ydl = FakeYDL()\n-        req = urllib.request.Request('https://yt-dl.org/ip')\n-        req.add_header('Ytdl-request-proxy', params['secondary_proxy'])\n-        self.assertEqual(\n-            ydl.urlopen(req).read().decode(),\n-            params['secondary_server_ip'])\n \n+        if use_remote_dns:\n+            socks_info['domain_address'] = self._read_until_null().decode()\n \n-@is_download_test\n-class TestSocks(unittest.TestCase):\n-    _SKIP_SOCKS_TEST = True\n+        # dummy response, the returned IP is just a placeholder\n+        self.connection.sendall(\n+            struct.pack(\n+                '!BBHI', SOCKS4_REPLY_VERSION,\n+                self.socks_kwargs.get('cd_reply', Socks4CD.REQUEST_GRANTED), 40000, 0x7f000001))\n \n-    def setUp(self):\n-        if self._SKIP_SOCKS_TEST:\n-            return\n+        self.request_handler_class(self.request, self.client_address, self.server, socks_info=socks_info)\n \n-        self.port = random.randint(20000, 30000)\n-        self.server_process = subprocess.Popen([\n-            'srelay', '-f', '-i', '127.0.0.1:%d' % self.port],\n-            stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n \n-    def tearDown(self):\n-        if self._SKIP_SOCKS_TEST:\n-            return\n+class IPv6ThreadingTCPServer(ThreadingTCPServer):\n+    address_family = socket.AF_INET6\n+\n+\n+class SocksHTTPTestRequestHandler(http.server.BaseHTTPRequestHandler, SocksTestRequestHandler):\n+    def do_GET(self):\n+        if self.path == '/socks_info':\n+            payload = json.dumps(self.socks_info.copy())\n+            self.send_response(200)\n+            self.send_header('Content-Type', 'application/json; charset=utf-8')\n+            self.send_header('Content-Length', str(len(payload)))\n+            self.end_headers()\n+            self.wfile.write(payload.encode())\n+\n+\n+class SocksWebSocketTestRequestHandler(SocksTestRequestHandler):\n+    def handle(self):\n+        import websockets.sync.server\n+        protocol = websockets.ServerProtocol()\n+        connection = websockets.sync.server.ServerConnection(socket=self.request, protocol=protocol, close_timeout=0)\n+        connection.handshake()\n+        connection.send(json.dumps(self.socks_info))\n+        connection.close()\n+\n+\n+@contextlib.contextmanager\n+def socks_server(socks_server_class, request_handler, bind_ip=None, **socks_server_kwargs):\n+    server = server_thread = None\n+    try:\n+        bind_address = bind_ip or '127.0.0.1'\n+        server_type = ThreadingTCPServer if '.' in bind_address else IPv6ThreadingTCPServer\n+        server = server_type(\n+            (bind_address, 0), functools.partial(socks_server_class, request_handler, socks_server_kwargs))\n+        server_port = http_server_port(server)\n+        server_thread = threading.Thread(target=server.serve_forever)\n+        server_thread.daemon = True\n+        server_thread.start()\n+        if '.' not in bind_address:\n+            yield f'[{bind_address}]:{server_port}'\n+        else:\n+            yield f'{bind_address}:{server_port}'\n+    finally:\n+        server.shutdown()\n+        server.server_close()\n+        server_thread.join(2.0)\n+\n+\n+class SocksProxyTestContext(abc.ABC):\n+    REQUEST_HANDLER_CLASS = None\n+\n+    def socks_server(self, server_class, *args, **kwargs):\n+        return socks_server(server_class, self.REQUEST_HANDLER_CLASS, *args, **kwargs)\n+\n+    @abc.abstractmethod\n+    def socks_info_request(self, handler, target_domain=None, target_port=None, **req_kwargs) -> dict:\n+        \"\"\"return a dict of socks_info\"\"\"\n+\n+\n+class HTTPSocksTestProxyContext(SocksProxyTestContext):\n+    REQUEST_HANDLER_CLASS = SocksHTTPTestRequestHandler\n+\n+    def socks_info_request(self, handler, target_domain=None, target_port=None, **req_kwargs):\n+        request = Request(f'http://{target_domain or \"127.0.0.1\"}:{target_port or \"40000\"}/socks_info', **req_kwargs)\n+        handler.validate(request)\n+        return json.loads(handler.send(request).read().decode())\n+\n+\n+class WebSocketSocksTestProxyContext(SocksProxyTestContext):\n+    REQUEST_HANDLER_CLASS = SocksWebSocketTestRequestHandler\n+\n+    def socks_info_request(self, handler, target_domain=None, target_port=None, **req_kwargs):\n+        request = Request(f'ws://{target_domain or \"127.0.0.1\"}:{target_port or \"40000\"}', **req_kwargs)\n+        handler.validate(request)\n+        ws = handler.send(request)\n+        ws.send('socks_info')\n+        socks_info = ws.recv()\n+        ws.close()\n+        return json.loads(socks_info)\n+\n+\n+CTX_MAP = {\n+    'http': HTTPSocksTestProxyContext,\n+    'ws': WebSocketSocksTestProxyContext,\n+}\n+\n+\n+@pytest.fixture(scope='module')\n+def ctx(request):\n+    return CTX_MAP[request.param]()\n+\n+\n+class TestSocks4Proxy:\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks4_no_auth(self, handler, ctx):\n+        with handler() as rh:\n+            with ctx.socks_server(Socks4ProxyHandler) as server_address:\n+                response = ctx.socks_info_request(\n+                    rh, proxies={'all': f'socks4://{server_address}'})\n+                assert response['version'] == 4\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks4_auth(self, handler, ctx):\n+        with handler() as rh:\n+            with ctx.socks_server(Socks4ProxyHandler, user_id='user') as server_address:\n+                with pytest.raises(ProxyError):\n+                    ctx.socks_info_request(rh, proxies={'all': f'socks4://{server_address}'})\n+                response = ctx.socks_info_request(\n+                    rh, proxies={'all': f'socks4://user:@{server_address}'})\n+                assert response['version'] == 4\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks4a_ipv4_target(self, handler, ctx):\n+        with ctx.socks_server(Socks4ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks4a://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='127.0.0.1')\n+                assert response['version'] == 4\n+                assert (response['ipv4_address'] == '127.0.0.1') != (response['domain_address'] == '127.0.0.1')\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks4a_domain_target(self, handler, ctx):\n+        with ctx.socks_server(Socks4ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks4a://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='localhost')\n+                assert response['version'] == 4\n+                assert response['ipv4_address'] is None\n+                assert response['domain_address'] == 'localhost'\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_ipv4_client_source_address(self, handler, ctx):\n+        with ctx.socks_server(Socks4ProxyHandler) as server_address:\n+            source_address = f'127.0.0.{random.randint(5, 255)}'\n+            with handler(proxies={'all': f'socks4://{server_address}'},\n+                         source_address=source_address) as rh:\n+                response = ctx.socks_info_request(rh)\n+                assert response['client_address'][0] == source_address\n+                assert response['version'] == 4\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    @pytest.mark.parametrize('reply_code', [\n+        Socks4CD.REQUEST_REJECTED_OR_FAILED,\n+        Socks4CD.REQUEST_REJECTED_CANNOT_CONNECT_TO_IDENTD,\n+        Socks4CD.REQUEST_REJECTED_DIFFERENT_USERID,\n+    ])\n+    def test_socks4_errors(self, handler, ctx, reply_code):\n+        with ctx.socks_server(Socks4ProxyHandler, cd_reply=reply_code) as server_address:\n+            with handler(proxies={'all': f'socks4://{server_address}'}) as rh:\n+                with pytest.raises(ProxyError):\n+                    ctx.socks_info_request(rh)\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_ipv6_socks4_proxy(self, handler, ctx):\n+        with ctx.socks_server(Socks4ProxyHandler, bind_ip='::1') as server_address:\n+            with handler(proxies={'all': f'socks4://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='127.0.0.1')\n+                assert response['client_address'][0] == '::1'\n+                assert response['ipv4_address'] == '127.0.0.1'\n+                assert response['version'] == 4\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_timeout(self, handler, ctx):\n+        with ctx.socks_server(Socks4ProxyHandler, sleep=2) as server_address:\n+            with handler(proxies={'all': f'socks4://{server_address}'}, timeout=0.5) as rh:\n+                with pytest.raises(TransportError):\n+                    ctx.socks_info_request(rh)\n+\n+\n+class TestSocks5Proxy:\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5_no_auth(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh)\n+                assert response['auth_methods'] == [0x0]\n+                assert response['version'] == 5\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5_user_pass(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler, auth=('test', 'testpass')) as server_address:\n+            with handler() as rh:\n+                with pytest.raises(ProxyError):\n+                    ctx.socks_info_request(rh, proxies={'all': f'socks5://{server_address}'})\n+\n+                response = ctx.socks_info_request(\n+                    rh, proxies={'all': f'socks5://test:testpass@{server_address}'})\n+\n+                assert response['auth_methods'] == [Socks5Auth.AUTH_NONE, Socks5Auth.AUTH_USER_PASS]\n+                assert response['version'] == 5\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5_ipv4_target(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='127.0.0.1')\n+                assert response['ipv4_address'] == '127.0.0.1'\n+                assert response['version'] == 5\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5_domain_target(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='localhost')\n+                assert (response['ipv4_address'] == '127.0.0.1') != (response['ipv6_address'] == '::1')\n+                assert response['version'] == 5\n+\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5h_domain_target(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks5h://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='localhost')\n+                assert response['ipv4_address'] is None\n+                assert response['domain_address'] == 'localhost'\n+                assert response['version'] == 5\n \n-        self.server_process.terminate()\n-        self.server_process.communicate()\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5h_ip_target(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks5h://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='127.0.0.1')\n+                assert response['ipv4_address'] == '127.0.0.1'\n+                assert response['domain_address'] is None\n+                assert response['version'] == 5\n \n-    def _get_ip(self, protocol):\n-        if self._SKIP_SOCKS_TEST:\n-            return '127.0.0.1'\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_socks5_ipv6_destination(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='[::1]')\n+                assert response['ipv6_address'] == '::1'\n+                assert response['version'] == 5\n \n-        ydl = FakeYDL({\n-            'proxy': '%s://127.0.0.1:%d' % (protocol, self.port),\n-        })\n-        return ydl.urlopen('http://yt-dl.org/ip').read().decode()\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_ipv6_socks5_proxy(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler, bind_ip='::1') as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}) as rh:\n+                response = ctx.socks_info_request(rh, target_domain='127.0.0.1')\n+                assert response['client_address'][0] == '::1'\n+                assert response['ipv4_address'] == '127.0.0.1'\n+                assert response['version'] == 5\n \n-    def test_socks4(self):\n-        self.assertTrue(isinstance(self._get_ip('socks4'), str))\n+    # XXX: is there any feasible way of testing IPv6 source addresses?\n+    # Same would go for non-proxy source_address test...\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_ipv4_client_source_address(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler) as server_address:\n+            source_address = f'127.0.0.{random.randint(5, 255)}'\n+            with handler(proxies={'all': f'socks5://{server_address}'}, source_address=source_address) as rh:\n+                response = ctx.socks_info_request(rh)\n+                assert response['client_address'][0] == source_address\n+                assert response['version'] == 5\n \n-    def test_socks4a(self):\n-        self.assertTrue(isinstance(self._get_ip('socks4a'), str))\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Requests', 'http'), ('Websockets', 'ws')], indirect=True)\n+    @pytest.mark.parametrize('reply_code', [\n+        Socks5Reply.GENERAL_FAILURE,\n+        Socks5Reply.CONNECTION_NOT_ALLOWED,\n+        Socks5Reply.NETWORK_UNREACHABLE,\n+        Socks5Reply.HOST_UNREACHABLE,\n+        Socks5Reply.CONNECTION_REFUSED,\n+        Socks5Reply.TTL_EXPIRED,\n+        Socks5Reply.COMMAND_NOT_SUPPORTED,\n+        Socks5Reply.ADDRESS_TYPE_NOT_SUPPORTED,\n+    ])\n+    def test_socks5_errors(self, handler, ctx, reply_code):\n+        with ctx.socks_server(Socks5ProxyHandler, reply=reply_code) as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}) as rh:\n+                with pytest.raises(ProxyError):\n+                    ctx.socks_info_request(rh)\n \n-    def test_socks5(self):\n-        self.assertTrue(isinstance(self._get_ip('socks5'), str))\n+    @pytest.mark.parametrize('handler,ctx', [('Urllib', 'http'), ('Websockets', 'ws')], indirect=True)\n+    def test_timeout(self, handler, ctx):\n+        with ctx.socks_server(Socks5ProxyHandler, sleep=2) as server_address:\n+            with handler(proxies={'all': f'socks5://{server_address}'}, timeout=1) as rh:\n+                with pytest.raises(TransportError):\n+                    ctx.socks_info_request(rh)\n \n \n if __name__ == '__main__':\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_update.py",
            "diff": "diff --git a/test/test_update.py b/test/test_update.py\nnew file mode 100644\nindex 00000000..bc139562\n--- /dev/null\n+++ b/test/test_update.py\n@@ -0,0 +1,228 @@\n+#!/usr/bin/env python3\n+\n+# Allow direct execution\n+import os\n+import sys\n+import unittest\n+\n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+\n+\n+from test.helper import FakeYDL, report_warning\n+from yt_dlp.update import UpdateInfo, Updater\n+\n+\n+# XXX: Keep in sync with yt_dlp.update.UPDATE_SOURCES\n+TEST_UPDATE_SOURCES = {\n+    'stable': 'yt-dlp/yt-dlp',\n+    'nightly': 'yt-dlp/yt-dlp-nightly-builds',\n+    'master': 'yt-dlp/yt-dlp-master-builds',\n+}\n+\n+TEST_API_DATA = {\n+    'yt-dlp/yt-dlp/latest': {\n+        'tag_name': '2023.12.31',\n+        'target_commitish': 'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',\n+        'name': 'yt-dlp 2023.12.31',\n+        'body': 'BODY',\n+    },\n+    'yt-dlp/yt-dlp-nightly-builds/latest': {\n+        'tag_name': '2023.12.31.123456',\n+        'target_commitish': 'master',\n+        'name': 'yt-dlp nightly 2023.12.31.123456',\n+        'body': 'Generated from: https://github.com/yt-dlp/yt-dlp/commit/cccccccccccccccccccccccccccccccccccccccc',\n+    },\n+    'yt-dlp/yt-dlp-master-builds/latest': {\n+        'tag_name': '2023.12.31.987654',\n+        'target_commitish': 'master',\n+        'name': 'yt-dlp master 2023.12.31.987654',\n+        'body': 'Generated from: https://github.com/yt-dlp/yt-dlp/commit/dddddddddddddddddddddddddddddddddddddddd',\n+    },\n+    'yt-dlp/yt-dlp/tags/testing': {\n+        'tag_name': 'testing',\n+        'target_commitish': '9999999999999999999999999999999999999999',\n+        'name': 'testing',\n+        'body': 'BODY',\n+    },\n+    'fork/yt-dlp/latest': {\n+        'tag_name': '2050.12.31',\n+        'target_commitish': 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee',\n+        'name': '2050.12.31',\n+        'body': 'BODY',\n+    },\n+    'fork/yt-dlp/tags/pr0000': {\n+        'tag_name': 'pr0000',\n+        'target_commitish': 'ffffffffffffffffffffffffffffffffffffffff',\n+        'name': 'pr1234 2023.11.11.000000',\n+        'body': 'BODY',\n+    },\n+    'fork/yt-dlp/tags/pr1234': {\n+        'tag_name': 'pr1234',\n+        'target_commitish': '0000000000000000000000000000000000000000',\n+        'name': 'pr1234 2023.12.31.555555',\n+        'body': 'BODY',\n+    },\n+    'fork/yt-dlp/tags/pr9999': {\n+        'tag_name': 'pr9999',\n+        'target_commitish': '1111111111111111111111111111111111111111',\n+        'name': 'pr9999',\n+        'body': 'BODY',\n+    },\n+    'fork/yt-dlp-satellite/tags/pr987': {\n+        'tag_name': 'pr987',\n+        'target_commitish': 'master',\n+        'name': 'pr987',\n+        'body': 'Generated from: https://github.com/yt-dlp/yt-dlp/commit/2222222222222222222222222222222222222222',\n+    },\n+}\n+\n+TEST_LOCKFILE_COMMENT = '# This file is used for regulating self-update'\n+\n+TEST_LOCKFILE_V1 = r'''%s\n+lock 2022.08.18.36 .+ Python 3\\.6\n+lock 2023.11.16 (?!win_x86_exe).+ Python 3\\.7\n+lock 2023.11.16 win_x86_exe .+ Windows-(?:Vista|2008Server)\n+''' % TEST_LOCKFILE_COMMENT\n+\n+TEST_LOCKFILE_V2_TMPL = r'''%s\n+lockV2 yt-dlp/yt-dlp 2022.08.18.36 .+ Python 3\\.6\n+lockV2 yt-dlp/yt-dlp 2023.11.16 (?!win_x86_exe).+ Python 3\\.7\n+lockV2 yt-dlp/yt-dlp 2023.11.16 win_x86_exe .+ Windows-(?:Vista|2008Server)\n+lockV2 yt-dlp/yt-dlp-nightly-builds 2023.11.15.232826 (?!win_x86_exe).+ Python 3\\.7\n+lockV2 yt-dlp/yt-dlp-nightly-builds 2023.11.15.232826 win_x86_exe .+ Windows-(?:Vista|2008Server)\n+lockV2 yt-dlp/yt-dlp-master-builds 2023.11.15.232812 (?!win_x86_exe).+ Python 3\\.7\n+lockV2 yt-dlp/yt-dlp-master-builds 2023.11.15.232812 win_x86_exe .+ Windows-(?:Vista|2008Server)\n+'''\n+\n+TEST_LOCKFILE_V2 = TEST_LOCKFILE_V2_TMPL % TEST_LOCKFILE_COMMENT\n+\n+TEST_LOCKFILE_ACTUAL = TEST_LOCKFILE_V2_TMPL % TEST_LOCKFILE_V1.rstrip('\\n')\n+\n+TEST_LOCKFILE_FORK = r'''%s# Test if a fork blocks updates to non-numeric tags\n+lockV2 fork/yt-dlp pr0000 .+ Python 3.6\n+lockV2 fork/yt-dlp pr1234 (?!win_x86_exe).+ Python 3\\.7\n+lockV2 fork/yt-dlp pr1234 win_x86_exe .+ Windows-(?:Vista|2008Server)\n+lockV2 fork/yt-dlp pr9999 .+ Python 3.11\n+''' % TEST_LOCKFILE_ACTUAL\n+\n+\n+class FakeUpdater(Updater):\n+    current_version = '2022.01.01'\n+    current_commit = 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\n+\n+    _channel = 'stable'\n+    _origin = 'yt-dlp/yt-dlp'\n+    _update_sources = TEST_UPDATE_SOURCES\n+\n+    def _download_update_spec(self, *args, **kwargs):\n+        return TEST_LOCKFILE_ACTUAL\n+\n+    def _call_api(self, tag):\n+        tag = f'tags/{tag}' if tag != 'latest' else tag\n+        return TEST_API_DATA[f'{self.requested_repo}/{tag}']\n+\n+    def _report_error(self, msg, *args, **kwargs):\n+        report_warning(msg)\n+\n+\n+class TestUpdate(unittest.TestCase):\n+    maxDiff = None\n+\n+    def test_update_spec(self):\n+        ydl = FakeYDL()\n+        updater = FakeUpdater(ydl, 'stable')\n+\n+        def test(lockfile, identifier, input_tag, expect_tag, exact=False, repo='yt-dlp/yt-dlp'):\n+            updater._identifier = identifier\n+            updater._exact = exact\n+            updater.requested_repo = repo\n+            result = updater._process_update_spec(lockfile, input_tag)\n+            self.assertEqual(\n+                result, expect_tag,\n+                f'{identifier!r} requesting {repo}@{input_tag} (exact={exact}) '\n+                f'returned {result!r} instead of {expect_tag!r}')\n+\n+        for lockfile in (TEST_LOCKFILE_V1, TEST_LOCKFILE_V2, TEST_LOCKFILE_ACTUAL, TEST_LOCKFILE_FORK):\n+            # Normal operation\n+            test(lockfile, 'zip Python 3.12.0', '2023.12.31', '2023.12.31')\n+            test(lockfile, 'zip stable Python 3.12.0', '2023.12.31', '2023.12.31', exact=True)\n+            # Python 3.6 --update should update only to its lock\n+            test(lockfile, 'zip Python 3.6.0', '2023.11.16', '2022.08.18.36')\n+            # --update-to an exact version later than the lock should return None\n+            test(lockfile, 'zip stable Python 3.6.0', '2023.11.16', None, exact=True)\n+            # Python 3.7 should be able to update to its lock\n+            test(lockfile, 'zip Python 3.7.0', '2023.11.16', '2023.11.16')\n+            test(lockfile, 'zip stable Python 3.7.1', '2023.11.16', '2023.11.16', exact=True)\n+            # Non-win_x86_exe builds on py3.7 must be locked\n+            test(lockfile, 'zip Python 3.7.1', '2023.12.31', '2023.11.16')\n+            test(lockfile, 'zip stable Python 3.7.1', '2023.12.31', None, exact=True)\n+            test(  # Windows Vista w/ win_x86_exe must be locked\n+                lockfile, 'win_x86_exe stable Python 3.7.9 (CPython x86 32bit) - Windows-Vista-6.0.6003-SP2',\n+                '2023.12.31', '2023.11.16')\n+            test(  # Windows 2008Server w/ win_x86_exe must be locked\n+                lockfile, 'win_x86_exe Python 3.7.9 (CPython x86 32bit) - Windows-2008Server',\n+                '2023.12.31', None, exact=True)\n+            test(  # Windows 7 w/ win_x86_exe py3.7 build should be able to update beyond lock\n+                lockfile, 'win_x86_exe stable Python 3.7.9 (CPython x86 32bit) - Windows-7-6.1.7601-SP1',\n+                '2023.12.31', '2023.12.31')\n+            test(  # Windows 8.1 w/ '2008Server' in platform string should be able to update beyond lock\n+                lockfile, 'win_x86_exe Python 3.7.9 (CPython x86 32bit) - Windows-post2008Server-6.2.9200',\n+                '2023.12.31', '2023.12.31', exact=True)\n+\n+        # Forks can block updates to non-numeric tags rather than lock\n+        test(TEST_LOCKFILE_FORK, 'zip Python 3.6.3', 'pr0000', None, repo='fork/yt-dlp')\n+        test(TEST_LOCKFILE_FORK, 'zip stable Python 3.7.4', 'pr0000', 'pr0000', repo='fork/yt-dlp')\n+        test(TEST_LOCKFILE_FORK, 'zip stable Python 3.7.4', 'pr1234', None, repo='fork/yt-dlp')\n+        test(TEST_LOCKFILE_FORK, 'zip Python 3.8.1', 'pr1234', 'pr1234', repo='fork/yt-dlp', exact=True)\n+        test(\n+            TEST_LOCKFILE_FORK, 'win_x86_exe stable Python 3.7.9 (CPython x86 32bit) - Windows-Vista-6.0.6003-SP2',\n+            'pr1234', None, repo='fork/yt-dlp')\n+        test(\n+            TEST_LOCKFILE_FORK, 'win_x86_exe stable Python 3.7.9 (CPython x86 32bit) - Windows-7-6.1.7601-SP1',\n+            '2023.12.31', '2023.12.31', repo='fork/yt-dlp')\n+        test(TEST_LOCKFILE_FORK, 'zip Python 3.11.2', 'pr9999', None, repo='fork/yt-dlp', exact=True)\n+        test(TEST_LOCKFILE_FORK, 'zip stable Python 3.12.0', 'pr9999', 'pr9999', repo='fork/yt-dlp')\n+\n+    def test_query_update(self):\n+        ydl = FakeYDL()\n+\n+        def test(target, expected, current_version=None, current_commit=None, identifier=None):\n+            updater = FakeUpdater(ydl, target)\n+            if current_version:\n+                updater.current_version = current_version\n+            if current_commit:\n+                updater.current_commit = current_commit\n+            updater._identifier = identifier or 'zip'\n+            update_info = updater.query_update(_output=True)\n+            self.assertDictEqual(\n+                update_info.__dict__ if update_info else {}, expected.__dict__ if expected else {})\n+\n+        test('yt-dlp/yt-dlp@latest', UpdateInfo(\n+            '2023.12.31', version='2023.12.31', requested_version='2023.12.31', commit='b' * 40))\n+        test('yt-dlp/yt-dlp-nightly-builds@latest', UpdateInfo(\n+            '2023.12.31.123456', version='2023.12.31.123456', requested_version='2023.12.31.123456', commit='c' * 40))\n+        test('yt-dlp/yt-dlp-master-builds@latest', UpdateInfo(\n+            '2023.12.31.987654', version='2023.12.31.987654', requested_version='2023.12.31.987654', commit='d' * 40))\n+        test('fork/yt-dlp@latest', UpdateInfo(\n+            '2050.12.31', version='2050.12.31', requested_version='2050.12.31', commit='e' * 40))\n+        test('fork/yt-dlp@pr0000', UpdateInfo(\n+            'pr0000', version='2023.11.11.000000', requested_version='2023.11.11.000000', commit='f' * 40))\n+        test('fork/yt-dlp@pr1234', UpdateInfo(\n+            'pr1234', version='2023.12.31.555555', requested_version='2023.12.31.555555', commit='0' * 40))\n+        test('fork/yt-dlp@pr9999', UpdateInfo(\n+            'pr9999', version=None, requested_version=None, commit='1' * 40))\n+        test('fork/yt-dlp-satellite@pr987', UpdateInfo(\n+            'pr987', version=None, requested_version=None, commit='2' * 40))\n+        test('yt-dlp/yt-dlp', None, current_version='2024.01.01')\n+        test('stable', UpdateInfo(\n+            '2023.12.31', version='2023.12.31', requested_version='2023.12.31', commit='b' * 40))\n+        test('nightly', UpdateInfo(\n+            '2023.12.31.123456', version='2023.12.31.123456', requested_version='2023.12.31.123456', commit='c' * 40))\n+        test('master', UpdateInfo(\n+            '2023.12.31.987654', version='2023.12.31.987654', requested_version='2023.12.31.987654', commit='d' * 40))\n+        test('testing', None, current_commit='9' * 40)\n+        test('testing', UpdateInfo('testing', commit='9' * 40))\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_update.py.disabled",
            "diff": "diff --git a/test/test_update.py.disabled b/test/test_update.py.disabled\ndeleted file mode 100644\nindex 85ac8669..00000000\n--- a/test/test_update.py.disabled\n+++ /dev/null\n@@ -1,30 +0,0 @@\n-#!/usr/bin/env python3\n-\n-# Allow direct execution\n-import os\n-import sys\n-import unittest\n-\n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n-\n-\n-import json\n-\n-from yt_dlp.update import rsa_verify\n-\n-\n-class TestUpdate(unittest.TestCase):\n-    def test_rsa_verify(self):\n-        UPDATES_RSA_KEY = (0x9d60ee4d8f805312fdb15a62f87b95bd66177b91df176765d13514a0f1754bcd2057295c5b6f1d35daa6742c3ffc9a82d3e118861c207995a8031e151d863c9927e304576bc80692bc8e094896fcf11b66f3e29e04e3a71e9a11558558acea1840aec37fc396fb6b65dc81a1c4144e03bd1c011de62e3f1357b327d08426fe93, 65537)\n-        with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'versions.json'), 'rb') as f:\n-            versions_info = f.read().decode()\n-        versions_info = json.loads(versions_info)\n-        signature = versions_info['signature']\n-        del versions_info['signature']\n-        self.assertTrue(rsa_verify(\n-            json.dumps(versions_info, sort_keys=True).encode(),\n-            signature, UPDATES_RSA_KEY))\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_utils.py",
            "diff": "diff --git a/test/test_utils.py b/test/test_utils.py\nindex 91e3ffd3..09c648cf 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -14,6 +14,7 @@\n import io\n import itertools\n import json\n+import subprocess\n import xml.etree.ElementTree\n \n from yt_dlp.compat import (\n@@ -28,6 +29,7 @@\n     InAdvancePagedList,\n     LazyList,\n     OnDemandPagedList,\n+    Popen,\n     age_restricted,\n     args_to_str,\n     base_url,\n@@ -1207,6 +1209,9 @@ def test_js_to_json_edgecases(self):\n         on = js_to_json('\\'\"\\\\\"\"\\'')\n         self.assertEqual(json.loads(on), '\"\"\"', msg='Unnecessary quote escape should be escaped')\n \n+        on = js_to_json('[new Date(\"spam\"), \\'(\"eggs\")\\']')\n+        self.assertEqual(json.loads(on), ['spam', '(\"eggs\")'], msg='Date regex should match a single string')\n+\n     def test_js_to_json_malformed(self):\n         self.assertEqual(js_to_json('42a1'), '42\"a1\"')\n         self.assertEqual(js_to_json('42a-1'), '42\"a\"-1')\n@@ -1218,6 +1223,14 @@ def test_js_to_json_template_literal(self):\n         self.assertEqual(js_to_json('`${name}\"${name}\"`', {'name': '5'}), '\"5\\\\\"5\\\\\"\"')\n         self.assertEqual(js_to_json('`${name}`', {}), '\"name\"')\n \n+    def test_js_to_json_common_constructors(self):\n+        self.assertEqual(json.loads(js_to_json('new Map([[\"a\", 5]])')), {'a': 5})\n+        self.assertEqual(json.loads(js_to_json('Array(5, 10)')), [5, 10])\n+        self.assertEqual(json.loads(js_to_json('new Array(15,5)')), [15, 5])\n+        self.assertEqual(json.loads(js_to_json('new Map([Array(5, 10),new Array(15,5)])')), {'5': 10, '15': 5})\n+        self.assertEqual(json.loads(js_to_json('new Date(\"123\")')), \"123\")\n+        self.assertEqual(json.loads(js_to_json('new Date(\\'2023-10-19\\')')), \"2023-10-19\")\n+\n     def test_extract_attributes(self):\n         self.assertEqual(extract_attributes('<e x=\"y\">'), {'x': 'y'})\n         self.assertEqual(extract_attributes(\"<e x='y'>\"), {'x': 'y'})\n@@ -2097,6 +2110,8 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj(_TEST_DATA, (..., {str_or_none})),\n                          [item for item in map(str_or_none, _TEST_DATA.values()) if item is not None],\n                          msg='Function in set should be a transformation')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('fail', {lambda _: 'const'})), 'const',\n+                         msg='Function in set should always be called')\n         if __debug__:\n             with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n                 traverse_obj(_TEST_DATA, set())\n@@ -2304,23 +2319,6 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj({}, (0, slice(1)), traverse_string=True), [],\n                          msg='branching should result in list if `traverse_string`')\n \n-        # Test is_user_input behavior\n-        _IS_USER_INPUT_DATA = {'range8': list(range(8))}\n-        self.assertEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', '3'),\n-                                      is_user_input=True), 3,\n-                         msg='allow for string indexing if `is_user_input`')\n-        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', '3:'),\n-                                           is_user_input=True), tuple(range(8))[3:],\n-                              msg='allow for string slice if `is_user_input`')\n-        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':4:2'),\n-                                           is_user_input=True), tuple(range(8))[:4:2],\n-                              msg='allow step in string slice if `is_user_input`')\n-        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':'),\n-                                           is_user_input=True), range(8),\n-                              msg='`:` should be treated as `...` if `is_user_input`')\n-        with self.assertRaises(TypeError, msg='too many params should result in error'):\n-            traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':::'), is_user_input=True)\n-\n         # Test re.Match as input obj\n         mobj = re.fullmatch(r'0(12)(?P<group>3)(4)?', '0123')\n         self.assertEqual(traverse_obj(mobj, ...), [x for x in mobj.groups() if x is not None],\n@@ -2342,6 +2340,58 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 'group')), ['0123', '3'],\n                          msg='function on a `re.Match` should give group name as well')\n \n+        # Test xml.etree.ElementTree.Element as input obj\n+        etree = xml.etree.ElementTree.fromstring('''<?xml version=\"1.0\"?>\n+        <data>\n+            <country name=\"Liechtenstein\">\n+                <rank>1</rank>\n+                <year>2008</year>\n+                <gdppc>141100</gdppc>\n+                <neighbor name=\"Austria\" direction=\"E\"/>\n+                <neighbor name=\"Switzerland\" direction=\"W\"/>\n+            </country>\n+            <country name=\"Singapore\">\n+                <rank>4</rank>\n+                <year>2011</year>\n+                <gdppc>59900</gdppc>\n+                <neighbor name=\"Malaysia\" direction=\"N\"/>\n+            </country>\n+            <country name=\"Panama\">\n+                <rank>68</rank>\n+                <year>2011</year>\n+                <gdppc>13600</gdppc>\n+                <neighbor name=\"Costa Rica\" direction=\"W\"/>\n+                <neighbor name=\"Colombia\" direction=\"E\"/>\n+            </country>\n+        </data>''')\n+        self.assertEqual(traverse_obj(etree, ''), etree,\n+                         msg='empty str key should return the element itself')\n+        self.assertEqual(traverse_obj(etree, 'country'), list(etree),\n+                         msg='str key should lead all children with that tag name')\n+        self.assertEqual(traverse_obj(etree, ...), list(etree),\n+                         msg='`...` as key should return all children')\n+        self.assertEqual(traverse_obj(etree, lambda _, x: x[0].text == '4'), [etree[1]],\n+                         msg='function as key should get element as value')\n+        self.assertEqual(traverse_obj(etree, lambda i, _: i == 1), [etree[1]],\n+                         msg='function as key should get index as key')\n+        self.assertEqual(traverse_obj(etree, 0), etree[0],\n+                         msg='int key should return the nth child')\n+        self.assertEqual(traverse_obj(etree, './/neighbor/@name'),\n+                         ['Austria', 'Switzerland', 'Malaysia', 'Costa Rica', 'Colombia'],\n+                         msg='`@<attribute>` at end of path should give that attribute')\n+        self.assertEqual(traverse_obj(etree, '//neighbor/@fail'), [None, None, None, None, None],\n+                         msg='`@<nonexistant>` at end of path should give `None`')\n+        self.assertEqual(traverse_obj(etree, ('//neighbor/@', 2)), {'name': 'Malaysia', 'direction': 'N'},\n+                         msg='`@` should give the full attribute dict')\n+        self.assertEqual(traverse_obj(etree, '//year/text()'), ['2008', '2011', '2011'],\n+                         msg='`text()` at end of path should give the inner text')\n+        self.assertEqual(traverse_obj(etree, '//*[@direction]/@direction'), ['E', 'W', 'N', 'W', 'E'],\n+                         msg='full python xpath features should be supported')\n+        self.assertEqual(traverse_obj(etree, (0, '@name')), 'Liechtenstein',\n+                         msg='special transformations should act on current element')\n+        self.assertEqual(traverse_obj(etree, ('country', 0, ..., 'text()', {int_or_none})), [1, 2008, 141100],\n+                         msg='special transformations should act on current element')\n+\n     def test_http_header_dict(self):\n         headers = HTTPHeaderDict()\n         headers['ytdl-test'] = b'0'\n@@ -2374,6 +2424,11 @@ def test_http_header_dict(self):\n         headers4 = HTTPHeaderDict({'ytdl-test': 'data;'})\n         self.assertEqual(set(headers4.items()), {('Ytdl-Test', 'data;')})\n \n+        # common mistake: strip whitespace from values\n+        # https://github.com/yt-dlp/yt-dlp/issues/8729\n+        headers5 = HTTPHeaderDict({'ytdl-test': ' data; '})\n+        self.assertEqual(set(headers5.items()), {('Ytdl-Test', 'data;')})\n+\n     def test_extract_basic_auth(self):\n         assert extract_basic_auth('http://:foo.bar') == ('http://:foo.bar', None)\n         assert extract_basic_auth('http://foo.bar') == ('http://foo.bar', None)\n@@ -2382,6 +2437,21 @@ def test_extract_basic_auth(self):\n         assert extract_basic_auth('http://user:@foo.bar') == ('http://foo.bar', 'Basic dXNlcjo=')\n         assert extract_basic_auth('http://user:pass@foo.bar') == ('http://foo.bar', 'Basic dXNlcjpwYXNz')\n \n+    @unittest.skipUnless(compat_os_name == 'nt', 'Only relevant on Windows')\n+    def test_Popen_windows_escaping(self):\n+        def run_shell(args):\n+            stdout, stderr, error = Popen.run(\n+                args, text=True, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+            assert not stderr\n+            assert not error\n+            return stdout\n+\n+        # Test escaping\n+        assert run_shell(['echo', 'test\"&']) == '\"test\"\"&\"\\n'\n+        # Test if delayed expansion is disabled\n+        assert run_shell(['echo', '^!']) == '\"^!\"\\n'\n+        assert run_shell('echo \"^!\"') == '\"^!\"\\n'\n+\n \n if __name__ == '__main__':\n     unittest.main()\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "test/test_websockets.py",
            "diff": "diff --git a/test/test_websockets.py b/test/test_websockets.py\nnew file mode 100644\nindex 00000000..af6142ea\n--- /dev/null\n+++ b/test/test_websockets.py\n@@ -0,0 +1,380 @@\n+#!/usr/bin/env python3\n+\n+# Allow direct execution\n+import os\n+import sys\n+\n+import pytest\n+\n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+\n+import http.client\n+import http.cookiejar\n+import http.server\n+import json\n+import random\n+import ssl\n+import threading\n+\n+from yt_dlp import socks\n+from yt_dlp.cookies import YoutubeDLCookieJar\n+from yt_dlp.dependencies import websockets\n+from yt_dlp.networking import Request\n+from yt_dlp.networking.exceptions import (\n+    CertificateVerifyError,\n+    HTTPError,\n+    ProxyError,\n+    RequestError,\n+    SSLError,\n+    TransportError,\n+)\n+from yt_dlp.utils.networking import HTTPHeaderDict\n+\n+from test.conftest import validate_and_send\n+\n+TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+\n+\n+def websocket_handler(websocket):\n+    for message in websocket:\n+        if isinstance(message, bytes):\n+            if message == b'bytes':\n+                return websocket.send('2')\n+        elif isinstance(message, str):\n+            if message == 'headers':\n+                return websocket.send(json.dumps(dict(websocket.request.headers)))\n+            elif message == 'path':\n+                return websocket.send(websocket.request.path)\n+            elif message == 'source_address':\n+                return websocket.send(websocket.remote_address[0])\n+            elif message == 'str':\n+                return websocket.send('1')\n+        return websocket.send(message)\n+\n+\n+def process_request(self, request):\n+    if request.path.startswith('/gen_'):\n+        status = http.HTTPStatus(int(request.path[5:]))\n+        if 300 <= status.value <= 300:\n+            return websockets.http11.Response(\n+                status.value, status.phrase, websockets.datastructures.Headers([('Location', '/')]), b'')\n+        return self.protocol.reject(status.value, status.phrase)\n+    return self.protocol.accept(request)\n+\n+\n+def create_websocket_server(**ws_kwargs):\n+    import websockets.sync.server\n+    wsd = websockets.sync.server.serve(websocket_handler, '127.0.0.1', 0, process_request=process_request, **ws_kwargs)\n+    ws_port = wsd.socket.getsockname()[1]\n+    ws_server_thread = threading.Thread(target=wsd.serve_forever)\n+    ws_server_thread.daemon = True\n+    ws_server_thread.start()\n+    return ws_server_thread, ws_port\n+\n+\n+def create_ws_websocket_server():\n+    return create_websocket_server()\n+\n+\n+def create_wss_websocket_server():\n+    certfn = os.path.join(TEST_DIR, 'testcert.pem')\n+    sslctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n+    sslctx.load_cert_chain(certfn, None)\n+    return create_websocket_server(ssl_context=sslctx)\n+\n+\n+MTLS_CERT_DIR = os.path.join(TEST_DIR, 'testdata', 'certificate')\n+\n+\n+def create_mtls_wss_websocket_server():\n+    certfn = os.path.join(TEST_DIR, 'testcert.pem')\n+    cacertfn = os.path.join(MTLS_CERT_DIR, 'ca.crt')\n+\n+    sslctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n+    sslctx.verify_mode = ssl.CERT_REQUIRED\n+    sslctx.load_verify_locations(cafile=cacertfn)\n+    sslctx.load_cert_chain(certfn, None)\n+\n+    return create_websocket_server(ssl_context=sslctx)\n+\n+\n+@pytest.mark.skipif(not websockets, reason='websockets must be installed to test websocket request handlers')\n+class TestWebsSocketRequestHandlerConformance:\n+    @classmethod\n+    def setup_class(cls):\n+        cls.ws_thread, cls.ws_port = create_ws_websocket_server()\n+        cls.ws_base_url = f'ws://127.0.0.1:{cls.ws_port}'\n+\n+        cls.wss_thread, cls.wss_port = create_wss_websocket_server()\n+        cls.wss_base_url = f'wss://127.0.0.1:{cls.wss_port}'\n+\n+        cls.bad_wss_thread, cls.bad_wss_port = create_websocket_server(ssl_context=ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER))\n+        cls.bad_wss_host = f'wss://127.0.0.1:{cls.bad_wss_port}'\n+\n+        cls.mtls_wss_thread, cls.mtls_wss_port = create_mtls_wss_websocket_server()\n+        cls.mtls_wss_base_url = f'wss://127.0.0.1:{cls.mtls_wss_port}'\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_basic_websockets(self, handler):\n+        with handler() as rh:\n+            ws = validate_and_send(rh, Request(self.ws_base_url))\n+            assert 'upgrade' in ws.headers\n+            assert ws.status == 101\n+            ws.send('foo')\n+            assert ws.recv() == 'foo'\n+            ws.close()\n+\n+    # https://www.rfc-editor.org/rfc/rfc6455.html#section-5.6\n+    @pytest.mark.parametrize('msg,opcode', [('str', 1), (b'bytes', 2)])\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_send_types(self, handler, msg, opcode):\n+        with handler() as rh:\n+            ws = validate_and_send(rh, Request(self.ws_base_url))\n+            ws.send(msg)\n+            assert int(ws.recv()) == opcode\n+            ws.close()\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_verify_cert(self, handler):\n+        with handler() as rh:\n+            with pytest.raises(CertificateVerifyError):\n+                validate_and_send(rh, Request(self.wss_base_url))\n+\n+        with handler(verify=False) as rh:\n+            ws = validate_and_send(rh, Request(self.wss_base_url))\n+            assert ws.status == 101\n+            ws.close()\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_ssl_error(self, handler):\n+        with handler(verify=False) as rh:\n+            with pytest.raises(SSLError, match=r'ssl(?:v3|/tls) alert handshake failure') as exc_info:\n+                validate_and_send(rh, Request(self.bad_wss_host))\n+            assert not issubclass(exc_info.type, CertificateVerifyError)\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    @pytest.mark.parametrize('path,expected', [\n+        # Unicode characters should be encoded with uppercase percent-encoding\n+        ('/\u4e2d\u6587', '/%E4%B8%AD%E6%96%87'),\n+        # don't normalize existing percent encodings\n+        ('/%c7%9f', '/%c7%9f'),\n+    ])\n+    def test_percent_encode(self, handler, path, expected):\n+        with handler() as rh:\n+            ws = validate_and_send(rh, Request(f'{self.ws_base_url}{path}'))\n+            ws.send('path')\n+            assert ws.recv() == expected\n+            assert ws.status == 101\n+            ws.close()\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_remove_dot_segments(self, handler):\n+        with handler() as rh:\n+            # This isn't a comprehensive test,\n+            # but it should be enough to check whether the handler is removing dot segments\n+            ws = validate_and_send(rh, Request(f'{self.ws_base_url}/a/b/./../../test'))\n+            assert ws.status == 101\n+            ws.send('path')\n+            assert ws.recv() == '/test'\n+            ws.close()\n+\n+    # We are restricted to known HTTP status codes in http.HTTPStatus\n+    # Redirects are not supported for websockets\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    @pytest.mark.parametrize('status', (200, 204, 301, 302, 303, 400, 500, 511))\n+    def test_raise_http_error(self, handler, status):\n+        with handler() as rh:\n+            with pytest.raises(HTTPError) as exc_info:\n+                validate_and_send(rh, Request(f'{self.ws_base_url}/gen_{status}'))\n+            assert exc_info.value.status == status\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    @pytest.mark.parametrize('params,extensions', [\n+        ({'timeout': 0.00001}, {}),\n+        ({}, {'timeout': 0.00001}),\n+    ])\n+    def test_timeout(self, handler, params, extensions):\n+        with handler(**params) as rh:\n+            with pytest.raises(TransportError):\n+                validate_and_send(rh, Request(self.ws_base_url, extensions=extensions))\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_cookies(self, handler):\n+        cookiejar = YoutubeDLCookieJar()\n+        cookiejar.set_cookie(http.cookiejar.Cookie(\n+            version=0, name='test', value='ytdlp', port=None, port_specified=False,\n+            domain='127.0.0.1', domain_specified=True, domain_initial_dot=False, path='/',\n+            path_specified=True, secure=False, expires=None, discard=False, comment=None,\n+            comment_url=None, rest={}))\n+\n+        with handler(cookiejar=cookiejar) as rh:\n+            ws = validate_and_send(rh, Request(self.ws_base_url))\n+            ws.send('headers')\n+            assert json.loads(ws.recv())['cookie'] == 'test=ytdlp'\n+            ws.close()\n+\n+        with handler() as rh:\n+            ws = validate_and_send(rh, Request(self.ws_base_url))\n+            ws.send('headers')\n+            assert 'cookie' not in json.loads(ws.recv())\n+            ws.close()\n+\n+            ws = validate_and_send(rh, Request(self.ws_base_url, extensions={'cookiejar': cookiejar}))\n+            ws.send('headers')\n+            assert json.loads(ws.recv())['cookie'] == 'test=ytdlp'\n+            ws.close()\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_source_address(self, handler):\n+        source_address = f'127.0.0.{random.randint(5, 255)}'\n+        with handler(source_address=source_address) as rh:\n+            ws = validate_and_send(rh, Request(self.ws_base_url))\n+            ws.send('source_address')\n+            assert source_address == ws.recv()\n+            ws.close()\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_response_url(self, handler):\n+        with handler() as rh:\n+            url = f'{self.ws_base_url}/something'\n+            ws = validate_and_send(rh, Request(url))\n+            assert ws.url == url\n+            ws.close()\n+\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_request_headers(self, handler):\n+        with handler(headers=HTTPHeaderDict({'test1': 'test', 'test2': 'test2'})) as rh:\n+            # Global Headers\n+            ws = validate_and_send(rh, Request(self.ws_base_url))\n+            ws.send('headers')\n+            headers = HTTPHeaderDict(json.loads(ws.recv()))\n+            assert headers['test1'] == 'test'\n+            ws.close()\n+\n+            # Per request headers, merged with global\n+            ws = validate_and_send(rh, Request(\n+                self.ws_base_url, headers={'test2': 'changed', 'test3': 'test3'}))\n+            ws.send('headers')\n+            headers = HTTPHeaderDict(json.loads(ws.recv()))\n+            assert headers['test1'] == 'test'\n+            assert headers['test2'] == 'changed'\n+            assert headers['test3'] == 'test3'\n+            ws.close()\n+\n+    @pytest.mark.parametrize('client_cert', (\n+        {'client_certificate': os.path.join(MTLS_CERT_DIR, 'clientwithkey.crt')},\n+        {\n+            'client_certificate': os.path.join(MTLS_CERT_DIR, 'client.crt'),\n+            'client_certificate_key': os.path.join(MTLS_CERT_DIR, 'client.key'),\n+        },\n+        {\n+            'client_certificate': os.path.join(MTLS_CERT_DIR, 'clientwithencryptedkey.crt'),\n+            'client_certificate_password': 'foobar',\n+        },\n+        {\n+            'client_certificate': os.path.join(MTLS_CERT_DIR, 'client.crt'),\n+            'client_certificate_key': os.path.join(MTLS_CERT_DIR, 'clientencrypted.key'),\n+            'client_certificate_password': 'foobar',\n+        }\n+    ))\n+    @pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+    def test_mtls(self, handler, client_cert):\n+        with handler(\n+            # Disable client-side validation of unacceptable self-signed testcert.pem\n+            # The test is of a check on the server side, so unaffected\n+            verify=False,\n+            client_cert=client_cert\n+        ) as rh:\n+            validate_and_send(rh, Request(self.mtls_wss_base_url)).close()\n+\n+\n+def create_fake_ws_connection(raised):\n+    import websockets.sync.client\n+\n+    class FakeWsConnection(websockets.sync.client.ClientConnection):\n+        def __init__(self, *args, **kwargs):\n+            class FakeResponse:\n+                body = b''\n+                headers = {}\n+                status_code = 101\n+                reason_phrase = 'test'\n+\n+            self.response = FakeResponse()\n+\n+        def send(self, *args, **kwargs):\n+            raise raised()\n+\n+        def recv(self, *args, **kwargs):\n+            raise raised()\n+\n+        def close(self, *args, **kwargs):\n+            return\n+\n+    return FakeWsConnection()\n+\n+\n+@pytest.mark.parametrize('handler', ['Websockets'], indirect=True)\n+class TestWebsocketsRequestHandler:\n+    @pytest.mark.parametrize('raised,expected', [\n+        # https://websockets.readthedocs.io/en/stable/reference/exceptions.html\n+        (lambda: websockets.exceptions.InvalidURI(msg='test', uri='test://'), RequestError),\n+        # Requires a response object. Should be covered by HTTP error tests.\n+        # (lambda: websockets.exceptions.InvalidStatus(), TransportError),\n+        (lambda: websockets.exceptions.InvalidHandshake(), TransportError),\n+        # These are subclasses of InvalidHandshake\n+        (lambda: websockets.exceptions.InvalidHeader(name='test'), TransportError),\n+        (lambda: websockets.exceptions.NegotiationError(), TransportError),\n+        # Catch-all\n+        (lambda: websockets.exceptions.WebSocketException(), TransportError),\n+        (lambda: TimeoutError(), TransportError),\n+        # These may be raised by our create_connection implementation, which should also be caught\n+        (lambda: OSError(), TransportError),\n+        (lambda: ssl.SSLError(), SSLError),\n+        (lambda: ssl.SSLCertVerificationError(), CertificateVerifyError),\n+        (lambda: socks.ProxyError(), ProxyError),\n+    ])\n+    def test_request_error_mapping(self, handler, monkeypatch, raised, expected):\n+        import websockets.sync.client\n+\n+        import yt_dlp.networking._websockets\n+        with handler() as rh:\n+            def fake_connect(*args, **kwargs):\n+                raise raised()\n+            monkeypatch.setattr(yt_dlp.networking._websockets, 'create_connection', lambda *args, **kwargs: None)\n+            monkeypatch.setattr(websockets.sync.client, 'connect', fake_connect)\n+            with pytest.raises(expected) as exc_info:\n+                rh.send(Request('ws://fake-url'))\n+            assert exc_info.type is expected\n+\n+    @pytest.mark.parametrize('raised,expected,match', [\n+        # https://websockets.readthedocs.io/en/stable/reference/sync/client.html#websockets.sync.client.ClientConnection.send\n+        (lambda: websockets.exceptions.ConnectionClosed(None, None), TransportError, None),\n+        (lambda: RuntimeError(), TransportError, None),\n+        (lambda: TimeoutError(), TransportError, None),\n+        (lambda: TypeError(), RequestError, None),\n+        (lambda: socks.ProxyError(), ProxyError, None),\n+        # Catch-all\n+        (lambda: websockets.exceptions.WebSocketException(), TransportError, None),\n+    ])\n+    def test_ws_send_error_mapping(self, handler, monkeypatch, raised, expected, match):\n+        from yt_dlp.networking._websockets import WebsocketsResponseAdapter\n+        ws = WebsocketsResponseAdapter(create_fake_ws_connection(raised), url='ws://fake-url')\n+        with pytest.raises(expected, match=match) as exc_info:\n+            ws.send('test')\n+        assert exc_info.type is expected\n+\n+    @pytest.mark.parametrize('raised,expected,match', [\n+        # https://websockets.readthedocs.io/en/stable/reference/sync/client.html#websockets.sync.client.ClientConnection.recv\n+        (lambda: websockets.exceptions.ConnectionClosed(None, None), TransportError, None),\n+        (lambda: RuntimeError(), TransportError, None),\n+        (lambda: TimeoutError(), TransportError, None),\n+        (lambda: socks.ProxyError(), ProxyError, None),\n+        # Catch-all\n+        (lambda: websockets.exceptions.WebSocketException(), TransportError, None),\n+    ])\n+    def test_ws_recv_error_mapping(self, handler, monkeypatch, raised, expected, match):\n+        from yt_dlp.networking._websockets import WebsocketsResponseAdapter\n+        ws = WebsocketsResponseAdapter(create_fake_ws_connection(raised), url='ws://fake-url')\n+        with pytest.raises(expected, match=match) as exc_info:\n+            ws.recv()\n+        assert exc_info.type is expected\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt-dlp.cmd",
            "diff": "diff --git a/yt-dlp.cmd b/yt-dlp.cmd\nindex aa4500f9..5537e0ea 100644\n--- a/yt-dlp.cmd\n+++ b/yt-dlp.cmd\n@@ -1 +1 @@\n-@py -bb -Werror -Xdev \"%~dp0yt_dlp\\__main__.py\" %*\n+@py -Werror -Xdev \"%~dp0yt_dlp\\__main__.py\" %*\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt-dlp.sh",
            "diff": "diff --git a/yt-dlp.sh b/yt-dlp.sh\nindex 22a69250..ce74df80 100755\n--- a/yt-dlp.sh\n+++ b/yt-dlp.sh\n@@ -1,2 +1,2 @@\n #!/usr/bin/env sh\n-exec \"${PYTHON:-python3}\" -bb -Werror -Xdev \"$(dirname \"$(realpath \"$0\")\")/yt_dlp/__main__.py\" \"$@\"\n+exec \"${PYTHON:-python3}\" -Werror -Xdev \"$(dirname \"$(realpath \"$0\")\")/yt_dlp/__main__.py\" \"$@\"\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/YoutubeDL.py",
            "diff": "diff --git a/yt_dlp/YoutubeDL.py b/yt_dlp/YoutubeDL.py\nindex 666d89b4..8d96498a 100644\n--- a/yt_dlp/YoutubeDL.py\n+++ b/yt_dlp/YoutubeDL.py\n@@ -60,7 +60,13 @@\n     get_postprocessor,\n )\n from .postprocessor.ffmpeg import resolve_mapping as resolve_recode_mapping\n-from .update import REPOSITORY, current_git_head, detect_variant\n+from .update import (\n+    REPOSITORY,\n+    _get_system_deprecation,\n+    _make_label,\n+    current_git_head,\n+    detect_variant,\n+)\n from .utils import (\n     DEFAULT_OUTTMPL,\n     IDENTITY,\n@@ -158,7 +164,7 @@\n     clean_proxies,\n     std_headers,\n )\n-from .version import CHANNEL, RELEASE_GIT_HEAD, VARIANT, __version__\n+from .version import CHANNEL, ORIGIN, RELEASE_GIT_HEAD, VARIANT, __version__\n \n if compat_os_name == 'nt':\n     import ctypes\n@@ -239,9 +245,9 @@ class YoutubeDL:\n                        'selected' (check selected formats),\n                        or None (check only if requested by extractor)\n     paths:             Dictionary of output paths. The allowed keys are 'home'\n-                       'temp' and the keys of OUTTMPL_TYPES (in utils.py)\n+                       'temp' and the keys of OUTTMPL_TYPES (in utils/_utils.py)\n     outtmpl:           Dictionary of templates for output names. Allowed keys\n-                       are 'default' and the keys of OUTTMPL_TYPES (in utils.py).\n+                       are 'default' and the keys of OUTTMPL_TYPES (in utils/_utils.py).\n                        For compatibility with youtube-dl, a single string can also be used\n     outtmpl_na_placeholder: Placeholder for unavailable meta fields.\n     restrictfilenames: Do not allow \"&\" and spaces in file names\n@@ -422,7 +428,7 @@ class YoutubeDL:\n                          asked whether to download the video.\n                        - Raise utils.DownloadCancelled(msg) to abort remaining\n                          downloads when a video is rejected.\n-                       match_filter_func in utils.py is one example for this.\n+                       match_filter_func in utils/_utils.py is one example for this.\n     color:             A Dictionary with output stream names as keys\n                        and their respective color policy as values.\n                        Can also just be a single color policy,\n@@ -625,13 +631,16 @@ def __init__(self, params=None, auto_init=True):\n                     'Overwriting params from \"color\" with \"no_color\"')\n             self.params['color'] = 'no_color'\n \n-        term_allow_color = os.environ.get('TERM', '').lower() != 'dumb'\n+        term_allow_color = os.getenv('TERM', '').lower() != 'dumb'\n+        no_color = bool(os.getenv('NO_COLOR'))\n \n         def process_color_policy(stream):\n             stream_name = {sys.stdout: 'stdout', sys.stderr: 'stderr'}[stream]\n             policy = traverse_obj(self.params, ('color', (stream_name, None), {str}), get_all=False)\n             if policy in ('auto', None):\n-                return term_allow_color and supports_terminal_sequences(stream)\n+                if term_allow_color and supports_terminal_sequences(stream):\n+                    return 'no_color' if no_color else True\n+                return False\n             assert policy in ('always', 'never', 'no_color'), policy\n             return {'always': True, 'never': False}.get(policy, policy)\n \n@@ -640,17 +649,9 @@ def process_color_policy(stream):\n             for name, stream in self._out_files.items_ if name != 'console'\n         })\n \n-        # The code is left like this to be reused for future deprecations\n-        MIN_SUPPORTED, MIN_RECOMMENDED = (3, 7), (3, 7)\n-        current_version = sys.version_info[:2]\n-        if current_version < MIN_RECOMMENDED:\n-            msg = ('Support for Python version %d.%d has been deprecated. '\n-                   'See  https://github.com/yt-dlp/yt-dlp/issues/3764  for more details.'\n-                   '\\n                    You will no longer receive updates on this version')\n-            if current_version < MIN_SUPPORTED:\n-                msg = 'Python version %d.%d is no longer supported'\n-            self.deprecated_feature(\n-                f'{msg}! Please update to Python %d.%d or above' % (*current_version, *MIN_RECOMMENDED))\n+        system_deprecation = _get_system_deprecation()\n+        if system_deprecation:\n+            self.deprecated_feature(system_deprecation.replace('\\n', '\\n                    '))\n \n         if self.params.get('allow_unplayable_formats'):\n             self.report_warning(\n@@ -1184,6 +1185,7 @@ def prepare_outtmpl(self, outtmpl, info_dict, sanitize=False):\n         MATH_FUNCTIONS = {\n             '+': float.__add__,\n             '-': float.__sub__,\n+            '*': float.__mul__,\n         }\n         # Field is of the form key1.key2...\n         # where keys (except first) can be string, int, slice or \"{field, ...}\"\n@@ -1205,6 +1207,15 @@ def prepare_outtmpl(self, outtmpl, info_dict, sanitize=False):\n                 (?:\\|(?P<default>.*?))?\n             )$''')\n \n+        def _from_user_input(field):\n+            if field == ':':\n+                return ...\n+            elif ':' in field:\n+                return slice(*map(int_or_none, field.split(':')))\n+            elif int_or_none(field) is not None:\n+                return int(field)\n+            return field\n+\n         def _traverse_infodict(fields):\n             fields = [f for x in re.split(r'\\.({.+?})\\.?', fields)\n                       for f in ([x] if x.startswith('{') else x.split('.'))]\n@@ -1214,11 +1225,12 @@ def _traverse_infodict(fields):\n \n             for i, f in enumerate(fields):\n                 if not f.startswith('{'):\n+                    fields[i] = _from_user_input(f)\n                     continue\n                 assert f.endswith('}'), f'No closing brace for {f} in {fields}'\n-                fields[i] = {k: k.split('.') for k in f[1:-1].split(',')}\n+                fields[i] = {k: list(map(_from_user_input, k.split('.'))) for k in f[1:-1].split(',')}\n \n-            return traverse_obj(info_dict, fields, is_user_input=True, traverse_string=True)\n+            return traverse_obj(info_dict, fields, traverse_string=True)\n \n         def get_value(mdict):\n             # Object traversal\n@@ -2346,7 +2358,7 @@ def _check_formats(formats):\n                 return\n \n             for f in formats:\n-                if f.get('has_drm'):\n+                if f.get('has_drm') or f.get('__needs_testing'):\n                     yield from self._check_formats([f])\n                 else:\n                     yield f\n@@ -2459,9 +2471,16 @@ def final_selector(ctx):\n                 return selector_function(ctx_copy)\n             return final_selector\n \n-        stream = io.BytesIO(format_spec.encode())\n+        # HACK: Python 3.12 changed the underlying parser, rendering '7_a' invalid\n+        #       Prefix numbers with random letters to avoid it being classified as a number\n+        #       See: https://github.com/yt-dlp/yt-dlp/pulls/8797\n+        # TODO: Implement parser not reliant on tokenize.tokenize\n+        prefix = ''.join(random.choices(string.ascii_letters, k=32))\n+        stream = io.BytesIO(re.sub(r'\\d[_\\d]*', rf'{prefix}\\g<0>', format_spec).encode())\n         try:\n-            tokens = list(_remove_unused_ops(tokenize.tokenize(stream.readline)))\n+            tokens = list(_remove_unused_ops(\n+                token._replace(string=token.string.replace(prefix, ''))\n+                for token in tokenize.tokenize(stream.readline)))\n         except tokenize.TokenError:\n             raise syntax_error('Missing closing/opening brackets or parenthesis', (0, len(format_spec)))\n \n@@ -2591,9 +2610,12 @@ def _fill_common_fields(self, info_dict, final=True):\n                 # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n                 # see http://bugs.python.org/issue1646728)\n                 with contextlib.suppress(ValueError, OverflowError, OSError):\n-                    upload_date = datetime.datetime.utcfromtimestamp(info_dict[ts_key])\n+                    upload_date = datetime.datetime.fromtimestamp(info_dict[ts_key], datetime.timezone.utc)\n                     info_dict[date_key] = upload_date.strftime('%Y%m%d')\n \n+        if not info_dict.get('release_year'):\n+            info_dict['release_year'] = traverse_obj(info_dict, ('release_date', {lambda x: int(x[:4])}))\n+\n         live_keys = ('is_live', 'was_live')\n         live_status = info_dict.get('live_status')\n         if live_status is None:\n@@ -2772,7 +2794,8 @@ def is_wellformed(f):\n                 format['dynamic_range'] = 'SDR'\n             if format.get('aspect_ratio') is None:\n                 format['aspect_ratio'] = try_call(lambda: round(format['width'] / format['height'], 2))\n-            if (not format.get('manifest_url')  # For fragmented formats, \"tbr\" is often max bitrate and not average\n+            # For fragmented formats, \"tbr\" is often max bitrate and not average\n+            if (('manifest-filesize-approx' in self.params['compat_opts'] or not format.get('manifest_url'))\n                     and info_dict.get('duration') and format.get('tbr')\n                     and not format.get('filesize') and not format.get('filesize_approx')):\n                 format['filesize_approx'] = int(info_dict['duration'] * format['tbr'] * (1024 / 8))\n@@ -3551,14 +3574,14 @@ def sanitize_info(info_dict, remove_private_keys=False):\n             'version': __version__,\n             'current_git_head': current_git_head(),\n             'release_git_head': RELEASE_GIT_HEAD,\n-            'repository': REPOSITORY,\n+            'repository': ORIGIN,\n         })\n \n         if remove_private_keys:\n             reject = lambda k, v: v is None or k.startswith('__') or k in {\n                 'requested_downloads', 'requested_formats', 'requested_subtitles', 'requested_entries',\n                 'entries', 'filepath', '_filename', 'filename', 'infojson_filename', 'original_url',\n-                'playlist_autonumber', '_format_sort_fields',\n+                'playlist_autonumber',\n             }\n         else:\n             reject = lambda k, v: False\n@@ -3934,8 +3957,8 @@ def get_encoding(stream):\n             source += '*'\n         klass = type(self)\n         write_debug(join_nonempty(\n-            f'{\"yt-dlp\" if REPOSITORY == \"yt-dlp/yt-dlp\" else REPOSITORY} version',\n-            f'{CHANNEL}@{__version__}',\n+            f'{REPOSITORY.rpartition(\"/\")[2]} version',\n+            _make_label(ORIGIN, CHANNEL.partition('@')[2] or __version__, __version__),\n             f'[{RELEASE_GIT_HEAD[:9]}]' if RELEASE_GIT_HEAD else '',\n             '' if source == 'unknown' else f'({source})',\n             '' if _IN_CLI else 'API' if klass == YoutubeDL else f'API:{self.__module__}.{klass.__qualname__}',\n@@ -3976,7 +3999,7 @@ def get_encoding(stream):\n         })) or 'none'))\n \n         write_debug(f'Proxy map: {self.proxies}')\n-        # write_debug(f'Request Handlers: {\", \".join(rh.RH_NAME for rh in self._request_director.handlers.values())}')\n+        write_debug(f'Request Handlers: {\", \".join(rh.RH_NAME for rh in self._request_director.handlers.values())}')\n         for plugin_type, plugins in {'Extractor': plugin_ies, 'Post-Processor': plugin_pps}.items():\n             display_list = ['%s%s' % (\n                 klass.__name__, '' if klass.__name__ == name else f' as {name}')\n@@ -4059,12 +4082,25 @@ def urlopen(self, req):\n             return self._request_director.send(req)\n         except NoSupportingHandlers as e:\n             for ue in e.unsupported_errors:\n+                # FIXME: This depends on the order of errors.\n                 if not (ue.handler and ue.msg):\n                     continue\n                 if ue.handler.RH_KEY == 'Urllib' and 'unsupported url scheme: \"file\"' in ue.msg.lower():\n                     raise RequestError(\n                         'file:// URLs are disabled by default in yt-dlp for security reasons. '\n                         'Use --enable-file-urls to enable at your own risk.', cause=ue) from ue\n+                if 'unsupported proxy type: \"https\"' in ue.msg.lower():\n+                    raise RequestError(\n+                        'To use an HTTPS proxy for this request, one of the following dependencies needs to be installed: requests')\n+\n+                elif (\n+                    re.match(r'unsupported url scheme: \"wss?\"', ue.msg.lower())\n+                    and 'websockets' not in self._request_director.handlers\n+                ):\n+                    raise RequestError(\n+                        'This request requires WebSocket support. '\n+                        'Ensure one of the following dependencies are installed: websockets',\n+                        cause=ue) from ue\n             raise\n         except SSLError as e:\n             if 'UNSAFE_LEGACY_RENEGOTIATION_DISABLED' in str(e):\n@@ -4107,6 +4143,8 @@ def build_request_director(self, handlers, preferences=None):\n                 }),\n             ))\n         director.preferences.update(preferences or [])\n+        if 'prefer-legacy-http-handler' in self.params['compat_opts']:\n+            director.preferences.add(lambda rh, _: 500 if rh.RH_KEY == 'Urllib' else 0)\n         return director\n \n     def encode(self, s):\n@@ -4229,7 +4267,7 @@ def _write_subtitles(self, info_dict, filename):\n         return ret\n \n     def _write_thumbnails(self, label, info_dict, filename, thumb_filename_base=None):\n-        ''' Write thumbnails to file and return list of (thumb_filename, final_thumb_filename) '''\n+        ''' Write thumbnails to file and return list of (thumb_filename, final_thumb_filename); or None if error '''\n         write_all = self.params.get('write_all_thumbnails', False)\n         thumbnails, ret = [], []\n         if write_all or self.params.get('writethumbnail', False):\n@@ -4245,6 +4283,9 @@ def _write_thumbnails(self, label, info_dict, filename, thumb_filename_base=None\n             self.write_debug(f'Skipping writing {label} thumbnail')\n             return ret\n \n+        if thumbnails and not self._ensure_dir_exists(filename):\n+            return None\n+\n         for idx, t in list(enumerate(thumbnails))[::-1]:\n             thumb_ext = (f'{t[\"id\"]}.' if multiple else '') + determine_ext(t['url'], 'jpg')\n             thumb_display_id = f'{label} thumbnail {t[\"id\"]}'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/__init__.py",
            "diff": "diff --git a/yt_dlp/__init__.py b/yt_dlp/__init__.py\nindex 991dbcda..57a48715 100644\n--- a/yt_dlp/__init__.py\n+++ b/yt_dlp/__init__.py\n@@ -1,8 +1,8 @@\n-try:\n-    import contextvars  # noqa: F401\n-except Exception:\n-    raise Exception(\n-        f'You are using an unsupported version of Python. Only Python versions 3.7 and above are supported by yt-dlp')  # noqa: F541\n+import sys\n+\n+if sys.version_info < (3, 8):\n+    raise ImportError(\n+        f'You are using an unsupported version of Python. Only Python versions 3.8 and above are supported by yt-dlp')  # noqa: F541\n \n __license__ = 'Public Domain'\n \n@@ -12,7 +12,6 @@\n import optparse\n import os\n import re\n-import sys\n import traceback\n \n from .compat import compat_shlex_quote\n@@ -74,14 +73,16 @@ def _exit(status=0, *args):\n \n \n def get_urls(urls, batchfile, verbose):\n-    # Batch file verification\n+    \"\"\"\n+    @param verbose      -1: quiet, 0: normal, 1: verbose\n+    \"\"\"\n     batch_urls = []\n     if batchfile is not None:\n         try:\n             batch_urls = read_batch_urls(\n-                read_stdin('URLs') if batchfile == '-'\n+                read_stdin(None if verbose == -1 else 'URLs') if batchfile == '-'\n                 else open(expand_path(batchfile), encoding='utf-8', errors='ignore'))\n-            if verbose:\n+            if verbose == 1:\n                 write_string('[debug] Batch file urls: ' + repr(batch_urls) + '\\n')\n         except OSError:\n             _exit(f'ERROR: batch file {batchfile} could not be read')\n@@ -722,7 +723,7 @@ def get_postprocessors(opts):\n def parse_options(argv=None):\n     \"\"\"@returns ParsedOptions(parser, opts, urls, ydl_opts)\"\"\"\n     parser, opts, urls = parseOpts(argv)\n-    urls = get_urls(urls, opts.batchfile, opts.verbose)\n+    urls = get_urls(urls, opts.batchfile, -1 if opts.quiet and not opts.verbose else opts.verbose)\n \n     set_compat_opts(opts)\n     try:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/__pyinstaller/hook-yt_dlp.py",
            "diff": "diff --git a/yt_dlp/__pyinstaller/hook-yt_dlp.py b/yt_dlp/__pyinstaller/hook-yt_dlp.py\nindex 88c2b8b2..20f037d3 100644\n--- a/yt_dlp/__pyinstaller/hook-yt_dlp.py\n+++ b/yt_dlp/__pyinstaller/hook-yt_dlp.py\n@@ -21,9 +21,11 @@ def get_hidden_imports():\n     yield from ('yt_dlp.compat._legacy', 'yt_dlp.compat._deprecated')\n     yield from ('yt_dlp.utils._legacy', 'yt_dlp.utils._deprecated')\n     yield pycryptodome_module()\n-    yield from collect_submodules('websockets')\n+    # Only `websockets` is required, others are collected just in case\n+    for module in ('websockets', 'requests', 'urllib3'):\n+        yield from collect_submodules(module)\n     # These are auto-detected, but explicitly add them just in case\n-    yield from ('mutagen', 'brotli', 'certifi')\n+    yield from ('mutagen', 'brotli', 'certifi', 'secretstorage')\n \n \n hiddenimports = list(get_hidden_imports())\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/compat/__init__.py",
            "diff": "diff --git a/yt_dlp/compat/__init__.py b/yt_dlp/compat/__init__.py\nindex 832a9138..5ad5c70e 100644\n--- a/yt_dlp/compat/__init__.py\n+++ b/yt_dlp/compat/__init__.py\n@@ -30,7 +30,7 @@ def compat_etree_fromstring(text):\n if compat_os_name == 'nt':\n     def compat_shlex_quote(s):\n         import re\n-        return s if re.match(r'^[-_\\w./]+$', s) else '\"%s\"' % s.replace('\"', '\\\\\"')\n+        return s if re.match(r'^[-_\\w./]+$', s) else s.replace('\"', '\"\"').join('\"\"')\n else:\n     from shlex import quote as compat_shlex_quote  # noqa: F401\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/compat/compat_utils.py",
            "diff": "diff --git a/yt_dlp/compat/compat_utils.py b/yt_dlp/compat/compat_utils.py\nindex 3ca46d27..d62b7d04 100644\n--- a/yt_dlp/compat/compat_utils.py\n+++ b/yt_dlp/compat/compat_utils.py\n@@ -15,7 +15,7 @@ def get_package_info(module):\n         name=getattr(module, '_yt_dlp__identifier', module.__name__),\n         version=str(next(filter(None, (\n             getattr(module, attr, None)\n-            for attr in ('__version__', 'version_string', 'version')\n+            for attr in ('_yt_dlp__version', '__version__', 'version_string', 'version')\n         )), None)))\n \n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/compat/functools.py",
            "diff": "diff --git a/yt_dlp/compat/functools.py b/yt_dlp/compat/functools.py\nindex ec003ea9..36c98364 100644\n--- a/yt_dlp/compat/functools.py\n+++ b/yt_dlp/compat/functools.py\n@@ -10,17 +10,3 @@\n     cache  # >= 3.9\n except NameError:\n     cache = lru_cache(maxsize=None)\n-\n-try:\n-    cached_property  # >= 3.8\n-except NameError:\n-    class cached_property:\n-        def __init__(self, func):\n-            update_wrapper(self, func)\n-            self.func = func\n-\n-        def __get__(self, instance, _):\n-            if instance is None:\n-                return self\n-            setattr(instance, self.func.__name__, self.func(instance))\n-            return getattr(instance, self.func.__name__)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/compat/urllib/__init__.py",
            "diff": "diff --git a/yt_dlp/compat/urllib/__init__.py b/yt_dlp/compat/urllib/__init__.py\nindex b27cc613..9084b3c2 100644\n--- a/yt_dlp/compat/urllib/__init__.py\n+++ b/yt_dlp/compat/urllib/__init__.py\n@@ -1,7 +1,7 @@\n # flake8: noqa: F405\n from urllib import *  # noqa: F403\n \n-del request\n+del request  # noqa: F821\n from . import request  # noqa: F401\n \n from ..compat_utils import passthrough_module\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/cookies.py",
            "diff": "diff --git a/yt_dlp/cookies.py b/yt_dlp/cookies.py\nindex a71fbc28..eac033e3 100644\n--- a/yt_dlp/cookies.py\n+++ b/yt_dlp/cookies.py\n@@ -186,7 +186,7 @@ def _firefox_browser_dir():\n     if sys.platform in ('cygwin', 'win32'):\n         return os.path.expandvars(R'%APPDATA%\\Mozilla\\Firefox\\Profiles')\n     elif sys.platform == 'darwin':\n-        return os.path.expanduser('~/Library/Application Support/Firefox')\n+        return os.path.expanduser('~/Library/Application Support/Firefox/Profiles')\n     return os.path.expanduser('~/.mozilla/firefox')\n \n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/dependencies/__init__.py",
            "diff": "diff --git a/yt_dlp/dependencies/__init__.py b/yt_dlp/dependencies/__init__.py\nindex 6e7d29c5..ef83739a 100644\n--- a/yt_dlp/dependencies/__init__.py\n+++ b/yt_dlp/dependencies/__init__.py\n@@ -43,6 +43,8 @@\n \n try:\n     import sqlite3\n+    # We need to get the underlying `sqlite` version, see https://github.com/yt-dlp/yt-dlp/issues/8152\n+    sqlite3._yt_dlp__version = sqlite3.sqlite_version\n except ImportError:\n     # although sqlite3 is part of the standard library, it is possible to compile python without\n     # sqlite support. See: https://github.com/yt-dlp/yt-dlp/issues/544\n@@ -56,6 +58,15 @@\n     # See https://github.com/yt-dlp/yt-dlp/issues/2633\n     websockets = None\n \n+try:\n+    import urllib3\n+except ImportError:\n+    urllib3 = None\n+\n+try:\n+    import requests\n+except ImportError:\n+    requests = None\n \n try:\n     import xattr  # xattr or pyxattr\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/downloader/dash.py",
            "diff": "diff --git a/yt_dlp/downloader/dash.py b/yt_dlp/downloader/dash.py\nindex 4328d739..afc79b6c 100644\n--- a/yt_dlp/downloader/dash.py\n+++ b/yt_dlp/downloader/dash.py\n@@ -15,12 +15,15 @@ class DashSegmentsFD(FragmentFD):\n     FD_NAME = 'dashsegments'\n \n     def real_download(self, filename, info_dict):\n-        if info_dict.get('is_live') and set(info_dict['protocol'].split('+')) != {'http_dash_segments_generator'}:\n-            self.report_error('Live DASH videos are not supported')\n+        if 'http_dash_segments_generator' in info_dict['protocol'].split('+'):\n+            real_downloader = None  # No external FD can support --live-from-start\n+        else:\n+            if info_dict.get('is_live'):\n+                self.report_error('Live DASH videos are not supported')\n+            real_downloader = get_suitable_downloader(\n+                info_dict, self.params, None, protocol='dash_frag_urls', to_stdout=(filename == '-'))\n \n         real_start = time.time()\n-        real_downloader = get_suitable_downloader(\n-            info_dict, self.params, None, protocol='dash_frag_urls', to_stdout=(filename == '-'))\n \n         requested_formats = [{**info_dict, **fmt} for fmt in info_dict.get('requested_formats', [])]\n         args = []\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/downloader/external.py",
            "diff": "diff --git a/yt_dlp/downloader/external.py b/yt_dlp/downloader/external.py\nindex 4ce8a3bf..ce5eeb0a 100644\n--- a/yt_dlp/downloader/external.py\n+++ b/yt_dlp/downloader/external.py\n@@ -335,7 +335,7 @@ def _make_cmd(self, tmpfilename, info_dict):\n         cmd += ['--auto-file-renaming=false']\n \n         if 'fragments' in info_dict:\n-            cmd += ['--file-allocation=none', '--uri-selector=inorder']\n+            cmd += ['--uri-selector=inorder']\n             url_list_file = '%s.frag.urls' % tmpfilename\n             url_list = []\n             for frag_index, fragment in enumerate(info_dict['fragments']):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/downloader/fragment.py",
            "diff": "diff --git a/yt_dlp/downloader/fragment.py b/yt_dlp/downloader/fragment.py\nindex b4b680da..b4f003d3 100644\n--- a/yt_dlp/downloader/fragment.py\n+++ b/yt_dlp/downloader/fragment.py\n@@ -14,6 +14,7 @@\n from ..networking.exceptions import HTTPError, IncompleteRead\n from ..utils import DownloadError, RetryManager, encodeFilename, traverse_obj\n from ..utils.networking import HTTPHeaderDict\n+from ..utils.progress import ProgressCalculator\n \n \n class HttpQuietDownloader(HttpFD):\n@@ -226,8 +227,7 @@ def _start_frag_download(self, ctx, info_dict):\n         resume_len = ctx['complete_frags_downloaded_bytes']\n         total_frags = ctx['total_frags']\n         ctx_id = ctx.get('ctx_id')\n-        # This dict stores the download progress, it's updated by the progress\n-        # hook\n+        # Stores the download progress, updated by the progress hook\n         state = {\n             'status': 'downloading',\n             'downloaded_bytes': resume_len,\n@@ -237,14 +237,8 @@ def _start_frag_download(self, ctx, info_dict):\n             'tmpfilename': ctx['tmpfilename'],\n         }\n \n-        start = time.time()\n-        ctx.update({\n-            'started': start,\n-            'fragment_started': start,\n-            # Amount of fragment's bytes downloaded by the time of the previous\n-            # frag progress hook invocation\n-            'prev_frag_downloaded_bytes': 0,\n-        })\n+        ctx['started'] = time.time()\n+        progress = ProgressCalculator(resume_len)\n \n         def frag_progress_hook(s):\n             if s['status'] not in ('downloading', 'finished'):\n@@ -259,38 +253,35 @@ def frag_progress_hook(s):\n             state['max_progress'] = ctx.get('max_progress')\n             state['progress_idx'] = ctx.get('progress_idx')\n \n-            time_now = time.time()\n-            state['elapsed'] = time_now - start\n+            state['elapsed'] = progress.elapsed\n             frag_total_bytes = s.get('total_bytes') or 0\n             s['fragment_info_dict'] = s.pop('info_dict', {})\n+\n+            # XXX: Fragment resume is not accounted for here\n             if not ctx['live']:\n                 estimated_size = (\n                     (ctx['complete_frags_downloaded_bytes'] + frag_total_bytes)\n                     / (state['fragment_index'] + 1) * total_frags)\n-                state['total_bytes_estimate'] = estimated_size\n+                progress.total = estimated_size\n+                progress.update(s.get('downloaded_bytes'))\n+                state['total_bytes_estimate'] = progress.total\n+            else:\n+                progress.update(s.get('downloaded_bytes'))\n \n             if s['status'] == 'finished':\n                 state['fragment_index'] += 1\n                 ctx['fragment_index'] = state['fragment_index']\n-                state['downloaded_bytes'] += frag_total_bytes - ctx['prev_frag_downloaded_bytes']\n-                ctx['complete_frags_downloaded_bytes'] = state['downloaded_bytes']\n-                ctx['speed'] = state['speed'] = self.calc_speed(\n-                    ctx['fragment_started'], time_now, frag_total_bytes)\n-                ctx['fragment_started'] = time.time()\n-                ctx['prev_frag_downloaded_bytes'] = 0\n-            else:\n-                frag_downloaded_bytes = s['downloaded_bytes']\n-                state['downloaded_bytes'] += frag_downloaded_bytes - ctx['prev_frag_downloaded_bytes']\n-                ctx['speed'] = state['speed'] = self.calc_speed(\n-                    ctx['fragment_started'], time_now, frag_downloaded_bytes - ctx.get('frag_resume_len', 0))\n-                if not ctx['live']:\n-                    state['eta'] = self.calc_eta(state['speed'], estimated_size - state['downloaded_bytes'])\n-                ctx['prev_frag_downloaded_bytes'] = frag_downloaded_bytes\n+                progress.thread_reset()\n+\n+            state['downloaded_bytes'] = ctx['complete_frags_downloaded_bytes'] = progress.downloaded\n+            state['speed'] = ctx['speed'] = progress.speed.smooth\n+            state['eta'] = progress.eta.smooth\n+\n             self._hook_progress(state, info_dict)\n \n         ctx['dl'].add_progress_hook(frag_progress_hook)\n \n-        return start\n+        return ctx['started']\n \n     def _finish_frag_download(self, ctx, info_dict):\n         ctx['dest_stream'].close()\n@@ -500,7 +491,6 @@ def _download_fragment(fragment):\n                 download_fragment(fragment, ctx_copy)\n                 return fragment, fragment['frag_index'], ctx_copy.get('fragment_filename_sanitized')\n \n-            self.report_warning('The download speed shown is only of one thread. This is a known issue')\n             with tpe or concurrent.futures.ThreadPoolExecutor(max_workers) as pool:\n                 try:\n                     for fragment, frag_index, frag_filename in pool.map(_download_fragment, fragments):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/downloader/niconico.py",
            "diff": "diff --git a/yt_dlp/downloader/niconico.py b/yt_dlp/downloader/niconico.py\nindex 5720f6eb..fef8bff7 100644\n--- a/yt_dlp/downloader/niconico.py\n+++ b/yt_dlp/downloader/niconico.py\n@@ -6,7 +6,7 @@\n from .common import FileDownloader\n from .external import FFmpegFD\n from ..networking import Request\n-from ..utils import DownloadError, WebSocketsWrapper, str_or_none, try_get\n+from ..utils import DownloadError, str_or_none, try_get\n \n \n class NiconicoDmcFD(FileDownloader):\n@@ -64,7 +64,6 @@ def real_download(self, filename, info_dict):\n         ws_url = info_dict['url']\n         ws_extractor = info_dict['ws']\n         ws_origin_host = info_dict['origin']\n-        cookies = info_dict.get('cookies')\n         live_quality = info_dict.get('live_quality', 'high')\n         live_latency = info_dict.get('live_latency', 'high')\n         dl = FFmpegFD(self.ydl, self.params or {})\n@@ -76,12 +75,7 @@ def real_download(self, filename, info_dict):\n \n         def communicate_ws(reconnect):\n             if reconnect:\n-                ws = WebSocketsWrapper(ws_url, {\n-                    'Cookies': str_or_none(cookies) or '',\n-                    'Origin': f'https://{ws_origin_host}',\n-                    'Accept': '*/*',\n-                    'User-Agent': self.params['http_headers']['User-Agent'],\n-                })\n+                ws = self.ydl.urlopen(Request(ws_url, headers={'Origin': f'https://{ws_origin_host}'}))\n                 if self.ydl.params.get('verbose', False):\n                     self.to_screen('[debug] Sending startWatching request')\n                 ws.send(json.dumps({\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/_extractors.py",
            "diff": "diff --git a/yt_dlp/extractor/_extractors.py b/yt_dlp/extractor/_extractors.py\nindex 22d6d547..e5c6b219 100644\n--- a/yt_dlp/extractor/_extractors.py\n+++ b/yt_dlp/extractor/_extractors.py\n@@ -77,16 +77,23 @@\n     WyborczaPodcastIE,\n     WyborczaVideoIE,\n )\n-from .airmozilla import AirMozillaIE\n from .airtv import AirTVIE\n from .aitube import AitubeKZVideoIE\n from .aljazeera import AlJazeeraIE\n+from .allstar import (\n+    AllstarIE,\n+    AllstarProfileIE,\n+)\n from .alphaporno import AlphaPornoIE\n-from .amara import AmaraIE\n+from .altcensored import (\n+    AltCensoredIE,\n+    AltCensoredChannelIE,\n+)\n from .alura import (\n     AluraIE,\n     AluraCourseIE\n )\n+from .amara import AmaraIE\n from .amcnetworks import AMCNetworksIE\n from .amazon import (\n     AmazonStoreIE,\n@@ -122,14 +129,13 @@\n from .archiveorg import (\n     ArchiveOrgIE,\n     YoutubeWebArchiveIE,\n-    VLiveWebArchiveIE,\n )\n from .arcpublishing import ArcPublishingIE\n from .arkena import ArkenaIE\n from .ard import (\n     ARDBetaMediathekIE,\n+    ARDMediathekCollectionIE,\n     ARDIE,\n-    ARDMediathekIE,\n )\n from .arte import (\n     ArteTVIE,\n@@ -138,13 +144,8 @@\n     ArteTVCategoryIE,\n )\n from .arnes import ArnesIE\n-from .asiancrush import (\n-    AsianCrushIE,\n-    AsianCrushPlaylistIE,\n-)\n from .atresplayer import AtresPlayerIE\n from .atscaleconf import AtScaleConfEventIE\n-from .atttechchannel import ATTTechChannelIE\n from .atvat import ATVAtIE\n from .audimedia import AudiMediaIE\n from .audioboom import AudioBoomIE\n@@ -165,6 +166,7 @@\n     AWAANLiveIE,\n     AWAANSeasonIE,\n )\n+from .axs import AxsIE\n from .azmedien import AZMedienIE\n from .baidu import BaiduVideoIE\n from .banbye import (\n@@ -216,6 +218,8 @@\n     BiliBiliBangumiIE,\n     BiliBiliBangumiSeasonIE,\n     BiliBiliBangumiMediaIE,\n+    BilibiliCheeseIE,\n+    BilibiliCheeseSeasonIE,\n     BiliBiliSearchIE,\n     BilibiliCategoryIE,\n     BilibiliAudioIE,\n@@ -223,7 +227,11 @@\n     BiliBiliPlayerIE,\n     BilibiliSpaceVideoIE,\n     BilibiliSpaceAudioIE,\n-    BilibiliSpacePlaylistIE,\n+    BilibiliCollectionListIE,\n+    BilibiliSeriesListIE,\n+    BilibiliFavoritesListIE,\n+    BilibiliWatchlaterIE,\n+    BilibiliPlaylistIE,\n     BiliIntlIE,\n     BiliIntlSeriesIE,\n     BiliLiveIE,\n@@ -233,11 +241,6 @@\n     BitChuteIE,\n     BitChuteChannelIE,\n )\n-from .bitwave import (\n-    BitwaveReplayIE,\n-    BitwaveStreamIE,\n-)\n-from .biqle import BIQLEIE\n from .blackboardcollaborate import BlackboardCollaborateIE\n from .bleacherreport import (\n     BleacherReportIE,\n@@ -252,10 +255,7 @@\n from .box import BoxIE\n from .boxcast import BoxCastVideoIE\n from .bpb import BpbIE\n-from .br import (\n-    BRIE,\n-    BRMediathekIE,\n-)\n+from .br import BRIE\n from .bravotv import BravoTVIE\n from .brainpop import (\n     BrainPOPIE,\n@@ -265,14 +265,18 @@\n     BrainPOPFrIE,\n     BrainPOPIlIE,\n )\n-from .breakcom import BreakIE\n from .breitbart import BreitBartIE\n from .brightcove import (\n     BrightcoveLegacyIE,\n     BrightcoveNewIE,\n )\n+from .brilliantpala import (\n+    BrilliantpalaElearnIE,\n+    BrilliantpalaClassesIE,\n+)\n from .businessinsider import BusinessInsiderIE\n from .bundesliga import BundesligaIE\n+from .bundestag import BundestagIE\n from .buzzfeed import BuzzFeedIE\n from .byutv import BYUtvIE\n from .c56 import C56IE\n@@ -291,18 +295,16 @@\n from .cammodels import CamModelsIE\n from .camsoda import CamsodaIE\n from .camtasia import CamtasiaEmbedIE\n-from .camwithher import CamWithHerIE\n+from .canal1 import Canal1IE\n from .canalalpha import CanalAlphaIE\n from .canalplus import CanalplusIE\n from .canalc2 import Canalc2IE\n-from .carambatv import (\n-    CarambaTVIE,\n-    CarambaTVPageIE,\n-)\n+from .caracoltv import CaracolTvPlayIE\n from .cartoonnetwork import CartoonNetworkIE\n from .cbc import (\n     CBCIE,\n     CBCPlayerIE,\n+    CBCPlayerPlaylistIE,\n     CBCGemIE,\n     CBCGemPlaylistIE,\n     CBCGemLiveIE,\n@@ -336,7 +338,6 @@\n from .cellebrite import CellebriteIE\n from .ceskatelevize import CeskaTelevizeIE\n from .cgtn import CGTNIE\n-from .channel9 import Channel9IE\n from .charlierose import CharlieRoseIE\n from .chaturbate import ChaturbateIE\n from .chilloutzone import ChilloutzoneIE\n@@ -344,13 +345,12 @@\n     ChingariIE,\n     ChingariUserIE,\n )\n-from .chirbit import (\n-    ChirbitIE,\n-    ChirbitProfileIE,\n-)\n-from .cinchcast import CinchcastIE\n from .cinemax import CinemaxIE\n from .cinetecamilano import CinetecaMilanoIE\n+from .cineverse import (\n+    CineverseIE,\n+    CineverseDetailsIE,\n+)\n from .ciscolive import (\n     CiscoLiveSessionIE,\n     CiscoLiveSearchIE,\n@@ -361,10 +361,8 @@\n from .cliphunter import CliphunterIE\n from .clippit import ClippitIE\n from .cliprs import ClipRsIE\n-from .clipsyndicate import ClipsyndicateIE\n from .closertotruth import CloserToTruthIE\n from .cloudflarestream import CloudflareStreamIE\n-from .cloudy import CloudyIE\n from .clubic import ClubicIE\n from .clyp import ClypIE\n from .cmt import CMTIE\n@@ -431,7 +429,6 @@\n     DacastVODIE,\n     DacastPlaylistIE,\n )\n-from .daftsex import DaftsexIE\n from .dailymail import DailyMailIE\n from .dailymotion import (\n     DailymotionIE,\n@@ -468,7 +465,6 @@\n from .dfb import DFBIE\n from .dhm import DHMIE\n from .digg import DiggIE\n-from .dotsub import DotsubIE\n from .douyutv import (\n     DouyuShowIE,\n     DouyuTVIE,\n@@ -515,7 +511,6 @@\n     DubokuPlaylistIE\n )\n from .dumpert import DumpertIE\n-from .defense import DefenseGouvFrIE\n from .deuxm import (\n     DeuxMIE,\n     DeuxMNewsIE\n@@ -530,6 +525,7 @@\n     DropoutSeasonIE,\n     DropoutIE\n )\n+from .duoplay import DuoplayIE\n from .dw import (\n     DWIE,\n     DWArticleIE,\n@@ -537,12 +533,10 @@\n from .eagleplatform import EaglePlatformIE, ClipYouEmbedIE\n from .ebaumsworld import EbaumsWorldIE\n from .ebay import EbayIE\n-from .echomsk import EchoMskIE\n from .egghead import (\n     EggheadCourseIE,\n     EggheadLessonIE,\n )\n-from .ehow import EHowIE\n from .eighttracks import EightTracksIE\n from .einthusan import EinthusanIE\n from .eitb import EitbIE\n@@ -555,14 +549,17 @@\n )\n from .elonet import ElonetIE\n from .elpais import ElPaisIE\n+from .eltrecetv import ElTreceTVIE\n from .embedly import EmbedlyIE\n-from .engadget import EngadgetIE\n from .epicon import (\n     EpiconIE,\n     EpiconSeriesIE,\n )\n+from .epidemicsound import EpidemicSoundIE\n+from .eplus import EplusIbIE\n from .epoch import EpochIE\n from .eporner import EpornerIE\n+from .erocast import ErocastIE\n from .eroprofile import (\n     EroProfileIE,\n     EroProfileAlbumIE,\n@@ -572,7 +569,6 @@\n     ERTFlixIE,\n     ERTWebtvEmbedIE,\n )\n-from .escapist import EscapistIE\n from .espn import (\n     ESPNIE,\n     WatchESPNIE,\n@@ -580,15 +576,12 @@\n     FiveThirtyEightIE,\n     ESPNCricInfoIE,\n )\n-from .esri import EsriVideoIE\n from .ettutv import EttuTvIE\n from .europa import EuropaIE, EuroParlWebstreamIE\n from .europeantour import EuropeanTourIE\n from .eurosport import EurosportIE\n from .euscreen import EUScreenIE\n-from .expotv import ExpoTVIE\n from .expressen import ExpressenIE\n-from .extremetube import ExtremeTubeIE\n from .eyedotv import EyedoTVIE\n from .facebook import (\n     FacebookIE,\n@@ -618,6 +611,10 @@\n from .firsttv import FirstTVIE\n from .fivetv import FiveTVIE\n from .flickr import FlickrIE\n+from .floatplane import (\n+    FloatplaneIE,\n+    FloatplaneChannelIE,\n+)\n from .folketinget import FolketingetIE\n from .footyroom import FootyRoomIE\n from .formula1 import Formula1IE\n@@ -627,16 +624,11 @@\n     PornerBrosIE,\n     FuxIE,\n )\n-from .fourzerostudio import (\n-    FourZeroStudioArchiveIE,\n-    FourZeroStudioClipIE,\n-)\n from .fox import FOXIE\n from .fox9 import (\n     FOX9IE,\n     FOX9NewsIE,\n )\n-from .foxgay import FoxgayIE\n from .foxnews import (\n     FoxNewsIE,\n     FoxNewsArticleIE,\n@@ -669,7 +661,6 @@\n )\n from .funk import FunkIE\n from .funker530 import Funker530IE\n-from .fusion import FusionIE\n from .fuyintv import FuyinTVIE\n from .gab import (\n     GabTVIE,\n@@ -700,7 +691,6 @@\n     GettrIE,\n     GettrStreamingIE,\n )\n-from .gfycat import GfycatIE\n from .giantbomb import GiantBombIE\n from .giga import GigaIE\n from .glide import GlideIE\n@@ -746,12 +736,10 @@\n from .hearthisat import HearThisAtIE\n from .heise import HeiseIE\n from .hellporno import HellPornoIE\n-from .helsinki import HelsinkiIE\n from .hgtv import HGTVComShowIE\n from .hketv import HKETVIE\n from .hidive import HiDiveIE\n from .historicfilms import HistoricFilmsIE\n-from .hitbox import HitboxIE, HitboxLiveIE\n from .hitrecord import HitRecordIE\n from .hollywoodreporter import (\n     HollywoodReporterIE,\n@@ -766,8 +754,6 @@\n     HotStarSeasonIE,\n     HotStarSeriesIE,\n )\n-from .howcast import HowcastIE\n-from .howstuffworks import HowStuffWorksIE\n from .hrefli import HrefLiRedirectIE\n from .hrfensehen import HRFernsehenIE\n from .hrti import (\n@@ -881,9 +867,18 @@\n     SangiinIE,\n )\n from .jeuxvideo import JeuxVideoIE\n+from .jiosaavn import (\n+    JioSaavnSongIE,\n+    JioSaavnAlbumIE,\n+)\n from .jove import JoveIE\n from .joj import JojIE\n+from .joqrag import JoqrAgIE\n from .jstream import JStreamIE\n+from .jtbc import (\n+    JTBCIE,\n+    JTBCProgramIE,\n+)\n from .jwplatform import JWPlatformIE\n from .kakao import KakaoIE\n from .kaltura import KalturaIE\n@@ -891,7 +886,6 @@\n from .kankanews import KankaNewsIE\n from .karaoketv import KaraoketvIE\n from .karrierevideos import KarriereVideosIE\n-from .keezmovies import KeezMoviesIE\n from .kelbyone import KelbyOneIE\n from .khanacademy import (\n     KhanAcademyIE,\n@@ -926,20 +920,16 @@\n     LA7PodcastEpisodeIE,\n     LA7PodcastIE,\n )\n-from .laola1tv import (\n-    Laola1TvEmbedIE,\n-    Laola1TvIE,\n-    EHFTVIE,\n-    ITTFIE,\n-)\n from .lastfm import (\n     LastFMIE,\n     LastFMPlaylistIE,\n     LastFMUserIE,\n )\n+from .laxarxames import LaXarxaMesIE\n from .lbry import (\n     LBRYIE,\n     LBRYChannelIE,\n+    LBRYPlaylistIE,\n )\n from .lci import LCIIE\n from .lcp import (\n@@ -984,7 +974,6 @@\n     LinkedInLearningIE,\n     LinkedInLearningCourseIE,\n )\n-from .linuxacademy import LinuxAcademyIE\n from .liputan6 import Liputan6IE\n from .listennotes import ListenNotesIE\n from .litv import LiTVIE\n@@ -1012,7 +1001,7 @@\n     LyndaIE,\n     LyndaCourseIE\n )\n-from .m6 import M6IE\n+from .maariv import MaarivIE\n from .magellantv import MagellanTVIE\n from .magentamusik360 import MagentaMusik360IE\n from .mailru import (\n@@ -1040,6 +1029,7 @@\n from .massengeschmacktv import MassengeschmackTVIE\n from .masters import MastersIE\n from .matchtv import MatchTVIE\n+from .mbn import MBNIE\n from .mdr import MDRIE\n from .medaltv import MedalTVIE\n from .mediaite import MediaiteIE\n@@ -1062,10 +1052,7 @@\n from .megaphone import MegaphoneIE\n from .meipai import MeipaiIE\n from .melonvod import MelonVODIE\n-from .meta import METAIE\n-from .metacafe import MetacafeIE\n from .metacritic import MetacriticIE\n-from .mgoon import MgoonIE\n from .mgtv import MGTVIE\n from .miaopai import MiaoPaiIE\n from .microsoftstream import MicrosoftStreamIE\n@@ -1087,7 +1074,6 @@\n )\n from .ministrygrid import MinistryGridIE\n from .minoto import MinotoIE\n-from .miomio import MioMioIE\n from .mirrativ import (\n     MirrativIE,\n     MirrativUserIE,\n@@ -1111,14 +1097,9 @@\n     MLBArticleIE,\n )\n from .mlssoccer import MLSSoccerIE\n-from .mnet import MnetIE\n from .mocha import MochaVideoIE\n-from .moevideo import MoeVideoIE\n-from .mofosex import (\n-    MofosexIE,\n-    MofosexEmbedIE,\n-)\n from .mojvideo import MojvideoIE\n+from .monstercat import MonstercatIE\n from .morningstar import MorningstarIE\n from .motherless import (\n     MotherlessIE,\n@@ -1126,7 +1107,6 @@\n     MotherlessGalleryIE,\n )\n from .motorsport import MotorsportIE\n-from .movieclips import MovieClipsIE\n from .moviepilot import MoviepilotIE\n from .moview import MoviewPlayIE\n from .moviezine import MoviezineIE\n@@ -1151,18 +1131,12 @@\n     MusicdexArtistIE,\n     MusicdexPlaylistIE,\n )\n-from .mwave import MwaveIE, MwaveMeetGreetIE\n from .mxplayer import (\n     MxplayerIE,\n     MxplayerShowIE,\n )\n-from .mychannels import MyChannelsIE\n from .myspace import MySpaceIE, MySpaceAlbumIE\n from .myspass import MySpassIE\n-from .myvi import (\n-    MyviIE,\n-    MyviEmbedIE,\n-)\n from .myvideoge import MyVideoGeIE\n from .myvidster import MyVidsterIE\n from .mzaalo import MzaaloIE\n@@ -1211,6 +1185,7 @@\n from .ndtv import NDTVIE\n from .nebula import (\n     NebulaIE,\n+    NebulaClassIE,\n     NebulaSubscriptionsIE,\n     NebulaChannelIE,\n )\n@@ -1237,7 +1212,6 @@\n     NewgroundsUserIE,\n )\n from .newspicks import NewsPicksIE\n-from .newstube import NewstubeIE\n from .newsy import NewsyIE\n from .nextmedia import (\n     NextMediaIE,\n@@ -1272,7 +1246,6 @@\n     NickIE,\n     NickBrIE,\n     NickDeIE,\n-    NickNightIE,\n     NickRuIE,\n )\n from .niconico import (\n@@ -1291,18 +1264,20 @@\n     NineCNineMediaIE,\n     CPTwentyFourIE,\n )\n+from .niconicochannelplus import (\n+    NiconicoChannelPlusIE,\n+    NiconicoChannelPlusChannelVideosIE,\n+    NiconicoChannelPlusChannelLivesIE,\n+)\n from .ninegag import NineGagIE\n from .ninenow import NineNowIE\n from .nintendo import NintendoIE\n from .nitter import NitterIE\n-from .njpwworld import NJPWWorldIE\n from .nobelprize import NobelPrizeIE\n from .noice import NoicePodcastIE\n from .nonktube import NonkTubeIE\n from .noodlemagazine import NoodleMagazineIE\n from .noovo import NoovoIE\n-from .normalboots import NormalbootsIE\n-from .nosvideo import NosVideoIE\n from .nosnl import NOSNLArticleIE\n from .nova import (\n     NovaEmbedIE,\n@@ -1363,7 +1338,10 @@\n from .oktoberfesttv import OktoberfestTVIE\n from .olympics import OlympicsReplayIE\n from .on24 import On24IE\n-from .ondemandkorea import OnDemandKoreaIE\n+from .ondemandkorea import (\n+    OnDemandKoreaIE,\n+    OnDemandKoreaProgramIE,\n+)\n from .onefootball import OneFootballIE\n from .onenewsnz import OneNewsNZIE\n from .oneplace import OnePlacePodcastIE\n@@ -1374,10 +1352,6 @@\n     OnetPlIE,\n )\n from .onionstudios import OnionStudiosIE\n-from .ooyala import (\n-    OoyalaIE,\n-    OoyalaExternalIE,\n-)\n from .opencast import (\n     OpencastIE,\n     OpencastPlaylistIE,\n@@ -1392,6 +1366,7 @@\n     ORFTVthekIE,\n     ORFFM4StoryIE,\n     ORFRadioIE,\n+    ORFPodcastIE,\n     ORFIPTVIE,\n )\n from .outsidetv import OutsideTVIE\n@@ -1405,7 +1380,6 @@\n     PalcoMP3ArtistIE,\n     PalcoMP3VideoIE,\n )\n-from .pandoratv import PandoraTVIE\n from .panopto import (\n     PanoptoIE,\n     PanoptoListIE,\n@@ -1433,7 +1407,6 @@\n     PelotonIE,\n     PelotonLiveIE\n )\n-from .people import PeopleIE\n from .performgroup import PerformGroupIE\n from .periscope import (\n     PeriscopeIE,\n@@ -1444,6 +1417,7 @@\n from .phoenix import PhoenixIE\n from .photobucket import PhotobucketIE\n from .piapro import PiaproIE\n+from .piaulizaportal import PIAULIZAPortalIE\n from .picarto import (\n     PicartoIE,\n     PicartoVodIE,\n@@ -1464,13 +1438,10 @@\n     PlatziIE,\n     PlatziCourseIE,\n )\n-from .playfm import PlayFMIE\n from .playplustv import PlayPlusTVIE\n-from .plays import PlaysTVIE\n from .playstuff import PlayStuffIE\n from .playsuisse import PlaySuisseIE\n from .playtvak import PlaytvakIE\n-from .playvid import PlayvidIE\n from .playwire import PlaywireIE\n from .plutotv import PlutoTVIE\n from .pluralsight import (\n@@ -1501,9 +1472,8 @@\n from .popcorntimes import PopcorntimesIE\n from .popcorntv import PopcornTVIE\n from .porn91 import Porn91IE\n-from .porncom import PornComIE\n+from .pornbox import PornboxIE\n from .pornflip import PornFlipIE\n-from .pornhd import PornHdIE\n from .pornhub import (\n     PornHubIE,\n     PornHubUserIE,\n@@ -1514,12 +1484,11 @@\n from .pornotube import PornotubeIE\n from .pornovoisines import PornoVoisinesIE\n from .pornoxo import PornoXOIE\n-from .pornez import PornezIE\n from .puhutv import (\n     PuhuTVIE,\n     PuhuTVSerieIE,\n )\n-from .pr0gramm import Pr0grammStaticIE, Pr0grammIE\n+from .pr0gramm import Pr0grammIE\n from .prankcast import PrankCastIE\n from .premiershiprugby import PremiershipRugbyIE\n from .presstv import PressTVIE\n@@ -1552,10 +1521,20 @@\n     RadioCanadaIE,\n     RadioCanadaAudioVideoIE,\n )\n+from .radiocomercial import (\n+    RadioComercialIE,\n+    RadioComercialPlaylistIE,\n+)\n from .radiode import RadioDeIE\n from .radiojavan import RadioJavanIE\n-from .radiobremen import RadioBremenIE\n-from .radiofrance import FranceCultureIE, RadioFranceIE\n+from .radiofrance import (\n+    FranceCultureIE,\n+    RadioFranceIE,\n+    RadioFranceLiveIE,\n+    RadioFrancePodcastIE,\n+    RadioFranceProfileIE,\n+    RadioFranceProgramScheduleIE,\n+)\n from .radiozet import RadioZetPodcastIE\n from .radiokapital import (\n     RadioKapitalIE,\n@@ -1586,6 +1565,7 @@\n from .rbgtum import (\n     RbgTumIE,\n     RbgTumCourseIE,\n+    RbgTumNewCourseIE,\n )\n from .rcs import (\n     RCSIE,\n@@ -1598,7 +1578,6 @@\n     RCTIPlusTVIE,\n )\n from .rds import RDSIE\n-from .recurbate import RecurbateIE\n from .redbee import ParliamentLiveUKIE, RTBFIE\n from .redbulltv import (\n     RedBullTVIE,\n@@ -1622,7 +1601,7 @@\n from .reuters import ReutersIE\n from .reverbnation import ReverbNationIE\n from .rheinmaintv import RheinMainTVIE\n-from .rice import RICEIE\n+from .rinsefm import RinseFMIE\n from .rmcdecouverte import RMCDecouverteIE\n from .rockstargames import RockstarGamesIE\n from .rokfin import (\n@@ -1646,11 +1625,7 @@\n     RTLLuLiveIE,\n     RTLLuRadioIE,\n )\n-from .rtl2 import (\n-    RTL2IE,\n-    RTL2YouIE,\n-    RTL2YouSeriesIE,\n-)\n+from .rtl2 import RTL2IE\n from .rtnews import (\n     RTNewsIE,\n     RTDocumentryIE,\n@@ -1672,16 +1647,15 @@\n     RTVEInfantilIE,\n     RTVETelevisionIE,\n )\n-from .rtvnh import RTVNHIE\n from .rtvs import RTVSIE\n from .rtvslo import RTVSLOIE\n-from .ruhd import RUHDIE\n from .rule34video import Rule34VideoIE\n from .rumble import (\n     RumbleEmbedIE,\n     RumbleIE,\n     RumbleChannelIE,\n )\n+from .rudovideo import RudoVideoIE\n from .rutube import (\n     RutubeIE,\n     RutubeChannelIE,\n@@ -1699,8 +1673,8 @@\n     MegaTVComIE,\n     MegaTVComEmbedIE,\n )\n-from .ant1newsgr import (\n-    Ant1NewsGrWatchIE,\n+from .antenna import (\n+    AntennaGrWatchIE,\n     Ant1NewsGrArticleIE,\n     Ant1NewsGrEmbedIE,\n )\n@@ -1710,7 +1684,10 @@\n     RuvIE,\n     RuvSpilaIE\n )\n-from .s4c import S4CIE\n+from .s4c import (\n+    S4CIE,\n+    S4CSeriesIE\n+)\n from .safari import (\n     SafariIE,\n     SafariApiIE,\n@@ -1721,6 +1698,11 @@\n from .sapo import SapoIE\n from .savefrom import SaveFromIE\n from .sbs import SBSIE\n+from .sbscokr import (\n+    SBSCoKrIE,\n+    SBSCoKrAllvodProgramIE,\n+    SBSCoKrProgramsVodIE,\n+)\n from .screen9 import Screen9IE\n from .screencast import ScreencastIE\n from .screencastify import ScreencastifyIE\n@@ -1749,10 +1731,6 @@\n     ShahidIE,\n     ShahidShowIE,\n )\n-from .shared import (\n-    SharedIE,\n-    VivoIE,\n-)\n from .sharevideos import ShareVideosEmbedIE\n from .sibnet import SibnetEmbedIE\n from .shemaroome import ShemarooMeIE\n@@ -1791,7 +1769,10 @@\n from .slutload import SlutloadIE\n from .smotrim import SmotrimIE\n from .snotr import SnotrIE\n-from .sohu import SohuIE\n+from .sohu import (\n+    SohuIE,\n+    SohuVIE,\n+)\n from .sonyliv import (\n     SonyLIVIE,\n     SonyLIVSeriesIE,\n@@ -1827,7 +1808,6 @@\n     SpankBangIE,\n     SpankBangPlaylistIE,\n )\n-from .spankwire import SpankwireIE\n from .spiegel import SpiegelIE\n from .spike import (\n     BellatorIE,\n@@ -1862,6 +1842,8 @@\n from .stacommu import (\n     StacommuLiveIE,\n     StacommuVODIE,\n+    TheaterComplexTownVODIE,\n+    TheaterComplexTownPPVIE,\n )\n from .stanfordoc import StanfordOpenClassroomIE\n from .startv import StarTVIE\n@@ -1875,7 +1857,6 @@\n     StoryFireSeriesIE,\n )\n from .streamable import StreamableIE\n-from .streamcloud import StreamcloudIE\n from .streamcz import StreamCZIE\n from .streamff import StreamFFIE\n from .streetvoice import StreetVoiceIE\n@@ -1895,7 +1876,6 @@\n     SVTSeriesIE,\n )\n from .swearnet import SwearnetEpisodeIE\n-from .swrmediathek import SWRMediathekIE\n from .syvdk import SYVDKIE\n from .syfy import SyfyIE\n from .sztvhu import SztvHuIE\n@@ -1922,7 +1902,6 @@\n     ConanClassicIE,\n )\n from .teamtreehouse import TeamTreeHouseIE\n-from .techtalks import TechTalksIE\n from .ted import (\n     TedEmbedIE,\n     TedPlaylistIE,\n@@ -1957,10 +1936,17 @@\n     WeTvSeriesIE,\n )\n from .tennistv import TennisTVIE\n-from .tenplay import TenPlayIE\n+from .tenplay import (\n+    TenPlayIE,\n+    TenPlaySeasonIE,\n+)\n from .testurl import TestURLIE\n from .tf1 import TF1IE\n from .tfo import TFOIE\n+from .theguardian import (\n+    TheGuardianPodcastIE,\n+    TheGuardianPodcastPlaylistIE,\n+)\n from .theholetv import TheHoleTvIE\n from .theintercept import TheInterceptIE\n from .theplatform import (\n@@ -1969,13 +1955,8 @@\n )\n from .thestar import TheStarIE\n from .thesun import TheSunIE\n-from .theta import (\n-    ThetaVideoIE,\n-    ThetaStreamIE,\n-)\n from .theweatherchannel import TheWeatherChannelIE\n from .thisamericanlife import ThisAmericanLifeIE\n-from .thisav import ThisAVIE\n from .thisoldhouse import ThisOldHouseIE\n from .thisvid import (\n     ThisVidIE,\n@@ -1997,7 +1978,6 @@\n     TikTokLiveIE,\n     DouyinIE,\n )\n-from .tinypic import TinyPicIE\n from .tmz import TMZIE\n from .tnaflix import (\n     TNAFlixNetworkEmbedIE,\n@@ -2012,10 +1992,6 @@\n from .toggo import (\n     ToggoIE,\n )\n-from .tokentube import (\n-    TokentubeIE,\n-    TokentubeChannelIE\n-)\n from .tonline import TOnlineIE\n from .toongoggles import ToonGogglesIE\n from .toutv import TouTvIE\n@@ -2026,7 +2002,6 @@\n     TrillerUserIE,\n     TrillerShortIE,\n )\n-from .trilulilu import TriluliluIE\n from .trovo import (\n     TrovoIE,\n     TrovoVodIE,\n@@ -2051,8 +2026,6 @@\n     TuneInPodcastEpisodeIE,\n     TuneInShortenerIE,\n )\n-from .tunepk import TunePkIE\n-from .turbo import TurboIE\n from .tv2 import (\n     TV2IE,\n     TV2ArticleIE,\n@@ -2093,16 +2066,7 @@\n from .tviplayer import TVIPlayerIE\n from .tvland import TVLandIE\n from .tvn24 import TVN24IE\n-from .tvnet import TVNetIE\n from .tvnoe import TVNoeIE\n-from .tvnow import (\n-    TVNowIE,\n-    TVNowFilmIE,\n-    TVNowNewIE,\n-    TVNowSeasonIE,\n-    TVNowAnnualIE,\n-    TVNowShowIE,\n-)\n from .tvopengr import (\n     TVOpenGrWatchIE,\n     TVOpenGrEmbedIE,\n@@ -2120,7 +2084,6 @@\n )\n from .tvplayer import TVPlayerIE\n from .tweakers import TweakersIE\n-from .twentyfourvideo import TwentyFourVideoIE\n from .twentymin import TwentyMinutenIE\n from .twentythreevideo import TwentyThreeVideoIE\n from .twitcasting import (\n@@ -2169,7 +2132,6 @@\n from .umg import UMGDeIE\n from .unistra import UnistraIE\n from .unity import UnityIE\n-from .unscripted import UnscriptedNewsVideoIE\n from .unsupported import KnownDRMIE, KnownPiracyIE\n from .uol import UOLIE\n from .uplynk import (\n@@ -2188,7 +2150,6 @@\n from .utreon import UtreonIE\n from .varzesh3 import Varzesh3IE\n from .vbox7 import Vbox7IE\n-from .veehd import VeeHDIE\n from .veo import VeoIE\n from .veoh import (\n     VeohIE,\n@@ -2210,7 +2171,6 @@\n     ViceArticleIE,\n     ViceShowIE,\n )\n-from .vidbit import VidbitIE\n from .viddler import ViddlerIE\n from .videa import VideaIE\n from .videocampus_sachsen import (\n@@ -2238,6 +2198,7 @@\n     VidioLiveIE\n )\n from .vidlii import VidLiiIE\n+from .vidly import VidlyIE\n from .viewlift import (\n     ViewLiftIE,\n     ViewLiftEmbedIE,\n@@ -2260,7 +2221,6 @@\n     VimmIE,\n     VimmRecordingIE,\n )\n-from .vimple import VimpleIE\n from .vine import (\n     VineIE,\n     VineUserIE,\n@@ -2269,6 +2229,7 @@\n     VikiIE,\n     VikiChannelIE,\n )\n+from .viously import ViouslyIE\n from .viqeo import ViqeoIE\n from .viu import (\n     ViuIE,\n@@ -2284,10 +2245,8 @@\n     VKPlayLiveIE,\n )\n from .vocaroo import VocarooIE\n-from .vodlocker import VodlockerIE\n from .vodpl import VODPlIE\n from .vodplatform import VODPlatformIE\n-from .voicerepublic import VoiceRepublicIE\n from .voicy import (\n     VoicyIE,\n     VoicyChannelIE,\n@@ -2307,23 +2266,13 @@\n     KetnetIE,\n     DagelijkseKostIE,\n )\n-from .vrak import VrakIE\n-from .vrv import (\n-    VRVIE,\n-    VRVSeriesIE,\n-)\n-from .vshare import VShareIE\n from .vtm import VTMIE\n from .medialaan import MedialaanIE\n from .vuclip import VuClipIE\n-from .vupload import VuploadIE\n from .vvvvid import (\n     VVVVIDIE,\n     VVVVIDShowIE,\n )\n-from .vyborymos import VyboryMosIE\n-from .vzaar import VzaarIE\n-from .wakanim import WakanimIE\n from .walla import WallaIE\n from .washingtonpost import (\n     WashingtonPostIE,\n@@ -2335,8 +2284,6 @@\n     WASDTVClipIE,\n )\n from .wat import WatIE\n-from .watchbox import WatchBoxIE\n-from .watchindianporn import WatchIndianPornIE\n from .wdr import (\n     WDRIE,\n     WDRPageIE,\n@@ -2354,7 +2301,8 @@\n )\n from .weibo import (\n     WeiboIE,\n-    WeiboMobileIE\n+    WeiboVideoIE,\n+    WeiboUserIE,\n )\n from .weiqitv import WeiqiTVIE\n from .weverse import (\n@@ -2369,7 +2317,6 @@\n from .weyyak import WeyyakIE\n from .whyp import WhypIE\n from .wikimedia import WikimediaIE\n-from .willow import WillowIE\n from .wimbledon import WimbledonIE\n from .wimtv import WimTVIE\n from .whowatch import WhoWatchIE\n@@ -2403,7 +2350,6 @@\n     WykopPostCommentIE,\n )\n from .xanimu import XanimuIE\n-from .xbef import XBefIE\n from .xboxclips import XboxClipsIE\n from .xfileshare import XFileShareIE\n from .xhamster import (\n@@ -2419,8 +2365,6 @@\n from .xminus import XMinusIE\n from .xnxx import XNXXIE\n from .xstream import XstreamIE\n-from .xtube import XTubeUserIE, XTubeIE\n-from .xuite import XuiteIE\n from .xvideos import (\n     XVideosIE,\n     XVideosQuickiesIE\n@@ -2450,10 +2394,7 @@\n     YappyIE,\n     YappyProfileIE,\n )\n-from .yesjapan import YesJapanIE\n-from .yinyuetai import YinYueTaiIE\n from .yle_areena import YleAreenaIE\n-from .ynet import YnetIE\n from .youjizz import YouJizzIE\n from .youku import (\n     YoukuIE,\n@@ -2529,6 +2470,9 @@\n     ZingMp3ChartMusicVideoIE,\n     ZingMp3UserIE,\n     ZingMp3HubIE,\n+    ZingMp3LiveRadioIE,\n+    ZingMp3PodcastEpisodeIE,\n+    ZingMp3PodcastIE,\n )\n from .zoom import ZoomIE\n from .zype import ZypeIE\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/abc.py",
            "diff": "diff --git a/yt_dlp/extractor/abc.py b/yt_dlp/extractor/abc.py\nindex f56133eb..a7b614ca 100644\n--- a/yt_dlp/extractor/abc.py\n+++ b/yt_dlp/extractor/abc.py\n@@ -16,6 +16,7 @@\n     try_get,\n     unescapeHTML,\n     update_url_query,\n+    url_or_none,\n )\n \n \n@@ -180,20 +181,103 @@ class ABCIViewIE(InfoExtractor):\n     _VALID_URL = r'https?://iview\\.abc\\.net\\.au/(?:[^/]+/)*video/(?P<id>[^/?#]+)'\n     _GEO_COUNTRIES = ['AU']\n \n-    # ABC iview programs are normally available for 14 days only.\n     _TESTS = [{\n+        'url': 'https://iview.abc.net.au/show/utopia/series/1/video/CO1211V001S00',\n+        'md5': '52a942bfd7a0b79a6bfe9b4ce6c9d0ed',\n+        'info_dict': {\n+            'id': 'CO1211V001S00',\n+            'ext': 'mp4',\n+            'title': 'Series 1 Ep 1 Wood For The Trees',\n+            'series': 'Utopia',\n+            'description': 'md5:0cfb2c183c1b952d1548fd65c8a95c00',\n+            'upload_date': '20230726',\n+            'uploader_id': 'abc1',\n+            'series_id': 'CO1211V',\n+            'episode_id': 'CO1211V001S00',\n+            'season_number': 1,\n+            'season': 'Season 1',\n+            'episode_number': 1,\n+            'episode': 'Wood For The Trees',\n+            'thumbnail': 'https://cdn.iview.abc.net.au/thumbs/i/co/CO1211V001S00_5ad8353f4df09_1280.jpg',\n+            'timestamp': 1690403700,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        'note': 'No episode name',\n         'url': 'https://iview.abc.net.au/show/gruen/series/11/video/LE1927H001S00',\n         'md5': '67715ce3c78426b11ba167d875ac6abf',\n         'info_dict': {\n             'id': 'LE1927H001S00',\n             'ext': 'mp4',\n-            'title': \"Series 11 Ep 1\",\n-            'series': \"Gruen\",\n+            'title': 'Series 11 Ep 1',\n+            'series': 'Gruen',\n             'description': 'md5:52cc744ad35045baf6aded2ce7287f67',\n             'upload_date': '20190925',\n             'uploader_id': 'abc1',\n+            'series_id': 'LE1927H',\n+            'episode_id': 'LE1927H001S00',\n+            'season_number': 11,\n+            'season': 'Season 11',\n+            'episode_number': 1,\n+            'episode': 'Episode 1',\n+            'thumbnail': 'https://cdn.iview.abc.net.au/thumbs/i/le/LE1927H001S00_5d954fbd79e25_1280.jpg',\n             'timestamp': 1569445289,\n         },\n+        'expected_warnings': ['Ignoring subtitle tracks found in the HLS manifest'],\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        'note': 'No episode number',\n+        'url': 'https://iview.abc.net.au/show/four-corners/series/2022/video/NC2203H039S00',\n+        'md5': '77cb7d8434440e3b28fbebe331c2456a',\n+        'info_dict': {\n+            'id': 'NC2203H039S00',\n+            'ext': 'mp4',\n+            'title': 'Series 2022 Locking Up Kids',\n+            'series': 'Four Corners',\n+            'description': 'md5:54829ca108846d1a70e1fcce2853e720',\n+            'upload_date': '20221114',\n+            'uploader_id': 'abc1',\n+            'series_id': 'NC2203H',\n+            'episode_id': 'NC2203H039S00',\n+            'season_number': 2022,\n+            'season': 'Season 2022',\n+            'episode_number': None,\n+            'episode': 'Locking Up Kids',\n+            'thumbnail': 'https://cdn.iview.abc.net.au/thumbs/i/nc/NC2203H039S00_636d8a0944a22_1920.jpg',\n+            'timestamp': 1668460497,\n+\n+        },\n+        'expected_warnings': ['Ignoring subtitle tracks found in the HLS manifest'],\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        'note': 'No episode name or number',\n+        'url': 'https://iview.abc.net.au/show/landline/series/2021/video/RF2004Q043S00',\n+        'md5': '2e17dec06b13cc81dc119d2565289396',\n+        'info_dict': {\n+            'id': 'RF2004Q043S00',\n+            'ext': 'mp4',\n+            'title': 'Series 2021',\n+            'series': 'Landline',\n+            'description': 'md5:c9f30d9c0c914a7fd23842f6240be014',\n+            'upload_date': '20211205',\n+            'uploader_id': 'abc1',\n+            'series_id': 'RF2004Q',\n+            'episode_id': 'RF2004Q043S00',\n+            'season_number': 2021,\n+            'season': 'Season 2021',\n+            'episode_number': None,\n+            'episode': None,\n+            'thumbnail': 'https://cdn.iview.abc.net.au/thumbs/i/rf/RF2004Q043S00_61a950639dbc0_1920.jpg',\n+            'timestamp': 1638710705,\n+\n+        },\n+        'expected_warnings': ['Ignoring subtitle tracks found in the HLS manifest'],\n         'params': {\n             'skip_download': True,\n         },\n@@ -255,6 +339,8 @@ def tokenize_url(url, token):\n             'episode_number': int_or_none(self._search_regex(\n                 r'\\bEp\\s+(\\d+)\\b', title, 'episode number', default=None)),\n             'episode_id': house_number,\n+            'episode': self._search_regex(\n+                r'^(?:Series\\s+\\d+)?\\s*(?:Ep\\s+\\d+)?\\s*(.*)$', title, 'episode', default='') or None,\n             'uploader_id': video_params.get('channel'),\n             'formats': formats,\n             'subtitles': subtitles,\n@@ -294,6 +380,18 @@ class ABCIViewShowSeriesIE(InfoExtractor):\n             'noplaylist': True,\n             'skip_download': 'm3u8',\n         },\n+    }, {\n+        # 'videoEpisodes' is a dict with `items` key\n+        'url': 'https://iview.abc.net.au/show/7-30-mark-humphries-satire',\n+        'info_dict': {\n+            'id': '178458-0',\n+            'title': 'Episodes',\n+            'description': 'Satirist Mark Humphries brings his unique perspective on current political events for 7.30.',\n+            'series': '7.30 Mark Humphries Satire',\n+            'season': 'Episodes',\n+            'thumbnail': r're:^https?://cdn\\.iview\\.abc\\.net\\.au/thumbs/.*\\.jpg$'\n+        },\n+        'playlist_count': 15,\n     }]\n \n     def _real_extract(self, url):\n@@ -313,12 +411,14 @@ def _real_extract(self, url):\n         series = video_data['selectedSeries']\n         return {\n             '_type': 'playlist',\n-            'entries': [self.url_result(episode['shareUrl'])\n-                        for episode in series['_embedded']['videoEpisodes']],\n+            'entries': [self.url_result(episode_url, ABCIViewIE)\n+                        for episode_url in traverse_obj(series, (\n+                            '_embedded', 'videoEpisodes', (None, 'items'), ..., 'shareUrl', {url_or_none}))],\n             'id': series.get('id'),\n             'title': dict_get(series, ('title', 'displaySubtitle')),\n             'description': series.get('description'),\n             'series': dict_get(series, ('showTitle', 'displayTitle')),\n             'season': dict_get(series, ('title', 'displaySubtitle')),\n-            'thumbnail': series.get('thumbnail'),\n+            'thumbnail': traverse_obj(\n+                series, 'thumbnail', ('images', lambda _, v: v['name'] == 'seriesThumbnail', 'url'), get_all=False),\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/abematv.py",
            "diff": "diff --git a/yt_dlp/extractor/abematv.py b/yt_dlp/extractor/abematv.py\nindex 163b83c6..57ccb928 100644\n--- a/yt_dlp/extractor/abematv.py\n+++ b/yt_dlp/extractor/abematv.py\n@@ -12,7 +12,7 @@\n import urllib.request\n import urllib.response\n import uuid\n-\n+from ..utils.networking import clean_proxies\n from .common import InfoExtractor\n from ..aes import aes_ecb_decrypt\n from ..utils import (\n@@ -35,7 +35,10 @@ def add_opener(ydl, handler):  # FIXME: Create proper API in .networking\n     rh = ydl._request_director.handlers['Urllib']\n     if 'abematv-license' in rh._SUPPORTED_URL_SCHEMES:\n         return\n-    opener = rh._get_instance(cookiejar=ydl.cookiejar, proxies=ydl.proxies)\n+    headers = ydl.params['http_headers'].copy()\n+    proxies = ydl.proxies.copy()\n+    clean_proxies(proxies, headers)\n+    opener = rh._get_instance(cookiejar=ydl.cookiejar, proxies=proxies)\n     assert isinstance(opener, urllib.request.OpenerDirector)\n     opener.add_handler(handler)\n     rh._SUPPORTED_URL_SCHEMES = (*rh._SUPPORTED_URL_SCHEMES, 'abematv-license')\n@@ -208,7 +211,8 @@ class AbemaTVIE(AbemaTVBaseIE):\n             'id': '194-25_s2_p1',\n             'title': '\u7b2c1\u8a71 \u300c\u30c1\u30fc\u30ba\u30b1\u30fc\u30ad\u300d\u3000\u300c\u30e2\u30fc\u30cb\u30f3\u30b0\u518d\u3073\u300d',\n             'series': '\u7570\u4e16\u754c\u98df\u5802\uff12',\n-            'series_number': 2,\n+            'season': '\u30b7\u30fc\u30ba\u30f32',\n+            'season_number': 2,\n             'episode': '\u7b2c1\u8a71 \u300c\u30c1\u30fc\u30ba\u30b1\u30fc\u30ad\u300d\u3000\u300c\u30e2\u30fc\u30cb\u30f3\u30b0\u518d\u3073\u300d',\n             'episode_number': 1,\n         },\n@@ -344,12 +348,12 @@ def _real_extract(self, url):\n                     )?\n                 ''', r'\\1', og_desc)\n \n-        # canonical URL may contain series and episode number\n+        # canonical URL may contain season and episode number\n         mobj = re.search(r's(\\d+)_p(\\d+)$', canonical_url)\n         if mobj:\n             seri = int_or_none(mobj.group(1), default=float('inf'))\n             epis = int_or_none(mobj.group(2), default=float('inf'))\n-            info['series_number'] = seri if seri < 100 else None\n+            info['season_number'] = seri if seri < 100 else None\n             # some anime like Detective Conan (though not available in AbemaTV)\n             # has more than 1000 episodes (1026 as of 2021/11/15)\n             info['episode_number'] = epis if epis < 2000 else None\n@@ -378,7 +382,7 @@ def _real_extract(self, url):\n                 self.report_warning('This is a premium-only stream')\n             info.update(traverse_obj(api_response, {\n                 'series': ('series', 'title'),\n-                'season': ('season', 'title'),\n+                'season': ('season', 'name'),\n                 'season_number': ('season', 'sequence'),\n                 'episode_number': ('episode', 'number'),\n             }))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/aenetworks.py",
            "diff": "diff --git a/yt_dlp/extractor/aenetworks.py b/yt_dlp/extractor/aenetworks.py\nindex f049a0fb..63a0532e 100644\n--- a/yt_dlp/extractor/aenetworks.py\n+++ b/yt_dlp/extractor/aenetworks.py\n@@ -121,11 +121,21 @@ class AENetworksIE(AENetworksBaseIE):\n         'info_dict': {\n             'id': '22253814',\n             'ext': 'mp4',\n-            'title': 'Winter is Coming',\n-            'description': 'md5:641f424b7a19d8e24f26dea22cf59d74',\n+            'title': 'Winter Is Coming',\n+            'description': 'md5:a40e370925074260b1c8a633c632c63a',\n             'timestamp': 1338306241,\n             'upload_date': '20120529',\n             'uploader': 'AENE-NEW',\n+            'duration': 2592.0,\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'chapters': 'count:5',\n+            'tags': 'count:14',\n+            'categories': ['Mountain Men'],\n+            'episode_number': 1,\n+            'episode': 'Episode 1',\n+            'season': 'Season 1',\n+            'season_number': 1,\n+            'series': 'Mountain Men',\n         },\n         'params': {\n             # m3u8 download\n@@ -143,6 +153,15 @@ class AENetworksIE(AENetworksBaseIE):\n             'timestamp': 1452634428,\n             'upload_date': '20160112',\n             'uploader': 'AENE-NEW',\n+            'duration': 1277.695,\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'chapters': 'count:4',\n+            'tags': 'count:23',\n+            'episode': 'Episode 1',\n+            'episode_number': 1,\n+            'season': 'Season 9',\n+            'season_number': 9,\n+            'series': 'Duck Dynasty',\n         },\n         'params': {\n             # m3u8 download\n@@ -338,6 +357,7 @@ class BiographyIE(AENetworksBaseIE):\n             'skip_download': True,\n         },\n         'add_ie': ['ThePlatform'],\n+        'skip': '404 Not Found',\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/airmozilla.py",
            "diff": "diff --git a/yt_dlp/extractor/airmozilla.py b/yt_dlp/extractor/airmozilla.py\ndeleted file mode 100644\nindex 669556b9..00000000\n--- a/yt_dlp/extractor/airmozilla.py\n+++ /dev/null\n@@ -1,63 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    parse_duration,\n-    parse_iso8601,\n-)\n-\n-\n-class AirMozillaIE(InfoExtractor):\n-    _VALID_URL = r'https?://air\\.mozilla\\.org/(?P<id>[0-9a-z-]+)/?'\n-    _TEST = {\n-        'url': 'https://air.mozilla.org/privacy-lab-a-meetup-for-privacy-minded-people-in-san-francisco/',\n-        'md5': '8d02f53ee39cf006009180e21df1f3ba',\n-        'info_dict': {\n-            'id': '6x4q2w',\n-            'ext': 'mp4',\n-            'title': 'Privacy Lab - a meetup for privacy minded people in San Francisco',\n-            'thumbnail': r're:https?://.*/poster\\.jpg',\n-            'description': 'Brings together privacy professionals and others interested in privacy at for-profits, non-profits, and NGOs in an effort to contribute to the state of the ecosystem...',\n-            'timestamp': 1422487800,\n-            'upload_date': '20150128',\n-            'location': 'SFO Commons',\n-            'duration': 3780,\n-            'view_count': int,\n-            'categories': ['Main', 'Privacy'],\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        webpage = self._download_webpage(url, display_id)\n-        video_id = self._html_search_regex(r'//vid\\.ly/(.*?)/embed', webpage, 'id')\n-\n-        embed_script = self._download_webpage('https://vid.ly/{0}/embed'.format(video_id), video_id)\n-        jwconfig = self._parse_json(self._search_regex(\n-            r'initCallback\\((.*)\\);', embed_script, 'metadata'), video_id)['config']\n-\n-        info_dict = self._parse_jwplayer_data(jwconfig, video_id)\n-        view_count = int_or_none(self._html_search_regex(\n-            r'Views since archived: ([0-9]+)',\n-            webpage, 'view count', fatal=False))\n-        timestamp = parse_iso8601(self._html_search_regex(\n-            r'<time datetime=\"(.*?)\"', webpage, 'timestamp', fatal=False))\n-        duration = parse_duration(self._search_regex(\n-            r'Duration:\\s*(\\d+\\s*hours?\\s*\\d+\\s*minutes?)',\n-            webpage, 'duration', fatal=False))\n-\n-        info_dict.update({\n-            'id': video_id,\n-            'title': self._og_search_title(webpage),\n-            'url': self._og_search_url(webpage),\n-            'display_id': display_id,\n-            'description': self._og_search_description(webpage),\n-            'timestamp': timestamp,\n-            'location': self._html_search_regex(r'Location: (.*)', webpage, 'location', default=None),\n-            'duration': duration,\n-            'view_count': view_count,\n-            'categories': re.findall(r'<a href=\".*?\" class=\"channel\">(.*?)</a>', webpage),\n-        })\n-\n-        return info_dict\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/allstar.py",
            "diff": "diff --git a/yt_dlp/extractor/allstar.py b/yt_dlp/extractor/allstar.py\nnew file mode 100644\nindex 00000000..87219f2f\n--- /dev/null\n+++ b/yt_dlp/extractor/allstar.py\n@@ -0,0 +1,253 @@\n+import functools\n+import json\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    OnDemandPagedList,\n+    int_or_none,\n+    join_nonempty,\n+    parse_qs,\n+    urljoin,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+_FIELDS = '''\n+    _id\n+    clipImageSource\n+    clipImageThumb\n+    clipLink\n+    clipTitle\n+    createdDate\n+    shareId\n+    user { _id }\n+    username\n+    views'''\n+\n+_EXTRA_FIELDS = '''\n+    clipLength\n+    clipSizeBytes'''\n+\n+_QUERIES = {\n+    'clip': '''query ($id: String!) {\n+        video: getClip(clipIdentifier: $id) {\n+            %s %s\n+        }\n+    }''' % (_FIELDS, _EXTRA_FIELDS),\n+    'montage': '''query ($id: String!) {\n+        video: getMontage(clipIdentifier: $id) {\n+            %s\n+        }\n+    }''' % _FIELDS,\n+    'Clips': '''query ($page: Int!, $user: String!, $game: Int) {\n+        videos: clips(search: createdDate, page: $page, user: $user, mobile: false, game: $game) {\n+            data { %s %s }\n+        }\n+    }''' % (_FIELDS, _EXTRA_FIELDS),\n+    'Montages': '''query ($page: Int!, $user: String!) {\n+        videos: montages(search: createdDate, page: $page, user: $user) {\n+            data { %s }\n+        }\n+    }''' % _FIELDS,\n+    'Mobile Clips': '''query ($page: Int!, $user: String!) {\n+        videos: clips(search: createdDate, page: $page, user: $user, mobile: true) {\n+            data { %s %s }\n+        }\n+    }''' % (_FIELDS, _EXTRA_FIELDS),\n+}\n+\n+\n+class AllstarBaseIE(InfoExtractor):\n+    @staticmethod\n+    def _parse_video_data(video_data):\n+        def media_url_or_none(path):\n+            return urljoin('https://media.allstar.gg/', path)\n+\n+        info = traverse_obj(video_data, {\n+            'id': ('_id', {str}),\n+            'display_id': ('shareId', {str}),\n+            'title': ('clipTitle', {str}),\n+            'url': ('clipLink', {media_url_or_none}),\n+            'thumbnails': (('clipImageThumb', 'clipImageSource'), {'url': {media_url_or_none}}),\n+            'duration': ('clipLength', {int_or_none}),\n+            'filesize': ('clipSizeBytes', {int_or_none}),\n+            'timestamp': ('createdDate', {functools.partial(int_or_none, scale=1000)}),\n+            'uploader': ('username', {str}),\n+            'uploader_id': ('user', '_id', {str}),\n+            'view_count': ('views', {int_or_none}),\n+        })\n+\n+        if info.get('id') and info.get('url'):\n+            basename = 'clip' if '/clips/' in info['url'] else 'montage'\n+            info['webpage_url'] = f'https://allstar.gg/{basename}?{basename}={info[\"id\"]}'\n+\n+        info.update({\n+            'extractor_key': AllstarIE.ie_key(),\n+            'extractor': AllstarIE.IE_NAME,\n+            'uploader_url': urljoin('https://allstar.gg/u/', info.get('uploader_id')),\n+        })\n+\n+        return info\n+\n+    def _call_api(self, query, variables, path, video_id=None, note=None):\n+        response = self._download_json(\n+            'https://a1.allstar.gg/graphql', video_id, note=note,\n+            headers={'content-type': 'application/json'},\n+            data=json.dumps({'variables': variables, 'query': query}).encode())\n+\n+        errors = traverse_obj(response, ('errors', ..., 'message', {str}))\n+        if errors:\n+            raise ExtractorError('; '.join(errors))\n+\n+        return traverse_obj(response, path)\n+\n+\n+class AllstarIE(AllstarBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?allstar\\.gg/(?P<type>(?:clip|montage))\\?(?P=type)=(?P<id>[^/?#&]+)'\n+\n+    _TESTS = [{\n+        'url': 'https://allstar.gg/clip?clip=64482c2da9eec30008a67d1b',\n+        'info_dict': {\n+            'id': '64482c2da9eec30008a67d1b',\n+            'title': '4K on Inferno',\n+            'url': 'md5:66befb5381eef0c9456026386c25fa55',\n+            'thumbnail': r're:https://media\\.allstar\\.gg/.+\\.(?:png|jpg)$',\n+            'uploader': 'chrk.',\n+            'ext': 'mp4',\n+            'duration': 20,\n+            'filesize': 21199257,\n+            'timestamp': 1682451501,\n+            'uploader_id': '62b8bdfc9021052f7905882d',\n+            'uploader_url': 'https://allstar.gg/u/62b8bdfc9021052f7905882d',\n+            'upload_date': '20230425',\n+            'view_count': int,\n+        }\n+    }, {\n+        'url': 'https://allstar.gg/clip?clip=8LJLY4JKB',\n+        'info_dict': {\n+            'id': '64a1ec6b887f4c0008dc50b8',\n+            'display_id': '8LJLY4JKB',\n+            'title': 'AK-47 3K on Mirage',\n+            'url': 'md5:dde224fd12f035c0e2529a4ae34c4283',\n+            'ext': 'mp4',\n+            'thumbnail': r're:https://media\\.allstar\\.gg/.+\\.(?:png|jpg)$',\n+            'duration': 16,\n+            'filesize': 30175859,\n+            'timestamp': 1688333419,\n+            'uploader': 'cherokee',\n+            'uploader_id': '62b8bdfc9021052f7905882d',\n+            'uploader_url': 'https://allstar.gg/u/62b8bdfc9021052f7905882d',\n+            'upload_date': '20230702',\n+            'view_count': int,\n+        }\n+    }, {\n+        'url': 'https://allstar.gg/montage?montage=643e64089da7e9363e1fa66c',\n+        'info_dict': {\n+            'id': '643e64089da7e9363e1fa66c',\n+            'display_id': 'APQLGM2IMXW',\n+            'title': 'cherokee Rapid Fire Snipers Montage',\n+            'url': 'md5:a3ee356022115db2b27c81321d195945',\n+            'thumbnail': r're:https://media\\.allstar\\.gg/.+\\.(?:png|jpg)$',\n+            'ext': 'mp4',\n+            'timestamp': 1681810448,\n+            'uploader': 'cherokee',\n+            'uploader_id': '62b8bdfc9021052f7905882d',\n+            'uploader_url': 'https://allstar.gg/u/62b8bdfc9021052f7905882d',\n+            'upload_date': '20230418',\n+            'view_count': int,\n+        }\n+    }, {\n+        'url': 'https://allstar.gg/montage?montage=RILJMH6QOS',\n+        'info_dict': {\n+            'id': '64a2697372ce3703de29e868',\n+            'display_id': 'RILJMH6QOS',\n+            'title': 'cherokee Rapid Fire Snipers Montage',\n+            'url': 'md5:d5672e6f88579730c2310a80fdbc4030',\n+            'thumbnail': r're:https://media\\.allstar\\.gg/.+\\.(?:png|jpg)$',\n+            'ext': 'mp4',\n+            'timestamp': 1688365434,\n+            'uploader': 'cherokee',\n+            'uploader_id': '62b8bdfc9021052f7905882d',\n+            'uploader_url': 'https://allstar.gg/u/62b8bdfc9021052f7905882d',\n+            'upload_date': '20230703',\n+            'view_count': int,\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        query_id, video_id = self._match_valid_url(url).group('type', 'id')\n+\n+        return self._parse_video_data(\n+            self._call_api(\n+                _QUERIES.get(query_id), {'id': video_id}, ('data', 'video'), video_id))\n+\n+\n+class AllstarProfileIE(AllstarBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?allstar\\.gg/(?:profile\\?user=|u/)(?P<id>[^/?#&]+)'\n+\n+    _TESTS = [{\n+        'url': 'https://allstar.gg/profile?user=62b8bdfc9021052f7905882d',\n+        'info_dict': {\n+            'id': '62b8bdfc9021052f7905882d-clips',\n+            'title': 'cherokee - Clips',\n+        },\n+        'playlist_mincount': 15\n+    }, {\n+        'url': 'https://allstar.gg/u/cherokee?game=730&view=Clips',\n+        'info_dict': {\n+            'id': '62b8bdfc9021052f7905882d-clips-730',\n+            'title': 'cherokee - Clips - 730',\n+        },\n+        'playlist_mincount': 15\n+    }, {\n+        'url': 'https://allstar.gg/u/62b8bdfc9021052f7905882d?view=Montages',\n+        'info_dict': {\n+            'id': '62b8bdfc9021052f7905882d-montages',\n+            'title': 'cherokee - Montages',\n+        },\n+        'playlist_mincount': 4\n+    }, {\n+        'url': 'https://allstar.gg/profile?user=cherokee&view=Mobile Clips',\n+        'info_dict': {\n+            'id': '62b8bdfc9021052f7905882d-mobile',\n+            'title': 'cherokee - Mobile Clips',\n+        },\n+        'playlist_mincount': 1\n+    }]\n+\n+    _PAGE_SIZE = 10\n+\n+    def _get_page(self, user_id, display_id, game, query, page_num):\n+        page_num += 1\n+\n+        for video_data in self._call_api(\n+                query, {\n+                    'user': user_id,\n+                    'page': page_num,\n+                    'game': game,\n+                }, ('data', 'videos', 'data'), display_id, f'Downloading page {page_num}'):\n+            yield self._parse_video_data(video_data)\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+        profile_data = self._download_json(\n+            urljoin('https://api.allstar.gg/v1/users/profile/', display_id), display_id)\n+        user_id = traverse_obj(profile_data, ('data', ('_id'), {str}))\n+        if not user_id:\n+            raise ExtractorError('Unable to extract the user id')\n+\n+        username = traverse_obj(profile_data, ('data', 'profile', ('username'), {str}))\n+        url_query = parse_qs(url)\n+        game = traverse_obj(url_query, ('game', 0, {int_or_none}))\n+        query_id = traverse_obj(url_query, ('view', 0), default='Clips')\n+\n+        if query_id not in ('Clips', 'Montages', 'Mobile Clips'):\n+            raise ExtractorError(f'Unsupported playlist URL type {query_id!r}')\n+\n+        return self.playlist_result(\n+            OnDemandPagedList(\n+                functools.partial(\n+                    self._get_page, user_id, display_id, game, _QUERIES.get(query_id)), self._PAGE_SIZE),\n+            playlist_id=join_nonempty(user_id, query_id.lower().split()[0], game),\n+            playlist_title=join_nonempty((username or display_id), query_id, game, delim=' - '))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/altcensored.py",
            "diff": "diff --git a/yt_dlp/extractor/altcensored.py b/yt_dlp/extractor/altcensored.py\nnew file mode 100644\nindex 00000000..0e1627bf\n--- /dev/null\n+++ b/yt_dlp/extractor/altcensored.py\n@@ -0,0 +1,96 @@\n+import re\n+\n+from .archiveorg import ArchiveOrgIE\n+from .common import InfoExtractor\n+from ..utils import (\n+    InAdvancePagedList,\n+    int_or_none,\n+    orderedSet,\n+    str_to_int,\n+    urljoin,\n+)\n+\n+\n+class AltCensoredIE(InfoExtractor):\n+    IE_NAME = 'altcensored'\n+    _VALID_URL = r'https?://(?:www\\.)?altcensored\\.com/(?:watch\\?v=|embed/)(?P<id>[^/?#]+)'\n+    _TESTS = [{\n+        'url': 'https://www.altcensored.com/watch?v=k0srjLSkga8',\n+        'info_dict': {\n+            'id': 'youtube-k0srjLSkga8',\n+            'ext': 'webm',\n+            'title': \"QUELLES SONT LES CONS\u00c9QUENCES DE L'HYPERSEXUALISATION DE LA SOCI\u00c9T\u00c9 ?\",\n+            'display_id': 'k0srjLSkga8.webm',\n+            'release_date': '20180403',\n+            'creator': 'Virginie Vota',\n+            'release_year': 2018,\n+            'upload_date': '20230318',\n+            'uploader': 'admin@altcensored.com',\n+            'description': 'md5:0b38a8fc04103579d5c1db10a247dc30',\n+            'timestamp': 1679161343,\n+            'track': 'k0srjLSkga8',\n+            'duration': 926.09,\n+            'thumbnail': 'https://archive.org/download/youtube-k0srjLSkga8/youtube-k0srjLSkga8.thumbs/k0srjLSkga8_000925.jpg',\n+            'view_count': int,\n+            'categories': ['News & Politics'],\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+\n+        return {\n+            '_type': 'url_transparent',\n+            'url': f'https://archive.org/details/youtube-{video_id}',\n+            'ie_key': ArchiveOrgIE.ie_key(),\n+            'view_count': str_to_int(self._html_search_regex(\n+                r'YouTube Views:(?:\\s|&nbsp;)*([\\d,]+)', webpage, 'view count', default=None)),\n+            'categories': self._html_search_regex(\n+                r'<a href=\"/category/\\d+\">\\s*\\n?\\s*([^<]+)</a>',\n+                webpage, 'category', default='').split() or None,\n+        }\n+\n+\n+class AltCensoredChannelIE(InfoExtractor):\n+    IE_NAME = 'altcensored:channel'\n+    _VALID_URL = r'https?://(?:www\\.)?altcensored\\.com/channel/(?!page|table)(?P<id>[^/?#]+)'\n+    _PAGE_SIZE = 24\n+    _TESTS = [{\n+        'url': 'https://www.altcensored.com/channel/UCFPTO55xxHqFqkzRZHu4kcw',\n+        'info_dict': {\n+            'title': 'Virginie Vota',\n+            'id': 'UCFPTO55xxHqFqkzRZHu4kcw',\n+        },\n+        'playlist_count': 91\n+    }, {\n+        'url': 'https://altcensored.com/channel/UC9CcJ96HKMWn0LZlcxlpFTw',\n+        'info_dict': {\n+            'title': 'yukikaze775',\n+            'id': 'UC9CcJ96HKMWn0LZlcxlpFTw',\n+        },\n+        'playlist_count': 4\n+    }]\n+\n+    def _real_extract(self, url):\n+        channel_id = self._match_id(url)\n+        webpage = self._download_webpage(\n+            url, channel_id, 'Download channel webpage', 'Unable to get channel webpage')\n+        title = self._html_search_meta('altcen_title', webpage, 'title', fatal=False)\n+        page_count = int_or_none(self._html_search_regex(\n+            r'<a[^>]+href=\"/channel/\\w+/page/(\\d+)\">(?:\\1)</a>',\n+            webpage, 'page count', default='1'))\n+\n+        def page_func(page_num):\n+            page_num += 1\n+            webpage = self._download_webpage(\n+                f'https://altcensored.com/channel/{channel_id}/page/{page_num}',\n+                channel_id, note=f'Downloading page {page_num}')\n+\n+            items = re.findall(r'<a[^>]+href=\"(/watch\\?v=[^\"]+)', webpage)\n+            return [self.url_result(urljoin('https://www.altcensored.com', path), AltCensoredIE)\n+                    for path in orderedSet(items)]\n+\n+        return self.playlist_result(\n+            InAdvancePagedList(page_func, page_count, self._PAGE_SIZE),\n+            playlist_id=channel_id, playlist_title=title)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/amazonminitv.py",
            "diff": "diff --git a/yt_dlp/extractor/amazonminitv.py b/yt_dlp/extractor/amazonminitv.py\nindex b57d985d..2c71c5ef 100644\n--- a/yt_dlp/extractor/amazonminitv.py\n+++ b/yt_dlp/extractor/amazonminitv.py\n@@ -22,8 +22,11 @@ def _call_api(self, asin, data=None, note=None):\n \n         resp = self._download_json(\n             f'https://www.amazon.in/minitv/api/web/{\"graphql\" if data else \"prs\"}',\n-            asin, note=note, headers={'Content-Type': 'application/json'},\n-            data=json.dumps(data).encode() if data else None,\n+            asin, note=note, headers={\n+                'Content-Type': 'application/json',\n+                'currentpageurl': '/',\n+                'currentplatform': 'dWeb'\n+            }, data=json.dumps(data).encode() if data else None,\n             query=None if data else {\n                 'deviceType': 'A1WMMUXPCUJL4N',\n                 'contentId': asin,\n@@ -46,7 +49,7 @@ class AmazonMiniTVIE(AmazonMiniTVBaseIE):\n             'ext': 'mp4',\n             'title': 'May I Kiss You?',\n             'language': 'Hindi',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'thumbnail': r're:^https?://.*\\.(?:jpg|png)$',\n             'description': 'md5:a549bfc747973e04feb707833474e59d',\n             'release_timestamp': 1644710400,\n             'release_date': '20220213',\n@@ -68,7 +71,7 @@ class AmazonMiniTVIE(AmazonMiniTVBaseIE):\n             'ext': 'mp4',\n             'title': 'Jahaan',\n             'language': 'Hindi',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n+            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n             'description': 'md5:05eb765a77bf703f322f120ec6867339',\n             'release_timestamp': 1647475200,\n             'release_date': '20220317',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/amcnetworks.py",
            "diff": "diff --git a/yt_dlp/extractor/amcnetworks.py b/yt_dlp/extractor/amcnetworks.py\nindex c58bc7bf..10bd021c 100644\n--- a/yt_dlp/extractor/amcnetworks.py\n+++ b/yt_dlp/extractor/amcnetworks.py\n@@ -26,6 +26,7 @@ class AMCNetworksIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n             # m3u8 download\n             'skip_download': True,\n         },\n+        'skip': '404 Not Found',\n     }, {\n         'url': 'http://www.bbcamerica.com/shows/the-hunt/full-episodes/season-1/episode-01-the-hardest-challenge',\n         'only_matching': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/antenna.py",
            "diff": "diff --git a/yt_dlp/extractor/antenna.py b/yt_dlp/extractor/antenna.py\nnew file mode 100644\nindex 00000000..c78717aa\n--- /dev/null\n+++ b/yt_dlp/extractor/antenna.py\n@@ -0,0 +1,143 @@\n+import urllib.parse\n+\n+from .common import InfoExtractor\n+from ..networking import HEADRequest\n+from ..utils import (\n+    ExtractorError,\n+    determine_ext,\n+    make_archive_id,\n+    scale_thumbnails_to_max_format_width,\n+)\n+\n+\n+class AntennaBaseIE(InfoExtractor):\n+    def _download_and_extract_api_data(self, video_id, netloc, cid=None):\n+        info = self._download_json(f'{self.http_scheme()}//{netloc}{self._API_PATH}',\n+                                   video_id, query={'cid': cid or video_id})\n+        if not info.get('url'):\n+            raise ExtractorError(f'No source found for {video_id}')\n+\n+        ext = determine_ext(info['url'])\n+        if ext == 'm3u8':\n+            formats, subs = self._extract_m3u8_formats_and_subtitles(info['url'], video_id, 'mp4')\n+        else:\n+            formats, subs = [{'url': info['url'], 'format_id': ext}], {}\n+\n+        thumbnails = scale_thumbnails_to_max_format_width(\n+            formats, [{'url': info['thumb']}], r'(?<=/imgHandler/)\\d+') if info.get('thumb') else []\n+        return {\n+            'id': video_id,\n+            'title': info.get('title'),\n+            'thumbnails': thumbnails,\n+            'formats': formats,\n+            'subtitles': subs,\n+        }\n+\n+\n+class AntennaGrWatchIE(AntennaBaseIE):\n+    IE_NAME = 'antenna:watch'\n+    IE_DESC = 'antenna.gr and ant1news.gr videos'\n+    _VALID_URL = r'https?://(?P<netloc>(?:www\\.)?(?:antenna|ant1news)\\.gr)/watch/(?P<id>\\d+)/'\n+    _API_PATH = '/templates/data/player'\n+\n+    _TESTS = [{\n+        'url': 'https://www.ant1news.gr/watch/1506168/ant1-news-09112021-stis-18-45',\n+        'md5': 'c472d9dd7cd233c63aff2ea42201cda6',\n+        'info_dict': {\n+            'id': '1506168',\n+            'ext': 'mp4',\n+            'title': 'md5:0ad00fa66ecf8aa233d26ab0dba7514a',\n+            'description': 'md5:18665af715a6dcfeac1d6153a44f16b0',\n+            'thumbnail': r're:https://ant1media\\.azureedge\\.net/imgHandler/\\d+/26d46bf6-8158-4f02-b197-7096c714b2de\\.jpg',\n+        },\n+    }, {\n+        'url': 'https://www.antenna.gr/watch/1643812/oi-prodotes-epeisodio-01',\n+        'md5': '8f6f7dd3b1dba4d835ba990e25f31243',\n+        'info_dict': {\n+            'id': '1643812',\n+            'ext': 'mp4',\n+            'format_id': 'mp4',\n+            'title': '\u039f\u0399 \u03a0\u03a1\u039f\u0394\u039f\u03a4\u0395\u03a3 \u2013 \u0395\u03a0\u0395\u0399\u03a3\u039f\u0394\u0399\u039f 01',\n+            'thumbnail': r're:https://ant1media\\.azureedge\\.net/imgHandler/\\d+/b3d63096-e72d-43c4-87a0-00d4363d242f\\.jpg',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, netloc = self._match_valid_url(url).group('id', 'netloc')\n+        webpage = self._download_webpage(url, video_id)\n+        info = self._download_and_extract_api_data(video_id, netloc)\n+        info['description'] = self._og_search_description(webpage, default=None)\n+        info['_old_archive_ids'] = [make_archive_id('Ant1NewsGrWatch', video_id)],\n+        return info\n+\n+\n+class Ant1NewsGrArticleIE(AntennaBaseIE):\n+    IE_NAME = 'ant1newsgr:article'\n+    IE_DESC = 'ant1news.gr articles'\n+    _VALID_URL = r'https?://(?:www\\.)?ant1news\\.gr/[^/]+/article/(?P<id>\\d+)/'\n+\n+    _TESTS = [{\n+        'url': 'https://www.ant1news.gr/afieromata/article/549468/o-tzeims-mpont-sta-meteora-oi-apeiles-kai-o-xesikomos-ton-kalogeron',\n+        'md5': '294f18331bb516539d72d85a82887dcc',\n+        'info_dict': {\n+            'id': '_xvg/m_cmbatw=',\n+            'ext': 'mp4',\n+            'title': 'md5:a93e8ecf2e4073bfdffcb38f59945411',\n+            'timestamp': 1603092840,\n+            'upload_date': '20201019',\n+            'thumbnail': 'https://ant1media.azureedge.net/imgHandler/640/756206d2-d640-40e2-b201-3555abdfc0db.jpg',\n+        },\n+    }, {\n+        'url': 'https://ant1news.gr/Society/article/620286/symmoria-anilikon-dikigoros-thymaton-ithelan-na-toys-apoteleiosoyn',\n+        'info_dict': {\n+            'id': '620286',\n+            'title': 'md5:91fe569e952e4d146485740ae927662b',\n+        },\n+        'playlist_mincount': 2,\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+        info = self._search_json_ld(webpage, video_id, expected_type='NewsArticle')\n+        embed_urls = list(Ant1NewsGrEmbedIE._extract_embed_urls(url, webpage))\n+        if not embed_urls:\n+            raise ExtractorError('no videos found for %s' % video_id, expected=True)\n+        return self.playlist_from_matches(\n+            embed_urls, video_id, info.get('title'), ie=Ant1NewsGrEmbedIE.ie_key(),\n+            video_kwargs={'url_transparent': True, 'timestamp': info.get('timestamp')})\n+\n+\n+class Ant1NewsGrEmbedIE(AntennaBaseIE):\n+    IE_NAME = 'ant1newsgr:embed'\n+    IE_DESC = 'ant1news.gr embedded videos'\n+    _BASE_PLAYER_URL_RE = r'(?:https?:)?//(?:[a-zA-Z0-9\\-]+\\.)?(?:antenna|ant1news)\\.gr/templates/pages/player'\n+    _VALID_URL = rf'{_BASE_PLAYER_URL_RE}\\?([^#]+&)?cid=(?P<id>[^#&]+)'\n+    _EMBED_REGEX = [rf'<iframe[^>]+?src=(?P<_q1>[\"\\'])(?P<url>{_BASE_PLAYER_URL_RE}\\?(?:(?!(?P=_q1)).)+)(?P=_q1)']\n+    _API_PATH = '/news/templates/data/jsonPlayer'\n+\n+    _TESTS = [{\n+        'url': 'https://www.antenna.gr/templates/pages/player?cid=3f_li_c_az_jw_y_u=&w=670&h=377',\n+        'md5': 'dfc58c3a11a5a9aad2ba316ed447def3',\n+        'info_dict': {\n+            'id': '3f_li_c_az_jw_y_u=',\n+            'ext': 'mp4',\n+            'title': 'md5:a30c93332455f53e1e84ae0724f0adf7',\n+            'thumbnail': 'https://ant1media.azureedge.net/imgHandler/640/bbe31201-3f09-4a4e-87f5-8ad2159fffe2.jpg',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+\n+        canonical_url = self._request_webpage(\n+            HEADRequest(url), video_id,\n+            note='Resolve canonical player URL',\n+            errnote='Could not resolve canonical player URL').url\n+        _, netloc, _, _, query, _ = urllib.parse.urlparse(canonical_url)\n+        cid = urllib.parse.parse_qs(query)['cid'][0]\n+\n+        return self._download_and_extract_api_data(video_id, netloc, cid=cid)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/aol.py",
            "diff": "diff --git a/yt_dlp/extractor/aol.py b/yt_dlp/extractor/aol.py\nindex 6949ca97..455f6679 100644\n--- a/yt_dlp/extractor/aol.py\n+++ b/yt_dlp/extractor/aol.py\n@@ -10,6 +10,7 @@\n \n \n class AolIE(YahooIE):  # XXX: Do not subclass from concrete IE\n+    _WORKING = False\n     IE_NAME = 'aol.com'\n     _VALID_URL = r'(?:aol-video:|https?://(?:www\\.)?aol\\.(?:com|ca|co\\.uk|de|jp)/video/(?:[^/]+/)*)(?P<id>\\d{9}|[0-9a-f]{24}|[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})'\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/archiveorg.py",
            "diff": "diff --git a/yt_dlp/extractor/archiveorg.py b/yt_dlp/extractor/archiveorg.py\nindex 2541cd6f..3bb6f2e3 100644\n--- a/yt_dlp/extractor/archiveorg.py\n+++ b/yt_dlp/extractor/archiveorg.py\n@@ -3,7 +3,6 @@\n import urllib.parse\n \n from .common import InfoExtractor\n-from .naver import NaverBaseIE\n from .youtube import YoutubeBaseInfoExtractor, YoutubeIE\n from ..compat import compat_urllib_parse_unquote\n from ..networking import HEADRequest\n@@ -53,7 +52,6 @@ class ArchiveOrgIE(InfoExtractor):\n             'creator': 'SRI International',\n             'uploader': 'laura@archive.org',\n             'thumbnail': r're:https://archive\\.org/download/.*\\.jpg',\n-            'release_year': 1968,\n             'display_id': 'XD300-23_68HighlightsAResearchCntAugHumanIntellect.cdr',\n             'track': 'XD300-23 68HighlightsAResearchCntAugHumanIntellect',\n \n@@ -135,7 +133,6 @@ class ArchiveOrgIE(InfoExtractor):\n             'album': '1977-05-08 - Barton Hall - Cornell University',\n             'release_date': '19770508',\n             'display_id': 'gd1977-05-08d01t07.flac',\n-            'release_year': 1977,\n             'track_number': 7,\n         },\n     }, {\n@@ -947,237 +944,3 @@ def _real_extract(self, url):\n         if not info.get('title'):\n             info['title'] = video_id\n         return info\n-\n-\n-class VLiveWebArchiveIE(InfoExtractor):\n-    IE_NAME = 'web.archive:vlive'\n-    IE_DESC = 'web.archive.org saved vlive videos'\n-    _VALID_URL = r'''(?x)\n-            (?:https?://)?web\\.archive\\.org/\n-            (?:web/)?(?:(?P<date>[0-9]{14})?[0-9A-Za-z_*]*/)?  # /web and the version index is optional\n-            (?:https?(?::|%3[Aa])//)?(?:\n-                (?:(?:www|m)\\.)?vlive\\.tv(?::(?:80|443))?/(?:video|embed)/(?P<id>[0-9]+)  # VLive URL\n-            )\n-        '''\n-    _TESTS = [{\n-        'url': 'https://web.archive.org/web/20221221144331/http://www.vlive.tv/video/1326',\n-        'md5': 'cc7314812855ce56de70a06a27314983',\n-        'info_dict': {\n-            'id': '1326',\n-            'ext': 'mp4',\n-            'title': \"Girl's Day's Broadcast\",\n-            'creator': \"Girl's Day\",\n-            'view_count': int,\n-            'uploader_id': 'muploader_a',\n-            'uploader_url': None,\n-            'uploader': None,\n-            'upload_date': '20150817',\n-            'thumbnail': r're:^https?://.*\\.(?:jpg|png)$',\n-            'timestamp': 1439816449,\n-            'like_count': int,\n-            'channel': 'Girl\\'s Day',\n-            'channel_id': 'FDF27',\n-            'comment_count': int,\n-            'release_timestamp': 1439818140,\n-            'release_date': '20150817',\n-            'duration': 1014,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://web.archive.org/web/20221221182103/http://www.vlive.tv/video/16937',\n-        'info_dict': {\n-            'id': '16937',\n-            'ext': 'mp4',\n-            'title': '\uccb8\ubc31\uc2dc \uac4d\ubc29',\n-            'creator': 'EXO',\n-            'view_count': int,\n-            'subtitles': 'mincount:12',\n-            'uploader_id': 'muploader_j',\n-            'uploader_url': 'http://vlive.tv',\n-            'uploader': None,\n-            'upload_date': '20161112',\n-            'thumbnail': r're:^https?://.*\\.(?:jpg|png)$',\n-            'timestamp': 1478923074,\n-            'like_count': int,\n-            'channel': 'EXO',\n-            'channel_id': 'F94BD',\n-            'comment_count': int,\n-            'release_timestamp': 1478924280,\n-            'release_date': '20161112',\n-            'duration': 906,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://web.archive.org/web/20221127190050/http://www.vlive.tv/video/101870',\n-        'info_dict': {\n-            'id': '101870',\n-            'ext': 'mp4',\n-            'title': '[\u24d3 xV] \u201c\ub808\ubca8\uc774\ub4e4 \ub9e4\ub825\uc5d0 \ubc18\ud574? \uc548 \ubc18\ud574?\u201d \uc6c0\uc9c1\uc774\ub294 HD \ud3ec\ud1a0 (\ub808\ub4dc\ubca8\ubcb3:Red Velvet)',\n-            'creator': 'Dispatch',\n-            'view_count': int,\n-            'subtitles': 'mincount:6',\n-            'uploader_id': 'V__FRA08071',\n-            'uploader_url': 'http://vlive.tv',\n-            'uploader': None,\n-            'upload_date': '20181130',\n-            'thumbnail': r're:^https?://.*\\.(?:jpg|png)$',\n-            'timestamp': 1543601327,\n-            'like_count': int,\n-            'channel': 'Dispatch',\n-            'channel_id': 'C796F3',\n-            'comment_count': int,\n-            'release_timestamp': 1543601040,\n-            'release_date': '20181130',\n-            'duration': 279,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }]\n-\n-    # The wayback machine has special timestamp and \"mode\" values:\n-    # timestamp:\n-    #   1 = the first capture\n-    #   2 = the last capture\n-    # mode:\n-    #   id_ = Identity - perform no alterations of the original resource, return it as it was archived.\n-    _WAYBACK_BASE_URL = 'https://web.archive.org/web/2id_/'\n-\n-    def _download_archived_page(self, url, video_id, *, timestamp='2', **kwargs):\n-        for retry in self.RetryManager():\n-            try:\n-                return self._download_webpage(f'https://web.archive.org/web/{timestamp}id_/{url}', video_id, **kwargs)\n-            except ExtractorError as e:\n-                if isinstance(e.cause, HTTPError) and e.cause.status == 404:\n-                    raise ExtractorError('Page was not archived', expected=True)\n-                retry.error = e\n-                continue\n-\n-    def _download_archived_json(self, url, video_id, **kwargs):\n-        page = self._download_archived_page(url, video_id, **kwargs)\n-        if not page:\n-            raise ExtractorError('Page was not archived', expected=True)\n-        else:\n-            return self._parse_json(page, video_id)\n-\n-    def _extract_formats_from_m3u8(self, m3u8_url, params, video_id):\n-        m3u8_doc = self._download_archived_page(m3u8_url, video_id, note='Downloading m3u8', query=params, fatal=False)\n-        if not m3u8_doc:\n-            return\n-\n-        # M3U8 document should be changed to archive domain\n-        m3u8_doc = m3u8_doc.splitlines()\n-        url_base = m3u8_url.rsplit('/', 1)[0]\n-        first_segment = None\n-        for i, line in enumerate(m3u8_doc):\n-            if not line.startswith('#'):\n-                m3u8_doc[i] = f'{self._WAYBACK_BASE_URL}{url_base}/{line}?{urllib.parse.urlencode(params)}'\n-                first_segment = first_segment or m3u8_doc[i]\n-\n-        # Segments may not have been archived. See https://web.archive.org/web/20221127190050/http://www.vlive.tv/video/101870\n-        urlh = self._request_webpage(HEADRequest(first_segment), video_id, errnote=False,\n-                                     fatal=False, note='Check first segment availablity')\n-        if urlh:\n-            formats, subtitles = self._parse_m3u8_formats_and_subtitles('\\n'.join(m3u8_doc), ext='mp4', video_id=video_id)\n-            if subtitles:\n-                self._report_ignoring_subs('m3u8')\n-            return formats\n-\n-    # Closely follows the logic of the ArchiveTeam grab script\n-    # See: https://github.com/ArchiveTeam/vlive-grab/blob/master/vlive.lua\n-    def _real_extract(self, url):\n-        video_id, url_date = self._match_valid_url(url).group('id', 'date')\n-\n-        webpage = self._download_archived_page(f'https://www.vlive.tv/video/{video_id}', video_id, timestamp=url_date)\n-\n-        player_info = self._search_json(r'__PRELOADED_STATE__\\s*=', webpage, 'player info', video_id)\n-        user_country = traverse_obj(player_info, ('common', 'userCountry'))\n-\n-        main_script_url = self._search_regex(r'<script\\s+src=\"([^\"]+/js/main\\.[^\"]+\\.js)\"', webpage, 'main script url')\n-        main_script = self._download_archived_page(main_script_url, video_id, note='Downloading main script')\n-        app_id = self._search_regex(r'appId\\s*=\\s*\"([^\"]+)\"', main_script, 'app id')\n-\n-        inkey = self._download_archived_json(\n-            f'https://www.vlive.tv/globalv-web/vam-web/video/v1.0/vod/{video_id}/inkey', video_id, note='Fetching inkey', query={\n-                'appId': app_id,\n-                'platformType': 'PC',\n-                'gcc': user_country,\n-                'locale': 'en_US',\n-            }, fatal=False)\n-\n-        vod_id = traverse_obj(player_info, ('postDetail', 'post', 'officialVideo', 'vodId'))\n-\n-        vod_data = self._download_archived_json(\n-            f'https://apis.naver.com/rmcnmv/rmcnmv/vod/play/v2.0/{vod_id}', video_id, note='Fetching vod data', query={\n-                'key': inkey.get('inkey'),\n-                'pid': 'rmcPlayer_16692457559726800',  # partially unix time and partially random. Fixed value used by archiveteam project\n-                'sid': '2024',\n-                'ver': '2.0',\n-                'devt': 'html5_pc',\n-                'doct': 'json',\n-                'ptc': 'https',\n-                'sptc': 'https',\n-                'cpt': 'vtt',\n-                'ctls': '%7B%22visible%22%3A%7B%22fullscreen%22%3Atrue%2C%22logo%22%3Afalse%2C%22playbackRate%22%3Afalse%2C%22scrap%22%3Afalse%2C%22playCount%22%3Atrue%2C%22commentCount%22%3Atrue%2C%22title%22%3Atrue%2C%22writer%22%3Atrue%2C%22expand%22%3Afalse%2C%22subtitles%22%3Atrue%2C%22thumbnails%22%3Atrue%2C%22quality%22%3Atrue%2C%22setting%22%3Atrue%2C%22script%22%3Afalse%2C%22logoDimmed%22%3Atrue%2C%22badge%22%3Atrue%2C%22seekingTime%22%3Atrue%2C%22muted%22%3Atrue%2C%22muteButton%22%3Afalse%2C%22viewerNotice%22%3Afalse%2C%22linkCount%22%3Afalse%2C%22createTime%22%3Afalse%2C%22thumbnail%22%3Atrue%7D%2C%22clicked%22%3A%7B%22expand%22%3Afalse%2C%22subtitles%22%3Afalse%7D%7D',\n-                'pv': '4.26.9',\n-                'dr': '1920x1080',\n-                'cpl': 'en_US',\n-                'lc': 'en_US',\n-                'adi': '%5B%7B%22type%22%3A%22pre%22%2C%22exposure%22%3Afalse%2C%22replayExposure%22%3Afalse%7D%5D',\n-                'adu': '%2F',\n-                'videoId': vod_id,\n-                'cc': user_country,\n-            })\n-\n-        formats = []\n-\n-        streams = traverse_obj(vod_data, ('streams', ...))\n-        if len(streams) > 1:\n-            self.report_warning('Multiple streams found. Only the first stream will be downloaded.')\n-        stream = streams[0]\n-\n-        max_stream = max(\n-            stream.get('videos') or [],\n-            key=lambda v: traverse_obj(v, ('bitrate', 'video'), default=0), default=None)\n-        if max_stream is not None:\n-            params = {arg.get('name'): arg.get('value') for arg in stream.get('keys', []) if arg.get('type') == 'param'}\n-            formats = self._extract_formats_from_m3u8(max_stream.get('source'), params, video_id) or []\n-\n-        # For parts of the project MP4 files were archived\n-        max_video = max(\n-            traverse_obj(vod_data, ('videos', 'list', ...)),\n-            key=lambda v: traverse_obj(v, ('bitrate', 'video'), default=0), default=None)\n-        if max_video is not None:\n-            video_url = self._WAYBACK_BASE_URL + max_video.get('source')\n-            urlh = self._request_webpage(HEADRequest(video_url), video_id, errnote=False,\n-                                         fatal=False, note='Check video availablity')\n-            if urlh:\n-                formats.append({'url': video_url})\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            **traverse_obj(player_info, ('postDetail', 'post', {\n-                'title': ('officialVideo', 'title', {str}),\n-                'creator': ('author', 'nickname', {str}),\n-                'channel': ('channel', 'channelName', {str}),\n-                'channel_id': ('channel', 'channelCode', {str}),\n-                'duration': ('officialVideo', 'playTime', {int_or_none}),\n-                'view_count': ('officialVideo', 'playCount', {int_or_none}),\n-                'like_count': ('officialVideo', 'likeCount', {int_or_none}),\n-                'comment_count': ('officialVideo', 'commentCount', {int_or_none}),\n-                'timestamp': ('officialVideo', 'createdAt', {lambda x: int_or_none(x, scale=1000)}),\n-                'release_timestamp': ('officialVideo', 'willStartAt', {lambda x: int_or_none(x, scale=1000)}),\n-            })),\n-            **traverse_obj(vod_data, ('meta', {\n-                'uploader_id': ('user', 'id', {str}),\n-                'uploader': ('user', 'name', {str}),\n-                'uploader_url': ('user', 'url', {url_or_none}),\n-                'thumbnail': ('cover', 'source', {url_or_none}),\n-            }), expected_type=lambda x: x or None),\n-            **NaverBaseIE.process_subtitles(vod_data, lambda x: [self._WAYBACK_BASE_URL + x]),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ard.py",
            "diff": "diff --git a/yt_dlp/extractor/ard.py b/yt_dlp/extractor/ard.py\nindex ca1faa7d..f4b1cd07 100644\n--- a/yt_dlp/extractor/ard.py\n+++ b/yt_dlp/extractor/ard.py\n@@ -1,24 +1,24 @@\n-import json\n import re\n+from functools import partial\n \n from .common import InfoExtractor\n-from .generic import GenericIE\n from ..utils import (\n+    OnDemandPagedList,\n+    bug_reports_message,\n     determine_ext,\n-    ExtractorError,\n     int_or_none,\n+    join_nonempty,\n+    make_archive_id,\n     parse_duration,\n-    qualities,\n+    parse_iso8601,\n+    remove_start,\n     str_or_none,\n-    try_get,\n     unified_strdate,\n-    unified_timestamp,\n-    update_url,\n     update_url_query,\n     url_or_none,\n     xpath_text,\n )\n-from ..compat import compat_etree_fromstring\n+from ..utils.traversal import traverse_obj\n \n \n class ARDMediathekBaseIE(InfoExtractor):\n@@ -61,45 +61,6 @@ def _parse_media_info(self, media_info, video_id, fsk):\n             'subtitles': subtitles,\n         }\n \n-    def _ARD_extract_episode_info(self, title):\n-        \"\"\"Try to extract season/episode data from the title.\"\"\"\n-        res = {}\n-        if not title:\n-            return res\n-\n-        for pattern in [\n-            # Pattern for title like \"Homo sapiens (S06/E07) - Originalversion\"\n-            # from: https://www.ardmediathek.de/one/sendung/doctor-who/Y3JpZDovL3dkci5kZS9vbmUvZG9jdG9yIHdobw\n-            r'.*(?P<ep_info> \\(S(?P<season_number>\\d+)/E(?P<episode_number>\\d+)\\)).*',\n-            # E.g.: title=\"Fritjof aus Norwegen (2) (AD)\"\n-            # from: https://www.ardmediathek.de/ard/sammlung/der-krieg-und-ich/68cMkqJdllm639Skj4c7sS/\n-            r'.*(?P<ep_info> \\((?:Folge |Teil )?(?P<episode_number>\\d+)(?:/\\d+)?\\)).*',\n-            r'.*(?P<ep_info>Folge (?P<episode_number>\\d+)(?:\\:| -|) )\\\"(?P<episode>.+)\\\".*',\n-            # E.g.: title=\"Folge 25/42: Symmetrie\"\n-            # from: https://www.ardmediathek.de/ard/video/grips-mathe/folge-25-42-symmetrie/ard-alpha/Y3JpZDovL2JyLmRlL3ZpZGVvLzMyYzI0ZjczLWQ1N2MtNDAxNC05ZmZhLTFjYzRkZDA5NDU5OQ/\n-            # E.g.: title=\"Folge 1063 - Vertrauen\"\n-            # from: https://www.ardmediathek.de/ard/sendung/die-fallers/Y3JpZDovL3N3ci5kZS8yMzAyMDQ4/\n-            r'.*(?P<ep_info>Folge (?P<episode_number>\\d+)(?:/\\d+)?(?:\\:| -|) ).*',\n-        ]:\n-            m = re.match(pattern, title)\n-            if m:\n-                groupdict = m.groupdict()\n-                res['season_number'] = int_or_none(groupdict.get('season_number'))\n-                res['episode_number'] = int_or_none(groupdict.get('episode_number'))\n-                res['episode'] = str_or_none(groupdict.get('episode'))\n-                # Build the episode title by removing numeric episode information:\n-                if groupdict.get('ep_info') and not res['episode']:\n-                    res['episode'] = str_or_none(\n-                        title.replace(groupdict.get('ep_info'), ''))\n-                if res['episode']:\n-                    res['episode'] = res['episode'].strip()\n-                break\n-\n-        # As a fallback use the whole title as the episode name:\n-        if not res.get('episode'):\n-            res['episode'] = title.strip()\n-        return res\n-\n     def _extract_formats(self, media_info, video_id):\n         type_ = media_info.get('_type')\n         media_array = media_info.get('_mediaArray', [])\n@@ -155,144 +116,12 @@ def _extract_formats(self, media_info, video_id):\n         return formats\n \n \n-class ARDMediathekIE(ARDMediathekBaseIE):\n-    IE_NAME = 'ARD:mediathek'\n-    _VALID_URL = r'^https?://(?:(?:(?:www|classic)\\.)?ardmediathek\\.de|mediathek\\.(?:daserste|rbb-online)\\.de|one\\.ard\\.de)/(?:.*/)(?P<video_id>[0-9]+|[^0-9][^/\\?]+)[^/\\?]*(?:\\?.*)?'\n-\n-    _TESTS = [{\n-        # available till 26.07.2022\n-        'url': 'http://www.ardmediathek.de/tv/S%C3%9CDLICHT/Was-ist-die-Kunst-der-Zukunft-liebe-Ann/BR-Fernsehen/Video?bcastId=34633636&documentId=44726822',\n-        'info_dict': {\n-            'id': '44726822',\n-            'ext': 'mp4',\n-            'title': 'Was ist die Kunst der Zukunft, liebe Anna McCarthy?',\n-            'description': 'md5:4ada28b3e3b5df01647310e41f3a62f5',\n-            'duration': 1740,\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        }\n-    }, {\n-        'url': 'https://one.ard.de/tv/Mord-mit-Aussicht/Mord-mit-Aussicht-6-39-T%C3%B6dliche-Nach/ONE/Video?bcastId=46384294&documentId=55586872',\n-        'only_matching': True,\n-    }, {\n-        # audio\n-        'url': 'http://www.ardmediathek.de/tv/WDR-H%C3%B6rspiel-Speicher/Tod-eines-Fu%C3%9Fballers/WDR-3/Audio-Podcast?documentId=28488308&bcastId=23074086',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://mediathek.daserste.de/sendungen_a-z/328454_anne-will/22429276_vertrauen-ist-gut-spionieren-ist-besser-geht',\n-        'only_matching': True,\n-    }, {\n-        # audio\n-        'url': 'http://mediathek.rbb-online.de/radio/H\u00f6rspiel/Vor-dem-Fest/kulturradio/Audio?documentId=30796318&topRessort=radio&bcastId=9839158',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://classic.ardmediathek.de/tv/Panda-Gorilla-Co/Panda-Gorilla-Co-Folge-274/Das-Erste/Video?bcastId=16355486&documentId=58234698',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        return False if ARDBetaMediathekIE.suitable(url) else super(ARDMediathekIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        # determine video id from url\n-        m = self._match_valid_url(url)\n-\n-        document_id = None\n-\n-        numid = re.search(r'documentId=([0-9]+)', url)\n-        if numid:\n-            document_id = video_id = numid.group(1)\n-        else:\n-            video_id = m.group('video_id')\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        ERRORS = (\n-            ('>Leider liegt eine St\u00f6rung vor.', 'Video %s is unavailable'),\n-            ('>Der gew\u00fcnschte Beitrag ist nicht mehr verf\u00fcgbar.<',\n-             'Video %s is no longer available'),\n-        )\n-\n-        for pattern, message in ERRORS:\n-            if pattern in webpage:\n-                raise ExtractorError(message % video_id, expected=True)\n-\n-        if re.search(r'[\\?&]rss($|[=&])', url):\n-            doc = compat_etree_fromstring(webpage.encode('utf-8'))\n-            if doc.tag == 'rss':\n-                return GenericIE()._extract_rss(url, video_id, doc)\n-\n-        title = self._og_search_title(webpage, default=None) or self._html_search_regex(\n-            [r'<h1(?:\\s+class=\"boxTopHeadline\")?>(.*?)</h1>',\n-             r'<meta name=\"dcterms\\.title\" content=\"(.*?)\"/>',\n-             r'<h4 class=\"headline\">(.*?)</h4>',\n-             r'<title[^>]*>(.*?)</title>'],\n-            webpage, 'title')\n-        description = self._og_search_description(webpage, default=None) or self._html_search_meta(\n-            'dcterms.abstract', webpage, 'description', default=None)\n-        if description is None:\n-            description = self._html_search_meta(\n-                'description', webpage, 'meta description', default=None)\n-        if description is None:\n-            description = self._html_search_regex(\n-                r'<p\\s+class=\"teasertext\">(.+?)</p>',\n-                webpage, 'teaser text', default=None)\n-\n-        # Thumbnail is sometimes not present.\n-        # It is in the mobile version, but that seems to use a different URL\n-        # structure altogether.\n-        thumbnail = self._og_search_thumbnail(webpage, default=None)\n-\n-        media_streams = re.findall(r'''(?x)\n-            mediaCollection\\.addMediaStream\\([0-9]+,\\s*[0-9]+,\\s*\"[^\"]*\",\\s*\n-            \"([^\"]+)\"''', webpage)\n-\n-        if media_streams:\n-            QUALITIES = qualities(['lo', 'hi', 'hq'])\n-            formats = []\n-            for furl in set(media_streams):\n-                if furl.endswith('.f4m'):\n-                    fid = 'f4m'\n-                else:\n-                    fid_m = re.match(r'.*\\.([^.]+)\\.[^.]+$', furl)\n-                    fid = fid_m.group(1) if fid_m else None\n-                formats.append({\n-                    'quality': QUALITIES(fid),\n-                    'format_id': fid,\n-                    'url': furl,\n-                })\n-            info = {\n-                'formats': formats,\n-            }\n-        else:  # request JSON file\n-            if not document_id:\n-                video_id = self._search_regex(\n-                    (r'/play/(?:config|media|sola)/(\\d+)', r'contentId[\"\\']\\s*:\\s*(\\d+)'),\n-                    webpage, 'media id', default=None)\n-            info = self._extract_media_info(\n-                'http://www.ardmediathek.de/play/media/%s' % video_id,\n-                webpage, video_id)\n-\n-        info.update({\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-        })\n-        info.update(self._ARD_extract_episode_info(info['title']))\n-\n-        return info\n-\n-\n class ARDIE(InfoExtractor):\n     _VALID_URL = r'(?P<mainurl>https?://(?:www\\.)?daserste\\.de/(?:[^/?#&]+/)+(?P<id>[^/?#&]+))\\.html'\n     _TESTS = [{\n         # available till 7.12.2023\n         'url': 'https://www.daserste.de/information/talk/maischberger/videos/maischberger-video-424.html',\n-        'md5': 'a438f671e87a7eba04000336a119ccc4',\n+        'md5': '94812e6438488fb923c361a44469614b',\n         'info_dict': {\n             'id': 'maischberger-video-424',\n             'display_id': 'maischberger-video-424',\n@@ -399,31 +228,35 @@ def _real_extract(self, url):\n         }\n \n \n-class ARDBetaMediathekIE(ARDMediathekBaseIE):\n+class ARDBetaMediathekIE(InfoExtractor):\n+    IE_NAME = 'ARDMediathek'\n     _VALID_URL = r'''(?x)https://\n         (?:(?:beta|www)\\.)?ardmediathek\\.de/\n-        (?:(?P<client>[^/]+)/)?\n-        (?:player|live|video|(?P<playlist>sendung|sammlung))/\n-        (?:(?P<display_id>(?(playlist)[^?#]+?|[^?#]+))/)?\n-        (?P<id>(?(playlist)|Y3JpZDovL)[a-zA-Z0-9]+)\n-        (?(playlist)/(?P<season>\\d+)?/?(?:[?#]|$))'''\n+        (?:[^/]+/)?\n+        (?:player|live|video)/\n+        (?:[^?#]+/)?\n+        (?P<id>[a-zA-Z0-9]+)\n+        /?(?:[?#]|$)'''\n+    _GEO_COUNTRIES = ['DE']\n \n     _TESTS = [{\n-        'url': 'https://www.ardmediathek.de/video/filme-im-mdr/wolfsland-die-traurigen-schwestern/mdr-fernsehen/Y3JpZDovL21kci5kZS9iZWl0cmFnL2Ntcy8xZGY0ZGJmZS00ZWQwLTRmMGItYjhhYy0wOGQ4ZmYxNjVhZDI',\n-        'md5': '3fd5fead7a370a819341129c8d713136',\n+        'url': 'https://www.ardmediathek.de/video/filme-im-mdr/liebe-auf-vier-pfoten/mdr-fernsehen/Y3JpZDovL21kci5kZS9zZW5kdW5nLzI4MjA0MC80MjIwOTEtNDAyNTM0',\n+        'md5': 'b6e8ab03f2bcc6e1f9e6cef25fcc03c4',\n         'info_dict': {\n-            'display_id': 'filme-im-mdr/wolfsland-die-traurigen-schwestern/mdr-fernsehen',\n-            'id': '12172961',\n-            'title': 'Wolfsland - Die traurigen Schwestern',\n-            'description': r're:^Als der Polizeiobermeister Raaben',\n-            'duration': 5241,\n-            'thumbnail': 'https://api.ardmediathek.de/image-service/images/urn:ard:image:efa186f7b0054957',\n-            'timestamp': 1670710500,\n-            'upload_date': '20221210',\n+            'display_id': 'Y3JpZDovL21kci5kZS9zZW5kdW5nLzI4MjA0MC80MjIwOTEtNDAyNTM0',\n+            'id': '12939099',\n+            'title': 'Liebe auf vier Pfoten',\n+            'description': r're:^Claudia Schmitt, Anw\u00e4ltin in Salzburg',\n+            'duration': 5222,\n+            'thumbnail': 'https://api.ardmediathek.de/image-service/images/urn:ard:image:aee7cbf8f06de976?w=960&ch=ae4d0f2ee47d8b9b',\n+            'timestamp': 1701343800,\n+            'upload_date': '20231130',\n             'ext': 'mp4',\n-            'age_limit': 12,\n-            'episode': 'Wolfsland - Die traurigen Schwestern',\n-            'series': 'Filme im MDR'\n+            'episode': 'Liebe auf vier Pfoten',\n+            'series': 'Filme im MDR',\n+            'age_limit': 0,\n+            'channel': 'MDR',\n+            '_old_archive_ids': ['ardbetamediathek Y3JpZDovL21kci5kZS9zZW5kdW5nLzI4MjA0MC80MjIwOTEtNDAyNTM0'],\n         },\n     }, {\n         'url': 'https://www.ardmediathek.de/mdr/video/die-robuste-roswita/Y3JpZDovL21kci5kZS9iZWl0cmFnL2Ntcy84MWMxN2MzZC0wMjkxLTRmMzUtODk4ZS0wYzhlOWQxODE2NGI/',\n@@ -450,11 +283,31 @@ class ARDBetaMediathekIE(ARDMediathekBaseIE):\n             'timestamp': 1636398000,\n             'description': 'md5:39578c7b96c9fe50afdf5674ad985e6b',\n             'upload_date': '20211108',\n-            'display_id': 'tagesschau-oder-tagesschau-20-00-uhr/das-erste',\n+            'display_id': 'Y3JpZDovL2Rhc2Vyc3RlLmRlL3RhZ2Vzc2NoYXUvZmM4ZDUxMjgtOTE0ZC00Y2MzLTgzNzAtNDZkNGNiZWJkOTll',\n             'duration': 915,\n             'episode': 'tagesschau, 20:00 Uhr',\n             'series': 'tagesschau',\n-            'thumbnail': 'https://api.ardmediathek.de/image-service/images/urn:ard:image:fbb21142783b0a49',\n+            'thumbnail': 'https://api.ardmediathek.de/image-service/images/urn:ard:image:fbb21142783b0a49?w=960&ch=ee69108ae344f678',\n+            'channel': 'ARD-Aktuell',\n+            '_old_archive_ids': ['ardbetamediathek Y3JpZDovL2Rhc2Vyc3RlLmRlL3RhZ2Vzc2NoYXUvZmM4ZDUxMjgtOTE0ZC00Y2MzLTgzNzAtNDZkNGNiZWJkOTll'],\n+        },\n+    }, {\n+        'url': 'https://www.ardmediathek.de/video/7-tage/7-tage-unter-harten-jungs/hr-fernsehen/N2I2YmM5MzgtNWFlOS00ZGFlLTg2NzMtYzNjM2JlNjk4MDg3',\n+        'md5': 'c428b9effff18ff624d4f903bda26315',\n+        'info_dict': {\n+            'id': '94834686',\n+            'ext': 'mp4',\n+            'duration': 2700,\n+            'episode': '7 Tage ... unter harten Jungs',\n+            'description': 'md5:0f215470dcd2b02f59f4bd10c963f072',\n+            'upload_date': '20231005',\n+            'timestamp': 1696491171,\n+            'display_id': 'N2I2YmM5MzgtNWFlOS00ZGFlLTg2NzMtYzNjM2JlNjk4MDg3',\n+            'series': '7 Tage ...',\n+            'channel': 'HR',\n+            'thumbnail': 'https://api.ardmediathek.de/image-service/images/urn:ard:image:f6e6d5ffac41925c?w=960&ch=fa32ba69bc87989a',\n+            'title': '7 Tage ... unter harten Jungs',\n+            '_old_archive_ids': ['ardbetamediathek N2I2YmM5MzgtNWFlOS00ZGFlLTg2NzMtYzNjM2JlNjk4MDg3'],\n         },\n     }, {\n         'url': 'https://beta.ardmediathek.de/ard/video/Y3JpZDovL2Rhc2Vyc3RlLmRlL3RhdG9ydC9mYmM4NGM1NC0xNzU4LTRmZGYtYWFhZS0wYzcyZTIxNGEyMDE',\n@@ -472,202 +325,238 @@ class ARDBetaMediathekIE(ARDMediathekBaseIE):\n         'url': 'https://www.ardmediathek.de/swr/live/Y3JpZDovL3N3ci5kZS8xMzQ4MTA0Mg',\n         'only_matching': True,\n     }, {\n-        # playlist of type 'sendung'\n-        'url': 'https://www.ardmediathek.de/ard/sendung/doctor-who/Y3JpZDovL3dkci5kZS9vbmUvZG9jdG9yIHdobw/',\n+        'url': 'https://www.ardmediathek.de/video/coronavirus-update-ndr-info/astrazeneca-kurz-lockdown-und-pims-syndrom-81/ndr/Y3JpZDovL25kci5kZS84NzE0M2FjNi0wMWEwLTQ5ODEtOTE5NS1mOGZhNzdhOTFmOTI/',\n         'only_matching': True,\n+    }]\n+\n+    def _extract_episode_info(self, title):\n+        patterns = [\n+            # Pattern for title like \"Homo sapiens (S06/E07) - Originalversion\"\n+            # from: https://www.ardmediathek.de/one/sendung/doctor-who/Y3JpZDovL3dkci5kZS9vbmUvZG9jdG9yIHdobw\n+            r'.*(?P<ep_info> \\(S(?P<season_number>\\d+)/E(?P<episode_number>\\d+)\\)).*',\n+            # E.g.: title=\"Fritjof aus Norwegen (2) (AD)\"\n+            # from: https://www.ardmediathek.de/ard/sammlung/der-krieg-und-ich/68cMkqJdllm639Skj4c7sS/\n+            r'.*(?P<ep_info> \\((?:Folge |Teil )?(?P<episode_number>\\d+)(?:/\\d+)?\\)).*',\n+            r'.*(?P<ep_info>Folge (?P<episode_number>\\d+)(?:\\:| -|) )\\\"(?P<episode>.+)\\\".*',\n+            # E.g.: title=\"Folge 25/42: Symmetrie\"\n+            # from: https://www.ardmediathek.de/ard/video/grips-mathe/folge-25-42-symmetrie/ard-alpha/Y3JpZDovL2JyLmRlL3ZpZGVvLzMyYzI0ZjczLWQ1N2MtNDAxNC05ZmZhLTFjYzRkZDA5NDU5OQ/\n+            # E.g.: title=\"Folge 1063 - Vertrauen\"\n+            # from: https://www.ardmediathek.de/ard/sendung/die-fallers/Y3JpZDovL3N3ci5kZS8yMzAyMDQ4/\n+            r'.*(?P<ep_info>Folge (?P<episode_number>\\d+)(?:/\\d+)?(?:\\:| -|) ).*',\n+            # As a fallback use the full title\n+            r'(?P<title>.*)',\n+        ]\n+\n+        return traverse_obj(patterns, (..., {partial(re.match, string=title)}, {\n+            'season_number': ('season_number', {int_or_none}),\n+            'episode_number': ('episode_number', {int_or_none}),\n+            'episode': ((\n+                ('episode', {str_or_none}),\n+                ('ep_info', {lambda x: title.replace(x, '')}),\n+                ('title', {str}),\n+            ), {str.strip}),\n+        }), get_all=False)\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+\n+        page_data = self._download_json(\n+            f'https://api.ardmediathek.de/page-gateway/pages/ard/item/{display_id}', display_id, query={\n+                'embedded': 'false',\n+                'mcV6': 'true',\n+            })\n+\n+        # For user convenience we use the old contentId instead of the longer crid\n+        # Ref: https://github.com/yt-dlp/yt-dlp/issues/8731#issuecomment-1874398283\n+        old_id = traverse_obj(page_data, ('tracking', 'atiCustomVars', 'contentId', {int}))\n+        if old_id is not None:\n+            video_id = str(old_id)\n+            archive_ids = [make_archive_id(ARDBetaMediathekIE, display_id)]\n+        else:\n+            self.report_warning(f'Could not extract contentId{bug_reports_message()}')\n+            video_id = display_id\n+            archive_ids = None\n+\n+        player_data = traverse_obj(\n+            page_data, ('widgets', lambda _, v: v['type'] in ('player_ondemand', 'player_live'), {dict}), get_all=False)\n+        is_live = player_data.get('type') == 'player_live'\n+        media_data = traverse_obj(player_data, ('mediaCollection', 'embedded', {dict}))\n+\n+        if player_data.get('blockedByFsk'):\n+            self.raise_no_formats('This video is only available after 22:00', expected=True)\n+\n+        formats = []\n+        subtitles = {}\n+        for stream in traverse_obj(media_data, ('streams', ..., {dict})):\n+            kind = stream.get('kind')\n+            # Prioritize main stream over sign language and others\n+            preference = 1 if kind == 'main' else None\n+            for media in traverse_obj(stream, ('media', lambda _, v: url_or_none(v['url']))):\n+                media_url = media['url']\n+\n+                audio_kind = traverse_obj(media, (\n+                    'audios', 0, 'kind', {str}), default='').replace('standard', '')\n+                lang_code = traverse_obj(media, ('audios', 0, 'languageCode', {str})) or 'deu'\n+                lang = join_nonempty(lang_code, audio_kind)\n+                language_preference = 10 if lang == 'deu' else -10\n+\n+                if determine_ext(media_url) == 'm3u8':\n+                    fmts, subs = self._extract_m3u8_formats_and_subtitles(\n+                        media_url, video_id, m3u8_id=f'hls-{kind}', preference=preference, fatal=False, live=is_live)\n+                    for f in fmts:\n+                        f['language'] = lang\n+                        f['language_preference'] = language_preference\n+                    formats.extend(fmts)\n+                    self._merge_subtitles(subs, target=subtitles)\n+                else:\n+                    formats.append({\n+                        'url': media_url,\n+                        'format_id': f'http-{kind}',\n+                        'preference': preference,\n+                        'language': lang,\n+                        'language_preference': language_preference,\n+                        **traverse_obj(media, {\n+                            'format_note': ('forcedLabel', {str}),\n+                            'width': ('maxHResolutionPx', {int_or_none}),\n+                            'height': ('maxVResolutionPx', {int_or_none}),\n+                            'vcodec': ('videoCodec', {str}),\n+                        }),\n+                    })\n+\n+        for sub in traverse_obj(media_data, ('subtitles', ..., {dict})):\n+            for sources in traverse_obj(sub, ('sources', lambda _, v: url_or_none(v['url']))):\n+                subtitles.setdefault(sub.get('languageCode') or 'deu', []).append({\n+                    'url': sources['url'],\n+                    'ext': {'webvtt': 'vtt', 'ebutt': 'ttml'}.get(sources.get('kind')),\n+                })\n+\n+        age_limit = traverse_obj(page_data, ('fskRating', {lambda x: remove_start(x, 'FSK')}, {int_or_none}))\n+        return {\n+            'id': video_id,\n+            'display_id': display_id,\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            'is_live': is_live,\n+            'age_limit': age_limit,\n+            **traverse_obj(media_data, ('meta', {\n+                'title': 'title',\n+                'description': 'synopsis',\n+                'timestamp': ('broadcastedOnDateTime', {parse_iso8601}),\n+                'series': 'seriesTitle',\n+                'thumbnail': ('images', 0, 'url', {url_or_none}),\n+                'duration': ('durationSeconds', {int_or_none}),\n+                'channel': 'clipSourceName',\n+            })),\n+            **self._extract_episode_info(page_data.get('title')),\n+            '_old_archive_ids': archive_ids,\n+        }\n+\n+\n+class ARDMediathekCollectionIE(InfoExtractor):\n+    _VALID_URL = r'''(?x)https://\n+        (?:(?:beta|www)\\.)?ardmediathek\\.de/\n+        (?:[^/?#]+/)?\n+        (?P<playlist>sendung|serie|sammlung)/\n+        (?:(?P<display_id>[^?#]+?)/)?\n+        (?P<id>[a-zA-Z0-9]+)\n+        (?:/(?P<season>\\d+)(?:/(?P<version>OV|AD))?)?/?(?:[?#]|$)'''\n+    _GEO_COUNTRIES = ['DE']\n+\n+    _TESTS = [{\n+        'url': 'https://www.ardmediathek.de/serie/quiz/staffel-1-originalversion/Y3JpZDovL3dkci5kZS9vbmUvcXVpeg/1/OV',\n+        'info_dict': {\n+            'id': 'Y3JpZDovL3dkci5kZS9vbmUvcXVpeg_1_OV',\n+            'display_id': 'quiz/staffel-1-originalversion',\n+            'title': 'Staffel 1 Originalversion',\n+        },\n+        'playlist_count': 3,\n     }, {\n-        # playlist of type 'sammlung'\n-        'url': 'https://www.ardmediathek.de/ard/sammlung/team-muenster/5JpTzLSbWUAK8184IOvEir/',\n+        'url': 'https://www.ardmediathek.de/serie/babylon-berlin/staffel-4-mit-audiodeskription/Y3JpZDovL2Rhc2Vyc3RlLmRlL2JhYnlsb24tYmVybGlu/4/AD',\n+        'info_dict': {\n+            'id': 'Y3JpZDovL2Rhc2Vyc3RlLmRlL2JhYnlsb24tYmVybGlu_4_AD',\n+            'display_id': 'babylon-berlin/staffel-4-mit-audiodeskription',\n+            'title': 'Staffel 4 mit Audiodeskription',\n+        },\n+        'playlist_count': 12,\n+    }, {\n+        'url': 'https://www.ardmediathek.de/serie/babylon-berlin/staffel-1/Y3JpZDovL2Rhc2Vyc3RlLmRlL2JhYnlsb24tYmVybGlu/1/',\n+        'info_dict': {\n+            'id': 'Y3JpZDovL2Rhc2Vyc3RlLmRlL2JhYnlsb24tYmVybGlu_1',\n+            'display_id': 'babylon-berlin/staffel-1',\n+            'title': 'Staffel 1',\n+        },\n+        'playlist_count': 8,\n+    }, {\n+        'url': 'https://www.ardmediathek.de/sendung/tatort/Y3JpZDovL2Rhc2Vyc3RlLmRlL3RhdG9ydA',\n+        'info_dict': {\n+            'id': 'Y3JpZDovL2Rhc2Vyc3RlLmRlL3RhdG9ydA',\n+            'display_id': 'tatort',\n+            'title': 'Tatort',\n+        },\n+        'playlist_mincount': 500,\n+    }, {\n+        'url': 'https://www.ardmediathek.de/sammlung/die-kirche-bleibt-im-dorf/5eOHzt8XB2sqeFXbIoJlg2',\n+        'info_dict': {\n+            'id': '5eOHzt8XB2sqeFXbIoJlg2',\n+            'display_id': 'die-kirche-bleibt-im-dorf',\n+            'title': 'Die Kirche bleibt im Dorf',\n+            'description': 'Die Kirche bleibt im Dorf',\n+        },\n+        'playlist_count': 4,\n+    }, {\n+        # playlist of type 'sendung'\n+        'url': 'https://www.ardmediathek.de/ard/sendung/doctor-who/Y3JpZDovL3dkci5kZS9vbmUvZG9jdG9yIHdobw/',\n         'only_matching': True,\n     }, {\n-        'url': 'https://www.ardmediathek.de/video/coronavirus-update-ndr-info/astrazeneca-kurz-lockdown-und-pims-syndrom-81/ndr/Y3JpZDovL25kci5kZS84NzE0M2FjNi0wMWEwLTQ5ODEtOTE5NS1mOGZhNzdhOTFmOTI/',\n+        # playlist of type 'serie'\n+        'url': 'https://www.ardmediathek.de/serie/nachtstreife/staffel-1/Y3JpZDovL3N3ci5kZS9zZGIvc3RJZC8xMjQy/1',\n         'only_matching': True,\n     }, {\n-        'url': 'https://www.ardmediathek.de/ard/player/Y3JpZDovL3dkci5kZS9CZWl0cmFnLWQ2NDJjYWEzLTMwZWYtNGI4NS1iMTI2LTU1N2UxYTcxOGIzOQ/tatort-duo-koeln-leipzig-ihr-kinderlein-kommet',\n+        # playlist of type 'sammlung'\n+        'url': 'https://www.ardmediathek.de/ard/sammlung/team-muenster/5JpTzLSbWUAK8184IOvEir/',\n         'only_matching': True,\n     }]\n \n-    def _ARD_load_playlist_snipped(self, playlist_id, display_id, client, mode, pageNumber):\n-        \"\"\" Query the ARD server for playlist information\n-        and returns the data in \"raw\" format \"\"\"\n-        if mode == 'sendung':\n-            graphQL = json.dumps({\n-                'query': '''{\n-                    showPage(\n-                        client: \"%s\"\n-                        showId: \"%s\"\n-                        pageNumber: %d\n-                    ) {\n-                        pagination {\n-                            pageSize\n-                            totalElements\n-                        }\n-                        teasers {        # Array\n-                            mediumTitle\n-                            links { target { id href title } }\n-                            type\n-                        }\n-                    }}''' % (client, playlist_id, pageNumber),\n-            }).encode()\n-        else:  # mode == 'sammlung'\n-            graphQL = json.dumps({\n-                'query': '''{\n-                    morePage(\n-                        client: \"%s\"\n-                        compilationId: \"%s\"\n-                        pageNumber: %d\n-                    ) {\n-                        widget {\n-                            pagination {\n-                                pageSize\n-                                totalElements\n-                            }\n-                            teasers {        # Array\n-                                mediumTitle\n-                                links { target { id href title } }\n-                                type\n-                            }\n-                        }\n-                    }}''' % (client, playlist_id, pageNumber),\n-            }).encode()\n-        # Ressources for ARD graphQL debugging:\n-        # https://api-test.ardmediathek.de/public-gateway\n-        show_page = self._download_json(\n-            'https://api.ardmediathek.de/public-gateway',\n-            '[Playlist] %s' % display_id,\n-            data=graphQL,\n-            headers={'Content-Type': 'application/json'})['data']\n-        # align the structure of the returned data:\n-        if mode == 'sendung':\n-            show_page = show_page['showPage']\n-        else:  # mode == 'sammlung'\n-            show_page = show_page['morePage']['widget']\n-        return show_page\n-\n-    def _ARD_extract_playlist(self, url, playlist_id, display_id, client, mode):\n-        \"\"\" Collects all playlist entries and returns them as info dict.\n-        Supports playlists of mode 'sendung' and 'sammlung', and also nested\n-        playlists. \"\"\"\n-        entries = []\n-        pageNumber = 0\n-        while True:  # iterate by pageNumber\n-            show_page = self._ARD_load_playlist_snipped(\n-                playlist_id, display_id, client, mode, pageNumber)\n-            for teaser in show_page['teasers']:  # process playlist items\n-                if '/compilation/' in teaser['links']['target']['href']:\n-                    # alternativ cond.: teaser['type'] == \"compilation\"\n-                    # => This is an nested compilation, e.g. like:\n-                    # https://www.ardmediathek.de/ard/sammlung/die-kirche-bleibt-im-dorf/5eOHzt8XB2sqeFXbIoJlg2/\n-                    link_mode = 'sammlung'\n-                else:\n-                    link_mode = 'video'\n-\n-                item_url = 'https://www.ardmediathek.de/%s/%s/%s/%s/%s' % (\n-                    client, link_mode, display_id,\n-                    # perform HTLM quoting of episode title similar to ARD:\n-                    re.sub('^-|-$', '',  # remove '-' from begin/end\n-                           re.sub('[^a-zA-Z0-9]+', '-',  # replace special chars by -\n-                                  teaser['links']['target']['title'].lower()\n-                                  .replace('\u00e4', 'ae').replace('\u00f6', 'oe')\n-                                  .replace('\u00fc', 'ue').replace('\u00df', 'ss'))),\n-                    teaser['links']['target']['id'])\n-                entries.append(self.url_result(\n-                    item_url,\n-                    ie=ARDBetaMediathekIE.ie_key()))\n-\n-            if (show_page['pagination']['pageSize'] * (pageNumber + 1)\n-               >= show_page['pagination']['totalElements']):\n-                # we've processed enough pages to get all playlist entries\n-                break\n-            pageNumber = pageNumber + 1\n-\n-        return self.playlist_result(entries, playlist_id, playlist_title=display_id)\n+    _PAGE_SIZE = 100\n \n     def _real_extract(self, url):\n-        video_id, display_id, playlist_type, client, season_number = self._match_valid_url(url).group(\n-            'id', 'display_id', 'playlist', 'client', 'season')\n-        display_id, client = display_id or video_id, client or 'ard'\n-\n-        if playlist_type:\n-            # TODO: Extract only specified season\n-            return self._ARD_extract_playlist(url, video_id, display_id, client, playlist_type)\n-\n-        player_page = self._download_json(\n-            'https://api.ardmediathek.de/public-gateway',\n-            display_id, data=json.dumps({\n-                'query': '''{\n-  playerPage(client:\"%s\", clipId: \"%s\") {\n-    blockedByFsk\n-    broadcastedOn\n-    maturityContentRating\n-    mediaCollection {\n-      _duration\n-      _geoblocked\n-      _isLive\n-      _mediaArray {\n-        _mediaStreamArray {\n-          _quality\n-          _server\n-          _stream\n-        }\n-      }\n-      _previewImage\n-      _subtitleUrl\n-      _type\n-    }\n-    show {\n-      title\n-    }\n-    image {\n-      src\n-    }\n-    synopsis\n-    title\n-    tracking {\n-      atiCustomVars {\n-        contentId\n-      }\n-    }\n-  }\n-}''' % (client, video_id),\n-            }).encode(), headers={\n-                'Content-Type': 'application/json'\n-            })['data']['playerPage']\n-        title = player_page['title']\n-        content_id = str_or_none(try_get(\n-            player_page, lambda x: x['tracking']['atiCustomVars']['contentId']))\n-        media_collection = player_page.get('mediaCollection') or {}\n-        if not media_collection and content_id:\n-            media_collection = self._download_json(\n-                'https://www.ardmediathek.de/play/media/' + content_id,\n-                content_id, fatal=False) or {}\n-        info = self._parse_media_info(\n-            media_collection, content_id or video_id,\n-            player_page.get('blockedByFsk'))\n-        age_limit = None\n-        description = player_page.get('synopsis')\n-        maturity_content_rating = player_page.get('maturityContentRating')\n-        if maturity_content_rating:\n-            age_limit = int_or_none(maturity_content_rating.lstrip('FSK'))\n-        if not age_limit and description:\n-            age_limit = int_or_none(self._search_regex(\n-                r'\\(FSK\\s*(\\d+)\\)\\s*$', description, 'age limit', default=None))\n-        info.update({\n-            'age_limit': age_limit,\n-            'display_id': display_id,\n-            'title': title,\n-            'description': description,\n-            'timestamp': unified_timestamp(player_page.get('broadcastedOn')),\n-            'series': try_get(player_page, lambda x: x['show']['title']),\n-            'thumbnail': (media_collection.get('_previewImage')\n-                          or try_get(player_page, lambda x: update_url(x['image']['src'], query=None, fragment=None))\n-                          or self.get_thumbnail_from_html(display_id, url)),\n-        })\n-        info.update(self._ARD_extract_episode_info(info['title']))\n-        return info\n-\n-    def get_thumbnail_from_html(self, display_id, url):\n-        webpage = self._download_webpage(url, display_id, fatal=False) or ''\n-        return (\n-            self._og_search_thumbnail(webpage, default=None)\n-            or self._html_search_meta('thumbnailUrl', webpage, default=None))\n+        playlist_id, display_id, playlist_type, season_number, version = self._match_valid_url(url).group(\n+            'id', 'display_id', 'playlist', 'season', 'version')\n+\n+        def call_api(page_num):\n+            api_path = 'compilations/ard' if playlist_type == 'sammlung' else 'widgets/ard/asset'\n+            return self._download_json(\n+                f'https://api.ardmediathek.de/page-gateway/{api_path}/{playlist_id}', playlist_id,\n+                f'Downloading playlist page {page_num}', query={\n+                    'pageNumber': page_num,\n+                    'pageSize': self._PAGE_SIZE,\n+                    **({\n+                        'seasoned': 'true',\n+                        'seasonNumber': season_number,\n+                        'withOriginalversion': 'true' if version == 'OV' else 'false',\n+                        'withAudiodescription': 'true' if version == 'AD' else 'false',\n+                    } if season_number else {}),\n+                })\n+\n+        def fetch_page(page_num):\n+            for item in traverse_obj(call_api(page_num), ('teasers', ..., {dict})):\n+                item_id = traverse_obj(item, ('links', 'target', ('urlId', 'id')), 'id', get_all=False)\n+                if not item_id or item_id == playlist_id:\n+                    continue\n+                item_mode = 'sammlung' if item.get('type') == 'compilation' else 'video'\n+                yield self.url_result(\n+                    f'https://www.ardmediathek.de/{item_mode}/{item_id}',\n+                    ie=(ARDMediathekCollectionIE if item_mode == 'sammlung' else ARDBetaMediathekIE),\n+                    **traverse_obj(item, {\n+                        'id': ('id', {str}),\n+                        'title': ('longTitle', {str}),\n+                        'duration': ('duration', {int_or_none}),\n+                        'timestamp': ('broadcastedOn', {parse_iso8601}),\n+                    }))\n+\n+        page_data = call_api(0)\n+        full_id = join_nonempty(playlist_id, season_number, version, delim='_')\n+\n+        return self.playlist_result(\n+            OnDemandPagedList(fetch_page, self._PAGE_SIZE), full_id, display_id=display_id,\n+            title=page_data.get('title'), description=page_data.get('synopsis'))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/arte.py",
            "diff": "diff --git a/yt_dlp/extractor/arte.py b/yt_dlp/extractor/arte.py\nindex e3cc5afb..139a3a72 100644\n--- a/yt_dlp/extractor/arte.py\n+++ b/yt_dlp/extractor/arte.py\n@@ -48,17 +48,7 @@ class ArteTVIE(ArteTVBaseIE):\n     }, {\n         'note': 'No alt_title',\n         'url': 'https://www.arte.tv/fr/videos/110371-000-A/la-chaleur-supplice-des-arbres-de-rue/',\n-        'info_dict': {\n-            'id': '110371-000-A',\n-            'ext': 'mp4',\n-            'upload_date': '20220718',\n-            'duration': 154,\n-            'timestamp': 1658162460,\n-            'description': 'md5:5890f36fe7dccfadb8b7c0891de54786',\n-            'title': 'La chaleur, supplice des arbres de rue',\n-            'thumbnail': 'https://api-cdn.arte.tv/img/v2/image/CPE2sQDtD8GLQgt8DuYHLf/940x530',\n-        },\n-        'params': {'skip_download': 'm3u8'}\n+        'only_matching': True,\n     }, {\n         'url': 'https://api.arte.tv/api/player/v2/config/de/100605-013-A',\n         'only_matching': True,\n@@ -67,19 +57,20 @@ class ArteTVIE(ArteTVBaseIE):\n         'only_matching': True,\n     }, {\n         'url': 'https://www.arte.tv/de/videos/110203-006-A/zaz/',\n+        'only_matching': True,\n+    }, {\n+        'note': 'age-restricted',\n+        'url': 'https://www.arte.tv/de/videos/006785-000-A/the-element-of-crime/',\n         'info_dict': {\n-            'id': '110203-006-A',\n-            'chapters': 'count:16',\n-            'description': 'md5:cf592f1df52fe52007e3f8eac813c084',\n-            'alt_title': 'Zaz',\n-            'title': 'Baloise Session 2022',\n-            'timestamp': 1668445200,\n-            'duration': 4054,\n-            'thumbnail': 'https://api-cdn.arte.tv/img/v2/image/ubQjmVCGyRx3hmBuZEK9QZ/940x530',\n-            'upload_date': '20221114',\n+            'id': '006785-000-A',\n+            'description': 'md5:c2f94fdfefc8a280e4dab68ab96ab0ba',\n+            'title': 'The Element of Crime',\n+            'timestamp': 1696111200,\n+            'duration': 5849,\n+            'thumbnail': 'https://api-cdn.arte.tv/img/v2/image/q82dTTfyuCXupPsGxXsd7B/940x530',\n+            'upload_date': '20230930',\n             'ext': 'mp4',\n-        },\n-        'expected_warnings': ['geo restricted']\n+        }\n     }]\n \n     _GEO_BYPASS = True\n@@ -136,7 +127,9 @@ def _real_extract(self, url):\n         lang = mobj.group('lang') or mobj.group('lang_2')\n         langauge_code = self._LANG_MAP.get(lang)\n \n-        config = self._download_json(f'{self._API_BASE}/config/{lang}/{video_id}', video_id)\n+        config = self._download_json(f'{self._API_BASE}/config/{lang}/{video_id}', video_id, headers={\n+            'x-validated-age': '18'\n+        })\n \n         geoblocking = traverse_obj(config, ('data', 'attributes', 'restriction', 'geoblocking')) or {}\n         if geoblocking.get('restrictedArea'):\n@@ -169,7 +162,7 @@ def _real_extract(self, url):\n                 )))\n \n             short_label = traverse_obj(stream_version, 'shortLabel', expected_type=str, default='?')\n-            if stream['protocol'].startswith('HLS'):\n+            if 'HLS' in stream['protocol']:\n                 fmts, subs = self._extract_m3u8_formats_and_subtitles(\n                     stream['url'], video_id=video_id, ext='mp4', m3u8_id=stream_version_code, fatal=False)\n                 for fmt in fmts:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/asiancrush.py",
            "diff": "diff --git a/yt_dlp/extractor/asiancrush.py b/yt_dlp/extractor/asiancrush.py\ndeleted file mode 100644\nindex 23f310ed..00000000\n--- a/yt_dlp/extractor/asiancrush.py\n+++ /dev/null\n@@ -1,196 +0,0 @@\n-import functools\n-import re\n-\n-from .common import InfoExtractor\n-from .kaltura import KalturaIE\n-from ..utils import (\n-    extract_attributes,\n-    int_or_none,\n-    OnDemandPagedList,\n-    parse_age_limit,\n-    strip_or_none,\n-    try_get,\n-)\n-\n-\n-class AsianCrushBaseIE(InfoExtractor):\n-    _VALID_URL_BASE = r'https?://(?:www\\.)?(?P<host>(?:(?:asiancrush|yuyutv|midnightpulp)\\.com|(?:cocoro|retrocrush)\\.tv))'\n-    _KALTURA_KEYS = [\n-        'video_url', 'progressive_url', 'download_url', 'thumbnail_url',\n-        'widescreen_thumbnail_url', 'screencap_widescreen',\n-    ]\n-    _API_SUFFIX = {'retrocrush.tv': '-ott'}\n-\n-    def _call_api(self, host, endpoint, video_id, query, resource):\n-        return self._download_json(\n-            'https://api%s.%s/%s' % (self._API_SUFFIX.get(host, ''), host, endpoint), video_id,\n-            'Downloading %s JSON metadata' % resource, query=query,\n-            headers=self.geo_verification_headers())['objects']\n-\n-    def _download_object_data(self, host, object_id, resource):\n-        return self._call_api(\n-            host, 'search', object_id, {'id': object_id}, resource)[0]\n-\n-    def _get_object_description(self, obj):\n-        return strip_or_none(obj.get('long_description') or obj.get('short_description'))\n-\n-    def _parse_video_data(self, video):\n-        title = video['name']\n-\n-        entry_id, partner_id = [None] * 2\n-        for k in self._KALTURA_KEYS:\n-            k_url = video.get(k)\n-            if k_url:\n-                mobj = re.search(r'/p/(\\d+)/.+?/entryId/([^/]+)/', k_url)\n-                if mobj:\n-                    partner_id, entry_id = mobj.groups()\n-                    break\n-\n-        meta_categories = try_get(video, lambda x: x['meta']['categories'], list) or []\n-        categories = list(filter(None, [c.get('name') for c in meta_categories]))\n-\n-        show_info = video.get('show_info') or {}\n-\n-        return {\n-            '_type': 'url_transparent',\n-            'url': 'kaltura:%s:%s' % (partner_id, entry_id),\n-            'ie_key': KalturaIE.ie_key(),\n-            'id': entry_id,\n-            'title': title,\n-            'description': self._get_object_description(video),\n-            'age_limit': parse_age_limit(video.get('mpaa_rating') or video.get('tv_rating')),\n-            'categories': categories,\n-            'series': show_info.get('show_name'),\n-            'season_number': int_or_none(show_info.get('season_num')),\n-            'season_id': show_info.get('season_id'),\n-            'episode_number': int_or_none(show_info.get('episode_num')),\n-        }\n-\n-\n-class AsianCrushIE(AsianCrushBaseIE):\n-    _VALID_URL = r'%s/video/(?:[^/]+/)?0+(?P<id>\\d+)v\\b' % AsianCrushBaseIE._VALID_URL_BASE\n-    _TESTS = [{\n-        'url': 'https://www.asiancrush.com/video/004289v/women-who-flirt',\n-        'md5': 'c3b740e48d0ba002a42c0b72857beae6',\n-        'info_dict': {\n-            'id': '1_y4tmjm5r',\n-            'ext': 'mp4',\n-            'title': 'Women Who Flirt',\n-            'description': 'md5:b65c7e0ae03a85585476a62a186f924c',\n-            'timestamp': 1496936429,\n-            'upload_date': '20170608',\n-            'uploader_id': 'craig@crifkin.com',\n-            'age_limit': 13,\n-            'categories': 'count:5',\n-            'duration': 5812,\n-        },\n-    }, {\n-        'url': 'https://www.asiancrush.com/video/she-was-pretty/011886v-pretty-episode-3/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.yuyutv.com/video/013886v/the-act-of-killing/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.yuyutv.com/video/peep-show/013922v-warring-factions/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.midnightpulp.com/video/010400v/drifters/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.midnightpulp.com/video/mononoke/016378v-zashikiwarashi-part-1/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.cocoro.tv/video/the-wonderful-wizard-of-oz/008878v-the-wonderful-wizard-of-oz-ep01/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.retrocrush.tv/video/true-tears/012328v-i...gave-away-my-tears',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        host, video_id = self._match_valid_url(url).groups()\n-\n-        if host == 'cocoro.tv':\n-            webpage = self._download_webpage(url, video_id)\n-            embed_vars = self._parse_json(self._search_regex(\n-                r'iEmbedVars\\s*=\\s*({.+?})', webpage, 'embed vars',\n-                default='{}'), video_id, fatal=False) or {}\n-            video_id = embed_vars.get('entry_id') or video_id\n-\n-        video = self._download_object_data(host, video_id, 'video')\n-        return self._parse_video_data(video)\n-\n-\n-class AsianCrushPlaylistIE(AsianCrushBaseIE):\n-    _VALID_URL = r'%s/series/0+(?P<id>\\d+)s\\b' % AsianCrushBaseIE._VALID_URL_BASE\n-    _TESTS = [{\n-        'url': 'https://www.asiancrush.com/series/006447s/fruity-samurai',\n-        'info_dict': {\n-            'id': '6447',\n-            'title': 'Fruity Samurai',\n-            'description': 'md5:7535174487e4a202d3872a7fc8f2f154',\n-        },\n-        'playlist_count': 13,\n-    }, {\n-        'url': 'https://www.yuyutv.com/series/013920s/peep-show/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.midnightpulp.com/series/016375s/mononoke/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.cocoro.tv/series/008549s/the-wonderful-wizard-of-oz/',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.retrocrush.tv/series/012355s/true-tears',\n-        'only_matching': True,\n-    }]\n-    _PAGE_SIZE = 1000000000\n-\n-    def _fetch_page(self, domain, parent_id, page):\n-        videos = self._call_api(\n-            domain, 'getreferencedobjects', parent_id, {\n-                'max': self._PAGE_SIZE,\n-                'object_type': 'video',\n-                'parent_id': parent_id,\n-                'start': page * self._PAGE_SIZE,\n-            }, 'page %d' % (page + 1))\n-        for video in videos:\n-            yield self._parse_video_data(video)\n-\n-    def _real_extract(self, url):\n-        host, playlist_id = self._match_valid_url(url).groups()\n-\n-        if host == 'cocoro.tv':\n-            webpage = self._download_webpage(url, playlist_id)\n-\n-            entries = []\n-\n-            for mobj in re.finditer(\n-                    r'<a[^>]+href=([\"\\'])(?P<url>%s.*?)\\1[^>]*>' % AsianCrushIE._VALID_URL,\n-                    webpage):\n-                attrs = extract_attributes(mobj.group(0))\n-                if attrs.get('class') == 'clearfix':\n-                    entries.append(self.url_result(\n-                        mobj.group('url'), ie=AsianCrushIE.ie_key()))\n-\n-            title = self._html_search_regex(\n-                r'(?s)<h1\\b[^>]\\bid=[\"\\']movieTitle[^>]+>(.+?)</h1>', webpage,\n-                'title', default=None) or self._og_search_title(\n-                webpage, default=None) or self._html_search_meta(\n-                'twitter:title', webpage, 'title',\n-                default=None) or self._html_extract_title(webpage)\n-            if title:\n-                title = re.sub(r'\\s*\\|\\s*.+?$', '', title)\n-\n-            description = self._og_search_description(\n-                webpage, default=None) or self._html_search_meta(\n-                'twitter:description', webpage, 'description', fatal=False)\n-        else:\n-            show = self._download_object_data(host, playlist_id, 'show')\n-            title = show.get('name')\n-            description = self._get_object_description(show)\n-            entries = OnDemandPagedList(\n-                functools.partial(self._fetch_page, host, playlist_id),\n-                self._PAGE_SIZE)\n-\n-        return self.playlist_result(entries, playlist_id, title, description)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/atttechchannel.py",
            "diff": "diff --git a/yt_dlp/extractor/atttechchannel.py b/yt_dlp/extractor/atttechchannel.py\ndeleted file mode 100644\nindex 6ff4ec0a..00000000\n--- a/yt_dlp/extractor/atttechchannel.py\n+++ /dev/null\n@@ -1,53 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import unified_strdate\n-\n-\n-class ATTTechChannelIE(InfoExtractor):\n-    _VALID_URL = r'https?://techchannel\\.att\\.com/play-video\\.cfm/([^/]+/)*(?P<id>.+)'\n-    _TEST = {\n-        'url': 'http://techchannel.att.com/play-video.cfm/2014/1/27/ATT-Archives-The-UNIX-System-Making-Computers-Easier-to-Use',\n-        'info_dict': {\n-            'id': '11316',\n-            'display_id': 'ATT-Archives-The-UNIX-System-Making-Computers-Easier-to-Use',\n-            'ext': 'flv',\n-            'title': 'AT&T Archives : The UNIX System: Making Computers Easier to Use',\n-            'description': 'A 1982 film about UNIX is the foundation for software in use around Bell Labs and AT&T.',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'upload_date': '20140127',\n-        },\n-        'params': {\n-            # rtmp download\n-            'skip_download': True,\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, display_id)\n-\n-        video_url = self._search_regex(\n-            r\"url\\s*:\\s*'(rtmp://[^']+)'\",\n-            webpage, 'video URL')\n-\n-        video_id = self._search_regex(\n-            r'mediaid\\s*=\\s*(\\d+)',\n-            webpage, 'video id', fatal=False)\n-\n-        title = self._og_search_title(webpage)\n-        description = self._og_search_description(webpage)\n-        thumbnail = self._og_search_thumbnail(webpage)\n-        upload_date = unified_strdate(self._search_regex(\n-            r'[Rr]elease\\s+date:\\s*(\\d{1,2}/\\d{1,2}/\\d{4})',\n-            webpage, 'upload date', fatal=False), False)\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'url': video_url,\n-            'ext': 'flv',\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'upload_date': upload_date,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/aws.py",
            "diff": "diff --git a/yt_dlp/extractor/aws.py b/yt_dlp/extractor/aws.py\nindex eb831a15..c4741a6a 100644\n--- a/yt_dlp/extractor/aws.py\n+++ b/yt_dlp/extractor/aws.py\n@@ -12,7 +12,7 @@ class AWSIE(InfoExtractor):  # XXX: Conventionally, base classes should end with\n \n     def _aws_execute_api(self, aws_dict, video_id, query=None):\n         query = query or {}\n-        amz_date = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n+        amz_date = datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n         date = amz_date[:8]\n         headers = {\n             'Accept': 'application/json',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/axs.py",
            "diff": "diff --git a/yt_dlp/extractor/axs.py b/yt_dlp/extractor/axs.py\nnew file mode 100644\nindex 00000000..4b263725\n--- /dev/null\n+++ b/yt_dlp/extractor/axs.py\n@@ -0,0 +1,87 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    float_or_none,\n+    js_to_json,\n+    parse_iso8601,\n+    traverse_obj,\n+    url_or_none,\n+)\n+\n+\n+class AxsIE(InfoExtractor):\n+    IE_NAME = 'axs.tv'\n+    _VALID_URL = r'https?://(?:www\\.)?axs\\.tv/(?:channel/(?:[^/?#]+/)+)?video/(?P<id>[^/?#]+)'\n+\n+    _TESTS = [{\n+        'url': 'https://www.axs.tv/video/5f4dc776b70e4f1c194f22ef/',\n+        'md5': '8d97736ae8e50c64df528e5e676778cf',\n+        'info_dict': {\n+            'id': '5f4dc776b70e4f1c194f22ef',\n+            'title': 'Small Town',\n+            'ext': 'mp4',\n+            'description': 'md5:e314d28bfaa227a4d7ec965fae19997f',\n+            'upload_date': '20230602',\n+            'timestamp': 1685729564,\n+            'duration': 1284.216,\n+            'series': 'Rock & Roll Road Trip with Sammy Hagar',\n+            'season': 2,\n+            'episode': '3',\n+            'thumbnail': 'https://images.dotstudiopro.com/5f4e9d330a0c3b295a7e8394',\n+        },\n+    }, {\n+        'url': 'https://www.axs.tv/channel/rock-star-interview/video/daryl-hall',\n+        'md5': '300ae795cd8f9984652c0949734ffbdc',\n+        'info_dict': {\n+            'id': '5f488148b70e4f392572977c',\n+            'display_id': 'daryl-hall',\n+            'title': 'Daryl Hall',\n+            'ext': 'mp4',\n+            'description': 'md5:e54ecaa0f4b5683fc9259e9e4b196628',\n+            'upload_date': '20230214',\n+            'timestamp': 1676403615,\n+            'duration': 2570.668,\n+            'series': 'The Big Interview with Dan Rather',\n+            'season': 3,\n+            'episode': '5',\n+            'thumbnail': 'https://images.dotstudiopro.com/5f4d1901f340b50d937cec32',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+        webpage = self._download_webpage(url, display_id)\n+\n+        webpage_json_data = self._search_json(\n+            r'mountObj\\s*=', webpage, 'video ID data', display_id,\n+            transform_source=js_to_json)\n+        video_id = webpage_json_data['video_id']\n+        company_id = webpage_json_data['company_id']\n+\n+        meta = self._download_json(\n+            f'https://api.myspotlight.tv/dotplayer/video/{company_id}/{video_id}',\n+            video_id, query={'device_type': 'desktop_web'})['video']\n+\n+        formats = self._extract_m3u8_formats(\n+            meta['video_m3u8'], video_id, 'mp4', m3u8_id='hls')\n+\n+        subtitles = {}\n+        for cc in traverse_obj(meta, ('closeCaption', lambda _, v: url_or_none(v['srtPath']))):\n+            subtitles.setdefault(cc.get('srtShortLang') or 'en', []).append(\n+                {'ext': cc.get('srtExt'), 'url': cc['srtPath']})\n+\n+        return {\n+            'id': video_id,\n+            'display_id': display_id,\n+            'formats': formats,\n+            **traverse_obj(meta, {\n+                'title': ('title', {str}),\n+                'description': ('description', {str}),\n+                'series': ('seriestitle', {str}),\n+                'season': ('season', {int}),\n+                'episode': ('episode', {str}),\n+                'duration': ('duration', {float_or_none}),\n+                'timestamp': ('updated_at', {parse_iso8601}),\n+                'thumbnail': ('thumb', {url_or_none}),\n+            }),\n+            'subtitles': subtitles,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/banbye.py",
            "diff": "diff --git a/yt_dlp/extractor/banbye.py b/yt_dlp/extractor/banbye.py\nindex c8734256..67af29a9 100644\n--- a/yt_dlp/extractor/banbye.py\n+++ b/yt_dlp/extractor/banbye.py\n@@ -31,7 +31,7 @@ def _extract_playlist(self, playlist_id):\n \n \n class BanByeIE(BanByeBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.)?banbye.com/(?:en/)?watch/(?P<id>\\w+)'\n+    _VALID_URL = r'https?://(?:www\\.)?banbye\\.com/(?:en/)?watch/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://banbye.com/watch/v_ytfmvkVYLE8T',\n         'md5': '2f4ea15c5ca259a73d909b2cfd558eb5',\n@@ -59,7 +59,27 @@ class BanByeIE(BanByeBaseIE):\n             'title': 'Krzysztof Karo\u0144',\n             'id': 'p_Ld82N6gBw_OJ',\n         },\n-        'playlist_count': 9,\n+        'playlist_mincount': 9,\n+    }, {\n+        'url': 'https://banbye.com/watch/v_kb6_o1Kyq-CD',\n+        'info_dict': {\n+            'id': 'v_kb6_o1Kyq-CD',\n+            'ext': 'mp4',\n+            'title': 'Co tak naprawd\u0119 dzieje si\u0119 we Francji?! Czy Warszawa a potem ca\u0142a Polska b\u0119dzie drugim Pary\u017cem?!\ud83e\udd14\ud83c\uddf5\ud83c\uddf1',\n+            'description': 'md5:82be4c0e13eae8ea1ca8b9f2e07226a8',\n+            'uploader': 'Marcin Rola - MOIM ZDANIEM!\ud83c\uddf5\ud83c\uddf1',\n+            'channel_id': 'ch_QgWnHvDG2fo5',\n+            'channel_url': 'https://banbye.com/channel/ch_QgWnHvDG2fo5',\n+            'duration': 597,\n+            'timestamp': 1688642656,\n+            'upload_date': '20230706',\n+            'thumbnail': 'https://cdn.banbye.com/video/v_kb6_o1Kyq-CD/96.webp',\n+            'tags': ['Pary\u017c', 'Francja', 'Polska', 'Imigranci', 'Morawiecki', 'Tusk'],\n+            'like_count': int,\n+            'dislike_count': int,\n+            'view_count': int,\n+            'comment_count': int,\n+        },\n     }]\n \n     def _real_extract(self, url):\n@@ -100,7 +120,7 @@ def _real_extract(self, url):\n \n \n class BanByeChannelIE(BanByeBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.)?banbye.com/(?:en/)?channel/(?P<id>\\w+)'\n+    _VALID_URL = r'https?://(?:www\\.)?banbye\\.com/(?:en/)?channel/(?P<id>\\w+)'\n     _TESTS = [{\n         'url': 'https://banbye.com/channel/ch_wrealu24',\n         'info_dict': {\n@@ -132,7 +152,7 @@ def page_func(page_num):\n                 'sort': 'new',\n                 'limit': self._PAGE_SIZE,\n                 'offset': page_num * self._PAGE_SIZE,\n-            }, note=f'Downloading page {page_num+1}')\n+            }, note=f'Downloading page {page_num + 1}')\n             return [\n                 self.url_result(f\"{self._VIDEO_BASE}/{video['_id']}\", BanByeIE)\n                 for video in data['items']\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bbc.py",
            "diff": "diff --git a/yt_dlp/extractor/bbc.py b/yt_dlp/extractor/bbc.py\nindex a55cdef2..015af9e1 100644\n--- a/yt_dlp/extractor/bbc.py\n+++ b/yt_dlp/extractor/bbc.py\n@@ -15,11 +15,13 @@\n     float_or_none,\n     get_element_by_class,\n     int_or_none,\n+    join_nonempty,\n     js_to_json,\n     parse_duration,\n     parse_iso8601,\n     parse_qs,\n     strip_or_none,\n+    traverse_obj,\n     try_get,\n     unescapeHTML,\n     unified_timestamp,\n@@ -41,7 +43,6 @@ class BBCCoUkIE(InfoExtractor):\n                             iplayer(?:/[^/]+)?/(?:episode/|playlist/)|\n                             music/(?:clips|audiovideo/popular)[/#]|\n                             radio/player/|\n-                            sounds/play/|\n                             events/[^/]+/play/[^/]+/\n                         )\n                         (?P<id>%s)(?!/(?:episodes|broadcasts|clips))\n@@ -218,20 +219,6 @@ class BBCCoUkIE(InfoExtractor):\n                 # rtmp download\n                 'skip_download': True,\n             },\n-        }, {\n-            'url': 'https://www.bbc.co.uk/sounds/play/m0007jzb',\n-            'note': 'Audio',\n-            'info_dict': {\n-                'id': 'm0007jz9',\n-                'ext': 'mp4',\n-                'title': 'BBC Proms, 2019, Prom 34: West\u2013Eastern Divan Orchestra',\n-                'description': \"Live BBC Proms. West\u2013Eastern Divan Orchestra with Daniel Barenboim and Martha Argerich.\",\n-                'duration': 9840,\n-            },\n-            'params': {\n-                # rtmp download\n-                'skip_download': True,\n-            }\n         }, {\n             'url': 'http://www.bbc.co.uk/iplayer/playlist/p01dvks4',\n             'only_matching': True,\n@@ -330,16 +317,25 @@ def _raise_extractor_error(self, media_selection_error):\n \n     def _download_media_selector(self, programme_id):\n         last_exception = None\n+        formats, subtitles = [], {}\n         for media_set in self._MEDIA_SETS:\n             try:\n-                return self._download_media_selector_url(\n+                fmts, subs = self._download_media_selector_url(\n                     self._MEDIA_SELECTOR_URL_TEMPL % (media_set, programme_id), programme_id)\n+                formats.extend(fmts)\n+                if subs:\n+                    self._merge_subtitles(subs, target=subtitles)\n             except BBCCoUkIE.MediaSelectionError as e:\n                 if e.id in ('notukerror', 'geolocation', 'selectionunavailable'):\n                     last_exception = e\n                     continue\n                 self._raise_extractor_error(e)\n-        self._raise_extractor_error(last_exception)\n+        if last_exception:\n+            if formats or subtitles:\n+                self.report_warning(f'{self.IE_NAME} returned error: {last_exception.id}')\n+            else:\n+                self._raise_extractor_error(last_exception)\n+        return formats, subtitles\n \n     def _download_media_selector_url(self, url, programme_id=None):\n         media_selection = self._download_json(\n@@ -844,6 +840,20 @@ class BBCIE(BBCCoUkIE):  # XXX: Do not subclass from concrete IE\n             'upload_date': '20190604',\n             'categories': ['Psychology'],\n         },\n+    }, {\n+        # BBC Sounds\n+        'url': 'https://www.bbc.co.uk/sounds/play/m001q78b',\n+        'info_dict': {\n+            'id': 'm001q789',\n+            'ext': 'mp4',\n+            'title': 'The Night Tracks Mix - Music for the darkling hour',\n+            'thumbnail': 'https://ichef.bbci.co.uk/images/ic/raw/p0c00hym.jpg',\n+            'chapters': 'count:8',\n+            'description': 'md5:815fb51cbdaa270040aab8145b3f1d67',\n+            'uploader': 'Radio 3',\n+            'duration': 1800,\n+            'uploader_id': 'bbc_radio_three',\n+        },\n     }, {  # onion routes\n         'url': 'https://www.bbcnewsd73hkzno2ini43t4gblxvycyac5aw4gnv7t2rccijh7745uqd.onion/news/av/world-europe-63208576',\n         'only_matching': True,\n@@ -1128,6 +1138,13 @@ def _real_extract(self, url):\n                     'uploader_id': network.get('id'),\n                     'formats': formats,\n                     'subtitles': subtitles,\n+                    'chapters': traverse_obj(preload_state, (\n+                        'tracklist', 'tracks', lambda _, v: float_or_none(v['offset']['start']), {\n+                            'title': ('titles', {lambda x: join_nonempty(\n+                                'primary', 'secondary', 'tertiary', delim=' - ', from_dict=x)}),\n+                            'start_time': ('offset', 'start', {float_or_none}),\n+                            'end_time': ('offset', 'end', {float_or_none}),\n+                        })) or None,\n                 }\n \n         bbc3_config = self._parse_json(\n@@ -1180,7 +1197,7 @@ def _real_extract(self, url):\n         if initial_data is None:\n             initial_data = self._search_regex(\n                 r'window\\.__INITIAL_DATA__\\s*=\\s*({.+?})\\s*;', webpage,\n-                'preload state', default={})\n+                'preload state', default='{}')\n         else:\n             initial_data = self._parse_json(initial_data or '\"{}\"', playlist_id, fatal=False)\n         initial_data = self._parse_json(initial_data, playlist_id, fatal=False)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/beatbump.py",
            "diff": "diff --git a/yt_dlp/extractor/beatbump.py b/yt_dlp/extractor/beatbump.py\nindex 0f40ebe7..f48566b2 100644\n--- a/yt_dlp/extractor/beatbump.py\n+++ b/yt_dlp/extractor/beatbump.py\n@@ -3,14 +3,13 @@\n \n \n class BeatBumpVideoIE(InfoExtractor):\n-    _VALID_URL = r'https://beatbump\\.ml/listen\\?id=(?P<id>[\\w-]+)'\n+    _VALID_URL = r'https://beatbump\\.(?:ml|io)/listen\\?id=(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://beatbump.ml/listen?id=MgNrAu2pzNs',\n         'md5': '5ff3fff41d3935b9810a9731e485fe66',\n         'info_dict': {\n             'id': 'MgNrAu2pzNs',\n             'ext': 'mp4',\n-            'uploader_url': 'http://www.youtube.com/channel/UC-pWHpBjdGG69N9mM2auIAA',\n             'artist': 'Stephen',\n             'thumbnail': 'https://i.ytimg.com/vi_webp/MgNrAu2pzNs/maxresdefault.webp',\n             'channel_url': 'https://www.youtube.com/channel/UC-pWHpBjdGG69N9mM2auIAA',\n@@ -22,10 +21,9 @@ class BeatBumpVideoIE(InfoExtractor):\n             'alt_title': 'Voyeur Girl',\n             'view_count': int,\n             'track': 'Voyeur Girl',\n-            'uploader': 'Stephen - Topic',\n+            'uploader': 'Stephen',\n             'title': 'Voyeur Girl',\n             'channel_follower_count': int,\n-            'uploader_id': 'UC-pWHpBjdGG69N9mM2auIAA',\n             'age_limit': 0,\n             'availability': 'public',\n             'live_status': 'not_live',\n@@ -36,7 +34,12 @@ class BeatBumpVideoIE(InfoExtractor):\n             'tags': 'count:11',\n             'creator': 'Stephen',\n             'channel_id': 'UC-pWHpBjdGG69N9mM2auIAA',\n-        }\n+            'channel_is_verified': True,\n+            'heatmap': 'count:100',\n+        },\n+    }, {\n+        'url': 'https://beatbump.io/listen?id=LDGZAprNGWo',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n@@ -45,7 +48,7 @@ def _real_extract(self, url):\n \n \n class BeatBumpPlaylistIE(InfoExtractor):\n-    _VALID_URL = r'https://beatbump\\.ml/(?:release\\?id=|artist/|playlist/)(?P<id>[\\w-]+)'\n+    _VALID_URL = r'https://beatbump\\.(?:ml|io)/(?:release\\?id=|artist/|playlist/)(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://beatbump.ml/release?id=MPREb_gTAcphH99wE',\n         'playlist_count': 50,\n@@ -56,25 +59,28 @@ class BeatBumpPlaylistIE(InfoExtractor):\n             'title': 'Album - Royalty Free Music Library V2 (50 Songs)',\n             'description': '',\n             'tags': [],\n-            'modified_date': '20221223',\n-        }\n+            'modified_date': '20231110',\n+        },\n+        'expected_warnings': ['YouTube Music is not directly supported'],\n     }, {\n         'url': 'https://beatbump.ml/artist/UC_aEa8K-EOJ3D6gOs7HcyNg',\n         'playlist_mincount': 1,\n         'params': {'flatplaylist': True},\n         'info_dict': {\n             'id': 'UC_aEa8K-EOJ3D6gOs7HcyNg',\n-            'uploader_url': 'https://www.youtube.com/channel/UC_aEa8K-EOJ3D6gOs7HcyNg',\n+            'uploader_url': 'https://www.youtube.com/@NoCopyrightSounds',\n             'channel_url': 'https://www.youtube.com/channel/UC_aEa8K-EOJ3D6gOs7HcyNg',\n-            'uploader_id': 'UC_aEa8K-EOJ3D6gOs7HcyNg',\n+            'uploader_id': '@NoCopyrightSounds',\n             'channel_follower_count': int,\n-            'title': 'NoCopyrightSounds - Videos',\n+            'title': 'NoCopyrightSounds',\n             'uploader': 'NoCopyrightSounds',\n             'description': 'md5:cd4fd53d81d363d05eee6c1b478b491a',\n             'channel': 'NoCopyrightSounds',\n-            'tags': 'count:12',\n+            'tags': 'count:65',\n             'channel_id': 'UC_aEa8K-EOJ3D6gOs7HcyNg',\n+            'channel_is_verified': True,\n         },\n+        'expected_warnings': ['YouTube Music is not directly supported'],\n     }, {\n         'url': 'https://beatbump.ml/playlist/VLPLRBp0Fe2GpgmgoscNFLxNyBVSFVdYmFkq',\n         'playlist_mincount': 1,\n@@ -84,16 +90,20 @@ class BeatBumpPlaylistIE(InfoExtractor):\n             'uploader_url': 'https://www.youtube.com/@NoCopyrightSounds',\n             'description': 'Providing you with copyright free / safe music for gaming, live streaming, studying and more!',\n             'view_count': int,\n-            'channel_url': 'https://www.youtube.com/@NoCopyrightSounds',\n-            'uploader_id': 'UC_aEa8K-EOJ3D6gOs7HcyNg',\n+            'channel_url': 'https://www.youtube.com/channel/UC_aEa8K-EOJ3D6gOs7HcyNg',\n+            'uploader_id': '@NoCopyrightSounds',\n             'title': 'NCS : All Releases \ud83d\udcbf',\n             'uploader': 'NoCopyrightSounds',\n             'availability': 'public',\n             'channel': 'NoCopyrightSounds',\n             'tags': [],\n-            'modified_date': '20221225',\n+            'modified_date': '20231112',\n             'channel_id': 'UC_aEa8K-EOJ3D6gOs7HcyNg',\n-        }\n+        },\n+        'expected_warnings': ['YouTube Music is not directly supported'],\n+    }, {\n+        'url': 'https://beatbump.io/playlist/VLPLFCHGavqRG-q_2ZhmgU2XB2--ZY6irT1c',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/behindkink.py",
            "diff": "diff --git a/yt_dlp/extractor/behindkink.py b/yt_dlp/extractor/behindkink.py\nindex ca449815..9d2324f4 100644\n--- a/yt_dlp/extractor/behindkink.py\n+++ b/yt_dlp/extractor/behindkink.py\n@@ -3,6 +3,7 @@\n \n \n class BehindKinkIE(InfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?behindkink\\.com/(?P<year>[0-9]{4})/(?P<month>[0-9]{2})/(?P<day>[0-9]{2})/(?P<id>[^/#?_]+)'\n     _TEST = {\n         'url': 'http://www.behindkink.com/2014/12/05/what-are-you-passionate-about-marley-blaze/',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bet.py",
            "diff": "diff --git a/yt_dlp/extractor/bet.py b/yt_dlp/extractor/bet.py\nindex 6b867d13..cbf3dd08 100644\n--- a/yt_dlp/extractor/bet.py\n+++ b/yt_dlp/extractor/bet.py\n@@ -1,10 +1,9 @@\n from .mtv import MTVServicesInfoExtractor\n from ..utils import unified_strdate\n \n-# TODO Remove - Reason: Outdated Site\n-\n \n class BetIE(MTVServicesInfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?bet\\.com/(?:[^/]+/)+(?P<id>.+?)\\.html'\n     _TESTS = [\n         {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bfi.py",
            "diff": "diff --git a/yt_dlp/extractor/bfi.py b/yt_dlp/extractor/bfi.py\nindex 76f0516a..a6ebfedf 100644\n--- a/yt_dlp/extractor/bfi.py\n+++ b/yt_dlp/extractor/bfi.py\n@@ -5,6 +5,7 @@\n \n \n class BFIPlayerIE(InfoExtractor):\n+    _WORKING = False\n     IE_NAME = 'bfi:player'\n     _VALID_URL = r'https?://player\\.bfi\\.org\\.uk/[^/]+/film/watch-(?P<id>[\\w-]+)-online'\n     _TEST = {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bfmtv.py",
            "diff": "diff --git a/yt_dlp/extractor/bfmtv.py b/yt_dlp/extractor/bfmtv.py\nindex a7be0e67..5d0c73ff 100644\n--- a/yt_dlp/extractor/bfmtv.py\n+++ b/yt_dlp/extractor/bfmtv.py\n@@ -7,7 +7,7 @@\n class BFMTVBaseIE(InfoExtractor):\n     _VALID_URL_BASE = r'https?://(?:www\\.|rmc\\.)?bfmtv\\.com/'\n     _VALID_URL_TMPL = _VALID_URL_BASE + r'(?:[^/]+/)*[^/?&#]+_%s[A-Z]-(?P<id>\\d{12})\\.html'\n-    _VIDEO_BLOCK_REGEX = r'(<div[^>]+class=\"video_block\"[^>]*>)'\n+    _VIDEO_BLOCK_REGEX = r'(<div[^>]+class=\"video_block[^\"]*\"[^>]*>)'\n     BRIGHTCOVE_URL_TEMPLATE = 'http://players.brightcove.net/%s/%s_default/index.html?videoId=%s'\n \n     def _brightcove_url_result(self, video_id, video_block):\n@@ -55,8 +55,11 @@ class BFMTVLiveIE(BFMTVIE):  # XXX: Do not subclass from concrete IE\n             'ext': 'mp4',\n             'title': r're:^le direct BFMTV WEB \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}$',\n             'uploader_id': '876450610001',\n-            'upload_date': '20171018',\n-            'timestamp': 1508329950,\n+            'upload_date': '20220926',\n+            'timestamp': 1664207191,\n+            'live_status': 'is_live',\n+            'thumbnail': r're:https://.+/image\\.jpg',\n+            'tags': [],\n         },\n         'params': {\n             'skip_download': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bigo.py",
            "diff": "diff --git a/yt_dlp/extractor/bigo.py b/yt_dlp/extractor/bigo.py\nindex 1cb6e58b..acf78e49 100644\n--- a/yt_dlp/extractor/bigo.py\n+++ b/yt_dlp/extractor/bigo.py\n@@ -29,7 +29,8 @@ def _real_extract(self, url):\n \n         info_raw = self._download_json(\n             'https://ta.bigo.tv/official_website/studio/getInternalStudioInfo',\n-            user_id, data=urlencode_postdata({'siteId': user_id}))\n+            user_id, data=urlencode_postdata({'siteId': user_id}),\n+            headers={'Accept': 'application/json'})\n \n         if not isinstance(info_raw, dict):\n             raise ExtractorError('Received invalid JSON data')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bild.py",
            "diff": "diff --git a/yt_dlp/extractor/bild.py b/yt_dlp/extractor/bild.py\nindex f3dea33c..eb289329 100644\n--- a/yt_dlp/extractor/bild.py\n+++ b/yt_dlp/extractor/bild.py\n@@ -1,6 +1,7 @@\n from .common import InfoExtractor\n from ..utils import (\n     int_or_none,\n+    traverse_obj,\n     unescapeHTML,\n )\n \n@@ -8,7 +9,8 @@\n class BildIE(InfoExtractor):\n     _VALID_URL = r'https?://(?:www\\.)?bild\\.de/(?:[^/]+/)+(?P<display_id>[^/]+)-(?P<id>\\d+)(?:,auto=true)?\\.bild\\.html'\n     IE_DESC = 'Bild.de'\n-    _TEST = {\n+    _TESTS = [{\n+        'note': 'static MP4 only',\n         'url': 'http://www.bild.de/video/clip/apple-ipad-air/das-koennen-die-neuen-ipads-38184146.bild.html',\n         'md5': 'dd495cbd99f2413502a1713a1156ac8a',\n         'info_dict': {\n@@ -19,7 +21,19 @@ class BildIE(InfoExtractor):\n             'thumbnail': r're:^https?://.*\\.jpg$',\n             'duration': 196,\n         }\n-    }\n+    }, {\n+        'note': 'static MP4 and HLS',\n+        'url': 'https://www.bild.de/video/clip/news-ausland/deftiger-abgang-vom-10m-turm-bademeister-sorgt-fuer-skandal-85158620.bild.html',\n+        'md5': 'fb0ed4f09c495d4ba7ce2eee0bb90de1',\n+        'info_dict': {\n+            'id': '85158620',\n+            'ext': 'mp4',\n+            'title': 'Der Sprungturm-Skandal',\n+            'description': 'md5:709b543c24dc31bbbffee73bccda34ad',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'duration': 69,\n+        }\n+    }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n@@ -27,11 +41,23 @@ def _real_extract(self, url):\n         video_data = self._download_json(\n             url.split('.bild.html')[0] + ',view=json.bild.html', video_id)\n \n+        formats = []\n+        for src in traverse_obj(video_data, ('clipList', 0, 'srces', lambda _, v: v['src'])):\n+            src_type = src.get('type')\n+            if src_type == 'application/x-mpegURL':\n+                formats.extend(\n+                    self._extract_m3u8_formats(\n+                        src['src'], video_id, 'mp4', m3u8_id='hls', fatal=False))\n+            elif src_type == 'video/mp4':\n+                formats.append({'url': src['src'], 'format_id': 'http-mp4'})\n+            else:\n+                self.report_warning(f'Skipping unsupported format type: \"{src_type}\"')\n+\n         return {\n             'id': video_id,\n             'title': unescapeHTML(video_data['title']).strip(),\n             'description': unescapeHTML(video_data.get('description')),\n-            'url': video_data['clipList'][0]['srces'][0]['src'],\n+            'formats': formats,\n             'thumbnail': video_data.get('poster'),\n             'duration': int_or_none(video_data.get('durationSec')),\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bilibili.py",
            "diff": "diff --git a/yt_dlp/extractor/bilibili.py b/yt_dlp/extractor/bilibili.py\nindex cb7ab2a1..bc25dc75 100644\n--- a/yt_dlp/extractor/bilibili.py\n+++ b/yt_dlp/extractor/bilibili.py\n@@ -2,7 +2,9 @@\n import functools\n import hashlib\n import itertools\n+import json\n import math\n+import re\n import time\n import urllib.parse\n \n@@ -14,9 +16,12 @@\n     GeoRestrictedError,\n     InAdvancePagedList,\n     OnDemandPagedList,\n+    bool_or_none,\n+    clean_html,\n     filter_dict,\n     float_or_none,\n     format_field,\n+    get_element_by_class,\n     int_or_none,\n     join_nonempty,\n     make_archive_id,\n@@ -34,27 +39,31 @@\n     unsmuggle_url,\n     url_or_none,\n     urlencode_postdata,\n+    variadic,\n )\n \n \n class BilibiliBaseIE(InfoExtractor):\n+    _FORMAT_ID_RE = re.compile(r'-(\\d+)\\.m4s\\?')\n+\n     def extract_formats(self, play_info):\n         format_names = {\n             r['quality']: traverse_obj(r, 'new_description', 'display_desc')\n             for r in traverse_obj(play_info, ('support_formats', lambda _, v: v['quality']))\n         }\n \n-        audios = traverse_obj(play_info, ('dash', 'audio', ...))\n+        audios = traverse_obj(play_info, ('dash', (None, 'dolby'), 'audio', ..., {dict}))\n         flac_audio = traverse_obj(play_info, ('dash', 'flac', 'audio'))\n         if flac_audio:\n             audios.append(flac_audio)\n         formats = [{\n             'url': traverse_obj(audio, 'baseUrl', 'base_url', 'url'),\n             'ext': mimetype2ext(traverse_obj(audio, 'mimeType', 'mime_type')),\n-            'acodec': audio.get('codecs'),\n+            'acodec': traverse_obj(audio, ('codecs', {str.lower})),\n             'vcodec': 'none',\n             'tbr': float_or_none(audio.get('bandwidth'), scale=1000),\n-            'filesize': int_or_none(audio.get('size'))\n+            'filesize': int_or_none(audio.get('size')),\n+            'format_id': str_or_none(audio.get('id')),\n         } for audio in audios]\n \n         formats.extend({\n@@ -65,9 +74,13 @@ def extract_formats(self, play_info):\n             'height': int_or_none(video.get('height')),\n             'vcodec': video.get('codecs'),\n             'acodec': 'none' if audios else None,\n+            'dynamic_range': {126: 'DV', 125: 'HDR10'}.get(int_or_none(video.get('id'))),\n             'tbr': float_or_none(video.get('bandwidth'), scale=1000),\n             'filesize': int_or_none(video.get('size')),\n             'quality': int_or_none(video.get('id')),\n+            'format_id': traverse_obj(\n+                video, (('baseUrl', 'base_url'), {self._FORMAT_ID_RE.search}, 1),\n+                ('id', {str_or_none}), get_all=False),\n             'format': format_names.get(video.get('id')),\n         } for video in traverse_obj(play_info, ('dash', 'video', ...)))\n \n@@ -78,6 +91,12 @@ def extract_formats(self, play_info):\n \n         return formats\n \n+    def _download_playinfo(self, video_id, cid):\n+        return self._download_json(\n+            'https://api.bilibili.com/x/player/playurl', video_id,\n+            query={'bvid': video_id, 'cid': cid, 'fnval': 4048},\n+            note=f'Downloading video formats for cid {cid}')['data']\n+\n     def json2srt(self, json_data):\n         srt_data = ''\n         for idx, line in enumerate(json_data.get('body') or []):\n@@ -86,7 +105,7 @@ def json2srt(self, json_data):\n                          f'{line[\"content\"]}\\n\\n')\n         return srt_data\n \n-    def _get_subtitles(self, video_id, aid, cid):\n+    def _get_subtitles(self, video_id, cid, aid=None):\n         subtitles = {\n             'danmaku': [{\n                 'ext': 'xml',\n@@ -94,8 +113,15 @@ def _get_subtitles(self, video_id, aid, cid):\n             }]\n         }\n \n-        video_info_json = self._download_json(f'https://api.bilibili.com/x/player/v2?aid={aid}&cid={cid}', video_id)\n-        for s in traverse_obj(video_info_json, ('data', 'subtitle', 'subtitles', ...)):\n+        subtitle_info = traverse_obj(self._download_json(\n+            'https://api.bilibili.com/x/player/v2', video_id,\n+            query={'aid': aid, 'cid': cid} if aid else {'bvid': video_id, 'cid': cid},\n+            note=f'Extracting subtitle info {cid}'), ('data', 'subtitle'))\n+        subs_list = traverse_obj(subtitle_info, ('subtitles', lambda _, v: v['subtitle_url'] and v['lan']))\n+        if not subs_list and traverse_obj(subtitle_info, 'allow_submit'):\n+            if not self._get_cookies('https://api.bilibili.com').get('SESSDATA'):  # no login session cookie\n+                self.report_warning(f'CC subtitles (if any) are only visible when logged in. {self._login_hint()}', only_once=True)\n+        for s in subs_list:\n             subtitles.setdefault(s['lan'], []).append({\n                 'ext': 'srt',\n                 'data': self.json2srt(self._download_json(s['subtitle_url'], video_id))\n@@ -145,11 +171,58 @@ def _get_episodes_from_season(self, ss_id, url):\n         for entry in traverse_obj(season_info, (\n                 'result', 'main_section', 'episodes',\n                 lambda _, v: url_or_none(v['share_url']) and v['id'])):\n-            yield self.url_result(entry['share_url'], BiliBiliBangumiIE, f'ep{entry[\"id\"]}')\n+            yield self.url_result(entry['share_url'], BiliBiliBangumiIE, str_or_none(entry.get('id')))\n+\n+    def _get_divisions(self, video_id, graph_version, edges, edge_id, cid_edges=None):\n+        cid_edges = cid_edges or {}\n+        division_data = self._download_json(\n+            'https://api.bilibili.com/x/stein/edgeinfo_v2', video_id,\n+            query={'graph_version': graph_version, 'edge_id': edge_id, 'bvid': video_id},\n+            note=f'Extracting divisions from edge {edge_id}')\n+        edges.setdefault(edge_id, {}).update(\n+            traverse_obj(division_data, ('data', 'story_list', lambda _, v: v['edge_id'] == edge_id, {\n+                'title': ('title', {str}),\n+                'cid': ('cid', {int_or_none}),\n+            }), get_all=False))\n+\n+        edges[edge_id].update(traverse_obj(division_data, ('data', {\n+            'title': ('title', {str}),\n+            'choices': ('edges', 'questions', ..., 'choices', ..., {\n+                'edge_id': ('id', {int_or_none}),\n+                'cid': ('cid', {int_or_none}),\n+                'text': ('option', {str}),\n+            }),\n+        })))\n+        # use dict to combine edges that use the same video section (same cid)\n+        cid_edges.setdefault(edges[edge_id]['cid'], {})[edge_id] = edges[edge_id]\n+        for choice in traverse_obj(edges, (edge_id, 'choices', ...)):\n+            if choice['edge_id'] not in edges:\n+                edges[choice['edge_id']] = {'cid': choice['cid']}\n+                self._get_divisions(video_id, graph_version, edges, choice['edge_id'], cid_edges=cid_edges)\n+        return cid_edges\n+\n+    def _get_interactive_entries(self, video_id, cid, metainfo):\n+        graph_version = traverse_obj(\n+            self._download_json(\n+                'https://api.bilibili.com/x/player/wbi/v2', video_id,\n+                'Extracting graph version', query={'bvid': video_id, 'cid': cid}),\n+            ('data', 'interaction', 'graph_version', {int_or_none}))\n+        cid_edges = self._get_divisions(video_id, graph_version, {1: {'cid': cid}}, 1)\n+        for cid, edges in cid_edges.items():\n+            play_info = self._download_playinfo(video_id, cid)\n+            yield {\n+                **metainfo,\n+                'id': f'{video_id}_{cid}',\n+                'title': f'{metainfo.get(\"title\")} - {list(edges.values())[0].get(\"title\")}',\n+                'formats': self.extract_formats(play_info),\n+                'description': f'{json.dumps(edges, ensure_ascii=False)}\\n{metainfo.get(\"description\", \"\")}',\n+                'duration': float_or_none(play_info.get('timelength'), scale=1000),\n+                'subtitles': self.extract_subtitles(video_id, cid),\n+            }\n \n \n class BiliBiliIE(BilibiliBaseIE):\n-    _VALID_URL = r'https?://www\\.bilibili\\.com/(?:video/|festival/\\w+\\?(?:[^#]*&)?bvid=)[aAbB][vV](?P<id>[^/?#&]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/(?:video/|festival/\\w+\\?(?:[^#]*&)?bvid=)[aAbB][vV](?P<id>[^/?#&]+)'\n \n     _TESTS = [{\n         'url': 'https://www.bilibili.com/video/BV13x41117TL',\n@@ -170,7 +243,7 @@ class BiliBiliIE(BilibiliBaseIE):\n             'view_count': int,\n         },\n     }, {\n-        # old av URL version\n+        'note': 'old av URL version',\n         'url': 'http://www.bilibili.com/video/av1074402/',\n         'info_dict': {\n             'thumbnail': r're:^https?://.*\\.(jpg|jpeg)$',\n@@ -202,7 +275,7 @@ class BiliBiliIE(BilibiliBaseIE):\n                 'id': 'BV1bK411W797_p1',\n                 'ext': 'mp4',\n                 'title': '\u7269\u8bed\u4e2d\u7684\u4eba\u7269\u662f\u5982\u4f55\u5410\u69fd\u81ea\u5df1\u7684OP\u7684 p01 Staple Stable/\u6218\u573a\u539f+\u7fbd\u5ddd',\n-                'tags': 'count:11',\n+                'tags': 'count:10',\n                 'timestamp': 1589601697,\n                 'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n                 'uploader': '\u6253\u724c\u8fd8\u662f\u6253\u6869',\n@@ -222,7 +295,7 @@ class BiliBiliIE(BilibiliBaseIE):\n             'id': 'BV1bK411W797_p1',\n             'ext': 'mp4',\n             'title': '\u7269\u8bed\u4e2d\u7684\u4eba\u7269\u662f\u5982\u4f55\u5410\u69fd\u81ea\u5df1\u7684OP\u7684 p01 Staple Stable/\u6218\u573a\u539f+\u7fbd\u5ddd',\n-            'tags': 'count:11',\n+            'tags': 'count:10',\n             'timestamp': 1589601697,\n             'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n             'uploader': '\u6253\u724c\u8fd8\u662f\u6253\u6869',\n@@ -245,7 +318,7 @@ class BiliBiliIE(BilibiliBaseIE):\n             'description': 'md5:afde2b7ba9025c01d9e3dde10de221e4',\n             'duration': 313.557,\n             'upload_date': '20220709',\n-            'uploader': '\u5c0f\u592bTech',\n+            'uploader': '\u5c0f\u592b\u592a\u6e34',\n             'timestamp': 1657347907,\n             'uploader_id': '1326814124',\n             'comment_count': int,\n@@ -333,18 +406,120 @@ class BiliBiliIE(BilibiliBaseIE):\n             'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n         },\n         'params': {'skip_download': True},\n+    }, {\n+        'note': 'interactive/split-path video',\n+        'url': 'https://www.bilibili.com/video/BV1af4y1H7ga/',\n+        'info_dict': {\n+            'id': 'BV1af4y1H7ga',\n+            'title': '\u3010\u4e92\u52a8\u6e38\u620f\u3011\u82b1\u4e86\u5927\u534a\u5e74\u65f6\u95f4\u505a\u7684\u81ea\u6211\u4ecb\u7ecd~\u8bf7\u67e5\u6536\uff01\uff01',\n+            'timestamp': 1630500414,\n+            'upload_date': '20210901',\n+            'description': 'md5:01113e39ab06e28042d74ac356a08786',\n+            'tags': list,\n+            'uploader': '\u9489\u5bab\u59ae\u59aeNinico',\n+            'duration': 1503,\n+            'uploader_id': '8881297',\n+            'comment_count': int,\n+            'view_count': int,\n+            'like_count': int,\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n+        },\n+        'playlist_count': 33,\n+        'playlist': [{\n+            'info_dict': {\n+                'id': 'BV1af4y1H7ga_400950101',\n+                'ext': 'mp4',\n+                'title': '\u3010\u4e92\u52a8\u6e38\u620f\u3011\u82b1\u4e86\u5927\u534a\u5e74\u65f6\u95f4\u505a\u7684\u81ea\u6211\u4ecb\u7ecd~\u8bf7\u67e5\u6536\uff01\uff01 - \u542c\u89c1\u732b\u732b\u53eb~',\n+                'timestamp': 1630500414,\n+                'upload_date': '20210901',\n+                'description': 'md5:db66ac7a2813a94b8291dbce990cc5b2',\n+                'tags': list,\n+                'uploader': '\u9489\u5bab\u59ae\u59aeNinico',\n+                'duration': 11.605,\n+                'uploader_id': '8881297',\n+                'comment_count': int,\n+                'view_count': int,\n+                'like_count': int,\n+                'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n+            },\n+        }],\n+    }, {\n+        'note': '301 redirect to bangumi link',\n+        'url': 'https://www.bilibili.com/video/BV1TE411f7f1',\n+        'info_dict': {\n+            'id': '288525',\n+            'title': '\u674e\u6c38\u4e50\u8001\u5e08 \u94b1\u5b66\u68ee\u5f39\u9053\u548c\u4e58\u6ce2\u4f53\u98de\u884c\u5668\u662f\u4ec0\u4e48\uff1f',\n+            'ext': 'mp4',\n+            'series': '\u6211\u548c\u6211\u7684\u7956\u56fd',\n+            'series_id': '4780',\n+            'season': '\u5e55\u540e\u7eaa\u5b9e',\n+            'season_id': '28609',\n+            'season_number': 1,\n+            'episode': '\u94b1\u5b66\u68ee\u5f39\u9053\u548c\u4e58\u6ce2\u4f53\u98de\u884c\u5668\u662f\u4ec0\u4e48\uff1f',\n+            'episode_id': '288525',\n+            'episode_number': 105,\n+            'duration': 1183.957,\n+            'timestamp': 1571648124,\n+            'upload_date': '20191021',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n+        },\n+    }, {\n+        'url': 'https://www.bilibili.com/video/BV1jL41167ZG/',\n+        'info_dict': {\n+            'id': 'BV1jL41167ZG',\n+            'title': '\u4e00\u573a\u5927\u706b\u5f15\u53d1\u7684\u79bb\u5947\u6b7b\u4ea1\uff01\u53e4\u5178\u63a8\u7406\u7ecf\u5178\u77ed\u7bc7\u96c6\u300a\u4e0d\u53ef\u80fd\u72af\u7f6a\u8bca\u65ad\u4e66\u300b\uff01',\n+            'ext': 'mp4',\n+        },\n+        'skip': 'supporter-only video',\n+    }, {\n+        'url': 'https://www.bilibili.com/video/BV1Ks411f7aQ/',\n+        'info_dict': {\n+            'id': 'BV1Ks411f7aQ',\n+            'title': '\u3010BD1080P\u3011\u72fc\u4e0e\u9999\u8f9b\u6599I\u3010\u534e\u76df\u3011',\n+            'ext': 'mp4',\n+        },\n+        'skip': 'login required',\n+    }, {\n+        'url': 'https://www.bilibili.com/video/BV1GJ411x7h7/',\n+        'info_dict': {\n+            'id': 'BV1GJ411x7h7',\n+            'title': '\u3010\u5b98\u65b9 MV\u3011Never Gonna Give You Up - Rick Astley',\n+            'ext': 'mp4',\n+        },\n+        'skip': 'geo-restricted',\n     }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n+        webpage, urlh = self._download_webpage_handle(url, video_id)\n+        if not self._match_valid_url(urlh.url):\n+            return self.url_result(urlh.url)\n+\n         initial_state = self._search_json(r'window\\.__INITIAL_STATE__\\s*=', webpage, 'initial state', video_id)\n \n         is_festival = 'videoData' not in initial_state\n         if is_festival:\n             video_data = initial_state['videoInfo']\n         else:\n-            play_info = self._search_json(r'window\\.__playinfo__\\s*=', webpage, 'play info', video_id)['data']\n+            play_info_obj = self._search_json(\n+                r'window\\.__playinfo__\\s*=', webpage, 'play info', video_id, fatal=False)\n+            if not play_info_obj:\n+                if traverse_obj(initial_state, ('error', 'trueCode')) == -403:\n+                    self.raise_login_required()\n+                if traverse_obj(initial_state, ('error', 'trueCode')) == -404:\n+                    raise ExtractorError(\n+                        'This video may be deleted or geo-restricted. '\n+                        'You might want to try a VPN or a proxy server (with --proxy)', expected=True)\n+            play_info = traverse_obj(play_info_obj, ('data', {dict}))\n+            if not play_info:\n+                if traverse_obj(play_info_obj, 'code') == 87007:\n+                    toast = get_element_by_class('tips-toast', webpage) or ''\n+                    msg = clean_html(\n+                        f'{get_element_by_class(\"belongs-to\", toast) or \"\"}\uff0c'\n+                        + (get_element_by_class('level', toast) or ''))\n+                    raise ExtractorError(\n+                        f'This is a supporter-only video: {msg}. {self._login_hint()}', expected=True)\n+                raise ExtractorError('Failed to extract play info')\n             video_data = initial_state['videoData']\n \n         video_id, title = video_data['bvid'], video_data.get('title')\n@@ -375,10 +550,7 @@ def _real_extract(self, url):\n \n         festival_info = {}\n         if is_festival:\n-            play_info = self._download_json(\n-                'https://api.bilibili.com/x/player/playurl', video_id,\n-                query={'bvid': video_id, 'cid': cid, 'fnval': 4048},\n-                note='Extracting festival video formats')['data']\n+            play_info = self._download_playinfo(video_id, cid)\n \n             festival_info = traverse_obj(initial_state, {\n                 'uploader': ('videoInfo', 'upName'),\n@@ -387,7 +559,7 @@ def _real_extract(self, url):\n                 'thumbnail': ('sectionEpisodes', lambda _, v: v['bvid'] == video_id, 'cover'),\n             }, get_all=False)\n \n-        return {\n+        metainfo = {\n             **traverse_obj(initial_state, {\n                 'uploader': ('upData', 'name'),\n                 'uploader_id': ('upData', 'mid', {str_or_none}),\n@@ -403,28 +575,59 @@ def _real_extract(self, url):\n                 'comment_count': ('stat', 'reply', {int_or_none}),\n             }, get_all=False),\n             'id': f'{video_id}{format_field(part_id, None, \"_p%d\")}',\n-            'formats': self.extract_formats(play_info),\n             '_old_archive_ids': [make_archive_id(self, old_video_id)] if old_video_id else None,\n             'title': title,\n-            'duration': float_or_none(play_info.get('timelength'), scale=1000),\n-            'chapters': self._get_chapters(aid, cid),\n-            'subtitles': self.extract_subtitles(video_id, aid, cid),\n-            '__post_extractor': self.extract_comments(aid),\n             'http_headers': {'Referer': url},\n         }\n \n+        is_interactive = traverse_obj(video_data, ('rights', 'is_stein_gate'))\n+        if is_interactive:\n+            return self.playlist_result(\n+                self._get_interactive_entries(video_id, cid, metainfo), **metainfo, **{\n+                    'duration': traverse_obj(initial_state, ('videoData', 'duration', {int_or_none})),\n+                    '__post_extractor': self.extract_comments(aid),\n+                })\n+        else:\n+            return {\n+                **metainfo,\n+                'duration': float_or_none(play_info.get('timelength'), scale=1000),\n+                'chapters': self._get_chapters(aid, cid),\n+                'subtitles': self.extract_subtitles(video_id, cid),\n+                'formats': self.extract_formats(play_info),\n+                '__post_extractor': self.extract_comments(aid),\n+            }\n+\n \n class BiliBiliBangumiIE(BilibiliBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/bangumi/play/(?P<id>ep\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/bangumi/play/ep(?P<id>\\d+)'\n \n     _TESTS = [{\n+        'url': 'https://www.bilibili.com/bangumi/play/ep21495/',\n+        'info_dict': {\n+            'id': '21495',\n+            'ext': 'mp4',\n+            'series': '\u60a0\u4e45\u4e4b\u7ffc',\n+            'series_id': '774',\n+            'season': '\u7b2c\u4e8c\u5b63',\n+            'season_id': '1182',\n+            'season_number': 2,\n+            'episode': 'forever\uff0fef',\n+            'episode_id': '21495',\n+            'episode_number': 12,\n+            'title': '12 forever\uff0fef',\n+            'duration': 1420.791,\n+            'timestamp': 1320412200,\n+            'upload_date': '20111104',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n+        },\n+    }, {\n         'url': 'https://www.bilibili.com/bangumi/play/ep267851',\n         'info_dict': {\n             'id': '267851',\n             'ext': 'mp4',\n             'series': '\u9b3c\u706d\u4e4b\u5203',\n             'series_id': '4358',\n-            'season': '\u9b3c\u706d\u4e4b\u5203',\n+            'season': '\u7acb\u5fd7\u7bc7',\n             'season_id': '26801',\n             'season_number': 1,\n             'episode': '\u6b8b\u9177',\n@@ -436,13 +639,32 @@ class BiliBiliBangumiIE(BilibiliBaseIE):\n             'upload_date': '20190406',\n             'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$'\n         },\n-        'skip': 'According to the copyright owner\\'s request, you may only watch the video after you are premium member.'\n+        'skip': 'Geo-restricted',\n+    }, {\n+        'note': 'a making-of which falls outside main section',\n+        'url': 'https://www.bilibili.com/bangumi/play/ep345120',\n+        'info_dict': {\n+            'id': '345120',\n+            'ext': 'mp4',\n+            'series': '\u9b3c\u706d\u4e4b\u5203',\n+            'series_id': '4358',\n+            'season': '\u7acb\u5fd7\u7bc7',\n+            'season_id': '26801',\n+            'season_number': 1,\n+            'episode': '\u70ad\u6cbb\u90ce\u7bc7',\n+            'episode_id': '345120',\n+            'episode_number': 27,\n+            'title': '#1 \u70ad\u6cbb\u90ce\u7bc7',\n+            'duration': 1922.129,\n+            'timestamp': 1602853860,\n+            'upload_date': '20201016',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$'\n+        },\n     }]\n \n     def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        episode_id = video_id[2:]\n-        webpage = self._download_webpage(url, video_id)\n+        episode_id = self._match_id(url)\n+        webpage = self._download_webpage(url, episode_id)\n \n         if '\u60a8\u6240\u5728\u7684\u5730\u533a\u65e0\u6cd5\u89c2\u770b\u672c\u7247' in webpage:\n             raise GeoRestrictedError('This video is restricted')\n@@ -451,7 +673,7 @@ def _real_extract(self, url):\n \n         headers = {'Referer': url, **self.geo_verification_headers()}\n         play_info = self._download_json(\n-            'https://api.bilibili.com/pgc/player/web/v2/playurl', video_id,\n+            'https://api.bilibili.com/pgc/player/web/v2/playurl', episode_id,\n             'Extracting episode', query={'fnval': '4048', 'ep_id': episode_id},\n             headers=headers)\n         premium_only = play_info.get('code') == -10403\n@@ -462,78 +684,285 @@ def _real_extract(self, url):\n             self.raise_login_required('This video is for premium members only')\n \n         bangumi_info = self._download_json(\n-            'https://api.bilibili.com/pgc/view/web/season', video_id, 'Get episode details',\n+            'https://api.bilibili.com/pgc/view/web/season', episode_id, 'Get episode details',\n             query={'ep_id': episode_id}, headers=headers)['result']\n \n         episode_number, episode_info = next((\n             (idx, ep) for idx, ep in enumerate(traverse_obj(\n-                bangumi_info, ('episodes', ..., {dict})), 1)\n+                bangumi_info, (('episodes', ('section', ..., 'episodes')), ..., {dict})), 1)\n             if str_or_none(ep.get('id')) == episode_id), (1, {}))\n \n         season_id = bangumi_info.get('season_id')\n-        season_number = season_id and next((\n-            idx + 1 for idx, e in enumerate(\n+        season_number, season_title = season_id and next((\n+            (idx + 1, e.get('season_title')) for idx, e in enumerate(\n                 traverse_obj(bangumi_info, ('seasons', ...)))\n             if e.get('season_id') == season_id\n-        ), None)\n+        ), (None, None))\n \n         aid = episode_info.get('aid')\n \n         return {\n-            'id': video_id,\n+            'id': episode_id,\n             'formats': formats,\n             **traverse_obj(bangumi_info, {\n                 'series': ('series', 'series_title', {str}),\n                 'series_id': ('series', 'series_id', {str_or_none}),\n                 'thumbnail': ('square_cover', {url_or_none}),\n             }),\n-            'title': join_nonempty('title', 'long_title', delim=' ', from_dict=episode_info),\n-            'episode': episode_info.get('long_title'),\n+            **traverse_obj(episode_info, {\n+                'episode': ('long_title', {str}),\n+                'episode_number': ('title', {int_or_none}, {lambda x: x or episode_number}),\n+                'timestamp': ('pub_time', {int_or_none}),\n+                'title': {lambda v: v and join_nonempty('title', 'long_title', delim=' ', from_dict=v)},\n+            }),\n             'episode_id': episode_id,\n-            'episode_number': int_or_none(episode_info.get('title')) or episode_number,\n+            'season': str_or_none(season_title),\n             'season_id': str_or_none(season_id),\n             'season_number': season_number,\n-            'timestamp': int_or_none(episode_info.get('pub_time')),\n             'duration': float_or_none(play_info.get('timelength'), scale=1000),\n-            'subtitles': self.extract_subtitles(video_id, aid, episode_info.get('cid')),\n+            'subtitles': self.extract_subtitles(episode_id, episode_info.get('cid'), aid=aid),\n             '__post_extractor': self.extract_comments(aid),\n             'http_headers': headers,\n         }\n \n \n class BiliBiliBangumiMediaIE(BilibiliBaseIE):\n-    _VALID_URL = r'https?://www\\.bilibili\\.com/bangumi/media/md(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/bangumi/media/md(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://www.bilibili.com/bangumi/media/md24097891',\n         'info_dict': {\n             'id': '24097891',\n+            'title': 'CAROLE & TUESDAY',\n+            'description': 'md5:42417ad33d1eaa1c93bfd2dd1626b829',\n         },\n         'playlist_mincount': 25,\n+    }, {\n+        'url': 'https://www.bilibili.com/bangumi/media/md1565/',\n+        'info_dict': {\n+            'id': '1565',\n+            'title': '\u653b\u58f3\u673a\u52a8\u961f S.A.C. 2nd GIG',\n+            'description': 'md5:46cac00bafd645b97f4d6df616fc576d',\n+        },\n+        'playlist_count': 26,\n+        'playlist': [{\n+            'info_dict': {\n+                'id': '68540',\n+                'ext': 'mp4',\n+                'series': '\u653b\u58f3\u673a\u52a8\u961f',\n+                'series_id': '1077',\n+                'season': '\u7b2c\u4e8c\u5b63',\n+                'season_id': '1565',\n+                'season_number': 2,\n+                'episode': '\u518d\u542f\u52a8 REEMBODY',\n+                'episode_id': '68540',\n+                'episode_number': 1,\n+                'title': '1 \u518d\u542f\u52a8 REEMBODY',\n+                'duration': 1525.777,\n+                'timestamp': 1425074413,\n+                'upload_date': '20150227',\n+                'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$'\n+            },\n+        }],\n     }]\n \n     def _real_extract(self, url):\n         media_id = self._match_id(url)\n         webpage = self._download_webpage(url, media_id)\n-        ss_id = self._search_json(\n-            r'window\\.__INITIAL_STATE__\\s*=', webpage, 'initial_state', media_id)['mediaInfo']['season_id']\n \n-        return self.playlist_result(self._get_episodes_from_season(ss_id, url), media_id)\n+        initial_state = self._search_json(\n+            r'window\\.__INITIAL_STATE__\\s*=', webpage, 'initial_state', media_id)\n+        ss_id = initial_state['mediaInfo']['season_id']\n+\n+        return self.playlist_result(\n+            self._get_episodes_from_season(ss_id, url), media_id,\n+            **traverse_obj(initial_state, ('mediaInfo', {\n+                'title': ('title', {str}),\n+                'description': ('evaluate', {str}),\n+            })))\n \n \n class BiliBiliBangumiSeasonIE(BilibiliBaseIE):\n-    _VALID_URL = r'(?x)https?://www\\.bilibili\\.com/bangumi/play/ss(?P<id>\\d+)'\n+    _VALID_URL = r'(?x)https?://(?:www\\.)?bilibili\\.com/bangumi/play/ss(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://www.bilibili.com/bangumi/play/ss26801',\n         'info_dict': {\n-            'id': '26801'\n+            'id': '26801',\n+            'title': '\u9b3c\u706d\u4e4b\u5203',\n+            'description': 'md5:e2cc9848b6f69be6db79fc2a82d9661b',\n         },\n         'playlist_mincount': 26\n+    }, {\n+        'url': 'https://www.bilibili.com/bangumi/play/ss2251',\n+        'info_dict': {\n+            'id': '2251',\n+            'title': '\u73b2\u97f3',\n+            'description': 'md5:1fd40e3df4c08d4d9d89a6a34844bdc4',\n+        },\n+        'playlist_count': 13,\n+        'playlist': [{\n+            'info_dict': {\n+                'id': '50188',\n+                'ext': 'mp4',\n+                'series': '\u73b2\u97f3',\n+                'series_id': '1526',\n+                'season': 'TV',\n+                'season_id': '2251',\n+                'season_number': 1,\n+                'episode': 'WEIRD',\n+                'episode_id': '50188',\n+                'episode_number': 1,\n+                'title': '1 WEIRD',\n+                'duration': 1436.992,\n+                'timestamp': 1343185080,\n+                'upload_date': '20120725',\n+                'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$'\n+            },\n+        }],\n     }]\n \n     def _real_extract(self, url):\n         ss_id = self._match_id(url)\n+        webpage = self._download_webpage(url, ss_id)\n+        metainfo = traverse_obj(\n+            self._search_json(r'<script[^>]+type=\"application/ld\\+json\"[^>]*>', webpage, 'info', ss_id),\n+            ('itemListElement', ..., {\n+                'title': ('name', {str}),\n+                'description': ('description', {str}),\n+            }), get_all=False)\n+\n+        return self.playlist_result(self._get_episodes_from_season(ss_id, url), ss_id, **metainfo)\n+\n+\n+class BilibiliCheeseBaseIE(BilibiliBaseIE):\n+    _HEADERS = {'Referer': 'https://www.bilibili.com/'}\n+\n+    def _extract_episode(self, season_info, ep_id):\n+        episode_info = traverse_obj(season_info, (\n+            'episodes', lambda _, v: v['id'] == int(ep_id)), get_all=False)\n+        aid, cid = episode_info['aid'], episode_info['cid']\n \n-        return self.playlist_result(self._get_episodes_from_season(ss_id, url), ss_id)\n+        if traverse_obj(episode_info, 'ep_status') == -1:\n+            raise ExtractorError('This course episode is not yet available.', expected=True)\n+        if not traverse_obj(episode_info, 'playable'):\n+            self.raise_login_required('You need to purchase the course to download this episode')\n+\n+        play_info = self._download_json(\n+            'https://api.bilibili.com/pugv/player/web/playurl', ep_id,\n+            query={'avid': aid, 'cid': cid, 'ep_id': ep_id, 'fnval': 16, 'fourk': 1},\n+            headers=self._HEADERS, note='Downloading playinfo')['data']\n+\n+        return {\n+            'id': str_or_none(ep_id),\n+            'episode_id': str_or_none(ep_id),\n+            'formats': self.extract_formats(play_info),\n+            'extractor_key': BilibiliCheeseIE.ie_key(),\n+            'extractor': BilibiliCheeseIE.IE_NAME,\n+            'webpage_url': f'https://www.bilibili.com/cheese/play/ep{ep_id}',\n+            **traverse_obj(episode_info, {\n+                'episode': ('title', {str}),\n+                'title': {lambda v: v and join_nonempty('index', 'title', delim=' - ', from_dict=v)},\n+                'alt_title': ('subtitle', {str}),\n+                'duration': ('duration', {int_or_none}),\n+                'episode_number': ('index', {int_or_none}),\n+                'thumbnail': ('cover', {url_or_none}),\n+                'timestamp': ('release_date', {int_or_none}),\n+                'view_count': ('play', {int_or_none}),\n+            }),\n+            **traverse_obj(season_info, {\n+                'uploader': ('up_info', 'uname', {str}),\n+                'uploader_id': ('up_info', 'mid', {str_or_none}),\n+            }),\n+            'subtitles': self.extract_subtitles(ep_id, cid, aid=aid),\n+            '__post_extractor': self.extract_comments(aid),\n+            'http_headers': self._HEADERS,\n+        }\n+\n+    def _download_season_info(self, query_key, video_id):\n+        return self._download_json(\n+            f'https://api.bilibili.com/pugv/view/web/season?{query_key}={video_id}', video_id,\n+            headers=self._HEADERS, note='Downloading season info')['data']\n+\n+\n+class BilibiliCheeseIE(BilibiliCheeseBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/cheese/play/ep(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.bilibili.com/cheese/play/ep229832',\n+        'info_dict': {\n+            'id': '229832',\n+            'ext': 'mp4',\n+            'title': '1 - \u8bfe\u7a0b\u5148\u5bfc\u7247',\n+            'alt_title': '\u89c6\u9891\u8bfe\u2006\u00b7\u20063\u520641\u79d2',\n+            'uploader': '\u9a6c\u7763\u5de5',\n+            'uploader_id': '316568752',\n+            'episode': '\u8bfe\u7a0b\u5148\u5bfc\u7247',\n+            'episode_id': '229832',\n+            'episode_number': 1,\n+            'duration': 221,\n+            'timestamp': 1695549606,\n+            'upload_date': '20230924',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n+            'view_count': int,\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        ep_id = self._match_id(url)\n+        return self._extract_episode(self._download_season_info('ep_id', ep_id), ep_id)\n+\n+\n+class BilibiliCheeseSeasonIE(BilibiliCheeseBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/cheese/play/ss(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.bilibili.com/cheese/play/ss5918',\n+        'info_dict': {\n+            'id': '5918',\n+            'title': '\u3010\u9650\u65f6\u4e94\u6298\u3011\u65b0\u95fb\u7cfb\u5b66\u4e0d\u5230\uff1a\u9a6c\u7763\u5de5\u6559\u4f60\u505a\u81ea\u5a92\u4f53',\n+            'description': '\u5e2e\u666e\u901a\u4eba\u5efa\u7acb\u4e16\u754c\u6a21\u578b\uff0c\u964d\u4f4e\u4eba\u4e0e\u4eba\u7684\u6c9f\u901a\u95e8\u69db',\n+        },\n+        'playlist': [{\n+            'info_dict': {\n+                'id': '229832',\n+                'ext': 'mp4',\n+                'title': '1 - \u8bfe\u7a0b\u5148\u5bfc\u7247',\n+                'alt_title': '\u89c6\u9891\u8bfe\u2006\u00b7\u20063\u520641\u79d2',\n+                'uploader': '\u9a6c\u7763\u5de5',\n+                'uploader_id': '316568752',\n+                'episode': '\u8bfe\u7a0b\u5148\u5bfc\u7247',\n+                'episode_id': '229832',\n+                'episode_number': 1,\n+                'duration': 221,\n+                'timestamp': 1695549606,\n+                'upload_date': '20230924',\n+                'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)$',\n+                'view_count': int,\n+            }\n+        }],\n+        'params': {'playlist_items': '1'},\n+    }, {\n+        'url': 'https://www.bilibili.com/cheese/play/ss5918',\n+        'info_dict': {\n+            'id': '5918',\n+            'title': '\u3010\u9650\u65f6\u4e94\u6298\u3011\u65b0\u95fb\u7cfb\u5b66\u4e0d\u5230\uff1a\u9a6c\u7763\u5de5\u6559\u4f60\u505a\u81ea\u5a92\u4f53',\n+            'description': '\u5e2e\u666e\u901a\u4eba\u5efa\u7acb\u4e16\u754c\u6a21\u578b\uff0c\u964d\u4f4e\u4eba\u4e0e\u4eba\u7684\u6c9f\u901a\u95e8\u69db',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'paid video in list',\n+    }]\n+\n+    def _get_cheese_entries(self, season_info):\n+        for ep_id in traverse_obj(season_info, ('episodes', lambda _, v: v['episode_can_view'], 'id')):\n+            yield self._extract_episode(season_info, ep_id)\n+\n+    def _real_extract(self, url):\n+        season_id = self._match_id(url)\n+        season_info = self._download_season_info('season_id', season_id)\n+\n+        return self.playlist_result(\n+            self._get_cheese_entries(season_info), season_id,\n+            **traverse_obj(season_info, {\n+                'title': ('title', {str}),\n+                'description': ('subtitle', {str}),\n+            }))\n \n \n class BilibiliSpaceBaseIE(InfoExtractor):\n@@ -672,13 +1101,35 @@ def get_entries(page_data):\n         return self.playlist_result(paged_list, playlist_id)\n \n \n-class BilibiliSpacePlaylistIE(BilibiliSpaceBaseIE):\n-    _VALID_URL = r'https?://space.bilibili\\.com/(?P<mid>\\d+)/channel/collectiondetail\\?sid=(?P<sid>\\d+)'\n+class BilibiliSpaceListBaseIE(BilibiliSpaceBaseIE):\n+    def _get_entries(self, page_data, bvid_keys, ending_key='bvid'):\n+        for bvid in traverse_obj(page_data, (*variadic(bvid_keys, (str, bytes, dict, set)), ..., ending_key, {str})):\n+            yield self.url_result(f'https://www.bilibili.com/video/{bvid}', BiliBiliIE, bvid)\n+\n+    def _get_uploader(self, uid, playlist_id):\n+        webpage = self._download_webpage(f'https://space.bilibili.com/{uid}', playlist_id, fatal=False)\n+        return self._search_regex(r'(?s)<title\\b[^>]*>([^<]+)\u7684\u4e2a\u4eba\u7a7a\u95f4-', webpage, 'uploader', fatal=False)\n+\n+    def _extract_playlist(self, fetch_page, get_metadata, get_entries):\n+        metadata, page_list = super()._extract_playlist(fetch_page, get_metadata, get_entries)\n+        metadata.pop('page_count', None)\n+        metadata.pop('page_size', None)\n+        return metadata, page_list\n+\n+\n+class BilibiliCollectionListIE(BilibiliSpaceListBaseIE):\n+    _VALID_URL = r'https?://space\\.bilibili\\.com/(?P<mid>\\d+)/channel/collectiondetail/?\\?sid=(?P<sid>\\d+)'\n     _TESTS = [{\n         'url': 'https://space.bilibili.com/2142762/channel/collectiondetail?sid=57445',\n         'info_dict': {\n             'id': '2142762_57445',\n-            'title': '\u300a\u5e95\u7279\u5f8b \u53d8\u4eba\u300b'\n+            'title': '\u3010\u5b8c\u7ed3\u3011\u300a\u5e95\u7279\u5f8b \u53d8\u4eba\u300b\u5168\u7ed3\u5c40\u6d41\u7a0b\u89e3\u8bf4',\n+            'description': '',\n+            'uploader': '\u8001\u6234\u5728\u6b64',\n+            'uploader_id': '2142762',\n+            'timestamp': int,\n+            'upload_date': str,\n+            'thumbnail': 'https://archive.biliimg.com/bfs/archive/e0e543ae35ad3df863ea7dea526bc32e70f4c091.jpg',\n         },\n         'playlist_mincount': 31,\n     }]\n@@ -699,22 +1150,251 @@ def get_metadata(page_data):\n             return {\n                 'page_count': math.ceil(entry_count / page_size),\n                 'page_size': page_size,\n-                'title': traverse_obj(page_data, ('meta', 'name'))\n+                'uploader': self._get_uploader(mid, playlist_id),\n+                **traverse_obj(page_data, {\n+                    'title': ('meta', 'name', {str}),\n+                    'description': ('meta', 'description', {str}),\n+                    'uploader_id': ('meta', 'mid', {str_or_none}),\n+                    'timestamp': ('meta', 'ptime', {int_or_none}),\n+                    'thumbnail': ('meta', 'cover', {url_or_none}),\n+                })\n+            }\n+\n+        def get_entries(page_data):\n+            return self._get_entries(page_data, 'archives')\n+\n+        metadata, paged_list = self._extract_playlist(fetch_page, get_metadata, get_entries)\n+        return self.playlist_result(paged_list, playlist_id, **metadata)\n+\n+\n+class BilibiliSeriesListIE(BilibiliSpaceListBaseIE):\n+    _VALID_URL = r'https?://space\\.bilibili\\.com/(?P<mid>\\d+)/channel/seriesdetail/?\\?\\bsid=(?P<sid>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://space.bilibili.com/1958703906/channel/seriesdetail?sid=547718&ctype=0',\n+        'info_dict': {\n+            'id': '1958703906_547718',\n+            'title': '\u76f4\u64ad\u56de\u653e',\n+            'description': '\u76f4\u64ad\u56de\u653e',\n+            'uploader': '\u9761\u70dfmiya',\n+            'uploader_id': '1958703906',\n+            'timestamp': 1637985853,\n+            'upload_date': '20211127',\n+            'modified_timestamp': int,\n+            'modified_date': str,\n+        },\n+        'playlist_mincount': 513,\n+    }]\n+\n+    def _real_extract(self, url):\n+        mid, sid = self._match_valid_url(url).group('mid', 'sid')\n+        playlist_id = f'{mid}_{sid}'\n+        playlist_meta = traverse_obj(self._download_json(\n+            f'https://api.bilibili.com/x/series/series?series_id={sid}', playlist_id, fatal=False\n+        ), {\n+            'title': ('data', 'meta', 'name', {str}),\n+            'description': ('data', 'meta', 'description', {str}),\n+            'uploader_id': ('data', 'meta', 'mid', {str_or_none}),\n+            'timestamp': ('data', 'meta', 'ctime', {int_or_none}),\n+            'modified_timestamp': ('data', 'meta', 'mtime', {int_or_none}),\n+        })\n+\n+        def fetch_page(page_idx):\n+            return self._download_json(\n+                'https://api.bilibili.com/x/series/archives',\n+                playlist_id, note=f'Downloading page {page_idx}',\n+                query={'mid': mid, 'series_id': sid, 'pn': page_idx + 1, 'ps': 30})['data']\n+\n+        def get_metadata(page_data):\n+            page_size = page_data['page']['size']\n+            entry_count = page_data['page']['total']\n+            return {\n+                'page_count': math.ceil(entry_count / page_size),\n+                'page_size': page_size,\n+                'uploader': self._get_uploader(mid, playlist_id),\n+                **playlist_meta\n             }\n \n         def get_entries(page_data):\n-            for entry in page_data.get('archives', []):\n-                yield self.url_result(f'https://www.bilibili.com/video/{entry[\"bvid\"]}',\n-                                      BiliBiliIE, entry['bvid'])\n+            return self._get_entries(page_data, 'archives')\n \n         metadata, paged_list = self._extract_playlist(fetch_page, get_metadata, get_entries)\n-        return self.playlist_result(paged_list, playlist_id, metadata['title'])\n+        return self.playlist_result(paged_list, playlist_id, **metadata)\n+\n+\n+class BilibiliFavoritesListIE(BilibiliSpaceListBaseIE):\n+    _VALID_URL = r'https?://(?:space\\.bilibili\\.com/\\d+/favlist/?\\?fid=|(?:www\\.)?bilibili\\.com/medialist/detail/ml)(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://space.bilibili.com/84912/favlist?fid=1103407912&ftype=create',\n+        'info_dict': {\n+            'id': '1103407912',\n+            'title': '\u3010V2\u3011\uff08\u65e7\uff09',\n+            'description': '',\n+            'uploader': '\u6653\u6708\u6625\u65e5',\n+            'uploader_id': '84912',\n+            'timestamp': 1604905176,\n+            'upload_date': '20201109',\n+            'modified_timestamp': int,\n+            'modified_date': str,\n+            'thumbnail': r\"re:http://i\\d\\.hdslb\\.com/bfs/archive/14b83c62aa8871b79083df1e9ab4fbc699ad16fe\\.jpg\",\n+            'view_count': int,\n+            'like_count': int,\n+        },\n+        'playlist_mincount': 22,\n+    }, {\n+        'url': 'https://www.bilibili.com/medialist/detail/ml1103407912',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        fid = self._match_id(url)\n+\n+        list_info = self._download_json(\n+            f'https://api.bilibili.com/x/v3/fav/resource/list?media_id={fid}&pn=1&ps=20',\n+            fid, note='Downloading favlist metadata')\n+        if list_info['code'] == -403:\n+            self.raise_login_required(msg='This is a private favorites list. You need to log in as its owner')\n+\n+        entries = self._get_entries(self._download_json(\n+            f'https://api.bilibili.com/x/v3/fav/resource/ids?media_id={fid}',\n+            fid, note='Download favlist entries'), 'data')\n+\n+        return self.playlist_result(entries, fid, **traverse_obj(list_info, ('data', 'info', {\n+            'title': ('title', {str}),\n+            'description': ('intro', {str}),\n+            'uploader': ('upper', 'name', {str}),\n+            'uploader_id': ('upper', 'mid', {str_or_none}),\n+            'timestamp': ('ctime', {int_or_none}),\n+            'modified_timestamp': ('mtime', {int_or_none}),\n+            'thumbnail': ('cover', {url_or_none}),\n+            'view_count': ('cnt_info', 'play', {int_or_none}),\n+            'like_count': ('cnt_info', 'thumb_up', {int_or_none}),\n+        })))\n+\n+\n+class BilibiliWatchlaterIE(BilibiliSpaceListBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/watchlater/?(?:[?#]|$)'\n+    _TESTS = [{\n+        'url': 'https://www.bilibili.com/watchlater/#/list',\n+        'info_dict': {'id': 'watchlater'},\n+        'playlist_mincount': 0,\n+        'skip': 'login required',\n+    }]\n+\n+    def _real_extract(self, url):\n+        list_id = getattr(self._get_cookies(url).get('DedeUserID'), 'value', 'watchlater')\n+        watchlater_info = self._download_json(\n+            'https://api.bilibili.com/x/v2/history/toview/web?jsonp=jsonp', list_id)\n+        if watchlater_info['code'] == -101:\n+            self.raise_login_required(msg='You need to login to access your watchlater list')\n+        entries = self._get_entries(watchlater_info, ('data', 'list'))\n+        return self.playlist_result(entries, id=list_id, title='\u7a0d\u540e\u518d\u770b')\n+\n+\n+class BilibiliPlaylistIE(BilibiliSpaceListBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/(?:medialist/play|list)/(?P<id>\\w+)'\n+    _TESTS = [{\n+        'url': 'https://www.bilibili.com/list/1958703906?sid=547718',\n+        'info_dict': {\n+            'id': '5_547718',\n+            'title': '\u76f4\u64ad\u56de\u653e',\n+            'uploader': '\u9761\u70dfmiya',\n+            'uploader_id': '1958703906',\n+            'timestamp': 1637985853,\n+            'upload_date': '20211127',\n+        },\n+        'playlist_mincount': 513,\n+    }, {\n+        'url': 'https://www.bilibili.com/medialist/play/1958703906?business=space_series&business_id=547718&desc=1',\n+        'info_dict': {\n+            'id': '5_547718',\n+        },\n+        'playlist_mincount': 513,\n+        'skip': 'redirect url',\n+    }, {\n+        'url': 'https://www.bilibili.com/list/ml1103407912',\n+        'info_dict': {\n+            'id': '3_1103407912',\n+            'title': '\u3010V2\u3011\uff08\u65e7\uff09',\n+            'uploader': '\u6653\u6708\u6625\u65e5',\n+            'uploader_id': '84912',\n+            'timestamp': 1604905176,\n+            'upload_date': '20201109',\n+            'thumbnail': r\"re:http://i\\d\\.hdslb\\.com/bfs/archive/14b83c62aa8871b79083df1e9ab4fbc699ad16fe\\.jpg\",\n+        },\n+        'playlist_mincount': 22,\n+    }, {\n+        'url': 'https://www.bilibili.com/medialist/play/ml1103407912',\n+        'info_dict': {\n+            'id': '3_1103407912',\n+        },\n+        'playlist_mincount': 22,\n+        'skip': 'redirect url',\n+    }, {\n+        'url': 'https://www.bilibili.com/list/watchlater',\n+        'info_dict': {'id': 'watchlater'},\n+        'playlist_mincount': 0,\n+        'skip': 'login required',\n+    }, {\n+        'url': 'https://www.bilibili.com/medialist/play/watchlater',\n+        'info_dict': {'id': 'watchlater'},\n+        'playlist_mincount': 0,\n+        'skip': 'login required',\n+    }]\n+\n+    def _extract_medialist(self, query, list_id):\n+        for page_num in itertools.count(1):\n+            page_data = self._download_json(\n+                'https://api.bilibili.com/x/v2/medialist/resource/list',\n+                list_id, query=query, note=f'getting playlist {query[\"biz_id\"]} page {page_num}'\n+            )['data']\n+            yield from self._get_entries(page_data, 'media_list', ending_key='bv_id')\n+            query['oid'] = traverse_obj(page_data, ('media_list', -1, 'id'))\n+            if not page_data.get('has_more', False):\n+                break\n+\n+    def _real_extract(self, url):\n+        list_id = self._match_id(url)\n+        webpage = self._download_webpage(url, list_id)\n+        initial_state = self._search_json(r'window\\.__INITIAL_STATE__\\s*=', webpage, 'initial state', list_id)\n+        if traverse_obj(initial_state, ('error', 'code', {int_or_none})) != 200:\n+            error_code = traverse_obj(initial_state, ('error', 'trueCode', {int_or_none}))\n+            error_message = traverse_obj(initial_state, ('error', 'message', {str_or_none}))\n+            if error_code == -400 and list_id == 'watchlater':\n+                self.raise_login_required('You need to login to access your watchlater playlist')\n+            elif error_code == -403:\n+                self.raise_login_required('This is a private playlist. You need to login as its owner')\n+            elif error_code == 11010:\n+                raise ExtractorError('Playlist is no longer available', expected=True)\n+            raise ExtractorError(f'Could not access playlist: {error_code} {error_message}')\n+\n+        query = {\n+            'ps': 20,\n+            'with_current': False,\n+            **traverse_obj(initial_state, {\n+                'type': ('playlist', 'type', {int_or_none}),\n+                'biz_id': ('playlist', 'id', {int_or_none}),\n+                'tid': ('tid', {int_or_none}),\n+                'sort_field': ('sortFiled', {int_or_none}),\n+                'desc': ('desc', {bool_or_none}, {str_or_none}, {str.lower}),\n+            })\n+        }\n+        metadata = {\n+            'id': f'{query[\"type\"]}_{query[\"biz_id\"]}',\n+            **traverse_obj(initial_state, ('mediaListInfo', {\n+                'title': ('title', {str}),\n+                'uploader': ('upper', 'name', {str}),\n+                'uploader_id': ('upper', 'mid', {str_or_none}),\n+                'timestamp': ('ctime', {int_or_none}),\n+                'thumbnail': ('cover', {url_or_none}),\n+            })),\n+        }\n+        return self.playlist_result(self._extract_medialist(query, list_id), **metadata)\n \n \n class BilibiliCategoryIE(InfoExtractor):\n     IE_NAME = 'Bilibili category extractor'\n     _MAX_RESULTS = 1000000\n-    _VALID_URL = r'https?://www\\.bilibili\\.com/v/[a-zA-Z]+\\/[a-zA-Z]+'\n+    _VALID_URL = r'https?://(?:www\\.)?bilibili\\.com/v/[a-zA-Z]+\\/[a-zA-Z]+'\n     _TESTS = [{\n         'url': 'https://www.bilibili.com/v/kichiku/mad',\n         'info_dict': {\n@@ -1399,7 +2079,7 @@ def _real_extract(self, url):\n \n \n class BiliLiveIE(InfoExtractor):\n-    _VALID_URL = r'https?://live.bilibili.com/(?:blanc/)?(?P<id>\\d+)'\n+    _VALID_URL = r'https?://live\\.bilibili\\.com/(?:blanc/)?(?P<id>\\d+)'\n \n     _TESTS = [{\n         'url': 'https://live.bilibili.com/196',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/biqle.py",
            "diff": "diff --git a/yt_dlp/extractor/biqle.py b/yt_dlp/extractor/biqle.py\ndeleted file mode 100644\nindex 02775350..00000000\n--- a/yt_dlp/extractor/biqle.py\n+++ /dev/null\n@@ -1,110 +0,0 @@\n-from .common import InfoExtractor\n-from .vk import VKIE\n-from ..compat import compat_b64decode\n-from ..utils import (\n-    int_or_none,\n-    js_to_json,\n-    traverse_obj,\n-    unified_timestamp,\n-)\n-\n-\n-class BIQLEIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?biqle\\.(?:com|org|ru)/watch/(?P<id>-?\\d+_\\d+)'\n-    _TESTS = [{\n-        'url': 'https://biqle.ru/watch/-2000421746_85421746',\n-        'md5': 'ae6ef4f04d19ac84e4658046d02c151c',\n-        'info_dict': {\n-            'id': '-2000421746_85421746',\n-            'ext': 'mp4',\n-            'title': 'Forsaken By Hope Studio Clip',\n-            'description': 'Forsaken By Hope Studio Clip \u2014 \u0421\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043e\u043d\u043b\u0430\u0439\u043d',\n-            'upload_date': '19700101',\n-            'thumbnail': r're:https://[^/]+/impf/7vN3ACwSTgChP96OdOfzFjUCzFR6ZglDQgWsIw/KPaACiVJJxM\\.jpg\\?size=800x450&quality=96&keep_aspect_ratio=1&background=000000&sign=b48ea459c4d33dbcba5e26d63574b1cb&type=video_thumb',\n-            'timestamp': 0,\n-        },\n-    }, {\n-        'url': 'http://biqle.org/watch/-44781847_168547604',\n-        'md5': '7f24e72af1db0edf7c1aaba513174f97',\n-        'info_dict': {\n-            'id': '-44781847_168547604',\n-            'ext': 'mp4',\n-            'title': '\u0420\u0435\u0431\u0435\u043d\u043e\u043a \u0432 \u0448\u043e\u043a\u0435 \u043e\u0442 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043c\u043e\u0439\u043a\u0438',\n-            'description': '\u0420\u0435\u0431\u0435\u043d\u043e\u043a \u0432 \u0448\u043e\u043a\u0435 \u043e\u0442 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043c\u043e\u0439\u043a\u0438 \u2014 \u0421\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043e\u043d\u043b\u0430\u0439\u043d',\n-            'timestamp': 1396633454,\n-            'upload_date': '20140404',\n-            'thumbnail': r're:https://[^/]+/c535507/u190034692/video/l_b84df002\\.jpg',\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = self._html_search_meta('name', webpage, 'Title', fatal=False)\n-        timestamp = unified_timestamp(self._html_search_meta('uploadDate', webpage, 'Upload Date', default=None))\n-        description = self._html_search_meta('description', webpage, 'Description', default=None)\n-\n-        global_embed_url = self._search_regex(\n-            r'<script[^<]+?window.globEmbedUrl\\s*=\\s*\\'((?:https?:)?//(?:daxab\\.com|dxb\\.to|[^/]+/player)/[^\\']+)\\'',\n-            webpage, 'global Embed url')\n-        hash = self._search_regex(\n-            r'<script id=\"data-embed-video[^<]+?hash: \"([^\"]+)\"[^<]*</script>', webpage, 'Hash')\n-\n-        embed_url = global_embed_url + hash\n-\n-        if VKIE.suitable(embed_url):\n-            return self.url_result(embed_url, VKIE.ie_key(), video_id)\n-\n-        embed_page = self._download_webpage(\n-            embed_url, video_id, 'Downloading embed webpage', headers={'Referer': url})\n-\n-        glob_params = self._parse_json(self._search_regex(\n-            r'<script id=\"globParams\">[^<]*window.globParams = ([^;]+);[^<]+</script>',\n-            embed_page, 'Global Parameters'), video_id, transform_source=js_to_json)\n-        host_name = compat_b64decode(glob_params['server'][::-1]).decode()\n-\n-        item = self._download_json(\n-            f'https://{host_name}/method/video.get/{video_id}', video_id,\n-            headers={'Referer': url}, query={\n-                'token': glob_params['video']['access_token'],\n-                'videos': video_id,\n-                'ckey': glob_params['c_key'],\n-                'credentials': glob_params['video']['credentials'],\n-            })['response']['items'][0]\n-\n-        formats = []\n-        for f_id, f_url in item.get('files', {}).items():\n-            if f_id == 'external':\n-                return self.url_result(f_url)\n-            ext, height = f_id.split('_')\n-            height_extra_key = traverse_obj(glob_params, ('video', 'partial', 'quality', height))\n-            if height_extra_key:\n-                formats.append({\n-                    'format_id': f'{height}p',\n-                    'url': f'https://{host_name}/{f_url[8:]}&videos={video_id}&extra_key={height_extra_key}',\n-                    'height': int_or_none(height),\n-                    'ext': ext,\n-                })\n-\n-        thumbnails = []\n-        for k, v in item.items():\n-            if k.startswith('photo_') and v:\n-                width = k.replace('photo_', '')\n-                thumbnails.append({\n-                    'id': width,\n-                    'url': v,\n-                    'width': int_or_none(width),\n-                })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'comment_count': int_or_none(item.get('comments')),\n-            'description': description,\n-            'duration': int_or_none(item.get('duration')),\n-            'thumbnails': thumbnails,\n-            'timestamp': timestamp,\n-            'view_count': int_or_none(item.get('views')),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bitchute.py",
            "diff": "diff --git a/yt_dlp/extractor/bitchute.py b/yt_dlp/extractor/bitchute.py\nindex 0805b8b4..41367c5b 100644\n--- a/yt_dlp/extractor/bitchute.py\n+++ b/yt_dlp/extractor/bitchute.py\n@@ -7,8 +7,10 @@\n     ExtractorError,\n     OnDemandPagedList,\n     clean_html,\n+    extract_attributes,\n     get_element_by_class,\n     get_element_by_id,\n+    get_element_html_by_class,\n     get_elements_html_by_class,\n     int_or_none,\n     orderedSet,\n@@ -17,6 +19,7 @@\n     traverse_obj,\n     unified_strdate,\n     urlencode_postdata,\n+    urljoin,\n )\n \n \n@@ -34,6 +37,25 @@ class BitChuteIE(InfoExtractor):\n             'thumbnail': r're:^https?://.*\\.jpg$',\n             'uploader': 'BitChute',\n             'upload_date': '20170103',\n+            'uploader_url': 'https://www.bitchute.com/profile/I5NgtHZn9vPj/',\n+            'channel': 'BitChute',\n+            'channel_url': 'https://www.bitchute.com/channel/bitchute/'\n+        },\n+    }, {\n+        # test case: video with different channel and uploader\n+        'url': 'https://www.bitchute.com/video/Yti_j9A-UZ4/',\n+        'md5': 'f10e6a8e787766235946d0868703f1d0',\n+        'info_dict': {\n+            'id': 'Yti_j9A-UZ4',\n+            'ext': 'mp4',\n+            'title': 'Israel at War | Full Measure',\n+            'description': 'md5:38cf7bc6f42da1a877835539111c69ef',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'uploader': 'sharylattkisson',\n+            'upload_date': '20231106',\n+            'uploader_url': 'https://www.bitchute.com/profile/9K0kUWA9zmd9/',\n+            'channel': 'Full Measure with Sharyl Attkisson',\n+            'channel_url': 'https://www.bitchute.com/channel/sharylattkisson/'\n         },\n     }, {\n         # video not downloadable in browser, but we can recover it\n@@ -48,6 +70,9 @@ class BitChuteIE(InfoExtractor):\n             'thumbnail': r're:^https?://.*\\.jpg$',\n             'uploader': 'BitChute',\n             'upload_date': '20181113',\n+            'uploader_url': 'https://www.bitchute.com/profile/I5NgtHZn9vPj/',\n+            'channel': 'BitChute',\n+            'channel_url': 'https://www.bitchute.com/channel/bitchute/'\n         },\n         'params': {'check_formats': None},\n     }, {\n@@ -99,6 +124,11 @@ def _raise_if_restricted(self, webpage):\n             reason = clean_html(get_element_by_id('page-detail', webpage)) or page_title\n             self.raise_geo_restricted(reason)\n \n+    @staticmethod\n+    def _make_url(html):\n+        path = extract_attributes(get_element_html_by_class('spa', html) or '').get('href')\n+        return urljoin('https://www.bitchute.com', path)\n+\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(\n@@ -121,12 +151,19 @@ def _real_extract(self, url):\n                 'Video is unavailable. Please make sure this video is playable in the browser '\n                 'before reporting this issue.', expected=True, video_id=video_id)\n \n+        details = get_element_by_class('details', webpage) or ''\n+        uploader_html = get_element_html_by_class('creator', details) or ''\n+        channel_html = get_element_html_by_class('name', details) or ''\n+\n         return {\n             'id': video_id,\n             'title': self._html_extract_title(webpage) or self._og_search_title(webpage),\n             'description': self._og_search_description(webpage, default=None),\n             'thumbnail': self._og_search_thumbnail(webpage),\n-            'uploader': clean_html(get_element_by_class('owner', webpage)),\n+            'uploader': clean_html(uploader_html),\n+            'uploader_url': self._make_url(uploader_html),\n+            'channel': clean_html(channel_html),\n+            'channel_url': self._make_url(channel_html),\n             'upload_date': unified_strdate(self._search_regex(\n                 r'at \\d+:\\d+ UTC on (.+?)\\.', publish_date, 'upload date', fatal=False)),\n             'formats': formats,\n@@ -154,6 +191,9 @@ class BitChuteChannelIE(InfoExtractor):\n                     'thumbnail': r're:^https?://.*\\.jpg$',\n                     'uploader': 'BitChute',\n                     'upload_date': '20170103',\n+                    'uploader_url': 'https://www.bitchute.com/profile/I5NgtHZn9vPj/',\n+                    'channel': 'BitChute',\n+                    'channel_url': 'https://www.bitchute.com/channel/bitchute/',\n                     'duration': 16,\n                     'view_count': int,\n                 },\n@@ -169,7 +209,7 @@ class BitChuteChannelIE(InfoExtractor):\n         'info_dict': {\n             'id': 'wV9Imujxasw9',\n             'title': 'Bruce MacDonald and \"The Light of Darkness\"',\n-            'description': 'md5:04913227d2714af1d36d804aa2ab6b1e',\n+            'description': 'md5:747724ef404eebdfc04277714f81863e',\n         }\n     }]\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bitwave.py",
            "diff": "diff --git a/yt_dlp/extractor/bitwave.py b/yt_dlp/extractor/bitwave.py\ndeleted file mode 100644\nindex a82cd263..00000000\n--- a/yt_dlp/extractor/bitwave.py\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class BitwaveReplayIE(InfoExtractor):\n-    IE_NAME = 'bitwave:replay'\n-    _VALID_URL = r'https?://(?:www\\.)?bitwave\\.tv/(?P<user>\\w+)/replay/(?P<id>\\w+)/?$'\n-    _TEST = {\n-        'url': 'https://bitwave.tv/RhythmicCarnage/replay/z4P6eq5L7WDrM85UCrVr',\n-        'only_matching': True\n-    }\n-\n-    def _real_extract(self, url):\n-        replay_id = self._match_id(url)\n-        replay = self._download_json(\n-            'https://api.bitwave.tv/v1/replays/' + replay_id,\n-            replay_id\n-        )\n-\n-        return {\n-            'id': replay_id,\n-            'title': replay['data']['title'],\n-            'uploader': replay['data']['name'],\n-            'uploader_id': replay['data']['name'],\n-            'url': replay['data']['url'],\n-            'thumbnails': [\n-                {'url': x} for x in replay['data']['thumbnails']\n-            ],\n-        }\n-\n-\n-class BitwaveStreamIE(InfoExtractor):\n-    IE_NAME = 'bitwave:stream'\n-    _VALID_URL = r'https?://(?:www\\.)?bitwave\\.tv/(?P<id>\\w+)/?$'\n-    _TEST = {\n-        'url': 'https://bitwave.tv/doomtube',\n-        'only_matching': True\n-    }\n-\n-    def _real_extract(self, url):\n-        username = self._match_id(url)\n-        channel = self._download_json(\n-            'https://api.bitwave.tv/v1/channels/' + username,\n-            username)\n-\n-        formats = self._extract_m3u8_formats(\n-            channel['data']['url'], username,\n-            'mp4')\n-\n-        return {\n-            'id': username,\n-            'title': channel['data']['title'],\n-            'uploader': username,\n-            'uploader_id': username,\n-            'formats': formats,\n-            'thumbnail': channel['data']['thumbnail'],\n-            'is_live': True,\n-            'view_count': channel['data']['viewCount']\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bleacherreport.py",
            "diff": "diff --git a/yt_dlp/extractor/bleacherreport.py b/yt_dlp/extractor/bleacherreport.py\nindex 8d8fabe3..5e5155af 100644\n--- a/yt_dlp/extractor/bleacherreport.py\n+++ b/yt_dlp/extractor/bleacherreport.py\n@@ -22,7 +22,7 @@ class BleacherReportIE(InfoExtractor):\n             'upload_date': '20150615',\n             'uploader': 'Team Stream Now ',\n         },\n-        'add_ie': ['Ooyala'],\n+        'skip': 'Video removed',\n     }, {\n         'url': 'http://bleacherreport.com/articles/2586817-aussie-golfers-get-fright-of-their-lives-after-being-chased-by-angry-kangaroo',\n         'md5': '6a5cd403418c7b01719248ca97fb0692',\n@@ -70,8 +70,6 @@ def _real_extract(self, url):\n             video_type = video['type']\n             if video_type in ('cms.bleacherreport.com', 'vid.bleacherreport.com'):\n                 info['url'] = 'http://bleacherreport.com/video_embed?id=%s' % video['id']\n-            elif video_type == 'ooyala.com':\n-                info['url'] = 'ooyala:%s' % video['id']\n             elif video_type == 'youtube.com':\n                 info['url'] = video['id']\n             elif video_type == 'vine.co':\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/box.py",
            "diff": "diff --git a/yt_dlp/extractor/box.py b/yt_dlp/extractor/box.py\nindex 8ab14962..7281b3c6 100644\n--- a/yt_dlp/extractor/box.py\n+++ b/yt_dlp/extractor/box.py\n@@ -1,16 +1,17 @@\n import json\n+import urllib.parse\n \n from .common import InfoExtractor\n from ..utils import (\n-    determine_ext,\n     parse_iso8601,\n-    # try_get,\n     update_url_query,\n+    url_or_none,\n )\n+from ..utils.traversal import traverse_obj\n \n \n class BoxIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:[^.]+\\.)?app\\.box\\.com/s/(?P<shared_name>[^/]+)/file/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:[^.]+\\.)?app\\.box\\.com/s/(?P<shared_name>[^/?#]+)/file/(?P<id>\\d+)'\n     _TEST = {\n         'url': 'https://mlssoccer.app.box.com/s/0evd2o3e08l60lr4ygukepvnkord1o1x/file/510727257538',\n         'md5': '1f81b2fd3960f38a40a3b8823e5fcd43',\n@@ -18,11 +19,12 @@ class BoxIE(InfoExtractor):\n             'id': '510727257538',\n             'ext': 'mp4',\n             'title': 'Garber   St. Louis will be 28th MLS team  +scarving.mp4',\n-            'uploader': 'MLS Video',\n+            'uploader': '',\n             'timestamp': 1566320259,\n             'upload_date': '20190820',\n             'uploader_id': '235196876',\n-        }\n+        },\n+        'params': {'skip_download': 'dash fragment too small'},\n     }\n \n     def _real_extract(self, url):\n@@ -58,26 +60,15 @@ def _real_extract(self, url):\n \n         formats = []\n \n-        # for entry in (try_get(f, lambda x: x['representations']['entries'], list) or []):\n-        #     entry_url_template = try_get(\n-        #         entry, lambda x: x['content']['url_template'])\n-        #     if not entry_url_template:\n-        #         continue\n-        #     representation = entry.get('representation')\n-        #     if representation == 'dash':\n-        #         TODO: append query to every fragment URL\n-        #         formats.extend(self._extract_mpd_formats(\n-        #             entry_url_template.replace('{+asset_path}', 'manifest.mpd'),\n-        #             file_id, query=query))\n-\n-        authenticated_download_url = f.get('authenticated_download_url')\n-        if authenticated_download_url and f.get('is_download_available'):\n-            formats.append({\n-                'ext': f.get('extension') or determine_ext(title),\n-                'filesize': f.get('size'),\n-                'format_id': 'download',\n-                'url': update_url_query(authenticated_download_url, query),\n-            })\n+        for url_tmpl in traverse_obj(f, (\n+            'representations', 'entries', lambda _, v: v['representation'] == 'dash',\n+            'content', 'url_template', {url_or_none}\n+        )):\n+            manifest_url = update_url_query(url_tmpl.replace('{+asset_path}', 'manifest.mpd'), query)\n+            fmts = self._extract_mpd_formats(manifest_url, file_id)\n+            for fmt in fmts:\n+                fmt['extra_param_to_segment_url'] = urllib.parse.urlparse(manifest_url).query\n+            formats.extend(fmts)\n \n         creator = f.get('created_by') or {}\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bpb.py",
            "diff": "diff --git a/yt_dlp/extractor/bpb.py b/yt_dlp/extractor/bpb.py\nindex f28e581b..7fe08994 100644\n--- a/yt_dlp/extractor/bpb.py\n+++ b/yt_dlp/extractor/bpb.py\n@@ -1,56 +1,170 @@\n+import functools\n import re\n \n from .common import InfoExtractor\n from ..utils import (\n+    clean_html,\n+    extract_attributes,\n+    get_element_text_and_html_by_tag,\n+    get_elements_by_class,\n+    join_nonempty,\n     js_to_json,\n-    determine_ext,\n+    mimetype2ext,\n+    unified_strdate,\n+    url_or_none,\n+    urljoin,\n+    variadic,\n )\n+from ..utils.traversal import traverse_obj\n+\n+\n+def html_get_element(tag=None, cls=None):\n+    assert tag or cls, 'One of tag or class is required'\n+\n+    if cls:\n+        func = functools.partial(get_elements_by_class, cls, tag=tag)\n+    else:\n+        func = functools.partial(get_element_text_and_html_by_tag, tag)\n+\n+    def html_get_element_wrapper(html):\n+        return variadic(func(html))[0]\n+\n+    return html_get_element_wrapper\n \n \n class BpbIE(InfoExtractor):\n     IE_DESC = 'Bundeszentrale f\u00fcr politische Bildung'\n-    _VALID_URL = r'https?://(?:www\\.)?bpb\\.de/mediathek/(?P<id>[0-9]+)/'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?bpb\\.de/(?:[^/?#]+/)*(?P<id>\\d+)(?:[/?#]|$)'\n \n-    _TEST = {\n+    _TESTS = [{\n         'url': 'http://www.bpb.de/mediathek/297/joachim-gauck-zu-1989-und-die-erinnerung-an-die-ddr',\n-        'md5': 'c4f84c8a8044ca9ff68bb8441d300b3f',\n         'info_dict': {\n             'id': '297',\n             'ext': 'mp4',\n+            'creator': 'Kooperative Berlin',\n+            'description': 'md5:f4f75885ba009d3e2b156247a8941ce6',\n+            'release_date': '20160115',\n+            'series': 'Interview auf dem Geschichtsforum 1989 | 2009',\n+            'tags': ['Friedliche Revolution', 'Erinnerungskultur', 'Vergangenheitspolitik', 'DDR 1949 - 1990', 'Freiheitsrecht', 'BStU', 'Deutschland'],\n+            'thumbnail': 'https://www.bpb.de/cache/images/7/297_teaser_16x9_1240.jpg?8839D',\n             'title': 'Joachim Gauck zu 1989 und die Erinnerung an die DDR',\n-            'description': 'Joachim Gauck, erster Beauftragter f\u00fcr die Stasi-Unterlagen, spricht auf dem Geschichtsforum \u00fcber die friedliche Revolution 1989 und eine \"gewisse Traurigkeit\" im Umgang mit der DDR-Vergangenheit.'\n+            'uploader': 'Bundeszentrale f\u00fcr politische Bildung',\n+        },\n+    }, {\n+        'url': 'https://www.bpb.de/mediathek/video/522184/krieg-flucht-und-falschmeldungen-wirstattdesinformation-2/',\n+        'info_dict': {\n+            'id': '522184',\n+            'ext': 'mp4',\n+            'creator': 'Institute for Strategic Dialogue Germany gGmbH (ISD)',\n+            'description': 'md5:f83c795ff8f825a69456a9e51fc15903',\n+            'release_date': '20230621',\n+            'tags': ['Desinformation', 'Ukraine', 'Russland', 'Gefl\u00fcchtete'],\n+            'thumbnail': 'https://www.bpb.de/cache/images/4/522184_teaser_16x9_1240.png?EABFB',\n+            'title': 'md5:9b01ccdbf58dbf9e5c9f6e771a803b1c',\n+            'uploader': 'Bundeszentrale f\u00fcr politische Bildung',\n+        },\n+    }, {\n+        'url': 'https://www.bpb.de/lernen/bewegtbild-und-politische-bildung/webvideo/518789/krieg-flucht-und-falschmeldungen-wirstattdesinformation-1/',\n+        'info_dict': {\n+            'id': '518789',\n+            'ext': 'mp4',\n+            'creator': 'Institute for Strategic Dialogue Germany gGmbH (ISD)',\n+            'description': 'md5:85228aed433e84ff0ff9bc582abd4ea8',\n+            'release_date': '20230302',\n+            'tags': ['Desinformation', 'Ukraine', 'Russland', 'Gefl\u00fcchtete'],\n+            'thumbnail': 'https://www.bpb.de/cache/images/9/518789_teaser_16x9_1240.jpeg?56D0D',\n+            'title': 'md5:3e956f264bb501f6383f10495a401da4',\n+            'uploader': 'Bundeszentrale f\u00fcr politische Bildung',\n+        },\n+    }, {\n+        'url': 'https://www.bpb.de/mediathek/podcasts/apuz-podcast/539727/apuz-20-china/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.bpb.de/mediathek/audio/315813/folge-1-eine-einfuehrung/',\n+        'info_dict': {\n+            'id': '315813',\n+            'ext': 'mp3',\n+            'creator': 'Axel Schr\u00f6der',\n+            'description': 'md5:eda9d1af34e5912efef5baf54fba4427',\n+            'release_date': '20200921',\n+            'series': 'Auf Endlagersuche. Der deutsche Weg zu einem sicheren Atomm\u00fclllager',\n+            'tags': ['Atomenergie', 'Endlager', 'hoch-radioaktiver Abfall', 'Endlagersuche', 'Atomm\u00fcll', 'Atomendlager', 'Gorleben', 'Deutschland'],\n+            'thumbnail': 'https://www.bpb.de/cache/images/3/315813_teaser_16x9_1240.png?92A94',\n+            'title': 'Folge 1: Eine Einf\u00fchrung',\n+            'uploader': 'Bundeszentrale f\u00fcr politische Bildung',\n+        },\n+    }, {\n+        'url': 'https://www.bpb.de/517806/die-weltanschauung-der-neuen-rechten/',\n+        'info_dict': {\n+            'id': '517806',\n+            'ext': 'mp3',\n+            'creator': 'Bundeszentrale f\u00fcr politische Bildung',\n+            'description': 'md5:594689600e919912aade0b2871cc3fed',\n+            'release_date': '20230127',\n+            'series': 'Vortr\u00e4ge des Fachtags \"Modernisierer. Grenzg\u00e4nger. Anstifter. Sechs Jahrzehnte \\'Neue Rechte\\'\"',\n+            'tags': ['Rechtsextremismus', 'Konservatismus', 'Konservativismus', 'neue Rechte', 'Rechtspopulismus', 'Schnellroda', 'Deutschland'],\n+            'thumbnail': 'https://www.bpb.de/cache/images/6/517806_teaser_16x9_1240.png?7A7A0',\n+            'title': 'Die Weltanschauung der \"Neuen Rechten\"',\n+            'uploader': 'Bundeszentrale f\u00fcr politische Bildung',\n+        },\n+    }, {\n+        'url': 'https://www.bpb.de/mediathek/reihen/zahlen-und-fakten-soziale-situation-filme/520153/zahlen-und-fakten-die-soziale-situation-in-deutschland-migration/',\n+        'only_matching': True,\n+    }]\n+\n+    _TITLE_RE = re.compile('(?P<title>[^<]*)<[^>]+>(?P<series>[^<]*)')\n+\n+    def _parse_vue_attributes(self, name, string, video_id):\n+        attributes = extract_attributes(self._search_regex(rf'(<{name}(?:\"[^\"]*?\"|[^>])*>)', string, name))\n+\n+        for key, value in attributes.items():\n+            if key.startswith(':'):\n+                attributes[key] = self._parse_json(value, video_id, transform_source=js_to_json, fatal=False)\n+\n+        return attributes\n+\n+    @staticmethod\n+    def _process_source(source):\n+        url = url_or_none(source['src'])\n+        if not url:\n+            return None\n+\n+        source_type = source.get('type', '')\n+        extension = mimetype2ext(source_type)\n+        is_video = source_type.startswith('video')\n+        note = url.rpartition('.')[0].rpartition('_')[2] if is_video else None\n+\n+        return {\n+            'url': url,\n+            'ext': extension,\n+            'vcodec': None if is_video else 'none',\n+            'quality': 10 if note == 'high' else 0,\n+            'format_note': note,\n+            'format_id': join_nonempty(extension, note),\n         }\n-    }\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(url, video_id)\n \n-        title = self._html_search_regex(\n-            r'<h2 class=\"white\">(.*?)</h2>', webpage, 'title')\n-        video_info_dicts = re.findall(\n-            r\"({\\s*src\\s*:\\s*'https?://film\\.bpb\\.de/[^}]+})\", webpage)\n-\n-        formats = []\n-        for video_info in video_info_dicts:\n-            video_info = self._parse_json(\n-                video_info, video_id, transform_source=js_to_json, fatal=False)\n-            if not video_info:\n-                continue\n-            video_url = video_info.get('src')\n-            if not video_url:\n-                continue\n-            quality = 'high' if '_high' in video_url else 'low'\n-            formats.append({\n-                'url': video_url,\n-                'quality': 10 if quality == 'high' else 0,\n-                'format_note': quality,\n-                'format_id': '%s-%s' % (quality, determine_ext(video_url)),\n-            })\n+        title_result = traverse_obj(webpage, ({html_get_element(cls='opening-header__title')}, {self._TITLE_RE.match}))\n+        json_lds = list(self._yield_json_ld(webpage, video_id, fatal=False))\n \n         return {\n             'id': video_id,\n-            'formats': formats,\n-            'title': title,\n-            'description': self._og_search_description(webpage),\n+            'title': traverse_obj(title_result, ('title', {str.strip})) or None,\n+            # This metadata could be interpreted otherwise, but it fits \"series\" the most\n+            'series': traverse_obj(title_result, ('series', {str.strip})) or None,\n+            'description': join_nonempty(*traverse_obj(webpage, [(\n+                {html_get_element(cls='opening-intro')},\n+                [{html_get_element(tag='bpb-accordion-item')}, {html_get_element(cls='text-content')}],\n+            ), {clean_html}]), delim='\\n\\n') or None,\n+            'creator': self._html_search_meta('author', webpage),\n+            'uploader': self._html_search_meta('publisher', webpage),\n+            'release_date': unified_strdate(self._html_search_meta('date', webpage)),\n+            'tags': traverse_obj(json_lds, (..., 'keywords', {lambda x: x.split(',')}, ...)),\n+            **traverse_obj(self._parse_vue_attributes('bpb-player', webpage, video_id), {\n+                'formats': (':sources', ..., {self._process_source}),\n+                'thumbnail': ('poster', {lambda x: urljoin(url, x)}),\n+            }),\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/br.py",
            "diff": "diff --git a/yt_dlp/extractor/br.py b/yt_dlp/extractor/br.py\nindex 309452d2..6e1c63e2 100644\n--- a/yt_dlp/extractor/br.py\n+++ b/yt_dlp/extractor/br.py\n@@ -1,18 +1,15 @@\n-import json\n-\n from .common import InfoExtractor\n from ..utils import (\n-    determine_ext,\n     ExtractorError,\n     int_or_none,\n     parse_duration,\n-    parse_iso8601,\n     xpath_element,\n     xpath_text,\n )\n \n \n class BRIE(InfoExtractor):\n+    _WORKING = False\n     IE_DESC = 'Bayerischer Rundfunk'\n     _VALID_URL = r'(?P<base_url>https?://(?:www\\.)?br(?:-klassik)?\\.de)/(?:[a-z0-9\\-_]+/)+(?P<id>[a-z0-9\\-_]+)\\.html'\n \n@@ -167,142 +164,3 @@ def _extract_thumbnails(self, variants, base_url):\n         } for variant in variants.findall('variant') if xpath_text(variant, 'url')]\n         thumbnails.sort(key=lambda x: x['width'] * x['height'], reverse=True)\n         return thumbnails\n-\n-\n-class BRMediathekIE(InfoExtractor):\n-    IE_DESC = 'Bayerischer Rundfunk Mediathek'\n-    _VALID_URL = r'https?://(?:www\\.)?br\\.de/mediathek//?video/(?:[^/?&#]+?-)?(?P<id>av:[0-9a-f]{24})'\n-\n-    _TESTS = [{\n-        'url': 'https://www.br.de/mediathek/video/gesundheit-die-sendung-vom-28112017-av:5a1e6a6e8fce6d001871cc8e',\n-        'md5': 'fdc3d485835966d1622587d08ba632ec',\n-        'info_dict': {\n-            'id': 'av:5a1e6a6e8fce6d001871cc8e',\n-            'ext': 'mp4',\n-            'title': 'Die Sendung vom 28.11.2017',\n-            'description': 'md5:6000cdca5912ab2277e5b7339f201ccc',\n-            'timestamp': 1511942766,\n-            'upload_date': '20171129',\n-        }\n-    }, {\n-        'url': 'https://www.br.de/mediathek//video/av:61b0db581aed360007558c12',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        clip_id = self._match_id(url)\n-\n-        clip = self._download_json(\n-            'https://proxy-base.master.mango.express/graphql',\n-            clip_id, data=json.dumps({\n-                \"query\": \"\"\"{\n-  viewer {\n-    clip(id: \"%s\") {\n-      title\n-      description\n-      duration\n-      createdAt\n-      ageRestriction\n-      videoFiles {\n-        edges {\n-          node {\n-            publicLocation\n-            fileSize\n-            videoProfile {\n-              width\n-              height\n-              bitrate\n-              encoding\n-            }\n-          }\n-        }\n-      }\n-      captionFiles {\n-        edges {\n-          node {\n-            publicLocation\n-          }\n-        }\n-      }\n-      teaserImages {\n-        edges {\n-          node {\n-            imageFiles {\n-              edges {\n-                node {\n-                  publicLocation\n-                  width\n-                  height\n-                }\n-              }\n-            }\n-          }\n-        }\n-      }\n-    }\n-  }\n-}\"\"\" % clip_id}).encode(), headers={\n-                'Content-Type': 'application/json',\n-            })['data']['viewer']['clip']\n-        title = clip['title']\n-\n-        formats = []\n-        for edge in clip.get('videoFiles', {}).get('edges', []):\n-            node = edge.get('node', {})\n-            n_url = node.get('publicLocation')\n-            if not n_url:\n-                continue\n-            ext = determine_ext(n_url)\n-            if ext == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    n_url, clip_id, 'mp4', 'm3u8_native',\n-                    m3u8_id='hls', fatal=False))\n-            else:\n-                video_profile = node.get('videoProfile', {})\n-                tbr = int_or_none(video_profile.get('bitrate'))\n-                format_id = 'http'\n-                if tbr:\n-                    format_id += '-%d' % tbr\n-                formats.append({\n-                    'format_id': format_id,\n-                    'url': n_url,\n-                    'width': int_or_none(video_profile.get('width')),\n-                    'height': int_or_none(video_profile.get('height')),\n-                    'tbr': tbr,\n-                    'filesize': int_or_none(node.get('fileSize')),\n-                })\n-\n-        subtitles = {}\n-        for edge in clip.get('captionFiles', {}).get('edges', []):\n-            node = edge.get('node', {})\n-            n_url = node.get('publicLocation')\n-            if not n_url:\n-                continue\n-            subtitles.setdefault('de', []).append({\n-                'url': n_url,\n-            })\n-\n-        thumbnails = []\n-        for edge in clip.get('teaserImages', {}).get('edges', []):\n-            for image_edge in edge.get('node', {}).get('imageFiles', {}).get('edges', []):\n-                node = image_edge.get('node', {})\n-                n_url = node.get('publicLocation')\n-                if not n_url:\n-                    continue\n-                thumbnails.append({\n-                    'url': n_url,\n-                    'width': int_or_none(node.get('width')),\n-                    'height': int_or_none(node.get('height')),\n-                })\n-\n-        return {\n-            'id': clip_id,\n-            'title': title,\n-            'description': clip.get('description'),\n-            'duration': int_or_none(clip.get('duration')),\n-            'timestamp': parse_iso8601(clip.get('createdAt')),\n-            'age_limit': int_or_none(clip.get('ageRestriction')),\n-            'formats': formats,\n-            'subtitles': subtitles,\n-            'thumbnails': thumbnails,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/breakcom.py",
            "diff": "diff --git a/yt_dlp/extractor/breakcom.py b/yt_dlp/extractor/breakcom.py\ndeleted file mode 100644\nindex 00cf308c..00000000\n--- a/yt_dlp/extractor/breakcom.py\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-from .common import InfoExtractor\n-from .youtube import YoutubeIE\n-from ..utils import (\n-    int_or_none,\n-    url_or_none,\n-)\n-\n-\n-class BreakIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?break\\.com/video/(?P<display_id>[^/]+?)(?:-(?P<id>\\d+))?(?:[/?#&]|$)'\n-    _TESTS = [{\n-        'url': 'http://www.break.com/video/when-girls-act-like-guys-2468056',\n-        'info_dict': {\n-            'id': '2468056',\n-            'ext': 'mp4',\n-            'title': 'When Girls Act Like D-Bags',\n-            'age_limit': 13,\n-        },\n-    }, {\n-        # youtube embed\n-        'url': 'http://www.break.com/video/someone-forgot-boat-brakes-work',\n-        'info_dict': {\n-            'id': 'RrrDLdeL2HQ',\n-            'ext': 'mp4',\n-            'title': 'Whale Watching Boat Crashing Into San Diego Dock',\n-            'description': 'md5:afc1b2772f0a8468be51dd80eb021069',\n-            'upload_date': '20160331',\n-            'uploader': 'Steve Holden',\n-            'uploader_id': 'sdholden07',\n-        },\n-        'params': {\n-            'skip_download': True,\n-        }\n-    }, {\n-        'url': 'http://www.break.com/video/ugc/baby-flex-2773063',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        display_id, video_id = self._match_valid_url(url).groups()\n-\n-        webpage = self._download_webpage(url, display_id)\n-\n-        youtube_url = YoutubeIE._extract_url(webpage)\n-        if youtube_url:\n-            return self.url_result(youtube_url, ie=YoutubeIE.ie_key())\n-\n-        content = self._parse_json(\n-            self._search_regex(\n-                r'(?s)content[\"\\']\\s*:\\s*(\\[.+?\\])\\s*[,\\n]', webpage,\n-                'content'),\n-            display_id)\n-\n-        formats = []\n-        for video in content:\n-            video_url = url_or_none(video.get('url'))\n-            if not video_url:\n-                continue\n-            bitrate = int_or_none(self._search_regex(\n-                r'(\\d+)_kbps', video_url, 'tbr', default=None))\n-            formats.append({\n-                'url': video_url,\n-                'format_id': 'http-%d' % bitrate if bitrate else 'http',\n-                'tbr': bitrate,\n-            })\n-\n-        title = self._search_regex(\n-            (r'title[\"\\']\\s*:\\s*([\"\\'])(?P<value>(?:(?!\\1).)+)\\1',\n-             r'<h1[^>]*>(?P<value>[^<]+)'), webpage, 'title', group='value')\n-\n-        def get(key, name):\n-            return int_or_none(self._search_regex(\n-                r'%s[\"\\']\\s*:\\s*[\"\\'](\\d+)' % key, webpage, name,\n-                default=None))\n-\n-        age_limit = get('ratings', 'age limit')\n-        video_id = video_id or get('pid', 'video id') or display_id\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-            'age_limit': age_limit,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/breitbart.py",
            "diff": "diff --git a/yt_dlp/extractor/breitbart.py b/yt_dlp/extractor/breitbart.py\nindex ea0a59c8..b5abb7f1 100644\n--- a/yt_dlp/extractor/breitbart.py\n+++ b/yt_dlp/extractor/breitbart.py\n@@ -2,7 +2,7 @@\n \n \n class BreitBartIE(InfoExtractor):\n-    _VALID_URL = r'https?:\\/\\/(?:www\\.)breitbart.com/videos/v/(?P<id>[^/]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?breitbart\\.com/videos/v/(?P<id>[^/?#]+)'\n     _TESTS = [{\n         'url': 'https://www.breitbart.com/videos/v/5cOz1yup/?pl=Ij6NDOji',\n         'md5': '0aa6d1d6e183ac5ca09207fe49f17ade',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/brilliantpala.py",
            "diff": "diff --git a/yt_dlp/extractor/brilliantpala.py b/yt_dlp/extractor/brilliantpala.py\nnew file mode 100644\nindex 00000000..0bf8622c\n--- /dev/null\n+++ b/yt_dlp/extractor/brilliantpala.py\n@@ -0,0 +1,127 @@\n+import hashlib\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    traverse_obj,\n+    urlencode_postdata,\n+)\n+\n+\n+class BrilliantpalaBaseIE(InfoExtractor):\n+    _NETRC_MACHINE = 'brilliantpala'\n+    _DOMAIN = '{subdomain}.brilliantpala.org'\n+\n+    def _initialize_pre_login(self):\n+        self._HOMEPAGE = f'https://{self._DOMAIN}'\n+        self._LOGIN_API = f'{self._HOMEPAGE}/login/'\n+        self._LOGOUT_DEVICES_API = f'{self._HOMEPAGE}/logout_devices/?next=/'\n+        self._CONTENT_API = f'{self._HOMEPAGE}/api/v2.4/contents/{{content_id}}/'\n+        self._HLS_AES_URI = f'{self._HOMEPAGE}/api/v2.5/video_contents/{{content_id}}/key/'\n+\n+    def _get_logged_in_username(self, url, video_id):\n+        webpage, urlh = self._download_webpage_handle(url, video_id)\n+        if urlh.url.startswith(self._LOGIN_API):\n+            self.raise_login_required()\n+        return self._html_search_regex(\n+            r'\"username\"\\s*:\\s*\"(?P<username>[^\"]+)\"', webpage, 'logged-in username')\n+\n+    def _perform_login(self, username, password):\n+        login_form = self._hidden_inputs(self._download_webpage(\n+            self._LOGIN_API, None, 'Downloading login page'))\n+        login_form.update({\n+            'username': username,\n+            'password': password,\n+        })\n+        self._set_cookie(self._DOMAIN, 'csrftoken', login_form['csrfmiddlewaretoken'])\n+\n+        logged_page = self._download_webpage(\n+            self._LOGIN_API, None, note='Logging in', headers={'Referer': self._LOGIN_API},\n+            data=urlencode_postdata(login_form))\n+\n+        if self._html_search_regex(\n+                r'(Your username / email and password)', logged_page, 'auth fail', default=None):\n+            raise ExtractorError('wrong username or password', expected=True)\n+\n+        # the maximum number of logins is one\n+        if self._html_search_regex(\n+                r'(Logout Other Devices)', logged_page, 'logout devices button', default=None):\n+            logout_device_form = self._hidden_inputs(logged_page)\n+            self._download_webpage(\n+                self._LOGOUT_DEVICES_API, None, headers={'Referer': self._LOGIN_API},\n+                note='Logging out other devices', data=urlencode_postdata(logout_device_form))\n+\n+    def _real_extract(self, url):\n+        course_id, content_id = self._match_valid_url(url).group('course_id', 'content_id')\n+        video_id = f'{course_id}-{content_id}'\n+\n+        username = self._get_logged_in_username(url, video_id)\n+\n+        content_json = self._download_json(\n+            self._CONTENT_API.format(content_id=content_id), video_id,\n+            note='Fetching content info', errnote='Unable to fetch content info')\n+\n+        entries = []\n+        for stream in traverse_obj(content_json, ('video', 'streams', lambda _, v: v['id'] and v['url'])):\n+            formats = self._extract_m3u8_formats(stream['url'], video_id, fatal=False)\n+            if not formats:\n+                continue\n+            entries.append({\n+                'id': str(stream['id']),\n+                'title': content_json.get('title'),\n+                'formats': formats,\n+                'hls_aes': {'uri': self._HLS_AES_URI.format(content_id=content_id)},\n+                'http_headers': {'X-Key': hashlib.sha256(username.encode('ascii')).hexdigest()},\n+                'thumbnail': content_json.get('cover_image'),\n+            })\n+\n+        return self.playlist_result(\n+            entries, playlist_id=video_id, playlist_title=content_json.get('title'))\n+\n+\n+class BrilliantpalaElearnIE(BrilliantpalaBaseIE):\n+    IE_NAME = 'Brilliantpala:Elearn'\n+    IE_DESC = 'VoD on elearn.brilliantpala.org'\n+    _VALID_URL = r'https?://elearn\\.brilliantpala\\.org/courses/(?P<course_id>\\d+)/contents/(?P<content_id>\\d+)/?'\n+    _TESTS = [{\n+        'url': 'https://elearn.brilliantpala.org/courses/42/contents/12345/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://elearn.brilliantpala.org/courses/98/contents/36683/',\n+        'info_dict': {\n+            'id': '23577',\n+            'ext': 'mp4',\n+            'title': 'Physical World, Units and Measurements  - 1',\n+            'thumbnail': 'https://d1j3vi2u94ebt0.cloudfront.net/institute/brilliantpalalms/chapter_contents/26237/e657f81b90874be19795c7ea081f8d5c.png',\n+            'live_status': 'not_live',\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }]\n+\n+    _DOMAIN = BrilliantpalaBaseIE._DOMAIN.format(subdomain='elearn')\n+\n+\n+class BrilliantpalaClassesIE(BrilliantpalaBaseIE):\n+    IE_NAME = 'Brilliantpala:Classes'\n+    IE_DESC = 'VoD on classes.brilliantpala.org'\n+    _VALID_URL = r'https?://classes\\.brilliantpala\\.org/courses/(?P<course_id>\\d+)/contents/(?P<content_id>\\d+)/?'\n+    _TESTS = [{\n+        'url': 'https://classes.brilliantpala.org/courses/42/contents/12345/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://classes.brilliantpala.org/courses/416/contents/25445/',\n+        'info_dict': {\n+            'id': '9128',\n+            'ext': 'mp4',\n+            'title': 'Motion in a Straight Line - Class 1',\n+            'thumbnail': 'https://d3e4y8hquds3ek.cloudfront.net/institute/brilliantpalaelearn/chapter_contents/ff5ba838d0ec43419f67387fe1a01fa8.png',\n+            'live_status': 'not_live',\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }]\n+\n+    _DOMAIN = BrilliantpalaBaseIE._DOMAIN.format(subdomain='classes')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/bundestag.py",
            "diff": "diff --git a/yt_dlp/extractor/bundestag.py b/yt_dlp/extractor/bundestag.py\nnew file mode 100644\nindex 00000000..9fd7c7de\n--- /dev/null\n+++ b/yt_dlp/extractor/bundestag.py\n@@ -0,0 +1,123 @@\n+import re\n+from functools import partial\n+\n+from .common import InfoExtractor\n+from ..networking.exceptions import HTTPError\n+from ..utils import (\n+    ExtractorError,\n+    bug_reports_message,\n+    clean_html,\n+    format_field,\n+    get_element_text_and_html_by_tag,\n+    int_or_none,\n+    url_or_none,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class BundestagIE(InfoExtractor):\n+    _VALID_URL = [\n+        r'https?://dbtg\\.tv/[cf]vid/(?P<id>\\d+)',\n+        r'https?://www\\.bundestag\\.de/mediathek/?\\?(?:[^#]+&)?videoid=(?P<id>\\d+)',\n+    ]\n+    _TESTS = [{\n+        'url': 'https://dbtg.tv/cvid/7605304',\n+        'info_dict': {\n+            'id': '7605304',\n+            'ext': 'mp4',\n+            'title': '145. Sitzung vom 15.12.2023, TOP 24 Barrierefreiheit',\n+            'description': 'md5:321a9dc6bdad201264c0045efc371561',\n+        },\n+    }, {\n+        'url': 'https://www.bundestag.de/mediathek?videoid=7602120&url=L21lZGlhdGhla292ZXJsYXk=&mod=mediathek',\n+        'info_dict': {\n+            'id': '7602120',\n+            'ext': 'mp4',\n+            'title': '130. Sitzung vom 18.10.2023, TOP 1 Befragung der Bundesregierung',\n+            'description': 'Befragung der Bundesregierung',\n+        },\n+    }, {\n+        'url': 'https://www.bundestag.de/mediathek?videoid=7604941#url=L21lZGlhdGhla292ZXJsYXk/dmlkZW9pZD03NjA0OTQx&mod=mediathek',\n+        'only_matching': True,\n+    }, {\n+        'url': 'http://dbtg.tv/fvid/3594346',\n+        'only_matching': True,\n+    }]\n+\n+    _OVERLAY_URL = 'https://www.bundestag.de/mediathekoverlay'\n+    _INSTANCE_FORMAT = 'https://cldf-wzw-od.r53.cdn.tv1.eu/13014bundestagod/_definst_/13014bundestag/ondemand/3777parlamentsfernsehen/archiv/app144277506/145293313/{0}/{0}_playlist.smil/playlist.m3u8'\n+\n+    _SHARE_URL = 'https://webtv.bundestag.de/player/macros/_x_s-144277506/shareData.json?contentId='\n+    _SHARE_AUDIO_REGEX = r'/\\d+_(?P<codec>\\w+)_(?P<bitrate>\\d+)kb_(?P<channels>\\w+)_\\w+_\\d+\\.(?P<ext>\\w+)'\n+    _SHARE_VIDEO_REGEX = r'/\\d+_(?P<codec>\\w+)_(?P<width>\\w+)_(?P<height>\\w+)_(?P<bitrate>\\d+)kb_\\w+_\\w+_\\d+\\.(?P<ext>\\w+)'\n+\n+    def _bt_extract_share_formats(self, video_id):\n+        share_data = self._download_json(\n+            f'{self._SHARE_URL}{video_id}', video_id, note='Downloading share format JSON')\n+        if traverse_obj(share_data, ('status', 'code', {int})) != 1:\n+            self.report_warning(format_field(\n+                share_data, [('status', 'message', {str})],\n+                'Share API response: %s', default='Unknown Share API Error')\n+                + bug_reports_message())\n+            return\n+\n+        for name, url in share_data.items():\n+            if not isinstance(name, str) or not url_or_none(url):\n+                continue\n+\n+            elif name.startswith('audio'):\n+                match = re.search(self._SHARE_AUDIO_REGEX, url)\n+                yield {\n+                    'format_id': name,\n+                    'url': url,\n+                    'vcodec': 'none',\n+                    **traverse_obj(match, {\n+                        'acodec': 'codec',\n+                        'audio_channels': ('channels', {{'mono': 1, 'stereo': 2}.get}),\n+                        'abr': ('bitrate', {int_or_none}),\n+                        'ext': 'ext',\n+                    }),\n+                }\n+\n+            elif name.startswith('download'):\n+                match = re.search(self._SHARE_VIDEO_REGEX, url)\n+                yield {\n+                    'format_id': name,\n+                    'url': url,\n+                    **traverse_obj(match, {\n+                        'vcodec': 'codec',\n+                        'tbr': ('bitrate', {int_or_none}),\n+                        'width': ('width', {int_or_none}),\n+                        'height': ('height', {int_or_none}),\n+                        'ext': 'ext',\n+                    }),\n+                }\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        formats = []\n+        result = {'id': video_id, 'formats': formats}\n+\n+        try:\n+            formats.extend(self._extract_m3u8_formats(\n+                self._INSTANCE_FORMAT.format(video_id), video_id, m3u8_id='instance'))\n+        except ExtractorError as error:\n+            if isinstance(error.cause, HTTPError) and error.cause.status == 404:\n+                raise ExtractorError('Could not find video id', expected=True)\n+            self.report_warning(f'Error extracting hls formats: {error}', video_id)\n+        formats.extend(self._bt_extract_share_formats(video_id))\n+        if not formats:\n+            self.raise_no_formats('Could not find suitable formats', video_id=video_id)\n+\n+        result.update(traverse_obj(self._download_webpage(\n+            self._OVERLAY_URL, video_id,\n+            query={'videoid': video_id, 'view': 'main'},\n+            note='Downloading metadata overlay', fatal=False,\n+        ), {\n+            'title': (\n+                {partial(get_element_text_and_html_by_tag, 'h3')}, 0,\n+                {partial(re.sub, r'<span[^>]*>[^<]+</span>', '')}, {clean_html}),\n+            'description': ({partial(get_element_text_and_html_by_tag, 'p')}, 0, {clean_html}),\n+        }))\n+\n+        return result\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/byutv.py",
            "diff": "diff --git a/yt_dlp/extractor/byutv.py b/yt_dlp/extractor/byutv.py\nindex 9ed6efe7..ad35427e 100644\n--- a/yt_dlp/extractor/byutv.py\n+++ b/yt_dlp/extractor/byutv.py\n@@ -8,9 +8,9 @@\n \n \n class BYUtvIE(InfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?byutv\\.org/(?:watch|player)/(?!event/)(?P<id>[0-9a-f-]+)(?:/(?P<display_id>[^/?#&]+))?'\n     _TESTS = [{\n-        # ooyalaVOD\n         'url': 'http://www.byutv.org/watch/6587b9a3-89d2-42a6-a7f7-fd2f81840a7d/studio-c-season-5-episode-5',\n         'info_dict': {\n             'id': 'ZvanRocTpW-G5_yZFeltTAMv6jxOU9KH',\n@@ -24,7 +24,6 @@ class BYUtvIE(InfoExtractor):\n         'params': {\n             'skip_download': True,\n         },\n-        'add_ie': ['Ooyala'],\n     }, {\n         # dvr\n         'url': 'https://www.byutv.org/player/8f1dab9b-b243-47c8-b525-3e2d021a3451/byu-softball-pacific-vs-byu-41219---game-2',\n@@ -63,19 +62,6 @@ def _real_extract(self, url):\n                 'x-byutv-platformkey': 'xsaaw9c7y5',\n             })\n \n-        ep = video.get('ooyalaVOD')\n-        if ep:\n-            return {\n-                '_type': 'url_transparent',\n-                'ie_key': 'Ooyala',\n-                'url': 'ooyala:%s' % ep['providerId'],\n-                'id': video_id,\n-                'display_id': display_id,\n-                'title': ep.get('title'),\n-                'description': ep.get('description'),\n-                'thumbnail': ep.get('imageThumbnail'),\n-            }\n-\n         info = {}\n         formats = []\n         subtitles = {}\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/camwithher.py",
            "diff": "diff --git a/yt_dlp/extractor/camwithher.py b/yt_dlp/extractor/camwithher.py\ndeleted file mode 100644\nindex a0b3749e..00000000\n--- a/yt_dlp/extractor/camwithher.py\n+++ /dev/null\n@@ -1,87 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    parse_duration,\n-    unified_strdate,\n-)\n-\n-\n-class CamWithHerIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?camwithher\\.tv/view_video\\.php\\?.*\\bviewkey=(?P<id>\\w+)'\n-\n-    _TESTS = [{\n-        'url': 'http://camwithher.tv/view_video.php?viewkey=6e9a24e2c0e842e1f177&page=&viewtype=&category=',\n-        'info_dict': {\n-            'id': '5644',\n-            'ext': 'flv',\n-            'title': 'Periscope Tease',\n-            'description': 'In the clouds teasing on periscope to my favorite song',\n-            'duration': 240,\n-            'view_count': int,\n-            'comment_count': int,\n-            'uploader': 'MileenaK',\n-            'upload_date': '20160322',\n-            'age_limit': 18,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        }\n-    }, {\n-        'url': 'http://camwithher.tv/view_video.php?viewkey=6dfd8b7c97531a459937',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://camwithher.tv/view_video.php?page=&viewkey=6e9a24e2c0e842e1f177&viewtype=&category=',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://camwithher.tv/view_video.php?viewkey=b6c3b5bea9515d1a1fc4&page=&viewtype=&category=mv',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        flv_id = self._html_search_regex(\n-            r'<a[^>]+href=[\"\\']/download/\\?v=(\\d+)', webpage, 'video id')\n-\n-        # Video URL construction algorithm is reverse-engineered from cwhplayer.swf\n-        rtmp_url = 'rtmp://camwithher.tv/clipshare/%s' % (\n-            ('mp4:%s.mp4' % flv_id) if int(flv_id) > 2010 else flv_id)\n-\n-        title = self._html_search_regex(\n-            r'<div[^>]+style=\"float:left\"[^>]*>\\s*<h2>(.+?)</h2>', webpage, 'title')\n-        description = self._html_search_regex(\n-            r'>Description:</span>(.+?)</div>', webpage, 'description', default=None)\n-\n-        runtime = self._search_regex(\n-            r'Runtime\\s*:\\s*(.+?) \\|', webpage, 'duration', default=None)\n-        if runtime:\n-            runtime = re.sub(r'[\\s-]', '', runtime)\n-        duration = parse_duration(runtime)\n-        view_count = int_or_none(self._search_regex(\n-            r'Views\\s*:\\s*(\\d+)', webpage, 'view count', default=None))\n-        comment_count = int_or_none(self._search_regex(\n-            r'Comments\\s*:\\s*(\\d+)', webpage, 'comment count', default=None))\n-\n-        uploader = self._search_regex(\n-            r'Added by\\s*:\\s*<a[^>]+>([^<]+)</a>', webpage, 'uploader', default=None)\n-        upload_date = unified_strdate(self._search_regex(\n-            r'Added on\\s*:\\s*([\\d-]+)', webpage, 'upload date', default=None))\n-\n-        return {\n-            'id': flv_id,\n-            'url': rtmp_url,\n-            'ext': 'flv',\n-            'no_resume': True,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'view_count': view_count,\n-            'comment_count': comment_count,\n-            'uploader': uploader,\n-            'upload_date': upload_date,\n-            'age_limit': 18\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/canal1.py",
            "diff": "diff --git a/yt_dlp/extractor/canal1.py b/yt_dlp/extractor/canal1.py\nnew file mode 100644\nindex 00000000..587a11ab\n--- /dev/null\n+++ b/yt_dlp/extractor/canal1.py\n@@ -0,0 +1,39 @@\n+from .common import InfoExtractor\n+\n+\n+class Canal1IE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.|noticias\\.)?canal1\\.com\\.co/(?:[^?#&])+/(?P<id>[\\w-]+)'\n+\n+    _TESTS = [{\n+        'url': 'https://canal1.com.co/noticias/napa-i-una-cadena-de-produccion-de-arroz-que-se-quedo-en-veremos-y-abandonada-en-el-departamento-del-choco/',\n+        'info_dict': {\n+            'id': '63b39f6b354977084b85ab54',\n+            'display_id': 'napa-i-una-cadena-de-produccion-de-arroz-que-se-quedo-en-veremos-y-abandonada-en-el-departamento-del-choco',\n+            'title': '\u00d1apa I Una cadena de producci\u00f3n de arroz que se qued\u00f3 en veremos y abandonada en el departamento del Choc\u00f3',\n+            'description': 'md5:bc49c6d64d20610ea1e7daf079a0d013',\n+            'thumbnail': r're:^https?://[^?#]+63b39f6b354977084b85ab54',\n+            'ext': 'mp4',\n+        },\n+    }, {\n+        'url': 'https://noticias.canal1.com.co/noticias/tres-i-el-triste-record-que-impuso-elon-musk-el-dueno-de-tesla-y-de-twitter/',\n+        'info_dict': {\n+            'id': '63b39e93f5fd223aa32250fb',\n+            'display_id': 'tres-i-el-triste-record-que-impuso-elon-musk-el-dueno-de-tesla-y-de-twitter',\n+            'title': 'Tres I El triste r\u00e9cord que impuso Elon Musk, el due\u00f1o de Tesla y de Twitter',\n+            'description': 'md5:d9f691f131a21ce6767ca6c05d17d791',\n+            'thumbnail': r're:^https?://[^?#]+63b39e93f5fd223aa32250fb',\n+            'ext': 'mp4',\n+        },\n+    }, {\n+        # Geo-restricted to Colombia\n+        'url': 'https://canal1.com.co/programas/guerreros-canal-1/video-inedito-guerreros-despedida-kewin-zarate/',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+        webpage = self._download_webpage(url, display_id)\n+\n+        return self.url_result(\n+            self._search_regex(r'\"embedUrl\"\\s*:\\s*\"([^\"]+)', webpage, 'embed url'),\n+            display_id=display_id, url_transparent=True)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/caracoltv.py",
            "diff": "diff --git a/yt_dlp/extractor/caracoltv.py b/yt_dlp/extractor/caracoltv.py\nnew file mode 100644\nindex 00000000..79f7752f\n--- /dev/null\n+++ b/yt_dlp/extractor/caracoltv.py\n@@ -0,0 +1,136 @@\n+import base64\n+import json\n+import uuid\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    int_or_none,\n+    js_to_json,\n+    traverse_obj,\n+    urljoin,\n+)\n+\n+\n+class CaracolTvPlayIE(InfoExtractor):\n+    _VALID_URL = r'https?://play\\.caracoltv\\.com/videoDetails/(?P<id>[^/?#]+)'\n+    _NETRC_MACHINE = 'caracoltv-play'\n+\n+    _TESTS = [{\n+        'url': 'https://play.caracoltv.com/videoDetails/OTo4NGFmNjUwOWQ2ZmM0NTg2YWRiOWU0MGNhOWViOWJkYQ==',\n+        'info_dict': {\n+            'id': 'OTo4NGFmNjUwOWQ2ZmM0NTg2YWRiOWU0MGNhOWViOWJkYQ==',\n+            'title': 'La teor\u00eda del promedio',\n+            'description': 'md5:1cdd6d2c13f19ef0d9649ab81a023ac3',\n+        },\n+        'playlist_count': 6,\n+    }, {\n+        'url': 'https://play.caracoltv.com/videoDetails/OTo3OWM4ZTliYzQxMmM0MTMxYTk4Mjk2YjdjNGQ4NGRkOQ==/ella?season=0',\n+        'info_dict': {\n+            'id': 'OTo3OWM4ZTliYzQxMmM0MTMxYTk4Mjk2YjdjNGQ4NGRkOQ==',\n+            'title': 'Ella',\n+            'description': 'md5:a639b1feb5ddcc0cff92a489b4e544b8',\n+        },\n+        'playlist_count': 10,\n+    }, {\n+        'url': 'https://play.caracoltv.com/videoDetails/OTpiYTY1YTVmOTI5MzI0ZWJhOGZiY2Y3MmRlOWZlYmJkOA==/la-vuelta-al-mundo-en-80-risas-2022?season=0',\n+        'info_dict': {\n+            'id': 'OTpiYTY1YTVmOTI5MzI0ZWJhOGZiY2Y3MmRlOWZlYmJkOA==',\n+            'title': 'La vuelta al mundo en 80 risas 2022',\n+            'description': 'md5:e97aac36106e5c37ebf947b3350106a4',\n+        },\n+        'playlist_count': 17,\n+    }, {\n+        'url': 'https://play.caracoltv.com/videoDetails/MzoxX3BwbjRmNjB1',\n+        'only_matching': True,\n+    }]\n+\n+    _USER_TOKEN = None\n+\n+    def _extract_app_token(self, webpage):\n+        config_js_path = self._search_regex(\n+            r'<script[^>]+src\\s*=\\s*\"([^\"]+coreConfig.js[^\"]+)', webpage, 'config js url', fatal=False)\n+\n+        mediation_config = {} if not config_js_path else self._search_json(\n+            r'mediation\\s*:', self._download_webpage(\n+                urljoin('https://play.caracoltv.com/', config_js_path), None, fatal=False, note='Extracting JS config'),\n+            'mediation_config', None, transform_source=js_to_json, fatal=False)\n+\n+        key = traverse_obj(\n+            mediation_config, ('live', 'key')) or '795cd9c089a1fc48094524a5eba85a3fca1331817c802f601735907c8bbb4f50'\n+        secret = traverse_obj(\n+            mediation_config, ('live', 'secret')) or '64dec00a6989ba83d087621465b5e5d38bdac22033b0613b659c442c78976fa0'\n+\n+        return base64.b64encode(f'{key}:{secret}'.encode()).decode()\n+\n+    def _perform_login(self, email, password):\n+        webpage = self._download_webpage('https://play.caracoltv.com/', None, fatal=False)\n+        app_token = self._extract_app_token(webpage)\n+\n+        bearer_token = self._download_json(\n+            'https://eu-gateway.inmobly.com/applications/oauth', None, data=b'', note='Retrieving bearer token',\n+            headers={'Authorization': f'Basic {app_token}'})['token']\n+\n+        self._USER_TOKEN = self._download_json(\n+            'https://eu-gateway.inmobly.com/user/login', None, note='Performing login', headers={\n+                'Content-Type': 'application/json',\n+                'Authorization': f'Bearer {bearer_token}',\n+            }, data=json.dumps({\n+                'device_data': {\n+                    'device_id': str(uuid.uuid4()),\n+                    'device_token': '',\n+                    'device_type': 'web'\n+                },\n+                'login_data': {\n+                    'enabled': True,\n+                    'email': email,\n+                    'password': password,\n+                }\n+            }).encode())['user_token']\n+\n+    def _extract_video(self, video_data, series_id=None, season_id=None, season_number=None):\n+        formats, subtitles = self._extract_m3u8_formats_and_subtitles(video_data['stream_url'], series_id, 'mp4')\n+\n+        return {\n+            'id': video_data['id'],\n+            'title': video_data.get('name'),\n+            'description': video_data.get('description'),\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            'thumbnails': traverse_obj(\n+                video_data, ('extra_thumbs', ..., {'url': 'thumb_url', 'height': 'height', 'width': 'width'})),\n+            'series_id': series_id,\n+            'season_id': season_id,\n+            'season_number': int_or_none(season_number),\n+            'episode_number': int_or_none(video_data.get('item_order')),\n+            'is_live': video_data.get('entry_type') == 3,\n+        }\n+\n+    def _extract_series_seasons(self, seasons, series_id):\n+        for season in seasons:\n+            api_response = self._download_json(\n+                'https://eu-gateway.inmobly.com/feed', series_id, query={'season_id': season['id']},\n+                headers={'Authorization': f'Bearer {self._USER_TOKEN}'})\n+\n+            season_number = season.get('order')\n+            for episode in api_response['items']:\n+                yield self._extract_video(episode, series_id, season['id'], season_number)\n+\n+    def _real_extract(self, url):\n+        series_id = self._match_id(url)\n+\n+        if self._USER_TOKEN is None:\n+            self._perform_login('guest@inmobly.com', 'Test@gus1')\n+\n+        api_response = self._download_json(\n+            'https://eu-gateway.inmobly.com/feed', series_id, query={'include_ids': series_id},\n+            headers={'Authorization': f'Bearer {self._USER_TOKEN}'})['items'][0]\n+\n+        if not api_response.get('seasons'):\n+            return self._extract_video(api_response)\n+\n+        return self.playlist_result(\n+            self._extract_series_seasons(api_response['seasons'], series_id),\n+            series_id, **traverse_obj(api_response, {\n+                'title': 'name',\n+                'description': 'description',\n+            }))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/carambatv.py",
            "diff": "diff --git a/yt_dlp/extractor/carambatv.py b/yt_dlp/extractor/carambatv.py\ndeleted file mode 100644\nindex d6044a31..00000000\n--- a/yt_dlp/extractor/carambatv.py\n+++ /dev/null\n@@ -1,105 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    format_field,\n-    float_or_none,\n-    int_or_none,\n-    try_get,\n-)\n-\n-from .videomore import VideomoreIE\n-\n-\n-class CarambaTVIE(InfoExtractor):\n-    _VALID_URL = r'(?:carambatv:|https?://video1\\.carambatv\\.ru/v/)(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'http://video1.carambatv.ru/v/191910501',\n-        'md5': '2f4a81b7cfd5ab866ee2d7270cb34a2a',\n-        'info_dict': {\n-            'id': '191910501',\n-            'ext': 'mp4',\n-            'title': '[BadComedian] - \u0420\u0430\u0437\u0431\u043e\u0440\u043a\u0430 \u0432 \u041c\u0430\u043d\u0438\u043b\u0435 (\u0410\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0439 \u043e\u0431\u0437\u043e\u0440)',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n-            'duration': 2678.31,\n-        },\n-    }, {\n-        'url': 'carambatv:191910501',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        video = self._download_json(\n-            'http://video1.carambatv.ru/v/%s/videoinfo.js' % video_id,\n-            video_id)\n-\n-        title = video['title']\n-\n-        base_url = video.get('video') or 'http://video1.carambatv.ru/v/%s/' % video_id\n-\n-        formats = [{\n-            'url': base_url + f['fn'],\n-            'height': int_or_none(f.get('height')),\n-            'format_id': format_field(f, 'height', '%sp'),\n-        } for f in video['qualities'] if f.get('fn')]\n-\n-        thumbnail = video.get('splash')\n-        duration = float_or_none(try_get(\n-            video, lambda x: x['annotations'][0]['end_time'], compat_str))\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'formats': formats,\n-        }\n-\n-\n-class CarambaTVPageIE(InfoExtractor):\n-    _VALID_URL = r'https?://carambatv\\.ru/(?:[^/]+/)+(?P<id>[^/?#&]+)'\n-    _TEST = {\n-        'url': 'http://carambatv.ru/movie/bad-comedian/razborka-v-manile/',\n-        'md5': 'a49fb0ec2ad66503eeb46aac237d3c86',\n-        'info_dict': {\n-            'id': '475222',\n-            'ext': 'flv',\n-            'title': '[BadComedian] - \u0420\u0430\u0437\u0431\u043e\u0440\u043a\u0430 \u0432 \u041c\u0430\u043d\u0438\u043b\u0435 (\u0410\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0439 \u043e\u0431\u0437\u043e\u0440)',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n-            # duration reported by videomore is incorrect\n-            'duration': int,\n-        },\n-        'add_ie': [VideomoreIE.ie_key()],\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        videomore_url = VideomoreIE._extract_url(webpage)\n-        if not videomore_url:\n-            videomore_id = self._search_regex(\n-                r'getVMCode\\s*\\(\\s*[\"\\']?(\\d+)', webpage, 'videomore id',\n-                default=None)\n-            if videomore_id:\n-                videomore_url = 'videomore:%s' % videomore_id\n-        if videomore_url:\n-            title = self._og_search_title(webpage)\n-            return {\n-                '_type': 'url_transparent',\n-                'url': videomore_url,\n-                'ie_key': VideomoreIE.ie_key(),\n-                'title': title,\n-            }\n-\n-        video_url = self._og_search_property('video:iframe', webpage, default=None)\n-\n-        if not video_url:\n-            video_id = self._search_regex(\n-                r'(?:video_id|crmb_vuid)\\s*[:=]\\s*[\"\\']?(\\d+)',\n-                webpage, 'video id')\n-            video_url = 'carambatv:%s' % video_id\n-\n-        return self.url_result(video_url, CarambaTVIE.ie_key())\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cbc.py",
            "diff": "diff --git a/yt_dlp/extractor/cbc.py b/yt_dlp/extractor/cbc.py\nindex 9413281a..b5beb1ec 100644\n--- a/yt_dlp/extractor/cbc.py\n+++ b/yt_dlp/extractor/cbc.py\n@@ -1,7 +1,9 @@\n-import re\n-import json\n import base64\n+import json\n+import re\n import time\n+import urllib.parse\n+import xml.etree.ElementTree\n \n from .common import InfoExtractor\n from ..compat import (\n@@ -65,6 +67,7 @@ class CBCIE(InfoExtractor):\n             'uploader': 'CBCC-NEW',\n             'timestamp': 255977160,\n         },\n+        'skip': '404 Not Found',\n     }, {\n         # multiple iframes\n         'url': 'http://www.cbc.ca/natureofthings/blog/birds-eye-view-from-vancouvers-burrard-street-bridge-how-we-got-the-shot',\n@@ -96,7 +99,7 @@ class CBCIE(InfoExtractor):\n         # multiple CBC.APP.Caffeine.initInstance(...)\n         'url': 'http://www.cbc.ca/news/canada/calgary/dog-indoor-exercise-winter-1.3928238',\n         'info_dict': {\n-            'title': 'Keep Rover active during the deep freeze with doggie pushups and other fun indoor tasks',\n+            'title': 'Keep Rover active during the deep freeze with doggie pushups and other fun indoor tasks',  # FIXME\n             'id': 'dog-indoor-exercise-winter-1.3928238',\n             'description': 'md5:c18552e41726ee95bd75210d1ca9194c',\n         },\n@@ -177,6 +180,13 @@ class CBCPlayerIE(InfoExtractor):\n             'thumbnail': 'http://thumbnails.cbc.ca/maven_legacy/thumbnails/sonali-karnick-220.jpg',\n             'chapters': [],\n             'duration': 494.811,\n+            'categories': ['AudioMobile/All in a Weekend Montreal'],\n+            'tags': 'count:8',\n+            'location': 'Quebec',\n+            'series': 'All in a Weekend Montreal',\n+            'season': 'Season 2015',\n+            'season_number': 2015,\n+            'media_type': 'Excerpt',\n         },\n     }, {\n         'url': 'http://www.cbc.ca/player/play/2164402062',\n@@ -192,25 +202,37 @@ class CBCPlayerIE(InfoExtractor):\n             'thumbnail': 'https://thumbnails.cbc.ca/maven_legacy/thumbnails/277/67/cancer_852x480_2164412612.jpg',\n             'chapters': [],\n             'duration': 186.867,\n+            'series': 'CBC News: Windsor at 6:00',\n+            'categories': ['News/Canada/Windsor'],\n+            'location': 'Windsor',\n+            'tags': ['cancer'],\n+            'creator': 'Allison Johnson',\n+            'media_type': 'Excerpt',\n         },\n     }, {\n         # Has subtitles\n         # These broadcasts expire after ~1 month, can find new test URL here:\n         # https://www.cbc.ca/player/news/TV%20Shows/The%20National/Latest%20Broadcast\n-        'url': 'http://www.cbc.ca/player/play/2249992771553',\n-        'md5': '2f2fb675dd4f0f8a5bb7588d1b13bacd',\n+        'url': 'http://www.cbc.ca/player/play/2284799043667',\n+        'md5': '9b49f0839e88b6ec0b01d840cf3d42b5',\n         'info_dict': {\n-            'id': '2249992771553',\n+            'id': '2284799043667',\n             'ext': 'mp4',\n-            'title': 'The National | Women\u2019s soccer pay, Florida seawater, Swift quake',\n-            'description': 'md5:adba28011a56cfa47a080ff198dad27a',\n-            'timestamp': 1690596000,\n-            'duration': 2716.333,\n+            'title': 'The National | Hockey coach charged, Green grants, Safer drugs',\n+            'description': 'md5:84ef46321c94bcf7d0159bb565d26bfa',\n+            'timestamp': 1700272800,\n+            'duration': 2718.833,\n             'subtitles': {'eng': [{'ext': 'vtt', 'protocol': 'm3u8_native'}]},\n-            'thumbnail': 'https://thumbnails.cbc.ca/maven_legacy/thumbnails/481/326/thumbnail.jpeg',\n+            'thumbnail': 'https://thumbnails.cbc.ca/maven_legacy/thumbnails/907/171/thumbnail.jpeg',\n             'uploader': 'CBCC-NEW',\n             'chapters': 'count:5',\n-            'upload_date': '20230729',\n+            'upload_date': '20231118',\n+            'categories': 'count:4',\n+            'series': 'The National - Full Show',\n+            'tags': 'count:1',\n+            'creator': 'News',\n+            'location': 'Canada',\n+            'media_type': 'Full Program',\n         },\n     }]\n \n@@ -228,6 +250,38 @@ def _real_extract(self, url):\n         }\n \n \n+class CBCPlayerPlaylistIE(InfoExtractor):\n+    IE_NAME = 'cbc.ca:player:playlist'\n+    _VALID_URL = r'https?://(?:www\\.)?cbc\\.ca/(?:player/)(?!play/)(?P<id>[^?#]+)'\n+    _TESTS = [{\n+        'url': 'https://www.cbc.ca/player/news/TV%20Shows/The%20National/Latest%20Broadcast',\n+        'playlist_mincount': 25,\n+        'info_dict': {\n+            'id': 'news/tv shows/the national/latest broadcast',\n+        }\n+    }, {\n+        'url': 'https://www.cbc.ca/player/news/Canada/North',\n+        'playlist_mincount': 25,\n+        'info_dict': {\n+            'id': 'news/canada/north',\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        playlist_id = urllib.parse.unquote(self._match_id(url)).lower()\n+        webpage = self._download_webpage(url, playlist_id)\n+        json_content = self._search_json(\n+            r'window\\.__INITIAL_STATE__\\s*=', webpage, 'initial state', playlist_id)\n+\n+        def entries():\n+            for video_id in traverse_obj(json_content, (\n+                'video', 'clipsByCategory', lambda k, _: k.lower() == playlist_id, 'items', ..., 'id'\n+            )):\n+                yield self.url_result(f'https://www.cbc.ca/player/play/{video_id}', CBCPlayerIE)\n+\n+        return self.playlist_result(entries(), playlist_id)\n+\n+\n class CBCGemIE(InfoExtractor):\n     IE_NAME = 'gem.cbc.ca'\n     _VALID_URL = r'https?://gem\\.cbc\\.ca/(?:media/)?(?P<id>[0-9a-z-]+/s[0-9]+[a-z][0-9]+)'\n@@ -306,12 +360,12 @@ def _new_claims_token(self, email, password):\n         data = json.dumps({'jwt': sig}).encode()\n         headers = {'content-type': 'application/json', 'ott-device-type': 'web'}\n         resp = self._download_json('https://services.radio-canada.ca/ott/cbc-api/v2/token',\n-                                   None, data=data, headers=headers)\n+                                   None, data=data, headers=headers, expected_status=426)\n         cbc_access_token = resp['accessToken']\n \n         headers = {'content-type': 'application/json', 'ott-device-type': 'web', 'ott-access-token': cbc_access_token}\n         resp = self._download_json('https://services.radio-canada.ca/ott/cbc-api/v2/profile',\n-                                   None, headers=headers)\n+                                   None, headers=headers, expected_status=426)\n         return resp['claimsToken']\n \n     def _get_claims_token_expiry(self):\n@@ -353,7 +407,7 @@ def _find_secret_formats(self, formats, video_id):\n         url = re.sub(r'(Manifest\\(.*?),format=[\\w-]+(.*?\\))', r'\\1\\2', base_url)\n \n         secret_xml = self._download_xml(url, video_id, note='Downloading secret XML', fatal=False)\n-        if not secret_xml:\n+        if not isinstance(secret_xml, xml.etree.ElementTree.Element):\n             return\n \n         for child in secret_xml:\n@@ -443,6 +497,10 @@ class CBCGemPlaylistIE(InfoExtractor):\n             'id': 'schitts-creek/s06',\n             'title': 'Season 6',\n             'description': 'md5:6a92104a56cbeb5818cc47884d4326a2',\n+            'series': 'Schitt\\'s Creek',\n+            'season_number': 6,\n+            'season': 'Season 6',\n+            'thumbnail': 'https://images.radio-canada.ca/v1/synps-cbc/season/perso/cbc_schitts_creek_season_06_carousel_v03.jpg?impolicy=ott&im=Resize=(_Size_)&quality=75',\n         },\n     }, {\n         'url': 'https://gem.cbc.ca/schitts-creek/s06',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cbs.py",
            "diff": "diff --git a/yt_dlp/extractor/cbs.py b/yt_dlp/extractor/cbs.py\nindex 1c0dbdea..d97fbd75 100644\n--- a/yt_dlp/extractor/cbs.py\n+++ b/yt_dlp/extractor/cbs.py\n@@ -101,6 +101,7 @@ class CBSIE(CBSBaseIE):\n             # m3u8 download\n             'skip_download': True,\n         },\n+        'skip': 'Subscription required',\n     }, {\n         'url': 'https://www.cbs.com/shows/video/sZH1MGgomIosZgxGJ1l263MFq16oMtW1/',\n         'info_dict': {\n@@ -117,6 +118,7 @@ class CBSIE(CBSBaseIE):\n         },\n         'expected_warnings': [\n             'This content expired on', 'No video formats found', 'Requested format is not available'],\n+        'skip': '404 Not Found',\n     }, {\n         'url': 'http://colbertlateshow.com/video/8GmB0oY0McANFvp2aEffk9jZZZ2YyXxy/the-colbeard/',\n         'only_matching': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ccc.py",
            "diff": "diff --git a/yt_dlp/extractor/ccc.py b/yt_dlp/extractor/ccc.py\nindex 22e3a22e..ca6b82c9 100644\n--- a/yt_dlp/extractor/ccc.py\n+++ b/yt_dlp/extractor/ccc.py\n@@ -90,10 +90,17 @@ class CCCPlaylistIE(InfoExtractor):\n             'id': '30c3',\n         },\n         'playlist_count': 135,\n+    }, {\n+        'url': 'https://media.ccc.de/c/DS2023',\n+        'info_dict': {\n+            'title': 'Datenspuren 2023',\n+            'id': 'DS2023',\n+        },\n+        'playlist_count': 37\n     }]\n \n     def _real_extract(self, url):\n-        playlist_id = self._match_id(url).lower()\n+        playlist_id = self._match_id(url)\n \n         conf = self._download_json(\n             'https://media.ccc.de/public/conferences/' + playlist_id,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/channel9.py",
            "diff": "diff --git a/yt_dlp/extractor/channel9.py b/yt_dlp/extractor/channel9.py\ndeleted file mode 100644\nindex a8847406..00000000\n--- a/yt_dlp/extractor/channel9.py\n+++ /dev/null\n@@ -1,252 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    clean_html,\n-    int_or_none,\n-    parse_iso8601,\n-    qualities,\n-    unescapeHTML,\n-)\n-\n-\n-class Channel9IE(InfoExtractor):\n-    IE_DESC = 'Channel 9'\n-    IE_NAME = 'channel9'\n-    _VALID_URL = r'https?://(?:www\\.)?(?:channel9\\.msdn\\.com|s\\.ch9\\.ms)/(?P<contentpath>.+?)(?P<rss>/RSS)?/?(?:[?#&]|$)'\n-    _EMBED_REGEX = [r'<iframe[^>]+src=[\"\\'](?P<url>https?://channel9\\.msdn\\.com/(?:[^/]+/)+)player\\b']\n-\n-    _TESTS = [{\n-        'url': 'http://channel9.msdn.com/Events/TechEd/Australia/2013/KOS002',\n-        'md5': '32083d4eaf1946db6d454313f44510ca',\n-        'info_dict': {\n-            'id': '6c413323-383a-49dc-88f9-a22800cab024',\n-            'ext': 'wmv',\n-            'title': 'Developer Kick-Off Session: Stuff We Love',\n-            'description': 'md5:b80bf9355a503c193aff7ec6cd5a7731',\n-            'duration': 4576,\n-            'thumbnail': r're:https?://.*\\.jpg',\n-            'timestamp': 1377717420,\n-            'upload_date': '20130828',\n-            'session_code': 'KOS002',\n-            'session_room': 'Arena 1A',\n-            'session_speakers': 'count:5',\n-        },\n-    }, {\n-        'url': 'http://channel9.msdn.com/posts/Self-service-BI-with-Power-BI-nuclear-testing',\n-        'md5': 'dcf983ee6acd2088e7188c3cf79b46bc',\n-        'info_dict': {\n-            'id': 'fe8e435f-bb93-4e01-8e97-a28c01887024',\n-            'ext': 'wmv',\n-            'title': 'Self-service BI with Power BI - nuclear testing',\n-            'description': 'md5:2d17fec927fc91e9e17783b3ecc88f54',\n-            'duration': 1540,\n-            'thumbnail': r're:https?://.*\\.jpg',\n-            'timestamp': 1386381991,\n-            'upload_date': '20131207',\n-            'authors': ['Mike Wilmot'],\n-        },\n-    }, {\n-        # low quality mp4 is best\n-        'url': 'https://channel9.msdn.com/Events/CPP/CppCon-2015/Ranges-for-the-Standard-Library',\n-        'info_dict': {\n-            'id': '33ad69d2-6a4e-4172-83a1-a523013dec76',\n-            'ext': 'mp4',\n-            'title': 'Ranges for the Standard Library',\n-            'description': 'md5:9895e0a9fd80822d2f01c454b8f4a372',\n-            'duration': 5646,\n-            'thumbnail': r're:https?://.*\\.jpg',\n-            'upload_date': '20150930',\n-            'timestamp': 1443640735,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://channel9.msdn.com/Events/DEVintersection/DEVintersection-2016/RSS',\n-        'info_dict': {\n-            'id': 'Events/DEVintersection/DEVintersection-2016',\n-            'title': 'DEVintersection 2016 Orlando Sessions',\n-        },\n-        'playlist_mincount': 14,\n-    }, {\n-        'url': 'https://channel9.msdn.com/Niners/Splendid22/Queue/76acff796e8f411184b008028e0d492b/RSS',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://channel9.msdn.com/Events/Speakers/scott-hanselman/RSS?UrlSafeName=scott-hanselman',\n-        'only_matching': True,\n-    }]\n-\n-    _RSS_URL = 'http://channel9.msdn.com/%s/RSS'\n-\n-    def _extract_list(self, video_id, rss_url=None):\n-        if not rss_url:\n-            rss_url = self._RSS_URL % video_id\n-        rss = self._download_xml(rss_url, video_id, 'Downloading RSS')\n-        entries = [self.url_result(session_url.text, 'Channel9')\n-                   for session_url in rss.findall('./channel/item/link')]\n-        title_text = rss.find('./channel/title').text\n-        return self.playlist_result(entries, video_id, title_text)\n-\n-    def _real_extract(self, url):\n-        content_path, rss = self._match_valid_url(url).groups()\n-\n-        if rss:\n-            return self._extract_list(content_path, url)\n-\n-        webpage = self._download_webpage(\n-            url, content_path, 'Downloading web page')\n-\n-        episode_data = self._search_regex(\n-            r\"data-episode='([^']+)'\", webpage, 'episode data', default=None)\n-        if episode_data:\n-            episode_data = self._parse_json(unescapeHTML(\n-                episode_data), content_path)\n-            content_id = episode_data['contentId']\n-            is_session = '/Sessions(' in episode_data['api']\n-            content_url = 'https://channel9.msdn.com/odata' + episode_data['api'] + '?$select=Captions,CommentCount,MediaLengthInSeconds,PublishedDate,Rating,RatingCount,Title,VideoMP4High,VideoMP4Low,VideoMP4Medium,VideoPlayerPreviewImage,VideoWMV,VideoWMVHQ,Views,'\n-            if is_session:\n-                content_url += 'Code,Description,Room,Slides,Speakers,ZipFile&$expand=Speakers'\n-            else:\n-                content_url += 'Authors,Body&$expand=Authors'\n-            content_data = self._download_json(content_url, content_id)\n-            title = content_data['Title']\n-\n-            QUALITIES = (\n-                'mp3',\n-                'wmv', 'mp4',\n-                'wmv-low', 'mp4-low',\n-                'wmv-mid', 'mp4-mid',\n-                'wmv-high', 'mp4-high',\n-            )\n-\n-            quality_key = qualities(QUALITIES)\n-\n-            def quality(quality_id, format_url):\n-                return (len(QUALITIES) if '_Source.' in format_url\n-                        else quality_key(quality_id))\n-\n-            formats = []\n-            urls = set()\n-\n-            SITE_QUALITIES = {\n-                'MP3': 'mp3',\n-                'MP4': 'mp4',\n-                'Low Quality WMV': 'wmv-low',\n-                'Low Quality MP4': 'mp4-low',\n-                'Mid Quality WMV': 'wmv-mid',\n-                'Mid Quality MP4': 'mp4-mid',\n-                'High Quality WMV': 'wmv-high',\n-                'High Quality MP4': 'mp4-high',\n-            }\n-\n-            formats_select = self._search_regex(\n-                r'(?s)<select[^>]+name=[\"\\']format[^>]+>(.+?)</select', webpage,\n-                'formats select', default=None)\n-            if formats_select:\n-                for mobj in re.finditer(\n-                        r'<option\\b[^>]+\\bvalue=([\"\\'])(?P<url>(?:(?!\\1).)+)\\1[^>]*>\\s*(?P<format>[^<]+?)\\s*<',\n-                        formats_select):\n-                    format_url = mobj.group('url')\n-                    if format_url in urls:\n-                        continue\n-                    urls.add(format_url)\n-                    format_id = mobj.group('format')\n-                    quality_id = SITE_QUALITIES.get(format_id, format_id)\n-                    formats.append({\n-                        'url': format_url,\n-                        'format_id': quality_id,\n-                        'quality': quality(quality_id, format_url),\n-                        'vcodec': 'none' if quality_id == 'mp3' else None,\n-                    })\n-\n-            API_QUALITIES = {\n-                'VideoMP4Low': 'mp4-low',\n-                'VideoWMV': 'wmv-mid',\n-                'VideoMP4Medium': 'mp4-mid',\n-                'VideoMP4High': 'mp4-high',\n-                'VideoWMVHQ': 'wmv-hq',\n-            }\n-\n-            for format_id, q in API_QUALITIES.items():\n-                q_url = content_data.get(format_id)\n-                if not q_url or q_url in urls:\n-                    continue\n-                urls.add(q_url)\n-                formats.append({\n-                    'url': q_url,\n-                    'format_id': q,\n-                    'quality': quality(q, q_url),\n-                })\n-\n-            slides = content_data.get('Slides')\n-            zip_file = content_data.get('ZipFile')\n-\n-            if not formats and not slides and not zip_file:\n-                self.raise_no_formats(\n-                    'None of recording, slides or zip are available for %s' % content_path)\n-\n-            subtitles = {}\n-            for caption in content_data.get('Captions', []):\n-                caption_url = caption.get('Url')\n-                if not caption_url:\n-                    continue\n-                subtitles.setdefault(caption.get('Language', 'en'), []).append({\n-                    'url': caption_url,\n-                    'ext': 'vtt',\n-                })\n-\n-            common = {\n-                'id': content_id,\n-                'title': title,\n-                'description': clean_html(content_data.get('Description') or content_data.get('Body')),\n-                'thumbnail': content_data.get('VideoPlayerPreviewImage'),\n-                'duration': int_or_none(content_data.get('MediaLengthInSeconds')),\n-                'timestamp': parse_iso8601(content_data.get('PublishedDate')),\n-                'avg_rating': int_or_none(content_data.get('Rating')),\n-                'rating_count': int_or_none(content_data.get('RatingCount')),\n-                'view_count': int_or_none(content_data.get('Views')),\n-                'comment_count': int_or_none(content_data.get('CommentCount')),\n-                'subtitles': subtitles,\n-            }\n-            if is_session:\n-                speakers = []\n-                for s in content_data.get('Speakers', []):\n-                    speaker_name = s.get('FullName')\n-                    if not speaker_name:\n-                        continue\n-                    speakers.append(speaker_name)\n-\n-                common.update({\n-                    'session_code': content_data.get('Code'),\n-                    'session_room': content_data.get('Room'),\n-                    'session_speakers': speakers,\n-                })\n-            else:\n-                authors = []\n-                for a in content_data.get('Authors', []):\n-                    author_name = a.get('DisplayName')\n-                    if not author_name:\n-                        continue\n-                    authors.append(author_name)\n-                common['authors'] = authors\n-\n-            contents = []\n-\n-            if slides:\n-                d = common.copy()\n-                d.update({'title': title + '-Slides', 'url': slides})\n-                contents.append(d)\n-\n-            if zip_file:\n-                d = common.copy()\n-                d.update({'title': title + '-Zip', 'url': zip_file})\n-                contents.append(d)\n-\n-            if formats:\n-                d = common.copy()\n-                d.update({'title': title, 'formats': formats})\n-                contents.append(d)\n-            return self.playlist_result(contents)\n-        else:\n-            return self._extract_list(content_path)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/chirbit.py",
            "diff": "diff --git a/yt_dlp/extractor/chirbit.py b/yt_dlp/extractor/chirbit.py\ndeleted file mode 100644\nindex 452711d9..00000000\n--- a/yt_dlp/extractor/chirbit.py\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_b64decode\n-from ..utils import parse_duration\n-\n-\n-class ChirbitIE(InfoExtractor):\n-    IE_NAME = 'chirbit'\n-    _VALID_URL = r'https?://(?:www\\.)?chirb\\.it/(?:(?:wp|pl)/|fb_chirbit_player\\.swf\\?key=)?(?P<id>[\\da-zA-Z]+)'\n-    _TESTS = [{\n-        'url': 'http://chirb.it/be2abG',\n-        'info_dict': {\n-            'id': 'be2abG',\n-            'ext': 'mp3',\n-            'title': 'md5:f542ea253f5255240be4da375c6a5d7e',\n-            'description': 'md5:f24a4e22a71763e32da5fed59e47c770',\n-            'duration': 306,\n-            'uploader': 'Gerryaudio',\n-        },\n-        'params': {\n-            'skip_download': True,\n-        }\n-    }, {\n-        'url': 'https://chirb.it/fb_chirbit_player.swf?key=PrIPv5',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://chirb.it/wp/MN58c2',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        audio_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            'http://chirb.it/%s' % audio_id, audio_id)\n-\n-        data_fd = self._search_regex(\n-            r'data-fd=([\"\\'])(?P<url>(?:(?!\\1).)+)\\1',\n-            webpage, 'data fd', group='url')\n-\n-        # Reverse engineered from https://chirb.it/js/chirbit.player.js (look\n-        # for soundURL)\n-        audio_url = compat_b64decode(data_fd[::-1]).decode('utf-8')\n-\n-        title = self._search_regex(\n-            r'class=[\"\\']chirbit-title[\"\\'][^>]*>([^<]+)', webpage, 'title')\n-        description = self._search_regex(\n-            r'<h3>Description</h3>\\s*<pre[^>]*>([^<]+)</pre>',\n-            webpage, 'description', default=None)\n-        duration = parse_duration(self._search_regex(\n-            r'class=[\"\\']c-length[\"\\'][^>]*>([^<]+)',\n-            webpage, 'duration', fatal=False))\n-        uploader = self._search_regex(\n-            r'id=[\"\\']chirbit-username[\"\\'][^>]*>([^<]+)',\n-            webpage, 'uploader', fatal=False)\n-\n-        return {\n-            'id': audio_id,\n-            'url': audio_url,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'uploader': uploader,\n-        }\n-\n-\n-class ChirbitProfileIE(InfoExtractor):\n-    IE_NAME = 'chirbit:profile'\n-    _VALID_URL = r'https?://(?:www\\.)?chirbit\\.com/(?:rss/)?(?P<id>[^/]+)'\n-    _TEST = {\n-        'url': 'http://chirbit.com/ScarletBeauty',\n-        'info_dict': {\n-            'id': 'ScarletBeauty',\n-        },\n-        'playlist_mincount': 3,\n-    }\n-\n-    def _real_extract(self, url):\n-        profile_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, profile_id)\n-\n-        entries = [\n-            self.url_result(self._proto_relative_url('//chirb.it/' + video_id))\n-            for _, video_id in re.findall(r'<input[^>]+id=([\\'\"])copy-btn-(?P<id>[0-9a-zA-Z]+)\\1', webpage)]\n-\n-        return self.playlist_result(entries, profile_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cinchcast.py",
            "diff": "diff --git a/yt_dlp/extractor/cinchcast.py b/yt_dlp/extractor/cinchcast.py\ndeleted file mode 100644\nindex 7a7ea8b2..00000000\n--- a/yt_dlp/extractor/cinchcast.py\n+++ /dev/null\n@@ -1,56 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    unified_strdate,\n-    xpath_text,\n-)\n-\n-\n-class CinchcastIE(InfoExtractor):\n-    _VALID_URL = r'https?://player\\.cinchcast\\.com/.*?(?:assetId|show_id)=(?P<id>[0-9]+)'\n-    _EMBED_REGEX = [r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://player\\.cinchcast\\.com/.+?)\\1']\n-\n-    _TESTS = [{\n-        'url': 'http://player.cinchcast.com/?show_id=5258197&platformId=1&assetType=single',\n-        'info_dict': {\n-            'id': '5258197',\n-            'ext': 'mp3',\n-            'title': 'Train Your Brain to Up Your Game with Coach Mandy',\n-            'upload_date': '20130816',\n-        },\n-    }, {\n-        # Actual test is run in generic, look for undergroundwellness\n-        'url': 'http://player.cinchcast.com/?platformId=1&#038;assetType=single&#038;assetId=7141703',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        doc = self._download_xml(\n-            'http://www.blogtalkradio.com/playerasset/mrss?assetType=single&assetId=%s' % video_id,\n-            video_id)\n-\n-        item = doc.find('.//item')\n-        title = xpath_text(item, './title', fatal=True)\n-        date_str = xpath_text(\n-            item, './{http://developer.longtailvideo.com/trac/}date')\n-        upload_date = unified_strdate(date_str, day_first=False)\n-        # duration is present but wrong\n-        formats = [{\n-            'format_id': 'main',\n-            'url': item.find('./{http://search.yahoo.com/mrss/}content').attrib['url'],\n-        }]\n-        backup_url = xpath_text(\n-            item, './{http://developer.longtailvideo.com/trac/}backupContent')\n-        if backup_url:\n-            formats.append({\n-                'preference': 2,  # seems to be more reliable\n-                'format_id': 'backup',\n-                'url': backup_url,\n-            })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'upload_date': upload_date,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cineverse.py",
            "diff": "diff --git a/yt_dlp/extractor/cineverse.py b/yt_dlp/extractor/cineverse.py\nnew file mode 100644\nindex 00000000..c9fa789b\n--- /dev/null\n+++ b/yt_dlp/extractor/cineverse.py\n@@ -0,0 +1,136 @@\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    filter_dict,\n+    int_or_none,\n+    parse_age_limit,\n+    smuggle_url,\n+    traverse_obj,\n+    unsmuggle_url,\n+    url_or_none,\n+)\n+\n+\n+class CineverseBaseIE(InfoExtractor):\n+    _VALID_URL_BASE = r'https://www\\.(?P<host>%s)' % '|'.join(map(re.escape, (\n+        'cineverse.com',\n+        'asiancrush.com',\n+        'dovechannel.com',\n+        'screambox.com',\n+        'midnightpulp.com',\n+        'fandor.com',\n+        'retrocrush.tv',\n+    )))\n+\n+\n+class CineverseIE(CineverseBaseIE):\n+    _VALID_URL = rf'{CineverseBaseIE._VALID_URL_BASE}/watch/(?P<id>[A-Z0-9]+)'\n+    _TESTS = [{\n+        'url': 'https://www.asiancrush.com/watch/DMR00018919/Women-Who-Flirt',\n+        'skip': 'geo-blocked',\n+        'info_dict': {\n+            'title': 'Women Who Flirt',\n+            'ext': 'mp4',\n+            'id': 'DMR00018919',\n+            'modified_timestamp': 1678744575289,\n+            'cast': ['Xun Zhou', 'Xiaoming Huang', 'Yi-Lin Sie', 'Sonia Sui', 'Quniciren'],\n+            'duration': 5811.597,\n+            'description': 'md5:892fd62a05611d394141e8394ace0bc6',\n+            'age_limit': 13,\n+        }\n+    }, {\n+        'url': 'https://www.retrocrush.tv/watch/1000000023016/Archenemy! Crystal Bowie',\n+        'skip': 'geo-blocked',\n+        'info_dict': {\n+            'title': 'Archenemy! Crystal Bowie',\n+            'ext': 'mp4',\n+            'id': '1000000023016',\n+            'episode_number': 3,\n+            'season_number': 1,\n+            'cast': ['Nachi Nozawa', 'Yoshiko Sakakibara', 'Toshiko Fujita'],\n+            'age_limit': 0,\n+            'episode': 'Episode 3',\n+            'season': 'Season 1',\n+            'duration': 1485.067,\n+            'description': 'Cobra meets a beautiful bounty hunter by the name of Jane Royal.',\n+            'series': 'Space Adventure COBRA (Original Japanese)',\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        url, smuggled_data = unsmuggle_url(url, default={})\n+        self._initialize_geo_bypass({\n+            'countries': smuggled_data.get('geo_countries'),\n+        })\n+        video_id = self._match_id(url)\n+        html = self._download_webpage(url, video_id)\n+        idetails = self._search_nextjs_data(html, video_id)['props']['pageProps']['idetails']\n+\n+        if idetails.get('err_code') == 1200:\n+            self.raise_geo_restricted(\n+                'This video is not available from your location due to geo restriction. '\n+                'You may be able to bypass it by using the /details/ page instead of the /watch/ page',\n+                countries=smuggled_data.get('geo_countries'))\n+\n+        return {\n+            'subtitles': filter_dict({\n+                'en': traverse_obj(idetails, (('cc_url_vtt', 'subtitle_url'), {'url': {url_or_none}})) or None,\n+            }),\n+            'formats': self._extract_m3u8_formats(idetails['url'], video_id),\n+            **traverse_obj(idetails, {\n+                'title': 'title',\n+                'id': ('details', 'item_id'),\n+                'description': ('details', 'description'),\n+                'duration': ('duration', {lambda x: x / 1000}),\n+                'cast': ('details', 'cast', {lambda x: x.split(', ')}),\n+                'modified_timestamp': ('details', 'updated_by', 0, 'update_time', 'time', {int_or_none}),\n+                'season_number': ('details', 'season', {int_or_none}),\n+                'episode_number': ('details', 'episode', {int_or_none}),\n+                'age_limit': ('details', 'rating_code', {parse_age_limit}),\n+                'series': ('details', 'series_details', 'title'),\n+            }),\n+        }\n+\n+\n+class CineverseDetailsIE(CineverseBaseIE):\n+    _VALID_URL = rf'{CineverseBaseIE._VALID_URL_BASE}/details/(?P<id>[A-Z0-9]+)'\n+    _TESTS = [{\n+        'url': 'https://www.retrocrush.tv/details/1000000023012/Space-Adventure-COBRA-(Original-Japanese)',\n+        'playlist_mincount': 30,\n+        'info_dict': {\n+            'title': 'Space Adventure COBRA (Original Japanese)',\n+            'id': '1000000023012',\n+        }\n+    }, {\n+        'url': 'https://www.asiancrush.com/details/NNVG4938/Hansel-and-Gretel',\n+        'info_dict': {\n+            'id': 'NNVG4938',\n+            'ext': 'mp4',\n+            'title': 'Hansel and Gretel',\n+            'description': 'md5:e3e4c35309c2e82aee044f972c2fb05d',\n+            'cast': ['Jeong-myeong Cheon', 'Eun Won-jae', 'Shim Eun-gyeong', 'Ji-hee Jin', 'Hee-soon Park', 'Lydia Park', 'Kyeong-ik Kim'],\n+            'duration': 7030.732,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        host, series_id = self._match_valid_url(url).group('host', 'id')\n+        html = self._download_webpage(url, series_id)\n+        pageprops = self._search_nextjs_data(html, series_id)['props']['pageProps']\n+\n+        geo_countries = traverse_obj(pageprops, ('itemDetailsData', 'geo_country', {lambda x: x.split(', ')}))\n+        geoblocked = traverse_obj(pageprops, (\n+            'itemDetailsData', 'playback_err_msg')) == 'This title is not available in your location.'\n+\n+        def item_result(item):\n+            item_url = f'https://www.{host}/watch/{item[\"item_id\"]}/{item[\"title\"]}'\n+            if geoblocked:\n+                item_url = smuggle_url(item_url, {'geo_countries': geo_countries})\n+            return self.url_result(item_url, CineverseIE)\n+\n+        season = traverse_obj(pageprops, ('seasonEpisodes', ..., 'episodes', lambda _, v: v['item_id'] and v['title']))\n+        if season:\n+            return self.playlist_result([item_result(ep) for ep in season], playlist_id=series_id,\n+                                        playlist_title=traverse_obj(pageprops, ('itemDetailsData', 'title')))\n+        return item_result(pageprops['itemDetailsData'])\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/clipsyndicate.py",
            "diff": "diff --git a/yt_dlp/extractor/clipsyndicate.py b/yt_dlp/extractor/clipsyndicate.py\ndeleted file mode 100644\nindex 60644432..00000000\n--- a/yt_dlp/extractor/clipsyndicate.py\n+++ /dev/null\n@@ -1,52 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    find_xpath_attr,\n-    fix_xml_ampersands\n-)\n-\n-\n-class ClipsyndicateIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:chic|www)\\.clipsyndicate\\.com/video/play(list/\\d+)?/(?P<id>\\d+)'\n-\n-    _TESTS = [{\n-        'url': 'http://www.clipsyndicate.com/video/play/4629301/brick_briscoe',\n-        'md5': '4d7d549451bad625e0ff3d7bd56d776c',\n-        'info_dict': {\n-            'id': '4629301',\n-            'ext': 'mp4',\n-            'title': 'Brick Briscoe',\n-            'duration': 612,\n-            'thumbnail': r're:^https?://.+\\.jpg',\n-        },\n-    }, {\n-        'url': 'http://chic.clipsyndicate.com/video/play/5844117/shark_attack',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        js_player = self._download_webpage(\n-            'http://eplayer.clipsyndicate.com/embed/player.js?va_id=%s' % video_id,\n-            video_id, 'Downlaoding player')\n-        # it includes a required token\n-        flvars = self._search_regex(r'flvars: \"(.*?)\"', js_player, 'flvars')\n-\n-        pdoc = self._download_xml(\n-            'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n-            video_id, 'Downloading video info',\n-            transform_source=fix_xml_ampersands)\n-\n-        track_doc = pdoc.find('trackList/track')\n-\n-        def find_param(name):\n-            node = find_xpath_attr(track_doc, './/param', 'name', name)\n-            if node is not None:\n-                return node.attrib['value']\n-\n-        return {\n-            'id': video_id,\n-            'title': find_param('title'),\n-            'url': track_doc.find('location').text,\n-            'thumbnail': find_param('thumbnail'),\n-            'duration': int(find_param('duration')),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cloudy.py",
            "diff": "diff --git a/yt_dlp/extractor/cloudy.py b/yt_dlp/extractor/cloudy.py\ndeleted file mode 100644\nindex 848643e2..00000000\n--- a/yt_dlp/extractor/cloudy.py\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    str_to_int,\n-    unified_strdate,\n-)\n-\n-\n-class CloudyIE(InfoExtractor):\n-    _IE_DESC = 'cloudy.ec'\n-    _VALID_URL = r'https?://(?:www\\.)?cloudy\\.ec/(?:v/|embed\\.php\\?.*?\\bid=)(?P<id>[A-Za-z0-9]+)'\n-    _TESTS = [{\n-        'url': 'https://www.cloudy.ec/v/af511e2527aac',\n-        'md5': '29832b05028ead1b58be86bf319397ca',\n-        'info_dict': {\n-            'id': 'af511e2527aac',\n-            'ext': 'mp4',\n-            'title': 'Funny Cats and Animals Compilation june 2013',\n-            'upload_date': '20130913',\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'http://www.cloudy.ec/embed.php?autoplay=1&id=af511e2527aac',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            'https://www.cloudy.ec/embed.php', video_id, query={\n-                'id': video_id,\n-                'playerPage': 1,\n-                'autoplay': 1,\n-            })\n-\n-        info = self._parse_html5_media_entries(url, webpage, video_id)[0]\n-\n-        webpage = self._download_webpage(\n-            'https://www.cloudy.ec/v/%s' % video_id, video_id, fatal=False)\n-\n-        if webpage:\n-            info.update({\n-                'title': self._search_regex(\n-                    r'<h\\d[^>]*>([^<]+)<', webpage, 'title'),\n-                'upload_date': unified_strdate(self._search_regex(\n-                    r'>Published at (\\d{4}-\\d{1,2}-\\d{1,2})', webpage,\n-                    'upload date', fatal=False)),\n-                'view_count': str_to_int(self._search_regex(\n-                    r'([\\d,.]+) views<', webpage, 'view count', fatal=False)),\n-            })\n-\n-        if not info.get('title'):\n-            info['title'] = video_id\n-\n-        info['id'] = video_id\n-\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/clubic.py",
            "diff": "diff --git a/yt_dlp/extractor/clubic.py b/yt_dlp/extractor/clubic.py\nindex 403e44aa..716f2596 100644\n--- a/yt_dlp/extractor/clubic.py\n+++ b/yt_dlp/extractor/clubic.py\n@@ -6,6 +6,7 @@\n \n \n class ClubicIE(InfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?clubic\\.com/video/(?:[^/]+/)*video.*-(?P<id>[0-9]+)\\.html'\n \n     _TESTS = [{\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cmt.py",
            "diff": "diff --git a/yt_dlp/extractor/cmt.py b/yt_dlp/extractor/cmt.py\nindex 8aed7708..6359102a 100644\n--- a/yt_dlp/extractor/cmt.py\n+++ b/yt_dlp/extractor/cmt.py\n@@ -4,6 +4,7 @@\n \n \n class CMTIE(MTVIE):  # XXX: Do not subclass from concrete IE\n+    _WORKING = False\n     IE_NAME = 'cmt.com'\n     _VALID_URL = r'https?://(?:www\\.)?cmt\\.com/(?:videos|shows|(?:full-)?episodes|video-clips)/(?P<id>[^/]+)'\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cnbc.py",
            "diff": "diff --git a/yt_dlp/extractor/cnbc.py b/yt_dlp/extractor/cnbc.py\nindex 68fd025b..7d209b6d 100644\n--- a/yt_dlp/extractor/cnbc.py\n+++ b/yt_dlp/extractor/cnbc.py\n@@ -19,6 +19,7 @@ class CNBCIE(InfoExtractor):\n             # m3u8 download\n             'skip_download': True,\n         },\n+        'skip': 'Dead link',\n     }\n \n     def _real_extract(self, url):\n@@ -49,6 +50,7 @@ class CNBCVideoIE(InfoExtractor):\n         'params': {\n             'skip_download': True,\n         },\n+        'skip': 'Dead link',\n     }\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/common.py",
            "diff": "diff --git a/yt_dlp/extractor/common.py b/yt_dlp/extractor/common.py\nindex 7deab995..af534775 100644\n--- a/yt_dlp/extractor/common.py\n+++ b/yt_dlp/extractor/common.py\n@@ -286,6 +286,9 @@ class InfoExtractor:\n                     If it is not clear whether to use timestamp or this, use the former\n     release_date:   The date (YYYYMMDD) when the video was released in UTC.\n                     If not explicitly set, calculated from release_timestamp\n+    release_year:   Year (YYYY) as integer when the video or album was released.\n+                    To be used if no exact release date is known.\n+                    If not explicitly set, calculated from release_date.\n     modified_timestamp: UNIX timestamp of the moment the video was last modified.\n     modified_date:   The date (YYYYMMDD) when the video was last modified in UTC.\n                     If not explicitly set, calculated from modified_timestamp\n@@ -379,6 +382,7 @@ class InfoExtractor:\n                     'private', 'premium_only', 'subscriber_only', 'needs_auth',\n                     'unlisted' or 'public'. Use 'InfoExtractor._availability'\n                     to set it\n+    media_type:     The type of media as classified by the site, e.g. \"episode\", \"clip\", \"trailer\"\n     _old_archive_ids: A list of old archive ids needed for backward compatibility\n     _format_sort_fields: A list of fields to use for sorting formats\n     __post_extractor: A function to be called just before the metadata is\n@@ -427,7 +431,6 @@ class InfoExtractor:\n                     and compilations).\n     disc_number:    Number of the disc or other physical medium the track belongs to,\n                     as an integer.\n-    release_year:   Year (YYYY) when the album was released.\n     composer:       Composer of the piece\n \n     The following fields should only be set for clips that should be cut from the original video:\n@@ -1687,7 +1690,7 @@ def _search_nextjs_data(self, webpage, video_id, *, transform_source=None, fatal\n     def _search_nuxt_data(self, webpage, video_id, context_name='__NUXT__', *, fatal=True, traverse=('data', 0)):\n         \"\"\"Parses Nuxt.js metadata. This works as long as the function __NUXT__ invokes is a pure function\"\"\"\n         rectx = re.escape(context_name)\n-        FUNCTION_RE = r'\\(function\\((?P<arg_keys>.*?)\\){return\\s+(?P<js>{.*?})\\s*;?\\s*}\\((?P<arg_vals>.*?)\\)'\n+        FUNCTION_RE = r'\\(function\\((?P<arg_keys>.*?)\\){.*?\\breturn\\s+(?P<js>{.*?})\\s*;?\\s*}\\((?P<arg_vals>.*?)\\)'\n         js, arg_keys, arg_vals = self._search_regex(\n             (rf'<script>\\s*window\\.{rectx}={FUNCTION_RE}\\s*\\)\\s*;?\\s*</script>', rf'{rectx}\\(.*?{FUNCTION_RE}'),\n             webpage, context_name, group=('js', 'arg_keys', 'arg_vals'),\n@@ -2225,7 +2228,9 @@ def _extract_mpd_vod_duration(\n             mpd_url, video_id,\n             note='Downloading MPD VOD manifest' if note is None else note,\n             errnote='Failed to download VOD manifest' if errnote is None else errnote,\n-            fatal=False, data=data, headers=headers, query=query) or {}\n+            fatal=False, data=data, headers=headers, query=query)\n+        if not isinstance(mpd_doc, xml.etree.ElementTree.Element):\n+            return None\n         return int_or_none(parse_duration(mpd_doc.get('mediaPresentationDuration')))\n \n     @staticmethod\n@@ -2339,7 +2344,9 @@ def _parse_smil_formats_and_subtitles(\n         imgs_count = 0\n \n         srcs = set()\n-        media = smil.findall(self._xpath_ns('.//video', namespace)) + smil.findall(self._xpath_ns('.//audio', namespace))\n+        media = itertools.chain.from_iterable(\n+            smil.findall(self._xpath_ns(arg, namespace))\n+            for arg in ['.//video', './/audio', './/media'])\n         for medium in media:\n             src = medium.get('src')\n             if not src or src in srcs:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/corus.py",
            "diff": "diff --git a/yt_dlp/extractor/corus.py b/yt_dlp/extractor/corus.py\nindex c03d6531..bcc34ddd 100644\n--- a/yt_dlp/extractor/corus.py\n+++ b/yt_dlp/extractor/corus.py\n@@ -41,7 +41,7 @@ class CorusIE(ThePlatformFeedIE):  # XXX: Do not subclass from concrete IE\n                         )\n                     '''\n     _TESTS = [{\n-        'url': 'http://www.hgtv.ca/shows/bryan-inc/videos/movie-night-popcorn-with-bryan-870923331648/',\n+        'url': 'https://www.hgtv.ca/video/bryan-inc/movie-night-popcorn-with-bryan/870923331648/',\n         'info_dict': {\n             'id': '870923331648',\n             'ext': 'mp4',\n@@ -54,6 +54,7 @@ class CorusIE(ThePlatformFeedIE):  # XXX: Do not subclass from concrete IE\n             'skip_download': True,\n         },\n         'expected_warnings': ['Failed to parse JSON'],\n+        # FIXME: yt-dlp wrongly raises for geo restriction\n     }, {\n         'url': 'http://www.foodnetwork.ca/shows/chopped/video/episode/chocolate-obsession/video.html?v=872683587753',\n         'only_matching': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/craftsy.py",
            "diff": "diff --git a/yt_dlp/extractor/craftsy.py b/yt_dlp/extractor/craftsy.py\nindex 307bfb94..5d373314 100644\n--- a/yt_dlp/extractor/craftsy.py\n+++ b/yt_dlp/extractor/craftsy.py\n@@ -10,7 +10,7 @@\n \n \n class CraftsyIE(InfoExtractor):\n-    _VALID_URL = r'https?://www.craftsy.com/class/(?P<id>[a-z0-9_-]+)/'\n+    _VALID_URL = r'https?://www\\.craftsy\\.com/class/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://www.craftsy.com/class/the-midnight-quilt-show-season-5/',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cwtv.py",
            "diff": "diff --git a/yt_dlp/extractor/cwtv.py b/yt_dlp/extractor/cwtv.py\nindex 9b83264e..69d50daf 100644\n--- a/yt_dlp/extractor/cwtv.py\n+++ b/yt_dlp/extractor/cwtv.py\n@@ -46,6 +46,10 @@ class CWTVIE(InfoExtractor):\n             'timestamp': 1444107300,\n             'age_limit': 14,\n             'uploader': 'CWTV',\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'chapters': 'count:4',\n+            'episode': 'Episode 20',\n+            'season': 'Season 11',\n         },\n         'params': {\n             # m3u8 download\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/cybrary.py",
            "diff": "diff --git a/yt_dlp/extractor/cybrary.py b/yt_dlp/extractor/cybrary.py\nindex 73f2439b..c4c78ee1 100644\n--- a/yt_dlp/extractor/cybrary.py\n+++ b/yt_dlp/extractor/cybrary.py\n@@ -45,7 +45,7 @@ def _get_vimeo_id(self, activity_id):\n \n \n class CybraryIE(CybraryBaseIE):\n-    _VALID_URL = r'https?://app.cybrary.it/immersive/(?P<enrollment>[0-9]+)/activity/(?P<id>[0-9]+)'\n+    _VALID_URL = r'https?://app\\.cybrary\\.it/immersive/(?P<enrollment>[0-9]+)/activity/(?P<id>[0-9]+)'\n     _TESTS = [{\n         'url': 'https://app.cybrary.it/immersive/12487950/activity/63102',\n         'md5': '9ae12d37e555cb2ed554223a71a701d0',\n@@ -105,12 +105,12 @@ def _real_extract(self, url):\n             'chapter': module.get('title'),\n             'chapter_id': str_or_none(module.get('id')),\n             'title': activity.get('title'),\n-            'url': smuggle_url(f'https://player.vimeo.com/video/{vimeo_id}', {'http_headers': {'Referer': 'https://api.cybrary.it'}})\n+            'url': smuggle_url(f'https://player.vimeo.com/video/{vimeo_id}', {'referer': 'https://api.cybrary.it'})\n         }\n \n \n class CybraryCourseIE(CybraryBaseIE):\n-    _VALID_URL = r'https://app.cybrary.it/browse/course/(?P<id>[\\w-]+)/?(?:$|[#?])'\n+    _VALID_URL = r'https://app\\.cybrary\\.it/browse/course/(?P<id>[\\w-]+)/?(?:$|[#?])'\n     _TESTS = [{\n         'url': 'https://app.cybrary.it/browse/course/az-500-microsoft-azure-security-technologies',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/daftsex.py",
            "diff": "diff --git a/yt_dlp/extractor/daftsex.py b/yt_dlp/extractor/daftsex.py\ndeleted file mode 100644\nindex 92510c76..00000000\n--- a/yt_dlp/extractor/daftsex.py\n+++ /dev/null\n@@ -1,150 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_b64decode\n-from ..utils import (\n-    ExtractorError,\n-    int_or_none,\n-    js_to_json,\n-    parse_count,\n-    parse_duration,\n-    traverse_obj,\n-    try_get,\n-    unified_timestamp,\n-)\n-\n-\n-class DaftsexIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?daft\\.sex/watch/(?P<id>-?\\d+_\\d+)'\n-    _TESTS = [{\n-        'url': 'https://daft.sex/watch/-35370899_456246186',\n-        'md5': '64c04ef7b4c7b04b308f3b0c78efe7cd',\n-        'info_dict': {\n-            'id': '-35370899_456246186',\n-            'ext': 'mp4',\n-            'title': 'just relaxing',\n-            'description': 'just relaxing \u2013 Watch video Watch video in high quality',\n-            'upload_date': '20201113',\n-            'timestamp': 1605261911,\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'age_limit': 18,\n-            'duration': 15.0,\n-            'view_count': int\n-        },\n-    }, {\n-        'url': 'https://daft.sex/watch/-156601359_456242791',\n-        'info_dict': {\n-            'id': '-156601359_456242791',\n-            'ext': 'mp4',\n-            'title': 'Skye Blue - Dinner And A Show',\n-            'description': 'Skye Blue - Dinner And A Show - Watch video Watch video in high quality',\n-            'upload_date': '20200916',\n-            'timestamp': 1600250735,\n-            'thumbnail': 'https://psv153-1.crazycloud.ru/videos/-156601359/456242791/thumb.jpg?extra=i3D32KaBbBFf9TqDRMAVmQ',\n-        },\n-        'skip': 'deleted / private'\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        title = self._html_search_meta('name', webpage, 'title')\n-        timestamp = unified_timestamp(self._html_search_meta('uploadDate', webpage, 'Upload Date', default=None))\n-        description = self._html_search_meta('description', webpage, 'Description', default=None)\n-\n-        duration = parse_duration(self._search_regex(\n-            r'Duration: ((?:[0-9]{2}:){0,2}[0-9]{2})',\n-            webpage, 'duration', fatal=False))\n-        views = parse_count(self._search_regex(\n-            r'Views: ([0-9 ]+)',\n-            webpage, 'views', fatal=False))\n-\n-        player_hash = self._search_regex(\n-            r'DaxabPlayer\\.Init\\({[\\s\\S]*hash:\\s*\"([0-9a-zA-Z_\\-]+)\"[\\s\\S]*}',\n-            webpage, 'player hash')\n-        player_color = self._search_regex(\n-            r'DaxabPlayer\\.Init\\({[\\s\\S]*color:\\s*\"([0-9a-z]+)\"[\\s\\S]*}',\n-            webpage, 'player color', fatal=False) or ''\n-\n-        embed_page = self._download_webpage(\n-            'https://dxb.to/player/%s?color=%s' % (player_hash, player_color),\n-            video_id, headers={'Referer': url})\n-        video_params = self._parse_json(\n-            self._search_regex(\n-                r'window\\.globParams\\s*=\\s*({[\\S\\s]+})\\s*;\\s*<\\/script>',\n-                embed_page, 'video parameters'),\n-            video_id, transform_source=js_to_json)\n-\n-        server_domain = 'https://%s' % compat_b64decode(video_params['server'][::-1]).decode('utf-8')\n-\n-        cdn_files = traverse_obj(video_params, ('video', 'cdn_files')) or {}\n-        if cdn_files:\n-            formats = []\n-            for format_id, format_data in cdn_files.items():\n-                ext, height = format_id.split('_')\n-                formats.append({\n-                    'format_id': format_id,\n-                    'url': f'{server_domain}/videos/{video_id.replace(\"_\", \"/\")}/{height}.mp4?extra={format_data.split(\".\")[-1]}',\n-                    'height': int_or_none(height),\n-                    'ext': ext,\n-                })\n-\n-            return {\n-                'id': video_id,\n-                'title': title,\n-                'formats': formats,\n-                'description': description,\n-                'duration': duration,\n-                'thumbnail': try_get(video_params, lambda vi: 'https:' + compat_b64decode(vi['video']['thumb']).decode('utf-8')),\n-                'timestamp': timestamp,\n-                'view_count': views,\n-                'age_limit': 18,\n-            }\n-\n-        items = self._download_json(\n-            f'{server_domain}/method/video.get/{video_id}', video_id,\n-            headers={'Referer': url}, query={\n-                'token': video_params['video']['access_token'],\n-                'videos': video_id,\n-                'ckey': video_params['c_key'],\n-                'credentials': video_params['video']['credentials'],\n-            })['response']['items']\n-\n-        if not items:\n-            raise ExtractorError('Video is not available', video_id=video_id, expected=True)\n-\n-        item = items[0]\n-        formats = []\n-        for f_id, f_url in item.get('files', {}).items():\n-            if f_id == 'external':\n-                return self.url_result(f_url)\n-            ext, height = f_id.split('_')\n-            height_extra_key = traverse_obj(video_params, ('video', 'partial', 'quality', height))\n-            if height_extra_key:\n-                formats.append({\n-                    'format_id': f'{height}p',\n-                    'url': f'{server_domain}/{f_url[8:]}&videos={video_id}&extra_key={height_extra_key}',\n-                    'height': int_or_none(height),\n-                    'ext': ext,\n-                })\n-\n-        thumbnails = []\n-        for k, v in item.items():\n-            if k.startswith('photo_') and v:\n-                width = k.replace('photo_', '')\n-                thumbnails.append({\n-                    'id': width,\n-                    'url': v,\n-                    'width': int_or_none(width),\n-                })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'comment_count': int_or_none(item.get('comments')),\n-            'description': description,\n-            'duration': duration,\n-            'thumbnails': thumbnails,\n-            'timestamp': timestamp,\n-            'view_count': views,\n-            'age_limit': 18,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/dailymotion.py",
            "diff": "diff --git a/yt_dlp/extractor/dailymotion.py b/yt_dlp/extractor/dailymotion.py\nindex 21263d41..708d6fed 100644\n--- a/yt_dlp/extractor/dailymotion.py\n+++ b/yt_dlp/extractor/dailymotion.py\n@@ -93,7 +93,7 @@ class DailymotionIE(DailymotionBaseInfoExtractor):\n     _VALID_URL = r'''(?ix)\n                     https?://\n                         (?:\n-                            (?:(?:www|touch|geo)\\.)?dailymotion\\.[a-z]{2,3}/(?:(?:(?:(?:embed|swf|\\#)/)|player\\.html\\?)?video|swf)|\n+                            (?:(?:www|touch|geo)\\.)?dailymotion\\.[a-z]{2,3}/(?:(?:(?:(?:embed|swf|\\#)/)|player(?:/\\w+)?\\.html\\?)?video|swf)|\n                             (?:www\\.)?lequipe\\.fr/video\n                         )\n                         [/=](?P<id>[^/?_&]+)(?:.+?\\bplaylist=(?P<playlist_id>x[0-9a-z]+))?\n@@ -107,13 +107,17 @@ class DailymotionIE(DailymotionBaseInfoExtractor):\n             'id': 'x5kesuj',\n             'ext': 'mp4',\n             'title': 'Office Christmas Party Review \u2013  Jason Bateman, Olivia Munn, T.J. Miller',\n-            'description': 'Office Christmas Party Review -  Jason Bateman, Olivia Munn, T.J. Miller',\n+            'description': 'Office Christmas Party Review - Jason Bateman, Olivia Munn, T.J. Miller',\n             'duration': 187,\n             'timestamp': 1493651285,\n             'upload_date': '20170501',\n             'uploader': 'Deadline',\n             'uploader_id': 'x1xm8ri',\n             'age_limit': 0,\n+            'view_count': int,\n+            'like_count': int,\n+            'tags': ['hollywood', 'celeb', 'celebrity', 'movies', 'red carpet'],\n+            'thumbnail': r're:https://(?:s[12]\\.)dmcdn\\.net/v/K456B1aXqIx58LKWQ/x1080',\n         },\n     }, {\n         'url': 'https://geo.dailymotion.com/player.html?video=x89eyek&mute=true',\n@@ -132,7 +136,7 @@ class DailymotionIE(DailymotionBaseInfoExtractor):\n             'view_count': int,\n             'like_count': int,\n             'tags': ['en_quete_d_esprit'],\n-            'thumbnail': 'https://s2.dmcdn.net/v/Tncwi1YGKdvFbDuDY/x1080',\n+            'thumbnail': r're:https://(?:s[12]\\.)dmcdn\\.net/v/Tncwi1YNg_RUl7ueu/x1080',\n         }\n     }, {\n         'url': 'https://www.dailymotion.com/video/x2iuewm_steam-machine-models-pricing-listed-on-steam-store-ign-news_videogames',\n@@ -201,6 +205,12 @@ class DailymotionIE(DailymotionBaseInfoExtractor):\n     }, {\n         'url': 'https://www.dailymotion.com/video/x3z49k?playlist=xv4bw',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://geo.dailymotion.com/player/x86gw.html?video=k46oCapRs4iikoz9DWy',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://geo.dailymotion.com/player/xakln.html?video=x8mjju4&customConfig%5BcustomParams%5D=%2Ffr-fr%2Ftennis%2Fwimbledon-mens-singles%2Farticles-video',\n+        'only_matching': True,\n     }]\n     _GEO_BYPASS = False\n     _COMMON_MEDIA_FIELDS = '''description\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/defense.py",
            "diff": "diff --git a/yt_dlp/extractor/defense.py b/yt_dlp/extractor/defense.py\ndeleted file mode 100644\nindex 7d73ea86..00000000\n--- a/yt_dlp/extractor/defense.py\n+++ /dev/null\n@@ -1,37 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class DefenseGouvFrIE(InfoExtractor):\n-    IE_NAME = 'defense.gouv.fr'\n-    _VALID_URL = r'https?://.*?\\.defense\\.gouv\\.fr/layout/set/ligthboxvideo/base-de-medias/webtv/(?P<id>[^/?#]*)'\n-\n-    _TEST = {\n-        'url': 'http://www.defense.gouv.fr/layout/set/ligthboxvideo/base-de-medias/webtv/attaque-chimique-syrienne-du-21-aout-2013-1',\n-        'md5': '75bba6124da7e63d2d60b5244ec9430c',\n-        'info_dict': {\n-            'id': '11213',\n-            'ext': 'mp4',\n-            'title': 'attaque-chimique-syrienne-du-21-aout-2013-1'\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        title = self._match_id(url)\n-        webpage = self._download_webpage(url, title)\n-\n-        video_id = self._search_regex(\n-            r\"flashvars.pvg_id=\\\"(\\d+)\\\";\",\n-            webpage, 'ID')\n-\n-        json_url = (\n-            'http://static.videos.gouv.fr/brightcovehub/export/json/%s' %\n-            video_id)\n-        info = self._download_json(json_url, title, 'Downloading JSON config')\n-        video_url = info['renditions'][0]['url']\n-\n-        return {\n-            'id': video_id,\n-            'ext': 'mp4',\n-            'url': video_url,\n-            'title': title,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/dhm.py",
            "diff": "diff --git a/yt_dlp/extractor/dhm.py b/yt_dlp/extractor/dhm.py\nindex 3d42fc2b..a5f5f794 100644\n--- a/yt_dlp/extractor/dhm.py\n+++ b/yt_dlp/extractor/dhm.py\n@@ -3,6 +3,7 @@\n \n \n class DHMIE(InfoExtractor):\n+    _WORKING = False\n     IE_DESC = 'Filmarchiv - Deutsches Historisches Museum'\n     _VALID_URL = r'https?://(?:www\\.)?dhm\\.de/filmarchiv/(?:[^/]+/)+(?P<id>[^/]+)'\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/dotsub.py",
            "diff": "diff --git a/yt_dlp/extractor/dotsub.py b/yt_dlp/extractor/dotsub.py\ndeleted file mode 100644\nindex 079f8375..00000000\n--- a/yt_dlp/extractor/dotsub.py\n+++ /dev/null\n@@ -1,81 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    float_or_none,\n-    int_or_none,\n-)\n-\n-\n-class DotsubIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?dotsub\\.com/view/(?P<id>[^/]+)'\n-    _TESTS = [{\n-        'url': 'https://dotsub.com/view/9c63db2a-fa95-4838-8e6e-13deafe47f09',\n-        'md5': '21c7ff600f545358134fea762a6d42b6',\n-        'info_dict': {\n-            'id': '9c63db2a-fa95-4838-8e6e-13deafe47f09',\n-            'ext': 'flv',\n-            'title': 'MOTIVATION - \"It\\'s Possible\" Best Inspirational Video Ever',\n-            'description': 'md5:41af1e273edbbdfe4e216a78b9d34ac6',\n-            'thumbnail': 're:^https?://dotsub.com/media/9c63db2a-fa95-4838-8e6e-13deafe47f09/p',\n-            'duration': 198,\n-            'uploader': 'liuxt',\n-            'timestamp': 1385778501.104,\n-            'upload_date': '20131130',\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'https://dotsub.com/view/747bcf58-bd59-45b7-8c8c-ac312d084ee6',\n-        'md5': '2bb4a83896434d5c26be868c609429a3',\n-        'info_dict': {\n-            'id': '168006778',\n-            'ext': 'mp4',\n-            'title': 'Apartments and flats in Raipur the white symphony',\n-            'description': 'md5:784d0639e6b7d1bc29530878508e38fe',\n-            'thumbnail': 're:^https?://dotsub.com/media/747bcf58-bd59-45b7-8c8c-ac312d084ee6/p',\n-            'duration': 290,\n-            'timestamp': 1476767794.2809999,\n-            'upload_date': '20161018',\n-            'uploader': 'parthivi001',\n-            'uploader_id': 'user52596202',\n-            'view_count': int,\n-        },\n-        'add_ie': ['Vimeo'],\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        info = self._download_json(\n-            'https://dotsub.com/api/media/%s/metadata' % video_id, video_id)\n-        video_url = info.get('mediaURI')\n-\n-        if not video_url:\n-            webpage = self._download_webpage(url, video_id)\n-            video_url = self._search_regex(\n-                [r'<source[^>]+src=\"([^\"]+)\"', r'\"file\"\\s*:\\s*\\'([^\\']+)'],\n-                webpage, 'video url', default=None)\n-            info_dict = {\n-                'id': video_id,\n-                'url': video_url,\n-                'ext': 'flv',\n-            }\n-\n-        if not video_url:\n-            setup_data = self._parse_json(self._html_search_regex(\n-                r'(?s)data-setup=([\\'\"])(?P<content>(?!\\1).+?)\\1',\n-                webpage, 'setup data', group='content'), video_id)\n-            info_dict = {\n-                '_type': 'url_transparent',\n-                'url': setup_data['src'],\n-            }\n-\n-        info_dict.update({\n-            'title': info['title'],\n-            'description': info.get('description'),\n-            'thumbnail': info.get('screenshotURI'),\n-            'duration': int_or_none(info.get('duration'), 1000),\n-            'uploader': info.get('user'),\n-            'timestamp': float_or_none(info.get('dateCreated'), 1000),\n-            'view_count': int_or_none(info.get('numberOfViews')),\n-        })\n-\n-        return info_dict\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/douyutv.py",
            "diff": "diff --git a/yt_dlp/extractor/douyutv.py b/yt_dlp/extractor/douyutv.py\nindex fa40844d..ee8893d5 100644\n--- a/yt_dlp/extractor/douyutv.py\n+++ b/yt_dlp/extractor/douyutv.py\n@@ -1,31 +1,72 @@\n import time\n import hashlib\n-import re\n import urllib\n+import uuid\n \n from .common import InfoExtractor\n+from .openload import PhantomJSwrapper\n from ..utils import (\n     ExtractorError,\n+    UserNotLive,\n+    determine_ext,\n+    int_or_none,\n+    js_to_json,\n+    parse_resolution,\n+    str_or_none,\n+    traverse_obj,\n     unescapeHTML,\n-    unified_strdate,\n+    url_or_none,\n+    urlencode_postdata,\n     urljoin,\n )\n \n \n-class DouyuTVIE(InfoExtractor):\n-    IE_DESC = '\u6597\u9c7c'\n+class DouyuBaseIE(InfoExtractor):\n+    def _download_cryptojs_md5(self, video_id):\n+        for url in [\n+            'https://cdnjs.cloudflare.com/ajax/libs/crypto-js/3.1.2/rollups/md5.js',\n+            'https://cdn.bootcdn.net/ajax/libs/crypto-js/3.1.2/rollups/md5.js',\n+        ]:\n+            js_code = self._download_webpage(\n+                url, video_id, note='Downloading signing dependency', fatal=False)\n+            if js_code:\n+                self.cache.store('douyu', 'crypto-js-md5', js_code)\n+                return js_code\n+        raise ExtractorError('Unable to download JS dependency (crypto-js/md5)')\n+\n+    def _get_cryptojs_md5(self, video_id):\n+        return self.cache.load('douyu', 'crypto-js-md5') or self._download_cryptojs_md5(video_id)\n+\n+    def _calc_sign(self, sign_func, video_id, a):\n+        b = uuid.uuid4().hex\n+        c = round(time.time())\n+        js_script = f'{self._get_cryptojs_md5(video_id)};{sign_func};console.log(ub98484234(\"{a}\",\"{b}\",\"{c}\"))'\n+        phantom = PhantomJSwrapper(self)\n+        result = phantom.execute(js_script, video_id,\n+                                 note='Executing JS signing script').strip()\n+        return {i: v[0] for i, v in urllib.parse.parse_qs(result).items()}\n+\n+    def _search_js_sign_func(self, webpage, fatal=True):\n+        # The greedy look-behind ensures last possible script tag is matched\n+        return self._search_regex(\n+            r'(?:<script.*)?<script[^>]*>(.*?ub98484234.*?)</script>', webpage, 'JS sign func', fatal=fatal)\n+\n+\n+class DouyuTVIE(DouyuBaseIE):\n+    IE_DESC = '\u6597\u9c7c\u76f4\u64ad'\n     _VALID_URL = r'https?://(?:www\\.)?douyu(?:tv)?\\.com/(topic/\\w+\\?rid=|(?:[^/]+/))*(?P<id>[A-Za-z0-9]+)'\n     _TESTS = [{\n-        'url': 'http://www.douyutv.com/iseven',\n+        'url': 'https://www.douyu.com/pigff',\n         'info_dict': {\n-            'id': '17732',\n-            'display_id': 'iseven',\n-            'ext': 'flv',\n-            'title': 're:^\u6e05\u6668\u9192\u8111\uff01\u6839\u672c\u505c\u4e0d\u4e0b\u6765\uff01 [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$',\n-            'description': r're:.*m7show@163\\.com.*',\n-            'thumbnail': r're:^https?://.*\\.png',\n-            'uploader': '7\u5e08\u5085',\n+            'id': '24422',\n+            'display_id': 'pigff',\n+            'ext': 'mp4',\n+            'title': 're:^\u3010PIGFF\u3011.* [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$',\n+            'description': r'\u226515\u7ea7\u724c\u5b50\u770b\u9c7c\u5427\u7f6e\u9876\u5e16\u8fdb\u7c89\u4e1dvx\u7fa4',\n+            'thumbnail': str,\n+            'uploader': 'pigff',\n             'is_live': True,\n+            'live_status': 'is_live',\n         },\n         'params': {\n             'skip_download': True,\n@@ -85,15 +126,43 @@ class DouyuTVIE(InfoExtractor):\n         'only_matching': True,\n     }]\n \n+    def _get_sign_func(self, room_id, video_id):\n+        return self._download_json(\n+            f'https://www.douyu.com/swf_api/homeH5Enc?rids={room_id}', video_id,\n+            note='Getting signing script')['data'][f'room{room_id}']\n+\n+    def _extract_stream_formats(self, stream_formats):\n+        formats = []\n+        for stream_info in traverse_obj(stream_formats, (..., 'data')):\n+            stream_url = urljoin(\n+                traverse_obj(stream_info, 'rtmp_url'), traverse_obj(stream_info, 'rtmp_live'))\n+            if stream_url:\n+                rate_id = traverse_obj(stream_info, ('rate', {int_or_none}))\n+                rate_info = traverse_obj(stream_info, ('multirates', lambda _, v: v['rate'] == rate_id), get_all=False)\n+                ext = determine_ext(stream_url)\n+                formats.append({\n+                    'url': stream_url,\n+                    'format_id': str_or_none(rate_id),\n+                    'ext': 'mp4' if ext == 'm3u8' else ext,\n+                    'protocol': 'm3u8_native' if ext == 'm3u8' else 'https',\n+                    'quality': rate_id % -10000 if rate_id is not None else None,\n+                    **traverse_obj(rate_info, {\n+                        'format': ('name', {str_or_none}),\n+                        'tbr': ('bit', {int_or_none}),\n+                    }),\n+                })\n+        return formats\n+\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n \n-        if video_id.isdigit():\n-            room_id = video_id\n-        else:\n-            page = self._download_webpage(url, video_id)\n-            room_id = self._html_search_regex(\n-                r'\"room_id\\\\?\"\\s*:\\s*(\\d+),', page, 'room id')\n+        webpage = self._download_webpage(url, video_id)\n+        room_id = self._search_regex(r'\\$ROOM\\.room_id\\s*=\\s*(\\d+)', webpage, 'room id')\n+\n+        if self._search_regex(r'\"videoLoop\"\\s*:\\s*(\\d+)', webpage, 'loop', default='') == '1':\n+            raise UserNotLive('The channel is auto-playing VODs', video_id=video_id)\n+        if self._search_regex(r'\\$ROOM\\.show_status\\s*=\\s*(\\d+)', webpage, 'status', default='') == '2':\n+            raise UserNotLive(video_id=video_id)\n \n         # Grab metadata from API\n         params = {\n@@ -102,110 +171,136 @@ def _real_extract(self, url):\n             'time': int(time.time()),\n         }\n         params['auth'] = hashlib.md5(\n-            f'room/{video_id}?{urllib.parse.urlencode(params)}zNzMV1y4EMxOHS6I5WKm'.encode()).hexdigest()\n-        room = self._download_json(\n+            f'room/{room_id}?{urllib.parse.urlencode(params)}zNzMV1y4EMxOHS6I5WKm'.encode()).hexdigest()\n+        room = traverse_obj(self._download_json(\n             f'http://www.douyutv.com/api/v1/room/{room_id}', video_id,\n-            note='Downloading room info', query=params)['data']\n+            note='Downloading room info', query=params, fatal=False), 'data')\n \n         # 1 = live, 2 = offline\n-        if room.get('show_status') == '2':\n-            raise ExtractorError('Live stream is offline', expected=True)\n+        if traverse_obj(room, 'show_status') == '2':\n+            raise UserNotLive(video_id=video_id)\n \n-        video_url = urljoin('https://hls3-akm.douyucdn.cn/', self._search_regex(r'(live/.*)', room['hls_url'], 'URL'))\n-        formats, subs = self._extract_m3u8_formats_and_subtitles(video_url, room_id)\n+        js_sign_func = self._search_js_sign_func(webpage, fatal=False) or self._get_sign_func(room_id, video_id)\n+        form_data = {\n+            'rate': 0,\n+            **self._calc_sign(js_sign_func, video_id, room_id),\n+        }\n+        stream_formats = [self._download_json(\n+            f'https://www.douyu.com/lapi/live/getH5Play/{room_id}',\n+            video_id, note=\"Downloading livestream format\",\n+            data=urlencode_postdata(form_data))]\n \n-        title = unescapeHTML(room['room_name'])\n-        description = room.get('show_details')\n-        thumbnail = room.get('room_src')\n-        uploader = room.get('nickname')\n+        for rate_id in traverse_obj(stream_formats[0], ('data', 'multirates', ..., 'rate')):\n+            if rate_id != traverse_obj(stream_formats[0], ('data', 'rate')):\n+                form_data['rate'] = rate_id\n+                stream_formats.append(self._download_json(\n+                    f'https://www.douyu.com/lapi/live/getH5Play/{room_id}',\n+                    video_id, note=f'Downloading livestream format {rate_id}',\n+                    data=urlencode_postdata(form_data)))\n \n         return {\n             'id': room_id,\n-            'display_id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'uploader': uploader,\n+            'formats': self._extract_stream_formats(stream_formats),\n             'is_live': True,\n-            'subtitles': subs,\n-            'formats': formats,\n+            **traverse_obj(room, {\n+                'display_id': ('url', {str}, {lambda i: i[1:]}),\n+                'title': ('room_name', {unescapeHTML}),\n+                'description': ('show_details', {str}),\n+                'uploader': ('nickname', {str}),\n+                'thumbnail': ('room_src', {url_or_none}),\n+            })\n         }\n \n \n-class DouyuShowIE(InfoExtractor):\n+class DouyuShowIE(DouyuBaseIE):\n     _VALID_URL = r'https?://v(?:mobile)?\\.douyu\\.com/show/(?P<id>[0-9a-zA-Z]+)'\n \n     _TESTS = [{\n-        'url': 'https://v.douyu.com/show/rjNBdvnVXNzvE2yw',\n-        'md5': '0c2cfd068ee2afe657801269b2d86214',\n+        'url': 'https://v.douyu.com/show/mPyq7oVNe5Yv1gLY',\n         'info_dict': {\n-            'id': 'rjNBdvnVXNzvE2yw',\n+            'id': 'mPyq7oVNe5Yv1gLY',\n             'ext': 'mp4',\n-            'title': '\u9648\u4e00\u53d1\u513f\uff1a\u7812\u971c \u6211\u6709\u4e2a\u5ba4\u53cb\u7cfb\u5217\uff0104-01 22\u70b9\u573a',\n-            'duration': 7150.08,\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'uploader': '\u9648\u4e00\u53d1\u513f',\n-            'uploader_id': 'XrZwYelr5wbK',\n-            'uploader_url': 'https://v.douyu.com/author/XrZwYelr5wbK',\n-            'upload_date': '20170402',\n+            'title': '\u56db\u5ddd\u4eba\u5c0f\u65f6\u5019\u7684\u5473\u9053\u201c\u849c\u82d7\u56de\u9505\u8089\u201d\uff0c\u4f20\u7edf\u83dc\u4e0d\u80fd\u4e22\uff0c\u8981\u5e38\u505a\u6765\u5403',\n+            'duration': 633,\n+            'thumbnail': str,\n+            'uploader': '\u7f8e\u98df\u4f5c\u5bb6\u738b\u521aV',\n+            'uploader_id': 'OVAO4NVx1m7Q',\n+            'timestamp': 1661850002,\n+            'upload_date': '20220830',\n+            'view_count': int,\n+            'tags': ['\u7f8e\u98df', '\u7f8e\u98df\u7efc\u5408'],\n         },\n     }, {\n         'url': 'https://vmobile.douyu.com/show/rjNBdvnVXNzvE2yw',\n         'only_matching': True,\n     }]\n \n+    _FORMATS = {\n+        'super': '\u539f\u753b',\n+        'high': '\u8d85\u6e05',\n+        'normal': '\u9ad8\u6e05',\n+    }\n+\n+    _QUALITIES = {\n+        'super': -1,\n+        'high': -2,\n+        'normal': -3,\n+    }\n+\n+    _RESOLUTIONS = {\n+        'super': '1920x1080',\n+        'high': '1280x720',\n+        'normal': '852x480',\n+    }\n+\n     def _real_extract(self, url):\n         url = url.replace('vmobile.', 'v.')\n         video_id = self._match_id(url)\n \n         webpage = self._download_webpage(url, video_id)\n \n-        room_info = self._parse_json(self._search_regex(\n-            r'var\\s+\\$ROOM\\s*=\\s*({.+});', webpage, 'room info'), video_id)\n-\n-        video_info = None\n-\n-        for trial in range(5):\n-            # Sometimes Douyu rejects our request. Let's try it more times\n-            try:\n-                video_info = self._download_json(\n-                    'https://vmobile.douyu.com/video/getInfo', video_id,\n-                    query={'vid': video_id},\n-                    headers={\n-                        'Referer': url,\n-                        'x-requested-with': 'XMLHttpRequest',\n-                    })\n-                break\n-            except ExtractorError:\n-                self._sleep(1, video_id)\n-\n-        if not video_info:\n-            raise ExtractorError('Can\\'t fetch video info')\n-\n-        formats = self._extract_m3u8_formats(\n-            video_info['data']['video_url'], video_id,\n-            entry_protocol='m3u8_native', ext='mp4')\n-\n-        upload_date = unified_strdate(self._html_search_regex(\n-            r'<em>\u4e0a\u4f20\u65f6\u95f4\uff1a</em><span>([^<]+)</span>', webpage,\n-            'upload date', fatal=False))\n-\n-        uploader = uploader_id = uploader_url = None\n-        mobj = re.search(\n-            r'(?m)<a[^>]+href=\"/author/([0-9a-zA-Z]+)\".+?<strong[^>]+title=\"([^\"]+)\"',\n-            webpage)\n-        if mobj:\n-            uploader_id, uploader = mobj.groups()\n-            uploader_url = urljoin(url, '/author/' + uploader_id)\n+        video_info = self._search_json(\n+            r'<script>\\s*window\\.\\$DATA\\s*=', webpage,\n+            'video info', video_id, transform_source=js_to_json)\n+\n+        js_sign_func = self._search_js_sign_func(webpage)\n+        form_data = {\n+            'vid': video_id,\n+            **self._calc_sign(js_sign_func, video_id, video_info['ROOM']['point_id']),\n+        }\n+        url_info = self._download_json(\n+            'https://v.douyu.com/api/stream/getStreamUrl', video_id,\n+            data=urlencode_postdata(form_data), note=\"Downloading video formats\")\n+\n+        formats = []\n+        for name, url in traverse_obj(url_info, ('data', 'thumb_video', {dict.items}, ...)):\n+            video_url = traverse_obj(url, ('url', {url_or_none}))\n+            if video_url:\n+                ext = determine_ext(video_url)\n+                formats.append({\n+                    'format': self._FORMATS.get(name),\n+                    'format_id': name,\n+                    'url': video_url,\n+                    'quality': self._QUALITIES.get(name),\n+                    'ext': 'mp4' if ext == 'm3u8' else ext,\n+                    'protocol': 'm3u8_native' if ext == 'm3u8' else 'https',\n+                    **parse_resolution(self._RESOLUTIONS.get(name))\n+                })\n+            else:\n+                self.to_screen(\n+                    f'\"{self._FORMATS.get(name, name)}\" format may require logging in. {self._login_hint()}')\n \n         return {\n             'id': video_id,\n-            'title': room_info['name'],\n             'formats': formats,\n-            'duration': room_info.get('duration'),\n-            'thumbnail': room_info.get('pic'),\n-            'upload_date': upload_date,\n-            'uploader': uploader,\n-            'uploader_id': uploader_id,\n-            'uploader_url': uploader_url,\n+            **traverse_obj(video_info, ('DATA', {\n+                'title': ('content', 'title', {str}),\n+                'uploader': ('content', 'author', {str}),\n+                'uploader_id': ('content', 'up_id', {str_or_none}),\n+                'duration': ('content', 'video_duration', {int_or_none}),\n+                'thumbnail': ('content', 'video_pic', {url_or_none}),\n+                'timestamp': ('content', 'create_time', {int_or_none}),\n+                'view_count': ('content', 'view_num', {int_or_none}),\n+                'tags': ('videoTag', ..., 'tagName', {str}),\n+            }))\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/dropbox.py",
            "diff": "diff --git a/yt_dlp/extractor/dropbox.py b/yt_dlp/extractor/dropbox.py\nindex 214b309b..bc2efce1 100644\n--- a/yt_dlp/extractor/dropbox.py\n+++ b/yt_dlp/extractor/dropbox.py\n@@ -1,3 +1,4 @@\n+import base64\n import os.path\n import re\n \n@@ -5,14 +6,13 @@\n from ..compat import compat_urllib_parse_unquote\n from ..utils import (\n     ExtractorError,\n-    traverse_obj,\n-    try_get,\n+    update_url_query,\n     url_basename,\n )\n \n \n class DropboxIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?dropbox[.]com/sh?/(?P<id>[a-zA-Z0-9]{15})/.*'\n+    _VALID_URL = r'https?://(?:www\\.)?dropbox\\.com/(?:(?:e/)?scl/fi|sh?)/(?P<id>\\w+)'\n     _TESTS = [\n         {\n             'url': 'https://www.dropbox.com/s/nelirfsxnmcfbfh/youtube-dl%20test%20video%20%27%C3%A4%22BaW_jenozKc.mp4?dl=0',\n@@ -22,7 +22,16 @@ class DropboxIE(InfoExtractor):\n                 'title': 'youtube-dl test video \\'\u00e4\"BaW_jenozKc'\n             }\n         }, {\n-            'url': 'https://www.dropbox.com/sh/662glsejgzoj9sr/AAByil3FGH9KFNZ13e08eSa1a/Pregame%20Ceremony%20Program%20PA%2020140518.m4v',\n+            'url': 'https://www.dropbox.com/s/nelirfsxnmcfbfh',\n+            'only_matching': True,\n+        }, {\n+            'url': 'https://www.dropbox.com/sh/2mgpiuq7kv8nqdf/AABy-fW4dkydT4GmWi2mdOUDa?dl=0&preview=Drone+Shot.mp4',\n+            'only_matching': True,\n+        }, {\n+            'url': 'https://www.dropbox.com/scl/fi/r2kd2skcy5ylbbta5y1pz/DJI_0003.MP4?dl=0&rlkey=wcdgqangn7t3lnmmv6li9mu9h',\n+            'only_matching': True,\n+        }, {\n+            'url': 'https://www.dropbox.com/e/scl/fi/r2kd2skcy5ylbbta5y1pz/DJI_0003.MP4?dl=0&rlkey=wcdgqangn7t3lnmmv6li9mu9h',\n             'only_matching': True,\n         },\n     ]\n@@ -53,16 +62,25 @@ def _real_extract(self, url):\n             else:\n                 raise ExtractorError('Password protected video, use --video-password <password>', expected=True)\n \n-        info_json = self._search_json(r'InitReact\\.mountComponent\\(.*?,', webpage, 'mountComponent', video_id,\n-                                      contains_pattern=r'{.+?\"preview\".+?}', end_pattern=r'\\)')['props']\n-        transcode_url = traverse_obj(info_json, ((None, 'preview'), 'file', 'preview', 'content', 'transcode_url'), get_all=False)\n-        formats, subtitles = self._extract_m3u8_formats_and_subtitles(transcode_url, video_id)\n+        formats, subtitles, has_anonymous_download = [], {}, False\n+        for encoded in reversed(re.findall(r'registerStreamedPrefetch\\s*\\(\\s*\"[\\w/+=]+\"\\s*,\\s*\"([\\w/+=]+)\"', webpage)):\n+            decoded = base64.b64decode(encoded).decode('utf-8', 'ignore')\n+            transcode_url = self._search_regex(\n+                r'\\n.(https://[^\\x03\\x08\\x12\\n]+\\.m3u8)', decoded, 'transcode url', default=None)\n+            if not transcode_url:\n+                continue\n+            formats, subtitles = self._extract_m3u8_formats_and_subtitles(transcode_url, video_id, 'mp4')\n+            has_anonymous_download = self._search_regex(r'(anonymous:\\tanonymous)', decoded, 'anonymous', default=False)\n+            break\n \n         # downloads enabled we can get the original file\n-        if 'anonymous' in (try_get(info_json, lambda x: x['sharePermission']['canDownloadRoles']) or []):\n-            video_url = re.sub(r'[?&]dl=0', '', url)\n-            video_url += ('?' if '?' not in video_url else '&') + 'dl=1'\n-            formats.append({'url': video_url, 'format_id': 'original', 'format_note': 'Original', 'quality': 1})\n+        if has_anonymous_download:\n+            formats.append({\n+                'url': update_url_query(url, {'dl': '1'}),\n+                'format_id': 'original',\n+                'format_note': 'Original',\n+                'quality': 1\n+            })\n \n         return {\n             'id': video_id,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/drtv.py",
            "diff": "diff --git a/yt_dlp/extractor/drtv.py b/yt_dlp/extractor/drtv.py\nindex 6c381aa1..2a6e337b 100644\n--- a/yt_dlp/extractor/drtv.py\n+++ b/yt_dlp/extractor/drtv.py\n@@ -1,21 +1,17 @@\n-import binascii\n-import hashlib\n-import re\n+import json\n+import uuid\n \n from .common import InfoExtractor\n-from ..aes import aes_cbc_decrypt_bytes, unpad_pkcs7\n-from ..compat import compat_urllib_parse_unquote\n from ..utils import (\n     ExtractorError,\n-    float_or_none,\n     int_or_none,\n     mimetype2ext,\n-    str_or_none,\n-    traverse_obj,\n-    unified_timestamp,\n+    parse_iso8601,\n+    try_call,\n     update_url_query,\n     url_or_none,\n )\n+from ..utils.traversal import traverse_obj\n \n SERIES_API = 'https://production-cdn.dr-massive.com/api/page?device=web_browser&item_detail_expand=all&lang=da&max_list_prefetch=3&path=%s'\n \n@@ -24,7 +20,7 @@ class DRTVIE(InfoExtractor):\n     _VALID_URL = r'''(?x)\n                     https?://\n                         (?:\n-                            (?:www\\.)?dr\\.dk/(?:tv/se|nyheder|(?P<radio>radio|lyd)(?:/ondemand)?)/(?:[^/]+/)*|\n+                            (?:www\\.)?dr\\.dk/tv/se(?:/ondemand)?/(?:[^/?#]+/)*|\n                             (?:www\\.)?(?:dr\\.dk|dr-massive\\.com)/drtv/(?:se|episode|program)/\n                         )\n                         (?P<id>[\\da-z_-]+)\n@@ -53,22 +49,6 @@ class DRTVIE(InfoExtractor):\n         },\n         'expected_warnings': ['Unable to download f4m manifest'],\n         'skip': 'this video has been removed',\n-    }, {\n-        # embed\n-        'url': 'https://www.dr.dk/nyheder/indland/live-christianias-rydning-af-pusher-street-er-i-gang',\n-        'info_dict': {\n-            'id': 'urn:dr:mu:programcard:57c926176187a50a9c6e83c6',\n-            'ext': 'mp4',\n-            'title': 'christiania pusher street ryddes drdkrjpo',\n-            'description': 'md5:2a71898b15057e9b97334f61d04e6eb5',\n-            'timestamp': 1472800279,\n-            'upload_date': '20160902',\n-            'duration': 131.4,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'expected_warnings': ['Unable to download f4m manifest'],\n     }, {\n         # with SignLanguage formats\n         'url': 'https://www.dr.dk/tv/se/historien-om-danmark/-/historien-om-danmark-stenalder',\n@@ -87,33 +67,54 @@ class DRTVIE(InfoExtractor):\n             'season': 'Historien om Danmark',\n             'series': 'Historien om Danmark',\n         },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://www.dr.dk/lyd/p4kbh/regionale-nyheder-kh4/p4-nyheder-2019-06-26-17-30-9',\n-        'only_matching': True,\n+        'skip': 'this video has been removed',\n     }, {\n-        'url': 'https://www.dr.dk/drtv/se/bonderoeven_71769',\n+        'url': 'https://www.dr.dk/drtv/se/frank-and-kastaniegaarden_71769',\n         'info_dict': {\n             'id': '00951930010',\n             'ext': 'mp4',\n-            'title': 'Bonder\u00f8ven 2019 (1:8)',\n-            'description': 'md5:b6dcfe9b6f0bea6703e9a0092739a5bd',\n-            'timestamp': 1654856100,\n-            'upload_date': '20220610',\n-            'duration': 2576.6,\n-            'season': 'Bonder\u00f8ven 2019',\n-            'season_id': 'urn:dr:mu:bundle:5c201667a11fa01ca4528ce5',\n+            'title': 'Frank & Kastaniegaarden',\n+            'description': 'md5:974e1780934cf3275ef10280204bccb0',\n+            'release_timestamp': 1546545600,\n+            'release_date': '20190103',\n+            'duration': 2576,\n+            'season': 'Frank & Kastaniegaarden',\n+            'season_id': '67125',\n             'release_year': 2019,\n             'season_number': 2019,\n             'series': 'Frank & Kastaniegaarden',\n             'episode_number': 1,\n-            'episode': 'Episode 1',\n+            'episode': 'Frank & Kastaniegaarden',\n+            'thumbnail': r're:https?://.+',\n         },\n         'params': {\n             'skip_download': True,\n         },\n+    }, {\n+        # Foreign and Regular subtitle track\n+        'url': 'https://www.dr.dk/drtv/se/spise-med-price_-pasta-selv_397445',\n+        'info_dict': {\n+            'id': '00212301010',\n+            'ext': 'mp4',\n+            'episode_number': 1,\n+            'title': 'Spise med Price: Pasta Selv',\n+            'alt_title': '1. Pasta Selv',\n+            'release_date': '20230807',\n+            'description': 'md5:2da9060524fed707810d71080b3d0cd8',\n+            'duration': 1750,\n+            'season': 'Spise med Price',\n+            'release_timestamp': 1691438400,\n+            'season_id': '397440',\n+            'episode': 'Spise med Price: Pasta Selv',\n+            'thumbnail': r're:https?://.+',\n+            'season_number': 15,\n+            'series': 'Spise med Price',\n+            'release_year': 2022,\n+            'subtitles': 'mincount:2',\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+        },\n     }, {\n         'url': 'https://www.dr.dk/drtv/episode/bonderoeven_71769',\n         'only_matching': True,\n@@ -123,226 +124,127 @@ class DRTVIE(InfoExtractor):\n     }, {\n         'url': 'https://www.dr.dk/drtv/program/jagten_220924',\n         'only_matching': True,\n-    }, {\n-        'url': 'https://www.dr.dk/lyd/p4aarhus/regionale-nyheder-ar4/regionale-nyheder-2022-05-05-12-30-3',\n-        'info_dict': {\n-            'id': 'urn:dr:mu:programcard:6265cb2571401424d0360113',\n-            'title': \"Regionale nyheder\",\n-            'ext': 'mp4',\n-            'duration': 120.043,\n-            'series': 'P4 \u00d8stjylland regionale nyheder',\n-            'timestamp': 1651746600,\n-            'season': 'Regionale nyheder',\n-            'release_year': 0,\n-            'season_id': 'urn:dr:mu:bundle:61c26889539f0201586b73c5',\n-            'description': '',\n-            'upload_date': '20220505',\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'skip': 'this video has been removed',\n-    }, {\n-        'url': 'https://www.dr.dk/lyd/p4kbh/regionale-nyheder-kh4/regionale-nyheder-2023-03-14-10-30-9',\n-        'info_dict': {\n-            'ext': 'mp4',\n-            'id': '14802310112',\n-            'timestamp': 1678786200,\n-            'duration': 120.043,\n-            'season_id': 'urn:dr:mu:bundle:63a4f7c87140143504b6710f',\n-            'series': 'P4 K\u00f8benhavn regionale nyheder',\n-            'upload_date': '20230314',\n-            'release_year': 0,\n-            'description': 'H\u00f8r seneste regionale nyheder fra P4 K\u00f8benhavn.',\n-            'season': 'Regionale nyheder',\n-            'title': 'Regionale nyheder',\n-        },\n     }]\n \n-    def _real_extract(self, url):\n-        raw_video_id, is_radio_url = self._match_valid_url(url).group('id', 'radio')\n-\n-        webpage = self._download_webpage(url, raw_video_id)\n-\n-        if '>Programmet er ikke l\u00e6ngere tilg\u00e6ngeligt' in webpage:\n-            raise ExtractorError(\n-                'Video %s is not available' % raw_video_id, expected=True)\n-\n-        video_id = self._search_regex(\n-            (r'data-(?:material-identifier|episode-slug)=\"([^\"]+)\"',\n-             r'data-resource=\"[^>\"]+mu/programcard/expanded/([^\"]+)\"'),\n-            webpage, 'video id', default=None)\n-\n-        if not video_id:\n-            video_id = self._search_regex(\n-                r'(urn(?:%3A|:)dr(?:%3A|:)mu(?:%3A|:)programcard(?:%3A|:)[\\da-f]+)',\n-                webpage, 'urn', default=None)\n-            if video_id:\n-                video_id = compat_urllib_parse_unquote(video_id)\n+    SUBTITLE_LANGS = {\n+        'DanishLanguageSubtitles': 'da',\n+        'ForeignLanguageSubtitles': 'da_foreign',\n+        'CombinedLanguageSubtitles': 'da_combined',\n+    }\n \n-        _PROGRAMCARD_BASE = 'https://www.dr.dk/mu-online/api/1.4/programcard'\n-        query = {'expanded': 'true'}\n+    _TOKEN = None\n+\n+    def _real_initialize(self):\n+        if self._TOKEN:\n+            return\n+\n+        token_response = self._download_json(\n+            'https://production.dr-massive.com/api/authorization/anonymous-sso', None,\n+            note='Downloading anonymous token', headers={\n+                'content-type': 'application/json',\n+            }, query={\n+                'device': 'web_browser',\n+                'ff': 'idp,ldp,rpt',\n+                'lang': 'da',\n+                'supportFallbackToken': 'true',\n+            }, data=json.dumps({\n+                'deviceId': str(uuid.uuid4()),\n+                'scopes': ['Catalog'],\n+                'optout': True,\n+            }).encode())\n+\n+        self._TOKEN = traverse_obj(\n+            token_response, (lambda _, x: x['type'] == 'UserAccount', 'value', {str}), get_all=False)\n+        if not self._TOKEN:\n+            raise ExtractorError('Unable to get anonymous token')\n \n-        if video_id:\n-            programcard_url = '%s/%s' % (_PROGRAMCARD_BASE, video_id)\n+    def _real_extract(self, url):\n+        url_slug = self._match_id(url)\n+        webpage = self._download_webpage(url, url_slug)\n+\n+        json_data = self._search_json(\n+            r'window\\.__data\\s*=', webpage, 'data', url_slug, fatal=False) or {}\n+        item = traverse_obj(\n+            json_data, ('cache', 'page', ..., (None, ('entries', 0)), 'item', {dict}), get_all=False)\n+        if item:\n+            item_id = item.get('id')\n         else:\n-            programcard_url = _PROGRAMCARD_BASE\n-            if is_radio_url:\n-                video_id = self._search_nextjs_data(\n-                    webpage, raw_video_id)['props']['pageProps']['episode']['productionNumber']\n-            else:\n-                json_data = self._search_json(\n-                    r'window\\.__data\\s*=', webpage, 'data', raw_video_id)\n-                video_id = traverse_obj(json_data, (\n-                    'cache', 'page', ..., (None, ('entries', 0)), 'item', 'customId',\n-                    {lambda x: x.split(':')[-1]}), get_all=False)\n-                if not video_id:\n-                    raise ExtractorError('Unable to extract video id')\n-            query['productionnumber'] = video_id\n-\n-        data = self._download_json(\n-            programcard_url, video_id, 'Downloading video JSON', query=query)\n-\n-        supplementary_data = {}\n-        if re.search(r'_\\d+$', raw_video_id):\n-            supplementary_data = self._download_json(\n-                SERIES_API % f'/episode/{raw_video_id}', raw_video_id, fatal=False) or {}\n-\n-        title = str_or_none(data.get('Title')) or re.sub(\n-            r'\\s*\\|\\s*(?:TV\\s*\\|\\s*DR|DRTV)$', '',\n-            self._og_search_title(webpage))\n-        description = self._og_search_description(\n-            webpage, default=None) or data.get('Description')\n-\n-        timestamp = unified_timestamp(\n-            data.get('PrimaryBroadcastStartTime') or data.get('SortDateTime'))\n-\n-        thumbnail = None\n-        duration = None\n-\n-        restricted_to_denmark = False\n+            item_id = url_slug.rsplit('_', 1)[-1]\n+            item = self._download_json(\n+                f'https://production-cdn.dr-massive.com/api/items/{item_id}', item_id,\n+                note='Attempting to download backup item data', query={\n+                    'device': 'web_browser',\n+                    'expand': 'all',\n+                    'ff': 'idp,ldp,rpt',\n+                    'geoLocation': 'dk',\n+                    'isDeviceAbroad': 'false',\n+                    'lang': 'da',\n+                    'segments': 'drtv,optedout',\n+                    'sub': 'Anonymous',\n+                })\n+\n+        video_id = try_call(lambda: item['customId'].rsplit(':', 1)[-1]) or item_id\n+        stream_data = self._download_json(\n+            f'https://production.dr-massive.com/api/account/items/{item_id}/videos', video_id,\n+            note='Downloading stream data', query={\n+                'delivery': 'stream',\n+                'device': 'web_browser',\n+                'ff': 'idp,ldp,rpt',\n+                'lang': 'da',\n+                'resolution': 'HD-1080',\n+                'sub': 'Anonymous',\n+            }, headers={'authorization': f'Bearer {self._TOKEN}'})\n \n         formats = []\n         subtitles = {}\n-\n-        assets = []\n-        primary_asset = data.get('PrimaryAsset')\n-        if isinstance(primary_asset, dict):\n-            assets.append(primary_asset)\n-        secondary_assets = data.get('SecondaryAssets')\n-        if isinstance(secondary_assets, list):\n-            for secondary_asset in secondary_assets:\n-                if isinstance(secondary_asset, dict):\n-                    assets.append(secondary_asset)\n-\n-        def hex_to_bytes(hex):\n-            return binascii.a2b_hex(hex.encode('ascii'))\n-\n-        def decrypt_uri(e):\n-            n = int(e[2:10], 16)\n-            a = e[10 + n:]\n-            data = hex_to_bytes(e[10:10 + n])\n-            key = hashlib.sha256(('%s:sRBzYNXBzkKgnjj8pGtkACch' % a).encode('utf-8')).digest()\n-            iv = hex_to_bytes(a)\n-            decrypted = unpad_pkcs7(aes_cbc_decrypt_bytes(data, key, iv))\n-            return decrypted.decode('utf-8').split('?')[0]\n-\n-        for asset in assets:\n-            kind = asset.get('Kind')\n-            if kind == 'Image':\n-                thumbnail = url_or_none(asset.get('Uri'))\n-            elif kind in ('VideoResource', 'AudioResource'):\n-                duration = float_or_none(asset.get('DurationInMilliseconds'), 1000)\n-                restricted_to_denmark = asset.get('RestrictedToDenmark')\n-                asset_target = asset.get('Target')\n-                for link in asset.get('Links', []):\n-                    uri = link.get('Uri')\n-                    if not uri:\n-                        encrypted_uri = link.get('EncryptedUri')\n-                        if not encrypted_uri:\n-                            continue\n-                        try:\n-                            uri = decrypt_uri(encrypted_uri)\n-                        except Exception:\n-                            self.report_warning(\n-                                'Unable to decrypt EncryptedUri', video_id)\n-                            continue\n-                    uri = url_or_none(uri)\n-                    if not uri:\n-                        continue\n-                    target = link.get('Target')\n-                    format_id = target or ''\n-                    if asset_target in ('SpokenSubtitles', 'SignLanguage', 'VisuallyInterpreted'):\n-                        preference = -1\n-                        format_id += '-%s' % asset_target\n-                    elif asset_target == 'Default':\n-                        preference = 1\n-                    else:\n-                        preference = None\n-                    if target == 'HDS':\n-                        f4m_formats = self._extract_f4m_formats(\n-                            uri + '?hdcore=3.3.0&plugin=aasp-3.3.0.99.43',\n-                            video_id, preference, f4m_id=format_id, fatal=False)\n-                        if kind == 'AudioResource':\n-                            for f in f4m_formats:\n-                                f['vcodec'] = 'none'\n-                        formats.extend(f4m_formats)\n-                    elif target == 'HLS':\n-                        fmts, subs = self._extract_m3u8_formats_and_subtitles(\n-                            uri, video_id, 'mp4', entry_protocol='m3u8_native',\n-                            quality=preference, m3u8_id=format_id, fatal=False)\n-                        formats.extend(fmts)\n-                        self._merge_subtitles(subs, target=subtitles)\n-                    else:\n-                        bitrate = link.get('Bitrate')\n-                        if bitrate:\n-                            format_id += '-%s' % bitrate\n-                        formats.append({\n-                            'url': uri,\n-                            'format_id': format_id,\n-                            'tbr': int_or_none(bitrate),\n-                            'ext': link.get('FileFormat'),\n-                            'vcodec': 'none' if kind == 'AudioResource' else None,\n-                            'quality': preference,\n-                        })\n-            subtitles_list = asset.get('SubtitlesList') or asset.get('Subtitleslist')\n-            if isinstance(subtitles_list, list):\n-                LANGS = {\n-                    'Danish': 'da',\n-                }\n-                for subs in subtitles_list:\n-                    if not isinstance(subs, dict):\n-                        continue\n-                    sub_uri = url_or_none(subs.get('Uri'))\n-                    if not sub_uri:\n-                        continue\n-                    lang = subs.get('Language') or 'da'\n-                    subtitles.setdefault(LANGS.get(lang, lang), []).append({\n-                        'url': sub_uri,\n-                        'ext': mimetype2ext(subs.get('MimeType')) or 'vtt'\n-                    })\n-\n-        if not formats and restricted_to_denmark:\n-            self.raise_geo_restricted(\n-                'Unfortunately, DR is not allowed to show this program outside Denmark.',\n-                countries=self._GEO_COUNTRIES)\n+        for stream in traverse_obj(stream_data, (lambda _, x: x['url'])):\n+            format_id = stream.get('format', 'na')\n+            access_service = stream.get('accessService')\n+            preference = None\n+            subtitle_suffix = ''\n+            if access_service in ('SpokenSubtitles', 'SignLanguage', 'VisuallyInterpreted'):\n+                preference = -1\n+                format_id += f'-{access_service}'\n+                subtitle_suffix = f'-{access_service}'\n+            elif access_service == 'StandardVideo':\n+                preference = 1\n+            fmts, subs = self._extract_m3u8_formats_and_subtitles(\n+                stream.get('url'), video_id, ext='mp4', preference=preference, m3u8_id=format_id, fatal=False)\n+            formats.extend(fmts)\n+\n+            api_subtitles = traverse_obj(stream, ('subtitles', lambda _, v: url_or_none(v['link']), {dict}))\n+            if not api_subtitles:\n+                self._merge_subtitles(subs, target=subtitles)\n+\n+            for sub_track in api_subtitles:\n+                lang = sub_track.get('language') or 'da'\n+                subtitles.setdefault(self.SUBTITLE_LANGS.get(lang, lang) + subtitle_suffix, []).append({\n+                    'url': sub_track['link'],\n+                    'ext': mimetype2ext(sub_track.get('format')) or 'vtt'\n+                })\n+\n+        if not formats and traverse_obj(item, ('season', 'customFields', 'IsGeoRestricted')):\n+            self.raise_geo_restricted(countries=self._GEO_COUNTRIES)\n \n         return {\n             'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'timestamp': timestamp,\n-            'duration': duration,\n             'formats': formats,\n             'subtitles': subtitles,\n-            'series': str_or_none(data.get('SeriesTitle')),\n-            'season': str_or_none(data.get('SeasonTitle')),\n-            'season_number': int_or_none(data.get('SeasonNumber')),\n-            'season_id': str_or_none(data.get('SeasonUrn')),\n-            'episode': traverse_obj(supplementary_data, ('entries', 0, 'item', 'contextualTitle')) or str_or_none(data.get('EpisodeTitle')),\n-            'episode_number': traverse_obj(supplementary_data, ('entries', 0, 'item', 'episodeNumber')) or int_or_none(data.get('EpisodeNumber')),\n-            'release_year': int_or_none(data.get('ProductionYear')),\n+            **traverse_obj(item, {\n+                'title': 'title',\n+                'alt_title': 'contextualTitle',\n+                'description': 'description',\n+                'thumbnail': ('images', 'wallpaper'),\n+                'release_timestamp': ('customFields', 'BroadcastTimeDK', {parse_iso8601}),\n+                'duration': ('duration', {int_or_none}),\n+                'series': ('season', 'show', 'title'),\n+                'season': ('season', 'title'),\n+                'season_number': ('season', 'seasonNumber', {int_or_none}),\n+                'season_id': 'seasonId',\n+                'episode': 'episodeName',\n+                'episode_number': ('episodeNumber', {int_or_none}),\n+                'release_year': ('releaseYear', {int_or_none}),\n+            }),\n         }\n \n \n@@ -412,6 +314,8 @@ class DRTVSeasonIE(InfoExtractor):\n             'display_id': 'frank-and-kastaniegaarden',\n             'title': 'Frank & Kastaniegaarden',\n             'series': 'Frank & Kastaniegaarden',\n+            'season_number': 2008,\n+            'alt_title': 'Season 2008',\n         },\n         'playlist_mincount': 8\n     }, {\n@@ -421,6 +325,8 @@ class DRTVSeasonIE(InfoExtractor):\n             'display_id': 'frank-and-kastaniegaarden',\n             'title': 'Frank & Kastaniegaarden',\n             'series': 'Frank & Kastaniegaarden',\n+            'season_number': 2009,\n+            'alt_title': 'Season 2009',\n         },\n         'playlist_mincount': 19\n     }]\n@@ -434,6 +340,7 @@ def _real_extract(self, url):\n             'url': f'https://www.dr.dk/drtv{episode[\"path\"]}',\n             'ie_key': DRTVIE.ie_key(),\n             'title': episode.get('title'),\n+            'alt_title': episode.get('contextualTitle'),\n             'episode': episode.get('episodeName'),\n             'description': episode.get('shortDescription'),\n             'series': traverse_obj(data, ('entries', 0, 'item', 'title')),\n@@ -446,6 +353,7 @@ def _real_extract(self, url):\n             'id': season_id,\n             'display_id': display_id,\n             'title': traverse_obj(data, ('entries', 0, 'item', 'title')),\n+            'alt_title': traverse_obj(data, ('entries', 0, 'item', 'contextualTitle')),\n             'series': traverse_obj(data, ('entries', 0, 'item', 'title')),\n             'entries': entries,\n             'season_number': traverse_obj(data, ('entries', 0, 'item', 'seasonNumber'))\n@@ -463,6 +371,7 @@ class DRTVSeriesIE(InfoExtractor):\n             'display_id': 'frank-and-kastaniegaarden',\n             'title': 'Frank & Kastaniegaarden',\n             'series': 'Frank & Kastaniegaarden',\n+            'alt_title': '',\n         },\n         'playlist_mincount': 15\n     }]\n@@ -476,6 +385,7 @@ def _real_extract(self, url):\n             'url': f'https://www.dr.dk/drtv{season.get(\"path\")}',\n             'ie_key': DRTVSeasonIE.ie_key(),\n             'title': season.get('title'),\n+            'alt_title': season.get('contextualTitle'),\n             'series': traverse_obj(data, ('entries', 0, 'item', 'title')),\n             'season_number': traverse_obj(data, ('entries', 0, 'item', 'seasonNumber'))\n         } for season in traverse_obj(data, ('entries', 0, 'item', 'show', 'seasons', 'items'))]\n@@ -485,6 +395,7 @@ def _real_extract(self, url):\n             'id': series_id,\n             'display_id': display_id,\n             'title': traverse_obj(data, ('entries', 0, 'item', 'title')),\n+            'alt_title': traverse_obj(data, ('entries', 0, 'item', 'contextualTitle')),\n             'series': traverse_obj(data, ('entries', 0, 'item', 'title')),\n             'entries': entries\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/duboku.py",
            "diff": "diff --git a/yt_dlp/extractor/duboku.py b/yt_dlp/extractor/duboku.py\nindex fb0546ca..fc9564ce 100644\n--- a/yt_dlp/extractor/duboku.py\n+++ b/yt_dlp/extractor/duboku.py\n@@ -138,7 +138,7 @@ def _real_extract(self, url):\n             # of the video.\n             return {\n                 '_type': 'url_transparent',\n-                'url': smuggle_url(data_url, {'http_headers': headers}),\n+                'url': smuggle_url(data_url, {'referer': webpage_url}),\n                 'id': video_id,\n                 'title': title,\n                 'series': series_title,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/duoplay.py",
            "diff": "diff --git a/yt_dlp/extractor/duoplay.py b/yt_dlp/extractor/duoplay.py\nnew file mode 100644\nindex 00000000..7d3f3994\n--- /dev/null\n+++ b/yt_dlp/extractor/duoplay.py\n@@ -0,0 +1,104 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    extract_attributes,\n+    get_element_text_and_html_by_tag,\n+    int_or_none,\n+    join_nonempty,\n+    str_or_none,\n+    try_call,\n+    unified_timestamp,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class DuoplayIE(InfoExtractor):\n+    _VALID_URL = r'https://duoplay\\.ee/(?P<id>\\d+)/[\\w-]+/?(?:\\?(?:[^#]+&)?ep=(?P<ep>\\d+))?'\n+    _TESTS = [{\n+        'note': 'Siberi v\u00f5mm S02E12',\n+        'url': 'https://duoplay.ee/4312/siberi-vomm?ep=24',\n+        'md5': '1ff59d535310ac9c5cf5f287d8f91b2d',\n+        'info_dict': {\n+            'id': '4312_24',\n+            'ext': 'mp4',\n+            'title': 'Operatsioon \"\u00d6\u00f6\"',\n+            'thumbnail': r're:https://.+\\.jpg(?:\\?c=\\d+)?$',\n+            'description': 'md5:8ef98f38569d6b8b78f3d350ccc6ade8',\n+            'upload_date': '20170523',\n+            'timestamp': 1495567800,\n+            'series': 'Siberi v\u00f5mm',\n+            'series_id': '4312',\n+            'season': 'Season 2',\n+            'season_number': 2,\n+            'episode': 'Operatsioon \"\u00d6\u00f6\"',\n+            'episode_number': 12,\n+            'episode_id': 24,\n+        },\n+    }, {\n+        'note': 'Empty title',\n+        'url': 'https://duoplay.ee/17/uhikarotid?ep=14',\n+        'md5': '6aca68be71112314738dd17cced7f8bf',\n+        'info_dict': {\n+            'id': '17_14',\n+            'ext': 'mp4',\n+            'title': '\u00dchikarotid',\n+            'thumbnail': r're:https://.+\\.jpg(?:\\?c=\\d+)?$',\n+            'description': 'md5:4719b418e058c209def41d48b601276e',\n+            'upload_date': '20100916',\n+            'timestamp': 1284661800,\n+            'series': '\u00dchikarotid',\n+            'series_id': '17',\n+            'season': 'Season 2',\n+            'season_number': 2,\n+            'episode_id': 14,\n+            'release_year': 2010,\n+        },\n+    }, {\n+        'note': 'Movie without expiry',\n+        'url': 'https://duoplay.ee/5501/pilvede-all.-neljas-ode',\n+        'md5': '7abf63d773a49ef7c39f2c127842b8fd',\n+        'info_dict': {\n+            'id': '5501',\n+            'ext': 'mp4',\n+            'title': 'Pilvede all. Neljas \u00f5de',\n+            'thumbnail': r're:https://.+\\.jpg(?:\\?c=\\d+)?$',\n+            'description': 'md5:d86a70f8f31e82c369d4d4f4c79b1279',\n+            'cast': 'count:9',\n+            'upload_date': '20221214',\n+            'timestamp': 1671054000,\n+            'release_year': 2018,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        telecast_id, episode = self._match_valid_url(url).group('id', 'ep')\n+        video_id = join_nonempty(telecast_id, episode, delim='_')\n+        webpage = self._download_webpage(url, video_id)\n+        video_player = try_call(lambda: extract_attributes(\n+            get_element_text_and_html_by_tag('video-player', webpage)[1]))\n+        if not video_player or not video_player.get('manifest-url'):\n+            raise ExtractorError('No video found', expected=True)\n+\n+        episode_attr = self._parse_json(video_player.get(':episode') or '', video_id, fatal=False) or {}\n+\n+        return {\n+            'id': video_id,\n+            'formats': self._extract_m3u8_formats(video_player['manifest-url'], video_id, 'mp4'),\n+            **traverse_obj(episode_attr, {\n+                'title': 'title',\n+                'description': 'synopsis',\n+                'thumbnail': ('images', 'original'),\n+                'timestamp': ('airtime', {lambda x: unified_timestamp(x + ' +0200')}),\n+                'cast': ('cast', {lambda x: x.split(', ')}),\n+                'release_year': ('year', {int_or_none}),\n+            }),\n+            **(traverse_obj(episode_attr, {\n+                'title': (None, ('subtitle', ('episode_nr', {lambda x: f'Episode {x}' if x else None}))),\n+                'series': 'title',\n+                'series_id': ('telecast_id', {str_or_none}),\n+                'season_number': ('season_id', {int_or_none}),\n+                'episode': 'subtitle',\n+                'episode_number': ('episode_nr', {int_or_none}),\n+                'episode_id': ('episode_id', {int_or_none}),\n+            }, get_all=False) if episode_attr.get('category') != 'movies' else {}),\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/echomsk.py",
            "diff": "diff --git a/yt_dlp/extractor/echomsk.py b/yt_dlp/extractor/echomsk.py\ndeleted file mode 100644\nindex 850eabbf..00000000\n--- a/yt_dlp/extractor/echomsk.py\n+++ /dev/null\n@@ -1,43 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-\n-\n-class EchoMskIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?echo\\.msk\\.ru/sounds/(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'http://www.echo.msk.ru/sounds/1464134.html',\n-        'md5': '2e44b3b78daff5b458e4dbc37f191f7c',\n-        'info_dict': {\n-            'id': '1464134',\n-            'ext': 'mp3',\n-            'title': '\u041e\u0441\u043e\u0431\u043e\u0435 \u043c\u043d\u0435\u043d\u0438\u0435 - 29 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2014, 19:08',\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        audio_url = self._search_regex(\n-            r'<a rel=\"mp3\" href=\"([^\"]+)\">', webpage, 'audio URL')\n-\n-        title = self._html_search_regex(\n-            r'<a href=\"/programs/[^\"]+\" target=\"_blank\">([^<]+)</a>',\n-            webpage, 'title')\n-\n-        air_date = self._html_search_regex(\n-            r'(?s)<div class=\"date\">(.+?)</div>',\n-            webpage, 'date', fatal=False, default=None)\n-\n-        if air_date:\n-            air_date = re.sub(r'(\\s)\\1+', r'\\1', air_date)\n-            if air_date:\n-                title = '%s - %s' % (title, air_date)\n-\n-        return {\n-            'id': video_id,\n-            'url': audio_url,\n-            'title': title,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ehow.py",
            "diff": "diff --git a/yt_dlp/extractor/ehow.py b/yt_dlp/extractor/ehow.py\ndeleted file mode 100644\nindex 74469ce3..00000000\n--- a/yt_dlp/extractor/ehow.py\n+++ /dev/null\n@@ -1,36 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_urllib_parse_unquote\n-\n-\n-class EHowIE(InfoExtractor):\n-    IE_NAME = 'eHow'\n-    _VALID_URL = r'https?://(?:www\\.)?ehow\\.com/[^/_?]*_(?P<id>[0-9]+)'\n-    _TEST = {\n-        'url': 'http://www.ehow.com/video_12245069_hardwood-flooring-basics.html',\n-        'md5': '9809b4e3f115ae2088440bcb4efbf371',\n-        'info_dict': {\n-            'id': '12245069',\n-            'ext': 'flv',\n-            'title': 'Hardwood Flooring Basics',\n-            'description': 'Hardwood flooring may be time consuming, but its ultimately a pretty straightforward concept. Learn about hardwood flooring basics with help from a hardware flooring business owner in this free video...',\n-            'uploader': 'Erick Nathan',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        video_url = self._search_regex(\n-            r'(?:file|source)=(http[^\\'\"&]*)', webpage, 'video URL')\n-        final_url = compat_urllib_parse_unquote(video_url)\n-        uploader = self._html_search_meta('uploader', webpage)\n-        title = self._og_search_title(webpage).replace(' | eHow', '')\n-\n-        return {\n-            'id': video_id,\n-            'url': final_url,\n-            'title': title,\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-            'description': self._og_search_description(webpage),\n-            'uploader': uploader,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/elevensports.py",
            "diff": "diff --git a/yt_dlp/extractor/elevensports.py b/yt_dlp/extractor/elevensports.py\ndeleted file mode 100644\nindex 99c52b3a..00000000\n--- a/yt_dlp/extractor/elevensports.py\n+++ /dev/null\n@@ -1,59 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    parse_iso8601,\n-    traverse_obj,\n-    url_or_none,\n-)\n-\n-\n-class ElevenSportsIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?elevensports\\.com/view/event/(?P<id>\\w+)'\n-    _TESTS = [{\n-        'url': 'https://elevensports.com/view/event/clf46yr3kenn80jgrqsjmwefk',\n-        'md5': 'c0958d9ff90e4503a75544358758921d',\n-        'info_dict': {\n-            'id': 'clf46yr3kenn80jgrqsjmwefk',\n-            'title': 'Cleveland SC vs Lionsbridge FC',\n-            'ext': 'mp4',\n-            'description': 'md5:03b5238d6549f4ea1fddadf69b5e0b58',\n-            'upload_date': '20230323',\n-            'timestamp': 1679612400,\n-            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n-        },\n-        'params': {'skip_download': 'm3u8'}\n-    }, {\n-        'url': 'https://elevensports.com/view/event/clhpyd53b06160jez74qhgkmf',\n-        'md5': 'c0958d9ff90e4503a75544358758921d',\n-        'info_dict': {\n-            'id': 'clhpyd53b06160jez74qhgkmf',\n-            'title': 'AJNLF vs ARRAF',\n-            'ext': 'mp4',\n-            'description': 'md5:c8c5e75c78f37c6d15cd6c475e43a8c1',\n-            'upload_date': '20230521',\n-            'timestamp': 1684684800,\n-            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n-        },\n-        'params': {'skip_download': 'm3u8'}\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        event_id = self._search_nextjs_data(webpage, video_id)['props']['pageProps']['event']['mclsEventId']\n-        event_data = self._download_json(\n-            f'https://mcls-api.mycujoo.tv/bff/events/v1beta1/{event_id}', video_id,\n-            headers={'Authorization': 'Bearer FBVKACGN37JQC5SFA0OVK8KKSIOP153G'})\n-        formats, subtitles = self._extract_m3u8_formats_and_subtitles(\n-            event_data['streams'][0]['full_url'], video_id, 'mp4', m3u8_id='hls')\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            'subtitles': subtitles,\n-            **traverse_obj(event_data, {\n-                'title': ('title', {str}),\n-                'description': ('description', {str}),\n-                'timestamp': ('start_time', {parse_iso8601}),\n-                'thumbnail': ('thumbnail_url', {url_or_none}),\n-            }),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ellentube.py",
            "diff": "diff --git a/yt_dlp/extractor/ellentube.py b/yt_dlp/extractor/ellentube.py\ndeleted file mode 100644\nindex 6eb00f9c..00000000\n--- a/yt_dlp/extractor/ellentube.py\n+++ /dev/null\n@@ -1,130 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    clean_html,\n-    extract_attributes,\n-    float_or_none,\n-    int_or_none,\n-    try_get,\n-)\n-\n-\n-class EllenTubeBaseIE(InfoExtractor):\n-    def _extract_data_config(self, webpage, video_id):\n-        details = self._search_regex(\n-            r'(<[^>]+\\bdata-component=([\"\\'])[Dd]etails.+?></div>)', webpage,\n-            'details')\n-        return self._parse_json(\n-            extract_attributes(details)['data-config'], video_id)\n-\n-    def _extract_video(self, data, video_id):\n-        title = data['title']\n-\n-        formats = []\n-        duration = None\n-        for entry in data.get('media'):\n-            if entry.get('id') == 'm3u8':\n-                formats, subtitles = self._extract_m3u8_formats_and_subtitles(\n-                    entry['url'], video_id, 'mp4',\n-                    entry_protocol='m3u8_native', m3u8_id='hls')\n-                duration = int_or_none(entry.get('duration'))\n-                break\n-\n-        def get_insight(kind):\n-            return int_or_none(try_get(\n-                data, lambda x: x['insight']['%ss' % kind]))\n-\n-        return {\n-            'extractor_key': EllenTubeIE.ie_key(),\n-            'id': video_id,\n-            'title': title,\n-            'description': data.get('description'),\n-            'duration': duration,\n-            'thumbnail': data.get('thumbnail'),\n-            'timestamp': float_or_none(data.get('publishTime'), scale=1000),\n-            'view_count': get_insight('view'),\n-            'like_count': get_insight('like'),\n-            'formats': formats,\n-            'subtitles': subtitles,\n-        }\n-\n-\n-class EllenTubeIE(EllenTubeBaseIE):\n-    _VALID_URL = r'''(?x)\n-                        (?:\n-                            ellentube:|\n-                            https://api-prod\\.ellentube\\.com/ellenapi/api/item/\n-                        )\n-                        (?P<id>[\\da-f]{8}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{12})\n-                    '''\n-    _TESTS = [{\n-        'url': 'https://api-prod.ellentube.com/ellenapi/api/item/0822171c-3829-43bf-b99f-d77358ae75e3',\n-        'md5': '2fabc277131bddafdd120e0fc0f974c9',\n-        'info_dict': {\n-            'id': '0822171c-3829-43bf-b99f-d77358ae75e3',\n-            'ext': 'mp4',\n-            'title': 'Ellen Meets Las Vegas Survivors Jesus Campos and Stephen Schuck',\n-            'description': 'md5:76e3355e2242a78ad9e3858e5616923f',\n-            'thumbnail': r're:^https?://.+?',\n-            'duration': 514,\n-            'timestamp': 1508505120,\n-            'upload_date': '20171020',\n-            'view_count': int,\n-            'like_count': int,\n-        }\n-    }, {\n-        'url': 'ellentube:734a3353-f697-4e79-9ca9-bfc3002dc1e0',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        data = self._download_json(\n-            'https://api-prod.ellentube.com/ellenapi/api/item/%s' % video_id,\n-            video_id)\n-        return self._extract_video(data, video_id)\n-\n-\n-class EllenTubeVideoIE(EllenTubeBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.)?ellentube\\.com/video/(?P<id>.+?)\\.html'\n-    _TEST = {\n-        'url': 'https://www.ellentube.com/video/ellen-meets-las-vegas-survivors-jesus-campos-and-stephen-schuck.html',\n-        'only_matching': True,\n-    }\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        webpage = self._download_webpage(url, display_id)\n-        video_id = self._extract_data_config(webpage, display_id)['id']\n-        return self.url_result(\n-            'ellentube:%s' % video_id, ie=EllenTubeIE.ie_key(),\n-            video_id=video_id)\n-\n-\n-class EllenTubePlaylistIE(EllenTubeBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.)?ellentube\\.com/(?:episode|studios)/(?P<id>.+?)\\.html'\n-    _TESTS = [{\n-        'url': 'https://www.ellentube.com/episode/dax-shepard-jordan-fisher-haim.html',\n-        'info_dict': {\n-            'id': 'dax-shepard-jordan-fisher-haim',\n-            'title': \"Dax Shepard, 'DWTS' Team Jordan Fisher & Lindsay Arnold, HAIM\",\n-            'description': 'md5:bfc982194dabb3f4e325e43aa6b2e21c',\n-        },\n-        'playlist_count': 6,\n-    }, {\n-        'url': 'https://www.ellentube.com/studios/macey-goes-rving0.html',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        webpage = self._download_webpage(url, display_id)\n-        data = self._extract_data_config(webpage, display_id)['data']\n-        feed = self._download_json(\n-            'https://api-prod.ellentube.com/ellenapi/api/feed/?%s'\n-            % data['filter'], display_id)\n-        entries = [\n-            self._extract_video(elem, elem['id'])\n-            for elem in feed if elem.get('type') == 'VIDEO' and elem.get('id')]\n-        return self.playlist_result(\n-            entries, display_id, data.get('title'),\n-            clean_html(data.get('description')))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/eltrecetv.py",
            "diff": "diff --git a/yt_dlp/extractor/eltrecetv.py b/yt_dlp/extractor/eltrecetv.py\nnew file mode 100644\nindex 00000000..f64023af\n--- /dev/null\n+++ b/yt_dlp/extractor/eltrecetv.py\n@@ -0,0 +1,62 @@\n+from .common import InfoExtractor\n+\n+\n+class ElTreceTVIE(InfoExtractor):\n+    IE_DESC = 'El Trece TV (Argentina)'\n+    _VALID_URL = r'https?://(?:www\\.)?eltrecetv\\.com\\.ar/[\\w-]+/capitulos/temporada-\\d+/(?P<id>[\\w-]+)'\n+    _TESTS = [\n+        {\n+            'url': 'https://www.eltrecetv.com.ar/ahora-caigo/capitulos/temporada-2023/programa-del-061023/',\n+            'md5': '71a66673dc63f9a5939d97bfe4b311ba',\n+            'info_dict': {\n+                'id': 'AHCA05102023145553329621094',\n+                'ext': 'mp4',\n+                'title': 'AHORA CAIGO - Programa 06/10/23',\n+                'thumbnail': 'https://thumbs.vodgc.net/AHCA05102023145553329621094.JPG?649339',\n+            }\n+        },\n+        {\n+            'url': 'https://www.eltrecetv.com.ar/poco-correctos/capitulos/temporada-2023/programa-del-250923-invitada-dalia-gutmann/',\n+            'only_matching': True,\n+        },\n+        {\n+            'url': 'https://www.eltrecetv.com.ar/argentina-tierra-de-amor-y-venganza/capitulos/temporada-2023/atav-2-capitulo-121-del-250923/',\n+            'only_matching': True,\n+        },\n+        {\n+            'url': 'https://www.eltrecetv.com.ar/ahora-caigo/capitulos/temporada-2023/programa-del-250923/',\n+            'only_matching': True,\n+        },\n+        {\n+            'url': 'https://www.eltrecetv.com.ar/pasaplatos/capitulos/temporada-2023/pasaplatos-el-restaurante-del-250923/',\n+            'only_matching': True,\n+        },\n+        {\n+            'url': 'https://www.eltrecetv.com.ar/el-galpon/capitulos/temporada-2023/programa-del-160923-invitado-raul-lavie/',\n+            'only_matching': True,\n+        }\n+    ]\n+\n+    def _real_extract(self, url):\n+        slug = self._match_id(url)\n+        webpage = self._download_webpage(url, slug)\n+        config = self._search_json(\n+            r'Fusion.globalContent\\s*=', webpage, 'content', slug)['promo_items']['basic']['embed']['config']\n+        video_url = config['m3u8']\n+        video_id = self._search_regex(r'/(\\w+)\\.m3u8', video_url, 'video id', default=slug)\n+\n+        formats, subtitles = self._extract_m3u8_formats_and_subtitles(video_url, video_id, 'mp4', m3u8_id='hls')\n+        formats.extend([{\n+            'url': f['url'][:-23],\n+            'format_id': f['format_id'].replace('hls', 'http'),\n+            'width': f.get('width'),\n+            'height': f.get('height'),\n+        } for f in formats if f['url'].endswith('/tracks-v1a1/index.m3u8') and f.get('height') != 1080])\n+\n+        return {\n+            'id': video_id,\n+            'title': config.get('title'),\n+            'thumbnail': config.get('thumbnail'),\n+            'formats': formats,\n+            'subtitles': subtitles,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/embedly.py",
            "diff": "diff --git a/yt_dlp/extractor/embedly.py b/yt_dlp/extractor/embedly.py\nindex 458aaa0a..a424b49d 100644\n--- a/yt_dlp/extractor/embedly.py\n+++ b/yt_dlp/extractor/embedly.py\n@@ -106,4 +106,4 @@ def _real_extract(self, url):\n             return self.url_result(src, YoutubeTabIE)\n         return self.url_result(smuggle_url(\n             urllib.parse.unquote(traverse_obj(qs, ('src', 0), ('url', 0))),\n-            {'http_headers': {'Referer': url}}))\n+            {'referer': url}))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/engadget.py",
            "diff": "diff --git a/yt_dlp/extractor/engadget.py b/yt_dlp/extractor/engadget.py\ndeleted file mode 100644\nindex e7c5d7bf..00000000\n--- a/yt_dlp/extractor/engadget.py\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class EngadgetIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?engadget\\.com/video/(?P<id>[^/?#]+)'\n-\n-    _TESTS = [{\n-        # video with vidible ID\n-        'url': 'https://www.engadget.com/video/57a28462134aa15a39f0421a/',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        return self.url_result('aol-video:%s' % video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/epidemicsound.py",
            "diff": "diff --git a/yt_dlp/extractor/epidemicsound.py b/yt_dlp/extractor/epidemicsound.py\nnew file mode 100644\nindex 00000000..0d81b11c\n--- /dev/null\n+++ b/yt_dlp/extractor/epidemicsound.py\n@@ -0,0 +1,107 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    float_or_none,\n+    int_or_none,\n+    orderedSet,\n+    parse_iso8601,\n+    parse_qs,\n+    parse_resolution,\n+    str_or_none,\n+    traverse_obj,\n+    url_or_none,\n+)\n+\n+\n+class EpidemicSoundIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?epidemicsound\\.com/track/(?P<id>[0-9a-zA-Z]+)'\n+    _TESTS = [{\n+        'url': 'https://www.epidemicsound.com/track/yFfQVRpSPz/',\n+        'md5': 'd98ff2ddb49e8acab9716541cbc9dfac',\n+        'info_dict': {\n+            'id': '45014',\n+            'display_id': 'yFfQVRpSPz',\n+            'ext': 'mp3',\n+            'title': 'Door Knock Door 1',\n+            'alt_title': 'Door Knock Door 1',\n+            'tags': ['foley', 'door', 'knock', 'glass', 'window', 'glass door knock'],\n+            'categories': ['Misc. Door'],\n+            'duration': 1,\n+            'thumbnail': 'https://cdn.epidemicsound.com/curation-assets/commercial-release-cover-images/default-sfx/3000x3000.jpg',\n+            'timestamp': 1415320353,\n+            'upload_date': '20141107',\n+        },\n+    }, {\n+        'url': 'https://www.epidemicsound.com/track/mj8GTTwsZd/',\n+        'md5': 'c82b745890f9baf18dc2f8d568ee3830',\n+        'info_dict': {\n+            'id': '148700',\n+            'display_id': 'mj8GTTwsZd',\n+            'ext': 'mp3',\n+            'title': 'Noplace',\n+            'tags': ['liquid drum n bass', 'energetic'],\n+            'categories': ['drum and bass'],\n+            'duration': 237,\n+            'timestamp': 1694426482,\n+            'thumbnail': 'https://cdn.epidemicsound.com/curation-assets/commercial-release-cover-images/11138/3000x3000.jpg',\n+            'upload_date': '20230911',\n+            'release_timestamp': 1700535606,\n+            'release_date': '20231121',\n+        },\n+    }]\n+\n+    @staticmethod\n+    def _epidemic_parse_thumbnail(url: str):\n+        if not url_or_none(url):\n+            return None\n+\n+        return {\n+            'url': url,\n+            **(traverse_obj(url, ({parse_qs}, {\n+                'width': ('width', 0, {int_or_none}),\n+                'height': ('height', 0, {int_or_none}),\n+            })) or parse_resolution(url)),\n+        }\n+\n+    @staticmethod\n+    def _epidemic_fmt_or_none(f):\n+        if not f.get('format'):\n+            f['format'] = f.get('format_id')\n+        elif not f.get('format_id'):\n+            f['format_id'] = f['format']\n+        if not f['url'] or not f['format']:\n+            return None\n+        if f.get('format_note'):\n+            f['format_note'] = f'track ID {f[\"format_note\"]}'\n+        if f['format'] != 'full':\n+            f['preference'] = -2\n+        return f\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        json_data = self._download_json(f'https://www.epidemicsound.com/json/track/{video_id}', video_id)\n+\n+        thumbnails = traverse_obj(json_data, [('imageUrl', 'cover')])\n+        thumb_base_url = traverse_obj(json_data, ('coverArt', 'baseUrl', {url_or_none}))\n+        if thumb_base_url:\n+            thumbnails.extend(traverse_obj(json_data, (\n+                'coverArt', 'sizes', ..., {thumb_base_url.__add__})))\n+\n+        return traverse_obj(json_data, {\n+            'id': ('id', {str_or_none}),\n+            'display_id': ('publicSlug', {str}),\n+            'title': ('title', {str}),\n+            'alt_title': ('oldTitle', {str}),\n+            'duration': ('length', {float_or_none}),\n+            'timestamp': ('added', {parse_iso8601}),\n+            'release_timestamp': ('releaseDate', {parse_iso8601}),\n+            'categories': ('genres', ..., 'tag', {str}),\n+            'tags': ('metadataTags', ..., {str}),\n+            'age_limit': ('isExplicit', {lambda b: 18 if b else None}),\n+            'thumbnails': ({lambda _: thumbnails}, {orderedSet}, ..., {self._epidemic_parse_thumbnail}),\n+            'formats': ('stems', {dict.items}, ..., {\n+                'format': (0, {str_or_none}),\n+                'format_note': (1, 's3TrackId', {str_or_none}),\n+                'format_id': (1, 'stemType', {str}),\n+                'url': (1, 'lqMp3Url', {url_or_none}),\n+            }, {self._epidemic_fmt_or_none}),\n+        })\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/eplus.py",
            "diff": "diff --git a/yt_dlp/extractor/eplus.py b/yt_dlp/extractor/eplus.py\nnew file mode 100644\nindex 00000000..6383691a\n--- /dev/null\n+++ b/yt_dlp/extractor/eplus.py\n@@ -0,0 +1,184 @@\n+import json\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    try_call,\n+    unified_timestamp,\n+    urlencode_postdata,\n+)\n+\n+\n+class EplusIbIE(InfoExtractor):\n+    _NETRC_MACHINE = 'eplus'\n+    IE_NAME = 'eplus'\n+    IE_DESC = 'e+ (\u30a4\u30fc\u30d7\u30e9\u30b9)'\n+    _VALID_URL = [r'https?://live\\.eplus\\.jp/ex/player\\?ib=(?P<id>(?:\\w|%2B|%2F){86}%3D%3D)',\n+                  r'https?://live\\.eplus\\.jp/(?P<id>sample|\\d+)']\n+    _TESTS = [{\n+        'url': 'https://live.eplus.jp/ex/player?ib=YEFxb3Vyc2Dombnjg7blkrLlrablnJLjgrnjgq%2Fjg7zjg6vjgqLjgqTjg4njg6vlkIzlpb3kvJpgTGllbGxhIQ%3D%3D',\n+        'info_dict': {\n+            'id': '354502-0001-002',\n+            'title': 'LoveLive!Series Presents COUNTDOWN LoveLive! 2021\u21922022\uff5eLIVE with a smile!\uff5e\u3010Streaming+(\u914d\u4fe1)\u3011',\n+            'live_status': 'was_live',\n+            'release_date': '20211231',\n+            'release_timestamp': 1640952000,\n+            'description': str,\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True,\n+        },\n+        'expected_warnings': [\n+            'Could not find the playlist URL. This event may not be accessible',\n+            'No video formats found!',\n+            'Requested format is not available',\n+        ],\n+    }, {\n+        'url': 'https://live.eplus.jp/sample',\n+        'info_dict': {\n+            'id': 'stream1ng20210719-test-005',\n+            'title': 'Online streaming test for DRM',\n+            'live_status': 'was_live',\n+            'release_date': '20210719',\n+            'release_timestamp': 1626703200,\n+            'description': None,\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True,\n+        },\n+        'expected_warnings': [\n+            'Could not find the playlist URL. This event may not be accessible',\n+            'No video formats found!',\n+            'Requested format is not available',\n+            'This video is DRM protected',\n+        ],\n+    }, {\n+        'url': 'https://live.eplus.jp/2053935',\n+        'info_dict': {\n+            'id': '331320-0001-001',\n+            'title': '\u4e18\u307f\u3069\u308a2020\u914d\u4fe1LIVE Vol.2 \uff5e\u79cb\u9e97\uff5e \u3010Streaming+(\u914d\u4fe1\u30c1\u30b1\u30c3\u30c8)\u3011',\n+            'live_status': 'was_live',\n+            'release_date': '20200920',\n+            'release_timestamp': 1600596000,\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True,\n+        },\n+        'expected_warnings': [\n+            'Could not find the playlist URL. This event may not be accessible',\n+            'No video formats found!',\n+            'Requested format is not available',\n+        ],\n+    }]\n+\n+    _USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'\n+\n+    def _login(self, username, password, urlh):\n+        if not self._get_cookies('https://live.eplus.jp/').get('ci_session'):\n+            raise ExtractorError('Unable to get ci_session cookie')\n+\n+        cltft_token = urlh.headers.get('X-CLTFT-Token')\n+        if not cltft_token:\n+            raise ExtractorError('Unable to get X-CLTFT-Token')\n+        self._set_cookie('live.eplus.jp', 'X-CLTFT-Token', cltft_token)\n+\n+        login_json = self._download_json(\n+            'https://live.eplus.jp/member/api/v1/FTAuth/idpw', None,\n+            note='Sending pre-login info', errnote='Unable to send pre-login info', headers={\n+                'Content-Type': 'application/json; charset=UTF-8',\n+                'Referer': urlh.url,\n+                'X-Cltft-Token': cltft_token,\n+                'Accept': '*/*',\n+            }, data=json.dumps({\n+                'loginId': username,\n+                'loginPassword': password,\n+            }).encode())\n+        if not login_json.get('isSuccess'):\n+            raise ExtractorError('Login failed: Invalid id or password', expected=True)\n+\n+        self._request_webpage(\n+            urlh.url, None, note='Logging in', errnote='Unable to log in',\n+            data=urlencode_postdata({\n+                'loginId': username,\n+                'loginPassword': password,\n+                'Token.Default': cltft_token,\n+                'op': 'nextPage',\n+            }), headers={'Referer': urlh.url})\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage, urlh = self._download_webpage_handle(\n+            url, video_id, headers={'User-Agent': self._USER_AGENT})\n+        if urlh.url.startswith('https://live.eplus.jp/member/auth'):\n+            username, password = self._get_login_info()\n+            if not username:\n+                self.raise_login_required()\n+            self._login(username, password, urlh)\n+            webpage = self._download_webpage(\n+                url, video_id, headers={'User-Agent': self._USER_AGENT})\n+\n+        data_json = self._search_json(r'<script>\\s*var app\\s*=', webpage, 'data json', video_id)\n+\n+        if data_json.get('drm_mode') == 'ON':\n+            self.report_drm(video_id)\n+\n+        delivery_status = data_json.get('delivery_status')\n+        archive_mode = data_json.get('archive_mode')\n+        release_timestamp = try_call(lambda: unified_timestamp(data_json['event_datetime']) - 32400)\n+        release_timestamp_str = data_json.get('event_datetime_text')  # JST\n+\n+        self.write_debug(f'delivery_status = {delivery_status}, archive_mode = {archive_mode}')\n+\n+        if delivery_status == 'PREPARING':\n+            live_status = 'is_upcoming'\n+        elif delivery_status == 'STARTED':\n+            live_status = 'is_live'\n+        elif delivery_status == 'STOPPED':\n+            if archive_mode != 'ON':\n+                raise ExtractorError(\n+                    'This event has ended and there is no archive for this event', expected=True)\n+            live_status = 'post_live'\n+        elif delivery_status == 'WAIT_CONFIRM_ARCHIVED':\n+            live_status = 'post_live'\n+        elif delivery_status == 'CONFIRMED_ARCHIVE':\n+            live_status = 'was_live'\n+        else:\n+            self.report_warning(f'Unknown delivery_status {delivery_status}, treat it as a live')\n+            live_status = 'is_live'\n+\n+        formats = []\n+\n+        m3u8_playlist_urls = self._search_json(\n+            r'var\\s+listChannels\\s*=', webpage, 'hls URLs', video_id, contains_pattern=r'\\[.+\\]', default=[])\n+        if not m3u8_playlist_urls:\n+            if live_status == 'is_upcoming':\n+                self.raise_no_formats(\n+                    f'Could not find the playlist URL. This live event will begin at {release_timestamp_str} JST', expected=True)\n+            else:\n+                self.raise_no_formats(\n+                    'Could not find the playlist URL. This event may not be accessible', expected=True)\n+        elif live_status == 'is_upcoming':\n+            self.raise_no_formats(f'This live event will begin at {release_timestamp_str} JST', expected=True)\n+        elif live_status == 'post_live':\n+            self.raise_no_formats('This event has ended, and the archive will be available shortly', expected=True)\n+        else:\n+            for m3u8_playlist_url in m3u8_playlist_urls:\n+                formats.extend(self._extract_m3u8_formats(m3u8_playlist_url, video_id))\n+            # FIXME: HTTP request headers need to be updated to continue download\n+            warning = 'Due to technical limitations, the download will be interrupted after one hour'\n+            if live_status == 'is_live':\n+                self.report_warning(warning)\n+            elif live_status == 'was_live':\n+                self.report_warning(f'{warning}. You can restart to continue the download')\n+\n+        return {\n+            'id': data_json['app_id'],\n+            'title': data_json.get('app_name'),\n+            'formats': formats,\n+            'live_status': live_status,\n+            'description': data_json.get('content'),\n+            'release_timestamp': release_timestamp,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/erocast.py",
            "diff": "diff --git a/yt_dlp/extractor/erocast.py b/yt_dlp/extractor/erocast.py\nnew file mode 100644\nindex 00000000..92a57536\n--- /dev/null\n+++ b/yt_dlp/extractor/erocast.py\n@@ -0,0 +1,63 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    int_or_none,\n+    parse_iso8601,\n+    str_or_none,\n+    traverse_obj,\n+    url_or_none,\n+)\n+\n+\n+class ErocastIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?erocast\\.me/track/(?P<id>[0-9]+)'\n+    _TESTS = [{\n+        'url': 'https://erocast.me/track/9787/f',\n+        'md5': 'af63b91f5f231096aba54dd682abea3b',\n+        'info_dict': {\n+            'id': '9787',\n+            'title': '[F4M] Your roommate, who is definitely not possessed by an alien, suddenly wants to fuck you',\n+            'url': 'https://erocast.s3.us-east-2.wasabisys.com/1220419/track.m3u8',\n+            'ext': 'm4a',\n+            'age_limit': 18,\n+            'release_timestamp': 1696178652,\n+            'release_date': '20231001',\n+            'modified_timestamp': int,\n+            'modified_date': str,\n+            'description': 'ExtraTerrestrial Tuesday!',\n+            'uploader': 'clarissaisshy',\n+            'uploader_id': '8113',\n+            'uploader_url': 'https://erocast.me/clarissaisshy',\n+            'thumbnail': 'https://erocast.s3.us-east-2.wasabisys.com/1220418/conversions/1696179247-lg.jpg',\n+            'duration': 2307,\n+            'view_count': int,\n+            'comment_count': int,\n+            'webpage_url': 'https://erocast.me/track/9787/f4m-your-roommate-who-is-definitely-not-possessed-by-an-alien-suddenly-wants-to-fuck-you',\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+        data = self._search_json(\n+            rf'<script>\\s*var song_data_{video_id}\\s*=', webpage, 'data', video_id, end_pattern=r'</script>')\n+\n+        return {\n+            'id': video_id,\n+            'formats': self._extract_m3u8_formats(\n+                data.get('file_url') or data['stream_url'], video_id, 'm4a', m3u8_id='hls'),\n+            'age_limit': 18,\n+            **traverse_obj(data, {\n+                'title': ('title', {str}),\n+                'description': ('description', {str}),\n+                'release_timestamp': ('created_at', {parse_iso8601}),\n+                'modified_timestamp': ('updated_at', {parse_iso8601}),\n+                'uploader': ('user', 'name', {str}),\n+                'uploader_id': ('user', 'id', {str_or_none}),\n+                'uploader_url': ('user', 'permalink_url', {url_or_none}),\n+                'thumbnail': ('artwork_url', {url_or_none}),\n+                'duration': ('duration', {int_or_none}),\n+                'view_count': ('plays', {int_or_none}),\n+                'comment_count': ('comment_count', {int_or_none}),\n+                'webpage_url': ('permalink_url', {url_or_none}),\n+            }),\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/escapist.py",
            "diff": "diff --git a/yt_dlp/extractor/escapist.py b/yt_dlp/extractor/escapist.py\ndeleted file mode 100644\nindex 85a1cbf4..00000000\n--- a/yt_dlp/extractor/escapist.py\n+++ /dev/null\n@@ -1,108 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    determine_ext,\n-    clean_html,\n-    int_or_none,\n-    float_or_none,\n-)\n-\n-\n-def _decrypt_config(key, string):\n-    a = ''\n-    i = ''\n-    r = ''\n-\n-    while len(a) < (len(string) / 2):\n-        a += key\n-\n-    a = a[0:int(len(string) / 2)]\n-\n-    t = 0\n-    while t < len(string):\n-        i += chr(int(string[t] + string[t + 1], 16))\n-        t += 2\n-\n-    icko = [s for s in i]\n-\n-    for t, c in enumerate(a):\n-        r += chr(ord(c) ^ ord(icko[t]))\n-\n-    return r\n-\n-\n-class EscapistIE(InfoExtractor):\n-    _VALID_URL = r'https?://?(?:(?:www|v1)\\.)?escapistmagazine\\.com/videos/view/[^/]+/(?P<id>[0-9]+)'\n-    _TESTS = [{\n-        'url': 'http://www.escapistmagazine.com/videos/view/the-escapist-presents/6618-Breaking-Down-Baldurs-Gate',\n-        'md5': 'ab3a706c681efca53f0a35f1415cf0d1',\n-        'info_dict': {\n-            'id': '6618',\n-            'ext': 'mp4',\n-            'description': \"Baldur's Gate: Original, Modded or Enhanced Edition? I'll break down what you can expect from the new Baldur's Gate: Enhanced Edition.\",\n-            'title': \"Breaking Down Baldur's Gate\",\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 264,\n-            'uploader': 'The Escapist',\n-        }\n-    }, {\n-        'url': 'http://www.escapistmagazine.com/videos/view/zero-punctuation/10044-Evolve-One-vs-Multiplayer',\n-        'md5': '9e8c437b0dbb0387d3bd3255ca77f6bf',\n-        'info_dict': {\n-            'id': '10044',\n-            'ext': 'mp4',\n-            'description': 'This week, Zero Punctuation reviews Evolve.',\n-            'title': 'Evolve - One vs Multiplayer',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 304,\n-            'uploader': 'The Escapist',\n-        }\n-    }, {\n-        'url': 'http://escapistmagazine.com/videos/view/the-escapist-presents/6618',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://v1.escapistmagazine.com/videos/view/the-escapist-presents/6618-Breaking-Down-Baldurs-Gate',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        ims_video = self._parse_json(\n-            self._search_regex(\n-                r'imsVideo\\.play\\(({.+?})\\);', webpage, 'imsVideo'),\n-            video_id)\n-        video_id = ims_video['videoID']\n-        key = ims_video['hash']\n-\n-        config = self._download_webpage(\n-            'http://www.escapistmagazine.com/videos/vidconfig.php',\n-            video_id, 'Downloading video config', headers={\n-                'Referer': url,\n-            }, query={\n-                'videoID': video_id,\n-                'hash': key,\n-            })\n-\n-        data = self._parse_json(_decrypt_config(key, config), video_id)\n-\n-        video_data = data['videoData']\n-\n-        title = clean_html(video_data['title'])\n-\n-        formats = [{\n-            'url': video['src'],\n-            'format_id': '%s-%sp' % (determine_ext(video['src']), video['res']),\n-            'height': int_or_none(video.get('res')),\n-        } for video in data['files']['videos']]\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            'title': title,\n-            'thumbnail': self._og_search_thumbnail(webpage) or data.get('poster'),\n-            'description': self._og_search_description(webpage),\n-            'duration': float_or_none(video_data.get('duration'), 1000),\n-            'uploader': video_data.get('publisher'),\n-            'series': video_data.get('show'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/esri.py",
            "diff": "diff --git a/yt_dlp/extractor/esri.py b/yt_dlp/extractor/esri.py\ndeleted file mode 100644\nindex 02e7efaf..00000000\n--- a/yt_dlp/extractor/esri.py\n+++ /dev/null\n@@ -1,70 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_urlparse\n-from ..utils import (\n-    int_or_none,\n-    parse_filesize,\n-    unified_strdate,\n-)\n-\n-\n-class EsriVideoIE(InfoExtractor):\n-    _VALID_URL = r'https?://video\\.esri\\.com/watch/(?P<id>[0-9]+)'\n-    _TEST = {\n-        'url': 'https://video.esri.com/watch/1124/arcgis-online-_dash_-developing-applications',\n-        'md5': 'd4aaf1408b221f1b38227a9bbaeb95bc',\n-        'info_dict': {\n-            'id': '1124',\n-            'ext': 'mp4',\n-            'title': 'ArcGIS Online - Developing Applications',\n-            'description': 'Jeremy Bartley demonstrates how to develop applications with ArcGIS Online.',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 185,\n-            'upload_date': '20120419',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        formats = []\n-        for width, height, content in re.findall(\n-                r'(?s)<li><strong>(\\d+)x(\\d+):</strong>(.+?)</li>', webpage):\n-            for video_url, ext, filesize in re.findall(\n-                    r'<a[^>]+href=\"([^\"]+)\">([^<]+)&nbsp;\\(([^<]+)\\)</a>', content):\n-                formats.append({\n-                    'url': compat_urlparse.urljoin(url, video_url),\n-                    'ext': ext.lower(),\n-                    'format_id': '%s-%s' % (ext.lower(), height),\n-                    'width': int(width),\n-                    'height': int(height),\n-                    'filesize_approx': parse_filesize(filesize),\n-                })\n-\n-        title = self._html_search_meta('title', webpage, 'title')\n-        description = self._html_search_meta(\n-            'description', webpage, 'description', fatal=False)\n-\n-        thumbnail = self._html_search_meta('thumbnail', webpage, 'thumbnail', fatal=False)\n-        if thumbnail:\n-            thumbnail = re.sub(r'_[st]\\.jpg$', '_x.jpg', thumbnail)\n-\n-        duration = int_or_none(self._search_regex(\n-            [r'var\\s+videoSeconds\\s*=\\s*(\\d+)', r\"'duration'\\s*:\\s*(\\d+)\"],\n-            webpage, 'duration', fatal=False))\n-\n-        upload_date = unified_strdate(self._html_search_meta(\n-            'last-modified', webpage, 'upload date', fatal=False))\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'upload_date': upload_date,\n-            'formats': formats\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/expotv.py",
            "diff": "diff --git a/yt_dlp/extractor/expotv.py b/yt_dlp/extractor/expotv.py\ndeleted file mode 100644\nindex bda6e3cb..00000000\n--- a/yt_dlp/extractor/expotv.py\n+++ /dev/null\n@@ -1,74 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    unified_strdate,\n-)\n-\n-\n-class ExpoTVIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?expotv\\.com/videos/[^?#]*/(?P<id>[0-9]+)($|[?#])'\n-    _TEST = {\n-        'url': 'http://www.expotv.com/videos/reviews/3/40/NYX-Butter-lipstick/667916',\n-        'md5': 'fe1d728c3a813ff78f595bc8b7a707a8',\n-        'info_dict': {\n-            'id': '667916',\n-            'ext': 'mp4',\n-            'title': 'NYX Butter Lipstick Little Susie',\n-            'description': 'Goes on like butter, but looks better!',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'uploader': 'Stephanie S.',\n-            'upload_date': '20150520',\n-            'view_count': int,\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-        player_key = self._search_regex(\n-            r'<param name=\"playerKey\" value=\"([^\"]+)\"', webpage, 'player key')\n-        config = self._download_json(\n-            'http://client.expotv.com/video/config/%s/%s' % (video_id, player_key),\n-            video_id, 'Downloading video configuration')\n-\n-        formats = []\n-        for fcfg in config['sources']:\n-            media_url = fcfg.get('file')\n-            if not media_url:\n-                continue\n-            if fcfg.get('type') == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    media_url, video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls'))\n-            else:\n-                formats.append({\n-                    'url': media_url,\n-                    'height': int_or_none(fcfg.get('height')),\n-                    'format_id': fcfg.get('label'),\n-                    'ext': self._search_regex(\n-                        r'filename=.*\\.([a-z0-9_A-Z]+)&', media_url,\n-                        'file extension', default=None) or fcfg.get('type'),\n-                })\n-\n-        title = self._og_search_title(webpage)\n-        description = self._og_search_description(webpage)\n-        thumbnail = config.get('image')\n-        view_count = int_or_none(self._search_regex(\n-            r'<h5>Plays: ([0-9]+)</h5>', webpage, 'view counts'))\n-        uploader = self._search_regex(\n-            r'<div class=\"reviewer\">\\s*<img alt=\"([^\"]+)\"', webpage, 'uploader',\n-            fatal=False)\n-        upload_date = unified_strdate(self._search_regex(\n-            r'<h5>Reviewed on ([0-9/.]+)</h5>', webpage, 'upload date',\n-            fatal=False), day_first=False)\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            'title': title,\n-            'description': description,\n-            'view_count': view_count,\n-            'thumbnail': thumbnail,\n-            'uploader': uploader,\n-            'upload_date': upload_date,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/expressen.py",
            "diff": "diff --git a/yt_dlp/extractor/expressen.py b/yt_dlp/extractor/expressen.py\nindex 86967b63..b96f2e4c 100644\n--- a/yt_dlp/extractor/expressen.py\n+++ b/yt_dlp/extractor/expressen.py\n@@ -11,8 +11,8 @@ class ExpressenIE(InfoExtractor):\n     _VALID_URL = r'''(?x)\n                     https?://\n                         (?:www\\.)?(?:expressen|di)\\.se/\n-                        (?:(?:tvspelare/video|videoplayer/embed)/)?\n-                        tv/(?:[^/]+/)*\n+                        (?:(?:tvspelare/video|video-?player/embed)/)?\n+                        (?:tv|nyheter)/(?:[^/?#]+/)*\n                         (?P<id>[^/?#&]+)\n                     '''\n     _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?(?:expressen|di)\\.se/(?:tvspelare/video|videoplayer/embed)/tv/.+?)\\1']\n@@ -42,6 +42,12 @@ class ExpressenIE(InfoExtractor):\n     }, {\n         'url': 'https://www.di.se/videoplayer/embed/tv/ditv/borsmorgon/implantica-rusar-70--under-borspremiaren-hor-styrelsemedlemmen/?embed=true&external=true&autoplay=true&startVolume=0&partnerId=di',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://www.expressen.se/video-player/embed/tv/nyheter/ekero-fodda-olof-gustafsson-forvaltar-knarkbaronen-pablo-escobars-namn',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.expressen.se/nyheter/efter-egna-telefonbluffen-escobar-stammer-klarna/',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/extremetube.py",
            "diff": "diff --git a/yt_dlp/extractor/extremetube.py b/yt_dlp/extractor/extremetube.py\ndeleted file mode 100644\nindex 2c196989..00000000\n--- a/yt_dlp/extractor/extremetube.py\n+++ /dev/null\n@@ -1,48 +0,0 @@\n-from ..utils import str_to_int\n-from .keezmovies import KeezMoviesIE\n-\n-\n-class ExtremeTubeIE(KeezMoviesIE):  # XXX: Do not subclass from concrete IE\n-    _VALID_URL = r'https?://(?:www\\.)?extremetube\\.com/(?:[^/]+/)?video/(?P<id>[^/#?&]+)'\n-    _TESTS = [{\n-        'url': 'http://www.extremetube.com/video/music-video-14-british-euro-brit-european-cumshots-swallow-652431',\n-        'md5': '92feaafa4b58e82f261e5419f39c60cb',\n-        'info_dict': {\n-            'id': 'music-video-14-british-euro-brit-european-cumshots-swallow-652431',\n-            'ext': 'mp4',\n-            'title': 'Music Video 14 british euro brit european cumshots swallow',\n-            'uploader': 'anonim',\n-            'view_count': int,\n-            'age_limit': 18,\n-        }\n-    }, {\n-        'url': 'http://www.extremetube.com/gay/video/abcde-1234',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.extremetube.com/video/latina-slut-fucked-by-fat-black-dick',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.extremetube.com/video/652431',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        webpage, info = self._extract_info(url)\n-\n-        if not info['title']:\n-            info['title'] = self._search_regex(\n-                r'<h1[^>]+title=\"([^\"]+)\"[^>]*>', webpage, 'title')\n-\n-        uploader = self._html_search_regex(\n-            r'Uploaded by:\\s*</[^>]+>\\s*<a[^>]+>(.+?)</a>',\n-            webpage, 'uploader', fatal=False)\n-        view_count = str_to_int(self._search_regex(\n-            r'Views:\\s*</[^>]+>\\s*<[^>]+>([\\d,\\.]+)</',\n-            webpage, 'view count', fatal=False))\n-\n-        info.update({\n-            'uploader': uploader,\n-            'view_count': view_count,\n-        })\n-\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/facebook.py",
            "diff": "diff --git a/yt_dlp/extractor/facebook.py b/yt_dlp/extractor/facebook.py\nindex 4fd17b57..a07a0d34 100644\n--- a/yt_dlp/extractor/facebook.py\n+++ b/yt_dlp/extractor/facebook.py\n@@ -16,6 +16,7 @@\n     determine_ext,\n     error_to_compat_str,\n     float_or_none,\n+    format_field,\n     get_element_by_id,\n     get_first,\n     int_or_none,\n@@ -51,7 +52,7 @@ class FacebookIE(InfoExtractor):\n                             )\\?(?:.*?)(?:v|video_id|story_fbid)=|\n                             [^/]+/videos/(?:[^/]+/)?|\n                             [^/]+/posts/|\n-                            groups/[^/]+/permalink/|\n+                            groups/[^/]+/(?:permalink|posts)/|\n                             watchparty/\n                         )|\n                     facebook:\n@@ -74,6 +75,22 @@ class FacebookIE(InfoExtractor):\n     _VIDEO_PAGE_TAHOE_TEMPLATE = 'https://www.facebook.com/video/tahoe/async/%s/?chain=true&isvideo=true&payloadtype=primary'\n \n     _TESTS = [{\n+        'url': 'https://www.facebook.com/radiokicksfm/videos/3676516585958356/',\n+        'info_dict': {\n+            'id': '3676516585958356',\n+            'ext': 'mp4',\n+            'title': 'dr Adam Przygoda',\n+            'description': 'md5:34675bda53336b1d16400265c2bb9b3b',\n+            'uploader': 'RADIO KICKS FM',\n+            'upload_date': '20230818',\n+            'timestamp': 1692346159,\n+            'thumbnail': r're:^https?://.*',\n+            'uploader_id': '100063551323670',\n+            'duration': 3132.184,\n+            'view_count': int,\n+            'concurrent_view_count': 0,\n+        },\n+    }, {\n         'url': 'https://www.facebook.com/video.php?v=637842556329505&fref=nf',\n         'md5': '6a40d33c0eccbb1af76cf0485a052659',\n         'info_dict': {\n@@ -97,7 +114,7 @@ class FacebookIE(InfoExtractor):\n             'upload_date': '20140506',\n             'timestamp': 1399398998,\n             'thumbnail': r're:^https?://.*',\n-            'uploader_id': 'pfbid04scW44U4P9iTyLZAGy8y8W3pR3i2VugvHCimiRudUAVbN3MPp9eXBaYFcgVworZwl',\n+            'uploader_id': 'pfbid028wxorhX2ErLFJ578N6P3crHD3PHmXTCqCvfBpsnbSLmbokwSY75p5hWBjHGkG4zxl',\n             'duration': 131.03,\n             'concurrent_view_count': int,\n         },\n@@ -179,7 +196,7 @@ class FacebookIE(InfoExtractor):\n             'timestamp': 1486648217,\n             'upload_date': '20170209',\n             'uploader': 'Yaroslav Korpan',\n-            'uploader_id': 'pfbid029y8j22EwH3ikeqgH3SEP9G3CAi9kmWKgXJJG9s5geV7mo3J2bvURqHCdgucRgAyhl',\n+            'uploader_id': 'pfbid06AScABAWcW91qpiuGrLt99Ef9tvwHoXP6t8KeFYEqkSfreMtfa9nTveh8b2ZEVSWl',\n             'concurrent_view_count': int,\n             'thumbnail': r're:^https?://.*',\n             'view_count': int,\n@@ -215,6 +232,21 @@ class FacebookIE(InfoExtractor):\n             'uploader_id': '100013949973717',\n         },\n         'skip': 'Requires logging in',\n+    }, {\n+        # data.node.comet_sections.content.story.attachments[].throwbackStyles.attachment_target_renderer.attachment.target.attachments[].styles.attachment.media\n+        'url': 'https://www.facebook.com/groups/1645456212344334/posts/3737828833107051/',\n+        'info_dict': {\n+            'id': '1569199726448814',\n+            'ext': 'mp4',\n+            'title': 'Pence MUST GO!',\n+            'description': 'Vickie Gentry shared a memory.',\n+            'timestamp': 1511548260,\n+            'upload_date': '20171124',\n+            'uploader': 'Vickie Gentry',\n+            'uploader_id': 'pfbid0FuZhHCeWDAxWxEbr3yKPFaRstXvRxgsp9uCPG6GjD4J2AitB35NUAuJ4Q75KcjiDl',\n+            'thumbnail': r're:^https?://.*',\n+            'duration': 148.435,\n+        },\n     }, {\n         'url': 'https://www.facebook.com/video.php?v=10204634152394104',\n         'only_matching': True,\n@@ -274,7 +306,7 @@ class FacebookIE(InfoExtractor):\n             'title': 'Josef',\n             'thumbnail': r're:^https?://.*',\n             'concurrent_view_count': int,\n-            'uploader_id': 'pfbid02gXHbDwxumkaKJQaTGUf3znYfYzTuidGEWawiramNx4YamSj2afwYSRkpcjtHtMRJl',\n+            'uploader_id': 'pfbid0cibUN6tV7DYgdbJdsUFN46wc4jKpVSPAvJQhFofGqBGmVn3V3JtAs2tfUwziw2hUl',\n             'timestamp': 1549275572,\n             'duration': 3.413,\n             'uploader': 'Josef Novak',\n@@ -401,9 +433,32 @@ def _extract_from_url(self, url, video_id):\n \n         def extract_metadata(webpage):\n             post_data = [self._parse_json(j, video_id, fatal=False) for j in re.findall(\n-                r'handleWithCustomApplyEach\\(\\s*ScheduledApplyEach\\s*,\\s*(\\{.+?\\})\\s*\\);', webpage)]\n+                r'data-sjs>({.*?ScheduledServerJS.*?})</script>', webpage)]\n             post = traverse_obj(post_data, (\n-                ..., 'require', ..., ..., ..., '__bbox', 'result', 'data'), expected_type=dict) or []\n+                ..., 'require', ..., ..., ..., '__bbox', 'require', ..., ..., ..., '__bbox', 'result', 'data'), expected_type=dict) or []\n+\n+            automatic_captions, subtitles = {}, {}\n+            subs_data = traverse_obj(post, (..., 'video', ..., 'attachments', ..., lambda k, v: (\n+                k == 'media' and str(v['id']) == video_id and v['__typename'] == 'Video')))\n+            is_video_broadcast = get_first(subs_data, 'is_video_broadcast', expected_type=bool)\n+            captions = get_first(subs_data, 'video_available_captions_locales', 'captions_url')\n+            if url_or_none(captions):  # if subs_data only had a 'captions_url'\n+                locale = self._html_search_meta(['og:locale', 'twitter:locale'], webpage, 'locale', default='en_US')\n+                subtitles[locale] = [{'url': captions}]\n+            # or else subs_data had 'video_available_captions_locales', a list of dicts\n+            for caption in traverse_obj(captions, (\n+                {lambda x: sorted(x, key=lambda c: c['locale'])}, lambda _, v: v['captions_url'])\n+            ):\n+                lang = caption.get('localized_language') or ''\n+                subs = {\n+                    'url': caption['captions_url'],\n+                    'name': format_field(caption, 'localized_country', f'{lang} (%s)', default=lang),\n+                }\n+                if caption.get('localized_creation_method') or is_video_broadcast:\n+                    automatic_captions.setdefault(caption['locale'], []).append(subs)\n+                else:\n+                    subtitles.setdefault(caption['locale'], []).append(subs)\n+\n             media = traverse_obj(post, (..., 'attachments', ..., lambda k, v: (\n                 k == 'media' and str(v['id']) == video_id and v['__typename'] == 'Video')), expected_type=dict)\n             title = get_first(media, ('title', 'text'))\n@@ -447,6 +502,8 @@ def extract_metadata(webpage):\n                     webpage, 'view count', default=None)),\n                 'concurrent_view_count': get_first(post, (\n                     ('video', (..., ..., 'attachments', ..., 'media')), 'liveViewerCount', {int_or_none})),\n+                'automatic_captions': automatic_captions,\n+                'subtitles': subtitles,\n             }\n \n             info_json_ld = self._search_json_ld(webpage, video_id, default={})\n@@ -489,18 +546,17 @@ def process_formats(info):\n             # with non-browser User-Agent.\n             for f in info['formats']:\n                 f.setdefault('http_headers', {})['User-Agent'] = 'facebookexternalhit/1.1'\n-            info['_format_sort_fields'] = ('res', 'quality')\n \n         def extract_relay_data(_filter):\n             return self._parse_json(self._search_regex(\n-                r'handleWithCustomApplyEach\\([^,]+,\\s*({.*?%s.*?})\\);' % _filter,\n+                r'data-sjs>({.*?%s.*?})</script>' % _filter,\n                 webpage, 'replay data', default='{}'), video_id, fatal=False) or {}\n \n         def extract_relay_prefetched_data(_filter):\n-            replay_data = extract_relay_data(_filter)\n-            for require in (replay_data.get('require') or []):\n-                if require[0] == 'RelayPrefetchedStreamCache':\n-                    return try_get(require, lambda x: x[3][1]['__bbox']['result']['data'], dict) or {}\n+            return traverse_obj(extract_relay_data(_filter), (\n+                'require', (None, (..., ..., ..., '__bbox', 'require')),\n+                lambda _, v: 'RelayPrefetchedStreamCache' in v, ..., ...,\n+                '__bbox', 'result', 'data', {dict}), get_all=False) or {}\n \n         if not video_data:\n             server_js_data = self._parse_json(self._search_regex([\n@@ -511,7 +567,7 @@ def extract_relay_prefetched_data(_filter):\n \n         if not video_data:\n             data = extract_relay_prefetched_data(\n-                r'\"(?:dash_manifest|playable_url(?:_quality_hd)?)\"\\s*:\\s*\"[^\"]+\"')\n+                r'\"(?:dash_manifest|playable_url(?:_quality_hd)?)')\n             if data:\n                 entries = []\n \n@@ -526,7 +582,8 @@ def parse_graphql_video(video):\n                     formats = []\n                     q = qualities(['sd', 'hd'])\n                     for key, format_id in (('playable_url', 'sd'), ('playable_url_quality_hd', 'hd'),\n-                                           ('playable_url_dash', '')):\n+                                           ('playable_url_dash', ''), ('browser_native_hd_url', 'hd'),\n+                                           ('browser_native_sd_url', 'sd')):\n                         playable_url = video.get(key)\n                         if not playable_url:\n                             continue\n@@ -535,7 +592,8 @@ def parse_graphql_video(video):\n                         else:\n                             formats.append({\n                                 'format_id': format_id,\n-                                'quality': q(format_id),\n+                                # sd, hd formats w/o resolution info should be deprioritized below DASH\n+                                'quality': q(format_id) - 3,\n                                 'url': playable_url,\n                             })\n                     extract_dash_manifest(video, formats)\n@@ -569,9 +627,11 @@ def parse_attachment(attachment, key='media'):\n                 nodes = variadic(traverse_obj(data, 'nodes', 'node') or [])\n                 attachments = traverse_obj(nodes, (\n                     ..., 'comet_sections', 'content', 'story', (None, 'attached_story'), 'attachments',\n-                    ..., ('styles', 'style_type_renderer'), 'attachment'), expected_type=dict) or []\n+                    ..., ('styles', 'style_type_renderer', ('throwbackStyles', 'attachment_target_renderer')),\n+                    'attachment', {dict}))\n                 for attachment in attachments:\n-                    ns = try_get(attachment, lambda x: x['all_subattachments']['nodes'], list) or []\n+                    ns = traverse_obj(attachment, ('all_subattachments', 'nodes', ..., {dict}),\n+                                      ('target', 'attachments', ..., 'styles', 'attachment', {dict}))\n                     for n in ns:\n                         parse_attachment(n)\n                     parse_attachment(attachment)\n@@ -594,7 +654,7 @@ def parse_attachment(attachment, key='media'):\n                 if len(entries) > 1:\n                     return self.playlist_result(entries, video_id)\n \n-                video_info = entries[0]\n+                video_info = entries[0] if entries else {'id': video_id}\n                 webpage_info = extract_metadata(webpage)\n                 # honor precise duration in video info\n                 if video_info.get('duration'):\n@@ -702,9 +762,11 @@ def parse_attachment(attachment, key='media'):\n                 for src_type in ('src', 'src_no_ratelimit'):\n                     src = f[0].get('%s_%s' % (quality, src_type))\n                     if src:\n-                        preference = -10 if format_id == 'progressive' else -1\n+                        # sd, hd formats w/o resolution info should be deprioritized below DASH\n+                        # TODO: investigate if progressive or src formats still exist\n+                        preference = -10 if format_id == 'progressive' else -3\n                         if quality == 'hd':\n-                            preference += 5\n+                            preference += 1\n                         formats.append({\n                             'format_id': '%s_%s_%s' % (format_id, quality, src_type),\n                             'url': src,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/fc2.py",
            "diff": "diff --git a/yt_dlp/extractor/fc2.py b/yt_dlp/extractor/fc2.py\nindex ba19b6ca..bbc4b569 100644\n--- a/yt_dlp/extractor/fc2.py\n+++ b/yt_dlp/extractor/fc2.py\n@@ -2,11 +2,9 @@\n \n from .common import InfoExtractor\n from ..compat import compat_parse_qs\n-from ..dependencies import websockets\n from ..networking import Request\n from ..utils import (\n     ExtractorError,\n-    WebSocketsWrapper,\n     js_to_json,\n     traverse_obj,\n     update_url_query,\n@@ -167,8 +165,6 @@ class FC2LiveIE(InfoExtractor):\n     }]\n \n     def _real_extract(self, url):\n-        if not websockets:\n-            raise ExtractorError('websockets library is not available. Please install it.', expected=True)\n         video_id = self._match_id(url)\n         webpage = self._download_webpage('https://live.fc2.com/%s/' % video_id, video_id)\n \n@@ -199,13 +195,9 @@ def _real_extract(self, url):\n         ws_url = update_url_query(control_server['url'], {'control_token': control_server['control_token']})\n         playlist_data = None\n \n-        self.to_screen('%s: Fetching HLS playlist info via WebSocket' % video_id)\n-        ws = WebSocketsWrapper(ws_url, {\n-            'Cookie': str(self._get_cookies('https://live.fc2.com/'))[12:],\n+        ws = self._request_webpage(Request(ws_url, headers={\n             'Origin': 'https://live.fc2.com',\n-            'Accept': '*/*',\n-            'User-Agent': self.get_param('http_headers')['User-Agent'],\n-        })\n+        }), video_id, note='Fetching HLS playlist info via WebSocket')\n \n         self.write_debug('Sending HLS server request')\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/fifa.py",
            "diff": "diff --git a/yt_dlp/extractor/fifa.py b/yt_dlp/extractor/fifa.py\nindex 8b4db3a8..f604cbd4 100644\n--- a/yt_dlp/extractor/fifa.py\n+++ b/yt_dlp/extractor/fifa.py\n@@ -8,7 +8,7 @@\n \n \n class FifaIE(InfoExtractor):\n-    _VALID_URL = r'https?://www.fifa.com/fifaplus/(?P<locale>\\w{2})/watch/([^#?]+/)?(?P<id>\\w+)'\n+    _VALID_URL = r'https?://www\\.fifa\\.com/fifaplus/(?P<locale>\\w{2})/watch/([^#?]+/)?(?P<id>\\w+)'\n     _TESTS = [{\n         'url': 'https://www.fifa.com/fifaplus/en/watch/7on10qPcnyLajDDU3ntg6y',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/filmmodu.py",
            "diff": "diff --git a/yt_dlp/extractor/filmmodu.py b/yt_dlp/extractor/filmmodu.py\nindex 9eb550ee..1e793560 100644\n--- a/yt_dlp/extractor/filmmodu.py\n+++ b/yt_dlp/extractor/filmmodu.py\n@@ -3,7 +3,7 @@\n \n \n class FilmmoduIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www.)?filmmodu.org/(?P<id>[^/]+-(?:turkce-dublaj-izle|altyazili-izle))'\n+    _VALID_URL = r'https?://(?:www\\.)?filmmodu\\.org/(?P<id>[^/]+-(?:turkce-dublaj-izle|altyazili-izle))'\n     _TESTS = [{\n         'url': 'https://www.filmmodu.org/f9-altyazili-izle',\n         'md5': 'aeefd955c2a508a5bdaa3bcec8eeb0d4',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/floatplane.py",
            "diff": "diff --git a/yt_dlp/extractor/floatplane.py b/yt_dlp/extractor/floatplane.py\nnew file mode 100644\nindex 00000000..2cf4d4e6\n--- /dev/null\n+++ b/yt_dlp/extractor/floatplane.py\n@@ -0,0 +1,268 @@\n+import functools\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    OnDemandPagedList,\n+    clean_html,\n+    determine_ext,\n+    format_field,\n+    int_or_none,\n+    join_nonempty,\n+    parse_codecs,\n+    parse_iso8601,\n+    urljoin,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class FloatplaneIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:(?:www|beta)\\.)?floatplane\\.com/post/(?P<id>\\w+)'\n+    _TESTS = [{\n+        'url': 'https://www.floatplane.com/post/2Yf3UedF7C',\n+        'info_dict': {\n+            'id': 'yuleLogLTT',\n+            'ext': 'mp4',\n+            'display_id': '2Yf3UedF7C',\n+            'title': '8K Yule Log Fireplace with Crackling Fire Sounds - 10 Hours',\n+            'description': 'md5:adf2970e0de1c5e3df447818bb0309f6',\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'duration': 36035,\n+            'comment_count': int,\n+            'like_count': int,\n+            'dislike_count': int,\n+            'release_date': '20191206',\n+            'release_timestamp': 1575657000,\n+            'uploader': 'LinusTechTips',\n+            'uploader_id': '59f94c0bdd241b70349eb72b',\n+            'uploader_url': 'https://www.floatplane.com/channel/linustechtips/home',\n+            'channel': 'Linus Tech Tips',\n+            'channel_id': '63fe42c309e691e4e36de93d',\n+            'channel_url': 'https://www.floatplane.com/channel/linustechtips/home/main',\n+            'availability': 'subscriber_only',\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://www.floatplane.com/post/j2jqG3JmgJ',\n+        'info_dict': {\n+            'id': 'j2jqG3JmgJ',\n+            'title': 'TJM: Does Anyone Care About Avatar: The Way of Water?',\n+            'description': 'md5:00bf17dc5733e4031e99b7fd6489f274',\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'comment_count': int,\n+            'like_count': int,\n+            'dislike_count': int,\n+            'release_timestamp': 1671915900,\n+            'release_date': '20221224',\n+            'uploader': 'LinusTechTips',\n+            'uploader_id': '59f94c0bdd241b70349eb72b',\n+            'uploader_url': 'https://www.floatplane.com/channel/linustechtips/home',\n+            'channel': \"They're Just Movies\",\n+            'channel_id': '64135f82fc76ab7f9fbdc876',\n+            'channel_url': 'https://www.floatplane.com/channel/linustechtips/home/tajm',\n+            'availability': 'subscriber_only',\n+        },\n+        'playlist_count': 2,\n+    }, {\n+        'url': 'https://www.floatplane.com/post/3tK2tInhoN',\n+        'info_dict': {\n+            'id': '3tK2tInhoN',\n+            'title': 'Extras - How Linus Communicates with Editors (Compensator 4)',\n+            'description': 'md5:83cd40aae1ce124df33769600c80ca5b',\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'comment_count': int,\n+            'like_count': int,\n+            'dislike_count': int,\n+            'release_timestamp': 1700529120,\n+            'release_date': '20231121',\n+            'uploader': 'LinusTechTips',\n+            'uploader_id': '59f94c0bdd241b70349eb72b',\n+            'uploader_url': 'https://www.floatplane.com/channel/linustechtips/home',\n+            'channel': 'FP Exclusives',\n+            'channel_id': '6413623f5b12cca228a28e78',\n+            'channel_url': 'https://www.floatplane.com/channel/linustechtips/home/fpexclusive',\n+            'availability': 'subscriber_only',\n+        },\n+        'playlist_count': 2,\n+    }, {\n+        'url': 'https://beta.floatplane.com/post/d870PEFXS1',\n+        'info_dict': {\n+            'id': 'bg9SuYKEww',\n+            'ext': 'mp4',\n+            'display_id': 'd870PEFXS1',\n+            'title': 'LCS Drama, TLOU 2 Remaster, Destiny 2 Player Count Drops, + More!',\n+            'description': 'md5:80d612dcabf41b17487afcbe303ec57d',\n+            'thumbnail': r're:^https?://.*\\.jpe?g$',\n+            'release_timestamp': 1700622000,\n+            'release_date': '20231122',\n+            'duration': 513,\n+            'like_count': int,\n+            'dislike_count': int,\n+            'comment_count': int,\n+            'uploader': 'LinusTechTips',\n+            'uploader_id': '59f94c0bdd241b70349eb72b',\n+            'uploader_url': 'https://www.floatplane.com/channel/linustechtips/home',\n+            'channel': 'GameLinked',\n+            'channel_id': '649dbade3540dbc3945eeda7',\n+            'channel_url': 'https://www.floatplane.com/channel/linustechtips/home/gamelinked',\n+            'availability': 'subscriber_only',\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }]\n+\n+    def _real_initialize(self):\n+        if not self._get_cookies('https://www.floatplane.com').get('sails.sid'):\n+            self.raise_login_required()\n+\n+    def _real_extract(self, url):\n+        post_id = self._match_id(url)\n+\n+        post_data = self._download_json(\n+            'https://www.floatplane.com/api/v3/content/post', post_id, query={'id': post_id},\n+            note='Downloading post data', errnote='Unable to download post data')\n+\n+        if not any(traverse_obj(post_data, ('metadata', ('hasVideo', 'hasAudio')))):\n+            raise ExtractorError('Post does not contain a video or audio track', expected=True)\n+\n+        items = []\n+        for media in traverse_obj(post_data, (('videoAttachments', 'audioAttachments'), ...)):\n+            media_id = media['id']\n+            media_typ = media.get('type') or 'video'\n+\n+            metadata = self._download_json(\n+                f'https://www.floatplane.com/api/v3/content/{media_typ}', media_id, query={'id': media_id},\n+                note=f'Downloading {media_typ} metadata')\n+\n+            stream = self._download_json(\n+                'https://www.floatplane.com/api/v2/cdn/delivery', media_id, query={\n+                    'type': 'vod' if media_typ == 'video' else 'aod',\n+                    'guid': metadata['guid']\n+                }, note=f'Downloading {media_typ} stream data')\n+\n+            path_template = traverse_obj(stream, ('resource', 'uri', {str}))\n+\n+            def format_path(params):\n+                path = path_template\n+                for i, val in (params or {}).items():\n+                    path = path.replace(f'{{qualityLevelParams.{i}}}', val)\n+                return path\n+\n+            formats = []\n+            for quality in traverse_obj(stream, ('resource', 'data', 'qualityLevels', ...)):\n+                url = urljoin(stream['cdn'], format_path(traverse_obj(\n+                    stream, ('resource', 'data', 'qualityLevelParams', quality['name']))))\n+                formats.append({\n+                    **traverse_obj(quality, {\n+                        'format_id': 'name',\n+                        'format_note': 'label',\n+                        'width': ('width', {int}),\n+                        'height': ('height', {int}),\n+                    }),\n+                    **parse_codecs(quality.get('codecs')),\n+                    'url': url,\n+                    'ext': determine_ext(url.partition('/chunk.m3u8')[0], 'mp4'),\n+                })\n+\n+            items.append({\n+                'id': media_id,\n+                **traverse_obj(metadata, {\n+                    'title': 'title',\n+                    'duration': ('duration', {int_or_none}),\n+                    'thumbnail': ('thumbnail', 'path'),\n+                }),\n+                'formats': formats,\n+            })\n+\n+        uploader_url = format_field(\n+            post_data, [('creator', 'urlname')], 'https://www.floatplane.com/channel/%s/home') or None\n+        channel_url = urljoin(f'{uploader_url}/', traverse_obj(post_data, ('channel', 'urlname')))\n+\n+        post_info = {\n+            'id': post_id,\n+            'display_id': post_id,\n+            **traverse_obj(post_data, {\n+                'title': 'title',\n+                'description': ('text', {clean_html}),\n+                'uploader': ('creator', 'title'),\n+                'uploader_id': ('creator', 'id'),\n+                'channel': ('channel', 'title'),\n+                'channel_id': ('channel', 'id'),\n+                'like_count': ('likes', {int_or_none}),\n+                'dislike_count': ('dislikes', {int_or_none}),\n+                'comment_count': ('comments', {int_or_none}),\n+                'release_timestamp': ('releaseDate', {parse_iso8601}),\n+                'thumbnail': ('thumbnail', 'path'),\n+            }),\n+            'uploader_url': uploader_url,\n+            'channel_url': channel_url,\n+            'availability': self._availability(needs_subscription=True),\n+        }\n+\n+        if len(items) > 1:\n+            return self.playlist_result(items, **post_info)\n+\n+        post_info.update(items[0])\n+        return post_info\n+\n+\n+class FloatplaneChannelIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:(?:www|beta)\\.)?floatplane\\.com/channel/(?P<id>[\\w-]+)/home(?:/(?P<channel>[\\w-]+))?'\n+    _PAGE_SIZE = 20\n+    _TESTS = [{\n+        'url': 'https://www.floatplane.com/channel/linustechtips/home/ltxexpo',\n+        'info_dict': {\n+            'id': 'linustechtips/ltxexpo',\n+            'title': 'LTX Expo',\n+            'description': 'md5:9819002f9ebe7fd7c75a3a1d38a59149',\n+        },\n+        'playlist_mincount': 51,\n+    }, {\n+        'url': 'https://www.floatplane.com/channel/ShankMods/home',\n+        'info_dict': {\n+            'id': 'ShankMods',\n+            'title': 'Shank Mods',\n+            'description': 'md5:6dff1bb07cad8e5448e04daad9be1b30',\n+        },\n+        'playlist_mincount': 14,\n+    }, {\n+        'url': 'https://beta.floatplane.com/channel/bitwit_ultra/home',\n+        'info_dict': {\n+            'id': 'bitwit_ultra',\n+            'title': 'Bitwit Ultra',\n+            'description': 'md5:1452f280bb45962976d4789200f676dd',\n+        },\n+        'playlist_mincount': 200,\n+    }]\n+\n+    def _fetch_page(self, display_id, creator_id, channel_id, page):\n+        query = {\n+            'id': creator_id,\n+            'limit': self._PAGE_SIZE,\n+            'fetchAfter': page * self._PAGE_SIZE,\n+        }\n+        if channel_id:\n+            query['channel'] = channel_id\n+        page_data = self._download_json(\n+            'https://www.floatplane.com/api/v3/content/creator', display_id,\n+            query=query, note=f'Downloading page {page + 1}')\n+        for post in page_data or []:\n+            yield self.url_result(\n+                f'https://www.floatplane.com/post/{post[\"id\"]}',\n+                FloatplaneIE, id=post['id'], title=post.get('title'),\n+                release_timestamp=parse_iso8601(post.get('releaseDate')))\n+\n+    def _real_extract(self, url):\n+        creator, channel = self._match_valid_url(url).group('id', 'channel')\n+        display_id = join_nonempty(creator, channel, delim='/')\n+\n+        creator_data = self._download_json(\n+            'https://www.floatplane.com/api/v3/creator/named',\n+            display_id, query={'creatorURL[0]': creator})[0]\n+\n+        channel_data = traverse_obj(\n+            creator_data, ('channels', lambda _, v: v['urlname'] == channel), get_all=False) or {}\n+\n+        return self.playlist_result(OnDemandPagedList(functools.partial(\n+            self._fetch_page, display_id, creator_data['id'], channel_data.get('id')), self._PAGE_SIZE),\n+            display_id, title=channel_data.get('title') or creator_data.get('title'),\n+            description=channel_data.get('about') or creator_data.get('about'))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/fourzerostudio.py",
            "diff": "diff --git a/yt_dlp/extractor/fourzerostudio.py b/yt_dlp/extractor/fourzerostudio.py\ndeleted file mode 100644\nindex c388a3a0..00000000\n--- a/yt_dlp/extractor/fourzerostudio.py\n+++ /dev/null\n@@ -1,106 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import traverse_obj, unified_timestamp\n-\n-\n-class FourZeroStudioArchiveIE(InfoExtractor):\n-    _VALID_URL = r'https?://0000\\.studio/(?P<uploader_id>[^/]+)/broadcasts/(?P<id>[^/]+)/archive'\n-    IE_NAME = '0000studio:archive'\n-    _TESTS = [{\n-        'url': 'https://0000.studio/mumeijiten/broadcasts/1290f433-fce0-4909-a24a-5f7df09665dc/archive',\n-        'info_dict': {\n-            'id': '1290f433-fce0-4909-a24a-5f7df09665dc',\n-            'title': 'note\u3067\u300ecanape\u300f\u69d8\u3078\u306e\u30d5\u30a1\u30f3\u30ec\u30bf\u30fc\u3092\u57f7\u7b46\u3057\u307e\u3059\u3002\uff08\u6570\u79d8\u8853\u305d\u306e2\uff09',\n-            'timestamp': 1653802534,\n-            'release_timestamp': 1653796604,\n-            'thumbnails': 'count:1',\n-            'comments': 'count:7',\n-            'uploader': '\u300e\u4e2d\u5d0e\u96c4\u5fc3\u300f\u306e\u57f7\u52d9\u5ba4\u3002',\n-            'uploader_id': 'mumeijiten',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id, uploader_id = self._match_valid_url(url).group('id', 'uploader_id')\n-        webpage = self._download_webpage(url, video_id)\n-        nuxt_data = self._search_nuxt_data(webpage, video_id, traverse=None)\n-\n-        pcb = traverse_obj(nuxt_data, ('ssrRefs', lambda _, v: v['__typename'] == 'PublicCreatorBroadcast'), get_all=False)\n-        uploader_internal_id = traverse_obj(nuxt_data, (\n-            'ssrRefs', lambda _, v: v['__typename'] == 'PublicUser', 'id'), get_all=False)\n-\n-        formats, subs = self._extract_m3u8_formats_and_subtitles(pcb['archiveUrl'], video_id, ext='mp4')\n-\n-        return {\n-            'id': video_id,\n-            'title': pcb.get('title'),\n-            'age_limit': 18 if pcb.get('isAdult') else None,\n-            'timestamp': unified_timestamp(pcb.get('finishTime')),\n-            'release_timestamp': unified_timestamp(pcb.get('createdAt')),\n-            'thumbnails': [{\n-                'url': pcb['thumbnailUrl'],\n-                'ext': 'png',\n-            }] if pcb.get('thumbnailUrl') else None,\n-            'formats': formats,\n-            'subtitles': subs,\n-            'comments': [{\n-                'author': c.get('username'),\n-                'author_id': c.get('postedUserId'),\n-                'author_thumbnail': c.get('userThumbnailUrl'),\n-                'id': c.get('id'),\n-                'text': c.get('body'),\n-                'timestamp': unified_timestamp(c.get('createdAt')),\n-                'like_count': c.get('likeCount'),\n-                'is_favorited': c.get('isLikedByOwner'),\n-                'author_is_uploader': c.get('postedUserId') == uploader_internal_id,\n-            } for c in traverse_obj(nuxt_data, (\n-                'ssrRefs', ..., lambda _, v: v['__typename'] == 'PublicCreatorBroadcastComment')) or []],\n-            'uploader_id': uploader_id,\n-            'uploader': traverse_obj(nuxt_data, (\n-                'ssrRefs', lambda _, v: v['__typename'] == 'PublicUser', 'username'), get_all=False),\n-        }\n-\n-\n-class FourZeroStudioClipIE(InfoExtractor):\n-    _VALID_URL = r'https?://0000\\.studio/(?P<uploader_id>[^/]+)/archive-clip/(?P<id>[^/]+)'\n-    IE_NAME = '0000studio:clip'\n-    _TESTS = [{\n-        'url': 'https://0000.studio/soeji/archive-clip/e46b0278-24cd-40a8-92e1-b8fc2b21f34f',\n-        'info_dict': {\n-            'id': 'e46b0278-24cd-40a8-92e1-b8fc2b21f34f',\n-            'title': '\u308f\u305f\u30d9\u30fc\u3055\u3093\u304b\u3089\u30a4\u30e9\u30b9\u30c8\u5dee\u3057\u5165\u308c\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01',\n-            'timestamp': 1652109105,\n-            'like_count': 1,\n-            'uploader': '\u30bd\u30a8\u30b8\u30de\u30b1\u30a4\u30bf',\n-            'uploader_id': 'soeji',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id, uploader_id = self._match_valid_url(url).group('id', 'uploader_id')\n-        webpage = self._download_webpage(url, video_id)\n-        nuxt_data = self._search_nuxt_data(webpage, video_id, traverse=None)\n-\n-        clip_info = traverse_obj(nuxt_data, ('ssrRefs', lambda _, v: v['__typename'] == 'PublicCreatorArchivedClip'), get_all=False)\n-\n-        info = next((\n-            m for m in self._parse_html5_media_entries(url, webpage, video_id)\n-            if 'mp4' in traverse_obj(m, ('formats', ..., 'ext'))\n-        ), None)\n-        if not info:\n-            self.report_warning('Failed to find a desired media element. Falling back to using NUXT data.')\n-            info = {\n-                'formats': [{\n-                    'ext': 'mp4',\n-                    'url': url,\n-                } for url in clip_info.get('mediaFiles') or [] if url],\n-            }\n-        return {\n-            **info,\n-            'id': video_id,\n-            'title': clip_info.get('clipComment'),\n-            'timestamp': unified_timestamp(clip_info.get('createdAt')),\n-            'like_count': clip_info.get('likeCount'),\n-            'uploader_id': uploader_id,\n-            'uploader': traverse_obj(nuxt_data, (\n-                'ssrRefs', lambda _, v: v['__typename'] == 'PublicUser', 'username'), get_all=False),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/foxgay.py",
            "diff": "diff --git a/yt_dlp/extractor/foxgay.py b/yt_dlp/extractor/foxgay.py\ndeleted file mode 100644\nindex f4f29c65..00000000\n--- a/yt_dlp/extractor/foxgay.py\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-import itertools\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    get_element_by_id,\n-    int_or_none,\n-    remove_end,\n-)\n-\n-\n-class FoxgayIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?foxgay\\.com/videos/(?:\\S+-)?(?P<id>\\d+)\\.shtml'\n-    _TEST = {\n-        'url': 'http://foxgay.com/videos/fuck-turkish-style-2582.shtml',\n-        'md5': '344558ccfea74d33b7adbce22e577f54',\n-        'info_dict': {\n-            'id': '2582',\n-            'ext': 'mp4',\n-            'title': 'Fuck Turkish-style',\n-            'description': 'md5:6ae2d9486921891efe89231ace13ffdf',\n-            'age_limit': 18,\n-            'thumbnail': r're:https?://.*\\.jpg$',\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = remove_end(self._html_extract_title(webpage), ' - Foxgay.com')\n-        description = get_element_by_id('inf_tit', webpage)\n-\n-        # The default user-agent with foxgay cookies leads to pages without videos\n-        self.cookiejar.clear('.foxgay.com')\n-        # Find the URL for the iFrame which contains the actual video.\n-        iframe_url = self._html_search_regex(\n-            r'<iframe[^>]+src=([\\'\"])(?P<url>[^\\'\"]+)\\1', webpage,\n-            'video frame', group='url')\n-        iframe = self._download_webpage(\n-            iframe_url, video_id, headers={'User-Agent': 'curl/7.50.1'},\n-            note='Downloading video frame')\n-        video_data = self._parse_json(self._search_regex(\n-            r'video_data\\s*=\\s*([^;]+);', iframe, 'video data'), video_id)\n-\n-        formats = [{\n-            'url': source,\n-            'height': int_or_none(resolution),\n-        } for source, resolution in zip(\n-            video_data['sources'], video_data.get('resolutions', itertools.repeat(None)))]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'description': description,\n-            'thumbnail': video_data.get('act_vid', {}).get('thumb'),\n-            'age_limit': 18,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/francetv.py",
            "diff": "diff --git a/yt_dlp/extractor/francetv.py b/yt_dlp/extractor/francetv.py\nindex 05231720..0ceecde7 100644\n--- a/yt_dlp/extractor/francetv.py\n+++ b/yt_dlp/extractor/francetv.py\n@@ -1,12 +1,14 @@\n from .common import InfoExtractor\n+from .dailymotion import DailymotionIE\n from ..utils import (\n-    determine_ext,\n     ExtractorError,\n+    determine_ext,\n     format_field,\n+    int_or_none,\n+    join_nonempty,\n     parse_iso8601,\n     parse_qs,\n )\n-from .dailymotion import DailymotionIE\n \n \n class FranceTVBaseInfoExtractor(InfoExtractor):\n@@ -82,6 +84,8 @@ def _extract_video(self, video_id, catalogue=None):\n         videos = []\n         title = None\n         subtitle = None\n+        episode_number = None\n+        season_number = None\n         image = None\n         duration = None\n         timestamp = None\n@@ -112,7 +116,9 @@ def _extract_video(self, video_id, catalogue=None):\n             if meta:\n                 if title is None:\n                     title = meta.get('title')\n-                # XXX: what is meta['pre_title']?\n+                # meta['pre_title'] contains season and episode number for series in format \"S<ID> E<ID>\"\n+                season_number, episode_number = self._search_regex(\n+                    r'S(\\d+)\\s*E(\\d+)', meta.get('pre_title'), 'episode info', group=(1, 2), default=(None, None))\n                 if subtitle is None:\n                     subtitle = meta.get('additional_title')\n                 if image is None:\n@@ -191,19 +197,19 @@ def _extract_video(self, video_id, catalogue=None):\n                 } for sheet in spritesheets]\n             })\n \n-        if subtitle:\n-            title += ' - %s' % subtitle\n-        title = title.strip()\n-\n         return {\n             'id': video_id,\n-            'title': title,\n+            'title': join_nonempty(title, subtitle, delim=' - ').strip(),\n             'thumbnail': image,\n             'duration': duration,\n             'timestamp': timestamp,\n             'is_live': is_live,\n             'formats': formats,\n             'subtitles': subtitles,\n+            'episode': subtitle if episode_number else None,\n+            'series': title if episode_number else None,\n+            'episode_number': int_or_none(episode_number),\n+            'season_number': int_or_none(season_number),\n         }\n \n     def _real_extract(self, url):\n@@ -230,14 +236,31 @@ class FranceTVSiteIE(FranceTVBaseInfoExtractor):\n             'id': 'ec217ecc-0733-48cf-ac06-af1347b849d1',\n             'ext': 'mp4',\n             'title': '13h15, le dimanche... - Les myst\u00e8res de J\u00e9sus',\n-            'description': 'md5:75efe8d4c0a8205e5904498ffe1e1a42',\n             'timestamp': 1502623500,\n+            'duration': 2580,\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n             'upload_date': '20170813',\n         },\n         'params': {\n             'skip_download': True,\n         },\n         'add_ie': [FranceTVIE.ie_key()],\n+    }, {\n+        'url': 'https://www.france.tv/enfants/six-huit-ans/foot2rue/saison-1/3066387-duel-au-vieux-port.html',\n+        'info_dict': {\n+            'id': 'a9050959-eedd-4b4a-9b0d-de6eeaa73e44',\n+            'ext': 'mp4',\n+            'title': 'Foot2Rue - Duel au vieux port',\n+            'episode': 'Duel au vieux port',\n+            'series': 'Foot2Rue',\n+            'episode_number': 1,\n+            'season_number': 1,\n+            'timestamp': 1642761360,\n+            'upload_date': '20220121',\n+            'season': 'Season 1',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'duration': 1441,\n+        },\n     }, {\n         # france3\n         'url': 'https://www.france.tv/france-3/des-chiffres-et-des-lettres/139063-emission-du-mardi-9-mai-2017.html',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/funker530.py",
            "diff": "diff --git a/yt_dlp/extractor/funker530.py b/yt_dlp/extractor/funker530.py\nindex ba5ab7d4..62fd7f6d 100644\n--- a/yt_dlp/extractor/funker530.py\n+++ b/yt_dlp/extractor/funker530.py\n@@ -60,6 +60,7 @@ class Funker530IE(InfoExtractor):\n     def _real_extract(self, url):\n         display_id = self._match_id(url)\n         webpage = self._download_webpage(url, display_id)\n+        info = {}\n         rumble_url = list(RumbleEmbedIE._extract_embed_urls(url, webpage))\n         if rumble_url:\n             info = {'url': rumble_url[0], 'ie_key': RumbleEmbedIE.ie_key()}\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/fusion.py",
            "diff": "diff --git a/yt_dlp/extractor/fusion.py b/yt_dlp/extractor/fusion.py\ndeleted file mode 100644\nindex 689422fc..00000000\n--- a/yt_dlp/extractor/fusion.py\n+++ /dev/null\n@@ -1,81 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    determine_ext,\n-    int_or_none,\n-    mimetype2ext,\n-    parse_iso8601,\n-)\n-\n-\n-class FusionIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?fusion\\.(?:net|tv)/(?:video/|show/.+?\\bvideo=)(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'http://fusion.tv/video/201781/u-s-and-panamanian-forces-work-together-to-stop-a-vessel-smuggling-drugs/',\n-        'info_dict': {\n-            'id': '3145868',\n-            'ext': 'mp4',\n-            'title': 'U.S. and Panamanian forces work together to stop a vessel smuggling drugs',\n-            'description': 'md5:0cc84a9943c064c0f46b128b41b1b0d7',\n-            'duration': 140.0,\n-            'timestamp': 1442589635,\n-            'uploader': 'UNIVISON',\n-            'upload_date': '20150918',\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'add_ie': ['Anvato'],\n-    }, {\n-        'url': 'http://fusion.tv/video/201781',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://fusion.tv/show/food-exposed-with-nelufar-hedayat/?ancla=full-episodes&video=588644',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        video = self._download_json(\n-            'https://platform.fusion.net/wp-json/fusiondotnet/v1/video/' + video_id, video_id)\n-\n-        info = {\n-            'id': video_id,\n-            'title': video['title'],\n-            'description': video.get('excerpt'),\n-            'timestamp': parse_iso8601(video.get('published')),\n-            'series': video.get('show'),\n-        }\n-\n-        formats = []\n-        src = video.get('src') or {}\n-        for f_id, f in src.items():\n-            for q_id, q in f.items():\n-                q_url = q.get('url')\n-                if not q_url:\n-                    continue\n-                ext = determine_ext(q_url, mimetype2ext(q.get('type')))\n-                if ext == 'smil':\n-                    formats.extend(self._extract_smil_formats(q_url, video_id, fatal=False))\n-                elif f_id == 'm3u8-variant' or (ext == 'm3u8' and q_id == 'Variant'):\n-                    formats.extend(self._extract_m3u8_formats(\n-                        q_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))\n-                else:\n-                    formats.append({\n-                        'format_id': '-'.join([f_id, q_id]),\n-                        'url': q_url,\n-                        'width': int_or_none(q.get('width')),\n-                        'height': int_or_none(q.get('height')),\n-                        'tbr': int_or_none(self._search_regex(r'_(\\d+)\\.m(?:p4|3u8)', q_url, 'bitrate')),\n-                        'ext': 'mp4' if ext == 'm3u8' else ext,\n-                        'protocol': 'm3u8_native' if ext == 'm3u8' else 'https',\n-                    })\n-        if formats:\n-            info['formats'] = formats\n-        else:\n-            info.update({\n-                '_type': 'url',\n-                'url': 'anvato:uni:' + video['video_ids']['anvato'],\n-                'ie_key': 'Anvato',\n-            })\n-\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/generic.py",
            "diff": "diff --git a/yt_dlp/extractor/generic.py b/yt_dlp/extractor/generic.py\nindex f5c59a09..1f0011c0 100644\n--- a/yt_dlp/extractor/generic.py\n+++ b/yt_dlp/extractor/generic.py\n@@ -17,6 +17,7 @@\n     determine_protocol,\n     dict_get,\n     extract_basic_auth,\n+    filter_dict,\n     format_field,\n     int_or_none,\n     is_html,\n@@ -35,6 +36,7 @@\n     unsmuggle_url,\n     update_url_query,\n     url_or_none,\n+    urlhandle_detect_ext,\n     urljoin,\n     variadic,\n     xpath_attr,\n@@ -58,6 +60,8 @@ class GenericIE(InfoExtractor):\n                 'ext': 'mp4',\n                 'title': 'trailer',\n                 'upload_date': '20100513',\n+                'direct': True,\n+                'timestamp': 1273772943.0,\n             }\n         },\n         # Direct link to media delivered compressed (until Accept-Encoding is *)\n@@ -101,6 +105,8 @@ class GenericIE(InfoExtractor):\n                 'ext': 'webm',\n                 'title': '5_Lennart_Poettering_-_Systemd',\n                 'upload_date': '20141120',\n+                'direct': True,\n+                'timestamp': 1416498816.0,\n             },\n             'expected_warnings': [\n                 'URL could be a direct video link, returning it as such.'\n@@ -133,6 +139,7 @@ class GenericIE(InfoExtractor):\n                     'upload_date': '20201204',\n                 },\n             }],\n+            'skip': 'Dead link',\n         },\n         # RSS feed with item with description and thumbnails\n         {\n@@ -145,12 +152,12 @@ class GenericIE(InfoExtractor):\n             'playlist': [{\n                 'info_dict': {\n                     'ext': 'm4a',\n-                    'id': 'c1c879525ce2cb640b344507e682c36d',\n+                    'id': '818a5d38-01cd-152f-2231-ee479677fa82',\n                     'title': 're:Hydrogen!',\n                     'description': 're:.*In this episode we are going.*',\n                     'timestamp': 1567977776,\n                     'upload_date': '20190908',\n-                    'duration': 459,\n+                    'duration': 423,\n                     'thumbnail': r're:^https?://.*\\.jpg$',\n                     'episode_number': 1,\n                     'season_number': 1,\n@@ -267,6 +274,7 @@ class GenericIE(InfoExtractor):\n             'params': {\n                 'skip_download': True,\n             },\n+            'skip': '404 Not Found',\n         },\n         # MPD from http://dash-mse-test.appspot.com/media.html\n         {\n@@ -278,6 +286,7 @@ class GenericIE(InfoExtractor):\n                 'title': 'car-20120827-manifest',\n                 'formats': 'mincount:9',\n                 'upload_date': '20130904',\n+                'timestamp': 1378272859.0,\n             },\n         },\n         # m3u8 served with Content-Type: audio/x-mpegURL; charset=utf-8\n@@ -318,7 +327,7 @@ class GenericIE(InfoExtractor):\n                 'id': 'cmQHVoWB5FY',\n                 'ext': 'mp4',\n                 'upload_date': '20130224',\n-                'uploader_id': 'TheVerge',\n+                'uploader_id': '@TheVerge',\n                 'description': r're:^Chris Ziegler takes a look at the\\.*',\n                 'uploader': 'The Verge',\n                 'title': 'First Firefox OS phones side-by-side',\n@@ -365,46 +374,6 @@ class GenericIE(InfoExtractor):\n             },\n             'skip': 'There is a limit of 200 free downloads / month for the test song',\n         },\n-        # ooyala video\n-        {\n-            'url': 'http://www.rollingstone.com/music/videos/norwegian-dj-cashmere-cat-goes-spartan-on-with-me-premiere-20131219',\n-            'md5': '166dd577b433b4d4ebfee10b0824d8ff',\n-            'info_dict': {\n-                'id': 'BwY2RxaTrTkslxOfcan0UCf0YqyvWysJ',\n-                'ext': 'mp4',\n-                'title': '2cc213299525360.mov',  # that's what we get\n-                'duration': 238.231,\n-            },\n-            'add_ie': ['Ooyala'],\n-        },\n-        {\n-            # ooyala video embedded with http://player.ooyala.com/iframe.js\n-            'url': 'http://www.macrumors.com/2015/07/24/steve-jobs-the-man-in-the-machine-first-trailer/',\n-            'info_dict': {\n-                'id': 'p0MGJndjoG5SOKqO_hZJuZFPB-Tr5VgB',\n-                'ext': 'mp4',\n-                'title': '\"Steve Jobs: Man in the Machine\" trailer',\n-                'description': 'The first trailer for the Alex Gibney documentary \"Steve Jobs: Man in the Machine.\"',\n-                'duration': 135.427,\n-            },\n-            'params': {\n-                'skip_download': True,\n-            },\n-            'skip': 'movie expired',\n-        },\n-        # ooyala video embedded with http://player.ooyala.com/static/v4/production/latest/core.min.js\n-        {\n-            'url': 'http://wnep.com/2017/07/22/steampunk-fest-comes-to-honesdale/',\n-            'info_dict': {\n-                'id': 'lwYWYxYzE6V5uJMjNGyKtwwiw9ZJD7t2',\n-                'ext': 'mp4',\n-                'title': 'Steampunk Fest Comes to Honesdale',\n-                'duration': 43.276,\n-            },\n-            'params': {\n-                'skip_download': True,\n-            }\n-        },\n         # embed.ly video\n         {\n             'url': 'http://www.tested.com/science/weird/460206-tested-grinding-coffee-2000-frames-second/',\n@@ -497,7 +466,8 @@ class GenericIE(InfoExtractor):\n                 'title': '\u0423\u0436\u0430\u0441\u0442\u0438\u043a\u0438, \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u0442\u0440\u0435\u0439\u043b\u0435\u0440 (2015)',\n                 'thumbnail': r're:^https?://.*\\.jpg$',\n                 'duration': 153,\n-            }\n+            },\n+            'skip': 'Site dead',\n         },\n         # XHamster embed\n         {\n@@ -769,14 +739,16 @@ class GenericIE(InfoExtractor):\n             'playlist_mincount': 1,\n             'add_ie': ['Youtube'],\n         },\n-        # Cinchcast embed\n+        # Libsyn embed\n         {\n             'url': 'http://undergroundwellness.com/podcasts/306-5-steps-to-permanent-gut-healing/',\n             'info_dict': {\n-                'id': '7141703',\n+                'id': '3793998',\n                 'ext': 'mp3',\n                 'upload_date': '20141126',\n-                'title': 'Jack Tips: 5 Steps to Permanent Gut Healing',\n+                'title': 'Underground Wellness Radio - Jack Tips: 5 Steps to Permanent Gut Healing',\n+                'thumbnail': 'https://assets.libsyn.com/secure/item/3793998/?height=90&width=90',\n+                'duration': 3989.0,\n             }\n         },\n         # Cinerama player\n@@ -1558,16 +1530,6 @@ class GenericIE(InfoExtractor):\n                 'title': '\u0421\u0442\u0430\u0441 \u041d\u0430\u043c\u0438\u043d: \u00ab\u041c\u044b \u043d\u0430\u0440\u0443\u0448\u0438\u043b\u0438 \u0434\u0435\u0432\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u041a\u0440\u0435\u043c\u043b\u044f\u00bb',\n             },\n         },\n-        {\n-            # vzaar embed\n-            'url': 'http://help.vzaar.com/article/165-embedding-video',\n-            'md5': '7e3919d9d2620b89e3e00bec7fe8c9d4',\n-            'info_dict': {\n-                'id': '8707641',\n-                'ext': 'mp4',\n-                'title': 'Building A Business Online: Principal Chairs Q & A',\n-            },\n-        },\n         {\n             # multiple HTML5 videos on one page\n             'url': 'https://www.paragon-software.com/home/rk-free/keyscenarios.html',\n@@ -2370,7 +2332,7 @@ def _extract_kvs(self, url, webpage, video_id):\n             'id': flashvars['video_id'],\n             'display_id': display_id,\n             'title': title,\n-            'thumbnail': thumbnail,\n+            'thumbnail': urljoin(url, thumbnail),\n             'formats': formats,\n         }\n \n@@ -2427,10 +2389,10 @@ def _real_extract(self, url):\n         # to accept raw bytes and being able to download only a chunk.\n         # It may probably better to solve this by checking Content-Type for application/octet-stream\n         # after a HEAD request, but not sure if we can rely on this.\n-        full_response = self._request_webpage(url, video_id, headers={\n+        full_response = self._request_webpage(url, video_id, headers=filter_dict({\n             'Accept-Encoding': 'identity',\n-            **smuggled_data.get('http_headers', {})\n-        })\n+            'Referer': smuggled_data.get('referer'),\n+        }))\n         new_url = full_response.url\n         url = urllib.parse.urlparse(url)._replace(scheme=urllib.parse.urlparse(new_url).scheme).geturl()\n         if new_url != extract_basic_auth(url)[0]:\n@@ -2450,9 +2412,9 @@ def _real_extract(self, url):\n         m = re.match(r'^(?P<type>audio|video|application(?=/(?:ogg$|(?:vnd\\.apple\\.|x-)?mpegurl)))/(?P<format_id>[^;\\s]+)', content_type)\n         if m:\n             self.report_detected('direct video link')\n-            headers = smuggled_data.get('http_headers', {})\n+            headers = filter_dict({'Referer': smuggled_data.get('referer')})\n             format_id = str(m.group('format_id'))\n-            ext = determine_ext(url)\n+            ext = determine_ext(url, default_ext=None) or urlhandle_detect_ext(full_response)\n             subtitles = {}\n             if format_id.endswith('mpegurl') or ext == 'm3u8':\n                 formats, subtitles = self._extract_m3u8_formats_and_subtitles(url, video_id, 'mp4', headers=headers)\n@@ -2464,6 +2426,7 @@ def _real_extract(self, url):\n                 formats = [{\n                     'format_id': format_id,\n                     'url': url,\n+                    'ext': ext,\n                     'vcodec': 'none' if m.group('type') == 'audio' else None\n                 }]\n                 info_dict['direct'] = True\n@@ -2701,7 +2664,7 @@ def _extract_embeds(self, url, webpage, *, urlh=None, info_dict={}):\n                 'url': smuggle_url(json_ld['url'], {\n                     'force_videoid': video_id,\n                     'to_generic': True,\n-                    'http_headers': {'Referer': url},\n+                    'referer': url,\n                 }),\n             }, json_ld)]\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/gfycat.py",
            "diff": "diff --git a/yt_dlp/extractor/gfycat.py b/yt_dlp/extractor/gfycat.py\ndeleted file mode 100644\nindex edc2e56e..00000000\n--- a/yt_dlp/extractor/gfycat.py\n+++ /dev/null\n@@ -1,145 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    float_or_none,\n-    qualities,\n-    ExtractorError,\n-)\n-\n-\n-class GfycatIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:(?:www|giant|thumbs)\\.)?gfycat\\.com/(?i:ru/|ifr/|gifs/detail/)?(?P<id>[^-/?#\\.\"\\']+)'\n-    _EMBED_REGEX = [rf'<(?:iframe|source)[^>]+\\bsrc=[\"\\'](?P<url>{_VALID_URL})']\n-    _TESTS = [{\n-        'url': 'http://gfycat.com/DeadlyDecisiveGermanpinscher',\n-        'info_dict': {\n-            'id': 'DeadlyDecisiveGermanpinscher',\n-            'ext': 'mp4',\n-            'title': 'Ghost in the Shell',\n-            'timestamp': 1410656006,\n-            'upload_date': '20140914',\n-            'uploader': 'anonymous',\n-            'duration': 10.4,\n-            'view_count': int,\n-            'like_count': int,\n-            'categories': list,\n-            'age_limit': 0,\n-            'uploader_id': 'anonymous',\n-            'description': '',\n-        }\n-    }, {\n-        'url': 'http://gfycat.com/ifr/JauntyTimelyAmazontreeboa',\n-        'info_dict': {\n-            'id': 'JauntyTimelyAmazontreeboa',\n-            'ext': 'mp4',\n-            'title': 'JauntyTimelyAmazontreeboa',\n-            'timestamp': 1411720126,\n-            'upload_date': '20140926',\n-            'uploader': 'anonymous',\n-            'duration': 3.52,\n-            'view_count': int,\n-            'like_count': int,\n-            'categories': list,\n-            'age_limit': 0,\n-            'uploader_id': 'anonymous',\n-            'description': '',\n-        }\n-    }, {\n-        'url': 'https://gfycat.com/alienatedsolidgreathornedowl',\n-        'info_dict': {\n-            'id': 'alienatedsolidgreathornedowl',\n-            'ext': 'mp4',\n-            'upload_date': '20211226',\n-            'uploader_id': 'reactions',\n-            'timestamp': 1640536930,\n-            'like_count': int,\n-            'description': '',\n-            'title': 'Ingrid Michaelson, Zooey Deschanel - Merry Christmas Happy New Year',\n-            'categories': list,\n-            'age_limit': 0,\n-            'duration': 2.9583333333333335,\n-            'uploader': 'Reaction GIFs',\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'https://gfycat.com/ru/RemarkableDrearyAmurstarfish',\n-        'only_matching': True\n-    }, {\n-        'url': 'https://gfycat.com/gifs/detail/UnconsciousLankyIvorygull',\n-        'only_matching': True\n-    }, {\n-        'url': 'https://gfycat.com/acceptablehappygoluckyharborporpoise-baseball',\n-        'only_matching': True\n-    }, {\n-        'url': 'https://thumbs.gfycat.com/acceptablehappygoluckyharborporpoise-size_restricted.gif',\n-        'only_matching': True\n-    }, {\n-        'url': 'https://giant.gfycat.com/acceptablehappygoluckyharborporpoise.mp4',\n-        'only_matching': True\n-    }, {\n-        'url': 'http://gfycat.com/IFR/JauntyTimelyAmazontreeboa',\n-        'only_matching': True\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        gfy = self._download_json(\n-            'https://api.gfycat.com/v1/gfycats/%s' % video_id,\n-            video_id, 'Downloading video info')\n-        if 'error' in gfy:\n-            raise ExtractorError('Gfycat said: ' + gfy['error'], expected=True)\n-        gfy = gfy['gfyItem']\n-\n-        title = gfy.get('title') or gfy['gfyName']\n-        description = gfy.get('description')\n-        timestamp = int_or_none(gfy.get('createDate'))\n-        uploader = gfy.get('userName') or gfy.get('username')\n-        view_count = int_or_none(gfy.get('views'))\n-        like_count = int_or_none(gfy.get('likes'))\n-        dislike_count = int_or_none(gfy.get('dislikes'))\n-        age_limit = 18 if gfy.get('nsfw') == '1' else 0\n-\n-        width = int_or_none(gfy.get('width'))\n-        height = int_or_none(gfy.get('height'))\n-        fps = int_or_none(gfy.get('frameRate'))\n-        num_frames = int_or_none(gfy.get('numFrames'))\n-\n-        duration = float_or_none(num_frames, fps) if num_frames and fps else None\n-\n-        categories = gfy.get('tags') or gfy.get('extraLemmas') or []\n-\n-        FORMATS = ('gif', 'webm', 'mp4')\n-        quality = qualities(FORMATS)\n-\n-        formats = []\n-        for format_id in FORMATS:\n-            video_url = gfy.get('%sUrl' % format_id)\n-            if not video_url:\n-                continue\n-            filesize = int_or_none(gfy.get('%sSize' % format_id))\n-            formats.append({\n-                'url': video_url,\n-                'format_id': format_id,\n-                'width': width,\n-                'height': height,\n-                'fps': fps,\n-                'filesize': filesize,\n-                'quality': quality(format_id),\n-            })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'timestamp': timestamp,\n-            'uploader': gfy.get('userDisplayName') or uploader,\n-            'uploader_id': uploader,\n-            'duration': duration,\n-            'view_count': view_count,\n-            'like_count': like_count,\n-            'dislike_count': dislike_count,\n-            'categories': categories,\n-            'age_limit': age_limit,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/gofile.py",
            "diff": "diff --git a/yt_dlp/extractor/gofile.py b/yt_dlp/extractor/gofile.py\nindex ddbce2ee..ef14b57d 100644\n--- a/yt_dlp/extractor/gofile.py\n+++ b/yt_dlp/extractor/gofile.py\n@@ -60,13 +60,13 @@ def _real_initialize(self):\n         account_data = self._download_json(\n             'https://api.gofile.io/createAccount', None, note='Getting a new guest account')\n         self._TOKEN = account_data['data']['token']\n-        self._set_cookie('gofile.io', 'accountToken', self._TOKEN)\n+        self._set_cookie('.gofile.io', 'accountToken', self._TOKEN)\n \n     def _entries(self, file_id):\n         query_params = {\n             'contentId': file_id,\n             'token': self._TOKEN,\n-            'websiteToken': 12345,\n+            'websiteToken': '7fd94ds12fds4',  # From https://gofile.io/dist/js/alljs.js\n         }\n         password = self.get_param('videopassword')\n         if password:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/goplay.py",
            "diff": "diff --git a/yt_dlp/extractor/goplay.py b/yt_dlp/extractor/goplay.py\nindex 960d7d7b..0a3c8340 100644\n--- a/yt_dlp/extractor/goplay.py\n+++ b/yt_dlp/extractor/goplay.py\n@@ -383,9 +383,9 @@ def __get_current_timestamp():\n         months = [None, 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n         days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n \n-        time_now = datetime.datetime.utcnow()\n+        time_now = datetime.datetime.now(datetime.timezone.utc)\n         format_string = \"{} {} {} %H:%M:%S UTC %Y\".format(days[time_now.weekday()], months[time_now.month], time_now.day)\n-        time_string = datetime.datetime.utcnow().strftime(format_string)\n+        time_string = time_now.strftime(format_string)\n         return time_string\n \n     def __str__(self):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/groupon.py",
            "diff": "diff --git a/yt_dlp/extractor/groupon.py b/yt_dlp/extractor/groupon.py\nindex 362d3ff8..c1cbda35 100644\n--- a/yt_dlp/extractor/groupon.py\n+++ b/yt_dlp/extractor/groupon.py\n@@ -31,7 +31,6 @@ class GrouponIE(InfoExtractor):\n     }\n \n     _PROVIDERS = {\n-        'ooyala': ('ooyala:%s', 'Ooyala'),\n         'youtube': ('%s', 'Youtube'),\n     }\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/harpodeon.py",
            "diff": "diff --git a/yt_dlp/extractor/harpodeon.py b/yt_dlp/extractor/harpodeon.py\nindex 0aa47337..46eaddb3 100644\n--- a/yt_dlp/extractor/harpodeon.py\n+++ b/yt_dlp/extractor/harpodeon.py\n@@ -1,5 +1,5 @@\n from .common import InfoExtractor\n-from ..utils import unified_strdate\n+from ..utils import int_or_none\n \n \n class HarpodeonIE(InfoExtractor):\n@@ -14,7 +14,7 @@ class HarpodeonIE(InfoExtractor):\n             'title': 'The Smoking Out of Bella Butts',\n             'description': 'md5:47e16bdb41fc8a79c83ab83af11c8b77',\n             'creator': 'Vitagraph Company of America',\n-            'release_date': '19150101'\n+            'release_year': 1915,\n         }\n     }, {\n         'url': 'https://www.harpodeon.com/preview/The_Smoking_Out_of_Bella_Butts/268068288',\n@@ -25,7 +25,7 @@ class HarpodeonIE(InfoExtractor):\n             'title': 'The Smoking Out of Bella Butts',\n             'description': 'md5:47e16bdb41fc8a79c83ab83af11c8b77',\n             'creator': 'Vitagraph Company of America',\n-            'release_date': '19150101'\n+            'release_year': 1915,\n         }\n     }, {\n         'url': 'https://www.harpodeon.com/preview/Behind_the_Screen/421838710',\n@@ -36,7 +36,7 @@ class HarpodeonIE(InfoExtractor):\n             'title': 'Behind the Screen',\n             'description': 'md5:008972a3dc51fba3965ee517d2ba9155',\n             'creator': 'Lone Star Corporation',\n-            'release_date': '19160101'\n+            'release_year': 1916,\n         }\n     }]\n \n@@ -66,5 +66,5 @@ def _real_extract(self, url):\n             'http_headers': {'Referer': url},\n             'description': self._html_search_meta('description', webpage, fatal=False),\n             'creator': creator,\n-            'release_date': unified_strdate(f'{release_year}0101')\n+            'release_year': int_or_none(release_year),\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/helsinki.py",
            "diff": "diff --git a/yt_dlp/extractor/helsinki.py b/yt_dlp/extractor/helsinki.py\ndeleted file mode 100644\nindex e518cae1..00000000\n--- a/yt_dlp/extractor/helsinki.py\n+++ /dev/null\n@@ -1,38 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import js_to_json\n-\n-\n-class HelsinkiIE(InfoExtractor):\n-    IE_DESC = 'helsinki.fi'\n-    _VALID_URL = r'https?://video\\.helsinki\\.fi/Arkisto/flash\\.php\\?id=(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'http://video.helsinki.fi/Arkisto/flash.php?id=20258',\n-        'info_dict': {\n-            'id': '20258',\n-            'ext': 'mp4',\n-            'title': 'Tietotekniikkafoorumi-iltap\u00e4iv\u00e4',\n-            'description': 'md5:f5c904224d43c133225130fe156a5ee0',\n-        },\n-        'params': {\n-            'skip_download': True,  # RTMP\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        params = self._parse_json(self._html_search_regex(\n-            r'(?s)jwplayer\\(\"player\"\\).setup\\((\\{.*?\\})\\);',\n-            webpage, 'player code'), video_id, transform_source=js_to_json)\n-        formats = [{\n-            'url': s['file'],\n-            'ext': 'mp4',\n-        } for s in params['sources']]\n-\n-        return {\n-            'id': video_id,\n-            'title': self._og_search_title(webpage).replace('Video: ', ''),\n-            'description': self._og_search_description(webpage),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/hitbox.py",
            "diff": "diff --git a/yt_dlp/extractor/hitbox.py b/yt_dlp/extractor/hitbox.py\ndeleted file mode 100644\nindex f0c68988..00000000\n--- a/yt_dlp/extractor/hitbox.py\n+++ /dev/null\n@@ -1,209 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    clean_html,\n-    determine_ext,\n-    float_or_none,\n-    int_or_none,\n-    parse_iso8601,\n-)\n-\n-\n-class HitboxIE(InfoExtractor):\n-    IE_NAME = 'hitbox'\n-    _VALID_URL = r'https?://(?:www\\.)?(?:hitbox|smashcast)\\.tv/(?:[^/]+/)*videos?/(?P<id>[0-9]+)'\n-    _TESTS = [{\n-        'url': 'http://www.hitbox.tv/video/203213',\n-        'info_dict': {\n-            'id': '203213',\n-            'title': 'hitbox @ gamescom, Sub Button Hype extended, Giveaway - hitbox News Update with Oxy',\n-            'alt_title': 'hitboxlive - Aug 9th #6',\n-            'description': '',\n-            'ext': 'mp4',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 215.1666,\n-            'resolution': 'HD 720p',\n-            'uploader': 'hitboxlive',\n-            'view_count': int,\n-            'timestamp': 1407576133,\n-            'upload_date': '20140809',\n-            'categories': ['Live Show'],\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://www.smashcast.tv/hitboxlive/videos/203213',\n-        'only_matching': True,\n-    }]\n-\n-    def _extract_metadata(self, url, video_id):\n-        thumb_base = 'https://edge.sf.hitbox.tv'\n-        metadata = self._download_json(\n-            '%s/%s' % (url, video_id), video_id, 'Downloading metadata JSON')\n-\n-        date = 'media_live_since'\n-        media_type = 'livestream'\n-        if metadata.get('media_type') == 'video':\n-            media_type = 'video'\n-            date = 'media_date_added'\n-\n-        video_meta = metadata.get(media_type, [])[0]\n-        title = video_meta.get('media_status')\n-        alt_title = video_meta.get('media_title')\n-        description = clean_html(\n-            video_meta.get('media_description')\n-            or video_meta.get('media_description_md'))\n-        duration = float_or_none(video_meta.get('media_duration'))\n-        uploader = video_meta.get('media_user_name')\n-        views = int_or_none(video_meta.get('media_views'))\n-        timestamp = parse_iso8601(video_meta.get(date), ' ')\n-        categories = [video_meta.get('category_name')]\n-        thumbs = [{\n-            'url': thumb_base + video_meta.get('media_thumbnail'),\n-            'width': 320,\n-            'height': 180\n-        }, {\n-            'url': thumb_base + video_meta.get('media_thumbnail_large'),\n-            'width': 768,\n-            'height': 432\n-        }]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'alt_title': alt_title,\n-            'description': description,\n-            'ext': 'mp4',\n-            'thumbnails': thumbs,\n-            'duration': duration,\n-            'uploader': uploader,\n-            'view_count': views,\n-            'timestamp': timestamp,\n-            'categories': categories,\n-        }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        player_config = self._download_json(\n-            'https://www.smashcast.tv/api/player/config/video/%s' % video_id,\n-            video_id, 'Downloading video JSON')\n-\n-        formats = []\n-        for video in player_config['clip']['bitrates']:\n-            label = video.get('label')\n-            if label == 'Auto':\n-                continue\n-            video_url = video.get('url')\n-            if not video_url:\n-                continue\n-            bitrate = int_or_none(video.get('bitrate'))\n-            if determine_ext(video_url) == 'm3u8':\n-                if not video_url.startswith('http'):\n-                    continue\n-                formats.append({\n-                    'url': video_url,\n-                    'ext': 'mp4',\n-                    'tbr': bitrate,\n-                    'format_note': label,\n-                    'protocol': 'm3u8_native',\n-                })\n-            else:\n-                formats.append({\n-                    'url': video_url,\n-                    'tbr': bitrate,\n-                    'format_note': label,\n-                })\n-\n-        metadata = self._extract_metadata(\n-            'https://www.smashcast.tv/api/media/video', video_id)\n-        metadata['formats'] = formats\n-\n-        return metadata\n-\n-\n-class HitboxLiveIE(HitboxIE):  # XXX: Do not subclass from concrete IE\n-    IE_NAME = 'hitbox:live'\n-    _VALID_URL = r'https?://(?:www\\.)?(?:hitbox|smashcast)\\.tv/(?P<id>[^/?#&]+)'\n-    _TESTS = [{\n-        'url': 'http://www.hitbox.tv/dimak',\n-        'info_dict': {\n-            'id': 'dimak',\n-            'ext': 'mp4',\n-            'description': 'md5:c9f80fa4410bc588d7faa40003fc7d0e',\n-            'timestamp': int,\n-            'upload_date': compat_str,\n-            'title': compat_str,\n-            'uploader': 'Dimak',\n-        },\n-        'params': {\n-            # live\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://www.smashcast.tv/dimak',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        return False if HitboxIE.suitable(url) else super(HitboxLiveIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        player_config = self._download_json(\n-            'https://www.smashcast.tv/api/player/config/live/%s' % video_id,\n-            video_id)\n-\n-        formats = []\n-        cdns = player_config.get('cdns')\n-        servers = []\n-        for cdn in cdns:\n-            # Subscribe URLs are not playable\n-            if cdn.get('rtmpSubscribe') is True:\n-                continue\n-            base_url = cdn.get('netConnectionUrl')\n-            host = re.search(r'.+\\.([^\\.]+\\.[^\\./]+)/.+', base_url).group(1)\n-            if base_url not in servers:\n-                servers.append(base_url)\n-                for stream in cdn.get('bitrates'):\n-                    label = stream.get('label')\n-                    if label == 'Auto':\n-                        continue\n-                    stream_url = stream.get('url')\n-                    if not stream_url:\n-                        continue\n-                    bitrate = int_or_none(stream.get('bitrate'))\n-                    if stream.get('provider') == 'hls' or determine_ext(stream_url) == 'm3u8':\n-                        if not stream_url.startswith('http'):\n-                            continue\n-                        formats.append({\n-                            'url': stream_url,\n-                            'ext': 'mp4',\n-                            'tbr': bitrate,\n-                            'format_note': label,\n-                            'rtmp_live': True,\n-                        })\n-                    else:\n-                        formats.append({\n-                            'url': '%s/%s' % (base_url, stream_url),\n-                            'ext': 'mp4',\n-                            'tbr': bitrate,\n-                            'rtmp_live': True,\n-                            'format_note': host,\n-                            'page_url': url,\n-                            'player_url': 'http://www.hitbox.tv/static/player/flowplayer/flowplayer.commercial-3.2.16.swf',\n-                        })\n-\n-        metadata = self._extract_metadata(\n-            'https://www.smashcast.tv/api/media/live', video_id)\n-        metadata['formats'] = formats\n-        metadata['is_live'] = True\n-        metadata['title'] = metadata.get('title')\n-\n-        return metadata\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/hotstar.py",
            "diff": "diff --git a/yt_dlp/extractor/hotstar.py b/yt_dlp/extractor/hotstar.py\nindex cdd93794..541792b9 100644\n--- a/yt_dlp/extractor/hotstar.py\n+++ b/yt_dlp/extractor/hotstar.py\n@@ -142,6 +142,26 @@ class HotStarIE(HotStarBaseIE):\n             'duration': 1272,\n             'channel_id': 3,\n         },\n+        'skip': 'HTTP Error 504: Gateway Time-out',  # XXX: Investigate 504 errors on some episodes\n+    }, {\n+        'url': 'https://www.hotstar.com/in/shows/kana-kaanum-kaalangal/1260097087/back-to-school/1260097320',\n+        'info_dict': {\n+            'id': '1260097320',\n+            'ext': 'mp4',\n+            'title': 'Back To School',\n+            'season': 'Chapter 1',\n+            'description': 'md5:b0d6a4c8a650681491e7405496fc7e13',\n+            'timestamp': 1650564000,\n+            'channel': 'Hotstar Specials',\n+            'series': 'Kana Kaanum Kaalangal',\n+            'season_number': 1,\n+            'season_id': 9441,\n+            'upload_date': '20220421',\n+            'episode': 'Back To School',\n+            'episode_number': 1,\n+            'duration': 1810,\n+            'channel_id': 54,\n+        },\n     }, {\n         'url': 'https://www.hotstar.com/in/clips/e3-sairat-kahani-pyaar-ki/1000262286',\n         'info_dict': {\n@@ -154,6 +174,19 @@ class HotStarIE(HotStarBaseIE):\n             'timestamp': 1622943900,\n             'duration': 5395,\n         },\n+    }, {\n+        'url': 'https://www.hotstar.com/in/movies/premam/1000091195',\n+        'info_dict': {\n+            'id': '1000091195',\n+            'ext': 'mp4',\n+            'title': 'Premam',\n+            'release_year': 2015,\n+            'description': 'md5:d833c654e4187b5e34757eafb5b72d7f',\n+            'timestamp': 1462149000,\n+            'upload_date': '20160502',\n+            'episode': 'Premam',\n+            'duration': 8994,\n+        },\n     }, {\n         'url': 'https://www.hotstar.com/movies/radha-gopalam/1000057157',\n         'only_matching': True,\n@@ -200,8 +233,10 @@ def _real_extract(self, url):\n         video_type = self._TYPE.get(video_type, video_type)\n         cookies = self._get_cookies(url)  # Cookies before any request\n \n-        video_data = self._call_api_v1(f'{video_type}/detail', video_id,\n-                                       query={'tas': 10000, 'contentId': video_id})['body']['results']['item']\n+        video_data = traverse_obj(\n+            self._call_api_v1(\n+                f'{video_type}/detail', video_id, fatal=False, query={'tas': 10000, 'contentId': video_id}),\n+            ('body', 'results', 'item', {dict})) or {}\n         if not self.get_param('allow_unplayable_formats') and video_data.get('drmProtected'):\n             self.report_drm(video_id)\n \n@@ -286,6 +321,7 @@ def _real_extract(self, url):\n             'description': video_data.get('description'),\n             'duration': int_or_none(video_data.get('duration')),\n             'timestamp': int_or_none(traverse_obj(video_data, 'broadcastDate', 'startDate')),\n+            'release_year': int_or_none(video_data.get('year')),\n             'formats': formats,\n             'subtitles': subs,\n             'channel': video_data.get('channelName'),\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/howcast.py",
            "diff": "diff --git a/yt_dlp/extractor/howcast.py b/yt_dlp/extractor/howcast.py\ndeleted file mode 100644\nindex 59cf80f1..00000000\n--- a/yt_dlp/extractor/howcast.py\n+++ /dev/null\n@@ -1,41 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import parse_iso8601\n-\n-\n-class HowcastIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?howcast\\.com/videos/(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'http://www.howcast.com/videos/390161-How-to-Tie-a-Square-Knot-Properly',\n-        'md5': '7d45932269a288149483144f01b99789',\n-        'info_dict': {\n-            'id': '390161',\n-            'ext': 'mp4',\n-            'title': 'How to Tie a Square Knot Properly',\n-            'description': 'md5:dbe792e5f6f1489027027bf2eba188a3',\n-            'timestamp': 1276081287,\n-            'upload_date': '20100609',\n-            'duration': 56.823,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'add_ie': ['Ooyala'],\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        embed_code = self._search_regex(\n-            r'<iframe[^>]+src=\"[^\"]+\\bembed_code=([^\\b]+)\\b',\n-            webpage, 'ooyala embed code')\n-\n-        return {\n-            '_type': 'url_transparent',\n-            'ie_key': 'Ooyala',\n-            'url': 'ooyala:%s' % embed_code,\n-            'id': video_id,\n-            'timestamp': parse_iso8601(self._html_search_meta(\n-                'article:published_time', webpage, 'timestamp')),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/howstuffworks.py",
            "diff": "diff --git a/yt_dlp/extractor/howstuffworks.py b/yt_dlp/extractor/howstuffworks.py\ndeleted file mode 100644\nindex 238fc0b4..00000000\n--- a/yt_dlp/extractor/howstuffworks.py\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    find_xpath_attr,\n-    int_or_none,\n-    js_to_json,\n-    unescapeHTML,\n-    determine_ext,\n-)\n-\n-\n-class HowStuffWorksIE(InfoExtractor):\n-    _VALID_URL = r'https?://[\\da-z-]+\\.(?:howstuffworks|stuff(?:(?:youshould|theydontwantyouto)know|toblowyourmind|momnevertoldyou)|(?:brain|car)stuffshow|fwthinking|geniusstuff)\\.com/(?:[^/]+/)*(?:\\d+-)?(?P<id>.+?)-video\\.htm'\n-    _TESTS = [\n-        {\n-            'url': 'http://www.stufftoblowyourmind.com/videos/optical-illusions-video.htm',\n-            'md5': '76646a5acc0c92bf7cd66751ca5db94d',\n-            'info_dict': {\n-                'id': '855410',\n-                'ext': 'mp4',\n-                'title': 'Your Trickster Brain: Optical Illusions -- Science on the Web',\n-                'description': 'md5:e374ff9561f6833ad076a8cc0a5ab2fb',\n-            },\n-        },\n-        {\n-            'url': 'http://shows.howstuffworks.com/more-shows/why-does-balloon-stick-to-hair-video.htm',\n-            'only_matching': True,\n-        }\n-    ]\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        webpage = self._download_webpage(url, display_id)\n-        clip_js = self._search_regex(\n-            r'(?s)var clip = ({.*?});', webpage, 'clip info')\n-        clip_info = self._parse_json(\n-            clip_js, display_id, transform_source=js_to_json)\n-\n-        video_id = clip_info['content_id']\n-        formats = []\n-        m3u8_url = clip_info.get('m3u8')\n-        if m3u8_url and determine_ext(m3u8_url) == 'm3u8':\n-            formats.extend(self._extract_m3u8_formats(m3u8_url, video_id, 'mp4', format_id='hls', fatal=True))\n-        flv_url = clip_info.get('flv_url')\n-        if flv_url:\n-            formats.append({\n-                'url': flv_url,\n-                'format_id': 'flv',\n-            })\n-        for video in clip_info.get('mp4', []):\n-            formats.append({\n-                'url': video['src'],\n-                'format_id': 'mp4-%s' % video['bitrate'],\n-                'vbr': int_or_none(video['bitrate'].rstrip('k')),\n-            })\n-\n-        if not formats:\n-            smil = self._download_xml(\n-                'http://services.media.howstuffworks.com/videos/%s/smil-service.smil' % video_id,\n-                video_id, 'Downloading video SMIL')\n-\n-            http_base = find_xpath_attr(\n-                smil,\n-                './{0}head/{0}meta'.format('{http://www.w3.org/2001/SMIL20/Language}'),\n-                'name',\n-                'httpBase').get('content')\n-\n-            URL_SUFFIX = '?v=2.11.3&fp=LNX 11,2,202,356&r=A&g=A'\n-\n-            for video in smil.findall(\n-                    './{0}body/{0}switch/{0}video'.format('{http://www.w3.org/2001/SMIL20/Language}')):\n-                vbr = int_or_none(video.attrib['system-bitrate'], scale=1000)\n-                formats.append({\n-                    'url': '%s/%s%s' % (http_base, video.attrib['src'], URL_SUFFIX),\n-                    'format_id': '%dk' % vbr,\n-                    'vbr': vbr,\n-                })\n-\n-        return {\n-            'id': '%s' % video_id,\n-            'display_id': display_id,\n-            'title': unescapeHTML(clip_info['clip_title']),\n-            'description': unescapeHTML(clip_info.get('caption')),\n-            'thumbnail': clip_info.get('video_still_url'),\n-            'duration': int_or_none(clip_info.get('duration')),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/hungama.py",
            "diff": "diff --git a/yt_dlp/extractor/hungama.py b/yt_dlp/extractor/hungama.py\nindex 2e993960..cdec3683 100644\n--- a/yt_dlp/extractor/hungama.py\n+++ b/yt_dlp/extractor/hungama.py\n@@ -1,19 +1,32 @@\n-import re\n-\n from .common import InfoExtractor\n from ..utils import (\n     int_or_none,\n+    remove_end,\n+    traverse_obj,\n     try_get,\n+    unified_timestamp,\n+    url_or_none,\n     urlencode_postdata,\n )\n \n \n-class HungamaIE(InfoExtractor):\n+class HungamaBaseIE(InfoExtractor):\n+    def _call_api(self, path, content_id, fatal=False):\n+        return traverse_obj(self._download_json(\n+            f'https://cpage.api.hungama.com/v2/page/content/{content_id}/{path}/detail',\n+            content_id, fatal=fatal, query={\n+                'device': 'web',\n+                'platform': 'a',\n+                'storeId': '1',\n+            }), ('data', {dict})) or {}\n+\n+\n+class HungamaIE(HungamaBaseIE):\n     _VALID_URL = r'''(?x)\n                     https?://\n-                        (?:www\\.)?hungama\\.com/\n+                        (?:www\\.|un\\.)?hungama\\.com/\n                         (?:\n-                            (?:video|movie)/[^/]+/|\n+                            (?:video|movie|short-film)/[^/]+/|\n                             tv-show/(?:[^/]+/){2}\\d+/episode/[^/]+/\n                         )\n                         (?P<id>\\d+)\n@@ -25,13 +38,28 @@ class HungamaIE(InfoExtractor):\n             'id': '39349649',\n             'ext': 'mp4',\n             'title': 'Krishna Chants',\n-            'description': 'Watch Krishna Chants video now. You can also watch other latest videos only at Hungama',\n+            'description': ' ',\n             'upload_date': '20180829',\n             'duration': 264,\n             'timestamp': 1535500800,\n             'view_count': int,\n-            'thumbnail': 'https://images.hungama.com/c/1/0dc/2ca/39349649/39349649_700x394.jpg',\n-        }\n+            'thumbnail': 'https://images1.hungama.com/tr:n-a_169_m/c/1/0dc/2ca/39349649/39349649_350x197.jpg?v=8',\n+            'tags': 'count:6',\n+        },\n+    }, {\n+        'url': 'https://un.hungama.com/short-film/adira/102524179/',\n+        'md5': '2278463f5dc9db9054d0c02602d44666',\n+        'info_dict': {\n+            'id': '102524179',\n+            'ext': 'mp4',\n+            'title': 'Adira',\n+            'description': 'md5:df20cd4d41eabb33634f06de1025a4b4',\n+            'upload_date': '20230417',\n+            'timestamp': 1681689600,\n+            'view_count': int,\n+            'thumbnail': 'https://images1.hungama.com/tr:n-a_23_m/c/1/197/ac9/102524179/102524179_350x525.jpg?v=1',\n+            'tags': 'count:7',\n+        },\n     }, {\n         'url': 'https://www.hungama.com/movie/kahaani-2/44129919/',\n         'only_matching': True,\n@@ -51,14 +79,19 @@ def _real_extract(self, url):\n                 'c': 'common',\n                 'm': 'get_video_mdn_url',\n             })\n-\n         formats = self._extract_m3u8_formats(video_json['stream_url'], video_id, ext='mp4', m3u8_id='hls')\n-\n-        json_ld = self._search_json_ld(\n-            self._download_webpage(url, video_id, fatal=False) or '', video_id, fatal=False)\n+        metadata = self._call_api('movie', video_id)\n \n         return {\n-            **json_ld,\n+            **traverse_obj(metadata, ('head', 'data', {\n+                'title': ('title', {str}),\n+                'description': ('misc', 'description', {str}),\n+                'duration': ('duration', {int}),  # duration in JSON is incorrect if string\n+                'timestamp': ('releasedate', {unified_timestamp}),\n+                'view_count': ('misc', 'playcount', {int_or_none}),\n+                'thumbnail': ('image', {url_or_none}),\n+                'tags': ('misc', 'keywords', ..., {str}),\n+            })),\n             'id': video_id,\n             'formats': formats,\n             'subtitles': {\n@@ -71,10 +104,10 @@ def _real_extract(self, url):\n \n \n class HungamaSongIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?hungama\\.com/song/[^/]+/(?P<id>\\d+)'\n-    _TEST = {\n+    _VALID_URL = r'https?://(?:www\\.|un\\.)?hungama\\.com/song/[^/]+/(?P<id>\\d+)'\n+    _TESTS = [{\n         'url': 'https://www.hungama.com/song/kitni-haseen-zindagi/2931166/',\n-        'md5': 'd4a6a05a394ad0453a9bea3ca00e6024',\n+        'md5': '964f46828e8b250aa35e5fdcfdcac367',\n         'info_dict': {\n             'id': '2931166',\n             'ext': 'mp3',\n@@ -83,8 +116,22 @@ class HungamaSongIE(InfoExtractor):\n             'artist': 'Lucky Ali',\n             'album': None,\n             'release_year': 2000,\n-        }\n-    }\n+            'thumbnail': 'https://stat2.hungama.ind.in/assets/images/default_images/da-200x200.png',\n+        },\n+    }, {\n+        'url': 'https://un.hungama.com/song/tum-kya-mile-from-rocky-aur-rani-kii-prem-kahaani/103553672',\n+        'md5': '964f46828e8b250aa35e5fdcfdcac367',\n+        'info_dict': {\n+            'id': '103553672',\n+            'ext': 'mp3',\n+            'title': 'md5:5ebeb1e10771b634ce5f700ce68ae5f4',\n+            'track': 'Tum Kya Mile (From \"Rocky Aur Rani Kii Prem Kahaani\")',\n+            'artist': 'Pritam Chakraborty, Arijit Singh, Shreya Ghoshal, Amitabh Bhattacharya',\n+            'album': 'Tum Kya Mile (From \"Rocky Aur Rani Kii Prem Kahaani\")',\n+            'release_year': 2023,\n+            'thumbnail': 'https://images.hungama.com/c/1/7c2/c7b/103553671/103553671_200x200.jpg',\n+        },\n+    }]\n \n     def _real_extract(self, url):\n         audio_id = self._match_id(url)\n@@ -122,8 +169,8 @@ def _real_extract(self, url):\n         }\n \n \n-class HungamaAlbumPlaylistIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?hungama\\.com/(?:playlists|album)/[^/]+/(?P<id>\\d+)'\n+class HungamaAlbumPlaylistIE(HungamaBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.|un\\.)?hungama\\.com/(?P<path>playlists|album)/[^/]+/(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://www.hungama.com/album/bhuj-the-pride-of-india/69481490/',\n         'playlist_mincount': 7,\n@@ -132,16 +179,24 @@ class HungamaAlbumPlaylistIE(InfoExtractor):\n         },\n     }, {\n         'url': 'https://www.hungama.com/playlists/hindi-jan-to-june-2021/123063/',\n-        'playlist_mincount': 50,\n+        'playlist_mincount': 33,\n         'info_dict': {\n             'id': '123063',\n         },\n+    }, {\n+        'url': 'https://un.hungama.com/album/what-jhumka-%3F-from-rocky-aur-rani-kii-prem-kahaani/103891805/',\n+        'playlist_mincount': 1,\n+        'info_dict': {\n+            'id': '103891805',\n+        },\n     }]\n \n     def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        ptrn = r'<meta[^>]+?property=[\\\"\\']?music:song:url[\\\"\\']?[^>]+?content=[\\\"\\']?([^\\\"\\']+)'\n-        items = re.findall(ptrn, webpage)\n-        entries = [self.url_result(item, ie=HungamaSongIE.ie_key()) for item in items]\n-        return self.playlist_result(entries, video_id)\n+        playlist_id, path = self._match_valid_url(url).group('id', 'path')\n+        data = self._call_api(remove_end(path, 's'), playlist_id, fatal=True)\n+\n+        def entries():\n+            for song_url in traverse_obj(data, ('body', 'rows', ..., 'data', 'misc', 'share', {url_or_none})):\n+                yield self.url_result(song_url, HungamaSongIE)\n+\n+        return self.playlist_result(entries(), playlist_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ign.py",
            "diff": "diff --git a/yt_dlp/extractor/ign.py b/yt_dlp/extractor/ign.py\nindex 64875f8c..1c4f105e 100644\n--- a/yt_dlp/extractor/ign.py\n+++ b/yt_dlp/extractor/ign.py\n@@ -197,10 +197,6 @@ class IGNVideoIE(IGNBaseIE):\n             'thumbnail': 'https://sm.ign.com/ign_me/video/h/how-hitman/how-hitman-aims-to-be-different-than-every-other-s_8z14.jpg',\n             'duration': 298,\n             'tags': 'count:13',\n-            'display_id': '112203',\n-            'thumbnail': 'https://sm.ign.com/ign_me/video/h/how-hitman/how-hitman-aims-to-be-different-than-every-other-s_8z14.jpg',\n-            'duration': 298,\n-            'tags': 'count:13',\n         },\n         'expected_warnings': ['HTTP Error 400: Bad Request'],\n     }, {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/iheart.py",
            "diff": "diff --git a/yt_dlp/extractor/iheart.py b/yt_dlp/extractor/iheart.py\nindex 2c6a5b6a..fb6f51e2 100644\n--- a/yt_dlp/extractor/iheart.py\n+++ b/yt_dlp/extractor/iheart.py\n@@ -23,7 +23,7 @@ def _extract_episode(self, episode):\n \n \n class IHeartRadioIE(IHeartRadioBaseIE):\n-    IENAME = 'iheartradio'\n+    IE_NAME = 'iheartradio'\n     _VALID_URL = r'(?:https?://(?:www\\.)?iheart\\.com/podcast/[^/]+/episode/(?P<display_id>[^/?&#]+)-|iheartradio:)(?P<id>\\d+)'\n     _TEST = {\n         'url': 'https://www.iheart.com/podcast/105-behind-the-bastards-29236323/episode/part-one-alexander-lukashenko-the-dictator-70346499/?embed=true',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/imgur.py",
            "diff": "diff --git a/yt_dlp/extractor/imgur.py b/yt_dlp/extractor/imgur.py\nindex bff6ed57..1fa0a2a7 100644\n--- a/yt_dlp/extractor/imgur.py\n+++ b/yt_dlp/extractor/imgur.py\n@@ -1,99 +1,243 @@\n+import functools\n import re\n \n from .common import InfoExtractor\n from ..utils import (\n+    ExtractorError,\n+    determine_ext,\n+    float_or_none,\n     int_or_none,\n     js_to_json,\n     mimetype2ext,\n-    ExtractorError,\n+    parse_iso8601,\n+    str_or_none,\n+    strip_or_none,\n+    traverse_obj,\n+    url_or_none,\n )\n \n \n-class ImgurIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/(?!(?:a|gallery|(?:t(?:opic)?|r)/[^/]+)/)(?P<id>[a-zA-Z0-9]+)'\n+class ImgurBaseIE(InfoExtractor):\n+    _CLIENT_ID = '546c25a59c58ad7'\n+\n+    @classmethod\n+    def _imgur_result(cls, item_id):\n+        return cls.url_result(f'https://imgur.com/{item_id}', ImgurIE, item_id)\n+\n+    def _call_api(self, endpoint, video_id, **kwargs):\n+        return self._download_json(\n+            f'https://api.imgur.com/post/v1/{endpoint}/{video_id}?client_id={self._CLIENT_ID}&include=media,account',\n+            video_id, **kwargs)\n+\n+    @staticmethod\n+    def get_description(s):\n+        if 'Discover the magic of the internet at Imgur' in s:\n+            return None\n+        return s or None\n+\n+\n+class ImgurIE(ImgurBaseIE):\n+    _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/(?!(?:a|gallery|t|topic|r)/)(?P<id>[a-zA-Z0-9]+)'\n \n     _TESTS = [{\n-        'url': 'https://i.imgur.com/A61SaA1.gifv',\n+        'url': 'https://imgur.com/A61SaA1',\n         'info_dict': {\n             'id': 'A61SaA1',\n             'ext': 'mp4',\n-            'title': 're:Imgur GIF$|MRW gifv is up and running without any bugs$',\n+            'title': 'MRW gifv is up and running without any bugs',\n+            'timestamp': 1416446068,\n+            'upload_date': '20141120',\n+            'dislike_count': int,\n+            'comment_count': int,\n+            'release_timestamp': 1416446068,\n+            'release_date': '20141120',\n+            'like_count': int,\n+            'thumbnail': 'https://i.imgur.com/A61SaA1h.jpg',\n         },\n     }, {\n-        'url': 'https://imgur.com/A61SaA1',\n+        'url': 'https://i.imgur.com/A61SaA1.gifv',\n         'only_matching': True,\n     }, {\n         'url': 'https://i.imgur.com/crGpqCV.mp4',\n         'only_matching': True,\n     }, {\n-        # no title\n         'url': 'https://i.imgur.com/jxBXAMC.gifv',\n-        'only_matching': True,\n+        'info_dict': {\n+            'id': 'jxBXAMC',\n+            'ext': 'mp4',\n+            'title': 'Fahaka puffer feeding',\n+            'timestamp': 1533835503,\n+            'upload_date': '20180809',\n+            'release_date': '20180809',\n+            'like_count': int,\n+            'duration': 30.0,\n+            'comment_count': int,\n+            'release_timestamp': 1533835503,\n+            'thumbnail': 'https://i.imgur.com/jxBXAMCh.jpg',\n+            'dislike_count': int,\n+        },\n     }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+        data = self._call_api('media', video_id)\n+        if not traverse_obj(data, ('media', 0, (\n+                ('type', {lambda t: t == 'video' or None}),\n+                ('metadata', 'is_animated'))), get_all=False):\n+            raise ExtractorError(f'{video_id} is not a video or animated image', expected=True)\n         webpage = self._download_webpage(\n-            'https://i.imgur.com/{id}.gifv'.format(id=video_id), video_id)\n+            f'https://i.imgur.com/{video_id}.gifv', video_id, fatal=False) or ''\n+        formats = []\n \n-        width = int_or_none(self._og_search_property(\n-            'video:width', webpage, default=None))\n-        height = int_or_none(self._og_search_property(\n-            'video:height', webpage, default=None))\n+        media_fmt = traverse_obj(data, ('media', 0, {\n+            'url': ('url', {url_or_none}),\n+            'ext': ('ext', {str}),\n+            'width': ('width', {int_or_none}),\n+            'height': ('height', {int_or_none}),\n+            'filesize': ('size', {int_or_none}),\n+            'acodec': ('metadata', 'has_sound', {lambda b: None if b else 'none'}),\n+        }))\n+        media_url = media_fmt.get('url')\n+        if media_url:\n+            if not media_fmt.get('ext'):\n+                media_fmt['ext'] = mimetype2ext(traverse_obj(\n+                    data, ('media', 0, 'mime_type'))) or determine_ext(media_url)\n+            if traverse_obj(data, ('media', 0, 'type')) == 'image':\n+                media_fmt['acodec'] = 'none'\n+                media_fmt.setdefault('preference', -10)\n+            formats.append(media_fmt)\n \n         video_elements = self._search_regex(\n             r'(?s)<div class=\"video-elements\">(.*?)</div>',\n             webpage, 'video elements', default=None)\n-        if not video_elements:\n-            raise ExtractorError(\n-                'No sources found for video %s. Maybe an image?' % video_id,\n-                expected=True)\n \n-        formats = []\n-        for m in re.finditer(r'<source\\s+src=\"(?P<src>[^\"]+)\"\\s+type=\"(?P<type>[^\"]+)\"', video_elements):\n-            formats.append({\n-                'format_id': m.group('type').partition('/')[2],\n-                'url': self._proto_relative_url(m.group('src')),\n-                'ext': mimetype2ext(m.group('type')),\n-                'width': width,\n-                'height': height,\n-                'http_headers': {\n-                    'User-Agent': 'yt-dlp (like wget)',\n-                },\n-            })\n+        if video_elements:\n+            def og_get_size(media_type):\n+                return {\n+                    p: int_or_none(self._og_search_property(f'{media_type}:{p}', webpage, default=None))\n+                    for p in ('width', 'height')\n+                }\n+\n+            size = og_get_size('video')\n+            if not any(size.values()):\n+                size = og_get_size('image')\n+\n+            formats = traverse_obj(\n+                re.finditer(r'<source\\s+src=\"(?P<src>[^\"]+)\"\\s+type=\"(?P<type>[^\"]+)\"', video_elements),\n+                (..., {\n+                    'format_id': ('type', {lambda s: s.partition('/')[2]}),\n+                    'url': ('src', {self._proto_relative_url}),\n+                    'ext': ('type', {mimetype2ext}),\n+                }))\n+            for f in formats:\n+                f.update(size)\n \n-        gif_json = self._search_regex(\n-            r'(?s)var\\s+videoItem\\s*=\\s*(\\{.*?\\})',\n-            webpage, 'GIF code', fatal=False)\n-        if gif_json:\n-            gifd = self._parse_json(\n-                gif_json, video_id, transform_source=js_to_json)\n-            formats.append({\n-                'format_id': 'gif',\n-                'preference': -10,  # gifs are worse than videos\n-                'width': width,\n-                'height': height,\n-                'ext': 'gif',\n-                'acodec': 'none',\n-                'vcodec': 'gif',\n-                'container': 'gif',\n-                'url': self._proto_relative_url(gifd['gifUrl']),\n-                'filesize': gifd.get('size'),\n-                'http_headers': {\n-                    'User-Agent': 'yt-dlp (like wget)',\n-                },\n+            # We can get the original gif format from the webpage as well\n+            gif_json = traverse_obj(self._search_json(\n+                r'var\\s+videoItem\\s*=', webpage, 'GIF info', video_id,\n+                transform_source=js_to_json, fatal=False), {\n+                    'url': ('gifUrl', {self._proto_relative_url}),\n+                    'filesize': ('size', {int_or_none}),\n             })\n+            if gif_json:\n+                gif_json.update(size)\n+                gif_json.update({\n+                    'format_id': 'gif',\n+                    'preference': -10,  # gifs < videos\n+                    'ext': 'gif',\n+                    'acodec': 'none',\n+                    'vcodec': 'gif',\n+                    'container': 'gif',\n+                })\n+                formats.append(gif_json)\n+\n+        search = functools.partial(self._html_search_meta, html=webpage, default=None)\n+\n+        twitter_fmt = {\n+            'format_id': 'twitter',\n+            'url': url_or_none(search('twitter:player:stream')),\n+            'ext': mimetype2ext(search('twitter:player:stream:content_type')),\n+            'width': int_or_none(search('twitter:width')),\n+            'height': int_or_none(search('twitter:height')),\n+        }\n+        if twitter_fmt['url']:\n+            formats.append(twitter_fmt)\n+\n+        if not formats:\n+            self.raise_no_formats(\n+                f'No sources found for video {video_id}. Maybe a plain image?', expected=True)\n+        self._remove_duplicate_formats(formats)\n \n         return {\n+            'title': self._og_search_title(webpage, default=None),\n+            'description': self.get_description(self._og_search_description(webpage, default='')),\n+            **traverse_obj(data, {\n+                'uploader_id': ('account_id', {lambda a: str(a) if int_or_none(a) else None}),\n+                'uploader': ('account', 'username', {lambda x: strip_or_none(x) or None}),\n+                'uploader_url': ('account', 'avatar_url', {url_or_none}),\n+                'like_count': ('upvote_count', {int_or_none}),\n+                'dislike_count': ('downvote_count', {int_or_none}),\n+                'comment_count': ('comment_count', {int_or_none}),\n+                'age_limit': ('is_mature', {lambda x: 18 if x else None}),\n+                'timestamp': (('updated_at', 'created_at'), {parse_iso8601}),\n+                'release_timestamp': ('created_at', {parse_iso8601}),\n+            }, get_all=False),\n+            **traverse_obj(data, ('media', 0, 'metadata', {\n+                'title': ('title', {lambda x: strip_or_none(x) or None}),\n+                'description': ('description', {self.get_description}),\n+                'duration': ('duration', {float_or_none}),\n+                'timestamp': (('updated_at', 'created_at'), {parse_iso8601}),\n+                'release_timestamp': ('created_at', {parse_iso8601}),\n+            }), get_all=False),\n             'id': video_id,\n             'formats': formats,\n-            'title': self._og_search_title(webpage, default=video_id),\n+            'thumbnail': url_or_none(search('thumbnailUrl')),\n         }\n \n \n-class ImgurGalleryIE(InfoExtractor):\n+class ImgurGalleryBaseIE(ImgurBaseIE):\n+    _GALLERY = True\n+\n+    def _real_extract(self, url):\n+        gallery_id = self._match_id(url)\n+\n+        data = self._call_api('albums', gallery_id, fatal=False, expected_status=404)\n+\n+        info = traverse_obj(data, {\n+            'title': ('title', {lambda x: strip_or_none(x) or None}),\n+            'description': ('description', {self.get_description}),\n+        })\n+\n+        if traverse_obj(data, 'is_album'):\n+\n+            def yield_media_ids():\n+                for m_id in traverse_obj(data, (\n+                        'media', lambda _, v: v.get('type') == 'video' or v['metadata']['is_animated'],\n+                        'id', {lambda x: str_or_none(x) or None})):\n+                    yield m_id\n+\n+            # if a gallery with exactly one video, apply album metadata to video\n+            media_id = (\n+                self._GALLERY\n+                and traverse_obj(data, ('image_count', {lambda c: c == 1}))\n+                and next(yield_media_ids(), None))\n+\n+            if not media_id:\n+                result = self.playlist_result(\n+                    map(self._imgur_result, yield_media_ids()), gallery_id)\n+                result.update(info)\n+                return result\n+            gallery_id = media_id\n+\n+        result = self._imgur_result(gallery_id)\n+        info['_type'] = 'url_transparent'\n+        result.update(info)\n+        return result\n+\n+\n+class ImgurGalleryIE(ImgurGalleryBaseIE):\n     IE_NAME = 'imgur:gallery'\n-    _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/(?:gallery|(?:t(?:opic)?|r)/[^/]+)/(?P<id>[a-zA-Z0-9]+)'\n+    _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/(?:gallery|(?:t(?:opic)?|r)/[^/?#]+)/(?P<id>[a-zA-Z0-9]+)'\n \n     _TESTS = [{\n         'url': 'http://imgur.com/gallery/Q95ko',\n@@ -102,49 +246,121 @@ class ImgurGalleryIE(InfoExtractor):\n             'title': 'Adding faces make every GIF better',\n         },\n         'playlist_count': 25,\n+        'skip': 'Zoinks! You\\'ve taken a wrong turn.',\n     }, {\n+        # TODO: static images - replace with animated/video gallery\n         'url': 'http://imgur.com/topic/Aww/ll5Vk',\n         'only_matching': True,\n     }, {\n         'url': 'https://imgur.com/gallery/YcAQlkx',\n+        'add_ies': ['Imgur'],\n         'info_dict': {\n             'id': 'YcAQlkx',\n             'ext': 'mp4',\n             'title': 'Classic Steve Carell gif...cracks me up everytime....damn the repost downvotes....',\n-        }\n+            'timestamp': 1358554297,\n+            'upload_date': '20130119',\n+            'uploader_id': '1648642',\n+            'uploader': 'wittyusernamehere',\n+            'release_timestamp': 1358554297,\n+            'thumbnail': 'https://i.imgur.com/YcAQlkxh.jpg',\n+            'release_date': '20130119',\n+            'uploader_url': 'https://i.imgur.com/u3R4I2S_d.png?maxwidth=290&fidelity=grand',\n+            'comment_count': int,\n+            'dislike_count': int,\n+            'like_count': int,\n+        },\n     }, {\n+        # TODO: static image - replace with animated/video gallery\n         'url': 'http://imgur.com/topic/Funny/N8rOudd',\n         'only_matching': True,\n     }, {\n         'url': 'http://imgur.com/r/aww/VQcQPhM',\n-        'only_matching': True,\n+        'add_ies': ['Imgur'],\n+        'info_dict': {\n+            'id': 'VQcQPhM',\n+            'ext': 'mp4',\n+            'title': 'The boss is here',\n+            'timestamp': 1476494751,\n+            'upload_date': '20161015',\n+            'uploader_id': '19138530',\n+            'uploader': 'thematrixcam',\n+            'comment_count': int,\n+            'dislike_count': int,\n+            'uploader_url': 'https://i.imgur.com/qCjr5Pi_d.png?maxwidth=290&fidelity=grand',\n+            'release_timestamp': 1476494751,\n+            'like_count': int,\n+            'release_date': '20161015',\n+            'thumbnail': 'https://i.imgur.com/VQcQPhMh.jpg',\n+        },\n+    },\n+        # from https://github.com/ytdl-org/youtube-dl/pull/16674\n+        {\n+        'url': 'https://imgur.com/t/unmuted/6lAn9VQ',\n+        'info_dict': {\n+            'id': '6lAn9VQ',\n+            'title': 'Penguins !',\n+        },\n+        'playlist_count': 3,\n+    }, {\n+        'url': 'https://imgur.com/t/unmuted/kx2uD3C',\n+        'add_ies': ['Imgur'],\n+        'info_dict': {\n+            'id': 'ZVMv45i',\n+            'ext': 'mp4',\n+            'title': 'Intruder',\n+            'timestamp': 1528129683,\n+            'upload_date': '20180604',\n+            'release_timestamp': 1528129683,\n+            'release_date': '20180604',\n+            'like_count': int,\n+            'dislike_count': int,\n+            'comment_count': int,\n+            'duration': 30.03,\n+            'thumbnail': 'https://i.imgur.com/ZVMv45ih.jpg',\n+        },\n+    }, {\n+        'url': 'https://imgur.com/t/unmuted/wXSK0YH',\n+        'add_ies': ['Imgur'],\n+        'info_dict': {\n+            'id': 'JCAP4io',\n+            'ext': 'mp4',\n+            'title': 're:I got the blues$',\n+            'description': 'Luka\u2019s vocal stylings.\\n\\nFP edit: don\u2019t encourage me. I\u2019ll never stop posting Luka and friends.',\n+            'timestamp': 1527809525,\n+            'upload_date': '20180531',\n+            'like_count': int,\n+            'dislike_count': int,\n+            'duration': 30.03,\n+            'comment_count': int,\n+            'release_timestamp': 1527809525,\n+            'thumbnail': 'https://i.imgur.com/JCAP4ioh.jpg',\n+            'release_date': '20180531',\n+        },\n     }]\n \n-    def _real_extract(self, url):\n-        gallery_id = self._match_id(url)\n-\n-        data = self._download_json(\n-            'https://imgur.com/gallery/%s.json' % gallery_id,\n-            gallery_id)['data']['image']\n-\n-        if data.get('is_album'):\n-            entries = [\n-                self.url_result('http://imgur.com/%s' % image['hash'], ImgurIE.ie_key(), image['hash'])\n-                for image in data['album_images']['images'] if image.get('hash')]\n-            return self.playlist_result(entries, gallery_id, data.get('title'), data.get('description'))\n \n-        return self.url_result('http://imgur.com/%s' % gallery_id, ImgurIE.ie_key(), gallery_id)\n-\n-\n-class ImgurAlbumIE(ImgurGalleryIE):  # XXX: Do not subclass from concrete IE\n+class ImgurAlbumIE(ImgurGalleryBaseIE):\n     IE_NAME = 'imgur:album'\n     _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/a/(?P<id>[a-zA-Z0-9]+)'\n-\n+    _GALLERY = False\n     _TESTS = [{\n+        # TODO: only static images - replace with animated/video gallery\n         'url': 'http://imgur.com/a/j6Orj',\n+        'only_matching': True,\n+    },\n+        # from https://github.com/ytdl-org/youtube-dl/pull/21693\n+        {\n+        'url': 'https://imgur.com/a/iX265HX',\n+        'info_dict': {\n+            'id': 'iX265HX',\n+            'title': 'enen-no-shouboutai'\n+        },\n+        'playlist_count': 2,\n+    }, {\n+        'url': 'https://imgur.com/a/8pih2Ed',\n         'info_dict': {\n-            'id': 'j6Orj',\n-            'title': 'A Literary Analysis of \"Star Wars: The Force Awakens\"',\n+            'id': '8pih2Ed'\n         },\n-        'playlist_count': 12,\n+        'playlist_mincount': 1,\n     }]\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/indavideo.py",
            "diff": "diff --git a/yt_dlp/extractor/indavideo.py b/yt_dlp/extractor/indavideo.py\nindex 4fa97d8b..564bf8a0 100644\n--- a/yt_dlp/extractor/indavideo.py\n+++ b/yt_dlp/extractor/indavideo.py\n@@ -1,9 +1,9 @@\n from .common import InfoExtractor\n-from ..compat import compat_str\n from ..utils import (\n     int_or_none,\n     parse_age_limit,\n     parse_iso8601,\n+    time_seconds,\n     update_url_query,\n )\n \n@@ -11,15 +11,14 @@\n class IndavideoEmbedIE(InfoExtractor):\n     _VALID_URL = r'https?://(?:(?:embed\\.)?indavideo\\.hu/player/video/|assets\\.indavideo\\.hu/swf/player\\.swf\\?.*\\b(?:v(?:ID|id))=)(?P<id>[\\da-f]+)'\n     # Some example URLs covered by generic extractor:\n-    #   http://indavideo.hu/video/Vicces_cica_1\n-    #   http://index.indavideo.hu/video/2015_0728_beregszasz\n-    #   http://auto.indavideo.hu/video/Sajat_utanfutoban_a_kis_tacsko\n-    #   http://erotika.indavideo.hu/video/Amator_tini_punci\n-    #   http://film.indavideo.hu/video/f_hrom_nagymamm_volt\n-    #   http://palyazat.indavideo.hu/video/Embertelen_dal_Dodgem_egyuttes\n-    _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)?//embed\\.indavideo\\.hu/player/video/[\\da-f]+)']\n+    #   https://indavideo.hu/video/Vicces_cica_1\n+    #   https://index.indavideo.hu/video/Hod_Nemetorszagban\n+    #   https://auto.indavideo.hu/video/Sajat_utanfutoban_a_kis_tacsko\n+    #   https://film.indavideo.hu/video/f_farkaslesen\n+    #   https://palyazat.indavideo.hu/video/Embertelen_dal_Dodgem_egyuttes\n+    _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)//embed\\.indavideo\\.hu/player/video/[\\da-f]+)']\n     _TESTS = [{\n-        'url': 'http://indavideo.hu/player/video/1bdc3c6d80/',\n+        'url': 'https://indavideo.hu/player/video/1bdc3c6d80/',\n         'md5': 'c8a507a1c7410685f83a06eaeeaafeab',\n         'info_dict': {\n             'id': '1837039',\n@@ -36,21 +35,33 @@ class IndavideoEmbedIE(InfoExtractor):\n             'tags': ['t\u00e1nc', 'cica', 'cuki', 'cukiajanlo', 'newsroom'],\n         },\n     }, {\n-        'url': 'http://embed.indavideo.hu/player/video/1bdc3c6d80?autostart=1&hide=1',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://assets.indavideo.hu/swf/player.swf?v=fe25e500&vID=1bdc3c6d80&autostart=1&hide=1&i=1',\n+        'url': 'https://embed.indavideo.hu/player/video/1bdc3c6d80?autostart=1&hide=1',\n         'only_matching': True,\n     }]\n+    _WEBPAGE_TESTS = [{\n+        'url': 'https://indavideo.hu/video/Vicces_cica_1',\n+        'info_dict': {\n+            'id': '1335611',\n+            'ext': 'mp4',\n+            'title': 'Vicces cica',\n+            'description': 'J\u00e1tszik a tablettel. :D',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'uploader': 'Jet_Pack',\n+            'uploader_id': '491217',\n+            'timestamp': 1390821212,\n+            'upload_date': '20140127',\n+            'duration': 7,\n+            'age_limit': 0,\n+            'tags': ['cica', 'Jet_Pack'],\n+        },\n+    }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n \n         video = self._download_json(\n-            'https://amfphp.indavideo.hu/SYm0json.php/player.playerHandler.getVideoData/%s' % video_id,\n-            video_id)['data']\n-\n-        title = video['title']\n+            f'https://amfphp.indavideo.hu/SYm0json.php/player.playerHandler.getVideoData/{video_id}/',\n+            video_id, query={'_': time_seconds()})['data']\n \n         video_urls = []\n \n@@ -60,33 +71,21 @@ def _real_extract(self, url):\n         elif isinstance(video_files, dict):\n             video_urls.extend(video_files.values())\n \n-        video_file = video.get('video_file')\n-        if video:\n-            video_urls.append(video_file)\n         video_urls = list(set(video_urls))\n \n-        video_prefix = video_urls[0].rsplit('/', 1)[0]\n-\n-        for flv_file in video.get('flv_files', []):\n-            flv_url = '%s/%s' % (video_prefix, flv_file)\n-            if flv_url not in video_urls:\n-                video_urls.append(flv_url)\n-\n-        filesh = video.get('filesh')\n+        filesh = video.get('filesh') or {}\n \n         formats = []\n         for video_url in video_urls:\n             height = int_or_none(self._search_regex(\n                 r'\\.(\\d{3,4})\\.mp4(?:\\?|$)', video_url, 'height', default=None))\n-            if filesh:\n-                if not height:\n-                    continue\n-                token = filesh.get(compat_str(height))\n-                if token is None:\n-                    continue\n-                video_url = update_url_query(video_url, {'token': token})\n+            if not height and len(filesh) == 1:\n+                height = int_or_none(list(filesh.keys())[0])\n+            token = filesh.get(str(height))\n+            if token is None:\n+                continue\n             formats.append({\n-                'url': video_url,\n+                'url': update_url_query(video_url, {'token': token}),\n                 'height': height,\n             })\n \n@@ -103,7 +102,7 @@ def _real_extract(self, url):\n \n         return {\n             'id': video.get('id') or video_id,\n-            'title': title,\n+            'title': video.get('title'),\n             'description': video.get('description'),\n             'thumbnails': thumbnails,\n             'uploader': video.get('user_name'),\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/instagram.py",
            "diff": "diff --git a/yt_dlp/extractor/instagram.py b/yt_dlp/extractor/instagram.py\nindex bfc4b7b8..dbaa332c 100644\n--- a/yt_dlp/extractor/instagram.py\n+++ b/yt_dlp/extractor/instagram.py\n@@ -10,6 +10,7 @@\n     ExtractorError,\n     decode_base_n,\n     encode_base_n,\n+    filter_dict,\n     float_or_none,\n     format_field,\n     get_element_by_attribute,\n@@ -703,28 +704,31 @@ def _real_extract(self, url):\n         user_info = self._search_json(r'\"user\":', story_info, 'user info', story_id, fatal=False)\n         if not user_info:\n             self.raise_login_required('This content is unreachable')\n-        user_id = user_info.get('id')\n \n+        user_id = traverse_obj(user_info, 'pk', 'id', expected_type=str)\n         story_info_url = user_id if username != 'highlights' else f'highlight:{story_id}'\n+        if not story_info_url:  # user id is only mandatory for non-highlights\n+            raise ExtractorError('Unable to extract user id')\n+\n         videos = traverse_obj(self._download_json(\n             f'{self._API_BASE_URL}/feed/reels_media/?reel_ids={story_info_url}',\n             story_id, errnote=False, fatal=False, headers=self._API_HEADERS), 'reels')\n         if not videos:\n             self.raise_login_required('You need to log in to access this content')\n \n-        full_name = traverse_obj(videos, (f'highlight:{story_id}', 'user', 'full_name'), (str(user_id), 'user', 'full_name'))\n+        full_name = traverse_obj(videos, (f'highlight:{story_id}', 'user', 'full_name'), (user_id, 'user', 'full_name'))\n         story_title = traverse_obj(videos, (f'highlight:{story_id}', 'title'))\n         if not story_title:\n             story_title = f'Story by {username}'\n \n-        highlights = traverse_obj(videos, (f'highlight:{story_id}', 'items'), (str(user_id), 'items'))\n+        highlights = traverse_obj(videos, (f'highlight:{story_id}', 'items'), (user_id, 'items'))\n         info_data = []\n         for highlight in highlights:\n             highlight_data = self._extract_product(highlight)\n             if highlight_data.get('formats'):\n                 info_data.append({\n-                    **highlight_data,\n                     'uploader': full_name,\n                     'uploader_id': user_id,\n+                    **filter_dict(highlight_data),\n                 })\n         return self.playlist_result(info_data, playlist_id=story_id, playlist_title=story_title)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/iprima.py",
            "diff": "diff --git a/yt_dlp/extractor/iprima.py b/yt_dlp/extractor/iprima.py\nindex 6dec1510..f7aa579b 100644\n--- a/yt_dlp/extractor/iprima.py\n+++ b/yt_dlp/extractor/iprima.py\n@@ -134,10 +134,17 @@ def _real_extract(self, url):\n         ), webpage, 'real id', group='id', default=None)\n \n         if not video_id:\n-            nuxt_data = self._search_nuxt_data(webpage, video_id, traverse='data')\n+            nuxt_data = self._search_nuxt_data(webpage, video_id, traverse='data', fatal=False)\n             video_id = traverse_obj(\n                 nuxt_data, (..., 'content', 'additionals', 'videoPlayId', {str}), get_all=False)\n \n+        if not video_id:\n+            nuxt_data = self._search_json(\n+                r'<script[^>]+\\bid=[\"\\']__NUXT_DATA__[\"\\'][^>]*>',\n+                webpage, 'nuxt data', None, end_pattern=r'</script>', contains_pattern=r'\\[(?s:.+)\\]')\n+\n+            video_id = traverse_obj(nuxt_data, lambda _, v: re.fullmatch(r'p\\d+', v), get_all=False)\n+\n         if not video_id:\n             self.raise_no_formats('Unable to extract video ID from webpage')\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/iqiyi.py",
            "diff": "diff --git a/yt_dlp/extractor/iqiyi.py b/yt_dlp/extractor/iqiyi.py\nindex fa602ba8..3368ab1d 100644\n--- a/yt_dlp/extractor/iqiyi.py\n+++ b/yt_dlp/extractor/iqiyi.py\n@@ -499,9 +499,10 @@ class IqIE(InfoExtractor):\n                     'tm': tm,\n                     'qdy': 'a',\n                     'qds': 0,\n-                    'k_ft1': 141287244169348,\n-                    'k_ft4': 34359746564,\n-                    'k_ft5': 1,\n+                    'k_ft1': '143486267424900',\n+                    'k_ft4': '1572868',\n+                    'k_ft7': '4',\n+                    'k_ft5': '1',\n                     'bop': JSON.stringify({\n                         'version': '10.0',\n                         'dfp': dfp\n@@ -529,14 +530,22 @@ def _extract_vms_player_js(self, webpage, video_id):\n         webpack_js_url = self._proto_relative_url(self._search_regex(\n             r'<script src=\"((?:https?:)?//stc\\.iqiyipic\\.com/_next/static/chunks/webpack-\\w+\\.js)\"', webpage, 'webpack URL'))\n         webpack_js = self._download_webpage(webpack_js_url, video_id, note='Downloading webpack JS', errnote='Unable to download webpack JS')\n+\n         webpack_map = self._search_json(\n             r'[\"\\']\\s*\\+\\s*', webpack_js, 'JS locations', video_id,\n             contains_pattern=r'{\\s*(?:\\d+\\s*:\\s*[\"\\'][\\da-f]+[\"\\']\\s*,?\\s*)+}',\n             end_pattern=r'\\[\\w+\\]\\+[\"\\']\\.js', transform_source=js_to_json)\n \n+        replacement_map = self._search_json(\n+            r'[\"\\']\\s*\\+\\(\\s*', webpack_js, 'replacement map', video_id,\n+            contains_pattern=r'{\\s*(?:\\d+\\s*:\\s*[\"\\'][\\w.-]+[\"\\']\\s*,?\\s*)+}',\n+            end_pattern=r'\\[\\w+\\]\\|\\|\\w+\\)\\+[\"\\']\\.', transform_source=js_to_json,\n+            fatal=False) or {}\n+\n         for module_index in reversed(webpack_map):\n+            real_module = replacement_map.get(module_index) or module_index\n             module_js = self._download_webpage(\n-                f'https://stc.iqiyipic.com/_next/static/chunks/{module_index}.{webpack_map[module_index]}.js',\n+                f'https://stc.iqiyipic.com/_next/static/chunks/{real_module}.{webpack_map[module_index]}.js',\n                 video_id, note=f'Downloading #{module_index} module JS', errnote='Unable to download module JS', fatal=False) or ''\n             if 'vms request' in module_js:\n                 self.cache.store('iq', 'player_js', module_js)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/itprotv.py",
            "diff": "diff --git a/yt_dlp/extractor/itprotv.py b/yt_dlp/extractor/itprotv.py\nindex 4ac12603..b9d5c196 100644\n--- a/yt_dlp/extractor/itprotv.py\n+++ b/yt_dlp/extractor/itprotv.py\n@@ -31,7 +31,7 @@ def _check_if_logged_in(self, webpage):\n \n \n class ITProTVIE(ITProTVBaseIE):\n-    _VALID_URL = r'https://app.itpro.tv/course/(?P<course>[\\w-]+)/(?P<id>[\\w-]+)'\n+    _VALID_URL = r'https://app\\.itpro\\.tv/course/(?P<course>[\\w-]+)/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://app.itpro.tv/course/guided-tour/introductionitprotv',\n         'md5': 'bca4a28c2667fd1a63052e71a94bb88c',\n@@ -102,7 +102,7 @@ def _real_extract(self, url):\n \n \n class ITProTVCourseIE(ITProTVBaseIE):\n-    _VALID_URL = r'https?://app.itpro.tv/course/(?P<id>[\\w-]+)/?(?:$|[#?])'\n+    _VALID_URL = r'https?://app\\.itpro\\.tv/course/(?P<id>[\\w-]+)/?(?:$|[#?])'\n     _TESTS = [\n         {\n             'url': 'https://app.itpro.tv/course/guided-tour',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/jable.py",
            "diff": "diff --git a/yt_dlp/extractor/jable.py b/yt_dlp/extractor/jable.py\nindex 84c3225e..71fed49e 100644\n--- a/yt_dlp/extractor/jable.py\n+++ b/yt_dlp/extractor/jable.py\n@@ -10,7 +10,7 @@\n \n \n class JableIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?jable.tv/videos/(?P<id>[\\w-]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?jable\\.tv/videos/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://jable.tv/videos/pppd-812/',\n         'md5': 'f1537283a9bc073c31ff86ca35d9b2a6',\n@@ -64,7 +64,7 @@ def _real_extract(self, url):\n \n \n class JablePlaylistIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?jable.tv/(?:categories|models|tags)/(?P<id>[\\w-]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?jable\\.tv/(?:categories|models|tags)/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://jable.tv/models/kaede-karen/',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/jiosaavn.py",
            "diff": "diff --git a/yt_dlp/extractor/jiosaavn.py b/yt_dlp/extractor/jiosaavn.py\nnew file mode 100644\nindex 00000000..552b73f7\n--- /dev/null\n+++ b/yt_dlp/extractor/jiosaavn.py\n@@ -0,0 +1,79 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    js_to_json,\n+    url_or_none,\n+    urlencode_postdata,\n+    urljoin,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class JioSaavnBaseIE(InfoExtractor):\n+    def _extract_initial_data(self, url, audio_id):\n+        webpage = self._download_webpage(url, audio_id)\n+        return self._search_json(\n+            r'window\\.__INITIAL_DATA__\\s*=', webpage,\n+            'init json', audio_id, transform_source=js_to_json)\n+\n+\n+class JioSaavnSongIE(JioSaavnBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?(?:jiosaavn\\.com/song/[^/?#]+/|saavn\\.com/s/song/(?:[^/?#]+/){3})(?P<id>[^/?#]+)'\n+    _TESTS = [{\n+        'url': 'https://www.jiosaavn.com/song/leja-re/OQsEfQFVUXk',\n+        'md5': '7b1f70de088ede3a152ea34aece4df42',\n+        'info_dict': {\n+            'id': 'OQsEfQFVUXk',\n+            'ext': 'mp3',\n+            'title': 'Leja Re',\n+            'album': 'Leja Re',\n+            'thumbnail': 'https://c.saavncdn.com/258/Leja-Re-Hindi-2018-20181124024539-500x500.jpg',\n+        },\n+    }, {\n+        'url': 'https://www.saavn.com/s/song/hindi/Saathiya/O-Humdum-Suniyo-Re/KAMiazoCblU',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        audio_id = self._match_id(url)\n+        song_data = self._extract_initial_data(url, audio_id)['song']['song']\n+        media_data = self._download_json(\n+            'https://www.jiosaavn.com/api.php', audio_id, data=urlencode_postdata({\n+                '__call': 'song.generateAuthToken',\n+                '_format': 'json',\n+                'bitrate': '128',\n+                'url': song_data['encrypted_media_url'],\n+            }))\n+\n+        return {\n+            'id': audio_id,\n+            'url': media_data['auth_url'],\n+            'ext': media_data.get('type'),\n+            'vcodec': 'none',\n+            **traverse_obj(song_data, {\n+                'title': ('title', 'text'),\n+                'album': ('album', 'text'),\n+                'thumbnail': ('image', 0, {url_or_none}),\n+            }),\n+        }\n+\n+\n+class JioSaavnAlbumIE(JioSaavnBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?(?:jio)?saavn\\.com/album/[^/?#]+/(?P<id>[^/?#]+)'\n+    _TESTS = [{\n+        'url': 'https://www.jiosaavn.com/album/96/buIOjYZDrNA_',\n+        'info_dict': {\n+            'id': 'buIOjYZDrNA_',\n+            'title': '96',\n+        },\n+        'playlist_count': 10,\n+    }]\n+\n+    def _real_extract(self, url):\n+        album_id = self._match_id(url)\n+        album_view = self._extract_initial_data(url, album_id)['albumView']\n+\n+        return self.playlist_from_matches(\n+            traverse_obj(album_view, (\n+                'modules', lambda _, x: x['key'] == 'list', 'data', ..., 'title', 'action', {str})),\n+            album_id, traverse_obj(album_view, ('album', 'title', 'text', {str})), ie=JioSaavnSongIE,\n+            getter=lambda x: urljoin('https://www.jiosaavn.com/', x))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/joqrag.py",
            "diff": "diff --git a/yt_dlp/extractor/joqrag.py b/yt_dlp/extractor/joqrag.py\nnew file mode 100644\nindex 00000000..3bb28af9\n--- /dev/null\n+++ b/yt_dlp/extractor/joqrag.py\n@@ -0,0 +1,112 @@\n+import datetime\n+import urllib.parse\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    datetime_from_str,\n+    unified_timestamp,\n+    urljoin,\n+)\n+\n+\n+class JoqrAgIE(InfoExtractor):\n+    IE_DESC = '\u8d85!A&G+ \u6587\u5316\u653e\u9001 (f.k.a. AGQR) Nippon Cultural Broadcasting, Inc. (JOQR)'\n+    _VALID_URL = [r'https?://www\\.uniqueradio\\.jp/agplayer5/(?:player|inc-player-hls)\\.php',\n+                  r'https?://(?:www\\.)?joqr\\.co\\.jp/ag/',\n+                  r'https?://(?:www\\.)?joqr\\.co\\.jp/qr/ag(?:daily|regular)program/?(?:$|[#?])']\n+    _TESTS = [{\n+        'url': 'https://www.uniqueradio.jp/agplayer5/player.php',\n+        'info_dict': {\n+            'id': 'live',\n+            'title': str,\n+            'channel': '\u8d85!A&G+',\n+            'description': str,\n+            'live_status': 'is_live',\n+            'release_timestamp': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True,\n+        },\n+    }, {\n+        'url': 'https://www.uniqueradio.jp/agplayer5/inc-player-hls.php',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.joqr.co.jp/ag/article/103760/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'http://www.joqr.co.jp/qr/agdailyprogram/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'http://www.joqr.co.jp/qr/agregularprogram/',\n+        'only_matching': True,\n+    }]\n+\n+    def _extract_metadata(self, variable, html):\n+        return clean_html(urllib.parse.unquote_plus(self._search_regex(\n+            rf'var\\s+{variable}\\s*=\\s*([\"\\'])(?P<value>(?:(?!\\1).)+)\\1',\n+            html, 'metadata', group='value', default=''))) or None\n+\n+    def _extract_start_timestamp(self, video_id, is_live):\n+        def extract_start_time_from(date_str):\n+            dt = datetime_from_str(date_str) + datetime.timedelta(hours=9)\n+            date = dt.strftime('%Y%m%d')\n+            start_time = self._search_regex(\n+                r'<h3[^>]+\\bclass=\"dailyProgram-itemHeaderTime\"[^>]*>[\\s\\d:]+\u2013\\s*(\\d{1,2}:\\d{1,2})',\n+                self._download_webpage(\n+                    f'https://www.joqr.co.jp/qr/agdailyprogram/?date={date}', video_id,\n+                    note=f'Downloading program list of {date}', fatal=False,\n+                    errnote=f'Failed to download program list of {date}') or '',\n+                'start time', default=None)\n+            if start_time:\n+                return unified_timestamp(f'{dt.strftime(\"%Y/%m/%d\")} {start_time} +09:00')\n+            return None\n+\n+        start_timestamp = extract_start_time_from('today')\n+        if not start_timestamp:\n+            return None\n+\n+        if not is_live or start_timestamp < datetime_from_str('now').timestamp():\n+            return start_timestamp\n+        else:\n+            return extract_start_time_from('yesterday')\n+\n+    def _real_extract(self, url):\n+        video_id = 'live'\n+\n+        metadata = self._download_webpage(\n+            'https://www.uniqueradio.jp/aandg', video_id,\n+            note='Downloading metadata', errnote='Failed to download metadata')\n+        title = self._extract_metadata('Program_name', metadata)\n+\n+        if title == '\u653e\u9001\u4f11\u6b62':\n+            formats = []\n+            live_status = 'is_upcoming'\n+            release_timestamp = self._extract_start_timestamp(video_id, False)\n+            msg = 'This stream is not currently live'\n+            if release_timestamp:\n+                msg += (' and will start at '\n+                        + datetime.datetime.fromtimestamp(release_timestamp).strftime('%Y-%m-%d %H:%M:%S'))\n+            self.raise_no_formats(msg, expected=True)\n+        else:\n+            m3u8_path = self._search_regex(\n+                r'<source\\s[^>]*\\bsrc=\"([^\"]+)\"',\n+                self._download_webpage(\n+                    'https://www.uniqueradio.jp/agplayer5/inc-player-hls.php', video_id,\n+                    note='Downloading player data', errnote='Failed to download player data'),\n+                'm3u8 url')\n+            formats = self._extract_m3u8_formats(\n+                urljoin('https://www.uniqueradio.jp/', m3u8_path), video_id)\n+            live_status = 'is_live'\n+            release_timestamp = self._extract_start_timestamp(video_id, True)\n+\n+        return {\n+            'id': video_id,\n+            'title': title,\n+            'channel': '\u8d85!A&G+',\n+            'description': self._extract_metadata('Program_text', metadata),\n+            'formats': formats,\n+            'live_status': live_status,\n+            'release_timestamp': release_timestamp,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/jtbc.py",
            "diff": "diff --git a/yt_dlp/extractor/jtbc.py b/yt_dlp/extractor/jtbc.py\nnew file mode 100644\nindex 00000000..573f7492\n--- /dev/null\n+++ b/yt_dlp/extractor/jtbc.py\n@@ -0,0 +1,156 @@\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    int_or_none,\n+    parse_duration,\n+    url_or_none,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class JTBCIE(InfoExtractor):\n+    IE_DESC = 'jtbc.co.kr'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:\n+            vod\\.jtbc\\.co\\.kr/player/(?:program|clip)\n+            |tv\\.jtbc\\.co\\.kr/(?:replay|trailer|clip)/pr\\d+/pm\\d+\n+        )/(?P<id>(?:ep|vo)\\d+)'''\n+    _GEO_COUNTRIES = ['KR']\n+\n+    _TESTS = [{\n+        'url': 'https://tv.jtbc.co.kr/replay/pr10011629/pm10067930/ep20216321/view',\n+        'md5': 'e6ade71d8c8685bbfd6e6ce4167c6a6c',\n+        'info_dict': {\n+            'id': 'VO10721192',\n+            'display_id': 'ep20216321',\n+            'ext': 'mp4',\n+            'title': '\ud798\uc388\uc5ec\uc790 \uac15\ub0a8\uc21c 2\ud68c \ub2e4\uc2dc\ubcf4\uae30',\n+            'description': 'md5:043c1d9019100ce271dba09995dbd1e2',\n+            'duration': 3770.0,\n+            'release_date': '20231008',\n+            'age_limit': 15,\n+            'thumbnail': 'https://fs.jtbc.co.kr//joydata/CP00000001/prog/drama/stronggirlnamsoon/img/20231008_163541_522_1.jpg',\n+            'series': '\ud798\uc388\uc5ec\uc790 \uac15\ub0a8\uc21c',\n+        },\n+    }, {\n+        'url': 'https://vod.jtbc.co.kr/player/program/ep20216733',\n+        'md5': '217a6d190f115a75e4bda0ceaa4cd7f4',\n+        'info_dict': {\n+            'id': 'VO10721429',\n+            'display_id': 'ep20216733',\n+            'ext': 'mp4',\n+            'title': '\ud5ec\ub85c \ub9c8\uc774 \ub2e5\ud130 \uce5c\uc808\ud55c \uc9c4\ub8cc\uc2e4 149\ud68c \ub2e4\uc2dc\ubcf4\uae30',\n+            'description': 'md5:1d70788a982dd5de26874a92fcffddb8',\n+            'duration': 2720.0,\n+            'release_date': '20231009',\n+            'age_limit': 15,\n+            'thumbnail': 'https://fs.jtbc.co.kr//joydata/CP00000001/prog/culture/hellomydoctor/img/20231009_095002_528_1.jpg',\n+            'series': '\ud5ec\ub85c \ub9c8\uc774 \ub2e5\ud130 \uce5c\uc808\ud55c \uc9c4\ub8cc\uc2e4',\n+        },\n+    }, {\n+        'url': 'https://vod.jtbc.co.kr/player/clip/vo10721270',\n+        'md5': '05782e2dc22a9c548aebefe62ae4328a',\n+        'info_dict': {\n+            'id': 'VO10721270',\n+            'display_id': 'vo10721270',\n+            'ext': 'mp4',\n+            'title': '\ubb49\uccd0\uc57c \ucc2c\ub2e43 2\ud68c \uc608\uace0\ud3b8 - A\ub9e4\uce58\ub85c \ud5a5\ud558\ub294 \ub9c8\uc9c0\ub9c9 \uad00\ubb38\ud83d\udca5',\n+            'description': 'md5:d48b51a8655c84843b4ed8d0c39aae68',\n+            'duration': 46.0,\n+            'release_date': '20231015',\n+            'age_limit': 15,\n+            'thumbnail': 'https://fs.jtbc.co.kr//joydata/CP00000001/prog/enter/soccer3/img/20231008_210957_775_1.jpg',\n+            'series': '\ubb49\uccd0\uc57c \ucc2c\ub2e43',\n+        },\n+    }, {\n+        'url': 'https://tv.jtbc.co.kr/trailer/pr10010392/pm10032526/vo10720912/view',\n+        'md5': '367d480eb3ef54a9cd7a4b4d69c4b32d',\n+        'info_dict': {\n+            'id': 'VO10720912',\n+            'display_id': 'vo10720912',\n+            'ext': 'mp4',\n+            'title': '\uc544\ub294 \ud615\ub2d8 404\ud68c \uc608\uace0\ud3b8 | 10\uc6d4 14\uc77c(\ud1a0) \uc800\ub141 8\uc2dc 50\ubd84 \ubc29\uc1a1!',\n+            'description': 'md5:2743bb1079ceb85bb00060f2ad8f0280',\n+            'duration': 148.0,\n+            'release_date': '20231014',\n+            'age_limit': 15,\n+            'thumbnail': 'https://fs.jtbc.co.kr//joydata/CP00000001/prog/enter/jtbcbros/img/20231006_230023_802_1.jpg',\n+            'series': '\uc544\ub294 \ud615\ub2d8',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+\n+        if display_id.startswith('vo'):\n+            video_id = display_id.upper()\n+        else:\n+            webpage = self._download_webpage(url, display_id)\n+            video_id = self._search_regex(r'data-vod=\"(VO\\d+)\"', webpage, 'vod id')\n+\n+        playback_data = self._download_json(\n+            f'https://api.jtbc.co.kr/vod/{video_id}', video_id, note='Downloading VOD playback data')\n+\n+        subtitles = {}\n+        for sub in traverse_obj(playback_data, ('tracks', lambda _, v: v['file'])):\n+            subtitles.setdefault(sub.get('label', 'und'), []).append({'url': sub['file']})\n+\n+        formats = []\n+        for stream_url in traverse_obj(playback_data, ('sources', 'HLS', ..., 'file', {url_or_none})):\n+            stream_url = re.sub(r'/playlist(?:_pd\\d+)?\\.m3u8', '/index.m3u8', stream_url)\n+            formats.extend(self._extract_m3u8_formats(stream_url, video_id, fatal=False))\n+\n+        metadata = self._download_json(\n+            'https://now-api.jtbc.co.kr/v1/vod/detail', video_id,\n+            note='Downloading mobile details', fatal=False, query={'vodFileId': video_id})\n+        return {\n+            'id': video_id,\n+            'display_id': display_id,\n+            **traverse_obj(metadata, ('vodDetail', {\n+                'title': 'vodTitleView',\n+                'series': 'programTitle',\n+                'age_limit': ('watchAge', {int_or_none}),\n+                'release_date': ('broadcastDate', {lambda x: re.match(r'\\d{8}', x.replace('.', ''))}, 0),\n+                'description': 'episodeContents',\n+                'thumbnail': ('imgFileUrl', {url_or_none}),\n+            })),\n+            'duration': parse_duration(playback_data.get('playTime')),\n+            'formats': formats,\n+            'subtitles': subtitles,\n+        }\n+\n+\n+class JTBCProgramIE(InfoExtractor):\n+    IE_NAME = 'JTBC:program'\n+    _VALID_URL = r'https?://(?:vod\\.jtbc\\.co\\.kr/program|tv\\.jtbc\\.co\\.kr/replay)/(?P<id>pr\\d+)/(?:replay|pm\\d+)/?(?:$|[?#])'\n+\n+    _TESTS = [{\n+        'url': 'https://tv.jtbc.co.kr/replay/pr10010392/pm10032710',\n+        'info_dict': {\n+            '_type': 'playlist',\n+            'id': 'pr10010392',\n+        },\n+        'playlist_count': 398,\n+    }, {\n+        'url': 'https://vod.jtbc.co.kr/program/pr10011491/replay',\n+        'info_dict': {\n+            '_type': 'playlist',\n+            'id': 'pr10011491',\n+        },\n+        'playlist_count': 59,\n+    }]\n+\n+    def _real_extract(self, url):\n+        program_id = self._match_id(url)\n+\n+        vod_list = self._download_json(\n+            'https://now-api.jtbc.co.kr/v1/vodClip/programHome/programReplayVodList', program_id,\n+            note='Downloading program replay list', query={\n+                'programId': program_id,\n+                'rowCount': '10000',\n+            })\n+\n+        entries = [self.url_result(f'https://vod.jtbc.co.kr/player/program/{video_id}', JTBCIE, video_id)\n+                   for video_id in traverse_obj(vod_list, ('programReplayVodList', ..., 'episodeId'))]\n+        return self.playlist_result(entries, program_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/keezmovies.py",
            "diff": "diff --git a/yt_dlp/extractor/keezmovies.py b/yt_dlp/extractor/keezmovies.py\ndeleted file mode 100644\nindex b50da420..00000000\n--- a/yt_dlp/extractor/keezmovies.py\n+++ /dev/null\n@@ -1,125 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..aes import aes_decrypt_text\n-from ..compat import compat_urllib_parse_unquote\n-from ..utils import (\n-    determine_ext,\n-    format_field,\n-    int_or_none,\n-    str_to_int,\n-    strip_or_none,\n-    url_or_none,\n-)\n-\n-\n-class KeezMoviesIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?keezmovies\\.com/video/(?:(?P<display_id>[^/]+)-)?(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'https://www.keezmovies.com/video/arab-wife-want-it-so-bad-i-see-she-thirsty-and-has-tiny-money-18070681',\n-        'md5': '2ac69cdb882055f71d82db4311732a1a',\n-        'info_dict': {\n-            'id': '18070681',\n-            'display_id': 'arab-wife-want-it-so-bad-i-see-she-thirsty-and-has-tiny-money',\n-            'ext': 'mp4',\n-            'title': 'Arab wife want it so bad I see she thirsty and has tiny money.',\n-            'thumbnail': None,\n-            'view_count': int,\n-            'age_limit': 18,\n-        }\n-    }, {\n-        'url': 'http://www.keezmovies.com/video/18070681',\n-        'only_matching': True,\n-    }]\n-\n-    def _extract_info(self, url, fatal=True):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        display_id = (mobj.group('display_id')\n-                      if 'display_id' in mobj.groupdict()\n-                      else None) or mobj.group('id')\n-\n-        webpage = self._download_webpage(\n-            url, display_id, headers={'Cookie': 'age_verified=1'})\n-\n-        formats = []\n-        format_urls = set()\n-\n-        title = None\n-        thumbnail = None\n-        duration = None\n-        encrypted = False\n-\n-        def extract_format(format_url, height=None):\n-            format_url = url_or_none(format_url)\n-            if not format_url or not format_url.startswith(('http', '//')):\n-                return\n-            if format_url in format_urls:\n-                return\n-            format_urls.add(format_url)\n-            tbr = int_or_none(self._search_regex(\n-                r'[/_](\\d+)[kK][/_]', format_url, 'tbr', default=None))\n-            if not height:\n-                height = int_or_none(self._search_regex(\n-                    r'[/_](\\d+)[pP][/_]', format_url, 'height', default=None))\n-            if encrypted:\n-                format_url = aes_decrypt_text(\n-                    video_url, title, 32).decode('utf-8')\n-            formats.append({\n-                'url': format_url,\n-                'format_id': format_field(height, None, '%dp'),\n-                'height': height,\n-                'tbr': tbr,\n-            })\n-\n-        flashvars = self._parse_json(\n-            self._search_regex(\n-                r'flashvars\\s*=\\s*({.+?});', webpage,\n-                'flashvars', default='{}'),\n-            display_id, fatal=False)\n-\n-        if flashvars:\n-            title = flashvars.get('video_title')\n-            thumbnail = flashvars.get('image_url')\n-            duration = int_or_none(flashvars.get('video_duration'))\n-            encrypted = flashvars.get('encrypted') is True\n-            for key, value in flashvars.items():\n-                mobj = re.search(r'quality_(\\d+)[pP]', key)\n-                if mobj:\n-                    extract_format(value, int(mobj.group(1)))\n-            video_url = flashvars.get('video_url')\n-            if video_url and determine_ext(video_url, None):\n-                extract_format(video_url)\n-\n-        video_url = self._html_search_regex(\n-            r'flashvars\\.video_url\\s*=\\s*([\"\\'])(?P<url>http.+?)\\1',\n-            webpage, 'video url', default=None, group='url')\n-        if video_url:\n-            extract_format(compat_urllib_parse_unquote(video_url))\n-\n-        if not formats:\n-            if 'title=\"This video is no longer available\"' in webpage:\n-                self.raise_no_formats(\n-                    'Video %s is no longer available' % video_id, expected=True)\n-\n-        if not title:\n-            title = self._html_search_regex(\n-                r'<h1[^>]*>([^<]+)', webpage, 'title')\n-\n-        return webpage, {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': strip_or_none(title),\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'age_limit': 18,\n-            'formats': formats,\n-        }\n-\n-    def _real_extract(self, url):\n-        webpage, info = self._extract_info(url, fatal=False)\n-        if not info['formats']:\n-            return self.url_result(url, 'Generic')\n-        info['view_count'] = str_to_int(self._search_regex(\n-            r'<b>([\\d,.]+)</b> Views?', webpage, 'view count', fatal=False))\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/kinja.py",
            "diff": "diff --git a/yt_dlp/extractor/kinja.py b/yt_dlp/extractor/kinja.py\nindex df1386fb..f4e5c4c4 100644\n--- a/yt_dlp/extractor/kinja.py\n+++ b/yt_dlp/extractor/kinja.py\n@@ -12,7 +12,7 @@\n \n \n class KinjaEmbedIE(InfoExtractor):\n-    IENAME = 'kinja:embed'\n+    IE_NAME = 'kinja:embed'\n     _DOMAIN_REGEX = r'''(?:[^.]+\\.)?\n         (?:\n             avclub|\n@@ -41,7 +41,6 @@ class KinjaEmbedIE(InfoExtractor):\n             kinjavideo|\n             mcp|\n             megaphone|\n-            ooyala|\n             soundcloud(?:-playlist)?|\n             tumblr-post|\n             twitch-stream|\n@@ -61,9 +60,6 @@ class KinjaEmbedIE(InfoExtractor):\n     }, {\n         'url': 'https://kinja.com/ajax/inset/iframe?id=megaphone-PPY1300931075',\n         'only_matching': True,\n-    }, {\n-        'url': 'https://kinja.com/ajax/inset/iframe?id=ooyala-xzMXhleDpopuT0u1ijt_qZj3Va-34pEX%2FZTIxYmJjZDM2NWYzZDViZGRiOWJjYzc5',\n-        'only_matching': True,\n     }, {\n         'url': 'https://kinja.com/ajax/inset/iframe?id=soundcloud-128574047',\n         'only_matching': True,\n@@ -103,7 +99,6 @@ class KinjaEmbedIE(InfoExtractor):\n         'jwplayer-video': _JWPLATFORM_PROVIDER,\n         'jwp-video': _JWPLATFORM_PROVIDER,\n         'megaphone': ('player.megaphone.fm/', 'Generic'),\n-        'ooyala': ('player.ooyala.com/player.js?embedCode=', 'Ooyala'),\n         'soundcloud': ('api.soundcloud.com/tracks/', 'Soundcloud'),\n         'soundcloud-playlist': ('api.soundcloud.com/playlists/', 'SoundcloudPlaylist'),\n         'tumblr-post': ('%s.tumblr.com/post/%s', 'Tumblr'),\n@@ -129,8 +124,6 @@ def _real_extract(self, url):\n                 video_id, playlist_id = video_id.split('/')\n                 result_url = provider[0] % (video_id, playlist_id)\n             else:\n-                if video_type == 'ooyala':\n-                    video_id = video_id.split('/')[0]\n                 result_url = provider[0] + video_id\n             return self.url_result('http://' + result_url, provider[1])\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/kommunetv.py",
            "diff": "diff --git a/yt_dlp/extractor/kommunetv.py b/yt_dlp/extractor/kommunetv.py\nindex e21e556b..a30905b5 100644\n--- a/yt_dlp/extractor/kommunetv.py\n+++ b/yt_dlp/extractor/kommunetv.py\n@@ -3,7 +3,7 @@\n \n \n class KommunetvIE(InfoExtractor):\n-    _VALID_URL = r'https://(\\w+).kommunetv.no/archive/(?P<id>\\w+)'\n+    _VALID_URL = r'https://\\w+\\.kommunetv\\.no/archive/(?P<id>\\w+)'\n     _TEST = {\n         'url': 'https://oslo.kommunetv.no/archive/921',\n         'md5': '5f102be308ee759be1e12b63d5da4bbc',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/la7.py",
            "diff": "diff --git a/yt_dlp/extractor/la7.py b/yt_dlp/extractor/la7.py\nindex a3cd12b0..f5fd2413 100644\n--- a/yt_dlp/extractor/la7.py\n+++ b/yt_dlp/extractor/la7.py\n@@ -208,9 +208,9 @@ class LA7PodcastIE(LA7PodcastEpisodeIE):  # XXX: Do not subclass from concrete I\n         'url': 'https://www.la7.it/propagandalive/podcast',\n         'info_dict': {\n             'id': 'propagandalive',\n-            'title': \"Propaganda Live\",\n+            'title': 'Propaganda Live',\n         },\n-        'playlist_count_min': 10,\n+        'playlist_mincount': 10,\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/laola1tv.py",
            "diff": "diff --git a/yt_dlp/extractor/laola1tv.py b/yt_dlp/extractor/laola1tv.py\ndeleted file mode 100644\nindex 416dd7eb..00000000\n--- a/yt_dlp/extractor/laola1tv.py\n+++ /dev/null\n@@ -1,261 +0,0 @@\n-import json\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    ExtractorError,\n-    unified_strdate,\n-    urlencode_postdata,\n-    xpath_element,\n-    xpath_text,\n-    update_url_query,\n-    js_to_json,\n-)\n-\n-\n-class Laola1TvEmbedIE(InfoExtractor):\n-    IE_NAME = 'laola1tv:embed'\n-    _VALID_URL = r'https?://(?:www\\.)?laola1\\.tv/titanplayer\\.php\\?.*?\\bvideoid=(?P<id>\\d+)'\n-    _TESTS = [{\n-        # flashvars.premium = \"false\";\n-        'url': 'https://www.laola1.tv/titanplayer.php?videoid=708065&type=V&lang=en&portal=int&customer=1024',\n-        'info_dict': {\n-            'id': '708065',\n-            'ext': 'mp4',\n-            'title': 'MA Long CHN - FAN Zhendong CHN',\n-            'uploader': 'ITTF - International Table Tennis Federation',\n-            'upload_date': '20161211',\n-        },\n-    }]\n-\n-    def _extract_token_url(self, stream_access_url, video_id, data):\n-        return self._download_json(\n-            self._proto_relative_url(stream_access_url, 'https:'), video_id,\n-            headers={\n-                'Content-Type': 'application/json',\n-            }, data=json.dumps(data).encode())['data']['stream-access'][0]\n-\n-    def _extract_formats(self, token_url, video_id):\n-        token_doc = self._download_xml(\n-            token_url, video_id, 'Downloading token',\n-            headers=self.geo_verification_headers())\n-\n-        token_attrib = xpath_element(token_doc, './/token').attrib\n-\n-        if token_attrib['status'] != '0':\n-            raise ExtractorError(\n-                'Token error: %s' % token_attrib['comment'], expected=True)\n-\n-        formats = self._extract_akamai_formats(\n-            '%s?hdnea=%s' % (token_attrib['url'], token_attrib['auth']),\n-            video_id)\n-        return formats\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        flash_vars = self._search_regex(\n-            r'(?s)flashvars\\s*=\\s*({.+?});', webpage, 'flash vars')\n-\n-        def get_flashvar(x, *args, **kwargs):\n-            flash_var = self._search_regex(\n-                r'%s\\s*:\\s*\"([^\"]+)\"' % x,\n-                flash_vars, x, default=None)\n-            if not flash_var:\n-                flash_var = self._search_regex([\n-                    r'flashvars\\.%s\\s*=\\s*\"([^\"]+)\"' % x,\n-                    r'%s\\s*=\\s*\"([^\"]+)\"' % x],\n-                    webpage, x, *args, **kwargs)\n-            return flash_var\n-\n-        hd_doc = self._download_xml(\n-            'http://www.laola1.tv/server/hd_video.php', video_id, query={\n-                'play': get_flashvar('streamid'),\n-                'partner': get_flashvar('partnerid'),\n-                'portal': get_flashvar('portalid'),\n-                'lang': get_flashvar('sprache'),\n-                'v5ident': '',\n-            })\n-\n-        _v = lambda x, **k: xpath_text(hd_doc, './/video/' + x, **k)\n-        title = _v('title', fatal=True)\n-\n-        token_url = None\n-        premium = get_flashvar('premium', default=None)\n-        if premium:\n-            token_url = update_url_query(\n-                _v('url', fatal=True), {\n-                    'timestamp': get_flashvar('timestamp'),\n-                    'auth': get_flashvar('auth'),\n-                })\n-        else:\n-            data_abo = urlencode_postdata(\n-                dict((i, v) for i, v in enumerate(_v('req_liga_abos').split(','))))\n-            stream_access_url = update_url_query(\n-                'https://club.laola1.tv/sp/laola1/api/v3/user/session/premium/player/stream-access', {\n-                    'videoId': _v('id'),\n-                    'target': self._search_regex(r'vs_target = (\\d+);', webpage, 'vs target'),\n-                    'label': _v('label'),\n-                    'area': _v('area'),\n-                })\n-            token_url = self._extract_token_url(stream_access_url, video_id, data_abo)\n-\n-        formats = self._extract_formats(token_url, video_id)\n-\n-        categories_str = _v('meta_sports')\n-        categories = categories_str.split(',') if categories_str else []\n-        is_live = _v('islive') == 'true'\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'upload_date': unified_strdate(_v('time_date')),\n-            'uploader': _v('meta_organisation'),\n-            'categories': categories,\n-            'is_live': is_live,\n-            'formats': formats,\n-        }\n-\n-\n-class Laola1TvBaseIE(Laola1TvEmbedIE):  # XXX: Do not subclass from concrete IE\n-    def _extract_video(self, url):\n-        display_id = self._match_id(url)\n-        webpage = self._download_webpage(url, display_id)\n-\n-        if 'Dieser Livestream ist bereits beendet.' in webpage:\n-            raise ExtractorError('This live stream has already finished.', expected=True)\n-\n-        conf = self._parse_json(self._search_regex(\n-            r'(?s)conf\\s*=\\s*({.+?});', webpage, 'conf'),\n-            display_id,\n-            transform_source=lambda s: js_to_json(re.sub(r'shareurl:.+,', '', s)))\n-        video_id = conf['videoid']\n-\n-        config = self._download_json(conf['configUrl'], video_id, query={\n-            'videoid': video_id,\n-            'partnerid': conf['partnerid'],\n-            'language': conf.get('language', ''),\n-            'portal': conf.get('portalid', ''),\n-        })\n-        error = config.get('error')\n-        if error:\n-            raise ExtractorError('%s said: %s' % (self.IE_NAME, error), expected=True)\n-\n-        video_data = config['video']\n-        title = video_data['title']\n-        is_live = video_data.get('isLivestream') and video_data.get('isLive')\n-        meta = video_data.get('metaInformation')\n-        sports = meta.get('sports')\n-        categories = sports.split(',') if sports else []\n-\n-        token_url = self._extract_token_url(\n-            video_data['streamAccess'], video_id,\n-            video_data['abo']['required'])\n-\n-        formats = self._extract_formats(token_url, video_id)\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'description': video_data.get('description'),\n-            'thumbnail': video_data.get('image'),\n-            'categories': categories,\n-            'formats': formats,\n-            'is_live': is_live,\n-        }\n-\n-\n-class Laola1TvIE(Laola1TvBaseIE):\n-    IE_NAME = 'laola1tv'\n-    _VALID_URL = r'https?://(?:www\\.)?laola1\\.tv/[a-z]+-[a-z]+/[^/]+/(?P<id>[^/?#&]+)'\n-\n-    _TESTS = [{\n-        'url': 'http://www.laola1.tv/de-de/video/straubing-tigers-koelner-haie/227883.html',\n-        'info_dict': {\n-            'id': '227883',\n-            'display_id': 'straubing-tigers-koelner-haie',\n-            'ext': 'flv',\n-            'title': 'Straubing Tigers - K\u00f6lner Haie',\n-            'upload_date': '20140912',\n-            'is_live': False,\n-            'categories': ['Eishockey'],\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'http://www.laola1.tv/de-de/video/straubing-tigers-koelner-haie',\n-        'info_dict': {\n-            'id': '464602',\n-            'display_id': 'straubing-tigers-koelner-haie',\n-            'ext': 'flv',\n-            'title': 'Straubing Tigers - K\u00f6lner Haie',\n-            'upload_date': '20160129',\n-            'is_live': False,\n-            'categories': ['Eishockey'],\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'http://www.laola1.tv/de-de/livestream/2016-03-22-belogorie-belgorod-trentino-diatec-lde',\n-        'info_dict': {\n-            'id': '487850',\n-            'display_id': '2016-03-22-belogorie-belgorod-trentino-diatec-lde',\n-            'ext': 'flv',\n-            'title': 'Belogorie BELGOROD - TRENTINO Diatec',\n-            'upload_date': '20160322',\n-            'uploader': 'CEV - Europ\u00e4ischer Volleyball Verband',\n-            'is_live': True,\n-            'categories': ['Volleyball'],\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'skip': 'This live stream has already finished.',\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self._extract_video(url)\n-\n-\n-class EHFTVIE(Laola1TvBaseIE):\n-    IE_NAME = 'ehftv'\n-    _VALID_URL = r'https?://(?:www\\.)?ehftv\\.com/[a-z]+(?:-[a-z]+)?/[^/]+/(?P<id>[^/?#&]+)'\n-\n-    _TESTS = [{\n-        'url': 'https://www.ehftv.com/int/video/paris-saint-germain-handball-pge-vive-kielce/1166761',\n-        'info_dict': {\n-            'id': '1166761',\n-            'display_id': 'paris-saint-germain-handball-pge-vive-kielce',\n-            'ext': 'mp4',\n-            'title': 'Paris Saint-Germain Handball - PGE Vive Kielce',\n-            'is_live': False,\n-            'categories': ['Handball'],\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self._extract_video(url)\n-\n-\n-class ITTFIE(InfoExtractor):\n-    _VALID_URL = r'https?://tv\\.ittf\\.com/video/[^/]+/(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'https://tv.ittf.com/video/peng-wang-wei-matsudaira-kenta/951802',\n-        'only_matching': True,\n-    }\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            update_url_query('https://www.laola1.tv/titanplayer.php', {\n-                'videoid': self._match_id(url),\n-                'type': 'V',\n-                'lang': 'en',\n-                'portal': 'int',\n-                'customer': 1024,\n-            }), Laola1TvEmbedIE.ie_key())\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/laxarxames.py",
            "diff": "diff --git a/yt_dlp/extractor/laxarxames.py b/yt_dlp/extractor/laxarxames.py\nnew file mode 100644\nindex 00000000..e157f7c0\n--- /dev/null\n+++ b/yt_dlp/extractor/laxarxames.py\n@@ -0,0 +1,73 @@\n+import json\n+\n+from .brightcove import BrightcoveNewIE\n+from .common import InfoExtractor\n+from ..utils import ExtractorError\n+from ..utils.traversal import traverse_obj\n+\n+\n+class LaXarxaMesIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?laxarxames\\.cat/(?:[^/?#]+/)*?(player|movie-details)/(?P<id>\\d+)'\n+    _NETRC_MACHINE = 'laxarxames'\n+    _TOKEN = None\n+    _TESTS = [{\n+        'url': 'https://www.laxarxames.cat/player/3459421',\n+        'md5': '0966f46c34275934c19af78f3df6e2bc',\n+        'info_dict': {\n+            'id': '6339612436112',\n+            'ext': 'mp4',\n+            'title': 'Resum | UA Horta \u2014 UD Viladecans',\n+            'timestamp': 1697905186,\n+            'thumbnail': r're:https?://.*\\.jpg',\n+            'description': '',\n+            'upload_date': '20231021',\n+            'duration': 129.44,\n+            'tags': ['ott', 'esports', '23-24', ' futbol', ' futbol-partits', 'elit', 'resum'],\n+            'uploader_id': '5779379807001',\n+        },\n+        'skip': 'Requires login',\n+    }]\n+\n+    def _perform_login(self, username, password):\n+        if self._TOKEN:\n+            return\n+\n+        login = self._download_json(\n+            'https://api.laxarxames.cat/Authorization/SignIn', None, note='Logging in', headers={\n+                'X-Tenantorigin': 'https://laxarxames.cat',\n+                'Content-Type': 'application/json',\n+            }, data=json.dumps({\n+                'Username': username,\n+                'Password': password,\n+                'Device': {\n+                    'PlatformCode': 'WEB',\n+                    'Name': 'Mac OS ()',\n+                },\n+            }).encode(), expected_status=401)\n+\n+        self._TOKEN = traverse_obj(login, ('AuthorizationToken', 'Token', {str}))\n+        if not self._TOKEN:\n+            raise ExtractorError('Login failed', expected=True)\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        if not self._TOKEN:\n+            self.raise_login_required()\n+\n+        media_play_info = self._download_json(\n+            'https://api.laxarxames.cat/Media/GetMediaPlayInfo', video_id,\n+            data=json.dumps({\n+                'MediaId': int(video_id),\n+                'StreamType': 'MAIN'\n+            }).encode(), headers={\n+                'Authorization': f'Bearer {self._TOKEN}',\n+                'X-Tenantorigin': 'https://laxarxames.cat',\n+                'Content-Type': 'application/json',\n+            })\n+\n+        if not traverse_obj(media_play_info, ('ContentUrl', {str})):\n+            self.raise_no_formats('No video found', expected=True)\n+\n+        return self.url_result(\n+            f'https://players.brightcove.net/5779379807001/default_default/index.html?videoId={media_play_info[\"ContentUrl\"]}',\n+            BrightcoveNewIE, video_id, media_play_info.get('Title'))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/lbry.py",
            "diff": "diff --git a/yt_dlp/extractor/lbry.py b/yt_dlp/extractor/lbry.py\nindex 9a9f9256..cc37c41e 100644\n--- a/yt_dlp/extractor/lbry.py\n+++ b/yt_dlp/extractor/lbry.py\n@@ -22,10 +22,11 @@\n \n \n class LBRYBaseIE(InfoExtractor):\n-    _BASE_URL_REGEX = r'(?:https?://(?:www\\.)?(?:lbry\\.tv|odysee\\.com)/|lbry://)'\n+    _BASE_URL_REGEX = r'(?x)(?:https?://(?:www\\.)?(?:lbry\\.tv|odysee\\.com)/|lbry://)'\n     _CLAIM_ID_REGEX = r'[0-9a-f]{1,40}'\n-    _OPT_CLAIM_ID = '[^:/?#&]+(?:[:#]%s)?' % _CLAIM_ID_REGEX\n+    _OPT_CLAIM_ID = '[^$@:/?#&]+(?:[:#]%s)?' % _CLAIM_ID_REGEX\n     _SUPPORTED_STREAM_TYPES = ['video', 'audio']\n+    _PAGE_SIZE = 50\n \n     def _call_api_proxy(self, method, display_id, params, resource):\n         headers = {'Content-Type': 'application/json-rpc'}\n@@ -69,18 +70,78 @@ def _parse_stream(self, stream, url):\n             'duration': ('value', stream_type, 'duration', {int_or_none}),\n             'channel': ('signing_channel', 'value', 'title', {str}),\n             'channel_id': ('signing_channel', 'claim_id', {str}),\n+            'uploader_id': ('signing_channel', 'name', {str}),\n         })\n \n-        channel_name = traverse_obj(stream, ('signing_channel', 'name', {str}))\n-        if channel_name and info.get('channel_id'):\n-            info['channel_url'] = self._permanent_url(url, channel_name, info['channel_id'])\n+        if info.get('uploader_id') and info.get('channel_id'):\n+            info['channel_url'] = self._permanent_url(url, info['uploader_id'], info['channel_id'])\n \n         return info\n \n+    def _fetch_page(self, display_id, url, params, page):\n+        page += 1\n+        page_params = {\n+            'no_totals': True,\n+            'page': page,\n+            'page_size': self._PAGE_SIZE,\n+            **params,\n+        }\n+        result = self._call_api_proxy(\n+            'claim_search', display_id, page_params, f'page {page}')\n+        for item in traverse_obj(result, ('items', lambda _, v: v['name'] and v['claim_id'])):\n+            yield {\n+                **self._parse_stream(item, url),\n+                '_type': 'url',\n+                'id': item['claim_id'],\n+                'url': self._permanent_url(url, item['name'], item['claim_id']),\n+            }\n+\n+    def _playlist_entries(self, url, display_id, claim_param, metadata):\n+        qs = parse_qs(url)\n+        content = qs.get('content', [None])[0]\n+        params = {\n+            'fee_amount': qs.get('fee_amount', ['>=0'])[0],\n+            'order_by': {\n+                'new': ['release_time'],\n+                'top': ['effective_amount'],\n+                'trending': ['trending_group', 'trending_mixed'],\n+            }[qs.get('order', ['new'])[0]],\n+            'claim_type': 'stream',\n+            'stream_types': [content] if content in ['audio', 'video'] else self._SUPPORTED_STREAM_TYPES,\n+            **claim_param,\n+        }\n+        duration = qs.get('duration', [None])[0]\n+        if duration:\n+            params['duration'] = {\n+                'long': '>=1200',\n+                'short': '<=240',\n+            }[duration]\n+        language = qs.get('language', ['all'])[0]\n+        if language != 'all':\n+            languages = [language]\n+            if language == 'en':\n+                languages.append('none')\n+            params['any_languages'] = languages\n+\n+        entries = OnDemandPagedList(\n+            functools.partial(self._fetch_page, display_id, url, params),\n+            self._PAGE_SIZE)\n+\n+        return self.playlist_result(\n+            entries, display_id, **traverse_obj(metadata, ('value', {\n+                'title': 'title',\n+                'description': 'description',\n+            })))\n+\n \n class LBRYIE(LBRYBaseIE):\n     IE_NAME = 'lbry'\n-    _VALID_URL = LBRYBaseIE._BASE_URL_REGEX + r'(?P<id>\\$/[^/]+/[^/]+/{1}|@{0}/{0}|(?!@){0})'.format(LBRYBaseIE._OPT_CLAIM_ID, LBRYBaseIE._CLAIM_ID_REGEX)\n+    _VALID_URL = LBRYBaseIE._BASE_URL_REGEX + rf'''\n+        (?:\\$/(?:download|embed)/)?\n+        (?P<id>\n+            [^$@:/?#]+/{LBRYBaseIE._CLAIM_ID_REGEX}\n+            |(?:@{LBRYBaseIE._OPT_CLAIM_ID}/)?{LBRYBaseIE._OPT_CLAIM_ID}\n+        )'''\n     _TESTS = [{\n         # Video\n         'url': 'https://lbry.tv/@Mantega:1/First-day-LBRY:1',\n@@ -98,6 +159,7 @@ class LBRYIE(LBRYBaseIE):\n             'height': 720,\n             'thumbnail': 'https://spee.ch/7/67f2d809c263288c.png',\n             'license': 'None',\n+            'uploader_id': '@Mantega',\n             'duration': 346,\n             'channel': 'LBRY/Odysee rats united!!!',\n             'channel_id': '1c8ad6a2ab4e889a71146ae4deeb23bb92dab627',\n@@ -131,6 +193,7 @@ class LBRYIE(LBRYBaseIE):\n             'vcodec': 'none',\n             'thumbnail': 'https://spee.ch/d/0bc63b0e6bf1492d.png',\n             'license': 'None',\n+            'uploader_id': '@LBRYFoundation',\n         }\n     }, {\n         'url': 'https://odysee.com/@gardeningincanada:b/plants-i-will-never-grow-again.-the:e',\n@@ -149,6 +212,7 @@ class LBRYIE(LBRYBaseIE):\n             'channel': 'Gardening In Canada',\n             'channel_id': 'b8be0e93b423dad221abe29545fbe8ec36e806bc',\n             'channel_url': 'https://odysee.com/@gardeningincanada:b8be0e93b423dad221abe29545fbe8ec36e806bc',\n+            'uploader_id': '@gardeningincanada',\n             'formats': 'mincount:3',\n             'thumbnail': 'https://thumbnails.lbry.com/AgHSc_HzrrE',\n             'license': 'Copyrighted (contact publisher)',\n@@ -174,6 +238,7 @@ class LBRYIE(LBRYBaseIE):\n             'formats': 'mincount:1',\n             'thumbnail': 'startswith:https://thumb',\n             'license': 'None',\n+            'uploader_id': '@RT',\n         },\n         'params': {'skip_download': True}\n     }, {\n@@ -184,12 +249,13 @@ class LBRYIE(LBRYBaseIE):\n             'id': '41fbfe805eb73c8d3012c0c49faa0f563274f634',\n             'ext': 'mp4',\n             'title': 'Biotechnological Invasion of Skin (April 2023)',\n-            'description': 'md5:709a2f4c07bd8891cda3a7cc2d6fcf5c',\n+            'description': 'md5:fe28689db2cb7ba3436d819ac3ffc378',\n             'channel': 'Wicked Truths',\n             'channel_id': '23d2bbf856b0ceed5b1d7c5960bcc72da5a20cb0',\n             'channel_url': 'https://odysee.com/@wickedtruths:23d2bbf856b0ceed5b1d7c5960bcc72da5a20cb0',\n-            'timestamp': 1685790036,\n-            'upload_date': '20230603',\n+            'uploader_id': '@wickedtruths',\n+            'timestamp': 1695114347,\n+            'upload_date': '20230919',\n             'release_timestamp': 1685617473,\n             'release_date': '20230601',\n             'duration': 1063,\n@@ -229,10 +295,10 @@ class LBRYIE(LBRYBaseIE):\n \n     def _real_extract(self, url):\n         display_id = self._match_id(url)\n-        if display_id.startswith('$/'):\n-            display_id = display_id.split('/', 2)[-1].replace('/', ':')\n-        else:\n+        if display_id.startswith('@'):\n             display_id = display_id.replace(':', '#')\n+        else:\n+            display_id = display_id.replace('/', ':')\n         display_id = urllib.parse.unquote(display_id)\n         uri = 'lbry://' + display_id\n         result = self._resolve_url(uri, display_id, 'stream')\n@@ -299,7 +365,7 @@ def _real_extract(self, url):\n \n class LBRYChannelIE(LBRYBaseIE):\n     IE_NAME = 'lbry:channel'\n-    _VALID_URL = LBRYBaseIE._BASE_URL_REGEX + r'(?P<id>@%s)/?(?:[?&]|$)' % LBRYBaseIE._OPT_CLAIM_ID\n+    _VALID_URL = LBRYBaseIE._BASE_URL_REGEX + rf'(?P<id>@{LBRYBaseIE._OPT_CLAIM_ID})/?(?:[?&]|$)'\n     _TESTS = [{\n         'url': 'https://lbry.tv/@LBRYFoundation:0',\n         'info_dict': {\n@@ -315,65 +381,50 @@ class LBRYChannelIE(LBRYBaseIE):\n         'url': 'lbry://@lbry#3f',\n         'only_matching': True,\n     }]\n-    _PAGE_SIZE = 50\n-\n-    def _fetch_page(self, claim_id, url, params, page):\n-        page += 1\n-        page_params = {\n-            'channel_ids': [claim_id],\n-            'claim_type': 'stream',\n-            'no_totals': True,\n-            'page': page,\n-            'page_size': self._PAGE_SIZE,\n-        }\n-        page_params.update(params)\n-        result = self._call_api_proxy(\n-            'claim_search', claim_id, page_params, 'page %d' % page)\n-        for item in (result.get('items') or []):\n-            stream_claim_name = item.get('name')\n-            stream_claim_id = item.get('claim_id')\n-            if not (stream_claim_name and stream_claim_id):\n-                continue\n-\n-            yield {\n-                **self._parse_stream(item, url),\n-                '_type': 'url',\n-                'id': stream_claim_id,\n-                'url': self._permanent_url(url, stream_claim_name, stream_claim_id),\n-            }\n \n     def _real_extract(self, url):\n         display_id = self._match_id(url).replace(':', '#')\n-        result = self._resolve_url(\n-            'lbry://' + display_id, display_id, 'channel')\n+        result = self._resolve_url(f'lbry://{display_id}', display_id, 'channel')\n         claim_id = result['claim_id']\n-        qs = parse_qs(url)\n-        content = qs.get('content', [None])[0]\n-        params = {\n-            'fee_amount': qs.get('fee_amount', ['>=0'])[0],\n-            'order_by': {\n-                'new': ['release_time'],\n-                'top': ['effective_amount'],\n-                'trending': ['trending_group', 'trending_mixed'],\n-            }[qs.get('order', ['new'])[0]],\n-            'stream_types': [content] if content in ['audio', 'video'] else self._SUPPORTED_STREAM_TYPES,\n-        }\n-        duration = qs.get('duration', [None])[0]\n-        if duration:\n-            params['duration'] = {\n-                'long': '>=1200',\n-                'short': '<=240',\n-            }[duration]\n-        language = qs.get('language', ['all'])[0]\n-        if language != 'all':\n-            languages = [language]\n-            if language == 'en':\n-                languages.append('none')\n-            params['any_languages'] = languages\n-        entries = OnDemandPagedList(\n-            functools.partial(self._fetch_page, claim_id, url, params),\n-            self._PAGE_SIZE)\n-        result_value = result.get('value') or {}\n-        return self.playlist_result(\n-            entries, claim_id, result_value.get('title'),\n-            result_value.get('description'))\n+\n+        return self._playlist_entries(url, claim_id, {'channel_ids': [claim_id]}, result)\n+\n+\n+class LBRYPlaylistIE(LBRYBaseIE):\n+    IE_NAME = 'lbry:playlist'\n+    _VALID_URL = LBRYBaseIE._BASE_URL_REGEX + r'\\$/(?:play)?list/(?P<id>[0-9a-f-]+)'\n+    _TESTS = [{\n+        'url': 'https://odysee.com/$/playlist/ffef782f27486f0ac138bde8777f72ebdd0548c2',\n+        'info_dict': {\n+            'id': 'ffef782f27486f0ac138bde8777f72ebdd0548c2',\n+            'title': 'Th\u00e9\u00e2tre Classique',\n+            'description': 'Th\u00e9\u00e2tre Classique',\n+        },\n+        'playlist_mincount': 4,\n+    }, {\n+        'url': 'https://odysee.com/$/list/9c6658b3dd21e4f2a0602d523a13150e2b48b770',\n+        'info_dict': {\n+            'id': '9c6658b3dd21e4f2a0602d523a13150e2b48b770',\n+            'title': 'Social Media Exposed',\n+            'description': 'md5:98af97317aacd5b85d595775ea37d80e',\n+        },\n+        'playlist_mincount': 34,\n+    }, {\n+        'url': 'https://odysee.com/$/playlist/938fb11d-215f-4d1c-ad64-723954df2184',\n+        'info_dict': {\n+            'id': '938fb11d-215f-4d1c-ad64-723954df2184',\n+        },\n+        'playlist_mincount': 1000,\n+    }]\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+        result = traverse_obj(self._call_api_proxy('claim_search', display_id, {\n+            'claim_ids': [display_id],\n+            'no_totals': True,\n+            'page': 1,\n+            'page_size': self._PAGE_SIZE,\n+        }, 'playlist'), ('items', 0))\n+        claim_param = {'claim_ids': traverse_obj(result, ('value', 'claims', ..., {str}))}\n+\n+        return self._playlist_entries(url, display_id, claim_param, result)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/lecturio.py",
            "diff": "diff --git a/yt_dlp/extractor/lecturio.py b/yt_dlp/extractor/lecturio.py\nindex bb059d3a..79501254 100644\n--- a/yt_dlp/extractor/lecturio.py\n+++ b/yt_dlp/extractor/lecturio.py\n@@ -57,8 +57,8 @@ class LecturioIE(LecturioBaseIE):\n     _VALID_URL = r'''(?x)\n                     https://\n                         (?:\n-                            app\\.lecturio\\.com/([^/]+/(?P<nt>[^/?#&]+)\\.lecture|(?:\\#/)?lecture/c/\\d+/(?P<id>\\d+))|\n-                            (?:www\\.)?lecturio\\.de/[^/]+/(?P<nt_de>[^/?#&]+)\\.vortrag\n+                            app\\.lecturio\\.com/([^/?#]+/(?P<nt>[^/?#&]+)\\.lecture|(?:\\#/)?lecture/c/\\d+/(?P<id>\\d+))|\n+                            (?:www\\.)?lecturio\\.de/(?:[^/?#]+/)+(?P<nt_de>[^/?#&]+)\\.vortrag\n                         )\n                     '''\n     _TESTS = [{\n@@ -73,6 +73,9 @@ class LecturioIE(LecturioBaseIE):\n     }, {\n         'url': 'https://www.lecturio.de/jura/oeffentliches-recht-staatsexamen.vortrag',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://www.lecturio.de/jura/oeffentliches-recht-at-1-staatsexamen/oeffentliches-recht-staatsexamen.vortrag',\n+        'only_matching': True,\n     }, {\n         'url': 'https://app.lecturio.com/#/lecture/c/6434/39634',\n         'only_matching': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/linuxacademy.py",
            "diff": "diff --git a/yt_dlp/extractor/linuxacademy.py b/yt_dlp/extractor/linuxacademy.py\ndeleted file mode 100644\nindex 0b164429..00000000\n--- a/yt_dlp/extractor/linuxacademy.py\n+++ /dev/null\n@@ -1,238 +0,0 @@\n-import json\n-import random\n-\n-from .common import InfoExtractor\n-from ..compat import compat_b64decode, compat_str\n-from ..networking.exceptions import HTTPError\n-from ..utils import (\n-    clean_html,\n-    ExtractorError,\n-    js_to_json,\n-    parse_duration,\n-    try_get,\n-    unified_timestamp,\n-    urlencode_postdata,\n-    urljoin,\n-)\n-\n-\n-class LinuxAcademyIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?:www\\.)?linuxacademy\\.com/cp/\n-                        (?:\n-                            courses/lesson/course/(?P<chapter_id>\\d+)/lesson/(?P<lesson_id>\\d+)|\n-                            modules/view/id/(?P<course_id>\\d+)\n-                        )\n-                    '''\n-    _TESTS = [{\n-        'url': 'https://linuxacademy.com/cp/courses/lesson/course/7971/lesson/2/module/675',\n-        'info_dict': {\n-            'id': '7971-2',\n-            'ext': 'mp4',\n-            'title': 'What Is Data Science',\n-            'description': 'md5:c574a3c20607144fb36cb65bdde76c99',\n-            'timestamp': int,  # The timestamp and upload date changes\n-            'upload_date': r're:\\d+',\n-            'duration': 304,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'skip': 'Requires Linux Academy account credentials',\n-    }, {\n-        'url': 'https://linuxacademy.com/cp/courses/lesson/course/1498/lesson/2',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://linuxacademy.com/cp/modules/view/id/154',\n-        'info_dict': {\n-            'id': '154',\n-            'title': 'AWS Certified Cloud Practitioner',\n-            'description': 'md5:a68a299ca9bb98d41cca5abc4d4ce22c',\n-            'duration': 28835,\n-        },\n-        'playlist_count': 41,\n-        'skip': 'Requires Linux Academy account credentials',\n-    }, {\n-        'url': 'https://linuxacademy.com/cp/modules/view/id/39',\n-        'info_dict': {\n-            'id': '39',\n-            'title': 'Red Hat Certified Systems Administrator - RHCSA (EX200) Exam Prep  (legacy)',\n-            'description': 'md5:0f1d3369e90c3fb14a79813b863c902f',\n-            'duration': 89280,\n-        },\n-        'playlist_count': 73,\n-        'skip': 'Requires Linux Academy account credentials',\n-    }]\n-\n-    _AUTHORIZE_URL = 'https://login.linuxacademy.com/authorize'\n-    _ORIGIN_URL = 'https://linuxacademy.com'\n-    _CLIENT_ID = 'KaWxNn1C2Gc7n83W9OFeXltd8Utb5vvx'\n-    _NETRC_MACHINE = 'linuxacademy'\n-\n-    def _perform_login(self, username, password):\n-        def random_string():\n-            return ''.join(random.choices(\n-                '0123456789ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijklmnopqrstuvwxyz-._~', k=32))\n-\n-        webpage, urlh = self._download_webpage_handle(\n-            self._AUTHORIZE_URL, None, 'Downloading authorize page', query={\n-                'client_id': self._CLIENT_ID,\n-                'response_type': 'token id_token',\n-                'response_mode': 'web_message',\n-                'redirect_uri': self._ORIGIN_URL,\n-                'scope': 'openid email user_impersonation profile',\n-                'audience': self._ORIGIN_URL,\n-                'state': random_string(),\n-                'nonce': random_string(),\n-            })\n-\n-        login_data = self._parse_json(\n-            self._search_regex(\n-                r'atob\\(\\s*([\"\\'])(?P<value>(?:(?!\\1).)+)\\1', webpage,\n-                'login info', group='value'), None,\n-            transform_source=lambda x: compat_b64decode(x).decode('utf-8')\n-        )['extraParams']\n-\n-        login_data.update({\n-            'client_id': self._CLIENT_ID,\n-            'redirect_uri': self._ORIGIN_URL,\n-            'tenant': 'lacausers',\n-            'connection': 'Username-Password-ACG-Proxy',\n-            'username': username,\n-            'password': password,\n-            'sso': 'true',\n-        })\n-\n-        login_state_url = urlh.url\n-\n-        try:\n-            login_page = self._download_webpage(\n-                'https://login.linuxacademy.com/usernamepassword/login', None,\n-                'Downloading login page', data=json.dumps(login_data).encode(),\n-                headers={\n-                    'Content-Type': 'application/json',\n-                    'Origin': 'https://login.linuxacademy.com',\n-                    'Referer': login_state_url,\n-                })\n-        except ExtractorError as e:\n-            if isinstance(e.cause, HTTPError) and e.cause.status == 401:\n-                error = self._parse_json(e.cause.response.read(), None)\n-                message = error.get('description') or error['code']\n-                raise ExtractorError(\n-                    '%s said: %s' % (self.IE_NAME, message), expected=True)\n-            raise\n-\n-        callback_page, urlh = self._download_webpage_handle(\n-            'https://login.linuxacademy.com/login/callback', None,\n-            'Downloading callback page',\n-            data=urlencode_postdata(self._hidden_inputs(login_page)),\n-            headers={\n-                'Content-Type': 'application/x-www-form-urlencoded',\n-                'Origin': 'https://login.linuxacademy.com',\n-                'Referer': login_state_url,\n-            })\n-\n-        access_token = self._search_regex(\n-            r'access_token=([^=&]+)', urlh.url,\n-            'access token', default=None)\n-        if not access_token:\n-            access_token = self._parse_json(\n-                self._search_regex(\n-                    r'authorizationResponse\\s*=\\s*({.+?})\\s*;', callback_page,\n-                    'authorization response'), None,\n-                transform_source=js_to_json)['response']['access_token']\n-\n-        self._download_webpage(\n-            'https://linuxacademy.com/cp/login/tokenValidateLogin/token/%s'\n-            % access_token, None, 'Downloading token validation page')\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        chapter_id, lecture_id, course_id = mobj.group('chapter_id', 'lesson_id', 'course_id')\n-        item_id = course_id if course_id else '%s-%s' % (chapter_id, lecture_id)\n-\n-        webpage = self._download_webpage(url, item_id)\n-\n-        # course path\n-        if course_id:\n-            module = self._parse_json(\n-                self._search_regex(\n-                    r'window\\.module\\s*=\\s*({(?:(?!};)[^\"]|\"([^\"]|\\\\\")*\")+})\\s*;', webpage, 'module'),\n-                item_id)\n-            entries = []\n-            chapter_number = None\n-            chapter = None\n-            chapter_id = None\n-            for item in module['items']:\n-                if not isinstance(item, dict):\n-                    continue\n-\n-                def type_field(key):\n-                    return (try_get(item, lambda x: x['type'][key], compat_str) or '').lower()\n-                type_fields = (type_field('name'), type_field('slug'))\n-                # Move to next module section\n-                if 'section' in type_fields:\n-                    chapter = item.get('course_name')\n-                    chapter_id = item.get('course_module')\n-                    chapter_number = 1 if not chapter_number else chapter_number + 1\n-                    continue\n-                # Skip non-lessons\n-                if 'lesson' not in type_fields:\n-                    continue\n-                lesson_url = urljoin(url, item.get('url'))\n-                if not lesson_url:\n-                    continue\n-                title = item.get('title') or item.get('lesson_name')\n-                description = item.get('md_desc') or clean_html(item.get('description')) or clean_html(item.get('text'))\n-                entries.append({\n-                    '_type': 'url_transparent',\n-                    'url': lesson_url,\n-                    'ie_key': LinuxAcademyIE.ie_key(),\n-                    'title': title,\n-                    'description': description,\n-                    'timestamp': unified_timestamp(item.get('date')) or unified_timestamp(item.get('created_on')),\n-                    'duration': parse_duration(item.get('duration')),\n-                    'chapter': chapter,\n-                    'chapter_id': chapter_id,\n-                    'chapter_number': chapter_number,\n-                })\n-            return {\n-                '_type': 'playlist',\n-                'entries': entries,\n-                'id': course_id,\n-                'title': module.get('title'),\n-                'description': module.get('md_desc') or clean_html(module.get('desc')),\n-                'duration': parse_duration(module.get('duration')),\n-            }\n-\n-        # single video path\n-        m3u8_url = self._parse_json(\n-            self._search_regex(\n-                r'player\\.playlist\\s*=\\s*(\\[.+?\\])\\s*;', webpage, 'playlist'),\n-            item_id)[0]['file']\n-        formats = self._extract_m3u8_formats(\n-            m3u8_url, item_id, 'mp4', entry_protocol='m3u8_native',\n-            m3u8_id='hls')\n-        info = {\n-            'id': item_id,\n-            'formats': formats,\n-        }\n-        lesson = self._parse_json(\n-            self._search_regex(\n-                (r'window\\.lesson\\s*=\\s*({.+?})\\s*;',\n-                 r'player\\.lesson\\s*=\\s*({.+?})\\s*;'),\n-                webpage, 'lesson', default='{}'), item_id, fatal=False)\n-        if lesson:\n-            info.update({\n-                'title': lesson.get('lesson_name'),\n-                'description': lesson.get('md_desc') or clean_html(lesson.get('desc')),\n-                'timestamp': unified_timestamp(lesson.get('date')) or unified_timestamp(lesson.get('created_on')),\n-                'duration': parse_duration(lesson.get('duration')),\n-            })\n-        if not info.get('title'):\n-            info['title'] = self._search_regex(\n-                (r'>Lecture\\s*:\\s*(?P<value>[^<]+)',\n-                 r'lessonName\\s*=\\s*([\"\\'])(?P<value>(?:(?!\\1).)+)\\1'), webpage,\n-                'title', group='value')\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/litv.py",
            "diff": "diff --git a/yt_dlp/extractor/litv.py b/yt_dlp/extractor/litv.py\nindex 19b298ec..1003fb2f 100644\n--- a/yt_dlp/extractor/litv.py\n+++ b/yt_dlp/extractor/litv.py\n@@ -6,6 +6,7 @@\n     int_or_none,\n     smuggle_url,\n     traverse_obj,\n+    try_call,\n     unsmuggle_url,\n )\n \n@@ -13,7 +14,7 @@\n class LiTVIE(InfoExtractor):\n     _VALID_URL = r'https?://(?:www\\.)?litv\\.tv/(?:vod|promo)/[^/]+/(?:content\\.do)?\\?.*?\\b(?:content_)?id=(?P<id>[^&]+)'\n \n-    _URL_TEMPLATE = 'https://www.litv.tv/vod/%s/content.do?id=%s'\n+    _URL_TEMPLATE = 'https://www.litv.tv/vod/%s/content.do?content_id=%s'\n \n     _TESTS = [{\n         'url': 'https://www.litv.tv/vod/drama/content.do?brc_id=root&id=VOD00041610&isUHEnabled=true&autoPlay=1',\n@@ -21,16 +22,18 @@ class LiTVIE(InfoExtractor):\n             'id': 'VOD00041606',\n             'title': '\u82b1\u5343\u9aa8',\n         },\n-        'playlist_count': 50,\n+        'playlist_count': 51,  # 50 episodes + 1 trailer\n     }, {\n         'url': 'https://www.litv.tv/vod/drama/content.do?brc_id=root&id=VOD00041610&isUHEnabled=true&autoPlay=1',\n-        'md5': '969e343d9244778cb29acec608e53640',\n+        'md5': 'b90ff1e9f1d8f5cfcd0a44c3e2b34c7a',\n         'info_dict': {\n             'id': 'VOD00041610',\n             'ext': 'mp4',\n             'title': '\u82b1\u5343\u9aa8\u7b2c1\u96c6',\n             'thumbnail': r're:https?://.*\\.jpg$',\n-            'description': 'md5:c7017aa144c87467c4fb2909c4b05d6f',\n+            'description': '\u300a\u82b1\u5343\u9aa8\u300b\u9678\u5287\u7dda\u4e0a\u770b\u3002\u5341\u516d\u5e74\u524d\uff0c\u5e73\u975c\u7684\u6751\u838a\u5167\uff0c\u4e00\u540d\u5973\u5b30\u96a8\u7570\u76f8\u51fa\u751f\uff0c\u9014\u5f91\u6b64\u5730\u7684\u8700\u5c71\u638c\u9580\u6e05\u865b\u9053\u9577\u7b97\u51fa\u6b64\u5973\u547d\u904b\u975e\u540c\u4e00\u822c\uff0c\u5979\u9ad4\u5167\u6563\u767c\u7684\u7570\u9999\u6613\u62db\u60f9\u5996\u9b54\u3002\u4e00\u5ff5\u6148\u60b2\u4e0b\uff0c\u4ed6\u5728\u6751\u838a\u5468\u908a\u8a2d\u4e0b\u7d50\u754c\u963b\u64cb\u5996\u9b54\u5165\u4fb5\uff0c\u8b93\u5176\u5e74\u6eff\u5341\u516d\u5f8c\u53bb\u8700\u5c71\uff0c\u4e26\u8cdc\u540d\u82b1\u5343\u9aa8\u3002',\n+            'categories': ['\u5947\u5e7b', '\u611b\u60c5', '\u4e2d\u570b', '\u4ed9\u4fe0'],\n+            'episode': 'Episode 1',\n             'episode_number': 1,\n         },\n         'params': {\n@@ -46,20 +49,17 @@ class LiTVIE(InfoExtractor):\n             'title': '\u8288\u6708\u50b3\u7b2c1\u96c6\u3000\u9738\u661f\u8288\u6708\u964d\u4e16\u695a\u570b',\n             'description': '\u695a\u5a01\u738b\u4e8c\u5e74\uff0c\u592a\u53f2\u4ee4\u5510\u6627\u591c\u89c0\u661f\u8c61\uff0c\u767c\u73fe\u9738\u661f\u5373\u5c07\u73fe\u4e16\u3002\u738b\u540e\u5f97\u77e5\u9738\u661f\u7684\u9810\u8a00\u5f8c\uff0c\u60f3\u76e1\u8fa6\u6cd5\u4e0d\u8b93\u5b69\u5b50\u9806\u5229\u51fa\u751f\uff0c\u5e78\u5f97\u8392\u59ec\u76f8\u8b77\u5316\u89e3\u5371\u6a5f\u3002\u6c92\u60f3\u5230\u773e\u4eba\u671f\u5f85\u4e0b\u51fa\u751f\u7684\u9738\u661f\u537b\u662f\u4f4d\u516c\u4e3b\uff0c\u695a\u5a01\u738b\u5c0d\u6b64\u5931\u671b\u81f3\u6975\u3002\u695a\u738b\u540e\u547d\u4eba\u5c07\u5973\u5b30\u4e1f\u68c4\u6cb3\u4e2d\uff0c\u5c45\u7136\u5947\u8e5f\u4f3c\u7684\u88ab\u5c11\u53f8\u547d\u50cf\u6514\u4e0b\uff0c\u695a\u5a01\u738b\u8a8d\u70ba\u6b64\u5973\u975e\u540c\u51e1\u97ff\uff0c\u70ba\u5979\u53d6\u540d\u8288\u6708\u3002',\n         },\n-        'skip': 'Georestricted to Taiwan',\n+        'skip': 'No longer exists',\n     }]\n \n-    def _extract_playlist(self, season_list, video_id, program_info, prompt=True):\n-        episode_title = program_info['title']\n-        content_id = season_list['contentId']\n-\n+    def _extract_playlist(self, playlist_data, content_type):\n         all_episodes = [\n             self.url_result(smuggle_url(\n-                self._URL_TEMPLATE % (program_info['contentType'], episode['contentId']),\n+                self._URL_TEMPLATE % (content_type, episode['contentId']),\n                 {'force_noplaylist': True}))  # To prevent infinite recursion\n-            for episode in season_list['episode']]\n+            for episode in traverse_obj(playlist_data, ('seasons', ..., 'episode', lambda _, v: v['contentId']))]\n \n-        return self.playlist_result(all_episodes, content_id, episode_title)\n+        return self.playlist_result(all_episodes, playlist_data['contentId'], playlist_data.get('title'))\n \n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})\n@@ -68,35 +68,51 @@ def _real_extract(self, url):\n \n         webpage = self._download_webpage(url, video_id)\n \n+        if self._search_regex(\n+                r'(?i)<meta\\s[^>]*http-equiv=\"refresh\"\\s[^>]*content=\"[0-9]+;\\s*url=https://www\\.litv\\.tv/\"',\n+                webpage, 'meta refresh redirect', default=False, group=0):\n+            raise ExtractorError('No such content found', expected=True)\n+\n         program_info = self._parse_json(self._search_regex(\n             r'var\\s+programInfo\\s*=\\s*([^;]+)', webpage, 'VOD data', default='{}'),\n             video_id)\n \n-        season_list = list(program_info.get('seasonList', {}).values())\n-        playlist_id = traverse_obj(season_list, 0, 'contentId')\n-        if self._yes_playlist(playlist_id, video_id, smuggled_data):\n-            return self._extract_playlist(season_list[0], video_id, program_info)\n-\n-        # In browsers `getMainUrl` request is always issued. Usually this\n+        # In browsers `getProgramInfo` request is always issued. Usually this\n         # endpoint gives the same result as the data embedded in the webpage.\n-        # If georestricted, there are no embedded data, so an extra request is\n-        # necessary to get the error code\n+        # If, for some reason, there are no embedded data, we do an extra request.\n         if 'assetId' not in program_info:\n             program_info = self._download_json(\n                 'https://www.litv.tv/vod/ajax/getProgramInfo', video_id,\n                 query={'contentId': video_id},\n                 headers={'Accept': 'application/json'})\n+\n+        series_id = program_info['seriesId']\n+        if self._yes_playlist(series_id, video_id, smuggled_data):\n+            playlist_data = self._download_json(\n+                'https://www.litv.tv/vod/ajax/getSeriesTree', video_id,\n+                query={'seriesId': series_id}, headers={'Accept': 'application/json'})\n+            return self._extract_playlist(playlist_data, program_info['contentType'])\n+\n         video_data = self._parse_json(self._search_regex(\n             r'uiHlsUrl\\s*=\\s*testBackendData\\(([^;]+)\\);',\n             webpage, 'video data', default='{}'), video_id)\n         if not video_data:\n-            payload = {\n-                'assetId': program_info['assetId'],\n-                'watchDevices': program_info['watchDevices'],\n-                'contentType': program_info['contentType'],\n-            }\n+            payload = {'assetId': program_info['assetId']}\n+            puid = try_call(lambda: self._get_cookies('https://www.litv.tv/')['PUID'].value)\n+            if puid:\n+                payload.update({\n+                    'type': 'auth',\n+                    'puid': puid,\n+                })\n+                endpoint = 'getUrl'\n+            else:\n+                payload.update({\n+                    'watchDevices': program_info['watchDevices'],\n+                    'contentType': program_info['contentType'],\n+                })\n+                endpoint = 'getMainUrlNoAuth'\n             video_data = self._download_json(\n-                'https://www.litv.tv/vod/getMainUrl', video_id,\n+                f'https://www.litv.tv/vod/ajax/{endpoint}', video_id,\n                 data=json.dumps(payload).encode('utf-8'),\n                 headers={'Content-Type': 'application/json'})\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/m6.py",
            "diff": "diff --git a/yt_dlp/extractor/m6.py b/yt_dlp/extractor/m6.py\ndeleted file mode 100644\nindex 9dcc6016..00000000\n--- a/yt_dlp/extractor/m6.py\n+++ /dev/null\n@@ -1,22 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class M6IE(InfoExtractor):\n-    IE_NAME = 'm6'\n-    _VALID_URL = r'https?://(?:www\\.)?m6\\.fr/[^/]+/videos/(?P<id>\\d+)-[^\\.]+\\.html'\n-\n-    _TEST = {\n-        'url': 'http://www.m6.fr/emission-les_reines_du_shopping/videos/11323908-emeline_est_la_reine_du_shopping_sur_le_theme_ma_fete_d_8217_anniversaire.html',\n-        'md5': '242994a87de2c316891428e0176bcb77',\n-        'info_dict': {\n-            'id': '11323908',\n-            'ext': 'mp4',\n-            'title': 'Emeline est la Reine du Shopping sur le th\u00e8me \u00ab Ma f\u00eate d\u2019anniversaire ! \u00bb',\n-            'description': 'md5:1212ae8fb4b7baa4dc3886c5676007c2',\n-            'duration': 100,\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        return self.url_result('6play:%s' % video_id, 'SixPlay', video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/maariv.py",
            "diff": "diff --git a/yt_dlp/extractor/maariv.py b/yt_dlp/extractor/maariv.py\nnew file mode 100644\nindex 00000000..425a8b3b\n--- /dev/null\n+++ b/yt_dlp/extractor/maariv.py\n@@ -0,0 +1,62 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    int_or_none,\n+    parse_resolution,\n+    unified_timestamp,\n+    url_or_none,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class MaarivIE(InfoExtractor):\n+    IE_NAME = 'maariv.co.il'\n+    _VALID_URL = r'https?://player\\.maariv\\.co\\.il/public/player\\.html\\?(?:[^#]+&)?media=(?P<id>\\d+)'\n+    _EMBED_REGEX = [rf'<iframe[^>]+\\bsrc=[\\'\"](?P<url>{_VALID_URL})']\n+    _TESTS = [{\n+        'url': 'https://player.maariv.co.il/public/player.html?player=maariv-desktop&media=3611585',\n+        'info_dict': {\n+            'id': '3611585',\n+            'duration': 75,\n+            'ext': 'mp4',\n+            'upload_date': '20231009',\n+            'title': '\u05de\u05d1\u05e6\u05e2 \u05d7\u05e8\u05d1\u05d5\u05ea \u05d1\u05e8\u05d6\u05dc',\n+            'timestamp': 1696851301,\n+        },\n+    }]\n+    _WEBPAGE_TESTS = [{\n+        'url': 'https://www.maariv.co.il/news/law/Article-1044008',\n+        'info_dict': {\n+            'id': '3611585',\n+            'duration': 75,\n+            'ext': 'mp4',\n+            'upload_date': '20231009',\n+            'title': '\u05de\u05d1\u05e6\u05e2 \u05d7\u05e8\u05d1\u05d5\u05ea \u05d1\u05e8\u05d6\u05dc',\n+            'timestamp': 1696851301,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        data = self._download_json(\n+            f'https://dal.walla.co.il/media/{video_id}?origin=player.maariv.co.il', video_id)['data']\n+\n+        formats = []\n+        if hls_url := traverse_obj(data, ('video', 'url', {url_or_none})):\n+            formats.extend(self._extract_m3u8_formats(hls_url, video_id, m3u8_id='hls', fatal=False))\n+\n+        for http_format in traverse_obj(data, ('video', 'stream_urls', ..., 'stream_url', {url_or_none})):\n+            formats.append({\n+                'url': http_format,\n+                'format_id': 'http',\n+                **parse_resolution(http_format),\n+            })\n+\n+        return {\n+            'id': video_id,\n+            **traverse_obj(data, {\n+                'title': 'title',\n+                'duration': ('video', 'duration', {int_or_none}),\n+                'timestamp': ('upload_date', {unified_timestamp}),\n+            }),\n+            'formats': formats,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mainstreaming.py",
            "diff": "diff --git a/yt_dlp/extractor/mainstreaming.py b/yt_dlp/extractor/mainstreaming.py\nindex fe5589d5..fd9bba8b 100644\n--- a/yt_dlp/extractor/mainstreaming.py\n+++ b/yt_dlp/extractor/mainstreaming.py\n@@ -13,7 +13,7 @@\n \n \n class MainStreamingIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:webtools-?)?(?P<host>[A-Za-z0-9-]*\\.msvdn.net)/(?:embed|amp_embed|content)/(?P<id>\\w+)'\n+    _VALID_URL = r'https?://(?:webtools-?)?(?P<host>[A-Za-z0-9-]*\\.msvdn\\.net)/(?:embed|amp_embed|content)/(?P<id>\\w+)'\n     _EMBED_REGEX = [rf'<iframe[^>]+?src=[\"\\']?(?P<url>{_VALID_URL})[\"\\']?']\n     IE_DESC = 'MainStreaming Player'\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/massengeschmacktv.py",
            "diff": "diff --git a/yt_dlp/extractor/massengeschmacktv.py b/yt_dlp/extractor/massengeschmacktv.py\nindex 7dacb43e..1490e9b2 100644\n--- a/yt_dlp/extractor/massengeschmacktv.py\n+++ b/yt_dlp/extractor/massengeschmacktv.py\n@@ -17,11 +17,12 @@ class MassengeschmackTVIE(InfoExtractor):\n \n     _TEST = {\n         'url': 'https://massengeschmack.tv/play/fktv202',\n-        'md5': 'a9e054db9c2b5a08f0a0527cc201e8d3',\n+        'md5': '9996f314994a49fefe5f39aa1b07ae21',\n         'info_dict': {\n             'id': 'fktv202',\n             'ext': 'mp4',\n-            'title': 'Fernsehkritik-TV - Folge 202',\n+            'title': 'Fernsehkritik-TV #202',\n+            'thumbnail': 'https://cache.massengeschmack.tv/img/mag/fktv202.jpg'\n         },\n     }\n \n@@ -29,9 +30,6 @@ def _real_extract(self, url):\n         episode = self._match_id(url)\n \n         webpage = self._download_webpage(url, episode)\n-        title = clean_html(self._html_search_regex(\n-            '<h3>([^<]+)</h3>', webpage, 'title'))\n-        thumbnail = self._search_regex(r'POSTER\\s*=\\s*\"([^\"]+)', webpage, 'thumbnail', fatal=False)\n         sources = self._parse_json(self._search_regex(r'(?s)MEDIA\\s*=\\s*(\\[.+?\\]);', webpage, 'media'), episode, js_to_json)\n \n         formats = []\n@@ -67,7 +65,8 @@ def _real_extract(self, url):\n \n         return {\n             'id': episode,\n-            'title': title,\n+            'title': clean_html(self._html_search_regex(\n+                r'<span[^>]+\\bid=[\"\\']clip-title[\"\\'][^>]*>([^<]+)', webpage, 'title', fatal=False)),\n             'formats': formats,\n-            'thumbnail': thumbnail,\n+            'thumbnail': self._search_regex(r'POSTER\\s*=\\s*\"([^\"]+)', webpage, 'thumbnail', fatal=False),\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mbn.py",
            "diff": "diff --git a/yt_dlp/extractor/mbn.py b/yt_dlp/extractor/mbn.py\nnew file mode 100644\nindex 00000000..4917c469\n--- /dev/null\n+++ b/yt_dlp/extractor/mbn.py\n@@ -0,0 +1,89 @@\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    int_or_none,\n+    unified_strdate,\n+    url_or_none,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class MBNIE(InfoExtractor):\n+    IE_DESC = 'mbn.co.kr (\ub9e4\uc77c\ubc29\uc1a1)'\n+    _VALID_URL = r'https?://(?:www\\.)?mbn\\.co\\.kr/vod/programContents/preview(?:list)?/\\d+/\\d+/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://mbn.co.kr/vod/programContents/previewlist/861/5433/1276155',\n+        'md5': '85e1694e5b247c04d1386b7e3c90fd76',\n+        'info_dict': {\n+            'id': '1276155',\n+            'ext': 'mp4',\n+            'title': '\uacb0\uad6d \uc0ac\ub85c\uc7a1\ud78c \uad8c\uc720\ub9ac, \uadf8\ub140\ub97c \ubaa9\uc228 \uac78\uace0 \uad6c\ud558\ub824\ub294 \uc815\uc77c\uc6b0!',\n+            'duration': 3891,\n+            'release_date': '20210703',\n+            'thumbnail': 'http://img.vod.mbn.co.kr/mbnvod2img/861/2021/07/03/20210703230811_20_861_1276155_360_7_0.jpg',\n+            'series': '\ubcf4\uc308 - \uc6b4\uba85\uc744 \ud6d4\uce58\ub2e4',\n+            'episode': 'Episode 19',\n+            'episode_number': 19,\n+        },\n+    }, {\n+        'url': 'https://www.mbn.co.kr/vod/programContents/previewlist/835/5294/1084744',\n+        'md5': 'fc65d3aac85e85e0b5056f4ef99cde4a',\n+        'info_dict': {\n+            'id': '1084744',\n+            'ext': 'mp4',\n+            'title': '\uae40\uc815\uc740\u2665\ucd5c\uc6d0\uc601, \uc81c\uc790\ub9ac\ub97c \ucc3e\uc740 \uc704\ud5d8\ud55c \ubd80\ubd80! \uff02\uacb0\ud63c\uc740 \ud22c\uc7c1\uc774\uba74\uc11c, \uc5b4\ub824\uc6b4 \ubc29\uc2dd\uc774\uc57c..\uff02',\n+            'duration': 93,\n+            'release_date': '20201124',\n+            'thumbnail': 'http://img.vod.mbn.co.kr/mbnvod2img/835/2020/11/25/20201125000221_21_835_1084744_360_7_0.jpg',\n+            'series': '\ub098\uc758 \uc704\ud5d8\ud55c \uc544\ub0b4',\n+        },\n+    }, {\n+        'url': 'https://www.mbn.co.kr/vod/programContents/preview/952/6088/1054797?next=1',\n+        'md5': 'c711103c72aeac8323a5cf1751f10097',\n+        'info_dict': {\n+            'id': '1054797',\n+            'ext': 'mp4',\n+            'title': '[2\ucc28 \ud2f0\uc800] MBN \uc8fc\ub9d0 \ubbf8\ub2c8\uc2dc\ub9ac\uc988 <\uc644\ubcbd\ud55c \uacb0\ud63c\uc758 \uc815\uc11d> l \uadf8\ub140\uc5d0\uac8c \uc8fc\uc5b4\uc9c4 \ub450 \ubc88\uc9f8 \uc778\uc0dd',\n+            'duration': 65,\n+            'release_date': '20231028',\n+            'thumbnail': 'http://img.vod.mbn.co.kr/vod2/952/2023/09/11/20230911130223_22_952_1054797_1080_7.jpg',\n+            'series': '\uc644\ubcbd\ud55c \uacb0\ud63c\uc758 \uc815\uc11d',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        content_id = self._match_id(url)\n+        webpage = self._download_webpage(url, content_id)\n+\n+        content_cls_cd = self._search_regex(\n+            r'\"\\?content_cls_cd=(\\d+)&', webpage, 'content cls cd', fatal=False) or '20'\n+        media_info = self._download_json(\n+            'https://www.mbn.co.kr/player/mbnVodPlayer_2020.mbn', content_id,\n+            note='Fetching playback data', query={\n+                'content_cls_cd': content_cls_cd,\n+                'content_id': content_id,\n+                'relay_type': '1',\n+            })\n+\n+        formats = []\n+        for stream_url in traverse_obj(media_info, ('movie_list', ..., 'url', {url_or_none})):\n+            stream_url = re.sub(r'/(?:chunk|play)list(?:_pd\\d+)?\\.m3u8', '/manifest.m3u8', stream_url)\n+            final_url = url_or_none(self._download_webpage(\n+                f'https://www.mbn.co.kr/player/mbnStreamAuth_new_vod.mbn?vod_url={stream_url}',\n+                content_id, note='Fetching authenticated m3u8 url'))\n+\n+            formats.extend(self._extract_m3u8_formats(final_url, content_id, fatal=False))\n+\n+        return {\n+            'id': content_id,\n+            **traverse_obj(media_info, {\n+                'title': ('movie_title', {str}),\n+                'duration': ('play_sec', {int_or_none}),\n+                'release_date': ('bcast_date', {lambda x: x.replace('.', '')}, {unified_strdate}),\n+                'thumbnail': ('movie_start_Img', {url_or_none}),\n+                'series': ('prog_nm', {str}),\n+                'episode_number': ('ad_contentnumber', {int_or_none}),\n+            }),\n+            'formats': formats,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mediaite.py",
            "diff": "diff --git a/yt_dlp/extractor/mediaite.py b/yt_dlp/extractor/mediaite.py\nindex 0f9079b1..32887cbd 100644\n--- a/yt_dlp/extractor/mediaite.py\n+++ b/yt_dlp/extractor/mediaite.py\n@@ -2,7 +2,7 @@\n \n \n class MediaiteIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?mediaite.com(?!/category)(?:/[\\w-]+){2}'\n+    _VALID_URL = r'https?://(?:www\\.)?mediaite\\.com(?!/category)(?:/[\\w-]+){2}'\n     _TESTS = [{\n         'url': 'https://www.mediaite.com/sports/bill-burr-roasts-nfl-for-promoting-black-lives-matter-while-scheduling-more-games-after-all-the-sht-they-know-about-cte/',\n         'info_dict': {\n@@ -81,10 +81,24 @@ class MediaiteIE(InfoExtractor):\n             'upload_date': '20210930',\n         },\n         'params': {'skip_download': True}\n+    }, {\n+        'url': 'https://www.mediaite.com/politics/i-cant-read-it-fast-enough-while-defending-trump-larry-kudlow-overwhelmed-by-volume-of-ex-presidents-legal-troubles/',\n+        'info_dict': {\n+            'id': 'E6EhDX5z',\n+            'ext': 'mp4',\n+            'title': 'Fox Business Network - 4:00 PM - 5:00 PM - 1:39:42 pm - 1:42:20 pm',\n+            'description': '',\n+            'thumbnail': 'https://cdn.jwplayer.com/v2/media/E6EhDX5z/poster.jpg?width=720',\n+            'duration': 157,\n+            'timestamp': 1691015535,\n+            'upload_date': '20230802',\n+        },\n+        'params': {'skip_download': True}\n     }]\n \n     def _real_extract(self, url):\n         webpage = self._download_webpage(url, None)\n-        id = self._search_regex(r'data-video-id\\s?=\\s?\\\"([^\\\"]+)\\\"', webpage, 'id')\n-        data_json = self._download_json(f'https://cdn.jwplayer.com/v2/media/{id}', id)\n+        video_id = self._search_regex(\n+            [r'\"https://cdn\\.jwplayer\\.com/players/(\\w+)', r'data-video-id\\s*=\\s*\\\"([^\\\"]+)\\\"'], webpage, 'id')\n+        data_json = self._download_json(f'https://cdn.jwplayer.com/v2/media/{video_id}', video_id)\n         return self._parse_jwplayer_data(data_json)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mediaklikk.py",
            "diff": "diff --git a/yt_dlp/extractor/mediaklikk.py b/yt_dlp/extractor/mediaklikk.py\nindex 46365081..fcc4827b 100644\n--- a/yt_dlp/extractor/mediaklikk.py\n+++ b/yt_dlp/extractor/mediaklikk.py\n@@ -1,5 +1,8 @@\n from ..utils import (\n-    unified_strdate\n+    ExtractorError,\n+    traverse_obj,\n+    unified_strdate,\n+    url_or_none,\n )\n from .common import InfoExtractor\n from ..compat import (\n@@ -15,7 +18,7 @@ class MediaKlikkIE(InfoExtractor):\n                         (?P<id>[^/#?_]+)'''\n \n     _TESTS = [{\n-        # mediaklikk. date in html.\n+        # (old) mediaklikk. date in html.\n         'url': 'https://mediaklikk.hu/video/hazajaro-delnyugat-bacska-a-duna-menten-palankatol-doroszloig/',\n         'info_dict': {\n             'id': '4754129',\n@@ -23,9 +26,21 @@ class MediaKlikkIE(InfoExtractor):\n             'ext': 'mp4',\n             'upload_date': '20210901',\n             'thumbnail': 'http://mediaklikk.hu/wp-content/uploads/sites/4/2014/02/hazajarouj_JO.jpg'\n+        },\n+        'skip': 'Webpage redirects to 404 page',\n+    }, {\n+        # mediaklikk. date in html.\n+        'url': 'https://mediaklikk.hu/video/hazajaro-fabova-hegyseg-kishont-koronaja/',\n+        'info_dict': {\n+            'id': '6696133',\n+            'title': 'Hazaj\u00e1r\u00f3, Fabova-hegys\u00e9g - Kishont koron\u00e1ja',\n+            'display_id': 'hazajaro-fabova-hegyseg-kishont-koronaja',\n+            'ext': 'mp4',\n+            'upload_date': '20230903',\n+            'thumbnail': 'https://mediaklikk.hu/wp-content/uploads/sites/4/2014/02/hazajarouj_JO.jpg'\n         }\n     }, {\n-        # m4sport\n+        # (old) m4sport\n         'url': 'https://m4sport.hu/video/2021/08/30/gyemant-liga-parizs/',\n         'info_dict': {\n             'id': '4754999',\n@@ -33,6 +48,18 @@ class MediaKlikkIE(InfoExtractor):\n             'ext': 'mp4',\n             'upload_date': '20210830',\n             'thumbnail': 'http://m4sport.hu/wp-content/uploads/sites/4/2021/08/vlcsnap-2021-08-30-18h21m20s10-1024x576.jpg'\n+        },\n+        'skip': 'Webpage redirects to 404 page',\n+    }, {\n+        # m4sport\n+        'url': 'https://m4sport.hu/sportkozvetitesek/video/2023/09/08/atletika-gyemant-liga-brusszel/',\n+        'info_dict': {\n+            'id': '6711136',\n+            'title': 'Atl\u00e9tika \u2013 Gy\u00e9m\u00e1nt Liga, Br\u00fcsszel',\n+            'display_id': 'atletika-gyemant-liga-brusszel',\n+            'ext': 'mp4',\n+            'upload_date': '20230908',\n+            'thumbnail': 'https://m4sport.hu/wp-content/uploads/sites/4/2023/09/vlcsnap-2023-09-08-22h43m18s691.jpg'\n         }\n     }, {\n         # m4sport with *video/ url and no date\n@@ -40,20 +67,33 @@ class MediaKlikkIE(InfoExtractor):\n         'info_dict': {\n             'id': '4492099',\n             'title': 'Real Madrid - Chelsea 1-1',\n+            'display_id': 'real-madrid-chelsea-1-1',\n             'ext': 'mp4',\n-            'thumbnail': 'http://m4sport.hu/wp-content/uploads/sites/4/2021/04/Sequence-01.Still001-1024x576.png'\n+            'thumbnail': 'https://m4sport.hu/wp-content/uploads/sites/4/2021/04/Sequence-01.Still001-1024x576.png'\n         }\n     }, {\n-        # hirado\n+        # (old) hirado\n         'url': 'https://hirado.hu/videok/felteteleket-szabott-a-fovaros/',\n         'info_dict': {\n             'id': '4760120',\n             'title': 'Felt\u00e9teleket szabott a f\u0151v\u00e1ros',\n             'ext': 'mp4',\n             'thumbnail': 'http://hirado.hu/wp-content/uploads/sites/4/2021/09/vlcsnap-2021-09-01-20h20m37s165.jpg'\n+        },\n+        'skip': 'Webpage redirects to video list page',\n+    }, {\n+        # hirado\n+        'url': 'https://hirado.hu/belfold/video/2023/09/11/marad-az-eves-elszamolas-a-napelemekre-beruhazo-csaladoknal',\n+        'info_dict': {\n+            'id': '6716068',\n+            'title': 'Marad az \u00e9ves elsz\u00e1mol\u00e1s a napelemekre beruh\u00e1z\u00f3 csal\u00e1dokn\u00e1l',\n+            'display_id': 'marad-az-eves-elszamolas-a-napelemekre-beruhazo-csaladoknal',\n+            'ext': 'mp4',\n+            'upload_date': '20230911',\n+            'thumbnail': 'https://hirado.hu/wp-content/uploads/sites/4/2023/09/vlcsnap-2023-09-11-09h16m09s882.jpg'\n         }\n     }, {\n-        # petofilive\n+        # (old) petofilive\n         'url': 'https://petofilive.hu/video/2021/06/07/tha-shudras-az-akusztikban/',\n         'info_dict': {\n             'id': '4571948',\n@@ -61,6 +101,18 @@ class MediaKlikkIE(InfoExtractor):\n             'ext': 'mp4',\n             'upload_date': '20210607',\n             'thumbnail': 'http://petofilive.hu/wp-content/uploads/sites/4/2021/06/vlcsnap-2021-06-07-22h14m23s915-1024x576.jpg'\n+        },\n+        'skip': 'Webpage redirects to empty page',\n+    }, {\n+        # petofilive\n+        'url': 'https://petofilive.hu/video/2023/09/09/futball-fesztival-a-margitszigeten/',\n+        'info_dict': {\n+            'id': '6713233',\n+            'title': 'Futball Fesztiv\u00e1l a Margitszigeten',\n+            'display_id': 'futball-fesztival-a-margitszigeten',\n+            'ext': 'mp4',\n+            'upload_date': '20230909',\n+            'thumbnail': 'https://petofilive.hu/wp-content/uploads/sites/4/2023/09/Clipboard11-2.jpg'\n         }\n     }]\n \n@@ -84,8 +136,12 @@ def _real_extract(self, url):\n \n         player_data['video'] = player_data.pop('token')\n         player_page = self._download_webpage('https://player.mediaklikk.hu/playernew/player.php', video_id, query=player_data)\n-        playlist_url = self._proto_relative_url(compat_urllib_parse_unquote(\n-            self._html_search_regex(r'\\\"file\\\":\\s*\\\"(\\\\?/\\\\?/.*playlist\\.m3u8)\\\"', player_page, 'playlist_url')).replace('\\\\/', '/'))\n+        player_json = self._search_json(\n+            r'\\bpl\\.setup\\s*\\(', player_page, 'player json', video_id, end_pattern=r'\\);')\n+        playlist_url = traverse_obj(\n+            player_json, ('playlist', lambda _, v: v['type'] == 'hls', 'file', {url_or_none}), get_all=False)\n+        if not playlist_url:\n+            raise ExtractorError('Unable to extract playlist url')\n \n         formats = self._extract_wowza_formats(\n             playlist_url, video_id, skip_protocols=['f4m', 'smil', 'dash'])\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mediaset.py",
            "diff": "diff --git a/yt_dlp/extractor/mediaset.py b/yt_dlp/extractor/mediaset.py\nindex e3b728dc..e04a1ce9 100644\n--- a/yt_dlp/extractor/mediaset.py\n+++ b/yt_dlp/extractor/mediaset.py\n@@ -73,6 +73,7 @@ class MediasetIE(ThePlatformBaseIE):\n             'season_number': 5,\n             'episode_number': 5,\n             'chapters': [{'start_time': 0.0, 'end_time': 3409.08}, {'start_time': 3409.08, 'end_time': 6565.008}],\n+            'categories': ['Informazione'],\n         },\n     }, {\n         # DRM\n@@ -127,7 +128,8 @@ class MediasetIE(ThePlatformBaseIE):\n         },\n         'params': {\n             'skip_download': True,\n-        }\n+        },\n+        'skip': 'Dead link',\n     }, {\n         # WittyTV embed\n         'url': 'https://www.wittytv.it/mauriziocostanzoshow/ultima-puntata-venerdi-25-novembre/',\n@@ -148,6 +150,7 @@ class MediasetIE(ThePlatformBaseIE):\n             'season_number': 12,\n             'episode': 'Episode 8',\n             'episode_number': 8,\n+            'categories': ['Intrattenimento'],\n         },\n         'params': {\n             'skip_download': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mediastream.py",
            "diff": "diff --git a/yt_dlp/extractor/mediastream.py b/yt_dlp/extractor/mediastream.py\nindex cef769f2..ae0fb2ae 100644\n--- a/yt_dlp/extractor/mediastream.py\n+++ b/yt_dlp/extractor/mediastream.py\n@@ -3,8 +3,11 @@\n from .common import InfoExtractor\n from ..utils import (\n     clean_html,\n+    filter_dict,\n+    parse_qs,\n     remove_end,\n     traverse_obj,\n+    update_url_query,\n     urljoin,\n )\n \n@@ -14,7 +17,7 @@ class MediaStreamBaseIE(InfoExtractor):\n     _BASE_URL_RE = r'https?://mdstrm\\.com/(?:embed|live-stream)'\n \n     def _extract_mediastream_urls(self, webpage):\n-        yield from traverse_obj(list(self._yield_json_ld(webpage, None)), (\n+        yield from traverse_obj(list(self._yield_json_ld(webpage, None, fatal=False)), (\n             lambda _, v: v['@type'] == 'VideoObject', ('embedUrl', 'contentUrl'),\n             {lambda x: x if re.match(rf'{self._BASE_URL_RE}/\\w+', x) else None}))\n \n@@ -106,15 +109,30 @@ def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(url, video_id)\n \n-        if 'Debido a tu ubicaci\u00f3n no puedes ver el contenido' in webpage:\n-            self.raise_geo_restricted()\n+        for message in [\n+            'Debido a tu ubicaci\u00f3n no puedes ver el contenido',\n+            'You are not allowed to watch this video: Geo Fencing Restriction',\n+            'Este contenido no est\u00e1 disponible en tu zona geogr\u00e1fica.',\n+            'El contenido s\u00f3lo est\u00e1 disponible dentro de',\n+        ]:\n+            if message in webpage:\n+                self.raise_geo_restricted()\n \n         player_config = self._search_json(r'window\\.MDSTRM\\.OPTIONS\\s*=', webpage, 'metadata', video_id)\n \n         formats, subtitles = [], {}\n         for video_format in player_config['src']:\n             if video_format == 'hls':\n-                fmts, subs = self._extract_m3u8_formats_and_subtitles(player_config['src'][video_format], video_id)\n+                params = {\n+                    'at': 'web-app',\n+                    'access_token': traverse_obj(parse_qs(url), ('access_token', 0)),\n+                }\n+                for name, key in (('MDSTRMUID', 'uid'), ('MDSTRMSID', 'sid'), ('MDSTRMPID', 'pid'), ('VERSION', 'av')):\n+                    params[key] = self._search_regex(\n+                        rf'window\\.{name}\\s*=\\s*[\"\\']([^\"\\']+)[\"\\'];', webpage, key, default=None)\n+\n+                fmts, subs = self._extract_m3u8_formats_and_subtitles(\n+                    update_url_query(player_config['src'][video_format], filter_dict(params)), video_id)\n                 formats.extend(fmts)\n                 self._merge_subtitles(subs, target=subtitles)\n             elif video_format == 'mpd':\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/meta.py",
            "diff": "diff --git a/yt_dlp/extractor/meta.py b/yt_dlp/extractor/meta.py\ndeleted file mode 100644\nindex 7c11e601..00000000\n--- a/yt_dlp/extractor/meta.py\n+++ /dev/null\n@@ -1,70 +0,0 @@\n-from .common import InfoExtractor\n-from .pladform import PladformIE\n-from ..utils import (\n-    unescapeHTML,\n-    int_or_none,\n-    ExtractorError,\n-)\n-\n-\n-class METAIE(InfoExtractor):\n-    _VALID_URL = r'https?://video\\.meta\\.ua/(?:iframe/)?(?P<id>[0-9]+)'\n-    _TESTS = [{\n-        'url': 'http://video.meta.ua/5502115.video',\n-        'md5': '71b6f3ee274bef16f1ab410f7f56b476',\n-        'info_dict': {\n-            'id': '5502115',\n-            'ext': 'mp4',\n-            'title': 'Sony Xperia Z camera test [HQ]',\n-            'description': 'Xperia Z shoots video in FullHD HDR.',\n-            'uploader_id': 'nomobile',\n-            'uploader': 'CH\u0401ZA.TV',\n-            'upload_date': '20130211',\n-        },\n-        'add_ie': ['Youtube'],\n-    }, {\n-        'url': 'http://video.meta.ua/iframe/5502115',\n-        'only_matching': True,\n-    }, {\n-        # pladform embed\n-        'url': 'http://video.meta.ua/7121015.video',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        st_html5 = self._search_regex(\n-            r\"st_html5\\s*=\\s*'#([^']+)'\", webpage, 'uppod html5 st', default=None)\n-\n-        if st_html5:\n-            # uppod st decryption algorithm is reverse engineered from function un(s) at uppod.js\n-            json_str = ''\n-            for i in range(0, len(st_html5), 3):\n-                json_str += '&#x0%s;' % st_html5[i:i + 3]\n-            uppod_data = self._parse_json(unescapeHTML(json_str), video_id)\n-            error = uppod_data.get('customnotfound')\n-            if error:\n-                raise ExtractorError('%s said: %s' % (self.IE_NAME, error), expected=True)\n-\n-            video_url = uppod_data['file']\n-            info = {\n-                'id': video_id,\n-                'url': video_url,\n-                'title': uppod_data.get('comment') or self._og_search_title(webpage),\n-                'description': self._og_search_description(webpage, default=None),\n-                'thumbnail': uppod_data.get('poster') or self._og_search_thumbnail(webpage),\n-                'duration': int_or_none(self._og_search_property(\n-                    'video:duration', webpage, default=None)),\n-            }\n-            if 'youtube.com/' in video_url:\n-                info.update({\n-                    '_type': 'url_transparent',\n-                    'ie_key': 'Youtube',\n-                })\n-            return info\n-\n-        pladform_url = PladformIE._extract_url(webpage)\n-        if pladform_url:\n-            return self.url_result(pladform_url)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/metacafe.py",
            "diff": "diff --git a/yt_dlp/extractor/metacafe.py b/yt_dlp/extractor/metacafe.py\ndeleted file mode 100644\nindex d7f5def0..00000000\n--- a/yt_dlp/extractor/metacafe.py\n+++ /dev/null\n@@ -1,281 +0,0 @@\n-import json\n-import re\n-import urllib.parse\n-\n-from .common import InfoExtractor\n-from ..compat import compat_parse_qs, compat_urllib_parse_unquote\n-from ..utils import (\n-    ExtractorError,\n-    determine_ext,\n-    get_element_by_attribute,\n-    int_or_none,\n-    mimetype2ext,\n-)\n-\n-\n-class MetacafeIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?metacafe\\.com/watch/(?P<id>[^/]+)/(?P<display_id>[^/?#]+)'\n-    _DISCLAIMER = 'http://www.metacafe.com/family_filter/'\n-    _FILTER_POST = 'http://www.metacafe.com/f/index.php?inputType=filter&controllerGroup=user'\n-    IE_NAME = 'metacafe'\n-    _TESTS = [\n-        # Youtube video\n-        {\n-            'add_ie': ['Youtube'],\n-            'url': 'http://metacafe.com/watch/yt-_aUehQsCQtM/the_electric_company_short_i_pbs_kids_go/',\n-            'info_dict': {\n-                'id': '_aUehQsCQtM',\n-                'ext': 'mp4',\n-                'upload_date': '20090102',\n-                'title': 'The Electric Company | \"Short I\" | PBS KIDS GO!',\n-                'description': 'md5:2439a8ef6d5a70e380c22f5ad323e5a8',\n-                'uploader': 'PBS',\n-                'uploader_id': 'PBS'\n-            }\n-        },\n-        # Normal metacafe video\n-        {\n-            'url': 'http://www.metacafe.com/watch/11121940/news_stuff_you_wont_do_with_your_playstation_4/',\n-            'md5': '6e0bca200eaad2552e6915ed6fd4d9ad',\n-            'info_dict': {\n-                'id': '11121940',\n-                'ext': 'mp4',\n-                'title': 'News: Stuff You Won\\'t Do with Your PlayStation 4',\n-                'uploader': 'ign',\n-                'description': 'Sony released a massive FAQ on the PlayStation Blog detailing the PS4\\'s capabilities and limitations.',\n-            },\n-            'skip': 'Page is temporarily unavailable.',\n-        },\n-        # metacafe video with family filter\n-        {\n-            'url': 'http://www.metacafe.com/watch/2155630/adult_art_by_david_hart_156/',\n-            'md5': 'b06082c5079bbdcde677a6291fbdf376',\n-            'info_dict': {\n-                'id': '2155630',\n-                'ext': 'mp4',\n-                'title': 'Adult Art By David Hart 156',\n-                'uploader': '63346',\n-                'description': 'md5:9afac8fc885252201ad14563694040fc',\n-            },\n-            'params': {\n-                'skip_download': True,\n-            },\n-        },\n-        # AnyClip video\n-        {\n-            'url': 'http://www.metacafe.com/watch/an-dVVXnuY7Jh77J/the_andromeda_strain_1971_stop_the_bomb_part_3/',\n-            'info_dict': {\n-                'id': 'an-dVVXnuY7Jh77J',\n-                'ext': 'mp4',\n-                'title': 'The Andromeda Strain (1971): Stop the Bomb Part 3',\n-                'uploader': 'AnyClip',\n-                'description': 'md5:cbef0460d31e3807f6feb4e7a5952e5b',\n-            },\n-        },\n-        # age-restricted video\n-        {\n-            'url': 'http://www.metacafe.com/watch/5186653/bbc_internal_christmas_tape_79_uncensored_outtakes_etc/',\n-            'md5': '98dde7c1a35d02178e8ab7560fe8bd09',\n-            'info_dict': {\n-                'id': '5186653',\n-                'ext': 'mp4',\n-                'title': 'BBC INTERNAL Christmas Tape \\'79 - UNCENSORED Outtakes, Etc.',\n-                'uploader': 'Dwayne Pipe',\n-                'description': 'md5:950bf4c581e2c059911fa3ffbe377e4b',\n-                'age_limit': 18,\n-            },\n-        },\n-        # cbs video\n-        {\n-            'url': 'http://www.metacafe.com/watch/cb-8VD4r_Zws8VP/open_this_is_face_the_nation_february_9/',\n-            'info_dict': {\n-                'id': '8VD4r_Zws8VP',\n-                'ext': 'flv',\n-                'title': 'Open: This is Face the Nation, February 9',\n-                'description': 'md5:8a9ceec26d1f7ed6eab610834cc1a476',\n-                'duration': 96,\n-                'uploader': 'CBSI-NEW',\n-                'upload_date': '20140209',\n-                'timestamp': 1391959800,\n-            },\n-            'params': {\n-                # rtmp download\n-                'skip_download': True,\n-            },\n-        },\n-        # Movieclips.com video\n-        {\n-            'url': 'http://www.metacafe.com/watch/mv-Wy7ZU/my_week_with_marilyn_do_you_love_me/',\n-            'info_dict': {\n-                'id': 'mv-Wy7ZU',\n-                'ext': 'mp4',\n-                'title': 'My Week with Marilyn - Do You Love Me?',\n-                'description': 'From the movie My Week with Marilyn - Colin (Eddie Redmayne) professes his love to Marilyn (Michelle Williams) and gets her to promise to return to set and finish the movie.',\n-                'uploader': 'movie_trailers',\n-                'duration': 176,\n-            },\n-            'params': {\n-                'skip_download': 'requires rtmpdump',\n-            }\n-        }\n-    ]\n-\n-    def report_disclaimer(self):\n-        self.to_screen('Retrieving disclaimer')\n-\n-    def _real_extract(self, url):\n-        # Extract id and simplified title from URL\n-        video_id, display_id = self._match_valid_url(url).groups()\n-\n-        # the video may come from an external site\n-        m_external = re.match(r'^(\\w{2})-(.*)$', video_id)\n-        if m_external is not None:\n-            prefix, ext_id = m_external.groups()\n-            # Check if video comes from YouTube\n-            if prefix == 'yt':\n-                return self.url_result('http://www.youtube.com/watch?v=%s' % ext_id, 'Youtube')\n-            # CBS videos use theplatform.com\n-            if prefix == 'cb':\n-                return self.url_result('theplatform:%s' % ext_id, 'ThePlatform')\n-\n-        headers = {\n-            # Disable family filter\n-            'Cookie': 'user=%s; ' % urllib.parse.quote(json.dumps({'ffilter': False}))\n-        }\n-\n-        # AnyClip videos require the flashversion cookie so that we get the link\n-        # to the mp4 file\n-        if video_id.startswith('an-'):\n-            headers['Cookie'] += 'flashVersion=0; '\n-\n-        # Retrieve video webpage to extract further information\n-        webpage = self._download_webpage(url, video_id, headers=headers)\n-\n-        error = get_element_by_attribute(\n-            'class', 'notfound-page-title', webpage)\n-        if error:\n-            raise ExtractorError(error, expected=True)\n-\n-        video_title = self._html_search_meta(\n-            ['og:title', 'twitter:title'], webpage, 'title', default=None) or self._search_regex(r'<h1>(.*?)</h1>', webpage, 'title')\n-\n-        # Extract URL, uploader and title from webpage\n-        self.report_extraction(video_id)\n-        video_url = None\n-        mobj = re.search(r'(?m)&(?:media|video)URL=([^&]+)', webpage)\n-        if mobj is not None:\n-            mediaURL = compat_urllib_parse_unquote(mobj.group(1))\n-            video_ext = determine_ext(mediaURL)\n-\n-            # Extract gdaKey if available\n-            mobj = re.search(r'(?m)&gdaKey=(.*?)&', webpage)\n-            if mobj is None:\n-                video_url = mediaURL\n-            else:\n-                gdaKey = mobj.group(1)\n-                video_url = '%s?__gda__=%s' % (mediaURL, gdaKey)\n-        if video_url is None:\n-            mobj = re.search(r'<video src=\"([^\"]+)\"', webpage)\n-            if mobj:\n-                video_url = mobj.group(1)\n-                video_ext = 'mp4'\n-        if video_url is None:\n-            flashvars = self._search_regex(\n-                r' name=\"flashvars\" value=\"(.*?)\"', webpage, 'flashvars',\n-                default=None)\n-            if flashvars:\n-                vardict = compat_parse_qs(flashvars)\n-                if 'mediaData' not in vardict:\n-                    raise ExtractorError('Unable to extract media URL')\n-                mobj = re.search(\n-                    r'\"mediaURL\":\"(?P<mediaURL>http.*?)\",(.*?)\"key\":\"(?P<key>.*?)\"', vardict['mediaData'][0])\n-                if mobj is None:\n-                    raise ExtractorError('Unable to extract media URL')\n-                mediaURL = mobj.group('mediaURL').replace('\\\\/', '/')\n-                video_url = '%s?__gda__=%s' % (mediaURL, mobj.group('key'))\n-                video_ext = determine_ext(video_url)\n-        if video_url is None:\n-            player_url = self._search_regex(\n-                r\"swfobject\\.embedSWF\\('([^']+)'\",\n-                webpage, 'config URL', default=None)\n-            if player_url:\n-                config_url = self._search_regex(\n-                    r'config=(.+)$', player_url, 'config URL')\n-                config_doc = self._download_xml(\n-                    config_url, video_id,\n-                    note='Downloading video config')\n-                smil_url = config_doc.find('.//properties').attrib['smil_file']\n-                smil_doc = self._download_xml(\n-                    smil_url, video_id,\n-                    note='Downloading SMIL document')\n-                base_url = smil_doc.find('./head/meta').attrib['base']\n-                video_url = []\n-                for vn in smil_doc.findall('.//video'):\n-                    br = int(vn.attrib['system-bitrate'])\n-                    play_path = vn.attrib['src']\n-                    video_url.append({\n-                        'format_id': 'smil-%d' % br,\n-                        'url': base_url,\n-                        'play_path': play_path,\n-                        'page_url': url,\n-                        'player_url': player_url,\n-                        'ext': play_path.partition(':')[0],\n-                    })\n-        if video_url is None:\n-            flashvars = self._parse_json(self._search_regex(\n-                r'flashvars\\s*=\\s*({.*});', webpage, 'flashvars',\n-                default=None), video_id, fatal=False)\n-            if flashvars:\n-                video_url = []\n-                for source in flashvars.get('sources'):\n-                    source_url = source.get('src')\n-                    if not source_url:\n-                        continue\n-                    ext = mimetype2ext(source.get('type')) or determine_ext(source_url)\n-                    if ext == 'm3u8':\n-                        video_url.extend(self._extract_m3u8_formats(\n-                            source_url, video_id, 'mp4',\n-                            'm3u8_native', m3u8_id='hls', fatal=False))\n-                    else:\n-                        video_url.append({\n-                            'url': source_url,\n-                            'ext': ext,\n-                        })\n-\n-        if video_url is None:\n-            raise ExtractorError('Unsupported video type')\n-\n-        description = self._html_search_meta(\n-            ['og:description', 'twitter:description', 'description'],\n-            webpage, 'title', fatal=False)\n-        thumbnail = self._html_search_meta(\n-            ['og:image', 'twitter:image'], webpage, 'title', fatal=False)\n-        video_uploader = self._html_search_regex(\n-            r'submitter=(.*?);|googletag\\.pubads\\(\\)\\.setTargeting\\(\"(?:channel|submiter)\",\"([^\"]+)\"\\);',\n-            webpage, 'uploader nickname', fatal=False)\n-        duration = int_or_none(\n-            self._html_search_meta('video:duration', webpage, default=None))\n-        age_limit = (\n-            18\n-            if re.search(r'(?:\"contentRating\":|\"rating\",)\"restricted\"', webpage)\n-            else 0)\n-\n-        if isinstance(video_url, list):\n-            formats = video_url\n-        else:\n-            formats = [{\n-                'url': video_url,\n-                'ext': video_ext,\n-            }]\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'description': description,\n-            'uploader': video_uploader,\n-            'title': video_title,\n-            'thumbnail': thumbnail,\n-            'age_limit': age_limit,\n-            'formats': formats,\n-            'duration': duration,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mgoon.py",
            "diff": "diff --git a/yt_dlp/extractor/mgoon.py b/yt_dlp/extractor/mgoon.py\ndeleted file mode 100644\nindex 2388a719..00000000\n--- a/yt_dlp/extractor/mgoon.py\n+++ /dev/null\n@@ -1,81 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    ExtractorError,\n-    qualities,\n-    unified_strdate,\n-)\n-\n-\n-class MgoonIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)https?://(?:www\\.)?\n-    (?:(:?m\\.)?mgoon\\.com/(?:ch/(?:.+)/v|play/view)|\n-        video\\.mgoon\\.com)/(?P<id>[0-9]+)'''\n-    _API_URL = 'http://mpos.mgoon.com/player/video?id={0:}'\n-    _TESTS = [\n-        {\n-            'url': 'http://m.mgoon.com/ch/hi6618/v/5582148',\n-            'md5': 'dd46bb66ab35cf6d51cc812fd82da79d',\n-            'info_dict': {\n-                'id': '5582148',\n-                'uploader_id': 'hi6618',\n-                'duration': 240.419,\n-                'upload_date': '20131220',\n-                'ext': 'mp4',\n-                'title': 'md5:543aa4c27a4931d371c3f433e8cebebc',\n-                'thumbnail': r're:^https?://.*\\.jpg$',\n-            }\n-        },\n-        {\n-            'url': 'http://www.mgoon.com/play/view/5582148',\n-            'only_matching': True,\n-        },\n-        {\n-            'url': 'http://video.mgoon.com/5582148',\n-            'only_matching': True,\n-        },\n-    ]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-\n-        data = self._download_json(self._API_URL.format(video_id), video_id)\n-\n-        if data.get('errorInfo', {}).get('code') != 'NONE':\n-            raise ExtractorError('%s encountered an error: %s' % (\n-                self.IE_NAME, data['errorInfo']['message']), expected=True)\n-\n-        v_info = data['videoInfo']\n-        title = v_info.get('v_title')\n-        thumbnail = v_info.get('v_thumbnail')\n-        duration = v_info.get('v_duration')\n-        upload_date = unified_strdate(v_info.get('v_reg_date'))\n-        uploader_id = data.get('userInfo', {}).get('u_alias')\n-        if duration:\n-            duration /= 1000.0\n-\n-        age_limit = None\n-        if data.get('accessInfo', {}).get('code') == 'VIDEO_STATUS_ADULT':\n-            age_limit = 18\n-\n-        formats = []\n-        get_quality = qualities(['360p', '480p', '720p', '1080p'])\n-        for fmt in data['videoFiles']:\n-            formats.append({\n-                'format_id': fmt['label'],\n-                'quality': get_quality(fmt['label']),\n-                'url': fmt['url'],\n-                'ext': fmt['format'],\n-\n-            })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'upload_date': upload_date,\n-            'uploader_id': uploader_id,\n-            'age_limit': age_limit,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/miomio.py",
            "diff": "diff --git a/yt_dlp/extractor/miomio.py b/yt_dlp/extractor/miomio.py\ndeleted file mode 100644\nindex 8df8cba1..00000000\n--- a/yt_dlp/extractor/miomio.py\n+++ /dev/null\n@@ -1,134 +0,0 @@\n-import random\n-\n-from .common import InfoExtractor\n-from ..compat import compat_urlparse\n-from ..networking import Request\n-from ..utils import ExtractorError, int_or_none, xpath_text\n-\n-\n-class MioMioIE(InfoExtractor):\n-    IE_NAME = 'miomio.tv'\n-    _VALID_URL = r'https?://(?:www\\.)?miomio\\.tv/watch/cc(?P<id>[0-9]+)'\n-    _TESTS = [{\n-        # \"type=video\" in flashvars\n-        'url': 'http://www.miomio.tv/watch/cc88912/',\n-        'info_dict': {\n-            'id': '88912',\n-            'ext': 'flv',\n-            'title': '\u3010SKY\u3011\u5b57\u5e55 \u94e0\u6b66\u662d\u548cVS\u5e73\u6210 \u5047\u9762\u9a91\u58eb\u5927\u6218FEAT\u6218\u961f \u9b54\u661f\u5b57\u5e55\u7ec4 \u5b57\u5e55',\n-            'duration': 5923,\n-        },\n-        'skip': 'Unable to load videos',\n-    }, {\n-        'url': 'http://www.miomio.tv/watch/cc184024/',\n-        'info_dict': {\n-            'id': '43729',\n-            'title': '\u300a\u52a8\u6f2b\u540c\u4eba\u63d2\u753b\u7ed8\u5236\u300b',\n-        },\n-        'playlist_mincount': 86,\n-        'skip': 'Unable to load videos',\n-    }, {\n-        'url': 'http://www.miomio.tv/watch/cc173113/',\n-        'info_dict': {\n-            'id': '173113',\n-            'title': 'The New Macbook 2015 \u4e0a\u624b\u8bd5\u73a9\u4e0e\u7b80\u8bc4'\n-        },\n-        'playlist_mincount': 2,\n-        'skip': 'Unable to load videos',\n-    }, {\n-        # new 'h5' player\n-        'url': 'http://www.miomio.tv/watch/cc273997/',\n-        'md5': '0b27a4b4495055d826813f8c3a6b2070',\n-        'info_dict': {\n-            'id': '273997',\n-            'ext': 'mp4',\n-            'title': '\u30de\u30c4\u30b3\u306e\u77e5\u3089\u306a\u3044\u4e16\u754c\u3010\u5287\u7684\u9032\u5316SP\uff01\u30d3\u30cb\u30fc\u30eb\u5098\uff06\u51b7\u51cd\u98df\u54c12016\u3011 1_2 - 16 05 31',\n-        },\n-        'skip': 'Unable to load videos',\n-    }]\n-\n-    def _extract_mioplayer(self, webpage, video_id, title, http_headers):\n-        xml_config = self._search_regex(\n-            r'flashvars=\"type=(?:sina|video)&amp;(.+?)&amp;',\n-            webpage, 'xml config')\n-\n-        # skipping the following page causes lags and eventually connection drop-outs\n-        self._request_webpage(\n-            'http://www.miomio.tv/mioplayer/mioplayerconfigfiles/xml.php?id=%s&r=%s' % (id, random.randint(100, 999)),\n-            video_id)\n-\n-        vid_config_request = Request(\n-            'http://www.miomio.tv/mioplayer/mioplayerconfigfiles/sina.php?{0}'.format(xml_config),\n-            headers=http_headers)\n-\n-        # the following xml contains the actual configuration information on the video file(s)\n-        vid_config = self._download_xml(vid_config_request, video_id)\n-\n-        if not int_or_none(xpath_text(vid_config, 'timelength')):\n-            raise ExtractorError('Unable to load videos!', expected=True)\n-\n-        entries = []\n-        for f in vid_config.findall('./durl'):\n-            segment_url = xpath_text(f, 'url', 'video url')\n-            if not segment_url:\n-                continue\n-            order = xpath_text(f, 'order', 'order')\n-            segment_id = video_id\n-            segment_title = title\n-            if order:\n-                segment_id += '-%s' % order\n-                segment_title += ' part %s' % order\n-            entries.append({\n-                'id': segment_id,\n-                'url': segment_url,\n-                'title': segment_title,\n-                'duration': int_or_none(xpath_text(f, 'length', 'duration'), 1000),\n-                'http_headers': http_headers,\n-            })\n-\n-        return entries\n-\n-    def _download_chinese_webpage(self, *args, **kwargs):\n-        # Requests with English locales return garbage\n-        headers = {\n-            'Accept-Language': 'zh-TW,en-US;q=0.7,en;q=0.3',\n-        }\n-        kwargs.setdefault('headers', {}).update(headers)\n-        return self._download_webpage(*args, **kwargs)\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_chinese_webpage(\n-            url, video_id)\n-\n-        title = self._html_search_meta(\n-            'description', webpage, 'title', fatal=True)\n-\n-        mioplayer_path = self._search_regex(\n-            r'src=\"(/mioplayer(?:_h5)?/[^\"]+)\"', webpage, 'ref_path')\n-\n-        if '_h5' in mioplayer_path:\n-            player_url = compat_urlparse.urljoin(url, mioplayer_path)\n-            player_webpage = self._download_chinese_webpage(\n-                player_url, video_id,\n-                note='Downloading player webpage', headers={'Referer': url})\n-            entries = self._parse_html5_media_entries(player_url, player_webpage, video_id)\n-            http_headers = {'Referer': player_url}\n-        else:\n-            http_headers = {'Referer': 'http://www.miomio.tv%s' % mioplayer_path}\n-            entries = self._extract_mioplayer(webpage, video_id, title, http_headers)\n-\n-        if len(entries) == 1:\n-            segment = entries[0]\n-            segment['id'] = video_id\n-            segment['title'] = title\n-            segment['http_headers'] = http_headers\n-            return segment\n-\n-        return {\n-            '_type': 'multi_video',\n-            'id': video_id,\n-            'entries': entries,\n-            'title': title,\n-            'http_headers': http_headers,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mixcloud.py",
            "diff": "diff --git a/yt_dlp/extractor/mixcloud.py b/yt_dlp/extractor/mixcloud.py\nindex fb5a08ca..8a95d1a5 100644\n--- a/yt_dlp/extractor/mixcloud.py\n+++ b/yt_dlp/extractor/mixcloud.py\n@@ -20,7 +20,7 @@ class MixcloudBaseIE(InfoExtractor):\n     def _call_api(self, object_type, object_fields, display_id, username, slug=None):\n         lookup_key = object_type + 'Lookup'\n         return self._download_json(\n-            'https://www.mixcloud.com/graphql', display_id, query={\n+            'https://app.mixcloud.com/graphql', display_id, query={\n                 'query': '''{\n   %s(lookup: {username: \"%s\"%s}) {\n     %s\n@@ -46,7 +46,15 @@ class MixcloudIE(MixcloudBaseIE):\n             'view_count': int,\n             'timestamp': 1321359578,\n             'upload_date': '20111115',\n+            'uploader_url': 'https://www.mixcloud.com/dholbach/',\n+            'artist': 'Submorphics & Chino , Telekinesis, Porter Robinson, Enei, Breakage ft Jess Mills',\n+            'duration': 3723,\n+            'tags': [],\n+            'comment_count': int,\n+            'repost_count': int,\n+            'like_count': int,\n         },\n+        'params': {'skip_download': 'm3u8'},\n     }, {\n         'url': 'http://www.mixcloud.com/gillespeterson/caribou-7-inch-vinyl-mix-chat/',\n         'info_dict': {\n@@ -60,7 +68,14 @@ class MixcloudIE(MixcloudBaseIE):\n             'view_count': int,\n             'timestamp': 1422987057,\n             'upload_date': '20150203',\n+            'uploader_url': 'https://www.mixcloud.com/gillespeterson/',\n+            'duration': 2992,\n+            'tags': [],\n+            'comment_count': int,\n+            'repost_count': int,\n+            'like_count': int,\n         },\n+        'params': {'skip_download': '404 playback error on site'},\n     }, {\n         'url': 'https://beta.mixcloud.com/RedLightRadio/nosedrip-15-red-light-radio-01-18-2016/',\n         'only_matching': True,\n@@ -259,9 +274,9 @@ def _real_extract(self, url):\n                 cloudcast_url = cloudcast.get('url')\n                 if not cloudcast_url:\n                     continue\n-                slug = try_get(cloudcast, lambda x: x['slug'], compat_str)\n+                item_slug = try_get(cloudcast, lambda x: x['slug'], compat_str)\n                 owner_username = try_get(cloudcast, lambda x: x['owner']['username'], compat_str)\n-                video_id = '%s_%s' % (owner_username, slug) if slug and owner_username else None\n+                video_id = f'{owner_username}_{item_slug}' if item_slug and owner_username else None\n                 entries.append(self.url_result(\n                     cloudcast_url, MixcloudIE.ie_key(), video_id))\n \n@@ -284,7 +299,7 @@ class MixcloudUserIE(MixcloudPlaylistBaseIE):\n         'info_dict': {\n             'id': 'dholbach_uploads',\n             'title': 'Daniel Holbach (uploads)',\n-            'description': 'md5:b60d776f0bab534c5dabe0a34e47a789',\n+            'description': 'md5:a3f468a60ac8c3e1f8616380fc469b2b',\n         },\n         'playlist_mincount': 36,\n     }, {\n@@ -292,7 +307,7 @@ class MixcloudUserIE(MixcloudPlaylistBaseIE):\n         'info_dict': {\n             'id': 'dholbach_uploads',\n             'title': 'Daniel Holbach (uploads)',\n-            'description': 'md5:b60d776f0bab534c5dabe0a34e47a789',\n+            'description': 'md5:a3f468a60ac8c3e1f8616380fc469b2b',\n         },\n         'playlist_mincount': 36,\n     }, {\n@@ -300,7 +315,7 @@ class MixcloudUserIE(MixcloudPlaylistBaseIE):\n         'info_dict': {\n             'id': 'dholbach_favorites',\n             'title': 'Daniel Holbach (favorites)',\n-            'description': 'md5:b60d776f0bab534c5dabe0a34e47a789',\n+            'description': 'md5:a3f468a60ac8c3e1f8616380fc469b2b',\n         },\n         # 'params': {\n         #     'playlist_items': '1-100',\n@@ -323,9 +338,9 @@ class MixcloudUserIE(MixcloudPlaylistBaseIE):\n         'info_dict': {\n             'id': 'FirstEar_stream',\n             'title': 'First Ear (stream)',\n-            'description': 'Curators of good music\\r\\n\\r\\nfirstearmusic.com',\n+            'description': 'we maraud for ears',\n         },\n-        'playlist_mincount': 271,\n+        'playlist_mincount': 269,\n     }]\n \n     _TITLE_KEY = 'displayName'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mnet.py",
            "diff": "diff --git a/yt_dlp/extractor/mnet.py b/yt_dlp/extractor/mnet.py\ndeleted file mode 100644\nindex 98bab2e1..00000000\n--- a/yt_dlp/extractor/mnet.py\n+++ /dev/null\n@@ -1,85 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    parse_duration,\n-    parse_iso8601,\n-)\n-\n-\n-class MnetIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?mnet\\.(?:com|interest\\.me)/tv/vod/(?:.*?\\bclip_id=)?(?P<id>[0-9]+)'\n-    _TESTS = [{\n-        'url': 'http://www.mnet.com/tv/vod/171008',\n-        'info_dict': {\n-            'id': '171008',\n-            'title': 'SS_\uc774\ud574\uc778@\ud788\ub4e0\ubc15\uc2a4',\n-            'description': 'md5:b9efa592c3918b615ba69fe9f8a05c55',\n-            'duration': 88,\n-            'upload_date': '20151231',\n-            'timestamp': 1451564040,\n-            'age_limit': 0,\n-            'thumbnails': 'mincount:5',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'ext': 'flv',\n-        },\n-        'params': {\n-            # rtmp download\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'http://mnet.interest.me/tv/vod/172790',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.mnet.com/tv/vod/vod_view.asp?clip_id=172790&tabMenu=',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        # TODO: extract rtmp formats\n-        # no stype -> rtmp url\n-        # stype=H -> m3u8 url\n-        # stype=M -> mpd url\n-        info = self._download_json(\n-            'http://content.api.mnet.com/player/vodConfig',\n-            video_id, 'Downloading vod config JSON', query={\n-                'id': video_id,\n-                'ctype': 'CLIP',\n-                'stype': 'H',\n-            })['data']['info']\n-\n-        title = info['title']\n-\n-        cdn_data = self._download_json(\n-            info['cdn'], video_id, 'Downloading vod cdn JSON')['data'][0]\n-        m3u8_url = cdn_data['url']\n-        token = cdn_data.get('token')\n-        if token and token != '-':\n-            m3u8_url += '?' + token\n-        formats = self._extract_wowza_formats(\n-            m3u8_url, video_id, skip_protocols=['rtmp', 'rtsp', 'f4m'])\n-\n-        description = info.get('ment')\n-        duration = parse_duration(info.get('time'))\n-        timestamp = parse_iso8601(info.get('date'), delimiter=' ')\n-        age_limit = info.get('adult')\n-        if age_limit is not None:\n-            age_limit = 0 if age_limit == 'N' else 18\n-        thumbnails = [{\n-            'id': thumb_format,\n-            'url': thumb['url'],\n-            'width': int_or_none(thumb.get('width')),\n-            'height': int_or_none(thumb.get('height')),\n-        } for thumb_format, thumb in info.get('cover', {}).items() if thumb.get('url')]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'timestamp': timestamp,\n-            'age_limit': age_limit,\n-            'thumbnails': thumbnails,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mocha.py",
            "diff": "diff --git a/yt_dlp/extractor/mocha.py b/yt_dlp/extractor/mocha.py\nindex 5f72b810..2fbc0e91 100644\n--- a/yt_dlp/extractor/mocha.py\n+++ b/yt_dlp/extractor/mocha.py\n@@ -3,7 +3,7 @@\n \n \n class MochaVideoIE(InfoExtractor):\n-    _VALID_URL = r'https?://video.mocha.com.vn/(?P<video_slug>[\\w-]+)'\n+    _VALID_URL = r'https?://video\\.mocha\\.com\\.vn/(?P<video_slug>[\\w-]+)'\n     _TESTS = [{\n         'url': 'http://video.mocha.com.vn/chuyen-meo-gia-su-tu-thong-diep-cuoc-song-v18694039',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/moevideo.py",
            "diff": "diff --git a/yt_dlp/extractor/moevideo.py b/yt_dlp/extractor/moevideo.py\ndeleted file mode 100644\nindex fda08cae..00000000\n--- a/yt_dlp/extractor/moevideo.py\n+++ /dev/null\n@@ -1,74 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    clean_html,\n-    int_or_none,\n-)\n-\n-\n-class MoeVideoIE(InfoExtractor):\n-    IE_DESC = 'LetitBit video services: moevideo.net, playreplay.net and videochart.net'\n-    _VALID_URL = r'''(?x)\n-        https?://(?P<host>(?:www\\.)?\n-        (?:(?:moevideo|playreplay|videochart)\\.net|thesame\\.tv))/\n-        (?:video|framevideo|embed)/(?P<id>[0-9a-z]+\\.[0-9A-Za-z]+)'''\n-    _API_URL = 'http://api.letitbit.net/'\n-    _API_KEY = 'tVL0gjqo5'\n-    _TESTS = [\n-        {\n-            'url': 'http://moevideo.net/video/00297.0036103fe3d513ef27915216fd29',\n-            'md5': '129f5ae1f6585d0e9bb4f38e774ffb3a',\n-            'info_dict': {\n-                'id': '00297.0036103fe3d513ef27915216fd29',\n-                'ext': 'flv',\n-                'title': 'Sink cut out machine',\n-                'description': 'md5:f29ff97b663aefa760bf7ca63c8ca8a8',\n-                'thumbnail': r're:^https?://.*\\.jpg$',\n-                'width': 540,\n-                'height': 360,\n-                'duration': 179,\n-                'filesize': 17822500,\n-            },\n-            'skip': 'Video has been removed',\n-        },\n-        {\n-            'url': 'http://playreplay.net/video/77107.7f325710a627383d40540d8e991a',\n-            'md5': '74f0a014d5b661f0f0e2361300d1620e',\n-            'info_dict': {\n-                'id': '77107.7f325710a627383d40540d8e991a',\n-                'ext': 'flv',\n-                'title': 'Operacion Condor.',\n-                'description': 'md5:7e68cb2fcda66833d5081c542491a9a3',\n-                'thumbnail': r're:^https?://.*\\.jpg$',\n-                'width': 480,\n-                'height': 296,\n-                'duration': 6027,\n-                'filesize': 588257923,\n-            },\n-            'skip': 'Video has been removed',\n-        },\n-    ]\n-\n-    def _real_extract(self, url):\n-        host, video_id = self._match_valid_url(url).groups()\n-\n-        webpage = self._download_webpage(\n-            'http://%s/video/%s' % (host, video_id),\n-            video_id, 'Downloading webpage')\n-\n-        title = self._og_search_title(webpage)\n-\n-        embed_webpage = self._download_webpage(\n-            'http://%s/embed/%s' % (host, video_id),\n-            video_id, 'Downloading embed webpage')\n-        video = self._parse_json(self._search_regex(\n-            r'mvplayer\\(\"#player\"\\s*,\\s*({.+})',\n-            embed_webpage, 'mvplayer'), video_id)['video']\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': video.get('poster') or self._og_search_thumbnail(webpage),\n-            'description': clean_html(self._og_search_description(webpage)),\n-            'duration': int_or_none(self._og_search_property('video:duration', webpage)),\n-            'url': video['ourUrl'],\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mofosex.py",
            "diff": "diff --git a/yt_dlp/extractor/mofosex.py b/yt_dlp/extractor/mofosex.py\ndeleted file mode 100644\nindex 9cb6980c..00000000\n--- a/yt_dlp/extractor/mofosex.py\n+++ /dev/null\n@@ -1,70 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    str_to_int,\n-    unified_strdate,\n-)\n-from .keezmovies import KeezMoviesIE\n-\n-\n-class MofosexIE(KeezMoviesIE):  # XXX: Do not subclass from concrete IE\n-    _VALID_URL = r'https?://(?:www\\.)?mofosex\\.com/videos/(?P<id>\\d+)/(?P<display_id>[^/?#&.]+)\\.html'\n-    _TESTS = [{\n-        'url': 'http://www.mofosex.com/videos/318131/amateur-teen-playing-and-masturbating-318131.html',\n-        'md5': '558fcdafbb63a87c019218d6e49daf8a',\n-        'info_dict': {\n-            'id': '318131',\n-            'display_id': 'amateur-teen-playing-and-masturbating-318131',\n-            'ext': 'mp4',\n-            'title': 'amateur teen playing and masturbating',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'upload_date': '20121114',\n-            'view_count': int,\n-            'like_count': int,\n-            'dislike_count': int,\n-            'age_limit': 18,\n-        }\n-    }, {\n-        # This video is no longer available\n-        'url': 'http://www.mofosex.com/videos/5018/japanese-teen-music-video.html',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        webpage, info = self._extract_info(url)\n-\n-        view_count = str_to_int(self._search_regex(\n-            r'VIEWS:</span>\\s*([\\d,.]+)', webpage, 'view count', fatal=False))\n-        like_count = int_or_none(self._search_regex(\n-            r'id=[\"\\']amountLikes[\"\\'][^>]*>(\\d+)', webpage,\n-            'like count', fatal=False))\n-        dislike_count = int_or_none(self._search_regex(\n-            r'id=[\"\\']amountDislikes[\"\\'][^>]*>(\\d+)', webpage,\n-            'like count', fatal=False))\n-        upload_date = unified_strdate(self._html_search_regex(\n-            r'Added:</span>([^<]+)', webpage, 'upload date', fatal=False))\n-\n-        info.update({\n-            'view_count': view_count,\n-            'like_count': like_count,\n-            'dislike_count': dislike_count,\n-            'upload_date': upload_date,\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-        })\n-\n-        return info\n-\n-\n-class MofosexEmbedIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?mofosex\\.com/embed/?\\?.*?\\bvideoid=(?P<id>\\d+)'\n-    _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)?//(?:www\\.)?mofosex\\.com/embed/?\\?.*?\\bvideoid=\\d+)']\n-    _TESTS = [{\n-        'url': 'https://www.mofosex.com/embed/?videoid=318131&referrer=KM',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        return self.url_result(\n-            'http://www.mofosex.com/videos/{0}/{0}.html'.format(video_id),\n-            ie=MofosexIE.ie_key(), video_id=video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/monstercat.py",
            "diff": "diff --git a/yt_dlp/extractor/monstercat.py b/yt_dlp/extractor/monstercat.py\nnew file mode 100644\nindex 00000000..cf5e0996\n--- /dev/null\n+++ b/yt_dlp/extractor/monstercat.py\n@@ -0,0 +1,77 @@\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    extract_attributes,\n+    get_element_by_class,\n+    get_element_html_by_class,\n+    get_element_text_and_html_by_tag,\n+    int_or_none,\n+    unified_strdate,\n+    strip_or_none,\n+    traverse_obj,\n+    try_call,\n+)\n+\n+\n+class MonstercatIE(InfoExtractor):\n+    _VALID_URL = r'https://www\\.monstercat\\.com/release/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.monstercat.com/release/742779548009',\n+        'playlist_count': 20,\n+        'info_dict': {\n+            'title': 'The Secret Language of Trees',\n+            'id': '742779548009',\n+            'thumbnail': 'https://www.monstercat.com/release/742779548009/cover',\n+            'release_date': '20230711',\n+            'album': 'The Secret Language of Trees',\n+            'album_artist': 'BT',\n+        }\n+    }]\n+\n+    def _extract_tracks(self, table, album_meta):\n+        for td in re.findall(r'<tr[^<]*>((?:(?!</tr>)[\\w\\W])+)', table):  # regex by chatgpt due to lack of get_elements_by_tag\n+            title = clean_html(try_call(\n+                lambda: get_element_by_class('d-inline-flex flex-column', td).partition(' <span')[0]))\n+            ids = extract_attributes(try_call(lambda: get_element_html_by_class('btn-play cursor-pointer mr-small', td)) or '')\n+            track_id = ids.get('data-track-id')\n+            release_id = ids.get('data-release-id')\n+\n+            track_number = int_or_none(try_call(lambda: get_element_by_class('py-xsmall', td)))\n+            if not track_id or not release_id:\n+                self.report_warning(f'Skipping track {track_number}, ID(s) not found')\n+                self.write_debug(f'release_id={repr(release_id)} track_id={repr(track_id)}')\n+                continue\n+            yield {\n+                **album_meta,\n+                'title': title,\n+                'track': title,\n+                'track_number': track_number,\n+                'artist': clean_html(try_call(lambda: get_element_by_class('d-block fs-xxsmall', td))),\n+                'url': f'https://www.monstercat.com/api/release/{release_id}/track-stream/{track_id}',\n+                'id': track_id,\n+                'ext': 'mp3'\n+            }\n+\n+    def _real_extract(self, url):\n+        url_id = self._match_id(url)\n+        html = self._download_webpage(url, url_id)\n+        # wrap all `get_elements` in `try_call`, HTMLParser has problems with site's html\n+        tracklist_table = try_call(lambda: get_element_by_class('table table-small', html)) or ''\n+\n+        title = try_call(lambda: get_element_text_and_html_by_tag('h1', html)[0])\n+        date = traverse_obj(html, ({lambda html: get_element_by_class('font-italic mb-medium d-tablet-none d-phone-block',\n+                            html).partition('Released ')}, 2, {strip_or_none}, {unified_strdate}))\n+\n+        album_meta = {\n+            'title': title,\n+            'album': title,\n+            'thumbnail': f'https://www.monstercat.com/release/{url_id}/cover',\n+            'album_artist': try_call(\n+                lambda: get_element_by_class('h-normal text-uppercase mb-desktop-medium mb-smallish', html)),\n+            'release_date': date,\n+        }\n+\n+        return self.playlist_result(\n+            self._extract_tracks(tracklist_table, album_meta), playlist_id=url_id, **album_meta)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/motherless.py",
            "diff": "diff --git a/yt_dlp/extractor/motherless.py b/yt_dlp/extractor/motherless.py\nindex 769b52ce..e359c44e 100644\n--- a/yt_dlp/extractor/motherless.py\n+++ b/yt_dlp/extractor/motherless.py\n@@ -151,7 +151,7 @@ def _real_extract(self, url):\n                     'd': 'days',\n                 }\n                 kwargs = {_AGO_UNITS.get(uploaded_ago[-1]): delta}\n-                upload_date = (datetime.datetime.utcnow() - datetime.timedelta(**kwargs)).strftime('%Y%m%d')\n+                upload_date = (datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(**kwargs)).strftime('%Y%m%d')\n \n         comment_count = len(re.findall(r'''class\\s*=\\s*['\"]media-comment-contents\\b''', webpage))\n         uploader_id = self._html_search_regex(\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/movieclips.py",
            "diff": "diff --git a/yt_dlp/extractor/movieclips.py b/yt_dlp/extractor/movieclips.py\ndeleted file mode 100644\nindex 4777f440..00000000\n--- a/yt_dlp/extractor/movieclips.py\n+++ /dev/null\n@@ -1,46 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    smuggle_url,\n-    float_or_none,\n-    parse_iso8601,\n-    update_url_query,\n-)\n-\n-\n-class MovieClipsIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?movieclips\\.com/videos/.+-(?P<id>\\d+)(?:\\?|$)'\n-    _TEST = {\n-        'url': 'http://www.movieclips.com/videos/warcraft-trailer-1-561180739597',\n-        'md5': '42b5a0352d4933a7bd54f2104f481244',\n-        'info_dict': {\n-            'id': 'pKIGmG83AqD9',\n-            'ext': 'mp4',\n-            'title': 'Warcraft Trailer 1',\n-            'description': 'Watch Trailer 1 from Warcraft (2016). Legendary\u2019s WARCRAFT is a 3D epic adventure of world-colliding conflict based.',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'timestamp': 1446843055,\n-            'upload_date': '20151106',\n-            'uploader': 'Movieclips',\n-        },\n-        'add_ie': ['ThePlatform'],\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        video = next(v for v in self._parse_json(self._search_regex(\n-            r'var\\s+__REACT_ENGINE__\\s*=\\s*({.+});',\n-            webpage, 'react engine'), video_id)['playlist']['videos'] if v['id'] == video_id)\n-\n-        return {\n-            '_type': 'url_transparent',\n-            'ie_key': 'ThePlatform',\n-            'url': smuggle_url(update_url_query(\n-                video['contentUrl'], {'mbr': 'true'}), {'force_smil_url': True}),\n-            'title': self._og_search_title(webpage),\n-            'description': self._html_search_meta('description', webpage),\n-            'duration': float_or_none(video.get('duration')),\n-            'timestamp': parse_iso8601(video.get('dateCreated')),\n-            'thumbnail': video.get('defaultImage'),\n-            'uploader': video.get('provider'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/msn.py",
            "diff": "diff --git a/yt_dlp/extractor/msn.py b/yt_dlp/extractor/msn.py\nindex f91c53eb..77d1806a 100644\n--- a/yt_dlp/extractor/msn.py\n+++ b/yt_dlp/extractor/msn.py\n@@ -11,6 +11,7 @@\n \n \n class MSNIE(InfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:(?:www|preview)\\.)?msn\\.com/(?:[^/]+/)+(?P<display_id>[^/]+)/[a-z]{2}-(?P<id>[\\da-zA-Z]+)'\n     _TESTS = [{\n         'url': 'https://www.msn.com/en-in/money/video/7-ways-to-get-rid-of-chest-congestion/vi-BBPxU6d',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mtv.py",
            "diff": "diff --git a/yt_dlp/extractor/mtv.py b/yt_dlp/extractor/mtv.py\nindex 0d700b9a..e192453c 100644\n--- a/yt_dlp/extractor/mtv.py\n+++ b/yt_dlp/extractor/mtv.py\n@@ -1,4 +1,5 @@\n import re\n+import xml.etree.ElementTree\n \n from .common import InfoExtractor\n from ..compat import compat_str\n@@ -137,7 +138,7 @@ def _get_video_info(self, itemdoc, use_hls=True):\n         mediagen_doc = self._download_xml(\n             mediagen_url, video_id, 'Downloading video urls', fatal=False)\n \n-        if mediagen_doc is False:\n+        if not isinstance(mediagen_doc, xml.etree.ElementTree.Element):\n             return None\n \n         item = mediagen_doc.find('./video/item')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mwave.py",
            "diff": "diff --git a/yt_dlp/extractor/mwave.py b/yt_dlp/extractor/mwave.py\ndeleted file mode 100644\nindex efbfd9d4..00000000\n--- a/yt_dlp/extractor/mwave.py\n+++ /dev/null\n@@ -1,87 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    int_or_none,\n-    parse_duration,\n-)\n-\n-\n-class MwaveIE(InfoExtractor):\n-    _VALID_URL = r'https?://mwave\\.interest\\.me/(?:[^/]+/)?mnettv/videodetail\\.m\\?searchVideoDetailVO\\.clip_id=(?P<id>[0-9]+)'\n-    _URL_TEMPLATE = 'http://mwave.interest.me/mnettv/videodetail.m?searchVideoDetailVO.clip_id=%s'\n-    _TESTS = [{\n-        'url': 'http://mwave.interest.me/mnettv/videodetail.m?searchVideoDetailVO.clip_id=168859',\n-        # md5 is unstable\n-        'info_dict': {\n-            'id': '168859',\n-            'ext': 'flv',\n-            'title': '[M COUNTDOWN] SISTAR - SHAKE IT',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'uploader': 'M COUNTDOWN',\n-            'duration': 206,\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'http://mwave.interest.me/en/mnettv/videodetail.m?searchVideoDetailVO.clip_id=176199',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        vod_info = self._download_json(\n-            'http://mwave.interest.me/onair/vod_info.m?vodtype=CL&sectorid=&endinfo=Y&id=%s' % video_id,\n-            video_id, 'Download vod JSON')\n-\n-        formats = []\n-        for num, cdn_info in enumerate(vod_info['cdn']):\n-            stream_url = cdn_info.get('url')\n-            if not stream_url:\n-                continue\n-            stream_name = cdn_info.get('name') or compat_str(num)\n-            f4m_stream = self._download_json(\n-                stream_url, video_id,\n-                'Download %s stream JSON' % stream_name)\n-            f4m_url = f4m_stream.get('fileurl')\n-            if not f4m_url:\n-                continue\n-            formats.extend(\n-                self._extract_f4m_formats(f4m_url + '&hdcore=3.0.3', video_id, f4m_id=stream_name))\n-\n-        return {\n-            'id': video_id,\n-            'title': vod_info['title'],\n-            'thumbnail': vod_info.get('cover'),\n-            'uploader': vod_info.get('program_title'),\n-            'duration': parse_duration(vod_info.get('time')),\n-            'view_count': int_or_none(vod_info.get('hit')),\n-            'formats': formats,\n-        }\n-\n-\n-class MwaveMeetGreetIE(InfoExtractor):\n-    _VALID_URL = r'https?://mwave\\.interest\\.me/(?:[^/]+/)?meetgreet/view/(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'http://mwave.interest.me/meetgreet/view/256',\n-        'info_dict': {\n-            'id': '173294',\n-            'ext': 'flv',\n-            'title': '[MEET&GREET] Park BoRam',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'uploader': 'Mwave',\n-            'duration': 3634,\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'http://mwave.interest.me/en/meetgreet/view/256',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        clip_id = self._html_search_regex(\n-            r'<iframe[^>]+src=\"/mnettv/ifr_clip\\.m\\?searchVideoDetailVO\\.clip_id=(\\d+)',\n-            webpage, 'clip ID')\n-        clip_url = MwaveIE._URL_TEMPLATE % clip_id\n-        return self.url_result(clip_url, 'Mwave', clip_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mychannels.py",
            "diff": "diff --git a/yt_dlp/extractor/mychannels.py b/yt_dlp/extractor/mychannels.py\ndeleted file mode 100644\nindex 8a70c1f7..00000000\n--- a/yt_dlp/extractor/mychannels.py\n+++ /dev/null\n@@ -1,35 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class MyChannelsIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?mychannels\\.com/.*(?P<id_type>video|production)_id=(?P<id>[0-9]+)'\n-    _TEST = {\n-        'url': 'https://mychannels.com/missholland/miss-holland?production_id=3416',\n-        'md5': 'b8993daad4262dd68d89d651c0c52c45',\n-        'info_dict': {\n-            'id': 'wUUDZZep6vQD',\n-            'ext': 'mp4',\n-            'title': 'Miss Holland joins VOTE LEAVE',\n-            'description': 'Miss Holland | #13 Not a potato',\n-            'uploader': 'Miss Holland',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        id_type, url_id = self._match_valid_url(url).groups()\n-        webpage = self._download_webpage(url, url_id)\n-        video_data = self._html_search_regex(r'<div([^>]+data-%s-id=\"%s\"[^>]+)>' % (id_type, url_id), webpage, 'video data')\n-\n-        def extract_data_val(attr, fatal=False):\n-            return self._html_search_regex(r'data-%s\\s*=\\s*\"([^\"]+)\"' % attr, video_data, attr, fatal=fatal)\n-        minoto_id = extract_data_val('minoto-id') or self._search_regex(r'/id/([a-zA-Z0-9]+)', extract_data_val('video-src', True), 'minoto id')\n-\n-        return {\n-            '_type': 'url_transparent',\n-            'url': 'minoto:%s' % minoto_id,\n-            'id': url_id,\n-            'title': extract_data_val('title', True),\n-            'description': extract_data_val('description'),\n-            'thumbnail': extract_data_val('image'),\n-            'uploader': extract_data_val('channel'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/myvi.py",
            "diff": "diff --git a/yt_dlp/extractor/myvi.py b/yt_dlp/extractor/myvi.py\ndeleted file mode 100644\nindex df7200be..00000000\n--- a/yt_dlp/extractor/myvi.py\n+++ /dev/null\n@@ -1,100 +0,0 @@\n-from .common import InfoExtractor\n-from .vimple import SprutoBaseIE\n-\n-\n-class MyviIE(SprutoBaseIE):\n-    _VALID_URL = r'''(?x)\n-                        (?:\n-                            https?://\n-                                (?:www\\.)?\n-                                myvi\\.\n-                                (?:\n-                                    (?:ru/player|tv)/\n-                                    (?:\n-                                        (?:\n-                                            embed/html|\n-                                            flash|\n-                                            api/Video/Get\n-                                        )/|\n-                                        content/preloader\\.swf\\?.*\\bid=\n-                                    )|\n-                                    ru/watch/\n-                                )|\n-                            myvi:\n-                        )\n-                        (?P<id>[\\da-zA-Z_-]+)\n-                    '''\n-    _EMBED_REGEX = [r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//myvi\\.(?:ru/player|tv)/(?:embed/html|flash)/[^\"]+)\\1']\n-    _TESTS = [{\n-        'url': 'http://myvi.ru/player/embed/html/oOy4euHA6LVwNNAjhD9_Jq5Ha2Qf0rtVMVFMAZav8wObeRTZaCATzucDQIDph8hQU0',\n-        'md5': '571bbdfba9f9ed229dc6d34cc0f335bf',\n-        'info_dict': {\n-            'id': 'f16b2bbd-cde8-481c-a981-7cd48605df43',\n-            'ext': 'mp4',\n-            'title': '\u0445\u043e\u0437\u044f\u0438\u043d \u0436\u0438\u0437\u043d\u0438',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 25,\n-        },\n-    }, {\n-        'url': 'http://myvi.ru/player/content/preloader.swf?id=oOy4euHA6LVwNNAjhD9_Jq5Ha2Qf0rtVMVFMAZav8wOYf1WFpPfc_bWTKGVf_Zafr0',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://myvi.ru/player/api/Video/Get/oOy4euHA6LVwNNAjhD9_Jq5Ha2Qf0rtVMVFMAZav8wObeRTZaCATzucDQIDph8hQU0',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://myvi.tv/embed/html/oTGTNWdyz4Zwy_u1nraolwZ1odenTd9WkTnRfIL9y8VOgHYqOHApE575x4_xxS9Vn0?ap=0',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://myvi.ru/player/flash/ocp2qZrHI-eZnHKQBK4cZV60hslH8LALnk0uBfKsB-Q4WnY26SeGoYPi8HWHxu0O30',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.myvi.ru/watch/YwbqszQynUaHPn_s82sx0Q2',\n-        'only_matching': True,\n-    }, {\n-        'url': 'myvi:YwbqszQynUaHPn_s82sx0Q2',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        spruto = self._download_json(\n-            'http://myvi.ru/player/api/Video/Get/%s?sig' % video_id, video_id)['sprutoData']\n-\n-        return self._extract_spruto(spruto, video_id)\n-\n-\n-class MyviEmbedIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?myvi\\.tv/(?:[^?]+\\?.*?\\bv=|embed/)(?P<id>[\\da-z]+)'\n-    _TESTS = [{\n-        'url': 'https://www.myvi.tv/embed/ccdqic3wgkqwpb36x9sxg43t4r',\n-        'info_dict': {\n-            'id': 'b3ea0663-3234-469d-873e-7fecf36b31d1',\n-            'ext': 'mp4',\n-            'title': '\u0422\u0432\u043e\u044f (original song).mp4',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 277,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://www.myvi.tv/idmi6o?v=ccdqic3wgkqwpb36x9sxg43t4r#watch',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        return False if MyviIE.suitable(url) else super(MyviEmbedIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            'https://www.myvi.tv/embed/%s' % video_id, video_id)\n-\n-        myvi_id = self._search_regex(\n-            r'CreatePlayer\\s*\\(\\s*[\"\\'].*?\\bv=([\\da-zA-Z_]+)',\n-            webpage, 'video id')\n-\n-        return self.url_result('myvi:%s' % myvi_id, ie=MyviIE.ie_key())\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/mzaalo.py",
            "diff": "diff --git a/yt_dlp/extractor/mzaalo.py b/yt_dlp/extractor/mzaalo.py\nindex c6f420ce..1996368c 100644\n--- a/yt_dlp/extractor/mzaalo.py\n+++ b/yt_dlp/extractor/mzaalo.py\n@@ -8,7 +8,7 @@\n \n \n class MzaaloIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?mzaalo\\.com/play/(?P<type>movie|original|clip)/(?P<id>[a-fA-F0-9-]+)/[\\w-]+'\n+    _VALID_URL = r'(?i)https?://(?:www\\.)?mzaalo\\.com/(?:play|watch)/(?P<type>movie|original|clip)/(?P<id>[a-f0-9-]+)/[\\w-]+'\n     _TESTS = [{\n         # Movies\n         'url': 'https://www.mzaalo.com/play/movie/c0958d9f-f90e-4503-a755-44358758921d/Jamun',\n@@ -55,6 +55,9 @@ class MzaaloIE(InfoExtractor):\n             'language': 'hin',\n         },\n         'params': {'skip_download': 'm3u8'}\n+    }, {\n+        'url': 'https://mzaalo.com/watch/MOVIE/389c892d-0b65-4019-bf73-d4edcb1c014f/Chalo-Dilli',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/n1.py",
            "diff": "diff --git a/yt_dlp/extractor/n1.py b/yt_dlp/extractor/n1.py\nindex 55345f39..edc41443 100644\n--- a/yt_dlp/extractor/n1.py\n+++ b/yt_dlp/extractor/n1.py\n@@ -33,7 +33,7 @@ def _real_extract(self, url):\n \n class N1InfoIIE(InfoExtractor):\n     IE_NAME = 'N1Info:article'\n-    _VALID_URL = r'https?://(?:(?:(?:ba|rs|hr)\\.)?n1info\\.(?:com|si)|nova\\.rs)/(?:[^/]+/){1,2}(?P<id>[^/]+)'\n+    _VALID_URL = r'https?://(?:(?:\\w+\\.)?n1info\\.\\w+|nova\\.rs)/(?:[^/?#]+/){1,2}(?P<id>[^/?#]+)'\n     _TESTS = [{\n         # Youtube embedded\n         'url': 'https://rs.n1info.com/sport-klub/tenis/kako-je-djokovic-propustio-istorijsku-priliku-video/',\n@@ -94,6 +94,16 @@ class N1InfoIIE(InfoExtractor):\n             'upload_date': '20211102',\n             'timestamp': 1635861677,\n         },\n+    }, {\n+        'url': 'https://n1info.rs/vesti/cuta-biti-u-kosovskoj-mitrovici-znaci-da-te-docekaju-eksplozivnim-napravama/',\n+        'info_dict': {\n+            'id': '1332368',\n+            'ext': 'mp4',\n+            'title': '\u0106uta: Biti u Kosovskoj Mitrovici zna\u010di da te do\u010dekaju eksplozivnim napravama',\n+            'upload_date': '20230620',\n+            'timestamp': 1687290536,\n+            'thumbnail': 'https://cdn.brid.tv/live/partners/26827/snapshot/1332368_th_6492013a8356f_1687290170.jpg'\n+        },\n     }, {\n         'url': 'https://hr.n1info.com/vijesti/pravobraniteljica-o-ubojstvu-u-zagrebu-radi-se-o-doista-nezapamcenoj-situaciji/',\n         'only_matching': True,\n@@ -105,19 +115,35 @@ def _real_extract(self, url):\n \n         title = self._html_search_regex(r'<h1[^>]+>(.+?)</h1>', webpage, 'title')\n         timestamp = unified_timestamp(self._html_search_meta('article:published_time', webpage))\n-\n-        videos = re.findall(r'(?m)(<video[^>]+>)', webpage)\n+        plugin_data = self._html_search_meta('BridPlugin', webpage)\n         entries = []\n-        for video in videos:\n-            video_data = extract_attributes(video)\n-            entries.append({\n-                '_type': 'url_transparent',\n-                'url': video_data.get('data-url'),\n-                'id': video_data.get('id'),\n-                'title': title,\n-                'thumbnail': video_data.get('data-thumbnail'),\n-                'timestamp': timestamp,\n-                'ie_key': 'N1InfoAsset'})\n+        if plugin_data:\n+            site_id = self._html_search_regex(r'site:(\\d+)', webpage, 'site id')\n+            for video_data in re.findall(r'\\$bp\\(\"Brid_\\d+\", (.+)\\);', webpage):\n+                video_id = self._parse_json(video_data, title)['video']\n+                entries.append({\n+                    'id': video_id,\n+                    'title': title,\n+                    'timestamp': timestamp,\n+                    'thumbnail': self._html_search_meta('thumbnailURL', webpage),\n+                    'formats': self._extract_m3u8_formats(\n+                        f'https://cdn-uc.brid.tv/live/partners/{site_id}/streaming/{video_id}/{video_id}.m3u8',\n+                        video_id, fatal=False),\n+                })\n+        else:\n+            # Old player still present in older articles\n+            videos = re.findall(r'(?m)(<video[^>]+>)', webpage)\n+            for video in videos:\n+                video_data = extract_attributes(video)\n+                entries.append({\n+                    '_type': 'url_transparent',\n+                    'url': video_data.get('data-url'),\n+                    'id': video_data.get('id'),\n+                    'title': title,\n+                    'thumbnail': video_data.get('data-thumbnail'),\n+                    'timestamp': timestamp,\n+                    'ie_key': 'N1InfoAsset',\n+                })\n \n         embedded_videos = re.findall(r'(<iframe[^>]+>)', webpage)\n         for embedded_video in embedded_videos:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nationalgeographic.py",
            "diff": "diff --git a/yt_dlp/extractor/nationalgeographic.py b/yt_dlp/extractor/nationalgeographic.py\nindex ad525c25..6f046bc2 100644\n--- a/yt_dlp/extractor/nationalgeographic.py\n+++ b/yt_dlp/extractor/nationalgeographic.py\n@@ -24,6 +24,7 @@ class NationalGeographicVideoIE(InfoExtractor):\n                 'uploader': 'NAGS',\n             },\n             'add_ie': ['ThePlatform'],\n+            'skip': 'Redirects to main page',\n         },\n         {\n             'url': 'http://video.nationalgeographic.com/wild/when-sharks-attack/the-real-jaws',\n@@ -38,6 +39,7 @@ class NationalGeographicVideoIE(InfoExtractor):\n                 'uploader': 'NAGS',\n             },\n             'add_ie': ['ThePlatform'],\n+            'skip': 'Redirects to main page',\n         },\n     ]\n \n@@ -75,6 +77,7 @@ class NationalGeographicTVIE(FOXIE):  # XXX: Do not subclass from concrete IE\n         'params': {\n             'skip_download': True,\n         },\n+        'skip': 'Content not available',\n     }]\n     _HOME_PAGE_URL = 'https://www.nationalgeographic.com/tv/'\n     _API_KEY = '238bb0a0c2aba67922c48709ce0c06fd'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/naver.py",
            "diff": "diff --git a/yt_dlp/extractor/naver.py b/yt_dlp/extractor/naver.py\nindex d79caf5f..2d8459b0 100644\n--- a/yt_dlp/extractor/naver.py\n+++ b/yt_dlp/extractor/naver.py\n@@ -21,7 +21,7 @@\n class NaverBaseIE(InfoExtractor):\n     _CAPTION_EXT_RE = r'\\.(?:ttml|vtt)'\n \n-    @staticmethod  # NB: Used in VLiveWebArchiveIE, WeverseIE\n+    @staticmethod  # NB: Used in WeverseIE\n     def process_subtitles(vod_data, process_url):\n         ret = {'subtitles': {}, 'automatic_captions': {}}\n         for caption in traverse_obj(vod_data, ('captions', 'list', ...)):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nba.py",
            "diff": "diff --git a/yt_dlp/extractor/nba.py b/yt_dlp/extractor/nba.py\nindex d8fc8248..81d11e3a 100644\n--- a/yt_dlp/extractor/nba.py\n+++ b/yt_dlp/extractor/nba.py\n@@ -97,7 +97,7 @@ def _extract_video(self, filter_key, filter_value):\n \n \n class NBAWatchEmbedIE(NBAWatchBaseIE):\n-    IENAME = 'nba:watch:embed'\n+    IE_NAME = 'nba:watch:embed'\n     _VALID_URL = NBAWatchBaseIE._VALID_URL_BASE + r'embed\\?.*?\\bid=(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'http://watch.nba.com/embed?id=659395',\n@@ -339,7 +339,7 @@ def _real_extract(self, url):\n \n \n class NBAEmbedIE(NBABaseIE):\n-    IENAME = 'nba:embed'\n+    IE_NAME = 'nba:embed'\n     _VALID_URL = r'https?://secure\\.nba\\.com/assets/amp/include/video/(?:topI|i)frame\\.html\\?.*?\\bcontentId=(?P<id>[^?#&]+)'\n     _TESTS = [{\n         'url': 'https://secure.nba.com/assets/amp/include/video/topIframe.html?contentId=teams/bulls/2020/12/04/3478774/1607105587854-20201204_SCHEDULE_RELEASE_FINAL_DRUPAL-3478774&team=bulls&adFree=false&profile=71&videoPlayerName=TAMPCVP&baseUrl=&videoAdsection=nba.com_mobile_web_teamsites_chicagobulls&ampEnv=',\n@@ -361,7 +361,7 @@ def _real_extract(self, url):\n \n \n class NBAIE(NBABaseIE):\n-    IENAME = 'nba'\n+    IE_NAME = 'nba'\n     _VALID_URL = NBABaseIE._VALID_URL_BASE + '(?!%s)video/(?P<id>(?:[^/]+/)*[^/?#&]+)' % NBABaseIE._CHANNEL_PATH_REGEX\n     _TESTS = [{\n         'url': 'https://www.nba.com/bulls/video/teams/bulls/2020/12/04/3478774/1607105587854-20201204schedulereleasefinaldrupal-3478774',\n@@ -388,7 +388,7 @@ def _extract_url_results(self, team, content_id):\n \n \n class NBAChannelIE(NBABaseIE):\n-    IENAME = 'nba:channel'\n+    IE_NAME = 'nba:channel'\n     _VALID_URL = NBABaseIE._VALID_URL_BASE + '(?:%s)/(?P<id>[^/?#&]+)' % NBABaseIE._CHANNEL_PATH_REGEX\n     _TESTS = [{\n         'url': 'https://www.nba.com/blazers/video/channel/summer_league',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nbc.py",
            "diff": "diff --git a/yt_dlp/extractor/nbc.py b/yt_dlp/extractor/nbc.py\nindex b3c28ab5..267fa835 100644\n--- a/yt_dlp/extractor/nbc.py\n+++ b/yt_dlp/extractor/nbc.py\n@@ -1,6 +1,7 @@\n import base64\n import json\n import re\n+import xml.etree.ElementTree\n \n from .common import InfoExtractor\n from .theplatform import ThePlatformIE, default_ns\n@@ -52,6 +53,8 @@ class NBCIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'chapters': 'count:1',\n                 'tags': 'count:4',\n                 'thumbnail': r're:https?://.+\\.jpg',\n+                'categories': ['Series/The Tonight Show Starring Jimmy Fallon'],\n+                'media_type': 'Full Episode',\n             },\n             'params': {\n                 'skip_download': 'm3u8',\n@@ -130,6 +133,8 @@ class NBCIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'tags': 'count:10',\n                 'age_limit': 0,\n                 'thumbnail': r're:https?://.+\\.jpg',\n+                'categories': ['Series/Quantum Leap 2022'],\n+                'media_type': 'Highlight',\n             },\n             'params': {\n                 'skip_download': 'm3u8',\n@@ -284,7 +289,7 @@ class NBCSportsIE(InfoExtractor):\n \n     _TESTS = [{\n         # iframe src\n-        'url': 'http://www.nbcsports.com//college-basketball/ncaab/tom-izzo-michigan-st-has-so-much-respect-duke',\n+        'url': 'https://www.nbcsports.com/watch/nfl/profootballtalk/pft-pm/unpacking-addisons-reckless-driving-citation',\n         'info_dict': {\n             'id': 'PHJSaFWbrTY9',\n             'ext': 'mp4',\n@@ -379,7 +384,7 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n     _TESTS = [\n         {\n             'url': 'http://www.nbcnews.com/watch/nbcnews-com/how-twitter-reacted-to-the-snowden-interview-269389891880',\n-            'md5': 'cf4bc9e6ce0130f00f545d80ecedd4bf',\n+            'md5': 'fb3dcd2d7b1dd9804305fa2fc95ab610',  # md5 tends to fluctuate\n             'info_dict': {\n                 'id': '269389891880',\n                 'ext': 'mp4',\n@@ -387,6 +392,8 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'description': 'md5:65a0bd5d76fe114f3c2727aa3a81fe64',\n                 'timestamp': 1401363060,\n                 'upload_date': '20140529',\n+                'duration': 46.0,\n+                'thumbnail': 'https://media-cldnry.s-nbcnews.com/image/upload/MSNBC/Components/Video/140529/p_tweet_snow_140529.jpg',\n             },\n         },\n         {\n@@ -402,7 +409,7 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n         },\n         {\n             'url': 'http://www.nbcnews.com/nightly-news/video/nightly-news-with-brian-williams-full-broadcast-february-4-394064451844',\n-            'md5': '8eb831eca25bfa7d25ddd83e85946548',\n+            'md5': '40d0e48c68896359c80372306ece0fc3',\n             'info_dict': {\n                 'id': '394064451844',\n                 'ext': 'mp4',\n@@ -410,11 +417,13 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'description': 'md5:1c10c1eccbe84a26e5debb4381e2d3c5',\n                 'timestamp': 1423104900,\n                 'upload_date': '20150205',\n+                'duration': 1236.0,\n+                'thumbnail': 'https://media-cldnry.s-nbcnews.com/image/upload/MSNBC/Components/Video/__NEW/nn_netcast_150204.jpg',\n             },\n         },\n         {\n             'url': 'http://www.nbcnews.com/business/autos/volkswagen-11-million-vehicles-could-have-suspect-software-emissions-scandal-n431456',\n-            'md5': '4a8c4cec9e1ded51060bdda36ff0a5c0',\n+            'md5': 'ffb59bcf0733dc3c7f0ace907f5e3939',\n             'info_dict': {\n                 'id': 'n431456',\n                 'ext': 'mp4',\n@@ -422,11 +431,13 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'description': 'md5:d22d1281a24f22ea0880741bb4dd6301',\n                 'upload_date': '20150922',\n                 'timestamp': 1442917800,\n+                'duration': 37.0,\n+                'thumbnail': 'https://media-cldnry.s-nbcnews.com/image/upload/MSNBC/Components/Video/__NEW/x_lon_vwhorn_150922.jpg',\n             },\n         },\n         {\n             'url': 'http://www.today.com/video/see-the-aurora-borealis-from-space-in-stunning-new-nasa-video-669831235788',\n-            'md5': '118d7ca3f0bea6534f119c68ef539f71',\n+            'md5': '693d1fa21d23afcc9b04c66b227ed9ff',\n             'info_dict': {\n                 'id': '669831235788',\n                 'ext': 'mp4',\n@@ -434,6 +445,8 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'description': 'md5:74752b7358afb99939c5f8bb2d1d04b1',\n                 'upload_date': '20160420',\n                 'timestamp': 1461152093,\n+                'duration': 69.0,\n+                'thumbnail': 'https://media-cldnry.s-nbcnews.com/image/upload/MSNBC/Components/Video/201604/2016-04-20T11-35-09-133Z--1280x720.jpg',\n             },\n         },\n         {\n@@ -447,6 +460,7 @@ class NBCNewsIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n                 'thumbnail': r're:^https?://.*\\.jpg$',\n                 'timestamp': 1406937606,\n                 'upload_date': '20140802',\n+                'duration': 940.0,\n             },\n         },\n         {\n@@ -535,6 +549,7 @@ class NBCOlympicsIE(InfoExtractor):\n             'upload_date': '20160815',\n             'uploader': 'NBCU-SPORTS',\n         },\n+        'skip': '404 Not Found',\n     }\n \n     def _real_extract(self, url):\n@@ -578,6 +593,7 @@ class NBCOlympicsStreamIE(AdobePassIE):\n             'params': {\n                 'skip_download': 'm3u8',\n             },\n+            'skip': 'Livestream',\n         }, {\n             'note': 'Plain m3u8 source URL',\n             'url': 'https://stream.nbcolympics.com/gymnastics-event-finals-mens-floor-pommel-horse-womens-vault-bars',\n@@ -589,6 +605,7 @@ class NBCOlympicsStreamIE(AdobePassIE):\n             'params': {\n                 'skip_download': 'm3u8',\n             },\n+            'skip': 'Livestream',\n         },\n     ]\n \n@@ -791,8 +808,10 @@ def _real_extract(self, url):\n             smil = self._download_xml(\n                 f'https://link.theplatform.com/s/{pdk_acct}/{player_id}', video_id,\n                 note='Downloading SMIL data', query=query, fatal=is_live)\n-        subtitles = self._parse_smil_subtitles(smil, default_ns) if smil else {}\n-        for video in smil.findall(self._xpath_ns('.//video', default_ns)) if smil else []:\n+            if not isinstance(smil, xml.etree.ElementTree.Element):\n+                smil = None\n+        subtitles = self._parse_smil_subtitles(smil, default_ns) if smil is not None else {}\n+        for video in smil.findall(self._xpath_ns('.//video', default_ns)) if smil is not None else []:\n             info['duration'] = float_or_none(remove_end(video.get('dur'), 'ms'), 1000)\n             video_src_url = video.get('src')\n             ext = mimetype2ext(video.get('type'), default=determine_ext(video_src_url))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nebula.py",
            "diff": "diff --git a/yt_dlp/extractor/nebula.py b/yt_dlp/extractor/nebula.py\nindex 4f3e691b..136b0e10 100644\n--- a/yt_dlp/extractor/nebula.py\n+++ b/yt_dlp/extractor/nebula.py\n@@ -3,231 +3,306 @@\n \n from .common import InfoExtractor\n from ..networking.exceptions import HTTPError\n-from ..utils import ExtractorError, make_archive_id, parse_iso8601, remove_start\n+from ..utils import (\n+    ExtractorError,\n+    int_or_none,\n+    make_archive_id,\n+    parse_iso8601,\n+    smuggle_url,\n+    try_call,\n+    unsmuggle_url,\n+    update_url_query,\n+    url_or_none,\n+    urljoin,\n+)\n+from ..utils.traversal import traverse_obj\n \n _BASE_URL_RE = r'https?://(?:www\\.|beta\\.)?(?:watchnebula\\.com|nebula\\.app|nebula\\.tv)'\n \n \n class NebulaBaseIE(InfoExtractor):\n     _NETRC_MACHINE = 'watchnebula'\n+    _token = _api_token = None\n \n-    _nebula_api_token = None\n-    _nebula_bearer_token = None\n-\n-    def _perform_nebula_auth(self, username, password):\n-        if not username or not password:\n-            self.raise_login_required(method='password')\n-\n-        data = json.dumps({'email': username, 'password': password}).encode('utf8')\n-        response = self._download_json(\n-            'https://api.watchnebula.com/api/v1/auth/login/',\n-            data=data, fatal=False, video_id=None,\n-            headers={\n-                'content-type': 'application/json',\n-                # Submitting the 'sessionid' cookie always causes a 403 on auth endpoint\n-                'cookie': ''\n-            },\n-            note='Logging in to Nebula with supplied credentials',\n-            errnote='Authentication failed or rejected')\n-        if not response or not response.get('key'):\n-            self.raise_login_required(method='password')\n-\n-        return response['key']\n-\n-    def _call_nebula_api(self, url, video_id=None, method='GET', auth_type='api', note=''):\n-        assert method in ('GET', 'POST',)\n-        assert auth_type in ('api', 'bearer',)\n-\n-        def inner_call():\n-            authorization = f'Token {self._nebula_api_token}' if auth_type == 'api' else f'Bearer {self._nebula_bearer_token}'\n-            return self._download_json(\n-                url, video_id, note=note, headers={'Authorization': authorization},\n-                data=b'' if method == 'POST' else None)\n+    def _perform_login(self, username, password):\n+        try:\n+            response = self._download_json(\n+                'https://nebula.tv/auth/login/', None,\n+                'Logging in to Nebula', 'Login failed',\n+                data=json.dumps({'email': username, 'password': password}).encode(),\n+                headers={'content-type': 'application/json'})\n+        except ExtractorError as e:\n+            if isinstance(e.cause, HTTPError) and e.cause.status == 400:\n+                raise ExtractorError('Login failed: Invalid username or password', expected=True)\n+            raise\n+        self._api_token = traverse_obj(response, ('key', {str}))\n+        if not self._api_token:\n+            raise ExtractorError('Login failed: No token')\n \n+    def _call_api(self, *args, **kwargs):\n+        if self._token:\n+            kwargs.setdefault('headers', {})['Authorization'] = f'Bearer {self._token}'\n         try:\n-            return inner_call()\n-        except ExtractorError as exc:\n-            # if 401 or 403, attempt credential re-auth and retry\n-            if exc.cause and isinstance(exc.cause, HTTPError) and exc.cause.status in (401, 403):\n-                self.to_screen(f'Reauthenticating to Nebula and retrying, because last {auth_type} call resulted in error {exc.cause.code}')\n-                self._perform_login()\n-                return inner_call()\n-            else:\n+            return self._download_json(*args, **kwargs)\n+        except ExtractorError as e:\n+            if not isinstance(e.cause, HTTPError) or e.cause.status not in (401, 403):\n                 raise\n+            self.to_screen(\n+                f'Reauthorizing with Nebula and retrying, because last API call resulted in error {e.cause.status}')\n+            self._real_initialize()\n+            if self._token:\n+                kwargs.setdefault('headers', {})['Authorization'] = f'Bearer {self._token}'\n+            return self._download_json(*args, **kwargs)\n \n-    def _fetch_nebula_bearer_token(self):\n-        \"\"\"\n-        Get a Bearer token for the Nebula API. This will be required to fetch video meta data.\n-        \"\"\"\n-        response = self._call_nebula_api('https://api.watchnebula.com/api/v1/authorization/',\n-                                         method='POST',\n-                                         note='Authorizing to Nebula')\n-        return response['token']\n+    def _real_initialize(self):\n+        if not self._api_token:\n+            self._api_token = try_call(\n+                lambda: self._get_cookies('https://nebula.tv')['nebula_auth.apiToken'].value)\n+        self._token = self._download_json(\n+            'https://users.api.nebula.app/api/v1/authorization/', None,\n+            headers={'Authorization': f'Token {self._api_token}'} if self._api_token else None,\n+            note='Authorizing to Nebula', data=b'')['token']\n \n-    def _fetch_video_formats(self, slug):\n-        stream_info = self._call_nebula_api(f'https://content.api.nebula.app/video/{slug}/stream/',\n-                                            video_id=slug,\n-                                            auth_type='bearer',\n-                                            note='Fetching video stream info')\n-        manifest_url = stream_info['manifest']\n-        return self._extract_m3u8_formats_and_subtitles(manifest_url, slug, 'mp4')\n+    def _extract_formats(self, content_id, slug):\n+        for retry in (False, True):\n+            try:\n+                fmts, subs = self._extract_m3u8_formats_and_subtitles(\n+                    f'https://content.api.nebula.app/{content_id.split(\":\")[0]}s/{content_id}/manifest.m3u8',\n+                    slug, 'mp4', query={\n+                        'token': self._token,\n+                        'app_version': '23.10.0',\n+                        'platform': 'ios',\n+                    })\n+                return {'formats': fmts, 'subtitles': subs}\n+            except ExtractorError as e:\n+                if isinstance(e.cause, HTTPError) and e.cause.status == 401:\n+                    self.raise_login_required()\n+                if not retry and isinstance(e.cause, HTTPError) and e.cause.status == 403:\n+                    self.to_screen('Reauthorizing with Nebula and retrying, because fetching video resulted in error')\n+                    self._real_initialize()\n+                    continue\n+                raise\n \n-    def _build_video_info(self, episode):\n-        fmts, subs = self._fetch_video_formats(episode['slug'])\n-        channel_slug = episode['channel_slug']\n-        channel_title = episode['channel_title']\n-        zype_id = episode.get('zype_id')\n+    def _extract_video_metadata(self, episode):\n+        channel_url = traverse_obj(\n+            episode, (('channel_slug', 'class_slug'), {lambda x: urljoin('https://nebula.tv/', x)}), get_all=False)\n         return {\n-            'id': remove_start(episode['id'], 'video_episode:'),\n-            'display_id': episode['slug'],\n-            'formats': fmts,\n-            'subtitles': subs,\n-            'webpage_url': f'https://nebula.tv/{episode[\"slug\"]}',\n-            'title': episode['title'],\n-            'description': episode['description'],\n-            'timestamp': parse_iso8601(episode['published_at']),\n-            'thumbnails': [{\n-                # 'id': tn.get('name'),  # this appears to be null\n-                'url': tn['original'],\n-                'height': key,\n-            } for key, tn in episode['assets']['thumbnail'].items()],\n-            'duration': episode['duration'],\n-            'channel': channel_title,\n-            'channel_id': channel_slug,\n-            'channel_url': f'https://nebula.tv/{channel_slug}',\n-            'uploader': channel_title,\n-            'uploader_id': channel_slug,\n-            'uploader_url': f'https://nebula.tv/{channel_slug}',\n-            'series': channel_title,\n-            'creator': channel_title,\n-            'extractor_key': NebulaIE.ie_key(),\n-            'extractor': NebulaIE.IE_NAME,\n-            '_old_archive_ids': [make_archive_id(NebulaIE, zype_id)] if zype_id else None,\n+            'id': episode['id'].partition(':')[2],\n+            **traverse_obj(episode, {\n+                'display_id': 'slug',\n+                'title': 'title',\n+                'description': 'description',\n+                'timestamp': ('published_at', {parse_iso8601}),\n+                'duration': ('duration', {int_or_none}),\n+                'channel_id': 'channel_slug',\n+                'uploader_id': 'channel_slug',\n+                'channel': 'channel_title',\n+                'uploader': 'channel_title',\n+                'series': 'channel_title',\n+                'creator': 'channel_title',\n+                'thumbnail': ('images', 'thumbnail', 'src', {url_or_none}),\n+                'episode_number': ('order', {int_or_none}),\n+                # Old code was wrongly setting extractor_key from NebulaSubscriptionsIE\n+                '_old_archive_ids': ('zype_id', {lambda x: [\n+                    make_archive_id(NebulaIE, x), make_archive_id(NebulaSubscriptionsIE, x)] if x else None}),\n+            }),\n+            'channel_url': channel_url,\n+            'uploader_url': channel_url,\n         }\n \n-    def _perform_login(self, username=None, password=None):\n-        self._nebula_api_token = self._perform_nebula_auth(username, password)\n-        self._nebula_bearer_token = self._fetch_nebula_bearer_token()\n-\n \n class NebulaIE(NebulaBaseIE):\n     _VALID_URL = rf'{_BASE_URL_RE}/videos/(?P<id>[-\\w]+)'\n-    _TESTS = [\n-        {\n-            'url': 'https://nebula.tv/videos/that-time-disney-remade-beauty-and-the-beast',\n-            'md5': '14944cfee8c7beeea106320c47560efc',\n-            'info_dict': {\n-                'id': '84ed544d-4afd-4723-8cd5-2b95261f0abf',\n-                'ext': 'mp4',\n-                'title': 'That Time Disney Remade Beauty and the Beast',\n-                'description': 'Note: this video was originally posted on YouTube with the sponsor read included. We weren\u2019t able to remove it without reducing video quality, so it\u2019s presented here in its original context.',\n-                'upload_date': '20180731',\n-                'timestamp': 1533009600,\n-                'channel': 'Lindsay Ellis',\n-                'channel_id': 'lindsayellis',\n-                'uploader': 'Lindsay Ellis',\n-                'uploader_id': 'lindsayellis',\n-                'timestamp': 1533009600,\n-                'uploader_url': 'https://nebula.tv/lindsayellis',\n-                'series': 'Lindsay Ellis',\n-                'display_id': 'that-time-disney-remade-beauty-and-the-beast',\n-                'channel_url': 'https://nebula.tv/lindsayellis',\n-                'creator': 'Lindsay Ellis',\n-                'duration': 2212,\n-                'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+\\.jpeg?.*',\n-            },\n-        },\n-        {\n-            'url': 'https://nebula.tv/videos/the-logistics-of-d-day-landing-craft-how-the-allies-got-ashore',\n-            'md5': 'd05739cf6c38c09322422f696b569c23',\n-            'info_dict': {\n-                'id': '7e623145-1b44-4ca3-aa0b-ed25a247ea34',\n-                'ext': 'mp4',\n-                'title': 'Landing Craft - How The Allies Got Ashore',\n-                'description': r're:^In this episode we explore the unsung heroes of D-Day, the landing craft.',\n-                'upload_date': '20200327',\n-                'timestamp': 1585348140,\n-                'channel': 'Real Engineering \u2014 The Logistics of D-Day',\n-                'channel_id': 'd-day',\n-                'uploader': 'Real Engineering \u2014 The Logistics of D-Day',\n-                'uploader_id': 'd-day',\n-                'series': 'Real Engineering \u2014 The Logistics of D-Day',\n-                'display_id': 'the-logistics-of-d-day-landing-craft-how-the-allies-got-ashore',\n-                'creator': 'Real Engineering \u2014 The Logistics of D-Day',\n-                'duration': 841,\n-                'channel_url': 'https://nebula.tv/d-day',\n-                'uploader_url': 'https://nebula.tv/d-day',\n-                'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+\\.jpeg?.*',\n-            },\n+    _TESTS = [{\n+        'url': 'https://nebula.tv/videos/that-time-disney-remade-beauty-and-the-beast',\n+        'info_dict': {\n+            'id': '84ed544d-4afd-4723-8cd5-2b95261f0abf',\n+            'ext': 'mp4',\n+            'title': 'That Time Disney Remade Beauty and the Beast',\n+            'description': 'md5:2aae3c4cfc5ee09a1ecdff0909618cf4',\n+            'upload_date': '20180731',\n+            'timestamp': 1533009600,\n+            'channel': 'Lindsay Ellis',\n+            'channel_id': 'lindsayellis',\n+            'uploader': 'Lindsay Ellis',\n+            'uploader_id': 'lindsayellis',\n+            'uploader_url': r're:https://nebula\\.(tv|app)/lindsayellis',\n+            'series': 'Lindsay Ellis',\n+            'display_id': 'that-time-disney-remade-beauty-and-the-beast',\n+            'channel_url': r're:https://nebula\\.(tv|app)/lindsayellis',\n+            'creator': 'Lindsay Ellis',\n+            'duration': 2212,\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+',\n+            '_old_archive_ids': ['nebula 5c271b40b13fd613090034fd', 'nebulasubscriptions 5c271b40b13fd613090034fd'],\n         },\n-        {\n-            'url': 'https://nebula.tv/videos/money-episode-1-the-draw',\n-            'md5': 'ebe28a7ad822b9ee172387d860487868',\n-            'info_dict': {\n-                'id': 'b96c5714-9e2b-4ec3-b3f1-20f6e89cc553',\n-                'ext': 'mp4',\n-                'title': 'Episode 1: The Draw',\n-                'description': r'contains:There\u2019s free money on offer\u2026 if the players can all work together.',\n-                'upload_date': '20200323',\n-                'timestamp': 1584980400,\n-                'channel': 'Tom Scott Presents: Money',\n-                'channel_id': 'tom-scott-presents-money',\n-                'uploader': 'Tom Scott Presents: Money',\n-                'uploader_id': 'tom-scott-presents-money',\n-                'uploader_url': 'https://nebula.tv/tom-scott-presents-money',\n-                'duration': 825,\n-                'channel_url': 'https://nebula.tv/tom-scott-presents-money',\n-                'series': 'Tom Scott Presents: Money',\n-                'display_id': 'money-episode-1-the-draw',\n-                'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+\\.jpeg?.*',\n-                'creator': 'Tom Scott Presents: Money',\n-            },\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://nebula.tv/videos/the-logistics-of-d-day-landing-craft-how-the-allies-got-ashore',\n+        'md5': 'd05739cf6c38c09322422f696b569c23',\n+        'info_dict': {\n+            'id': '7e623145-1b44-4ca3-aa0b-ed25a247ea34',\n+            'ext': 'mp4',\n+            'title': 'Landing Craft - How The Allies Got Ashore',\n+            'description': r're:^In this episode we explore the unsung heroes of D-Day, the landing craft.',\n+            'upload_date': '20200327',\n+            'timestamp': 1585348140,\n+            'channel': 'Real Engineering \u2014 The Logistics of D-Day',\n+            'channel_id': 'd-day',\n+            'uploader': 'Real Engineering \u2014 The Logistics of D-Day',\n+            'uploader_id': 'd-day',\n+            'series': 'Real Engineering \u2014 The Logistics of D-Day',\n+            'display_id': 'the-logistics-of-d-day-landing-craft-how-the-allies-got-ashore',\n+            'creator': 'Real Engineering \u2014 The Logistics of D-Day',\n+            'duration': 841,\n+            'channel_url': 'https://nebula.tv/d-day',\n+            'uploader_url': 'https://nebula.tv/d-day',\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+',\n+            '_old_archive_ids': ['nebula 5e7e78171aaf320001fbd6be', 'nebulasubscriptions 5e7e78171aaf320001fbd6be'],\n         },\n-        {\n-            'url': 'https://watchnebula.com/videos/money-episode-1-the-draw',\n-            'only_matching': True,\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://nebula.tv/videos/money-episode-1-the-draw',\n+        'md5': 'ebe28a7ad822b9ee172387d860487868',\n+        'info_dict': {\n+            'id': 'b96c5714-9e2b-4ec3-b3f1-20f6e89cc553',\n+            'ext': 'mp4',\n+            'title': 'Episode 1: The Draw',\n+            'description': r'contains:There\u2019s free money on offer\u2026 if the players can all work together.',\n+            'upload_date': '20200323',\n+            'timestamp': 1584980400,\n+            'channel': 'Tom Scott Presents: Money',\n+            'channel_id': 'tom-scott-presents-money',\n+            'uploader': 'Tom Scott Presents: Money',\n+            'uploader_id': 'tom-scott-presents-money',\n+            'uploader_url': 'https://nebula.tv/tom-scott-presents-money',\n+            'duration': 825,\n+            'channel_url': 'https://nebula.tv/tom-scott-presents-money',\n+            'series': 'Tom Scott Presents: Money',\n+            'display_id': 'money-episode-1-the-draw',\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+',\n+            'creator': 'Tom Scott Presents: Money',\n+            '_old_archive_ids': ['nebula 5e779ebdd157bc0001d1c75a', 'nebulasubscriptions 5e779ebdd157bc0001d1c75a'],\n         },\n-        {\n-            'url': 'https://beta.nebula.tv/videos/money-episode-1-the-draw',\n-            'only_matching': True,\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://watchnebula.com/videos/money-episode-1-the-draw',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://nebula.tv/videos/tldrnewseu-did-the-us-really-blow-up-the-nordstream-pipelines',\n+        'info_dict': {\n+            'id': 'e389af9d-1dab-44f2-8788-ee24deb7ff0d',\n+            'ext': 'mp4',\n+            'display_id': 'tldrnewseu-did-the-us-really-blow-up-the-nordstream-pipelines',\n+            'title': 'Did the US Really Blow Up the NordStream Pipelines?',\n+            'description': 'md5:b4e2a14e3ff08f546a3209c75261e789',\n+            'upload_date': '20230223',\n+            'timestamp': 1677144070,\n+            'channel': 'TLDR News EU',\n+            'channel_id': 'tldrnewseu',\n+            'uploader': 'TLDR News EU',\n+            'uploader_id': 'tldrnewseu',\n+            'uploader_url': r're:https://nebula\\.(tv|app)/tldrnewseu',\n+            'duration': 524,\n+            'channel_url': r're:https://nebula\\.(tv|app)/tldrnewseu',\n+            'series': 'TLDR News EU',\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/[\\w-]+',\n+            'creator': 'TLDR News EU',\n+            '_old_archive_ids': ['nebula 63f64c74366fcd00017c1513', 'nebulasubscriptions 63f64c74366fcd00017c1513'],\n         },\n-    ]\n-\n-    def _fetch_video_metadata(self, slug):\n-        return self._call_nebula_api(f'https://content.api.nebula.app/video/{slug}/',\n-                                     video_id=slug,\n-                                     auth_type='bearer',\n-                                     note='Fetching video meta data')\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://beta.nebula.tv/videos/money-episode-1-the-draw',\n+        'only_matching': True,\n+    }]\n \n     def _real_extract(self, url):\n         slug = self._match_id(url)\n-        video = self._fetch_video_metadata(slug)\n-        return self._build_video_info(video)\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        if smuggled_data.get('id'):\n+            return {\n+                'id': smuggled_data['id'],\n+                'display_id': slug,\n+                'title': '',\n+                **self._extract_formats(smuggled_data['id'], slug),\n+            }\n+\n+        metadata = self._call_api(\n+            f'https://content.api.nebula.app/content/videos/{slug}',\n+            slug, note='Fetching video metadata')\n+        return {\n+            **self._extract_video_metadata(metadata),\n+            **self._extract_formats(metadata['id'], slug),\n+        }\n+\n+\n+class NebulaClassIE(NebulaBaseIE):\n+    IE_NAME = 'nebula:class'\n+    _VALID_URL = rf'{_BASE_URL_RE}/(?P<id>[-\\w]+)/(?P<ep>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://nebula.tv/copyright-for-fun-and-profit/14',\n+        'info_dict': {\n+            'id': 'd7432cdc-c608-474d-942c-f74345daed7b',\n+            'ext': 'mp4',\n+            'display_id': '14',\n+            'channel_url': 'https://nebula.tv/copyright-for-fun-and-profit',\n+            'episode_number': 14,\n+            'thumbnail': 'https://dj423fildxgac.cloudfront.net/d533718d-9307-42d4-8fb0-e283285e99c9',\n+            'uploader_url': 'https://nebula.tv/copyright-for-fun-and-profit',\n+            'duration': 646,\n+            'episode': 'Episode 14',\n+            'title': 'Photos, Sculpture, and Video',\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }]\n+\n+    def _real_extract(self, url):\n+        slug, episode = self._match_valid_url(url).group('id', 'ep')\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        if smuggled_data.get('id'):\n+            return {\n+                'id': smuggled_data['id'],\n+                'display_id': slug,\n+                'title': '',\n+                **self._extract_formats(smuggled_data['id'], slug),\n+            }\n+\n+        metadata = self._call_api(\n+            f'https://content.api.nebula.app/content/{slug}/{episode}/?include=lessons',\n+            slug, note='Fetching video metadata')\n+        return {\n+            **self._extract_video_metadata(metadata),\n+            **self._extract_formats(metadata['id'], slug),\n+        }\n \n \n class NebulaSubscriptionsIE(NebulaBaseIE):\n     IE_NAME = 'nebula:subscriptions'\n-    _VALID_URL = rf'{_BASE_URL_RE}/myshows'\n-    _TESTS = [\n-        {\n-            'url': 'https://nebula.tv/myshows',\n-            'playlist_mincount': 1,\n-            'info_dict': {\n-                'id': 'myshows',\n-            },\n+    _VALID_URL = rf'{_BASE_URL_RE}/(?P<id>myshows|library/latest-videos)'\n+    _TESTS = [{\n+        'url': 'https://nebula.tv/myshows',\n+        'playlist_mincount': 1,\n+        'info_dict': {\n+            'id': 'myshows',\n         },\n-    ]\n+    }]\n \n     def _generate_playlist_entries(self):\n-        next_url = 'https://content.watchnebula.com/library/video/?page_size=100'\n-        page_num = 1\n-        while next_url:\n-            channel = self._call_nebula_api(next_url, 'myshows', auth_type='bearer',\n-                                            note=f'Retrieving subscriptions page {page_num}')\n+        next_url = update_url_query('https://content.api.nebula.app/video_episodes/', {\n+            'following': 'true',\n+            'include': 'engagement',\n+            'ordering': '-published_at',\n+        })\n+        for page_num in itertools.count(1):\n+            channel = self._call_api(\n+                next_url, 'myshows', note=f'Retrieving subscriptions page {page_num}')\n             for episode in channel['results']:\n-                yield self._build_video_info(episode)\n-            next_url = channel['next']\n-            page_num += 1\n+                metadata = self._extract_video_metadata(episode)\n+                yield self.url_result(smuggle_url(\n+                    f'https://nebula.tv/videos/{metadata[\"display_id\"]}',\n+                    {'id': episode['id']}), NebulaIE, url_transparent=True, **metadata)\n+            next_url = channel.get('next')\n+            if not next_url:\n+                return\n \n     def _real_extract(self, url):\n         return self.playlist_result(self._generate_playlist_entries(), 'myshows')\n@@ -235,48 +310,74 @@ def _real_extract(self, url):\n \n class NebulaChannelIE(NebulaBaseIE):\n     IE_NAME = 'nebula:channel'\n-    _VALID_URL = rf'{_BASE_URL_RE}/(?!myshows|videos/)(?P<id>[-\\w]+)'\n-    _TESTS = [\n-        {\n-            'url': 'https://nebula.tv/tom-scott-presents-money',\n-            'info_dict': {\n-                'id': 'tom-scott-presents-money',\n-                'title': 'Tom Scott Presents: Money',\n-                'description': 'Tom Scott hosts a series all about trust, negotiation and money.',\n-            },\n-            'playlist_count': 5,\n-        }, {\n-            'url': 'https://nebula.tv/lindsayellis',\n-            'info_dict': {\n-                'id': 'lindsayellis',\n-                'title': 'Lindsay Ellis',\n-                'description': 'Enjoy these hottest of takes on Disney, Transformers, and Musicals.',\n-            },\n-            'playlist_mincount': 2,\n+    _VALID_URL = rf'{_BASE_URL_RE}/(?!myshows|library|videos/)(?P<id>[-\\w]+)/?(?:$|[?#])'\n+    _TESTS = [{\n+        'url': 'https://nebula.tv/tom-scott-presents-money',\n+        'info_dict': {\n+            'id': 'tom-scott-presents-money',\n+            'title': 'Tom Scott Presents: Money',\n+            'description': 'Tom Scott hosts a series all about trust, negotiation and money.',\n+        },\n+        'playlist_count': 5,\n+    }, {\n+        'url': 'https://nebula.tv/lindsayellis',\n+        'info_dict': {\n+            'id': 'lindsayellis',\n+            'title': 'Lindsay Ellis',\n+            'description': 'Enjoy these hottest of takes on Disney, Transformers, and Musicals.',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        'url': 'https://nebula.tv/johnnyharris',\n+        'info_dict': {\n+            'id': 'johnnyharris',\n+            'title': 'Johnny Harris',\n+            'description': 'I make videos about maps and many other things.',\n         },\n-    ]\n+        'playlist_mincount': 90,\n+    }, {\n+        'url': 'https://nebula.tv/copyright-for-fun-and-profit',\n+        'info_dict': {\n+            'id': 'copyright-for-fun-and-profit',\n+            'title': 'Copyright for Fun and Profit',\n+            'description': 'md5:6690248223eed044a9f11cd5a24f9742',\n+        },\n+        'playlist_count': 23,\n+    }]\n \n-    def _generate_playlist_entries(self, collection_id, channel):\n-        episodes = channel['episodes']['results']\n-        for page_num in itertools.count(2):\n-            for episode in episodes:\n-                yield self._build_video_info(episode)\n-            next_url = channel['episodes']['next']\n+    def _generate_playlist_entries(self, collection_id, collection_slug):\n+        next_url = f'https://content.api.nebula.app/video_channels/{collection_id}/video_episodes/?ordering=-published_at'\n+        for page_num in itertools.count(1):\n+            episodes = self._call_api(next_url, collection_slug, note=f'Retrieving channel page {page_num}')\n+            for episode in episodes['results']:\n+                metadata = self._extract_video_metadata(episode)\n+                yield self.url_result(smuggle_url(\n+                    episode.get('share_url') or f'https://nebula.tv/videos/{metadata[\"display_id\"]}',\n+                    {'id': episode['id']}), NebulaIE, url_transparent=True, **metadata)\n+            next_url = episodes.get('next')\n             if not next_url:\n                 break\n-            channel = self._call_nebula_api(next_url, collection_id, auth_type='bearer',\n-                                            note=f'Retrieving channel page {page_num}')\n-            episodes = channel['episodes']['results']\n+\n+    def _generate_class_entries(self, channel):\n+        for lesson in channel['lessons']:\n+            metadata = self._extract_video_metadata(lesson)\n+            yield self.url_result(smuggle_url(\n+                lesson.get('share_url') or f'https://nebula.tv/{metadata[\"class_slug\"]}/{metadata[\"slug\"]}',\n+                {'id': lesson['id']}), NebulaClassIE, url_transparent=True, **metadata)\n \n     def _real_extract(self, url):\n-        collection_id = self._match_id(url)\n-        channel_url = f'https://content.watchnebula.com/video/channels/{collection_id}/'\n-        channel = self._call_nebula_api(channel_url, collection_id, auth_type='bearer', note='Retrieving channel')\n-        channel_details = channel['details']\n+        collection_slug = self._match_id(url)\n+        channel = self._call_api(\n+            f'https://content.api.nebula.app/content/{collection_slug}/?include=lessons',\n+            collection_slug, note='Retrieving channel')\n+\n+        if channel.get('type') == 'class':\n+            entries = self._generate_class_entries(channel)\n+        else:\n+            entries = self._generate_playlist_entries(channel['id'], collection_slug)\n \n         return self.playlist_result(\n-            entries=self._generate_playlist_entries(collection_id, channel),\n-            playlist_id=collection_id,\n-            playlist_title=channel_details['title'],\n-            playlist_description=channel_details['description']\n-        )\n+            entries=entries,\n+            playlist_id=collection_slug,\n+            playlist_title=channel.get('title'),\n+            playlist_description=channel.get('description'))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/neteasemusic.py",
            "diff": "diff --git a/yt_dlp/extractor/neteasemusic.py b/yt_dlp/extractor/neteasemusic.py\nindex 5b7307bc..d332b840 100644\n--- a/yt_dlp/extractor/neteasemusic.py\n+++ b/yt_dlp/extractor/neteasemusic.py\n@@ -2,105 +2,74 @@\n import json\n import re\n import time\n-from base64 import b64encode\n-from binascii import hexlify\n-from datetime import datetime\n from hashlib import md5\n from random import randint\n \n from .common import InfoExtractor\n from ..aes import aes_ecb_encrypt, pkcs7_padding\n-from ..compat import compat_urllib_parse_urlencode\n-from ..networking import Request\n from ..utils import (\n     ExtractorError,\n-    bytes_to_intlist,\n-    error_to_compat_str,\n-    float_or_none,\n     int_or_none,\n-    intlist_to_bytes,\n-    try_get,\n+    join_nonempty,\n+    str_or_none,\n+    strftime_or_none,\n+    traverse_obj,\n+    unified_strdate,\n+    url_or_none,\n+    urljoin,\n+    variadic,\n )\n \n \n class NetEaseMusicBaseIE(InfoExtractor):\n     _FORMATS = ['bMusic', 'mMusic', 'hMusic']\n-    _NETEASE_SALT = '3go8&$8*3*3h0k(2)2'\n     _API_BASE = 'http://music.163.com/api/'\n+    _GEO_BYPASS = False\n \n-    @classmethod\n-    def _encrypt(cls, dfsid):\n-        salt_bytes = bytearray(cls._NETEASE_SALT.encode('utf-8'))\n-        string_bytes = bytearray(str(dfsid).encode('ascii'))\n-        salt_len = len(salt_bytes)\n-        for i in range(len(string_bytes)):\n-            string_bytes[i] = string_bytes[i] ^ salt_bytes[i % salt_len]\n-        m = md5()\n-        m.update(bytes(string_bytes))\n-        result = b64encode(m.digest()).decode('ascii')\n-        return result.replace('/', '_').replace('+', '-')\n-\n-    def make_player_api_request_data_and_headers(self, song_id, bitrate):\n-        KEY = b'e82ckenh8dichen8'\n-        URL = '/api/song/enhance/player/url'\n-        now = int(time.time() * 1000)\n-        rand = randint(0, 1000)\n-        cookie = {\n-            'osver': None,\n-            'deviceId': None,\n+    @staticmethod\n+    def kilo_or_none(value):\n+        return int_or_none(value, scale=1000)\n+\n+    def _create_eapi_cipher(self, api_path, query_body, cookies):\n+        request_text = json.dumps({**query_body, 'header': cookies}, separators=(',', ':'))\n+\n+        message = f'nobody{api_path}use{request_text}md5forencrypt'.encode('latin1')\n+        msg_digest = md5(message).hexdigest()\n+\n+        data = pkcs7_padding(list(str.encode(\n+            f'{api_path}-36cd479b6b5-{request_text}-36cd479b6b5-{msg_digest}')))\n+        encrypted = bytes(aes_ecb_encrypt(data, list(b'e82ckenh8dichen8')))\n+        return f'params={encrypted.hex().upper()}'.encode()\n+\n+    def _download_eapi_json(self, path, video_id, query_body, headers={}, **kwargs):\n+        cookies = {\n+            'osver': 'undefined',\n+            'deviceId': 'undefined',\n             'appver': '8.0.0',\n             'versioncode': '140',\n-            'mobilename': None,\n+            'mobilename': 'undefined',\n             'buildver': '1623435496',\n             'resolution': '1920x1080',\n             '__csrf': '',\n             'os': 'pc',\n-            'channel': None,\n-            'requestId': '{0}_{1:04}'.format(now, rand),\n-        }\n-        request_text = json.dumps(\n-            {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie},\n-            separators=(',', ':'))\n-        message = 'nobody{0}use{1}md5forencrypt'.format(\n-            URL, request_text).encode('latin1')\n-        msg_digest = md5(message).hexdigest()\n-\n-        data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format(\n-            URL, request_text, msg_digest)\n-        data = pkcs7_padding(bytes_to_intlist(data))\n-        encrypted = intlist_to_bytes(aes_ecb_encrypt(data, bytes_to_intlist(KEY)))\n-        encrypted_params = hexlify(encrypted).decode('ascii').upper()\n-\n-        cookie = '; '.join(\n-            ['{0}={1}'.format(k, v if v is not None else 'undefined')\n-             for [k, v] in cookie.items()])\n-\n-        headers = {\n-            'User-Agent': self.extractor.get_param('http_headers')['User-Agent'],\n-            'Content-Type': 'application/x-www-form-urlencoded',\n-            'Referer': 'https://music.163.com',\n-            'Cookie': cookie,\n+            'channel': 'undefined',\n+            'requestId': f'{int(time.time() * 1000)}_{randint(0, 1000):04}',\n+            **traverse_obj(self._get_cookies(self._API_BASE), {\n+                'MUSIC_U': ('MUSIC_U', {lambda i: i.value}),\n+            })\n         }\n-        return ('params={0}'.format(encrypted_params), headers)\n+        return self._download_json(\n+            urljoin('https://interface3.music.163.com/', f'/eapi{path}'), video_id,\n+            data=self._create_eapi_cipher(f'/api{path}', query_body, cookies), headers={\n+                'Referer': 'https://music.163.com',\n+                'Cookie': '; '.join([f'{k}={v}' for k, v in cookies.items()]),\n+                **headers,\n+            }, **kwargs)\n \n     def _call_player_api(self, song_id, bitrate):\n-        url = 'https://interface3.music.163.com/eapi/song/enhance/player/url'\n-        data, headers = self.make_player_api_request_data_and_headers(song_id, bitrate)\n-        try:\n-            msg = 'empty result'\n-            result = self._download_json(\n-                url, song_id, data=data.encode('ascii'), headers=headers)\n-            if result:\n-                return result\n-        except ExtractorError as e:\n-            if type(e.cause) in (ValueError, TypeError):\n-                # JSON load failure\n-                raise\n-        except Exception as e:\n-            msg = error_to_compat_str(e)\n-            self.report_warning('%s API call (%s) failed: %s' % (\n-                song_id, bitrate, msg))\n-        return {}\n+        return self._download_eapi_json(\n+            '/song/enhance/player/url', song_id, {'ids': f'[{song_id}]', 'br': bitrate},\n+            note=f'Downloading song URL info: bitrate {bitrate}')\n \n     def extract_formats(self, info):\n         err = 0\n@@ -110,45 +79,50 @@ def extract_formats(self, info):\n             details = info.get(song_format)\n             if not details:\n                 continue\n-\n             bitrate = int_or_none(details.get('bitrate')) or 999000\n-            data = self._call_player_api(song_id, bitrate)\n-            for song in try_get(data, lambda x: x['data'], list) or []:\n-                song_url = try_get(song, lambda x: x['url'])\n-                if not song_url:\n-                    continue\n+            for song in traverse_obj(self._call_player_api(song_id, bitrate), ('data', lambda _, v: url_or_none(v['url']))):\n+                song_url = song['url']\n                 if self._is_valid_url(song_url, info['id'], 'song'):\n                     formats.append({\n                         'url': song_url,\n-                        'ext': details.get('extension'),\n-                        'abr': float_or_none(song.get('br'), scale=1000),\n                         'format_id': song_format,\n-                        'filesize': int_or_none(song.get('size')),\n-                        'asr': int_or_none(details.get('sr')),\n+                        'asr': traverse_obj(details, ('sr', {int_or_none})),\n+                        **traverse_obj(song, {\n+                            'ext': ('type', {str}),\n+                            'abr': ('br', {self.kilo_or_none}),\n+                            'filesize': ('size', {int_or_none}),\n+                        }),\n                     })\n                 elif err == 0:\n-                    err = try_get(song, lambda x: x['code'], int)\n+                    err = traverse_obj(song, ('code', {int})) or 0\n \n         if not formats:\n-            msg = 'No media links found'\n             if err != 0 and (err < 200 or err >= 400):\n-                raise ExtractorError(\n-                    '%s (site code %d)' % (msg, err, ), expected=True)\n+                raise ExtractorError(f'No media links found (site code {err})', expected=True)\n             else:\n                 self.raise_geo_restricted(\n-                    msg + ': probably this video is not available from your location due to geo restriction.',\n-                    countries=['CN'])\n-\n+                    'No media links found: probably due to geo restriction.', countries=['CN'])\n         return formats\n \n-    @classmethod\n-    def convert_milliseconds(cls, ms):\n-        return int(round(ms / 1000.0))\n-\n     def query_api(self, endpoint, video_id, note):\n-        req = Request('%s%s' % (self._API_BASE, endpoint))\n-        req.headers['Referer'] = self._API_BASE\n-        return self._download_json(req, video_id, note)\n+        result = self._download_json(\n+            f'{self._API_BASE}{endpoint}', video_id, note, headers={'Referer': self._API_BASE})\n+        code = traverse_obj(result, ('code', {int}))\n+        message = traverse_obj(result, ('message', {str})) or ''\n+        if code == -462:\n+            self.raise_login_required(f'Login required to download: {message}')\n+        elif code != 200:\n+            raise ExtractorError(f'Failed to get meta info: {code} {message}')\n+        return result\n+\n+    def _get_entries(self, songs_data, entry_keys=None, id_key='id', name_key='name'):\n+        for song in traverse_obj(songs_data, (\n+                *variadic(entry_keys, (str, bytes, dict, set)),\n+                lambda _, v: int_or_none(v[id_key]) is not None)):\n+            song_id = str(song[id_key])\n+            yield self.url_result(\n+                f'http://music.163.com/#/song?id={song_id}', NetEaseMusicIE,\n+                song_id, traverse_obj(song, (name_key, {str})))\n \n \n class NetEaseMusicIE(NetEaseMusicBaseIE):\n@@ -156,16 +130,21 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n     IE_DESC = '\u7f51\u6613\u4e91\u97f3\u4e50'\n     _VALID_URL = r'https?://(y\\.)?music\\.163\\.com/(?:[#m]/)?song\\?.*?\\bid=(?P<id>[0-9]+)'\n     _TESTS = [{\n-        'url': 'http://music.163.com/#/song?id=32102397',\n-        'md5': '3e909614ce09b1ccef4a3eb205441190',\n+        'url': 'https://music.163.com/#/song?id=548648087',\n         'info_dict': {\n-            'id': '32102397',\n+            'id': '548648087',\n             'ext': 'mp3',\n-            'title': 'Bad Blood',\n-            'creator': 'Taylor Swift / Kendrick Lamar',\n-            'upload_date': '20150516',\n-            'timestamp': 1431792000,\n-            'description': 'md5:25fc5f27e47aad975aa6d36382c7833c',\n+            'title': '\u6212\u70df (Live)',\n+            'creator': '\u674e\u8363\u6d69 / \u6731\u6b63\u5ef7 / \u9648\u7acb\u519c / \u5c24\u957f\u9756 / ONER\u7075\u8d85 / ONER\u6728\u5b50\u6d0b / \u6768\u975e\u540c / \u9646\u5b9a\u660a',\n+            'timestamp': 1522944000,\n+            'upload_date': '20180405',\n+            'description': 'md5:3650af9ee22c87e8637cb2dde22a765c',\n+            'subtitles': {'lyrics': [{'ext': 'lrc'}]},\n+            \"duration\": 256,\n+            'thumbnail': r're:^http.*\\.jpg',\n+            'album': '\u5076\u50cf\u7ec3\u4e60\u751f \u8868\u6f14\u66f2\u76ee\u5408\u96c6',\n+            'average_rating': int,\n+            'album_artist': '\u5076\u50cf\u7ec3\u4e60\u751f',\n         },\n     }, {\n         'note': 'No lyrics.',\n@@ -176,21 +155,12 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n             'title': 'Opus 28',\n             'creator': 'Dustin O\\'Halloran',\n             'upload_date': '20080211',\n-            'description': 'md5:f12945b0f6e0365e3b73c5032e1b0ff4',\n             'timestamp': 1202745600,\n-        },\n-    }, {\n-        'note': 'Has translated name.',\n-        'url': 'http://music.163.com/#/song?id=22735043',\n-        'info_dict': {\n-            'id': '22735043',\n-            'ext': 'mp3',\n-            'title': '\uc18c\uc6d0\uc744 \ub9d0\ud574\ubd10 (Genie)',\n-            'creator': '\u5c11\u5973\u65f6\u4ee3',\n-            'description': 'md5:79d99cc560e4ca97e0c4d86800ee4184',\n-            'upload_date': '20100127',\n-            'timestamp': 1264608000,\n-            'alt_title': '\u8bf4\u51fa\u613f\u671b\u5427(Genie)',\n+            'duration': 263,\n+            'thumbnail': r're:^http.*\\.jpg',\n+            'album': 'Piano Solos Vol. 2',\n+            'album_artist': 'Dustin O\\'Halloran',\n+            'average_rating': int,\n         },\n     }, {\n         'url': 'https://y.music.163.com/m/song?app_version=8.8.45&id=95670&uct2=sKnvS4+0YStsWkqsPhFijw%3D%3D&dlt=0846',\n@@ -203,59 +173,111 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n             'upload_date': '19911130',\n             'timestamp': 691516800,\n             'description': 'md5:1ba2f911a2b0aa398479f595224f2141',\n+            'subtitles': {'lyrics': [{'ext': 'lrc'}]},\n+            'duration': 268,\n+            'alt_title': '\u4f34\u5531:\u73b0\u4ee3\u4eba\u4e50\u961f \u5408\u5531:\u603b\u653f\u6b4c\u821e\u56e2',\n+            'thumbnail': r're:^http.*\\.jpg',\n+            'average_rating': int,\n+            'album': '\u7ea2\u8272\u6447\u6eda',\n+            'album_artist': '\u4faf\u7267\u4eba',\n+        },\n+    }, {\n+        'url': 'http://music.163.com/#/song?id=32102397',\n+        'md5': '3e909614ce09b1ccef4a3eb205441190',\n+        'info_dict': {\n+            'id': '32102397',\n+            'ext': 'mp3',\n+            'title': 'Bad Blood',\n+            'creator': 'Taylor Swift / Kendrick Lamar',\n+            'upload_date': '20150516',\n+            'timestamp': 1431792000,\n+            'description': 'md5:21535156efb73d6d1c355f95616e285a',\n+            'subtitles': {'lyrics': [{'ext': 'lrc'}]},\n+            'duration': 199,\n+            'thumbnail': r're:^http.*\\.jpg',\n+            'album': 'Bad Blood',\n+            'average_rating': int,\n+            'album_artist': 'Taylor Swift',\n         },\n+        'skip': 'Blocked outside Mainland China',\n+    }, {\n+        'note': 'Has translated name.',\n+        'url': 'http://music.163.com/#/song?id=22735043',\n+        'info_dict': {\n+            'id': '22735043',\n+            'ext': 'mp3',\n+            'title': '\uc18c\uc6d0\uc744 \ub9d0\ud574\ubd10 (Genie)',\n+            'creator': '\u5c11\u5973\u65f6\u4ee3',\n+            'upload_date': '20100127',\n+            'timestamp': 1264608000,\n+            'description': 'md5:03d1ffebec3139aa4bafe302369269c5',\n+            'subtitles': {'lyrics': [{'ext': 'lrc'}]},\n+            'duration': 229,\n+            'alt_title': '\u8bf4\u51fa\u613f\u671b\u5427(Genie)',\n+            'thumbnail': r're:^http.*\\.jpg',\n+            'average_rating': int,\n+            'album': 'Oh!',\n+            'album_artist': '\u5c11\u5973\u65f6\u4ee3',\n+        },\n+        'skip': 'Blocked outside Mainland China',\n     }]\n \n     def _process_lyrics(self, lyrics_info):\n-        original = lyrics_info.get('lrc', {}).get('lyric')\n-        translated = lyrics_info.get('tlyric', {}).get('lyric')\n+        original = traverse_obj(lyrics_info, ('lrc', 'lyric', {str}))\n+        translated = traverse_obj(lyrics_info, ('tlyric', 'lyric', {str}))\n+\n+        if not original or original == '[99:00.00]\u7eaf\u97f3\u4e50\uff0c\u8bf7\u6b23\u8d4f\\n':\n+            return None\n \n         if not translated:\n-            return original\n+            return {\n+                'lyrics': [{'data': original, 'ext': 'lrc'}],\n+            }\n \n         lyrics_expr = r'(\\[[0-9]{2}:[0-9]{2}\\.[0-9]{2,}\\])([^\\n]+)'\n         original_ts_texts = re.findall(lyrics_expr, original)\n-        translation_ts_dict = dict(\n-            (time_stamp, text) for time_stamp, text in re.findall(lyrics_expr, translated)\n-        )\n-        lyrics = '\\n'.join([\n-            '%s%s / %s' % (time_stamp, text, translation_ts_dict.get(time_stamp, ''))\n-            for time_stamp, text in original_ts_texts\n-        ])\n-        return lyrics\n+        translation_ts_dict = dict(re.findall(lyrics_expr, translated))\n+\n+        merged = '\\n'.join(\n+            join_nonempty(f'{timestamp}{text}', translation_ts_dict.get(timestamp, ''), delim=' / ')\n+            for timestamp, text in original_ts_texts)\n+\n+        return {\n+            'lyrics_merged': [{'data': merged, 'ext': 'lrc'}],\n+            'lyrics': [{'data': original, 'ext': 'lrc'}],\n+            'lyrics_translated': [{'data': translated, 'ext': 'lrc'}],\n+        }\n \n     def _real_extract(self, url):\n         song_id = self._match_id(url)\n \n-        params = {\n-            'id': song_id,\n-            'ids': '[%s]' % song_id\n-        }\n         info = self.query_api(\n-            'song/detail?' + compat_urllib_parse_urlencode(params),\n-            song_id, 'Downloading song info')['songs'][0]\n+            f'song/detail?id={song_id}&ids=%5B{song_id}%5D', song_id, 'Downloading song info')['songs'][0]\n \n         formats = self.extract_formats(info)\n \n-        lyrics_info = self.query_api(\n-            'song/lyric?id=%s&lv=-1&tv=-1' % song_id,\n-            song_id, 'Downloading lyrics data')\n-        lyrics = self._process_lyrics(lyrics_info)\n-\n-        alt_title = None\n-        if info.get('transNames'):\n-            alt_title = '/'.join(info.get('transNames'))\n+        lyrics = self._process_lyrics(self.query_api(\n+            f'song/lyric?id={song_id}&lv=-1&tv=-1', song_id, 'Downloading lyrics data'))\n+        lyric_data = {\n+            'description': traverse_obj(lyrics, (('lyrics_merged', 'lyrics'), 0, 'data'), get_all=False),\n+            'subtitles': lyrics,\n+        } if lyrics else {}\n \n         return {\n             'id': song_id,\n-            'title': info['name'],\n-            'alt_title': alt_title,\n-            'creator': ' / '.join([artist['name'] for artist in info.get('artists', [])]),\n-            'timestamp': self.convert_milliseconds(info.get('album', {}).get('publishTime')),\n-            'thumbnail': info.get('album', {}).get('picUrl'),\n-            'duration': self.convert_milliseconds(info.get('duration', 0)),\n-            'description': lyrics,\n             'formats': formats,\n+            'alt_title': '/'.join(traverse_obj(info, (('transNames', 'alias'), ...))) or None,\n+            'creator': ' / '.join(traverse_obj(info, ('artists', ..., 'name'))) or None,\n+            'album_artist': ' / '.join(traverse_obj(info, ('album', 'artists', ..., 'name'))) or None,\n+            **lyric_data,\n+            **traverse_obj(info, {\n+                'title': ('name', {str}),\n+                'timestamp': ('album', 'publishTime', {self.kilo_or_none}),\n+                'thumbnail': ('album', 'picUrl', {url_or_none}),\n+                'duration': ('duration', {self.kilo_or_none}),\n+                'album': ('album', 'name', {str}),\n+                'average_rating': ('score', {int_or_none}),\n+            }),\n         }\n \n \n@@ -263,31 +285,44 @@ class NetEaseMusicAlbumIE(NetEaseMusicBaseIE):\n     IE_NAME = 'netease:album'\n     IE_DESC = '\u7f51\u6613\u4e91\u97f3\u4e50 - \u4e13\u8f91'\n     _VALID_URL = r'https?://music\\.163\\.com/(#/)?album\\?id=(?P<id>[0-9]+)'\n-    _TEST = {\n+    _TESTS = [{\n+        'url': 'https://music.163.com/#/album?id=133153666',\n+        'info_dict': {\n+            'id': '133153666',\n+            'title': '\u6843\u51e0\u7684\u7ffb\u5531',\n+            'upload_date': '20210913',\n+            'description': '\u6843\u51e02021\u5e74\u7ffb\u5531\u5408\u96c6',\n+            'thumbnail': r're:^http.*\\.jpg',\n+        },\n+        'playlist_mincount': 13,\n+    }, {\n         'url': 'http://music.163.com/#/album?id=220780',\n         'info_dict': {\n             'id': '220780',\n-            'title': 'B\\'day',\n+            'title': 'B\\'Day',\n+            'upload_date': '20060904',\n+            'description': 'md5:71a74e1d8f392d88cf1bbe48879ad0b0',\n+            'thumbnail': r're:^http.*\\.jpg',\n         },\n         'playlist_count': 23,\n-        'skip': 'Blocked outside Mainland China',\n-    }\n+    }]\n \n     def _real_extract(self, url):\n         album_id = self._match_id(url)\n-\n-        info = self.query_api(\n-            'album/%s?id=%s' % (album_id, album_id),\n-            album_id, 'Downloading album data')['album']\n-\n-        name = info['name']\n-        desc = info.get('description')\n-        entries = [\n-            self.url_result('http://music.163.com/#/song?id=%s' % song['id'],\n-                            'NetEaseMusic', song['id'])\n-            for song in info['songs']\n-        ]\n-        return self.playlist_result(entries, album_id, name, desc)\n+        webpage = self._download_webpage(f'https://music.163.com/album?id={album_id}', album_id)\n+\n+        songs = self._search_json(\n+            r'<textarea[^>]+\\bid=\"song-list-pre-data\"[^>]*>', webpage, 'metainfo', album_id,\n+            end_pattern=r'</textarea>', contains_pattern=r'\\[(?s:.+)\\]')\n+        metainfo = {\n+            'title': self._og_search_property('title', webpage, 'title', fatal=False),\n+            'description': self._html_search_regex(\n+                (rf'<div[^>]+\\bid=\"album-desc-{suffix}\"[^>]*>(.*?)</div>' for suffix in ('more', 'dot')),\n+                webpage, 'description', flags=re.S, fatal=False),\n+            'thumbnail': self._og_search_property('image', webpage, 'thumbnail', fatal=False),\n+            'upload_date': unified_strdate(self._html_search_meta('music:release_date', webpage, 'date', fatal=False)),\n+        }\n+        return self.playlist_result(self._get_entries(songs), album_id, **metainfo)\n \n \n class NetEaseMusicSingerIE(NetEaseMusicBaseIE):\n@@ -299,10 +334,9 @@ class NetEaseMusicSingerIE(NetEaseMusicBaseIE):\n         'url': 'http://music.163.com/#/artist?id=10559',\n         'info_dict': {\n             'id': '10559',\n-            'title': '\u5f20\u60e0\u59b9 - aMEI;\u963f\u5bc6\u7279',\n+            'title': '\u5f20\u60e0\u59b9 - aMEI;\u963f\u59b9;\u963f\u5bc6\u7279',\n         },\n         'playlist_count': 50,\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'Singer has translated name.',\n         'url': 'http://music.163.com/#/artist?id=124098',\n@@ -311,28 +345,28 @@ class NetEaseMusicSingerIE(NetEaseMusicBaseIE):\n             'title': '\u674e\u6607\u57fa - \uc774\uc2b9\uae30',\n         },\n         'playlist_count': 50,\n-        'skip': 'Blocked outside Mainland China',\n+    }, {\n+        'note': 'Singer with both translated and alias',\n+        'url': 'https://music.163.com/#/artist?id=159692',\n+        'info_dict': {\n+            'id': '159692',\n+            'title': '\u521d\u97f3\u30df\u30af - \u521d\u97f3\u672a\u6765;Hatsune Miku',\n+        },\n+        'playlist_count': 50,\n     }]\n \n     def _real_extract(self, url):\n         singer_id = self._match_id(url)\n \n         info = self.query_api(\n-            'artist/%s?id=%s' % (singer_id, singer_id),\n-            singer_id, 'Downloading singer data')\n-\n-        name = info['artist']['name']\n-        if info['artist']['trans']:\n-            name = '%s - %s' % (name, info['artist']['trans'])\n-        if info['artist']['alias']:\n-            name = '%s - %s' % (name, ';'.join(info['artist']['alias']))\n-\n-        entries = [\n-            self.url_result('http://music.163.com/#/song?id=%s' % song['id'],\n-                            'NetEaseMusic', song['id'])\n-            for song in info['hotSongs']\n-        ]\n-        return self.playlist_result(entries, singer_id, name)\n+            f'artist/{singer_id}?id={singer_id}', singer_id, note='Downloading singer data')\n+\n+        name = join_nonempty(\n+            traverse_obj(info, ('artist', 'name', {str})),\n+            join_nonempty(*traverse_obj(info, ('artist', ('trans', ('alias', ...)), {str})), delim=';'),\n+            delim=' - ')\n+\n+        return self.playlist_result(self._get_entries(info, 'hotSongs'), singer_id, name)\n \n \n class NetEaseMusicListIE(NetEaseMusicBaseIE):\n@@ -344,10 +378,28 @@ class NetEaseMusicListIE(NetEaseMusicBaseIE):\n         'info_dict': {\n             'id': '79177352',\n             'title': 'Billboard 2007 Top 100',\n-            'description': 'md5:12fd0819cab2965b9583ace0f8b7b022'\n+            'description': 'md5:12fd0819cab2965b9583ace0f8b7b022',\n+            'tags': ['\u6b27\u7f8e'],\n+            'uploader': '\u6d51\u7136\u7834\u706d',\n+            'uploader_id': '67549805',\n+            'timestamp': int,\n+            'upload_date': r're:\\d{8}',\n         },\n-        'playlist_count': 99,\n-        'skip': 'Blocked outside Mainland China',\n+        'playlist_mincount': 95,\n+    }, {\n+        'note': 'Toplist/Charts sample',\n+        'url': 'https://music.163.com/#/discover/toplist?id=60198',\n+        'info_dict': {\n+            'id': '60198',\n+            'title': 're:\u7f8e\u56fdBillboard\u699c [0-9]{4}-[0-9]{2}-[0-9]{2}',\n+            'description': '\u7f8e\u56fdBillboard\u6392\u884c\u699c',\n+            'tags': ['\u6d41\u884c', '\u6b27\u7f8e', '\u699c\u5355'],\n+            'uploader': 'Billboard\u516c\u544a\u724c',\n+            'uploader_id': '48171',\n+            'timestamp': int,\n+            'upload_date': r're:\\d{8}',\n+        },\n+        'playlist_count': 100,\n     }, {\n         'note': 'Toplist/Charts sample',\n         'url': 'http://music.163.com/#/discover/toplist?id=3733003',\n@@ -363,64 +415,86 @@ class NetEaseMusicListIE(NetEaseMusicBaseIE):\n     def _real_extract(self, url):\n         list_id = self._match_id(url)\n \n-        info = self.query_api(\n-            'playlist/detail?id=%s&lv=-1&tv=-1' % list_id,\n-            list_id, 'Downloading playlist data')['result']\n-\n-        name = info['name']\n-        desc = info.get('description')\n+        info = self._download_eapi_json(\n+            '/v3/playlist/detail', list_id,\n+            {'id': list_id, 't': '-1', 'n': '500', 's': '0'},\n+            note=\"Downloading playlist info\")\n \n-        if info.get('specialType') == 10:  # is a chart/toplist\n-            datestamp = datetime.fromtimestamp(\n-                self.convert_milliseconds(info['updateTime'])).strftime('%Y-%m-%d')\n-            name = '%s %s' % (name, datestamp)\n+        metainfo = traverse_obj(info, ('playlist', {\n+            'title': ('name', {str}),\n+            'description': ('description', {str}),\n+            'tags': ('tags', ..., {str}),\n+            'uploader': ('creator', 'nickname', {str}),\n+            'uploader_id': ('creator', 'userId', {str_or_none}),\n+            'timestamp': ('updateTime', {self.kilo_or_none}),\n+        }))\n+        if traverse_obj(info, ('playlist', 'specialType')) == 10:\n+            metainfo['title'] = f'{metainfo.get(\"title\")} {strftime_or_none(metainfo.get(\"timestamp\"), \"%Y-%m-%d\")}'\n \n-        entries = [\n-            self.url_result('http://music.163.com/#/song?id=%s' % song['id'],\n-                            'NetEaseMusic', song['id'])\n-            for song in info['tracks']\n-        ]\n-        return self.playlist_result(entries, list_id, name, desc)\n+        return self.playlist_result(self._get_entries(info, ('playlist', 'tracks')), list_id, **metainfo)\n \n \n class NetEaseMusicMvIE(NetEaseMusicBaseIE):\n     IE_NAME = 'netease:mv'\n     IE_DESC = '\u7f51\u6613\u4e91\u97f3\u4e50 - MV'\n     _VALID_URL = r'https?://music\\.163\\.com/(#/)?mv\\?id=(?P<id>[0-9]+)'\n-    _TEST = {\n+    _TESTS = [{\n+        'url': 'https://music.163.com/#/mv?id=10958064',\n+        'info_dict': {\n+            'id': '10958064',\n+            'ext': 'mp4',\n+            'title': '\u4ea4\u6362\u4f59\u751f',\n+            'description': 'md5:e845872cff28820642a2b02eda428fea',\n+            'creator': '\u6797\u4fca\u6770',\n+            'upload_date': '20200916',\n+            'thumbnail': r're:http.*\\.jpg',\n+            'duration': 364,\n+            'view_count': int,\n+            'like_count': int,\n+            'comment_count': int,\n+        },\n+    }, {\n         'url': 'http://music.163.com/#/mv?id=415350',\n         'info_dict': {\n             'id': '415350',\n             'ext': 'mp4',\n             'title': '\uc774\ub7f4\uac70\uba74 \uadf8\ub7ec\uc9c0\ub9d0\uc9c0',\n             'description': '\u767d\u96c5\u8a00\u81ea\u4f5c\u66f2\u5531\u751c\u871c\u7231\u60c5',\n-            'creator': '\u767d\u96c5\u8a00',\n+            'creator': '\u767d\u5a25\u5a1f',\n             'upload_date': '20150520',\n+            'thumbnail': r're:http.*\\.jpg',\n+            'duration': 216,\n+            'view_count': int,\n+            'like_count': int,\n+            'comment_count': int,\n         },\n-        'skip': 'Blocked outside Mainland China',\n-    }\n+    }]\n \n     def _real_extract(self, url):\n         mv_id = self._match_id(url)\n \n         info = self.query_api(\n-            'mv/detail?id=%s&type=mp4' % mv_id,\n-            mv_id, 'Downloading mv info')['data']\n+            f'mv/detail?id={mv_id}&type=mp4', mv_id, 'Downloading mv info')['data']\n \n         formats = [\n-            {'url': mv_url, 'ext': 'mp4', 'format_id': '%sp' % brs, 'height': int(brs)}\n+            {'url': mv_url, 'ext': 'mp4', 'format_id': f'{brs}p', 'height': int_or_none(brs)}\n             for brs, mv_url in info['brs'].items()\n         ]\n \n         return {\n             'id': mv_id,\n-            'title': info['name'],\n-            'description': info.get('desc') or info.get('briefDesc'),\n-            'creator': info['artistName'],\n-            'upload_date': info['publishTime'].replace('-', ''),\n             'formats': formats,\n-            'thumbnail': info.get('cover'),\n-            'duration': self.convert_milliseconds(info.get('duration', 0)),\n+            **traverse_obj(info, {\n+                'title': ('name', {str}),\n+                'description': (('desc', 'briefDesc'), {str}, {lambda x: x or None}),\n+                'creator': ('artistName', {str}),\n+                'upload_date': ('publishTime', {unified_strdate}),\n+                'thumbnail': ('cover', {url_or_none}),\n+                'duration': ('duration', {self.kilo_or_none}),\n+                'view_count': ('playCount', {int_or_none}),\n+                'like_count': ('likeCount', {int_or_none}),\n+                'comment_count': ('commentCount', {int_or_none}),\n+            }, get_all=False),\n         }\n \n \n@@ -431,75 +505,74 @@ class NetEaseMusicProgramIE(NetEaseMusicBaseIE):\n     _TESTS = [{\n         'url': 'http://music.163.com/#/program?id=10109055',\n         'info_dict': {\n-            'id': '10109055',\n+            'id': '32593346',\n             'ext': 'mp3',\n             'title': '\u4e0d\u4e39\u8db3\u7403\u80cc\u540e\u7684\u6545\u4e8b',\n             'description': '\u559c\u9a6c\u62c9\u96c5\u4eba\u7684\u8db3\u7403\u68a6 ...',\n             'creator': '\u5927\u8bdd\u897f\u85cf',\n-            'timestamp': 1434179342,\n+            'timestamp': 1434179287,\n             'upload_date': '20150613',\n+            'thumbnail': r're:http.*\\.jpg',\n             'duration': 900,\n         },\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'This program has accompanying songs.',\n         'url': 'http://music.163.com/#/program?id=10141022',\n         'info_dict': {\n             'id': '10141022',\n-            'title': '25\u5c81\uff0c\u4f60\u662f\u81ea\u5728\u5982\u98ce\u7684\u5c11\u5e74<27\u00b0C>',\n+            'title': '\u6eda\u6eda\u7535\u53f0\u7684\u6709\u58f0\u8282\u76ee',\n             'description': 'md5:8d594db46cc3e6509107ede70a4aaa3b',\n+            'creator': '\u6eda\u6eda\u7535\u53f0ORZ',\n+            'timestamp': 1434450733,\n+            'upload_date': '20150616',\n+            'thumbnail': r're:http.*\\.jpg',\n         },\n         'playlist_count': 4,\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'This program has accompanying songs.',\n         'url': 'http://music.163.com/#/program?id=10141022',\n         'info_dict': {\n-            'id': '10141022',\n+            'id': '32647209',\n             'ext': 'mp3',\n-            'title': '25\u5c81\uff0c\u4f60\u662f\u81ea\u5728\u5982\u98ce\u7684\u5c11\u5e74<27\u00b0C>',\n+            'title': '\u6eda\u6eda\u7535\u53f0\u7684\u6709\u58f0\u8282\u76ee',\n             'description': 'md5:8d594db46cc3e6509107ede70a4aaa3b',\n-            'timestamp': 1434450841,\n+            'creator': '\u6eda\u6eda\u7535\u53f0ORZ',\n+            'timestamp': 1434450733,\n             'upload_date': '20150616',\n+            'thumbnail': r're:http.*\\.jpg',\n+            'duration': 1104,\n         },\n         'params': {\n             'noplaylist': True\n         },\n-        'skip': 'Blocked outside Mainland China',\n     }]\n \n     def _real_extract(self, url):\n         program_id = self._match_id(url)\n \n         info = self.query_api(\n-            'dj/program/detail?id=%s' % program_id,\n-            program_id, 'Downloading program info')['program']\n+            f'dj/program/detail?id={program_id}', program_id, note='Downloading program info')['program']\n \n-        name = info['name']\n-        description = info['description']\n+        metainfo = traverse_obj(info, {\n+            'title': ('name', {str}),\n+            'description': ('description', {str}),\n+            'creator': ('dj', 'brand', {str}),\n+            'thumbnail': ('coverUrl', {url_or_none}),\n+            'timestamp': ('createTime', {self.kilo_or_none}),\n+        })\n \n         if not self._yes_playlist(info['songs'] and program_id, info['mainSong']['id']):\n             formats = self.extract_formats(info['mainSong'])\n \n             return {\n-                'id': info['mainSong']['id'],\n-                'title': name,\n-                'description': description,\n-                'creator': info['dj']['brand'],\n-                'timestamp': self.convert_milliseconds(info['createTime']),\n-                'thumbnail': info['coverUrl'],\n-                'duration': self.convert_milliseconds(info.get('duration', 0)),\n+                'id': str(info['mainSong']['id']),\n                 'formats': formats,\n+                'duration': traverse_obj(info, ('mainSong', 'duration', {self.kilo_or_none})),\n+                **metainfo,\n             }\n \n-        song_ids = [info['mainSong']['id']]\n-        song_ids.extend([song['id'] for song in info['songs']])\n-        entries = [\n-            self.url_result('http://music.163.com/#/song?id=%s' % song_id,\n-                            'NetEaseMusic', song_id)\n-            for song_id in song_ids\n-        ]\n-        return self.playlist_result(entries, program_id, name, description)\n+        songs = traverse_obj(info, (('mainSong', ('songs', ...)),))\n+        return self.playlist_result(self._get_entries(songs), program_id, **metainfo)\n \n \n class NetEaseMusicDjRadioIE(NetEaseMusicBaseIE):\n@@ -511,38 +584,32 @@ class NetEaseMusicDjRadioIE(NetEaseMusicBaseIE):\n         'info_dict': {\n             'id': '42',\n             'title': '\u58f0\u97f3\u8513\u5ef6',\n-            'description': 'md5:766220985cbd16fdd552f64c578a6b15'\n+            'description': 'md5:c7381ebd7989f9f367668a5aee7d5f08'\n         },\n         'playlist_mincount': 40,\n-        'skip': 'Blocked outside Mainland China',\n     }\n     _PAGE_SIZE = 1000\n \n     def _real_extract(self, url):\n         dj_id = self._match_id(url)\n \n-        name = None\n-        desc = None\n+        metainfo = {}\n         entries = []\n         for offset in itertools.count(start=0, step=self._PAGE_SIZE):\n             info = self.query_api(\n-                'dj/program/byradio?asc=false&limit=%d&radioId=%s&offset=%d'\n-                % (self._PAGE_SIZE, dj_id, offset),\n-                dj_id, 'Downloading dj programs - %d' % offset)\n-\n-            entries.extend([\n-                self.url_result(\n-                    'http://music.163.com/#/program?id=%s' % program['id'],\n-                    'NetEaseMusicProgram', program['id'])\n-                for program in info['programs']\n-            ])\n-\n-            if name is None:\n-                radio = info['programs'][0]['radio']\n-                name = radio['name']\n-                desc = radio['desc']\n+                f'dj/program/byradio?asc=false&limit={self._PAGE_SIZE}&radioId={dj_id}&offset={offset}',\n+                dj_id, note=f'Downloading dj programs - {offset}')\n+\n+            entries.extend(self.url_result(\n+                f'http://music.163.com/#/program?id={program[\"id\"]}', NetEaseMusicProgramIE,\n+                program['id'], program.get('name')) for program in info['programs'])\n+            if not metainfo:\n+                metainfo = traverse_obj(info, ('programs', 0, 'radio', {\n+                    'title': ('name', {str}),\n+                    'description': ('desc', {str}),\n+                }))\n \n             if not info['more']:\n                 break\n \n-        return self.playlist_result(entries, dj_id, name, desc)\n+        return self.playlist_result(entries, dj_id, **metainfo)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/newstube.py",
            "diff": "diff --git a/yt_dlp/extractor/newstube.py b/yt_dlp/extractor/newstube.py\ndeleted file mode 100644\nindex 820eb4ba..00000000\n--- a/yt_dlp/extractor/newstube.py\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-import base64\n-import hashlib\n-\n-from .common import InfoExtractor\n-from ..aes import aes_cbc_decrypt_bytes, unpad_pkcs7\n-from ..utils import (\n-    int_or_none,\n-    parse_codecs,\n-    parse_duration,\n-)\n-\n-\n-class NewstubeIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?newstube\\.ru/media/(?P<id>.+)'\n-    _TEST = {\n-        'url': 'http://www.newstube.ru/media/telekanal-cnn-peremestil-gorod-slavyansk-v-krym',\n-        'md5': '9d10320ad473444352f72f746ccb8b8c',\n-        'info_dict': {\n-            'id': '728e0ef2-e187-4012-bac0-5a081fdcb1f6',\n-            'ext': 'mp4',\n-            'title': '\u0422\u0435\u043b\u0435\u043a\u0430\u043d\u0430\u043b CNN \u043f\u0435\u0440\u0435\u043c\u0435\u0441\u0442\u0438\u043b \u0433\u043e\u0440\u043e\u0434 \u0421\u043b\u0430\u0432\u044f\u043d\u0441\u043a \u0432 \u041a\u0440\u044b\u043c',\n-            'description': 'md5:419a8c9f03442bc0b0a794d689360335',\n-            'duration': 31.05,\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        page = self._download_webpage(url, video_id)\n-        title = self._html_search_meta(['og:title', 'twitter:title'], page, fatal=True)\n-\n-        video_guid = self._html_search_regex(\n-            r'<meta\\s+property=\"og:video(?::(?:(?:secure_)?url|iframe))?\"\\s+content=\"https?://(?:www\\.)?newstube\\.ru/embed/(?P<guid>[\\da-f]{8}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{12})',\n-            page, 'video GUID')\n-\n-        enc_data = base64.b64decode(self._download_webpage(\n-            'https://www.newstube.ru/embed/api/player/getsources2',\n-            video_guid, query={\n-                'guid': video_guid,\n-                'ff': 3,\n-            }))\n-        key = hashlib.pbkdf2_hmac(\n-            'sha1', video_guid.replace('-', '').encode(), enc_data[:16], 1)[:16]\n-        dec_data = unpad_pkcs7(aes_cbc_decrypt_bytes(enc_data[32:], key, enc_data[16:32]))\n-        sources = self._parse_json(dec_data, video_guid)\n-\n-        formats = []\n-        for source in sources:\n-            source_url = source.get('Src')\n-            if not source_url:\n-                continue\n-            height = int_or_none(source.get('Height'))\n-            f = {\n-                'format_id': 'http' + ('-%dp' % height if height else ''),\n-                'url': source_url,\n-                'width': int_or_none(source.get('Width')),\n-                'height': height,\n-            }\n-            source_type = source.get('Type')\n-            if source_type:\n-                f.update(parse_codecs(self._search_regex(\n-                    r'codecs=\"([^\"]+)\"', source_type, 'codecs', fatal=False)))\n-            formats.append(f)\n-\n-        self._check_formats(formats, video_guid)\n-\n-        return {\n-            'id': video_guid,\n-            'title': title,\n-            'description': self._html_search_meta(['description', 'og:description'], page),\n-            'thumbnail': self._html_search_meta(['og:image:secure_url', 'og:image', 'twitter:image'], page),\n-            'duration': parse_duration(self._html_search_meta('duration', page)),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nfl.py",
            "diff": "diff --git a/yt_dlp/extractor/nfl.py b/yt_dlp/extractor/nfl.py\nindex cc3f4495..3f83cd20 100644\n--- a/yt_dlp/extractor/nfl.py\n+++ b/yt_dlp/extractor/nfl.py\n@@ -64,6 +64,85 @@ class NFLBaseIE(InfoExtractor):\n     _VIDEO_CONFIG_REGEX = r'<script[^>]+id=\"[^\"]*video-config-[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12}[^\"]*\"[^>]*>\\s*({.+});?\\s*</script>'\n     _ANVATO_PREFIX = 'anvato:GXvEgwyJeWem8KCYXfeoHWknwP48Mboj:'\n \n+    _CLIENT_DATA = {\n+        'clientKey': '4cFUW6DmwJpzT9L7LrG3qRAcABG5s04g',\n+        'clientSecret': 'CZuvCL49d9OwfGsR',\n+        'deviceId': str(uuid.uuid4()),\n+        'deviceInfo': base64.b64encode(json.dumps({\n+            'model': 'desktop',\n+            'version': 'Chrome',\n+            'osName': 'Windows',\n+            'osVersion': '10.0',\n+        }, separators=(',', ':')).encode()).decode(),\n+        'networkType': 'other',\n+        'nflClaimGroupsToAdd': [],\n+        'nflClaimGroupsToRemove': [],\n+    }\n+    _ACCOUNT_INFO = {}\n+    _API_KEY = None\n+\n+    _TOKEN = None\n+    _TOKEN_EXPIRY = 0\n+\n+    def _get_account_info(self, url, slug):\n+        if not self._API_KEY:\n+            webpage = self._download_webpage(url, slug, fatal=False) or ''\n+            self._API_KEY = self._search_regex(\n+                r'window\\.gigyaApiKey\\s*=\\s*[\"\\'](\\w+)[\"\\'];', webpage, 'API key',\n+                fatal=False) or '3_Qa8TkWpIB8ESCBT8tY2TukbVKgO5F6BJVc7N1oComdwFzI7H2L9NOWdm11i_BY9f'\n+\n+        cookies = self._get_cookies('https://auth-id.nfl.com/')\n+        login_token = traverse_obj(cookies, (\n+            (f'glt_{self._API_KEY}', lambda k, _: k.startswith('glt_')), {lambda x: x.value}), get_all=False)\n+        if not login_token:\n+            self.raise_login_required()\n+        if 'ucid' not in cookies:\n+            raise ExtractorError(\n+                'Required cookies for the auth-id.nfl.com domain were not found among passed cookies. '\n+                'If using --cookies, these cookies must be exported along with .nfl.com cookies, '\n+                'or else try using --cookies-from-browser instead', expected=True)\n+\n+        account = self._download_json(\n+            'https://auth-id.nfl.com/accounts.getAccountInfo', slug,\n+            note='Downloading account info', data=urlencode_postdata({\n+                'include': 'profile,data',\n+                'lang': 'en',\n+                'APIKey': self._API_KEY,\n+                'sdk': 'js_latest',\n+                'login_token': login_token,\n+                'authMode': 'cookie',\n+                'pageURL': url,\n+                'sdkBuild': traverse_obj(cookies, (\n+                    'gig_canary_ver', {lambda x: x.value.partition('-')[0]}), default='15170'),\n+                'format': 'json',\n+            }), headers={'Content-Type': 'application/x-www-form-urlencoded'})\n+\n+        self._ACCOUNT_INFO = traverse_obj(account, {\n+            'signatureTimestamp': 'signatureTimestamp',\n+            'uid': 'UID',\n+            'uidSignature': 'UIDSignature',\n+        })\n+\n+        if len(self._ACCOUNT_INFO) != 3:\n+            raise ExtractorError('Failed to retrieve account info with provided cookies', expected=True)\n+\n+    def _get_auth_token(self, url, slug):\n+        if self._TOKEN and self._TOKEN_EXPIRY > int(time.time() + 30):\n+            return\n+\n+        if not self._ACCOUNT_INFO:\n+            self._get_account_info(url, slug)\n+\n+        token = self._download_json(\n+            'https://api.nfl.com/identity/v3/token%s' % (\n+                '/refresh' if self._ACCOUNT_INFO.get('refreshToken') else ''),\n+            slug, headers={'Content-Type': 'application/json'}, note='Downloading access token',\n+            data=json.dumps({**self._CLIENT_DATA, **self._ACCOUNT_INFO}, separators=(',', ':')).encode())\n+\n+        self._TOKEN = token['accessToken']\n+        self._TOKEN_EXPIRY = token['expiresIn']\n+        self._ACCOUNT_INFO['refreshToken'] = token['refreshToken']\n+\n     def _parse_video_config(self, video_config, display_id):\n         video_config = self._parse_json(video_config, display_id)\n         item = video_config['playlist'][0]\n@@ -168,7 +247,7 @@ def _real_extract(self, url):\n \n class NFLPlusReplayIE(NFLBaseIE):\n     IE_NAME = 'nfl.com:plus:replay'\n-    _VALID_URL = r'https?://(?:www\\.)?nfl.com/plus/games/[\\w-]+/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?nfl\\.com/plus/games/(?P<slug>[\\w-]+)(?:/(?P<id>\\d+))?'\n     _TESTS = [{\n         'url': 'https://www.nfl.com/plus/games/giants-at-vikings-2022-post-1/1572108',\n         'info_dict': {\n@@ -185,23 +264,92 @@ class NFLPlusReplayIE(NFLBaseIE):\n             'thumbnail': r're:^https?://.*\\.jpg',\n         },\n         'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'note': 'Subscription required',\n+        'url': 'https://www.nfl.com/plus/games/giants-at-vikings-2022-post-1',\n+        'playlist_count': 4,\n+        'info_dict': {\n+            'id': 'giants-at-vikings-2022-post-1',\n+        },\n+    }, {\n+        'note': 'Subscription required',\n+        'url': 'https://www.nfl.com/plus/games/giants-at-patriots-2011-pre-4',\n+        'playlist_count': 2,\n+        'info_dict': {\n+            'id': 'giants-at-patriots-2011-pre-4',\n+        },\n+    }, {\n+        'note': 'Subscription required',\n+        'url': 'https://www.nfl.com/plus/games/giants-at-patriots-2011-pre-4',\n+        'info_dict': {\n+            'id': '950701',\n+            'ext': 'mp4',\n+            'title': 'Giants @ Patriots',\n+            'description': 'Giants at Patriots on September 01, 2011',\n+            'uploader': 'NFL',\n+            'upload_date': '20210724',\n+            'timestamp': 1627085874,\n+            'duration': 1532,\n+            'categories': ['Game Highlights'],\n+            'tags': ['play-by-play'],\n+            'thumbnail': r're:^https?://.*\\.jpg',\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+            'extractor_args': {'nflplusreplay': {'type': ['condensed_game']}},\n+        },\n     }]\n \n+    _REPLAY_TYPES = {\n+        'full_game': 'Full Game',\n+        'full_game_spanish': 'Full Game - Spanish',\n+        'condensed_game': 'Condensed Game',\n+        'all_22': 'All-22',\n+    }\n+\n     def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        return self.url_result(f'{self._ANVATO_PREFIX}{video_id}', AnvatoIE, video_id)\n+        slug, video_id = self._match_valid_url(url).group('slug', 'id')\n+        requested_types = self._configuration_arg('type', ['all'])\n+        if 'all' in requested_types:\n+            requested_types = list(self._REPLAY_TYPES.keys())\n+        requested_types = traverse_obj(self._REPLAY_TYPES, (None, requested_types))\n+\n+        if not video_id:\n+            self._get_auth_token(url, slug)\n+            headers = {'Authorization': f'Bearer {self._TOKEN}'}\n+            game_id = self._download_json(\n+                f'https://api.nfl.com/football/v2/games/externalId/slug/{slug}', slug,\n+                'Downloading game ID', query={'withExternalIds': 'true'}, headers=headers)['id']\n+            replays = self._download_json(\n+                'https://api.nfl.com/content/v1/videos/replays', slug, 'Downloading replays JSON',\n+                query={'gameId': game_id}, headers=headers)\n+            if len(requested_types) == 1:\n+                video_id = traverse_obj(replays, (\n+                    'items', lambda _, v: v['subType'] == requested_types[0], 'mcpPlaybackId'), get_all=False)\n+\n+        if video_id:\n+            return self.url_result(f'{self._ANVATO_PREFIX}{video_id}', AnvatoIE, video_id)\n+\n+        def entries():\n+            for replay in traverse_obj(\n+                replays, ('items', lambda _, v: v['mcpPlaybackId'] and v['subType'] in requested_types)\n+            ):\n+                video_id = replay['mcpPlaybackId']\n+                yield self.url_result(f'{self._ANVATO_PREFIX}{video_id}', AnvatoIE, video_id)\n+\n+        return self.playlist_result(entries(), slug)\n \n \n class NFLPlusEpisodeIE(NFLBaseIE):\n     IE_NAME = 'nfl.com:plus:episode'\n-    _VALID_URL = r'https?://(?:www\\.)?nfl.com/plus/episodes/(?P<id>[\\w-]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?nfl\\.com/plus/episodes/(?P<id>[\\w-]+)'\n     _TESTS = [{\n-        'note': 'premium content',\n+        'note': 'Subscription required',\n         'url': 'https://www.nfl.com/plus/episodes/kurt-s-qb-insider-conference-championships',\n         'info_dict': {\n             'id': '1576832',\n             'ext': 'mp4',\n-            'title': 'Kurt\\'s QB Insider: Conference Championships',\n+            'title': 'Conference Championships',\n             'description': 'md5:944f7fab56f7a37430bf8473f5473857',\n             'uploader': 'NFL',\n             'upload_date': '20230127',\n@@ -214,85 +362,9 @@ class NFLPlusEpisodeIE(NFLBaseIE):\n         'params': {'skip_download': 'm3u8'},\n     }]\n \n-    _CLIENT_DATA = {\n-        'clientKey': '4cFUW6DmwJpzT9L7LrG3qRAcABG5s04g',\n-        'clientSecret': 'CZuvCL49d9OwfGsR',\n-        'deviceId': str(uuid.uuid4()),\n-        'deviceInfo': base64.b64encode(json.dumps({\n-            'model': 'desktop',\n-            'version': 'Chrome',\n-            'osName': 'Windows',\n-            'osVersion': '10.0',\n-        }, separators=(',', ':')).encode()).decode(),\n-        'networkType': 'other',\n-        'nflClaimGroupsToAdd': [],\n-        'nflClaimGroupsToRemove': [],\n-    }\n-    _ACCOUNT_INFO = {}\n-    _API_KEY = None\n-\n-    _TOKEN = None\n-    _TOKEN_EXPIRY = 0\n-\n-    def _get_account_info(self, url, video_id):\n-        cookies = self._get_cookies('https://www.nfl.com/')\n-        login_token = traverse_obj(cookies, (\n-            (f'glt_{self._API_KEY}', f'gig_loginToken_{self._API_KEY}',\n-             lambda k, _: k.startswith('glt_') or k.startswith('gig_loginToken_')),\n-            {lambda x: x.value}), get_all=False)\n-        if not login_token:\n-            self.raise_login_required()\n-\n-        account = self._download_json(\n-            'https://auth-id.nfl.com/accounts.getAccountInfo', video_id,\n-            note='Downloading account info', data=urlencode_postdata({\n-                'include': 'profile,data',\n-                'lang': 'en',\n-                'APIKey': self._API_KEY,\n-                'sdk': 'js_latest',\n-                'login_token': login_token,\n-                'authMode': 'cookie',\n-                'pageURL': url,\n-                'sdkBuild': traverse_obj(cookies, (\n-                    'gig_canary_ver', {lambda x: x.value.partition('-')[0]}), default='13642'),\n-                'format': 'json',\n-            }), headers={'Content-Type': 'application/x-www-form-urlencoded'})\n-\n-        self._ACCOUNT_INFO = traverse_obj(account, {\n-            'signatureTimestamp': 'signatureTimestamp',\n-            'uid': 'UID',\n-            'uidSignature': 'UIDSignature',\n-        })\n-\n-        if len(self._ACCOUNT_INFO) != 3:\n-            raise ExtractorError('Failed to retrieve account info with provided cookies', expected=True)\n-\n-    def _get_auth_token(self, url, video_id):\n-        if not self._ACCOUNT_INFO:\n-            self._get_account_info(url, video_id)\n-\n-        token = self._download_json(\n-            'https://api.nfl.com/identity/v3/token%s' % (\n-                '/refresh' if self._ACCOUNT_INFO.get('refreshToken') else ''),\n-            video_id, headers={'Content-Type': 'application/json'}, note='Downloading access token',\n-            data=json.dumps({**self._CLIENT_DATA, **self._ACCOUNT_INFO}, separators=(',', ':')).encode())\n-\n-        self._TOKEN = token['accessToken']\n-        self._TOKEN_EXPIRY = token['expiresIn']\n-        self._ACCOUNT_INFO['refreshToken'] = token['refreshToken']\n-\n     def _real_extract(self, url):\n         slug = self._match_id(url)\n-\n-        if not self._API_KEY:\n-            webpage = self._download_webpage(url, slug, fatal=False) or ''\n-            self._API_KEY = self._search_regex(\n-                r'window\\.gigyaApiKey=[\"\\'](\\w+)[\"\\'];', webpage, 'API key',\n-                default='3_Qa8TkWpIB8ESCBT8tY2TukbVKgO5F6BJVc7N1oComdwFzI7H2L9NOWdm11i_BY9f')\n-\n-        if not self._TOKEN or self._TOKEN_EXPIRY <= int(time.time()):\n-            self._get_auth_token(url, slug)\n-\n+        self._get_auth_token(url, slug)\n         video_id = self._download_json(\n             f'https://api.nfl.com/content/v1/videos/episodes/{slug}', slug, headers={\n                 'Authorization': f'Bearer {self._TOKEN}',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nhk.py",
            "diff": "diff --git a/yt_dlp/extractor/nhk.py b/yt_dlp/extractor/nhk.py\nindex fbd6a18f..4b3d185a 100644\n--- a/yt_dlp/extractor/nhk.py\n+++ b/yt_dlp/extractor/nhk.py\n@@ -3,6 +3,8 @@\n from .common import InfoExtractor\n from ..utils import (\n     ExtractorError,\n+    clean_html,\n+    get_element_by_class,\n     int_or_none,\n     join_nonempty,\n     parse_duration,\n@@ -28,21 +30,71 @@ def _call_api(self, m_id, lang, is_video, is_episode, is_clip):\n                 m_id, lang, '/all' if is_video else ''),\n             m_id, query={'apikey': 'EJfK8jdS57GqlupFgAfAAwr573q01y6k'})['data']['episodes'] or []\n \n+    def _get_api_info(self, refresh=True):\n+        if not refresh:\n+            return self.cache.load('nhk', 'api_info')\n+\n+        self.cache.store('nhk', 'api_info', {})\n+        movie_player_js = self._download_webpage(\n+            'https://movie-a.nhk.or.jp/world/player/js/movie-player.js', None,\n+            note='Downloading stream API information')\n+        api_info = {\n+            'url': self._search_regex(\n+                r'prod:[^;]+\\bapiUrl:\\s*[\\'\"]([^\\'\"]+)[\\'\"]', movie_player_js, None, 'stream API url'),\n+            'token': self._search_regex(\n+                r'prod:[^;]+\\btoken:\\s*[\\'\"]([^\\'\"]+)[\\'\"]', movie_player_js, None, 'stream API token'),\n+        }\n+        self.cache.store('nhk', 'api_info', api_info)\n+        return api_info\n+\n+    def _extract_stream_info(self, vod_id):\n+        for refresh in (False, True):\n+            api_info = self._get_api_info(refresh)\n+            if not api_info:\n+                continue\n+\n+            api_url = api_info.pop('url')\n+            meta = traverse_obj(\n+                self._download_json(\n+                    api_url, vod_id, 'Downloading stream url info', fatal=False, query={\n+                        **api_info,\n+                        'type': 'json',\n+                        'optional_id': vod_id,\n+                        'active_flg': 1,\n+                    }), ('meta', 0))\n+            stream_url = traverse_obj(\n+                meta, ('movie_url', ('mb_auto', 'auto_sp', 'auto_pc'), {url_or_none}), get_all=False)\n+\n+            if stream_url:\n+                formats, subtitles = self._extract_m3u8_formats_and_subtitles(stream_url, vod_id)\n+                return {\n+                    **traverse_obj(meta, {\n+                        'duration': ('duration', {int_or_none}),\n+                        'timestamp': ('publication_date', {unified_timestamp}),\n+                        'release_timestamp': ('insert_date', {unified_timestamp}),\n+                        'modified_timestamp': ('update_date', {unified_timestamp}),\n+                    }),\n+                    'formats': formats,\n+                    'subtitles': subtitles,\n+                }\n+        raise ExtractorError('Unable to extract stream url')\n+\n     def _extract_episode_info(self, url, episode=None):\n         fetch_episode = episode is None\n-        lang, m_type, episode_id = NhkVodIE._match_valid_url(url).groups()\n-        if len(episode_id) == 7:\n+        lang, m_type, episode_id = NhkVodIE._match_valid_url(url).group('lang', 'type', 'id')\n+        is_video = m_type == 'video'\n+\n+        if is_video:\n             episode_id = episode_id[:4] + '-' + episode_id[4:]\n \n-        is_video = m_type == 'video'\n         if fetch_episode:\n             episode = self._call_api(\n                 episode_id, lang, is_video, True, episode_id[:4] == '9999')[0]\n-        title = episode.get('sub_title_clean') or episode['sub_title']\n \n         def get_clean_field(key):\n-            return episode.get(key + '_clean') or episode.get(key)\n+            return clean_html(episode.get(key + '_clean') or episode.get(key))\n \n+        title = get_clean_field('sub_title')\n         series = get_clean_field('title')\n \n         thumbnails = []\n@@ -57,22 +109,32 @@ def get_clean_field(key):\n                 'url': 'https://www3.nhk.or.jp' + img_path,\n             })\n \n+        episode_name = title\n+        if series and title:\n+            title = f'{series} - {title}'\n+        elif series and not title:\n+            title = series\n+            series = None\n+            episode_name = None\n+        else:  # title, no series\n+            episode_name = None\n+\n         info = {\n             'id': episode_id + '-' + lang,\n-            'title': '%s - %s' % (series, title) if series and title else title,\n+            'title': title,\n             'description': get_clean_field('description'),\n             'thumbnails': thumbnails,\n             'series': series,\n-            'episode': title,\n+            'episode': episode_name,\n         }\n+\n         if is_video:\n             vod_id = episode['vod_id']\n             info.update({\n-                '_type': 'url_transparent',\n-                'ie_key': 'Piksel',\n-                'url': 'https://movie-s.nhk.or.jp/v/refid/nhkworld/prefid/' + vod_id,\n+                **self._extract_stream_info(vod_id),\n                 'id': vod_id,\n             })\n+\n         else:\n             if fetch_episode:\n                 audio_path = episode['audio']['audio']\n@@ -93,47 +155,61 @@ def get_clean_field(key):\n \n class NhkVodIE(NhkBaseIE):\n     # the 7-character IDs can have alphabetic chars too: assume [a-z] rather than just [a-f], eg\n-    _VALID_URL = r'%s%s(?P<id>[0-9a-z]{7}|[^/]+?-\\d{8}-[0-9a-z]+)' % (NhkBaseIE._BASE_URL_REGEX, NhkBaseIE._TYPE_REGEX)\n+    _VALID_URL = [rf'{NhkBaseIE._BASE_URL_REGEX}/(?P<type>video)/(?P<id>[0-9a-z]+)',\n+                  rf'{NhkBaseIE._BASE_URL_REGEX}/(?P<type>audio)/(?P<id>[^/?#]+?-\\d{{8}}-[0-9a-z]+)']\n     # Content available only for a limited period of time. Visit\n     # https://www3.nhk.or.jp/nhkworld/en/ondemand/ for working samples.\n     _TESTS = [{\n-        'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/video/2061601/',\n+        'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/video/2049126/',\n         'info_dict': {\n-            'id': 'yd8322ch',\n+            'id': 'nw_vod_v_en_2049_126_20230413233000_01_1681398302',\n             'ext': 'mp4',\n-            'description': 'md5:109c8b05d67a62d0592f2b445d2cd898',\n-            'title': 'GRAND SUMO Highlights - [Recap] May Tournament Day 1 (Opening Day)',\n-            'upload_date': '20230514',\n-            'timestamp': 1684083791,\n-            'series': 'GRAND SUMO Highlights',\n-            'episode': '[Recap] May Tournament Day 1 (Opening Day)',\n-            'thumbnail': 'https://mz-edge.stream.co.jp/thumbs/aid/t1684084443/4028649.jpg?w=1920&h=1080',\n+            'title': 'Japan Railway Journal - The Tohoku Shinkansen: Full Speed Ahead',\n+            'description': 'md5:49f7c5b206e03868a2fdf0d0814b92f6',\n+            'thumbnail': 'md5:51bcef4a21936e7fea1ff4e06353f463',\n+            'episode': 'The Tohoku Shinkansen: Full Speed Ahead',\n+            'series': 'Japan Railway Journal',\n+            'modified_timestamp': 1694243656,\n+            'timestamp': 1681428600,\n+            'release_timestamp': 1693883728,\n+            'duration': 1679,\n+            'upload_date': '20230413',\n+            'modified_date': '20230909',\n+            'release_date': '20230905',\n+\n         },\n     }, {\n         # video clip\n         'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/video/9999011/',\n-        'md5': '7a90abcfe610ec22a6bfe15bd46b30ca',\n+        'md5': '153c3016dfd252ba09726588149cf0e7',\n         'info_dict': {\n-            'id': 'a95j5iza',\n+            'id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5',\n             'ext': 'mp4',\n-            'title': \"Dining with the Chef - Chef Saito's Family recipe: MENCHI-KATSU\",\n+            'title': 'Dining with the Chef - Chef Saito\\'s Family recipe: MENCHI-KATSU',\n             'description': 'md5:5aee4a9f9d81c26281862382103b0ea5',\n-            'timestamp': 1565965194,\n-            'upload_date': '20190816',\n-            'thumbnail': 'https://mz-edge.stream.co.jp/thumbs/aid/t1567086278/3715195.jpg?w=1920&h=1080',\n+            'thumbnail': 'md5:d6a4d9b6e9be90aaadda0bcce89631ed',\n             'series': 'Dining with the Chef',\n             'episode': 'Chef Saito\\'s Family recipe: MENCHI-KATSU',\n+            'duration': 148,\n+            'upload_date': '20190816',\n+            'release_date': '20230902',\n+            'release_timestamp': 1693619292,\n+            'modified_timestamp': 1694168033,\n+            'modified_date': '20230908',\n+            'timestamp': 1565997540,\n         },\n     }, {\n-        # audio clip\n-        'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/audio/r_inventions-20201104-1/',\n+        # radio\n+        'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/audio/livinginjapan-20231001-1/',\n         'info_dict': {\n-            'id': 'r_inventions-20201104-1-en',\n+            'id': 'livinginjapan-20231001-1-en',\n             'ext': 'm4a',\n-            'title': \"Japan's Top Inventions - Miniature Video Cameras\",\n-            'description': 'md5:07ea722bdbbb4936fdd360b6a480c25b',\n+            'title': 'Living in Japan - Tips for Travelers to Japan / Ramen Vending Machines',\n+            'series': 'Living in Japan',\n+            'description': 'md5:0a0e2077d8f07a03071e990a6f51bfab',\n+            'thumbnail': 'md5:960622fb6e06054a4a1a0c97ea752545',\n+            'episode': 'Tips for Travelers to Japan / Ramen Vending Machines'\n         },\n-        'skip': '404 Not Found',\n     }, {\n         'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/video/2015173/',\n         'only_matching': True,\n@@ -159,6 +235,36 @@ class NhkVodIE(NhkBaseIE):\n             'timestamp': 1623722008,\n         },\n         'skip': '404 Not Found',\n+    }, {\n+        # japanese-language, longer id than english\n+        'url': 'https://www3.nhk.or.jp/nhkworld/ja/ondemand/video/0020271111/',\n+        'info_dict': {\n+            'id': 'nw_ja_v_jvod_ohayou_20231008',\n+            'ext': 'mp4',\n+            'title': '\u304a\u306f\u3088\u3046\u65e5\u672c\uff087\u6642\u53f0\uff09 - 10\u67088\u65e5\u653e\u9001',\n+            'series': '\u304a\u306f\u3088\u3046\u65e5\u672c\uff087\u6642\u53f0\uff09',\n+            'episode': '10\u67088\u65e5\u653e\u9001',\n+            'thumbnail': 'md5:d733b1c8e965ab68fb02b2d347d0e9b4',\n+            'description': 'md5:9c1d6cbeadb827b955b20e99ab920ff0',\n+        },\n+        'skip': 'expires 2023-10-15',\n+    }, {\n+        # a one-off (single-episode series). title from the api is just '<p></p>'\n+        'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/video/3004952/',\n+        'info_dict': {\n+            'id': 'nw_vod_v_en_3004_952_20230723091000_01_1690074552',\n+            'ext': 'mp4',\n+            'title': 'Barakan Discovers AMAMI OSHIMA: Isson\\'s Treasure Island',\n+            'description': 'md5:5db620c46a0698451cc59add8816b797',\n+            'thumbnail': 'md5:67d9ff28009ba379bfa85ad1aaa0e2bd',\n+            'release_date': '20230905',\n+            'timestamp': 1690103400,\n+            'duration': 2939,\n+            'release_timestamp': 1693898699,\n+            'modified_timestamp': 1698057495,\n+            'modified_date': '20231023',\n+            'upload_date': '20230723',\n+        },\n     }]\n \n     def _real_extract(self, url):\n@@ -166,20 +272,22 @@ def _real_extract(self, url):\n \n \n class NhkVodProgramIE(NhkBaseIE):\n-    _VALID_URL = r'%s/program%s(?P<id>[0-9a-z]+)(?:.+?\\btype=(?P<episode_type>clip|(?:radio|tv)Episode))?' % (NhkBaseIE._BASE_URL_REGEX, NhkBaseIE._TYPE_REGEX)\n+    _VALID_URL = rf'{NhkBaseIE._BASE_URL_REGEX}/program{NhkBaseIE._TYPE_REGEX}(?P<id>\\w+)(?:.+?\\btype=(?P<episode_type>clip|(?:radio|tv)Episode))?'\n     _TESTS = [{\n         # video program episodes\n         'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/program/video/sumo',\n         'info_dict': {\n             'id': 'sumo',\n             'title': 'GRAND SUMO Highlights',\n+            'description': 'md5:fc20d02dc6ce85e4b72e0273aa52fdbf',\n         },\n-        'playlist_mincount': 12,\n+        'playlist_mincount': 0,\n     }, {\n         'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/program/video/japanrailway',\n         'info_dict': {\n             'id': 'japanrailway',\n             'title': 'Japan Railway Journal',\n+            'description': 'md5:ea39d93af7d05835baadf10d1aae0e3f',\n         },\n         'playlist_mincount': 12,\n     }, {\n@@ -188,6 +296,7 @@ class NhkVodProgramIE(NhkBaseIE):\n         'info_dict': {\n             'id': 'japanrailway',\n             'title': 'Japan Railway Journal',\n+            'description': 'md5:ea39d93af7d05835baadf10d1aae0e3f',\n         },\n         'playlist_mincount': 5,\n     }, {\n@@ -200,8 +309,7 @@ class NhkVodProgramIE(NhkBaseIE):\n     }]\n \n     def _real_extract(self, url):\n-        lang, m_type, program_id, episode_type = self._match_valid_url(url).groups()\n-\n+        lang, m_type, program_id, episode_type = self._match_valid_url(url).group('lang', 'type', 'id', 'episode_type')\n         episodes = self._call_api(\n             program_id, lang, m_type == 'video', False, episode_type == 'clip')\n \n@@ -213,11 +321,11 @@ def _real_extract(self, url):\n             entries.append(self._extract_episode_info(\n                 urljoin(url, episode_path), episode))\n \n-        program_title = None\n-        if entries:\n-            program_title = entries[0].get('series')\n+        html = self._download_webpage(url, program_id)\n+        program_title = clean_html(get_element_by_class('p-programDetail__title', html))\n+        program_description = clean_html(get_element_by_class('p-programDetail__text', html))\n \n-        return self.playlist_result(entries, program_id, program_title)\n+        return self.playlist_result(entries, program_id, program_title, program_description)\n \n \n class NhkForSchoolBangumiIE(InfoExtractor):\n@@ -369,6 +477,7 @@ class NhkRadiruIE(InfoExtractor):\n         'skip': 'Episode expired on 2023-04-16',\n         'info_dict': {\n             'channel': 'NHK-FM',\n+            'uploader': 'NHK-FM',\n             'description': 'md5:94b08bdeadde81a97df4ec882acce3e9',\n             'ext': 'm4a',\n             'id': '0449_01_3853544',\n@@ -389,6 +498,7 @@ class NhkRadiruIE(InfoExtractor):\n             'title': '\u30d9\u30b9\u30c8\u30aa\u30d6\u30af\u30e9\u30b7\u30c3\u30af',\n             'description': '\u4e16\u754c\u4e2d\u306e\u4e0a\u8cea\u306a\u6f14\u594f\u4f1a\u3092\u3058\u3063\u304f\u308a\u582a\u80fd\u3059\u308b\u672c\u683c\u6d3e\u30af\u30e9\u30b7\u30c3\u30af\u756a\u7d44\u3002',\n             'channel': 'NHK-FM',\n+            'uploader': 'NHK-FM',\n             'thumbnail': 'https://www.nhk.or.jp/prog/img/458/g458.jpg',\n         },\n         'playlist_mincount': 3,\n@@ -402,6 +512,7 @@ class NhkRadiruIE(InfoExtractor):\n             'title': '\u6709\u5cf6\u6b66\u90ce\u300c\u4e00\u623f\u306e\u3075\u3099\u3068\u3099\u3046\u300d',\n             'description': '\u6717\u8aad\uff1a\u5ddd\u91ce\u4e00\u5b87\uff08\u30e9\u30b8\u30aa\u6df1\u591c\u4fbf\u30a2\u30f3\u30ab\u30fc\uff09\\r\\n\\r\\n\uff082016\u5e7412\u67088\u65e5\u653e\u9001\u300c\u30e9\u30b8\u30aa\u6df1\u591c\u4fbf\u300e\u30a2\u30f3\u30ab\u30fc\u6717\u8aad\u30b7\u30ea\u30fc\u30ba\u300f\u300d\u3088\u308a\uff09',\n             'channel': 'NHK\u30e9\u30b8\u30aa\u7b2c1\u3001NHK-FM',\n+            'uploader': 'NHK\u30e9\u30b8\u30aa\u7b2c1\u3001NHK-FM',\n             'timestamp': 1635757200,\n             'thumbnail': 'https://www.nhk.or.jp/radioondemand/json/F300/img/corner/box_109_thumbnail.jpg',\n             'release_date': '20161207',\n@@ -417,6 +528,7 @@ class NhkRadiruIE(InfoExtractor):\n             'id': 'F261_01_3855109',\n             'ext': 'm4a',\n             'channel': 'NHK\u30e9\u30b8\u30aa\u7b2c1',\n+            'uploader': 'NHK\u30e9\u30b8\u30aa\u7b2c1',\n             'timestamp': 1681635900,\n             'release_date': '20230416',\n             'series': 'NHK\u30e9\u30b8\u30aa\u30cb\u30e5\u30fc\u30b9',\n@@ -461,6 +573,7 @@ def _real_extract(self, url):\n         series_meta = traverse_obj(meta, {\n             'title': 'program_name',\n             'channel': 'media_name',\n+            'uploader': 'media_name',\n             'thumbnail': (('thumbnail_c', 'thumbnail_p'), {url_or_none}),\n         }, get_all=False)\n \n@@ -489,6 +602,7 @@ class NhkRadioNewsPageIE(InfoExtractor):\n             'thumbnail': 'https://www.nhk.or.jp/radioondemand/json/F261/img/RADIONEWS_640.jpg',\n             'description': 'md5:bf2c5b397e44bc7eb26de98d8f15d79d',\n             'channel': 'NHK\u30e9\u30b8\u30aa\u7b2c1',\n+            'uploader': 'NHK\u30e9\u30b8\u30aa\u7b2c1',\n             'title': 'NHK\u30e9\u30b8\u30aa\u30cb\u30e5\u30fc\u30b9',\n         }\n     }]\n@@ -551,7 +665,7 @@ def _real_extract(self, url):\n \n         noa_info = self._download_json(\n             f'https:{config.find(\".//url_program_noa\").text}'.format(area=data.find('areakey').text),\n-            station, note=f'Downloading {area} station metadata')\n+            station, note=f'Downloading {area} station metadata', fatal=False)\n         present_info = traverse_obj(noa_info, ('nowonair_list', self._NOA_STATION_IDS.get(station), 'present'))\n \n         return {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nick.py",
            "diff": "diff --git a/yt_dlp/extractor/nick.py b/yt_dlp/extractor/nick.py\nindex de22cb8d..165d8ce9 100644\n--- a/yt_dlp/extractor/nick.py\n+++ b/yt_dlp/extractor/nick.py\n@@ -188,26 +188,6 @@ def _get_feed_url(self, uri, url=None):\n         return self._remove_template_parameter(config['feedWithQueryParams'])\n \n \n-class NickNightIE(NickDeIE):  # XXX: Do not subclass from concrete IE\n-    IE_NAME = 'nicknight'\n-    _VALID_URL = r'https?://(?:www\\.)(?P<host>nicknight\\.(?:de|at|tv))/(?:playlist|shows)/(?:[^/]+/)*(?P<id>[^/?#&]+)'\n-    _TESTS = [{\n-        'url': 'http://www.nicknight.at/shows/977-awkward/videos/85987-nimmer-beste-freunde',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.nicknight.at/shows/977-awkward',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.nicknight.at/shows/1900-faking-it',\n-        'only_matching': True,\n-    }]\n-\n-    def _extract_mrss_url(self, webpage, *args):\n-        return self._search_regex(\n-            r'mrss\\s*:\\s*([\"\\'])(?P<url>http.+?)\\1', webpage,\n-            'mrss url', group='url')\n-\n-\n class NickRuIE(MTVServicesInfoExtractor):\n     IE_NAME = 'nickelodeonru'\n     _VALID_URL = r'https?://(?:www\\.)nickelodeon\\.(?:ru|fr|es|pt|ro|hu|com\\.tr)/[^/]+/(?:[^/]+/)*(?P<id>[^/?#&]+)'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/niconico.py",
            "diff": "diff --git a/yt_dlp/extractor/niconico.py b/yt_dlp/extractor/niconico.py\nindex fa2d709d..797b5268 100644\n--- a/yt_dlp/extractor/niconico.py\n+++ b/yt_dlp/extractor/niconico.py\n@@ -8,12 +8,11 @@\n from urllib.parse import urlparse\n \n from .common import InfoExtractor, SearchInfoExtractor\n-from ..dependencies import websockets\n+from ..networking import Request\n from ..networking.exceptions import HTTPError\n from ..utils import (\n     ExtractorError,\n     OnDemandPagedList,\n-    WebSocketsWrapper,\n     bug_reports_message,\n     clean_html,\n     float_or_none,\n@@ -934,8 +933,6 @@ class NiconicoLiveIE(InfoExtractor):\n     _KNOWN_LATENCY = ('high', 'low')\n \n     def _real_extract(self, url):\n-        if not websockets:\n-            raise ExtractorError('websockets library is not available. Please install it.', expected=True)\n         video_id = self._match_id(url)\n         webpage, urlh = self._download_webpage_handle(f'https://live.nicovideo.jp/watch/{video_id}', video_id)\n \n@@ -950,17 +947,13 @@ def _real_extract(self, url):\n         })\n \n         hostname = remove_start(urlparse(urlh.url).hostname, 'sp.')\n-        cookies = try_get(urlh.url, self._downloader._calc_cookies)\n         latency = try_get(self._configuration_arg('latency'), lambda x: x[0])\n         if latency not in self._KNOWN_LATENCY:\n             latency = 'high'\n \n-        ws = WebSocketsWrapper(ws_url, {\n-            'Cookies': str_or_none(cookies) or '',\n-            'Origin': f'https://{hostname}',\n-            'Accept': '*/*',\n-            'User-Agent': self.get_param('http_headers')['User-Agent'],\n-        })\n+        ws = self._request_webpage(\n+            Request(ws_url, headers={'Origin': f'https://{hostname}'}),\n+            video_id=video_id, note='Connecting to WebSocket server')\n \n         self.write_debug('[debug] Sending HLS server request')\n         ws.send(json.dumps({\n@@ -1034,7 +1027,6 @@ def _real_extract(self, url):\n                 'protocol': 'niconico_live',\n                 'ws': ws,\n                 'video_id': video_id,\n-                'cookies': cookies,\n                 'live_latency': latency,\n                 'origin': hostname,\n             })\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/niconicochannelplus.py",
            "diff": "diff --git a/yt_dlp/extractor/niconicochannelplus.py b/yt_dlp/extractor/niconicochannelplus.py\nnew file mode 100644\nindex 00000000..89af3f7b\n--- /dev/null\n+++ b/yt_dlp/extractor/niconicochannelplus.py\n@@ -0,0 +1,426 @@\n+import functools\n+import json\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    OnDemandPagedList,\n+    filter_dict,\n+    int_or_none,\n+    parse_qs,\n+    str_or_none,\n+    traverse_obj,\n+    unified_timestamp,\n+    url_or_none,\n+)\n+\n+\n+class NiconicoChannelPlusBaseIE(InfoExtractor):\n+    _WEBPAGE_BASE_URL = 'https://nicochannel.jp'\n+\n+    def _call_api(self, path, item_id, *args, **kwargs):\n+        return self._download_json(\n+            f'https://nfc-api.nicochannel.jp/fc/{path}', video_id=item_id, *args, **kwargs)\n+\n+    def _find_fanclub_site_id(self, channel_name):\n+        fanclub_list_json = self._call_api(\n+            'content_providers/channels', item_id=f'channels/{channel_name}',\n+            note='Fetching channel list', errnote='Unable to fetch channel list',\n+        )['data']['content_providers']\n+        fanclub_id = traverse_obj(fanclub_list_json, (\n+            lambda _, v: v['domain'] == f'{self._WEBPAGE_BASE_URL}/{channel_name}', 'id'),\n+            get_all=False)\n+        if not fanclub_id:\n+            raise ExtractorError(f'Channel {channel_name} does not exist', expected=True)\n+        return fanclub_id\n+\n+    def _get_channel_base_info(self, fanclub_site_id):\n+        return traverse_obj(self._call_api(\n+            f'fanclub_sites/{fanclub_site_id}/page_base_info', item_id=f'fanclub_sites/{fanclub_site_id}',\n+            note='Fetching channel base info', errnote='Unable to fetch channel base info', fatal=False,\n+        ), ('data', 'fanclub_site', {dict})) or {}\n+\n+    def _get_channel_user_info(self, fanclub_site_id):\n+        return traverse_obj(self._call_api(\n+            f'fanclub_sites/{fanclub_site_id}/user_info', item_id=f'fanclub_sites/{fanclub_site_id}',\n+            note='Fetching channel user info', errnote='Unable to fetch channel user info', fatal=False,\n+            data=json.dumps('null').encode('ascii'),\n+        ), ('data', 'fanclub_site', {dict})) or {}\n+\n+\n+class NiconicoChannelPlusIE(NiconicoChannelPlusBaseIE):\n+    IE_NAME = 'NiconicoChannelPlus'\n+    IE_DESC = '\u30cb\u30b3\u30cb\u30b3\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9'\n+    _VALID_URL = r'https?://nicochannel\\.jp/(?P<channel>[\\w.-]+)/(?:video|live)/(?P<code>sm\\w+)'\n+    _TESTS = [{\n+        'url': 'https://nicochannel.jp/kaorin/video/smsDd8EdFLcVZk9yyAhD6H7H',\n+        'info_dict': {\n+            'id': 'smsDd8EdFLcVZk9yyAhD6H7H',\n+            'title': '\u524d\u7530\u4f73\u7e54\u91cc\u306f\u30cb\u30b3\u751f\u304c\u3057\u305f\u3044\uff01',\n+            'ext': 'mp4',\n+            'channel': '\u524d\u7530\u4f73\u7e54\u91cc\u306e\u4e16\u754c\u653b\u7565\u8a08\u753b',\n+            'channel_id': 'kaorin',\n+            'channel_url': 'https://nicochannel.jp/kaorin',\n+            'live_status': 'not_live',\n+            'thumbnail': 'https://nicochannel.jp/public_html/contents/video_pages/74/thumbnail_path',\n+            'description': '\uff12\uff10\uff12\uff11\u5e74\uff11\uff11\u6708\u306b\u653e\u9001\u3055\u308c\u305f\\n\u300c\u524d\u7530\u4f73\u7e54\u91cc\u306f\u30cb\u30b3\u751f\u304c\u3057\u305f\u3044\uff01\u300d\u30a2\u30fc\u30ab\u30a4\u30d6\u306b\u306a\u308a\u307e\u3059\u3002',\n+            'timestamp': 1641360276,\n+            'duration': 4097,\n+            'comment_count': int,\n+            'view_count': int,\n+            'tags': [],\n+            'upload_date': '20220105',\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        # age limited video; test purpose channel.\n+        'url': 'https://nicochannel.jp/testman/video/smDXbcrtyPNxLx9jc4BW69Ve',\n+        'info_dict': {\n+            'id': 'smDXbcrtyPNxLx9jc4BW69Ve',\n+            'title': 'test oshiro',\n+            'ext': 'mp4',\n+            'channel': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3',\n+            'channel_id': 'testman',\n+            'channel_url': 'https://nicochannel.jp/testman',\n+            'age_limit': 18,\n+            'live_status': 'was_live',\n+            'timestamp': 1666344616,\n+            'duration': 86465,\n+            'comment_count': int,\n+            'view_count': int,\n+            'tags': [],\n+            'upload_date': '20221021',\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        content_code, channel_id = self._match_valid_url(url).group('code', 'channel')\n+        fanclub_site_id = self._find_fanclub_site_id(channel_id)\n+\n+        data_json = self._call_api(\n+            f'video_pages/{content_code}', item_id=content_code, headers={'fc_use_device': 'null'},\n+            note='Fetching video page info', errnote='Unable to fetch video page info',\n+        )['data']['video_page']\n+\n+        live_status, session_id = self._get_live_status_and_session_id(content_code, data_json)\n+\n+        release_timestamp_str = data_json.get('live_scheduled_start_at')\n+\n+        formats = []\n+\n+        if live_status == 'is_upcoming':\n+            if release_timestamp_str:\n+                msg = f'This live event will begin at {release_timestamp_str} UTC'\n+            else:\n+                msg = 'This event has not started yet'\n+            self.raise_no_formats(msg, expected=True, video_id=content_code)\n+        else:\n+            formats = self._extract_m3u8_formats(\n+                # \"authenticated_url\" is a format string that contains \"{session_id}\".\n+                m3u8_url=data_json['video_stream']['authenticated_url'].format(session_id=session_id),\n+                video_id=content_code)\n+\n+        return {\n+            'id': content_code,\n+            'formats': formats,\n+            '_format_sort_fields': ('tbr', 'vcodec', 'acodec'),\n+            'channel': self._get_channel_base_info(fanclub_site_id).get('fanclub_site_name'),\n+            'channel_id': channel_id,\n+            'channel_url': f'{self._WEBPAGE_BASE_URL}/{channel_id}',\n+            'age_limit': traverse_obj(self._get_channel_user_info(fanclub_site_id), ('content_provider', 'age_limit')),\n+            'live_status': live_status,\n+            'release_timestamp': unified_timestamp(release_timestamp_str),\n+            **traverse_obj(data_json, {\n+                'title': ('title', {str}),\n+                'thumbnail': ('thumbnail_url', {url_or_none}),\n+                'description': ('description', {str}),\n+                'timestamp': ('released_at', {unified_timestamp}),\n+                'duration': ('active_video_filename', 'length', {int_or_none}),\n+                'comment_count': ('video_aggregate_info', 'number_of_comments', {int_or_none}),\n+                'view_count': ('video_aggregate_info', 'total_views', {int_or_none}),\n+                'tags': ('video_tags', ..., 'tag', {str}),\n+            }),\n+            '__post_extractor': self.extract_comments(\n+                content_code=content_code,\n+                comment_group_id=traverse_obj(data_json, ('video_comment_setting', 'comment_group_id'))),\n+        }\n+\n+    def _get_comments(self, content_code, comment_group_id):\n+        item_id = f'{content_code}/comments'\n+\n+        if not comment_group_id:\n+            return None\n+\n+        comment_access_token = self._call_api(\n+            f'video_pages/{content_code}/comments_user_token', item_id,\n+            note='Getting comment token', errnote='Unable to get comment token',\n+        )['data']['access_token']\n+\n+        comment_list = self._download_json(\n+            'https://comm-api.sheeta.com/messages.history', video_id=item_id,\n+            note='Fetching comments', errnote='Unable to fetch comments',\n+            headers={'Content-Type': 'application/json'},\n+            query={\n+                'sort_direction': 'asc',\n+                'limit': int_or_none(self._configuration_arg('max_comments', [''])[0]) or 120,\n+            },\n+            data=json.dumps({\n+                'token': comment_access_token,\n+                'group_id': comment_group_id,\n+            }).encode('ascii'))\n+\n+        for comment in traverse_obj(comment_list, ...):\n+            yield traverse_obj(comment, {\n+                'author': ('nickname', {str}),\n+                'author_id': ('sender_id', {str_or_none}),\n+                'id': ('id', {str_or_none}),\n+                'text': ('message', {str}),\n+                'timestamp': (('updated_at', 'sent_at', 'created_at'), {unified_timestamp}),\n+                'author_is_uploader': ('sender_id', {lambda x: x == '-1'}),\n+            }, get_all=False)\n+\n+    def _get_live_status_and_session_id(self, content_code, data_json):\n+        video_type = data_json.get('type')\n+        live_finished_at = data_json.get('live_finished_at')\n+\n+        payload = {}\n+        if video_type == 'vod':\n+            if live_finished_at:\n+                live_status = 'was_live'\n+            else:\n+                live_status = 'not_live'\n+        elif video_type == 'live':\n+            if not data_json.get('live_started_at'):\n+                return 'is_upcoming', ''\n+\n+            if not live_finished_at:\n+                live_status = 'is_live'\n+            else:\n+                live_status = 'was_live'\n+                payload = {'broadcast_type': 'dvr'}\n+\n+                video_allow_dvr_flg = traverse_obj(data_json, ('video', 'allow_dvr_flg'))\n+                video_convert_to_vod_flg = traverse_obj(data_json, ('video', 'convert_to_vod_flg'))\n+\n+                self.write_debug(f'allow_dvr_flg = {video_allow_dvr_flg}, convert_to_vod_flg = {video_convert_to_vod_flg}.')\n+\n+                if not (video_allow_dvr_flg and video_convert_to_vod_flg):\n+                    raise ExtractorError(\n+                        'Live was ended, there is no video for download.', video_id=content_code, expected=True)\n+        else:\n+            raise ExtractorError(f'Unknown type: {video_type}', video_id=content_code, expected=False)\n+\n+        self.write_debug(f'{content_code}: video_type={video_type}, live_status={live_status}')\n+\n+        session_id = self._call_api(\n+            f'video_pages/{content_code}/session_ids', item_id=f'{content_code}/session',\n+            data=json.dumps(payload).encode('ascii'), headers={\n+                'Content-Type': 'application/json',\n+                'fc_use_device': 'null',\n+                'origin': 'https://nicochannel.jp',\n+            },\n+            note='Getting session id', errnote='Unable to get session id',\n+        )['data']['session_id']\n+\n+        return live_status, session_id\n+\n+\n+class NiconicoChannelPlusChannelBaseIE(NiconicoChannelPlusBaseIE):\n+    _PAGE_SIZE = 12\n+\n+    def _fetch_paged_channel_video_list(self, path, query, channel_name, item_id, page):\n+        response = self._call_api(\n+            path, item_id, query={\n+                **query,\n+                'page': (page + 1),\n+                'per_page': self._PAGE_SIZE,\n+            },\n+            headers={'fc_use_device': 'null'},\n+            note=f'Getting channel info (page {page + 1})',\n+            errnote=f'Unable to get channel info (page {page + 1})')\n+\n+        for content_code in traverse_obj(response, ('data', 'video_pages', 'list', ..., 'content_code')):\n+            # \"video/{content_code}\" works for both VOD and live, but \"live/{content_code}\" doesn't work for VOD\n+            yield self.url_result(\n+                f'{self._WEBPAGE_BASE_URL}/{channel_name}/video/{content_code}', NiconicoChannelPlusIE)\n+\n+\n+class NiconicoChannelPlusChannelVideosIE(NiconicoChannelPlusChannelBaseIE):\n+    IE_NAME = 'NiconicoChannelPlus:channel:videos'\n+    IE_DESC = '\u30cb\u30b3\u30cb\u30b3\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9 - \u30c1\u30e3\u30f3\u30cd\u30eb - \u52d5\u753b\u30ea\u30b9\u30c8. nicochannel.jp/channel/videos'\n+    _VALID_URL = r'https?://nicochannel\\.jp/(?P<id>[a-z\\d\\._-]+)/videos(?:\\?.*)?'\n+    _TESTS = [{\n+        # query: None\n+        'url': 'https://nicochannel.jp/testman/videos',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # query: None\n+        'url': 'https://nicochannel.jp/testtarou/videos',\n+        'info_dict': {\n+            'id': 'testtarou-videos',\n+            'title': '\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u592a\u90ce-videos',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        # query: None\n+        'url': 'https://nicochannel.jp/testjirou/videos',\n+        'info_dict': {\n+            'id': 'testjirou-videos',\n+            'title': '\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u4e8c\u90ce-videos',\n+        },\n+        'playlist_mincount': 12,\n+    }, {\n+        # query: tag\n+        'url': 'https://nicochannel.jp/testman/videos?tag=%E6%A4%9C%E8%A8%BC%E7%94%A8',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 6,\n+    }, {\n+        # query: vodType\n+        'url': 'https://nicochannel.jp/testman/videos?vodType=1',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # query: sort\n+        'url': 'https://nicochannel.jp/testman/videos?sort=-released_at',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # query: tag, vodType\n+        'url': 'https://nicochannel.jp/testman/videos?tag=%E6%A4%9C%E8%A8%BC%E7%94%A8&vodType=1',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 6,\n+    }, {\n+        # query: tag, sort\n+        'url': 'https://nicochannel.jp/testman/videos?tag=%E6%A4%9C%E8%A8%BC%E7%94%A8&sort=-released_at',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 6,\n+    }, {\n+        # query: vodType, sort\n+        'url': 'https://nicochannel.jp/testman/videos?vodType=1&sort=-released_at',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # query: tag, vodType, sort\n+        'url': 'https://nicochannel.jp/testman/videos?tag=%E6%A4%9C%E8%A8%BC%E7%94%A8&vodType=1&sort=-released_at',\n+        'info_dict': {\n+            'id': 'testman-videos',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-videos',\n+        },\n+        'playlist_mincount': 6,\n+    }]\n+\n+    def _real_extract(self, url):\n+        \"\"\"\n+        API parameters:\n+            sort:\n+                -released_at         \u516c\u958b\u65e5\u304c\u65b0\u3057\u3044\u9806 (newest to oldest)\n+                 released_at         \u516c\u958b\u65e5\u304c\u53e4\u3044\u9806 (oldest to newest)\n+                -number_of_vod_views \u518d\u751f\u6570\u304c\u591a\u3044\u9806 (most play count)\n+                 number_of_vod_views \u30b3\u30e1\u30f3\u30c8\u304c\u591a\u3044\u9806 (most comments)\n+            vod_type (is \"vodType\" in \"url\"):\n+                0 \u3059\u3079\u3066 (all)\n+                1 \u4f1a\u54e1\u9650\u5b9a (members only)\n+                2 \u4e00\u90e8\u7121\u6599 (partially free)\n+                3 \u30ec\u30f3\u30bf\u30eb (rental)\n+                4 \u751f\u653e\u9001\u30a2\u30fc\u30ab\u30a4\u30d6 (live archives)\n+                5 \u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u52d5\u753b (uploaded videos)\n+        \"\"\"\n+\n+        channel_id = self._match_id(url)\n+        fanclub_site_id = self._find_fanclub_site_id(channel_id)\n+        channel_name = self._get_channel_base_info(fanclub_site_id).get('fanclub_site_name')\n+        qs = parse_qs(url)\n+\n+        return self.playlist_result(\n+            OnDemandPagedList(\n+                functools.partial(\n+                    self._fetch_paged_channel_video_list, f'fanclub_sites/{fanclub_site_id}/video_pages',\n+                    filter_dict({\n+                        'tag': traverse_obj(qs, ('tag', 0)),\n+                        'sort': traverse_obj(qs, ('sort', 0), default='-released_at'),\n+                        'vod_type': traverse_obj(qs, ('vodType', 0), default='0'),\n+                    }),\n+                    channel_id, f'{channel_id}/videos'),\n+                self._PAGE_SIZE),\n+            playlist_id=f'{channel_id}-videos', playlist_title=f'{channel_name}-videos')\n+\n+\n+class NiconicoChannelPlusChannelLivesIE(NiconicoChannelPlusChannelBaseIE):\n+    IE_NAME = 'NiconicoChannelPlus:channel:lives'\n+    IE_DESC = '\u30cb\u30b3\u30cb\u30b3\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9 - \u30c1\u30e3\u30f3\u30cd\u30eb - \u30e9\u30a4\u30d6\u30ea\u30b9\u30c8. nicochannel.jp/channel/lives'\n+    _VALID_URL = r'https?://nicochannel\\.jp/(?P<id>[a-z\\d\\._-]+)/lives'\n+    _TESTS = [{\n+        'url': 'https://nicochannel.jp/testman/lives',\n+        'info_dict': {\n+            'id': 'testman-lives',\n+            'title': '\u672c\u756a\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u30de\u30f3-lives',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        'url': 'https://nicochannel.jp/testtarou/lives',\n+        'info_dict': {\n+            'id': 'testtarou-lives',\n+            'title': '\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u592a\u90ce-lives',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        'url': 'https://nicochannel.jp/testjirou/lives',\n+        'info_dict': {\n+            'id': 'testjirou-lives',\n+            'title': '\u30c1\u30e3\u30f3\u30cd\u30eb\u30d7\u30e9\u30b9\u30c6\u30b9\u30c8\u4e8c\u90ce-lives',\n+        },\n+        'playlist_mincount': 6,\n+    }]\n+\n+    def _real_extract(self, url):\n+        \"\"\"\n+        API parameters:\n+            live_type:\n+                1 \u653e\u9001\u4e2d (on air)\n+                2 \u653e\u9001\u4e88\u5b9a (scheduled live streams, oldest to newest)\n+                3 \u904e\u53bb\u306e\u653e\u9001 - \u3059\u3079\u3066 (all ended live streams, newest to oldest)\n+                4 \u904e\u53bb\u306e\u653e\u9001 - \u751f\u653e\u9001\u30a2\u30fc\u30ab\u30a4\u30d6 (all archives for live streams, oldest to newest)\n+            We use \"4\" instead of \"3\" because some recently ended live streams could not be downloaded.\n+        \"\"\"\n+\n+        channel_id = self._match_id(url)\n+        fanclub_site_id = self._find_fanclub_site_id(channel_id)\n+        channel_name = self._get_channel_base_info(fanclub_site_id).get('fanclub_site_name')\n+\n+        return self.playlist_result(\n+            OnDemandPagedList(\n+                functools.partial(\n+                    self._fetch_paged_channel_video_list, f'fanclub_sites/{fanclub_site_id}/live_pages',\n+                    {\n+                        'live_type': 4,\n+                    },\n+                    channel_id, f'{channel_id}/lives'),\n+                self._PAGE_SIZE),\n+            playlist_id=f'{channel_id}-lives', playlist_title=f'{channel_name}-lives')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nintendo.py",
            "diff": "diff --git a/yt_dlp/extractor/nintendo.py b/yt_dlp/extractor/nintendo.py\nindex ed839af2..853a169b 100644\n--- a/yt_dlp/extractor/nintendo.py\n+++ b/yt_dlp/extractor/nintendo.py\n@@ -1,57 +1,131 @@\n-import re\n+import json\n+import urllib.parse\n \n from .common import InfoExtractor\n-from .ooyala import OoyalaIE\n+from ..utils import (\n+    ExtractorError,\n+    make_archive_id,\n+    unified_timestamp,\n+    urljoin,\n+)\n+from ..utils.traversal import traverse_obj\n \n \n class NintendoIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?nintendo\\.com/(?:games/detail|nintendo-direct)/(?P<id>[^/?#&]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?nintendo\\.com/(?:(?P<locale>\\w{2}(?:-\\w{2})?)/)?nintendo-direct/(?P<slug>[^/?#]+)'\n     _TESTS = [{\n-        'url': 'https://www.nintendo.com/games/detail/duck-hunt-wii-u/',\n+        'url': 'https://www.nintendo.com/nintendo-direct/09-04-2019/',\n         'info_dict': {\n-            'id': 'MzMmticjp0VPzO3CCj4rmFOuohEuEWoW',\n-            'ext': 'flv',\n-            'title': 'Duck Hunt Wii U VC NES - Trailer',\n-            'duration': 60.326,\n-        },\n-        'params': {\n-            'skip_download': True,\n+            'ext': 'mp4',\n+            'id': '2oPmiviVePUA1IqAZzjuVh',\n+            'display_id': '09-04-2019',\n+            'title': 'Nintendo Direct 9.4.2019',\n+            'timestamp': 1567580400,\n+            'description': 'md5:8aac2780361d8cb772b6d1de66d7d6f4',\n+            'upload_date': '20190904',\n+            'age_limit': 17,\n+            '_old_archive_ids': ['nintendo J2bXdmaTE6fe3dWJTPcc7m23FNbc_A1V'],\n         },\n-        'add_ie': ['Ooyala'],\n     }, {\n-        'url': 'http://www.nintendo.com/games/detail/tokyo-mirage-sessions-fe-wii-u',\n+        'url': 'https://www.nintendo.com/en-ca/nintendo-direct/08-31-2023/',\n         'info_dict': {\n-            'id': 'tokyo-mirage-sessions-fe-wii-u',\n-            'title': 'Tokyo Mirage Sessions \u266fFE',\n+            'ext': 'mp4',\n+            'id': '2TB2w2rJhNYF84qQ9E57hU',\n+            'display_id': '08-31-2023',\n+            'title': 'Super Mario Bros. Wonder Direct 8.31.2023',\n+            'timestamp': 1693465200,\n+            'description': 'md5:3067c5b824bcfdae9090a7f38ab2d200',\n+            'tags': ['Mild Fantasy Violence', 'In-Game Purchases'],\n+            'upload_date': '20230831',\n+            'age_limit': 6,\n         },\n-        'playlist_count': 4,\n     }, {\n-        'url': 'https://www.nintendo.com/nintendo-direct/09-04-2019/',\n+        'url': 'https://www.nintendo.com/us/nintendo-direct/50-fact-extravaganza/',\n         'info_dict': {\n-            'id': 'J2bXdmaTE6fe3dWJTPcc7m23FNbc_A1V',\n             'ext': 'mp4',\n-            'title': 'Switch_ROS_ND0904-H264.mov',\n-            'duration': 2324.758,\n-        },\n-        'params': {\n-            'skip_download': True,\n+            'id': 'j0BBGzfw0pQ',\n+            'channel_follower_count': int,\n+            'view_count': int,\n+            'description': 'Learn new details about Super Smash Bros. for Wii U, which launches on November 21.',\n+            'duration': 2123,\n+            'availability': 'public',\n+            'thumbnail': 'https://i.ytimg.com/vi_webp/j0BBGzfw0pQ/maxresdefault.webp',\n+            'timestamp': 1414047600,\n+            'channel_id': 'UCGIY_O-8vW4rfX98KlMkvRg',\n+            'chapters': 'count:53',\n+            'heatmap': 'count:100',\n+            'upload_date': '20141023',\n+            'uploader_id': '@NintendoAmerica',\n+            'playable_in_embed': True,\n+            'categories': ['Gaming'],\n+            'display_id': '50-fact-extravaganza',\n+            'channel': 'Nintendo of America',\n+            'tags': ['Comic Mischief', 'Cartoon Violence', 'Mild Suggestive Themes'],\n+            'like_count': int,\n+            'channel_url': 'https://www.youtube.com/channel/UCGIY_O-8vW4rfX98KlMkvRg',\n+            'age_limit': 10,\n+            'uploader_url': 'https://www.youtube.com/@NintendoAmerica',\n+            'comment_count': int,\n+            'live_status': 'not_live',\n+            'uploader': 'Nintendo of America',\n+            'title': '50-FACT Extravaganza',\n         },\n-        'add_ie': ['Ooyala'],\n     }]\n \n+    def _create_asset_url(self, path):\n+        return urljoin('https://assets.nintendo.com/', urllib.parse.quote(path))\n+\n     def _real_extract(self, url):\n-        page_id = self._match_id(url)\n+        locale, slug = self._match_valid_url(url).group('locale', 'slug')\n+\n+        language, _, country = (locale or 'US').rpartition('-')\n+        parsed_locale = f'{language.lower() or \"en\"}_{country.upper()}'\n+        self.write_debug(f'Using locale {parsed_locale} (from {locale})', only_once=True)\n+\n+        response = self._download_json('https://graph.nintendo.com/', slug, query={\n+            'operationName': 'NintendoDirect',\n+            'variables': json.dumps({\n+                'locale': parsed_locale,\n+                'slug': slug,\n+            }, separators=(',', ':')),\n+            'extensions': json.dumps({\n+                'persistedQuery': {\n+                    'version': 1,\n+                    'sha256Hash': '969b16fe9f08b686fa37bc44d1fd913b6188e65794bb5e341c54fa683a8004cb'\n+                },\n+            }, separators=(',', ':')),\n+        })\n+        # API returns `{\"data\": {\"direct\": null}}` if no matching id\n+        direct_info = traverse_obj(response, ('data', 'direct', {dict}))\n+        if not direct_info:\n+            raise ExtractorError(f'No Nintendo Direct with id {slug} exists', expected=True)\n+\n+        errors = ', '.join(traverse_obj(response, ('errors', ..., 'message')))\n+        if errors:\n+            raise ExtractorError(f'GraphQL API error: {errors or \"Unknown error\"}')\n+\n+        result = traverse_obj(direct_info, {\n+            'id': ('id', {str}),\n+            'title': ('name', {str}),\n+            'timestamp': ('startDate', {unified_timestamp}),\n+            'description': ('description', 'text', {str}),\n+            'age_limit': ('contentRating', 'order', {int}),\n+            'tags': ('contentDescriptors', ..., 'label', {str}),\n+            'thumbnail': ('thumbnail', {self._create_asset_url}),\n+        })\n+        result['display_id'] = slug\n \n-        webpage = self._download_webpage(url, page_id)\n+        asset_id = traverse_obj(direct_info, ('video', 'publicId', {str}))\n+        if not asset_id:\n+            youtube_id = traverse_obj(direct_info, ('liveStream', {str}))\n+            if not youtube_id:\n+                self.raise_no_formats('Could not find any video formats', video_id=slug)\n \n-        entries = [\n-            OoyalaIE._build_url_result(m.group('code'))\n-            for m in re.finditer(\n-                r'data-(?:video-id|directVideoId)=([\"\\'])(?P<code>(?:(?!\\1).)+)\\1', webpage)]\n+            return self.url_result(youtube_id, **result, url_transparent=True)\n \n-        title = self._html_search_regex(\n-            r'(?s)<(?:span|div)[^>]+class=\"(?:title|wrapper)\"[^>]*>.*?<h1>(.+?)</h1>',\n-            webpage, 'title', fatal=False)\n+        if asset_id.startswith('Legacy Videos/'):\n+            result['_old_archive_ids'] = [make_archive_id(self, asset_id[14:])]\n+        result['formats'] = self._extract_m3u8_formats(\n+            self._create_asset_url(f'/video/upload/sp_full_hd/v1/{asset_id}.m3u8'), slug)\n \n-        return self.playlist_result(\n-            entries, page_id, title)\n+        return result\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nitter.py",
            "diff": "diff --git a/yt_dlp/extractor/nitter.py b/yt_dlp/extractor/nitter.py\nindex 5d1ca1f5..35d1311d 100644\n--- a/yt_dlp/extractor/nitter.py\n+++ b/yt_dlp/extractor/nitter.py\n@@ -265,6 +265,26 @@ class NitterIE(InfoExtractor):\n                 'repost_count': int,\n                 'comment_count': int,\n             }\n+        }, {  # no OpenGraph title\n+            'url': f'https://{current_instance}/LocalBateman/status/1678455464038735895#m',\n+            'info_dict': {\n+                'id': '1678455464038735895',\n+                'ext': 'mp4',\n+                'title': 'Your Typical Local Man - Local man, what did Romanians ever do to you?',\n+                'description': 'Local man, what did Romanians ever do to you?',\n+                'thumbnail': r're:^https?://.*\\.jpg$',\n+                'uploader': 'Your Typical Local Man',\n+                'uploader_id': 'LocalBateman',\n+                'uploader_url': f'https://{current_instance}/LocalBateman',\n+                'upload_date': '20230710',\n+                'timestamp': 1689009900,\n+                'view_count': int,\n+                'like_count': int,\n+                'repost_count': int,\n+                'comment_count': int,\n+            },\n+            'expected_warnings': ['Ignoring subtitle tracks found in the HLS manifest'],\n+            'params': {'skip_download': 'm3u8'},\n         }\n     ]\n \n@@ -292,7 +312,7 @@ def _real_extract(self, url):\n                 'ext': ext\n             }]\n \n-        title = description = self._og_search_description(full_webpage) or self._html_search_regex(\n+        title = description = self._og_search_description(full_webpage, default=None) or self._html_search_regex(\n             r'<div class=\"tweet-content[^>]+>([^<]+)</div>', webpage, 'title', fatal=False)\n \n         uploader_id = self._html_search_regex(\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/njpwworld.py",
            "diff": "diff --git a/yt_dlp/extractor/njpwworld.py b/yt_dlp/extractor/njpwworld.py\ndeleted file mode 100644\nindex 60783813..00000000\n--- a/yt_dlp/extractor/njpwworld.py\n+++ /dev/null\n@@ -1,82 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_urlparse\n-from ..utils import (\n-    get_element_by_class,\n-    urlencode_postdata,\n-)\n-\n-\n-class NJPWWorldIE(InfoExtractor):\n-    _VALID_URL = r'https?://(front\\.)?njpwworld\\.com/p/(?P<id>[a-z0-9_]+)'\n-    IE_DESC = '\u65b0\u65e5\u672c\u30d7\u30ed\u30ec\u30b9\u30ef\u30fc\u30eb\u30c9'\n-    _NETRC_MACHINE = 'njpwworld'\n-\n-    _TESTS = [{\n-        'url': 'http://njpwworld.com/p/s_series_00155_1_9/',\n-        'info_dict': {\n-            'id': 's_series_00155_1_9',\n-            'ext': 'mp4',\n-            'title': '\u95d8\u5f37\u5c0e\u59222000 2000\u5e741\u67084\u65e5 \u6771\u4eac\u30c9\u30fc\u30e0 \u7b2c9\u8a66\u5408 \u30e9\u30f3\u30c7\u30a3\u30fb\u30b5\u30d9\u30fc\u30b8 VS \u30ea\u30c3\u30af\u30fb\u30b9\u30bf\u30a4\u30ca\u30fc',\n-            'tags': list,\n-        },\n-        'params': {\n-            'skip_download': True,  # AES-encrypted m3u8\n-        },\n-        'skip': 'Requires login',\n-    }, {\n-        'url': 'https://front.njpwworld.com/p/s_series_00563_16_bs',\n-        'info_dict': {\n-            'id': 's_series_00563_16_bs',\n-            'ext': 'mp4',\n-            'title': 'WORLD TAG LEAGUE 2020 & BEST OF THE SUPER Jr.27 2020\u5e7412\u67086\u65e5 \u798f\u5ca1\u30fb\u798f\u5ca1\u56fd\u969b\u30bb\u30f3\u30bf\u30fc \u30d0\u30c3\u30af\u30b9\u30c6\u30fc\u30b8\u30b3\u30e1\u30f3\u30c8\uff08\u5b57\u5e55\u3042\u308a\uff09',\n-            'tags': [\"\u798f\u5ca1\u30fb\u798f\u5ca1\u56fd\u969b\u30bb\u30f3\u30bf\u30fc\", \"\u30d0\u30c3\u30af\u30b9\u30c6\u30fc\u30b8\u30b3\u30e1\u30f3\u30c8\", \"2020\", \"20\u5e74\u4ee3\"],\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }]\n-\n-    _LOGIN_URL = 'https://front.njpwworld.com/auth/login'\n-\n-    def _perform_login(self, username, password):\n-        # Setup session (will set necessary cookies)\n-        self._request_webpage(\n-            'https://njpwworld.com/', None, note='Setting up session')\n-\n-        webpage, urlh = self._download_webpage_handle(\n-            self._LOGIN_URL, None,\n-            note='Logging in', errnote='Unable to login',\n-            data=urlencode_postdata({'login_id': username, 'pw': password}),\n-            headers={'Referer': 'https://front.njpwworld.com/auth'})\n-        # /auth/login will return 302 for successful logins\n-        if urlh.url == self._LOGIN_URL:\n-            self.report_warning('unable to login')\n-            return False\n-\n-        return True\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        formats = []\n-        for kind, vid in re.findall(r'if\\s+\\(\\s*imageQualityType\\s*==\\s*\\'([^\\']+)\\'\\s*\\)\\s*{\\s*video_id\\s*=\\s*\"(\\d+)\"', webpage):\n-            player_path = '/intent?id=%s&type=url' % vid\n-            player_url = compat_urlparse.urljoin(url, player_path)\n-            formats += self._extract_m3u8_formats(\n-                player_url, video_id, 'mp4', 'm3u8_native', m3u8_id=kind, fatal=False, quality=int(kind == 'high'))\n-\n-        tag_block = get_element_by_class('tag-block', webpage)\n-        tags = re.findall(\n-            r'<a[^>]+class=\"tag-[^\"]+\"[^>]*>([^<]+)</a>', tag_block\n-        ) if tag_block else None\n-\n-        return {\n-            'id': video_id,\n-            'title': get_element_by_class('article-title', webpage) or self._og_search_title(webpage),\n-            'formats': formats,\n-            'tags': tags,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/noodlemagazine.py",
            "diff": "diff --git a/yt_dlp/extractor/noodlemagazine.py b/yt_dlp/extractor/noodlemagazine.py\nindex e6208956..1c1a763d 100644\n--- a/yt_dlp/extractor/noodlemagazine.py\n+++ b/yt_dlp/extractor/noodlemagazine.py\n@@ -1,9 +1,12 @@\n from .common import InfoExtractor\n from ..utils import (\n-    parse_duration,\n+    int_or_none,\n     parse_count,\n-    unified_strdate\n+    parse_duration,\n+    unified_strdate,\n+    urljoin,\n )\n+from ..utils.traversal import traverse_obj\n \n \n class NoodleMagazineIE(InfoExtractor):\n@@ -37,21 +40,36 @@ def _real_extract(self, url):\n         like_count = parse_count(self._html_search_meta('ya:ovs:likes', webpage, default=None))\n         upload_date = unified_strdate(self._html_search_meta('ya:ovs:upload_date', webpage, default=''))\n \n-        key = self._html_search_regex(rf'/{video_id}\\?(?:.*&)?m=([^&\"\\'\\s,]+)', webpage, 'key')\n-        playlist_info = self._download_json(f'https://adult.noodlemagazine.com/playlist/{video_id}?m={key}', video_id)\n-        thumbnail = self._og_search_property('image', webpage, default=None) or playlist_info.get('image')\n+        def build_url(url_or_path):\n+            return urljoin('https://adult.noodlemagazine.com', url_or_path)\n+\n+        headers = {'Referer': url}\n+        player_path = self._html_search_regex(\n+            r'<iframe[^>]+\\bid=\"iplayer\"[^>]+\\bsrc=\"([^\"]+)\"', webpage, 'player path')\n+        player_iframe = self._download_webpage(\n+            build_url(player_path), video_id, 'Downloading iframe page', headers=headers)\n+        playlist_url = self._search_regex(\n+            r'window\\.playlistUrl\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', player_iframe, 'playlist url')\n+        playlist_info = self._download_json(build_url(playlist_url), video_id, headers=headers)\n \n-        formats = [{\n-            'url': source.get('file'),\n-            'quality': source.get('label'),\n-            'ext': source.get('type'),\n-        } for source in playlist_info.get('sources')]\n+        formats = []\n+        for source in traverse_obj(playlist_info, ('sources', lambda _, v: v['file'])):\n+            if source.get('type') == 'hls':\n+                formats.extend(self._extract_m3u8_formats(\n+                    build_url(source['file']), video_id, 'mp4', fatal=False, m3u8_id='hls'))\n+            else:\n+                formats.append(traverse_obj(source, {\n+                    'url': ('file', {build_url}),\n+                    'format_id': 'label',\n+                    'height': ('label', {int_or_none}),\n+                    'ext': 'type',\n+                }))\n \n         return {\n             'id': video_id,\n             'formats': formats,\n             'title': title,\n-            'thumbnail': thumbnail,\n+            'thumbnail': self._og_search_property('image', webpage, default=None) or playlist_info.get('image'),\n             'duration': duration,\n             'description': description,\n             'tags': tags,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/normalboots.py",
            "diff": "diff --git a/yt_dlp/extractor/normalboots.py b/yt_dlp/extractor/normalboots.py\ndeleted file mode 100644\nindex 07babcd2..00000000\n--- a/yt_dlp/extractor/normalboots.py\n+++ /dev/null\n@@ -1,51 +0,0 @@\n-from .common import InfoExtractor\n-from .jwplatform import JWPlatformIE\n-\n-from ..utils import (\n-    unified_strdate,\n-)\n-\n-\n-class NormalbootsIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?normalboots\\.com/video/(?P<id>[0-9a-z-]*)/?$'\n-    _TEST = {\n-        'url': 'http://normalboots.com/video/home-alone-games-jontron/',\n-        'info_dict': {\n-            'id': 'home-alone-games-jontron',\n-            'ext': 'mp4',\n-            'title': 'Home Alone Games - JonTron - NormalBoots',\n-            'description': 'Jon is late for Christmas. Typical. Thanks to: Paul Ritchey for Co-Writing/Filming: http://www.youtube.com/user/ContinueShow Michael Azzi for Christmas Intro Animation: http://michafrar.tumblr.com/ Jerrod Waters for Christmas Intro Music: http://www.youtube.com/user/xXJerryTerryXx Casey Ormond for \u2018Tense Battle Theme\u2019:\\xa0http://www.youtube.com/Kiamet/',\n-            'uploader': 'JonTron',\n-            'upload_date': '20140125',\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n-        'add_ie': ['JWPlatform'],\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        video_uploader = self._html_search_regex(\n-            r'Posted\\sby\\s<a\\shref=\"[A-Za-z0-9/]*\">(?P<uploader>[A-Za-z]*)\\s</a>',\n-            webpage, 'uploader', fatal=False)\n-        video_upload_date = unified_strdate(self._html_search_regex(\n-            r'<span style=\"text-transform:uppercase; font-size:inherit;\">[A-Za-z]+, (?P<date>.*)</span>',\n-            webpage, 'date', fatal=False))\n-\n-        jwplatform_url = JWPlatformIE._extract_url(webpage)\n-\n-        return {\n-            '_type': 'url_transparent',\n-            'id': video_id,\n-            'url': jwplatform_url,\n-            'ie_key': JWPlatformIE.ie_key(),\n-            'title': self._og_search_title(webpage),\n-            'description': self._og_search_description(webpage),\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-            'uploader': video_uploader,\n-            'upload_date': video_upload_date,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nosvideo.py",
            "diff": "diff --git a/yt_dlp/extractor/nosvideo.py b/yt_dlp/extractor/nosvideo.py\ndeleted file mode 100644\nindex 7e9688c0..00000000\n--- a/yt_dlp/extractor/nosvideo.py\n+++ /dev/null\n@@ -1,72 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..networking import Request\n-from ..utils import (\n-    ExtractorError,\n-    urlencode_postdata,\n-    xpath_text,\n-    xpath_with_ns,\n-)\n-\n-_x = lambda p: xpath_with_ns(p, {'xspf': 'http://xspf.org/ns/0/'})\n-\n-\n-class NosVideoIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?nosvideo\\.com/' + \\\n-                 r'(?:embed/|\\?v=)(?P<id>[A-Za-z0-9]{12})/?'\n-    _PLAYLIST_URL = 'http://nosvideo.com/xml/{xml_id:s}.xml'\n-    _FILE_DELETED_REGEX = r'<b>File Not Found</b>'\n-    _TEST = {\n-        'url': 'http://nosvideo.com/?v=mu8fle7g7rpq',\n-        'md5': '6124ed47130d8be3eacae635b071e6b6',\n-        'info_dict': {\n-            'id': 'mu8fle7g7rpq',\n-            'ext': 'mp4',\n-            'title': 'big_buck_bunny_480p_surround-fix.avi.mp4',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        fields = {\n-            'id': video_id,\n-            'op': 'download1',\n-            'method_free': 'Continue to Video',\n-        }\n-        req = Request(url, urlencode_postdata(fields))\n-        req.headers['Content-type'] = 'application/x-www-form-urlencoded'\n-        webpage = self._download_webpage(req, video_id,\n-                                         'Downloading download page')\n-        if re.search(self._FILE_DELETED_REGEX, webpage) is not None:\n-            raise ExtractorError('Video %s does not exist' % video_id,\n-                                 expected=True)\n-\n-        xml_id = self._search_regex(r'php\\|([^\\|]+)\\|', webpage, 'XML ID')\n-        playlist_url = self._PLAYLIST_URL.format(xml_id=xml_id)\n-        playlist = self._download_xml(playlist_url, video_id)\n-\n-        track = playlist.find(_x('.//xspf:track'))\n-        if track is None:\n-            raise ExtractorError(\n-                'XML playlist is missing the \\'track\\' element',\n-                expected=True)\n-        title = xpath_text(track, _x('./xspf:title'), 'title')\n-        url = xpath_text(track, _x('./xspf:file'), 'URL', fatal=True)\n-        thumbnail = xpath_text(track, _x('./xspf:image'), 'thumbnail')\n-        if title is not None:\n-            title = title.strip()\n-\n-        formats = [{\n-            'format_id': 'sd',\n-            'url': url,\n-        }]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nova.py",
            "diff": "diff --git a/yt_dlp/extractor/nova.py b/yt_dlp/extractor/nova.py\nindex 8bd3fd47..8a7dfcee 100644\n--- a/yt_dlp/extractor/nova.py\n+++ b/yt_dlp/extractor/nova.py\n@@ -6,7 +6,6 @@\n     determine_ext,\n     int_or_none,\n     js_to_json,\n-    qualities,\n     traverse_obj,\n     unified_strdate,\n     url_or_none,\n@@ -14,7 +13,7 @@\n \n \n class NovaEmbedIE(InfoExtractor):\n-    _VALID_URL = r'https?://media\\.cms\\.nova\\.cz/embed/(?P<id>[^/?#&]+)'\n+    _VALID_URL = r'https?://media(?:tn)?\\.cms\\.nova\\.cz/embed/(?P<id>[^/?#&]+)'\n     _TESTS = [{\n         'url': 'https://media.cms.nova.cz/embed/8o0n0r?autoplay=1',\n         'info_dict': {\n@@ -38,6 +37,16 @@ class NovaEmbedIE(InfoExtractor):\n             'duration': 114,\n         },\n         'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://mediatn.cms.nova.cz/embed/EU5ELEsmOHt?autoplay=1',\n+        'info_dict': {\n+            'id': 'EU5ELEsmOHt',\n+            'ext': 'mp4',\n+            'title': 'Haptick\u00e9 k\u0159eslo, bionick\u00e1 ruka nebo roboti. Report\u00e9rka se pod\u00edvala na T\u00fdden inovac\u00ed',\n+            'thumbnail': r're:^https?://.*\\.jpg',\n+            'duration': 1780,\n+        },\n+        'params': {'skip_download': 'm3u8'},\n     }]\n \n     def _real_extract(self, url):\n@@ -49,77 +58,52 @@ def _real_extract(self, url):\n         duration = None\n         formats = []\n \n-        player = self._parse_json(\n-            self._search_regex(\n-                (r'(?:(?:replacePlaceholders|processAdTagModifier).*?:\\s*)?(?:replacePlaceholders|processAdTagModifier)\\s*\\(\\s*(?P<json>{.*?})\\s*\\)(?:\\s*\\))?\\s*,',\n-                    r'Player\\.init\\s*\\([^,]+,(?P<cndn>\\s*\\w+\\s*\\?)?\\s*(?P<json>{(?(cndn).+?|.+)})\\s*(?(cndn):|,\\s*{.+?}\\s*\\)\\s*;)'),\n-                webpage, 'player', default='{}', group='json'), video_id, fatal=False)\n+        def process_format_list(format_list, format_id=\"\"):\n+            nonlocal formats, has_drm\n+            if not isinstance(format_list, list):\n+                format_list = [format_list]\n+            for format_dict in format_list:\n+                if not isinstance(format_dict, dict):\n+                    continue\n+                if (not self.get_param('allow_unplayable_formats')\n+                        and traverse_obj(format_dict, ('drm', 'keySystem'))):\n+                    has_drm = True\n+                    continue\n+                format_url = url_or_none(format_dict.get('src'))\n+                format_type = format_dict.get('type')\n+                ext = determine_ext(format_url)\n+                if (format_type == 'application/x-mpegURL'\n+                        or format_id == 'HLS' or ext == 'm3u8'):\n+                    formats.extend(self._extract_m3u8_formats(\n+                        format_url, video_id, 'mp4',\n+                        entry_protocol='m3u8_native', m3u8_id='hls',\n+                        fatal=False))\n+                elif (format_type == 'application/dash+xml'\n+                      or format_id == 'DASH' or ext == 'mpd'):\n+                    formats.extend(self._extract_mpd_formats(\n+                        format_url, video_id, mpd_id='dash', fatal=False))\n+                else:\n+                    formats.append({\n+                        'url': format_url,\n+                    })\n+\n+        player = self._search_json(\n+            r'player:', webpage, 'player', video_id, fatal=False, end_pattern=r';\\s*</script>')\n         if player:\n-            for format_id, format_list in player['tracks'].items():\n-                if not isinstance(format_list, list):\n-                    format_list = [format_list]\n-                for format_dict in format_list:\n-                    if not isinstance(format_dict, dict):\n-                        continue\n-                    if (not self.get_param('allow_unplayable_formats')\n-                            and traverse_obj(format_dict, ('drm', 'keySystem'))):\n-                        has_drm = True\n-                        continue\n-                    format_url = url_or_none(format_dict.get('src'))\n-                    format_type = format_dict.get('type')\n-                    ext = determine_ext(format_url)\n-                    if (format_type == 'application/x-mpegURL'\n-                            or format_id == 'HLS' or ext == 'm3u8'):\n-                        formats.extend(self._extract_m3u8_formats(\n-                            format_url, video_id, 'mp4',\n-                            entry_protocol='m3u8_native', m3u8_id='hls',\n-                            fatal=False))\n-                    elif (format_type == 'application/dash+xml'\n-                          or format_id == 'DASH' or ext == 'mpd'):\n-                        formats.extend(self._extract_mpd_formats(\n-                            format_url, video_id, mpd_id='dash', fatal=False))\n-                    else:\n-                        formats.append({\n-                            'url': format_url,\n-                        })\n-            duration = int_or_none(player.get('duration'))\n-        else:\n-            # Old path, not actual as of 08.04.2020\n-            bitrates = self._parse_json(\n+            for src in traverse_obj(player, ('lib', 'source', 'sources', ...)):\n+                process_format_list(src)\n+            duration = traverse_obj(player, ('sourceInfo', 'duration', {int_or_none}))\n+        if not formats and not has_drm:\n+            # older code path, in use before August 2023\n+            player = self._parse_json(\n                 self._search_regex(\n-                    r'(?s)(?:src|bitrates)\\s*=\\s*({.+?})\\s*;', webpage, 'formats'),\n-                video_id, transform_source=js_to_json)\n-\n-            QUALITIES = ('lq', 'mq', 'hq', 'hd')\n-            quality_key = qualities(QUALITIES)\n-\n-            for format_id, format_list in bitrates.items():\n-                if not isinstance(format_list, list):\n-                    format_list = [format_list]\n-                for format_url in format_list:\n-                    format_url = url_or_none(format_url)\n-                    if not format_url:\n-                        continue\n-                    if format_id == 'hls':\n-                        formats.extend(self._extract_m3u8_formats(\n-                            format_url, video_id, ext='mp4',\n-                            entry_protocol='m3u8_native', m3u8_id='hls',\n-                            fatal=False))\n-                        continue\n-                    f = {\n-                        'url': format_url,\n-                    }\n-                    f_id = format_id\n-                    for quality in QUALITIES:\n-                        if '%s.mp4' % quality in format_url:\n-                            f_id += '-%s' % quality\n-                            f.update({\n-                                'quality': quality_key(quality),\n-                                'format_note': quality.upper(),\n-                            })\n-                            break\n-                    f['format_id'] = f_id\n-                    formats.append(f)\n+                    (r'(?:(?:replacePlaceholders|processAdTagModifier).*?:\\s*)?(?:replacePlaceholders|processAdTagModifier)\\s*\\(\\s*(?P<json>{.*?})\\s*\\)(?:\\s*\\))?\\s*,',\n+                     r'Player\\.init\\s*\\([^,]+,(?P<cndn>\\s*\\w+\\s*\\?)?\\s*(?P<json>{(?(cndn).+?|.+)})\\s*(?(cndn):|,\\s*{.+?}\\s*\\)\\s*;)'),\n+                    webpage, 'player', group='json'), video_id)\n+            if player:\n+                for format_id, format_list in player['tracks'].items():\n+                    process_format_list(format_list, format_id)\n+                duration = int_or_none(player.get('duration'))\n \n         if not formats and has_drm:\n             self.report_drm(video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/novaplay.py",
            "diff": "diff --git a/yt_dlp/extractor/novaplay.py b/yt_dlp/extractor/novaplay.py\nindex 92d1d136..d8849cd8 100644\n--- a/yt_dlp/extractor/novaplay.py\n+++ b/yt_dlp/extractor/novaplay.py\n@@ -3,7 +3,7 @@\n \n \n class NovaPlayIE(InfoExtractor):\n-    _VALID_URL = r'https://play.nova\\.bg/video/.*/(?P<id>\\d+)'\n+    _VALID_URL = r'https://play\\.nova\\.bg/video/[^?#]+/(?P<id>\\d+)'\n     _TESTS = [\n         {\n             'url': 'https://play.nova.bg/video/ochakvaite/season-0/ochakvaite-2022-07-22-sybudi-se-sat/606627',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/npo.py",
            "diff": "diff --git a/yt_dlp/extractor/npo.py b/yt_dlp/extractor/npo.py\nindex 40fee24d..4d5ff50d 100644\n--- a/yt_dlp/extractor/npo.py\n+++ b/yt_dlp/extractor/npo.py\n@@ -245,7 +245,7 @@ def _real_extract(self, url):\n                     'quality': 'npoplus',\n                     'tokenId': player_token,\n                     'streamType': 'broadcast',\n-                })\n+                }, data=b'')  # endpoint requires POST\n             if not streams:\n                 continue\n             stream = streams.get('stream')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nrl.py",
            "diff": "diff --git a/yt_dlp/extractor/nrl.py b/yt_dlp/extractor/nrl.py\nindex 798d0341..1e8cf0b7 100644\n--- a/yt_dlp/extractor/nrl.py\n+++ b/yt_dlp/extractor/nrl.py\n@@ -2,6 +2,7 @@\n \n \n class NRLTVIE(InfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?nrl\\.com/tv(/[^/]+)*/(?P<id>[^/?&#]+)'\n     _TEST = {\n         'url': 'https://www.nrl.com/tv/news/match-highlights-titans-v-knights-862805/',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ntvde.py",
            "diff": "diff --git a/yt_dlp/extractor/ntvde.py b/yt_dlp/extractor/ntvde.py\nindex 6d7ea3d1..9f3a498a 100644\n--- a/yt_dlp/extractor/ntvde.py\n+++ b/yt_dlp/extractor/ntvde.py\n@@ -1,21 +1,21 @@\n import re\n \n from .common import InfoExtractor\n-from ..compat import compat_urlparse\n from ..utils import (\n     int_or_none,\n     js_to_json,\n-    parse_duration,\n+    url_or_none,\n )\n+from ..utils.traversal import traverse_obj\n \n \n class NTVDeIE(InfoExtractor):\n     IE_NAME = 'n-tv.de'\n-    _VALID_URL = r'https?://(?:www\\.)?n-tv\\.de/mediathek/videos/[^/?#]+/[^/?#]+-article(?P<id>.+)\\.html'\n+    _VALID_URL = r'https?://(?:www\\.)?n-tv\\.de/mediathek/(?:videos|magazine)/[^/?#]+/[^/?#]+-article(?P<id>[^/?#]+)\\.html'\n \n     _TESTS = [{\n         'url': 'http://www.n-tv.de/mediathek/videos/panorama/Schnee-und-Glaette-fuehren-zu-zahlreichen-Unfaellen-und-Staus-article14438086.html',\n-        'md5': '6ef2514d4b1e8e03ca24b49e2f167153',\n+        'md5': '6bcf2a6638cb83f45d5561659a1cb498',\n         'info_dict': {\n             'id': '14438086',\n             'ext': 'mp4',\n@@ -23,51 +23,61 @@ class NTVDeIE(InfoExtractor):\n             'title': 'Schnee und Gl\u00e4tte f\u00fchren zu zahlreichen Unf\u00e4llen und Staus',\n             'alt_title': 'Winterchaos auf deutschen Stra\u00dfen',\n             'description': 'Schnee und Gl\u00e4tte sorgen deutschlandweit f\u00fcr einen chaotischen Start in die Woche: Auf den Stra\u00dfen kommt es zu kilometerlangen Staus und Dutzenden Gl\u00e4tteunf\u00e4llen. In D\u00fcsseldorf und M\u00fcnchen wirbelt der Schnee zudem den Flugplan durcheinander. Dutzende Fl\u00fcge landen zu sp\u00e4t, einige fallen ganz aus.',\n-            'duration': 4020,\n+            'duration': 67,\n             'timestamp': 1422892797,\n             'upload_date': '20150202',\n         },\n+    }, {\n+        'url': 'https://www.n-tv.de/mediathek/magazine/auslandsreport/Juedische-Siedler-wollten-Rache-die-wollten-nur-toeten-article24523089.html',\n+        'md5': 'c5c6014c014ccc3359470e1d34472bfd',\n+        'info_dict': {\n+            'id': '24523089',\n+            'ext': 'mp4',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'title': 'J\u00fcdische Siedler \"wollten Rache, die wollten nur t\u00f6ten\"',\n+            'alt_title': 'Israelische Gewalt fern von Gaza',\n+            'description': 'Vier Tage nach dem Massaker der Hamas greifen j\u00fcdische Siedler das Haus einer pal\u00e4stinensischen Familie im Westjordanland an. Die \u00dcberlebenden berichten, sie waren unbewaffnet, die Angreifer seien nur auf \"Rache und T\u00f6ten\" aus gewesen. Als die Toten beerdigt werden sollen, er\u00f6ffnen die Siedler erneut das Feuer.',\n+            'duration': 326,\n+            'timestamp': 1699688294,\n+            'upload_date': '20231111',\n+        },\n     }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(url, video_id)\n \n-        info = self._parse_json(self._search_regex(\n-            r'(?s)ntv\\.pageInfo\\.article\\s*=\\s*(\\{.*?\\});', webpage, 'info'),\n-            video_id, transform_source=js_to_json)\n-        timestamp = int_or_none(info.get('publishedDateAsUnixTimeStamp'))\n-        vdata = self._parse_json(self._search_regex(\n-            r'(?s)\\$\\(\\s*\"\\#player\"\\s*\\)\\s*\\.data\\(\\s*\"player\",\\s*(\\{.*?\\})\\);',\n-            webpage, 'player data'), video_id,\n-            transform_source=lambda s: js_to_json(re.sub(r'advertising:\\s*{[^}]+},', '', s)))\n-        duration = parse_duration(vdata.get('duration'))\n+        info = self._search_json(\n+            r'article:', webpage, 'info', video_id, transform_source=js_to_json)\n+\n+        vdata = self._search_json(\n+            r'\\$\\(\\s*\"#playerwrapper\"\\s*\\)\\s*\\.data\\(\\s*\"player\",',\n+            webpage, 'player data', video_id,\n+            transform_source=lambda s: js_to_json(re.sub(r'ivw:[^},]+', '', s)))['setup']['source']\n \n         formats = []\n-        if vdata.get('video'):\n-            formats.append({\n-                'format_id': 'flash',\n-                'url': 'rtmp://fms.n-tv.de/%s' % vdata['video'],\n-            })\n-        if vdata.get('videoMp4'):\n+        if vdata.get('progressive'):\n             formats.append({\n-                'format_id': 'mobile',\n-                'url': compat_urlparse.urljoin('http://video.n-tv.de', vdata['videoMp4']),\n-                'tbr': 400,  # estimation\n+                'format_id': 'http',\n+                'url': vdata['progressive'],\n             })\n-        if vdata.get('videoM3u8'):\n-            m3u8_url = compat_urlparse.urljoin('http://video.n-tv.de', vdata['videoM3u8'])\n+        if vdata.get('hls'):\n             formats.extend(self._extract_m3u8_formats(\n-                m3u8_url, video_id, ext='mp4', entry_protocol='m3u8_native',\n-                quality=1, m3u8_id='hls', fatal=False))\n+                vdata['hls'], video_id, 'mp4', m3u8_id='hls', fatal=False))\n+        if vdata.get('dash'):\n+            formats.extend(self._extract_mpd_formats(vdata['dash'], video_id, fatal=False, mpd_id='dash'))\n \n         return {\n             'id': video_id,\n-            'title': info['headline'],\n-            'description': info.get('intro'),\n-            'alt_title': info.get('kicker'),\n-            'timestamp': timestamp,\n-            'thumbnail': vdata.get('html5VideoPoster'),\n-            'duration': duration,\n+            **traverse_obj(info, {\n+                'title': 'headline',\n+                'description': 'intro',\n+                'alt_title': 'kicker',\n+                'timestamp': ('publishedDateAsUnixTimeStamp', {int_or_none}),\n+            }),\n+            **traverse_obj(vdata, {\n+                'thumbnail': ('poster', {url_or_none}),\n+                'duration': ('length', {int_or_none}),\n+            }),\n             'formats': formats,\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/nubilesporn.py",
            "diff": "diff --git a/yt_dlp/extractor/nubilesporn.py b/yt_dlp/extractor/nubilesporn.py\nindex d4f1d9d6..1d630f54 100644\n--- a/yt_dlp/extractor/nubilesporn.py\n+++ b/yt_dlp/extractor/nubilesporn.py\n@@ -19,7 +19,7 @@\n class NubilesPornIE(InfoExtractor):\n     _NETRC_MACHINE = 'nubiles-porn'\n     _VALID_URL = r'''(?x)\n-        https://members.nubiles-porn.com/video/watch/(?P<id>\\d+)\n+        https://members\\.nubiles-porn\\.com/video/watch/(?P<id>\\d+)\n         (?:/(?P<display_id>[\\w\\-]+-s(?P<season>\\d+)e(?P<episode>\\d+)))?\n     '''\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/oftv.py",
            "diff": "diff --git a/yt_dlp/extractor/oftv.py b/yt_dlp/extractor/oftv.py\nindex 3ae7278f..4cac5184 100644\n--- a/yt_dlp/extractor/oftv.py\n+++ b/yt_dlp/extractor/oftv.py\n@@ -4,7 +4,7 @@\n \n \n class OfTVIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?of.tv/video/(?P<id>\\w+)'\n+    _VALID_URL = r'https?://(?:www\\.)?of\\.tv/video/(?P<id>\\w+)'\n     _TESTS = [{\n         'url': 'https://of.tv/video/627d7d95b353db0001dadd1a',\n         'md5': 'cb9cd5db3bb9ee0d32bfd7e373d6ef0a',\n@@ -34,7 +34,7 @@ def _real_extract(self, url):\n \n \n class OfTVPlaylistIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?of.tv/creators/(?P<id>[a-zA-Z0-9-]+)/.?'\n+    _VALID_URL = r'https?://(?:www\\.)?of\\.tv/creators/(?P<id>[a-zA-Z0-9-]+)/?(?:$|[?#])'\n     _TESTS = [{\n         'url': 'https://of.tv/creators/this-is-fire/',\n         'playlist_count': 8,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ondemandkorea.py",
            "diff": "diff --git a/yt_dlp/extractor/ondemandkorea.py b/yt_dlp/extractor/ondemandkorea.py\nindex dd7d1d7d..94fcac72 100644\n--- a/yt_dlp/extractor/ondemandkorea.py\n+++ b/yt_dlp/extractor/ondemandkorea.py\n@@ -1,87 +1,168 @@\n+import functools\n import re\n+import uuid\n \n from .common import InfoExtractor\n from ..utils import (\n     ExtractorError,\n-    js_to_json,\n+    OnDemandPagedList,\n+    float_or_none,\n+    int_or_none,\n+    join_nonempty,\n+    parse_age_limit,\n+    parse_qs,\n+    unified_strdate,\n+    url_or_none,\n )\n+from ..utils.traversal import traverse_obj\n \n \n class OnDemandKoreaIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?ondemandkorea\\.com/(?P<id>[^/]+)\\.html'\n+    _VALID_URL = r'https?://(?:www\\.)?ondemandkorea\\.com/(?:en/)?player/vod/[a-z0-9-]+\\?(?:[^#]+&)?contentId=(?P<id>\\d+)'\n     _GEO_COUNTRIES = ['US', 'CA']\n+\n     _TESTS = [{\n-        'url': 'https://www.ondemandkorea.com/ask-us-anything-e351.html',\n+        'url': 'https://www.ondemandkorea.com/player/vod/ask-us-anything?contentId=686471',\n+        'md5': 'e2ff77255d989e3135bde0c5889fbce8',\n         'info_dict': {\n-            'id': 'ask-us-anything-e351',\n+            'id': '686471',\n             'ext': 'mp4',\n-            'title': 'Ask Us Anything : Jung Sung-ho, Park Seul-gi, Kim Bo-min, Yang Seung-won - 09/24/2022',\n-            'description': 'A talk show/game show with a school theme where celebrity guests appear as \u201ctransfer students.\u201d',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'title': 'Ask Us Anything: Jung Sung-ho, Park Seul-gi, Kim Bo-min, Yang Seung-won',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)',\n+            'duration': 5486.955,\n+            'release_date': '20220924',\n+            'series': 'Ask Us Anything',\n+            'series_id': 11790,\n+            'episode_number': 351,\n+            'episode': 'Jung Sung-ho, Park Seul-gi, Kim Bo-min, Yang Seung-won',\n         },\n-        'params': {\n-            'skip_download': 'm3u8 download'\n-        }\n     }, {\n-        'url': 'https://www.ondemandkorea.com/work-later-drink-now-e1.html',\n+        'url': 'https://www.ondemandkorea.com/player/vod/breakup-probation-a-week?contentId=1595796',\n+        'md5': '57266c720006962be7ff415b24775caa',\n         'info_dict': {\n-            'id': 'work-later-drink-now-e1',\n+            'id': '1595796',\n             'ext': 'mp4',\n-            'title': 'Work Later, Drink Now : E01',\n-            'description': 'Work Later, Drink First follows three women who find solace in a glass of liquor at the end of the day. So-hee, who gets comfort from a cup of soju af',\n-            'thumbnail': r're:^https?://.*\\.png$',\n-            'subtitles': {\n-                'English': 'mincount:1',\n-            },\n+            'title': 'Breakup Probation, A Week: E08',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)',\n+            'duration': 1586.0,\n+            'release_date': '20231001',\n+            'series': 'Breakup Probation, A Week',\n+            'series_id': 22912,\n+            'episode_number': 8,\n+            'episode': 'E08',\n         },\n-        'params': {\n-            'skip_download': 'm3u8 download'\n-        }\n+    }, {\n+        'url': 'https://www.ondemandkorea.com/player/vod/the-outlaws?contentId=369531',\n+        'md5': 'fa5523b87aa1f6d74fc622a97f2b47cd',\n+        'info_dict': {\n+            'id': '369531',\n+            'ext': 'mp4',\n+            'release_date': '20220519',\n+            'duration': 7267.0,\n+            'title': 'The Outlaws: Main Movie',\n+            'thumbnail': r're:^https?://.*\\.(jpg|jpeg|png)',\n+            'age_limit': 18,\n+        },\n+    }, {\n+        'url': 'https://www.ondemandkorea.com/en/player/vod/capture-the-moment-how-is-that-possible?contentId=1605006',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id, fatal=False)\n-\n-        if not webpage:\n-            # Page sometimes returns captcha page with HTTP 403\n-            raise ExtractorError(\n-                'Unable to access page. You may have been blocked.',\n-                expected=True)\n-\n-        if 'msg_block_01.png' in webpage:\n-            self.raise_geo_restricted(\n-                msg='This content is not available in your region',\n-                countries=self._GEO_COUNTRIES)\n-\n-        if 'This video is only available to ODK PLUS members.' in webpage:\n-            raise ExtractorError(\n-                'This video is only available to ODK PLUS members.',\n-                expected=True)\n-\n-        if 'ODK PREMIUM Members Only' in webpage:\n-            raise ExtractorError(\n-                'This video is only available to ODK PREMIUM members.',\n-                expected=True)\n-\n-        title = self._search_regex(\n-            r'class=[\"\\']episode_title[\"\\'][^>]*>([^<]+)',\n-            webpage, 'episode_title', fatal=False) or self._og_search_title(webpage)\n-\n-        jw_config = self._parse_json(\n-            self._search_regex((\n-                r'(?P<options>{\\s*[\\'\"]tracks[\\'\"].*?})[)\\];]+$',\n-                r'playlist\\s*=\\s*\\[(?P<options>.+)];?$',\n-                r'odkPlayer\\.init.*?(?P<options>{[^;]+}).*?;',\n-            ), webpage, 'jw config', flags=re.MULTILINE | re.DOTALL, group='options'),\n-            video_id, transform_source=js_to_json)\n-        info = self._parse_jwplayer_data(\n-            jw_config, video_id, require_title=False, m3u8_id='hls',\n-            base_url=url)\n-\n-        info.update({\n-            'title': title,\n-            'description': self._og_search_description(webpage),\n-            'thumbnail': self._og_search_thumbnail(webpage)\n-        })\n-        return info\n+\n+        data = self._download_json(\n+            f'https://odkmedia.io/odx/api/v3/playback/{video_id}/', video_id, fatal=False,\n+            headers={'service-name': 'odk'}, query={'did': str(uuid.uuid4())}, expected_status=(403, 404))\n+        if not traverse_obj(data, ('result', {dict})):\n+            msg = traverse_obj(data, ('messages', '__default'), 'title', expected_type=str)\n+            raise ExtractorError(msg or 'Got empty response from playback API', expected=True)\n+\n+        data = data['result']\n+\n+        def try_geo_bypass(url):\n+            return traverse_obj(url, ({parse_qs}, 'stream_url', 0, {url_or_none})) or url\n+\n+        formats = []\n+        for m3u8_url in traverse_obj(data, (('sources', 'manifest'), ..., 'url', {url_or_none}, {try_geo_bypass})):\n+            mod_url = re.sub(r'_720(p?)\\.m3u8', r'_1080\\1.m3u8', m3u8_url)\n+            if mod_url != m3u8_url:\n+                mod_format = self._extract_m3u8_formats(\n+                    mod_url, video_id, note='Checking for higher quality format',\n+                    errnote='No higher quality format found', fatal=False)\n+                if mod_format:\n+                    formats.extend(mod_format)\n+                    continue\n+            formats.extend(self._extract_m3u8_formats(m3u8_url, video_id, fatal=False))\n+\n+        subtitles = {}\n+        for track in traverse_obj(data, ('text_tracks', lambda _, v: url_or_none(v['url']))):\n+            subtitles.setdefault(track.get('language', 'und'), []).append({\n+                'url': track['url'],\n+                'ext': track.get('codec'),\n+                'name': track.get('label'),\n+            })\n+\n+        def if_series(key=None):\n+            return lambda obj: obj[key] if key and obj['kind'] == 'series' else None\n+\n+        return {\n+            'id': video_id,\n+            'title': join_nonempty(\n+                ('episode', 'program', 'title'),\n+                ('episode', 'title'), from_dict=data, delim=': '),\n+            **traverse_obj(data, {\n+                'thumbnail': ('episode', 'images', 'thumbnail', {url_or_none}),\n+                'release_date': ('episode', 'release_date', {lambda x: x.replace('-', '')}, {unified_strdate}),\n+                'duration': ('duration', {functools.partial(float_or_none, scale=1000)}),\n+                'age_limit': ('age_rating', 'name', {lambda x: x.replace('R', '')}, {parse_age_limit}),\n+                'series': ('episode', {if_series(key='program')}, 'title'),\n+                'series_id': ('episode', {if_series(key='program')}, 'id'),\n+                'episode': ('episode', {if_series(key='title')}),\n+                'episode_number': ('episode', {if_series(key='number')}, {int_or_none}),\n+            }, get_all=False),\n+            'formats': formats,\n+            'subtitles': subtitles,\n+        }\n+\n+\n+class OnDemandKoreaProgramIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?ondemandkorea\\.com/(?:en/)?player/vod/(?P<id>[a-z0-9-]+)(?:$|#)'\n+    _GEO_COUNTRIES = ['US', 'CA']\n+\n+    _TESTS = [{\n+        'url': 'https://www.ondemandkorea.com/player/vod/uskn-news',\n+        'info_dict': {\n+            'id': 'uskn-news',\n+        },\n+        'playlist_mincount': 755,\n+    }, {\n+        'url': 'https://www.ondemandkorea.com/en/player/vod/the-land',\n+        'info_dict': {\n+            'id': 'the-land',\n+        },\n+        'playlist_count': 52,\n+    }]\n+\n+    _PAGE_SIZE = 100\n+\n+    def _fetch_page(self, display_id, page):\n+        page += 1\n+        page_data = self._download_json(\n+            f'https://odkmedia.io/odx/api/v3/program/{display_id}/episodes/', display_id,\n+            headers={'service-name': 'odk'}, query={\n+                'page': page,\n+                'page_size': self._PAGE_SIZE,\n+            }, note=f'Downloading page {page}', expected_status=404)\n+        for episode in traverse_obj(page_data, ('result', 'results', ...)):\n+            yield self.url_result(\n+                f'https://www.ondemandkorea.com/player/vod/{display_id}?contentId={episode[\"id\"]}',\n+                ie=OnDemandKoreaIE, video_title=episode.get('title'))\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+\n+        entries = OnDemandPagedList(functools.partial(\n+            self._fetch_page, display_id), self._PAGE_SIZE)\n+\n+        return self.playlist_result(entries, display_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ooyala.py",
            "diff": "diff --git a/yt_dlp/extractor/ooyala.py b/yt_dlp/extractor/ooyala.py\ndeleted file mode 100644\nindex 65afccdb..00000000\n--- a/yt_dlp/extractor/ooyala.py\n+++ /dev/null\n@@ -1,230 +0,0 @@\n-import base64\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import (\n-    compat_b64decode,\n-    compat_str,\n-)\n-from ..utils import (\n-    determine_ext,\n-    float_or_none,\n-    int_or_none,\n-    smuggle_url,\n-    try_get,\n-    unsmuggle_url,\n-)\n-\n-\n-class OoyalaBaseIE(InfoExtractor):\n-    _PLAYER_BASE = 'http://player.ooyala.com/'\n-    _CONTENT_TREE_BASE = _PLAYER_BASE + 'player_api/v1/content_tree/'\n-    _AUTHORIZATION_URL_TEMPLATE = _PLAYER_BASE + 'sas/player_api/v2/authorization/embed_code/%s/%s'\n-\n-    def _extract(self, content_tree_url, video_id, domain=None, supportedformats=None, embed_token=None):\n-        content_tree = self._download_json(content_tree_url, video_id)['content_tree']\n-        metadata = content_tree[list(content_tree)[0]]\n-        embed_code = metadata['embed_code']\n-        pcode = metadata.get('asset_pcode') or embed_code\n-        title = metadata['title']\n-\n-        auth_data = self._download_json(\n-            self._AUTHORIZATION_URL_TEMPLATE % (pcode, embed_code),\n-            video_id, headers=self.geo_verification_headers(), query={\n-                'domain': domain or 'player.ooyala.com',\n-                'supportedFormats': supportedformats or 'mp4,rtmp,m3u8,hds,dash,smooth',\n-                'embedToken': embed_token,\n-            })['authorization_data'][embed_code]\n-\n-        urls = []\n-        formats = []\n-        streams = auth_data.get('streams') or [{\n-            'delivery_type': 'hls',\n-            'url': {\n-                'data': base64.b64encode(('http://player.ooyala.com/hls/player/all/%s.m3u8' % embed_code).encode()).decode(),\n-            }\n-        }]\n-        for stream in streams:\n-            url_data = try_get(stream, lambda x: x['url']['data'], compat_str)\n-            if not url_data:\n-                continue\n-            s_url = compat_b64decode(url_data).decode('utf-8')\n-            if not s_url or s_url in urls:\n-                continue\n-            urls.append(s_url)\n-            ext = determine_ext(s_url, None)\n-            delivery_type = stream.get('delivery_type')\n-            if delivery_type == 'hls' or ext == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    re.sub(r'/ip(?:ad|hone)/', '/all/', s_url), embed_code, 'mp4', 'm3u8_native',\n-                    m3u8_id='hls', fatal=False))\n-            elif delivery_type == 'hds' or ext == 'f4m':\n-                formats.extend(self._extract_f4m_formats(\n-                    s_url + '?hdcore=3.7.0', embed_code, f4m_id='hds', fatal=False))\n-            elif delivery_type == 'dash' or ext == 'mpd':\n-                formats.extend(self._extract_mpd_formats(\n-                    s_url, embed_code, mpd_id='dash', fatal=False))\n-            elif delivery_type == 'smooth':\n-                self._extract_ism_formats(\n-                    s_url, embed_code, ism_id='mss', fatal=False)\n-            elif ext == 'smil':\n-                formats.extend(self._extract_smil_formats(\n-                    s_url, embed_code, fatal=False))\n-            else:\n-                formats.append({\n-                    'url': s_url,\n-                    'ext': ext or delivery_type,\n-                    'vcodec': stream.get('video_codec'),\n-                    'format_id': delivery_type,\n-                    'width': int_or_none(stream.get('width')),\n-                    'height': int_or_none(stream.get('height')),\n-                    'abr': int_or_none(stream.get('audio_bitrate')),\n-                    'vbr': int_or_none(stream.get('video_bitrate')),\n-                    'fps': float_or_none(stream.get('framerate')),\n-                })\n-        if not formats and not auth_data.get('authorized'):\n-            self.raise_no_formats('%s said: %s' % (\n-                self.IE_NAME, auth_data['message']), expected=True)\n-\n-        subtitles = {}\n-        for lang, sub in metadata.get('closed_captions_vtt', {}).get('captions', {}).items():\n-            sub_url = sub.get('url')\n-            if not sub_url:\n-                continue\n-            subtitles[lang] = [{\n-                'url': sub_url,\n-            }]\n-\n-        return {\n-            'id': embed_code,\n-            'title': title,\n-            'description': metadata.get('description'),\n-            'thumbnail': metadata.get('thumbnail_image') or metadata.get('promo_image'),\n-            'duration': float_or_none(metadata.get('duration'), 1000),\n-            'subtitles': subtitles,\n-            'formats': formats,\n-        }\n-\n-\n-class OoyalaIE(OoyalaBaseIE):\n-    _VALID_URL = r'(?:ooyala:|https?://.+?\\.ooyala\\.com/.*?(?:embedCode|ec)=)(?P<id>.+?)(&|$)'\n-\n-    _TESTS = [\n-        {\n-            # From http://it.slashdot.org/story/13/04/25/178216/recovering-data-from-broken-hard-drives-and-ssds-video\n-            'url': 'http://player.ooyala.com/player.js?embedCode=pxczE2YjpfHfn1f3M-ykG_AmJRRn0PD8',\n-            'info_dict': {\n-                'id': 'pxczE2YjpfHfn1f3M-ykG_AmJRRn0PD8',\n-                'ext': 'mp4',\n-                'title': 'Explaining Data Recovery from Hard Drives and SSDs',\n-                'description': 'How badly damaged does a drive have to be to defeat Russell and his crew? Apparently, smashed to bits.',\n-                'duration': 853.386,\n-            },\n-            # The video in the original webpage now uses PlayWire\n-            'skip': 'Ooyala said: movie expired',\n-        }, {\n-            # Only available for ipad\n-            'url': 'http://player.ooyala.com/player.js?embedCode=x1b3lqZDq9y_7kMyC2Op5qo-p077tXD0',\n-            'info_dict': {\n-                'id': 'x1b3lqZDq9y_7kMyC2Op5qo-p077tXD0',\n-                'ext': 'mp4',\n-                'title': 'Simulation Overview - Levels of Simulation',\n-                'duration': 194.948,\n-            },\n-        },\n-        {\n-            # Information available only through SAS api\n-            # From http://community.plm.automation.siemens.com/t5/News-NX-Manufacturing/Tool-Path-Divide/ba-p/4187\n-            'url': 'http://player.ooyala.com/player.js?embedCode=FiOG81ZTrvckcchQxmalf4aQj590qTEx',\n-            'md5': 'a84001441b35ea492bc03736e59e7935',\n-            'info_dict': {\n-                'id': 'FiOG81ZTrvckcchQxmalf4aQj590qTEx',\n-                'ext': 'mp4',\n-                'title': 'Divide Tool Path.mp4',\n-                'duration': 204.405,\n-            }\n-        },\n-        {\n-            # empty stream['url']['data']\n-            'url': 'http://player.ooyala.com/player.js?embedCode=w2bnZtYjE6axZ_dw1Cd0hQtXd_ige2Is',\n-            'only_matching': True,\n-        }\n-    ]\n-\n-    def _extract_from_webpage(self, url, webpage):\n-        mobj = (re.search(r'player\\.ooyala\\.com/[^\"?]+[?#][^\"]*?(?:embedCode|ec)=(?P<ec>[^\"&]+)', webpage)\n-                or re.search(r'OO\\.Player\\.create\\([\\'\"].*?[\\'\"],\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage)\n-                or re.search(r'OO\\.Player\\.create\\.apply\\(\\s*OO\\.Player\\s*,\\s*op\\(\\s*\\[\\s*[\\'\"][^\\'\"]*[\\'\"]\\s*,\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage)\n-                or re.search(r'SBN\\.VideoLinkset\\.ooyala\\([\\'\"](?P<ec>.{32})[\\'\"]\\)', webpage)\n-                or re.search(r'data-ooyala-video-id\\s*=\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage))\n-        if mobj is not None:\n-            embed_token = self._search_regex(\n-                r'embedToken[\\'\"]?\\s*:\\s*[\\'\"]([^\\'\"]+)',\n-                webpage, 'ooyala embed token', default=None)\n-            yield self._build_url_result(smuggle_url(\n-                mobj.group('ec'), {\n-                    'domain': url,\n-                    'embed_token': embed_token,\n-                }))\n-            return\n-\n-        # Look for multiple Ooyala embeds on SBN network websites\n-        mobj = re.search(r'SBN\\.VideoLinkset\\.entryGroup\\((\\[.*?\\])', webpage)\n-        if mobj is not None:\n-            for v in self._parse_json(mobj.group(1), self._generic_id(url), fatal=False) or []:\n-                yield self._build_url_result(smuggle_url(v['provider_video_id'], {'domain': url}))\n-\n-    @staticmethod\n-    def _url_for_embed_code(embed_code):\n-        return 'http://player.ooyala.com/player.js?embedCode=%s' % embed_code\n-\n-    @classmethod\n-    def _build_url_result(cls, embed_code):\n-        return cls.url_result(cls._url_for_embed_code(embed_code),\n-                              ie=cls.ie_key())\n-\n-    def _real_extract(self, url):\n-        url, smuggled_data = unsmuggle_url(url, {})\n-        embed_code = self._match_id(url)\n-        domain = smuggled_data.get('domain')\n-        supportedformats = smuggled_data.get('supportedformats')\n-        embed_token = smuggled_data.get('embed_token')\n-        content_tree_url = self._CONTENT_TREE_BASE + 'embed_code/%s/%s' % (embed_code, embed_code)\n-        return self._extract(content_tree_url, embed_code, domain, supportedformats, embed_token)\n-\n-\n-class OoyalaExternalIE(OoyalaBaseIE):\n-    _VALID_URL = r'''(?x)\n-                    (?:\n-                        ooyalaexternal:|\n-                        https?://.+?\\.ooyala\\.com/.*?\\bexternalId=\n-                    )\n-                    (?P<partner_id>[^:]+)\n-                    :\n-                    (?P<id>.+)\n-                    (?:\n-                        :|\n-                        .*?&pcode=\n-                    )\n-                    (?P<pcode>.+?)\n-                    (?:&|$)\n-                    '''\n-\n-    _TEST = {\n-        'url': 'https://player.ooyala.com/player.js?externalId=espn:10365079&pcode=1kNG061cgaoolOncv54OAO1ceO-I&adSetCode=91cDU6NuXTGKz3OdjOxFdAgJVtQcKJnI&callback=handleEvents&hasModuleParams=1&height=968&playerBrandingId=7af3bd04449c444c964f347f11873075&targetReplaceId=videoPlayer&width=1656&wmode=opaque&allowScriptAccess=always',\n-        'info_dict': {\n-            'id': 'FkYWtmazr6Ed8xmvILvKLWjd4QvYZpzG',\n-            'ext': 'mp4',\n-            'title': 'dm_140128_30for30Shorts___JudgingJewellv2',\n-            'duration': 1302.0,\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        partner_id, video_id, pcode = self._match_valid_url(url).groups()\n-        content_tree_url = self._CONTENT_TREE_BASE + 'external_id/%s/%s:%s' % (pcode, partner_id, video_id)\n-        return self._extract(content_tree_url, video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/orf.py",
            "diff": "diff --git a/yt_dlp/extractor/orf.py b/yt_dlp/extractor/orf.py\nindex cc3c003f..9a48ae1b 100644\n--- a/yt_dlp/extractor/orf.py\n+++ b/yt_dlp/extractor/orf.py\n@@ -4,15 +4,16 @@\n from .common import InfoExtractor\n from ..networking import HEADRequest\n from ..utils import (\n+    InAdvancePagedList,\n     clean_html,\n     determine_ext,\n     float_or_none,\n-    InAdvancePagedList,\n     int_or_none,\n     join_nonempty,\n+    make_archive_id,\n+    mimetype2ext,\n     orderedSet,\n     remove_end,\n-    make_archive_id,\n     smuggle_url,\n     strip_jsonp,\n     try_call,\n@@ -21,6 +22,7 @@\n     unsmuggle_url,\n     url_or_none,\n )\n+from ..utils.traversal import traverse_obj\n \n \n class ORFTVthekIE(InfoExtractor):\n@@ -334,6 +336,45 @@ def _real_extract(self, url):\n             self._entries(data, station or station2), show_id, data.get('title'), clean_html(data.get('subtitle')))\n \n \n+class ORFPodcastIE(InfoExtractor):\n+    IE_NAME = 'orf:podcast'\n+    _STATION_RE = '|'.join(map(re.escape, (\n+        'bgl', 'fm4', 'ktn', 'noe', 'oe1', 'oe3',\n+        'ooe', 'sbg', 'stm', 'tir', 'tv', 'vbg', 'wie')))\n+    _VALID_URL = rf'https?://sound\\.orf\\.at/podcast/(?P<station>{_STATION_RE})/(?P<show>[\\w-]+)/(?P<id>[\\w-]+)'\n+    _TESTS = [{\n+        'url': 'https://sound.orf.at/podcast/oe3/fruehstueck-bei-mir/nicolas-stockhammer-15102023',\n+        'md5': '526a5700e03d271a1505386a8721ab9b',\n+        'info_dict': {\n+            'id': 'nicolas-stockhammer-15102023',\n+            'ext': 'mp3',\n+            'title': 'Nicolas Stockhammer (15.10.2023)',\n+            'duration': 3396.0,\n+            'series': 'Fr\u00fchst\u00fcck bei mir',\n+        },\n+        'skip': 'ORF podcasts are only available for a limited time'\n+    }]\n+\n+    def _real_extract(self, url):\n+        station, show, show_id = self._match_valid_url(url).group('station', 'show', 'id')\n+        data = self._download_json(\n+            f'https://audioapi.orf.at/radiothek/api/2.0/podcast/{station}/{show}/{show_id}', show_id)\n+\n+        return {\n+            'id': show_id,\n+            'ext': 'mp3',\n+            'vcodec': 'none',\n+            **traverse_obj(data, ('payload', {\n+                'url': ('enclosures', 0, 'url'),\n+                'ext': ('enclosures', 0, 'type', {mimetype2ext}),\n+                'title': 'title',\n+                'description': ('description', {clean_html}),\n+                'duration': ('duration', {functools.partial(float_or_none, scale=1000)}),\n+                'series': ('podcast', 'title'),\n+            })),\n+        }\n+\n+\n class ORFIPTVIE(InfoExtractor):\n     IE_NAME = 'orf:iptv'\n     IE_DESC = 'iptv.ORF.at'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/pandoratv.py",
            "diff": "diff --git a/yt_dlp/extractor/pandoratv.py b/yt_dlp/extractor/pandoratv.py\ndeleted file mode 100644\nindex ccc78da5..00000000\n--- a/yt_dlp/extractor/pandoratv.py\n+++ /dev/null\n@@ -1,128 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import (\n-    compat_str,\n-)\n-from ..utils import (\n-    ExtractorError,\n-    float_or_none,\n-    parse_duration,\n-    parse_qs,\n-    str_to_int,\n-    urlencode_postdata,\n-)\n-\n-\n-class PandoraTVIE(InfoExtractor):\n-    IE_NAME = 'pandora.tv'\n-    IE_DESC = '\ud310\ub3c4\ub77cTV'\n-    _VALID_URL = r'''(?x)\n-                        https?://\n-                            (?:\n-                                (?:www\\.)?pandora\\.tv/view/(?P<user_id>[^/]+)/(?P<id>\\d+)|  # new format\n-                                (?:.+?\\.)?channel\\.pandora\\.tv/channel/video\\.ptv\\?|        # old format\n-                                m\\.pandora\\.tv/?\\?                                          # mobile\n-                            )\n-                    '''\n-    _TESTS = [{\n-        'url': 'http://jp.channel.pandora.tv/channel/video.ptv?c1=&prgid=53294230&ch_userid=mikakim&ref=main&lot=cate_01_2',\n-        'info_dict': {\n-            'id': '53294230',\n-            'ext': 'flv',\n-            'title': '\u982d\u3092\u64ab\u3067\u3066\u304f\u308c\u308b\uff1f',\n-            'description': '\u982d\u3092\u64ab\u3067\u3066\u304f\u308c\u308b\uff1f',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 39,\n-            'upload_date': '20151218',\n-            'uploader': '\u30ab\u30ef\u30a4\u30a4\u52d5\u7269\u307e\u3068\u3081',\n-            'uploader_id': 'mikakim',\n-            'view_count': int,\n-            'like_count': int,\n-        }\n-    }, {\n-        'url': 'http://channel.pandora.tv/channel/video.ptv?ch_userid=gogoucc&prgid=54721744',\n-        'info_dict': {\n-            'id': '54721744',\n-            'ext': 'flv',\n-            'title': '[HD] JAPAN COUNTDOWN 170423',\n-            'description': '[HD] JAPAN COUNTDOWN 170423',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 1704.9,\n-            'upload_date': '20170423',\n-            'uploader': 'GOGO_UCC',\n-            'uploader_id': 'gogoucc',\n-            'view_count': int,\n-            'like_count': int,\n-        },\n-        'params': {\n-            # Test metadata only\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'http://www.pandora.tv/view/mikakim/53294230#36797454_new',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://m.pandora.tv/?c=view&ch_userid=mikakim&prgid=54600346',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        user_id = mobj.group('user_id')\n-        video_id = mobj.group('id')\n-\n-        if not user_id or not video_id:\n-            qs = parse_qs(url)\n-            video_id = qs.get('prgid', [None])[0]\n-            user_id = qs.get('ch_userid', [None])[0]\n-            if any(not f for f in (video_id, user_id,)):\n-                raise ExtractorError('Invalid URL', expected=True)\n-\n-        data = self._download_json(\n-            'http://m.pandora.tv/?c=view&m=viewJsonApi&ch_userid=%s&prgid=%s'\n-            % (user_id, video_id), video_id)\n-\n-        info = data['data']['rows']['vod_play_info']['result']\n-\n-        formats = []\n-        for format_id, format_url in info.items():\n-            if not format_url:\n-                continue\n-            height = self._search_regex(\n-                r'^v(\\d+)[Uu]rl$', format_id, 'height', default=None)\n-            if not height:\n-                continue\n-\n-            play_url = self._download_json(\n-                'http://m.pandora.tv/?c=api&m=play_url', video_id,\n-                data=urlencode_postdata({\n-                    'prgid': video_id,\n-                    'runtime': info.get('runtime'),\n-                    'vod_url': format_url,\n-                }),\n-                headers={\n-                    'Origin': url,\n-                    'Content-Type': 'application/x-www-form-urlencoded',\n-                })\n-            format_url = play_url.get('url')\n-            if not format_url:\n-                continue\n-\n-            formats.append({\n-                'format_id': '%sp' % height,\n-                'url': format_url,\n-                'height': int(height),\n-            })\n-\n-        return {\n-            'id': video_id,\n-            'title': info['subject'],\n-            'description': info.get('body'),\n-            'thumbnail': info.get('thumbnail') or info.get('poster'),\n-            'duration': float_or_none(info.get('runtime'), 1000) or parse_duration(info.get('time')),\n-            'upload_date': info['fid'].split('/')[-1][:8] if isinstance(info.get('fid'), compat_str) else None,\n-            'uploader': info.get('nickname'),\n-            'uploader_id': info.get('upload_userid'),\n-            'view_count': str_to_int(info.get('hit')),\n-            'like_count': str_to_int(info.get('likecnt')),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/panopto.py",
            "diff": "diff --git a/yt_dlp/extractor/panopto.py b/yt_dlp/extractor/panopto.py\nindex 6e3c9f44..ddea32d7 100644\n--- a/yt_dlp/extractor/panopto.py\n+++ b/yt_dlp/extractor/panopto.py\n@@ -1,7 +1,7 @@\n import calendar\n import json\n import functools\n-from datetime import datetime\n+from datetime import datetime, timezone\n from random import random\n \n from .common import InfoExtractor\n@@ -243,7 +243,7 @@ def _mark_watched(self, base_url, video_id, delivery_info):\n         invocation_id = delivery_info.get('InvocationId')\n         stream_id = traverse_obj(delivery_info, ('Delivery', 'Streams', ..., 'PublicID'), get_all=False, expected_type=str)\n         if invocation_id and stream_id and duration:\n-            timestamp_str = f'/Date({calendar.timegm(datetime.utcnow().timetuple())}000)/'\n+            timestamp_str = f'/Date({calendar.timegm(datetime.now(timezone.utc).timetuple())}000)/'\n             data = {\n                 'streamRequests': [\n                     {\n@@ -536,7 +536,7 @@ def _fetch_page(self, base_url, query_params, display_id, page):\n         }\n \n         response = self._call_api(\n-            base_url, '/Services/Data.svc/GetSessions', f'{display_id} page {page+1}',\n+            base_url, '/Services/Data.svc/GetSessions', f'{display_id} page {page + 1}',\n             data={'queryParameters': params}, fatal=False)\n \n         for result in get_first(response, 'Results', default=[]):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/peekvids.py",
            "diff": "diff --git a/yt_dlp/extractor/peekvids.py b/yt_dlp/extractor/peekvids.py\nindex d1fc058b..41f591b0 100644\n--- a/yt_dlp/extractor/peekvids.py\n+++ b/yt_dlp/extractor/peekvids.py\n@@ -146,7 +146,6 @@ class PlayVidsIE(PeekVidsBaseIE):\n             'uploader': 'Brazzers',\n             'age_limit': 18,\n             'view_count': int,\n-            'age_limit': 18,\n             'categories': list,\n             'tags': list,\n         },\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/people.py",
            "diff": "diff --git a/yt_dlp/extractor/people.py b/yt_dlp/extractor/people.py\ndeleted file mode 100644\nindex c5143c3e..00000000\n--- a/yt_dlp/extractor/people.py\n+++ /dev/null\n@@ -1,29 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class PeopleIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?people\\.com/people/videos/0,,(?P<id>\\d+),00\\.html'\n-\n-    _TEST = {\n-        'url': 'http://www.people.com/people/videos/0,,20995451,00.html',\n-        'info_dict': {\n-            'id': 'ref:20995451',\n-            'ext': 'mp4',\n-            'title': 'Astronaut Love Triangle Victim Speaks Out: \u201cThe Crime in 2007 Hasn\u2019t Defined Us\u201d',\n-            'description': 'Colleen Shipman speaks to PEOPLE for the first time about life after the attack',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n-            'duration': 246.318,\n-            'timestamp': 1458720585,\n-            'upload_date': '20160323',\n-            'uploader_id': '416418724',\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'add_ie': ['BrightcoveNew'],\n-    }\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'http://players.brightcove.net/416418724/default_default/index.html?videoId=ref:%s'\n-            % self._match_id(url), 'BrightcoveNew')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/periscope.py",
            "diff": "diff --git a/yt_dlp/extractor/periscope.py b/yt_dlp/extractor/periscope.py\nindex 84bcf157..d2351df1 100644\n--- a/yt_dlp/extractor/periscope.py\n+++ b/yt_dlp/extractor/periscope.py\n@@ -4,6 +4,7 @@\n     parse_iso8601,\n     unescapeHTML,\n )\n+from ..utils.traversal import traverse_obj\n \n \n class PeriscopeBaseIE(InfoExtractor):\n@@ -20,22 +21,26 @@ def _parse_broadcast_data(self, broadcast, video_id):\n         title = broadcast.get('status') or 'Periscope Broadcast'\n         uploader = broadcast.get('user_display_name') or broadcast.get('username')\n         title = '%s - %s' % (uploader, title) if uploader else title\n-        is_live = broadcast.get('state').lower() == 'running'\n-\n         thumbnails = [{\n             'url': broadcast[image],\n-        } for image in ('image_url', 'image_url_small') if broadcast.get(image)]\n+        } for image in ('image_url', 'image_url_medium', 'image_url_small') if broadcast.get(image)]\n \n         return {\n             'id': broadcast.get('id') or video_id,\n             'title': title,\n-            'timestamp': parse_iso8601(broadcast.get('created_at')),\n+            'timestamp': parse_iso8601(broadcast.get('created_at')) or int_or_none(\n+                broadcast.get('created_at_ms'), scale=1000),\n+            'release_timestamp': int_or_none(broadcast.get('scheduled_start_ms'), scale=1000),\n             'uploader': uploader,\n             'uploader_id': broadcast.get('user_id') or broadcast.get('username'),\n             'thumbnails': thumbnails,\n             'view_count': int_or_none(broadcast.get('total_watched')),\n+            'concurrent_view_count': int_or_none(broadcast.get('total_watching')),\n             'tags': broadcast.get('tags'),\n-            'is_live': is_live,\n+            'live_status': {\n+                'running': 'is_live',\n+                'not_started': 'is_upcoming',\n+            }.get(traverse_obj(broadcast, ('state', {str.lower}))) or 'was_live'\n         }\n \n     @staticmethod\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/piaulizaportal.py",
            "diff": "diff --git a/yt_dlp/extractor/piaulizaportal.py b/yt_dlp/extractor/piaulizaportal.py\nnew file mode 100644\nindex 00000000..1eb6d92b\n--- /dev/null\n+++ b/yt_dlp/extractor/piaulizaportal.py\n@@ -0,0 +1,70 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    int_or_none,\n+    parse_qs,\n+    time_seconds,\n+    traverse_obj,\n+)\n+\n+\n+class PIAULIZAPortalIE(InfoExtractor):\n+    IE_DESC = 'ulizaportal.jp - PIA LIVE STREAM'\n+    _VALID_URL = r'https?://(?:www\\.)?ulizaportal\\.jp/pages/(?P<id>[\\da-f]{8}-(?:[\\da-f]{4}-){3}[\\da-f]{12})'\n+    _TESTS = [{\n+        'url': 'https://ulizaportal.jp/pages/005f18b7-e810-5618-cb82-0987c5755d44',\n+        'info_dict': {\n+            'id': '005f18b7-e810-5618-cb82-0987c5755d44',\n+            'title': '\u30d7\u30ec\u30bc\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u30d7\u30ec\u30a4\u30e4\u30fc\u306e\u30b5\u30f3\u30d7\u30eb',\n+            'live_status': 'not_live',\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True,\n+        },\n+    }, {\n+        'url': 'https://ulizaportal.jp/pages/005e1b23-fe93-5780-19a0-98e917cc4b7d?expires=4102412400&signature=f422a993b683e1068f946caf406d211c17d1ef17da8bef3df4a519502155aa91&version=1',\n+        'info_dict': {\n+            'id': '005e1b23-fe93-5780-19a0-98e917cc4b7d',\n+            'title': '\u3010\u78ba\u8a8d\u7528\u3011\u8996\u8074\u30b5\u30f3\u30d7\u30eb\u30da\u30fc\u30b8\uff08ULIZA\uff09',\n+            'live_status': 'not_live',\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+\n+        expires = int_or_none(traverse_obj(parse_qs(url), ('expires', 0)))\n+        if expires and expires <= time_seconds():\n+            raise ExtractorError('The link is expired.', video_id=video_id, expected=True)\n+\n+        webpage = self._download_webpage(url, video_id)\n+\n+        player_data = self._download_webpage(\n+            self._search_regex(\n+                r'<script [^>]*\\bsrc=\"(https://player-api\\.p\\.uliza\\.jp/v1/players/[^\"]+)\"',\n+                webpage, 'player data url'),\n+            video_id, headers={'Referer': 'https://ulizaportal.jp/'},\n+            note='Fetching player data', errnote='Unable to fetch player data')\n+\n+        formats = self._extract_m3u8_formats(\n+            self._search_regex(\n+                r'[\"\\'](https://vms-api\\.p\\.uliza\\.jp/v1/prog-index\\.m3u8[^\"\\']+)', player_data,\n+                'm3u8 url', default=None),\n+            video_id, fatal=False)\n+        m3u8_type = self._search_regex(\n+            r'/hls/(dvr|video)/', traverse_obj(formats, (0, 'url')), 'm3u8 type', default=None)\n+\n+        return {\n+            'id': video_id,\n+            'title': self._html_extract_title(webpage),\n+            'formats': formats,\n+            'live_status': {\n+                'video': 'is_live',\n+                'dvr': 'was_live',  # short-term archives\n+            }.get(m3u8_type, 'not_live'),  # VOD or long-term archives\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/playfm.py",
            "diff": "diff --git a/yt_dlp/extractor/playfm.py b/yt_dlp/extractor/playfm.py\ndeleted file mode 100644\nindex e895ba48..00000000\n--- a/yt_dlp/extractor/playfm.py\n+++ /dev/null\n@@ -1,70 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    ExtractorError,\n-    int_or_none,\n-    parse_iso8601,\n-)\n-\n-\n-class PlayFMIE(InfoExtractor):\n-    IE_NAME = 'play.fm'\n-    _VALID_URL = r'https?://(?:www\\.)?play\\.fm/(?P<slug>(?:[^/]+/)+(?P<id>[^/]+))/?(?:$|[?#])'\n-\n-    _TEST = {\n-        'url': 'https://www.play.fm/dan-drastic/sven-tasnadi-leipzig-electronic-music-batofar-paris-fr-2014-07-12',\n-        'md5': 'c505f8307825a245d0c7ad1850001f22',\n-        'info_dict': {\n-            'id': '71276',\n-            'ext': 'mp3',\n-            'title': 'Sven Tasnadi - LEIPZIG ELECTRONIC MUSIC @ Batofar (Paris,FR) - 2014-07-12',\n-            'description': '',\n-            'duration': 5627,\n-            'timestamp': 1406033781,\n-            'upload_date': '20140722',\n-            'uploader': 'Dan Drastic',\n-            'uploader_id': '71170',\n-            'view_count': int,\n-            'comment_count': int,\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        slug = mobj.group('slug')\n-\n-        recordings = self._download_json(\n-            'http://v2api.play.fm/recordings/slug/%s' % slug, video_id)\n-\n-        error = recordings.get('error')\n-        if isinstance(error, dict):\n-            raise ExtractorError(\n-                '%s returned error: %s' % (self.IE_NAME, error.get('message')),\n-                expected=True)\n-\n-        audio_url = recordings['audio']\n-        video_id = compat_str(recordings.get('id') or video_id)\n-        title = recordings['title']\n-        description = recordings.get('description')\n-        duration = int_or_none(recordings.get('recordingDuration'))\n-        timestamp = parse_iso8601(recordings.get('created_at'))\n-        uploader = recordings.get('page', {}).get('title')\n-        uploader_id = compat_str(recordings.get('page', {}).get('id'))\n-        view_count = int_or_none(recordings.get('playCount'))\n-        comment_count = int_or_none(recordings.get('commentCount'))\n-        categories = [tag['name'] for tag in recordings.get('tags', []) if tag.get('name')]\n-\n-        return {\n-            'id': video_id,\n-            'url': audio_url,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'timestamp': timestamp,\n-            'uploader': uploader,\n-            'uploader_id': uploader_id,\n-            'view_count': view_count,\n-            'comment_count': comment_count,\n-            'categories': categories,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/plays.py",
            "diff": "diff --git a/yt_dlp/extractor/plays.py b/yt_dlp/extractor/plays.py\ndeleted file mode 100644\nindex 9371f7b2..00000000\n--- a/yt_dlp/extractor/plays.py\n+++ /dev/null\n@@ -1,49 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import int_or_none\n-\n-\n-class PlaysTVIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?plays\\.tv/(?:video|embeds)/(?P<id>[0-9a-f]{18})'\n-    _TESTS = [{\n-        'url': 'https://plays.tv/video/56af17f56c95335490/when-you-outplay-the-azir-wall',\n-        'md5': 'dfeac1198506652b5257a62762cec7bc',\n-        'info_dict': {\n-            'id': '56af17f56c95335490',\n-            'ext': 'mp4',\n-            'title': 'Bjergsen - When you outplay the Azir wall',\n-            'description': 'Posted by Bjergsen',\n-        }\n-    }, {\n-        'url': 'https://plays.tv/embeds/56af17f56c95335490',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(\n-            'https://plays.tv/video/%s' % video_id, video_id)\n-\n-        info = self._search_json_ld(webpage, video_id,)\n-\n-        mpd_url, sources = re.search(\n-            r'(?s)<video[^>]+data-mpd=\"([^\"]+)\"[^>]*>(.+?)</video>',\n-            webpage).groups()\n-        formats = self._extract_mpd_formats(\n-            self._proto_relative_url(mpd_url), video_id, mpd_id='DASH')\n-        for format_id, height, format_url in re.findall(r'<source\\s+res=\"((\\d+)h?)\"\\s+src=\"([^\"]+)\"', sources):\n-            formats.append({\n-                'url': self._proto_relative_url(format_url),\n-                'format_id': 'http-' + format_id,\n-                'height': int_or_none(height),\n-            })\n-\n-        info.update({\n-            'id': video_id,\n-            'description': self._og_search_description(webpage),\n-            'thumbnail': info.get('thumbnail') or self._og_search_thumbnail(webpage),\n-            'formats': formats,\n-        })\n-\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/playvid.py",
            "diff": "diff --git a/yt_dlp/extractor/playvid.py b/yt_dlp/extractor/playvid.py\ndeleted file mode 100644\nindex 1e0989d0..00000000\n--- a/yt_dlp/extractor/playvid.py\n+++ /dev/null\n@@ -1,90 +0,0 @@\n-import re\n-import urllib.parse\n-\n-from .common import InfoExtractor\n-from ..compat import compat_urllib_parse_unquote\n-from ..utils import ExtractorError, clean_html\n-\n-\n-class PlayvidIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?playvid\\.com/watch(\\?v=|/)(?P<id>.+?)(?:#|$)'\n-    _TESTS = [{\n-        'url': 'http://www.playvid.com/watch/RnmBNgtrrJu',\n-        'md5': 'ffa2f6b2119af359f544388d8c01eb6c',\n-        'info_dict': {\n-            'id': 'RnmBNgtrrJu',\n-            'ext': 'mp4',\n-            'title': 'md5:9256d01c6317e3f703848b5906880dc8',\n-            'duration': 82,\n-            'age_limit': 18,\n-        },\n-        'skip': 'Video removed due to ToS',\n-    }, {\n-        'url': 'http://www.playvid.com/watch/hwb0GpNkzgH',\n-        'md5': '39d49df503ad7b8f23a4432cbf046477',\n-        'info_dict': {\n-            'id': 'hwb0GpNkzgH',\n-            'ext': 'mp4',\n-            'title': 'Ellen Euro Cutie Blond Takes a Sexy Survey Get Facial in The Park',\n-            'age_limit': 18,\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        m_error = re.search(\n-            r'<div class=\"block-error\">\\s*<div class=\"heading\">\\s*<div>(?P<msg>.+?)</div>\\s*</div>', webpage)\n-        if m_error:\n-            raise ExtractorError(clean_html(m_error.group('msg')), expected=True)\n-\n-        video_title = None\n-        duration = None\n-        video_thumbnail = None\n-        formats = []\n-\n-        # most of the information is stored in the flashvars\n-        flashvars = self._html_search_regex(\n-            r'flashvars=\"(.+?)\"', webpage, 'flashvars')\n-\n-        infos = compat_urllib_parse_unquote(flashvars).split(r'&')\n-        for info in infos:\n-            videovars_match = re.match(r'^video_vars\\[(.+?)\\]=(.+?)$', info)\n-            if videovars_match:\n-                key = videovars_match.group(1)\n-                val = videovars_match.group(2)\n-\n-                if key == 'title':\n-                    video_title = urllib.parse.unquote_plus(val)\n-                if key == 'duration':\n-                    try:\n-                        duration = int(val)\n-                    except ValueError:\n-                        pass\n-                if key == 'big_thumb':\n-                    video_thumbnail = val\n-\n-                videourl_match = re.match(\n-                    r'^video_urls\\]\\[(?P<resolution>[0-9]+)p', key)\n-                if videourl_match:\n-                    height = int(videourl_match.group('resolution'))\n-                    formats.append({\n-                        'height': height,\n-                        'url': val,\n-                    })\n-\n-        # Extract title - should be in the flashvars; if not, look elsewhere\n-        if video_title is None:\n-            video_title = self._html_extract_title(webpage)\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            'title': video_title,\n-            'thumbnail': video_thumbnail,\n-            'duration': duration,\n-            'description': None,\n-            'age_limit': 18\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/polskieradio.py",
            "diff": "diff --git a/yt_dlp/extractor/polskieradio.py b/yt_dlp/extractor/polskieradio.py\nindex 5bf92b9b..e0b22fff 100644\n--- a/yt_dlp/extractor/polskieradio.py\n+++ b/yt_dlp/extractor/polskieradio.py\n@@ -262,14 +262,14 @@ def _call_lp3(self, path, query, video_id, note):\n             query=query, headers={'x-api-key': '9bf6c5a2-a7d0-4980-9ed7-a3f7291f2a81'})\n \n     def _entries(self, playlist_id, has_episodes, has_articles):\n-        for i in itertools.count(1) if has_episodes else []:\n+        for i in itertools.count(0) if has_episodes else []:\n             page = self._call_lp3(\n                 'AudioArticle/GetListByCategoryId', {\n                     'categoryId': playlist_id,\n                     'PageSize': 10,\n                     'skip': i,\n                     'format': 400,\n-                }, playlist_id, f'Downloading episode list page {i}')\n+                }, playlist_id, f'Downloading episode list page {i + 1}')\n             if not traverse_obj(page, 'data'):\n                 break\n             for episode in page['data']:\n@@ -281,14 +281,14 @@ def _entries(self, playlist_id, has_episodes, has_articles):\n                     'timestamp': parse_iso8601(episode.get('datePublic')),\n                 }\n \n-        for i in itertools.count(1) if has_articles else []:\n+        for i in itertools.count(0) if has_articles else []:\n             page = self._call_lp3(\n                 'Article/GetListByCategoryId', {\n                     'categoryId': playlist_id,\n                     'PageSize': 9,\n                     'skip': i,\n                     'format': 400,\n-                }, playlist_id, f'Downloading article list page {i}')\n+                }, playlist_id, f'Downloading article list page {i + 1}')\n             if not traverse_obj(page, 'data'):\n                 break\n             for article in page['data']:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/pornbox.py",
            "diff": "diff --git a/yt_dlp/extractor/pornbox.py b/yt_dlp/extractor/pornbox.py\nnew file mode 100644\nindex 00000000..c381382e\n--- /dev/null\n+++ b/yt_dlp/extractor/pornbox.py\n@@ -0,0 +1,113 @@\n+from .common import InfoExtractor\n+from ..compat import functools\n+from ..utils import (\n+    int_or_none,\n+    parse_duration,\n+    parse_iso8601,\n+    qualities,\n+    str_or_none,\n+    traverse_obj,\n+    url_or_none,\n+)\n+\n+\n+class PornboxIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?pornbox\\.com/application/watch-page/(?P<id>[0-9]+)'\n+    _TESTS = [{\n+        'url': 'https://pornbox.com/application/watch-page/212108',\n+        'md5': '3ff6b6e206f263be4c5e987a3162ac6e',\n+        'info_dict': {\n+            'id': '212108',\n+            'ext': 'mp4',\n+            'title': 'md5:ececc5c6e6c9dd35d290c45fed05fd49',\n+            'uploader': 'Lily Strong',\n+            'timestamp': 1665871200,\n+            'upload_date': '20221015',\n+            'age_limit': 18,\n+            'availability': 'needs_auth',\n+            'duration': 1505,\n+            'cast': ['Lily Strong', 'John Strong'],\n+            'tags': 'count:11',\n+            'description': 'md5:589c7f33e183aa8aa939537300efb859',\n+            'thumbnail': r're:^https?://cdn-image\\.gtflixtv\\.com.*\\.jpg.*$'\n+        }\n+    }, {\n+        'url': 'https://pornbox.com/application/watch-page/216045',\n+        'info_dict': {\n+            'id': '216045',\n+            'title': 'md5:3e48528e73a9a2b12f7a2772ed0b26a2',\n+            'description': 'md5:3e631dcaac029f15ed434e402d1b06c7',\n+            'uploader': 'VK Studio',\n+            'timestamp': 1618264800,\n+            'upload_date': '20210412',\n+            'age_limit': 18,\n+            'availability': 'premium_only',\n+            'duration': 2710,\n+            'cast': 'count:3',\n+            'tags': 'count:29',\n+            'thumbnail': r're:^https?://cdn-image\\.gtflixtv\\.com.*\\.jpg.*$',\n+            'subtitles': 'count:6'\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'ignore_no_formats_error': True\n+        },\n+        'expected_warnings': [\n+            'You are either not logged in or do not have access to this scene',\n+            'No video formats found', 'Requested format is not available']\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+\n+        public_data = self._download_json(f'https://pornbox.com/contents/{video_id}', video_id)\n+\n+        subtitles = {country_code: [{\n+            'url': f'https://pornbox.com/contents/{video_id}/subtitles/{country_code}',\n+            'ext': 'srt'\n+        }] for country_code in traverse_obj(public_data, ('subtitles', ..., {str}))}\n+\n+        is_free_scene = traverse_obj(\n+            public_data, ('price', 'is_available_for_free', {bool}), default=False)\n+\n+        metadata = {\n+            'id': video_id,\n+            **traverse_obj(public_data, {\n+                'title': ('scene_name', {str.strip}),\n+                'description': ('small_description', {str.strip}),\n+                'uploader': 'studio',\n+                'duration': ('runtime', {parse_duration}),\n+                'cast': (('models', 'male_models'), ..., 'model_name'),\n+                'thumbnail': ('player_poster', {url_or_none}),\n+                'tags': ('niches', ..., 'niche'),\n+            }),\n+            'age_limit': 18,\n+            'timestamp': parse_iso8601(traverse_obj(\n+                public_data, ('studios', 'release_date'), 'publish_date')),\n+            'availability': self._availability(needs_auth=True, needs_premium=not is_free_scene),\n+            'subtitles': subtitles,\n+        }\n+\n+        if not public_data.get('is_purchased') or not is_free_scene:\n+            self.raise_login_required(\n+                'You are either not logged in or do not have access to this scene', metadata_available=True)\n+            return metadata\n+\n+        media_id = traverse_obj(public_data, (\n+            'medias', lambda _, v: v['title'] == 'Full video', 'media_id', {int}), get_all=False)\n+        if not media_id:\n+            self.raise_no_formats('Could not find stream id', video_id=video_id)\n+\n+        stream_data = self._download_json(\n+            f'https://pornbox.com/media/{media_id}/stream', video_id=video_id, note='Getting manifest urls')\n+\n+        get_quality = qualities(['web', 'vga', 'hd', '1080p', '4k', '8k'])\n+        metadata['formats'] = traverse_obj(stream_data, ('qualities', lambda _, v: v['src'], {\n+            'url': 'src',\n+            'vbr': ('bitrate', {functools.partial(int_or_none, scale=1000)}),\n+            'format_id': ('quality', {str_or_none}),\n+            'quality': ('quality', {get_quality}),\n+            'width': ('size', {lambda x: int(x[:-1])}),\n+        }))\n+\n+        return metadata\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/porncom.py",
            "diff": "diff --git a/yt_dlp/extractor/porncom.py b/yt_dlp/extractor/porncom.py\ndeleted file mode 100644\nindex c8ef240d..00000000\n--- a/yt_dlp/extractor/porncom.py\n+++ /dev/null\n@@ -1,99 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_urlparse\n-from ..utils import (\n-    int_or_none,\n-    js_to_json,\n-    parse_filesize,\n-    str_to_int,\n-)\n-\n-\n-class PornComIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:[a-zA-Z]+\\.)?porn\\.com/videos/(?:(?P<display_id>[^/]+)-)?(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'http://www.porn.com/videos/teen-grabs-a-dildo-and-fucks-her-pussy-live-on-1hottie-i-rec-2603339',\n-        'md5': '3f30ce76267533cd12ba999263156de7',\n-        'info_dict': {\n-            'id': '2603339',\n-            'display_id': 'teen-grabs-a-dildo-and-fucks-her-pussy-live-on-1hottie-i-rec',\n-            'ext': 'mp4',\n-            'title': 'Teen grabs a dildo and fucks her pussy live on 1hottie, I rec',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 551,\n-            'view_count': int,\n-            'age_limit': 18,\n-            'categories': list,\n-            'tags': list,\n-        },\n-    }, {\n-        'url': 'http://se.porn.com/videos/marsha-may-rides-seth-on-top-of-his-thick-cock-2658067',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        display_id = mobj.group('display_id') or video_id\n-\n-        webpage = self._download_webpage(url, display_id)\n-\n-        config = self._parse_json(\n-            self._search_regex(\n-                (r'=\\s*({.+?})\\s*;\\s*v1ar\\b',\n-                 r'=\\s*({.+?})\\s*,\\s*[\\da-zA-Z_]+\\s*='),\n-                webpage, 'config', default='{}'),\n-            display_id, transform_source=js_to_json, fatal=False)\n-\n-        if config:\n-            title = config['title']\n-            formats = [{\n-                'url': stream['url'],\n-                'format_id': stream.get('id'),\n-                'height': int_or_none(self._search_regex(\n-                    r'^(\\d+)[pP]', stream.get('id') or '', 'height', default=None))\n-            } for stream in config['streams'] if stream.get('url')]\n-            thumbnail = (compat_urlparse.urljoin(\n-                config['thumbCDN'], config['poster'])\n-                if config.get('thumbCDN') and config.get('poster') else None)\n-            duration = int_or_none(config.get('length'))\n-        else:\n-            title = self._search_regex(\n-                (r'<title>([^<]+)</title>', r'<h1[^>]*>([^<]+)</h1>'),\n-                webpage, 'title')\n-            formats = [{\n-                'url': compat_urlparse.urljoin(url, format_url),\n-                'format_id': '%sp' % height,\n-                'height': int(height),\n-                'filesize_approx': parse_filesize(filesize),\n-            } for format_url, height, filesize in re.findall(\n-                r'<a[^>]+href=\"(/download/[^\"]+)\">[^<]*?(\\d+)p<span[^>]*>(\\d+\\s*[a-zA-Z]+)<',\n-                webpage)]\n-            thumbnail = None\n-            duration = None\n-\n-        view_count = str_to_int(self._search_regex(\n-            (r'Views:\\s*</span>\\s*<span>\\s*([\\d,.]+)',\n-             r'class=[\"\\']views[\"\\'][^>]*><p>([\\d,.]+)'), webpage,\n-            'view count', fatal=False))\n-\n-        def extract_list(kind):\n-            s = self._search_regex(\n-                (r'(?s)%s:\\s*</span>\\s*<span>(.+?)</span>' % kind.capitalize(),\n-                 r'(?s)<p[^>]*>%s:(.+?)</p>' % kind.capitalize()),\n-                webpage, kind, fatal=False)\n-            return re.findall(r'<a[^>]+>([^<]+)</a>', s or '')\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'view_count': view_count,\n-            'formats': formats,\n-            'age_limit': 18,\n-            'categories': extract_list('categories'),\n-            'tags': extract_list('tags'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/pornez.py",
            "diff": "diff --git a/yt_dlp/extractor/pornez.py b/yt_dlp/extractor/pornez.py\ndeleted file mode 100644\nindex bc45f865..00000000\n--- a/yt_dlp/extractor/pornez.py\n+++ /dev/null\n@@ -1,60 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    clean_html,\n-    int_or_none,\n-    get_element_by_class,\n-    urljoin,\n-)\n-\n-\n-class PornezIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?pornez\\.net/(?:video(?P<id>\\w+)|watch)/'\n-    _TESTS = [{\n-        'url': 'https://pornez.net/video344819/mistresst-funny_penis_names-wmv/',\n-        'info_dict': {\n-            'id': '344819',\n-            'ext': 'mp4',\n-            'title': 'mistresst funny_penis_names wmv',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'age_limit': 18,\n-        },\n-        'params': {'skip_download': 'm3u8'},\n-    }, {\n-        'url': 'https://pornez.net/watch/leana+lovings+stiff+for+stepdaughter/',\n-        'info_dict': {\n-            'id': '156161',\n-            'ext': 'mp4',\n-            'title': 'Watch leana lovings stiff for stepdaughter porn video.',\n-            'age_limit': 18,\n-        },\n-        'params': {'skip_download': 'm3u8'},\n-    }, {\n-        'url': 'https://pornez.net/videovzs27fj/tutor4k-e14-blue-wave-1080p-nbq-tutor4k-e14-blue-wave/',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        if not video_id:\n-            video_id = self._search_regex(\n-                r'<link[^>]+\\bhref=[\"\\']https?://pornez.net/\\?p=(\\w+)[\"\\']', webpage, 'id')\n-\n-        iframe_src = self._html_search_regex(r'<iframe[^>]+src=\"([^\"]+)\"', webpage, 'iframe')\n-        iframe = self._download_webpage(urljoin('https://pornez.net', iframe_src), video_id)\n-\n-        entries = self._parse_html5_media_entries(iframe_src, iframe, video_id)[0]\n-        for fmt in entries['formats']:\n-            height = self._search_regex(r'_(\\d+)\\.m3u8', fmt['url'], 'height')\n-            fmt['format_id'] = '%sp' % height\n-            fmt['height'] = int_or_none(height)\n-\n-        entries.update({\n-            'id': video_id,\n-            'title': (clean_html(get_element_by_class('video-title', webpage))\n-                      or self._html_search_meta(\n-                      ['twitter:title', 'og:title', 'description'], webpage, 'title', default=None)),\n-            'thumbnail': self._html_search_meta(['thumbnailUrl'], webpage, 'thumb', default=None),\n-            'age_limit': 18,\n-        })\n-        return entries\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/pornhd.py",
            "diff": "diff --git a/yt_dlp/extractor/pornhd.py b/yt_dlp/extractor/pornhd.py\ndeleted file mode 100644\nindex c8a1ec80..00000000\n--- a/yt_dlp/extractor/pornhd.py\n+++ /dev/null\n@@ -1,116 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    determine_ext,\n-    ExtractorError,\n-    int_or_none,\n-    js_to_json,\n-    merge_dicts,\n-    urljoin,\n-)\n-\n-\n-class PornHdIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?pornhd\\.com/(?:[a-z]{2,4}/)?videos/(?P<id>\\d+)(?:/(?P<display_id>.+))?'\n-    _TESTS = [{\n-        'url': 'http://www.pornhd.com/videos/9864/selfie-restroom-masturbation-fun-with-chubby-cutie-hd-porn-video',\n-        'md5': '87f1540746c1d32ec7a2305c12b96b25',\n-        'info_dict': {\n-            'id': '9864',\n-            'display_id': 'selfie-restroom-masturbation-fun-with-chubby-cutie-hd-porn-video',\n-            'ext': 'mp4',\n-            'title': 'Restroom selfie masturbation',\n-            'description': 'md5:3748420395e03e31ac96857a8f125b2b',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n-            'view_count': int,\n-            'like_count': int,\n-            'age_limit': 18,\n-        },\n-        'skip': 'HTTP Error 404: Not Found',\n-    }, {\n-        'url': 'http://www.pornhd.com/videos/1962/sierra-day-gets-his-cum-all-over-herself-hd-porn-video',\n-        'md5': '1b7b3a40b9d65a8e5b25f7ab9ee6d6de',\n-        'info_dict': {\n-            'id': '1962',\n-            'display_id': 'sierra-day-gets-his-cum-all-over-herself-hd-porn-video',\n-            'ext': 'mp4',\n-            'title': 'md5:98c6f8b2d9c229d0f0fde47f61a1a759',\n-            'description': 'md5:8ff0523848ac2b8f9b065ba781ccf294',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n-            'view_count': int,\n-            'like_count': int,\n-            'age_limit': 18,\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        display_id = mobj.group('display_id')\n-\n-        webpage = self._download_webpage(url, display_id or video_id)\n-\n-        title = self._html_search_regex(\n-            [r'<span[^>]+class=[\"\\']video-name[\"\\'][^>]*>([^<]+)',\n-             r'<title>(.+?) - .*?[Pp]ornHD.*?</title>'], webpage, 'title')\n-\n-        sources = self._parse_json(js_to_json(self._search_regex(\n-            r\"(?s)sources'?\\s*[:=]\\s*(\\{.+?\\})\",\n-            webpage, 'sources', default='{}')), video_id)\n-\n-        info = {}\n-        if not sources:\n-            entries = self._parse_html5_media_entries(url, webpage, video_id)\n-            if entries:\n-                info = entries[0]\n-\n-        if not sources and not info:\n-            message = self._html_search_regex(\n-                r'(?s)<(div|p)[^>]+class=\"no-video\"[^>]*>(?P<value>.+?)</\\1',\n-                webpage, 'error message', group='value')\n-            raise ExtractorError('%s said: %s' % (self.IE_NAME, message), expected=True)\n-\n-        formats = []\n-        for format_id, video_url in sources.items():\n-            video_url = urljoin(url, video_url)\n-            if not video_url:\n-                continue\n-            height = int_or_none(self._search_regex(\n-                r'^(\\d+)[pP]', format_id, 'height', default=None))\n-            formats.append({\n-                'url': video_url,\n-                'ext': determine_ext(video_url, 'mp4'),\n-                'format_id': format_id,\n-                'height': height,\n-            })\n-        if formats:\n-            info['formats'] = formats\n-\n-        description = self._html_search_regex(\n-            (r'(?s)<section[^>]+class=[\"\\']video-description[^>]+>(?P<value>.+?)</section>',\n-             r'<(div|p)[^>]+class=\"description\"[^>]*>(?P<value>[^<]+)</\\1'),\n-            webpage, 'description', fatal=False,\n-            group='value') or self._html_search_meta(\n-            'description', webpage, default=None) or self._og_search_description(webpage)\n-        view_count = int_or_none(self._html_search_regex(\n-            r'(\\d+) views\\s*<', webpage, 'view count', fatal=False))\n-        thumbnail = self._search_regex(\n-            r\"poster'?\\s*:\\s*([\\\"'])(?P<url>(?:(?!\\1).)+)\\1\", webpage,\n-            'thumbnail', default=None, group='url')\n-\n-        like_count = int_or_none(self._search_regex(\n-            (r'(\\d+)</span>\\s*likes',\n-             r'(\\d+)\\s*</11[^>]+>(?:&nbsp;|\\s)*\\blikes',\n-             r'class=[\"\\']save-count[\"\\'][^>]*>\\s*(\\d+)'),\n-            webpage, 'like count', fatal=False))\n-\n-        return merge_dicts(info, {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'view_count': view_count,\n-            'like_count': like_count,\n-            'formats': formats,\n-            'age_limit': 18,\n-        })\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/pr0gramm.py",
            "diff": "diff --git a/yt_dlp/extractor/pr0gramm.py b/yt_dlp/extractor/pr0gramm.py\nindex 2eb327fb..2a679420 100644\n--- a/yt_dlp/extractor/pr0gramm.py\n+++ b/yt_dlp/extractor/pr0gramm.py\n@@ -1,97 +1,189 @@\n-import re\n+import json\n+from datetime import date\n+from urllib.parse import unquote\n \n from .common import InfoExtractor\n-from ..utils import merge_dicts\n+from ..compat import functools\n+from ..utils import (\n+    ExtractorError,\n+    float_or_none,\n+    int_or_none,\n+    make_archive_id,\n+    mimetype2ext,\n+    urljoin,\n+)\n+from ..utils.traversal import traverse_obj\n \n \n-class Pr0grammStaticIE(InfoExtractor):\n-    # Possible urls:\n-    # https://pr0gramm.com/static/5466437\n-    _VALID_URL = r'https?://pr0gramm\\.com/static/(?P<id>[0-9]+)'\n-    _TEST = {\n-        'url': 'https://pr0gramm.com/static/5466437',\n-        'md5': '52fa540d70d3edc286846f8ca85938aa',\n+class Pr0grammIE(InfoExtractor):\n+    _VALID_URL = r'https?://pr0gramm\\.com\\/(?:[^/?#]+/)+(?P<id>[\\d]+)(?:[/?#:]|$)'\n+    _TESTS = [{\n+        # Tags require account\n+        'url': 'https://pr0gramm.com/new/video/5466437',\n         'info_dict': {\n             'id': '5466437',\n             'ext': 'mp4',\n             'title': 'pr0gramm-5466437 by g11st',\n+            'tags': ['Neon Genesis Evangelion', 'Touhou Project', 'Fly me to the Moon', 'Marisad', 'Marisa Kirisame', 'video', 'sound', 'Marisa', 'Anime'],\n             'uploader': 'g11st',\n+            'uploader_id': 394718,\n+            'upload_timestamp': 1671590240,\n             'upload_date': '20221221',\n-        }\n-    }\n+            'like_count': int,\n+            'dislike_count': int,\n+            'age_limit': 0,\n+            'thumbnail': r're:^https://thumb\\.pr0gramm\\.com/.*\\.jpg',\n+            '_old_archive_ids': ['pr0grammstatic 5466437'],\n+        },\n+    }, {\n+        # Tags require account\n+        'url': 'https://pr0gramm.com/new/3052805:comment28391322',\n+        'info_dict': {\n+            'id': '3052805',\n+            'ext': 'mp4',\n+            'title': 'pr0gramm-3052805 by Hansking1',\n+            'tags': 'count:15',\n+            'uploader': 'Hansking1',\n+            'uploader_id': 385563,\n+            'upload_timestamp': 1552930408,\n+            'upload_date': '20190318',\n+            'like_count': int,\n+            'dislike_count': int,\n+            'age_limit': 0,\n+            'thumbnail': r're:^https://thumb\\.pr0gramm\\.com/.*\\.jpg',\n+            '_old_archive_ids': ['pr0grammstatic 3052805'],\n+        },\n+    }, {\n+        # Requires verified account\n+        'url': 'https://pr0gramm.com/new/Gianna%20Michaels/5848332',\n+        'info_dict': {\n+            'id': '5848332',\n+            'ext': 'mp4',\n+            'title': 'pr0gramm-5848332 by erd0pfel',\n+            'tags': 'count:18',\n+            'uploader': 'erd0pfel',\n+            'uploader_id': 349094,\n+            'upload_timestamp': 1694489652,\n+            'upload_date': '20230912',\n+            'like_count': int,\n+            'dislike_count': int,\n+            'age_limit': 18,\n+            'thumbnail': r're:^https://thumb\\.pr0gramm\\.com/.*\\.jpg',\n+            '_old_archive_ids': ['pr0grammstatic 5848332'],\n+        },\n+    }, {\n+        'url': 'https://pr0gramm.com/static/5466437',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://pr0gramm.com/new/rowan%20atkinson%20herr%20bohne/3052805',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://pr0gramm.com/user/froschler/dafur-ist-man-hier/5091290',\n+        'only_matching': True,\n+    }]\n \n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        # Fetch media sources\n-        entries = self._parse_html5_media_entries(url, webpage, video_id)\n-        media_info = entries[0]\n-\n-        # Fetch author\n-        uploader = self._html_search_regex(r'by\\W+([\\w-]+)\\W+', webpage, 'uploader')\n-\n-        # Fetch approx upload timestamp from filename\n-        # Have None-defaults in case the extraction fails\n-        uploadDay = None\n-        uploadMon = None\n-        uploadYear = None\n-        uploadTimestr = None\n-        # (//img.pr0gramm.com/2022/12/21/62ae8aa5e2da0ebf.mp4)\n-        m = re.search(r'//img\\.pr0gramm\\.com/(?P<year>[\\d]+)/(?P<mon>[\\d]+)/(?P<day>[\\d]+)/\\w+\\.\\w{,4}', webpage)\n-\n-        if (m):\n-            # Up to a day of accuracy should suffice...\n-            uploadDay = m.groupdict().get('day')\n-            uploadMon = m.groupdict().get('mon')\n-            uploadYear = m.groupdict().get('year')\n-            uploadTimestr = uploadYear + uploadMon + uploadDay\n-\n-        return merge_dicts({\n-            'id': video_id,\n-            'title': 'pr0gramm-%s%s' % (video_id, (' by ' + uploader) if uploader else ''),\n-            'uploader': uploader,\n-            'upload_date': uploadTimestr\n-        }, media_info)\n+    BASE_URL = 'https://pr0gramm.com'\n \n+    @functools.cached_property\n+    def _is_logged_in(self):\n+        return 'pp' in self._get_cookies(self.BASE_URL)\n \n-# This extractor is for the primary url (used for sharing, and appears in the\n-# location bar) Since this page loads the DOM via JS, yt-dl can't find any\n-# video information here. So let's redirect to a compatibility version of\n-# the site, which does contain the <video>-element  by itself,  without requiring\n-# js to be ran.\n-class Pr0grammIE(InfoExtractor):\n-    # Possible urls:\n-    # https://pr0gramm.com/new/546637\n-    # https://pr0gramm.com/new/video/546637\n-    # https://pr0gramm.com/top/546637\n-    # https://pr0gramm.com/top/video/546637\n-    # https://pr0gramm.com/user/g11st/uploads/5466437\n-    # https://pr0gramm.com/user/froschler/dafur-ist-man-hier/5091290\n-    # https://pr0gramm.com/user/froschler/reinziehen-1elf/5232030\n-    # https://pr0gramm.com/user/froschler/1elf/5232030\n-    # https://pr0gramm.com/new/5495710:comment62621020 <- this is not the id!\n-    # https://pr0gramm.com/top/fruher war alles damals/5498175\n-\n-    _VALID_URL = r'https?:\\/\\/pr0gramm\\.com\\/(?!static/\\d+).+?\\/(?P<id>[\\d]+)(:|$)'\n-    _TEST = {\n-        'url': 'https://pr0gramm.com/new/video/5466437',\n-        'info_dict': {\n-            'id': '5466437',\n-            'ext': 'mp4',\n-            'title': 'pr0gramm-5466437 by g11st',\n-            'uploader': 'g11st',\n-            'upload_date': '20221221',\n-        }\n-    }\n+    @functools.cached_property\n+    def _maximum_flags(self):\n+        # We need to guess the flags for the content otherwise the api will raise an error\n+        # We can guess the maximum allowed flags for the account from the cookies\n+        # Bitflags are (msbf): nsfp, nsfl, nsfw, sfw\n+        flags = 0b0001\n+        if self._is_logged_in:\n+            flags |= 0b1000\n+            cookies = self._get_cookies(self.BASE_URL)\n+            if 'me' not in cookies:\n+                self._download_webpage(self.BASE_URL, None, 'Refreshing verification information')\n+            if traverse_obj(cookies, ('me', {lambda x: x.value}, {unquote}, {json.loads}, 'verified')):\n+                flags |= 0b0110\n+\n+        return flags\n+\n+    def _call_api(self, endpoint, video_id, query={}, note='Downloading API json'):\n+        data = self._download_json(\n+            f'https://pr0gramm.com/api/items/{endpoint}',\n+            video_id, note, query=query, expected_status=403)\n+\n+        error = traverse_obj(data, ('error', {str}))\n+        if error in ('nsfwRequired', 'nsflRequired', 'nsfpRequired', 'verificationRequired'):\n+            if not self._is_logged_in:\n+                self.raise_login_required()\n+            raise ExtractorError(f'Unverified account cannot access NSFW/NSFL ({error})', expected=True)\n+        elif error:\n+            message = traverse_obj(data, ('msg', {str})) or error\n+            raise ExtractorError(f'API returned error: {message}', expected=True)\n \n-    def _generic_title():\n-        return \"oof\"\n+        return data\n+\n+    @staticmethod\n+    def _create_source_url(path):\n+        return urljoin('https://img.pr0gramm.com', path)\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+        video_info = traverse_obj(\n+            self._call_api('get', video_id, {'id': video_id, 'flags': self._maximum_flags}),\n+            ('items', 0, {dict}))\n+\n+        source = video_info.get('image')\n+        if not source or not source.endswith('mp4'):\n+            self.raise_no_formats('Could not extract a video', expected=bool(source), video_id=video_id)\n+\n+        tags = None\n+        if self._is_logged_in:\n+            metadata = self._call_api('info', video_id, {'itemId': video_id}, note='Downloading tags')\n+            tags = traverse_obj(metadata, ('tags', ..., 'tag', {str}))\n+            # Sorted by \"confidence\", higher confidence = earlier in list\n+            confidences = traverse_obj(metadata, ('tags', ..., 'confidence', ({int}, {float})))\n+            if confidences:\n+                tags = [tag for _, tag in sorted(zip(confidences, tags), reverse=True)]\n+\n+        formats = traverse_obj(video_info, ('variants', ..., {\n+            'format_id': ('name', {str}),\n+            'url': ('path', {self._create_source_url}),\n+            'ext': ('mimeType', {mimetype2ext}),\n+            'vcodec': ('codec', {str}),\n+            'width': ('width', {int_or_none}),\n+            'height': ('height', {int_or_none}),\n+            'bitrate': ('bitRate', {float_or_none}),\n+            'filesize': ('fileSize', {int_or_none}),\n+        })) if video_info.get('variants') else [{\n+            'ext': 'mp4',\n+            'format_id': 'source',\n+            **traverse_obj(video_info, {\n+                'url': ('image', {self._create_source_url}),\n+                'width': ('width', {int_or_none}),\n+                'height': ('height', {int_or_none}),\n+            }),\n+        }]\n+\n+        subtitles = {}\n+        for subtitle in traverse_obj(video_info, ('subtitles', lambda _, v: v['language'])):\n+            subtitles.setdefault(subtitle['language'], []).append(traverse_obj(subtitle, {\n+                'url': ('path', {self._create_source_url}),\n+                'note': ('label', {str}),\n+            }))\n \n-        return self.url_result(\n-            'https://pr0gramm.com/static/' + video_id,\n-            video_id=video_id,\n-            ie=Pr0grammStaticIE.ie_key())\n+        return {\n+            'id': video_id,\n+            'title': f'pr0gramm-{video_id} by {video_info.get(\"user\")}',\n+            'tags': tags,\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            'age_limit': 18 if traverse_obj(video_info, ('flags', {0b110.__and__})) else 0,\n+            '_old_archive_ids': [make_archive_id('Pr0grammStatic', video_id)],\n+            **traverse_obj(video_info, {\n+                'uploader': ('user', {str}),\n+                'uploader_id': ('userId', {int}),\n+                'like_count': ('up', {int}),\n+                'dislike_count': ('down', {int}),\n+                'upload_timestamp': ('created', {int}),\n+                'upload_date': ('created', {int}, {date.fromtimestamp}, {lambda x: x.strftime('%Y%m%d')}),\n+                'thumbnail': ('thumb', {lambda x: urljoin('https://thumb.pr0gramm.com', x)})\n+            }),\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/qdance.py",
            "diff": "diff --git a/yt_dlp/extractor/qdance.py b/yt_dlp/extractor/qdance.py\nindex d817677f..934ebbfd 100644\n--- a/yt_dlp/extractor/qdance.py\n+++ b/yt_dlp/extractor/qdance.py\n@@ -15,7 +15,7 @@\n \n class QDanceIE(InfoExtractor):\n     _NETRC_MACHINE = 'qdance'\n-    _VALID_URL = r'https?://(?:www\\.)?q-dance\\.com/network/(?:library|live)/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?q-dance\\.com/network/(?:library|live)/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'note': 'vod',\n         'url': 'https://www.q-dance.com/network/library/146542138',\n@@ -53,6 +53,27 @@ class QDanceIE(InfoExtractor):\n             'channel_id': 'qdancenetwork.video_149170353',\n         },\n         'skip': 'Completed livestream',\n+    }, {\n+        'note': 'vod with alphanumeric id',\n+        'url': 'https://www.q-dance.com/network/library/WhDleSIWSfeT3Q9ObBKBeA',\n+        'info_dict': {\n+            'id': 'WhDleSIWSfeT3Q9ObBKBeA',\n+            'ext': 'mp4',\n+            'title': 'Aftershock I Defqon.1 Weekend Festival 2023 I Sunday I BLUE',\n+            'display_id': 'naam-i-defqon-1-weekend-festival-2023-i-dag-i-podium',\n+            'description': 'Relive Defqon.1 Path of the Warrior with Aftershock at the BLUE \ud83d\udd25',\n+            'series': 'Defqon.1',\n+            'series_id': '31840378',\n+            'season': 'Defqon.1 Weekend Festival 2023',\n+            'season_id': '141735599',\n+            'duration': 3507,\n+            'availability': 'premium_only',\n+            'thumbnail': 'https://images.q-dance.network/1698158361-230625-135716-defqon-1-aftershock.jpg',\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://www.q-dance.com/network/library/-uRFKXwmRZGVnve7av9uqA',\n+        'only_matching': True,\n     }]\n \n     _access_token = None\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/radiko.py",
            "diff": "diff --git a/yt_dlp/extractor/radiko.py b/yt_dlp/extractor/radiko.py\nindex cef68eba..c363d9ba 100644\n--- a/yt_dlp/extractor/radiko.py\n+++ b/yt_dlp/extractor/radiko.py\n@@ -1,4 +1,5 @@\n import base64\n+import random\n import urllib.parse\n \n from .common import InfoExtractor\n@@ -13,6 +14,7 @@\n \n \n class RadikoBaseIE(InfoExtractor):\n+    _GEO_BYPASS = False\n     _FULL_KEY = None\n     _HOSTS_FOR_TIME_FREE_FFMPEG_UNSUPPORTED = (\n         'https://c-rpaa.smartstream.ne.jp',\n@@ -32,7 +34,7 @@ class RadikoBaseIE(InfoExtractor):\n         'https://c-radiko.smartstream.ne.jp',\n     )\n \n-    def _auth_client(self):\n+    def _negotiate_token(self):\n         _, auth1_handle = self._download_webpage_handle(\n             'https://radiko.jp/v2/api/auth1', None, 'Downloading authentication page',\n             headers={\n@@ -58,10 +60,23 @@ def _auth_client(self):\n                 'x-radiko-partialkey': partial_key,\n             }).split(',')[0]\n \n+        if area_id == 'OUT':\n+            self.raise_geo_restricted(countries=['JP'])\n+\n         auth_data = (auth_token, area_id)\n         self.cache.store('radiko', 'auth_data', auth_data)\n         return auth_data\n \n+    def _auth_client(self):\n+        cachedata = self.cache.load('radiko', 'auth_data')\n+        if cachedata is not None:\n+            response = self._download_webpage(\n+                'https://radiko.jp/v2/api/auth_check', None, 'Checking cached token', expected_status=401,\n+                headers={'X-Radiko-AuthToken': cachedata[0], 'X-Radiko-AreaId': cachedata[1]})\n+            if response == 'OK':\n+                return cachedata\n+        return self._negotiate_token()\n+\n     def _extract_full_key(self):\n         if self._FULL_KEY:\n             return self._FULL_KEY\n@@ -75,7 +90,7 @@ def _extract_full_key(self):\n \n         if full_key:\n             full_key = full_key.encode()\n-        else:  # use full key ever known\n+        else:  # use only full key ever known\n             full_key = b'bcd151073c03b352e1ef2fd66c32209da9ca0afa'\n \n         self._FULL_KEY = full_key\n@@ -103,24 +118,24 @@ def _extract_formats(self, video_id, station, is_onair, ft, cursor, auth_token,\n         m3u8_playlist_data = self._download_xml(\n             f'https://radiko.jp/v3/station/stream/pc_html5/{station}.xml', video_id,\n             note='Downloading stream information')\n-        m3u8_urls = m3u8_playlist_data.findall('.//url')\n \n         formats = []\n         found = set()\n-        for url_tag in m3u8_urls:\n-            pcu = url_tag.find('playlist_create_url').text\n-            url_attrib = url_tag.attrib\n+\n+        timefree_int = 0 if is_onair else 1\n+\n+        for element in m3u8_playlist_data.findall(f'.//url[@timefree=\"{timefree_int}\"]/playlist_create_url'):\n+            pcu = element.text\n+            if pcu in found:\n+                continue\n+            found.add(pcu)\n             playlist_url = update_url_query(pcu, {\n                 'station_id': station,\n                 **query,\n                 'l': '15',\n-                'lsid': '88ecea37e968c1f17d5413312d9f8003',\n+                'lsid': ''.join(random.choices('0123456789abcdef', k=32)),\n                 'type': 'b',\n             })\n-            if playlist_url in found:\n-                continue\n-            else:\n-                found.add(playlist_url)\n \n             time_to_skip = None if is_onair else cursor - ft\n \n@@ -138,8 +153,8 @@ def _extract_formats(self, video_id, station, is_onair, ft, cursor, auth_token,\n                         not is_onair and pcu.startswith(self._HOSTS_FOR_TIME_FREE_FFMPEG_UNSUPPORTED)):\n                     sf['preference'] = -100\n                     sf['format_note'] = 'not preferred'\n-                if not is_onair and url_attrib['timefree'] == '1' and time_to_skip:\n-                    sf['downloader_options'] = {'ffmpeg_args': ['-ss', time_to_skip]}\n+                if not is_onair and timefree_int == 1 and time_to_skip:\n+                    sf['downloader_options'] = {'ffmpeg_args': ['-ss', str(time_to_skip)]}\n             formats.extend(subformats)\n \n         return formats\n@@ -166,21 +181,7 @@ def _real_extract(self, url):\n         vid_int = unified_timestamp(video_id, False)\n         prog, station_program, ft, radio_begin, radio_end = self._find_program(video_id, station, vid_int)\n \n-        auth_cache = self.cache.load('radiko', 'auth_data')\n-        for attempt in range(2):\n-            auth_token, area_id = (not attempt and auth_cache) or self._auth_client()\n-            formats = self._extract_formats(\n-                video_id=video_id, station=station, is_onair=False,\n-                ft=ft, cursor=vid_int, auth_token=auth_token, area_id=area_id,\n-                query={\n-                    'start_at': radio_begin,\n-                    'ft': radio_begin,\n-                    'end_at': radio_end,\n-                    'to': radio_end,\n-                    'seek': video_id,\n-                })\n-            if formats:\n-                break\n+        auth_token, area_id = self._auth_client()\n \n         return {\n             'id': video_id,\n@@ -189,8 +190,18 @@ def _real_extract(self, url):\n             'uploader': try_call(lambda: station_program.find('.//name').text),\n             'uploader_id': station,\n             'timestamp': vid_int,\n-            'formats': formats,\n             'is_live': True,\n+            'formats': self._extract_formats(\n+                video_id=video_id, station=station, is_onair=False,\n+                ft=ft, cursor=vid_int, auth_token=auth_token, area_id=area_id,\n+                query={\n+                    'start_at': radio_begin,\n+                    'ft': radio_begin,\n+                    'end_at': radio_end,\n+                    'to': radio_end,\n+                    'seek': video_id\n+                }\n+            ),\n         }\n \n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/radiobremen.py",
            "diff": "diff --git a/yt_dlp/extractor/radiobremen.py b/yt_dlp/extractor/radiobremen.py\ndeleted file mode 100644\nindex 99ba050d..00000000\n--- a/yt_dlp/extractor/radiobremen.py\n+++ /dev/null\n@@ -1,59 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import parse_duration\n-\n-\n-class RadioBremenIE(InfoExtractor):\n-    _VALID_URL = r'http?://(?:www\\.)?radiobremen\\.de/mediathek/(?:index\\.html)?\\?id=(?P<id>[0-9]+)'\n-    IE_NAME = 'radiobremen'\n-\n-    _TEST = {\n-        'url': 'http://www.radiobremen.de/mediathek/?id=141876',\n-        'info_dict': {\n-            'id': '141876',\n-            'ext': 'mp4',\n-            'duration': 178,\n-            'width': 512,\n-            'title': 'Druck auf Patrick \u00d6zt\u00fcrk',\n-            'thumbnail': r're:https?://.*\\.jpg$',\n-            'description': 'Gegen den SPD-B\u00fcrgerschaftsabgeordneten Patrick \u00d6zt\u00fcrk wird wegen Beihilfe zum gewerbsm\u00e4\u00dfigen Betrug ermittelt. Am Donnerstagabend sollte er dem Vorstand des SPD-Unterbezirks Bremerhaven dazu Rede und Antwort stehen.',\n-        },\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        meta_url = 'http://www.radiobremen.de/apps/php/mediathek/metadaten.php?id=%s' % video_id\n-        meta_doc = self._download_webpage(\n-            meta_url, video_id, 'Downloading metadata')\n-        title = self._html_search_regex(\n-            r'<h1.*>(?P<title>.+)</h1>', meta_doc, 'title')\n-        description = self._html_search_regex(\n-            r'<p>(?P<description>.*)</p>', meta_doc, 'description', fatal=False)\n-        duration = parse_duration(self._html_search_regex(\n-            r'L&auml;nge:</td>\\s+<td>(?P<duration>[0-9]+:[0-9]+)</td>',\n-            meta_doc, 'duration', fatal=False))\n-\n-        page_doc = self._download_webpage(\n-            url, video_id, 'Downloading video information')\n-        mobj = re.search(\n-            r\"ardformatplayerclassic\\(\\'playerbereich\\',\\'(?P<width>[0-9]+)\\',\\'.*\\',\\'(?P<video_id>[0-9]+)\\',\\'(?P<secret>[0-9]+)\\',\\'(?P<thumbnail>.+)\\',\\'\\'\\)\",\n-            page_doc)\n-        video_url = (\n-            \"http://dl-ondemand.radiobremen.de/mediabase/%s/%s_%s_%s.mp4\" %\n-            (video_id, video_id, mobj.group(\"secret\"), mobj.group('width')))\n-\n-        formats = [{\n-            'url': video_url,\n-            'ext': 'mp4',\n-            'width': int(mobj.group('width')),\n-        }]\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'formats': formats,\n-            'thumbnail': mobj.group('thumbnail'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/radiocomercial.py",
            "diff": "diff --git a/yt_dlp/extractor/radiocomercial.py b/yt_dlp/extractor/radiocomercial.py\nnew file mode 100644\nindex 00000000..07891fe4\n--- /dev/null\n+++ b/yt_dlp/extractor/radiocomercial.py\n@@ -0,0 +1,150 @@\n+import itertools\n+\n+from .common import InfoExtractor\n+from ..networking.exceptions import HTTPError\n+from ..utils import (\n+    ExtractorError,\n+    extract_attributes,\n+    get_element_by_class,\n+    get_element_html_by_class,\n+    get_element_text_and_html_by_tag,\n+    get_elements_html_by_class,\n+    int_or_none,\n+    join_nonempty,\n+    try_call,\n+    unified_strdate,\n+    update_url,\n+    urljoin\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class RadioComercialIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?radiocomercial\\.pt/podcasts/[^/?#]+/t?(?P<season>\\d+)/(?P<id>[\\w-]+)'\n+    _TESTS = [{\n+        'url': 'https://radiocomercial.pt/podcasts/o-homem-que-mordeu-o-cao/t6/taylor-swift-entranhando-se-que-nem-uma-espada-no-ventre-dos-fas#page-content-wrapper',\n+        'md5': '5f4fe8e485b29d2e8fd495605bc2c7e4',\n+        'info_dict': {\n+            'id': 'taylor-swift-entranhando-se-que-nem-uma-espada-no-ventre-dos-fas',\n+            'ext': 'mp3',\n+            'title': 'Taylor Swift entranhando-se que nem uma espada no ventre dos f\u00e3s.',\n+            'release_date': '20231025',\n+            'thumbnail': r're:https://radiocomercial.pt/upload/[^.]+.jpg',\n+            'season': 6\n+        }\n+    }, {\n+        'url': 'https://radiocomercial.pt/podcasts/convenca-me-num-minuto/t3/convenca-me-num-minuto-que-os-lobisomens-existem',\n+        'md5': '47e96c273aef96a8eb160cd6cf46d782',\n+        'info_dict': {\n+            'id': 'convenca-me-num-minuto-que-os-lobisomens-existem',\n+            'ext': 'mp3',\n+            'title': 'Conven\u00e7a-me num minuto que os lobisomens existem',\n+            'release_date': '20231026',\n+            'thumbnail': r're:https://radiocomercial.pt/upload/[^.]+.jpg',\n+            'season': 3\n+        }\n+    }, {\n+        'url': 'https://radiocomercial.pt/podcasts/inacreditavel-by-ines-castel-branco/t2/o-desastre-de-aviao',\n+        'md5': '69be64255420fec23b7259955d771e54',\n+        'info_dict': {\n+            'id': 'o-desastre-de-aviao',\n+            'ext': 'mp3',\n+            'title': 'O desastre de avi\u00e3o',\n+            'description': 'md5:8a82beeb372641614772baab7246245f',\n+            'release_date': '20231101',\n+            'thumbnail': r're:https://radiocomercial.pt/upload/[^.]+.jpg',\n+            'season': 2\n+        },\n+        'params': {\n+            # inconsistant md5\n+            'skip_download': True,\n+        },\n+    }, {\n+        'url': 'https://radiocomercial.pt/podcasts/tnt-todos-no-top/2023/t-n-t-29-de-outubro',\n+        'md5': '91d32d4d4b1407272068b102730fc9fa',\n+        'info_dict': {\n+            'id': 't-n-t-29-de-outubro',\n+            'ext': 'mp3',\n+            'title': 'T.N.T 29 de outubro',\n+            'release_date': '20231029',\n+            'thumbnail': r're:https://radiocomercial.pt/upload/[^.]+.jpg',\n+            'season': 2023\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, season = self._match_valid_url(url).group('id', 'season')\n+        webpage = self._download_webpage(url, video_id)\n+        return {\n+            'id': video_id,\n+            'title': self._html_extract_title(webpage),\n+            'description': self._og_search_description(webpage, default=None),\n+            'release_date': unified_strdate(get_element_by_class(\n+                'date', get_element_html_by_class('descriptions', webpage) or '')),\n+            'thumbnail': self._og_search_thumbnail(webpage),\n+            'season': int_or_none(season),\n+            'url': extract_attributes(get_element_html_by_class('audiofile', webpage) or '').get('href'),\n+        }\n+\n+\n+class RadioComercialPlaylistIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?radiocomercial\\.pt/podcasts/(?P<id>[\\w-]+)(?:/t?(?P<season>\\d+))?/?(?:$|[?#])'\n+    _TESTS = [{\n+        'url': 'https://radiocomercial.pt/podcasts/convenca-me-num-minuto/t3',\n+        'info_dict': {\n+            'id': 'convenca-me-num-minuto_t3',\n+            'title': 'Conven\u00e7a-me num Minuto - Temporada 3',\n+        },\n+        'playlist_mincount': 32\n+    }, {\n+        'url': 'https://radiocomercial.pt/podcasts/o-homem-que-mordeu-o-cao',\n+        'info_dict': {\n+            'id': 'o-homem-que-mordeu-o-cao',\n+            'title': 'O Homem Que Mordeu o C\u00e3o',\n+        },\n+        'playlist_mincount': 19\n+    }, {\n+        'url': 'https://radiocomercial.pt/podcasts/as-minhas-coisas-favoritas',\n+        'info_dict': {\n+            'id': 'as-minhas-coisas-favoritas',\n+            'title': 'As Minhas Coisas Favoritas',\n+        },\n+        'playlist_mincount': 131\n+    }, {\n+        'url': 'https://radiocomercial.pt/podcasts/tnt-todos-no-top/t2023',\n+        'info_dict': {\n+            'id': 'tnt-todos-no-top_t2023',\n+            'title': 'TNT - Todos No Top - Temporada 2023',\n+        },\n+        'playlist_mincount': 39\n+    }]\n+\n+    def _entries(self, url, playlist_id):\n+        for page in itertools.count(1):\n+            try:\n+                webpage = self._download_webpage(\n+                    f'{url}/{page}', playlist_id, f'Downloading page {page}')\n+            except ExtractorError as e:\n+                if isinstance(e.cause, HTTPError) and e.cause.status == 404:\n+                    break\n+                raise\n+\n+            episodes = get_elements_html_by_class('tm-ouvir-podcast', webpage)\n+            if not episodes:\n+                break\n+            for url_path in traverse_obj(episodes, (..., {extract_attributes}, 'href')):\n+                episode_url = urljoin(url, url_path)\n+                if RadioComercialIE.suitable(episode_url):\n+                    yield episode_url\n+\n+    def _real_extract(self, url):\n+        podcast, season = self._match_valid_url(url).group('id', 'season')\n+        playlist_id = join_nonempty(podcast, season, delim='_t')\n+        url = update_url(url, query=None, fragment=None)\n+        webpage = self._download_webpage(url, playlist_id)\n+\n+        name = try_call(lambda: get_element_text_and_html_by_tag('h1', webpage)[0])\n+        title = name if name == season else join_nonempty(name, season, delim=' - Temporada ')\n+\n+        return self.playlist_from_matches(\n+            self._entries(url, playlist_id), playlist_id, title, ie=RadioComercialIE)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/radiofrance.py",
            "diff": "diff --git a/yt_dlp/extractor/radiofrance.py b/yt_dlp/extractor/radiofrance.py\nindex 92e51b7f..6bd6fe9b 100644\n--- a/yt_dlp/extractor/radiofrance.py\n+++ b/yt_dlp/extractor/radiofrance.py\n@@ -1,7 +1,18 @@\n+import itertools\n import re\n+import urllib.parse\n \n from .common import InfoExtractor\n-from ..utils import parse_duration, unified_strdate\n+from ..utils import (\n+    int_or_none,\n+    join_nonempty,\n+    js_to_json,\n+    parse_duration,\n+    strftime_or_none,\n+    traverse_obj,\n+    unified_strdate,\n+    urljoin,\n+)\n \n \n class RadioFranceIE(InfoExtractor):\n@@ -56,8 +67,32 @@ def _real_extract(self, url):\n         }\n \n \n-class FranceCultureIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?radiofrance\\.fr/(?:franceculture|fip|francemusique|mouv|franceinter)/podcasts/(?:[^?#]+/)?(?P<display_id>[^?#]+)-(?P<id>\\d+)($|[?#])'\n+class RadioFranceBaseIE(InfoExtractor):\n+    _VALID_URL_BASE = r'https?://(?:www\\.)?radiofrance\\.fr'\n+\n+    _STATIONS_RE = '|'.join(map(re.escape, (\n+        'franceculture',\n+        'franceinfo',\n+        'franceinter',\n+        'francemusique',\n+        'fip',\n+        'mouv',\n+    )))\n+\n+    def _extract_data_from_webpage(self, webpage, display_id, key):\n+        return traverse_obj(self._search_json(\n+            r'\\bconst\\s+data\\s*=', webpage, key, display_id,\n+            contains_pattern=r'\\[\\{(?s:.+)\\}\\]', transform_source=js_to_json),\n+            (..., 'data', key, {dict}), get_all=False) or {}\n+\n+\n+class FranceCultureIE(RadioFranceBaseIE):\n+    _VALID_URL = rf'''(?x)\n+        {RadioFranceBaseIE._VALID_URL_BASE}\n+        /(?:{RadioFranceBaseIE._STATIONS_RE})\n+        /podcasts/(?:[^?#]+/)?(?P<display_id>[^?#]+)-(?P<id>\\d{{6,}})(?:$|[?#])\n+    '''\n+\n     _TESTS = [\n         {\n             'url': 'https://www.radiofrance.fr/franceculture/podcasts/science-en-questions/la-physique-d-einstein-aiderait-elle-a-comprendre-le-cerveau-8440487',\n@@ -67,14 +102,30 @@ class FranceCultureIE(InfoExtractor):\n                 'ext': 'mp3',\n                 'title': 'La physique d\u2019Einstein aiderait-elle \u00e0 comprendre le cerveau ?',\n                 'description': 'Existerait-il un pont conceptuel entre la physique de l\u2019espace-temps et les neurosciences ?',\n-                'thumbnail': 'https://cdn.radiofrance.fr/s3/cruiser-production/2022/05/d184e7a3-4827-4494-bf94-04ed7b120db4/1200x630_gettyimages-200171095-001.jpg',\n+                'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n                 'upload_date': '20220514',\n                 'duration': 2750,\n             },\n         },\n+        {\n+            'url': 'https://www.radiofrance.fr/franceinter/podcasts/le-7-9-30/le-7-9-30-du-vendredi-10-mars-2023-2107675',\n+            'info_dict': {\n+                'id': '2107675',\n+                'display_id': 'le-7-9-30-du-vendredi-10-mars-2023',\n+                'title': 'Inflation alimentaire : comment en sortir ? - R\u00e9gis Debray et Claude Grange - Cyb\u00e8le Idelot',\n+                'description': 'md5:36ee74351ede77a314fdebb94026b916',\n+                'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n+                'upload_date': '20230310',\n+                'duration': 8977,\n+                'ext': 'mp3',\n+            },\n+        },\n         {\n             'url': 'https://www.radiofrance.fr/franceinter/podcasts/la-rafle-du-vel-d-hiv-une-affaire-d-etat/les-racines-du-crime-episode-1-3715507',\n             'only_matching': True,\n+        }, {\n+            'url': 'https://www.radiofrance.fr/franceinfo/podcasts/le-billet-sciences/sante-bientot-un-vaccin-contre-l-asthme-allergique-3057200',\n+            'only_matching': True,\n         }\n     ]\n \n@@ -89,7 +140,6 @@ def _real_extract(self, url):\n             'id': video_id,\n             'display_id': display_id,\n             'url': video_data['contentUrl'],\n-            'ext': video_data.get('encodingFormat'),\n             'vcodec': 'none' if video_data.get('encodingFormat') == 'mp3' else None,\n             'duration': parse_duration(video_data.get('duration')),\n             'title': self._html_search_regex(r'(?s)<h1[^>]*itemprop=\"[^\"]*name[^\"]*\"[^>]*>(.+?)</h1>',\n@@ -102,3 +152,322 @@ def _real_extract(self, url):\n             'upload_date': unified_strdate(self._search_regex(\n                 r'\"datePublished\"\\s*:\\s*\"([^\"]+)', webpage, 'timestamp', fatal=False))\n         }\n+\n+\n+class RadioFranceLiveIE(RadioFranceBaseIE):\n+    _VALID_URL = rf'''(?x)\n+        https?://(?:www\\.)?radiofrance\\.fr\n+        /(?P<id>{RadioFranceBaseIE._STATIONS_RE})\n+        /?(?P<substation_id>radio-[\\w-]+)?(?:[#?]|$)\n+    '''\n+\n+    _TESTS = [{\n+        'url': 'https://www.radiofrance.fr/franceinter/',\n+        'info_dict': {\n+            'id': 'franceinter',\n+            'title': str,\n+            'live_status': 'is_live',\n+            'ext': 'aac',\n+        },\n+        'params': {\n+            'skip_download': 'Livestream',\n+        },\n+    }, {\n+        'url': 'https://www.radiofrance.fr/franceculture',\n+        'info_dict': {\n+            'id': 'franceculture',\n+            'title': str,\n+            'live_status': 'is_live',\n+            'ext': 'aac',\n+        },\n+        'params': {\n+            'skip_download': 'Livestream',\n+        },\n+    }, {\n+        'url': 'https://www.radiofrance.fr/mouv/radio-musique-kids-family',\n+        'info_dict': {\n+            'id': 'mouv-radio-musique-kids-family',\n+            'title': str,\n+            'live_status': 'is_live',\n+            'ext': 'aac',\n+        },\n+        'params': {\n+            'skip_download': 'Livestream',\n+        },\n+    }, {\n+        'url': 'https://www.radiofrance.fr/mouv/radio-rnb-soul',\n+        'info_dict': {\n+            'id': 'mouv-radio-rnb-soul',\n+            'title': str,\n+            'live_status': 'is_live',\n+            'ext': 'aac',\n+        },\n+        'params': {\n+            'skip_download': 'Livestream',\n+        },\n+    }, {\n+        'url': 'https://www.radiofrance.fr/mouv/radio-musique-mix',\n+        'info_dict': {\n+            'id': 'mouv-radio-musique-mix',\n+            'title': str,\n+            'live_status': 'is_live',\n+            'ext': 'aac',\n+        },\n+        'params': {\n+            'skip_download': 'Livestream',\n+        },\n+    }, {\n+        'url': 'https://www.radiofrance.fr/fip/radio-rock',\n+        'info_dict': {\n+            'id': 'fip-radio-rock',\n+            'title': str,\n+            'live_status': 'is_live',\n+            'ext': 'aac',\n+        },\n+        'params': {\n+            'skip_download': 'Livestream',\n+        },\n+    }, {\n+        'url': 'https://www.radiofrance.fr/mouv',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        station_id, substation_id = self._match_valid_url(url).group('id', 'substation_id')\n+\n+        if substation_id:\n+            webpage = self._download_webpage(url, station_id)\n+            api_response = self._extract_data_from_webpage(webpage, station_id, 'webRadioData')\n+        else:\n+            api_response = self._download_json(\n+                f'https://www.radiofrance.fr/{station_id}/api/live', station_id)\n+\n+        formats, subtitles = [], {}\n+        for media_source in traverse_obj(api_response, (('now', None), 'media', 'sources', lambda _, v: v['url'])):\n+            if media_source.get('format') == 'hls':\n+                fmts, subs = self._extract_m3u8_formats_and_subtitles(media_source['url'], station_id, fatal=False)\n+                formats.extend(fmts)\n+                self._merge_subtitles(subs, target=subtitles)\n+            else:\n+                formats.append({\n+                    'url': media_source['url'],\n+                    'abr': media_source.get('bitrate'),\n+                })\n+\n+        return {\n+            'id': join_nonempty(station_id, substation_id),\n+            'title': traverse_obj(api_response, ('visual', 'legend')) or join_nonempty(\n+                ('now', 'firstLine', 'title'), ('now', 'secondLine', 'title'), from_dict=api_response, delim=' - '),\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            'is_live': True,\n+        }\n+\n+\n+class RadioFrancePlaylistBaseIE(RadioFranceBaseIE):\n+    \"\"\"Subclasses must set _METADATA_KEY\"\"\"\n+\n+    def _call_api(self, content_id, cursor, page_num):\n+        raise NotImplementedError('This method must be implemented by subclasses')\n+\n+    def _generate_playlist_entries(self, content_id, content_response):\n+        for page_num in itertools.count(2):\n+            for entry in content_response['items']:\n+                yield self.url_result(\n+                    f'https://www.radiofrance.fr/{entry[\"path\"]}', url_transparent=True, **traverse_obj(entry, {\n+                        'title': 'title',\n+                        'description': 'standFirst',\n+                        'timestamp': ('publishedDate', {int_or_none}),\n+                        'thumbnail': ('visual', 'src'),\n+                    }))\n+\n+            next_cursor = traverse_obj(content_response, (('pagination', None), 'next'), get_all=False)\n+            if not next_cursor:\n+                break\n+\n+            content_response = self._call_api(content_id, next_cursor, page_num)\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+\n+        metadata = self._download_json(\n+            'https://www.radiofrance.fr/api/v2.1/path', display_id,\n+            query={'value': urllib.parse.urlparse(url).path})['content']\n+\n+        content_id = metadata['id']\n+\n+        return self.playlist_result(\n+            self._generate_playlist_entries(content_id, metadata[self._METADATA_KEY]), content_id,\n+            display_id=display_id, **{**traverse_obj(metadata, {\n+                'title': 'title',\n+                'description': 'standFirst',\n+                'thumbnail': ('visual', 'src'),\n+            }), **traverse_obj(metadata, {\n+                'title': 'name',\n+                'description': 'role',\n+            })})\n+\n+\n+class RadioFrancePodcastIE(RadioFrancePlaylistBaseIE):\n+    _VALID_URL = rf'''(?x)\n+        {RadioFranceBaseIE._VALID_URL_BASE}\n+        /(?:{RadioFranceBaseIE._STATIONS_RE})\n+        /podcasts/(?P<id>[\\w-]+)/?(?:[?#]|$)\n+    '''\n+\n+    _TESTS = [{\n+        'url': 'https://www.radiofrance.fr/franceinfo/podcasts/le-billet-vert',\n+        'info_dict': {\n+            'id': 'eaf6ef81-a980-4f1c-a7d1-8a75ecd54b17',\n+            'display_id': 'le-billet-vert',\n+            'title': 'Le billet sciences',\n+            'description': 'md5:eb1007b34b0c0a680daaa71525bbd4c1',\n+            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n+        },\n+        'playlist_mincount': 11,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/franceinter/podcasts/jean-marie-le-pen-l-obsession-nationale',\n+        'info_dict': {\n+            'id': '566fd524-3074-4fbc-ac69-8696f2152a54',\n+            'display_id': 'jean-marie-le-pen-l-obsession-nationale',\n+            'title': 'Jean-Marie Le Pen, l\\'obsession nationale',\n+            'description': 'md5:a07c0cfb894f6d07a62d0ad12c4b7d73',\n+            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n+        },\n+        'playlist_count': 7,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/franceculture/podcasts/serie-thomas-grjebine',\n+        'info_dict': {\n+            'id': '63c1ddc9-9f15-457a-98b2-411bac63f48d',\n+            'display_id': 'serie-thomas-grjebine',\n+            'title': 'Thomas Grjebine',\n+        },\n+        'playlist_count': 1,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/fip/podcasts/certains-l-aiment-fip',\n+        'info_dict': {\n+            'id': '143dff38-e956-4a5d-8576-1c0b7242b99e',\n+            'display_id': 'certains-l-aiment-fip',\n+            'title': 'Certains l\u2019aiment Fip',\n+            'description': 'md5:ff974672ba00d4fd5be80fb001c5b27e',\n+            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n+        },\n+        'playlist_mincount': 321,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/franceinter/podcasts/le-7-9',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/mouv/podcasts/dirty-mix',\n+        'only_matching': True,\n+    }]\n+\n+    _METADATA_KEY = 'expressions'\n+\n+    def _call_api(self, podcast_id, cursor, page_num):\n+        return self._download_json(\n+            f'https://www.radiofrance.fr/api/v2.1/concepts/{podcast_id}/expressions', podcast_id,\n+            note=f'Downloading page {page_num}', query={'pageCursor': cursor})\n+\n+\n+class RadioFranceProfileIE(RadioFrancePlaylistBaseIE):\n+    _VALID_URL = rf'{RadioFranceBaseIE._VALID_URL_BASE}/personnes/(?P<id>[\\w-]+)'\n+\n+    _TESTS = [{\n+        'url': 'https://www.radiofrance.fr/personnes/thomas-pesquet?p=3',\n+        'info_dict': {\n+            'id': '86c62790-e481-11e2-9f7b-782bcb6744eb',\n+            'display_id': 'thomas-pesquet',\n+            'title': 'Thomas Pesquet',\n+            'description': 'Astronaute \u00e0 l\\'agence spatiale europ\u00e9enne',\n+        },\n+        'playlist_mincount': 212,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/personnes/eugenie-bastie',\n+        'info_dict': {\n+            'id': '9593050b-0183-4972-a0b5-d8f699079e02',\n+            'display_id': 'eugenie-bastie',\n+            'title': 'Eug\u00e9nie Basti\u00e9',\n+            'description': 'Journaliste et essayiste',\n+            'thumbnail': r're:^https?://.*\\.(?:jpg|png)',\n+        },\n+        'playlist_mincount': 39,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/personnes/lea-salame',\n+        'only_matching': True,\n+    }]\n+\n+    _METADATA_KEY = 'documents'\n+\n+    def _call_api(self, profile_id, cursor, page_num):\n+        resp = self._download_json(\n+            f'https://www.radiofrance.fr/api/v2.1/taxonomy/{profile_id}/documents', profile_id,\n+            note=f'Downloading page {page_num}', query={\n+                'relation': 'personality',\n+                'cursor': cursor,\n+            })\n+\n+        resp['next'] = traverse_obj(resp, ('pagination', 'next'))\n+        return resp\n+\n+\n+class RadioFranceProgramScheduleIE(RadioFranceBaseIE):\n+    _VALID_URL = rf'''(?x)\n+        {RadioFranceBaseIE._VALID_URL_BASE}\n+        /(?P<station>{RadioFranceBaseIE._STATIONS_RE})\n+        /grille-programmes(?:\\?date=(?P<date>[\\d-]+))?\n+    '''\n+\n+    _TESTS = [{\n+        'url': 'https://www.radiofrance.fr/franceinter/grille-programmes?date=17-02-2023',\n+        'info_dict': {\n+            'id': 'franceinter-program-20230217',\n+            'upload_date': '20230217',\n+        },\n+        'playlist_count': 25,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/franceculture/grille-programmes?date=01-02-2023',\n+        'info_dict': {\n+            'id': 'franceculture-program-20230201',\n+            'upload_date': '20230201',\n+        },\n+        'playlist_count': 25,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/mouv/grille-programmes?date=19-03-2023',\n+        'info_dict': {\n+            'id': 'mouv-program-20230319',\n+            'upload_date': '20230319',\n+        },\n+        'playlist_count': 3,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/francemusique/grille-programmes?date=18-03-2023',\n+        'info_dict': {\n+            'id': 'francemusique-program-20230318',\n+            'upload_date': '20230318',\n+        },\n+        'playlist_count': 15,\n+    }, {\n+        'url': 'https://www.radiofrance.fr/franceculture/grille-programmes',\n+        'only_matching': True,\n+    }]\n+\n+    def _generate_playlist_entries(self, webpage_url, api_response):\n+        for entry in traverse_obj(api_response, ('steps', lambda _, v: v['expression']['path'])):\n+            yield self.url_result(\n+                urljoin(webpage_url, f'/{entry[\"expression\"][\"path\"]}'), ie=FranceCultureIE,\n+                url_transparent=True, **traverse_obj(entry, {\n+                    'title': ('expression', 'title'),\n+                    'thumbnail': ('expression', 'visual', 'src'),\n+                    'timestamp': ('startTime', {int_or_none}),\n+                    'series_id': ('concept', 'id'),\n+                    'series': ('concept', 'title'),\n+                }))\n+\n+    def _real_extract(self, url):\n+        station, date = self._match_valid_url(url).group('station', 'date')\n+        webpage = self._download_webpage(url, station)\n+        grid_data = self._extract_data_from_webpage(webpage, station, 'grid')\n+        upload_date = strftime_or_none(grid_data.get('date'), '%Y%m%d')\n+\n+        return self.playlist_result(\n+            self._generate_playlist_entries(url, grid_data),\n+            join_nonempty(station, 'program', upload_date), upload_date=upload_date)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rbgtum.py",
            "diff": "diff --git a/yt_dlp/extractor/rbgtum.py b/yt_dlp/extractor/rbgtum.py\nindex 47649cfc..c8a331f3 100644\n--- a/yt_dlp/extractor/rbgtum.py\n+++ b/yt_dlp/extractor/rbgtum.py\n@@ -1,10 +1,11 @@\n import re\n \n from .common import InfoExtractor\n+from ..utils import parse_qs, remove_start, traverse_obj, ExtractorError\n \n \n class RbgTumIE(InfoExtractor):\n-    _VALID_URL = r'https://live\\.rbg\\.tum\\.de/w/(?P<id>.+)'\n+    _VALID_URL = r'https://(?:live\\.rbg\\.tum\\.de|tum\\.live)/w/(?P<id>[^?#]+)'\n     _TESTS = [{\n         # Combined view\n         'url': 'https://live.rbg.tum.de/w/cpp/22128',\n@@ -35,16 +36,18 @@ class RbgTumIE(InfoExtractor):\n             'title': 'Fachschaftsvollversammlung',\n             'series': 'Fachschaftsvollversammlung Informatik',\n         }\n+    }, {\n+        'url': 'https://tum.live/w/linalginfo/27102',\n+        'only_matching': True,\n     }, ]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(url, video_id)\n \n-        m3u8 = self._html_search_regex(r'(https://.+?\\.m3u8)', webpage, 'm3u8')\n-        lecture_title = self._html_search_regex(r'(?si)<h1.*?>(.*)</h1>', webpage, 'title')\n-        lecture_series_title = self._html_search_regex(\n-            r'(?s)<title\\b[^>]*>\\s*(?:TUM-Live\\s\\|\\s?)?([^:]+):?.*?</title>', webpage, 'series')\n+        m3u8 = self._html_search_regex(r'\"(https://[^\"]+\\.m3u8[^\"]*)', webpage, 'm3u8')\n+        lecture_title = self._html_search_regex(r'<h1[^>]*>([^<]+)</h1>', webpage, 'title', fatal=False)\n+        lecture_series_title = remove_start(self._html_extract_title(webpage), 'TUM-Live | ')\n \n         formats = self._extract_m3u8_formats(m3u8, video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls')\n \n@@ -57,9 +60,9 @@ def _real_extract(self, url):\n \n \n class RbgTumCourseIE(InfoExtractor):\n-    _VALID_URL = r'https://live\\.rbg\\.tum\\.de/course/(?P<id>.+)'\n+    _VALID_URL = r'https://(?P<hostname>(?:live\\.rbg\\.tum\\.de|tum\\.live))/old/course/(?P<id>(?P<year>\\d+)/(?P<term>\\w+)/(?P<slug>[^/?#]+))'\n     _TESTS = [{\n-        'url': 'https://live.rbg.tum.de/course/2022/S/fpv',\n+        'url': 'https://live.rbg.tum.de/old/course/2022/S/fpv',\n         'info_dict': {\n             'title': 'Funktionale Programmierung und Verifikation (IN0003)',\n             'id': '2022/S/fpv',\n@@ -69,7 +72,7 @@ class RbgTumCourseIE(InfoExtractor):\n         },\n         'playlist_count': 13,\n     }, {\n-        'url': 'https://live.rbg.tum.de/course/2022/W/set',\n+        'url': 'https://live.rbg.tum.de/old/course/2022/W/set',\n         'info_dict': {\n             'title': 'SET FSMPIC',\n             'id': '2022/W/set',\n@@ -78,16 +81,62 @@ class RbgTumCourseIE(InfoExtractor):\n             'noplaylist': False,\n         },\n         'playlist_count': 6,\n+    }, {\n+        'url': 'https://tum.live/old/course/2023/S/linalginfo',\n+        'only_matching': True,\n     }, ]\n \n     def _real_extract(self, url):\n-        course_id = self._match_id(url)\n-        webpage = self._download_webpage(url, course_id)\n+        course_id, hostname, year, term, slug = self._match_valid_url(url).group('id', 'hostname', 'year', 'term', 'slug')\n+        meta = self._download_json(\n+            f'https://{hostname}/api/courses/{slug}/', course_id, fatal=False,\n+            query={'year': year, 'term': term}) or {}\n+        lecture_series_title = meta.get('Name')\n+        lectures = [self.url_result(f'https://{hostname}/w/{slug}/{stream_id}', RbgTumIE)\n+                    for stream_id in traverse_obj(meta, ('Streams', ..., 'ID'))]\n+\n+        if not lectures:\n+            webpage = self._download_webpage(url, course_id)\n+            lecture_series_title = remove_start(self._html_extract_title(webpage), 'TUM-Live | ')\n+            lectures = [self.url_result(f'https://{hostname}{lecture_path}', RbgTumIE)\n+                        for lecture_path in re.findall(r'href=\"(/w/[^/\"]+/[^/\"]+)\"', webpage)]\n+\n+        return self.playlist_result(lectures, course_id, lecture_series_title)\n \n-        lecture_series_title = self._html_search_regex(r'(?si)<h1.*?>(.*)</h1>', webpage, 'title')\n \n-        lecture_urls = []\n-        for lecture_url in re.findall(r'(?i)href=\"/w/(.+)(?<!/cam)(?<!/pres)(?<!/chat)\"', webpage):\n-            lecture_urls.append(self.url_result('https://live.rbg.tum.de/w/' + lecture_url, ie=RbgTumIE.ie_key()))\n+class RbgTumNewCourseIE(InfoExtractor):\n+    _VALID_URL = r'https://(?P<hostname>(?:live\\.rbg\\.tum\\.de|tum\\.live))/\\?'\n+    _TESTS = [{\n+        'url': 'https://live.rbg.tum.de/?year=2022&term=S&slug=fpv&view=3',\n+        'info_dict': {\n+            'title': 'Funktionale Programmierung und Verifikation (IN0003)',\n+            'id': '2022/S/fpv',\n+        },\n+        'params': {\n+            'noplaylist': False,\n+        },\n+        'playlist_count': 13,\n+    }, {\n+        'url': 'https://live.rbg.tum.de/?year=2022&term=W&slug=set&view=3',\n+        'info_dict': {\n+            'title': 'SET FSMPIC',\n+            'id': '2022/W/set',\n+        },\n+        'params': {\n+            'noplaylist': False,\n+        },\n+        'playlist_count': 6,\n+    }, {\n+        'url': 'https://tum.live/?year=2023&term=S&slug=linalginfo&view=3',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        query = parse_qs(url)\n+        errors = [key for key in ('year', 'term', 'slug') if not query.get(key)]\n+        if errors:\n+            raise ExtractorError(f'Input URL is missing query parameters: {\", \".join(errors)}')\n+        year, term, slug = query['year'][0], query['term'][0], query['slug'][0]\n+        hostname = self._match_valid_url(url).group('hostname')\n \n-        return self.playlist_result(lecture_urls, course_id, lecture_series_title)\n+        return self.url_result(f'https://{hostname}/old/course/{year}/{term}/{slug}', RbgTumCourseIE)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rcs.py",
            "diff": "diff --git a/yt_dlp/extractor/rcs.py b/yt_dlp/extractor/rcs.py\nindex 028d3d90..b865f63f 100644\n--- a/yt_dlp/extractor/rcs.py\n+++ b/yt_dlp/extractor/rcs.py\n@@ -239,10 +239,10 @@ class RCSEmbedsIE(RCSBaseIE):\n         }\n     }, {\n         'url': 'https://video.gazzanet.gazzetta.it/video-embed/gazzanet-mo05-0000260789',\n-        'match_only': True\n+        'only_matching': True\n     }, {\n         'url': 'https://video.gazzetta.it/video-embed/49612410-00ca-11eb-bcd8-30d4253e0140',\n-        'match_only': True\n+        'only_matching': True\n     }]\n     _WEBPAGE_TESTS = [{\n         'url': 'https://www.iodonna.it/video-iodonna/personaggi-video/monica-bellucci-piu-del-lavoro-oggi-per-me-sono-importanti-lamicizia-e-la-famiglia/',\n@@ -325,7 +325,7 @@ class RCSIE(RCSBaseIE):\n         }\n     }, {\n         'url': 'https://video.corriere.it/video-360/metro-copenaghen-tutta-italiana/a248a7f0-e2db-11e9-9830-af2de6b1f945',\n-        'match_only': True\n+        'only_matching': True\n     }]\n \n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/recurbate.py",
            "diff": "diff --git a/yt_dlp/extractor/recurbate.py b/yt_dlp/extractor/recurbate.py\ndeleted file mode 100644\nindex d7294cb1..00000000\n--- a/yt_dlp/extractor/recurbate.py\n+++ /dev/null\n@@ -1,42 +0,0 @@\n-from .common import InfoExtractor\n-from ..networking.exceptions import HTTPError\n-from ..utils import ExtractorError, merge_dicts\n-\n-\n-class RecurbateIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?recurbate\\.com/play\\.php\\?video=(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'https://recurbate.com/play.php?video=39161415',\n-        'md5': 'dd2b4ec57aa3e3572cb5cf0997fca99f',\n-        'info_dict': {\n-            'id': '39161415',\n-            'ext': 'mp4',\n-            'description': 'md5:db48d09e4d93fc715f47fd3d6b7edd51',\n-            'title': 'Performer zsnicole33 show on 2022-10-25 20:23, Chaturbate Archive \u2013 Recurbate',\n-            'age_limit': 18,\n-        },\n-        'skip': 'Website require membership.',\n-    }]\n-\n-    def _real_extract(self, url):\n-        SUBSCRIPTION_MISSING_MESSAGE = 'This video is only available for registered users; Set your authenticated browser user agent via the --user-agent parameter.'\n-        video_id = self._match_id(url)\n-        try:\n-            webpage = self._download_webpage(url, video_id)\n-        except ExtractorError as e:\n-            if isinstance(e.cause, HTTPError) and e.cause.status == 403:\n-                self.raise_login_required(msg=SUBSCRIPTION_MISSING_MESSAGE, method='cookies')\n-            raise\n-        token = self._html_search_regex(r'data-token=\"([^\"]+)\"', webpage, 'token')\n-        video_url = f'https://recurbate.com/api/get.php?video={video_id}&token={token}'\n-\n-        video_webpage = self._download_webpage(video_url, video_id)\n-        if video_webpage == 'shall_subscribe':\n-            self.raise_login_required(msg=SUBSCRIPTION_MISSING_MESSAGE, method='cookies')\n-        entries = self._parse_html5_media_entries(video_url, video_webpage, video_id)\n-        return merge_dicts({\n-            'id': video_id,\n-            'title': self._html_extract_title(webpage, 'title'),\n-            'description': self._og_search_description(webpage),\n-            'age_limit': self._rta_search(webpage),\n-        }, entries[0])\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/reddit.py",
            "diff": "diff --git a/yt_dlp/extractor/reddit.py b/yt_dlp/extractor/reddit.py\nindex 813e6287..62f669f3 100644\n--- a/yt_dlp/extractor/reddit.py\n+++ b/yt_dlp/extractor/reddit.py\n@@ -319,16 +319,20 @@ def add_thumbnail(src):\n                 'format_id': 'fallback',\n                 'format_note': 'DASH video, mp4_dash',\n             }]\n-            formats.extend(self._extract_m3u8_formats(\n-                hls_playlist_url, display_id, 'mp4', m3u8_id='hls', fatal=False))\n-            formats.extend(self._extract_mpd_formats(\n-                dash_playlist_url, display_id, mpd_id='dash', fatal=False))\n+            hls_fmts, subtitles = self._extract_m3u8_formats_and_subtitles(\n+                hls_playlist_url, display_id, 'mp4', m3u8_id='hls', fatal=False)\n+            formats.extend(hls_fmts)\n+            dash_fmts, dash_subs = self._extract_mpd_formats_and_subtitles(\n+                dash_playlist_url, display_id, mpd_id='dash', fatal=False)\n+            formats.extend(dash_fmts)\n+            self._merge_subtitles(dash_subs, target=subtitles)\n \n             return {\n                 **info,\n                 'id': video_id,\n                 'display_id': display_id,\n                 'formats': formats,\n+                'subtitles': subtitles,\n                 'duration': int_or_none(reddit_video.get('duration')),\n             }\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/redtube.py",
            "diff": "diff --git a/yt_dlp/extractor/redtube.py b/yt_dlp/extractor/redtube.py\nindex 49076ccd..172c31b3 100644\n--- a/yt_dlp/extractor/redtube.py\n+++ b/yt_dlp/extractor/redtube.py\n@@ -39,7 +39,7 @@ class RedTubeIE(InfoExtractor):\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(\n-            'http://www.redtube.com/%s' % video_id, video_id)\n+            f'https://www.redtube.com/{video_id}', video_id)\n \n         ERRORS = (\n             (('video-deleted-info', '>This video has been removed'), 'has been removed'),\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rice.py",
            "diff": "diff --git a/yt_dlp/extractor/rice.py b/yt_dlp/extractor/rice.py\ndeleted file mode 100644\nindex 3dd4d31d..00000000\n--- a/yt_dlp/extractor/rice.py\n+++ /dev/null\n@@ -1,112 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_parse_qs\n-from ..utils import (\n-    xpath_text,\n-    xpath_element,\n-    int_or_none,\n-    parse_iso8601,\n-    ExtractorError,\n-)\n-\n-\n-class RICEIE(InfoExtractor):\n-    _VALID_URL = r'https?://mediahub\\.rice\\.edu/app/[Pp]ortal/video\\.aspx\\?(?P<query>.+)'\n-    _TEST = {\n-        'url': 'https://mediahub.rice.edu/app/Portal/video.aspx?PortalID=25ffd62c-3d01-4b29-8c70-7c94270efb3e&DestinationID=66bc9434-03bd-4725-b47e-c659d8d809db&ContentID=YEWIvbhb40aqdjMD1ALSqw',\n-        'md5': '9b83b4a2eead4912dc3b7fac7c449b6a',\n-        'info_dict': {\n-            'id': 'YEWIvbhb40aqdjMD1ALSqw',\n-            'ext': 'mp4',\n-            'title': 'Active Learning in Archeology',\n-            'upload_date': '20140616',\n-            'timestamp': 1402926346,\n-        }\n-    }\n-    _NS = 'http://schemas.datacontract.org/2004/07/ensembleVideo.Data.Service.Contracts.Models.Player.Config'\n-\n-    def _real_extract(self, url):\n-        qs = compat_parse_qs(self._match_valid_url(url).group('query'))\n-        if not qs.get('PortalID') or not qs.get('DestinationID') or not qs.get('ContentID'):\n-            raise ExtractorError('Invalid URL', expected=True)\n-\n-        portal_id = qs['PortalID'][0]\n-        playlist_id = qs['DestinationID'][0]\n-        content_id = qs['ContentID'][0]\n-\n-        content_data = self._download_xml('https://mediahub.rice.edu/api/portal/GetContentTitle', content_id, query={\n-            'portalId': portal_id,\n-            'playlistId': playlist_id,\n-            'contentId': content_id\n-        })\n-        metadata = xpath_element(content_data, './/metaData', fatal=True)\n-        title = xpath_text(metadata, 'primaryTitle', fatal=True)\n-        encodings = xpath_element(content_data, './/encodings', fatal=True)\n-        player_data = self._download_xml('https://mediahub.rice.edu/api/player/GetPlayerConfig', content_id, query={\n-            'temporaryLinkId': xpath_text(encodings, 'temporaryLinkId', fatal=True),\n-            'contentId': content_id,\n-        })\n-\n-        common_fmt = {}\n-        dimensions = xpath_text(encodings, 'dimensions')\n-        if dimensions:\n-            wh = dimensions.split('x')\n-            if len(wh) == 2:\n-                common_fmt.update({\n-                    'width': int_or_none(wh[0]),\n-                    'height': int_or_none(wh[1]),\n-                })\n-\n-        formats = []\n-        rtsp_path = xpath_text(player_data, self._xpath_ns('RtspPath', self._NS))\n-        if rtsp_path:\n-            fmt = {\n-                'url': rtsp_path,\n-                'format_id': 'rtsp',\n-            }\n-            fmt.update(common_fmt)\n-            formats.append(fmt)\n-        for source in player_data.findall(self._xpath_ns('.//Source', self._NS)):\n-            video_url = xpath_text(source, self._xpath_ns('File', self._NS))\n-            if not video_url:\n-                continue\n-            if '.m3u8' in video_url:\n-                formats.extend(self._extract_m3u8_formats(video_url, content_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))\n-            else:\n-                fmt = {\n-                    'url': video_url,\n-                    'format_id': video_url.split(':')[0],\n-                }\n-                fmt.update(common_fmt)\n-                rtmp = re.search(r'^(?P<url>rtmp://[^/]+/(?P<app>.+))/(?P<playpath>mp4:.+)$', video_url)\n-                if rtmp:\n-                    fmt.update({\n-                        'url': rtmp.group('url'),\n-                        'play_path': rtmp.group('playpath'),\n-                        'app': rtmp.group('app'),\n-                        'ext': 'flv',\n-                    })\n-                formats.append(fmt)\n-\n-        thumbnails = []\n-        for content_asset in content_data.findall('.//contentAssets'):\n-            asset_type = xpath_text(content_asset, 'type')\n-            if asset_type == 'image':\n-                image_url = xpath_text(content_asset, 'httpPath')\n-                if not image_url:\n-                    continue\n-                thumbnails.append({\n-                    'id': xpath_text(content_asset, 'ID'),\n-                    'url': image_url,\n-                })\n-\n-        return {\n-            'id': content_id,\n-            'title': title,\n-            'description': xpath_text(metadata, 'abstract'),\n-            'duration': int_or_none(xpath_text(metadata, 'duration')),\n-            'timestamp': parse_iso8601(xpath_text(metadata, 'dateUpdated')),\n-            'thumbnails': thumbnails,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rinsefm.py",
            "diff": "diff --git a/yt_dlp/extractor/rinsefm.py b/yt_dlp/extractor/rinsefm.py\nnew file mode 100644\nindex 00000000..760adf0e\n--- /dev/null\n+++ b/yt_dlp/extractor/rinsefm.py\n@@ -0,0 +1,33 @@\n+from .common import InfoExtractor\n+from ..utils import format_field, parse_iso8601\n+\n+\n+class RinseFMIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?rinse\\.fm/episodes/(?P<id>[^/?#]+)'\n+    _TESTS = [{\n+        'url': 'https://rinse.fm/episodes/club-glow-15-12-2023-2000/',\n+        'md5': '76ee0b719315617df42e15e710f46c7b',\n+        'info_dict': {\n+            'id': '1536535',\n+            'ext': 'mp3',\n+            'title': 'Club Glow - 15/12/2023 - 20:00',\n+            'thumbnail': r're:^https://.+\\.(?:jpg|JPG)$',\n+            'release_timestamp': 1702598400,\n+            'release_date': '20231215'\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url)\n+        webpage = self._download_webpage(url, display_id)\n+        entry = self._search_nextjs_data(webpage, display_id)['props']['pageProps']['entry']\n+\n+        return {\n+            'id': entry['id'],\n+            'title': entry.get('title'),\n+            'url': entry['fileUrl'],\n+            'vcodec': 'none',\n+            'release_timestamp': parse_iso8601(entry.get('episodeDate')),\n+            'thumbnail': format_field(\n+                entry, [('featuredImage', 0, 'filename')], 'https://rinse.imgix.net/media/%s', default=None),\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rokfin.py",
            "diff": "diff --git a/yt_dlp/extractor/rokfin.py b/yt_dlp/extractor/rokfin.py\nindex 4a4d40be..cad76f0c 100644\n--- a/yt_dlp/extractor/rokfin.py\n+++ b/yt_dlp/extractor/rokfin.py\n@@ -40,7 +40,6 @@ class RokfinIE(InfoExtractor):\n             'channel': 'Jimmy Dore',\n             'channel_id': 65429,\n             'channel_url': 'https://rokfin.com/TheJimmyDoreShow',\n-            'duration': 213.0,\n             'availability': 'public',\n             'live_status': 'not_live',\n             'dislike_count': int,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rtl2.py",
            "diff": "diff --git a/yt_dlp/extractor/rtl2.py b/yt_dlp/extractor/rtl2.py\nindex 056cf87d..07e1aa3c 100644\n--- a/yt_dlp/extractor/rtl2.py\n+++ b/yt_dlp/extractor/rtl2.py\n@@ -1,16 +1,7 @@\n import re\n \n from .common import InfoExtractor\n-from ..aes import aes_cbc_decrypt_bytes, unpad_pkcs7\n-from ..compat import (\n-    compat_b64decode,\n-    compat_str,\n-)\n-from ..utils import (\n-    ExtractorError,\n-    int_or_none,\n-    strip_or_none,\n-)\n+from ..utils import int_or_none\n \n \n class RTL2IE(InfoExtractor):\n@@ -102,92 +93,3 @@ def _real_extract(self, url):\n             'duration': int_or_none(video_info.get('duration')),\n             'formats': formats,\n         }\n-\n-\n-class RTL2YouBaseIE(InfoExtractor):\n-    _BACKWERK_BASE_URL = 'https://p-you-backwerk.rtl2apps.de/'\n-\n-\n-class RTL2YouIE(RTL2YouBaseIE):\n-    IE_NAME = 'rtl2:you'\n-    _VALID_URL = r'http?://you\\.rtl2\\.de/(?:video/\\d+/|youplayer/index\\.html\\?.*?\\bvid=)(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'http://you.rtl2.de/video/3002/15740/MJUNIK%20%E2%80%93%20Home%20of%20YOU/307-hirn-wo-bist-du',\n-        'info_dict': {\n-            'id': '15740',\n-            'ext': 'mp4',\n-            'title': 'MJUNIK \u2013 Home of YOU - #307 Hirn, wo bist du?!',\n-            'description': 'md5:ddaa95c61b372b12b66e115b2772fe01',\n-            'age_limit': 12,\n-        },\n-    }, {\n-        'url': 'http://you.rtl2.de/youplayer/index.html?vid=15712',\n-        'only_matching': True,\n-    }]\n-    _AES_KEY = b'\\xe9W\\xe4.<*\\xb8\\x1a\\xd2\\xb6\\x92\\xf3C\\xd3\\xefL\\x1b\\x03*\\xbbbH\\xc0\\x03\\xffo\\xc2\\xf2(\\xaa\\xaa!'\n-    _GEO_COUNTRIES = ['DE']\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        stream_data = self._download_json(\n-            self._BACKWERK_BASE_URL + 'stream/video/' + video_id, video_id)\n-\n-        data, iv = compat_b64decode(stream_data['streamUrl']).decode().split(':')\n-        stream_url = unpad_pkcs7(aes_cbc_decrypt_bytes(\n-            compat_b64decode(data), self._AES_KEY, compat_b64decode(iv)))\n-        if b'rtl2_you_video_not_found' in stream_url:\n-            raise ExtractorError('video not found', expected=True)\n-\n-        formats = self._extract_m3u8_formats(stream_url.decode(), video_id, 'mp4', 'm3u8_native')\n-\n-        video_data = self._download_json(\n-            self._BACKWERK_BASE_URL + 'video/' + video_id, video_id)\n-\n-        series = video_data.get('formatTitle')\n-        title = episode = video_data.get('title') or series\n-        if series and series != title:\n-            title = '%s - %s' % (series, title)\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'description': strip_or_none(video_data.get('description')),\n-            'thumbnail': video_data.get('image'),\n-            'duration': int_or_none(stream_data.get('duration') or video_data.get('duration'), 1000),\n-            'series': series,\n-            'episode': episode,\n-            'age_limit': int_or_none(video_data.get('minimumAge')),\n-        }\n-\n-\n-class RTL2YouSeriesIE(RTL2YouBaseIE):\n-    IE_NAME = 'rtl2:you:series'\n-    _VALID_URL = r'http?://you\\.rtl2\\.de/videos/(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'http://you.rtl2.de/videos/115/dragon-ball',\n-        'info_dict': {\n-            'id': '115',\n-        },\n-        'playlist_mincount': 5,\n-    }\n-\n-    def _real_extract(self, url):\n-        series_id = self._match_id(url)\n-        stream_data = self._download_json(\n-            self._BACKWERK_BASE_URL + 'videos',\n-            series_id, query={\n-                'formatId': series_id,\n-                'limit': 1000000000,\n-            })\n-\n-        entries = []\n-        for video in stream_data.get('videos', []):\n-            video_id = compat_str(video['videoId'])\n-            if not video_id:\n-                continue\n-            entries.append(self.url_result(\n-                'http://you.rtl2.de/video/%s/%s' % (series_id, video_id),\n-                'RTL2You', video_id))\n-        return self.playlist_result(entries, series_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rtvnh.py",
            "diff": "diff --git a/yt_dlp/extractor/rtvnh.py b/yt_dlp/extractor/rtvnh.py\ndeleted file mode 100644\nindex 7c617449..00000000\n--- a/yt_dlp/extractor/rtvnh.py\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import ExtractorError\n-\n-\n-class RTVNHIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?rtvnh\\.nl/video/(?P<id>[0-9]+)'\n-    _TEST = {\n-        'url': 'http://www.rtvnh.nl/video/131946',\n-        'md5': 'cdbec9f44550763c8afc96050fa747dc',\n-        'info_dict': {\n-            'id': '131946',\n-            'ext': 'mp4',\n-            'title': 'Grote zoektocht in zee bij Zandvoort naar vermiste vrouw',\n-            'thumbnail': r're:^https?:.*\\.jpg$'\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        meta = self._parse_json(self._download_webpage(\n-            'http://www.rtvnh.nl/video/json?m=' + video_id, video_id), video_id)\n-\n-        status = meta.get('status')\n-        if status != 200:\n-            raise ExtractorError(\n-                '%s returned error code %d' % (self.IE_NAME, status), expected=True)\n-\n-        formats = []\n-        rtmp_formats = self._extract_smil_formats(\n-            'http://www.rtvnh.nl/video/smil?m=' + video_id, video_id)\n-        formats.extend(rtmp_formats)\n-\n-        for rtmp_format in rtmp_formats:\n-            rtmp_url = '%s/%s' % (rtmp_format['url'], rtmp_format['play_path'])\n-            rtsp_format = rtmp_format.copy()\n-            del rtsp_format['play_path']\n-            del rtsp_format['ext']\n-            rtsp_format.update({\n-                'format_id': rtmp_format['format_id'].replace('rtmp', 'rtsp'),\n-                'url': rtmp_url.replace('rtmp://', 'rtsp://'),\n-                'protocol': 'rtsp',\n-            })\n-            formats.append(rtsp_format)\n-            http_base_url = rtmp_url.replace('rtmp://', 'http://')\n-            formats.extend(self._extract_m3u8_formats(\n-                http_base_url + '/playlist.m3u8', video_id, 'mp4',\n-                'm3u8_native', m3u8_id='hls', fatal=False))\n-            formats.extend(self._extract_f4m_formats(\n-                http_base_url + '/manifest.f4m',\n-                video_id, f4m_id='hds', fatal=False))\n-\n-        return {\n-            'id': video_id,\n-            'title': meta['title'].strip(),\n-            'thumbnail': meta.get('image'),\n-            'formats': formats\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rtvslo.py",
            "diff": "diff --git a/yt_dlp/extractor/rtvslo.py b/yt_dlp/extractor/rtvslo.py\nindex 05942b6b..39ace7cc 100644\n--- a/yt_dlp/extractor/rtvslo.py\n+++ b/yt_dlp/extractor/rtvslo.py\n@@ -1,6 +1,7 @@\n from .common import InfoExtractor\n from ..utils import (\n     ExtractorError,\n+    int_or_none,\n     parse_duration,\n     traverse_obj,\n     unified_timestamp,\n@@ -25,7 +26,7 @@ class RTVSLOIE(InfoExtractor):\n             'url': 'https://www.rtvslo.si/rtv365/arhiv/174842550?s=tv',\n             'info_dict': {\n                 'id': '174842550',\n-                'ext': 'flv',\n+                'ext': 'mp4',\n                 'release_timestamp': 1643140032,\n                 'upload_date': '20220125',\n                 'series': 'Dnevnik',\n@@ -69,7 +70,21 @@ class RTVSLOIE(InfoExtractor):\n                 'tbr': 128000,\n                 'release_date': '20220201',\n             },\n-\n+        }, {\n+            'url': 'https://365.rtvslo.si/arhiv/razred-zase/148350750',\n+            'info_dict': {\n+                'id': '148350750',\n+                'ext': 'mp4',\n+                'title': 'Prvi \u0161olski dan, mozai\u010dna oddaja za mlade',\n+                'series': 'Razred zase',\n+                'series_id': '148185730',\n+                'duration': 1481,\n+                'upload_date': '20121019',\n+                'timestamp': 1350672122,\n+                'release_date': '20121019',\n+                'release_timestamp': 1350672122,\n+                'thumbnail': 'https://img.rtvcdn.si/_up/ava/ava_misc/show_logos/148185730/razred_zase_2014_logo_4d_wide2.jpg',\n+            },\n         }, {\n             'url': 'https://4d.rtvslo.si/arhiv/dnevnik/174842550',\n             'only_matching': True\n@@ -98,13 +113,14 @@ def _real_extract(self, url):\n         media = self._download_json(self._API_BASE.format('getMedia', v_id), v_id, query={'jwt': jwt})['response']\n \n         formats = []\n+        skip_protocols = ['smil', 'f4m', 'dash']\n         adaptive_url = traverse_obj(media, ('addaptiveMedia', 'hls_sec'), expected_type=url_or_none)\n         if adaptive_url:\n-            formats = self._extract_wowza_formats(adaptive_url, v_id, skip_protocols=['smil'])\n+            formats = self._extract_wowza_formats(adaptive_url, v_id, skip_protocols=skip_protocols)\n \n         adaptive_url = traverse_obj(media, ('addaptiveMedia_sl', 'hls_sec'), expected_type=url_or_none)\n         if adaptive_url:\n-            for f in self._extract_wowza_formats(adaptive_url, v_id, skip_protocols=['smil']):\n+            for f in self._extract_wowza_formats(adaptive_url, v_id, skip_protocols=skip_protocols):\n                 formats.append({\n                     **f,\n                     'format_id': 'sign-' + f['format_id'],\n@@ -114,19 +130,19 @@ def _real_extract(self, url):\n                         else f.get('language'))\n                 })\n \n-        formats.extend(\n-            {\n-                'url': f['streams'][strm],\n-                'ext': traverse_obj(f, 'mediaType', expected_type=str.lower),\n-                'width': f.get('width'),\n-                'height': f.get('height'),\n-                'tbr': f.get('bitrate'),\n-                'filesize': f.get('filesize'),\n-            }\n-            for strm in ('http', 'https')\n-            for f in media.get('mediaFiles') or []\n-            if traverse_obj(f, ('streams', strm))\n-        )\n+        for mediafile in traverse_obj(media, ('mediaFiles', lambda _, v: url_or_none(v['streams']['https']))):\n+            formats.append(traverse_obj(mediafile, {\n+                'url': ('streams', 'https'),\n+                'ext': ('mediaType', {str.lower}),\n+                'width': ('width', {int_or_none}),\n+                'height': ('height', {int_or_none}),\n+                'tbr': ('bitrate', {int_or_none}),\n+                'filesize': ('filesize', {int_or_none}),\n+            }))\n+\n+        for mediafile in traverse_obj(media, ('mediaFiles', lambda _, v: url_or_none(v['streams']['hls_sec']))):\n+            formats.extend(self._extract_wowza_formats(\n+                mediafile['streams']['hls_sec'], v_id, skip_protocols=skip_protocols))\n \n         if any('intermission.mp4' in x['url'] for x in formats):\n             self.raise_geo_restricted(countries=self._GEO_COUNTRIES, metadata_available=True)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rudovideo.py",
            "diff": "diff --git a/yt_dlp/extractor/rudovideo.py b/yt_dlp/extractor/rudovideo.py\nnew file mode 100644\nindex 00000000..1b859559\n--- /dev/null\n+++ b/yt_dlp/extractor/rudovideo.py\n@@ -0,0 +1,135 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    determine_ext,\n+    js_to_json,\n+    traverse_obj,\n+    update_url_query,\n+    url_or_none,\n+)\n+\n+\n+class RudoVideoIE(InfoExtractor):\n+    _VALID_URL = r'https?://rudo\\.video/(?P<type>vod|podcast|live)/(?P<id>[^/?&#]+)'\n+    _EMBED_REGEX = [r'<iframe[^>]+src=[\\'\"](?P<url>(?:https?:)//rudo\\.video/(?:vod|podcast|live)/[^\\'\"]+)']\n+    _TESTS = [{\n+        'url': 'https://rudo.video/podcast/cz2wrUy8l0o',\n+        'md5': '28ed82b477708dc5e12e072da2449221',\n+        'info_dict': {\n+            'id': 'cz2wrUy8l0o',\n+            'title': 'Diego Cabot',\n+            'ext': 'mp4',\n+            'thumbnail': r're:^(?:https?:)?//.*\\.(png|jpg)$',\n+        },\n+    }, {\n+        'url': 'https://rudo.video/podcast/bQkt07',\n+        'md5': '36b22a9863de0f47f00fc7532a32a898',\n+        'info_dict': {\n+            'id': 'bQkt07',\n+            'title': 'Tubular Bells',\n+            'ext': 'mp4',\n+            'thumbnail': r're:^(?:https?:)?//.*\\.(png|jpg)$',\n+        },\n+    }, {\n+        'url': 'https://rudo.video/podcast/b42ZUznHX0',\n+        'md5': 'b91c70d832938871367f8ad10c895821',\n+        'info_dict': {\n+            'id': 'b42ZUznHX0',\n+            'title': 'Columna Ruperto Concha',\n+            'ext': 'mp3',\n+            'thumbnail': r're:^(?:https?:)?//.*\\.(png|jpg)$',\n+        },\n+    }, {\n+        'url': 'https://rudo.video/vod/bN5AaJ',\n+        'md5': '01324a329227e2591530ecb4f555c881',\n+        'info_dict': {\n+            'id': 'bN5AaJ',\n+            'title': 'Ucrania 19.03',\n+            'creator': 'La Tercera',\n+            'ext': 'mp4',\n+            'thumbnail': r're:^(?:https?:)?//.*\\.(png|jpg)$',\n+        },\n+    }, {\n+        'url': 'https://rudo.video/live/bbtv',\n+        'info_dict': {\n+            'id': 'bbtv',\n+            'ext': 'mp4',\n+            'creator': 'BioBioTV',\n+            'live_status': 'is_live',\n+            'title': r're:^LIVE BBTV\\s\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}$',\n+            'thumbnail': r're:^(?:https?:)?//.*\\.(png|jpg)$',\n+        },\n+    }, {\n+        'url': 'https://rudo.video/live/c13',\n+        'info_dict': {\n+            'id': 'c13',\n+            'title': 'CANAL13',\n+            'ext': 'mp4',\n+        },\n+        'skip': 'Geo-restricted to Chile',\n+    }, {\n+        'url': 'https://rudo.video/live/t13-13cl',\n+        'info_dict': {\n+            'id': 't13-13cl',\n+            'title': 'T13',\n+            'ext': 'mp4',\n+        },\n+        'skip': 'Geo-restricted to Chile',\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, type_ = self._match_valid_url(url).group('id', 'type')\n+        is_live = type_ == 'live'\n+\n+        webpage = self._download_webpage(url, video_id)\n+        if 'Streaming is not available in your area' in webpage:\n+            self.raise_geo_restricted()\n+\n+        media_url = (\n+            self._search_regex(\n+                r'var\\s+streamURL\\s*=\\s*[\\'\"]([^?\\'\"]+)', webpage, 'stream url', default=None)\n+            # Source URL must be used only if streamURL is unavailable\n+            or self._search_regex(\n+                r'<source[^>]+src=[\\'\"]([^\\'\"]+)', webpage, 'source url', default=None))\n+        if not media_url:\n+            youtube_url = self._search_regex(r'file:\\s*[\\'\"]((?:https?:)//(?:www\\.)?youtube\\.com[^\\'\"]+)',\n+                                             webpage, 'youtube url', default=None)\n+            if youtube_url:\n+                return self.url_result(youtube_url, 'Youtube')\n+            raise ExtractorError('Unable to extract stream url')\n+\n+        token_array = self._search_json(\n+            r'<script>var\\s+_\\$_[a-zA-Z0-9]+\\s*=', webpage, 'access token array', video_id,\n+            contains_pattern=r'\\[(?s:.+)\\]', default=None, transform_source=js_to_json)\n+        if token_array:\n+            token_url = traverse_obj(token_array, (..., {url_or_none}), get_all=False)\n+            if not token_url:\n+                raise ExtractorError('Invalid access token array')\n+            access_token = self._download_json(\n+                token_url, video_id, note='Downloading access token')['data']['authToken']\n+            media_url = update_url_query(media_url, {'auth-token': access_token})\n+\n+        ext = determine_ext(media_url)\n+        if ext == 'm3u8':\n+            formats = self._extract_m3u8_formats(media_url, video_id, live=is_live)\n+        elif ext == 'mp3':\n+            formats = [{\n+                'url': media_url,\n+                'vcodec': 'none',\n+            }]\n+        else:\n+            formats = [{'url': media_url}]\n+\n+        return {\n+            'id': video_id,\n+            'title': (self._search_regex(r'var\\s+titleVideo\\s*=\\s*[\\'\"]([^\\'\"]+)',\n+                                         webpage, 'title', default=None)\n+                      or self._og_search_title(webpage)),\n+            'creator': self._search_regex(r'var\\s+videoAuthor\\s*=\\s*[\\'\"]([^?\\'\"]+)',\n+                                          webpage, 'videoAuthor', default=None),\n+            'thumbnail': (self._search_regex(r'var\\s+posterIMG\\s*=\\s*[\\'\"]([^?\\'\"]+)',\n+                                             webpage, 'thumbnail', default=None)\n+                          or self._og_search_thumbnail(webpage)),\n+            'formats': formats,\n+            'is_live': is_live,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ruhd.py",
            "diff": "diff --git a/yt_dlp/extractor/ruhd.py b/yt_dlp/extractor/ruhd.py\ndeleted file mode 100644\nindex abaa3f9e..00000000\n--- a/yt_dlp/extractor/ruhd.py\n+++ /dev/null\n@@ -1,42 +0,0 @@\n-from .common import InfoExtractor\n-\n-\n-class RUHDIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?ruhd\\.ru/play\\.php\\?vid=(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'http://www.ruhd.ru/play.php?vid=207',\n-        'md5': 'd1a9ec4edf8598e3fbd92bb16072ba83',\n-        'info_dict': {\n-            'id': '207',\n-            'ext': 'divx',\n-            'title': '\u041a\u041e\u0422 \u0431\u0430\u0430\u0430\u0430\u0430\u043c',\n-            'description': '\u043a\u043b\u0430\u0441\u0441\u043d\u044b\u0439 \u043a\u043e\u0442)',\n-            'thumbnail': r're:^http://.*\\.jpg$',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        video_url = self._html_search_regex(\n-            r'<param name=\"src\" value=\"([^\"]+)\"', webpage, 'video url')\n-        title = self._html_search_regex(\n-            r'<title>([^<]+)&nbsp;&nbsp; RUHD\\.ru - \u0412\u0438\u0434\u0435\u043e \u0412\u044b\u0441\u043e\u043a\u043e\u0433\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u21161 \u0432 \u0420\u043e\u0441\u0441\u0438\u0438!</title>',\n-            webpage, 'title')\n-        description = self._html_search_regex(\n-            r'(?s)<div id=\"longdesc\">(.+?)<span id=\"showlink\">',\n-            webpage, 'description', fatal=False)\n-        thumbnail = self._html_search_regex(\n-            r'<param name=\"previewImage\" value=\"([^\"]+)\"',\n-            webpage, 'thumbnail', fatal=False)\n-        if thumbnail:\n-            thumbnail = 'http://www.ruhd.ru' + thumbnail\n-\n-        return {\n-            'id': video_id,\n-            'url': video_url,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rule34video.py",
            "diff": "diff --git a/yt_dlp/extractor/rule34video.py b/yt_dlp/extractor/rule34video.py\nindex 9d15f4d2..f3250b55 100644\n--- a/yt_dlp/extractor/rule34video.py\n+++ b/yt_dlp/extractor/rule34video.py\n@@ -1,6 +1,6 @@\n import re\n \n-from ..utils import parse_duration\n+from ..utils import parse_duration, unescapeHTML\n from .common import InfoExtractor\n \n \n@@ -16,7 +16,8 @@ class Rule34VideoIE(InfoExtractor):\n                 'title': 'Shot It-(mmd hmv)',\n                 'thumbnail': 'https://rule34video.com/contents/videos_screenshots/3065000/3065157/preview.jpg',\n                 'duration': 347.0,\n-                'age_limit': 18\n+                'age_limit': 18,\n+                'tags': 'count:14'\n             }\n         },\n         {\n@@ -28,7 +29,8 @@ class Rule34VideoIE(InfoExtractor):\n                 'title': 'Lara in Trouble Ep. 7 [WildeerStudio]',\n                 'thumbnail': 'https://rule34video.com/contents/videos_screenshots/3065000/3065296/preview.jpg',\n                 'duration': 938.0,\n-                'age_limit': 18\n+                'age_limit': 18,\n+                'tags': 'count:50'\n             }\n         },\n     ]\n@@ -57,5 +59,7 @@ def _real_extract(self, url):\n             'title': title,\n             'thumbnail': thumbnail,\n             'duration': parse_duration(duration),\n-            'age_limit': 18\n+            'age_limit': 18,\n+            'tags': list(map(unescapeHTML, re.findall(\n+                r'<a class=\"tag_item\"[^>]+\\bhref=\"https://rule34video\\.com/tags/\\d+/\"[^>]*>(?P<tag>[^>]*)</a>', webpage))),\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/rumble.py",
            "diff": "diff --git a/yt_dlp/extractor/rumble.py b/yt_dlp/extractor/rumble.py\nindex f8bf4a18..85567d9a 100644\n--- a/yt_dlp/extractor/rumble.py\n+++ b/yt_dlp/extractor/rumble.py\n@@ -33,7 +33,7 @@ class RumbleEmbedIE(InfoExtractor):\n             'upload_date': '20191020',\n             'channel_url': 'https://rumble.com/c/WMAR',\n             'channel': 'WMAR',\n-            'thumbnail': 'https://sp.rmbl.ws/s8/1/5/M/z/1/5Mz1a.OvCc-small-WMAR-2-News-Latest-Headline.jpg',\n+            'thumbnail': 'https://sp.rmbl.ws/s8/1/5/M/z/1/5Mz1a.qR4e-small-WMAR-2-News-Latest-Headline.jpg',\n             'duration': 234,\n             'uploader': 'WMAR',\n             'live_status': 'not_live',\n@@ -84,7 +84,7 @@ class RumbleEmbedIE(InfoExtractor):\n         'info_dict': {\n             'id': 'v1essrt',\n             'ext': 'mp4',\n-            'title': 'startswith:lofi hip hop radio - beats to relax/study',\n+            'title': 'startswith:lofi hip hop radio \ud83d\udcda - beats to relax/study to',\n             'timestamp': 1661519399,\n             'upload_date': '20220826',\n             'channel_url': 'https://rumble.com/c/LofiGirl',\n@@ -99,7 +99,7 @@ class RumbleEmbedIE(InfoExtractor):\n         'url': 'https://rumble.com/embed/v1amumr',\n         'info_dict': {\n             'id': 'v1amumr',\n-            'ext': 'webm',\n+            'ext': 'mp4',\n             'fps': 60,\n             'title': 'Turning Point USA 2022 Student Action Summit DAY 1  - Rumble Exclusive Live',\n             'timestamp': 1658518457,\n@@ -129,7 +129,7 @@ class RumbleEmbedIE(InfoExtractor):\n                 'duration': 92,\n                 'title': '911 Audio From The Man Who Wanted To Kill Supreme Court Justice Kavanaugh',\n                 'channel_url': 'https://rumble.com/c/RichSementa',\n-                'thumbnail': 'https://sp.rmbl.ws/s8/1/P/j/f/A/PjfAe.OvCc-small-911-Audio-From-The-Man-Who-.jpg',\n+                'thumbnail': 'https://sp.rmbl.ws/s8/1/P/j/f/A/PjfAe.qR4e-small-911-Audio-From-The-Man-Who-.jpg',\n                 'timestamp': 1654892716,\n                 'uploader': 'Mr Producer Media',\n                 'upload_date': '20220610',\n@@ -144,7 +144,7 @@ def _extract_embed_urls(cls, url, webpage):\n         if embeds:\n             return embeds\n         return [f'https://rumble.com/embed/{mobj.group(\"id\")}' for mobj in re.finditer(\n-            r'<script>[^<]*\\bRumble\\(\\s*\"play\"\\s*,\\s*{\\s*[\\'\"]?video[\\'\"]?\\s*:\\s*[\\'\"](?P<id>[0-9a-z]+)[\\'\"]', webpage)]\n+            r'<script>[^<]*\\bRumble\\(\\s*\"play\"\\s*,\\s*{[^}]*[\\'\"]?video[\\'\"]?\\s*:\\s*[\\'\"](?P<id>[0-9a-z]+)[\\'\"]', webpage)]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n@@ -236,7 +236,9 @@ def _real_extract(self, url):\n \n class RumbleIE(InfoExtractor):\n     _VALID_URL = r'https?://(?:www\\.)?rumble\\.com/(?P<id>v(?!ideos)[\\w.-]+)[^/]*$'\n-    _EMBED_REGEX = [r'<a class=video-item--a href=(?P<url>/v[\\w.-]+\\.html)>']\n+    _EMBED_REGEX = [\n+        r'<a class=video-item--a href=(?P<url>/v[\\w.-]+\\.html)>',\n+        r'<a[^>]+class=\"videostream__link link\"[^>]+href=(?P<url>/v[\\w.-]+\\.html)[^>]*>']\n     _TESTS = [{\n         'add_ie': ['RumbleEmbed'],\n         'url': 'https://rumble.com/vdmum1-moose-the-dog-helps-girls-dig-a-snow-fort.html',\n@@ -254,6 +256,7 @@ class RumbleIE(InfoExtractor):\n             'thumbnail': r're:https://.+\\.jpg',\n             'duration': 103,\n             'like_count': int,\n+            'dislike_count': int,\n             'view_count': int,\n             'live_status': 'not_live',\n         }\n@@ -278,6 +281,9 @@ class RumbleIE(InfoExtractor):\n             'channel_url': 'https://rumble.com/c/Redacted',\n             'live_status': 'not_live',\n             'thumbnail': 'https://sp.rmbl.ws/s8/1/d/x/2/O/dx2Oi.qR4e-small-The-U.S.-CANNOT-hide-this-i.jpg',\n+            'like_count': int,\n+            'dislike_count': int,\n+            'view_count': int,\n         },\n     }, {\n         'url': 'https://rumble.com/v2e7fju-the-covid-twitter-files-drop-protecting-fauci-while-censoring-the-truth-wma.html',\n@@ -296,12 +302,15 @@ class RumbleIE(InfoExtractor):\n             'channel_url': 'https://rumble.com/c/KimIversen',\n             'channel': 'Kim Iversen',\n             'thumbnail': 'https://sp.rmbl.ws/s8/1/6/b/w/O/6bwOi.qR4e-small-The-Covid-Twitter-Files-Dro.jpg',\n+            'like_count': int,\n+            'dislike_count': int,\n+            'view_count': int,\n         },\n     }]\n \n     _WEBPAGE_TESTS = [{\n         'url': 'https://rumble.com/videos?page=2',\n-        'playlist_count': 25,\n+        'playlist_mincount': 24,\n         'info_dict': {\n             'id': 'videos?page=2',\n             'title': 'All videos',\n@@ -309,17 +318,16 @@ class RumbleIE(InfoExtractor):\n             'age_limit': 0,\n         },\n     }, {\n-        'url': 'https://rumble.com/live-videos',\n-        'playlist_mincount': 19,\n+        'url': 'https://rumble.com/browse/live',\n+        'playlist_mincount': 25,\n         'info_dict': {\n-            'id': 'live-videos',\n-            'title': 'Live Videos',\n-            'description': 'Live videos on Rumble.com',\n+            'id': 'live',\n+            'title': 'Browse',\n             'age_limit': 0,\n         },\n     }, {\n         'url': 'https://rumble.com/search/video?q=rumble&sort=views',\n-        'playlist_count': 24,\n+        'playlist_mincount': 24,\n         'info_dict': {\n             'id': 'video?q=rumble&sort=views',\n             'title': 'Search results for: rumble',\n@@ -334,19 +342,20 @@ def _real_extract(self, url):\n         if not url_info:\n             raise UnsupportedError(url)\n \n-        release_ts_str = self._search_regex(\n-            r'(?:Livestream begins|Streamed on):\\s+<time datetime=\"([^\"]+)',\n-            webpage, 'release date', fatal=False, default=None)\n-        view_count_str = self._search_regex(r'<span class=\"media-heading-info\">([\\d,]+) Views',\n-                                            webpage, 'view count', fatal=False, default=None)\n-\n-        return self.url_result(\n-            url_info['url'], ie_key=url_info['ie_key'], url_transparent=True,\n-            view_count=parse_count(view_count_str),\n-            release_timestamp=parse_iso8601(release_ts_str),\n-            like_count=parse_count(get_element_by_class('rumbles-count', webpage)),\n-            description=clean_html(get_element_by_class('media-description', webpage)),\n-        )\n+        return {\n+            '_type': 'url_transparent',\n+            'ie_key': url_info['ie_key'],\n+            'url': url_info['url'],\n+            'release_timestamp': parse_iso8601(self._search_regex(\n+                r'(?:Livestream begins|Streamed on):\\s+<time datetime=\"([^\"]+)', webpage, 'release date', default=None)),\n+            'view_count': int_or_none(self._search_regex(\n+                r'\"userInteractionCount\"\\s*:\\s*(\\d+)', webpage, 'view count', default=None)),\n+            'like_count': parse_count(self._search_regex(\n+                r'<span data-js=\"rumbles_up_votes\">\\s*([\\d,.KM]+)', webpage, 'like count', default=None)),\n+            'dislike_count': parse_count(self._search_regex(\n+                r'<span data-js=\"rumbles_down_votes\">\\s*([\\d,.KM]+)', webpage, 'dislike count', default=None)),\n+            'description': clean_html(get_element_by_class('media-description', webpage))\n+        }\n \n \n class RumbleChannelIE(InfoExtractor):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/s4c.py",
            "diff": "diff --git a/yt_dlp/extractor/s4c.py b/yt_dlp/extractor/s4c.py\nindex 38a90589..67eff723 100644\n--- a/yt_dlp/extractor/s4c.py\n+++ b/yt_dlp/extractor/s4c.py\n@@ -1,5 +1,5 @@\n from .common import InfoExtractor\n-from ..utils import traverse_obj\n+from ..utils import traverse_obj, url_or_none\n \n \n class S4CIE(InfoExtractor):\n@@ -11,7 +11,8 @@ class S4CIE(InfoExtractor):\n             'ext': 'mp4',\n             'title': 'Y Swn',\n             'description': 'md5:f7681a30e4955b250b3224aa9fe70cf0',\n-            'duration': 5340\n+            'duration': 5340,\n+            'thumbnail': 'https://www.s4c.cymru/amg/1920x1080/Y_Swn_2023S4C_099_ii.jpg'\n         },\n     }, {\n         'url': 'https://www.s4c.cymru/clic/programme/856636948',\n@@ -21,6 +22,7 @@ class S4CIE(InfoExtractor):\n             'title': 'Am Dro',\n             'duration': 2880,\n             'description': 'md5:100d8686fc9a632a0cb2db52a3433ffe',\n+            'thumbnail': 'https://www.s4c.cymru/amg/1920x1080/Am_Dro_2022-23S4C_P6_4005.jpg'\n         },\n     }]\n \n@@ -30,7 +32,7 @@ def _real_extract(self, url):\n             f'https://www.s4c.cymru/df/full_prog_details?lang=e&programme_id={video_id}',\n             video_id, fatal=False)\n \n-        filename = self._download_json(\n+        player_config = self._download_json(\n             'https://player-api.s4c-cdn.co.uk/player-configuration/prod', video_id, query={\n                 'programme_id': video_id,\n                 'signed': '0',\n@@ -38,7 +40,13 @@ def _real_extract(self, url):\n                 'mode': 'od',\n                 'appId': 'clic',\n                 'streamName': '',\n-            }, note='Downloading player config JSON')['filename']\n+            }, note='Downloading player config JSON')\n+        subtitles = {}\n+        for sub in traverse_obj(player_config, ('subtitles', lambda _, v: url_or_none(v['0']))):\n+            subtitles.setdefault(sub.get('3', 'en'), []).append({\n+                'url': sub['0'],\n+                'name': sub.get('1'),\n+            })\n         m3u8_url = self._download_json(\n             'https://player-api.s4c-cdn.co.uk/streaming-urls/prod', video_id, query={\n                 'mode': 'od',\n@@ -46,17 +54,50 @@ def _real_extract(self, url):\n                 'region': 'WW',\n                 'extra': 'false',\n                 'thirdParty': 'false',\n-                'filename': filename,\n+                'filename': player_config['filename'],\n             }, note='Downloading streaming urls JSON')['hls']\n-        formats, subtitles = self._extract_m3u8_formats_and_subtitles(m3u8_url, video_id, 'mp4', m3u8_id='hls')\n \n         return {\n             'id': video_id,\n-            'formats': formats,\n+            'formats': self._extract_m3u8_formats(m3u8_url, video_id, 'mp4', m3u8_id='hls'),\n             'subtitles': subtitles,\n+            'thumbnail': url_or_none(player_config.get('poster')),\n             **traverse_obj(details, ('full_prog_details', 0, {\n                 'title': (('programme_title', 'series_title'), {str}),\n                 'description': ('full_billing', {str.strip}),\n                 'duration': ('duration', {lambda x: int(x) * 60}),\n             }), get_all=False),\n         }\n+\n+\n+class S4CSeriesIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?s4c\\.cymru/clic/series/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.s4c.cymru/clic/series/864982911',\n+        'playlist_mincount': 6,\n+        'info_dict': {\n+            'id': '864982911',\n+            'title': 'Iaith ar Daith',\n+        },\n+    }, {\n+        'url': 'https://www.s4c.cymru/clic/series/866852587',\n+        'playlist_mincount': 8,\n+        'info_dict': {\n+            'id': '866852587',\n+            'title': 'FFIT Cymru',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        series_id = self._match_id(url)\n+        series_details = self._download_json(\n+            'https://www.s4c.cymru/df/series_details', series_id, query={\n+                'lang': 'e',\n+                'series_id': series_id,\n+                'show_prog_in_series': 'Y'\n+            }, note='Downloading series details JSON')\n+\n+        return self.playlist_result(\n+            [self.url_result(f'https://www.s4c.cymru/clic/programme/{episode_id}', S4CIE, episode_id)\n+             for episode_id in traverse_obj(series_details, ('other_progs_in_series', ..., 'id'))],\n+            series_id, traverse_obj(series_details, ('full_prog_details', 0, 'series_title', {str})))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/sbscokr.py",
            "diff": "diff --git a/yt_dlp/extractor/sbscokr.py b/yt_dlp/extractor/sbscokr.py\nnew file mode 100644\nindex 00000000..001d19ee\n--- /dev/null\n+++ b/yt_dlp/extractor/sbscokr.py\n@@ -0,0 +1,200 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    int_or_none,\n+    parse_iso8601,\n+    parse_resolution,\n+    url_or_none,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class SBSCoKrIE(InfoExtractor):\n+    IE_NAME = 'sbs.co.kr'\n+    _VALID_URL = [r'https?://allvod\\.sbs\\.co\\.kr/allvod/vod(?:Package)?EndPage\\.do\\?(?:[^#]+&)?mdaId=(?P<id>\\d+)',\n+                  r'https?://programs\\.sbs\\.co\\.kr/(?:enter|drama|culture|sports|plus|mtv|kth)/[a-z0-9]+/(?:vod|clip|movie)/\\d+/(?P<id>(?:OC)?\\d+)']\n+\n+    _TESTS = [{\n+        'url': 'https://programs.sbs.co.kr/enter/dongsang2/clip/52007/OC467706746?div=main_pop_clip',\n+        'md5': 'c3f6d45e1fb5682039d94cda23c36f19',\n+        'info_dict': {\n+            'id': 'OC467706746',\n+            'ext': 'mp4',\n+            'title': '\u2018\uc544\uc2ac\uc544\uc2ac\u2019 \ubc15\uad70\u2665\ud55c\uc601\uc758 \uc0c8 \uc9d1 \uc778\ud14c\ub9ac\uc5b4 \ub300\ucca9\u2668',\n+            'description': 'md5:6a71eb1979ee4a94ea380310068ccab4',\n+            'thumbnail': 'https://img2.sbs.co.kr/ops_clip_img/2023/10/10/34c4c0f9-a9a5-4ff6-a92e-9bb4b5f6fa65915w1280.jpg',\n+            'release_timestamp': 1696889400,\n+            'release_date': '20231009',\n+            'view_count': int,\n+            'like_count': int,\n+            'duration': 238,\n+            'age_limit': 15,\n+            'series': '\ub3d9\uc0c1\uc774\ubabd2_\ub108\ub294 \ub0b4 \uc6b4\uba85',\n+            'episode': '\ub808\uc774\ub514\uc81c\uc778, \u2018\ud63c\uc804\uc784\uc2e0\uc124\u2019 \u20183\uac1c\uc6d4\u2019 \uc55e\ub2f9\uae34 \uacb0\ud63c\uc2dd \ube44\ud558\uc778\ub4dc \uc2a4\ud1a0\ub9ac \ucd5c\ucd08 \uacf5\uac1c!',\n+            'episode_number': 311,\n+        },\n+    }, {\n+        'url': 'https://allvod.sbs.co.kr/allvod/vodPackageEndPage.do?mdaId=22000489324&combiId=PA000000284&packageType=A&isFreeYN=',\n+        'md5': 'bf46b2e89fda7ae7de01f5743cef7236',\n+        'info_dict': {\n+            'id': '22000489324',\n+            'ext': 'mp4',\n+            'title': '[\ub2e4\uc2dc\ubcf4\uae30] \ud2b8\ub864\ub9ac 15\ud68c',\n+            'description': 'md5:0e55d74bef1ac55c61ae90c73ac485f4',\n+            'thumbnail': 'https://img2.sbs.co.kr/img/sbs_cms/WE/2023/02/14/arC1676333794938-1280-720.jpg',\n+            'release_timestamp': 1676325600,\n+            'release_date': '20230213',\n+            'view_count': int,\n+            'like_count': int,\n+            'duration': 5931,\n+            'age_limit': 15,\n+            'series': '\ud2b8\ub864\ub9ac',\n+            'episode': '\uc774\uac70 \ub2e4 \uac70\uc9d3\ub9d0\uc774\uc57c',\n+            'episode_number': 15,\n+        },\n+    }, {\n+        'url': 'https://programs.sbs.co.kr/enter/fourman/vod/69625/22000508948',\n+        'md5': '41e8ae4cc6c8424f4e4d76661a4becbf',\n+        'info_dict': {\n+            'id': '22000508948',\n+            'ext': 'mp4',\n+            'title': '[\ub2e4\uc2dc\ubcf4\uae30] \uc2e0\ubc1c \ubc97\uace0 \ub3cc\uc2f1\ud3ec\ub9e8 104\ud68c',\n+            'description': 'md5:c6a247383c4dd661e4b956bf4d3b586e',\n+            'thumbnail': 'https://img2.sbs.co.kr/img/sbs_cms/WE/2023/08/30/2vb1693355446261-1280-720.jpg',\n+            'release_timestamp': 1693342800,\n+            'release_date': '20230829',\n+            'view_count': int,\n+            'like_count': int,\n+            'duration': 7036,\n+            'age_limit': 15,\n+            'series': '\uc2e0\ubc1c \ubc97\uace0 \ub3cc\uc2f1\ud3ec\ub9e8',\n+            'episode': '\ub3cc\uc2f1\ud3ec\ub9e8 \uc800\uaca9\uc218\ub4e4 \ub4f1\uc7a5!',\n+            'episode_number': 104,\n+        },\n+    }]\n+\n+    def _call_api(self, video_id, rscuse=''):\n+        return self._download_json(\n+            f'https://api.play.sbs.co.kr/1.0/sbs_vodall/{video_id}', video_id,\n+            note=f'Downloading m3u8 information {rscuse}',\n+            query={\n+                'platform': 'pcweb',\n+                'protocol': 'download',\n+                'absolute_show': 'Y',\n+                'service': 'program',\n+                'ssl': 'Y',\n+                'rscuse': rscuse,\n+            })\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+\n+        details = self._call_api(video_id)\n+        source = traverse_obj(details, ('vod', 'source', 'mediasource', {dict})) or {}\n+\n+        formats = []\n+        for stream in traverse_obj(details, (\n+            'vod', 'source', 'mediasourcelist', lambda _, v: v['mediaurl'] or v['mediarscuse']\n+        ), default=[source]):\n+            if not stream.get('mediaurl'):\n+                new_source = traverse_obj(\n+                    self._call_api(video_id, rscuse=stream['mediarscuse']),\n+                    ('vod', 'source', 'mediasource', {dict})) or {}\n+                if new_source.get('mediarscuse') == source.get('mediarscuse') or not new_source.get('mediaurl'):\n+                    continue\n+                stream = new_source\n+            formats.append({\n+                'url': stream['mediaurl'],\n+                'format_id': stream.get('mediarscuse'),\n+                'format_note': stream.get('medianame'),\n+                **parse_resolution(stream.get('quality')),\n+                'preference': int_or_none(stream.get('mediarscuse'))\n+            })\n+\n+        caption_url = traverse_obj(details, ('vod', 'source', 'subtitle', {url_or_none}))\n+\n+        return {\n+            'id': video_id,\n+            **traverse_obj(details, ('vod', {\n+                'title': ('info', 'title'),\n+                'duration': ('info', 'duration', {int_or_none}),\n+                'view_count': ('info', 'viewcount', {int_or_none}),\n+                'like_count': ('info', 'likecount', {int_or_none}),\n+                'description': ('info', 'synopsis', {clean_html}),\n+                'episode': ('info', 'content', ('contenttitle', 'title')),\n+                'episode_number': ('info', 'content', 'number', {int_or_none}),\n+                'series': ('info', 'program', 'programtitle'),\n+                'age_limit': ('info', 'targetage', {int_or_none}),\n+                'release_timestamp': ('info', 'broaddate', {parse_iso8601}),\n+                'thumbnail': ('source', 'thumbnail', 'origin', {url_or_none}),\n+            }), get_all=False),\n+            'formats': formats,\n+            'subtitles': {'ko': [{'url': caption_url}]} if caption_url else None,\n+        }\n+\n+\n+class SBSCoKrAllvodProgramIE(InfoExtractor):\n+    IE_NAME = 'sbs.co.kr:allvod_program'\n+    _VALID_URL = r'https?://allvod\\.sbs\\.co\\.kr/allvod/vod(?:Free)?ProgramDetail\\.do\\?(?:[^#]+&)?pgmId=(?P<id>P?\\d+)'\n+\n+    _TESTS = [{\n+        'url': 'https://allvod.sbs.co.kr/allvod/vodFreeProgramDetail.do?type=legend&pgmId=22000010159&listOrder=vodCntAsc',\n+        'info_dict': {\n+            '_type': 'playlist',\n+            'id': '22000010159',\n+        },\n+        'playlist_count': 18,\n+    }, {\n+        'url': 'https://allvod.sbs.co.kr/allvod/vodProgramDetail.do?pgmId=P460810577',\n+        'info_dict': {\n+            '_type': 'playlist',\n+            'id': 'P460810577',\n+        },\n+        'playlist_count': 13,\n+    }]\n+\n+    def _real_extract(self, url):\n+        program_id = self._match_id(url)\n+\n+        details = self._download_json(\n+            'https://allvod.sbs.co.kr/allvod/vodProgramDetail/vodProgramDetailAjax.do',\n+            program_id, note='Downloading program details',\n+            query={\n+                'pgmId': program_id,\n+                'currentCount': '10000',\n+            })\n+\n+        return self.playlist_result(\n+            [self.url_result(f'https://allvod.sbs.co.kr/allvod/vodEndPage.do?mdaId={video_id}', SBSCoKrIE)\n+             for video_id in traverse_obj(details, ('list', ..., 'mdaId'))], program_id)\n+\n+\n+class SBSCoKrProgramsVodIE(InfoExtractor):\n+    IE_NAME = 'sbs.co.kr:programs_vod'\n+    _VALID_URL = r'https?://programs\\.sbs\\.co\\.kr/(?:enter|drama|culture|sports|plus|mtv)/(?P<id>[a-z0-9]+)/vods'\n+\n+    _TESTS = [{\n+        'url': 'https://programs.sbs.co.kr/culture/morningwide/vods/65007',\n+        'info_dict': {\n+            '_type': 'playlist',\n+            'id': '00000210215',\n+        },\n+        'playlist_mincount': 9782,\n+    }, {\n+        'url': 'https://programs.sbs.co.kr/enter/dongsang2/vods/52006',\n+        'info_dict': {\n+            '_type': 'playlist',\n+            'id': '22000010476',\n+        },\n+        'playlist_mincount': 312,\n+    }]\n+\n+    def _real_extract(self, url):\n+        program_slug = self._match_id(url)\n+\n+        program_id = self._download_json(\n+            f'https://static.apis.sbs.co.kr/program-api/1.0/menu/{program_slug}', program_slug,\n+            note='Downloading program menu data')['program']['programid']\n+\n+        return self.url_result(\n+            f'https://allvod.sbs.co.kr/allvod/vodProgramDetail.do?pgmId={program_id}', SBSCoKrAllvodProgramIE)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/scrippsnetworks.py",
            "diff": "diff --git a/yt_dlp/extractor/scrippsnetworks.py b/yt_dlp/extractor/scrippsnetworks.py\nindex adfd7e5f..3912f778 100644\n--- a/yt_dlp/extractor/scrippsnetworks.py\n+++ b/yt_dlp/extractor/scrippsnetworks.py\n@@ -39,6 +39,7 @@ class ScrippsNetworksWatchIE(AWSIE):\n             'skip_download': True,\n         },\n         'add_ie': [AnvatoIE.ie_key()],\n+        'skip': '404 Not Found',\n     }]\n \n     _SNI_TABLE = {\n@@ -113,6 +114,11 @@ class ScrippsNetworksIE(InfoExtractor):\n             'timestamp': 1475678834,\n             'upload_date': '20161005',\n             'uploader': 'SCNI-SCND',\n+            'tags': 'count:10',\n+            'creator': 'Cooking Channel',\n+            'duration': 29.995,\n+            'chapters': [{'start_time': 0.0, 'end_time': 29.995, 'title': '<Untitled Chapter 1>'}],\n+            'thumbnail': 'https://images.dds.discovery.com/up/tp/Scripps_-_Food_Category_Prod/122/987/0260338_630x355.jpg',\n         },\n         'add_ie': ['ThePlatform'],\n         'expected_warnings': ['No HLS formats found'],\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/scte.py",
            "diff": "diff --git a/yt_dlp/extractor/scte.py b/yt_dlp/extractor/scte.py\nindex d839ffcd..9c2ca8c5 100644\n--- a/yt_dlp/extractor/scte.py\n+++ b/yt_dlp/extractor/scte.py\n@@ -46,6 +46,7 @@ def is_logged(webpage):\n \n \n class SCTEIE(SCTEBaseIE):\n+    _WORKING = False\n     _VALID_URL = r'https?://learning\\.scte\\.org/mod/scorm/view\\.php?.*?\\bid=(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://learning.scte.org/mod/scorm/view.php?id=31484',\n@@ -93,6 +94,7 @@ def _real_extract(self, url):\n \n \n class SCTECourseIE(SCTEBaseIE):\n+    _WORKING = False\n     _VALID_URL = r'https?://learning\\.scte\\.org/(?:mod/sub)?course/view\\.php?.*?\\bid=(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://learning.scte.org/mod/subcourse/view.php?id=31491',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/shared.py",
            "diff": "diff --git a/yt_dlp/extractor/shared.py b/yt_dlp/extractor/shared.py\ndeleted file mode 100644\nindex 9a237b32..00000000\n--- a/yt_dlp/extractor/shared.py\n+++ /dev/null\n@@ -1,138 +0,0 @@\n-import urllib.parse\n-\n-from .common import InfoExtractor\n-from ..compat import compat_b64decode\n-from ..utils import (\n-    KNOWN_EXTENSIONS,\n-    ExtractorError,\n-    determine_ext,\n-    int_or_none,\n-    js_to_json,\n-    parse_filesize,\n-    rot47,\n-    url_or_none,\n-    urlencode_postdata,\n-)\n-\n-\n-class SharedBaseIE(InfoExtractor):\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage, urlh = self._download_webpage_handle(url, video_id)\n-\n-        if self._FILE_NOT_FOUND in webpage:\n-            raise ExtractorError(\n-                'Video %s does not exist' % video_id, expected=True)\n-\n-        video_url = self._extract_video_url(webpage, video_id, url)\n-\n-        title = self._extract_title(webpage)\n-        filesize = int_or_none(self._extract_filesize(webpage))\n-\n-        return {\n-            'id': video_id,\n-            'url': video_url,\n-            'ext': 'mp4',\n-            'filesize': filesize,\n-            'title': title,\n-        }\n-\n-    def _extract_title(self, webpage):\n-        return compat_b64decode(self._html_search_meta(\n-            'full:title', webpage, 'title')).decode('utf-8')\n-\n-    def _extract_filesize(self, webpage):\n-        return self._html_search_meta(\n-            'full:size', webpage, 'file size', fatal=False)\n-\n-\n-class SharedIE(SharedBaseIE):\n-    IE_DESC = 'shared.sx'\n-    _VALID_URL = r'https?://shared\\.sx/(?P<id>[\\da-z]{10})'\n-    _FILE_NOT_FOUND = '>File does not exist<'\n-\n-    _TEST = {\n-        'url': 'http://shared.sx/0060718775',\n-        'md5': '106fefed92a8a2adb8c98e6a0652f49b',\n-        'info_dict': {\n-            'id': '0060718775',\n-            'ext': 'mp4',\n-            'title': 'Bmp4',\n-            'filesize': 1720110,\n-        },\n-    }\n-\n-    def _extract_video_url(self, webpage, video_id, url):\n-        download_form = self._hidden_inputs(webpage)\n-\n-        video_page = self._download_webpage(\n-            url, video_id, 'Downloading video page',\n-            data=urlencode_postdata(download_form),\n-            headers={\n-                'Content-Type': 'application/x-www-form-urlencoded',\n-                'Referer': url,\n-            })\n-\n-        video_url = self._html_search_regex(\n-            r'data-url=([\"\\'])(?P<url>(?:(?!\\1).)+)\\1',\n-            video_page, 'video URL', group='url')\n-\n-        return video_url\n-\n-\n-class VivoIE(SharedBaseIE):\n-    IE_DESC = 'vivo.sx'\n-    _VALID_URL = r'https?://vivo\\.s[xt]/(?P<id>[\\da-z]{10})'\n-    _FILE_NOT_FOUND = '>The file you have requested does not exists or has been removed'\n-\n-    _TESTS = [{\n-        'url': 'http://vivo.sx/d7ddda0e78',\n-        'md5': '15b3af41be0b4fe01f4df075c2678b2c',\n-        'info_dict': {\n-            'id': 'd7ddda0e78',\n-            'ext': 'mp4',\n-            'title': 'Chicken',\n-            'filesize': 515659,\n-        },\n-    }, {\n-        'url': 'http://vivo.st/d7ddda0e78',\n-        'only_matching': True,\n-    }]\n-\n-    def _extract_title(self, webpage):\n-        title = self._html_search_regex(\n-            r'data-name\\s*=\\s*([\"\\'])(?P<title>(?:(?!\\1).)+)\\1', webpage,\n-            'title', default=None, group='title')\n-        if title:\n-            ext = determine_ext(title)\n-            if ext.lower() in KNOWN_EXTENSIONS:\n-                title = title.rpartition('.' + ext)[0]\n-            return title\n-        return self._og_search_title(webpage)\n-\n-    def _extract_filesize(self, webpage):\n-        return parse_filesize(self._search_regex(\n-            r'data-type=[\"\\']video[\"\\'][^>]*>Watch.*?<strong>\\s*\\((.+?)\\)',\n-            webpage, 'filesize', fatal=False))\n-\n-    def _extract_video_url(self, webpage, video_id, url):\n-        def decode_url_old(encoded_url):\n-            return compat_b64decode(encoded_url).decode('utf-8')\n-\n-        stream_url = self._search_regex(\n-            r'data-stream\\s*=\\s*([\"\\'])(?P<url>(?:(?!\\1).)+)\\1', webpage,\n-            'stream url', default=None, group='url')\n-        if stream_url:\n-            stream_url = url_or_none(decode_url_old(stream_url))\n-        if stream_url:\n-            return stream_url\n-\n-        def decode_url(encoded_url):\n-            return rot47(urllib.parse.unquote_plus(encoded_url))\n-\n-        return decode_url(self._parse_json(\n-            self._search_regex(\n-                r'(?s)InitializeStream\\s*\\(\\s*({.+?})\\s*\\)\\s*;', webpage,\n-                'stream'),\n-            video_id, transform_source=js_to_json)['source'])\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/sina.py",
            "diff": "diff --git a/yt_dlp/extractor/sina.py b/yt_dlp/extractor/sina.py\nindex 98428118..eeb9ebb4 100644\n--- a/yt_dlp/extractor/sina.py\n+++ b/yt_dlp/extractor/sina.py\n@@ -11,7 +11,7 @@\n \n \n class SinaIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)https?://(?:.*?\\.)?video\\.sina\\.com\\.cn/\n+    _VALID_URL = r'''(?x)https?://(?:[^/?#]+\\.)?video\\.sina\\.com\\.cn/\n                         (?:\n                             (?:view/|.*\\#)(?P<id>\\d+)|\n                             .+?/(?P<pseudo_id>[^/?#]+)(?:\\.s?html)|\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/sky.py",
            "diff": "diff --git a/yt_dlp/extractor/sky.py b/yt_dlp/extractor/sky.py\nindex 0a8b6cc7..574ac219 100644\n--- a/yt_dlp/extractor/sky.py\n+++ b/yt_dlp/extractor/sky.py\n@@ -3,9 +3,7 @@\n from .common import InfoExtractor\n from ..utils import (\n     extract_attributes,\n-    smuggle_url,\n     strip_or_none,\n-    urljoin,\n )\n \n \n@@ -13,29 +11,10 @@ class SkyBaseIE(InfoExtractor):\n     BRIGHTCOVE_URL_TEMPLATE = 'http://players.brightcove.net/%s/%s_default/index.html?videoId=%s'\n     _SDC_EL_REGEX = r'(?s)(<div[^>]+data-(?:component-name|fn)=\"sdc-(?:articl|sit)e-video\"[^>]*>)'\n \n-    def _process_ooyala_element(self, webpage, sdc_el, url):\n+    def _process_video_element(self, webpage, sdc_el, url):\n         sdc = extract_attributes(sdc_el)\n         provider = sdc.get('data-provider')\n-        if provider == 'ooyala':\n-            video_id = sdc['data-sdc-video-id']\n-            video_url = 'ooyala:%s' % video_id\n-            ie_key = 'Ooyala'\n-            ooyala_el = self._search_regex(\n-                r'(<div[^>]+class=\"[^\"]*\\bsdc-article-video__media-ooyala\\b[^\"]*\"[^>]+data-video-id=\"%s\"[^>]*>)' % video_id,\n-                webpage, 'video data', fatal=False)\n-            if ooyala_el:\n-                ooyala_attrs = extract_attributes(ooyala_el) or {}\n-                if ooyala_attrs.get('data-token-required') == 'true':\n-                    token_fetch_url = (self._parse_json(ooyala_attrs.get(\n-                        'data-token-fetch-options', '{}'),\n-                        video_id, fatal=False) or {}).get('url')\n-                    if token_fetch_url:\n-                        embed_token = self._download_json(urljoin(\n-                            url, token_fetch_url), video_id, fatal=False)\n-                        if embed_token:\n-                            video_url = smuggle_url(\n-                                video_url, {'embed_token': embed_token})\n-        elif provider == 'brightcove':\n+        if provider == 'brightcove':\n             video_id = sdc['data-video-id']\n             account_id = sdc.get('data-account-id') or '6058004172001'\n             player_id = sdc.get('data-player-id') or 'RC9PQUaJ6'\n@@ -52,7 +31,7 @@ def _process_ooyala_element(self, webpage, sdc_el, url):\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n         webpage = self._download_webpage(url, video_id)\n-        info = self._process_ooyala_element(webpage, self._search_regex(\n+        info = self._process_video_element(webpage, self._search_regex(\n             self._SDC_EL_REGEX, webpage, 'sdc element'), url)\n         info.update({\n             'title': self._og_search_title(webpage),\n@@ -73,7 +52,7 @@ class SkySportsIE(SkyBaseIE):\n             'title': 'Bale: It\\'s our time to shine',\n             'description': 'md5:e88bda94ae15f7720c5cb467e777bb6d',\n         },\n-        'add_ie': ['Ooyala'],\n+        'add_ie': ['BrightcoveNew'],\n     }, {\n         'url': 'https://www.skysports.com/watch/video/sports/f1/12160544/abu-dhabi-gp-the-notebook',\n         'only_matching': True,\n@@ -122,7 +101,7 @@ def _real_extract(self, url):\n         article_id = self._match_id(url)\n         webpage = self._download_webpage(url, article_id)\n \n-        entries = [self._process_ooyala_element(webpage, sdc_el, url)\n+        entries = [self._process_video_element(webpage, sdc_el, url)\n                    for sdc_el in re.findall(self._SDC_EL_REGEX, webpage)]\n \n         return self.playlist_result(\n@@ -149,7 +128,7 @@ def _real_extract(self, url):\n \n         entries = []\n         for sdc_el in re.findall(self._SDC_EL_REGEX, webpage):\n-            entries.append(self._process_ooyala_element(webpage, sdc_el, url))\n+            entries.append(self._process_video_element(webpage, sdc_el, url))\n \n         return self.playlist_result(\n             entries, article_id, self._og_search_title(webpage),\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/slideslive.py",
            "diff": "diff --git a/yt_dlp/extractor/slideslive.py b/yt_dlp/extractor/slideslive.py\nindex 25f867a6..df2af3b3 100644\n--- a/yt_dlp/extractor/slideslive.py\n+++ b/yt_dlp/extractor/slideslive.py\n@@ -1,5 +1,6 @@\n import re\n import urllib.parse\n+import xml.etree.ElementTree\n \n from .common import InfoExtractor\n from ..utils import (\n@@ -469,11 +470,12 @@ def _real_extract(self, url):\n             slides = self._download_xml(\n                 player_info['slides_xml_url'], video_id, fatal=False,\n                 note='Downloading slides XML', errnote='Failed to download slides info')\n-            slide_url_template = 'https://cdn.slideslive.com/data/presentations/%s/slides/big/%s%s'\n-            for slide_id, slide in enumerate(slides.findall('./slide') if slides else [], 1):\n-                slides_info.append((\n-                    slide_id, xpath_text(slide, './slideName', 'name'), '.jpg',\n-                    int_or_none(xpath_text(slide, './timeSec', 'time'))))\n+            if isinstance(slides, xml.etree.ElementTree.Element):\n+                slide_url_template = 'https://cdn.slideslive.com/data/presentations/%s/slides/big/%s%s'\n+                for slide_id, slide in enumerate(slides.findall('./slide')):\n+                    slides_info.append((\n+                        slide_id, xpath_text(slide, './slideName', 'name'), '.jpg',\n+                        int_or_none(xpath_text(slide, './timeSec', 'time'))))\n \n         chapters, thumbnails = [], []\n         if url_or_none(player_info.get('thumbnail')):\n@@ -528,7 +530,7 @@ def _real_extract(self, url):\n             if service_name == 'vimeo':\n                 info['url'] = smuggle_url(\n                     f'https://player.vimeo.com/video/{service_id}',\n-                    {'http_headers': {'Referer': url}})\n+                    {'referer': url})\n \n         video_slides = traverse_obj(slides, ('slides', ..., 'video', 'id'))\n         if not video_slides:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/sohu.py",
            "diff": "diff --git a/yt_dlp/extractor/sohu.py b/yt_dlp/extractor/sohu.py\nindex a8f1e462..c0ff4f9a 100644\n--- a/yt_dlp/extractor/sohu.py\n+++ b/yt_dlp/extractor/sohu.py\n@@ -1,3 +1,4 @@\n+import base64\n import re\n \n from .common import InfoExtractor\n@@ -8,7 +9,12 @@\n from ..utils import (\n     ExtractorError,\n     int_or_none,\n+    float_or_none,\n+    url_or_none,\n+    unified_timestamp,\n     try_get,\n+    urljoin,\n+    traverse_obj,\n )\n \n \n@@ -31,13 +37,20 @@ class SohuIE(InfoExtractor):\n             'id': '409385080',\n             'ext': 'mp4',\n             'title': '\u300a2015\u6e56\u5357\u536b\u89c6\u7f8a\u5e74\u5143\u5bb5\u665a\u4f1a\u300b\u5510\u5ae3\u300a\u82b1\u597d\u6708\u5706\u300b',\n-        }\n+        },\n+        'skip': 'no longer available',\n     }, {\n         'url': 'http://my.tv.sohu.com/us/232799889/78693464.shtml',\n         'info_dict': {\n             'id': '78693464',\n             'ext': 'mp4',\n             'title': '\u3010\u7231\u8303\u54c1\u3011\u7b2c31\u671f\uff1aMWC\u89c1\u4e0d\u5230\u7684\u5947\u8469\u624b\u673a',\n+            'uploader': '\u7231\u8303\u513f\u89c6\u9891',\n+            'duration': 213,\n+            'timestamp': 1425519600,\n+            'upload_date': '20150305',\n+            'thumbnail': 'http://e3f49eaa46b57.cdn.sohucs.com//group1/M10/83/FA/MTAuMTAuODguODA=/6_14cbccdde5eg104SysCutcloud_78693464_7_0b.jpg',\n+            'tags': ['\u7231\u8303\u513f', '\u7231\u8303\u54c1', 'MWC', '\u624b\u673a'],\n         }\n     }, {\n         'note': 'Multipart video',\n@@ -45,6 +58,12 @@ class SohuIE(InfoExtractor):\n         'info_dict': {\n             'id': '78910339',\n             'title': '\u3010\u795e\u63a2\u82cd\u5b9e\u6218\u79d8\u7c4d\u3011\u7b2c13\u671f \u6218\u4e89\u4e4b\u5f71 \u8d6b\u5361\u91cc\u59c6',\n+            'uploader': '\u5c0f\u82cdcany',\n+            'duration': 744.0,\n+            'timestamp': 1426269360,\n+            'upload_date': '20150313',\n+            'thumbnail': 'http://e3f49eaa46b57.cdn.sohucs.com//group1/M11/89/57/MTAuMTAuODguODA=/6_14cea022a1dg102SysCutcloud_78910339_8_0b.jpg',\n+            'tags': ['\u5c0f\u82cdMM', '\u82f1\u96c4\u8054\u76df', '\u5b9e\u6218\u79d8\u7c4d'],\n         },\n         'playlist': [{\n             'info_dict': {\n@@ -75,6 +94,11 @@ class SohuIE(InfoExtractor):\n             'id': '78932792',\n             'ext': 'mp4',\n             'title': 'youtube-dl testing video',\n+            'duration': 360,\n+            'timestamp': 1426348620,\n+            'upload_date': '20150314',\n+            'thumbnail': 'http://e3f49eaa46b57.cdn.sohucs.com//group1/M02/8A/00/MTAuMTAuODguNzk=/6_14cee1be192g102SysCutcloud_78932792_7_7b.jpg',\n+            'tags': [],\n         },\n         'params': {\n             'skip_download': True\n@@ -100,7 +124,7 @@ def _fetch_data(vid_id, mytv=False):\n \n         webpage = self._download_webpage(url, video_id)\n \n-        title = re.sub(r' - \u641c\u72d0\u89c6\u9891$', '', self._og_search_title(webpage))\n+        title = re.sub(r'( - \u9ad8\u6e05\u6b63\u7248\u5728\u7ebf\u89c2\u770b)? - \u641c\u72d0\u89c6\u9891$', '', self._og_search_title(webpage))\n \n         vid = self._html_search_regex(\n             r'var vid ?= ?[\"\\'](\\d+)[\"\\']',\n@@ -132,7 +156,9 @@ def _fetch_data(vid_id, mytv=False):\n                 allot = format_data['allot']\n \n                 data = format_data['data']\n-                clips_url = data['clipsURL']\n+                clip_url = traverse_obj(data, (('clipsURL', 'mp4PlayUrl'), i, {url_or_none}), get_all=False)\n+                if not clip_url:\n+                    raise ExtractorError(f'Unable to extract url for clip {i}')\n                 su = data['su']\n \n                 video_url = 'newflv.sohu.ccgslb.net'\n@@ -142,9 +168,9 @@ def _fetch_data(vid_id, mytv=False):\n                 while 'newflv.sohu.ccgslb.net' in video_url:\n                     params = {\n                         'prot': 9,\n-                        'file': clips_url[i],\n+                        'file': clip_url,\n                         'new': su[i],\n-                        'prod': 'flash',\n+                        'prod': 'h5n',\n                         'rb': 1,\n                     }\n \n@@ -193,6 +219,75 @@ def _fetch_data(vid_id, mytv=False):\n                 'entries': playlist,\n                 'id': video_id,\n                 'title': title,\n+                'duration': traverse_obj(vid_data, ('data', 'totalDuration', {float_or_none})),\n             }\n \n-        return info\n+        if mytv:\n+            publish_time = unified_timestamp(self._search_regex(\n+                r'publishTime:\\s*[\"\\'](\\d+-\\d+-\\d+ \\d+:\\d+)[\"\\']', webpage, 'publish time', fatal=False))\n+        else:\n+            publish_time = traverse_obj(vid_data, ('tv_application_time', {unified_timestamp}))\n+\n+        return {\n+            'timestamp': publish_time - 8 * 3600 if publish_time else None,\n+            **traverse_obj(vid_data, {\n+                'alt_title': ('data', 'subName', {str}),\n+                'uploader': ('wm_data', 'wm_username', {str}),\n+                'thumbnail': ('data', 'coverImg', {url_or_none}),\n+                'tags': ('data', 'tag', {str.split}),\n+            }),\n+            **info,\n+        }\n+\n+\n+class SohuVIE(InfoExtractor):\n+    _VALID_URL = r'https?://tv\\.sohu\\.com/v/(?P<id>[\\w=-]+)\\.html(?:$|[#?])'\n+\n+    _TESTS = [{\n+        'note': 'Multipart video',\n+        'url': 'https://tv.sohu.com/v/MjAyMzA2MTQvbjYwMTMxNTE5Mi5zaHRtbA==.html',\n+        'info_dict': {\n+            'id': '601315192',\n+            'title': '\u300a\u6dec\u706b\u4e39\u5fc3\u300b\u7b2c1\u96c6',\n+            'alt_title': '\u201c\u70b9\u5929\u706f\u201d\u53d1\u751f\u4e8b\u6545',\n+            'duration': 2701.692,\n+            'timestamp': 1686758040,\n+            'upload_date': '20230614',\n+            'thumbnail': 'http://photocdn.tv.sohu.com/img/20230614/vrsa_hor_1686738763256_454010551.jpg',\n+        },\n+        'playlist_mincount': 9,\n+        'skip': 'Only available in China',\n+    }, {\n+        'url': 'https://tv.sohu.com/v/dXMvMjMyNzk5ODg5Lzc4NjkzNDY0LnNodG1s.html',\n+        'info_dict': {\n+            'id': '78693464',\n+            'ext': 'mp4',\n+            'title': '\u3010\u7231\u8303\u54c1\u3011\u7b2c31\u671f\uff1aMWC\u89c1\u4e0d\u5230\u7684\u5947\u8469\u624b\u673a',\n+            'uploader': '\u7231\u8303\u513f\u89c6\u9891',\n+            'duration': 213,\n+            'timestamp': 1425519600,\n+            'upload_date': '20150305',\n+            'thumbnail': 'http://e3f49eaa46b57.cdn.sohucs.com//group1/M10/83/FA/MTAuMTAuODguODA=/6_14cbccdde5eg104SysCutcloud_78693464_7_0b.jpg',\n+            'tags': ['\u7231\u8303\u513f', '\u7231\u8303\u54c1', 'MWC', '\u624b\u673a'],\n+        }\n+    }, {\n+        'note': 'Multipart video',\n+        'url': 'https://tv.sohu.com/v/dXMvMjQyNTYyMTYzLzc4OTEwMzM5LnNodG1s.html?src=pl',\n+        'info_dict': {\n+            'id': '78910339',\n+            'title': '\u3010\u795e\u63a2\u82cd\u5b9e\u6218\u79d8\u7c4d\u3011\u7b2c13\u671f \u6218\u4e89\u4e4b\u5f71 \u8d6b\u5361\u91cc\u59c6',\n+            'uploader': '\u5c0f\u82cdcany',\n+            'duration': 744.0,\n+            'timestamp': 1426269360,\n+            'upload_date': '20150313',\n+            'thumbnail': 'http://e3f49eaa46b57.cdn.sohucs.com//group1/M11/89/57/MTAuMTAuODguODA=/6_14cea022a1dg102SysCutcloud_78910339_8_0b.jpg',\n+            'tags': ['\u5c0f\u82cdMM', '\u82f1\u96c4\u8054\u76df', '\u5b9e\u6218\u79d8\u7c4d'],\n+        },\n+        'playlist_mincount': 3,\n+    }]\n+\n+    def _real_extract(self, url):\n+        encoded_id = self._match_id(url)\n+        path = base64.urlsafe_b64decode(encoded_id).decode()\n+        subdomain = 'tv' if re.match(r'\\d+/n\\d+\\.shtml', path) else 'my.tv'\n+        return self.url_result(urljoin(f'http://{subdomain}.sohu.com/', path), SohuIE)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/sovietscloset.py",
            "diff": "diff --git a/yt_dlp/extractor/sovietscloset.py b/yt_dlp/extractor/sovietscloset.py\nindex 453016cc..493eea2a 100644\n--- a/yt_dlp/extractor/sovietscloset.py\n+++ b/yt_dlp/extractor/sovietscloset.py\n@@ -76,7 +76,6 @@ class SovietsClosetIE(SovietsClosetBaseIE):\n                 'title': 'Arma 3 - Zeus Games #5',\n                 'uploader': 'SovietWomble',\n                 'thumbnail': r're:^https?://.*\\.b-cdn\\.net/c0e5e76f-3a93-40b4-bf01-12343c2eec5d/thumbnail\\.jpg$',\n-                'uploader': 'SovietWomble',\n                 'creator': 'SovietWomble',\n                 'release_timestamp': 1461157200,\n                 'release_date': '20160420',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/spankwire.py",
            "diff": "diff --git a/yt_dlp/extractor/spankwire.py b/yt_dlp/extractor/spankwire.py\ndeleted file mode 100644\nindex 334b2977..00000000\n--- a/yt_dlp/extractor/spankwire.py\n+++ /dev/null\n@@ -1,174 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    float_or_none,\n-    int_or_none,\n-    merge_dicts,\n-    str_or_none,\n-    str_to_int,\n-    url_or_none,\n-)\n-\n-\n-class SpankwireIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?:www\\.)?spankwire\\.com/\n-                        (?:\n-                            [^/]+/video|\n-                            EmbedPlayer\\.aspx/?\\?.*?\\bArticleId=\n-                        )\n-                        (?P<id>\\d+)\n-                    '''\n-    _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)?//(?:www\\.)?spankwire\\.com/EmbedPlayer\\.aspx/?\\?.*?\\bArticleId=\\d+)']\n-    _TESTS = [{\n-        # download URL pattern: */<height>P_<tbr>K_<video_id>.mp4\n-        'url': 'http://www.spankwire.com/Buckcherry-s-X-Rated-Music-Video-Crazy-Bitch/video103545/',\n-        'md5': '5aa0e4feef20aad82cbcae3aed7ab7cd',\n-        'info_dict': {\n-            'id': '103545',\n-            'ext': 'mp4',\n-            'title': 'Buckcherry`s X Rated Music Video Crazy Bitch',\n-            'description': 'Crazy Bitch X rated music video.',\n-            'duration': 222,\n-            'uploader': 'oreusz',\n-            'uploader_id': '124697',\n-            'timestamp': 1178587885,\n-            'upload_date': '20070508',\n-            'average_rating': float,\n-            'view_count': int,\n-            'comment_count': int,\n-            'age_limit': 18,\n-            'categories': list,\n-            'tags': list,\n-        },\n-    }, {\n-        # download URL pattern: */mp4_<format_id>_<video_id>.mp4\n-        'url': 'http://www.spankwire.com/Titcums-Compiloation-I/video1921551/',\n-        'md5': '09b3c20833308b736ae8902db2f8d7e6',\n-        'info_dict': {\n-            'id': '1921551',\n-            'ext': 'mp4',\n-            'title': 'Titcums Compiloation I',\n-            'description': 'cum on tits',\n-            'uploader': 'dannyh78999',\n-            'uploader_id': '3056053',\n-            'upload_date': '20150822',\n-            'age_limit': 18,\n-        },\n-        'params': {\n-            'proxy': '127.0.0.1:8118'\n-        },\n-        'skip': 'removed',\n-    }, {\n-        'url': 'https://www.spankwire.com/EmbedPlayer.aspx/?ArticleId=156156&autostart=true',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        video = self._download_json(\n-            'https://www.spankwire.com/api/video/%s.json' % video_id, video_id)\n-\n-        title = video['title']\n-\n-        formats = []\n-        videos = video.get('videos')\n-        if isinstance(videos, dict):\n-            for format_id, format_url in videos.items():\n-                video_url = url_or_none(format_url)\n-                if not format_url:\n-                    continue\n-                height = int_or_none(self._search_regex(\n-                    r'(\\d+)[pP]', format_id, 'height', default=None))\n-                m = re.search(\n-                    r'/(?P<height>\\d+)[pP]_(?P<tbr>\\d+)[kK]', video_url)\n-                if m:\n-                    tbr = int(m.group('tbr'))\n-                    height = height or int(m.group('height'))\n-                else:\n-                    tbr = None\n-                formats.append({\n-                    'url': video_url,\n-                    'format_id': '%dp' % height if height else format_id,\n-                    'height': height,\n-                    'tbr': tbr,\n-                })\n-        m3u8_url = url_or_none(video.get('HLS'))\n-        if m3u8_url:\n-            formats.extend(self._extract_m3u8_formats(\n-                m3u8_url, video_id, 'mp4', entry_protocol='m3u8_native',\n-                m3u8_id='hls', fatal=False))\n-\n-        view_count = str_to_int(video.get('viewed'))\n-\n-        thumbnails = []\n-        for preference, t in enumerate(('', '2x'), start=0):\n-            thumbnail_url = url_or_none(video.get('poster%s' % t))\n-            if not thumbnail_url:\n-                continue\n-            thumbnails.append({\n-                'url': thumbnail_url,\n-                'preference': preference,\n-            })\n-\n-        def extract_names(key):\n-            entries_list = video.get(key)\n-            if not isinstance(entries_list, list):\n-                return\n-            entries = []\n-            for entry in entries_list:\n-                name = str_or_none(entry.get('name'))\n-                if name:\n-                    entries.append(name)\n-            return entries\n-\n-        categories = extract_names('categories')\n-        tags = extract_names('tags')\n-\n-        uploader = None\n-        info = {}\n-\n-        webpage = self._download_webpage(\n-            'https://www.spankwire.com/_/video%s/' % video_id, video_id,\n-            fatal=False)\n-        if webpage:\n-            info = self._search_json_ld(webpage, video_id, default={})\n-            thumbnail_url = None\n-            if 'thumbnail' in info:\n-                thumbnail_url = url_or_none(info['thumbnail'])\n-                del info['thumbnail']\n-            if not thumbnail_url:\n-                thumbnail_url = self._og_search_thumbnail(webpage)\n-            if thumbnail_url:\n-                thumbnails.append({\n-                    'url': thumbnail_url,\n-                    'preference': 10,\n-                })\n-            uploader = self._html_search_regex(\n-                r'(?s)by\\s*<a[^>]+\\bclass=[\"\\']uploaded__by[^>]*>(.+?)</a>',\n-                webpage, 'uploader', fatal=False)\n-            if not view_count:\n-                view_count = str_to_int(self._search_regex(\n-                    r'data-views=[\"\\']([\\d,.]+)', webpage, 'view count',\n-                    fatal=False))\n-\n-        return merge_dicts({\n-            'id': video_id,\n-            'title': title,\n-            'description': video.get('description'),\n-            'duration': int_or_none(video.get('duration')),\n-            'thumbnails': thumbnails,\n-            'uploader': uploader,\n-            'uploader_id': str_or_none(video.get('userId')),\n-            'timestamp': int_or_none(video.get('time_approved_on')),\n-            'average_rating': float_or_none(video.get('rating')),\n-            'view_count': view_count,\n-            'comment_count': int_or_none(video.get('comments')),\n-            'age_limit': 18,\n-            'categories': categories,\n-            'tags': tags,\n-            'formats': formats,\n-        }, info)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/srmediathek.py",
            "diff": "diff --git a/yt_dlp/extractor/srmediathek.py b/yt_dlp/extractor/srmediathek.py\nindex 3cc39870..f0b3b585 100644\n--- a/yt_dlp/extractor/srmediathek.py\n+++ b/yt_dlp/extractor/srmediathek.py\n@@ -6,6 +6,7 @@\n \n \n class SRMediathekIE(ARDMediathekBaseIE):\n+    _WORKING = False\n     IE_NAME = 'sr:mediathek'\n     IE_DESC = 'Saarl\u00e4ndischer Rundfunk'\n     _VALID_URL = r'https?://sr-mediathek(?:\\.sr-online)?\\.de/index\\.php\\?.*?&id=(?P<id>[0-9]+)'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/stacommu.py",
            "diff": "diff --git a/yt_dlp/extractor/stacommu.py b/yt_dlp/extractor/stacommu.py\nindex 6f58f06d..1308c595 100644\n--- a/yt_dlp/extractor/stacommu.py\n+++ b/yt_dlp/extractor/stacommu.py\n@@ -38,9 +38,48 @@ def _extract_hls_key(self, data, path, decrypt):\n             return None\n         return traverse_obj(encryption_data, {'key': ('key', {decrypt}), 'iv': ('iv', {decrypt})})\n \n+    def _extract_vod(self, url):\n+        video_id = self._match_id(url)\n+        video_info = self._download_metadata(\n+            url, video_id, 'ja', ('dehydratedState', 'queries', 0, 'state', 'data'))\n+        hls_info, decrypt = self._call_encrypted_api(\n+            video_id, ':watch', 'stream information', data={'method': 1})\n+\n+        return {\n+            'id': video_id,\n+            'formats': self._get_formats(hls_info, ('protocolHls', 'url', {url_or_none}), video_id),\n+            'hls_aes': self._extract_hls_key(hls_info, 'protocolHls', decrypt),\n+            **traverse_obj(video_info, {\n+                'title': ('displayName', {str}),\n+                'description': ('description', {str}),\n+                'timestamp': ('watchStartTime', {int_or_none}),\n+                'thumbnail': ('keyVisualUrl', {url_or_none}),\n+                'cast': ('casts', ..., 'displayName', {str}),\n+                'duration': ('duration', {int}),\n+            }),\n+        }\n+\n+    def _extract_ppv(self, url):\n+        video_id = self._match_id(url)\n+        video_info = self._call_api(video_id, msg='video information', query={'al': 'ja'}, auth=False)\n+        hls_info, decrypt = self._call_encrypted_api(\n+            video_id, ':watchArchive', 'stream information', data={'method': 1})\n+\n+        return {\n+            'id': video_id,\n+            'formats': self._get_formats(hls_info, ('hls', 'urls', ..., {url_or_none}), video_id),\n+            'hls_aes': self._extract_hls_key(hls_info, 'hls', decrypt),\n+            **traverse_obj(video_info, {\n+                'title': ('displayName', {str}),\n+                'timestamp': ('startTime', {int_or_none}),\n+                'thumbnail': ('keyVisualUrl', {url_or_none}),\n+                'duration': ('duration', {int_or_none}),\n+            }),\n+        }\n+\n \n class StacommuVODIE(StacommuBaseIE):\n-    _VALID_URL = r'https?://www\\.stacommu\\.jp/videos/episodes/(?P<id>[\\da-zA-Z]+)'\n+    _VALID_URL = r'https?://www\\.stacommu\\.jp/(?:en/)?videos/episodes/(?P<id>[\\da-zA-Z]+)'\n     _TESTS = [{\n         # not encrypted\n         'url': 'https://www.stacommu.jp/videos/episodes/aXcVKjHyAENEjard61soZZ',\n@@ -79,34 +118,19 @@ class StacommuVODIE(StacommuBaseIE):\n         'params': {\n             'skip_download': 'm3u8',\n         },\n+    }, {\n+        'url': 'https://www.stacommu.jp/en/videos/episodes/aXcVKjHyAENEjard61soZZ',\n+        'only_matching': True,\n     }]\n \n     _API_PATH = 'videoEpisodes'\n \n     def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        video_info = self._download_metadata(\n-            url, video_id, 'ja', ('dehydratedState', 'queries', 0, 'state', 'data'))\n-        hls_info, decrypt = self._call_encrypted_api(\n-            video_id, ':watch', 'stream information', data={'method': 1})\n-\n-        return {\n-            'id': video_id,\n-            'formats': self._get_formats(hls_info, ('protocolHls', 'url', {url_or_none}), video_id),\n-            'hls_aes': self._extract_hls_key(hls_info, 'protocolHls', decrypt),\n-            **traverse_obj(video_info, {\n-                'title': ('displayName', {str}),\n-                'description': ('description', {str}),\n-                'timestamp': ('watchStartTime', {int_or_none}),\n-                'thumbnail': ('keyVisualUrl', {url_or_none}),\n-                'cast': ('casts', ..., 'displayName', {str}),\n-                'duration': ('duration', {int}),\n-            }),\n-        }\n+        return self._extract_vod(url)\n \n \n class StacommuLiveIE(StacommuBaseIE):\n-    _VALID_URL = r'https?://www\\.stacommu\\.jp/live/(?P<id>[\\da-zA-Z]+)'\n+    _VALID_URL = r'https?://www\\.stacommu\\.jp/(?:en/)?live/(?P<id>[\\da-zA-Z]+)'\n     _TESTS = [{\n         'url': 'https://www.stacommu.jp/live/d2FJ3zLnndegZJCAEzGM3m',\n         'info_dict': {\n@@ -125,24 +149,83 @@ class StacommuLiveIE(StacommuBaseIE):\n         'params': {\n             'skip_download': 'm3u8',\n         },\n+    }, {\n+        'url': 'https://www.stacommu.jp/en/live/d2FJ3zLnndegZJCAEzGM3m',\n+        'only_matching': True,\n     }]\n \n     _API_PATH = 'events'\n \n     def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        video_info = self._call_api(video_id, msg='video information', query={'al': 'ja'}, auth=False)\n-        hls_info, decrypt = self._call_encrypted_api(\n-            video_id, ':watchArchive', 'stream information', data={'method': 1})\n+        return self._extract_ppv(url)\n \n-        return {\n-            'id': video_id,\n-            'formats': self._get_formats(hls_info, ('hls', 'urls', ..., {url_or_none}), video_id),\n-            'hls_aes': self._extract_hls_key(hls_info, 'hls', decrypt),\n-            **traverse_obj(video_info, {\n-                'title': ('displayName', {str}),\n-                'timestamp': ('startTime', {int_or_none}),\n-                'thumbnail': ('keyVisualUrl', {url_or_none}),\n-                'duration': ('duration', {int_or_none}),\n-            }),\n-        }\n+\n+class TheaterComplexTownBaseIE(StacommuBaseIE):\n+    _NETRC_MACHINE = 'theatercomplextown'\n+    _API_HOST = 'api.theater-complex.town'\n+    _LOGIN_QUERY = {'key': 'AIzaSyAgNCqToaIz4a062EeIrkhI_xetVfAOrfc'}\n+    _LOGIN_HEADERS = {\n+        'Accept': '*/*',\n+        'Content-Type': 'application/json',\n+        'X-Client-Version': 'Chrome/JsCore/9.23.0/FirebaseCore-web',\n+        'Referer': 'https://www.theater-complex.town/',\n+        'Origin': 'https://www.theater-complex.town',\n+    }\n+\n+\n+class TheaterComplexTownVODIE(TheaterComplexTownBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?theater-complex\\.town/(?:en/)?videos/episodes/(?P<id>\\w+)'\n+    IE_NAME = 'theatercomplextown:vod'\n+    _TESTS = [{\n+        'url': 'https://www.theater-complex.town/videos/episodes/hoxqidYNoAn7bP92DN6p78',\n+        'info_dict': {\n+            'id': 'hoxqidYNoAn7bP92DN6p78',\n+            'ext': 'mp4',\n+            'title': '\u6f14\u5287\u30c9\u30e9\u30d5\u30c8\u30b0\u30e9\u30f3\u30d7\u30ea2023\u3000\u5287\u56e3\u300e\u604b\u306e\u307c\u308a\u300f\u301c\u5287\u56e3\u540d\u6c7a\u5b9a\u79d8\u8a71\u30e9\u30b8\u30aa',\n+            'description': 'md5:a7e2e9cf570379ea67fb630f345ff65d',\n+            'cast': ['\u7389\u57ce \u88d5\u898f', '\u77f3\u5ddd \u51cc\u96c5'],\n+            'thumbnail': 'https://image.theater-complex.town/5URnXX6KCeDysuFrPkP38o/5URnXX6KCeDysuFrPkP38o',\n+            'upload_date': '20231103',\n+            'timestamp': 1699016400,\n+            'duration': 868,\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+        },\n+    }, {\n+        'url': 'https://www.theater-complex.town/en/videos/episodes/6QT7XYwM9dJz5Gf9VB6K5y',\n+        'only_matching': True,\n+    }]\n+\n+    _API_PATH = 'videoEpisodes'\n+\n+    def _real_extract(self, url):\n+        return self._extract_vod(url)\n+\n+\n+class TheaterComplexTownPPVIE(TheaterComplexTownBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?theater-complex\\.town/(?:en/)?ppv/(?P<id>\\w+)'\n+    IE_NAME = 'theatercomplextown:ppv'\n+    _TESTS = [{\n+        'url': 'https://www.theater-complex.town/ppv/wytW3X7khrjJBUpKuV3jen',\n+        'info_dict': {\n+            'id': 'wytW3X7khrjJBUpKuV3jen',\n+            'ext': 'mp4',\n+            'title': 'BREAK FREE STARS\u300011\u67085\u65e5\uff08\u65e5\uff0912:30\u5343\u79cb\u697d\u516c\u6f14',\n+            'thumbnail': 'https://image.theater-complex.town/5GWEB31JcTUfjtgdeV5t6o/5GWEB31JcTUfjtgdeV5t6o',\n+            'upload_date': '20231105',\n+            'timestamp': 1699155000,\n+            'duration': 8378,\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+        },\n+    }, {\n+        'url': 'https://www.theater-complex.town/en/ppv/wytW3X7khrjJBUpKuV3jen',\n+        'only_matching': True,\n+    }]\n+\n+    _API_PATH = 'events'\n+\n+    def _real_extract(self, url):\n+        return self._extract_ppv(url)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/stageplus.py",
            "diff": "diff --git a/yt_dlp/extractor/stageplus.py b/yt_dlp/extractor/stageplus.py\nindex adb4ebbc..4bed4d64 100644\n--- a/yt_dlp/extractor/stageplus.py\n+++ b/yt_dlp/extractor/stageplus.py\n@@ -484,18 +484,15 @@ def _real_extract(self, url):\n             'url': 'url',\n         })) or None\n \n-        m3u8_headers = {'jwt': self._TOKEN}\n-\n         entries = []\n         for idx, video in enumerate(traverse_obj(data, (\n                 'performanceWorks', lambda _, v: v['id'] and url_or_none(v['stream']['url']))), 1):\n             formats, subtitles = self._extract_m3u8_formats_and_subtitles(\n-                video['stream']['url'], video['id'], 'mp4', m3u8_id='hls', headers=m3u8_headers)\n+                video['stream']['url'], video['id'], 'mp4', m3u8_id='hls', query={'token': self._TOKEN})\n             entries.append({\n                 'id': video['id'],\n                 'formats': formats,\n                 'subtitles': subtitles,\n-                'http_headers': m3u8_headers,\n                 'album': metadata.get('title'),\n                 'album_artist': metadata.get('artist'),\n                 'track_number': idx,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/storyfire.py",
            "diff": "diff --git a/yt_dlp/extractor/storyfire.py b/yt_dlp/extractor/storyfire.py\nindex 035747c3..566f7778 100644\n--- a/yt_dlp/extractor/storyfire.py\n+++ b/yt_dlp/extractor/storyfire.py\n@@ -32,9 +32,7 @@ def _parse_video(self, video):\n             'description': video.get('description'),\n             'url': smuggle_url(\n                 'https://player.vimeo.com/video/' + vimeo_id, {\n-                    'http_headers': {\n-                        'Referer': 'https://storyfire.com/',\n-                    }\n+                    'referer': 'https://storyfire.com/',\n                 }),\n             'thumbnail': video.get('storyImage'),\n             'view_count': int_or_none(video.get('views')),\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/streamcloud.py",
            "diff": "diff --git a/yt_dlp/extractor/streamcloud.py b/yt_dlp/extractor/streamcloud.py\ndeleted file mode 100644\nindex 72898092..00000000\n--- a/yt_dlp/extractor/streamcloud.py\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    ExtractorError,\n-    urlencode_postdata,\n-)\n-\n-\n-class StreamcloudIE(InfoExtractor):\n-    IE_NAME = 'streamcloud.eu'\n-    _VALID_URL = r'https?://streamcloud\\.eu/(?P<id>[a-zA-Z0-9_-]+)(?:/(?P<fname>[^#?]*)\\.html)?'\n-\n-    _TESTS = [{\n-        'url': 'http://streamcloud.eu/skp9j99s4bpz/youtube-dl_test_video_____________-BaW_jenozKc.mp4.html',\n-        'md5': '6bea4c7fa5daaacc2a946b7146286686',\n-        'info_dict': {\n-            'id': 'skp9j99s4bpz',\n-            'ext': 'mp4',\n-            'title': 'youtube-dl test video  \\'/\\\\ \u00e4 \u21ad',\n-        },\n-        'skip': 'Only available from the EU'\n-    }, {\n-        'url': 'http://streamcloud.eu/ua8cmfh1nbe6/NSHIP-148--KUC-NG--H264-.mp4.html',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        url = 'http://streamcloud.eu/%s' % video_id\n-\n-        orig_webpage = self._download_webpage(url, video_id)\n-\n-        if '>File Not Found<' in orig_webpage:\n-            raise ExtractorError(\n-                'Video %s does not exist' % video_id, expected=True)\n-\n-        fields = re.findall(r'''(?x)<input\\s+\n-            type=\"(?:hidden|submit)\"\\s+\n-            name=\"([^\"]+)\"\\s+\n-            (?:id=\"[^\"]+\"\\s+)?\n-            value=\"([^\"]*)\"\n-            ''', orig_webpage)\n-\n-        self._sleep(6, video_id)\n-\n-        webpage = self._download_webpage(\n-            url, video_id, data=urlencode_postdata(fields), headers={\n-                b'Content-Type': b'application/x-www-form-urlencoded',\n-            })\n-\n-        try:\n-            title = self._html_search_regex(\n-                r'<h1[^>]*>([^<]+)<', webpage, 'title')\n-            video_url = self._search_regex(\n-                r'file:\\s*\"([^\"]+)\"', webpage, 'video URL')\n-        except ExtractorError:\n-            message = self._html_search_regex(\n-                r'(?s)<div[^>]+class=([\"\\']).*?msgboxinfo.*?\\1[^>]*>(?P<message>.+?)</div>',\n-                webpage, 'message', default=None, group='message')\n-            if message:\n-                raise ExtractorError('%s said: %s' % (self.IE_NAME, message), expected=True)\n-            raise\n-        thumbnail = self._search_regex(\n-            r'image:\\s*\"([^\"]+)\"', webpage, 'thumbnail URL', fatal=False)\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'url': video_url,\n-            'thumbnail': thumbnail,\n-            'http_headers': {\n-                'Referer': url,\n-            },\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/substack.py",
            "diff": "diff --git a/yt_dlp/extractor/substack.py b/yt_dlp/extractor/substack.py\nindex 3782ceed..6ee3f75e 100644\n--- a/yt_dlp/extractor/substack.py\n+++ b/yt_dlp/extractor/substack.py\n@@ -50,16 +50,16 @@ def _extract_embed_urls(cls, url, webpage):\n         if not re.search(r'<script[^>]+src=[\"\\']https://substackcdn.com/[^\"\\']+\\.js', webpage):\n             return\n \n-        mobj = re.search(r'{[^}]*[\"\\']subdomain[\"\\']\\s*:\\s*[\"\\'](?P<subdomain>[^\"]+)', webpage)\n+        mobj = re.search(r'{[^}]*\\\\?[\"\\']subdomain\\\\?[\"\\']\\s*:\\s*\\\\?[\"\\'](?P<subdomain>[^\\\\\"\\']+)', webpage)\n         if mobj:\n             parsed = urllib.parse.urlparse(url)\n             yield parsed._replace(netloc=f'{mobj.group(\"subdomain\")}.substack.com').geturl()\n             raise cls.StopExtraction()\n \n-    def _extract_video_formats(self, video_id, username):\n+    def _extract_video_formats(self, video_id, url):\n         formats, subtitles = [], {}\n         for video_format in ('hls', 'mp4'):\n-            video_url = f'https://{username}.substack.com/api/v1/video/upload/{video_id}/src?type={video_format}'\n+            video_url = urllib.parse.urljoin(url, f'/api/v1/video/upload/{video_id}/src?type={video_format}')\n \n             if video_format == 'hls':\n                 fmts, subs = self._extract_m3u8_formats_and_subtitles(video_url, video_id, 'mp4', fatal=False)\n@@ -81,12 +81,17 @@ def _real_extract(self, url):\n             r'window\\._preloads\\s*=\\s*JSON\\.parse\\(', webpage, 'json string',\n             display_id, transform_source=js_to_json, contains_pattern=r'\"{(?s:.+)}\"'), display_id)\n \n+        canonical_url = url\n+        domain = traverse_obj(webpage_info, ('domainInfo', 'customDomain', {str}))\n+        if domain:\n+            canonical_url = urllib.parse.urlparse(url)._replace(netloc=domain).geturl()\n+\n         post_type = webpage_info['post']['type']\n         formats, subtitles = [], {}\n         if post_type == 'podcast':\n             formats, subtitles = [{'url': webpage_info['post']['podcast_url']}], {}\n         elif post_type == 'video':\n-            formats, subtitles = self._extract_video_formats(webpage_info['post']['videoUpload']['id'], username)\n+            formats, subtitles = self._extract_video_formats(webpage_info['post']['videoUpload']['id'], canonical_url)\n         else:\n             self.raise_no_formats(f'Page type \"{post_type}\" is not supported')\n \n@@ -99,4 +104,5 @@ def _real_extract(self, url):\n             'thumbnail': traverse_obj(webpage_info, ('post', 'cover_image')),\n             'uploader': traverse_obj(webpage_info, ('pub', 'name')),\n             'uploader_id': str_or_none(traverse_obj(webpage_info, ('post', 'publication_id'))),\n+            'webpage_url': canonical_url,\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/svt.py",
            "diff": "diff --git a/yt_dlp/extractor/svt.py b/yt_dlp/extractor/svt.py\nindex 31bf7f97..18da8753 100644\n--- a/yt_dlp/extractor/svt.py\n+++ b/yt_dlp/extractor/svt.py\n@@ -1,3 +1,4 @@\n+import json\n import re\n \n from .common import InfoExtractor\n@@ -6,10 +7,11 @@\n     determine_ext,\n     dict_get,\n     int_or_none,\n-    unified_timestamp,\n     str_or_none,\n     strip_or_none,\n+    traverse_obj,\n     try_get,\n+    unified_timestamp,\n )\n \n \n@@ -163,10 +165,46 @@ class SVTPlayIE(SVTPlayBaseIE):\n             },\n         },\n         'params': {\n-            # skip for now due to download test asserts that segment is > 10000 bytes and svt uses\n-            # init segments that are smaller\n-            # AssertionError: Expected test_SVTPlay_jNwpV9P.mp4 to be at least 9.77KiB, but it's only 864.00B\n-            'skip_download': True,\n+            'skip_download': 'm3u8',\n+        },\n+        'skip': 'Episode is no longer available',\n+    }, {\n+        'url': 'https://www.svtplay.se/video/emBxBQj',\n+        'md5': '2382036fd6f8c994856c323fe51c426e',\n+        'info_dict': {\n+            'id': 'eyBd9aj',\n+            'ext': 'mp4',\n+            'title': '1. Farlig kryssning',\n+            'timestamp': 1491019200,\n+            'upload_date': '20170401',\n+            'duration': 2566,\n+            'thumbnail': r're:^https?://(?:.*[\\.-]jpg|www.svtstatic.se/image/.*)$',\n+            'age_limit': 0,\n+            'episode': '1. Farlig kryssning',\n+            'series': 'Rederiet',\n+            'subtitles': {\n+                'sv': 'count:3'\n+            },\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+        },\n+    }, {\n+        'url': 'https://www.svtplay.se/video/jz2rYz7/anders-hansen-moter/james-fallon?info=visa',\n+        'info_dict': {\n+            'id': 'jvXAGVb',\n+            'ext': 'mp4',\n+            'title': 'James Fallon',\n+            'timestamp': 1673917200,\n+            'upload_date': '20230117',\n+            'duration': 1081,\n+            'thumbnail': r're:^https?://(?:.*[\\.-]jpg|www.svtstatic.se/image/.*)$',\n+            'age_limit': 0,\n+            'episode': 'James Fallon',\n+            'series': 'Anders Hansen m\u00f6ter...',\n+        },\n+        'params': {\n+            'skip_download': 'dash',\n         },\n     }, {\n         'url': 'https://www.svtplay.se/video/30479064/husdrommar/husdrommar-sasong-8-designdrommar-i-stenungsund?modalId=8zVbDPA',\n@@ -247,15 +285,16 @@ def _real_extract(self, url):\n                 data, lambda x: x['statistics']['dataLake']['content']['id'],\n                 compat_str)\n \n+        if not svt_id:\n+            nextjs_data = self._search_nextjs_data(webpage, video_id, fatal=False)\n+            svt_id = traverse_obj(nextjs_data, (\n+                'props', 'urqlState', ..., 'data', {json.loads}, 'detailsPageByPath',\n+                'video', 'svtId', {str}), get_all=False)\n+\n         if not svt_id:\n             svt_id = self._search_regex(\n                 (r'<video[^>]+data-video-id=[\"\\']([\\da-zA-Z-]+)',\n-                 r'<[^>]+\\bdata-rt=[\"\\']top-area-play-button[\"\\'][^>]+\\bhref=[\"\\'][^\"\\']*video/%s/[^\"\\']*\\b(?:modalId|id)=([\\da-zA-Z-]+)' % re.escape(video_id),\n-                 r'[\"\\']videoSvtId[\"\\']\\s*:\\s*[\"\\']([\\da-zA-Z-]+)',\n-                 r'[\"\\']videoSvtId\\\\?[\"\\']\\s*:\\s*\\\\?[\"\\']([\\da-zA-Z-]+)',\n-                 r'\"content\"\\s*:\\s*{.*?\"id\"\\s*:\\s*\"([\\da-zA-Z-]+)\"',\n-                 r'[\"\\']svtId[\"\\']\\s*:\\s*[\"\\']([\\da-zA-Z-]+)',\n-                 r'[\"\\']svtId\\\\?[\"\\']\\s*:\\s*\\\\?[\"\\']([\\da-zA-Z-]+)'),\n+                 r'<[^>]+\\bdata-rt=[\"\\']top-area-play-button[\"\\'][^>]+\\bhref=[\"\\'][^\"\\']*video/[\\w-]+/[^\"\\']*\\b(?:modalId|id)=([\\w-]+)'),\n                 webpage, 'video id')\n \n         info_dict = self._extract_by_video_id(svt_id, webpage)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/swrmediathek.py",
            "diff": "diff --git a/yt_dlp/extractor/swrmediathek.py b/yt_dlp/extractor/swrmediathek.py\ndeleted file mode 100644\nindex 38bdfced..00000000\n--- a/yt_dlp/extractor/swrmediathek.py\n+++ /dev/null\n@@ -1,111 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    parse_duration,\n-    int_or_none,\n-    determine_protocol,\n-)\n-\n-\n-class SWRMediathekIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?swrmediathek\\.de/(?:content/)?player\\.htm\\?show=(?P<id>[\\da-f]{8}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{12})'\n-\n-    _TESTS = [{\n-        'url': 'http://swrmediathek.de/player.htm?show=849790d0-dab8-11e3-a953-0026b975f2e6',\n-        'md5': '8c5f6f0172753368547ca8413a7768ac',\n-        'info_dict': {\n-            'id': '849790d0-dab8-11e3-a953-0026b975f2e6',\n-            'ext': 'mp4',\n-            'title': 'SWR odysso',\n-            'description': 'md5:2012e31baad36162e97ce9eb3f157b8a',\n-            'thumbnail': r're:^http:.*\\.jpg$',\n-            'duration': 2602,\n-            'upload_date': '20140515',\n-            'uploader': 'SWR Fernsehen',\n-            'uploader_id': '990030',\n-        },\n-    }, {\n-        'url': 'http://swrmediathek.de/player.htm?show=0e1a8510-ddf2-11e3-9be3-0026b975f2e6',\n-        'md5': 'b10ab854f912eecc5a6b55cd6fc1f545',\n-        'info_dict': {\n-            'id': '0e1a8510-ddf2-11e3-9be3-0026b975f2e6',\n-            'ext': 'mp4',\n-            'title': 'Nachtcaf\u00e9 - Alltagsdroge Alkohol - zwischen Sektempfang und Komasaufen',\n-            'description': 'md5:e0a3adc17e47db2c23aab9ebc36dbee2',\n-            'thumbnail': r're:http://.*\\.jpg',\n-            'duration': 5305,\n-            'upload_date': '20140516',\n-            'uploader': 'SWR Fernsehen',\n-            'uploader_id': '990030',\n-        },\n-        'skip': 'redirect to http://swrmediathek.de/index.htm?hinweis=swrlink',\n-    }, {\n-        'url': 'http://swrmediathek.de/player.htm?show=bba23e10-cb93-11e3-bf7f-0026b975f2e6',\n-        'md5': '4382e4ef2c9d7ce6852535fa867a0dd3',\n-        'info_dict': {\n-            'id': 'bba23e10-cb93-11e3-bf7f-0026b975f2e6',\n-            'ext': 'mp3',\n-            'title': 'Sa\u0161a Stani\u0161ic: Vor dem Fest',\n-            'description': 'md5:5b792387dc3fbb171eb709060654e8c9',\n-            'thumbnail': r're:http://.*\\.jpg',\n-            'duration': 3366,\n-            'upload_date': '20140520',\n-            'uploader': 'SWR 2',\n-            'uploader_id': '284670',\n-        },\n-        'skip': 'redirect to http://swrmediathek.de/index.htm?hinweis=swrlink',\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        video = self._download_json(\n-            'http://swrmediathek.de/AjaxEntry?ekey=%s' % video_id,\n-            video_id, 'Downloading video JSON')\n-\n-        attr = video['attr']\n-        title = attr['entry_title']\n-        media_type = attr.get('entry_etype')\n-\n-        formats = []\n-        for entry in video.get('sub', []):\n-            if entry.get('name') != 'entry_media':\n-                continue\n-\n-            entry_attr = entry.get('attr', {})\n-            f_url = entry_attr.get('val2')\n-            if not f_url:\n-                continue\n-            codec = entry_attr.get('val0')\n-            if codec == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    f_url, video_id, 'mp4', 'm3u8_native',\n-                    m3u8_id='hls', fatal=False))\n-            elif codec == 'f4m':\n-                formats.extend(self._extract_f4m_formats(\n-                    f_url + '?hdcore=3.7.0', video_id,\n-                    f4m_id='hds', fatal=False))\n-            else:\n-                formats.append({\n-                    'format_id': determine_protocol({'url': f_url}),\n-                    'url': f_url,\n-                    'quality': int_or_none(entry_attr.get('val1')),\n-                    'vcodec': codec if media_type == 'Video' else 'none',\n-                    'acodec': codec if media_type == 'Audio' else None,\n-                })\n-\n-        upload_date = None\n-        entry_pdatet = attr.get('entry_pdatet')\n-        if entry_pdatet:\n-            upload_date = entry_pdatet[:-4]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': attr.get('entry_descl'),\n-            'thumbnail': attr.get('entry_image_16_9'),\n-            'duration': parse_duration(attr.get('entry_durat')),\n-            'upload_date': upload_date,\n-            'uploader': attr.get('channel_title'),\n-            'uploader_id': attr.get('channel_idkey'),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/syfy.py",
            "diff": "diff --git a/yt_dlp/extractor/syfy.py b/yt_dlp/extractor/syfy.py\nindex c79d27a0..afcdbf78 100644\n--- a/yt_dlp/extractor/syfy.py\n+++ b/yt_dlp/extractor/syfy.py\n@@ -23,6 +23,7 @@ class SyfyIE(AdobePassIE):\n             'skip_download': True,\n         },\n         'add_ie': ['ThePlatform'],\n+        'skip': 'Redirects to main page',\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/techtalks.py",
            "diff": "diff --git a/yt_dlp/extractor/techtalks.py b/yt_dlp/extractor/techtalks.py\ndeleted file mode 100644\nindex d37de360..00000000\n--- a/yt_dlp/extractor/techtalks.py\n+++ /dev/null\n@@ -1,80 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    get_element_by_attribute,\n-    clean_html,\n-)\n-\n-\n-class TechTalksIE(InfoExtractor):\n-    _VALID_URL = r'https?://techtalks\\.tv/talks/(?:[^/]+/)?(?P<id>\\d+)'\n-\n-    _TESTS = [{\n-        'url': 'http://techtalks.tv/talks/learning-topic-models-going-beyond-svd/57758/',\n-        'info_dict': {\n-            'id': '57758',\n-            'title': 'Learning Topic Models --- Going beyond SVD',\n-        },\n-        'playlist': [\n-            {\n-                'info_dict': {\n-                    'id': '57758',\n-                    'ext': 'flv',\n-                    'title': 'Learning Topic Models --- Going beyond SVD',\n-                },\n-            },\n-            {\n-                'info_dict': {\n-                    'id': '57758-slides',\n-                    'ext': 'flv',\n-                    'title': 'Learning Topic Models --- Going beyond SVD',\n-                },\n-            },\n-        ],\n-        'params': {\n-            # rtmp download\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'http://techtalks.tv/talks/57758',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        talk_id = mobj.group('id')\n-        webpage = self._download_webpage(url, talk_id)\n-        rtmp_url = self._search_regex(\n-            r'netConnectionUrl: \\'(.*?)\\'', webpage, 'rtmp url')\n-        play_path = self._search_regex(\n-            r'href=\\'(.*?)\\' [^>]*id=\"flowplayer_presenter\"',\n-            webpage, 'presenter play path')\n-        title = clean_html(get_element_by_attribute('class', 'title', webpage))\n-        video_info = {\n-            'id': talk_id,\n-            'title': title,\n-            'url': rtmp_url,\n-            'play_path': play_path,\n-            'ext': 'flv',\n-        }\n-        m_slides = re.search(r'<a class=\"slides\" href=\\'(.*?)\\'', webpage)\n-        if m_slides is None:\n-            return video_info\n-        else:\n-            return {\n-                '_type': 'playlist',\n-                'id': talk_id,\n-                'title': title,\n-                'entries': [\n-                    video_info,\n-                    # The slides video\n-                    {\n-                        'id': talk_id + '-slides',\n-                        'title': title,\n-                        'url': rtmp_url,\n-                        'play_path': m_slides.group(1),\n-                        'ext': 'flv',\n-                    },\n-                ],\n-            }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/telecinco.py",
            "diff": "diff --git a/yt_dlp/extractor/telecinco.py b/yt_dlp/extractor/telecinco.py\nindex 20bb8242..a3f0c7cd 100644\n--- a/yt_dlp/extractor/telecinco.py\n+++ b/yt_dlp/extractor/telecinco.py\n@@ -77,7 +77,6 @@ class TelecincoIE(InfoExtractor):\n         'url': 'http://www.telecinco.es/espanasinirmaslejos/Espana-gran-destino-turistico_2_1240605043.html',\n         'only_matching': True,\n     }, {\n-        # ooyala video\n         'url': 'http://www.cuatro.com/chesterinlove/a-carta/chester-chester_in_love-chester_edu_2_2331030022.html',\n         'only_matching': True,\n     }]\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/telewebion.py",
            "diff": "diff --git a/yt_dlp/extractor/telewebion.py b/yt_dlp/extractor/telewebion.py\nindex 550549f0..9378ed02 100644\n--- a/yt_dlp/extractor/telewebion.py\n+++ b/yt_dlp/extractor/telewebion.py\n@@ -1,52 +1,133 @@\n+from __future__ import annotations\n+\n+import json\n+from functools import partial\n+from textwrap import dedent\n+\n from .common import InfoExtractor\n+from ..utils import ExtractorError, format_field, int_or_none, parse_iso8601\n+from ..utils.traversal import traverse_obj\n \n \n-class TelewebionIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?telewebion\\.com/#!/episode/(?P<id>\\d+)'\n+def _fmt_url(url):\n+    return partial(format_field, template=url, default=None)\n \n-    _TEST = {\n-        'url': 'http://www.telewebion.com/#!/episode/1263668/',\n+\n+class TelewebionIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?telewebion\\.com/episode/(?P<id>(?:0x[a-fA-F\\d]+|\\d+))'\n+    _TESTS = [{\n+        'url': 'http://www.telewebion.com/episode/0x1b3139c/',\n         'info_dict': {\n-            'id': '1263668',\n+            'id': '0x1b3139c',\n             'ext': 'mp4',\n-            'title': '\u0642\u0631\u0639\u0647\\u200c\u06a9\u0634\u06cc \u0644\u06cc\u06af \u0642\u0647\u0631\u0645\u0627\u0646\u0627\u0646 \u0627\u0631\u0648\u067e\u0627',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n+            'title': '\u0642\u0631\u0639\u0647\u200c\u06a9\u0634\u06cc \u0644\u06cc\u06af \u0642\u0647\u0631\u0645\u0627\u0646\u0627\u0646 \u0627\u0631\u0648\u067e\u0627',\n+            'series': '+ \u0641\u0648\u062a\u0628\u0627\u0644',\n+            'series_id': '0x1b2505c',\n+            'channel': '\u0634\u0628\u06a9\u0647 3',\n+            'channel_id': '0x1b1a761',\n+            'channel_url': 'https://telewebion.com/live/tv3',\n+            'timestamp': 1425522414,\n+            'upload_date': '20150305',\n+            'release_timestamp': 1425517020,\n+            'release_date': '20150305',\n+            'duration': 420,\n             'view_count': int,\n+            'tags': ['\u0648\u0631\u0632\u0634\u06cc', '\u0644\u06cc\u06af \u0627\u0631\u0648\u067e\u0627', '\u0627\u0631\u0648\u067e\u0627'],\n+            'thumbnail': 'https://static.telewebion.com/episodeImages/YjFhM2MxMDBkMDNiZTU0MjE5YjQ3ZDY0Mjk1ZDE0ZmUwZWU3OTE3OWRmMDAyODNhNzNkNjdmMWMzMWIyM2NmMA/default',\n         },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n+        'skip_download': 'm3u8',\n+    }, {\n+        'url': 'https://telewebion.com/episode/162175536',\n+        'info_dict': {\n+            'id': '0x9aa9a30',\n+            'ext': 'mp4',\n+            'title': '\u06a9\u0627\u0631\u0645\u0627 \u06cc\u0639\u0646\u06cc \u0627\u06cc\u0646 !',\n+            'series': '\u067e\u0627\u0648\u0631\u0642\u06cc',\n+            'series_id': '0x29a7426',\n+            'channel': '\u0634\u0628\u06a9\u0647 2',\n+            'channel_id': '0x1b1a719',\n+            'channel_url': 'https://telewebion.com/live/tv2',\n+            'timestamp': 1699979968,\n+            'upload_date': '20231114',\n+            'release_timestamp': 1699991638,\n+            'release_date': '20231114',\n+            'duration': 78,\n+            'view_count': int,\n+            'tags': ['\u06a9\u0644\u06cc\u067e \u0647\u0627\u06cc \u0645\u0646\u062a\u062e\u0628', ' \u06a9\u0644\u06cc\u067e \u0637\u0646\u0632 ', ' \u06a9\u0644\u06cc\u067e \u0633\u06cc\u0627\u0633\u062a ', '\u067e\u0627\u0648\u0631\u0642\u06cc', '\u0648\u06cc\u0698\u0647 \u0641\u0644\u0633\u0637\u06cc\u0646'],\n+            'thumbnail': 'https://static.telewebion.com/episodeImages/871e9455-7567-49a5-9648-34c22c197f5f/default',\n         },\n-    }\n+        'skip_download': 'm3u8',\n+    }]\n+\n+    def _call_graphql_api(\n+        self, operation, video_id, query,\n+        variables: dict[str, tuple[str, str]] | None = None,\n+        note='Downloading GraphQL JSON metadata',\n+    ):\n+        parameters = ''\n+        if variables:\n+            parameters = ', '.join(f'${name}: {type_}' for name, (type_, _) in variables.items())\n+            parameters = f'({parameters})'\n+\n+        result = self._download_json('https://graph.telewebion.com/graphql', video_id, note, data=json.dumps({\n+            'operationName': operation,\n+            'query': f'query {operation}{parameters} @cacheControl(maxAge: 60) {{{query}\\n}}\\n',\n+            'variables': {name: value for name, (_, value) in (variables or {}).items()}\n+        }, separators=(',', ':')).encode(), headers={\n+            'Content-Type': 'application/json',\n+            'Accept': 'application/json',\n+        })\n+        if not result or traverse_obj(result, 'errors'):\n+            message = ', '.join(traverse_obj(result, ('errors', ..., 'message', {str})))\n+            raise ExtractorError(message or 'Unknown GraphQL API error')\n+\n+        return result['data']\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+        if not video_id.startswith('0x'):\n+            video_id = hex(int(video_id))\n+\n+        episode_data = self._call_graphql_api('getEpisodeDetail', video_id, dedent('''\n+            queryEpisode(filter: {EpisodeID: $EpisodeId}, first: 1) {\n+              title\n+              program {\n+                ProgramID\n+                title\n+              }\n+              image\n+              view_count\n+              duration\n+              started_at\n+              created_at\n+              channel {\n+                ChannelID\n+                name\n+                descriptor\n+              }\n+              tags {\n+                name\n+              }\n+            }\n+        '''), {'EpisodeId': ('[ID!]', video_id)})\n \n-        secure_token = self._download_webpage(\n-            'http://m.s2.telewebion.com/op/op?action=getSecurityToken', video_id)\n-        episode_details = self._download_json(\n-            'http://m.s2.telewebion.com/op/op', video_id,\n-            query={'action': 'getEpisodeDetails', 'episode_id': video_id})\n-\n-        m3u8_url = 'http://m.s1.telewebion.com/smil/%s.m3u8?filepath=%s&m3u8=1&secure_token=%s' % (\n-            video_id, episode_details['file_path'], secure_token)\n-        formats = self._extract_m3u8_formats(\n-            m3u8_url, video_id, ext='mp4', m3u8_id='hls')\n-\n-        picture_paths = [\n-            episode_details.get('picture_path'),\n-            episode_details.get('large_picture_path'),\n-        ]\n-\n-        thumbnails = [{\n-            'url': picture_path,\n-            'preference': idx,\n-        } for idx, picture_path in enumerate(picture_paths) if picture_path is not None]\n-\n-        return {\n-            'id': video_id,\n-            'title': episode_details['title'],\n-            'formats': formats,\n-            'thumbnails': thumbnails,\n-            'view_count': episode_details.get('view_count'),\n-        }\n+        info_dict = traverse_obj(episode_data, ('queryEpisode', 0, {\n+            'title': ('title', {str}),\n+            'view_count': ('view_count', {int_or_none}),\n+            'duration': ('duration', {int_or_none}),\n+            'tags': ('tags', ..., 'name', {str}),\n+            'release_timestamp': ('started_at', {parse_iso8601}),\n+            'timestamp': ('created_at', {parse_iso8601}),\n+            'series': ('program', 'title', {str}),\n+            'series_id': ('program', 'ProgramID', {str}),\n+            'channel': ('channel', 'name', {str}),\n+            'channel_id': ('channel', 'ChannelID', {str}),\n+            'channel_url': ('channel', 'descriptor', {_fmt_url('https://telewebion.com/live/%s')}),\n+            'thumbnail': ('image', {_fmt_url('https://static.telewebion.com/episodeImages/%s/default')}),\n+            'formats': (\n+                'channel', 'descriptor', {str},\n+                {_fmt_url(f'https://cdna.telewebion.com/%s/episode/{video_id}/playlist.m3u8')},\n+                {partial(self._extract_m3u8_formats, video_id=video_id, ext='mp4', m3u8_id='hls')}),\n+        }))\n+        info_dict['id'] = video_id\n+        return info_dict\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tenplay.py",
            "diff": "diff --git a/yt_dlp/extractor/tenplay.py b/yt_dlp/extractor/tenplay.py\nindex c7097cf0..7ce7cbf8 100644\n--- a/yt_dlp/extractor/tenplay.py\n+++ b/yt_dlp/extractor/tenplay.py\n@@ -1,9 +1,11 @@\n-from datetime import datetime\n import base64\n+import functools\n+import itertools\n+from datetime import datetime\n \n from .common import InfoExtractor\n from ..networking import HEADRequest\n-from ..utils import int_or_none, urlencode_postdata\n+from ..utils import int_or_none, traverse_obj, urlencode_postdata, urljoin\n \n \n class TenPlayIE(InfoExtractor):\n@@ -113,3 +115,55 @@ def _real_extract(self, url):\n             'uploader': 'Channel 10',\n             'uploader_id': '2199827728001',\n         }\n+\n+\n+class TenPlaySeasonIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?10play\\.com\\.au/(?P<show>[^/?#]+)/episodes/(?P<season>[^/?#]+)/?(?:$|[?#])'\n+    _TESTS = [{\n+        'url': 'https://10play.com.au/masterchef/episodes/season-14',\n+        'info_dict': {\n+            'title': 'Season 14',\n+            'id': 'MjMyOTIy',\n+        },\n+        'playlist_mincount': 64,\n+    }, {\n+        'url': 'https://10play.com.au/the-bold-and-the-beautiful-fast-tracked/episodes/season-2022',\n+        'info_dict': {\n+            'title': 'Season 2022',\n+            'id': 'Mjc0OTIw',\n+        },\n+        'playlist_mincount': 256,\n+    }]\n+\n+    def _entries(self, load_more_url, display_id=None):\n+        skip_ids = []\n+        for page in itertools.count(1):\n+            episodes_carousel = self._download_json(\n+                load_more_url, display_id, query={'skipIds[]': skip_ids},\n+                note=f'Fetching episodes page {page}')\n+\n+            episodes_chunk = episodes_carousel['items']\n+            skip_ids.extend(ep['id'] for ep in episodes_chunk)\n+\n+            for ep in episodes_chunk:\n+                yield ep['cardLink']\n+            if not episodes_carousel['hasMore']:\n+                break\n+\n+    def _real_extract(self, url):\n+        show, season = self._match_valid_url(url).group('show', 'season')\n+        season_info = self._download_json(\n+            f'https://10play.com.au/api/shows/{show}/episodes/{season}', f'{show}/{season}')\n+\n+        episodes_carousel = traverse_obj(season_info, (\n+            'content', 0, 'components', (\n+                lambda _, v: v['title'].lower() == 'episodes',\n+                (..., {dict}),\n+            )), get_all=False) or {}\n+\n+        playlist_id = episodes_carousel['tpId']\n+\n+        return self.playlist_from_matches(\n+            self._entries(urljoin(url, episodes_carousel['loadMoreUrl']), playlist_id),\n+            playlist_id, traverse_obj(season_info, ('content', 0, 'title', {str})),\n+            getter=functools.partial(urljoin, url))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tf1.py",
            "diff": "diff --git a/yt_dlp/extractor/tf1.py b/yt_dlp/extractor/tf1.py\nindex 4cf0322b..aba4927a 100644\n--- a/yt_dlp/extractor/tf1.py\n+++ b/yt_dlp/extractor/tf1.py\n@@ -27,6 +27,25 @@ class TF1IE(InfoExtractor):\n             # Sometimes wat serves the whole file with the --test option\n             'skip_download': True,\n         },\n+    }, {\n+        'url': 'https://www.tf1.fr/tmc/burger-quiz/videos/burger-quiz-du-19-aout-2023-s03-episode-21-85585666.html',\n+        'info_dict': {\n+            'id': '14010600',\n+            'ext': 'mp4',\n+            'title': 'Burger Quiz - S03 EP21 avec Eye Haidara, Anne Dep\u00e9trini, Jonathan Zacca\u00ef et Pio Marma\u00ef',\n+            'thumbnail': 'https://photos.tf1.fr/1280/720/burger-quiz-11-9adb79-0@1x.jpg',\n+            'description': 'Manu Payet recevra Eye Haidara, Anne Dep\u00e9trini, Jonathan Zacca\u00ef et Pio Marma\u00ef.',\n+            'upload_date': '20230819',\n+            'timestamp': 1692469471,\n+            'season_number': 3,\n+            'series': 'Burger Quiz',\n+            'episode_number': 21,\n+            'season': 'Season 3',\n+            'tags': 'count:13',\n+            'episode': 'Episode 21',\n+            'duration': 2312\n+        },\n+        'params': {'skip_download': 'm3u8'},\n     }, {\n         'url': 'http://www.tf1.fr/tf1/koh-lanta/videos/replay-koh-lanta-22-mai-2015.html',\n         'only_matching': True,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/theguardian.py",
            "diff": "diff --git a/yt_dlp/extractor/theguardian.py b/yt_dlp/extractor/theguardian.py\nnew file mode 100644\nindex 00000000..a231eccf\n--- /dev/null\n+++ b/yt_dlp/extractor/theguardian.py\n@@ -0,0 +1,135 @@\n+import itertools\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    extract_attributes,\n+    get_element_by_class,\n+    get_element_html_by_class,\n+    get_elements_html_by_class,\n+    parse_qs,\n+    traverse_obj,\n+    unified_strdate,\n+    urljoin\n+)\n+\n+\n+class TheGuardianPodcastIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?theguardian\\.com/\\w+/audio/\\d{4}/\\w{3}/\\d{1,2}/(?P<id>[\\w-]+)'\n+    _TESTS = [{\n+        'url': 'https://www.theguardian.com/news/audio/2023/nov/03/we-are-just-getting-started-the-plastic-eating-bacteria-that-could-change-the-world-podcast',\n+        'md5': 'd1771744681789b4cd7da2a08e487702',\n+        'info_dict': {\n+            'id': 'we-are-just-getting-started-the-plastic-eating-bacteria-that-could-change-the-world-podcast',\n+            'ext': 'mp3',\n+            'title': '\u2018We are just getting started\u2019: the plastic-eating bacteria that could change the world \u2013 podcast',\n+            'description': 'md5:cfd3df2791d394d2ab62cd571d5207ee',\n+            'creator': 'Stephen Buranyi',\n+            'thumbnail': 'md5:73c12558fcb3b0e2a59422bfb33b3f79',\n+            'release_date': '20231103'\n+        }\n+    }, {\n+        'url': 'https://www.theguardian.com/news/audio/2023/oct/30/the-trials-of-robert-habeck-is-the-worlds-most-powerful-green-politician-doomed-to-fail-podcast',\n+        'md5': 'd1771744681789b4cd7da2a08e487702',\n+        'info_dict': {\n+            'id': 'the-trials-of-robert-habeck-is-the-worlds-most-powerful-green-politician-doomed-to-fail-podcast',\n+            'ext': 'mp3',\n+            'title': 'The trials of Robert Habeck: is the world\u2019s most powerful green politician doomed to fail? \u2013 podcast',\n+            'description': 'md5:1b5cf6582d1771c6b7077784b5456994',\n+            'creator': 'Philip Oltermann',\n+            'thumbnail': 'md5:6e5c5ec43843e956e20be793722e9080',\n+            'release_date': '20231030'\n+        }\n+    }, {\n+        'url': 'https://www.theguardian.com/football/audio/2023/nov/06/arsenal-feel-hard-done-by-and-luton-hold-liverpool-football-weekly',\n+        'md5': 'a2fcff6f8e060a95b1483295273dc35e',\n+        'info_dict': {\n+            'id': 'arsenal-feel-hard-done-by-and-luton-hold-liverpool-football-weekly',\n+            'ext': 'mp3',\n+            'title': 'Arsenal feel hard done by and Luton hold Liverpool \u2013 Football Weekly',\n+            'description': 'md5:286a9fbddaeb7c83cc65d1c4a5330b2a',\n+            'creator': 'Max Rushden',\n+            'thumbnail': 'md5:93eb7d6440f1bb94eb3a6cad63f48afd',\n+            'release_date': '20231106'\n+        }\n+    }, {\n+        'url': 'https://www.theguardian.com/politics/audio/2023/nov/02/the-covid-inquiry-politics-weekly-uk-podcast',\n+        'md5': '06a0f7e9701a80c8064a5d35690481ec',\n+        'info_dict': {\n+            'id': 'the-covid-inquiry-politics-weekly-uk-podcast',\n+            'ext': 'mp3',\n+            'title': 'The Covid inquiry | Politics Weekly UK - podcast',\n+            'description': 'md5:207c98859c14903582b17d25b014046e',\n+            'creator': 'Gaby Hinsliff',\n+            'thumbnail': 'md5:28932a7b5a25b057be330d2ed70ea7f3',\n+            'release_date': '20231102'\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+        return {\n+            'id': video_id,\n+            'title': self._og_search_title(webpage) or get_element_by_class('content__headline', webpage),\n+            'description': self._og_search_description(webpage),\n+            'creator': self._html_search_meta('author', webpage),\n+            'thumbnail': self._og_search_thumbnail(webpage),\n+            'release_date': unified_strdate(self._html_search_meta('article:published_time', webpage)),\n+            'url': extract_attributes(get_element_html_by_class(\n+                'podcast__player', webpage) or '').get('data-source'),\n+        }\n+\n+\n+class TheGuardianPodcastPlaylistIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?theguardian\\.com/\\w+/series/(?P<id>[\\w-]+)(?:\\?page=\\d+)?'\n+    _TESTS = [{\n+        'url': 'https://www.theguardian.com/football/series/theguardianswomensfootballweekly',\n+        'info_dict': {\n+            'id': 'theguardianswomensfootballweekly',\n+            'title': \"The Guardian's Women's Football Weekly\",\n+            'description': 'md5:e2cc021311e582d29935a73614a43f51'\n+        },\n+        'playlist_mincount': 69\n+    }, {\n+        'url': 'https://www.theguardian.com/news/series/todayinfocus?page=2',\n+        'info_dict': {\n+            'id': 'todayinfocus',\n+            'title': 'Today in Focus',\n+            'description': 'md5:0f097764fc0d359e0b6eb537be0387e2'\n+        },\n+        'playlist_mincount': 1261\n+    }, {\n+        'url': 'https://www.theguardian.com/news/series/the-audio-long-read',\n+        'info_dict': {\n+            'id': 'the-audio-long-read',\n+            'title': 'The Audio Long Read',\n+            'description': 'md5:5462994a27527309562b25b6defc4ef3'\n+        },\n+        'playlist_mincount': 996\n+    }]\n+\n+    def _entries(self, url, playlist_id):\n+        for page in itertools.count(1):\n+            webpage, urlh = self._download_webpage_handle(\n+                url, playlist_id, f'Downloading page {page}', query={'page': page})\n+            if 'page' not in parse_qs(urlh.url):\n+                break\n+\n+            episodes = get_elements_html_by_class('fc-item--type-media', webpage)\n+            for url_path in traverse_obj(episodes, (..., {extract_attributes}, 'data-id')):\n+                yield url_path\n+\n+    def _real_extract(self, url):\n+        podcast_id = self._match_id(url)\n+\n+        webpage = self._download_webpage(url, podcast_id)\n+\n+        title = clean_html(get_element_by_class(\n+            'index-page-header__title', webpage) or get_element_by_class('flagship-audio__title', webpage))\n+        description = self._og_search_description(webpage) or self._html_search_meta(\n+            'description', webpage)\n+\n+        return self.playlist_from_matches(\n+            self._entries(url, podcast_id), podcast_id, title, description=description,\n+            ie=TheGuardianPodcastIE, getter=lambda x: urljoin('https://www.theguardian.com', x))\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/theplatform.py",
            "diff": "diff --git a/yt_dlp/extractor/theplatform.py b/yt_dlp/extractor/theplatform.py\nindex 99caeb5f..9160f5ec 100644\n--- a/yt_dlp/extractor/theplatform.py\n+++ b/yt_dlp/extractor/theplatform.py\n@@ -104,6 +104,10 @@ def _add_chapter(start_time, end_time):\n                 _add_chapter(chapter.get('startTime'), chapter.get('endTime'))\n             _add_chapter(tp_chapters[-1].get('startTime'), tp_chapters[-1].get('endTime') or duration)\n \n+        def extract_site_specific_field(field):\n+            # A number of sites have custom-prefixed keys, e.g. 'cbc$seasonNumber'\n+            return traverse_obj(info, lambda k, v: v and k.endswith(f'${field}'), get_all=False)\n+\n         return {\n             'title': info['title'],\n             'subtitles': subtitles,\n@@ -113,6 +117,14 @@ def _add_chapter(start_time, end_time):\n             'timestamp': int_or_none(info.get('pubDate'), 1000) or None,\n             'uploader': info.get('billingCode'),\n             'chapters': chapters,\n+            'creator': traverse_obj(info, ('author', {str})) or None,\n+            'categories': traverse_obj(info, (\n+                'categories', lambda _, v: v.get('label') in ('category', None), 'name', {str})) or None,\n+            'tags': traverse_obj(info, ('keywords', {lambda x: re.split(r'[;,]\\s?', x) if x else None})),\n+            'location': extract_site_specific_field('region'),\n+            'series': extract_site_specific_field('show'),\n+            'season_number': int_or_none(extract_site_specific_field('seasonNumber')),\n+            'media_type': extract_site_specific_field('programmingType') or extract_site_specific_field('type'),\n         }\n \n     def _extract_theplatform_metadata(self, path, video_id):\n@@ -167,7 +179,7 @@ class ThePlatformIE(ThePlatformBaseIE, AdobePassIE):\n             # rtmp download\n             'skip_download': True,\n         },\n-        'skip': '404 Not Found',\n+        'skip': 'CNet no longer uses ThePlatform',\n     }, {\n         'url': 'https://player.theplatform.com/p/D6x-PC/pulse_preview/embed/select/media/yMBg9E8KFxZD',\n         'info_dict': {\n@@ -177,7 +189,7 @@ class ThePlatformIE(ThePlatformBaseIE, AdobePassIE):\n             'title': 'HIGHLIGHTS: USA bag first ever series Cup win',\n             'uploader': 'EGSM',\n         },\n-        'skip': '404 Not Found',\n+        'skip': 'Dead link',\n     }, {\n         'url': 'http://player.theplatform.com/p/NnzsPC/widget/select/media/4Y0TlYUr_ZT7',\n         'only_matching': True,\n@@ -195,7 +207,7 @@ class ThePlatformIE(ThePlatformBaseIE, AdobePassIE):\n             'upload_date': '20150701',\n             'uploader': 'NBCU-NEWS',\n         },\n-        'skip': '404 Not Found',\n+        'skip': 'Error: Player PID \"nbcNewsOffsite\" is disabled',\n     }, {\n         # From http://www.nbc.com/the-blacklist/video/sir-crispin-crandall/2928790?onid=137781#vc137781=1\n         # geo-restricted (US), HLS encrypted with AES-128\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/theta.py",
            "diff": "diff --git a/yt_dlp/extractor/theta.py b/yt_dlp/extractor/theta.py\ndeleted file mode 100644\nindex ecf0ea09..00000000\n--- a/yt_dlp/extractor/theta.py\n+++ /dev/null\n@@ -1,90 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import try_get\n-\n-\n-class ThetaStreamIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?theta\\.tv/(?!video/)(?P<id>[a-z0-9-]+)'\n-    _TESTS = [{\n-        'url': 'https://www.theta.tv/davirus',\n-        'skip': 'The live may have ended',\n-        'info_dict': {\n-            'id': 'DaVirus',\n-            'ext': 'mp4',\n-            'title': 'I choose you - My Community is King -\ud83d\udc40 - YO HABLO ESPANOL - CODE DAVIRUS',\n-            'thumbnail': r're:https://live-thumbnails-prod-theta-tv\\.imgix\\.net/thumbnail/.+\\.jpg',\n-        }\n-    }, {\n-        'url': 'https://www.theta.tv/mst3k',\n-        'note': 'This channel is live 24/7',\n-        'info_dict': {\n-            'id': 'MST3K',\n-            'ext': 'mp4',\n-            'title': 'Mystery Science Theatre 3000 24/7 Powered by the THETA Network.',\n-            'thumbnail': r're:https://user-prod-theta-tv\\.imgix\\.net/.+\\.jpg',\n-        }\n-    }, {\n-        'url': 'https://www.theta.tv/contv-anime',\n-        'info_dict': {\n-            'id': 'ConTVAnime',\n-            'ext': 'mp4',\n-            'title': 'CONTV ANIME 24/7. Powered by THETA Network.',\n-            'thumbnail': r're:https://user-prod-theta-tv\\.imgix\\.net/.+\\.jpg',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        channel_id = self._match_id(url)\n-        info = self._download_json(f'https://api.theta.tv/v1/channel?alias={channel_id}', channel_id)['body']\n-\n-        m3u8_playlist = next(\n-            data['url'] for data in info['live_stream']['video_urls']\n-            if data.get('type') != 'embed' and data.get('resolution') in ('master', 'source'))\n-\n-        formats = self._extract_m3u8_formats(m3u8_playlist, channel_id, 'mp4', m3u8_id='hls', live=True)\n-\n-        channel = try_get(info, lambda x: x['user']['username'])  # using this field instead of channel_id due to capitalization\n-\n-        return {\n-            'id': channel,\n-            'title': try_get(info, lambda x: x['live_stream']['title']),\n-            'channel': channel,\n-            'view_count': try_get(info, lambda x: x['live_stream']['view_count']),\n-            'is_live': True,\n-            'formats': formats,\n-            'thumbnail': try_get(info, lambda x: x['live_stream']['thumbnail_url']),\n-        }\n-\n-\n-class ThetaVideoIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?theta\\.tv/video/(?P<id>vid[a-z0-9]+)'\n-    _TEST = {\n-        'url': 'https://www.theta.tv/video/vidiq6aaet3kzf799p0',\n-        'md5': '633d8c29eb276bb38a111dbd591c677f',\n-        'info_dict': {\n-            'id': 'vidiq6aaet3kzf799p0',\n-            'ext': 'mp4',\n-            'title': 'Theta EdgeCast Tutorial',\n-            'uploader': 'Pixiekittie',\n-            'description': 'md5:e316253f5bdced8b5a46bb50ae60a09f',\n-            'thumbnail': r're:https://user-prod-theta-tv\\.imgix\\.net/.+/vod_thumb/.+.jpg',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        info = self._download_json(f'https://api.theta.tv/v1/video/{video_id}/raw', video_id)['body']\n-\n-        m3u8_playlist = try_get(info, lambda x: x['video_urls'][0]['url'])\n-\n-        formats = self._extract_m3u8_formats(m3u8_playlist, video_id, 'mp4', m3u8_id='hls')\n-\n-        return {\n-            'id': video_id,\n-            'title': info.get('title'),\n-            'uploader': try_get(info, lambda x: x['user']['username']),\n-            'description': info.get('description'),\n-            'view_count': info.get('view_count'),\n-            'like_count': info.get('like_count'),\n-            'formats': formats,\n-            'thumbnail': info.get('thumbnail_url'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/theweatherchannel.py",
            "diff": "diff --git a/yt_dlp/extractor/theweatherchannel.py b/yt_dlp/extractor/theweatherchannel.py\nindex 682e4335..d1921e4f 100644\n--- a/yt_dlp/extractor/theweatherchannel.py\n+++ b/yt_dlp/extractor/theweatherchannel.py\n@@ -11,17 +11,19 @@\n class TheWeatherChannelIE(ThePlatformIE):  # XXX: Do not subclass from concrete IE\n     _VALID_URL = r'https?://(?:www\\.)?weather\\.com(?P<asset_name>(?:/(?P<locale>[a-z]{2}-[A-Z]{2}))?/(?:[^/]+/)*video/(?P<id>[^/?#]+))'\n     _TESTS = [{\n-        'url': 'https://weather.com/series/great-outdoors/video/ice-climber-is-in-for-a-shock',\n-        'md5': 'c4cbe74c9c17c5676b704b950b73dd92',\n+        'url': 'https://weather.com/storms/hurricane/video/invest-95l-in-atlantic-has-a-medium-chance-of-development',\n+        'md5': '68f0cf616435683f27ce36bd9c927394',\n         'info_dict': {\n-            'id': 'cc82397e-cc3f-4d11-9390-a785add090e8',\n+            'id': '81acef2d-ee8c-4545-ba83-bff3cc80db97',\n             'ext': 'mp4',\n-            'title': 'Ice Climber Is In For A Shock',\n-            'description': 'md5:55606ce1378d4c72e6545e160c9d9695',\n-            'uploader': 'TWC - Digital (No Distro)',\n-            'uploader_id': '6ccd5455-16bb-46f2-9c57-ff858bb9f62c',\n-            'upload_date': '20160720',\n-            'timestamp': 1469018835,\n+            'title': 'Invest 95L In Atlantic Has A Medium Chance Of Development',\n+            'description': 'md5:0de720fd5f0d0e32207bd4c270fff824',\n+            'uploader': 'TWC - Digital',\n+            'uploader_id': 'b5a999e0-9e04-11e1-9ee2-001d092f5a10',\n+            'upload_date': '20230721',\n+            'timestamp': 1689967343,\n+            'display_id': 'invest-95l-in-atlantic-has-a-medium-chance-of-development',\n+            'duration': 34.0,\n         }\n     }, {\n         'url': 'https://weather.com/en-CA/international/videos/video/unidentified-object-falls-from-sky-in-india',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/thisav.py",
            "diff": "diff --git a/yt_dlp/extractor/thisav.py b/yt_dlp/extractor/thisav.py\ndeleted file mode 100644\nindex b1cd57d1..00000000\n--- a/yt_dlp/extractor/thisav.py\n+++ /dev/null\n@@ -1,66 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import remove_end\n-\n-\n-class ThisAVIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?thisav\\.com/video/(?P<id>[0-9]+)/.*'\n-    _TESTS = [{\n-        # jwplayer\n-        'url': 'http://www.thisav.com/video/47734/%98%26sup1%3B%83%9E%83%82---just-fit.html',\n-        'md5': '0480f1ef3932d901f0e0e719f188f19b',\n-        'info_dict': {\n-            'id': '47734',\n-            'ext': 'flv',\n-            'title': '\u9ad8\u6a39\u30de\u30ea\u30a2 - Just fit',\n-            'uploader': 'dj7970',\n-            'uploader_id': 'dj7970'\n-        }\n-    }, {\n-        # html5 media\n-        'url': 'http://www.thisav.com/video/242352/nerdy-18yo-big-ass-tattoos-and-glasses.html',\n-        'md5': 'ba90c076bd0f80203679e5b60bf523ee',\n-        'info_dict': {\n-            'id': '242352',\n-            'ext': 'mp4',\n-            'title': 'Nerdy 18yo Big Ass Tattoos and Glasses',\n-            'uploader': 'cybersluts',\n-            'uploader_id': 'cybersluts',\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-\n-        video_id = mobj.group('id')\n-        webpage = self._download_webpage(url, video_id)\n-        title = remove_end(self._html_extract_title(webpage), ' - \u8996\u983b - ThisAV.com-\u4e16\u754c\u7b2c\u4e00\u4e2d\u6587\u6210\u4eba\u5a1b\u6a02\u7db2\u7ad9')\n-        video_url = self._html_search_regex(\n-            r\"addVariable\\('file','([^']+)'\\);\", webpage, 'video url', default=None)\n-        if video_url:\n-            info_dict = {\n-                'formats': [{\n-                    'url': video_url,\n-                }],\n-            }\n-        else:\n-            entries = self._parse_html5_media_entries(url, webpage, video_id)\n-            if entries:\n-                info_dict = entries[0]\n-            else:\n-                info_dict = self._extract_jwplayer_data(\n-                    webpage, video_id, require_title=False)\n-        uploader = self._html_search_regex(\n-            r': <a href=\"http://www\\.thisav\\.com/user/[0-9]+/(?:[^\"]+)\">([^<]+)</a>',\n-            webpage, 'uploader name', fatal=False)\n-        uploader_id = self._html_search_regex(\n-            r': <a href=\"http://www\\.thisav\\.com/user/[0-9]+/([^\"]+)\">(?:[^<]+)</a>',\n-            webpage, 'uploader id', fatal=False)\n-\n-        info_dict.update({\n-            'id': video_id,\n-            'uploader': uploader,\n-            'uploader_id': uploader_id,\n-            'title': title,\n-        })\n-\n-        return info_dict\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/thisoldhouse.py",
            "diff": "diff --git a/yt_dlp/extractor/thisoldhouse.py b/yt_dlp/extractor/thisoldhouse.py\nindex cc7beeea..15f8380d 100644\n--- a/yt_dlp/extractor/thisoldhouse.py\n+++ b/yt_dlp/extractor/thisoldhouse.py\n@@ -1,11 +1,23 @@\n+import json\n+\n from .common import InfoExtractor\n+from .zype import ZypeIE\n from ..networking import HEADRequest\n+from ..networking.exceptions import HTTPError\n+from ..utils import (\n+    ExtractorError,\n+    filter_dict,\n+    parse_qs,\n+    try_call,\n+    urlencode_postdata,\n+)\n \n \n class ThisOldHouseIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?thisoldhouse\\.com/(?:watch|how-to|tv-episode|(?:[^/]+/)?\\d+)/(?P<id>[^/?#]+)'\n+    _NETRC_MACHINE = 'thisoldhouse'\n+    _VALID_URL = r'https?://(?:www\\.)?thisoldhouse\\.com/(?:watch|how-to|tv-episode|(?:[^/?#]+/)?\\d+)/(?P<id>[^/?#]+)'\n     _TESTS = [{\n-        'url': 'https://www.thisoldhouse.com/how-to/how-to-build-storage-bench',\n+        'url': 'https://www.thisoldhouse.com/furniture/21017078/how-to-build-a-storage-bench',\n         'info_dict': {\n             'id': '5dcdddf673c3f956ef5db202',\n             'ext': 'mp4',\n@@ -23,13 +35,16 @@ class ThisOldHouseIE(InfoExtractor):\n             'skip_download': True,\n         },\n     }, {\n+        # Page no longer has video\n         'url': 'https://www.thisoldhouse.com/watch/arlington-arts-crafts-arts-and-crafts-class-begins',\n         'only_matching': True,\n     }, {\n+        # 404 Not Found\n         'url': 'https://www.thisoldhouse.com/tv-episode/ask-toh-shelf-rough-electric',\n         'only_matching': True,\n     }, {\n-        'url': 'https://www.thisoldhouse.com/furniture/21017078/how-to-build-a-storage-bench',\n+        # 404 Not Found\n+        'url': 'https://www.thisoldhouse.com/how-to/how-to-build-storage-bench',\n         'only_matching': True,\n     }, {\n         'url': 'https://www.thisoldhouse.com/21113884/s41-e13-paradise-lost',\n@@ -39,17 +54,51 @@ class ThisOldHouseIE(InfoExtractor):\n         'url': 'https://www.thisoldhouse.com/21083431/seaside-transformation-the-westerly-project',\n         'only_matching': True,\n     }]\n-    _ZYPE_TMPL = 'https://player.zype.com/embed/%s.html?api_key=hsOk_yMSPYNrT22e9pu8hihLXjaZf0JW5jsOWv4ZqyHJFvkJn6rtToHl09tbbsbe'\n+\n+    _LOGIN_URL = 'https://login.thisoldhouse.com/usernamepassword/login'\n+\n+    def _perform_login(self, username, password):\n+        self._request_webpage(\n+            HEADRequest('https://www.thisoldhouse.com/insider'), None, 'Requesting session cookies')\n+        urlh = self._request_webpage(\n+            'https://www.thisoldhouse.com/wp-login.php', None, 'Requesting login info',\n+            errnote='Unable to login', query={'redirect_to': 'https://www.thisoldhouse.com/insider'})\n+\n+        try:\n+            auth_form = self._download_webpage(\n+                self._LOGIN_URL, None, 'Submitting credentials', headers={\n+                    'Content-Type': 'application/json',\n+                    'Referer': urlh.url,\n+                }, data=json.dumps(filter_dict({\n+                    **{('client_id' if k == 'client' else k): v[0] for k, v in parse_qs(urlh.url).items()},\n+                    'tenant': 'thisoldhouse',\n+                    'username': username,\n+                    'password': password,\n+                    'popup_options': {},\n+                    'sso': True,\n+                    '_csrf': try_call(lambda: self._get_cookies(self._LOGIN_URL)['_csrf'].value),\n+                    '_intstate': 'deprecated',\n+                }), separators=(',', ':')).encode())\n+        except ExtractorError as e:\n+            if isinstance(e.cause, HTTPError) and e.cause.status == 401:\n+                raise ExtractorError('Invalid username or password', expected=True)\n+            raise\n+\n+        self._request_webpage(\n+            'https://login.thisoldhouse.com/login/callback', None, 'Completing login',\n+            data=urlencode_postdata(self._hidden_inputs(auth_form)))\n \n     def _real_extract(self, url):\n         display_id = self._match_id(url)\n         webpage = self._download_webpage(url, display_id)\n         if 'To Unlock This content' in webpage:\n-            self.raise_login_required(method='cookies')\n-        video_url = self._search_regex(\n+            self.raise_login_required(\n+                'This video is only available for subscribers. '\n+                'Note that --cookies-from-browser may not work due to this site using session cookies')\n+\n+        video_url, video_id = self._search_regex(\n             r'<iframe[^>]+src=[\\'\"]((?:https?:)?//(?:www\\.)?thisoldhouse\\.(?:chorus\\.build|com)/videos/zype/([0-9a-f]{24})[^\\'\"]*)[\\'\"]',\n-            webpage, 'video url')\n-        if 'subscription_required=true' in video_url or 'c-entry-group-labels__image' in webpage:\n-            return self.url_result(self._request_webpage(HEADRequest(video_url), display_id).url, 'Zype', display_id)\n-        video_id = self._search_regex(r'(?:https?:)?//(?:www\\.)?thisoldhouse\\.(?:chorus\\.build|com)/videos/zype/([0-9a-f]{24})', video_url, 'video id')\n-        return self.url_result(self._ZYPE_TMPL % video_id, 'Zype', video_id)\n+            webpage, 'video url', group=(1, 2))\n+        video_url = self._request_webpage(HEADRequest(video_url), video_id, 'Resolving Zype URL').url\n+\n+        return self.url_result(video_url, ZypeIE, video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tiktok.py",
            "diff": "diff --git a/yt_dlp/extractor/tiktok.py b/yt_dlp/extractor/tiktok.py\nindex f14c4f9d..f26972cf 100644\n--- a/yt_dlp/extractor/tiktok.py\n+++ b/yt_dlp/extractor/tiktok.py\n@@ -15,7 +15,6 @@\n     UserNotLive,\n     determine_ext,\n     format_field,\n-    get_element_by_id,\n     get_first,\n     int_or_none,\n     join_nonempty,\n@@ -50,8 +49,9 @@ def _create_url(user_id, video_id):\n         return f'https://www.tiktok.com/@{user_id or \"_\"}/video/{video_id}'\n \n     def _get_sigi_state(self, webpage, display_id):\n-        return self._parse_json(get_element_by_id(\n-            'SIGI_STATE|sigi-persisted-data', webpage, escape_value=False), display_id)\n+        return self._search_json(\n+            r'<script[^>]+\\bid=\"(?:SIGI_STATE|sigi-persisted-data)\"[^>]*>', webpage,\n+            'sigi state', display_id, end_pattern=r'</script>')\n \n     def _call_api_impl(self, ep, query, manifest_app_version, video_id, fatal=True,\n                        note='Downloading API JSON', errnote='Unable to download API page'):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tinypic.py",
            "diff": "diff --git a/yt_dlp/extractor/tinypic.py b/yt_dlp/extractor/tinypic.py\ndeleted file mode 100644\nindex 216208cb..00000000\n--- a/yt_dlp/extractor/tinypic.py\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import ExtractorError\n-\n-\n-class TinyPicIE(InfoExtractor):\n-    IE_NAME = 'tinypic'\n-    IE_DESC = 'tinypic.com videos'\n-    _VALID_URL = r'https?://(?:.+?\\.)?tinypic\\.com/player\\.php\\?v=(?P<id>[^&]+)&s=\\d+'\n-\n-    _TESTS = [\n-        {\n-            'url': 'http://tinypic.com/player.php?v=6xw7tc%3E&s=5#.UtqZmbRFCM8',\n-            'md5': '609b74432465364e72727ebc6203f044',\n-            'info_dict': {\n-                'id': '6xw7tc',\n-                'ext': 'flv',\n-                'title': 'shadow phenomenon weird',\n-            },\n-        },\n-        {\n-            'url': 'http://de.tinypic.com/player.php?v=dy90yh&s=8',\n-            'only_matching': True,\n-        }\n-    ]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-\n-        webpage = self._download_webpage(url, video_id, 'Downloading page')\n-\n-        mobj = re.search(r'(?m)fo\\.addVariable\\(\"file\",\\s\"(?P<fileid>[\\da-z]+)\"\\);\\n'\n-                         r'\\s+fo\\.addVariable\\(\"s\",\\s\"(?P<serverid>\\d+)\"\\);', webpage)\n-        if mobj is None:\n-            raise ExtractorError('Video %s does not exist' % video_id, expected=True)\n-\n-        file_id = mobj.group('fileid')\n-        server_id = mobj.group('serverid')\n-\n-        KEYWORDS_SUFFIX = ', Video, images, photos, videos, myspace, ebay, video hosting, photo hosting'\n-        keywords = self._html_search_meta('keywords', webpage, 'title')\n-        title = keywords[:-len(KEYWORDS_SUFFIX)] if keywords.endswith(KEYWORDS_SUFFIX) else ''\n-\n-        video_url = 'http://v%s.tinypic.com/%s.flv' % (server_id, file_id)\n-        thumbnail = 'http://v%s.tinypic.com/%s_th.jpg' % (server_id, file_id)\n-\n-        return {\n-            'id': file_id,\n-            'url': video_url,\n-            'thumbnail': thumbnail,\n-            'title': title\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tmz.py",
            "diff": "diff --git a/yt_dlp/extractor/tmz.py b/yt_dlp/extractor/tmz.py\nindex ffb30c6b..edd16bc5 100644\n--- a/yt_dlp/extractor/tmz.py\n+++ b/yt_dlp/extractor/tmz.py\n@@ -8,158 +8,160 @@\n \n \n class TMZIE(InfoExtractor):\n-    _VALID_URL = r\"https?://(?:www\\.)?tmz\\.com/.*\"\n+    _VALID_URL = r'https?://(?:www\\.)?tmz\\.com/.*'\n     _TESTS = [\n         {\n-            \"url\": \"http://www.tmz.com/videos/0-cegprt2p/\",\n-            \"info_dict\": {\n-                \"id\": \"http://www.tmz.com/videos/0-cegprt2p/\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"No Charges Against Hillary Clinton? Harvey Says It Ain't Over Yet\",\n-                \"description\": \"Harvey talks about Director Comey\u2019s decision not to prosecute Hillary Clinton.\",\n-                \"timestamp\": 1467831837,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20160706\",\n-                \"thumbnail\": \"https://imagez.tmz.com/image/5e/4by3/2016/07/06/5eea7dc01baa5c2e83eb06930c170e46_xl.jpg\",\n-                \"duration\": 772.0,\n+            'url': 'http://www.tmz.com/videos/0-cegprt2p/',\n+            'info_dict': {\n+                'id': 'http://www.tmz.com/videos/0-cegprt2p/',\n+                'ext': 'mp4',\n+                'title': 'No Charges Against Hillary Clinton? Harvey Says It Ain\\'t Over Yet',\n+                'description': 'Harvey talks about Director Comey\u2019s decision not to prosecute Hillary Clinton.',\n+                'timestamp': 1467831837,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20160706',\n+                'thumbnail': 'https://imagez.tmz.com/image/5e/4by3/2016/07/06/5eea7dc01baa5c2e83eb06930c170e46_xl.jpg',\n+                'duration': 772.0,\n             },\n         },\n         {\n-            \"url\": \"https://www.tmz.com/videos/071119-chris-morgan-women-4590005-0-zcsejvcr/\",\n-            \"info_dict\": {\n-                \"id\": \"https://www.tmz.com/videos/071119-chris-morgan-women-4590005-0-zcsejvcr/\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"Angry Bagel Shop Guy Says He Doesn't Trust Women\",\n-                \"description\": \"The enraged man who went viral for ranting about women on dating sites before getting ragdolled in a bagel shop is defending his misogyny ... he says it's women's fault in the first place.\",\n-                \"timestamp\": 1562889485,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20190711\",\n-                \"thumbnail\": \"https://imagez.tmz.com/image/a8/4by3/2019/07/12/a85480d27b2f50a7bfea2322151d67a5_xl.jpg\",\n-                \"duration\": 123.0,\n+            'url': 'https://www.tmz.com/videos/071119-chris-morgan-women-4590005-0-zcsejvcr/',\n+            'info_dict': {\n+                'id': 'https://www.tmz.com/videos/071119-chris-morgan-women-4590005-0-zcsejvcr/',\n+                'ext': 'mp4',\n+                'title': 'Angry Bagel Shop Guy Says He Doesn\\'t Trust Women',\n+                'description': 'The enraged man who went viral for ranting about women on dating sites before getting ragdolled in a bagel shop is defending his misogyny ... he says it\\'s women\\'s fault in the first place.',\n+                'timestamp': 1562889485,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20190711',\n+                'thumbnail': 'https://imagez.tmz.com/image/a8/4by3/2019/07/12/a85480d27b2f50a7bfea2322151d67a5_xl.jpg',\n+                'duration': 123.0,\n             },\n         },\n         {\n-            \"url\": \"http://www.tmz.com/2015/04/19/bobby-brown-bobbi-kristina-awake-video-concert\",\n-            \"md5\": \"5429c85db8bde39a473a56ca8c4c5602\",\n-            \"info_dict\": {\n-                \"id\": \"http://www.tmz.com/2015/04/19/bobby-brown-bobbi-kristina-awake-video-concert\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"Bobby Brown Tells Crowd ... Bobbi Kristina is Awake\",\n-                \"description\": 'Bobby Brown stunned his audience during a concert Saturday night, when he told the crowd, \"Bobbi is awake.  She\\'s watching me.\"',\n-                \"timestamp\": 1429467813,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20150419\",\n-                \"duration\": 29.0,\n-                \"thumbnail\": \"https://imagez.tmz.com/image/15/4by3/2015/04/20/1539c7ae136359fc979236fa6a9449dd_xl.jpg\",\n+            'url': 'http://www.tmz.com/2015/04/19/bobby-brown-bobbi-kristina-awake-video-concert',\n+            'md5': '5429c85db8bde39a473a56ca8c4c5602',\n+            'info_dict': {\n+                'id': 'http://www.tmz.com/2015/04/19/bobby-brown-bobbi-kristina-awake-video-concert',\n+                'ext': 'mp4',\n+                'title': 'Bobby Brown Tells Crowd ... Bobbi Kristina is Awake',\n+                'description': 'Bobby Brown stunned his audience during a concert Saturday night, when he told the crowd, \"Bobbi is awake.  She\\'s watching me.\"',\n+                'timestamp': 1429467813,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20150419',\n+                'duration': 29.0,\n+                'thumbnail': 'https://imagez.tmz.com/image/15/4by3/2015/04/20/1539c7ae136359fc979236fa6a9449dd_xl.jpg',\n             },\n         },\n         {\n-            \"url\": \"http://www.tmz.com/2015/09/19/patti-labelle-concert-fan-stripping-kicked-out-nicki-minaj/\",\n-            \"info_dict\": {\n-                \"id\": \"http://www.tmz.com/2015/09/19/patti-labelle-concert-fan-stripping-kicked-out-nicki-minaj/\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"Patti LaBelle -- Goes Nuclear On Stripping Fan\",\n-                \"description\": \"Patti LaBelle made it known loud and clear last night ... NO \"\n-                \"ONE gets on her stage and strips down.\",\n-                \"timestamp\": 1442683746,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20150919\",\n-                \"duration\": 104.0,\n-                \"thumbnail\": \"https://imagez.tmz.com/image/5e/4by3/2015/09/20/5e57d7575062528082994e18ac3f0f48_xl.jpg\",\n+            'url': 'http://www.tmz.com/2015/09/19/patti-labelle-concert-fan-stripping-kicked-out-nicki-minaj/',\n+            'info_dict': {\n+                'id': 'http://www.tmz.com/2015/09/19/patti-labelle-concert-fan-stripping-kicked-out-nicki-minaj/',\n+                'ext': 'mp4',\n+                'title': 'Patti LaBelle -- Goes Nuclear On Stripping Fan',\n+                'description': 'Patti LaBelle made it known loud and clear last night ... NO '\n+                'ONE gets on her stage and strips down.',\n+                'timestamp': 1442683746,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20150919',\n+                'duration': 104.0,\n+                'thumbnail': 'https://imagez.tmz.com/image/5e/4by3/2015/09/20/5e57d7575062528082994e18ac3f0f48_xl.jpg',\n             },\n         },\n         {\n-            \"url\": \"http://www.tmz.com/2016/01/28/adam-silver-sting-drake-blake-griffin/\",\n-            \"info_dict\": {\n-                \"id\": \"http://www.tmz.com/2016/01/28/adam-silver-sting-drake-blake-griffin/\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"NBA's Adam Silver -- Blake Griffin's a Great Guy ... He'll Learn from This\",\n-                \"description\": \"Two pretty parts of this video with NBA Commish Adam Silver.\",\n-                \"timestamp\": 1454010989,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20160128\",\n-                \"duration\": 59.0,\n-                \"thumbnail\": \"https://imagez.tmz.com/image/38/4by3/2016/01/29/3856e83e0beb57059ec412122b842fb1_xl.jpg\",\n+            'url': 'http://www.tmz.com/2016/01/28/adam-silver-sting-drake-blake-griffin/',\n+            'info_dict': {\n+                'id': 'http://www.tmz.com/2016/01/28/adam-silver-sting-drake-blake-griffin/',\n+                'ext': 'mp4',\n+                'title': 'NBA\\'s Adam Silver -- Blake Griffin\\'s a Great Guy ... He\\'ll Learn from This',\n+                'description': 'Two pretty parts of this video with NBA Commish Adam Silver.',\n+                'timestamp': 1454010989,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20160128',\n+                'duration': 59.0,\n+                'thumbnail': 'https://imagez.tmz.com/image/38/4by3/2016/01/29/3856e83e0beb57059ec412122b842fb1_xl.jpg',\n             },\n         },\n         {\n-            \"url\": \"http://www.tmz.com/2016/10/27/donald-trump-star-vandal-arrested-james-otis/\",\n-            \"info_dict\": {\n-                \"id\": \"http://www.tmz.com/2016/10/27/donald-trump-star-vandal-arrested-james-otis/\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"Trump Star Vandal -- I'm Not Afraid of Donald or the Cops!\",\n-                \"description\": \"James Otis is the the guy who took a pickaxe to Donald Trump's star on the Walk of Fame, and he tells TMZ .. he's ready and willing to go to jail for the crime.\",\n-                \"timestamp\": 1477500095,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20161026\",\n-                \"thumbnail\": \"https://imagez.tmz.com/image/0d/4by3/2016/10/27/0d904814d4a75dcf9cc3b8cfd1edc1a3_xl.jpg\",\n-                \"duration\": 128.0,\n+            'url': 'http://www.tmz.com/2016/10/27/donald-trump-star-vandal-arrested-james-otis/',\n+            'info_dict': {\n+                'id': 'http://www.tmz.com/2016/10/27/donald-trump-star-vandal-arrested-james-otis/',\n+                'ext': 'mp4',\n+                'title': 'Trump Star Vandal -- I\\'m Not Afraid of Donald or the Cops!',\n+                'description': 'James Otis is the the guy who took a pickaxe to Donald Trump\\'s star on the Walk of Fame, and he tells TMZ .. he\\'s ready and willing to go to jail for the crime.',\n+                'timestamp': 1477500095,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20161026',\n+                'thumbnail': 'https://imagez.tmz.com/image/0d/4by3/2016/10/27/0d904814d4a75dcf9cc3b8cfd1edc1a3_xl.jpg',\n+                'duration': 128.0,\n             },\n         },\n         {\n-            \"url\": \"https://www.tmz.com/videos/2020-10-31-103120-beverly-hills-protest-4878209/\",\n-            \"info_dict\": {\n-                \"id\": \"https://www.tmz.com/videos/2020-10-31-103120-beverly-hills-protest-4878209/\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"Cops Use Billy Clubs Against Pro-Trump and Anti-Fascist \"\n-                \"Demonstrators\",\n-                \"description\": \"Beverly Hills may be an omen of what's coming next week, \"\n-                \"because things got crazy on the streets and cops started \"\n-                \"swinging their billy clubs at both Anti-Fascist and Pro-Trump \"\n-                \"demonstrators.\",\n-                \"timestamp\": 1604182772,\n-                \"uploader\": \"TMZ Staff\",\n-                \"upload_date\": \"20201031\",\n-                \"duration\": 96.0,\n-                \"thumbnail\": \"https://imagez.tmz.com/image/f3/4by3/2020/10/31/f37bd5a8aef84497866f425130c58be3_xl.jpg\",\n+            'url': 'https://www.tmz.com/videos/2020-10-31-103120-beverly-hills-protest-4878209/',\n+            'info_dict': {\n+                'id': 'https://www.tmz.com/videos/2020-10-31-103120-beverly-hills-protest-4878209/',\n+                'ext': 'mp4',\n+                'title': 'Cops Use Billy Clubs Against Pro-Trump and Anti-Fascist '\n+                'Demonstrators',\n+                'description': 'Beverly Hills may be an omen of what\\'s coming next week, '\n+                'because things got crazy on the streets and cops started '\n+                'swinging their billy clubs at both Anti-Fascist and Pro-Trump '\n+                'demonstrators.',\n+                'timestamp': 1604182772,\n+                'uploader': 'TMZ Staff',\n+                'upload_date': '20201031',\n+                'duration': 96.0,\n+                'thumbnail': 'https://imagez.tmz.com/image/f3/4by3/2020/10/31/f37bd5a8aef84497866f425130c58be3_xl.jpg',\n             },\n         },\n         {\n-            \"url\": \"https://www.tmz.com/2020/11/05/gervonta-davis-car-crash-hit-and-run-police/\",\n-            \"info_dict\": {\n-                \"id\": \"Dddb6IGe-ws\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"SICK LAMBO GERVONTA DAVIS IN HIS NEW RIDE RIGHT AFTER KO AFTER LEO  EsNews Boxing\",\n-                \"uploader\": \"ESNEWS\",\n-                \"description\": \"md5:49675bc58883ccf80474b8aa701e1064\",\n-                \"upload_date\": \"20201102\",\n-                \"uploader_id\": \"ESNEWS\",\n-                \"uploader_url\": \"http://www.youtube.com/user/ESNEWS\",\n-                \"like_count\": int,\n-                \"channel_id\": \"UCI-Oq7oFGakzSzHFlTtsUsQ\",\n-                \"channel\": \"ESNEWS\",\n-                \"view_count\": int,\n-                \"duration\": 225,\n-                \"live_status\": \"not_live\",\n-                \"thumbnail\": \"https://i.ytimg.com/vi_webp/Dddb6IGe-ws/maxresdefault.webp\",\n-                \"channel_url\": \"https://www.youtube.com/channel/UCI-Oq7oFGakzSzHFlTtsUsQ\",\n-                \"channel_follower_count\": int,\n-                \"playable_in_embed\": True,\n-                \"categories\": [\"Sports\"],\n-                \"age_limit\": 0,\n-                \"tags\": \"count:10\",\n-                \"availability\": \"public\",\n+            'url': 'https://www.tmz.com/2020/11/05/gervonta-davis-car-crash-hit-and-run-police/',\n+            'info_dict': {\n+                'id': 'Dddb6IGe-ws',\n+                'ext': 'mp4',\n+                'title': 'SICK LAMBO GERVONTA DAVIS IN HIS NEW RIDE RIGHT AFTER KO AFTER LEO  EsNews Boxing',\n+                'uploader': 'ESNEWS',\n+                'description': 'md5:49675bc58883ccf80474b8aa701e1064',\n+                'upload_date': '20201102',\n+                'uploader_id': '@ESNEWS',\n+                'uploader_url': 'https://www.youtube.com/@ESNEWS',\n+                'like_count': int,\n+                'channel_id': 'UCI-Oq7oFGakzSzHFlTtsUsQ',\n+                'channel': 'ESNEWS',\n+                'view_count': int,\n+                'duration': 225,\n+                'live_status': 'not_live',\n+                'thumbnail': 'https://i.ytimg.com/vi_webp/Dddb6IGe-ws/maxresdefault.webp',\n+                'channel_url': 'https://www.youtube.com/channel/UCI-Oq7oFGakzSzHFlTtsUsQ',\n+                'channel_follower_count': int,\n+                'playable_in_embed': True,\n+                'categories': ['Sports'],\n+                'age_limit': 0,\n+                'tags': 'count:10',\n+                'availability': 'public',\n+                'comment_count': int,\n             },\n         },\n         {\n-            \"url\": \"https://www.tmz.com/2020/11/19/conor-mcgregor-dustin-poirier-contract-fight-ufc-257-fight-island/\",\n-            \"info_dict\": {\n-                \"id\": \"1329450007125225473\",\n-                \"ext\": \"mp4\",\n-                \"title\": \"The Mac Life - BREAKING: Conor McGregor (@thenotoriousmma) has signed his bout agreement for his rematch with Dustin Poirier for January 23.\",\n-                \"uploader\": \"The Mac Life\",\n-                \"description\": \"md5:56e6009bbc3d12498e10d08a8e1f1c69\",\n-                \"upload_date\": \"20201119\",\n-                \"uploader_id\": \"TheMacLife\",\n-                \"timestamp\": 1605800556,\n-                \"thumbnail\": \"https://pbs.twimg.com/media/EnMmfT8XYAExgxJ.jpg?name=small\",\n-                \"like_count\": int,\n-                \"duration\": 11.812,\n-                \"uploader_url\": \"https://twitter.com/TheMacLife\",\n-                \"age_limit\": 0,\n-                \"repost_count\": int,\n-                \"tags\": [],\n-                \"comment_count\": int,\n+            'url': 'https://www.tmz.com/2020/11/19/conor-mcgregor-dustin-poirier-contract-fight-ufc-257-fight-island/',\n+            'info_dict': {\n+                'id': '1329448013937471491',\n+                'ext': 'mp4',\n+                'title': 'The Mac Life - BREAKING: Conor McGregor (@thenotoriousmma) has signed his bout agreement for his rematch with Dustin Poirier for January 23.',\n+                'uploader': 'The Mac Life',\n+                'description': 'md5:56e6009bbc3d12498e10d08a8e1f1c69',\n+                'upload_date': '20201119',\n+                'display_id': '1329450007125225473',\n+                'uploader_id': 'TheMacLife',\n+                'timestamp': 1605800556,\n+                'thumbnail': 'https://pbs.twimg.com/media/EnMmfT8XYAExgxJ.jpg?name=small',\n+                'like_count': int,\n+                'duration': 11.812,\n+                'uploader_url': 'https://twitter.com/TheMacLife',\n+                'age_limit': 0,\n+                'repost_count': int,\n+                'tags': [],\n+                'comment_count': int,\n             },\n         },\n     ]\n@@ -167,25 +169,25 @@ class TMZIE(InfoExtractor):\n     def _real_extract(self, url):\n         webpage = self._download_webpage(url, url)\n         jsonld = self._search_json_ld(webpage, url)\n-        if not jsonld or \"url\" not in jsonld:\n+        if not jsonld or 'url' not in jsonld:\n             # try to extract from YouTube Player API\n             # see https://developers.google.com/youtube/iframe_api_reference#Video_Queueing_Functions\n             match_obj = re.search(r'\\.cueVideoById\\(\\s*(?P<quote>[\\'\"])(?P<id>.*?)(?P=quote)', webpage)\n             if match_obj:\n-                res = self.url_result(match_obj.group(\"id\"))\n+                res = self.url_result(match_obj.group('id'))\n                 return res\n             # try to extract from twitter\n-            blockquote_el = get_element_by_attribute(\"class\", \"twitter-tweet\", webpage)\n+            blockquote_el = get_element_by_attribute('class', 'twitter-tweet', webpage)\n             if blockquote_el:\n                 matches = re.findall(\n                     r'<a[^>]+href=\\s*(?P<quote>[\\'\"])(?P<link>.*?)(?P=quote)',\n                     blockquote_el)\n                 if matches:\n                     for _, match in matches:\n-                        if \"/status/\" in match:\n+                        if '/status/' in match:\n                             res = self.url_result(match)\n                             return res\n-            raise ExtractorError(\"No video found!\")\n+            raise ExtractorError('No video found!')\n         if id not in jsonld:\n-            jsonld[\"id\"] = url\n+            jsonld['id'] = url\n         return jsonld\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tokentube.py",
            "diff": "diff --git a/yt_dlp/extractor/tokentube.py b/yt_dlp/extractor/tokentube.py\ndeleted file mode 100644\nindex d022e275..00000000\n--- a/yt_dlp/extractor/tokentube.py\n+++ /dev/null\n@@ -1,153 +0,0 @@\n-import functools\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    clean_html,\n-    get_element_by_class,\n-    parse_count,\n-    remove_end,\n-    unified_strdate,\n-    js_to_json,\n-    OnDemandPagedList,\n-)\n-\n-\n-class TokentubeIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?tokentube\\.net/(?:view\\?[vl]=|[vl]/)(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'https://tokentube.net/l/3236632011/Praise-A-Thon-Pastori-Chrisin-ja-Pastori-Bennyn-kanssa-27-8-2021',\n-        'info_dict': {\n-            'id': '3236632011',\n-            'ext': 'mp4',\n-            'title': 'Praise-A-Thon Pastori Chrisin ja Pastori Bennyn kanssa 27.8.2021',\n-            'description': '',\n-            'uploader': 'Pastori Chris - Rapsodia.fi',\n-            'upload_date': '20210827',\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://tokentube.net/v/3950239124/Linux-Ubuntu-Studio-perus-k%C3%A4ytt%C3%B6',\n-        'md5': '0e1f00421f501f5eada9890d38fcfb56',\n-        'info_dict': {\n-            'id': '3950239124',\n-            'ext': 'mp4',\n-            'title': 'Linux Ubuntu Studio perus k\u00e4ytt\u00f6',\n-            'description': 'md5:46077d0daaba1974f2dc381257f9d64c',\n-            'uploader': 'jyrilehtonen',\n-            'upload_date': '20210825',\n-        },\n-    }, {\n-        'url': 'https://tokentube.net/view?v=3582463289',\n-        'info_dict': {\n-            'id': '3582463289',\n-            'ext': 'mp4',\n-            'title': 'Police for Freedom - toiminta aloitetaan Suomessa \u2764\ufe0f??',\n-            'description': 'md5:37ebf1cb44264e0bf23ed98b337ee63e',\n-            'uploader': 'Voitontie',\n-            'upload_date': '20210428',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = self._html_search_regex(r'<h1\\s*class=[\"\\']title-text[\"\\']>(.+?)</h1>', webpage, 'title')\n-\n-        data_json = self._html_search_regex(r'({[\"\\']html5[\"\\'].+?}}}+)', webpage, 'data json')\n-        data_json = self._parse_json(js_to_json(data_json), video_id, fatal=False)\n-\n-        sources = data_json.get('sources') or self._parse_json(\n-            self._html_search_regex(r'updateSrc\\(([^\\)]+)\\)', webpage, 'sources'),\n-            video_id, transform_source=js_to_json)\n-\n-        formats = [{\n-            'url': format.get('src'),\n-            'format_id': format.get('label'),\n-            'height': format.get('res'),\n-        } for format in sources]\n-\n-        view_count = parse_count(self._html_search_regex(\n-            r'<p\\s*class=[\"\\']views_counter[\"\\']>\\s*([\\d\\.,]+)\\s*<span>views?</span></p>',\n-            webpage, 'view_count', fatal=False))\n-\n-        like_count = parse_count(self._html_search_regex(\n-            r'<div\\s*class=\"sh_button\\s*likes_count\">\\s*(\\d+)\\s*</div>',\n-            webpage, 'like count', fatal=False))\n-\n-        dislike_count = parse_count(self._html_search_regex(\n-            r'<div\\s*class=\"sh_button\\s*dislikes_count\">\\s*(\\d+)\\s*</div>',\n-            webpage, 'dislike count', fatal=False))\n-\n-        upload_date = unified_strdate(self._html_search_regex(\n-            r'<span\\s*class=\"p-date\">Published\\s*on\\s+([^<]+)',\n-            webpage, 'upload date', fatal=False))\n-\n-        uploader = self._html_search_regex(\n-            r'<a\\s*class=\"place-left\"[^>]+>(.+?)</a>',\n-            webpage, 'uploader', fatal=False)\n-\n-        description = (clean_html(get_element_by_class('p-d-txt', webpage))\n-                       or self._html_search_meta(('og:description', 'description', 'twitter:description'), webpage))\n-\n-        description = remove_end(description, 'Category')\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            'title': title,\n-            'view_count': view_count,\n-            'like_count': like_count,\n-            'dislike_count': dislike_count,\n-            'upload_date': upload_date,\n-            'description': description,\n-            'uploader': uploader,\n-        }\n-\n-\n-class TokentubeChannelIE(InfoExtractor):\n-    _PAGE_SIZE = 20\n-    IE_NAME = 'Tokentube:channel'\n-    _VALID_URL = r'https?://(?:www\\.)?tokentube\\.net/channel/(?P<id>\\d+)/[^/]+(?:/videos)?'\n-    _TESTS = [{\n-        'url': 'https://tokentube.net/channel/3697658904/TokenTube',\n-        'info_dict': {\n-            'id': '3697658904',\n-        },\n-        'playlist_mincount': 7,\n-    }, {\n-        'url': 'https://tokentube.net/channel/3353234420/Linux/videos',\n-        'info_dict': {\n-            'id': '3353234420',\n-        },\n-        'playlist_mincount': 20,\n-    }, {\n-        'url': 'https://tokentube.net/channel/3475834195/Voitontie',\n-        'info_dict': {\n-            'id': '3475834195',\n-        },\n-        'playlist_mincount': 150,\n-    }]\n-\n-    def _fetch_page(self, channel_id, page):\n-        page += 1\n-        videos_info = self._download_webpage(\n-            f'https://tokentube.net/videos?p=0&m=1&sort=recent&u={channel_id}&page={page}',\n-            channel_id, headers={'X-Requested-With': 'XMLHttpRequest'},\n-            note=f'Downloading page {page}', fatal=False)\n-        if '</i> Sorry, no results were found.' not in videos_info:\n-            for path, media_id in re.findall(\n-                    r'<a[^>]+\\bhref=[\"\\']([^\"\\']+/[lv]/(\\d+)/\\S+)[\"\\'][^>]+>',\n-                    videos_info):\n-                yield self.url_result(path, ie=TokentubeIE.ie_key(), video_id=media_id)\n-\n-    def _real_extract(self, url):\n-        channel_id = self._match_id(url)\n-\n-        entries = OnDemandPagedList(functools.partial(\n-            self._fetch_page, channel_id), self._PAGE_SIZE)\n-\n-        return self.playlist_result(entries, channel_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/toypics.py",
            "diff": "diff --git a/yt_dlp/extractor/toypics.py b/yt_dlp/extractor/toypics.py\nindex bc733618..aa7ee6c4 100644\n--- a/yt_dlp/extractor/toypics.py\n+++ b/yt_dlp/extractor/toypics.py\n@@ -3,6 +3,7 @@\n \n \n class ToypicsIE(InfoExtractor):\n+    _WORKING = False\n     IE_DESC = 'Toypics video'\n     _VALID_URL = r'https?://videos\\.toypics\\.net/view/(?P<id>[0-9]+)'\n     _TEST = {\n@@ -43,6 +44,7 @@ def _real_extract(self, url):\n \n \n class ToypicsUserIE(InfoExtractor):\n+    _WORKING = False\n     IE_DESC = 'Toypics user profile'\n     _VALID_URL = r'https?://videos\\.toypics\\.net/(?!view)(?P<id>[^/?#&]+)'\n     _TEST = {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/trilulilu.py",
            "diff": "diff --git a/yt_dlp/extractor/trilulilu.py b/yt_dlp/extractor/trilulilu.py\ndeleted file mode 100644\nindex fb97be73..00000000\n--- a/yt_dlp/extractor/trilulilu.py\n+++ /dev/null\n@@ -1,100 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    ExtractorError,\n-    int_or_none,\n-    parse_iso8601,\n-)\n-\n-\n-class TriluliluIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:(?:www|m)\\.)?trilulilu\\.ro/(?:[^/]+/)?(?P<id>[^/#\\?]+)'\n-    _TESTS = [{\n-        'url': 'http://www.trilulilu.ro/big-buck-bunny-1',\n-        'md5': '68da087b676a6196a413549212f60cc6',\n-        'info_dict': {\n-            'id': 'ae2899e124140b',\n-            'ext': 'mp4',\n-            'title': 'Big Buck Bunny',\n-            'description': ':) pentru copilul din noi',\n-            'uploader_id': 'chipy',\n-            'upload_date': '20120304',\n-            'timestamp': 1330830647,\n-            'uploader': 'chipy',\n-            'view_count': int,\n-            'like_count': int,\n-            'comment_count': int,\n-        },\n-    }, {\n-        'url': 'http://www.trilulilu.ro/adena-ft-morreti-inocenta',\n-        'md5': '929dfb8729dc71750463af88bbbbf4a4',\n-        'info_dict': {\n-            'id': 'f299710e3c91c5',\n-            'ext': 'mp4',\n-            'title': 'Adena ft. Morreti - Inocenta',\n-            'description': 'pop music',\n-            'uploader_id': 'VEVOmixt',\n-            'upload_date': '20151204',\n-            'uploader': 'VEVOmixt',\n-            'timestamp': 1449187937,\n-            'view_count': int,\n-            'like_count': int,\n-            'comment_count': int,\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        media_info = self._download_json('http://m.trilulilu.ro/%s?format=json' % display_id, display_id)\n-\n-        age_limit = 0\n-        errors = media_info.get('errors', {})\n-        if errors.get('friends'):\n-            raise ExtractorError('This video is private.', expected=True)\n-        elif errors.get('geoblock'):\n-            raise ExtractorError('This video is not available in your country.', expected=True)\n-        elif errors.get('xxx_unlogged'):\n-            age_limit = 18\n-\n-        media_class = media_info.get('class')\n-        if media_class not in ('video', 'audio'):\n-            raise ExtractorError('not a video or an audio')\n-\n-        user = media_info.get('user', {})\n-\n-        thumbnail = media_info.get('cover_url')\n-        if thumbnail:\n-            thumbnail.format(width='1600', height='1200')\n-\n-        # TODO: get correct ext for audio files\n-        stream_type = media_info.get('stream_type')\n-        formats = [{\n-            'url': media_info['href'],\n-            'ext': stream_type,\n-        }]\n-        if media_info.get('is_hd'):\n-            formats.append({\n-                'format_id': 'hd',\n-                'url': media_info['hrefhd'],\n-                'ext': stream_type,\n-            })\n-        if media_class == 'audio':\n-            formats[0]['vcodec'] = 'none'\n-        else:\n-            formats[0]['format_id'] = 'sd'\n-\n-        return {\n-            'id': media_info['identifier'].split('|')[1],\n-            'display_id': display_id,\n-            'formats': formats,\n-            'title': media_info['title'],\n-            'description': media_info.get('description'),\n-            'thumbnail': thumbnail,\n-            'uploader_id': user.get('username'),\n-            'uploader': user.get('fullname'),\n-            'timestamp': parse_iso8601(media_info.get('published'), ' '),\n-            'duration': int_or_none(media_info.get('duration')),\n-            'view_count': int_or_none(media_info.get('count_views')),\n-            'like_count': int_or_none(media_info.get('count_likes')),\n-            'comment_count': int_or_none(media_info.get('count_comments')),\n-            'age_limit': age_limit,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tube8.py",
            "diff": "diff --git a/yt_dlp/extractor/tube8.py b/yt_dlp/extractor/tube8.py\nindex 77ed05ff..5f15b458 100644\n--- a/yt_dlp/extractor/tube8.py\n+++ b/yt_dlp/extractor/tube8.py\n@@ -1,13 +1,20 @@\n import re\n \n+from .common import InfoExtractor\n+from ..aes import aes_decrypt_text\n+from ..compat import compat_urllib_parse_unquote\n from ..utils import (\n+    determine_ext,\n+    format_field,\n     int_or_none,\n     str_to_int,\n+    strip_or_none,\n+    url_or_none,\n )\n-from .keezmovies import KeezMoviesIE\n \n \n-class Tube8IE(KeezMoviesIE):  # XXX: Do not subclass from concrete IE\n+class Tube8IE(InfoExtractor):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?tube8\\.com/(?:[^/]+/)+(?P<display_id>[^/]+)/(?P<id>\\d+)'\n     _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)?//(?:www\\.)?tube8\\.com/embed/(?:[^/]+/)+\\d+)']\n     _TESTS = [{\n@@ -30,6 +37,90 @@ class Tube8IE(KeezMoviesIE):  # XXX: Do not subclass from concrete IE\n         'only_matching': True,\n     }]\n \n+    def _extract_info(self, url, fatal=True):\n+        mobj = self._match_valid_url(url)\n+        video_id = mobj.group('id')\n+        display_id = (mobj.group('display_id')\n+                      if 'display_id' in mobj.groupdict()\n+                      else None) or mobj.group('id')\n+\n+        webpage = self._download_webpage(\n+            url, display_id, headers={'Cookie': 'age_verified=1'})\n+\n+        formats = []\n+        format_urls = set()\n+\n+        title = None\n+        thumbnail = None\n+        duration = None\n+        encrypted = False\n+\n+        def extract_format(format_url, height=None):\n+            format_url = url_or_none(format_url)\n+            if not format_url or not format_url.startswith(('http', '//')):\n+                return\n+            if format_url in format_urls:\n+                return\n+            format_urls.add(format_url)\n+            tbr = int_or_none(self._search_regex(\n+                r'[/_](\\d+)[kK][/_]', format_url, 'tbr', default=None))\n+            if not height:\n+                height = int_or_none(self._search_regex(\n+                    r'[/_](\\d+)[pP][/_]', format_url, 'height', default=None))\n+            if encrypted:\n+                format_url = aes_decrypt_text(\n+                    video_url, title, 32).decode('utf-8')\n+            formats.append({\n+                'url': format_url,\n+                'format_id': format_field(height, None, '%dp'),\n+                'height': height,\n+                'tbr': tbr,\n+            })\n+\n+        flashvars = self._parse_json(\n+            self._search_regex(\n+                r'flashvars\\s*=\\s*({.+?});', webpage,\n+                'flashvars', default='{}'),\n+            display_id, fatal=False)\n+\n+        if flashvars:\n+            title = flashvars.get('video_title')\n+            thumbnail = flashvars.get('image_url')\n+            duration = int_or_none(flashvars.get('video_duration'))\n+            encrypted = flashvars.get('encrypted') is True\n+            for key, value in flashvars.items():\n+                mobj = re.search(r'quality_(\\d+)[pP]', key)\n+                if mobj:\n+                    extract_format(value, int(mobj.group(1)))\n+            video_url = flashvars.get('video_url')\n+            if video_url and determine_ext(video_url, None):\n+                extract_format(video_url)\n+\n+        video_url = self._html_search_regex(\n+            r'flashvars\\.video_url\\s*=\\s*([\"\\'])(?P<url>http.+?)\\1',\n+            webpage, 'video url', default=None, group='url')\n+        if video_url:\n+            extract_format(compat_urllib_parse_unquote(video_url))\n+\n+        if not formats:\n+            if 'title=\"This video is no longer available\"' in webpage:\n+                self.raise_no_formats(\n+                    'Video %s is no longer available' % video_id, expected=True)\n+\n+        if not title:\n+            title = self._html_search_regex(\n+                r'<h1[^>]*>([^<]+)', webpage, 'title')\n+\n+        return webpage, {\n+            'id': video_id,\n+            'display_id': display_id,\n+            'title': strip_or_none(title),\n+            'thumbnail': thumbnail,\n+            'duration': duration,\n+            'age_limit': 18,\n+            'formats': formats,\n+        }\n+\n     def _real_extract(self, url):\n         webpage, info = self._extract_info(url)\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tunepk.py",
            "diff": "diff --git a/yt_dlp/extractor/tunepk.py b/yt_dlp/extractor/tunepk.py\ndeleted file mode 100644\nindex e4e507b0..00000000\n--- a/yt_dlp/extractor/tunepk.py\n+++ /dev/null\n@@ -1,87 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    int_or_none,\n-    try_get,\n-    unified_timestamp,\n-)\n-\n-\n-class TunePkIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?:\n-                            (?:www\\.)?tune\\.pk/(?:video/|player/embed_player.php?.*?\\bvid=)|\n-                            embed\\.tune\\.pk/play/\n-                        )\n-                        (?P<id>\\d+)\n-                    '''\n-    _TESTS = [{\n-        'url': 'https://tune.pk/video/6919541/maudie-2017-international-trailer-1-ft-ethan-hawke-sally-hawkins',\n-        'md5': '0c537163b7f6f97da3c5dd1e3ef6dd55',\n-        'info_dict': {\n-            'id': '6919541',\n-            'ext': 'mp4',\n-            'title': 'Maudie (2017) | International Trailer # 1 ft Ethan Hawke, Sally Hawkins',\n-            'description': 'md5:eb5a04114fafef5cec90799a93a2d09c',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'timestamp': 1487327564,\n-            'upload_date': '20170217',\n-            'uploader': 'Movie Trailers',\n-            'duration': 107,\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'https://tune.pk/player/embed_player.php?vid=6919541&folder=2017/02/17/&width=600&height=350&autoplay=no',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://embed.tune.pk/play/6919541?autoplay=no&ssl=yes&inline=true',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            'https://tune.pk/video/%s' % video_id, video_id)\n-\n-        details = self._parse_json(\n-            self._search_regex(\n-                r'new\\s+TunePlayer\\(({.+?})\\)\\s*;\\s*\\n', webpage, 'tune player'),\n-            video_id)['details']\n-\n-        video = details['video']\n-        title = video.get('title') or self._og_search_title(\n-            webpage, default=None) or self._html_search_meta(\n-            'title', webpage, 'title', fatal=True)\n-\n-        formats = self._parse_jwplayer_formats(\n-            details['player']['sources'], video_id)\n-\n-        description = self._og_search_description(\n-            webpage, default=None) or self._html_search_meta(\n-            'description', webpage, 'description')\n-\n-        thumbnail = video.get('thumb') or self._og_search_thumbnail(\n-            webpage, default=None) or self._html_search_meta(\n-            'thumbnail', webpage, 'thumbnail')\n-\n-        timestamp = unified_timestamp(video.get('date_added'))\n-        uploader = try_get(\n-            video, lambda x: x['uploader']['name'],\n-            compat_str) or self._html_search_meta('author', webpage, 'author')\n-\n-        duration = int_or_none(video.get('duration'))\n-        view_count = int_or_none(video.get('views'))\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'timestamp': timestamp,\n-            'uploader': uploader,\n-            'duration': duration,\n-            'view_count': view_count,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/turbo.py",
            "diff": "diff --git a/yt_dlp/extractor/turbo.py b/yt_dlp/extractor/turbo.py\ndeleted file mode 100644\nindex cdb7dcff..00000000\n--- a/yt_dlp/extractor/turbo.py\n+++ /dev/null\n@@ -1,64 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    ExtractorError,\n-    int_or_none,\n-    qualities,\n-    xpath_text,\n-)\n-\n-\n-class TurboIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?turbo\\.fr/videos-voiture/(?P<id>[0-9]+)-'\n-    _API_URL = 'http://www.turbo.fr/api/tv/xml.php?player_generique=player_generique&id={0:}'\n-    _TEST = {\n-        'url': 'http://www.turbo.fr/videos-voiture/454443-turbo-du-07-09-2014-renault-twingo-3-bentley-continental-gt-speed-ces-guide-achat-dacia.html',\n-        'md5': '33f4b91099b36b5d5a91f84b5bcba600',\n-        'info_dict': {\n-            'id': '454443',\n-            'ext': 'mp4',\n-            'duration': 3715,\n-            'title': 'Turbo du 07/09/2014 : Renault Twingo 3, Bentley Continental GT Speed, CES, Guide Achat Dacia... ',\n-            'description': 'Turbo du 07/09/2014 : Renault Twingo 3, Bentley Continental GT Speed, CES, Guide Achat Dacia...',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        playlist = self._download_xml(self._API_URL.format(video_id), video_id)\n-        item = playlist.find('./channel/item')\n-        if item is None:\n-            raise ExtractorError('Playlist item was not found', expected=True)\n-\n-        title = xpath_text(item, './title', 'title')\n-        duration = int_or_none(xpath_text(item, './durate', 'duration'))\n-        thumbnail = xpath_text(item, './visuel_clip', 'thumbnail')\n-        description = self._html_search_meta('description', webpage)\n-\n-        formats = []\n-        get_quality = qualities(['3g', 'sd', 'hq'])\n-        for child in item:\n-            m = re.search(r'url_video_(?P<quality>.+)', child.tag)\n-            if m:\n-                quality = compat_str(m.group('quality'))\n-                formats.append({\n-                    'format_id': quality,\n-                    'url': child.text,\n-                    'quality': get_quality(quality),\n-                })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'duration': duration,\n-            'thumbnail': thumbnail,\n-            'description': description,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tv5mondeplus.py",
            "diff": "diff --git a/yt_dlp/extractor/tv5mondeplus.py b/yt_dlp/extractor/tv5mondeplus.py\nindex bd0be784..a445fae8 100644\n--- a/yt_dlp/extractor/tv5mondeplus.py\n+++ b/yt_dlp/extractor/tv5mondeplus.py\n@@ -1,10 +1,14 @@\n+import urllib.parse\n+\n from .common import InfoExtractor\n from ..utils import (\n     determine_ext,\n     extract_attributes,\n     int_or_none,\n     parse_duration,\n+    traverse_obj,\n     try_get,\n+    url_or_none,\n )\n \n \n@@ -12,6 +16,36 @@ class TV5MondePlusIE(InfoExtractor):\n     IE_DESC = 'TV5MONDE+'\n     _VALID_URL = r'https?://(?:www\\.)?(?:tv5mondeplus|revoir\\.tv5monde)\\.com/toutes-les-videos/[^/]+/(?P<id>[^/?#]+)'\n     _TESTS = [{\n+        # movie\n+        'url': 'https://revoir.tv5monde.com/toutes-les-videos/cinema/les-novices',\n+        'md5': 'c86f60bf8b75436455b1b205f9745955',\n+        'info_dict': {\n+            'id': 'ZX0ipMyFQq_6D4BA7b',\n+            'display_id': 'les-novices',\n+            'ext': 'mp4',\n+            'title': 'Les novices',\n+            'description': 'md5:2e7c33ba3ad48dabfcc2a956b88bde2b',\n+            'upload_date': '20230821',\n+            'thumbnail': 'https://revoir.tv5monde.com/uploads/media/video_thumbnail/0738/60/01e952b7ccf36b7c6007ec9131588954ab651de9.jpeg',\n+            'duration': 5177,\n+            'episode': 'Les novices',\n+        },\n+    }, {\n+        # series episode\n+        'url': 'https://revoir.tv5monde.com/toutes-les-videos/series-fictions/opj-les-dents-de-la-terre-2',\n+        'info_dict': {\n+            'id': 'wJ0eeEPozr_6D4BA7b',\n+            'display_id': 'opj-les-dents-de-la-terre-2',\n+            'ext': 'mp4',\n+            'title': \"OPJ - Les dents de la Terre (2)\",\n+            'description': 'md5:288f87fd68d993f814e66e60e5302d9d',\n+            'upload_date': '20230823',\n+            'series': 'OPJ',\n+            'episode': 'Les dents de la Terre (2)',\n+            'duration': 2877,\n+            'thumbnail': 'https://dl-revoir.tv5monde.com/images/1a/5753448.jpg'\n+        },\n+    }, {\n         # movie\n         'url': 'https://revoir.tv5monde.com/toutes-les-videos/cinema/ceux-qui-travaillent',\n         'md5': '32fa0cde16a4480d1251502a66856d5f',\n@@ -23,6 +57,7 @@ class TV5MondePlusIE(InfoExtractor):\n             'description': 'md5:570e8bb688036ace873b2d50d24c026d',\n             'upload_date': '20210819',\n         },\n+        'skip': 'no longer available',\n     }, {\n         # series episode\n         'url': 'https://revoir.tv5monde.com/toutes-les-videos/series-fictions/vestiaires-caro-actrice',\n@@ -39,6 +74,7 @@ class TV5MondePlusIE(InfoExtractor):\n         'params': {\n             'skip_download': True,\n         },\n+        'skip': 'no longer available',\n     }, {\n         'url': 'https://revoir.tv5monde.com/toutes-les-videos/series-fictions/neuf-jours-en-hiver-neuf-jours-en-hiver',\n         'only_matching': True,\n@@ -48,6 +84,13 @@ class TV5MondePlusIE(InfoExtractor):\n     }]\n     _GEO_BYPASS = False\n \n+    @staticmethod\n+    def _extract_subtitles(data_captions):\n+        subtitles = {}\n+        for f in traverse_obj(data_captions, ('files', lambda _, v: url_or_none(v['file']))):\n+            subtitles.setdefault(f.get('label') or 'fra', []).append({'url': f['file']})\n+        return subtitles\n+\n     def _real_extract(self, url):\n         display_id = self._match_id(url)\n         webpage = self._download_webpage(url, display_id)\n@@ -63,20 +106,45 @@ def _real_extract(self, url):\n         video_files = self._parse_json(\n             vpl_data['data-broadcast'], display_id)\n         formats = []\n-        for video_file in video_files:\n-            v_url = video_file.get('url')\n-            if not v_url:\n-                continue\n-            video_format = video_file.get('format') or determine_ext(v_url)\n-            if video_format == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    v_url, display_id, 'mp4', 'm3u8_native',\n-                    m3u8_id='hls', fatal=False))\n-            else:\n-                formats.append({\n-                    'url': v_url,\n-                    'format_id': video_format,\n-                })\n+        video_id = None\n+\n+        def process_video_files(v):\n+            nonlocal video_id\n+            for video_file in v:\n+                v_url = video_file.get('url')\n+                if not v_url:\n+                    continue\n+                if video_file.get('type') == 'application/deferred':\n+                    d_param = urllib.parse.quote(v_url)\n+                    token = video_file.get('token')\n+                    if not token:\n+                        continue\n+                    deferred_json = self._download_json(\n+                        f'https://api.tv5monde.com/player/asset/{d_param}/resolve?condenseKS=true', display_id,\n+                        note='Downloading deferred info', headers={'Authorization': f'Bearer {token}'}, fatal=False)\n+                    v_url = traverse_obj(deferred_json, (0, 'url', {url_or_none}))\n+                    if not v_url:\n+                        continue\n+                    # data-guid from the webpage isn't stable, use the material id from the json urls\n+                    video_id = self._search_regex(\n+                        r'materials/([\\da-zA-Z]{10}_[\\da-fA-F]{7})/', v_url, 'video id', default=None)\n+                    process_video_files(deferred_json)\n+\n+                video_format = video_file.get('format') or determine_ext(v_url)\n+                if video_format == 'm3u8':\n+                    formats.extend(self._extract_m3u8_formats(\n+                        v_url, display_id, 'mp4', 'm3u8_native',\n+                        m3u8_id='hls', fatal=False))\n+                elif video_format == 'mpd':\n+                    formats.extend(self._extract_mpd_formats(\n+                        v_url, display_id, fatal=False))\n+                else:\n+                    formats.append({\n+                        'url': v_url,\n+                        'format_id': video_format,\n+                    })\n+\n+        process_video_files(video_files)\n \n         metadata = self._parse_json(\n             vpl_data['data-metadata'], display_id)\n@@ -100,10 +168,11 @@ def _real_extract(self, url):\n         if upload_date:\n             upload_date = upload_date.replace('_', '')\n \n-        video_id = self._search_regex(\n-            (r'data-guid=[\"\\']([\\da-f]{8}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{12})',\n-             r'id_contenu[\"\\']\\s:\\s*(\\d+)'), webpage, 'video id',\n-            default=display_id)\n+        if not video_id:\n+            video_id = self._search_regex(\n+                (r'data-guid=[\"\\']([\\da-f]{8}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{12})',\n+                 r'id_contenu[\"\\']\\s:\\s*(\\d+)'), webpage, 'video id',\n+                default=display_id)\n \n         return {\n             'id': video_id,\n@@ -114,6 +183,8 @@ def _real_extract(self, url):\n             'duration': duration,\n             'upload_date': upload_date,\n             'formats': formats,\n+            'subtitles': self._extract_subtitles(self._parse_json(\n+                traverse_obj(vpl_data, ('data-captions', {str}), default='{}'), display_id, fatal=False)),\n             'series': series,\n             'episode': episode,\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tvnet.py",
            "diff": "diff --git a/yt_dlp/extractor/tvnet.py b/yt_dlp/extractor/tvnet.py\ndeleted file mode 100644\nindex 77426f7e..00000000\n--- a/yt_dlp/extractor/tvnet.py\n+++ /dev/null\n@@ -1,138 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    int_or_none,\n-    unescapeHTML,\n-    url_or_none,\n-)\n-\n-\n-class TVNetIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:[^/]+)\\.tvnet\\.gov\\.vn/[^/]+/(?:\\d+/)?(?P<id>\\d+)(?:/|$)'\n-    _TESTS = [{\n-        # video\n-        'url': 'http://de.tvnet.gov.vn/video/109788/vtv1---bac-tuyet-tai-lao-cai-va-ha-giang/tin-nong-24h',\n-        'md5': 'b4d7abe0252c9b47774760b7519c7558',\n-        'info_dict': {\n-            'id': '109788',\n-            'ext': 'mp4',\n-            'title': 'VTV1 - B\u1eafc tuy\u1ebft t\u1ea1i L\u00e0o Cai v\u00e0 H\u00e0 Giang',\n-            'thumbnail': r're:(?i)https?://.*\\.(?:jpg|png)',\n-            'is_live': False,\n-            'view_count': int,\n-        },\n-    }, {\n-        # audio\n-        'url': 'http://vn.tvnet.gov.vn/radio/27017/vov1---ban-tin-chieu-10062018/doi-song-va-xa-hoi',\n-        'md5': 'b5875ce9b0a2eecde029216d0e6db2ae',\n-        'info_dict': {\n-            'id': '27017',\n-            'ext': 'm4a',\n-            'title': 'VOV1 - B\u1ea3n tin chi\u1ec1u (10/06/2018)',\n-            'thumbnail': r're:(?i)https?://.*\\.(?:jpg|png)',\n-            'is_live': False,\n-        },\n-    }, {\n-        'url': 'http://us.tvnet.gov.vn/video/118023/129999/ngay-0705',\n-        'info_dict': {\n-            'id': '129999',\n-            'ext': 'mp4',\n-            'title': 'VTV1 - Qu\u1ed1c h\u1ed9i v\u1edbi c\u1eed tri (11/06/2018)',\n-            'thumbnail': r're:(?i)https?://.*\\.(?:jpg|png)',\n-            'is_live': False,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        # live stream\n-        'url': 'http://us.tvnet.gov.vn/kenh-truyen-hinh/1011/vtv1',\n-        'info_dict': {\n-            'id': '1011',\n-            'ext': 'mp4',\n-            'title': r're:^VTV1 \\| LiveTV [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$',\n-            'thumbnail': r're:(?i)https?://.*\\.(?:jpg|png)',\n-            'is_live': True,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        # radio live stream\n-        'url': 'http://vn.tvnet.gov.vn/kenh-truyen-hinh/1014',\n-        'info_dict': {\n-            'id': '1014',\n-            'ext': 'm4a',\n-            'title': r're:VOV1 \\| LiveTV [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$',\n-            'thumbnail': r're:(?i)https?://.*\\.(?:jpg|png)',\n-            'is_live': True,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'http://us.tvnet.gov.vn/phim/6136/25510/vtv3---ca-mot-doi-an-oan-tap-1-50/phim-truyen-hinh',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = self._og_search_title(\n-            webpage, default=None) or self._html_search_meta(\n-            'title', webpage, default=None) or self._search_regex(\n-            r'<title>([^<]+)<', webpage, 'title')\n-        title = re.sub(r'\\s*-\\s*TV Net\\s*$', '', title)\n-\n-        if '/video/' in url or '/radio/' in url:\n-            is_live = False\n-        elif '/kenh-truyen-hinh/' in url:\n-            is_live = True\n-        else:\n-            is_live = None\n-\n-        data_file = unescapeHTML(self._search_regex(\n-            r'data-file=([\"\\'])(?P<url>(?:https?:)?//.+?)\\1', webpage,\n-            'data file', group='url'))\n-\n-        stream_urls = set()\n-        formats = []\n-        for stream in self._download_json(data_file, video_id):\n-            if not isinstance(stream, dict):\n-                continue\n-            stream_url = url_or_none(stream.get('url'))\n-            if stream_url in stream_urls or not stream_url:\n-                continue\n-            stream_urls.add(stream_url)\n-            formats.extend(self._extract_m3u8_formats(\n-                stream_url, video_id, 'mp4', live=is_live, m3u8_id='hls', fatal=False))\n-\n-        # better support for radio streams\n-        if title.startswith('VOV'):\n-            for f in formats:\n-                f.update({\n-                    'ext': 'm4a',\n-                    'vcodec': 'none',\n-                })\n-\n-        thumbnail = self._og_search_thumbnail(\n-            webpage, default=None) or unescapeHTML(\n-            self._search_regex(\n-                r'data-image=([\"\\'])(?P<url>(?:https?:)?//.+?)\\1', webpage,\n-                'thumbnail', default=None, group='url'))\n-\n-        view_count = int_or_none(self._search_regex(\n-            r'(?s)<div[^>]+\\bclass=[\"\\'].*?view-count[^>]+>.*?(\\d+).*?</div>',\n-            webpage, 'view count', default=None))\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'is_live': is_live,\n-            'view_count': view_count,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/tvnow.py",
            "diff": "diff --git a/yt_dlp/extractor/tvnow.py b/yt_dlp/extractor/tvnow.py\ndeleted file mode 100644\nindex 0acc306d..00000000\n--- a/yt_dlp/extractor/tvnow.py\n+++ /dev/null\n@@ -1,639 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    ExtractorError,\n-    get_element_by_id,\n-    int_or_none,\n-    parse_iso8601,\n-    parse_duration,\n-    str_or_none,\n-    try_get,\n-    update_url_query,\n-    urljoin,\n-)\n-\n-\n-class TVNowBaseIE(InfoExtractor):\n-    _VIDEO_FIELDS = (\n-        'id', 'title', 'free', 'geoblocked', 'articleLong', 'articleShort',\n-        'broadcastStartDate', 'isDrm', 'duration', 'season', 'episode',\n-        'manifest.dashclear', 'manifest.hlsclear', 'manifest.smoothclear',\n-        'format.title', 'format.defaultImage169Format', 'format.defaultImage169Logo')\n-\n-    def _call_api(self, path, video_id, query):\n-        return self._download_json(\n-            'https://api.tvnow.de/v3/' + path, video_id, query=query)\n-\n-    def _extract_video(self, info, display_id):\n-        video_id = compat_str(info['id'])\n-        title = info['title']\n-\n-        paths = []\n-        for manifest_url in (info.get('manifest') or {}).values():\n-            if not manifest_url:\n-                continue\n-            manifest_url = update_url_query(manifest_url, {'filter': ''})\n-            path = self._search_regex(r'https?://[^/]+/(.+?)\\.ism/', manifest_url, 'path')\n-            if path in paths:\n-                continue\n-            paths.append(path)\n-\n-            def url_repl(proto, suffix):\n-                return re.sub(\n-                    r'(?:hls|dash|hss)([.-])', proto + r'\\1', re.sub(\n-                        r'\\.ism/(?:[^.]*\\.(?:m3u8|mpd)|[Mm]anifest)',\n-                        '.ism/' + suffix, manifest_url))\n-\n-            def make_urls(proto, suffix):\n-                urls = [url_repl(proto, suffix)]\n-                hd_url = urls[0].replace('/manifest/', '/ngvod/')\n-                if hd_url != urls[0]:\n-                    urls.append(hd_url)\n-                return urls\n-\n-            for man_url in make_urls('dash', '.mpd'):\n-                formats = self._extract_mpd_formats(\n-                    man_url, video_id, mpd_id='dash', fatal=False)\n-            for man_url in make_urls('hss', 'Manifest'):\n-                formats.extend(self._extract_ism_formats(\n-                    man_url, video_id, ism_id='mss', fatal=False))\n-            for man_url in make_urls('hls', '.m3u8'):\n-                formats.extend(self._extract_m3u8_formats(\n-                    man_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls',\n-                    fatal=False))\n-            if formats:\n-                break\n-        else:\n-            if not self.get_param('allow_unplayable_formats') and info.get('isDrm'):\n-                raise ExtractorError(\n-                    'Video %s is DRM protected' % video_id, expected=True)\n-            if info.get('geoblocked'):\n-                raise self.raise_geo_restricted()\n-            if not info.get('free', True):\n-                raise ExtractorError(\n-                    'Video %s is not available for free' % video_id, expected=True)\n-\n-        description = info.get('articleLong') or info.get('articleShort')\n-        timestamp = parse_iso8601(info.get('broadcastStartDate'), ' ')\n-        duration = parse_duration(info.get('duration'))\n-\n-        f = info.get('format', {})\n-\n-        thumbnails = [{\n-            'url': 'https://aistvnow-a.akamaihd.net/tvnow/movie/%s' % video_id,\n-        }]\n-        thumbnail = f.get('defaultImage169Format') or f.get('defaultImage169Logo')\n-        if thumbnail:\n-            thumbnails.append({\n-                'url': thumbnail,\n-            })\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnails': thumbnails,\n-            'timestamp': timestamp,\n-            'duration': duration,\n-            'series': f.get('title'),\n-            'season_number': int_or_none(info.get('season')),\n-            'episode_number': int_or_none(info.get('episode')),\n-            'episode': title,\n-            'formats': formats,\n-        }\n-\n-\n-class TVNowIE(TVNowBaseIE):\n-    _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?:www\\.)?tvnow\\.(?:de|at|ch)/(?P<station>[^/]+)/\n-                        (?P<show_id>[^/]+)/\n-                        (?!(?:list|jahr)(?:/|$))(?P<id>[^/?\\#&]+)\n-                    '''\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        return (False if TVNowNewIE.suitable(url) or TVNowSeasonIE.suitable(url) or TVNowAnnualIE.suitable(url) or TVNowShowIE.suitable(url)\n-                else super(TVNowIE, cls).suitable(url))\n-\n-    _TESTS = [{\n-        'url': 'https://www.tvnow.de/rtl2/grip-das-motormagazin/der-neue-porsche-911-gt-3/player',\n-        'info_dict': {\n-            'id': '331082',\n-            'display_id': 'grip-das-motormagazin/der-neue-porsche-911-gt-3',\n-            'ext': 'mp4',\n-            'title': 'Der neue Porsche 911 GT 3',\n-            'description': 'md5:6143220c661f9b0aae73b245e5d898bb',\n-            'timestamp': 1495994400,\n-            'upload_date': '20170528',\n-            'duration': 5283,\n-            'series': 'GRIP - Das Motormagazin',\n-            'season_number': 14,\n-            'episode_number': 405,\n-            'episode': 'Der neue Porsche 911 GT 3',\n-        },\n-    }, {\n-        # rtl2\n-        'url': 'https://www.tvnow.de/rtl2/armes-deutschland/episode-0008/player',\n-        'only_matching': True,\n-    }, {\n-        # rtlnitro\n-        'url': 'https://www.tvnow.de/nitro/alarm-fuer-cobra-11-die-autobahnpolizei/auf-eigene-faust-pilot/player',\n-        'only_matching': True,\n-    }, {\n-        # superrtl\n-        'url': 'https://www.tvnow.de/superrtl/die-lustigsten-schlamassel-der-welt/u-a-ketchup-effekt/player',\n-        'only_matching': True,\n-    }, {\n-        # ntv\n-        'url': 'https://www.tvnow.de/ntv/startup-news/goetter-in-weiss/player',\n-        'only_matching': True,\n-    }, {\n-        # vox\n-        'url': 'https://www.tvnow.de/vox/auto-mobil/neues-vom-automobilmarkt-2017-11-19-17-00-00/player',\n-        'only_matching': True,\n-    }, {\n-        # rtlplus\n-        'url': 'https://www.tvnow.de/rtlplus/op-ruft-dr-bruckner/die-vernaehte-frau/player',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.tvnow.de/rtl2/grip-das-motormagazin/der-neue-porsche-911-gt-3',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        display_id = '%s/%s' % mobj.group(2, 3)\n-\n-        info = self._call_api(\n-            'movies/' + display_id, display_id, query={\n-                'fields': ','.join(self._VIDEO_FIELDS),\n-            })\n-\n-        return self._extract_video(info, display_id)\n-\n-\n-class TVNowNewIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)\n-                    (?P<base_url>https?://\n-                        (?:www\\.)?tvnow\\.(?:de|at|ch)/\n-                        (?:shows|serien))/\n-                        (?P<show>[^/]+)-\\d+/\n-                        [^/]+/\n-                        episode-\\d+-(?P<episode>[^/?$&]+)-(?P<id>\\d+)\n-                    '''\n-\n-    _TESTS = [{\n-        'url': 'https://www.tvnow.de/shows/grip-das-motormagazin-1669/2017-05/episode-405-der-neue-porsche-911-gt-3-331082',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        base_url = re.sub(r'(?:shows|serien)', '_', mobj.group('base_url'))\n-        show, episode = mobj.group('show', 'episode')\n-        return self.url_result(\n-            # Rewrite new URLs to the old format and use extraction via old API\n-            # at api.tvnow.de as a loophole for bypassing premium content checks\n-            '%s/%s/%s' % (base_url, show, episode),\n-            ie=TVNowIE.ie_key(), video_id=mobj.group('id'))\n-\n-\n-class TVNowFilmIE(TVNowBaseIE):\n-    _VALID_URL = r'''(?x)\n-                    (?P<base_url>https?://\n-                        (?:www\\.)?tvnow\\.(?:de|at|ch)/\n-                        (?:filme))/\n-                        (?P<title>[^/?$&]+)-(?P<id>\\d+)\n-                    '''\n-    _TESTS = [{\n-        'url': 'https://www.tvnow.de/filme/lord-of-war-haendler-des-todes-7959',\n-        'info_dict': {\n-            'id': '1426690',\n-            'display_id': 'lord-of-war-haendler-des-todes',\n-            'ext': 'mp4',\n-            'title': 'Lord of War',\n-            'description': 'md5:5eda15c0d5b8cb70dac724c8a0ff89a9',\n-            'timestamp': 1550010000,\n-            'upload_date': '20190212',\n-            'duration': 7016,\n-        },\n-    }, {\n-        'url': 'https://www.tvnow.de/filme/the-machinist-12157',\n-        'info_dict': {\n-            'id': '328160',\n-            'display_id': 'the-machinist',\n-            'ext': 'mp4',\n-            'title': 'The Machinist',\n-            'description': 'md5:9a0e363fdd74b3a9e1cdd9e21d0ecc28',\n-            'timestamp': 1496469720,\n-            'upload_date': '20170603',\n-            'duration': 5836,\n-        },\n-    }, {\n-        'url': 'https://www.tvnow.de/filme/horst-schlaemmer-isch-kandidiere-17777',\n-        'only_matching': True,  # DRM protected\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        display_id = mobj.group('title')\n-\n-        webpage = self._download_webpage(url, display_id, fatal=False)\n-        if not webpage:\n-            raise ExtractorError('Cannot download \"%s\"' % url, expected=True)\n-\n-        json_text = get_element_by_id('now-web-state', webpage)\n-        if not json_text:\n-            raise ExtractorError('Cannot read video data', expected=True)\n-\n-        json_data = self._parse_json(\n-            json_text,\n-            display_id,\n-            transform_source=lambda x: x.replace('&q;', '\"'),\n-            fatal=False)\n-        if not json_data:\n-            raise ExtractorError('Cannot read video data', expected=True)\n-\n-        player_key = next(\n-            (key for key in json_data.keys() if 'module/player' in key),\n-            None)\n-        page_key = next(\n-            (key for key in json_data.keys() if 'page/filme' in key),\n-            None)\n-        movie_id = try_get(\n-            json_data,\n-            [\n-                lambda x: x[player_key]['body']['id'],\n-                lambda x: x[page_key]['body']['modules'][0]['id'],\n-                lambda x: x[page_key]['body']['modules'][1]['id']],\n-            int)\n-        if not movie_id:\n-            raise ExtractorError('Cannot extract movie ID', expected=True)\n-\n-        info = self._call_api(\n-            'movies/%d' % movie_id,\n-            display_id,\n-            query={'fields': ','.join(self._VIDEO_FIELDS)})\n-\n-        return self._extract_video(info, display_id)\n-\n-\n-class TVNowNewBaseIE(InfoExtractor):\n-    def _call_api(self, path, video_id, query={}):\n-        result = self._download_json(\n-            'https://apigw.tvnow.de/module/' + path, video_id, query=query)\n-        error = result.get('error')\n-        if error:\n-            raise ExtractorError(\n-                '%s said: %s' % (self.IE_NAME, error), expected=True)\n-        return result\n-\n-\n-r\"\"\"\n-TODO: new apigw.tvnow.de based version of TVNowIE. Replace old TVNowIE with it\n-when api.tvnow.de is shut down. This version can't bypass premium checks though.\n-class TVNowIE(TVNowNewBaseIE):\n-    _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?:www\\.)?tvnow\\.(?:de|at|ch)/\n-                        (?:shows|serien)/[^/]+/\n-                        (?:[^/]+/)+\n-                        (?P<display_id>[^/?$&]+)-(?P<id>\\d+)\n-                    '''\n-\n-    _TESTS = [{\n-        # episode with annual navigation\n-        'url': 'https://www.tvnow.de/shows/grip-das-motormagazin-1669/2017-05/episode-405-der-neue-porsche-911-gt-3-331082',\n-        'info_dict': {\n-            'id': '331082',\n-            'display_id': 'grip-das-motormagazin/der-neue-porsche-911-gt-3',\n-            'ext': 'mp4',\n-            'title': 'Der neue Porsche 911 GT 3',\n-            'description': 'md5:6143220c661f9b0aae73b245e5d898bb',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'timestamp': 1495994400,\n-            'upload_date': '20170528',\n-            'duration': 5283,\n-            'series': 'GRIP - Das Motormagazin',\n-            'season_number': 14,\n-            'episode_number': 405,\n-            'episode': 'Der neue Porsche 911 GT 3',\n-        },\n-    }, {\n-        # rtl2, episode with season navigation\n-        'url': 'https://www.tvnow.de/shows/armes-deutschland-11471/staffel-3/episode-14-bernd-steht-seit-der-trennung-von-seiner-frau-allein-da-526124',\n-        'only_matching': True,\n-    }, {\n-        # rtlnitro\n-        'url': 'https://www.tvnow.de/serien/alarm-fuer-cobra-11-die-autobahnpolizei-1815/staffel-13/episode-5-auf-eigene-faust-pilot-366822',\n-        'only_matching': True,\n-    }, {\n-        # superrtl\n-        'url': 'https://www.tvnow.de/shows/die-lustigsten-schlamassel-der-welt-1221/staffel-2/episode-14-u-a-ketchup-effekt-364120',\n-        'only_matching': True,\n-    }, {\n-        # ntv\n-        'url': 'https://www.tvnow.de/shows/startup-news-10674/staffel-2/episode-39-goetter-in-weiss-387630',\n-        'only_matching': True,\n-    }, {\n-        # vox\n-        'url': 'https://www.tvnow.de/shows/auto-mobil-174/2017-11/episode-46-neues-vom-automobilmarkt-2017-11-19-17-00-00-380072',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.tvnow.de/shows/grip-das-motormagazin-1669/2017-05/episode-405-der-neue-porsche-911-gt-3-331082',\n-        'only_matching': True,\n-    }]\n-\n-    def _extract_video(self, info, url, display_id):\n-        config = info['config']\n-        source = config['source']\n-\n-        video_id = compat_str(info.get('id') or source['videoId'])\n-        title = source['title'].strip()\n-\n-        paths = []\n-        for manifest_url in (info.get('manifest') or {}).values():\n-            if not manifest_url:\n-                continue\n-            manifest_url = update_url_query(manifest_url, {'filter': ''})\n-            path = self._search_regex(r'https?://[^/]+/(.+?)\\.ism/', manifest_url, 'path')\n-            if path in paths:\n-                continue\n-            paths.append(path)\n-\n-            def url_repl(proto, suffix):\n-                return re.sub(\n-                    r'(?:hls|dash|hss)([.-])', proto + r'\\1', re.sub(\n-                        r'\\.ism/(?:[^.]*\\.(?:m3u8|mpd)|[Mm]anifest)',\n-                        '.ism/' + suffix, manifest_url))\n-\n-            formats = self._extract_mpd_formats(\n-                url_repl('dash', '.mpd'), video_id,\n-                mpd_id='dash', fatal=False)\n-            formats.extend(self._extract_ism_formats(\n-                url_repl('hss', 'Manifest'),\n-                video_id, ism_id='mss', fatal=False))\n-            formats.extend(self._extract_m3u8_formats(\n-                url_repl('hls', '.m3u8'), video_id, 'mp4',\n-                'm3u8_native', m3u8_id='hls', fatal=False))\n-            if formats:\n-                break\n-        else:\n-            if try_get(info, lambda x: x['rights']['isDrm']):\n-                raise ExtractorError(\n-                    'Video %s is DRM protected' % video_id, expected=True)\n-            if try_get(config, lambda x: x['boards']['geoBlocking']['block']):\n-                raise self.raise_geo_restricted()\n-            if not info.get('free', True):\n-                raise ExtractorError(\n-                    'Video %s is not available for free' % video_id, expected=True)\n-\n-        description = source.get('description')\n-        thumbnail = url_or_none(source.get('poster'))\n-        timestamp = unified_timestamp(source.get('previewStart'))\n-        duration = parse_duration(source.get('length'))\n-\n-        series = source.get('format')\n-        season_number = int_or_none(self._search_regex(\n-            r'staffel-(\\d+)', url, 'season number', default=None))\n-        episode_number = int_or_none(self._search_regex(\n-            r'episode-(\\d+)', url, 'episode number', default=None))\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'timestamp': timestamp,\n-            'duration': duration,\n-            'series': series,\n-            'season_number': season_number,\n-            'episode_number': episode_number,\n-            'episode': title,\n-            'formats': formats,\n-        }\n-\n-    def _real_extract(self, url):\n-        display_id, video_id = self._match_valid_url(url).groups()\n-        info = self._call_api('player/' + video_id, video_id)\n-        return self._extract_video(info, video_id, display_id)\n-\n-\n-class TVNowFilmIE(TVNowIE):  # XXX: Do not subclass from concrete IE\n-    _VALID_URL = r'''(?x)\n-                    (?P<base_url>https?://\n-                        (?:www\\.)?tvnow\\.(?:de|at|ch)/\n-                        (?:filme))/\n-                        (?P<title>[^/?$&]+)-(?P<id>\\d+)\n-                    '''\n-    _TESTS = [{\n-        'url': 'https://www.tvnow.de/filme/lord-of-war-haendler-des-todes-7959',\n-        'info_dict': {\n-            'id': '1426690',\n-            'display_id': 'lord-of-war-haendler-des-todes',\n-            'ext': 'mp4',\n-            'title': 'Lord of War',\n-            'description': 'md5:5eda15c0d5b8cb70dac724c8a0ff89a9',\n-            'timestamp': 1550010000,\n-            'upload_date': '20190212',\n-            'duration': 7016,\n-        },\n-    }, {\n-        'url': 'https://www.tvnow.de/filme/the-machinist-12157',\n-        'info_dict': {\n-            'id': '328160',\n-            'display_id': 'the-machinist',\n-            'ext': 'mp4',\n-            'title': 'The Machinist',\n-            'description': 'md5:9a0e363fdd74b3a9e1cdd9e21d0ecc28',\n-            'timestamp': 1496469720,\n-            'upload_date': '20170603',\n-            'duration': 5836,\n-        },\n-    }, {\n-        'url': 'https://www.tvnow.de/filme/horst-schlaemmer-isch-kandidiere-17777',\n-        'only_matching': True,  # DRM protected\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        display_id = mobj.group('title')\n-\n-        webpage = self._download_webpage(url, display_id, fatal=False)\n-        if not webpage:\n-            raise ExtractorError('Cannot download \"%s\"' % url, expected=True)\n-\n-        json_text = get_element_by_id('now-web-state', webpage)\n-        if not json_text:\n-            raise ExtractorError('Cannot read video data', expected=True)\n-\n-        json_data = self._parse_json(\n-            json_text,\n-            display_id,\n-            transform_source=lambda x: x.replace('&q;', '\"'),\n-            fatal=False)\n-        if not json_data:\n-            raise ExtractorError('Cannot read video data', expected=True)\n-\n-        player_key = next(\n-            (key for key in json_data.keys() if 'module/player' in key),\n-            None)\n-        page_key = next(\n-            (key for key in json_data.keys() if 'page/filme' in key),\n-            None)\n-        movie_id = try_get(\n-            json_data,\n-            [\n-                lambda x: x[player_key]['body']['id'],\n-                lambda x: x[page_key]['body']['modules'][0]['id'],\n-                lambda x: x[page_key]['body']['modules'][1]['id']],\n-            int)\n-        if not movie_id:\n-            raise ExtractorError('Cannot extract movie ID', expected=True)\n-\n-        info = self._call_api('player/%d' % movie_id, display_id)\n-        return self._extract_video(info, url, display_id)\n-\"\"\"\n-\n-\n-class TVNowListBaseIE(TVNowNewBaseIE):\n-    _SHOW_VALID_URL = r'''(?x)\n-                    (?P<base_url>\n-                        https?://\n-                            (?:www\\.)?tvnow\\.(?:de|at|ch)/(?:shows|serien)/\n-                            [^/?#&]+-(?P<show_id>\\d+)\n-                    )\n-                    '''\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        return (False if TVNowNewIE.suitable(url)\n-                else super(TVNowListBaseIE, cls).suitable(url))\n-\n-    def _extract_items(self, url, show_id, list_id, query):\n-        items = self._call_api(\n-            'teaserrow/format/episode/' + show_id, list_id,\n-            query=query)['items']\n-\n-        entries = []\n-        for item in items:\n-            if not isinstance(item, dict):\n-                continue\n-            item_url = urljoin(url, item.get('url'))\n-            if not item_url:\n-                continue\n-            video_id = str_or_none(item.get('id') or item.get('videoId'))\n-            item_title = item.get('subheadline') or item.get('text')\n-            entries.append(self.url_result(\n-                item_url, ie=TVNowNewIE.ie_key(), video_id=video_id,\n-                video_title=item_title))\n-\n-        return self.playlist_result(entries, '%s/%s' % (show_id, list_id))\n-\n-\n-class TVNowSeasonIE(TVNowListBaseIE):\n-    _VALID_URL = r'%s/staffel-(?P<id>\\d+)' % TVNowListBaseIE._SHOW_VALID_URL\n-    _TESTS = [{\n-        'url': 'https://www.tvnow.de/serien/alarm-fuer-cobra-11-die-autobahnpolizei-1815/staffel-13',\n-        'info_dict': {\n-            'id': '1815/13',\n-        },\n-        'playlist_mincount': 22,\n-    }]\n-\n-    def _real_extract(self, url):\n-        _, show_id, season_id = self._match_valid_url(url).groups()\n-        return self._extract_items(\n-            url, show_id, season_id, {'season': season_id})\n-\n-\n-class TVNowAnnualIE(TVNowListBaseIE):\n-    _VALID_URL = r'%s/(?P<year>\\d{4})-(?P<month>\\d{2})' % TVNowListBaseIE._SHOW_VALID_URL\n-    _TESTS = [{\n-        'url': 'https://www.tvnow.de/shows/grip-das-motormagazin-1669/2017-05',\n-        'info_dict': {\n-            'id': '1669/2017-05',\n-        },\n-        'playlist_mincount': 2,\n-    }]\n-\n-    def _real_extract(self, url):\n-        _, show_id, year, month = self._match_valid_url(url).groups()\n-        return self._extract_items(\n-            url, show_id, '%s-%s' % (year, month), {\n-                'year': int(year),\n-                'month': int(month),\n-            })\n-\n-\n-class TVNowShowIE(TVNowListBaseIE):\n-    _VALID_URL = TVNowListBaseIE._SHOW_VALID_URL\n-    _TESTS = [{\n-        # annual navigationType\n-        'url': 'https://www.tvnow.de/shows/grip-das-motormagazin-1669',\n-        'info_dict': {\n-            'id': '1669',\n-        },\n-        'playlist_mincount': 73,\n-    }, {\n-        # season navigationType\n-        'url': 'https://www.tvnow.de/shows/armes-deutschland-11471',\n-        'info_dict': {\n-            'id': '11471',\n-        },\n-        'playlist_mincount': 3,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        return (False if TVNowNewIE.suitable(url) or TVNowSeasonIE.suitable(url) or TVNowAnnualIE.suitable(url)\n-                else super(TVNowShowIE, cls).suitable(url))\n-\n-    def _real_extract(self, url):\n-        base_url, show_id = self._match_valid_url(url).groups()\n-\n-        result = self._call_api(\n-            'teaserrow/format/navigation/' + show_id, show_id)\n-\n-        items = result['items']\n-\n-        entries = []\n-        navigation = result.get('navigationType')\n-        if navigation == 'annual':\n-            for item in items:\n-                if not isinstance(item, dict):\n-                    continue\n-                year = int_or_none(item.get('year'))\n-                if year is None:\n-                    continue\n-                months = item.get('months')\n-                if not isinstance(months, list):\n-                    continue\n-                for month_dict in months:\n-                    if not isinstance(month_dict, dict) or not month_dict:\n-                        continue\n-                    month_number = int_or_none(list(month_dict.keys())[0])\n-                    if month_number is None:\n-                        continue\n-                    entries.append(self.url_result(\n-                        '%s/%04d-%02d' % (base_url, year, month_number),\n-                        ie=TVNowAnnualIE.ie_key()))\n-        elif navigation == 'season':\n-            for item in items:\n-                if not isinstance(item, dict):\n-                    continue\n-                season_number = int_or_none(item.get('season'))\n-                if season_number is None:\n-                    continue\n-                entries.append(self.url_result(\n-                    '%s/staffel-%d' % (base_url, season_number),\n-                    ie=TVNowSeasonIE.ie_key()))\n-        else:\n-            raise ExtractorError('Unknown navigationType')\n-\n-        return self.playlist_result(entries, show_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/twentyfourvideo.py",
            "diff": "diff --git a/yt_dlp/extractor/twentyfourvideo.py b/yt_dlp/extractor/twentyfourvideo.py\ndeleted file mode 100644\nindex baeb85d4..00000000\n--- a/yt_dlp/extractor/twentyfourvideo.py\n+++ /dev/null\n@@ -1,128 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    parse_iso8601,\n-    int_or_none,\n-    xpath_attr,\n-    xpath_element,\n-)\n-\n-\n-class TwentyFourVideoIE(InfoExtractor):\n-    IE_NAME = '24video'\n-    _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?P<host>\n-                            (?:(?:www|porno?)\\.)?24video\\.\n-                            (?:net|me|xxx|sexy?|tube|adult|site|vip)\n-                        )/\n-                        (?:\n-                            video/(?:(?:view|xml)/)?|\n-                            player/new24_play\\.swf\\?id=\n-                        )\n-                        (?P<id>\\d+)\n-                    '''\n-\n-    _TESTS = [{\n-        'url': 'http://www.24video.net/video/view/1044982',\n-        'md5': 'e09fc0901d9eaeedac872f154931deeb',\n-        'info_dict': {\n-            'id': '1044982',\n-            'ext': 'mp4',\n-            'title': '\u042d\u0440\u043e\u0442\u0438\u043a\u0430 \u043a\u0430\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0432\u0435\u043a\u0430',\n-            'description': '\u041a\u0430\u043a \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u043f\u043e\u0440\u043d\u043e \u0432 \u043a\u0430\u043c\u0435\u043d\u043d\u043e\u043c \u0432\u0435\u043a\u0435.',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'uploader': 'SUPERTELO',\n-            'duration': 31,\n-            'timestamp': 1275937857,\n-            'upload_date': '20100607',\n-            'age_limit': 18,\n-            'like_count': int,\n-            'dislike_count': int,\n-        },\n-    }, {\n-        'url': 'http://www.24video.net/player/new24_play.swf?id=1044982',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.24video.me/video/view/1044982',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://www.24video.tube/video/view/2363750',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.24video.site/video/view/2640421',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://porno.24video.net/video/2640421-vsya-takaya-gibkaya-i-v-masle',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.24video.vip/video/view/1044982',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://porn.24video.net/video/2640421-vsya-takay',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        host = mobj.group('host')\n-\n-        webpage = self._download_webpage(\n-            'http://%s/video/view/%s' % (host, video_id), video_id)\n-\n-        title = self._og_search_title(webpage)\n-        description = self._html_search_regex(\n-            r'<(p|span)[^>]+itemprop=\"description\"[^>]*>(?P<description>[^<]+)</\\1>',\n-            webpage, 'description', fatal=False, group='description')\n-        thumbnail = self._og_search_thumbnail(webpage)\n-        duration = int_or_none(self._og_search_property(\n-            'duration', webpage, 'duration', fatal=False))\n-        timestamp = parse_iso8601(self._search_regex(\n-            r'<time[^>]+\\bdatetime=\"([^\"]+)\"[^>]+itemprop=\"uploadDate\"',\n-            webpage, 'upload date', fatal=False))\n-\n-        uploader = self._html_search_regex(\n-            r'class=\"video-uploaded\"[^>]*>\\s*<a href=\"/jsecUser/movies/[^\"]+\"[^>]*>([^<]+)</a>',\n-            webpage, 'uploader', fatal=False)\n-\n-        view_count = int_or_none(self._html_search_regex(\n-            r'<span class=\"video-views\">(\\d+) \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440',\n-            webpage, 'view count', fatal=False))\n-        comment_count = int_or_none(self._html_search_regex(\n-            r'<a[^>]+href=\"#tab-comments\"[^>]*>(\\d+) \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438',\n-            webpage, 'comment count', default=None))\n-\n-        # Sets some cookies\n-        self._download_xml(\n-            r'http://%s/video/xml/%s?mode=init' % (host, video_id),\n-            video_id, 'Downloading init XML')\n-\n-        video_xml = self._download_xml(\n-            'http://%s/video/xml/%s?mode=play' % (host, video_id),\n-            video_id, 'Downloading video XML')\n-\n-        video = xpath_element(video_xml, './/video', 'video', fatal=True)\n-\n-        formats = [{\n-            'url': xpath_attr(video, '', 'url', 'video URL', fatal=True),\n-        }]\n-\n-        like_count = int_or_none(video.get('ratingPlus'))\n-        dislike_count = int_or_none(video.get('ratingMinus'))\n-        age_limit = 18 if video.get('adult') == 'true' else 0\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'uploader': uploader,\n-            'duration': duration,\n-            'timestamp': timestamp,\n-            'view_count': view_count,\n-            'comment_count': comment_count,\n-            'like_count': like_count,\n-            'dislike_count': dislike_count,\n-            'age_limit': age_limit,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/twitcasting.py",
            "diff": "diff --git a/yt_dlp/extractor/twitcasting.py b/yt_dlp/extractor/twitcasting.py\nindex dff353a4..28ea16cc 100644\n--- a/yt_dlp/extractor/twitcasting.py\n+++ b/yt_dlp/extractor/twitcasting.py\n@@ -5,8 +5,9 @@\n from .common import InfoExtractor\n from ..dependencies import websockets\n from ..utils import (\n-    clean_html,\n     ExtractorError,\n+    UserNotLive,\n+    clean_html,\n     float_or_none,\n     get_element_by_class,\n     get_element_by_id,\n@@ -22,7 +23,7 @@\n \n \n class TwitCastingIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:[^/]+\\.)?twitcasting\\.tv/(?P<uploader_id>[^/]+)/(?:movie|twplayer)/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:[^/?#]+\\.)?twitcasting\\.tv/(?P<uploader_id>[^/?#]+)/(?:movie|twplayer)/(?P<id>\\d+)'\n     _M3U8_HEADERS = {\n         'Origin': 'https://twitcasting.tv',\n         'Referer': 'https://twitcasting.tv/',\n@@ -141,7 +142,7 @@ def _real_extract(self, url):\n             'https://twitcasting.tv/streamserver.php?target=%s&mode=client' % uploader_id, video_id,\n             'Downloading live info', fatal=False)\n \n-        is_live = 'data-status=\"online\"' in webpage\n+        is_live = any(f'data-{x}' in webpage for x in ['is-onlive=\"true\"', 'live-type=\"live\"', 'status=\"online\"'])\n         if not traverse_obj(stream_server_data, 'llfmp4') and is_live:\n             self.raise_login_required(method='cookies')\n \n@@ -231,10 +232,13 @@ def find_dmu(x):\n \n \n class TwitCastingLiveIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:[^/]+\\.)?twitcasting\\.tv/(?P<id>[^/]+)/?(?:[#?]|$)'\n+    _VALID_URL = r'https?://(?:[^/?#]+\\.)?twitcasting\\.tv/(?P<id>[^/?#]+)/?(?:[#?]|$)'\n     _TESTS = [{\n         'url': 'https://twitcasting.tv/ivetesangalo',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://twitcasting.tv/c:unusedlive',\n+        'expected_exception': 'UserNotLive',\n     }]\n \n     def _real_extract(self, url):\n@@ -243,30 +247,38 @@ def _real_extract(self, url):\n             'Downloading live video of user {0}. '\n             'Pass \"https://twitcasting.tv/{0}/show\" to download the history'.format(uploader_id))\n \n-        webpage = self._download_webpage(url, uploader_id)\n+        is_live = traverse_obj(self._download_json(\n+            f'https://frontendapi.twitcasting.tv/watch/user/{uploader_id}',\n+            uploader_id, 'Checking live status', data=b'', fatal=False), ('is_live', {bool}))\n+        if is_live is False:  # only raise here if API response was as expected\n+            raise UserNotLive(video_id=uploader_id)\n+\n+        # Use /show/ page so that password-protected and members-only livestreams can be found\n+        webpage = self._download_webpage(\n+            f'https://twitcasting.tv/{uploader_id}/show/', uploader_id, 'Downloading live history')\n+        is_live = is_live or self._search_regex(\n+            r'(?s)(<span\\s*class=\"tw-movie-thumbnail2-badge\"\\s*data-status=\"live\">\\s*LIVE)',\n+            webpage, 'is live?', default=False)\n+        # Current live is always the first match\n         current_live = self._search_regex(\n-            (r'data-type=\"movie\" data-id=\"(\\d+)\">',\n-             r'tw-sound-flag-open-link\" data-id=\"(\\d+)\" style=',),\n-            webpage, 'current live ID', default=None)\n-        if not current_live:\n-            # fetch unfiltered /show to find running livestreams; we can't get ID of the password-protected livestream above\n-            webpage = self._download_webpage(\n-                f'https://twitcasting.tv/{uploader_id}/show/', uploader_id,\n-                note='Downloading live history')\n-            is_live = self._search_regex(r'(?s)(<span\\s*class=\"tw-movie-thumbnail-badge\"\\s*data-status=\"live\">\\s*LIVE)', webpage, 'is live?', default=None)\n-            if is_live:\n-                # get the first live; running live is always at the first\n-                current_live = self._search_regex(\n-                    r'(?s)<a\\s+class=\"tw-movie-thumbnail\"\\s*href=\"/[^/]+/movie/(?P<video_id>\\d+)\"\\s*>.+?</a>',\n-                    webpage, 'current live ID 2', default=None, group='video_id')\n-        if not current_live:\n-            raise ExtractorError('The user is not currently live')\n-        return self.url_result('https://twitcasting.tv/%s/movie/%s' % (uploader_id, current_live))\n+            r'(?s)<a\\s+class=\"tw-movie-thumbnail2\"\\s+href=\"/[^/\"]+/movie/(?P<video_id>\\d+)\"',\n+            webpage, 'current live ID', default=None, group='video_id')\n+        if not is_live or not current_live:\n+            raise UserNotLive(video_id=uploader_id)\n+\n+        return self.url_result(f'https://twitcasting.tv/{uploader_id}/movie/{current_live}', TwitCastingIE)\n \n \n class TwitCastingUserIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:[^/]+\\.)?twitcasting\\.tv/(?P<id>[^/]+)/show/?(?:[#?]|$)'\n+    _VALID_URL = r'https?://(?:[^/?#]+\\.)?twitcasting\\.tv/(?P<id>[^/?#]+)/(:?show|archive)/?(?:[#?]|$)'\n     _TESTS = [{\n+        'url': 'https://twitcasting.tv/natsuiromatsuri/archive/',\n+        'info_dict': {\n+            'id': 'natsuiromatsuri',\n+            'title': 'natsuiromatsuri - Live History',\n+        },\n+        'playlist_mincount': 235,\n+    }, {\n         'url': 'https://twitcasting.tv/noriyukicas/show',\n         'only_matching': True,\n     }]\n@@ -277,8 +289,7 @@ def _entries(self, uploader_id):\n             webpage = self._download_webpage(\n                 next_url, uploader_id, query={'filter': 'watchable'}, note='Downloading page %d' % page_num)\n             matches = re.finditer(\n-                r'''(?isx)<a\\s+class=\"tw-movie-thumbnail\"\\s*href=\"(?P<url>/[^/]+/movie/\\d+)\"\\s*>.+?</a>''',\n-                webpage)\n+                r'(?s)<a\\s+class=\"tw-movie-thumbnail2\"\\s+href=\"(?P<url>/[^/\"]+/movie/\\d+)\"', webpage)\n             for mobj in matches:\n                 yield self.url_result(urljoin(base_url, mobj.group('url')))\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/twitch.py",
            "diff": "diff --git a/yt_dlp/extractor/twitch.py b/yt_dlp/extractor/twitch.py\nindex 3297ef09..6dc0993a 100644\n--- a/yt_dlp/extractor/twitch.py\n+++ b/yt_dlp/extractor/twitch.py\n@@ -8,7 +8,6 @@\n from ..compat import (\n     compat_parse_qs,\n     compat_str,\n-    compat_urllib_parse_urlencode,\n     compat_urllib_parse_urlparse,\n )\n from ..utils import (\n@@ -191,6 +190,20 @@ def _get_thumbnails(self, thumbnail):\n             'url': thumbnail,\n         }] if thumbnail else None\n \n+    def _extract_twitch_m3u8_formats(self, video_id, token, signature):\n+        \"\"\"Subclasses must define _M3U8_PATH\"\"\"\n+        return self._extract_m3u8_formats(\n+            f'{self._USHER_BASE}/{self._M3U8_PATH}/{video_id}.m3u8', video_id, 'mp4', query={\n+                'allow_source': 'true',\n+                'allow_audio_only': 'true',\n+                'allow_spectre': 'true',\n+                'p': random.randint(1000000, 10000000),\n+                'player': 'twitchweb',\n+                'playlist_include_framerate': 'true',\n+                'sig': signature,\n+                'token': token,\n+            })\n+\n \n class TwitchVodIE(TwitchBaseIE):\n     IE_NAME = 'twitch:vod'\n@@ -203,6 +216,7 @@ class TwitchVodIE(TwitchBaseIE):\n                         )\n                         (?P<id>\\d+)\n                     '''\n+    _M3U8_PATH = 'vod'\n \n     _TESTS = [{\n         'url': 'http://www.twitch.tv/riotgames/v/6528877?t=5m10s',\n@@ -532,20 +546,8 @@ def _real_extract(self, url):\n         info = self._extract_info_gql(video, vod_id)\n         access_token = self._download_access_token(vod_id, 'video', 'id')\n \n-        formats = self._extract_m3u8_formats(\n-            '%s/vod/%s.m3u8?%s' % (\n-                self._USHER_BASE, vod_id,\n-                compat_urllib_parse_urlencode({\n-                    'allow_source': 'true',\n-                    'allow_audio_only': 'true',\n-                    'allow_spectre': 'true',\n-                    'player': 'twitchweb',\n-                    'playlist_include_framerate': 'true',\n-                    'nauth': access_token['value'],\n-                    'nauthsig': access_token['signature'],\n-                })),\n-            vod_id, 'mp4', entry_protocol='m3u8_native')\n-\n+        formats = self._extract_twitch_m3u8_formats(\n+            vod_id, access_token['value'], access_token['signature'])\n         formats.extend(self._extract_storyboard(vod_id, video.get('storyboard'), info.get('duration')))\n \n         self._prefer_source(formats)\n@@ -924,6 +926,7 @@ class TwitchStreamIE(TwitchBaseIE):\n                         )\n                         (?P<id>[^/#?]+)\n                     '''\n+    _M3U8_PATH = 'api/channel/hls'\n \n     _TESTS = [{\n         'url': 'http://www.twitch.tv/shroomztv',\n@@ -1026,23 +1029,10 @@ def _real_extract(self, url):\n \n         access_token = self._download_access_token(\n             channel_name, 'stream', 'channelName')\n-        token = access_token['value']\n \n         stream_id = stream.get('id') or channel_name\n-        query = {\n-            'allow_source': 'true',\n-            'allow_audio_only': 'true',\n-            'allow_spectre': 'true',\n-            'p': random.randint(1000000, 10000000),\n-            'player': 'twitchweb',\n-            'playlist_include_framerate': 'true',\n-            'segment_preference': '4',\n-            'sig': access_token['signature'].encode('utf-8'),\n-            'token': token.encode('utf-8'),\n-        }\n-        formats = self._extract_m3u8_formats(\n-            '%s/api/channel/hls/%s.m3u8' % (self._USHER_BASE, channel_name),\n-            stream_id, 'mp4', query=query)\n+        formats = self._extract_twitch_m3u8_formats(\n+            channel_name, access_token['value'], access_token['signature'])\n         self._prefer_source(formats)\n \n         view_count = stream.get('viewers')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/twitter.py",
            "diff": "diff --git a/yt_dlp/extractor/twitter.py b/yt_dlp/extractor/twitter.py\nindex 34b8625c..c3a6e406 100644\n--- a/yt_dlp/extractor/twitter.py\n+++ b/yt_dlp/extractor/twitter.py\n@@ -1,14 +1,16 @@\n-import functools\n import json\n+import random\n import re\n \n from .common import InfoExtractor\n from .periscope import PeriscopeBaseIE, PeriscopeIE\n+from ..compat import functools  # isort: split\n from ..compat import (\n     compat_parse_qs,\n     compat_urllib_parse_unquote,\n     compat_urllib_parse_urlparse,\n )\n+from ..networking.exceptions import HTTPError\n from ..utils import (\n     ExtractorError,\n     dict_get,\n@@ -147,10 +149,14 @@ def _search_dimensions_in_video_url(a_format, video_url):\n     def is_logged_in(self):\n         return bool(self._get_cookies(self._API_BASE).get('auth_token'))\n \n+    @functools.cached_property\n+    def _selected_api(self):\n+        return self._configuration_arg('api', ['graphql'], ie_key='Twitter')[0]\n+\n     def _fetch_guest_token(self, display_id):\n         guest_token = traverse_obj(self._download_json(\n             f'{self._API_BASE}guest/activate.json', display_id, 'Downloading guest token', data=b'',\n-            headers=self._set_base_headers(legacy=display_id and self._configuration_arg('legacy_api'))),\n+            headers=self._set_base_headers(legacy=display_id and self._selected_api == 'legacy')),\n             ('guest_token', {str}))\n         if not guest_token:\n             raise ExtractorError('Could not retrieve guest token')\n@@ -295,7 +301,7 @@ def input_dict(subtask_id, text):\n         self.report_login()\n \n     def _call_api(self, path, video_id, query={}, graphql=False):\n-        headers = self._set_base_headers(legacy=not graphql and self._configuration_arg('legacy_api'))\n+        headers = self._set_base_headers(legacy=not graphql and self._selected_api == 'legacy')\n         headers.update({\n             'x-twitter-auth-type': 'OAuth2Session',\n             'x-twitter-client-language': 'en',\n@@ -474,9 +480,9 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': [],\n             'age_limit': 18,\n+            '_old_archive_ids': ['twitter 643211948184596480'],\n         },\n     }, {\n         'url': 'https://twitter.com/giphz/status/657991469417025536/photo/1',\n@@ -510,6 +516,7 @@ class TwitterIE(TwitterBaseIE):\n             'like_count': int,\n             'tags': ['TV', 'StarWars', 'TheForceAwakens'],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 665052190608723968'],\n         },\n     }, {\n         'url': 'https://twitter.com/BTNBrentYarina/status/705235433198714880',\n@@ -553,9 +560,9 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': ['Damndaniel'],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 700207533655363584'],\n         },\n     }, {\n         'url': 'https://twitter.com/Filmdrunk/status/713801302971588609',\n@@ -594,9 +601,9 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': [],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 719944021058060289'],\n         },\n     }, {\n         'url': 'https://twitter.com/OPP_HSD/status/779210622571536384',\n@@ -611,6 +618,7 @@ class TwitterIE(TwitterBaseIE):\n             'thumbnail': r're:^https?://.*\\.jpg',\n         },\n         'add_ie': ['Periscope'],\n+        'skip': 'Broadcast not found',\n     }, {\n         # has mp4 formats via mobile API\n         'url': 'https://twitter.com/news_al3alm/status/852138619213144067',\n@@ -630,9 +638,9 @@ class TwitterIE(TwitterBaseIE):\n             'thumbnail': r're:^https?://.*\\.jpg',\n             'tags': [],\n             'repost_count': int,\n-            'view_count': int,\n             'like_count': int,\n             'comment_count': int,\n+            '_old_archive_ids': ['twitter 852138619213144067'],\n         },\n     }, {\n         'url': 'https://twitter.com/i/web/status/910031516746514432',\n@@ -652,9 +660,9 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': ['Maria'],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 910031516746514432'],\n         },\n         'params': {\n             'skip_download': True,  # requires ffmpeg\n@@ -678,9 +686,9 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': [],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 1001551623938805763'],\n         },\n         'params': {\n             'skip_download': True,  # requires ffmpeg\n@@ -707,6 +715,7 @@ class TwitterIE(TwitterBaseIE):\n             'tags': [],\n             'age_limit': 0,\n         },\n+        'skip': 'This Tweet is unavailable',\n     }, {\n         # not available in Periscope\n         'url': 'https://twitter.com/ViviEducation/status/1136534865145286656',\n@@ -721,6 +730,7 @@ class TwitterIE(TwitterBaseIE):\n             'view_count': int,\n         },\n         'add_ie': ['TwitterBroadcast'],\n+        'skip': 'Broadcast no longer exists',\n     }, {\n         # unified card\n         'url': 'https://twitter.com/BrooklynNets/status/1349794411333394432?s=20',\n@@ -742,6 +752,7 @@ class TwitterIE(TwitterBaseIE):\n             'like_count': int,\n             'tags': [],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 1349794411333394432'],\n         },\n         'params': {\n             'skip_download': True,\n@@ -764,18 +775,18 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': [],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 1577855540407197696'],\n         },\n         'params': {'skip_download': True},\n     }, {\n         'url': 'https://twitter.com/UltimaShadowX/status/1577719286659006464',\n         'info_dict': {\n             'id': '1577719286659006464',\n-            'title': 'Ultima\ud83d\udcdb | #\u0432\u029f\u043c - Test',\n+            'title': 'Ultima - Test',\n             'description': 'Test https://t.co/Y3KEZD7Dad',\n-            'uploader': 'Ultima\ud83d\udcdb | #\u0432\u029f\u043c',\n+            'uploader': 'Ultima',\n             'uploader_id': 'UltimaShadowX',\n             'uploader_url': 'https://twitter.com/UltimaShadowX',\n             'upload_date': '20221005',\n@@ -806,12 +817,12 @@ class TwitterIE(TwitterBaseIE):\n             'comment_count': int,\n             'repost_count': int,\n             'like_count': int,\n-            'view_count': int,\n             'tags': ['HurricaneIan'],\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 1575560063510810624'],\n         },\n     }, {\n-        # Adult content, fails if not logged in (GraphQL)\n+        # Adult content, fails if not logged in\n         'url': 'https://twitter.com/Rizdraws/status/1575199173472927762',\n         'info_dict': {\n             'id': '1575199163847000068',\n@@ -831,9 +842,10 @@ class TwitterIE(TwitterBaseIE):\n             'age_limit': 18,\n             'tags': []\n         },\n+        'params': {'skip_download': 'The media could not be played'},\n         'skip': 'Requires authentication',\n     }, {\n-        # Playlist result only with auth\n+        # Playlist result only with graphql API\n         'url': 'https://twitter.com/Srirachachau/status/1395079556562706435',\n         'playlist_mincount': 2,\n         'info_dict': {\n@@ -898,7 +910,7 @@ class TwitterIE(TwitterBaseIE):\n             'uploader_id': 'MoniqueCamarra',\n             'live_status': 'was_live',\n             'release_timestamp': 1658417414,\n-            'description': 'md5:4dc8e972f1d8b3c6580376fabb02a3ad',\n+            'description': 'md5:acce559345fd49f129c20dbcda3f1201',\n             'timestamp': 1658407771,\n             'release_date': '20220721',\n             'upload_date': '20220721',\n@@ -943,10 +955,10 @@ class TwitterIE(TwitterBaseIE):\n             'uploader_url': 'https://twitter.com/CTVJLaidlaw',\n             'display_id': '1600649710662213632',\n             'like_count': int,\n-            'view_count': int,\n             'description': 'md5:591c19ce66fadc2359725d5cd0d1052c',\n             'upload_date': '20221208',\n             'age_limit': 0,\n+            '_old_archive_ids': ['twitter 1600649710662213632'],\n         },\n         'params': {'noplaylist': True},\n     }, {\n@@ -971,7 +983,7 @@ class TwitterIE(TwitterBaseIE):\n             'like_count': int,\n             'repost_count': int,\n             'comment_count': int,\n-            'view_count': int,\n+            '_old_archive_ids': ['twitter 1621117700482416640'],\n         },\n     }, {\n         'url': 'https://twitter.com/hlo_again/status/1599108751385972737/video/2',\n@@ -987,13 +999,13 @@ class TwitterIE(TwitterBaseIE):\n             'repost_count': int,\n             'duration': 9.531,\n             'comment_count': int,\n-            'view_count': int,\n             'upload_date': '20221203',\n             'age_limit': 0,\n             'timestamp': 1670092210.0,\n             'tags': [],\n             'uploader': '\\u06ea',\n             'description': '\\U0001F48B https://t.co/bTj9Qz7vQP',\n+            '_old_archive_ids': ['twitter 1599108751385972737'],\n         },\n         'params': {'noplaylist': True},\n     }, {\n@@ -1004,22 +1016,22 @@ class TwitterIE(TwitterBaseIE):\n             'ext': 'mp4',\n             'uploader_url': 'https://twitter.com/MunTheShinobi',\n             'description': 'This is a genius ad by Apple. \\U0001f525\\U0001f525\\U0001f525\\U0001f525\\U0001f525 https://t.co/cNsA0MoOml',\n-            'view_count': int,\n             'thumbnail': 'https://pbs.twimg.com/ext_tw_video_thumb/1600009362759733248/pu/img/XVhFQivj75H_YxxV.jpg?name=orig',\n             'age_limit': 0,\n-            'uploader': 'M\u00fcn The Friend Of YWAP',\n+            'uploader': 'M\u00fcn',\n             'repost_count': int,\n             'upload_date': '20221206',\n-            'title': 'M\u00fcn The Friend Of YWAP - This is a genius ad by Apple. \\U0001f525\\U0001f525\\U0001f525\\U0001f525\\U0001f525',\n+            'title': 'M\u00fcn - This is a genius ad by Apple. \\U0001f525\\U0001f525\\U0001f525\\U0001f525\\U0001f525',\n             'comment_count': int,\n             'like_count': int,\n             'tags': [],\n             'uploader_id': 'MunTheShinobi',\n             'duration': 139.987,\n             'timestamp': 1670306984.0,\n+            '_old_archive_ids': ['twitter 1600009574919962625'],\n         },\n     }, {\n-        # url to retweet id w/ legacy api\n+        # retweeted_status (private)\n         'url': 'https://twitter.com/liberdalau/status/1623739803874349067',\n         'info_dict': {\n             'id': '1623274794488659969',\n@@ -1039,32 +1051,114 @@ class TwitterIE(TwitterBaseIE):\n             'like_count': int,\n             'repost_count': int,\n         },\n-        'params': {'extractor_args': {'twitter': {'legacy_api': ['']}}},\n         'skip': 'Protected tweet',\n     }, {\n-        # orig tweet w/ graphql\n-        'url': 'https://twitter.com/liberdalau/status/1623739803874349067',\n+        # retweeted_status\n+        'url': 'https://twitter.com/playstrumpcard/status/1695424220702888009',\n         'info_dict': {\n-            'id': '1623274794488659969',\n-            'display_id': '1623739803874349067',\n+            'id': '1694928337846538240',\n             'ext': 'mp4',\n-            'title': '@selfisekai@hackerspace.pl \ud83d\udc00 - RT @Johnnybull3ts: Me after going viral to over 30million people:    Whoopsie-daisy',\n-            'description': 'md5:9258bdbb54793bdc124fe1cd47e96c6a',\n-            'uploader': '@selfisekai@hackerspace.pl \ud83d\udc00',\n-            'uploader_id': 'liberdalau',\n-            'uploader_url': 'https://twitter.com/liberdalau',\n+            'display_id': '1695424220702888009',\n+            'title': 'md5:e8daa9527bc2b947121395494f786d9d',\n+            'description': 'md5:004f2d37fd58737724ec75bc7e679938',\n+            'uploader': 'Benny Johnson',\n+            'uploader_id': 'bennyjohnson',\n+            'uploader_url': 'https://twitter.com/bennyjohnson',\n             'age_limit': 0,\n             'tags': [],\n-            'duration': 8.033,\n-            'timestamp': 1675964711.0,\n-            'upload_date': '20230209',\n-            'thumbnail': r're:https://pbs\\.twimg\\.com/ext_tw_video_thumb/.+',\n+            'duration': 45.001,\n+            'timestamp': 1692962814.0,\n+            'upload_date': '20230825',\n+            'thumbnail': r're:https://pbs\\.twimg\\.com/amplify_video_thumb/.+',\n             'like_count': int,\n+            'repost_count': int,\n+            'comment_count': int,\n+            '_old_archive_ids': ['twitter 1695424220702888009'],\n+        },\n+    }, {\n+        # retweeted_status w/ legacy API\n+        'url': 'https://twitter.com/playstrumpcard/status/1695424220702888009',\n+        'info_dict': {\n+            'id': '1694928337846538240',\n+            'ext': 'mp4',\n+            'display_id': '1695424220702888009',\n+            'title': 'md5:e8daa9527bc2b947121395494f786d9d',\n+            'description': 'md5:004f2d37fd58737724ec75bc7e679938',\n+            'uploader': 'Benny Johnson',\n+            'uploader_id': 'bennyjohnson',\n+            'uploader_url': 'https://twitter.com/bennyjohnson',\n+            'age_limit': 0,\n+            'tags': [],\n+            'duration': 45.001,\n+            'timestamp': 1692962814.0,\n+            'upload_date': '20230825',\n+            'thumbnail': r're:https://pbs\\.twimg\\.com/amplify_video_thumb/.+',\n+            'like_count': int,\n+            'repost_count': int,\n+            '_old_archive_ids': ['twitter 1695424220702888009'],\n+        },\n+        'params': {'extractor_args': {'twitter': {'api': ['legacy']}}},\n+    }, {\n+        # Broadcast embedded in tweet\n+        'url': 'https://twitter.com/JessicaDobsonWX/status/1731121063248175384',\n+        'info_dict': {\n+            'id': '1rmxPMjLzAXKN',\n+            'ext': 'mp4',\n+            'title': 'WAVE Weather Now - Saturday 12/2/23 Update',\n+            'uploader': 'Jessica Dobson',\n+            'uploader_id': 'JessicaDobsonWX',\n+            'uploader_url': 'https://twitter.com/JessicaDobsonWX',\n+            'timestamp': 1701566398,\n+            'upload_date': '20231203',\n+            'live_status': 'was_live',\n+            'thumbnail': r're:https://[^/]+pscp\\.tv/.+\\.jpg',\n+            'concurrent_view_count': int,\n             'view_count': int,\n+        },\n+        'add_ie': ['TwitterBroadcast'],\n+    }, {\n+        # Animated gif and quote tweet video, with syndication API\n+        'url': 'https://twitter.com/BAKKOOONN/status/1696256659889565950',\n+        'playlist_mincount': 2,\n+        'info_dict': {\n+            'id': '1696256659889565950',\n+            'title': 'BAKOON - https://t.co/zom968d0a0',\n+            'description': 'https://t.co/zom968d0a0',\n+            'tags': [],\n+            'uploader': 'BAKOON',\n+            'uploader_id': 'BAKKOOONN',\n+            'uploader_url': 'https://twitter.com/BAKKOOONN',\n+            'age_limit': 18,\n+            'timestamp': 1693254077.0,\n+            'upload_date': '20230828',\n+            'like_count': int,\n+        },\n+        'params': {'extractor_args': {'twitter': {'api': ['syndication']}}},\n+        'expected_warnings': ['Not all metadata'],\n+    }, {\n+        # \"stale tweet\" with typename \"TweetWithVisibilityResults\"\n+        'url': 'https://twitter.com/RobertKennedyJr/status/1724884212803834154',\n+        'md5': '62b1e11cdc2cdd0e527f83adb081f536',\n+        'info_dict': {\n+            'id': '1724883339285544960',\n+            'ext': 'mp4',\n+            'title': 'md5:cc56716f9ed0b368de2ba54c478e493c',\n+            'description': 'md5:9dc14f5b0f1311fc7caf591ae253a164',\n+            'display_id': '1724884212803834154',\n+            'uploader': 'Robert F. Kennedy Jr',\n+            'uploader_id': 'RobertKennedyJr',\n+            'uploader_url': 'https://twitter.com/RobertKennedyJr',\n+            'upload_date': '20231115',\n+            'timestamp': 1700079417.0,\n+            'duration': 341.048,\n+            'thumbnail': r're:https://pbs\\.twimg\\.com/amplify_video_thumb/.+',\n+            'tags': ['Kennedy24'],\n             'repost_count': int,\n+            'like_count': int,\n             'comment_count': int,\n+            'age_limit': 0,\n+            '_old_archive_ids': ['twitter 1724884212803834154'],\n         },\n-        'skip': 'Protected tweet',\n     }, {\n         # onion route\n         'url': 'https://twitter3e4tixl4xyajtrzo62zg5vztmjuricljdp2c5kshju4avyoid.onion/TwitterBlue/status/1484226494708662273',\n@@ -1103,6 +1197,14 @@ class TwitterIE(TwitterBaseIE):\n         'only_matching': True,\n     }]\n \n+    _MEDIA_ID_RE = re.compile(r'_video/(\\d+)/')\n+\n+    @property\n+    def _GRAPHQL_ENDPOINT(self):\n+        if self.is_logged_in:\n+            return 'zZXycP0V6H7m-2r0mOnFcA/TweetDetail'\n+        return '2ICDjqPd81tulZcYrtpTuQ/TweetResultByRestId'\n+\n     def _graphql_to_legacy(self, data, twid):\n         result = traverse_obj(data, (\n             'threaded_conversation_with_injections_v2', 'instructions', 0, 'entries',\n@@ -1111,28 +1213,37 @@ def _graphql_to_legacy(self, data, twid):\n         ), default={}, get_all=False) if self.is_logged_in else traverse_obj(\n             data, ('tweetResult', 'result', {dict}), default={})\n \n-        if result.get('__typename') not in ('Tweet', 'TweetTombstone', 'TweetUnavailable', None):\n-            self.report_warning(f'Unknown typename: {result.get(\"__typename\")}', twid, only_once=True)\n+        typename = result.get('__typename')\n+        if typename not in ('Tweet', 'TweetWithVisibilityResults', 'TweetTombstone', 'TweetUnavailable', None):\n+            self.report_warning(f'Unknown typename: {typename}', twid, only_once=True)\n \n         if 'tombstone' in result:\n             cause = remove_end(traverse_obj(result, ('tombstone', 'text', 'text', {str})), '. Learn more')\n             raise ExtractorError(f'Twitter API says: {cause or \"Unknown error\"}', expected=True)\n-        elif result.get('__typename') == 'TweetUnavailable':\n+        elif typename == 'TweetUnavailable':\n             reason = result.get('reason')\n             if reason == 'NsfwLoggedOut':\n                 self.raise_login_required('NSFW tweet requires authentication')\n             elif reason == 'Protected':\n                 self.raise_login_required('You are not authorized to view this protected tweet')\n             raise ExtractorError(reason or 'Requested tweet is unavailable', expected=True)\n+        # Result for \"stale tweet\" needs additional transformation\n+        elif typename == 'TweetWithVisibilityResults':\n+            result = traverse_obj(result, ('tweet', {dict})) or {}\n \n         status = result.get('legacy', {})\n         status.update(traverse_obj(result, {\n             'user': ('core', 'user_results', 'result', 'legacy'),\n             'card': ('card', 'legacy'),\n             'quoted_status': ('quoted_status_result', 'result', 'legacy'),\n+            'retweeted_status': ('legacy', 'retweeted_status_result', 'result', 'legacy'),\n         }, expected_type=dict, default={}))\n \n-        # extra transformation is needed since result does not match legacy format\n+        # extra transformations needed since result does not match legacy format\n+        if status.get('retweeted_status'):\n+            status['retweeted_status']['user'] = traverse_obj(status, (\n+                'retweeted_status_result', 'result', 'core', 'user_results', 'result', 'legacy', {dict})) or {}\n+\n         binding_values = {\n             binding_value.get('key'): binding_value.get('value')\n             for binding_value in traverse_obj(status, ('card', 'binding_values', ..., {dict}))\n@@ -1207,34 +1318,53 @@ def _build_graphql_query(self, media_id):\n             }\n         }\n \n+    def _call_syndication_api(self, twid):\n+        self.report_warning(\n+            'Not all metadata or media is available via syndication endpoint', twid, only_once=True)\n+        status = self._download_json(\n+            'https://cdn.syndication.twimg.com/tweet-result', twid, 'Downloading syndication JSON',\n+            headers={'User-Agent': 'Googlebot'}, query={\n+                'id': twid,\n+                # TODO: token = ((Number(twid) / 1e15) * Math.PI).toString(36).replace(/(0+|\\.)/g, '')\n+                'token': ''.join(random.choices('123456789abcdefghijklmnopqrstuvwxyz', k=10)),\n+            })\n+        if not status:\n+            raise ExtractorError('Syndication endpoint returned empty JSON response')\n+        # Transform the result so its structure matches that of legacy/graphql\n+        media = []\n+        for detail in traverse_obj(status, ((None, 'quoted_tweet'), 'mediaDetails', ..., {dict})):\n+            detail['id_str'] = traverse_obj(detail, (\n+                'video_info', 'variants', ..., 'url', {self._MEDIA_ID_RE.search}, 1), get_all=False) or twid\n+            media.append(detail)\n+        status['extended_entities'] = {'media': media}\n+\n+        return status\n+\n     def _extract_status(self, twid):\n-        if self.is_logged_in:\n-            return self._graphql_to_legacy(\n-                self._call_graphql_api('zZXycP0V6H7m-2r0mOnFcA/TweetDetail', twid), twid)\n+        if self._selected_api not in ('graphql', 'legacy', 'syndication'):\n+            raise ExtractorError(f'{self._selected_api!r} is not a valid API selection', expected=True)\n \n         try:\n-            if not self._configuration_arg('legacy_api'):\n-                return self._graphql_to_legacy(\n-                    self._call_graphql_api('2ICDjqPd81tulZcYrtpTuQ/TweetResultByRestId', twid), twid)\n-            return traverse_obj(self._call_api(f'statuses/show/{twid}.json', twid, {\n-                'cards_platform': 'Web-12',\n-                'include_cards': 1,\n-                'include_reply_count': 1,\n-                'include_user_entities': 0,\n-                'tweet_mode': 'extended',\n-            }), 'retweeted_status', None)\n-\n+            if self.is_logged_in or self._selected_api == 'graphql':\n+                status = self._graphql_to_legacy(self._call_graphql_api(self._GRAPHQL_ENDPOINT, twid), twid)\n+            elif self._selected_api == 'legacy':\n+                status = self._call_api(f'statuses/show/{twid}.json', twid, {\n+                    'cards_platform': 'Web-12',\n+                    'include_cards': 1,\n+                    'include_reply_count': 1,\n+                    'include_user_entities': 0,\n+                    'tweet_mode': 'extended',\n+                })\n         except ExtractorError as e:\n-            if e.expected:\n+            if not isinstance(e.cause, HTTPError) or not e.cause.status == 429:\n                 raise\n-            self.report_warning(\n-                f'{e.orig_msg}. Falling back to syndication endpoint; some metadata may be missing', twid)\n+            self.report_warning('Rate-limit exceeded; falling back to syndication endpoint')\n+            status = self._call_syndication_api(twid)\n \n-        status = self._download_json(\n-            'https://cdn.syndication.twimg.com/tweet-result', twid, 'Downloading syndication JSON',\n-            headers={'User-Agent': 'Googlebot'}, query={'id': twid})\n-        status['extended_entities'] = {'media': status.get('mediaDetails')}\n-        return status\n+        if self._selected_api == 'syndication':\n+            status = self._call_syndication_api(twid)\n+\n+        return traverse_obj(status, 'retweeted_status', None, expected_type=dict) or {}\n \n     def _real_extract(self, url):\n         twid, selected_index = self._match_valid_url(url).group('id', 'index')\n@@ -1266,10 +1396,7 @@ def _real_extract(self, url):\n         }\n \n         def extract_from_video_info(media):\n-            media_id = traverse_obj(media, 'id_str', 'id', (\n-                'video_info', 'variants', ..., 'url',\n-                {functools.partial(re.search, r'_video/(\\d+)/')}, 1\n-            ), get_all=False, expected_type=str_or_none) or twid\n+            media_id = traverse_obj(media, 'id_str', 'id', expected_type=str_or_none)\n             self.write_debug(f'Extracting from video info: {media_id}')\n \n             formats = []\n@@ -1298,10 +1425,10 @@ def add_thumbnail(name, size):\n                 'formats': formats,\n                 'subtitles': subtitles,\n                 'thumbnails': thumbnails,\n-                'view_count': traverse_obj(media, ('mediaStats', 'viewCount', {int_or_none})),\n+                'view_count': traverse_obj(media, ('mediaStats', 'viewCount', {int_or_none})),  # No longer available\n                 'duration': float_or_none(traverse_obj(media, ('video_info', 'duration_millis')), 1000),\n-                # The codec of http formats are unknown\n-                '_format_sort_fields': ('res', 'br', 'size', 'proto'),\n+                # Prioritize m3u8 formats for compat, see https://github.com/yt-dlp/yt-dlp/issues/8117\n+                '_format_sort_fields': ('res', 'proto:m3u8', 'br', 'size'),  # http format codec is unknown\n             }\n \n         def extract_from_card_info(card):\n@@ -1484,7 +1611,7 @@ class TwitterBroadcastIE(TwitterBaseIE, PeriscopeBaseIE):\n     IE_NAME = 'twitter:broadcast'\n     _VALID_URL = TwitterBaseIE._BASE_REGEX + r'i/broadcasts/(?P<id>[0-9a-zA-Z]{13})'\n \n-    _TEST = {\n+    _TESTS = [{\n         # untitled Periscope video\n         'url': 'https://twitter.com/i/broadcasts/1yNGaQLWpejGj',\n         'info_dict': {\n@@ -1492,18 +1619,57 @@ class TwitterBroadcastIE(TwitterBaseIE, PeriscopeBaseIE):\n             'ext': 'mp4',\n             'title': 'Andrea May Sahouri - Periscope Broadcast',\n             'uploader': 'Andrea May Sahouri',\n-            'uploader_id': '1PXEdBZWpGwKe',\n+            'uploader_id': 'andreamsahouri',\n+            'uploader_url': 'https://twitter.com/andreamsahouri',\n+            'timestamp': 1590973638,\n+            'upload_date': '20200601',\n             'thumbnail': r're:^https?://[^?#]+\\.jpg\\?token=',\n             'view_count': int,\n         },\n-    }\n+    }, {\n+        'url': 'https://twitter.com/i/broadcasts/1ZkKzeyrPbaxv',\n+        'info_dict': {\n+            'id': '1ZkKzeyrPbaxv',\n+            'ext': 'mp4',\n+            'title': 'Starship | SN10 | High-Altitude Flight Test',\n+            'uploader': 'SpaceX',\n+            'uploader_id': 'SpaceX',\n+            'uploader_url': 'https://twitter.com/SpaceX',\n+            'timestamp': 1614812942,\n+            'upload_date': '20210303',\n+            'thumbnail': r're:^https?://[^?#]+\\.jpg\\?token=',\n+            'view_count': int,\n+        },\n+    }, {\n+        'url': 'https://twitter.com/i/broadcasts/1OyKAVQrgzwGb',\n+        'info_dict': {\n+            'id': '1OyKAVQrgzwGb',\n+            'ext': 'mp4',\n+            'title': 'Starship Flight Test',\n+            'uploader': 'SpaceX',\n+            'uploader_id': 'SpaceX',\n+            'uploader_url': 'https://twitter.com/SpaceX',\n+            'timestamp': 1681993964,\n+            'upload_date': '20230420',\n+            'thumbnail': r're:^https?://[^?#]+\\.jpg\\?token=',\n+            'view_count': int,\n+        },\n+    }]\n \n     def _real_extract(self, url):\n         broadcast_id = self._match_id(url)\n         broadcast = self._call_api(\n             'broadcasts/show.json', broadcast_id,\n             {'ids': broadcast_id})['broadcasts'][broadcast_id]\n+        if not broadcast:\n+            raise ExtractorError('Broadcast no longer exists', expected=True)\n         info = self._parse_broadcast_data(broadcast, broadcast_id)\n+        info['title'] = broadcast.get('status') or info.get('title')\n+        info['uploader_id'] = broadcast.get('twitter_username') or info.get('uploader_id')\n+        info['uploader_url'] = format_field(broadcast, 'twitter_username', 'https://twitter.com/%s', default=None)\n+        if info['live_status'] == 'is_upcoming':\n+            return info\n+\n         media_key = broadcast['media_key']\n         source = self._call_api(\n             f'live_video_stream/status/{media_key}', media_key)['source']\n@@ -1618,6 +1784,7 @@ def _real_extract(self, url):\n         is_live = live_status == 'is_live'\n \n         formats = []\n+        headers = {'Referer': 'https://twitter.com/'}\n         if live_status == 'is_upcoming':\n             self.raise_no_formats('Twitter Space not started yet', expected=True)\n         elif not is_live and not metadata.get('is_space_available_for_replay'):\n@@ -1628,7 +1795,7 @@ def _real_extract(self, url):\n                 ('source', ('noRedirectPlaybackUrl', 'location'), {url_or_none}), get_all=False)\n             formats = self._extract_m3u8_formats(  # XXX: Some Spaces need ffmpeg as downloader\n                 source, metadata['media_key'], 'm4a', entry_protocol='m3u8', live=is_live,\n-                headers={'Referer': 'https://twitter.com/'}, fatal=False) if source else []\n+                headers=headers, fatal=False) if source else []\n             for fmt in formats:\n                 fmt.update({'vcodec': 'none', 'acodec': 'aac'})\n                 if not is_live:\n@@ -1653,12 +1820,13 @@ def _real_extract(self, url):\n                 lambda: int_or_none(metadata['scheduled_start'], scale=1000)),\n             'timestamp': int_or_none(metadata.get('created_at'), scale=1000),\n             'formats': formats,\n+            'http_headers': headers,\n         }\n \n \n class TwitterShortenerIE(TwitterBaseIE):\n     IE_NAME = 'twitter:shortener'\n-    _VALID_URL = r'https?://t.co/(?P<id>[^?]+)|tco:(?P<eid>[^?]+)'\n+    _VALID_URL = r'https?://t\\.co/(?P<id>[^?#]+)|tco:(?P<eid>[^?#]+)'\n     _BASE_URL = 'https://t.co/'\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/unscripted.py",
            "diff": "diff --git a/yt_dlp/extractor/unscripted.py b/yt_dlp/extractor/unscripted.py\ndeleted file mode 100644\nindex 6643a71b..00000000\n--- a/yt_dlp/extractor/unscripted.py\n+++ /dev/null\n@@ -1,53 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import parse_duration, traverse_obj\n-\n-\n-class UnscriptedNewsVideoIE(InfoExtractor):\n-    _VALID_URL = r'https?://www\\.unscripted\\.news/videos/(?P<id>[\\w-]+)'\n-    _TESTS = [{\n-        'url': 'https://www.unscripted.news/videos/a-day-at-the-farmers-protest',\n-        'info_dict': {\n-            'id': '60c0a55cd1e99b1079918a57',\n-            'display_id': 'a-day-at-the-farmers-protest',\n-            'ext': 'mp4',\n-            'title': 'A Day at the Farmers\\' Protest',\n-            'description': 'md5:4b3df22747a03e8f14f746dd72190384',\n-            'thumbnail': 'https://s3.unscripted.news/anj2/60c0a55cd1e99b1079918a57/5f199a65-c803-4a5c-8fce-2077359c3b72.jpg',\n-            'duration': 2251.0,\n-            'series': 'Ground Reports',\n-        }\n-    }, {\n-        'url': 'https://www.unscripted.news/videos/you-get-the-politicians-you-deserve-ft-shashi-tharoor',\n-        'info_dict': {\n-            'id': '5fb3afbf18ac817d341a74d8',\n-            'display_id': 'you-get-the-politicians-you-deserve-ft-shashi-tharoor',\n-            'ext': 'mp4',\n-            'cast': ['Avalok Langer', 'Ashwin Mehta'],\n-            'thumbnail': 'https://s3.unscripted.news/anj2/5fb3afbf18ac817d341a74d8/82bd7942-4f20-4cd8-98ae-83f9e814f998.jpg',\n-            'description': 'md5:1e91b069238a705ca3a40f87e6f1182c',\n-            'duration': 1046.0,\n-            'series': 'Dumb Questions Only',\n-            'title': 'You Get The Politicians You Deserve! ft. Shashi Tharoor',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        webpage = self._download_webpage(url, display_id)\n-        nextjs_data = self._search_nextjs_data(webpage, display_id)['props']['pageProps']['dataLocal']\n-\n-        # TODO: get subtitle from srt key\n-        formats, subtitles = self._extract_m3u8_formats_and_subtitles(nextjs_data['alt_content'], display_id)\n-\n-        return {\n-            'id': nextjs_data['_id'],\n-            'display_id': display_id,\n-            'title': nextjs_data.get('title') or self._og_search_title(webpage),\n-            'description': nextjs_data.get('sh_heading') or self._og_search_description(webpage),\n-            'formats': formats,\n-            'subtitles': subtitles,\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-            'duration': parse_duration(nextjs_data.get('duration')),\n-            'series': traverse_obj(nextjs_data, ('show', 'topic')),\n-            'cast': traverse_obj(nextjs_data, ('cast_crew', ..., 'displayname')),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/unsupported.py",
            "diff": "diff --git a/yt_dlp/extractor/unsupported.py b/yt_dlp/extractor/unsupported.py\nindex bbcbf3ac..a3f9911e 100644\n--- a/yt_dlp/extractor/unsupported.py\n+++ b/yt_dlp/extractor/unsupported.py\n@@ -48,6 +48,7 @@ class KnownDRMIE(UnsupportedInfoExtractor):\n         r'joyn\\.de',\n         r'amazon\\.(?:\\w{2}\\.)?\\w+/gp/video',\n         r'music\\.amazon\\.(?:\\w{2}\\.)?\\w+',\n+        r'(?:watch|front)\\.njpwworld\\.com',\n     )\n \n     _TESTS = [{\n@@ -141,6 +142,13 @@ class KnownDRMIE(UnsupportedInfoExtractor):\n         # https://github.com/yt-dlp/yt-dlp/issues/5767\n         'url': 'https://www.hulu.com/movie/anthem-6b25fac9-da2b-45a3-8e09-e4156b0471cc',\n         'only_matching': True,\n+    }, {\n+        # https://github.com/yt-dlp/yt-dlp/pull/8570\n+        'url': 'https://watch.njpwworld.com/player/36447/series?assetType=series',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://front.njpwworld.com/p/s_series_00563_16_bs',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n@@ -164,11 +172,15 @@ class KnownPiracyIE(UnsupportedInfoExtractor):\n         r'viewsb\\.com',\n         r'filemoon\\.sx',\n         r'hentai\\.animestigma\\.com',\n+        r'thisav\\.com',\n     )\n \n     _TESTS = [{\n         'url': 'http://dood.to/e/5s1wmbdacezb',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://thisav.com/en/terms',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/utreon.py",
            "diff": "diff --git a/yt_dlp/extractor/utreon.py b/yt_dlp/extractor/utreon.py\nindex 90c10c05..8a916910 100644\n--- a/yt_dlp/extractor/utreon.py\n+++ b/yt_dlp/extractor/utreon.py\n@@ -10,7 +10,7 @@\n \n \n class UtreonIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?utreon.com/v/(?P<id>[a-zA-Z0-9_-]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?utreon\\.com/v/(?P<id>[\\w-]+)'\n     _TESTS = [{\n         'url': 'https://utreon.com/v/z_I7ikQbuDw',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/veehd.py",
            "diff": "diff --git a/yt_dlp/extractor/veehd.py b/yt_dlp/extractor/veehd.py\ndeleted file mode 100644\nindex 5ecd8872..00000000\n--- a/yt_dlp/extractor/veehd.py\n+++ /dev/null\n@@ -1,116 +0,0 @@\n-import re\n-import json\n-\n-from .common import InfoExtractor\n-from ..compat import (\n-    compat_urllib_parse_unquote,\n-    compat_urlparse,\n-)\n-from ..utils import (\n-    ExtractorError,\n-    clean_html,\n-    get_element_by_id,\n-)\n-\n-\n-class VeeHDIE(InfoExtractor):\n-    _VALID_URL = r'https?://veehd\\.com/video/(?P<id>\\d+)'\n-\n-    # Seems VeeHD videos have multiple copies on several servers, all of\n-    # whom have different MD5 checksums, so omit md5 field in all tests\n-    _TESTS = [{\n-        'url': 'http://veehd.com/video/4639434_Solar-Sinter',\n-        'info_dict': {\n-            'id': '4639434',\n-            'ext': 'mp4',\n-            'title': 'Solar Sinter',\n-            'uploader_id': 'VideoEyes',\n-            'description': 'md5:46a840e8692ddbaffb5f81d9885cb457',\n-        },\n-        'skip': 'Video deleted',\n-    }, {\n-        'url': 'http://veehd.com/video/4905758_Elysian-Fields-Channeling',\n-        'info_dict': {\n-            'id': '4905758',\n-            'ext': 'mp4',\n-            'title': 'Elysian Fields - Channeling',\n-            'description': 'md5:360e4e95fdab58aefbea0f2a19e5604b',\n-            'uploader_id': 'spotted',\n-        }\n-    }, {\n-        'url': 'http://veehd.com/video/2046729_2012-2009-DivX-Trailer',\n-        'info_dict': {\n-            'id': '2046729',\n-            'ext': 'avi',\n-            'title': '2012 (2009) DivX Trailer',\n-            'description': 'md5:75435ee95255e6a9838ac6f6f3a2396b',\n-            'uploader_id': 'Movie_Trailers',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        # VeeHD seems to send garbage on the first request.\n-        # See https://github.com/ytdl-org/youtube-dl/issues/2102\n-        self._download_webpage(url, video_id, 'Requesting webpage')\n-        webpage = self._download_webpage(url, video_id)\n-\n-        if 'This video has been removed<' in webpage:\n-            raise ExtractorError('Video %s has been removed' % video_id, expected=True)\n-\n-        player_path = self._search_regex(\n-            r'\\$\\(\"#playeriframe\"\\).attr\\({src : \"(.+?)\"',\n-            webpage, 'player path')\n-        player_url = compat_urlparse.urljoin(url, player_path)\n-\n-        self._download_webpage(player_url, video_id, 'Requesting player page')\n-        player_page = self._download_webpage(\n-            player_url, video_id, 'Downloading player page')\n-\n-        video_url = None\n-\n-        config_json = self._search_regex(\n-            r'value=\\'config=({.+?})\\'', player_page, 'config json', default=None)\n-\n-        if config_json:\n-            config = json.loads(config_json)\n-            video_url = compat_urllib_parse_unquote(config['clip']['url'])\n-\n-        if not video_url:\n-            video_url = self._html_search_regex(\n-                r'<embed[^>]+type=\"video/divx\"[^>]+src=\"([^\"]+)\"',\n-                player_page, 'video url', default=None)\n-\n-        if not video_url:\n-            iframe_src = self._search_regex(\n-                r'<iframe[^>]+src=\"/?([^\"]+)\"', player_page, 'iframe url')\n-            iframe_url = 'http://veehd.com/%s' % iframe_src\n-\n-            self._download_webpage(iframe_url, video_id, 'Requesting iframe page')\n-            iframe_page = self._download_webpage(\n-                iframe_url, video_id, 'Downloading iframe page')\n-\n-            video_url = self._search_regex(\n-                r\"file\\s*:\\s*'([^']+)'\", iframe_page, 'video url')\n-\n-        title = clean_html(get_element_by_id('videoName', webpage).rpartition('|')[0])\n-        uploader_id = self._html_search_regex(\n-            r'<a href=\"/profile/\\d+\">(.+?)</a>',\n-            webpage, 'uploader')\n-        thumbnail = self._search_regex(\n-            r'<img id=\"veehdpreview\" src=\"(.+?)\"',\n-            webpage, 'thumbnail')\n-        description = self._html_search_regex(\n-            r'<td class=\"infodropdown\".*?<div>(.*?)<ul',\n-            webpage, 'description', flags=re.DOTALL)\n-\n-        return {\n-            '_type': 'video',\n-            'id': video_id,\n-            'title': title,\n-            'url': video_url,\n-            'uploader_id': uploader_id,\n-            'thumbnail': thumbnail,\n-            'description': description,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vice.py",
            "diff": "diff --git a/yt_dlp/extractor/vice.py b/yt_dlp/extractor/vice.py\nindex 8a712685..1a2d667e 100644\n--- a/yt_dlp/extractor/vice.py\n+++ b/yt_dlp/extractor/vice.py\n@@ -302,12 +302,6 @@ def _url_res(video_url, ie_key):\n         if vice_url:\n             return _url_res(vice_url, ViceIE.ie_key())\n \n-        embed_code = self._search_regex(\n-            r'embedCode=([^&\\'\"]+)', body,\n-            'ooyala embed code', default=None)\n-        if embed_code:\n-            return _url_res('ooyala:%s' % embed_code, 'Ooyala')\n-\n         youtube_url = YoutubeIE._extract_url(body)\n         if youtube_url:\n             return _url_res(youtube_url, YoutubeIE.ie_key())\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vidbit.py",
            "diff": "diff --git a/yt_dlp/extractor/vidbit.py b/yt_dlp/extractor/vidbit.py\ndeleted file mode 100644\nindex 2813032d..00000000\n--- a/yt_dlp/extractor/vidbit.py\n+++ /dev/null\n@@ -1,82 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_urlparse\n-from ..utils import (\n-    int_or_none,\n-    js_to_json,\n-    remove_end,\n-    unified_strdate,\n-)\n-\n-\n-class VidbitIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?vidbit\\.co/(?:watch|embed)\\?.*?\\bv=(?P<id>[\\da-zA-Z]+)'\n-    _TESTS = [{\n-        'url': 'http://www.vidbit.co/watch?v=jkL2yDOEq2',\n-        'md5': '1a34b7f14defe3b8fafca9796892924d',\n-        'info_dict': {\n-            'id': 'jkL2yDOEq2',\n-            'ext': 'mp4',\n-            'title': 'Intro to VidBit',\n-            'description': 'md5:5e0d6142eec00b766cbf114bfd3d16b7',\n-            'thumbnail': r're:https?://.*\\.jpg$',\n-            'upload_date': '20160618',\n-            'view_count': int,\n-            'comment_count': int,\n-        }\n-    }, {\n-        'url': 'http://www.vidbit.co/embed?v=jkL2yDOEq2&auto=0&water=0',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            compat_urlparse.urljoin(url, '/watch?v=%s' % video_id), video_id)\n-\n-        video_url, title = [None] * 2\n-\n-        config = self._parse_json(self._search_regex(\n-            r'(?s)\\.setup\\(({.+?})\\);', webpage, 'setup', default='{}'),\n-            video_id, transform_source=js_to_json)\n-        if config:\n-            if config.get('file'):\n-                video_url = compat_urlparse.urljoin(url, config['file'])\n-            title = config.get('title')\n-\n-        if not video_url:\n-            video_url = compat_urlparse.urljoin(url, self._search_regex(\n-                r'file\\s*:\\s*([\"\\'])(?P<url>(?:(?!\\1).)+)\\1',\n-                webpage, 'video URL', group='url'))\n-\n-        if not title:\n-            title = remove_end(\n-                self._html_search_regex(\n-                    (r'<h1>(.+?)</h1>', r'<title>(.+?)</title>'),\n-                    webpage, 'title', default=None) or self._og_search_title(webpage),\n-                ' - VidBit')\n-\n-        description = self._html_search_meta(\n-            ('description', 'og:description', 'twitter:description'),\n-            webpage, 'description')\n-\n-        upload_date = unified_strdate(self._html_search_meta(\n-            'datePublished', webpage, 'upload date'))\n-\n-        view_count = int_or_none(self._search_regex(\n-            r'<strong>(\\d+)</strong> views',\n-            webpage, 'view count', fatal=False))\n-        comment_count = int_or_none(self._search_regex(\n-            r'id=[\"\\']cmt_num[\"\\'][^>]*>\\((\\d+)\\)',\n-            webpage, 'comment count', fatal=False))\n-\n-        return {\n-            'id': video_id,\n-            'url': video_url,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-            'upload_date': upload_date,\n-            'view_count': view_count,\n-            'comment_count': comment_count,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/videa.py",
            "diff": "diff --git a/yt_dlp/extractor/videa.py b/yt_dlp/extractor/videa.py\nindex 59ae933b..634d2ede 100644\n--- a/yt_dlp/extractor/videa.py\n+++ b/yt_dlp/extractor/videa.py\n@@ -38,6 +38,7 @@ class VideaIE(InfoExtractor):\n             'title': 'Az \u0151r\u00fclt k\u00edgy\u00e1sz 285 k\u00edgy\u00f3t enged szabadon',\n             'thumbnail': r're:^https?://.*',\n             'duration': 21,\n+            'age_limit': 0,\n         },\n     }, {\n         'url': 'http://videa.hu/videok/origo/jarmuvek/supercars-elozes-jAHDWfWSJH5XuFhH',\n@@ -48,6 +49,7 @@ class VideaIE(InfoExtractor):\n             'title': 'Supercars el\u0151z\u00e9s',\n             'thumbnail': r're:^https?://.*',\n             'duration': 64,\n+            'age_limit': 0,\n         },\n     }, {\n         'url': 'http://videa.hu/player?v=8YfIAjxwWGwT8HVQ',\n@@ -58,6 +60,7 @@ class VideaIE(InfoExtractor):\n             'title': 'Az \u0151r\u00fclt k\u00edgy\u00e1sz 285 k\u00edgy\u00f3t enged szabadon',\n             'thumbnail': r're:^https?://.*',\n             'duration': 21,\n+            'age_limit': 0,\n         },\n     }, {\n         'url': 'http://videa.hu/player/v/8YfIAjxwWGwT8HVQ?autoplay=1',\n@@ -124,7 +127,7 @@ def _real_extract(self, url):\n         query['_t'] = result[:16]\n \n         b64_info, handle = self._download_webpage_handle(\n-            'http://videa.hu/videaplayer_get_xml.php', video_id, query=query)\n+            'http://videa.hu/player/xml', video_id, query=query)\n         if b64_info.startswith('<?xml'):\n             info = self._parse_xml(b64_info, video_id)\n         else:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/videoken.py",
            "diff": "diff --git a/yt_dlp/extractor/videoken.py b/yt_dlp/extractor/videoken.py\nindex 560b41a6..eaf0cc8a 100644\n--- a/yt_dlp/extractor/videoken.py\n+++ b/yt_dlp/extractor/videoken.py\n@@ -11,6 +11,7 @@\n     ExtractorError,\n     InAdvancePagedList,\n     int_or_none,\n+    remove_start,\n     traverse_obj,\n     update_url_query,\n     url_or_none,\n@@ -39,11 +40,11 @@ def _create_slideslive_url(self, video_url, video_id, referer):\n         if not video_url and not video_id:\n             return\n         elif not video_url or 'embed/sign-in' in video_url:\n-            video_url = f'https://slideslive.com/embed/{video_id.lstrip(\"slideslive-\")}'\n+            video_url = f'https://slideslive.com/embed/{remove_start(video_id, \"slideslive-\")}'\n         if url_or_none(referer):\n             return update_url_query(video_url, {\n                 'embed_parent_url': referer,\n-                'embed_container_origin': f'https://{urllib.parse.urlparse(referer).netloc}',\n+                'embed_container_origin': f'https://{urllib.parse.urlparse(referer).hostname}',\n             })\n         return video_url\n \n@@ -57,12 +58,12 @@ def _extract_videos(self, videos, url):\n                 video_url = video_id\n                 ie_key = 'Youtube'\n             else:\n-                video_url = traverse_obj(video, 'embed_url', 'embeddableurl')\n-                if urllib.parse.urlparse(video_url).netloc == 'slideslive.com':\n+                video_url = traverse_obj(video, 'embed_url', 'embeddableurl', expected_type=url_or_none)\n+                if not video_url:\n+                    continue\n+                elif urllib.parse.urlparse(video_url).hostname == 'slideslive.com':\n                     ie_key = SlidesLiveIE\n                     video_url = self._create_slideslive_url(video_url, video_id, url)\n-            if not video_url:\n-                continue\n             yield self.url_result(video_url, ie_key, video_id)\n \n \n@@ -178,7 +179,7 @@ def _real_extract(self, url):\n             return self.url_result(\n                 self._create_slideslive_url(None, video_id, url), SlidesLiveIE, video_id)\n         elif re.match(r'^[\\w-]{11}$', video_id):\n-            self.url_result(video_id, 'Youtube', video_id)\n+            return self.url_result(video_id, 'Youtube', video_id)\n         else:\n             raise ExtractorError('Unable to extract without VideoKen API response')\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vidly.py",
            "diff": "diff --git a/yt_dlp/extractor/vidly.py b/yt_dlp/extractor/vidly.py\nnew file mode 100644\nindex 00000000..49a19604\n--- /dev/null\n+++ b/yt_dlp/extractor/vidly.py\n@@ -0,0 +1,83 @@\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    mimetype2ext,\n+    url_or_none,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class VidlyIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:vid\\.ly/|(?:s\\.)?vid\\.ly/embeded\\.html\\?(?:[^#]+&)?link=)(?P<id>\\w+)'\n+    _EMBED_REGEX = [r'<script[^>]+\\bsrc=[\\'\"](?P<url>(?:https?:)?//vid\\.ly/\\w+/embed[^\\'\"]+)',\n+                    r'<iframe[^>]+\\bsrc=[\\'\"](?P<url>(?:https?:)?//(?:s\\.)?vid\\.ly/embeded\\.html\\?(?:[^#\\'\"]+&)?link=\\w+[^\\'\"]+)']\n+    _TESTS = [{\n+        # JWPlayer 7, Embeds forbidden\n+        'url': 'https://vid.ly/2i3o9j/embed',\n+        'info_dict': {\n+            'id': '2i3o9j',\n+            'ext': 'mp4',\n+            'title': '2i3o9j',\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/',\n+        },\n+    }, {\n+        # JWPlayer 6\n+        'url': 'http://s.vid.ly/embeded.html?link=jw_test&new=1&autoplay=true&controls=true',\n+        'info_dict': {\n+            'id': 'jw_test',\n+            'ext': 'mp4',\n+            'title': '2x8m8t',\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/',\n+        },\n+    }, {\n+        # Vidlyplayer\n+        'url': 'https://vid.ly/7x0e6l',\n+        'info_dict': {\n+            'id': '7x0e6l',\n+            'ext': 'mp4',\n+            'title': '7x0e6l',\n+        },\n+    }]\n+    _WEBPAGE_TESTS = [{\n+        'url': 'https://www.petfinder.com/dog/gus-57378930/tn/ooltewah/furever-furkids-rescue-tn592/',\n+        'info_dict': {\n+            'id': 'w8p5b0',\n+            'ext': 'mp4',\n+            'title': 'w8p5b0',\n+            'thumbnail': r're:https://\\w+\\.cloudfront\\.net/',\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+\n+        embed_script = self._download_webpage(\n+            f'https://vid.ly/{video_id}/embed', video_id, headers={'Referer': 'https://vid.ly/'})\n+        player = self._search_json(r'initCallback\\(', embed_script, 'player', video_id)\n+\n+        player_type = player.get('player') or ''\n+        if player_type.startswith('jwplayer'):\n+            return self._parse_jwplayer_data(player['config'], video_id)\n+        elif not player_type.startswith('vidly'):\n+            raise ExtractorError(f'Unknown player type {player_type!r}')\n+\n+        formats = []\n+        ext = mimetype2ext(traverse_obj(player, ('config', 'type')))\n+        for source, fid in [('source', 'sd'), ('source_hd', 'hd')]:\n+            if traverse_obj(player, ('config', source, {url_or_none})):\n+                formats.append({\n+                    'url': player['config'][source],\n+                    'format_id': f'http-{fid}',\n+                    'ext': ext,\n+                })\n+        # Has higher quality formats\n+        formats.extend(self._extract_m3u8_formats(\n+            f'https://d3fenhwk93s16g.cloudfront.net/{video_id}/hls.m3u8', video_id,\n+            fatal=False, note='Requesting higher quality m3u8 formats',\n+            errnote='No higher quality m3u8 formats found') or [])\n+\n+        return {\n+            'id': video_id,\n+            'title': video_id,\n+            'formats': formats,\n+        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vimeo.py",
            "diff": "diff --git a/yt_dlp/extractor/vimeo.py b/yt_dlp/extractor/vimeo.py\nindex e72fa50f..e5e8144b 100644\n--- a/yt_dlp/extractor/vimeo.py\n+++ b/yt_dlp/extractor/vimeo.py\n@@ -37,14 +37,14 @@ class VimeoBaseInfoExtractor(InfoExtractor):\n \n     @staticmethod\n     def _smuggle_referrer(url, referrer_url):\n-        return smuggle_url(url, {'http_headers': {'Referer': referrer_url}})\n+        return smuggle_url(url, {'referer': referrer_url})\n \n     def _unsmuggle_headers(self, url):\n         \"\"\"@returns (url, smuggled_data, headers)\"\"\"\n         url, data = unsmuggle_url(url, {})\n         headers = self.get_param('http_headers').copy()\n-        if 'http_headers' in data:\n-            headers.update(data['http_headers'])\n+        if 'referer' in data:\n+            headers['Referer'] = data['referer']\n         return url, data, headers\n \n     def _perform_login(self, username, password):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vimple.py",
            "diff": "diff --git a/yt_dlp/extractor/vimple.py b/yt_dlp/extractor/vimple.py\ndeleted file mode 100644\nindex fdccf465..00000000\n--- a/yt_dlp/extractor/vimple.py\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import int_or_none\n-\n-\n-class SprutoBaseIE(InfoExtractor):\n-    def _extract_spruto(self, spruto, video_id):\n-        playlist = spruto['playlist'][0]\n-        title = playlist['title']\n-        video_id = playlist.get('videoId') or video_id\n-        thumbnail = playlist.get('posterUrl') or playlist.get('thumbnailUrl')\n-        duration = int_or_none(playlist.get('duration'))\n-\n-        formats = [{\n-            'url': f['url'],\n-        } for f in playlist['video']]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'formats': formats,\n-        }\n-\n-\n-class VimpleIE(SprutoBaseIE):\n-    IE_DESC = 'Vimple - one-click video hosting'\n-    _VALID_URL = r'https?://(?:player\\.vimple\\.(?:ru|co)/iframe|vimple\\.(?:ru|co))/(?P<id>[\\da-f-]{32,36})'\n-    _TESTS = [{\n-        'url': 'http://vimple.ru/c0f6b1687dcd4000a97ebe70068039cf',\n-        'md5': '2e750a330ed211d3fd41821c6ad9a279',\n-        'info_dict': {\n-            'id': 'c0f6b168-7dcd-4000-a97e-be70068039cf',\n-            'ext': 'mp4',\n-            'title': 'Sunset',\n-            'duration': 20,\n-            'thumbnail': r're:https?://.*?\\.jpg',\n-        },\n-    }, {\n-        'url': 'http://player.vimple.ru/iframe/52e1beec-1314-4a83-aeac-c61562eadbf9',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://vimple.co/04506a053f124483b8fb05ed73899f19',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            'http://player.vimple.ru/iframe/%s' % video_id, video_id)\n-\n-        spruto = self._parse_json(\n-            self._search_regex(\n-                r'sprutoData\\s*:\\s*({.+?}),\\r\\n', webpage, 'spruto data'),\n-            video_id)\n-\n-        return self._extract_spruto(spruto, video_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/viously.py",
            "diff": "diff --git a/yt_dlp/extractor/viously.py b/yt_dlp/extractor/viously.py\nnew file mode 100644\nindex 00000000..9ec7ed35\n--- /dev/null\n+++ b/yt_dlp/extractor/viously.py\n@@ -0,0 +1,60 @@\n+import base64\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    extract_attributes,\n+    int_or_none,\n+    parse_iso8601,\n+)\n+from ..utils.traversal import traverse_obj\n+\n+\n+class ViouslyIE(InfoExtractor):\n+    _VALID_URL = False\n+    _WEBPAGE_TESTS = [{\n+        'url': 'http://www.turbo.fr/videos-voiture/454443-turbo-du-07-09-2014-renault-twingo-3-bentley-continental-gt-speed-ces-guide-achat-dacia.html',\n+        'md5': '37a6c3381599381ff53a7e1e0575c0bc',\n+        'info_dict': {\n+            'id': 'F_xQzS2jwb3',\n+            'ext': 'mp4',\n+            'title': 'Turbo du 07/09/2014\\xa0: Renault Twingo 3, Bentley Continental GT Speed, CES, Guide Achat Dacia...',\n+            'description': 'Turbo du 07/09/2014\\xa0: Renault Twingo 3, Bentley Continental GT Speed, CES, Guide Achat Dacia...',\n+            'age_limit': 0,\n+            'upload_date': '20230328',\n+            'timestamp': 1680037507,\n+            'duration': 3716,\n+            'categories': ['motors'],\n+        }\n+    }]\n+\n+    def _extract_from_webpage(self, url, webpage):\n+        viously_players = re.findall(r'<div[^>]*class=\"(?:[^\"]*\\s)?v(?:iou)?sly-player(?:\\s[^\"]*)?\"[^>]*>', webpage)\n+        if not viously_players:\n+            return\n+\n+        def custom_decode(text):\n+            STANDARD_ALPHABET = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/='\n+            CUSTOM_ALPHABET = 'VIOUSLYABCDEFGHJKMNPQRTWXZviouslyabcdefghjkmnpqrtwxz9876543210+/='\n+            data = base64.b64decode(text.translate(str.maketrans(CUSTOM_ALPHABET, STANDARD_ALPHABET)))\n+            return data.decode('utf-8').strip('\\x00')\n+\n+        for video_id in traverse_obj(viously_players, (..., {extract_attributes}, 'id')):\n+            formats = self._extract_m3u8_formats(\n+                f'https://www.viously.com/video/hls/{video_id}/index.m3u8', video_id, fatal=False)\n+            if not formats:\n+                continue\n+            data = self._download_json(\n+                f'https://www.viously.com/export/json/{video_id}', video_id,\n+                transform_source=custom_decode, fatal=False)\n+            yield {\n+                'id': video_id,\n+                'formats': formats,\n+                **traverse_obj(data, ('video', {\n+                    'title': ('title', {str}),\n+                    'description': ('description', {str}),\n+                    'duration': ('duration', {int_or_none}),\n+                    'timestamp': ('iso_date', {parse_iso8601}),\n+                    'categories': ('category', 'name', {str}, {lambda x: [x] if x else None}),\n+                })),\n+            }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vk.py",
            "diff": "diff --git a/yt_dlp/extractor/vk.py b/yt_dlp/extractor/vk.py\nindex 91542281..c12e8736 100644\n--- a/yt_dlp/extractor/vk.py\n+++ b/yt_dlp/extractor/vk.py\n@@ -97,12 +97,12 @@ class VKIE(VKBaseIE):\n                         (?:\n                             (?:\n                                 (?:(?:m|new)\\.)?vk\\.com/video_|\n-                                (?:www\\.)?daxab.com/\n+                                (?:www\\.)?daxab\\.com/\n                             )\n                             ext\\.php\\?(?P<embed_query>.*?\\boid=(?P<oid>-?\\d+).*?\\bid=(?P<id>\\d+).*)|\n                             (?:\n                                 (?:(?:m|new)\\.)?vk\\.com/(?:.+?\\?.*?z=)?(?:video|clip)|\n-                                (?:www\\.)?daxab.com/embed/\n+                                (?:www\\.)?daxab\\.com/embed/\n                             )\n                             (?P<videoid>-?\\d+_\\d+)(?:.*\\blist=(?P<list_id>([\\da-f]+)|(ln-[\\da-zA-Z]+)))?\n                         )\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vocaroo.py",
            "diff": "diff --git a/yt_dlp/extractor/vocaroo.py b/yt_dlp/extractor/vocaroo.py\nindex d98fbfd2..e30c9597 100644\n--- a/yt_dlp/extractor/vocaroo.py\n+++ b/yt_dlp/extractor/vocaroo.py\n@@ -57,7 +57,7 @@ def _real_extract(self, url):\n             'title': '',\n             'url': url,\n             'ext': 'mp3',\n-            'timestamp': float_or_none(resp.getheader('x-bz-upload-timestamp'), scale=1000),\n+            'timestamp': float_or_none(resp.headers.get('x-bz-upload-timestamp'), scale=1000),\n             'vcodec': 'none',\n             'http_headers': http_headers,\n         }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vodlocker.py",
            "diff": "diff --git a/yt_dlp/extractor/vodlocker.py b/yt_dlp/extractor/vodlocker.py\ndeleted file mode 100644\nindex b215d6c9..00000000\n--- a/yt_dlp/extractor/vodlocker.py\n+++ /dev/null\n@@ -1,73 +0,0 @@\n-from .common import InfoExtractor\n-from ..networking import Request\n-from ..utils import NO_DEFAULT, ExtractorError, urlencode_postdata\n-\n-\n-class VodlockerIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?vodlocker\\.(?:com|city)/(?:embed-)?(?P<id>[0-9a-zA-Z]+)(?:\\..*?)?'\n-\n-    _TESTS = [{\n-        'url': 'http://vodlocker.com/e8wvyzz4sl42',\n-        'md5': 'ce0c2d18fa0735f1bd91b69b0e54aacf',\n-        'info_dict': {\n-            'id': 'e8wvyzz4sl42',\n-            'ext': 'mp4',\n-            'title': 'Germany vs Brazil',\n-            'thumbnail': r're:http://.*\\.jpg',\n-        },\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        if any(p in webpage for p in (\n-                '>THIS FILE WAS DELETED<',\n-                '>File Not Found<',\n-                'The file you were looking for could not be found, sorry for any inconvenience.<',\n-                '>The file was removed')):\n-            raise ExtractorError('Video %s does not exist' % video_id, expected=True)\n-\n-        fields = self._hidden_inputs(webpage)\n-\n-        if fields['op'] == 'download1':\n-            self._sleep(3, video_id)  # they do detect when requests happen too fast!\n-            post = urlencode_postdata(fields)\n-            req = Request(url, post)\n-            req.headers['Content-type'] = 'application/x-www-form-urlencoded'\n-            webpage = self._download_webpage(\n-                req, video_id, 'Downloading video page')\n-\n-        def extract_file_url(html, default=NO_DEFAULT):\n-            return self._search_regex(\n-                r'file:\\s*\"(http[^\\\"]+)\",', html, 'file url', default=default)\n-\n-        video_url = extract_file_url(webpage, default=None)\n-\n-        if not video_url:\n-            embed_url = self._search_regex(\n-                r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?://)?vodlocker\\.(?:com|city)/embed-.+?)\\1',\n-                webpage, 'embed url', group='url')\n-            embed_webpage = self._download_webpage(\n-                embed_url, video_id, 'Downloading embed webpage')\n-            video_url = extract_file_url(embed_webpage)\n-            thumbnail_webpage = embed_webpage\n-        else:\n-            thumbnail_webpage = webpage\n-\n-        title = self._search_regex(\n-            r'id=\"file_title\".*?>\\s*(.*?)\\s*<(?:br|span)', webpage, 'title')\n-        thumbnail = self._search_regex(\n-            r'image:\\s*\"(http[^\\\"]+)\",', thumbnail_webpage, 'thumbnail', fatal=False)\n-\n-        formats = [{\n-            'format_id': 'sd',\n-            'url': video_url,\n-        }]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/voicerepublic.py",
            "diff": "diff --git a/yt_dlp/extractor/voicerepublic.py b/yt_dlp/extractor/voicerepublic.py\ndeleted file mode 100644\nindex 47502afb..00000000\n--- a/yt_dlp/extractor/voicerepublic.py\n+++ /dev/null\n@@ -1,59 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    ExtractorError,\n-    determine_ext,\n-    int_or_none,\n-    urljoin,\n-)\n-\n-\n-class VoiceRepublicIE(InfoExtractor):\n-    _VALID_URL = r'https?://voicerepublic\\.com/(?:talks|embed)/(?P<id>[0-9a-z-]+)'\n-    _TESTS = [{\n-        'url': 'http://voicerepublic.com/talks/watching-the-watchers-building-a-sousveillance-state',\n-        'md5': 'b9174d651323f17783000876347116e3',\n-        'info_dict': {\n-            'id': '2296',\n-            'display_id': 'watching-the-watchers-building-a-sousveillance-state',\n-            'ext': 'm4a',\n-            'title': 'Watching the Watchers: Building a Sousveillance State',\n-            'description': 'Secret surveillance programs have metadata too. The people and companies that operate secret surveillance programs can be surveilled.',\n-            'duration': 1556,\n-            'view_count': int,\n-        }\n-    }, {\n-        'url': 'http://voicerepublic.com/embed/watching-the-watchers-building-a-sousveillance-state',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, display_id)\n-\n-        if '>Queued for processing, please stand by...<' in webpage:\n-            raise ExtractorError(\n-                'Audio is still queued for processing', expected=True)\n-\n-        talk = self._parse_json(self._search_regex(\n-            r'initialSnapshot\\s*=\\s*({.+?});',\n-            webpage, 'talk'), display_id)['talk']\n-        title = talk['title']\n-        formats = [{\n-            'url': urljoin(url, talk_url),\n-            'format_id': format_id,\n-            'ext': determine_ext(talk_url) or format_id,\n-            'vcodec': 'none',\n-        } for format_id, talk_url in talk['media_links'].items()]\n-\n-        return {\n-            'id': compat_str(talk.get('id') or display_id),\n-            'display_id': display_id,\n-            'title': title,\n-            'description': talk.get('teaser'),\n-            'thumbnail': talk.get('image_url'),\n-            'duration': int_or_none(talk.get('archived_duration')),\n-            'view_count': int_or_none(talk.get('play_count')),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/voot.py",
            "diff": "diff --git a/yt_dlp/extractor/voot.py b/yt_dlp/extractor/voot.py\nindex b19a2793..ef77bedd 100644\n--- a/yt_dlp/extractor/voot.py\n+++ b/yt_dlp/extractor/voot.py\n@@ -81,6 +81,7 @@ def _real_initialize(self):\n \n \n class VootIE(VootBaseIE):\n+    _WORKING = False\n     _VALID_URL = r'''(?x)\n                     (?:\n                         voot:|\n@@ -169,6 +170,7 @@ def _real_extract(self, url):\n \n \n class VootSeriesIE(VootBaseIE):\n+    _WORKING = False\n     _VALID_URL = r'https?://(?:www\\.)?voot\\.com/shows/[^/]+/(?P<id>\\d{3,})'\n     _TESTS = [{\n         'url': 'https://www.voot.com/shows/chakravartin-ashoka-samrat/100002',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/voxmedia.py",
            "diff": "diff --git a/yt_dlp/extractor/voxmedia.py b/yt_dlp/extractor/voxmedia.py\nindex f9362002..f3690875 100644\n--- a/yt_dlp/extractor/voxmedia.py\n+++ b/yt_dlp/extractor/voxmedia.py\n@@ -51,7 +51,7 @@ def _real_extract(self, url):\n             info['duration'] = int_or_none(asset.get('duration'))\n             return info\n \n-        for provider_video_type in ('ooyala', 'youtube', 'brightcove'):\n+        for provider_video_type in ('youtube', 'brightcove'):\n             provider_video_id = video_data.get('%s_id' % provider_video_type)\n             if not provider_video_id:\n                 continue\n@@ -177,7 +177,6 @@ def _real_extract(self, url):\n         def create_entry(provider_video_id, provider_video_type, title=None, description=None):\n             video_url = {\n                 'youtube': '%s',\n-                'ooyala': 'ooyala:%s',\n                 'volume': 'http://volume.vox-cdn.com/embed/%s',\n             }[provider_video_type] % provider_video_id\n             return {\n@@ -205,11 +204,6 @@ def create_entry(provider_video_id, provider_video_type, title=None, description\n                         provider_video_id, provider_video_type,\n                         video_data.get('title'), video_data.get('description')))\n \n-        provider_video_id = self._search_regex(\n-            r'data-ooyala-id=\"([^\"]+)\"', webpage, 'ooyala id', default=None)\n-        if provider_video_id:\n-            entries.append(create_entry(provider_video_id, 'ooyala'))\n-\n         volume_uuid = self._search_regex(\n             r'data-volume-uuid=\"([^\"]+)\"', webpage, 'volume uuid', default=None)\n         if volume_uuid:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vrak.py",
            "diff": "diff --git a/yt_dlp/extractor/vrak.py b/yt_dlp/extractor/vrak.py\ndeleted file mode 100644\nindex 198c0a29..00000000\n--- a/yt_dlp/extractor/vrak.py\n+++ /dev/null\n@@ -1,77 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from .brightcove import BrightcoveNewIE\n-from ..utils import (\n-    int_or_none,\n-    parse_age_limit,\n-    smuggle_url,\n-    unescapeHTML,\n-)\n-\n-\n-class VrakIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?vrak\\.tv/videos\\?.*?\\btarget=(?P<id>[\\d.]+)'\n-    _TEST = {\n-        'url': 'http://www.vrak.tv/videos?target=1.2306782&filtre=emission&id=1.1806721',\n-        'info_dict': {\n-            'id': '5345661243001',\n-            'ext': 'mp4',\n-            'title': 'Ob\u00e9sit\u00e9, film de hockey et Roseline Filion',\n-            'timestamp': 1488492126,\n-            'upload_date': '20170302',\n-            'uploader_id': '2890187628001',\n-            'creator': 'VRAK.TV',\n-            'age_limit': 8,\n-            'series': 'ALT (Actualit\u00e9 L\u00e9g\u00e8rement Tordue)',\n-            'episode': 'Ob\u00e9sit\u00e9, film de hockey et Roseline Filion',\n-            'tags': list,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }\n-    BRIGHTCOVE_URL_TEMPLATE = 'http://players.brightcove.net/2890187628001/default_default/index.html?videoId=%s'\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = self._html_search_regex(\n-            r'<h\\d\\b[^>]+\\bclass=[\"\\']videoTitle[\"\\'][^>]*>([^<]+)',\n-            webpage, 'title', default=None) or self._og_search_title(webpage)\n-\n-        content = self._parse_json(\n-            self._search_regex(\n-                r'data-player-options-content=([\"\\'])(?P<content>{.+?})\\1',\n-                webpage, 'content', default='{}', group='content'),\n-            video_id, transform_source=unescapeHTML)\n-\n-        ref_id = content.get('refId') or self._search_regex(\n-            r'refId&quot;:&quot;([^&]+)&quot;', webpage, 'ref id')\n-\n-        brightcove_id = self._search_regex(\n-            r'''(?x)\n-                java\\.lang\\.String\\s+value\\s*=\\s*[\"']brightcove\\.article\\.\\d+\\.%s\n-                [^>]*\n-                java\\.lang\\.String\\s+value\\s*=\\s*[\"'](\\d+)\n-            ''' % re.escape(ref_id), webpage, 'brightcove id')\n-\n-        return {\n-            '_type': 'url_transparent',\n-            'ie_key': BrightcoveNewIE.ie_key(),\n-            'url': smuggle_url(\n-                self.BRIGHTCOVE_URL_TEMPLATE % brightcove_id,\n-                {'geo_countries': ['CA']}),\n-            'id': brightcove_id,\n-            'description': content.get('description'),\n-            'creator': content.get('brand'),\n-            'age_limit': parse_age_limit(content.get('rating')),\n-            'series': content.get('showName') or content.get(\n-                'episodeName'),  # this is intentional\n-            'season_number': int_or_none(content.get('seasonNumber')),\n-            'episode': title,\n-            'episode_number': int_or_none(content.get('episodeNumber')),\n-            'tags': content.get('tags', []),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vrv.py",
            "diff": "diff --git a/yt_dlp/extractor/vrv.py b/yt_dlp/extractor/vrv.py\ndeleted file mode 100644\nindex 523c442e..00000000\n--- a/yt_dlp/extractor/vrv.py\n+++ /dev/null\n@@ -1,269 +0,0 @@\n-import base64\n-import hashlib\n-import hmac\n-import json\n-import random\n-import string\n-import time\n-import urllib.parse\n-\n-from .common import InfoExtractor\n-from ..compat import compat_urllib_parse_urlencode\n-from ..networking.exceptions import HTTPError\n-from ..utils import (\n-    ExtractorError,\n-    float_or_none,\n-    int_or_none,\n-    join_nonempty,\n-    traverse_obj,\n-)\n-\n-\n-class VRVBaseIE(InfoExtractor):\n-    _API_DOMAIN = None\n-    _API_PARAMS = {}\n-    _CMS_SIGNING = {}\n-    _TOKEN = None\n-    _TOKEN_SECRET = ''\n-\n-    def _call_api(self, path, video_id, note, data=None):\n-        # https://tools.ietf.org/html/rfc5849#section-3\n-        base_url = self._API_DOMAIN + '/core/' + path\n-        query = [\n-            ('oauth_consumer_key', self._API_PARAMS['oAuthKey']),\n-            ('oauth_nonce', ''.join(random.choices(string.ascii_letters, k=32))),\n-            ('oauth_signature_method', 'HMAC-SHA1'),\n-            ('oauth_timestamp', int(time.time())),\n-        ]\n-        if self._TOKEN:\n-            query.append(('oauth_token', self._TOKEN))\n-        encoded_query = compat_urllib_parse_urlencode(query)\n-        headers = self.geo_verification_headers()\n-        if data:\n-            data = json.dumps(data).encode()\n-            headers['Content-Type'] = 'application/json'\n-        base_string = '&'.join([\n-            'POST' if data else 'GET',\n-            urllib.parse.quote(base_url, ''),\n-            urllib.parse.quote(encoded_query, '')])\n-        oauth_signature = base64.b64encode(hmac.new(\n-            (self._API_PARAMS['oAuthSecret'] + '&' + self._TOKEN_SECRET).encode('ascii'),\n-            base_string.encode(), hashlib.sha1).digest()).decode()\n-        encoded_query += '&oauth_signature=' + urllib.parse.quote(oauth_signature, '')\n-        try:\n-            return self._download_json(\n-                '?'.join([base_url, encoded_query]), video_id,\n-                note='Downloading %s JSON metadata' % note, headers=headers, data=data)\n-        except ExtractorError as e:\n-            if isinstance(e.cause, HTTPError) and e.cause.status == 401:\n-                raise ExtractorError(json.loads(e.cause.response.read().decode())['message'], expected=True)\n-            raise\n-\n-    def _call_cms(self, path, video_id, note):\n-        if not self._CMS_SIGNING:\n-            index = self._call_api('index', video_id, 'CMS Signing')\n-            self._CMS_SIGNING = index.get('cms_signing') or {}\n-            if not self._CMS_SIGNING:\n-                for signing_policy in index.get('signing_policies', []):\n-                    signing_path = signing_policy.get('path')\n-                    if signing_path and signing_path.startswith('/cms/'):\n-                        name, value = signing_policy.get('name'), signing_policy.get('value')\n-                        if name and value:\n-                            self._CMS_SIGNING[name] = value\n-        return self._download_json(\n-            self._API_DOMAIN + path, video_id, query=self._CMS_SIGNING,\n-            note='Downloading %s JSON metadata' % note, headers=self.geo_verification_headers())\n-\n-    def _get_cms_resource(self, resource_key, video_id):\n-        return self._call_api(\n-            'cms_resource', video_id, 'resource path', data={\n-                'resource_key': resource_key,\n-            })['__links__']['cms_resource']['href']\n-\n-    def _extract_vrv_formats(self, url, video_id, stream_format, audio_lang, hardsub_lang):\n-        if not url or stream_format not in ('hls', 'dash', 'adaptive_hls'):\n-            return []\n-        format_id = join_nonempty(\n-            stream_format,\n-            audio_lang and 'audio-%s' % audio_lang,\n-            hardsub_lang and 'hardsub-%s' % hardsub_lang)\n-        if 'hls' in stream_format:\n-            adaptive_formats = self._extract_m3u8_formats(\n-                url, video_id, 'mp4', m3u8_id=format_id,\n-                note='Downloading %s information' % format_id,\n-                fatal=False)\n-        elif stream_format == 'dash':\n-            adaptive_formats = self._extract_mpd_formats(\n-                url, video_id, mpd_id=format_id,\n-                note='Downloading %s information' % format_id,\n-                fatal=False)\n-        if audio_lang:\n-            for f in adaptive_formats:\n-                if f.get('acodec') != 'none':\n-                    f['language'] = audio_lang\n-        return adaptive_formats\n-\n-    def _set_api_params(self):\n-        webpage = self._download_webpage(\n-            'https://vrv.co/', None, headers=self.geo_verification_headers())\n-        self._API_PARAMS = self._parse_json(self._search_regex(\n-            [\n-                r'window\\.__APP_CONFIG__\\s*=\\s*({.+?})(?:</script>|;)',\n-                r'window\\.__APP_CONFIG__\\s*=\\s*({.+})'\n-            ], webpage, 'app config'), None)['cxApiParams']\n-        self._API_DOMAIN = self._API_PARAMS.get('apiDomain', 'https://api.vrv.co')\n-\n-\n-class VRVIE(VRVBaseIE):\n-    IE_NAME = 'vrv'\n-    _VALID_URL = r'https?://(?:www\\.)?vrv\\.co/watch/(?P<id>[A-Z0-9]+)'\n-    _TESTS = [{\n-        'url': 'https://vrv.co/watch/GR9PNZ396/Hidden-America-with-Jonah-Ray:BOSTON-WHERE-THE-PAST-IS-THE-PRESENT',\n-        'info_dict': {\n-            'id': 'GR9PNZ396',\n-            'ext': 'mp4',\n-            'title': 'BOSTON: WHERE THE PAST IS THE PRESENT',\n-            'description': 'md5:4ec8844ac262ca2df9e67c0983c6b83f',\n-            'uploader_id': 'seeso',\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n-    }, {\n-        # movie listing\n-        'url': 'https://vrv.co/watch/G6NQXZ1J6/Lily-CAT',\n-        'info_dict': {\n-            'id': 'G6NQXZ1J6',\n-            'title': 'Lily C.A.T',\n-            'description': 'md5:988b031e7809a6aeb60968be4af7db07',\n-        },\n-        'playlist_count': 2,\n-    }]\n-    _NETRC_MACHINE = 'vrv'\n-\n-    def _perform_login(self, username, password):\n-        token_credentials = self._call_api(\n-            'authenticate/by:credentials', None, 'Token Credentials', data={\n-                'email': username,\n-                'password': password,\n-            })\n-        self._TOKEN = token_credentials['oauth_token']\n-        self._TOKEN_SECRET = token_credentials['oauth_token_secret']\n-\n-    def _initialize_pre_login(self):\n-        return self._set_api_params()\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        object_data = self._call_cms(self._get_cms_resource(\n-            'cms:/objects/' + video_id, video_id), video_id, 'object')['items'][0]\n-        resource_path = object_data['__links__']['resource']['href']\n-        video_data = self._call_cms(resource_path, video_id, 'video')\n-        title = video_data['title']\n-        description = video_data.get('description')\n-\n-        if video_data.get('__class__') == 'movie_listing':\n-            items = self._call_cms(\n-                video_data['__links__']['movie_listing/movies']['href'],\n-                video_id, 'movie listing').get('items') or []\n-            if len(items) != 1:\n-                entries = []\n-                for item in items:\n-                    item_id = item.get('id')\n-                    if not item_id:\n-                        continue\n-                    entries.append(self.url_result(\n-                        'https://vrv.co/watch/' + item_id,\n-                        self.ie_key(), item_id, item.get('title')))\n-                return self.playlist_result(entries, video_id, title, description)\n-            video_data = items[0]\n-\n-        streams_path = video_data['__links__'].get('streams', {}).get('href')\n-        if not streams_path:\n-            self.raise_login_required()\n-        streams_json = self._call_cms(streams_path, video_id, 'streams')\n-\n-        audio_locale = streams_json.get('audio_locale')\n-        formats = []\n-        for stream_type, streams in streams_json.get('streams', {}).items():\n-            if stream_type in ('adaptive_hls', 'adaptive_dash'):\n-                for stream in streams.values():\n-                    formats.extend(self._extract_vrv_formats(\n-                        stream.get('url'), video_id, stream_type.split('_')[1],\n-                        audio_locale, stream.get('hardsub_locale')))\n-\n-        subtitles = {}\n-        for k in ('captions', 'subtitles'):\n-            for subtitle in streams_json.get(k, {}).values():\n-                subtitle_url = subtitle.get('url')\n-                if not subtitle_url:\n-                    continue\n-                subtitles.setdefault(subtitle.get('locale', 'en-US'), []).append({\n-                    'url': subtitle_url,\n-                    'ext': subtitle.get('format', 'ass'),\n-                })\n-\n-        thumbnails = []\n-        for thumbnail in traverse_obj(video_data, ('images', 'thumbnail', ..., ...)) or []:\n-            thumbnail_url = thumbnail.get('source')\n-            if not thumbnail_url:\n-                continue\n-            thumbnails.append({\n-                'url': thumbnail_url,\n-                'width': int_or_none(thumbnail.get('width')),\n-                'height': int_or_none(thumbnail.get('height')),\n-            })\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'subtitles': subtitles,\n-            'thumbnails': thumbnails,\n-            'description': description,\n-            'duration': float_or_none(video_data.get('duration_ms'), 1000),\n-            'uploader_id': video_data.get('channel_id'),\n-            'series': video_data.get('series_title'),\n-            'season': video_data.get('season_title'),\n-            'season_number': int_or_none(video_data.get('season_number')),\n-            'season_id': video_data.get('season_id'),\n-            'episode': title,\n-            'episode_number': int_or_none(video_data.get('episode_number')),\n-            'episode_id': video_data.get('production_episode_id'),\n-        }\n-\n-\n-class VRVSeriesIE(VRVBaseIE):\n-    IE_NAME = 'vrv:series'\n-    _VALID_URL = r'https?://(?:www\\.)?vrv\\.co/series/(?P<id>[A-Z0-9]+)'\n-    _TEST = {\n-        'url': 'https://vrv.co/series/G68VXG3G6/The-Perfect-Insider',\n-        'info_dict': {\n-            'id': 'G68VXG3G6',\n-        },\n-        'playlist_mincount': 11,\n-    }\n-\n-    def _initialize_pre_login(self):\n-        return self._set_api_params()\n-\n-    def _real_extract(self, url):\n-        series_id = self._match_id(url)\n-\n-        seasons_path = self._get_cms_resource(\n-            'cms:/seasons?series_id=' + series_id, series_id)\n-        seasons_data = self._call_cms(seasons_path, series_id, 'seasons')\n-\n-        entries = []\n-        for season in seasons_data.get('items', []):\n-            episodes_path = season['__links__']['season/episodes']['href']\n-            episodes = self._call_cms(episodes_path, series_id, 'episodes')\n-            for episode in episodes.get('items', []):\n-                episode_id = episode['id']\n-                entries.append(self.url_result(\n-                    'https://vrv.co/watch/' + episode_id,\n-                    'VRV', episode_id, episode.get('title')))\n-\n-        return self.playlist_result(entries, series_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vshare.py",
            "diff": "diff --git a/yt_dlp/extractor/vshare.py b/yt_dlp/extractor/vshare.py\ndeleted file mode 100644\nindex 443ed43c..00000000\n--- a/yt_dlp/extractor/vshare.py\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import ExtractorError, decode_packed_codes\n-\n-\n-class VShareIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?vshare\\.io/[dv]/(?P<id>[^/?#&]+)'\n-    _EMBED_REGEX = [r'<iframe[^>]+?src=[\"\\'](?P<url>(?:https?:)?//(?:www\\.)?vshare\\.io/v/[^/?#&]+)']\n-    _TESTS = [{\n-        'url': 'https://vshare.io/d/0f64ce6',\n-        'md5': '17b39f55b5497ae8b59f5fbce8e35886',\n-        'info_dict': {\n-            'id': '0f64ce6',\n-            'title': 'vl14062007715967',\n-            'ext': 'mp4',\n-        }\n-    }, {\n-        'url': 'https://vshare.io/v/0f64ce6/width-650/height-430/1',\n-        'only_matching': True,\n-    }]\n-\n-    def _extract_packed(self, webpage):\n-        packed = self._search_regex(\n-            r'(eval\\(function.+)', webpage, 'packed code')\n-        unpacked = decode_packed_codes(packed)\n-        digits = self._search_regex(r'\\[([\\d,]+)\\]', unpacked, 'digits')\n-        digits = [int(digit) for digit in digits.split(',')]\n-        key_digit = self._search_regex(\n-            r'fromCharCode\\(.+?(\\d+)\\)}', unpacked, 'key digit')\n-        chars = [chr(d - int(key_digit)) for d in digits]\n-        return ''.join(chars)\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(\n-            'https://vshare.io/v/%s/width-650/height-430/1' % video_id,\n-            video_id, headers={'Referer': url})\n-\n-        title = self._html_extract_title(webpage)\n-        title = title.split(' - ')[0]\n-\n-        error = self._html_search_regex(\n-            r'(?s)<div[^>]+\\bclass=[\"\\']xxx-error[^>]+>(.+?)</div', webpage,\n-            'error', default=None)\n-        if error:\n-            raise ExtractorError(error, expected=True)\n-\n-        info = self._parse_html5_media_entries(\n-            url, '<video>%s</video>' % self._extract_packed(webpage),\n-            video_id)[0]\n-\n-        info.update({\n-            'id': video_id,\n-            'title': title,\n-        })\n-\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vupload.py",
            "diff": "diff --git a/yt_dlp/extractor/vupload.py b/yt_dlp/extractor/vupload.py\ndeleted file mode 100644\nindex 23ea70c7..00000000\n--- a/yt_dlp/extractor/vupload.py\n+++ /dev/null\n@@ -1,52 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    parse_duration,\n-    parse_filesize,\n-    extract_attributes,\n-    int_or_none,\n-    js_to_json\n-)\n-\n-\n-class VuploadIE(InfoExtractor):\n-    _VALID_URL = r'https://vupload\\.com/v/(?P<id>[a-z0-9]+)'\n-    _TESTS = [{\n-        'url': 'https://vupload.com/v/u28d0pl2tphy',\n-        'md5': '9b42a4a193cca64d80248e58527d83c8',\n-        'info_dict': {\n-            'id': 'u28d0pl2tphy',\n-            'ext': 'mp4',\n-            'description': 'md5:e9e6c0045c78cbf0d5bb19a55ce199fb',\n-            'title': 'md5:e9e6c0045c78cbf0d5bb19a55ce199fb',\n-        }\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = self._html_extract_title(webpage)\n-        video_json = self._parse_json(self._html_search_regex(r'sources:\\s*(.+?]),', webpage, 'video'), video_id, transform_source=js_to_json)\n-        formats = []\n-        for source in video_json:\n-            if source['src'].endswith('.m3u8'):\n-                formats.extend(self._extract_m3u8_formats(source['src'], video_id, m3u8_id='hls'))\n-        duration = parse_duration(self._html_search_regex(\n-            r'<i\\s*class=[\"\\']fad\\s*fa-clock[\"\\']></i>\\s*([\\d:]+)\\s*</div>', webpage, 'duration', fatal=False))\n-        filesize_approx = parse_filesize(self._html_search_regex(\n-            r'<i\\s*class=[\"\\']fad\\s*fa-save[\"\\']></i>\\s*([^<]+)\\s*</div>', webpage, 'filesize', fatal=False))\n-        extra_video_info = extract_attributes(self._html_search_regex(\n-            r'(<video[^>]+>)', webpage, 'video_info', fatal=False))\n-        description = self._html_search_meta('description', webpage)\n-\n-        return {\n-            'id': video_id,\n-            'formats': formats,\n-            'duration': duration,\n-            'filesize_approx': filesize_approx,\n-            'width': int_or_none(extra_video_info.get('width')),\n-            'height': int_or_none(extra_video_info.get('height')),\n-            'format_id': extra_video_info.get('height', '') + 'p',\n-            'title': title,\n-            'description': description,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vvvvid.py",
            "diff": "diff --git a/yt_dlp/extractor/vvvvid.py b/yt_dlp/extractor/vvvvid.py\nindex ed725a55..b42ba853 100644\n--- a/yt_dlp/extractor/vvvvid.py\n+++ b/yt_dlp/extractor/vvvvid.py\n@@ -1,3 +1,4 @@\n+import functools\n import re\n \n from .common import InfoExtractor\n@@ -14,21 +15,21 @@ class VVVVIDIE(InfoExtractor):\n     _VALID_URL = r'%s(?P<show_id>\\d+)/[^/]+/(?P<season_id>\\d+)/(?P<id>[0-9]+)' % _VALID_URL_BASE\n     _TESTS = [{\n         # video_type == 'video/vvvvid'\n-        'url': 'https://www.vvvvid.it/#!show/434/perche-dovrei-guardarlo-di-dario-moccia/437/489048/ping-pong',\n-        'md5': 'b8d3cecc2e981adc3835adf07f6df91b',\n+        'url': 'https://www.vvvvid.it/show/498/the-power-of-computing/518/505692/playstation-vr-cambiera-il-nostro-modo-di-giocare',\n         'info_dict': {\n-            'id': '489048',\n+            'id': '505692',\n             'ext': 'mp4',\n-            'title': 'Ping Pong',\n-            'duration': 239,\n-            'series': '\"Perch\u00e9 dovrei guardarlo?\" di Dario Moccia',\n-            'season_id': '437',\n-            'episode': 'Ping Pong',\n-            'episode_number': 1,\n-            'episode_id': '3334',\n+            'title': 'Playstation VR cambier\u00e0 il nostro modo di giocare',\n+            'duration': 93,\n+            'series': 'The Power of Computing',\n+            'season_id': '518',\n+            'episode': 'Playstation VR cambier\u00e0 il nostro modo di giocare',\n+            'episode_number': None,\n+            'episode_id': '4747',\n             'view_count': int,\n             'like_count': int,\n             'repost_count': int,\n+            'thumbnail': 'https://static.vvvvid.it/img/zoomin/28CA2409-E663-34F0-2B02E72356556EA3_500k.jpg',\n         },\n         'params': {\n             'skip_download': True,\n@@ -36,7 +37,6 @@ class VVVVIDIE(InfoExtractor):\n     }, {\n         # video_type == 'video/rcs'\n         'url': 'https://www.vvvvid.it/#!show/376/death-note-live-action/377/482493/episodio-01',\n-        'md5': '33e0edfba720ad73a8782157fdebc648',\n         'info_dict': {\n             'id': '482493',\n             'ext': 'mp4',\n@@ -45,6 +45,7 @@ class VVVVIDIE(InfoExtractor):\n         'params': {\n             'skip_download': True,\n         },\n+        'skip': 'Every video/rcs is not working even in real website',\n     }, {\n         # video_type == 'video/youtube'\n         'url': 'https://www.vvvvid.it/show/404/one-punch-man/406/486683/trailer',\n@@ -55,19 +56,54 @@ class VVVVIDIE(InfoExtractor):\n             'title': 'Trailer',\n             'upload_date': '20150906',\n             'description': 'md5:a5e802558d35247fee285875328c0b80',\n-            'uploader_id': 'BandaiVisual',\n-            'uploader': 'BANDAI NAMCO Arts Channel',\n+            'uploader_id': '@EMOTIONLabelChannel',\n+            'uploader': 'EMOTION Label Channel',\n+            'episode_number': None,\n+            'episode_id': '3115',\n+            'view_count': int,\n+            'like_count': int,\n+            'repost_count': int,\n+            'availability': str,\n+            'categories': list,\n+            'age_limit': 0,\n+            'channel': 'EMOTION Label Channel',\n+            'channel_follower_count': int,\n+            'channel_id': 'UCQ5URCSs1f5Cz9rh-cDGxNQ',\n+            'channel_url': 'https://www.youtube.com/channel/UCQ5URCSs1f5Cz9rh-cDGxNQ',\n+            'comment_count': int,\n+            'duration': 133,\n+            'episode': 'Trailer',\n+            'heatmap': list,\n+            'live_status': 'not_live',\n+            'playable_in_embed': True,\n+            'season_id': '406',\n+            'series': 'One-Punch Man',\n+            'tags': list,\n+            'uploader_url': 'https://www.youtube.com/@EMOTIONLabelChannel',\n+            'thumbnail': 'https://i.ytimg.com/vi/RzmFKUDOUgw/maxresdefault.jpg',\n         },\n         'params': {\n             'skip_download': True,\n         },\n     }, {\n         # video_type == 'video/dash'\n-        'url': 'https://www.vvvvid.it/show/683/made-in-abyss/1542/693786/nanachi',\n+        'url': 'https://www.vvvvid.it/show/844/le-bizzarre-avventure-di-jojo-vento-aureo/938/527551/golden-wind',\n         'info_dict': {\n-            'id': '693786',\n+            'id': '527551',\n             'ext': 'mp4',\n-            'title': 'Nanachi',\n+            'title': 'Golden Wind',\n+            'duration': 1430,\n+            'series': 'Le bizzarre avventure di Jojo - Vento Aureo',\n+            'season_id': '938',\n+            'episode': 'Golden Wind',\n+            'episode_number': 1,\n+            'episode_id': '9089',\n+            'view_count': int,\n+            'like_count': int,\n+            'repost_count': int,\n+            'thumbnail': 'https://static.vvvvid.it/img/thumbs/Dynit/Jojo/Jojo_S05Ep01-t.jpg',\n+            'season': 'Season 5',\n+            'season_number': 5,\n         },\n         'params': {\n             'skip_download': True,\n@@ -79,10 +115,17 @@ class VVVVIDIE(InfoExtractor):\n     }]\n     _conn_id = None\n \n+    @functools.cached_property\n+    def _headers(self):\n+        return {\n+            **self.geo_verification_headers(),\n+            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.50 Safari/537.37',\n+        }\n+\n     def _real_initialize(self):\n         self._conn_id = self._download_json(\n             'https://www.vvvvid.it/user/login',\n-            None, headers=self.geo_verification_headers())['data']['conn_id']\n+            None, headers=self._headers)['data']['conn_id']\n \n     def _download_info(self, show_id, path, video_id, fatal=True, query=None):\n         q = {\n@@ -92,7 +135,7 @@ def _download_info(self, show_id, path, video_id, fatal=True, query=None):\n             q.update(query)\n         response = self._download_json(\n             'https://www.vvvvid.it/vvvvid/ondemand/%s/%s' % (show_id, path),\n-            video_id, headers=self.geo_verification_headers(), query=q, fatal=fatal)\n+            video_id, headers=self._headers, query=q, fatal=fatal)\n         if not (response or fatal):\n             return\n         if response.get('result') == 'error':\n@@ -219,7 +262,7 @@ def metadata_from_url(r_url):\n                     embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))\n             else:\n                 formats.extend(self._extract_wowza_formats(\n-                    'http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))\n+                    'http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id, skip_protocols=['f4m']))\n             metadata_from_url(embed_code)\n \n         if not is_youtube:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vyborymos.py",
            "diff": "diff --git a/yt_dlp/extractor/vyborymos.py b/yt_dlp/extractor/vyborymos.py\ndeleted file mode 100644\nindex 38651879..00000000\n--- a/yt_dlp/extractor/vyborymos.py\n+++ /dev/null\n@@ -1,52 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-\n-\n-class VyboryMosIE(InfoExtractor):\n-    _VALID_URL = r'https?://vybory\\.mos\\.ru/(?:#precinct/|account/channels\\?.*?\\bstation_id=)(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'http://vybory.mos.ru/#precinct/13636',\n-        'info_dict': {\n-            'id': '13636',\n-            'ext': 'mp4',\n-            'title': 're:^\u0423\u0447\u0430\u0441\u0442\u043a\u043e\u0432\u0430\u044f \u0438\u0437\u0431\u0438\u0440\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u044f \u21162231 [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$',\n-            'description': '\u0420\u043e\u0441\u0441\u0438\u044f, \u041c\u043e\u0441\u043a\u0432\u0430, \u0443\u043b\u0438\u0446\u0430 \u0412\u0432\u0435\u0434\u0435\u043d\u0441\u043a\u043e\u0433\u043e, 32\u0410',\n-            'is_live': True,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        }\n-    }, {\n-        'url': 'http://vybory.mos.ru/account/channels?station_id=13636',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        station_id = self._match_id(url)\n-\n-        channels = self._download_json(\n-            'http://vybory.mos.ru/account/channels?station_id=%s' % station_id,\n-            station_id, 'Downloading channels JSON')\n-\n-        formats = []\n-        for cam_num, (sid, hosts, name, _) in enumerate(channels, 1):\n-            for num, host in enumerate(hosts, 1):\n-                formats.append({\n-                    'url': 'http://%s/master.m3u8?sid=%s' % (host, sid),\n-                    'ext': 'mp4',\n-                    'format_id': 'camera%d-host%d' % (cam_num, num),\n-                    'format_note': '%s, %s' % (name, host),\n-                })\n-\n-        info = self._download_json(\n-            'http://vybory.mos.ru/json/voting_stations/%s/%s.json'\n-            % (compat_str(station_id)[:3], station_id),\n-            station_id, 'Downloading station JSON', fatal=False) or {}\n-\n-        return {\n-            'id': station_id,\n-            'title': info.get('name') or station_id,\n-            'description': info.get('address'),\n-            'is_live': True,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/vzaar.py",
            "diff": "diff --git a/yt_dlp/extractor/vzaar.py b/yt_dlp/extractor/vzaar.py\ndeleted file mode 100644\nindex 19908a92..00000000\n--- a/yt_dlp/extractor/vzaar.py\n+++ /dev/null\n@@ -1,100 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    int_or_none,\n-    float_or_none,\n-    unified_timestamp,\n-    url_or_none,\n-)\n-\n-\n-class VzaarIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:(?:www|view)\\.)?vzaar\\.com/(?:videos/)?(?P<id>\\d+)'\n-    _EMBED_REGEX = [r'<iframe[^>]+src=[\"\\'](?P<url>(?:https?:)?//(?:view\\.vzaar\\.com)/[0-9]+)']\n-    _TESTS = [{\n-        # HTTP and HLS\n-        'url': 'https://vzaar.com/videos/1152805',\n-        'md5': 'bde5ddfeb104a6c56a93a06b04901dbf',\n-        'info_dict': {\n-            'id': '1152805',\n-            'ext': 'mp4',\n-            'title': 'sample video (public)',\n-        },\n-    }, {\n-        'url': 'https://view.vzaar.com/27272/player',\n-        'md5': '3b50012ac9bbce7f445550d54e0508f2',\n-        'info_dict': {\n-            'id': '27272',\n-            'ext': 'mp3',\n-            'title': 'MP3',\n-        },\n-    }, {\n-        # hlsAes = true\n-        'url': 'https://view.vzaar.com/11379930/player',\n-        'info_dict': {\n-            'id': '11379930',\n-            'ext': 'mp4',\n-            'title': 'Videoaula',\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n-    }, {\n-        # with null videoTitle\n-        'url': 'https://view.vzaar.com/20313539/download',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        video_data = self._download_json(\n-            'http://view.vzaar.com/v2/%s/video' % video_id, video_id)\n-\n-        title = video_data.get('videoTitle') or video_id\n-\n-        formats = []\n-\n-        source_url = url_or_none(video_data.get('sourceUrl'))\n-        if source_url:\n-            f = {\n-                'url': source_url,\n-                'format_id': 'http',\n-                'quality': 1,\n-            }\n-            if 'audio' in source_url:\n-                f.update({\n-                    'vcodec': 'none',\n-                    'ext': 'mp3',\n-                })\n-            else:\n-                f.update({\n-                    'width': int_or_none(video_data.get('width')),\n-                    'height': int_or_none(video_data.get('height')),\n-                    'ext': 'mp4',\n-                    'fps': float_or_none(video_data.get('fps')),\n-                })\n-            formats.append(f)\n-\n-        video_guid = video_data.get('guid')\n-        usp = video_data.get('usp')\n-        if video_data.get('uspEnabled') and isinstance(video_guid, compat_str) and isinstance(usp, dict):\n-            hls_aes = video_data.get('hlsAes')\n-            qs = '&'.join('%s=%s' % (k, v) for k, v in usp.items())\n-            url_templ = 'http://%%s.vzaar.com/v5/usp%s/%s/%s.ism%%s?' % ('aes' if hls_aes else '', video_guid, video_id)\n-            m3u8_formats = self._extract_m3u8_formats(\n-                url_templ % ('fable', '/.m3u8') + qs, video_id, 'mp4', 'm3u8_native',\n-                m3u8_id='hls', fatal=False)\n-            if hls_aes:\n-                for f in m3u8_formats:\n-                    f['hls_aes'] = {'uri': url_templ % ('goose', '') + qs}\n-            formats.extend(m3u8_formats)\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'thumbnail': self._proto_relative_url(video_data.get('poster')),\n-            'duration': float_or_none(video_data.get('videoDuration')),\n-            'timestamp': unified_timestamp(video_data.get('ts')),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/wakanim.py",
            "diff": "diff --git a/yt_dlp/extractor/wakanim.py b/yt_dlp/extractor/wakanim.py\ndeleted file mode 100644\nindex 155008f8..00000000\n--- a/yt_dlp/extractor/wakanim.py\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-from urllib.parse import unquote\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    merge_dicts,\n-    urljoin,\n-)\n-\n-\n-class WakanimIE(InfoExtractor):\n-    _VALID_URL = r'https://(?:www\\.)?wakanim\\.tv/[^/]+/v2/catalogue/episode/(?P<id>\\d+)'\n-    _TESTS = [{\n-        'url': 'https://www.wakanim.tv/de/v2/catalogue/episode/2997/the-asterisk-war-omu-staffel-1-episode-02-omu',\n-        'info_dict': {\n-            'id': '2997',\n-            'ext': 'mp4',\n-            'title': 'Episode 02',\n-            'description': 'md5:2927701ea2f7e901de8bfa8d39b2852d',\n-            'series': 'The Asterisk War  (OmU.)',\n-            'season_number': 1,\n-            'episode': 'Episode 02',\n-            'episode_number': 2,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-    }, {\n-        # DRM Protected\n-        'url': 'https://www.wakanim.tv/de/v2/catalogue/episode/7843/sword-art-online-alicization-omu-arc-2-folge-15-omu',\n-        'only_matching': True,\n-    }]\n-    _GEO_BYPASS = False\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        if 'Geoblocking' in webpage:\n-            if '/de/' in url:\n-                self.raise_geo_restricted(countries=['DE', 'AT', 'CH'])\n-            else:\n-                self.raise_geo_restricted(countries=['RU'])\n-\n-        manifest_url = urljoin(url, self._search_regex(\n-            r'file\\s*:\\s*([\"\\'])(?P<url>(?:(?!\\1).)+)\\1', webpage, 'manifest url',\n-            group='url'))\n-        if not self.get_param('allow_unplayable_formats'):\n-            # https://docs.microsoft.com/en-us/azure/media-services/previous/media-services-content-protection-overview#streaming-urls\n-            encryption = self._search_regex(\n-                r'encryption%3D(c(?:enc|bc(?:s-aapl)?))',\n-                manifest_url, 'encryption', default=None)\n-            if encryption in ('cenc', 'cbcs-aapl'):\n-                self.report_drm(video_id)\n-\n-        if 'format=mpd-time-cmaf' in unquote(manifest_url):\n-            formats = self._extract_mpd_formats(\n-                manifest_url, video_id, mpd_id='dash')\n-        else:\n-            formats = self._extract_m3u8_formats(\n-                manifest_url, video_id, 'mp4', entry_protocol='m3u8_native',\n-                m3u8_id='hls')\n-\n-        info = self._search_json_ld(webpage, video_id, default={})\n-\n-        title = self._search_regex(\n-            (r'<h1[^>]+\\bclass=[\"\\']episode_h1[^>]+\\btitle=([\"\\'])(?P<title>(?:(?!\\1).)+)\\1',\n-             r'<span[^>]+\\bclass=[\"\\']episode_title[\"\\'][^>]*>(?P<title>[^<]+)'),\n-            webpage, 'title', default=None, group='title')\n-\n-        return merge_dicts(info, {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-        })\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/wat.py",
            "diff": "diff --git a/yt_dlp/extractor/wat.py b/yt_dlp/extractor/wat.py\nindex 7c62d286..9ea3fddd 100644\n--- a/yt_dlp/extractor/wat.py\n+++ b/yt_dlp/extractor/wat.py\n@@ -41,6 +41,18 @@ class WatIE(InfoExtractor):\n             'expected_warnings': [\"Ce contenu n'est pas disponible pour l'instant.\"],\n             'skip': 'This content is no longer available',\n         },\n+        {\n+            'url': 'wat:14010600',\n+            'info_dict': {\n+                'id': '14010600',\n+                'title': 'Burger Quiz - S03 EP21 avec Eye Haidara, Anne Dep\u00e9trini, Jonathan Zacca\u00ef et Pio Marma\u00ef',\n+                'thumbnail': 'https://photos.tf1.fr/1280/720/burger-quiz-11-9adb79-0@1x.jpg',\n+                'upload_date': '20230819',\n+                'duration': 2312,\n+                'ext': 'mp4',\n+            },\n+            'params': {'skip_download': 'm3u8'},\n+        }\n     ]\n     _GEO_BYPASS = False\n \n@@ -54,7 +66,7 @@ def _real_extract(self, url):\n         #     'http://www.wat.tv/interface/contentv4s/' + video_id, video_id)\n         video_data = self._download_json(\n             'https://mediainfo.tf1.fr/mediainfocombo/' + video_id,\n-            video_id, query={'context': 'MYTF1', 'pver': '4020003'})\n+            video_id, query={'pver': '5010000'})\n         video_info = video_data['media']\n \n         error_desc = video_info.get('error_desc')\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/watchbox.py",
            "diff": "diff --git a/yt_dlp/extractor/watchbox.py b/yt_dlp/extractor/watchbox.py\ndeleted file mode 100644\nindex c973ca99..00000000\n--- a/yt_dlp/extractor/watchbox.py\n+++ /dev/null\n@@ -1,153 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_str\n-from ..utils import (\n-    int_or_none,\n-    js_to_json,\n-    strip_or_none,\n-    try_get,\n-    unescapeHTML,\n-    unified_timestamp,\n-)\n-\n-\n-class WatchBoxIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?watchbox\\.de/(?P<kind>serien|filme)/(?:[^/]+/)*[^/]+-(?P<id>\\d+)'\n-    _TESTS = [{\n-        # film\n-        'url': 'https://www.watchbox.de/filme/free-jimmy-12325.html',\n-        'info_dict': {\n-            'id': '341368',\n-            'ext': 'mp4',\n-            'title': 'Free Jimmy',\n-            'description': 'md5:bcd8bafbbf9dc0ef98063d344d7cc5f6',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 4890,\n-            'age_limit': 16,\n-            'release_year': 2009,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'expected_warnings': ['Failed to download m3u8 information'],\n-    }, {\n-        # episode\n-        'url': 'https://www.watchbox.de/serien/ugly-americans-12231/staffel-1/date-in-der-hoelle-328286.html',\n-        'info_dict': {\n-            'id': '328286',\n-            'ext': 'mp4',\n-            'title': 'S01 E01 - Date in der H\u00f6lle',\n-            'description': 'md5:2f31c74a8186899f33cb5114491dae2b',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 1291,\n-            'age_limit': 12,\n-            'release_year': 2010,\n-            'series': 'Ugly Americans',\n-            'season_number': 1,\n-            'episode': 'Date in der H\u00f6lle',\n-            'episode_number': 1,\n-        },\n-        'params': {\n-            'skip_download': True,\n-        },\n-        'expected_warnings': ['Failed to download m3u8 information'],\n-    }, {\n-        'url': 'https://www.watchbox.de/serien/ugly-americans-12231/staffel-2/der-ring-des-powers-328270',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        kind, video_id = mobj.group('kind', 'id')\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        player_config = self._parse_json(\n-            self._search_regex(\n-                r'data-player-conf=([\"\\'])(?P<data>{.+?})\\1', webpage,\n-                'player config', default='{}', group='data'),\n-            video_id, transform_source=unescapeHTML, fatal=False)\n-\n-        if not player_config:\n-            player_config = self._parse_json(\n-                self._search_regex(\n-                    r'playerConf\\s*=\\s*({.+?})\\s*;', webpage, 'player config',\n-                    default='{}'),\n-                video_id, transform_source=js_to_json, fatal=False) or {}\n-\n-        source = player_config.get('source') or {}\n-\n-        video_id = compat_str(source.get('videoId') or video_id)\n-\n-        devapi = self._download_json(\n-            'http://api.watchbox.de/devapi/id/%s' % video_id, video_id, query={\n-                'format': 'json',\n-                'apikey': 'hbbtv',\n-            }, fatal=False)\n-\n-        item = try_get(devapi, lambda x: x['items'][0], dict) or {}\n-\n-        title = item.get('title') or try_get(\n-            item, lambda x: x['movie']['headline_movie'],\n-            compat_str) or source['title']\n-\n-        formats = []\n-        hls_url = item.get('media_videourl_hls') or source.get('hls')\n-        if hls_url:\n-            formats.extend(self._extract_m3u8_formats(\n-                hls_url, video_id, 'mp4', entry_protocol='m3u8_native',\n-                m3u8_id='hls', fatal=False))\n-        dash_url = item.get('media_videourl_wv') or source.get('dash')\n-        if dash_url:\n-            formats.extend(self._extract_mpd_formats(\n-                dash_url, video_id, mpd_id='dash', fatal=False))\n-        mp4_url = item.get('media_videourl')\n-        if mp4_url:\n-            formats.append({\n-                'url': mp4_url,\n-                'format_id': 'mp4',\n-                'width': int_or_none(item.get('width')),\n-                'height': int_or_none(item.get('height')),\n-                'tbr': int_or_none(item.get('bitrate')),\n-            })\n-\n-        description = strip_or_none(item.get('descr'))\n-        thumbnail = item.get('media_content_thumbnail_large') or source.get('poster') or item.get('media_thumbnail')\n-        duration = int_or_none(item.get('media_length') or source.get('length'))\n-        timestamp = unified_timestamp(item.get('pubDate'))\n-        view_count = int_or_none(item.get('media_views'))\n-        age_limit = int_or_none(try_get(item, lambda x: x['movie']['fsk']))\n-        release_year = int_or_none(try_get(item, lambda x: x['movie']['rel_year']))\n-\n-        info = {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'duration': duration,\n-            'timestamp': timestamp,\n-            'view_count': view_count,\n-            'age_limit': age_limit,\n-            'release_year': release_year,\n-            'formats': formats,\n-        }\n-\n-        if kind.lower() == 'serien':\n-            series = try_get(\n-                item, lambda x: x['special']['title'],\n-                compat_str) or source.get('format')\n-            season_number = int_or_none(self._search_regex(\n-                r'^S(\\d{1,2})\\s*E\\d{1,2}', title, 'season number',\n-                default=None) or self._search_regex(\n-                    r'/staffel-(\\d+)/', url, 'season number', default=None))\n-            episode = source.get('title')\n-            episode_number = int_or_none(self._search_regex(\n-                r'^S\\d{1,2}\\s*E(\\d{1,2})', title, 'episode number',\n-                default=None))\n-            info.update({\n-                'series': series,\n-                'season_number': season_number,\n-                'episode': episode,\n-                'episode_number': episode_number,\n-            })\n-\n-        return info\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/watchindianporn.py",
            "diff": "diff --git a/yt_dlp/extractor/watchindianporn.py b/yt_dlp/extractor/watchindianporn.py\ndeleted file mode 100644\nindex 3ded2d1d..00000000\n--- a/yt_dlp/extractor/watchindianporn.py\n+++ /dev/null\n@@ -1,65 +0,0 @@\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import parse_duration\n-\n-\n-class WatchIndianPornIE(InfoExtractor):\n-    IE_DESC = 'Watch Indian Porn'\n-    _VALID_URL = r'https?://(?:www\\.)?watchindianporn\\.net/(?:[^/]+/)*video/(?P<display_id>[^/]+)-(?P<id>[a-zA-Z0-9]+)\\.html'\n-    _TEST = {\n-        'url': 'http://www.watchindianporn.net/video/hot-milf-from-kerala-shows-off-her-gorgeous-large-breasts-on-camera-RZa2avywNPa.html',\n-        'md5': '249589a164dde236ec65832bfce17440',\n-        'info_dict': {\n-            'id': 'RZa2avywNPa',\n-            'display_id': 'hot-milf-from-kerala-shows-off-her-gorgeous-large-breasts-on-camera',\n-            'ext': 'mp4',\n-            'title': 'Hot milf from kerala shows off her gorgeous large breasts on camera',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 226,\n-            'view_count': int,\n-            'categories': list,\n-            'age_limit': 18,\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        display_id = mobj.group('display_id')\n-\n-        webpage = self._download_webpage(url, display_id)\n-\n-        info_dict = self._parse_html5_media_entries(url, webpage, video_id)[0]\n-\n-        title = self._html_search_regex((\n-            r'<title>(.+?)\\s*-\\s*Indian\\s+Porn</title>',\n-            r'<h4>(.+?)</h4>'\n-        ), webpage, 'title')\n-\n-        duration = parse_duration(self._search_regex(\n-            r'Time:\\s*<strong>\\s*(.+?)\\s*</strong>',\n-            webpage, 'duration', fatal=False))\n-\n-        view_count = int(self._search_regex(\n-            r'(?s)Time:\\s*<strong>.*?</strong>.*?<strong>\\s*(\\d+)\\s*</strong>',\n-            webpage, 'view count', fatal=False))\n-\n-        categories = re.findall(\n-            r'<a[^>]+class=[\\'\"]categories[\\'\"][^>]*>\\s*([^<]+)\\s*</a>',\n-            webpage)\n-\n-        info_dict.update({\n-            'id': video_id,\n-            'display_id': display_id,\n-            'http_headers': {\n-                'Referer': url,\n-            },\n-            'title': title,\n-            'duration': duration,\n-            'view_count': view_count,\n-            'categories': categories,\n-            'age_limit': 18,\n-        })\n-\n-        return info_dict\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/wdr.py",
            "diff": "diff --git a/yt_dlp/extractor/wdr.py b/yt_dlp/extractor/wdr.py\nindex de5dc266..6767f265 100644\n--- a/yt_dlp/extractor/wdr.py\n+++ b/yt_dlp/extractor/wdr.py\n@@ -173,6 +173,7 @@ class WDRPageIE(WDRIE):  # XXX: Do not subclass from concrete IE\n             'skip': 'HTTP Error 404: Not Found',\n         },\n         {\n+            # FIXME: Asset JSON is directly embedded in webpage\n             'url': 'http://www1.wdr.de/mediathek/video/live/index.html',\n             'info_dict': {\n                 'id': 'mdb-2296252',\n@@ -221,6 +222,8 @@ class WDRPageIE(WDRIE):  # XXX: Do not subclass from concrete IE\n                 'id': 'mdb-869971',\n                 'ext': 'mp4',\n                 'title': r're:^COSMO Livestream [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$',\n+                'alt_title': 'COSMO Livestream',\n+                'live_status': 'is_live',\n                 'upload_date': '20160101',\n             },\n             'params': {\n@@ -248,6 +251,16 @@ class WDRPageIE(WDRIE):  # XXX: Do not subclass from concrete IE\n             'url': 'https://kinder.wdr.de/tv/die-sendung-mit-dem-elefanten/av/video-folge---astronaut-100.html',\n             'only_matching': True,\n         },\n+        {\n+            'url': 'https://www1.wdr.de/mediathek/video/sendungen/rockpalast/video-baroness---freak-valley-festival--100.html',\n+            'info_dict': {\n+                'id': 'mdb-2741028',\n+                'ext': 'mp4',\n+                'title': 'Baroness - Freak Valley Festival 2022',\n+                'alt_title': 'Rockpalast',\n+                'upload_date': '20220725',\n+            },\n+        }\n     ]\n \n     def _real_extract(self, url):\n@@ -259,7 +272,7 @@ def _real_extract(self, url):\n \n         # Article with several videos\n \n-        # for wdr.de the data-extension is in a tag with the class \"mediaLink\"\n+        # for wdr.de the data-extension-ard is in a tag with the class \"mediaLink\"\n         # for wdr.de radio players, in a tag with the class \"wdrrPlayerPlayBtn\"\n         # for wdrmaus, in a tag with the class \"videoButton\" (previously a link\n         # to the page in a multiline \"videoLink\"-tag)\n@@ -268,7 +281,7 @@ def _real_extract(self, url):\n                     (?:\n                         ([\"\\'])(?:mediaLink|wdrrPlayerPlayBtn|videoButton)\\b.*?\\1[^>]+|\n                         ([\"\\'])videoLink\\b.*?\\2[\\s]*>\\n[^\\n]*\n-                    )data-extension=([\"\\'])(?P<data>(?:(?!\\3).)+)\\3\n+                    )data-extension(?:-ard)?=([\"\\'])(?P<data>(?:(?!\\3).)+)\\3\n                     ''', webpage):\n             media_link_obj = self._parse_json(\n                 mobj.group('data'), display_id, transform_source=js_to_json,\n@@ -295,7 +308,7 @@ def _real_extract(self, url):\n                     compat_urlparse.urljoin(url, mobj.group('href')),\n                     ie=WDRPageIE.ie_key())\n                 for mobj in re.finditer(\n-                    r'<a[^>]+\\bhref=([\"\\'])(?P<href>(?:(?!\\1).)+)\\1[^>]+\\bdata-extension=',\n+                    r'<a[^>]+\\bhref=([\"\\'])(?P<href>(?:(?!\\1).)+)\\1[^>]+\\bdata-extension(?:-ard)?=',\n                     webpage) if re.match(self._PAGE_REGEX, mobj.group('href'))\n             ]\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/weibo.py",
            "diff": "diff --git a/yt_dlp/extractor/weibo.py b/yt_dlp/extractor/weibo.py\nindex bc9a71ab..2fca745a 100644\n--- a/yt_dlp/extractor/weibo.py\n+++ b/yt_dlp/extractor/weibo.py\n@@ -1,134 +1,251 @@\n-from .common import InfoExtractor\n-\n import json\n import random\n-import re\n+import itertools\n+import urllib.parse\n \n-from ..compat import (\n-    compat_parse_qs,\n-    compat_str,\n-)\n+from .common import InfoExtractor\n from ..utils import (\n-    js_to_json,\n+    int_or_none,\n+    make_archive_id,\n+    mimetype2ext,\n+    parse_resolution,\n+    str_or_none,\n     strip_jsonp,\n+    traverse_obj,\n+    url_or_none,\n     urlencode_postdata,\n+    urljoin,\n )\n \n \n-class WeiboIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?weibo\\.com/[0-9]+/(?P<id>[a-zA-Z0-9]+)'\n-    _TEST = {\n-        'url': 'https://weibo.com/6275294458/Fp6RGfbff?type=comment',\n+class WeiboBaseIE(InfoExtractor):\n+    def _update_visitor_cookies(self, visitor_url, video_id):\n+        headers = {'Referer': visitor_url}\n+        chrome_ver = self._search_regex(\n+            r'Chrome/(\\d+)', self.get_param('http_headers')['User-Agent'], 'user agent version', default='90')\n+        visitor_data = self._download_json(\n+            'https://passport.weibo.com/visitor/genvisitor', video_id,\n+            note='Generating first-visit guest request',\n+            headers=headers, transform_source=strip_jsonp,\n+            data=urlencode_postdata({\n+                'cb': 'gen_callback',\n+                'fp': json.dumps({\n+                    'os': '1',\n+                    'browser': f'Chrome{chrome_ver},0,0,0',\n+                    'fonts': 'undefined',\n+                    'screenInfo': '1920*1080*24',\n+                    'plugins': ''\n+                }, separators=(',', ':'))}))['data']\n+\n+        self._download_webpage(\n+            'https://passport.weibo.com/visitor/visitor', video_id,\n+            note='Running first-visit callback to get guest cookies',\n+            headers=headers, query={\n+                'a': 'incarnate',\n+                't': visitor_data['tid'],\n+                'w': 3 if visitor_data.get('new_tid') else 2,\n+                'c': f'{visitor_data.get(\"confidence\", 100):03d}',\n+                'gc': '',\n+                'cb': 'cross_domain',\n+                'from': 'weibo',\n+                '_rand': random.random(),\n+            })\n+\n+    def _weibo_download_json(self, url, video_id, *args, fatal=True, note='Downloading JSON metadata', **kwargs):\n+        webpage, urlh = self._download_webpage_handle(url, video_id, *args, fatal=fatal, note=note, **kwargs)\n+        if urllib.parse.urlparse(urlh.url).netloc == 'passport.weibo.com':\n+            self._update_visitor_cookies(urlh.url, video_id)\n+            webpage = self._download_webpage(url, video_id, *args, fatal=fatal, note=note, **kwargs)\n+        return self._parse_json(webpage, video_id, fatal=fatal)\n+\n+    def _extract_formats(self, video_info):\n+        media_info = traverse_obj(video_info, ('page_info', 'media_info'))\n+        formats = traverse_obj(media_info, (\n+            'playback_list', lambda _, v: url_or_none(v['play_info']['url']), 'play_info', {\n+                'url': 'url',\n+                'format': ('quality_desc', {str}),\n+                'format_id': ('label', {str}),\n+                'ext': ('mime', {mimetype2ext}),\n+                'tbr': ('bitrate', {int_or_none}, {lambda x: x or None}),\n+                'vcodec': ('video_codecs', {str}),\n+                'fps': ('fps', {int_or_none}),\n+                'width': ('width', {int_or_none}),\n+                'height': ('height', {int_or_none}),\n+                'filesize': ('size', {int_or_none}),\n+                'acodec': ('audio_codecs', {str}),\n+                'asr': ('audio_sample_rate', {int_or_none}),\n+                'audio_channels': ('audio_channels', {int_or_none}),\n+            }))\n+        if not formats:  # fallback, should be barely used\n+            for url in set(traverse_obj(media_info, (..., {url_or_none}))):\n+                if 'label=' in url:  # filter out non-video urls\n+                    format_id, resolution = self._search_regex(\n+                        r'label=(\\w+)&template=(\\d+x\\d+)', url, 'format info',\n+                        group=(1, 2), default=(None, None))\n+                    formats.append({\n+                        'url': url,\n+                        'format_id': format_id,\n+                        **parse_resolution(resolution),\n+                        **traverse_obj(media_info, (\n+                            'video_details', lambda _, v: v['label'].startswith(format_id), {\n+                                'size': ('size', {int_or_none}),\n+                                'tbr': ('bitrate', {int_or_none}),\n+                            }\n+                        ), get_all=False),\n+                    })\n+        return formats\n+\n+    def _parse_video_info(self, video_info, video_id=None):\n+        return {\n+            'id': video_id,\n+            'extractor_key': WeiboIE.ie_key(),\n+            'extractor': WeiboIE.IE_NAME,\n+            'formats': self._extract_formats(video_info),\n+            'http_headers': {'Referer': 'https://weibo.com/'},\n+            '_old_archive_ids': [make_archive_id('WeiboMobile', video_id)],\n+            **traverse_obj(video_info, {\n+                'id': (('id', 'id_str', 'mid'), {str_or_none}),\n+                'display_id': ('mblogid', {str_or_none}),\n+                'title': ('page_info', 'media_info', ('video_title', 'kol_title', 'name'), {str}, {lambda x: x or None}),\n+                'description': ('text_raw', {str}),\n+                'duration': ('page_info', 'media_info', 'duration', {int_or_none}),\n+                'timestamp': ('page_info', 'media_info', 'video_publish_time', {int_or_none}),\n+                'thumbnail': ('page_info', 'page_pic', {url_or_none}),\n+                'uploader': ('user', 'screen_name', {str}),\n+                'uploader_id': ('user', ('id', 'id_str'), {str_or_none}),\n+                'uploader_url': ('user', 'profile_url', {lambda x: urljoin('https://weibo.com/', x)}),\n+                'view_count': ('page_info', 'media_info', 'online_users_number', {int_or_none}),\n+                'like_count': ('attitudes_count', {int_or_none}),\n+                'repost_count': ('reposts_count', {int_or_none}),\n+            }, get_all=False),\n+            'tags': traverse_obj(video_info, ('topic_struct', ..., 'topic_title', {str})) or None,\n+        }\n+\n+\n+class WeiboIE(WeiboBaseIE):\n+    _VALID_URL = r'https?://(?:m\\.weibo\\.cn/status|(?:www\\.)?weibo\\.com/\\d+)/(?P<id>[a-zA-Z0-9]+)'\n+    _TESTS = [{\n+        'url': 'https://weibo.com/7827771738/N4xlMvjhI',\n         'info_dict': {\n-            'id': 'Fp6RGfbff',\n+            'id': '4910815147462302',\n+            'ext': 'mp4',\n+            'display_id': 'N4xlMvjhI',\n+            'title': '\u3010\u7761\u524d\u6d88\u606f\u6691\u5047\u7248\u7b2c\u4e00\u671f\uff1a\u62c9\u6cf0\u56fd\u4e00\u628a  \u5bf9\u4e2d\u56fd\u6709\u597d\u5904\u3011',\n+            'description': 'md5:e2637a7673980d68694ea7c43cf12a5f',\n+            'duration': 918,\n+            'timestamp': 1686312819,\n+            'upload_date': '20230609',\n+            'thumbnail': r're:https://.*\\.jpg',\n+            'uploader': '\u7761\u524d\u89c6\u9891\u57fa\u5730',\n+            'uploader_id': '7827771738',\n+            'uploader_url': 'https://weibo.com/u/7827771738',\n+            'view_count': int,\n+            'like_count': int,\n+            'repost_count': int,\n+            'tags': ['\u6cf0\u56fd\u5927\u9009\u8fdc\u8fdb\u515a\u83b7\u80dc', '\u7761\u524d\u6d88\u606f', '\u6691\u671f\u7248'],\n+        },\n+    }, {\n+        'url': 'https://m.weibo.cn/status/4189191225395228',\n+        'info_dict': {\n+            'id': '4189191225395228',\n             'ext': 'mp4',\n-            'title': 'You should have servants to massage you,... \u6765\u81eaHosico_\u732b - \u5fae\u535a',\n+            'display_id': 'FBqgOmDxO',\n+            'title': '\u67f4\u72ac\u67f4\u72ac\u7684\u79d2\u62cd\u89c6\u9891',\n+            'description': 'md5:80f461ab5cdae6bbdb70efbf5a1db24f',\n+            'duration': 53,\n+            'timestamp': 1514264429,\n+            'upload_date': '20171226',\n+            'thumbnail': r're:https://.*\\.jpg',\n+            'uploader': '\u67f4\u72ac\u67f4\u72ac',\n+            'uploader_id': '5926682210',\n+            'uploader_url': 'https://weibo.com/u/5926682210',\n+            'view_count': int,\n+            'like_count': int,\n+            'repost_count': int,\n         }\n-    }\n+    }, {\n+        'url': 'https://weibo.com/0/4224132150961381',\n+        'note': 'no playback_list example',\n+        'only_matching': True,\n+    }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n-        # to get Referer url for genvisitor\n-        webpage, urlh = self._download_webpage_handle(url, video_id)\n-\n-        visitor_url = urlh.url\n-\n-        if 'passport.weibo.com' in visitor_url:\n-            # first visit\n-            visitor_data = self._download_json(\n-                'https://passport.weibo.com/visitor/genvisitor', video_id,\n-                note='Generating first-visit data',\n-                transform_source=strip_jsonp,\n-                headers={'Referer': visitor_url},\n-                data=urlencode_postdata({\n-                    'cb': 'gen_callback',\n-                    'fp': json.dumps({\n-                        'os': '2',\n-                        'browser': 'Gecko57,0,0,0',\n-                        'fonts': 'undefined',\n-                        'screenInfo': '1440*900*24',\n-                        'plugins': '',\n-                    }),\n-                }))\n-\n-            tid = visitor_data['data']['tid']\n-            cnfd = '%03d' % visitor_data['data']['confidence']\n-\n-            self._download_webpage(\n-                'https://passport.weibo.com/visitor/visitor', video_id,\n-                note='Running first-visit callback',\n-                query={\n-                    'a': 'incarnate',\n-                    't': tid,\n-                    'w': 2,\n-                    'c': cnfd,\n-                    'cb': 'cross_domain',\n-                    'from': 'weibo',\n-                    '_rand': random.random(),\n-                })\n-\n-            webpage = self._download_webpage(\n-                url, video_id, note='Revisiting webpage')\n-\n-        title = self._html_extract_title(webpage)\n-\n-        video_formats = compat_parse_qs(self._search_regex(\n-            r'video-sources=\\\\\\\"(.+?)\\\"', webpage, 'video_sources'))\n-\n-        formats = []\n-        supported_resolutions = (480, 720)\n-        for res in supported_resolutions:\n-            vid_urls = video_formats.get(compat_str(res))\n-            if not vid_urls or not isinstance(vid_urls, list):\n-                continue\n-\n-            vid_url = vid_urls[0]\n-            formats.append({\n-                'url': vid_url,\n-                'height': res,\n-            })\n \n-        uploader = self._og_search_property(\n-            'nick-name', webpage, 'uploader', default=None)\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'uploader': uploader,\n-            'formats': formats\n-        }\n+        return self._parse_video_info(self._weibo_download_json(\n+            f'https://weibo.com/ajax/statuses/show?id={video_id}', video_id))\n \n \n-class WeiboMobileIE(InfoExtractor):\n-    _VALID_URL = r'https?://m\\.weibo\\.cn/status/(?P<id>[0-9]+)(\\?.+)?'\n-    _TEST = {\n-        'url': 'https://m.weibo.cn/status/4189191225395228?wm=3333_2001&sourcetype=weixin&featurecode=newtitle&from=singlemessage&isappinstalled=0',\n+class WeiboVideoIE(WeiboBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?weibo\\.com/tv/show/(?P<id>\\d+:\\d+)'\n+    _TESTS = [{\n+        'url': 'https://weibo.com/tv/show/1034:4797699866951785?from=old_pc_videoshow',\n         'info_dict': {\n-            'id': '4189191225395228',\n+            'id': '4797700463137878',\n             'ext': 'mp4',\n-            'title': '\u5348\u7761\u5f53\u7136\u662f\u8981\u751c\u751c\u871c\u871c\u7684\u5566',\n-            'uploader': '\u67f4\u72ac\u67f4\u72ac'\n+            'display_id': 'LEZDodaiW',\n+            'title': '\u5443\uff0c\u7a0d\u5fae\u4e86\u89e3\u4e86\u4e00\u4e0b\u9761\u70dfmiya\uff0c\u611f\u89c9\u8fd9\u4e1c\u897f\u4e5f\u592a\u4e8c\u4e86',\n+            'description': '\u5443\uff0c\u7a0d\u5fae\u4e86\u89e3\u4e86\u4e00\u4e0b\u9761\u70dfmiya\uff0c\u611f\u89c9\u8fd9\u4e1c\u897f\u4e5f\u592a\u4e8c\u4e86 http://t.cn/A6aerGsM \u200b\u200b\u200b',\n+            'duration': 76,\n+            'timestamp': 1659344278,\n+            'upload_date': '20220801',\n+            'thumbnail': r're:https://.*\\.jpg',\n+            'uploader': '\u541b\u5b50\u7231\u8d22\u9648\u5e73\u5b89',\n+            'uploader_id': '3905382233',\n+            'uploader_url': 'https://weibo.com/u/3905382233',\n+            'view_count': int,\n+            'like_count': int,\n+            'repost_count': int,\n         }\n-    }\n+    }]\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n-        # to get Referer url for genvisitor\n-        webpage = self._download_webpage(url, video_id, note='visit the page')\n \n-        weibo_info = self._parse_json(self._search_regex(\n-            r'var\\s+\\$render_data\\s*=\\s*\\[({.*})\\]\\[0\\]\\s*\\|\\|\\s*{};',\n-            webpage, 'js_code', flags=re.DOTALL),\n-            video_id, transform_source=js_to_json)\n+        post_data = f'data={{\"Component_Play_Playinfo\":{{\"oid\":\"{video_id}\"}}}}'.encode()\n+        video_info = self._weibo_download_json(\n+            f'https://weibo.com/tv/api/component?page=%2Ftv%2Fshow%2F{video_id.replace(\":\", \"%3A\")}',\n+            video_id, headers={'Referer': url}, data=post_data)['data']['Component_Play_Playinfo']\n+        return self.url_result(f'https://weibo.com/0/{video_info[\"mid\"]}', WeiboIE)\n \n-        status_data = weibo_info.get('status', {})\n-        page_info = status_data.get('page_info')\n-        title = status_data['status_title']\n-        uploader = status_data.get('user', {}).get('screen_name')\n \n-        return {\n-            'id': video_id,\n-            'title': title,\n+class WeiboUserIE(WeiboBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?weibo\\.com/u/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://weibo.com/u/2066652961?tabtype=video',\n+        'info_dict': {\n+            'id': '2066652961',\n+            'title': '\u8427\u5f71\u6bbf\u4e0b\u7684\u89c6\u9891',\n+            'description': '\u8427\u5f71\u6bbf\u4e0b\u7684\u5168\u90e8\u89c6\u9891',\n+            'uploader': '\u8427\u5f71\u6bbf\u4e0b',\n+        },\n+        'playlist_mincount': 195,\n+    }]\n+\n+    def _fetch_page(self, uid, cursor=0, page=1):\n+        return self._weibo_download_json(\n+            'https://weibo.com/ajax/profile/getWaterFallContent',\n+            uid, note=f'Downloading videos page {page}',\n+            query={'uid': uid, 'cursor': cursor})['data']\n+\n+    def _entries(self, uid, first_page):\n+        cursor = 0\n+        for page in itertools.count(1):\n+            response = first_page if page == 1 else self._fetch_page(uid, cursor, page)\n+            for video_info in traverse_obj(response, ('list', ..., {dict})):\n+                yield self._parse_video_info(video_info)\n+            cursor = response.get('next_cursor')\n+            if (int_or_none(cursor) or -1) < 0:\n+                break\n+\n+    def _real_extract(self, url):\n+        uid = self._match_id(url)\n+        first_page = self._fetch_page(uid)\n+        uploader = traverse_obj(first_page, ('list', ..., 'user', 'screen_name', {str}), get_all=False)\n+        metainfo = {\n+            'title': f'{uploader}\u7684\u89c6\u9891',\n+            'description': f'{uploader}\u7684\u5168\u90e8\u89c6\u9891',\n             'uploader': uploader,\n-            'url': page_info['media_info']['stream_url']\n-        }\n+        } if uploader else {}\n+\n+        return self.playlist_result(self._entries(uid, first_page), uid, **metainfo)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/weverse.py",
            "diff": "diff --git a/yt_dlp/extractor/weverse.py b/yt_dlp/extractor/weverse.py\nindex 9a08b8e4..c94ca9db 100644\n--- a/yt_dlp/extractor/weverse.py\n+++ b/yt_dlp/extractor/weverse.py\n@@ -45,10 +45,10 @@ def _perform_login(self, username, password):\n             'x-acc-trace-id': str(uuid.uuid4()),\n             'x-clog-user-device-id': str(uuid.uuid4()),\n         }\n-        check_username = self._download_json(\n-            f'{self._ACCOUNT_API_BASE}/signup/email/status', None,\n-            note='Checking username', query={'email': username}, headers=headers)\n-        if not check_username.get('hasPassword'):\n+        valid_username = traverse_obj(self._download_json(\n+            f'{self._ACCOUNT_API_BASE}/signup/email/status', None, note='Checking username',\n+            query={'email': username}, headers=headers, expected_status=(400, 404)), 'hasPassword')\n+        if not valid_username:\n             raise ExtractorError('Invalid username provided', expected=True)\n \n         headers['content-type'] = 'application/json'\n@@ -70,10 +70,8 @@ def _real_initialize(self):\n             return\n \n         token = try_call(lambda: self._get_cookies('https://weverse.io/')['we2_access_token'].value)\n-        if not token:\n-            self.raise_login_required()\n-\n-        WeverseBaseIE._API_HEADERS['Authorization'] = f'Bearer {token}'\n+        if token:\n+            WeverseBaseIE._API_HEADERS['Authorization'] = f'Bearer {token}'\n \n     def _call_api(self, ep, video_id, data=None, note='Downloading API JSON'):\n         # Ref: https://ssl.pstatic.net/static/wevweb/2_3_2_11101725/public/static/js/2488.a09b41ff.chunk.js\n@@ -101,11 +99,14 @@ def _call_api(self, ep, video_id, data=None, note='Downloading API JSON'):\n                 self.raise_login_required(\n                     'Session token has expired. Log in again or refresh cookies in browser')\n             elif isinstance(e.cause, HTTPError) and e.cause.status == 403:\n-                raise ExtractorError('Your account does not have access to this content', expected=True)\n+                if 'Authorization' in self._API_HEADERS:\n+                    raise ExtractorError('Your account does not have access to this content', expected=True)\n+                self.raise_login_required()\n             raise\n \n     def _call_post_api(self, video_id):\n-        return self._call_api(f'/post/v1.0/post-{video_id}?fieldSet=postV1', video_id)\n+        path = '' if 'Authorization' in self._API_HEADERS else '/preview'\n+        return self._call_api(f'/post/v1.0/post-{video_id}{path}?fieldSet=postV1', video_id)\n \n     def _get_community_id(self, channel):\n         return str(self._call_api(\n@@ -181,7 +182,7 @@ def _extract_live_status(self, data):\n \n \n class WeverseIE(WeverseBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse.io/(?P<artist>[^/?#]+)/live/(?P<id>[\\d-]+)'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse\\.io/(?P<artist>[^/?#]+)/live/(?P<id>[\\d-]+)'\n     _TESTS = [{\n         'url': 'https://weverse.io/billlie/live/0-107323480',\n         'md5': '1fa849f00181eef9100d3c8254c47979',\n@@ -343,7 +344,7 @@ def _real_extract(self, url):\n \n \n class WeverseMediaIE(WeverseBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse.io/(?P<artist>[^/?#]+)/media/(?P<id>[\\d-]+)'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse\\.io/(?P<artist>[^/?#]+)/media/(?P<id>[\\d-]+)'\n     _TESTS = [{\n         'url': 'https://weverse.io/billlie/media/4-116372884',\n         'md5': '8efc9cfd61b2f25209eb1a5326314d28',\n@@ -419,7 +420,7 @@ def _real_extract(self, url):\n \n \n class WeverseMomentIE(WeverseBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse.io/(?P<artist>[^/?#]+)/moment/(?P<uid>[\\da-f]+)/post/(?P<id>[\\d-]+)'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse\\.io/(?P<artist>[^/?#]+)/moment/(?P<uid>[\\da-f]+)/post/(?P<id>[\\d-]+)'\n     _TESTS = [{\n         'url': 'https://weverse.io/secretnumber/moment/66a07e164b56a696ee71c99315ffe27b/post/1-117229444',\n         'md5': '87733ac19a54081b7dfc2442036d282b',\n@@ -515,7 +516,7 @@ def _real_extract(self, url):\n \n \n class WeverseLiveTabIE(WeverseTabBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse.io/(?P<id>[^/?#]+)/live/?(?:[?#]|$)'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse\\.io/(?P<id>[^/?#]+)/live/?(?:[?#]|$)'\n     _TESTS = [{\n         'url': 'https://weverse.io/billlie/live/',\n         'playlist_mincount': 55,\n@@ -533,7 +534,7 @@ class WeverseLiveTabIE(WeverseTabBaseIE):\n \n \n class WeverseMediaTabIE(WeverseTabBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse.io/(?P<id>[^/?#]+)/media(?:/|/all|/new)?(?:[?#]|$)'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse\\.io/(?P<id>[^/?#]+)/media(?:/|/all|/new)?(?:[?#]|$)'\n     _TESTS = [{\n         'url': 'https://weverse.io/billlie/media/',\n         'playlist_mincount': 231,\n@@ -557,7 +558,7 @@ class WeverseMediaTabIE(WeverseTabBaseIE):\n \n \n class WeverseLiveIE(WeverseBaseIE):\n-    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse.io/(?P<id>[^/?#]+)/?(?:[?#]|$)'\n+    _VALID_URL = r'https?://(?:www\\.|m\\.)?weverse\\.io/(?P<id>[^/?#]+)/?(?:[?#]|$)'\n     _TESTS = [{\n         'url': 'https://weverse.io/purplekiss',\n         'info_dict': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/willow.py",
            "diff": "diff --git a/yt_dlp/extractor/willow.py b/yt_dlp/extractor/willow.py\ndeleted file mode 100644\nindex 0ec9c9d6..00000000\n--- a/yt_dlp/extractor/willow.py\n+++ /dev/null\n@@ -1,56 +0,0 @@\n-from ..utils import ExtractorError\n-from .common import InfoExtractor\n-\n-\n-class WillowIE(InfoExtractor):\n-    _VALID_URL = r'https?://(www\\.)?willow\\.tv/videos/(?P<id>[0-9a-z-_]+)'\n-    _GEO_COUNTRIES = ['US']\n-\n-    _TESTS = [{\n-        'url': 'http://willow.tv/videos/d5winning-moment-eng-vs-ind-streaming-online-4th-test-india-tour-of-england-2021',\n-        'info_dict': {\n-            'id': '169662',\n-            'display_id': 'd5winning-moment-eng-vs-ind-streaming-online-4th-test-india-tour-of-england-2021',\n-            'ext': 'mp4',\n-            'title': 'Winning Moment: 4th Test, England vs India',\n-            'thumbnail': 'https://aimages.willow.tv/ytThumbnails/6748_D5winning_moment.jpg',\n-            'duration': 233,\n-            'timestamp': 1630947954,\n-            'upload_date': '20210906',\n-            'location': 'Kennington Oval, London',\n-            'series': 'India tour of England 2021',\n-        },\n-        'params': {\n-            'skip_download': True,  # AES-encrypted m3u8\n-        },\n-    }, {\n-        'url': 'http://willow.tv/videos/highlights-short-ind-vs-nz-streaming-online-2nd-t20i-new-zealand-tour-of-india-2021',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-        video_data = self._parse_json(self._html_search_regex(\n-            r'var\\s+data_js\\s*=\\s*JSON\\.parse\\(\\'(.+)\\'\\)', webpage,\n-            'data_js'), video_id)\n-\n-        video = next((v for v in video_data.get('trending_videos') or []\n-                      if v.get('secureurl')), None)\n-        if not video:\n-            raise ExtractorError('No videos found')\n-\n-        formats = self._extract_m3u8_formats(video['secureurl'], video_id, 'mp4')\n-\n-        return {\n-            'id': str(video.get('content_id')),\n-            'display_id': video.get('video_slug'),\n-            'title': video.get('video_name') or self._html_search_meta('twitter:title', webpage),\n-            'formats': formats,\n-            'thumbnail': video.get('yt_thumb_url') or self._html_search_meta(\n-                'twitter:image', webpage, default=None),\n-            'duration': video.get('duration_seconds'),\n-            'timestamp': video.get('created_date'),\n-            'location': video.get('venue'),\n-            'series': video.get('series_name'),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/wimtv.py",
            "diff": "diff --git a/yt_dlp/extractor/wimtv.py b/yt_dlp/extractor/wimtv.py\nindex 57111239..f9bf092d 100644\n--- a/yt_dlp/extractor/wimtv.py\n+++ b/yt_dlp/extractor/wimtv.py\n@@ -11,7 +11,7 @@ class WimTVIE(InfoExtractor):\n     _player = None\n     _UUID_RE = r'[\\da-f]{8}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{4}-[\\da-f]{12}'\n     _VALID_URL = r'''(?x:\n-        https?://platform.wim.tv/\n+        https?://platform\\.wim\\.tv/\n         (?:\n             (?:embed/)?\\?\n             |\\#/webtv/.+?/\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/wordpress.py",
            "diff": "diff --git a/yt_dlp/extractor/wordpress.py b/yt_dlp/extractor/wordpress.py\nindex 53820b57..378d99db 100644\n--- a/yt_dlp/extractor/wordpress.py\n+++ b/yt_dlp/extractor/wordpress.py\n@@ -70,7 +70,7 @@ def _extract_from_webpage(self, url, webpage):\n                 'height': int_or_none(traverse_obj(track, ('dimensions', 'original', 'height'))),\n                 'width': int_or_none(traverse_obj(track, ('dimensions', 'original', 'width'))),\n             } for track in traverse_obj(playlist_json, ('tracks', ...), expected_type=dict)]\n-            yield self.playlist_result(entries, self._generic_id(url) + f'-wp-playlist-{i+1}', 'Wordpress Playlist')\n+            yield self.playlist_result(entries, self._generic_id(url) + f'-wp-playlist-{i + 1}', 'Wordpress Playlist')\n \n \n class WordpressMiniAudioPlayerEmbedIE(InfoExtractor):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/wrestleuniverse.py",
            "diff": "diff --git a/yt_dlp/extractor/wrestleuniverse.py b/yt_dlp/extractor/wrestleuniverse.py\nindex dd12804d..145246a1 100644\n--- a/yt_dlp/extractor/wrestleuniverse.py\n+++ b/yt_dlp/extractor/wrestleuniverse.py\n@@ -190,10 +190,7 @@ class WrestleUniverseVODIE(WrestleUniverseBaseIE):\n     def _real_extract(self, url):\n         lang, video_id = self._match_valid_url(url).group('lang', 'id')\n         metadata = self._download_metadata(url, video_id, lang, 'videoEpisodeFallbackData')\n-        video_data = self._call_api(video_id, ':watch', 'watch', data={\n-            # 'deviceId' is required if ignoreDeviceRestriction is False\n-            'ignoreDeviceRestriction': True,\n-        })\n+        video_data = self._call_api(video_id, ':watch', 'watch', data={'deviceId': self._DEVICE_ID})\n \n         return {\n             'id': video_id,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/xbef.py",
            "diff": "diff --git a/yt_dlp/extractor/xbef.py b/yt_dlp/extractor/xbef.py\ndeleted file mode 100644\nindex ac69528a..00000000\n--- a/yt_dlp/extractor/xbef.py\n+++ /dev/null\n@@ -1,42 +0,0 @@\n-from .common import InfoExtractor\n-from ..compat import compat_urllib_parse_unquote\n-\n-\n-class XBefIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?xbef\\.com/video/(?P<id>[0-9]+)'\n-    _TEST = {\n-        'url': 'http://xbef.com/video/5119-glamourous-lesbians-smoking-drinking-and-fucking',\n-        'md5': 'a478b565baff61634a98f5e5338be995',\n-        'info_dict': {\n-            'id': '5119',\n-            'ext': 'mp4',\n-            'title': 'md5:7358a9faef8b7b57acda7c04816f170e',\n-            'age_limit': 18,\n-            'thumbnail': r're:^http://.*\\.jpg',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        title = self._html_search_regex(\n-            r'<h1[^>]*>(.*?)</h1>', webpage, 'title')\n-\n-        config_url_enc = self._download_webpage(\n-            'http://xbef.com/Main/GetVideoURLEncoded/%s' % video_id, video_id,\n-            note='Retrieving config URL')\n-        config_url = compat_urllib_parse_unquote(config_url_enc)\n-        config = self._download_xml(\n-            config_url, video_id, note='Retrieving config')\n-\n-        video_url = config.find('./file').text\n-        thumbnail = config.find('./image').text\n-\n-        return {\n-            'id': video_id,\n-            'url': video_url,\n-            'title': title,\n-            'thumbnail': thumbnail,\n-            'age_limit': 18,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/xhamster.py",
            "diff": "diff --git a/yt_dlp/extractor/xhamster.py b/yt_dlp/extractor/xhamster.py\nindex 37224799..01ac5ddb 100644\n--- a/yt_dlp/extractor/xhamster.py\n+++ b/yt_dlp/extractor/xhamster.py\n@@ -24,7 +24,7 @@ class XHamsterIE(InfoExtractor):\n     _DOMAINS = r'(?:xhamster\\.(?:com|one|desi)|xhms\\.pro|xhamster\\d+\\.com|xhday\\.com|xhvid\\.com)'\n     _VALID_URL = r'''(?x)\n                     https?://\n-                        (?:.+?\\.)?%s/\n+                        (?:[^/?#]+\\.)?%s/\n                         (?:\n                             movies/(?P<id>[\\dA-Za-z]+)/(?P<display_id>[^/]*)\\.html|\n                             videos/(?P<display_id_2>[^/]*)-(?P<id_2>[\\dA-Za-z]+)\n@@ -372,7 +372,7 @@ def get_height(s):\n \n \n class XHamsterEmbedIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:.+?\\.)?%s/xembed\\.php\\?video=(?P<id>\\d+)' % XHamsterIE._DOMAINS\n+    _VALID_URL = r'https?://(?:[^/?#]+\\.)?%s/xembed\\.php\\?video=(?P<id>\\d+)' % XHamsterIE._DOMAINS\n     _EMBED_REGEX = [r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?xhamster\\.com/xembed\\.php\\?video=\\d+)\\1']\n     _TEST = {\n         'url': 'http://xhamster.com/xembed.php?video=3328539',\n@@ -407,7 +407,7 @@ def _real_extract(self, url):\n \n \n class XHamsterUserIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:.+?\\.)?%s/users/(?P<id>[^/?#&]+)' % XHamsterIE._DOMAINS\n+    _VALID_URL = rf'https?://(?:[^/?#]+\\.)?{XHamsterIE._DOMAINS}/(?:(?P<user>users)|creators)/(?P<id>[^/?#&]+)'\n     _TESTS = [{\n         # Paginated user profile\n         'url': 'https://xhamster.com/users/netvideogirls/videos',\n@@ -422,6 +422,12 @@ class XHamsterUserIE(InfoExtractor):\n             'id': 'firatkaan',\n         },\n         'playlist_mincount': 1,\n+    }, {\n+        'url': 'https://xhamster.com/creators/squirt-orgasm-69',\n+        'info_dict': {\n+            'id': 'squirt-orgasm-69',\n+        },\n+        'playlist_mincount': 150,\n     }, {\n         'url': 'https://xhday.com/users/mobhunter',\n         'only_matching': True,\n@@ -430,8 +436,9 @@ class XHamsterUserIE(InfoExtractor):\n         'only_matching': True,\n     }]\n \n-    def _entries(self, user_id):\n-        next_page_url = 'https://xhamster.com/users/%s/videos/1' % user_id\n+    def _entries(self, user_id, is_user):\n+        prefix, suffix = ('users', 'videos') if is_user else ('creators', 'exclusive')\n+        next_page_url = f'https://xhamster.com/{prefix}/{user_id}/{suffix}/1'\n         for pagenum in itertools.count(1):\n             page = self._download_webpage(\n                 next_page_url, user_id, 'Downloading page %s' % pagenum)\n@@ -454,5 +461,5 @@ def _entries(self, user_id):\n                 break\n \n     def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-        return self.playlist_result(self._entries(user_id), user_id)\n+        user, user_id = self._match_valid_url(url).group('user', 'id')\n+        return self.playlist_result(self._entries(user_id, bool(user)), user_id)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/xtube.py",
            "diff": "diff --git a/yt_dlp/extractor/xtube.py b/yt_dlp/extractor/xtube.py\ndeleted file mode 100644\nindex db829258..00000000\n--- a/yt_dlp/extractor/xtube.py\n+++ /dev/null\n@@ -1,214 +0,0 @@\n-import itertools\n-import re\n-\n-from .common import InfoExtractor\n-from ..networking import Request\n-from ..utils import (\n-    int_or_none,\n-    js_to_json,\n-    orderedSet,\n-    parse_duration,\n-    str_to_int,\n-    url_or_none,\n-)\n-\n-\n-class XTubeIE(InfoExtractor):\n-    _VALID_URL = r'''(?x)\n-                        (?:\n-                            xtube:|\n-                            https?://(?:www\\.)?xtube\\.com/(?:watch\\.php\\?.*\\bv=|video-watch/(?:embedded/)?(?P<display_id>[^/]+)-)\n-                        )\n-                        (?P<id>[^/?&#]+)\n-                    '''\n-\n-    _TESTS = [{\n-        # old URL schema\n-        'url': 'http://www.xtube.com/watch.php?v=kVTUy_G222_',\n-        'md5': '092fbdd3cbe292c920ef6fc6a8a9cdab',\n-        'info_dict': {\n-            'id': 'kVTUy_G222_',\n-            'ext': 'mp4',\n-            'title': 'strange erotica',\n-            'description': 'contains:an ET kind of thing',\n-            'uploader': 'greenshowers',\n-            'duration': 450,\n-            'view_count': int,\n-            'comment_count': int,\n-            'age_limit': 18,\n-        }\n-    }, {\n-        # new URL schema\n-        'url': 'http://www.xtube.com/video-watch/strange-erotica-625837',\n-        'only_matching': True,\n-    }, {\n-        'url': 'xtube:625837',\n-        'only_matching': True,\n-    }, {\n-        'url': 'xtube:kVTUy_G222_',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.xtube.com/video-watch/embedded/milf-tara-and-teen-shared-and-cum-covered-extreme-bukkake-32203482?embedsize=big',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = self._match_valid_url(url)\n-        video_id = mobj.group('id')\n-        display_id = mobj.group('display_id')\n-\n-        if not display_id:\n-            display_id = video_id\n-\n-        if video_id.isdigit() and len(video_id) < 11:\n-            url_pattern = 'http://www.xtube.com/video-watch/-%s'\n-        else:\n-            url_pattern = 'http://www.xtube.com/watch.php?v=%s'\n-\n-        webpage = self._download_webpage(\n-            url_pattern % video_id, display_id, headers={\n-                'Cookie': 'age_verified=1; cookiesAccepted=1',\n-            })\n-\n-        title, thumbnail, duration, sources, media_definition = [None] * 5\n-\n-        config = self._parse_json(self._search_regex(\n-            r'playerConf\\s*=\\s*({.+?})\\s*,\\s*(?:\\n|loaderConf|playerWrapper)', webpage, 'config',\n-            default='{}'), video_id, transform_source=js_to_json, fatal=False)\n-        if config:\n-            config = config.get('mainRoll')\n-            if isinstance(config, dict):\n-                title = config.get('title')\n-                thumbnail = config.get('poster')\n-                duration = int_or_none(config.get('duration'))\n-                sources = config.get('sources') or config.get('format')\n-                media_definition = config.get('mediaDefinition')\n-\n-        if not isinstance(sources, dict) and not media_definition:\n-            sources = self._parse_json(self._search_regex(\n-                r'([\"\\'])?sources\\1?\\s*:\\s*(?P<sources>{.+?}),',\n-                webpage, 'sources', group='sources'), video_id,\n-                transform_source=js_to_json)\n-\n-        formats = []\n-        format_urls = set()\n-\n-        if isinstance(sources, dict):\n-            for format_id, format_url in sources.items():\n-                format_url = url_or_none(format_url)\n-                if not format_url:\n-                    continue\n-                if format_url in format_urls:\n-                    continue\n-                format_urls.add(format_url)\n-                formats.append({\n-                    'url': format_url,\n-                    'format_id': format_id,\n-                    'height': int_or_none(format_id),\n-                })\n-\n-        if isinstance(media_definition, list):\n-            for media in media_definition:\n-                video_url = url_or_none(media.get('videoUrl'))\n-                if not video_url:\n-                    continue\n-                if video_url in format_urls:\n-                    continue\n-                format_urls.add(video_url)\n-                format_id = media.get('format')\n-                if format_id == 'hls':\n-                    formats.extend(self._extract_m3u8_formats(\n-                        video_url, video_id, 'mp4', entry_protocol='m3u8_native',\n-                        m3u8_id='hls', fatal=False))\n-                elif format_id == 'mp4':\n-                    height = int_or_none(media.get('quality'))\n-                    formats.append({\n-                        'url': video_url,\n-                        'format_id': '%s-%d' % (format_id, height) if height else format_id,\n-                        'height': height,\n-                    })\n-\n-        self._remove_duplicate_formats(formats)\n-\n-        if not title:\n-            title = self._search_regex(\n-                (r'<h1>\\s*(?P<title>[^<]+?)\\s*</h1>', r'videoTitle\\s*:\\s*([\"\\'])(?P<title>.+?)\\1'),\n-                webpage, 'title', group='title')\n-        description = self._og_search_description(\n-            webpage, default=None) or self._html_search_meta(\n-            'twitter:description', webpage, default=None) or self._search_regex(\n-            r'</h1>\\s*<p>([^<]+)', webpage, 'description', fatal=False)\n-        uploader = self._search_regex(\n-            (r'<input[^>]+name=\"contentOwnerId\"[^>]+value=\"([^\"]+)\"',\n-             r'<span[^>]+class=\"nickname\"[^>]*>([^<]+)'),\n-            webpage, 'uploader', fatal=False)\n-        if not duration:\n-            duration = parse_duration(self._search_regex(\n-                r'<dt>Runtime:?</dt>\\s*<dd>([^<]+)</dd>',\n-                webpage, 'duration', fatal=False))\n-        view_count = str_to_int(self._search_regex(\n-            (r'[\"\\']viewsCount[\"\\'][^>]*>(\\d+)\\s+views',\n-             r'<dt>Views:?</dt>\\s*<dd>([\\d,\\.]+)</dd>'),\n-            webpage, 'view count', fatal=False))\n-        comment_count = str_to_int(self._html_search_regex(\n-            r'>Comments? \\(([\\d,\\.]+)\\)<',\n-            webpage, 'comment count', fatal=False))\n-\n-        return {\n-            'id': video_id,\n-            'display_id': display_id,\n-            'title': title,\n-            'description': description,\n-            'thumbnail': thumbnail,\n-            'uploader': uploader,\n-            'duration': duration,\n-            'view_count': view_count,\n-            'comment_count': comment_count,\n-            'age_limit': 18,\n-            'formats': formats,\n-        }\n-\n-\n-class XTubeUserIE(InfoExtractor):\n-    IE_DESC = 'XTube user profile'\n-    _VALID_URL = r'https?://(?:www\\.)?xtube\\.com/profile/(?P<id>[^/]+-\\d+)'\n-    _TEST = {\n-        'url': 'http://www.xtube.com/profile/greenshowers-4056496',\n-        'info_dict': {\n-            'id': 'greenshowers-4056496',\n-            'age_limit': 18,\n-        },\n-        'playlist_mincount': 154,\n-    }\n-\n-    def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-\n-        entries = []\n-        for pagenum in itertools.count(1):\n-            request = Request(\n-                'http://www.xtube.com/profile/%s/videos/%d' % (user_id, pagenum),\n-                headers={\n-                    'Cookie': 'popunder=4',\n-                    'X-Requested-With': 'XMLHttpRequest',\n-                    'Referer': url,\n-                })\n-\n-            page = self._download_json(\n-                request, user_id, 'Downloading videos JSON page %d' % pagenum)\n-\n-            html = page.get('html')\n-            if not html:\n-                break\n-\n-            for video_id in orderedSet([video_id for _, video_id in re.findall(\n-                    r'data-plid=([\"\\'])(.+?)\\1', html)]):\n-                entries.append(self.url_result('xtube:%s' % video_id, XTubeIE.ie_key()))\n-\n-            page_count = int_or_none(page.get('pageCount'))\n-            if not page_count or pagenum == page_count:\n-                break\n-\n-        playlist = self.playlist_result(entries, user_id)\n-        playlist['age_limit'] = 18\n-        return playlist\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/xuite.py",
            "diff": "diff --git a/yt_dlp/extractor/xuite.py b/yt_dlp/extractor/xuite.py\ndeleted file mode 100644\nindex 71ddadd4..00000000\n--- a/yt_dlp/extractor/xuite.py\n+++ /dev/null\n@@ -1,149 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import (\n-    ExtractorError,\n-    float_or_none,\n-    get_element_by_attribute,\n-    parse_iso8601,\n-    remove_end,\n-)\n-\n-\n-class XuiteIE(InfoExtractor):\n-    IE_DESC = '\u96a8\u610f\u7aa9Xuite\u5f71\u97f3'\n-    _REGEX_BASE64 = r'(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?'\n-    _VALID_URL = r'https?://vlog\\.xuite\\.net/(?:play|embed)/(?P<id>%s)' % _REGEX_BASE64\n-    _TESTS = [{\n-        # Audio\n-        'url': 'http://vlog.xuite.net/play/RGkzc1ZULTM4NjA5MTQuZmx2',\n-        'md5': 'e79284c87b371424885448d11f6398c8',\n-        'info_dict': {\n-            'id': '3860914',\n-            'ext': 'mp3',\n-            'title': '\u5b64\u55ae\u5357\u534a\u7403-\u6b50\u5fb7\u967d',\n-            'description': '\u5b64\u55ae\u5357\u534a\u7403-\u6b50\u5fb7\u967d',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 247.246,\n-            'timestamp': 1314932940,\n-            'upload_date': '20110902',\n-            'uploader': '\u963f\u80fd',\n-            'uploader_id': '15973816',\n-            'categories': ['\u500b\u4eba\u77ed\u7247'],\n-        },\n-    }, {\n-        # Video with only one format\n-        'url': 'http://vlog.xuite.net/play/WUxxR2xCLTI1OTI1MDk5LmZsdg==',\n-        'md5': '21f7b39c009b5a4615b4463df6eb7a46',\n-        'info_dict': {\n-            'id': '25925099',\n-            'ext': 'mp4',\n-            'title': 'BigBuckBunny_320x180',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 596.458,\n-            'timestamp': 1454242500,\n-            'upload_date': '20160131',\n-            'uploader': '\u5c41\u59e5',\n-            'uploader_id': '12158353',\n-            'categories': ['\u500b\u4eba\u77ed\u7247'],\n-            'description': 'http://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4',\n-        },\n-    }, {\n-        # Video with two formats\n-        'url': 'http://vlog.xuite.net/play/bWo1N1pLLTIxMzAxMTcwLmZsdg==',\n-        'md5': '1166e0f461efe55b62e26a2d2a68e6de',\n-        'info_dict': {\n-            'id': '21301170',\n-            'ext': 'mp4',\n-            'title': '\u6697\u6bba\u6559\u5ba4 02',\n-            'description': '\u5b57\u5e55:\u3010\u6975\u5f71\u5b57\u5e55\u793e\u3011',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-            'duration': 1384.907,\n-            'timestamp': 1421481240,\n-            'upload_date': '20150117',\n-            'uploader': '\u6211\u53ea\u662f\u60f3\u8a8d\u771f\u9ede',\n-            'uploader_id': '242127761',\n-            'categories': ['\u96fb\u73a9\u52d5\u6f2b'],\n-        },\n-        'skip': 'Video removed',\n-    }, {\n-        # Video with encoded media id\n-        # from http://forgetfulbc.blogspot.com/2016/06/date.html\n-        'url': 'http://vlog.xuite.net/embed/cE1xbENoLTI3NDQ3MzM2LmZsdg==?ar=0&as=0',\n-        'info_dict': {\n-            'id': '27447336',\n-            'ext': 'mp4',\n-            'title': '\u7537\u5973\u5e73\u6b0a\u53ea\u662f\u53e3\u865f\uff1f\u5c08\u5bb6\u89e3\u91cb\u7d04\u6703\u6642\u7537\u751f\u662f\u5426\u8a72\u5e6b\u5973\u751f\u4ed8\u9322 (\u4e2d\u5b57)',\n-            'description': 'md5:1223810fa123b179083a3aed53574706',\n-            'timestamp': 1466160960,\n-            'upload_date': '20160617',\n-            'uploader': 'B.C. & Lowy',\n-            'uploader_id': '232279340',\n-        },\n-    }, {\n-        'url': 'http://vlog.xuite.net/play/S1dDUjdyLTMyOTc3NjcuZmx2/%E5%AD%AB%E7%87%95%E5%A7%BF-%E7%9C%BC%E6%B7%9A%E6%88%90%E8%A9%A9',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        # /play/ URLs provide embedded video URL and more metadata\n-        url = url.replace('/embed/', '/play/')\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        error_msg = self._search_regex(\n-            r'<div id=\"error-message-content\">([^<]+)',\n-            webpage, 'error message', default=None)\n-        if error_msg:\n-            raise ExtractorError(\n-                '%s returned error: %s' % (self.IE_NAME, error_msg),\n-                expected=True)\n-\n-        media_info = self._parse_json(self._search_regex(\n-            r'var\\s+mediaInfo\\s*=\\s*({.*});', webpage, 'media info'), video_id)\n-\n-        video_id = media_info['MEDIA_ID']\n-\n-        formats = []\n-        for key in ('html5Url', 'html5HQUrl'):\n-            video_url = media_info.get(key)\n-            if not video_url:\n-                continue\n-            format_id = self._search_regex(\n-                r'\\bq=(.+?)\\b', video_url, 'format id', default=None)\n-            formats.append({\n-                'url': video_url,\n-                'ext': 'mp4' if format_id.isnumeric() else format_id,\n-                'format_id': format_id,\n-                'height': int(format_id) if format_id.isnumeric() else None,\n-            })\n-\n-        timestamp = media_info.get('PUBLISH_DATETIME')\n-        if timestamp:\n-            timestamp = parse_iso8601(timestamp + ' +0800', ' ')\n-\n-        category = media_info.get('catName')\n-        categories = [category] if category else []\n-\n-        uploader = media_info.get('NICKNAME')\n-        uploader_url = None\n-\n-        author_div = get_element_by_attribute('itemprop', 'author', webpage)\n-        if author_div:\n-            uploader = uploader or self._html_search_meta('name', author_div)\n-            uploader_url = self._html_search_regex(\n-                r'<link[^>]+itemprop=\"url\"[^>]+href=\"([^\"]+)\"', author_div,\n-                'uploader URL', fatal=False)\n-\n-        return {\n-            'id': video_id,\n-            'title': media_info['TITLE'],\n-            'description': remove_end(media_info.get('metaDesc'), ' (Xuite \u5f71\u97f3)'),\n-            'thumbnail': media_info.get('ogImageUrl'),\n-            'timestamp': timestamp,\n-            'uploader': uploader,\n-            'uploader_id': media_info.get('MEMBER_ID'),\n-            'uploader_url': uploader_url,\n-            'duration': float_or_none(media_info.get('MEDIA_DURATION'), 1000000),\n-            'categories': categories,\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/yandexvideo.py",
            "diff": "diff --git a/yt_dlp/extractor/yandexvideo.py b/yt_dlp/extractor/yandexvideo.py\nindex 727250ee..4382a568 100644\n--- a/yt_dlp/extractor/yandexvideo.py\n+++ b/yt_dlp/extractor/yandexvideo.py\n@@ -194,7 +194,7 @@ class ZenYandexIE(InfoExtractor):\n             'id': '60c7c443da18892ebfe85ed7',\n             'ext': 'mp4',\n             'title': '\u0412\u041e\u0422 \u042d\u0422\u041e Focus. \u0414\u0435\u0434\u044b \u041c\u043e\u0440\u043e\u0437\u044b \u043d\u0430 \u0433\u0438\u0434\u0440\u043e\u0446\u0438\u043a\u043b\u0430\u0445',\n-            'description': 'md5:f3db3d995763b9bbb7b56d4ccdedea89',\n+            'description': 'md5:8684912f6086f298f8078d4af0e8a600',\n             'thumbnail': 're:^https://avatars.dzeninfra.ru/',\n             'uploader': 'AcademeG DailyStream'\n         },\n@@ -209,7 +209,7 @@ class ZenYandexIE(InfoExtractor):\n             'id': '60c7c443da18892ebfe85ed7',\n             'ext': 'mp4',\n             'title': '\u0412\u041e\u0422 \u042d\u0422\u041e Focus. \u0414\u0435\u0434\u044b \u041c\u043e\u0440\u043e\u0437\u044b \u043d\u0430 \u0433\u0438\u0434\u0440\u043e\u0446\u0438\u043a\u043b\u0430\u0445',\n-            'description': 'md5:f3db3d995763b9bbb7b56d4ccdedea89',\n+            'description': 'md5:8684912f6086f298f8078d4af0e8a600',\n             'thumbnail': r're:^https://avatars\\.dzeninfra\\.ru/',\n             'uploader': 'AcademeG DailyStream',\n             'upload_date': '20191111',\n@@ -258,7 +258,7 @@ def _real_extract(self, url):\n             video_id = self._match_id(redirect)\n             webpage = self._download_webpage(redirect, video_id, note='Redirecting')\n         data_json = self._search_json(\n-            r'data\\s*=', webpage, 'metadata', video_id, contains_pattern=r'{[\"\\']_*serverState_*video.+}')\n+            r'(\"data\"\\s*:|data\\s*=)', webpage, 'metadata', video_id, contains_pattern=r'{[\"\\']_*serverState_*video.+}')\n         serverstate = self._search_regex(r'(_+serverState_+video-site_[^_]+_+)',\n                                          webpage, 'server state').replace('State', 'Settings')\n         uploader = self._search_regex(r'(<a\\s*class=[\"\\']card-channel-link[^\"\\']+[\"\\'][^>]+>)',\n@@ -266,22 +266,25 @@ def _real_extract(self, url):\n         uploader_name = extract_attributes(uploader).get('aria-label')\n         video_json = try_get(data_json, lambda x: x[serverstate]['exportData']['video'], dict)\n         stream_urls = try_get(video_json, lambda x: x['video']['streams'])\n-        formats = []\n+        formats, subtitles = [], {}\n         for s_url in stream_urls:\n             ext = determine_ext(s_url)\n             if ext == 'mpd':\n-                formats.extend(self._extract_mpd_formats(s_url, video_id, mpd_id='dash'))\n+                fmts, subs = self._extract_mpd_formats_and_subtitles(s_url, video_id, mpd_id='dash')\n             elif ext == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(s_url, video_id, 'mp4'))\n+                fmts, subs = self._extract_m3u8_formats_and_subtitles(s_url, video_id, 'mp4')\n+            formats.extend(fmts)\n+            subtitles = self._merge_subtitles(subtitles, subs)\n         return {\n             'id': video_id,\n             'title': video_json.get('title') or self._og_search_title(webpage),\n             'formats': formats,\n+            'subtitles': subtitles,\n             'duration': int_or_none(video_json.get('duration')),\n             'view_count': int_or_none(video_json.get('views')),\n             'timestamp': int_or_none(video_json.get('publicationDate')),\n             'uploader': uploader_name or data_json.get('authorName') or try_get(data_json, lambda x: x['publisher']['name']),\n-            'description': self._og_search_description(webpage) or try_get(data_json, lambda x: x['og']['description']),\n+            'description': video_json.get('description') or self._og_search_description(webpage),\n             'thumbnail': self._og_search_thumbnail(webpage) or try_get(data_json, lambda x: x['og']['imageUrl']),\n         }\n \n@@ -296,6 +299,7 @@ class ZenYandexChannelIE(InfoExtractor):\n             'description': 'md5:a9e5b3c247b7fe29fd21371a428bcf56',\n         },\n         'playlist_mincount': 169,\n+        'skip': 'The page does not exist',\n     }, {\n         'url': 'https://dzen.ru/tok_media',\n         'info_dict': {\n@@ -304,6 +308,7 @@ class ZenYandexChannelIE(InfoExtractor):\n             'description': 'md5:a9e5b3c247b7fe29fd21371a428bcf56',\n         },\n         'playlist_mincount': 169,\n+        'skip': 'The page does not exist',\n     }, {\n         'url': 'https://zen.yandex.ru/id/606fd806cc13cb3c58c05cf5',\n         'info_dict': {\n@@ -318,21 +323,21 @@ class ZenYandexChannelIE(InfoExtractor):\n         'url': 'https://zen.yandex.ru/jony_me',\n         'info_dict': {\n             'id': 'jony_me',\n-            'description': 'md5:a2c62b4ef5cf3e3efb13d25f61f739e1',\n+            'description': 'md5:ce0a5cad2752ab58701b5497835b2cc5',\n             'title': 'JONY ',\n         },\n-        'playlist_count': 20,\n+        'playlist_count': 18,\n     }, {\n         # Test that the playlist extractor finishes extracting when the\n         # channel has more than one page of entries\n         'url': 'https://zen.yandex.ru/tatyanareva',\n         'info_dict': {\n             'id': 'tatyanareva',\n-            'description': 'md5:296b588d60841c3756c9105f237b70c6',\n+            'description': 'md5:40a1e51f174369ec3ba9d657734ac31f',\n             'title': '\u0422\u0430\u0442\u044c\u044f\u043d\u0430 \u0420\u0435\u0432\u0430',\n             'entries': 'maxcount:200',\n         },\n-        'playlist_count': 46,\n+        'playlist_mincount': 46,\n     }, {\n         'url': 'https://dzen.ru/id/606fd806cc13cb3c58c05cf5',\n         'info_dict': {\n@@ -375,7 +380,7 @@ def _real_extract(self, url):\n             item_id = self._match_id(redirect)\n             webpage = self._download_webpage(redirect, item_id, note='Redirecting')\n         data = self._search_json(\n-            r'var\\s+data\\s*=', webpage, 'channel data', item_id, contains_pattern=r'{\\\"__serverState__.+}')\n+            r'(\"data\"\\s*:|data\\s*=)', webpage, 'channel data', item_id, contains_pattern=r'{\\\"__serverState__.+}')\n         server_state_json = traverse_obj(data, lambda k, _: k.startswith('__serverState__'), get_all=False)\n         server_settings_json = traverse_obj(data, lambda k, _: k.startswith('__serverSettings__'), get_all=False)\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/yesjapan.py",
            "diff": "diff --git a/yt_dlp/extractor/yesjapan.py b/yt_dlp/extractor/yesjapan.py\ndeleted file mode 100644\nindex 94e41660..00000000\n--- a/yt_dlp/extractor/yesjapan.py\n+++ /dev/null\n@@ -1,56 +0,0 @@\n-from .common import InfoExtractor\n-from ..networking import HEADRequest\n-from ..utils import get_element_by_attribute, parse_iso8601\n-\n-\n-class YesJapanIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?yesjapan\\.com/video/(?P<slug>[A-Za-z0-9\\-]*)_(?P<id>[A-Za-z0-9]+)\\.html'\n-    _TEST = {\n-        'url': 'http://www.yesjapan.com/video/japanese-in-5-20-wa-and-ga-particle-usages_726497834.html',\n-        'md5': 'f0be416314e5be21a12b499b330c21cf',\n-        'info_dict': {\n-            'id': '726497834',\n-            'title': 'Japanese in 5! #20 - WA And GA Particle Usages',\n-            'description': 'This should clear up some issues most students of Japanese encounter with WA and GA....',\n-            'ext': 'mp4',\n-            'timestamp': 1416391590,\n-            'upload_date': '20141119',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-        title = self._og_search_title(webpage)\n-        video_url = self._og_search_video_url(webpage)\n-        description = self._og_search_description(webpage)\n-        thumbnail = self._og_search_thumbnail(webpage)\n-\n-        timestamp = None\n-        submit_info = get_element_by_attribute('class', 'pm-submit-data', webpage)\n-        if submit_info:\n-            timestamp = parse_iso8601(self._search_regex(\n-                r'datetime=\"([^\"]+)\"', submit_info, 'upload date', fatal=False, default=None))\n-\n-        # attempt to resolve the final URL in order to get a proper extension\n-        redirect_req = HEADRequest(video_url)\n-        req = self._request_webpage(\n-            redirect_req, video_id, note='Resolving final URL', errnote='Could not resolve final URL', fatal=False)\n-        if req:\n-            video_url = req.url\n-\n-        formats = [{\n-            'format_id': 'sd',\n-            'url': video_url,\n-        }]\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'description': description,\n-            'timestamp': timestamp,\n-            'thumbnail': thumbnail,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/yinyuetai.py",
            "diff": "diff --git a/yt_dlp/extractor/yinyuetai.py b/yt_dlp/extractor/yinyuetai.py\ndeleted file mode 100644\nindex b2e3172f..00000000\n--- a/yt_dlp/extractor/yinyuetai.py\n+++ /dev/null\n@@ -1,52 +0,0 @@\n-from .common import InfoExtractor\n-from ..utils import ExtractorError\n-\n-\n-class YinYueTaiIE(InfoExtractor):\n-    IE_NAME = 'yinyuetai:video'\n-    IE_DESC = '\u97f3\u60a6Tai'\n-    _VALID_URL = r'https?://v\\.yinyuetai\\.com/video(?:/h5)?/(?P<id>[0-9]+)'\n-    _TESTS = [{\n-        'url': 'http://v.yinyuetai.com/video/2322376',\n-        'md5': '6e3abe28d38e3a54b591f9f040595ce0',\n-        'info_dict': {\n-            'id': '2322376',\n-            'ext': 'mp4',\n-            'title': '\u5c11\u5973\u65f6\u4ee3_PARTY_Music Video Teaser',\n-            'creator': '\u5c11\u5973\u65f6\u4ee3',\n-            'duration': 25,\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n-        },\n-    }, {\n-        'url': 'http://v.yinyuetai.com/video/h5/2322376',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        info = self._download_json(\n-            'http://ext.yinyuetai.com/main/get-h-mv-info?json=true&videoId=%s' % video_id, video_id,\n-            'Downloading mv info')['videoInfo']['coreVideoInfo']\n-\n-        if info['error']:\n-            raise ExtractorError(info['errorMsg'], expected=True)\n-\n-        formats = [{\n-            'url': format_info['videoUrl'],\n-            'format_id': format_info['qualityLevel'],\n-            'format': format_info.get('qualityLevelName'),\n-            'filesize': format_info.get('fileSize'),\n-            # though URLs ends with .flv, the downloaded files are in fact mp4\n-            'ext': 'mp4',\n-            'tbr': format_info.get('bitrate'),\n-        } for format_info in info['videoUrlModels']]\n-\n-        return {\n-            'id': video_id,\n-            'title': info['videoName'],\n-            'thumbnail': info.get('bigHeadImage'),\n-            'creator': info.get('artistNames'),\n-            'duration': info.get('duration'),\n-            'formats': formats,\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/ynet.py",
            "diff": "diff --git a/yt_dlp/extractor/ynet.py b/yt_dlp/extractor/ynet.py\ndeleted file mode 100644\nindex a7d7371f..00000000\n--- a/yt_dlp/extractor/ynet.py\n+++ /dev/null\n@@ -1,48 +0,0 @@\n-import json\n-import re\n-import urllib.parse\n-\n-from .common import InfoExtractor\n-\n-\n-class YnetIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:.+?\\.)?ynet\\.co\\.il/(?:.+?/)?0,7340,(?P<id>L(?:-[0-9]+)+),00\\.html'\n-    _TESTS = [\n-        {\n-            'url': 'http://hot.ynet.co.il/home/0,7340,L-11659-99244,00.html',\n-            'info_dict': {\n-                'id': 'L-11659-99244',\n-                'ext': 'flv',\n-                'title': '\u05d0\u05d9\u05e9 \u05dc\u05d0 \u05d9\u05d5\u05d3\u05e2 \u05de\u05d0\u05d9\u05e4\u05d4 \u05d1\u05d0\u05e0\u05d5',\n-                'thumbnail': r're:^https?://.*\\.jpg',\n-            }\n-        }, {\n-            'url': 'http://hot.ynet.co.il/home/0,7340,L-8859-84418,00.html',\n-            'info_dict': {\n-                'id': 'L-8859-84418',\n-                'ext': 'flv',\n-                'title': \"\u05e6\u05e4\u05d5: \u05d4\u05e0\u05e9\u05d9\u05e7\u05d4 \u05d4\u05dc\u05d5\u05d4\u05d8\u05ea \u05e9\u05dc \u05ea\u05d5\u05e8\u05d2\u05d9' \u05d5\u05d9\u05d5\u05dc\u05d9\u05d4 \u05e4\u05dc\u05d5\u05d8\u05e7\u05d9\u05df\",\n-                'thumbnail': r're:^https?://.*\\.jpg',\n-            }\n-        }\n-    ]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        webpage = self._download_webpage(url, video_id)\n-\n-        content = urllib.parse.unquote_plus(self._og_search_video_url(webpage))\n-        config = json.loads(self._search_regex(r'config=({.+?})$', content, 'video config'))\n-        f4m_url = config['clip']['url']\n-        title = self._og_search_title(webpage)\n-        m = re.search(r'ynet - HOT -- ([\"\\']+)(?P<title>.+?)\\1', title)\n-        if m:\n-            title = m.group('title')\n-        formats = self._extract_f4m_formats(f4m_url, video_id)\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'formats': formats,\n-            'thumbnail': self._og_search_thumbnail(webpage),\n-        }\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/youku.py",
            "diff": "diff --git a/yt_dlp/extractor/youku.py b/yt_dlp/extractor/youku.py\nindex 7ecd9f18..e3517658 100644\n--- a/yt_dlp/extractor/youku.py\n+++ b/yt_dlp/extractor/youku.py\n@@ -20,7 +20,7 @@ class YoukuIE(InfoExtractor):\n     _VALID_URL = r'''(?x)\n         (?:\n             https?://(\n-                (?:v|player)\\.youku\\.com/(?:v_show/id_|player\\.php/sid/)|\n+                (?:v|play(?:er)?)\\.(?:youku|tudou)\\.com/(?:v_show/id_|player\\.php/sid/)|\n                 video\\.tudou\\.com/v/)|\n             youku:)\n         (?P<id>[A-Za-z0-9]+)(?:\\.html|/v\\.swf|)\n@@ -87,6 +87,19 @@ class YoukuIE(InfoExtractor):\n             'uploader_url': 'https://www.youku.com/profile/index/?uid=UNjU2MzY1MzM1Ng==',\n             'tags': list,\n         },\n+    }, {\n+        'url': 'https://play.tudou.com/v_show/id_XNjAxNjI2OTU3Ng==.html?',\n+        'info_dict': {\n+            'id': 'XNjAxNjI2OTU3Ng',\n+            'ext': 'mp4',\n+            'title': '\u963f\u65af\u5854\u610f\u8bc6\u5230\u54c8\u91cc\u6740\u4e86\u4eba\uff0c\u81ea\u5df1\u88ab\u9a97\u4e86',\n+            'thumbnail': 'https://m.ykimg.com/0541010164F732752794D4D7B70331D1',\n+            'uploader_id': '88758207',\n+            'tags': [],\n+            'uploader_url': 'https://www.youku.com/profile/index/?uid=UMzU1MDMyODI4',\n+            'uploader': '\u82f1\u7f8e\u5267\u573a',\n+            'duration': 72.91,\n+        },\n     }]\n \n     @staticmethod\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/youtube.py",
            "diff": "diff --git a/yt_dlp/extractor/youtube.py b/yt_dlp/extractor/youtube.py\nindex 023d8fd8..88126d11 100644\n--- a/yt_dlp/extractor/youtube.py\n+++ b/yt_dlp/extractor/youtube.py\n@@ -428,7 +428,7 @@ class YoutubeBaseInfoExtractor(InfoExtractor):\n         r'(?:www\\.)?piped\\.adminforge\\.de',\n         r'(?:www\\.)?watch\\.whatevertinfoil\\.de',\n         r'(?:www\\.)?piped\\.qdi\\.fi',\n-        r'(?:www\\.)?piped\\.video',\n+        r'(?:(?:www|cf)\\.)?piped\\.video',\n         r'(?:www\\.)?piped\\.aeong\\.one',\n         r'(?:www\\.)?piped\\.moomoo\\.me',\n         r'(?:www\\.)?piped\\.chauvet\\.pro',\n@@ -902,7 +902,7 @@ def extract_relative_time(relative_time_text):\n         e.g. 'streamed 6 days ago', '5 seconds ago (edited)', 'updated today', '8 yr ago'\n         \"\"\"\n \n-        # XXX: this could be moved to a general function in utils.py\n+        # XXX: this could be moved to a general function in utils/_utils.py\n         # The relative time text strings are roughly the same as what\n         # Javascript's Intl.RelativeTimeFormat function generates.\n         # See: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/RelativeTimeFormat\n@@ -941,7 +941,16 @@ def _parse_time_text(self, text):\n     def _extract_response(self, item_id, query, note='Downloading API JSON', headers=None,\n                           ytcfg=None, check_get_keys=None, ep='browse', fatal=True, api_hostname=None,\n                           default_client='web'):\n-        for retry in self.RetryManager():\n+        raise_for_incomplete = bool(self._configuration_arg('raise_incomplete_data', ie_key=YoutubeIE))\n+        # Incomplete Data should be a warning by default when retries are exhausted, while other errors should be fatal.\n+        icd_retries = iter(self.RetryManager(fatal=raise_for_incomplete))\n+        icd_rm = next(icd_retries)\n+        main_retries = iter(self.RetryManager())\n+        main_rm = next(main_retries)\n+        # Manual retry loop for multiple RetryManagers\n+        # The proper RetryManager MUST be advanced after an error\n+        # and its result MUST be checked if the manager is non fatal\n+        while True:\n             try:\n                 response = self._call_api(\n                     ep=ep, fatal=True, headers=headers,\n@@ -953,7 +962,8 @@ def _extract_response(self, item_id, query, note='Downloading API JSON', headers\n                 if not isinstance(e.cause, network_exceptions):\n                     return self._error_or_warning(e, fatal=fatal)\n                 elif not isinstance(e.cause, HTTPError):\n-                    retry.error = e\n+                    main_rm.error = e\n+                    next(main_retries)\n                     continue\n \n                 first_bytes = e.cause.response.read(512)\n@@ -965,27 +975,32 @@ def _extract_response(self, item_id, query, note='Downloading API JSON', headers\n                     if yt_error:\n                         self._report_alerts([('ERROR', yt_error)], fatal=False)\n                 # Downloading page may result in intermittent 5xx HTTP error\n-                # Sometimes a 404 is also recieved. See: https://github.com/ytdl-org/youtube-dl/issues/28289\n+                # Sometimes a 404 is also received. See: https://github.com/ytdl-org/youtube-dl/issues/28289\n                 # We also want to catch all other network exceptions since errors in later pages can be troublesome\n                 # See https://github.com/yt-dlp/yt-dlp/issues/507#issuecomment-880188210\n                 if e.cause.status not in (403, 429):\n-                    retry.error = e\n+                    main_rm.error = e\n+                    next(main_retries)\n                     continue\n                 return self._error_or_warning(e, fatal=fatal)\n \n             try:\n                 self._extract_and_report_alerts(response, only_once=True)\n             except ExtractorError as e:\n-                # YouTube servers may return errors we want to retry on in a 200 OK response\n+                # YouTube's servers may return errors we want to retry on in a 200 OK response\n                 # See: https://github.com/yt-dlp/yt-dlp/issues/839\n                 if 'unknown error' in e.msg.lower():\n-                    retry.error = e\n+                    main_rm.error = e\n+                    next(main_retries)\n                     continue\n                 return self._error_or_warning(e, fatal=fatal)\n             # Youtube sometimes sends incomplete data\n             # See: https://github.com/ytdl-org/youtube-dl/issues/28194\n             if not traverse_obj(response, *variadic(check_get_keys)):\n-                retry.error = ExtractorError('Incomplete data received', expected=True)\n+                icd_rm.error = ExtractorError('Incomplete data received', expected=True)\n+                should_retry = next(icd_retries, None)\n+                if not should_retry:\n+                    return None\n                 continue\n \n             return response\n@@ -2057,7 +2072,6 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'track': 'Voyeur Girl',\n                 'album': 'it\\'s too much love to know my dear',\n                 'release_date': '20190313',\n-                'release_year': 2019,\n                 'alt_title': 'Voyeur Girl',\n                 'view_count': int,\n                 'playable_in_embed': True,\n@@ -3280,16 +3294,15 @@ def _extract_chapters_from_engagement_panel(self, data, duration):\n                                           chapter_time, chapter_title, duration)\n             for contents in content_list)), [])\n \n-    def _extract_heatmap_from_player_overlay(self, data):\n-        content_list = traverse_obj(data, (\n-            'playerOverlays', 'playerOverlayRenderer', 'decoratedPlayerBarRenderer', 'decoratedPlayerBarRenderer', 'playerBar',\n-            'multiMarkersPlayerBarRenderer', 'markersMap', ..., 'value', 'heatmap', 'heatmapRenderer', 'heatMarkers', {list}))\n-        return next(filter(None, (\n-            traverse_obj(contents, (..., 'heatMarkerRenderer', {\n-                'start_time': ('timeRangeStartMillis', {functools.partial(float_or_none, scale=1000)}),\n-                'end_time': {lambda x: (x['timeRangeStartMillis'] + x['markerDurationMillis']) / 1000},\n-                'value': ('heatMarkerIntensityScoreNormalized', {float_or_none}),\n-            })) for contents in content_list)), None)\n+    def _extract_heatmap(self, data):\n+        return traverse_obj(data, (\n+            'frameworkUpdates', 'entityBatchUpdate', 'mutations',\n+            lambda _, v: v['payload']['macroMarkersListEntity']['markersList']['markerType'] == 'MARKER_TYPE_HEATMAP',\n+            'payload', 'macroMarkersListEntity', 'markersList', 'markers', ..., {\n+                'start_time': ('startMillis', {functools.partial(float_or_none, scale=1000)}),\n+                'end_time': {lambda x: (int(x['startMillis']) + int(x['durationMillis'])) / 1000},\n+                'value': ('intensityScoreNormalized', {float_or_none}),\n+            })) or None\n \n     def _extract_comment(self, comment_renderer, parent=None):\n         comment_id = comment_renderer.get('commentId')\n@@ -4423,7 +4436,7 @@ def process_language(container, base_url, lang_code, sub_name, query):\n                 or self._extract_chapters_from_description(video_description, duration)\n                 or None)\n \n-            info['heatmap'] = self._extract_heatmap_from_player_overlay(initial_data)\n+            info['heatmap'] = self._extract_heatmap(initial_data)\n \n         contents = traverse_obj(\n             initial_data, ('contents', 'twoColumnWatchNextResults', 'results', 'results', 'contents'),\n@@ -4467,14 +4480,13 @@ def process_language(container, base_url, lang_code, sub_name, query):\n                             if mobj:\n                                 info[mobj.group('type') + '_count'] = str_to_int(mobj.group('count'))\n                                 break\n-            sbr_tooltip = try_get(\n-                vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n-            if sbr_tooltip:\n-                like_count, dislike_count = sbr_tooltip.split(' / ')\n-                info.update({\n-                    'like_count': str_to_int(like_count),\n-                    'dislike_count': str_to_int(dislike_count),\n-                })\n+\n+            info['like_count'] = traverse_obj(vpir, (\n+                'videoActions', 'menuRenderer', 'topLevelButtons', ...,\n+                'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                'buttonViewModel', 'accessibilityText', {parse_count}), get_all=False)\n+\n             vcr = traverse_obj(vpir, ('viewCount', 'videoViewCountRenderer'))\n             if vcr:\n                 vc = self._get_count(vcr, 'viewCount')\n@@ -4546,6 +4558,14 @@ def process_language(container, base_url, lang_code, sub_name, query):\n                 self._parse_time_text(self._get_text(vpir, 'dateText'))) or upload_date\n         info['upload_date'] = upload_date\n \n+        if upload_date and live_status not in ('is_live', 'post_live', 'is_upcoming'):\n+            # Newly uploaded videos' HLS formats are potentially problematic and need to be checked\n+            upload_datetime = datetime_from_str(upload_date).replace(tzinfo=datetime.timezone.utc)\n+            if upload_datetime >= datetime_from_str('today-2days'):\n+                for fmt in info['formats']:\n+                    if fmt.get('protocol') == 'm3u8_native':\n+                        fmt['__needs_testing'] = True\n+\n         for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n             v = info.get(s_k)\n             if v:\n@@ -5277,6 +5297,7 @@ def _extract_webpage(self, url, item_id, fatal=True):\n             # See: https://github.com/yt-dlp/yt-dlp/issues/116\n             if not traverse_obj(data, 'contents', 'currentVideoEndpoint', 'onResponseReceivedActions'):\n                 retry.error = ExtractorError('Incomplete yt initial data received')\n+                data = None\n                 continue\n \n         return webpage, data\n@@ -6448,6 +6469,9 @@ def _extract_tab_id_and_name(self, tab, base_url='https://www.youtube.com'):\n     def _has_tab(self, tabs, tab_id):\n         return any(self._extract_tab_id_and_name(tab)[0] == tab_id for tab in tabs)\n \n+    def _empty_playlist(self, item_id, data):\n+        return self.playlist_result([], item_id, **self._extract_metadata_from_tabs(item_id, data))\n+\n     @YoutubeTabBaseInfoExtractor.passthrough_smuggled_data\n     def _real_extract(self, url, smuggled_data):\n         item_id = self._match_id(url)\n@@ -6513,6 +6537,10 @@ def _real_extract(self, url, smuggled_data):\n             selected_tab_id, selected_tab_name = self._extract_tab_id_and_name(selected_tab, url)  # NB: Name may be translated\n             self.write_debug(f'Selected tab: {selected_tab_id!r} ({selected_tab_name}), Requested tab: {original_tab_id!r}')\n \n+            # /about is no longer a tab\n+            if original_tab_id == 'about':\n+                return self._empty_playlist(item_id, data)\n+\n             if not original_tab_id and selected_tab_name:\n                 self.to_screen('Downloading all uploads of the channel. '\n                                'To download only the videos in a specific tab, pass the tab\\'s URL')\n@@ -6525,7 +6553,7 @@ def _real_extract(self, url, smuggled_data):\n                 if not extra_tabs and selected_tab_id != 'videos':\n                     # Channel does not have streams, shorts or videos tabs\n                     if item_id[:2] != 'UC':\n-                        raise ExtractorError('This channel has no uploads', expected=True)\n+                        return self._empty_playlist(item_id, data)\n \n                     # Topic channels don't have /videos. Use the equivalent playlist instead\n                     pl_id = f'UU{item_id[2:]}'\n@@ -6533,7 +6561,7 @@ def _real_extract(self, url, smuggled_data):\n                     try:\n                         data, ytcfg = self._extract_data(pl_url, pl_id, ytcfg=ytcfg, fatal=True, webpage_fatal=True)\n                     except ExtractorError:\n-                        raise ExtractorError('This channel has no uploads', expected=True)\n+                        return self._empty_playlist(item_id, data)\n                     else:\n                         item_id, url = pl_id, pl_url\n                         self.to_screen(\n@@ -6665,7 +6693,7 @@ class YoutubePlaylistIE(InfoExtractor):\n             'uploader_url': 'https://www.youtube.com/@milan5503',\n             'availability': 'public',\n         },\n-        'expected_warnings': [r'[Uu]navailable videos? (is|are|will be) hidden'],\n+        'expected_warnings': [r'[Uu]navailable videos? (is|are|will be) hidden', 'Retrying', 'Giving up'],\n     }, {\n         'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n         'playlist_mincount': 455,\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/zaiko.py",
            "diff": "diff --git a/yt_dlp/extractor/zaiko.py b/yt_dlp/extractor/zaiko.py\nindex 0ccacbb6..2b6221da 100644\n--- a/yt_dlp/extractor/zaiko.py\n+++ b/yt_dlp/extractor/zaiko.py\n@@ -9,6 +9,7 @@\n     traverse_obj,\n     try_call,\n     unescapeHTML,\n+    url_basename,\n     url_or_none,\n )\n \n@@ -45,12 +46,14 @@ class ZaikoIE(ZaikoBaseIE):\n             'uploader_id': '454',\n             'uploader': 'ZAIKO ZERO',\n             'release_timestamp': 1583809200,\n-            'thumbnail': r're:https://[a-z0-9]+.cloudfront.net/[a-z0-9_]+/[a-z0-9_]+',\n+            'thumbnail': r're:^https://[\\w.-]+/\\w+/\\w+',\n+            'thumbnails': 'maxcount:2',\n             'release_date': '20200310',\n             'categories': ['Tech House'],\n             'live_status': 'was_live',\n         },\n         'params': {'skip_download': 'm3u8'},\n+        'skip': 'Your account does not have tickets to this event',\n     }]\n \n     def _real_extract(self, url):\n@@ -83,6 +86,12 @@ def _real_extract(self, url):\n         if not formats:\n             self.raise_no_formats(msg, expected=expected)\n \n+        thumbnail_urls = [\n+            traverse_obj(player_meta, ('initial_event_info', 'poster_url')),\n+            self._og_search_thumbnail(self._download_webpage(\n+                f'https://zaiko.io/event/{video_id}', video_id, 'Downloading event page', fatal=False) or ''),\n+        ]\n+\n         return {\n             'id': video_id,\n             'formats': formats,\n@@ -96,8 +105,8 @@ def _real_extract(self, url):\n             }),\n             **traverse_obj(player_meta, ('initial_event_info', {\n                 'alt_title': ('title', {str}),\n-                'thumbnail': ('poster_url', {url_or_none}),\n             })),\n+            'thumbnails': [{'url': url, 'id': url_basename(url)} for url in thumbnail_urls if url_or_none(url)]\n         }\n \n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/zee5.py",
            "diff": "diff --git a/yt_dlp/extractor/zee5.py b/yt_dlp/extractor/zee5.py\nindex b4734cc8..ca79cf0a 100644\n--- a/yt_dlp/extractor/zee5.py\n+++ b/yt_dlp/extractor/zee5.py\n@@ -133,8 +133,8 @@ def _perform_login(self, username, password):\n     def _real_extract(self, url):\n         video_id, display_id = self._match_valid_url(url).group('id', 'display_id')\n         access_token_request = self._download_json(\n-            'https://useraction.zee5.com/token/platform_tokens.php?platform_name=web_app',\n-            video_id, note='Downloading access token')\n+            'https://launchapi.zee5.com/launch?platform_name=web_app',\n+            video_id, note='Downloading access token')['platform_token']\n         data = {\n             'x-access-token': access_token_request['token']\n         }\n@@ -240,8 +240,8 @@ class Zee5SeriesIE(InfoExtractor):\n \n     def _entries(self, show_id):\n         access_token_request = self._download_json(\n-            'https://useraction.zee5.com/token/platform_tokens.php?platform_name=web_app',\n-            show_id, note='Downloading access token')\n+            'https://launchapi.zee5.com/launch?platform_name=web_app',\n+            show_id, note='Downloading access token')['platform_token']\n         headers = {\n             'X-Access-Token': access_token_request['token'],\n             'Referer': 'https://www.zee5.com/',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/zingmp3.py",
            "diff": "diff --git a/yt_dlp/extractor/zingmp3.py b/yt_dlp/extractor/zingmp3.py\nindex 007658c6..f664d88d 100644\n--- a/yt_dlp/extractor/zingmp3.py\n+++ b/yt_dlp/extractor/zingmp3.py\n@@ -5,7 +5,15 @@\n import urllib.parse\n \n from .common import InfoExtractor\n-from ..utils import int_or_none, traverse_obj, try_call, urljoin\n+from ..utils import (\n+    ExtractorError,\n+    int_or_none,\n+    join_nonempty,\n+    try_call,\n+    urljoin,\n+    url_or_none\n+)\n+from ..utils.traversal import traverse_obj\n \n \n class ZingMp3BaseIE(InfoExtractor):\n@@ -20,9 +28,17 @@ class ZingMp3BaseIE(InfoExtractor):\n         'video-clip': '/api/v2/page/get/video',\n         'lyric': '/api/v2/lyric/get/lyric',\n         'song-streaming': '/api/v2/song/get/streaming',\n+        'liveradio': '/api/v2/livestream/get/info',\n+        'eps': '/api/v2/page/get/podcast-episode',\n+        'episode-streaming': '/api/v2/podcast/episode/get/streaming',\n         # Playlist\n         'playlist': '/api/v2/page/get/playlist',\n         'album': '/api/v2/page/get/playlist',\n+        'pgr': '/api/v2/page/get/podcast-program',\n+        'pgr-list': '/api/v2/podcast/episode/get/list',\n+        'cgr': '/api/v2/page/get/podcast-category',\n+        'cgr-list': '/api/v2/podcast/program/get/list-by-cate',\n+        'cgrs': '/api/v2/page/get/podcast-categories',\n         # Chart\n         'zing-chart': '/api/v2/page/get/chart-home',\n         'zing-chart-tuan': '/api/v2/page/get/week-chart',\n@@ -33,6 +49,10 @@ class ZingMp3BaseIE(InfoExtractor):\n         'user-list-song': '/api/v2/song/get/list',\n         'user-list-video': '/api/v2/video/get/list',\n         'hub': '/api/v2/page/get/hub-detail',\n+        'new-release': '/api/v2/chart/get/new-release',\n+        'top100': '/api/v2/page/get/top-100',\n+        'podcast-new': '/api/v2/podcast/program/get/list-by-type',\n+        'top-podcast': '/api/v2/podcast/program/get/top-episode',\n     }\n \n     def _api_url(self, url_type, params):\n@@ -78,7 +98,7 @@ def _paged_list(self, _id, url_type):\n \n \n class ZingMp3IE(ZingMp3BaseIE):\n-    _VALID_URL = ZingMp3BaseIE._VALID_URL_TMPL % 'bai-hat|video-clip|embed'\n+    _VALID_URL = ZingMp3BaseIE._VALID_URL_TMPL % 'bai-hat|video-clip|embed|eps'\n     IE_NAME = 'zingmp3'\n     IE_DESC = 'zingmp3.vn'\n     _TESTS = [{\n@@ -102,7 +122,7 @@ class ZingMp3IE(ZingMp3BaseIE):\n         },\n     }, {\n         'url': 'https://zingmp3.vn/video-clip/Suong-Hoa-Dua-Loi-K-ICM-RYO/ZO8ZF7C7.html',\n-        'md5': '3c2081e79471a2f4a3edd90b70b185ea',\n+        'md5': '92c6e7a019f06b4682a6c35ae5785fab',\n         'info_dict': {\n             'id': 'ZO8ZF7C7',\n             'title': 'S\u01b0\u01a1ng Hoa \u0110\u01b0a L\u1ed1i',\n@@ -128,6 +148,20 @@ class ZingMp3IE(ZingMp3BaseIE):\n             'album': 'Ng\u01b0\u1eddi Y\u00eau T\u00f4i L\u1ea1nh L\u00f9ng S\u1eaft \u0110\u00e1 (Single)',\n             'album_artist': 'Mr. Siro',\n         },\n+    }, {\n+        'url': 'https://zingmp3.vn/eps/Cham-x-Ban-Noi-Goi-La-Nha/ZZD9ACWI.html',\n+        'md5': 'd52f9f63e2631e004e4f15188eedcf80',\n+        'info_dict': {\n+            'id': 'ZZD9ACWI',\n+            'title': 'Ch\u1ea1m x B\u1ea1n - N\u01a1i G\u1ecdi L\u00e0 Nh\u00e0',\n+            'ext': 'mp3',\n+            'duration': 3716,\n+            'thumbnail': r're:^https?://.+\\.jpg',\n+            'track': 'Ch\u1ea1m x B\u1ea1n - N\u01a1i G\u1ecdi L\u00e0 Nh\u00e0',\n+            'artist': 'On Air',\n+            'album': 'Top Podcast',\n+            'album_artist': 'On Air',\n+        },\n     }, {\n         'url': 'https://zingmp3.vn/embed/song/ZWZEI76B?start=false',\n         'only_matching': True,\n@@ -147,6 +181,8 @@ def _real_extract(self, url):\n                 'http://api.mp3.zing.vn/api/mobile/video/getvideoinfo', item_id,\n                 query={'requestdata': json.dumps({'id': item_id})},\n                 note='Downloading mp4 JSON metadata').get('source')\n+        elif url_type == 'eps':\n+            source = self._call_api('episode-streaming', {'id': item_id})\n         else:\n             source = self._call_api('song-streaming', {'id': item_id})\n \n@@ -189,9 +225,10 @@ def _real_extract(self, url):\n             'thumbnail': traverse_obj(item, 'thumbnail', 'thumbnailM'),\n             'duration': int_or_none(item.get('duration')),\n             'track': traverse_obj(item, 'title', 'alias'),\n-            'artist': traverse_obj(item, 'artistsNames', 'artists_names'),\n-            'album': traverse_obj(item, ('album', ('name', 'title')), get_all=False),\n-            'album_artist': traverse_obj(item, ('album', ('artistsNames', 'artists_names')), get_all=False),\n+            'artist': traverse_obj(item, 'artistsNames', 'artists_names', ('artists', 0, 'name')),\n+            'album': traverse_obj(item, ('album', ('name', 'title')), ('genres', 0, 'name'), get_all=False),\n+            'album_artist': traverse_obj(item, ('album', ('artistsNames', 'artists_names')),\n+                                         ('artists', 0, 'name'), get_all=False),\n             'formats': formats,\n             'subtitles': {'origin': [{'url': lyric}]} if lyric else None,\n         }\n@@ -200,12 +237,12 @@ def _real_extract(self, url):\n class ZingMp3AlbumIE(ZingMp3BaseIE):\n     _VALID_URL = ZingMp3BaseIE._VALID_URL_TMPL % 'album|playlist'\n     _TESTS = [{\n-        'url': 'http://mp3.zing.vn/album/Lau-Dai-Tinh-Ai-Bang-Kieu-Minh-Tuyet/ZWZBWDAF.html',\n+        'url': 'https://zingmp3.vn/album/Ca-Phe-Quan-Quen-Hoang-Dung-My-Anh-Da-LAB-Thinh-Suy/ZOC7WUZC.html',\n         'info_dict': {\n-            'id': 'ZWZBWDAF',\n-            'title': 'L\u00e2u \u0110\u00e0i T\u00ecnh \u00c1i',\n+            'id': 'ZOC7WUZC',\n+            'title': 'C\u00e0 Ph\u00ea Qu\u00e1n Quen',\n         },\n-        'playlist_mincount': 9,\n+        'playlist_mincount': 10,\n     }, {\n         'url': 'https://zingmp3.vn/album/Nhung-Bai-Hat-Hay-Nhat-Cua-Mr-Siro-Mr-Siro/ZWZAEZZD.html',\n         'info_dict': {\n@@ -231,7 +268,7 @@ def _real_extract(self, url):\n \n \n class ZingMp3ChartHomeIE(ZingMp3BaseIE):\n-    _VALID_URL = r'https?://(?:mp3\\.zing|zingmp3)\\.vn/(?P<id>(?:zing-chart|moi-phat-hanh))/?(?:[#?]|$)'\n+    _VALID_URL = r'https?://(?:mp3\\.zing|zingmp3)\\.vn/(?P<id>(?:zing-chart|moi-phat-hanh|top100|podcast-discover))/?(?:[#?]|$)'\n     _TESTS = [{\n         'url': 'https://zingmp3.vn/zing-chart',\n         'info_dict': {\n@@ -244,13 +281,34 @@ class ZingMp3ChartHomeIE(ZingMp3BaseIE):\n             'id': 'moi-phat-hanh',\n         },\n         'playlist_mincount': 100,\n+    }, {\n+        'url': 'https://zingmp3.vn/top100',\n+        'info_dict': {\n+            'id': 'top100',\n+        },\n+        'playlist_mincount': 50,\n+    }, {\n+        'url': 'https://zingmp3.vn/podcast-discover',\n+        'info_dict': {\n+            'id': 'podcast-discover',\n+        },\n+        'playlist_mincount': 4,\n     }]\n     IE_NAME = 'zingmp3:chart-home'\n \n     def _real_extract(self, url):\n         url_type = self._match_id(url)\n-        data = self._call_api(url_type, {'id': url_type})\n-        items = traverse_obj(data, ('RTChart', 'items') if url_type == 'zing-chart' else 'items')\n+        params = {'id': url_type}\n+        if url_type == 'podcast-discover':\n+            params['type'] = 'discover'\n+        data = self._call_api(url_type, params)\n+        items = []\n+        if url_type == 'top100':\n+            items.extend(traverse_obj(data, (..., 'items', ..., {dict})))\n+        elif url_type == 'zing-chart':\n+            items.extend(traverse_obj(data, ('RTChart', 'items', ..., {dict})))\n+        else:\n+            items.extend(traverse_obj(data, ('items', ..., {dict})))\n         return self.playlist_result(self._parse_items(items), url_type)\n \n \n@@ -334,7 +392,7 @@ def _real_extract(self, url):\n \n \n class ZingMp3UserIE(ZingMp3BaseIE):\n-    _VALID_URL = r'https?://(?:mp3\\.zing|zingmp3)\\.vn/(?P<user>[^/]+)/(?P<type>bai-hat|single|album|video)/?(?:[?#]|$)'\n+    _VALID_URL = r'https?://(?:mp3\\.zing|zingmp3)\\.vn/(?P<user>[^/]+)/(?P<type>bai-hat|single|album|video|song)/?(?:[?#]|$)'\n     IE_NAME = 'zingmp3:user'\n     _TESTS = [{\n         'url': 'https://zingmp3.vn/Mr-Siro/bai-hat',\n@@ -368,6 +426,18 @@ class ZingMp3UserIE(ZingMp3BaseIE):\n             'description': 'md5:5bdcf45e955dc1b8d7f518f322ffef36',\n         },\n         'playlist_mincount': 15,\n+    }, {\n+        'url': 'https://zingmp3.vn/new-release/song',\n+        'info_dict': {\n+            'id': 'new-release-song',\n+        },\n+        'playlist_mincount': 50,\n+    }, {\n+        'url': 'https://zingmp3.vn/new-release/album',\n+        'info_dict': {\n+            'id': 'new-release-album',\n+        },\n+        'playlist_mincount': 20,\n     }]\n \n     def _fetch_page(self, user_id, url_type, page):\n@@ -380,20 +450,28 @@ def _fetch_page(self, user_id, url_type, page):\n         })\n \n     def _real_extract(self, url):\n-        user_alias, url_type = self._match_valid_url(url).group('user', 'type')\n+        alias, url_type = self._match_valid_url(url).group('user', 'type')\n         if not url_type:\n             url_type = 'bai-hat'\n \n-        user_info = self._call_api('info-artist', {}, user_alias, query={'alias': user_alias})\n-        if url_type in ('bai-hat', 'video'):\n-            entries = self._paged_list(user_info['id'], url_type)\n+        user_info = self._call_api('info-artist', {}, alias, query={'alias': alias})\n+\n+        # Handle for new-release\n+        if alias == 'new-release' and url_type in ('song', 'album'):\n+            _id = f'{alias}-{url_type}'\n+            return self.playlist_result(self._parse_items(\n+                self._call_api('new-release', params={'type': url_type}, display_id=_id)), _id)\n         else:\n-            entries = self._parse_items(traverse_obj(user_info, (\n-                'sections',\n-                lambda _, v: v['sectionId'] == 'aAlbum' if url_type == 'album' else v['sectionId'] == 'aSingle',\n-                'items', ...)))\n-        return self.playlist_result(\n-            entries, user_info['id'], f'{user_info.get(\"name\")} - {url_type}', user_info.get('biography'))\n+            # Handle for user/artist\n+            if url_type in ('bai-hat', 'video'):\n+                entries = self._paged_list(user_info['id'], url_type)\n+            else:\n+                section_id = 'aAlbum' if url_type == 'album' else 'aSingle'\n+                entries = self._parse_items(traverse_obj(user_info, (\n+                    'sections', lambda _, v: v['sectionId'] == section_id, 'items', ...)))\n+            return self.playlist_result(\n+                entries, user_info['id'], join_nonempty(user_info.get('name'), url_type, delim=' - '),\n+                user_info.get('biography'))\n \n \n class ZingMp3HubIE(ZingMp3BaseIE):\n@@ -403,7 +481,7 @@ class ZingMp3HubIE(ZingMp3BaseIE):\n         'url': 'https://zingmp3.vn/hub/Nhac-Moi/IWZ9Z0CA.html',\n         'info_dict': {\n             'id': 'IWZ9Z0CA',\n-            'title': 'Nh\u1ea1c M\u1edbi',\n+            'title': 'BXH Nh\u1ea1c M\u1edbi',\n             'description': 'md5:1cc31b68a6f746427b07b2756c22a558',\n         },\n         'playlist_mincount': 20,\n@@ -424,3 +502,129 @@ def _real_extract(self, url):\n             'sections', lambda _, v: v['sectionId'] == 'hub', 'items', ...)))\n         return self.playlist_result(\n             entries, song_id, hub_detail.get('title'), hub_detail.get('description'))\n+\n+\n+class ZingMp3LiveRadioIE(ZingMp3BaseIE):\n+    IE_NAME = 'zingmp3:liveradio'\n+    _VALID_URL = r'https?://(?:mp3\\.zing|zingmp3)\\.vn/(?P<type>(?:liveradio))/(?P<id>\\w+)(?:\\.html|\\?)'\n+    _TESTS = [{\n+        'url': 'https://zingmp3.vn/liveradio/IWZ979UB.html',\n+        'info_dict': {\n+            'id': 'IWZ979UB',\n+            'title': r're:^V\\-POP',\n+            'description': 'md5:aa857f8a91dc9ce69e862a809e4bdc10',\n+            'protocol': 'm3u8_native',\n+            'ext': 'mp4',\n+            'view_count': int,\n+            'thumbnail': r're:^https?://.*\\.jpg',\n+            'like_count': int,\n+            'live_status': 'is_live',\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        'url': 'https://zingmp3.vn/liveradio/IWZ97CWB.html',\n+        'info_dict': {\n+            'id': 'IWZ97CWB',\n+            'title': r're:^Live\\s247',\n+            'description': 'md5:d41d8cd98f00b204e9800998ecf8427e',\n+            'protocol': 'm3u8_native',\n+            'ext': 'm4a',\n+            'view_count': int,\n+            'thumbnail': r're:^https?://.*\\.jpg',\n+            'like_count': int,\n+            'live_status': 'is_live',\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        url_type, live_radio_id = self._match_valid_url(url).group('type', 'id')\n+        info = self._call_api(url_type, {'id': live_radio_id})\n+        manifest_url = info.get('streaming')\n+        if not manifest_url:\n+            raise ExtractorError('This radio is offline.', expected=True)\n+        fmts, subtitles = self._extract_m3u8_formats_and_subtitles(manifest_url, live_radio_id, fatal=False)\n+        return {\n+            'id': live_radio_id,\n+            'is_live': True,\n+            'formats': fmts,\n+            'subtitles': subtitles,\n+            **traverse_obj(info, {\n+                'title': 'title',\n+                'thumbnail': (('thumbnail', 'thumbnailM', 'thumbnailV', 'thumbnailH'), {url_or_none}),\n+                'view_count': ('activeUsers', {int_or_none}),\n+                'like_count': ('totalReaction', {int_or_none}),\n+                'description': 'description',\n+            }, get_all=False),\n+        }\n+\n+\n+class ZingMp3PodcastEpisodeIE(ZingMp3BaseIE):\n+    IE_NAME = 'zingmp3:podcast-episode'\n+    _VALID_URL = ZingMp3BaseIE._VALID_URL_TMPL % 'pgr|cgr'\n+    _TESTS = [{\n+        'url': 'https://zingmp3.vn/pgr/Nhac-Moi-Moi-Ngay/68Z9W66B.html',\n+        'info_dict': {\n+            'id': '68Z9W66B',\n+            'title': 'Nh\u1ea1c M\u1edbi M\u1ed7i Ng\u00e0y',\n+            'description': 'md5:2875dfa951f8e5356742f1610cf20691'\n+        },\n+        'playlist_mincount': 20,\n+    }, {\n+        'url': 'https://zingmp3.vn/cgr/Am-nhac/IWZ980AO.html',\n+        'info_dict': {\n+            'id': 'IWZ980AO',\n+            'title': '\u00c2m nh\u1ea1c'\n+        },\n+        'playlist_mincount': 2,\n+    }]\n+\n+    def _fetch_page(self, eps_id, url_type, page):\n+        return self._call_api(url_type, {\n+            'id': eps_id,\n+            'page': page,\n+            'count': self._PER_PAGE\n+        })\n+\n+    def _real_extract(self, url):\n+        podcast_id, url_type = self._match_valid_url(url).group('id', 'type')\n+        podcast_info = self._call_api(url_type, {'id': podcast_id})\n+        entries = self._paged_list(podcast_id, 'pgr-list' if url_type == 'pgr' else 'cgr-list')\n+        return self.playlist_result(\n+            entries, podcast_id, podcast_info.get('title'), podcast_info.get('description'))\n+\n+\n+class ZingMp3PodcastIE(ZingMp3BaseIE):\n+    IE_NAME = 'zingmp3:podcast'\n+    _VALID_URL = r'https?://(?:mp3\\.zing|zingmp3)\\.vn/(?P<id>(?:cgr|top-podcast|podcast-new))/?(?:[#?]|$)'\n+    _TESTS = [{\n+        'url': 'https://zingmp3.vn/cgr',\n+        'info_dict': {\n+            'id': 'cgr',\n+        },\n+        'playlist_mincount': 5,\n+    }, {\n+        'url': 'https://zingmp3.vn/top-podcast',\n+        'info_dict': {\n+            'id': 'top-podcast',\n+        },\n+        'playlist_mincount': 7,\n+    }, {\n+        'url': 'https://zingmp3.vn/podcast-new',\n+        'info_dict': {\n+            'id': 'podcast-new',\n+        },\n+        'playlist_mincount': 4,\n+    }]\n+\n+    def _real_extract(self, url):\n+        url_type = self._match_id(url)\n+        params = {'id': url_type}\n+        if url_type == 'podcast-new':\n+            params['type'] = 'new'\n+        items = self._call_api('cgrs' if url_type == 'cgr' else url_type, params)['items']\n+        return self.playlist_result(self._parse_items(items), url_type)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/extractor/zoom.py",
            "diff": "diff --git a/yt_dlp/extractor/zoom.py b/yt_dlp/extractor/zoom.py\nindex 3d7ccca7..e2bf8172 100644\n--- a/yt_dlp/extractor/zoom.py\n+++ b/yt_dlp/extractor/zoom.py\n@@ -2,10 +2,12 @@\n from ..utils import (\n     ExtractorError,\n     int_or_none,\n-    str_or_none,\n     js_to_json,\n     parse_filesize,\n+    parse_resolution,\n+    str_or_none,\n     traverse_obj,\n+    url_basename,\n     urlencode_postdata,\n     urljoin,\n )\n@@ -13,7 +15,7 @@\n \n class ZoomIE(InfoExtractor):\n     IE_NAME = 'zoom'\n-    _VALID_URL = r'(?P<base_url>https?://(?:[^.]+\\.)?zoom.us/)rec(?:ording)?/(?P<type>play|share)/(?P<id>[A-Za-z0-9_.-]+)'\n+    _VALID_URL = r'(?P<base_url>https?://(?:[^.]+\\.)?zoom\\.us/)rec(?:ording)?/(?P<type>play|share)/(?P<id>[\\w.-]+)'\n     _TESTS = [{\n         'url': 'https://economist.zoom.us/rec/play/dUk_CNBETmZ5VA2BwEl-jjakPpJ3M1pcfVYAPRsoIbEByGsLjUZtaa4yCATQuOL3der8BlTwxQePl_j0.EImBkXzTIaPvdZO5',\n         'md5': 'ab445e8c911fddc4f9adc842c2c5d434',\n@@ -41,6 +43,18 @@ class ZoomIE(InfoExtractor):\n             'ext': 'mp4',\n             'title': 'Timea Andrea Lelik\\'s Personal Meeting Room',\n         },\n+        'skip': 'This recording has expired',\n+    }, {\n+        # view_with_share URL\n+        'url': 'https://cityofdetroit.zoom.us/rec/share/VjE-5kW3xmgbEYqR5KzRgZ1OFZvtMtiXk5HyRJo5kK4m5PYE6RF4rF_oiiO_9qaM.UTAg1MI7JSnF3ZjX',\n+        'md5': 'bdc7867a5934c151957fb81321b3c024',\n+        'info_dict': {\n+            'id': 'VjE-5kW3xmgbEYqR5KzRgZ1OFZvtMtiXk5HyRJo5kK4m5PYE6RF4rF_oiiO_9qaM.UTAg1MI7JSnF3ZjX',\n+            'ext': 'mp4',\n+            'title': 'February 2022 Detroit Revenue Estimating Conference',\n+            'duration': 7299,\n+            'formats': 'mincount:3',\n+        },\n     }]\n \n     def _get_page_data(self, webpage, video_id):\n@@ -72,6 +86,7 @@ def _get_real_webpage(self, url, base_url, video_id, url_type):\n \n     def _real_extract(self, url):\n         base_url, url_type, video_id = self._match_valid_url(url).group('base_url', 'type', 'id')\n+        query = {}\n \n         if url_type == 'share':\n             webpage = self._get_real_webpage(url, base_url, video_id, 'share')\n@@ -80,6 +95,7 @@ def _real_extract(self, url):\n                 f'{base_url}nws/recording/1.0/play/share-info/{meeting_id}',\n                 video_id, note='Downloading share info JSON')['result']['redirectUrl']\n             url = urljoin(base_url, redirect_path)\n+            query['continueMode'] = 'true'\n \n         webpage = self._get_real_webpage(url, base_url, video_id, 'play')\n         file_id = self._get_page_data(webpage, video_id)['fileId']\n@@ -88,7 +104,7 @@ def _real_extract(self, url):\n             raise ExtractorError('Unable to extract file ID')\n \n         data = self._download_json(\n-            f'{base_url}nws/recording/1.0/play/info/{file_id}', video_id,\n+            f'{base_url}nws/recording/1.0/play/info/{file_id}', video_id, query=query,\n             note='Downloading play info JSON')['result']\n \n         subtitles = {}\n@@ -104,10 +120,10 @@ def _real_extract(self, url):\n         if data.get('viewMp4Url'):\n             formats.append({\n                 'format_note': 'Camera stream',\n-                'url': str_or_none(data.get('viewMp4Url')),\n+                'url': data['viewMp4Url'],\n                 'width': int_or_none(traverse_obj(data, ('viewResolvtions', 0))),\n                 'height': int_or_none(traverse_obj(data, ('viewResolvtions', 1))),\n-                'format_id': str_or_none(traverse_obj(data, ('recording', 'id'))),\n+                'format_id': 'view',\n                 'ext': 'mp4',\n                 'filesize_approx': parse_filesize(str_or_none(traverse_obj(data, ('recording', 'fileSizeInMB')))),\n                 'preference': 0\n@@ -116,17 +132,30 @@ def _real_extract(self, url):\n         if data.get('shareMp4Url'):\n             formats.append({\n                 'format_note': 'Screen share stream',\n-                'url': str_or_none(data.get('shareMp4Url')),\n+                'url': data['shareMp4Url'],\n                 'width': int_or_none(traverse_obj(data, ('shareResolvtions', 0))),\n                 'height': int_or_none(traverse_obj(data, ('shareResolvtions', 1))),\n-                'format_id': str_or_none(traverse_obj(data, ('shareVideo', 'id'))),\n+                'format_id': 'share',\n                 'ext': 'mp4',\n                 'preference': -1\n             })\n \n+        view_with_share_url = data.get('viewMp4WithshareUrl')\n+        if view_with_share_url:\n+            formats.append({\n+                **parse_resolution(self._search_regex(\n+                    r'_(\\d+x\\d+)\\.mp4', url_basename(view_with_share_url), 'resolution', default=None)),\n+                'format_note': 'Screen share with camera',\n+                'url': view_with_share_url,\n+                'format_id': 'view_with_share',\n+                'ext': 'mp4',\n+                'preference': 1\n+            })\n+\n         return {\n             'id': video_id,\n             'title': str_or_none(traverse_obj(data, ('meet', 'topic'))),\n+            'duration': int_or_none(data.get('duration')),\n             'subtitles': subtitles,\n             'formats': formats,\n             'http_headers': {\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/__init__.py",
            "diff": "diff --git a/yt_dlp/networking/__init__.py b/yt_dlp/networking/__init__.py\nindex 5e887648..acadc014 100644\n--- a/yt_dlp/networking/__init__.py\n+++ b/yt_dlp/networking/__init__.py\n@@ -1,4 +1,6 @@\n-# flake8: noqa: 401\n+# flake8: noqa: F401\n+import warnings\n+\n from .common import (\n     HEADRequest,\n     PUTRequest,\n@@ -11,3 +13,18 @@\n # isort: split\n # TODO: all request handlers should be safely imported\n from . import _urllib\n+from ..utils import bug_reports_message\n+\n+try:\n+    from . import _requests\n+except ImportError:\n+    pass\n+except Exception as e:\n+    warnings.warn(f'Failed to import \"requests\" request handler: {e}' + bug_reports_message())\n+\n+try:\n+    from . import _websockets\n+except ImportError:\n+    pass\n+except Exception as e:\n+    warnings.warn(f'Failed to import \"websockets\" request handler: {e}' + bug_reports_message())\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/_helper.py",
            "diff": "diff --git a/yt_dlp/networking/_helper.py b/yt_dlp/networking/_helper.py\nindex a43c57bb..d79dd795 100644\n--- a/yt_dlp/networking/_helper.py\n+++ b/yt_dlp/networking/_helper.py\n@@ -2,6 +2,7 @@\n \n import contextlib\n import functools\n+import socket\n import ssl\n import sys\n import typing\n@@ -10,7 +11,7 @@\n \n from .exceptions import RequestError, UnsupportedRequest\n from ..dependencies import certifi\n-from ..socks import ProxyType\n+from ..socks import ProxyType, sockssocket\n from ..utils import format_field, traverse_obj\n \n if typing.TYPE_CHECKING:\n@@ -206,3 +207,77 @@ def wrapper(self, *args, **kwargs):\n                 e.handler = self\n             raise\n     return wrapper\n+\n+\n+def _socket_connect(ip_addr, timeout, source_address):\n+    af, socktype, proto, canonname, sa = ip_addr\n+    sock = socket.socket(af, socktype, proto)\n+    try:\n+        if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n+            sock.settimeout(timeout)\n+        if source_address:\n+            sock.bind(source_address)\n+        sock.connect(sa)\n+        return sock\n+    except OSError:\n+        sock.close()\n+        raise\n+\n+\n+def create_socks_proxy_socket(dest_addr, proxy_args, proxy_ip_addr, timeout, source_address):\n+    af, socktype, proto, canonname, sa = proxy_ip_addr\n+    sock = sockssocket(af, socktype, proto)\n+    try:\n+        connect_proxy_args = proxy_args.copy()\n+        connect_proxy_args.update({'addr': sa[0], 'port': sa[1]})\n+        sock.setproxy(**connect_proxy_args)\n+        if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:  # noqa: E721\n+            sock.settimeout(timeout)\n+        if source_address:\n+            sock.bind(source_address)\n+        sock.connect(dest_addr)\n+        return sock\n+    except OSError:\n+        sock.close()\n+        raise\n+\n+\n+def create_connection(\n+    address,\n+    timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n+    source_address=None,\n+    *,\n+    _create_socket_func=_socket_connect\n+):\n+    # Work around socket.create_connection() which tries all addresses from getaddrinfo() including IPv6.\n+    # This filters the addresses based on the given source_address.\n+    # Based on: https://github.com/python/cpython/blob/main/Lib/socket.py#L810\n+    host, port = address\n+    ip_addrs = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n+    if not ip_addrs:\n+        raise OSError('getaddrinfo returns an empty list')\n+    if source_address is not None:\n+        af = socket.AF_INET if ':' not in source_address[0] else socket.AF_INET6\n+        ip_addrs = [addr for addr in ip_addrs if addr[0] == af]\n+        if not ip_addrs:\n+            raise OSError(\n+                f'No remote IPv{4 if af == socket.AF_INET else 6} addresses available for connect. '\n+                f'Can\\'t use \"{source_address[0]}\" as source address')\n+\n+    err = None\n+    for ip_addr in ip_addrs:\n+        try:\n+            sock = _create_socket_func(ip_addr, timeout, source_address)\n+            # Explicitly break __traceback__ reference cycle\n+            # https://bugs.python.org/issue36820\n+            err = None\n+            return sock\n+        except OSError as e:\n+            err = e\n+\n+    try:\n+        raise err\n+    finally:\n+        # Explicitly break __traceback__ reference cycle\n+        # https://bugs.python.org/issue36820\n+        err = None\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/_requests.py",
            "diff": "diff --git a/yt_dlp/networking/_requests.py b/yt_dlp/networking/_requests.py\nnew file mode 100644\nindex 00000000..e129110c\n--- /dev/null\n+++ b/yt_dlp/networking/_requests.py\n@@ -0,0 +1,400 @@\n+import contextlib\n+import functools\n+import http.client\n+import logging\n+import re\n+import socket\n+import warnings\n+\n+from ..dependencies import brotli, requests, urllib3\n+from ..utils import bug_reports_message, int_or_none, variadic\n+\n+if requests is None:\n+    raise ImportError('requests module is not installed')\n+\n+if urllib3 is None:\n+    raise ImportError('urllib3 module is not installed')\n+\n+urllib3_version = tuple(int_or_none(x, default=0) for x in urllib3.__version__.split('.'))\n+\n+if urllib3_version < (1, 26, 17):\n+    raise ImportError('Only urllib3 >= 1.26.17 is supported')\n+\n+if requests.__build__ < 0x023100:\n+    raise ImportError('Only requests >= 2.31.0 is supported')\n+\n+import requests.adapters\n+import requests.utils\n+import urllib3.connection\n+import urllib3.exceptions\n+\n+from ._helper import (\n+    InstanceStoreMixin,\n+    add_accept_encoding_header,\n+    create_connection,\n+    create_socks_proxy_socket,\n+    get_redirect_method,\n+    make_socks_proxy_opts,\n+    select_proxy,\n+)\n+from .common import (\n+    Features,\n+    RequestHandler,\n+    Response,\n+    register_preference,\n+    register_rh,\n+)\n+from .exceptions import (\n+    CertificateVerifyError,\n+    HTTPError,\n+    IncompleteRead,\n+    ProxyError,\n+    RequestError,\n+    SSLError,\n+    TransportError,\n+)\n+from ..socks import ProxyError as SocksProxyError\n+\n+SUPPORTED_ENCODINGS = [\n+    'gzip', 'deflate'\n+]\n+\n+if brotli is not None:\n+    SUPPORTED_ENCODINGS.append('br')\n+\n+\"\"\"\n+Override urllib3's behavior to not convert lower-case percent-encoded characters\n+to upper-case during url normalization process.\n+\n+RFC3986 defines that the lower or upper case percent-encoded hexidecimal characters are equivalent\n+and normalizers should convert them to uppercase for consistency [1].\n+\n+However, some sites may have an incorrect implementation where they provide\n+a percent-encoded url that is then compared case-sensitively.[2]\n+\n+While this is a very rare case, since urllib does not do this normalization step, it\n+is best to avoid it in requests too for compatability reasons.\n+\n+1: https://tools.ietf.org/html/rfc3986#section-2.1\n+2: https://github.com/streamlink/streamlink/pull/4003\n+\"\"\"\n+\n+\n+class Urllib3PercentREOverride:\n+    def __init__(self, r: re.Pattern):\n+        self.re = r\n+\n+    # pass through all other attribute calls to the original re\n+    def __getattr__(self, item):\n+        return self.re.__getattribute__(item)\n+\n+    def subn(self, repl, string, *args, **kwargs):\n+        return string, self.re.subn(repl, string, *args, **kwargs)[1]\n+\n+\n+# urllib3 >= 1.25.8 uses subn:\n+# https://github.com/urllib3/urllib3/commit/a2697e7c6b275f05879b60f593c5854a816489f0\n+import urllib3.util.url  # noqa: E305\n+\n+if hasattr(urllib3.util.url, 'PERCENT_RE'):\n+    urllib3.util.url.PERCENT_RE = Urllib3PercentREOverride(urllib3.util.url.PERCENT_RE)\n+elif hasattr(urllib3.util.url, '_PERCENT_RE'):  # urllib3 >= 2.0.0\n+    urllib3.util.url._PERCENT_RE = Urllib3PercentREOverride(urllib3.util.url._PERCENT_RE)\n+else:\n+    warnings.warn('Failed to patch PERCENT_RE in urllib3 (does the attribute exist?)' + bug_reports_message())\n+\n+\"\"\"\n+Workaround for issue in urllib.util.ssl_.py: ssl_wrap_context does not pass\n+server_hostname to SSLContext.wrap_socket if server_hostname is an IP,\n+however this is an issue because we set check_hostname to True in our SSLContext.\n+\n+Monkey-patching IS_SECURETRANSPORT forces ssl_wrap_context to pass server_hostname regardless.\n+\n+This has been fixed in urllib3 2.0+.\n+See: https://github.com/urllib3/urllib3/issues/517\n+\"\"\"\n+\n+if urllib3_version < (2, 0, 0):\n+    with contextlib.suppress():\n+        urllib3.util.IS_SECURETRANSPORT = urllib3.util.ssl_.IS_SECURETRANSPORT = True\n+\n+\n+# Requests will not automatically handle no_proxy by default\n+# due to buggy no_proxy handling with proxy dict [1].\n+# 1. https://github.com/psf/requests/issues/5000\n+requests.adapters.select_proxy = select_proxy\n+\n+\n+class RequestsResponseAdapter(Response):\n+    def __init__(self, res: requests.models.Response):\n+        super().__init__(\n+            fp=res.raw, headers=res.headers, url=res.url,\n+            status=res.status_code, reason=res.reason)\n+\n+        self._requests_response = res\n+\n+    def read(self, amt: int = None):\n+        try:\n+            # Interact with urllib3 response directly.\n+            return self.fp.read(amt, decode_content=True)\n+\n+        # See urllib3.response.HTTPResponse.read() for exceptions raised on read\n+        except urllib3.exceptions.SSLError as e:\n+            raise SSLError(cause=e) from e\n+\n+        except urllib3.exceptions.ProtocolError as e:\n+            # IncompleteRead is always contained within ProtocolError\n+            # See urllib3.response.HTTPResponse._error_catcher()\n+            ir_err = next(\n+                (err for err in (e.__context__, e.__cause__, *variadic(e.args))\n+                 if isinstance(err, http.client.IncompleteRead)), None)\n+            if ir_err is not None:\n+                # `urllib3.exceptions.IncompleteRead` is subclass of `http.client.IncompleteRead`\n+                # but uses an `int` for its `partial` property.\n+                partial = ir_err.partial if isinstance(ir_err.partial, int) else len(ir_err.partial)\n+                raise IncompleteRead(partial=partial, expected=ir_err.expected) from e\n+            raise TransportError(cause=e) from e\n+\n+        except urllib3.exceptions.HTTPError as e:\n+            # catch-all for any other urllib3 response exceptions\n+            raise TransportError(cause=e) from e\n+\n+\n+class RequestsHTTPAdapter(requests.adapters.HTTPAdapter):\n+    def __init__(self, ssl_context=None, proxy_ssl_context=None, source_address=None, **kwargs):\n+        self._pm_args = {}\n+        if ssl_context:\n+            self._pm_args['ssl_context'] = ssl_context\n+        if source_address:\n+            self._pm_args['source_address'] = (source_address, 0)\n+        self._proxy_ssl_context = proxy_ssl_context or ssl_context\n+        super().__init__(**kwargs)\n+\n+    def init_poolmanager(self, *args, **kwargs):\n+        return super().init_poolmanager(*args, **kwargs, **self._pm_args)\n+\n+    def proxy_manager_for(self, proxy, **proxy_kwargs):\n+        extra_kwargs = {}\n+        if not proxy.lower().startswith('socks') and self._proxy_ssl_context:\n+            extra_kwargs['proxy_ssl_context'] = self._proxy_ssl_context\n+        return super().proxy_manager_for(proxy, **proxy_kwargs, **self._pm_args, **extra_kwargs)\n+\n+    def cert_verify(*args, **kwargs):\n+        # lean on SSLContext for cert verification\n+        pass\n+\n+\n+class RequestsSession(requests.sessions.Session):\n+    \"\"\"\n+    Ensure unified redirect method handling with our urllib redirect handler.\n+    \"\"\"\n+\n+    def rebuild_method(self, prepared_request, response):\n+        new_method = get_redirect_method(prepared_request.method, response.status_code)\n+\n+        # HACK: requests removes headers/body on redirect unless code was a 307/308.\n+        if new_method == prepared_request.method:\n+            response._real_status_code = response.status_code\n+            response.status_code = 308\n+\n+        prepared_request.method = new_method\n+\n+    def rebuild_auth(self, prepared_request, response):\n+        # HACK: undo status code change from rebuild_method, if applicable.\n+        # rebuild_auth runs after requests would remove headers/body based on status code\n+        if hasattr(response, '_real_status_code'):\n+            response.status_code = response._real_status_code\n+            del response._real_status_code\n+        return super().rebuild_auth(prepared_request, response)\n+\n+\n+class Urllib3LoggingFilter(logging.Filter):\n+\n+    def filter(self, record):\n+        # Ignore HTTP request messages since HTTPConnection prints those\n+        if record.msg == '%s://%s:%s \"%s %s %s\" %s %s':\n+            return False\n+        return True\n+\n+\n+class Urllib3LoggingHandler(logging.Handler):\n+    \"\"\"Redirect urllib3 logs to our logger\"\"\"\n+\n+    def __init__(self, logger, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._logger = logger\n+\n+    def emit(self, record):\n+        try:\n+            msg = self.format(record)\n+            if record.levelno >= logging.ERROR:\n+                self._logger.error(msg)\n+            else:\n+                self._logger.stdout(msg)\n+\n+        except Exception:\n+            self.handleError(record)\n+\n+\n+@register_rh\n+class RequestsRH(RequestHandler, InstanceStoreMixin):\n+\n+    \"\"\"Requests RequestHandler\n+    https://github.com/psf/requests\n+    \"\"\"\n+    _SUPPORTED_URL_SCHEMES = ('http', 'https')\n+    _SUPPORTED_ENCODINGS = tuple(SUPPORTED_ENCODINGS)\n+    _SUPPORTED_PROXY_SCHEMES = ('http', 'https', 'socks4', 'socks4a', 'socks5', 'socks5h')\n+    _SUPPORTED_FEATURES = (Features.NO_PROXY, Features.ALL_PROXY)\n+    RH_NAME = 'requests'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # Forward urllib3 debug messages to our logger\n+        logger = logging.getLogger('urllib3')\n+        handler = Urllib3LoggingHandler(logger=self._logger)\n+        handler.setFormatter(logging.Formatter('requests: %(message)s'))\n+        handler.addFilter(Urllib3LoggingFilter())\n+        logger.addHandler(handler)\n+        # TODO: Use a logger filter to suppress pool reuse warning instead\n+        logger.setLevel(logging.ERROR)\n+\n+        if self.verbose:\n+            # Setting this globally is not ideal, but is easier than hacking with urllib3.\n+            # It could technically be problematic for scripts embedding yt-dlp.\n+            # However, it is unlikely debug traffic is used in that context in a way this will cause problems.\n+            urllib3.connection.HTTPConnection.debuglevel = 1\n+            logger.setLevel(logging.DEBUG)\n+        # this is expected if we are using --no-check-certificate\n+        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n+\n+    def close(self):\n+        self._clear_instances()\n+\n+    def _check_extensions(self, extensions):\n+        super()._check_extensions(extensions)\n+        extensions.pop('cookiejar', None)\n+        extensions.pop('timeout', None)\n+\n+    def _create_instance(self, cookiejar):\n+        session = RequestsSession()\n+        http_adapter = RequestsHTTPAdapter(\n+            ssl_context=self._make_sslcontext(),\n+            source_address=self.source_address,\n+            max_retries=urllib3.util.retry.Retry(False),\n+        )\n+        session.adapters.clear()\n+        session.headers = requests.models.CaseInsensitiveDict({'Connection': 'keep-alive'})\n+        session.mount('https://', http_adapter)\n+        session.mount('http://', http_adapter)\n+        session.cookies = cookiejar\n+        session.trust_env = False  # no need, we already load proxies from env\n+        return session\n+\n+    def _send(self, request):\n+\n+        headers = self._merge_headers(request.headers)\n+        add_accept_encoding_header(headers, SUPPORTED_ENCODINGS)\n+\n+        max_redirects_exceeded = False\n+\n+        session = self._get_instance(\n+            cookiejar=request.extensions.get('cookiejar') or self.cookiejar)\n+\n+        try:\n+            requests_res = session.request(\n+                method=request.method,\n+                url=request.url,\n+                data=request.data,\n+                headers=headers,\n+                timeout=float(request.extensions.get('timeout') or self.timeout),\n+                proxies=request.proxies or self.proxies,\n+                allow_redirects=True,\n+                stream=True\n+            )\n+\n+        except requests.exceptions.TooManyRedirects as e:\n+            max_redirects_exceeded = True\n+            requests_res = e.response\n+\n+        except requests.exceptions.SSLError as e:\n+            if 'CERTIFICATE_VERIFY_FAILED' in str(e):\n+                raise CertificateVerifyError(cause=e) from e\n+            raise SSLError(cause=e) from e\n+\n+        except requests.exceptions.ProxyError as e:\n+            raise ProxyError(cause=e) from e\n+\n+        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n+            raise TransportError(cause=e) from e\n+\n+        except urllib3.exceptions.HTTPError as e:\n+            # Catch any urllib3 exceptions that may leak through\n+            raise TransportError(cause=e) from e\n+\n+        except requests.exceptions.RequestException as e:\n+            # Miscellaneous Requests exceptions. May not necessary be network related e.g. InvalidURL\n+            raise RequestError(cause=e) from e\n+\n+        res = RequestsResponseAdapter(requests_res)\n+\n+        if not 200 <= res.status < 300:\n+            raise HTTPError(res, redirect_loop=max_redirects_exceeded)\n+\n+        return res\n+\n+\n+@register_preference(RequestsRH)\n+def requests_preference(rh, request):\n+    return 100\n+\n+\n+# Use our socks proxy implementation with requests to avoid an extra dependency.\n+class SocksHTTPConnection(urllib3.connection.HTTPConnection):\n+    def __init__(self, _socks_options, *args, **kwargs):  # must use _socks_options to pass PoolKey checks\n+        self._proxy_args = _socks_options\n+        super().__init__(*args, **kwargs)\n+\n+    def _new_conn(self):\n+        try:\n+            return create_connection(\n+                address=(self._proxy_args['addr'], self._proxy_args['port']),\n+                timeout=self.timeout,\n+                source_address=self.source_address,\n+                _create_socket_func=functools.partial(\n+                    create_socks_proxy_socket, (self.host, self.port), self._proxy_args))\n+        except (socket.timeout, TimeoutError) as e:\n+            raise urllib3.exceptions.ConnectTimeoutError(\n+                self, f'Connection to {self.host} timed out. (connect timeout={self.timeout})') from e\n+        except SocksProxyError as e:\n+            raise urllib3.exceptions.ProxyError(str(e), e) from e\n+        except OSError as e:\n+            raise urllib3.exceptions.NewConnectionError(\n+                self, f'Failed to establish a new connection: {e}') from e\n+\n+\n+class SocksHTTPSConnection(SocksHTTPConnection, urllib3.connection.HTTPSConnection):\n+    pass\n+\n+\n+class SocksHTTPConnectionPool(urllib3.HTTPConnectionPool):\n+    ConnectionCls = SocksHTTPConnection\n+\n+\n+class SocksHTTPSConnectionPool(urllib3.HTTPSConnectionPool):\n+    ConnectionCls = SocksHTTPSConnection\n+\n+\n+class SocksProxyManager(urllib3.PoolManager):\n+\n+    def __init__(self, socks_proxy, username=None, password=None, num_pools=10, headers=None, **connection_pool_kw):\n+        connection_pool_kw['_socks_options'] = make_socks_proxy_opts(socks_proxy)\n+        super().__init__(num_pools, headers, **connection_pool_kw)\n+        self.pool_classes_by_scheme = {\n+            'http': SocksHTTPConnectionPool,\n+            'https': SocksHTTPSConnectionPool\n+        }\n+\n+\n+requests.adapters.SOCKSProxyManager = SocksProxyManager\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/_urllib.py",
            "diff": "diff --git a/yt_dlp/networking/_urllib.py b/yt_dlp/networking/_urllib.py\nindex 0c479495..68bab2b0 100644\n--- a/yt_dlp/networking/_urllib.py\n+++ b/yt_dlp/networking/_urllib.py\n@@ -1,10 +1,8 @@\n from __future__ import annotations\n \n import functools\n-import gzip\n import http.client\n import io\n-import socket\n import ssl\n import urllib.error\n import urllib.parse\n@@ -24,6 +22,8 @@\n from ._helper import (\n     InstanceStoreMixin,\n     add_accept_encoding_header,\n+    create_connection,\n+    create_socks_proxy_socket,\n     get_redirect_method,\n     make_socks_proxy_opts,\n     select_proxy,\n@@ -40,7 +40,6 @@\n )\n from ..dependencies import brotli\n from ..socks import ProxyError as SocksProxyError\n-from ..socks import sockssocket\n from ..utils import update_url_query\n from ..utils.networking import normalize_url\n \n@@ -55,44 +54,10 @@\n def _create_http_connection(http_class, source_address, *args, **kwargs):\n     hc = http_class(*args, **kwargs)\n \n+    if hasattr(hc, '_create_connection'):\n+        hc._create_connection = create_connection\n+\n     if source_address is not None:\n-        # This is to workaround _create_connection() from socket where it will try all\n-        # address data from getaddrinfo() including IPv6. This filters the result from\n-        # getaddrinfo() based on the source_address value.\n-        # This is based on the cpython socket.create_connection() function.\n-        # https://github.com/python/cpython/blob/master/Lib/socket.py#L691\n-        def _create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n-            host, port = address\n-            err = None\n-            addrs = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n-            af = socket.AF_INET if '.' in source_address[0] else socket.AF_INET6\n-            ip_addrs = [addr for addr in addrs if addr[0] == af]\n-            if addrs and not ip_addrs:\n-                ip_version = 'v4' if af == socket.AF_INET else 'v6'\n-                raise OSError(\n-                    \"No remote IP%s addresses available for connect, can't use '%s' as source address\"\n-                    % (ip_version, source_address[0]))\n-            for res in ip_addrs:\n-                af, socktype, proto, canonname, sa = res\n-                sock = None\n-                try:\n-                    sock = socket.socket(af, socktype, proto)\n-                    if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n-                        sock.settimeout(timeout)\n-                    sock.bind(source_address)\n-                    sock.connect(sa)\n-                    err = None  # Explicitly break reference cycle\n-                    return sock\n-                except OSError as _:\n-                    err = _\n-                    if sock is not None:\n-                        sock.close()\n-            if err is not None:\n-                raise err\n-            else:\n-                raise OSError('getaddrinfo returns an empty list')\n-        if hasattr(hc, '_create_connection'):\n-            hc._create_connection = _create_connection\n         hc.source_address = (source_address, 0)\n \n     return hc\n@@ -155,20 +120,11 @@ def brotli(data):\n \n     @staticmethod\n     def gz(data):\n-        gz = gzip.GzipFile(fileobj=io.BytesIO(data), mode='rb')\n-        try:\n-            return gz.read()\n-        except OSError as original_oserror:\n-            # There may be junk add the end of the file\n-            # See http://stackoverflow.com/q/4928560/35070 for details\n-            for i in range(1, 1024):\n-                try:\n-                    gz = gzip.GzipFile(fileobj=io.BytesIO(data[:-i]), mode='rb')\n-                    return gz.read()\n-                except OSError:\n-                    continue\n-            else:\n-                raise original_oserror\n+        # There may be junk added the end of the file\n+        # We ignore it by only ever decoding a single gzip payload\n+        if not data:\n+            return data\n+        return zlib.decompress(data, wbits=zlib.MAX_WBITS | 16)\n \n     def http_request(self, req):\n         # According to RFC 3986, URLs can not contain non-ASCII characters, however this is not\n@@ -230,13 +186,15 @@ def make_socks_conn_class(base_class, socks_proxy):\n     proxy_args = make_socks_proxy_opts(socks_proxy)\n \n     class SocksConnection(base_class):\n-        def connect(self):\n-            self.sock = sockssocket()\n-            self.sock.setproxy(**proxy_args)\n-            if type(self.timeout) in (int, float):  # noqa: E721\n-                self.sock.settimeout(self.timeout)\n-            self.sock.connect((self.host, self.port))\n+        _create_connection = create_connection\n \n+        def connect(self):\n+            self.sock = create_connection(\n+                (proxy_args['addr'], proxy_args['port']),\n+                timeout=self.timeout,\n+                source_address=self.source_address,\n+                _create_socket_func=functools.partial(\n+                    create_socks_proxy_socket, (self.host, self.port), proxy_args))\n             if isinstance(self, http.client.HTTPSConnection):\n                 self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host)\n \n@@ -365,7 +323,7 @@ def handle_sslerror(e: ssl.SSLError):\n \n def handle_response_read_exceptions(e):\n     if isinstance(e, http.client.IncompleteRead):\n-        raise IncompleteRead(partial=e.partial, cause=e, expected=e.expected) from e\n+        raise IncompleteRead(partial=len(e.partial), cause=e, expected=e.expected) from e\n     elif isinstance(e, ssl.SSLError):\n         handle_sslerror(e)\n     elif isinstance(e, (OSError, EOFError, http.client.HTTPException, *CONTENT_DECODE_ERRORS)):\n@@ -439,7 +397,7 @@ def _send(self, request):\n         except urllib.error.HTTPError as e:\n             if isinstance(e.fp, (http.client.HTTPResponse, urllib.response.addinfourl)):\n                 # Prevent file object from being closed when urllib.error.HTTPError is destroyed.\n-                e._closer.file = None\n+                e._closer.close_called = True\n                 raise HTTPError(UrllibResponseAdapter(e.fp), redirect_loop='redirect error' in str(e)) from e\n             raise  # unexpected\n         except urllib.error.URLError as e:\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/_websockets.py",
            "diff": "diff --git a/yt_dlp/networking/_websockets.py b/yt_dlp/networking/_websockets.py\nnew file mode 100644\nindex 00000000..ed64080d\n--- /dev/null\n+++ b/yt_dlp/networking/_websockets.py\n@@ -0,0 +1,165 @@\n+from __future__ import annotations\n+\n+import io\n+import logging\n+import ssl\n+import sys\n+\n+from ._helper import (\n+    create_connection,\n+    create_socks_proxy_socket,\n+    make_socks_proxy_opts,\n+    select_proxy,\n+)\n+from .common import Features, Response, register_rh\n+from .exceptions import (\n+    CertificateVerifyError,\n+    HTTPError,\n+    ProxyError,\n+    RequestError,\n+    SSLError,\n+    TransportError,\n+)\n+from .websocket import WebSocketRequestHandler, WebSocketResponse\n+from ..compat import functools\n+from ..dependencies import websockets\n+from ..socks import ProxyError as SocksProxyError\n+from ..utils import int_or_none\n+\n+if not websockets:\n+    raise ImportError('websockets is not installed')\n+\n+import websockets.version\n+\n+websockets_version = tuple(map(int_or_none, websockets.version.version.split('.')))\n+if websockets_version < (12, 0):\n+    raise ImportError('Only websockets>=12.0 is supported')\n+\n+import websockets.sync.client\n+from websockets.uri import parse_uri\n+\n+\n+class WebsocketsResponseAdapter(WebSocketResponse):\n+\n+    def __init__(self, wsw: websockets.sync.client.ClientConnection, url):\n+        super().__init__(\n+            fp=io.BytesIO(wsw.response.body or b''),\n+            url=url,\n+            headers=wsw.response.headers,\n+            status=wsw.response.status_code,\n+            reason=wsw.response.reason_phrase,\n+        )\n+        self.wsw = wsw\n+\n+    def close(self):\n+        self.wsw.close()\n+        super().close()\n+\n+    def send(self, message):\n+        # https://websockets.readthedocs.io/en/stable/reference/sync/client.html#websockets.sync.client.ClientConnection.send\n+        try:\n+            return self.wsw.send(message)\n+        except (websockets.exceptions.WebSocketException, RuntimeError, TimeoutError) as e:\n+            raise TransportError(cause=e) from e\n+        except SocksProxyError as e:\n+            raise ProxyError(cause=e) from e\n+        except TypeError as e:\n+            raise RequestError(cause=e) from e\n+\n+    def recv(self):\n+        # https://websockets.readthedocs.io/en/stable/reference/sync/client.html#websockets.sync.client.ClientConnection.recv\n+        try:\n+            return self.wsw.recv()\n+        except SocksProxyError as e:\n+            raise ProxyError(cause=e) from e\n+        except (websockets.exceptions.WebSocketException, RuntimeError, TimeoutError) as e:\n+            raise TransportError(cause=e) from e\n+\n+\n+@register_rh\n+class WebsocketsRH(WebSocketRequestHandler):\n+    \"\"\"\n+    Websockets request handler\n+    https://websockets.readthedocs.io\n+    https://github.com/python-websockets/websockets\n+    \"\"\"\n+    _SUPPORTED_URL_SCHEMES = ('wss', 'ws')\n+    _SUPPORTED_PROXY_SCHEMES = ('socks4', 'socks4a', 'socks5', 'socks5h')\n+    _SUPPORTED_FEATURES = (Features.ALL_PROXY, Features.NO_PROXY)\n+    RH_NAME = 'websockets'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        for name in ('websockets.client', 'websockets.server'):\n+            logger = logging.getLogger(name)\n+            handler = logging.StreamHandler(stream=sys.stdout)\n+            handler.setFormatter(logging.Formatter(f'{self.RH_NAME}: %(message)s'))\n+            logger.addHandler(handler)\n+            if self.verbose:\n+                logger.setLevel(logging.DEBUG)\n+\n+    def _check_extensions(self, extensions):\n+        super()._check_extensions(extensions)\n+        extensions.pop('timeout', None)\n+        extensions.pop('cookiejar', None)\n+\n+    def _send(self, request):\n+        timeout = float(request.extensions.get('timeout') or self.timeout)\n+        headers = self._merge_headers(request.headers)\n+        if 'cookie' not in headers:\n+            cookiejar = request.extensions.get('cookiejar') or self.cookiejar\n+            cookie_header = cookiejar.get_cookie_header(request.url)\n+            if cookie_header:\n+                headers['cookie'] = cookie_header\n+\n+        wsuri = parse_uri(request.url)\n+        create_conn_kwargs = {\n+            'source_address': (self.source_address, 0) if self.source_address else None,\n+            'timeout': timeout\n+        }\n+        proxy = select_proxy(request.url, request.proxies or self.proxies or {})\n+        try:\n+            if proxy:\n+                socks_proxy_options = make_socks_proxy_opts(proxy)\n+                sock = create_connection(\n+                    address=(socks_proxy_options['addr'], socks_proxy_options['port']),\n+                    _create_socket_func=functools.partial(\n+                        create_socks_proxy_socket, (wsuri.host, wsuri.port), socks_proxy_options),\n+                    **create_conn_kwargs\n+                )\n+            else:\n+                sock = create_connection(\n+                    address=(wsuri.host, wsuri.port),\n+                    **create_conn_kwargs\n+                )\n+            conn = websockets.sync.client.connect(\n+                sock=sock,\n+                uri=request.url,\n+                additional_headers=headers,\n+                open_timeout=timeout,\n+                user_agent_header=None,\n+                ssl_context=self._make_sslcontext() if wsuri.secure else None,\n+                close_timeout=0,  # not ideal, but prevents yt-dlp hanging\n+            )\n+            return WebsocketsResponseAdapter(conn, url=request.url)\n+\n+        # Exceptions as per https://websockets.readthedocs.io/en/stable/reference/sync/client.html\n+        except SocksProxyError as e:\n+            raise ProxyError(cause=e) from e\n+        except websockets.exceptions.InvalidURI as e:\n+            raise RequestError(cause=e) from e\n+        except ssl.SSLCertVerificationError as e:\n+            raise CertificateVerifyError(cause=e) from e\n+        except ssl.SSLError as e:\n+            raise SSLError(cause=e) from e\n+        except websockets.exceptions.InvalidStatus as e:\n+            raise HTTPError(\n+                Response(\n+                    fp=io.BytesIO(e.response.body),\n+                    url=request.url,\n+                    headers=e.response.headers,\n+                    status=e.response.status_code,\n+                    reason=e.response.reason_phrase),\n+            ) from e\n+        except (OSError, TimeoutError, websockets.exceptions.WebSocketException) as e:\n+            raise TransportError(cause=e) from e\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/exceptions.py",
            "diff": "diff --git a/yt_dlp/networking/exceptions.py b/yt_dlp/networking/exceptions.py\nindex 10afc9cc..12441901 100644\n--- a/yt_dlp/networking/exceptions.py\n+++ b/yt_dlp/networking/exceptions.py\n@@ -75,10 +75,10 @@ def __repr__(self):\n \n \n class IncompleteRead(TransportError):\n-    def __init__(self, partial, expected=None, **kwargs):\n+    def __init__(self, partial: int, expected: int | None = None, **kwargs):\n         self.partial = partial\n         self.expected = expected\n-        msg = f'{len(partial)} bytes read'\n+        msg = f'{partial} bytes read'\n         if expected is not None:\n             msg += f', {expected} more expected'\n \n@@ -115,7 +115,7 @@ def __init__(self, http_error: HTTPError):\n             hdrs=http_error.response.headers,\n             fp=http_error.response\n         )\n-        self._closer.file = None  # Disable auto close\n+        self._closer.close_called = True  # Disable auto close\n         self._http_error = http_error\n         HTTPError.__init__(self, http_error.response, redirect_loop=http_error.redirect_loop)\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/networking/websocket.py",
            "diff": "diff --git a/yt_dlp/networking/websocket.py b/yt_dlp/networking/websocket.py\nnew file mode 100644\nindex 00000000..0e7e73c9\n--- /dev/null\n+++ b/yt_dlp/networking/websocket.py\n@@ -0,0 +1,23 @@\n+from __future__ import annotations\n+\n+import abc\n+\n+from .common import RequestHandler, Response\n+\n+\n+class WebSocketResponse(Response):\n+\n+    def send(self, message: bytes | str):\n+        \"\"\"\n+        Send a message to the server.\n+\n+        @param message: The message to send. A string (str) is sent as a text frame, bytes is sent as a binary frame.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def recv(self):\n+        raise NotImplementedError\n+\n+\n+class WebSocketRequestHandler(RequestHandler, abc.ABC):\n+    pass\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/options.py",
            "diff": "diff --git a/yt_dlp/options.py b/yt_dlp/options.py\nindex 16380970..e9d92771 100644\n--- a/yt_dlp/options.py\n+++ b/yt_dlp/options.py\n@@ -471,11 +471,12 @@ def _alias_callback(option, opt_str, value, parser, opts, nargs):\n                 'no-attach-info-json', 'embed-thumbnail-atomicparsley', 'no-external-downloader-progress',\n                 'embed-metadata', 'seperate-video-versions', 'no-clean-infojson', 'no-keep-subs', 'no-certifi',\n                 'no-youtube-channel-redirect', 'no-youtube-unavailable-videos', 'no-youtube-prefer-utc-upload-date',\n+                'prefer-legacy-http-handler', 'manifest-filesize-approx'\n             }, 'aliases': {\n-                'youtube-dl': ['all', '-multistreams', '-playlist-match-filter'],\n-                'youtube-dlc': ['all', '-no-youtube-channel-redirect', '-no-live-chat', '-playlist-match-filter'],\n+                'youtube-dl': ['all', '-multistreams', '-playlist-match-filter', '-manifest-filesize-approx'],\n+                'youtube-dlc': ['all', '-no-youtube-channel-redirect', '-no-live-chat', '-playlist-match-filter', '-manifest-filesize-approx'],\n                 '2021': ['2022', 'no-certifi', 'filename-sanitization', 'no-youtube-prefer-utc-upload-date'],\n-                '2022': ['no-external-downloader-progress', 'playlist-match-filter'],\n+                '2022': ['no-external-downloader-progress', 'playlist-match-filter', 'prefer-legacy-http-handler', 'manifest-filesize-approx'],\n             }\n         }, help=(\n             'Options that can help keep compatibility with youtube-dl or youtube-dlc '\n@@ -727,7 +728,7 @@ def _alias_callback(option, opt_str, value, parser, opts, nargs):\n     authentication.add_option(\n         '--video-password',\n         dest='videopassword', metavar='PASSWORD',\n-        help='Video password (vimeo, youku)')\n+        help='Video-specific password')\n     authentication.add_option(\n         '--ap-mso',\n         dest='ap_mso', metavar='MSO',\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/postprocessor/exec.py",
            "diff": "diff --git a/yt_dlp/postprocessor/exec.py b/yt_dlp/postprocessor/exec.py\nindex cfc83167..c2e73fba 100644\n--- a/yt_dlp/postprocessor/exec.py\n+++ b/yt_dlp/postprocessor/exec.py\n@@ -1,8 +1,6 @@\n-import subprocess\n-\n from .common import PostProcessor\n from ..compat import compat_shlex_quote\n-from ..utils import PostProcessingError, encodeArgument, variadic\n+from ..utils import Popen, PostProcessingError, variadic\n \n \n class ExecPP(PostProcessor):\n@@ -27,10 +25,10 @@ def parse_cmd(self, cmd, info):\n     def run(self, info):\n         for tmpl in self.exec_cmd:\n             cmd = self.parse_cmd(tmpl, info)\n-            self.to_screen('Executing command: %s' % cmd)\n-            retCode = subprocess.call(encodeArgument(cmd), shell=True)\n-            if retCode != 0:\n-                raise PostProcessingError('Command returned error code %d' % retCode)\n+            self.to_screen(f'Executing command: {cmd}')\n+            _, _, return_code = Popen.run(cmd, shell=True)\n+            if return_code != 0:\n+                raise PostProcessingError(f'Command returned error code {return_code}')\n         return [], info\n \n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/postprocessor/ffmpeg.py",
            "diff": "diff --git a/yt_dlp/postprocessor/ffmpeg.py b/yt_dlp/postprocessor/ffmpeg.py\nindex 323f4303..7c904417 100644\n--- a/yt_dlp/postprocessor/ffmpeg.py\n+++ b/yt_dlp/postprocessor/ffmpeg.py\n@@ -780,7 +780,7 @@ def add(meta_list, info_list=None):\n             yield ('-metadata', f'{name}={value}')\n \n         stream_idx = 0\n-        for fmt in info.get('requested_formats') or []:\n+        for fmt in info.get('requested_formats') or [info]:\n             stream_count = 2 if 'none' not in (fmt.get('vcodec'), fmt.get('acodec')) else 1\n             lang = ISO639Utils.short2long(fmt.get('language') or '') or fmt.get('language')\n             for i in range(stream_idx, stream_idx + stream_count):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/socks.py",
            "diff": "diff --git a/yt_dlp/socks.py b/yt_dlp/socks.py\nindex f93328f6..b4957ac2 100644\n--- a/yt_dlp/socks.py\n+++ b/yt_dlp/socks.py\n@@ -49,7 +49,7 @@ class Socks5AddressType:\n     ATYP_IPV6 = 0x04\n \n \n-class ProxyError(socket.error):\n+class ProxyError(OSError):\n     ERR_SUCCESS = 0x00\n \n     def __init__(self, code=None, msg=None):\n@@ -134,26 +134,31 @@ def _check_response_version(self, expected_version, got_version):\n             self.close()\n             raise InvalidVersionError(expected_version, got_version)\n \n-    def _resolve_address(self, destaddr, default, use_remote_dns):\n-        try:\n-            return socket.inet_aton(destaddr)\n-        except OSError:\n-            if use_remote_dns and self._proxy.remote_dns:\n-                return default\n-            else:\n-                return socket.inet_aton(socket.gethostbyname(destaddr))\n+    def _resolve_address(self, destaddr, default, use_remote_dns, family=None):\n+        for f in (family,) if family else (socket.AF_INET, socket.AF_INET6):\n+            try:\n+                return f, socket.inet_pton(f, destaddr)\n+            except OSError:\n+                continue\n+\n+        if use_remote_dns and self._proxy.remote_dns:\n+            return 0, default\n+        else:\n+            res = socket.getaddrinfo(destaddr, None, family=family or 0)\n+            f, _, _, _, ipaddr = res[0]\n+            return f, socket.inet_pton(f, ipaddr[0])\n \n     def _setup_socks4(self, address, is_4a=False):\n         destaddr, port = address\n \n-        ipaddr = self._resolve_address(destaddr, SOCKS4_DEFAULT_DSTIP, use_remote_dns=is_4a)\n+        _, ipaddr = self._resolve_address(destaddr, SOCKS4_DEFAULT_DSTIP, use_remote_dns=is_4a, family=socket.AF_INET)\n \n         packet = struct.pack('!BBH', SOCKS4_VERSION, Socks4Command.CMD_CONNECT, port) + ipaddr\n \n         username = (self._proxy.username or '').encode()\n         packet += username + b'\\x00'\n \n-        if is_4a and self._proxy.remote_dns:\n+        if is_4a and self._proxy.remote_dns and ipaddr == SOCKS4_DEFAULT_DSTIP:\n             packet += destaddr.encode() + b'\\x00'\n \n         self.sendall(packet)\n@@ -210,7 +215,7 @@ def _socks5_auth(self):\n     def _setup_socks5(self, address):\n         destaddr, port = address\n \n-        ipaddr = self._resolve_address(destaddr, None, use_remote_dns=True)\n+        family, ipaddr = self._resolve_address(destaddr, None, use_remote_dns=True)\n \n         self._socks5_auth()\n \n@@ -220,8 +225,10 @@ def _setup_socks5(self, address):\n             destaddr = destaddr.encode()\n             packet += struct.pack('!B', Socks5AddressType.ATYP_DOMAINNAME)\n             packet += self._len_and_data(destaddr)\n-        else:\n+        elif family == socket.AF_INET:\n             packet += struct.pack('!B', Socks5AddressType.ATYP_IPV4) + ipaddr\n+        elif family == socket.AF_INET6:\n+            packet += struct.pack('!B', Socks5AddressType.ATYP_IPV6) + ipaddr\n         packet += struct.pack('!H', port)\n \n         self.sendall(packet)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/update.py",
            "diff": "diff --git a/yt_dlp/update.py b/yt_dlp/update.py\nindex d708b09e..ba7eadf8 100644\n--- a/yt_dlp/update.py\n+++ b/yt_dlp/update.py\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import atexit\n import contextlib\n import hashlib\n@@ -7,6 +9,7 @@\n import re\n import subprocess\n import sys\n+from dataclasses import dataclass\n from zipimport import zipimporter\n \n from .compat import functools  # isort: split\n@@ -14,24 +17,35 @@\n from .networking import Request\n from .networking.exceptions import HTTPError, network_exceptions\n from .utils import (\n+    NO_DEFAULT,\n     Popen,\n-    cached_method,\n     deprecation_warning,\n+    format_field,\n     remove_end,\n-    remove_start,\n     shell_quote,\n     system_identifier,\n     version_tuple,\n )\n-from .version import CHANNEL, UPDATE_HINT, VARIANT, __version__\n+from .version import (\n+    CHANNEL,\n+    ORIGIN,\n+    RELEASE_GIT_HEAD,\n+    UPDATE_HINT,\n+    VARIANT,\n+    __version__,\n+)\n \n UPDATE_SOURCES = {\n     'stable': 'yt-dlp/yt-dlp',\n     'nightly': 'yt-dlp/yt-dlp-nightly-builds',\n+    'master': 'yt-dlp/yt-dlp-master-builds',\n }\n REPOSITORY = UPDATE_SOURCES['stable']\n+_INVERSE_UPDATE_SOURCES = {value: key for key, value in UPDATE_SOURCES.items()}\n \n _VERSION_RE = re.compile(r'(\\d+\\.)*\\d+')\n+_HASH_PATTERN = r'[\\da-f]{40}'\n+_COMMIT_RE = re.compile(rf'Generated from: https://(?:[^/?#]+/){{3}}commit/(?P<hash>{_HASH_PATTERN})')\n \n API_BASE_URL = 'https://api.github.com/repos'\n \n@@ -112,6 +126,27 @@ def is_non_updateable():\n         detect_variant(), _NON_UPDATEABLE_REASONS['unknown' if VARIANT else 'other'])\n \n \n+def _get_binary_name():\n+    return format_field(_FILE_SUFFIXES, detect_variant(), template='yt-dlp%s', ignore=None, default=None)\n+\n+\n+def _get_system_deprecation():\n+    MIN_SUPPORTED, MIN_RECOMMENDED = (3, 8), (3, 8)\n+\n+    if sys.version_info > MIN_RECOMMENDED:\n+        return None\n+\n+    major, minor = sys.version_info[:2]\n+    if sys.version_info < MIN_SUPPORTED:\n+        msg = f'Python version {major}.{minor} is no longer supported'\n+    else:\n+        msg = (f'Support for Python version {major}.{minor} has been deprecated. '\n+               '\\nYou may stop receiving updates on this version at any time')\n+\n+    major, minor = MIN_RECOMMENDED\n+    return f'{msg}! Please update to Python {major}.{minor} or above'\n+\n+\n def _sha256_file(path):\n     h = hashlib.sha256()\n     mv = memoryview(bytearray(128 * 1024))\n@@ -121,73 +156,118 @@ def _sha256_file(path):\n     return h.hexdigest()\n \n \n-class Updater:\n-    _exact = True\n+def _make_label(origin, tag, version=None):\n+    if '/' in origin:\n+        channel = _INVERSE_UPDATE_SOURCES.get(origin, origin)\n+    else:\n+        channel = origin\n+    label = f'{channel}@{tag}'\n+    if version and version != tag:\n+        label += f' build {version}'\n+    if channel != origin:\n+        label += f' from {origin}'\n+    return label\n \n-    def __init__(self, ydl, target=None):\n-        self.ydl = ydl\n \n-        self.target_channel, sep, self.target_tag = (target or CHANNEL).rpartition('@')\n-        # stable => stable@latest\n-        if not sep and ('/' in self.target_tag or self.target_tag in UPDATE_SOURCES):\n-            self.target_channel = self.target_tag\n-            self.target_tag = None\n-        elif not self.target_channel:\n-            self.target_channel = CHANNEL.partition('@')[0]\n+@dataclass\n+class UpdateInfo:\n+    \"\"\"\n+    Update target information\n+\n+    Can be created by `query_update()` or manually.\n+\n+    Attributes:\n+        tag                The release tag that will be updated to. If from query_update,\n+                        the value is after API resolution and update spec processing.\n+                        The only property that is required.\n+        version            The actual numeric version (if available) of the binary to be updated to,\n+                        after API resolution and update spec processing. (default: None)\n+        requested_version  Numeric version of the binary being requested (if available),\n+                        after API resolution only. (default: None)\n+        commit             Commit hash (if available) of the binary to be updated to,\n+                        after API resolution and update spec processing. (default: None)\n+                        This value will only match the RELEASE_GIT_HEAD of prerelease builds.\n+        binary_name        Filename of the binary to be updated to. (default: current binary name)\n+        checksum           Expected checksum (if available) of the binary to be\n+                        updated to. (default: None)\n+    \"\"\"\n+    tag: str\n+    version: str | None = None\n+    requested_version: str | None = None\n+    commit: str | None = None\n \n-        if not self.target_tag:\n-            self.target_tag = 'latest'\n+    binary_name: str | None = _get_binary_name()\n+    checksum: str | None = None\n+\n+    _has_update = True\n+\n+\n+class Updater:\n+    # XXX: use class variables to simplify testing\n+    _channel = CHANNEL\n+    _origin = ORIGIN\n+    _update_sources = UPDATE_SOURCES\n+\n+    def __init__(self, ydl, target: str | None = None):\n+        self.ydl = ydl\n+        # For backwards compat, target needs to be treated as if it could be None\n+        self.requested_channel, sep, self.requested_tag = (target or self._channel).rpartition('@')\n+        # Check if requested_tag is actually the requested repo/channel\n+        if not sep and ('/' in self.requested_tag or self.requested_tag in self._update_sources):\n+            self.requested_channel = self.requested_tag\n+            self.requested_tag: str = None  # type: ignore (we set it later)\n+        elif not self.requested_channel:\n+            # User did not specify a channel, so we are requesting the default channel\n+            self.requested_channel = self._channel.partition('@')[0]\n+\n+        # --update should not be treated as an exact tag request even if CHANNEL has a @tag\n+        self._exact = bool(target) and target != self._channel\n+        if not self.requested_tag:\n+            # User did not specify a tag, so we request 'latest' and track that no exact tag was passed\n+            self.requested_tag = 'latest'\n             self._exact = False\n-        elif self.target_tag != 'latest':\n-            self.target_tag = f'tags/{self.target_tag}'\n \n-        if '/' in self.target_channel:\n-            self._target_repo = self.target_channel\n-            if self.target_channel not in (CHANNEL, *UPDATE_SOURCES.values()):\n+        if '/' in self.requested_channel:\n+            # requested_channel is actually a repository\n+            self.requested_repo = self.requested_channel\n+            if not self.requested_repo.startswith('yt-dlp/') and self.requested_repo != self._origin:\n                 self.ydl.report_warning(\n                     f'You are switching to an {self.ydl._format_err(\"unofficial\", \"red\")} executable '\n-                    f'from {self.ydl._format_err(self._target_repo, self.ydl.Styles.EMPHASIS)}. '\n+                    f'from {self.ydl._format_err(self.requested_repo, self.ydl.Styles.EMPHASIS)}. '\n                     f'Run {self.ydl._format_err(\"at your own risk\", \"light red\")}')\n                 self._block_restart('Automatically restarting into custom builds is disabled for security reasons')\n         else:\n-            self._target_repo = UPDATE_SOURCES.get(self.target_channel)\n-            if not self._target_repo:\n+            # Check if requested_channel resolves to a known repository or else raise\n+            self.requested_repo = self._update_sources.get(self.requested_channel)\n+            if not self.requested_repo:\n                 self._report_error(\n-                    f'Invalid update channel {self.target_channel!r} requested. '\n-                    f'Valid channels are {\", \".join(UPDATE_SOURCES)}', True)\n+                    f'Invalid update channel {self.requested_channel!r} requested. '\n+                    f'Valid channels are {\", \".join(self._update_sources)}', True)\n \n-    def _version_compare(self, a, b, channel=CHANNEL):\n-        if self._exact and channel != self.target_channel:\n-            return False\n+        self._identifier = f'{detect_variant()} {system_identifier()}'\n \n-        if _VERSION_RE.fullmatch(f'{a}.{b}'):\n-            a, b = version_tuple(a), version_tuple(b)\n-            return a == b if self._exact else a >= b\n-        return a == b\n+    @property\n+    def current_version(self):\n+        \"\"\"Current version\"\"\"\n+        return __version__\n \n-    @functools.cached_property\n-    def _tag(self):\n-        if self._version_compare(self.current_version, self.latest_version):\n-            return self.target_tag\n-\n-        identifier = f'{detect_variant()} {self.target_channel} {system_identifier()}'\n-        for line in self._download('_update_spec', 'latest').decode().splitlines():\n-            if not line.startswith('lock '):\n-                continue\n-            _, tag, pattern = line.split(' ', 2)\n-            if re.match(pattern, identifier):\n-                if not self._exact:\n-                    return f'tags/{tag}'\n-                elif self.target_tag == 'latest' or not self._version_compare(\n-                        tag, self.target_tag[5:], channel=self.target_channel):\n-                    self._report_error(\n-                        f'yt-dlp cannot be updated above {tag} since you are on an older Python version', True)\n-                    return f'tags/{self.current_version}'\n-        return self.target_tag\n-\n-    @cached_method\n-    def _get_version_info(self, tag):\n-        url = f'{API_BASE_URL}/{self._target_repo}/releases/{tag}'\n+    @property\n+    def current_commit(self):\n+        \"\"\"Current commit hash\"\"\"\n+        return RELEASE_GIT_HEAD\n+\n+    def _download_asset(self, name, tag=None):\n+        if not tag:\n+            tag = self.requested_tag\n+\n+        path = 'latest/download' if tag == 'latest' else f'download/{tag}'\n+        url = f'https://github.com/{self.requested_repo}/releases/{path}/{name}'\n+        self.ydl.write_debug(f'Downloading {name} from {url}')\n+        return self.ydl.urlopen(url).read()\n+\n+    def _call_api(self, tag):\n+        tag = f'tags/{tag}' if tag != 'latest' else tag\n+        url = f'{API_BASE_URL}/{self.requested_repo}/releases/{tag}'\n         self.ydl.write_debug(f'Fetching release info: {url}')\n         return json.loads(self.ydl.urlopen(Request(url, headers={\n             'Accept': 'application/vnd.github+json',\n@@ -195,105 +275,175 @@ def _get_version_info(self, tag):\n             'X-GitHub-Api-Version': '2022-11-28',\n         })).read().decode())\n \n-    @property\n-    def current_version(self):\n-        \"\"\"Current version\"\"\"\n-        return __version__\n+    def _get_version_info(self, tag: str) -> tuple[str | None, str | None]:\n+        if _VERSION_RE.fullmatch(tag):\n+            return tag, None\n \n-    @staticmethod\n-    def _label(channel, tag):\n-        \"\"\"Label for a given channel and tag\"\"\"\n-        return f'{channel}@{remove_start(tag, \"tags/\")}'\n+        api_info = self._call_api(tag)\n \n-    def _get_actual_tag(self, tag):\n-        if tag.startswith('tags/'):\n-            return tag[5:]\n-        return self._get_version_info(tag)['tag_name']\n+        if tag == 'latest':\n+            requested_version = api_info['tag_name']\n+        else:\n+            match = re.search(rf'\\s+(?P<version>{_VERSION_RE.pattern})$', api_info.get('name', ''))\n+            requested_version = match.group('version') if match else None\n \n-    @property\n-    def new_version(self):\n-        \"\"\"Version of the latest release we can update to\"\"\"\n-        return self._get_actual_tag(self._tag)\n+        if re.fullmatch(_HASH_PATTERN, api_info.get('target_commitish', '')):\n+            target_commitish = api_info['target_commitish']\n+        else:\n+            match = _COMMIT_RE.match(api_info.get('body', ''))\n+            target_commitish = match.group('hash') if match else None\n \n-    @property\n-    def latest_version(self):\n-        \"\"\"Version of the target release\"\"\"\n-        return self._get_actual_tag(self.target_tag)\n+        if not (requested_version or target_commitish):\n+            self._report_error('One of either version or commit hash must be available on the release', expected=True)\n \n-    @property\n-    def has_update(self):\n-        \"\"\"Whether there is an update available\"\"\"\n-        return not self._version_compare(self.current_version, self.new_version)\n+        return requested_version, target_commitish\n \n-    @functools.cached_property\n-    def filename(self):\n-        \"\"\"Filename of the executable\"\"\"\n-        return compat_realpath(_get_variant_and_executable_path()[1])\n+    def _download_update_spec(self, source_tags):\n+        for tag in source_tags:\n+            try:\n+                return self._download_asset('_update_spec', tag=tag).decode()\n+            except network_exceptions as error:\n+                if isinstance(error, HTTPError) and error.status == 404:\n+                    continue\n+                self._report_network_error(f'fetch update spec: {error}')\n \n-    def _download(self, name, tag):\n-        slug = 'latest/download' if tag == 'latest' else f'download/{tag[5:]}'\n-        url = f'https://github.com/{self._target_repo}/releases/{slug}/{name}'\n-        self.ydl.write_debug(f'Downloading {name} from {url}')\n-        return self.ydl.urlopen(url).read()\n+        self._report_error(\n+            f'The requested tag {self.requested_tag} does not exist for {self.requested_repo}', True)\n+        return None\n+\n+    def _process_update_spec(self, lockfile: str, resolved_tag: str):\n+        lines = lockfile.splitlines()\n+        is_version2 = any(line.startswith('lockV2 ') for line in lines)\n+\n+        for line in lines:\n+            if is_version2:\n+                if not line.startswith(f'lockV2 {self.requested_repo} '):\n+                    continue\n+                _, _, tag, pattern = line.split(' ', 3)\n+            else:\n+                if not line.startswith('lock '):\n+                    continue\n+                _, tag, pattern = line.split(' ', 2)\n+\n+            if re.match(pattern, self._identifier):\n+                if _VERSION_RE.fullmatch(tag):\n+                    if not self._exact:\n+                        return tag\n+                    elif self._version_compare(tag, resolved_tag):\n+                        return resolved_tag\n+                elif tag != resolved_tag:\n+                    continue\n \n-    @functools.cached_property\n-    def release_name(self):\n-        \"\"\"The release filename\"\"\"\n-        return f'yt-dlp{_FILE_SUFFIXES[detect_variant()]}'\n+                self._report_error(\n+                    f'yt-dlp cannot be updated to {resolved_tag} since you are on an older Python version', True)\n+                return None\n \n-    @functools.cached_property\n-    def release_hash(self):\n-        \"\"\"Hash of the latest release\"\"\"\n-        hash_data = dict(ln.split()[::-1] for ln in self._download('SHA2-256SUMS', self._tag).decode().splitlines())\n-        return hash_data[self.release_name]\n+        return resolved_tag\n \n-    def _report_error(self, msg, expected=False):\n-        self.ydl.report_error(msg, tb=False if expected else None)\n-        self.ydl._download_retcode = 100\n+    def _version_compare(self, a: str, b: str):\n+        \"\"\"\n+        Compare two version strings\n \n-    def _report_permission_error(self, file):\n-        self._report_error(f'Unable to write to {file}; Try running as administrator', True)\n+        This function SHOULD NOT be called if self._exact == True\n+        \"\"\"\n+        if _VERSION_RE.fullmatch(f'{a}.{b}'):\n+            return version_tuple(a) >= version_tuple(b)\n+        return a == b\n \n-    def _report_network_error(self, action, delim=';'):\n-        self._report_error(\n-            f'Unable to {action}{delim} visit  '\n-            f'https://github.com/{self._target_repo}/releases/{self.target_tag.replace(\"tags/\", \"tag/\")}', True)\n+    def query_update(self, *, _output=False) -> UpdateInfo | None:\n+        \"\"\"Fetches and returns info about the available update\"\"\"\n+        if not self.requested_repo:\n+            self._report_error('No target repository could be determined from input')\n+            return None\n \n-    def check_update(self):\n-        \"\"\"Report whether there is an update available\"\"\"\n-        if not self._target_repo:\n-            return False\n         try:\n-            self.ydl.to_screen((\n-                f'Available version: {self._label(self.target_channel, self.latest_version)}, ' if self.target_tag == 'latest' else ''\n-            ) + f'Current version: {self._label(CHANNEL, self.current_version)}')\n+            requested_version, target_commitish = self._get_version_info(self.requested_tag)\n         except network_exceptions as e:\n-            return self._report_network_error(f'obtain version info ({e})', delim='; Please try again later or')\n-\n+            self._report_network_error(f'obtain version info ({e})', delim='; Please try again later or')\n+            return None\n+\n+        if self._exact and self._origin != self.requested_repo:\n+            has_update = True\n+        elif requested_version:\n+            if self._exact:\n+                has_update = self.current_version != requested_version\n+            else:\n+                has_update = not self._version_compare(self.current_version, requested_version)\n+        elif target_commitish:\n+            has_update = target_commitish != self.current_commit\n+        else:\n+            has_update = False\n+\n+        resolved_tag = requested_version if self.requested_tag == 'latest' else self.requested_tag\n+        current_label = _make_label(self._origin, self._channel.partition(\"@\")[2] or self.current_version, self.current_version)\n+        requested_label = _make_label(self.requested_repo, resolved_tag, requested_version)\n+        latest_or_requested = f'{\"Latest\" if self.requested_tag == \"latest\" else \"Requested\"} version: {requested_label}'\n+        if not has_update:\n+            if _output:\n+                self.ydl.to_screen(f'{latest_or_requested}\\nyt-dlp is up to date ({current_label})')\n+            return None\n+\n+        update_spec = self._download_update_spec(('latest', None) if requested_version else (None,))\n+        if not update_spec:\n+            return None\n+        # `result_` prefixed vars == post-_process_update_spec() values\n+        result_tag = self._process_update_spec(update_spec, resolved_tag)\n+        if not result_tag or result_tag == self.current_version:\n+            return None\n+        elif result_tag == resolved_tag:\n+            result_version = requested_version\n+        elif _VERSION_RE.fullmatch(result_tag):\n+            result_version = result_tag\n+        else:  # actual version being updated to is unknown\n+            result_version = None\n+\n+        checksum = None\n+        # Non-updateable variants can get update_info but need to skip checksum\n         if not is_non_updateable():\n-            self.ydl.to_screen(f'Current Build Hash: {_sha256_file(self.filename)}')\n-\n-        if self.has_update:\n-            return True\n-\n-        if self.target_tag == self._tag:\n-            self.ydl.to_screen(f'yt-dlp is up to date ({self._label(CHANNEL, self.current_version)})')\n-        elif not self._exact:\n-            self.ydl.report_warning('yt-dlp cannot be updated any further since you are on an older Python version')\n-        return False\n-\n-    def update(self):\n+            try:\n+                hashes = self._download_asset('SHA2-256SUMS', result_tag)\n+            except network_exceptions as error:\n+                if not isinstance(error, HTTPError) or error.status != 404:\n+                    self._report_network_error(f'fetch checksums: {error}')\n+                    return None\n+                self.ydl.report_warning('No hash information found for the release, skipping verification')\n+            else:\n+                for ln in hashes.decode().splitlines():\n+                    if ln.endswith(_get_binary_name()):\n+                        checksum = ln.split()[0]\n+                        break\n+                if not checksum:\n+                    self.ydl.report_warning('The hash could not be found in the checksum file, skipping verification')\n+\n+        if _output:\n+            update_label = _make_label(self.requested_repo, result_tag, result_version)\n+            self.ydl.to_screen(\n+                f'Current version: {current_label}\\n{latest_or_requested}'\n+                + (f'\\nUpgradable to: {update_label}' if update_label != requested_label else ''))\n+\n+        return UpdateInfo(\n+            tag=result_tag,\n+            version=result_version,\n+            requested_version=requested_version,\n+            commit=target_commitish if result_tag == resolved_tag else None,\n+            checksum=checksum)\n+\n+    def update(self, update_info=NO_DEFAULT):\n         \"\"\"Update yt-dlp executable to the latest version\"\"\"\n-        if not self.check_update():\n-            return\n+        if update_info is NO_DEFAULT:\n+            update_info = self.query_update(_output=True)\n+        if not update_info:\n+            return False\n+\n         err = is_non_updateable()\n         if err:\n-            return self._report_error(err, True)\n-        self.ydl.to_screen(f'Updating to {self._label(self.target_channel, self.new_version)} ...')\n-        if (_VERSION_RE.fullmatch(self.target_tag[5:])\n-                and version_tuple(self.target_tag[5:]) < (2023, 3, 2)):\n-            self.ydl.report_warning('You are downgrading to a version without --update-to')\n-            self._block_restart('Cannot automatically restart to a version without --update-to')\n+            self._report_error(err, True)\n+            return False\n+\n+        self.ydl.to_screen(f'Current Build Hash: {_sha256_file(self.filename)}')\n+\n+        update_label = _make_label(self.requested_repo, update_info.tag, update_info.version)\n+        self.ydl.to_screen(f'Updating to {update_label} ...')\n \n         directory = os.path.dirname(self.filename)\n         if not os.access(self.filename, os.W_OK):\n@@ -312,20 +462,17 @@ def update(self):\n             return self._report_error('Unable to remove the old version')\n \n         try:\n-            newcontent = self._download(self.release_name, self._tag)\n+            newcontent = self._download_asset(update_info.binary_name, update_info.tag)\n         except network_exceptions as e:\n             if isinstance(e, HTTPError) and e.status == 404:\n                 return self._report_error(\n-                    f'The requested tag {self._label(self.target_channel, self.target_tag)} does not exist', True)\n-            return self._report_network_error(f'fetch updates: {e}')\n+                    f'The requested tag {self.requested_repo}@{update_info.tag} does not exist', True)\n+            return self._report_network_error(f'fetch updates: {e}', tag=update_info.tag)\n \n-        try:\n-            expected_hash = self.release_hash\n-        except Exception:\n-            self.ydl.report_warning('no hash information found for the release')\n-        else:\n-            if hashlib.sha256(newcontent).hexdigest() != expected_hash:\n-                return self._report_network_error('verify the new executable')\n+        if not update_info.checksum:\n+            self._block_restart('Automatically restarting into unverified builds is disabled for security reasons')\n+        elif hashlib.sha256(newcontent).hexdigest() != update_info.checksum:\n+            return self._report_network_error('verify the new executable', tag=update_info.tag)\n \n         try:\n             with open(new_filename, 'wb') as outf:\n@@ -362,9 +509,14 @@ def update(self):\n                 return self._report_error(\n                     f'Unable to set permissions. Run: sudo chmod a+rx {compat_shlex_quote(self.filename)}')\n \n-        self.ydl.to_screen(f'Updated yt-dlp to {self._label(self.target_channel, self.new_version)}')\n+        self.ydl.to_screen(f'Updated yt-dlp to {update_label}')\n         return True\n \n+    @functools.cached_property\n+    def filename(self):\n+        \"\"\"Filename of the executable\"\"\"\n+        return compat_realpath(_get_variant_and_executable_path()[1])\n+\n     @functools.cached_property\n     def cmd(self):\n         \"\"\"The command-line to run the executable, if known\"\"\"\n@@ -387,6 +539,71 @@ def wrapper():\n             return self.ydl._download_retcode\n         self.restart = wrapper\n \n+    def _report_error(self, msg, expected=False):\n+        self.ydl.report_error(msg, tb=False if expected else None)\n+        self.ydl._download_retcode = 100\n+\n+    def _report_permission_error(self, file):\n+        self._report_error(f'Unable to write to {file}; try running as administrator', True)\n+\n+    def _report_network_error(self, action, delim=';', tag=None):\n+        if not tag:\n+            tag = self.requested_tag\n+        self._report_error(\n+            f'Unable to {action}{delim} visit  https://github.com/{self.requested_repo}/releases/'\n+            + tag if tag == \"latest\" else f\"tag/{tag}\", True)\n+\n+    # XXX: Everything below this line in this class is deprecated / for compat only\n+    @property\n+    def _target_tag(self):\n+        \"\"\"Deprecated; requested tag with 'tags/' prepended when necessary for API calls\"\"\"\n+        return f'tags/{self.requested_tag}' if self.requested_tag != 'latest' else self.requested_tag\n+\n+    def _check_update(self):\n+        \"\"\"Deprecated; report whether there is an update available\"\"\"\n+        return bool(self.query_update(_output=True))\n+\n+    def __getattr__(self, attribute: str):\n+        \"\"\"Compat getter function for deprecated attributes\"\"\"\n+        deprecated_props_map = {\n+            'check_update': '_check_update',\n+            'target_tag': '_target_tag',\n+            'target_channel': 'requested_channel',\n+        }\n+        update_info_props_map = {\n+            'has_update': '_has_update',\n+            'new_version': 'version',\n+            'latest_version': 'requested_version',\n+            'release_name': 'binary_name',\n+            'release_hash': 'checksum',\n+        }\n+\n+        if attribute not in deprecated_props_map and attribute not in update_info_props_map:\n+            raise AttributeError(f'{type(self).__name__!r} object has no attribute {attribute!r}')\n+\n+        msg = f'{type(self).__name__}.{attribute} is deprecated and will be removed in a future version'\n+        if attribute in deprecated_props_map:\n+            source_name = deprecated_props_map[attribute]\n+            if not source_name.startswith('_'):\n+                msg += f'. Please use {source_name!r} instead'\n+            source = self\n+            mapping = deprecated_props_map\n+\n+        else:  # attribute in update_info_props_map\n+            msg += '. Please call query_update() instead'\n+            source = self.query_update()\n+            if source is None:\n+                source = UpdateInfo('', None, None, None)\n+                source._has_update = False\n+            mapping = update_info_props_map\n+\n+        deprecation_warning(msg)\n+        for target_name, source_name in mapping.items():\n+            value = getattr(source, source_name)\n+            setattr(self, target_name, value)\n+\n+        return getattr(self, attribute)\n+\n \n def run_update(ydl):\n     \"\"\"Update the program file with the latest version from the repository\n@@ -395,45 +612,4 @@ def run_update(ydl):\n     return Updater(ydl).update()\n \n \n-# Deprecated\n-def update_self(to_screen, verbose, opener):\n-    import traceback\n-\n-    deprecation_warning(f'\"{__name__}.update_self\" is deprecated and may be removed '\n-                        f'in a future version. Use \"{__name__}.run_update(ydl)\" instead')\n-\n-    printfn = to_screen\n-\n-    class FakeYDL():\n-        to_screen = printfn\n-\n-        def report_warning(self, msg, *args, **kwargs):\n-            return printfn(f'WARNING: {msg}', *args, **kwargs)\n-\n-        def report_error(self, msg, tb=None):\n-            printfn(f'ERROR: {msg}')\n-            if not verbose:\n-                return\n-            if tb is None:\n-                # Copied from YoutubeDL.trouble\n-                if sys.exc_info()[0]:\n-                    tb = ''\n-                    if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n-                        tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))\n-                    tb += traceback.format_exc()\n-                else:\n-                    tb_data = traceback.format_list(traceback.extract_stack())\n-                    tb = ''.join(tb_data)\n-            if tb:\n-                printfn(tb)\n-\n-        def write_debug(self, msg, *args, **kwargs):\n-            printfn(f'[debug] {msg}', *args, **kwargs)\n-\n-        def urlopen(self, url):\n-            return opener.open(url)\n-\n-    return run_update(FakeYDL())\n-\n-\n __all__ = ['Updater']\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/utils/_legacy.py",
            "diff": "diff --git a/yt_dlp/utils/_legacy.py b/yt_dlp/utils/_legacy.py\nindex dde02092..aa9f46d2 100644\n--- a/yt_dlp/utils/_legacy.py\n+++ b/yt_dlp/utils/_legacy.py\n@@ -1,4 +1,6 @@\n \"\"\"No longer used and new code should not use. Exists only for API compat.\"\"\"\n+import asyncio\n+import atexit\n import platform\n import struct\n import sys\n@@ -32,6 +34,77 @@\n has_websockets = bool(websockets)\n \n \n+class WebSocketsWrapper:\n+    \"\"\"Wraps websockets module to use in non-async scopes\"\"\"\n+    pool = None\n+\n+    def __init__(self, url, headers=None, connect=True, **ws_kwargs):\n+        self.loop = asyncio.new_event_loop()\n+        # XXX: \"loop\" is deprecated\n+        self.conn = websockets.connect(\n+            url, extra_headers=headers, ping_interval=None,\n+            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'), **ws_kwargs)\n+        if connect:\n+            self.__enter__()\n+        atexit.register(self.__exit__, None, None, None)\n+\n+    def __enter__(self):\n+        if not self.pool:\n+            self.pool = self.run_with_loop(self.conn.__aenter__(), self.loop)\n+        return self\n+\n+    def send(self, *args):\n+        self.run_with_loop(self.pool.send(*args), self.loop)\n+\n+    def recv(self, *args):\n+        return self.run_with_loop(self.pool.recv(*args), self.loop)\n+\n+    def __exit__(self, type, value, traceback):\n+        try:\n+            return self.run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)\n+        finally:\n+            self.loop.close()\n+            self._cancel_all_tasks(self.loop)\n+\n+    # taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications\n+    # for contributors: If there's any new library using asyncio needs to be run in non-async, move these function out of this class\n+    @staticmethod\n+    def run_with_loop(main, loop):\n+        if not asyncio.iscoroutine(main):\n+            raise ValueError(f'a coroutine was expected, got {main!r}')\n+\n+        try:\n+            return loop.run_until_complete(main)\n+        finally:\n+            loop.run_until_complete(loop.shutdown_asyncgens())\n+            if hasattr(loop, 'shutdown_default_executor'):\n+                loop.run_until_complete(loop.shutdown_default_executor())\n+\n+    @staticmethod\n+    def _cancel_all_tasks(loop):\n+        to_cancel = asyncio.all_tasks(loop)\n+\n+        if not to_cancel:\n+            return\n+\n+        for task in to_cancel:\n+            task.cancel()\n+\n+        # XXX: \"loop\" is removed in python 3.10+\n+        loop.run_until_complete(\n+            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True))\n+\n+        for task in to_cancel:\n+            if task.cancelled():\n+                continue\n+            if task.exception() is not None:\n+                loop.call_exception_handler({\n+                    'message': 'unhandled exception during asyncio.run() shutdown',\n+                    'exception': task.exception(),\n+                    'task': task,\n+                })\n+\n+\n def load_plugins(name, suffix, namespace):\n     from ..plugins import load_plugins\n     ret = load_plugins(name, suffix)\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/utils/_utils.py",
            "diff": "diff --git a/yt_dlp/utils/_utils.py b/yt_dlp/utils/_utils.py\nindex f5552ce8..89a0d4cf 100644\n--- a/yt_dlp/utils/_utils.py\n+++ b/yt_dlp/utils/_utils.py\n@@ -1,5 +1,3 @@\n-import asyncio\n-import atexit\n import base64\n import binascii\n import calendar\n@@ -54,7 +52,7 @@\n     compat_os_name,\n     compat_shlex_quote,\n )\n-from ..dependencies import websockets, xattr\n+from ..dependencies import xattr\n \n __name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module\n \n@@ -560,7 +558,7 @@ def decode(self, s):\n                     s = self._close_object(e)\n                     if s is not None:\n                         continue\n-                raise type(e)(f'{e.msg} in {s[e.pos-10:e.pos+10]!r}', s, e.pos)\n+                raise type(e)(f'{e.msg} in {s[e.pos - 10:e.pos + 10]!r}', s, e.pos)\n         assert False, 'Too many attempts to decode JSON'\n \n \n@@ -638,7 +636,7 @@ def replace_insane(char):\n         elif char in '\\\\/|*<>':\n             return '\\0_'\n         if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):\n-            return '\\0_'\n+            return '' if unicodedata.category(char)[0] in 'CM' else '\\0_'\n         return char\n \n     # Replace look-alike Unicode glyphs\n@@ -669,6 +667,7 @@ def replace_insane(char):\n \n def sanitize_path(s, force=False):\n     \"\"\"Sanitizes and normalizes path on Windows\"\"\"\n+    # XXX: this handles drive relative paths (c:sth) incorrectly\n     if sys.platform == 'win32':\n         force = False\n         drive_or_unc, _ = os.path.splitdrive(s)\n@@ -687,7 +686,10 @@ def sanitize_path(s, force=False):\n         sanitized_path.insert(0, drive_or_unc + os.path.sep)\n     elif force and s and s[0] == os.path.sep:\n         sanitized_path.insert(0, os.path.sep)\n-    return os.path.join(*sanitized_path)\n+    # TODO: Fix behavioral differences <3.12\n+    # The workaround using `normpath` only superficially passes tests\n+    # Ref: https://github.com/python/cpython/pull/100351\n+    return os.path.normpath(os.path.join(*sanitized_path))\n \n \n def sanitize_url(url, *, scheme='http'):\n@@ -821,7 +823,7 @@ def _fix(key):\n         _fix('LD_LIBRARY_PATH')  # Linux\n         _fix('DYLD_LIBRARY_PATH')  # macOS\n \n-    def __init__(self, *args, env=None, text=False, **kwargs):\n+    def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\n         if env is None:\n             env = os.environ.copy()\n         self._fix_pyinstaller_ld_path(env)\n@@ -831,7 +833,21 @@ def __init__(self, *args, env=None, text=False, **kwargs):\n             kwargs['universal_newlines'] = True  # For 3.6 compatibility\n             kwargs.setdefault('encoding', 'utf-8')\n             kwargs.setdefault('errors', 'replace')\n-        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\n+\n+        if shell and compat_os_name == 'nt' and kwargs.get('executable') is None:\n+            if not isinstance(args, str):\n+                args = ' '.join(compat_shlex_quote(a) for a in args)\n+            shell = False\n+            args = f'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"'\n+\n+        super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)\n+\n+    def __comspec(self):\n+        comspec = os.environ.get('ComSpec') or os.path.join(\n+            os.environ.get('SystemRoot', ''), 'System32', 'cmd.exe')\n+        if os.path.isabs(comspec):\n+            return comspec\n+        raise FileNotFoundError('shell not found: neither %ComSpec% nor %SystemRoot% is set')\n \n     def communicate_or_kill(self, *args, **kwargs):\n         try:\n@@ -1256,7 +1272,7 @@ def datetime_from_str(date_str, precision='auto', format='%Y%m%d'):\n     if precision == 'auto':\n         auto_precision = True\n         precision = 'microsecond'\n-    today = datetime_round(datetime.datetime.utcnow(), precision)\n+    today = datetime_round(datetime.datetime.now(datetime.timezone.utc), precision)\n     if date_str in ('now', 'today'):\n         return today\n     if date_str == 'yesterday':\n@@ -1319,8 +1335,8 @@ def datetime_round(dt, precision='day'):\n         'second': 1,\n     }\n     roundto = lambda x, n: ((x + n / 2) // n) * n\n-    timestamp = calendar.timegm(dt.timetuple())\n-    return datetime.datetime.utcfromtimestamp(roundto(timestamp, unit_seconds[precision]))\n+    timestamp = roundto(calendar.timegm(dt.timetuple()), unit_seconds[precision])\n+    return datetime.datetime.fromtimestamp(timestamp, datetime.timezone.utc)\n \n \n def hyphenate_date(date_str):\n@@ -1869,6 +1885,7 @@ def setproctitle(title):\n     buf = ctypes.create_string_buffer(len(title_bytes))\n     buf.value = title_bytes\n     try:\n+        # PR_SET_NAME = 15      Ref: /usr/include/linux/prctl.h\n         libc.prctl(15, buf, 0, 0, 0)\n     except AttributeError:\n         return  # Strange libc, just skip this\n@@ -2244,6 +2261,9 @@ def __getitem__(self, idx):\n             raise self.IndexError()\n         return entries[0]\n \n+    def __bool__(self):\n+        return bool(self.getslice(0, 1))\n+\n \n class OnDemandPagedList(PagedList):\n     \"\"\"Download pages until a page with less than maximum results\"\"\"\n@@ -2723,9 +2743,10 @@ def fix_kv(m):\n     def create_map(mobj):\n         return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n \n+    code = re.sub(r'(?:new\\s+)?Array\\((.*?)\\)', r'[\\g<1>]', code)\n     code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n     if not strict:\n-        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n+        code = re.sub(rf'new Date\\(({STRING_RE})\\)', r'\\g<1>', code)\n         code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n         code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n         code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n@@ -2847,6 +2868,7 @@ def mimetype2ext(mt, default=NO_DEFAULT):\n         'quicktime': 'mov',\n         'webm': 'webm',\n         'vp9': 'vp9',\n+        'video/ogg': 'ogv',\n         'x-flv': 'flv',\n         'x-m4v': 'm4v',\n         'x-matroska': 'mkv',\n@@ -4421,10 +4443,12 @@ def write_xattr(path, key, value):\n             raise XAttrMetadataError(e.errno, e.strerror)\n         return\n \n-    # UNIX Method 1. Use xattrs/pyxattrs modules\n+    # UNIX Method 1. Use os.setxattr/xattrs/pyxattrs modules\n \n     setxattr = None\n-    if getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':\n+    if callable(getattr(os, 'setxattr', None)):\n+        setxattr = os.setxattr\n+    elif getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':\n         # Unicode arguments are not supported in pyxattr until version 0.5.0\n         # See https://github.com/ytdl-org/youtube-dl/issues/5498\n         if version_tuple(xattr.__version__) >= (0, 5, 0):\n@@ -4769,8 +4793,9 @@ def parse_http_range(range):\n \n \n def read_stdin(what):\n-    eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'\n-    write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')\n+    if what:\n+        eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'\n+        write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')\n     return sys.stdin\n \n \n@@ -4901,77 +4926,6 @@ def parse_args(self):\n         return self.parser.parse_args(self.all_args)\n \n \n-class WebSocketsWrapper:\n-    \"\"\"Wraps websockets module to use in non-async scopes\"\"\"\n-    pool = None\n-\n-    def __init__(self, url, headers=None, connect=True):\n-        self.loop = asyncio.new_event_loop()\n-        # XXX: \"loop\" is deprecated\n-        self.conn = websockets.connect(\n-            url, extra_headers=headers, ping_interval=None,\n-            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'))\n-        if connect:\n-            self.__enter__()\n-        atexit.register(self.__exit__, None, None, None)\n-\n-    def __enter__(self):\n-        if not self.pool:\n-            self.pool = self.run_with_loop(self.conn.__aenter__(), self.loop)\n-        return self\n-\n-    def send(self, *args):\n-        self.run_with_loop(self.pool.send(*args), self.loop)\n-\n-    def recv(self, *args):\n-        return self.run_with_loop(self.pool.recv(*args), self.loop)\n-\n-    def __exit__(self, type, value, traceback):\n-        try:\n-            return self.run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)\n-        finally:\n-            self.loop.close()\n-            self._cancel_all_tasks(self.loop)\n-\n-    # taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications\n-    # for contributors: If there's any new library using asyncio needs to be run in non-async, move these function out of this class\n-    @staticmethod\n-    def run_with_loop(main, loop):\n-        if not asyncio.iscoroutine(main):\n-            raise ValueError(f'a coroutine was expected, got {main!r}')\n-\n-        try:\n-            return loop.run_until_complete(main)\n-        finally:\n-            loop.run_until_complete(loop.shutdown_asyncgens())\n-            if hasattr(loop, 'shutdown_default_executor'):\n-                loop.run_until_complete(loop.shutdown_default_executor())\n-\n-    @staticmethod\n-    def _cancel_all_tasks(loop):\n-        to_cancel = asyncio.all_tasks(loop)\n-\n-        if not to_cancel:\n-            return\n-\n-        for task in to_cancel:\n-            task.cancel()\n-\n-        # XXX: \"loop\" is removed in python 3.10+\n-        loop.run_until_complete(\n-            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True))\n-\n-        for task in to_cancel:\n-            if task.cancelled():\n-                continue\n-            if task.exception() is not None:\n-                loop.call_exception_handler({\n-                    'message': 'unhandled exception during asyncio.run() shutdown',\n-                    'exception': task.exception(),\n-                    'task': task,\n-                })\n-\n-\n def merge_headers(*dicts):\n     \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"\n     return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}\n@@ -5120,7 +5074,7 @@ def truncate_string(s, left, right=0):\n     assert left > 3 and right >= 0\n     if s is None or len(s) <= left + right:\n         return s\n-    return f'{s[:left-3]}...{s[-right:] if right else \"\"}'\n+    return f'{s[:left - 3]}...{s[-right:] if right else \"\"}'\n \n \n def orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/utils/networking.py",
            "diff": "diff --git a/yt_dlp/utils/networking.py b/yt_dlp/utils/networking.py\nindex ba0493cc..4b73252c 100644\n--- a/yt_dlp/utils/networking.py\n+++ b/yt_dlp/utils/networking.py\n@@ -67,7 +67,7 @@ def __init__(self, *args, **kwargs):\n     def __setitem__(self, key, value):\n         if isinstance(value, bytes):\n             value = value.decode('latin-1')\n-        super().__setitem__(key.title(), str(value))\n+        super().__setitem__(key.title(), str(value).strip())\n \n     def __getitem__(self, key):\n         return super().__getitem__(key.title())\n@@ -123,6 +123,7 @@ def clean_headers(headers: HTTPHeaderDict):\n     if 'Youtubedl-No-Compression' in headers:  # compat\n         del headers['Youtubedl-No-Compression']\n         headers['Accept-Encoding'] = 'identity'\n+    headers.pop('Ytdl-socks-proxy', None)\n \n \n def remove_dot_segments(path):\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/utils/progress.py",
            "diff": "diff --git a/yt_dlp/utils/progress.py b/yt_dlp/utils/progress.py\nnew file mode 100644\nindex 00000000..f254a388\n--- /dev/null\n+++ b/yt_dlp/utils/progress.py\n@@ -0,0 +1,109 @@\n+from __future__ import annotations\n+\n+import bisect\n+import threading\n+import time\n+\n+\n+class ProgressCalculator:\n+    # Time to calculate the speed over (seconds)\n+    SAMPLING_WINDOW = 3\n+    # Minimum timeframe before to sample next downloaded bytes (seconds)\n+    SAMPLING_RATE = 0.05\n+    # Time before showing eta (seconds)\n+    GRACE_PERIOD = 1\n+\n+    def __init__(self, initial: int):\n+        self._initial = initial or 0\n+        self.downloaded = self._initial\n+\n+        self.elapsed: float = 0\n+        self.speed = SmoothValue(0, smoothing=0.7)\n+        self.eta = SmoothValue(None, smoothing=0.9)\n+\n+        self._total = 0\n+        self._start_time = time.monotonic()\n+        self._last_update = self._start_time\n+\n+        self._lock = threading.Lock()\n+        self._thread_sizes: dict[int, int] = {}\n+\n+        self._times = [self._start_time]\n+        self._downloaded = [self.downloaded]\n+\n+    @property\n+    def total(self):\n+        return self._total\n+\n+    @total.setter\n+    def total(self, value: int | None):\n+        with self._lock:\n+            if value is not None and value < self.downloaded:\n+                value = self.downloaded\n+\n+            self._total = value\n+\n+    def thread_reset(self):\n+        current_thread = threading.get_ident()\n+        with self._lock:\n+            self._thread_sizes[current_thread] = 0\n+\n+    def update(self, size: int | None):\n+        if not size:\n+            return\n+\n+        current_thread = threading.get_ident()\n+\n+        with self._lock:\n+            last_size = self._thread_sizes.get(current_thread, 0)\n+            self._thread_sizes[current_thread] = size\n+            self._update(size - last_size)\n+\n+    def _update(self, size: int):\n+        current_time = time.monotonic()\n+\n+        self.downloaded += size\n+        self.elapsed = current_time - self._start_time\n+        if self.total is not None and self.downloaded > self.total:\n+            self._total = self.downloaded\n+\n+        if self._last_update + self.SAMPLING_RATE > current_time:\n+            return\n+        self._last_update = current_time\n+\n+        self._times.append(current_time)\n+        self._downloaded.append(self.downloaded)\n+\n+        offset = bisect.bisect_left(self._times, current_time - self.SAMPLING_WINDOW)\n+        del self._times[:offset]\n+        del self._downloaded[:offset]\n+        if len(self._times) < 2:\n+            self.speed.reset()\n+            self.eta.reset()\n+            return\n+\n+        download_time = current_time - self._times[0]\n+        if not download_time:\n+            return\n+\n+        self.speed.set((self.downloaded - self._downloaded[0]) / download_time)\n+        if self.total and self.speed.value and self.elapsed > self.GRACE_PERIOD:\n+            self.eta.set((self.total - self.downloaded) / self.speed.value)\n+        else:\n+            self.eta.reset()\n+\n+\n+class SmoothValue:\n+    def __init__(self, initial: float | None, smoothing: float):\n+        self.value = self.smooth = self._initial = initial\n+        self._smoothing = smoothing\n+\n+    def set(self, value: float):\n+        self.value = value\n+        if self.smooth is None:\n+            self.smooth = self.value\n+        else:\n+            self.smooth = (1 - self._smoothing) * value + self._smoothing * self.smooth\n+\n+    def reset(self):\n+        self.value = self.smooth = self._initial\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/utils/traversal.py",
            "diff": "diff --git a/yt_dlp/utils/traversal.py b/yt_dlp/utils/traversal.py\nindex 462c3ba5..8938f4c7 100644\n--- a/yt_dlp/utils/traversal.py\n+++ b/yt_dlp/utils/traversal.py\n@@ -3,12 +3,13 @@\n import inspect\n import itertools\n import re\n+import xml.etree.ElementTree\n \n from ._utils import (\n     IDENTITY,\n     NO_DEFAULT,\n     LazyList,\n-    int_or_none,\n+    deprecation_warning,\n     is_iterable_like,\n     try_call,\n     variadic,\n@@ -17,13 +18,13 @@\n \n def traverse_obj(\n         obj, *paths, default=NO_DEFAULT, expected_type=None, get_all=True,\n-        casesense=True, is_user_input=False, traverse_string=False):\n+        casesense=True, is_user_input=NO_DEFAULT, traverse_string=False):\n     \"\"\"\n     Safely traverse nested `dict`s and `Iterable`s\n \n     >>> obj = [{}, {\"key\": \"value\"}]\n     >>> traverse_obj(obj, (1, \"key\"))\n-    \"value\"\n+    'value'\n \n     Each of the provided `paths` is tested and the first producing a valid result will be returned.\n     The next path will also be tested if the path branched but no results could be found.\n@@ -63,10 +64,8 @@ def traverse_obj(\n     @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n     @param casesense        If `False`, consider string dictionary keys as case insensitive.\n \n-    The following are only meant to be used by YoutubeDL.prepare_outtmpl and are not part of the API\n+    `traverse_string` is only meant to be used by YoutubeDL.prepare_outtmpl and is not part of the API\n \n-    @param is_user_input    Whether the keys are generated from user input.\n-                            If `True` strings get converted to `int`/`slice` if needed.\n     @param traverse_string  Whether to traverse into objects as strings.\n                             If `True`, any non-compatible object will first be\n                             converted into a string and then traversed into.\n@@ -80,6 +79,9 @@ def traverse_obj(\n                             If no `default` is given and the last path branches, a `list` of results\n                             is always returned. If a path ends on a `dict` that result will always be a `dict`.\n     \"\"\"\n+    if is_user_input is not NO_DEFAULT:\n+        deprecation_warning('The is_user_input parameter is deprecated and no longer works')\n+\n     casefold = lambda k: k.casefold() if isinstance(k, str) else k\n \n     if isinstance(expected_type, type):\n@@ -117,7 +119,7 @@ def apply_key(key, obj, is_last):\n             branching = True\n             if isinstance(obj, collections.abc.Mapping):\n                 result = obj.values()\n-            elif is_iterable_like(obj):\n+            elif is_iterable_like(obj) or isinstance(obj, xml.etree.ElementTree.Element):\n                 result = obj\n             elif isinstance(obj, re.Match):\n                 result = obj.groups()\n@@ -131,7 +133,7 @@ def apply_key(key, obj, is_last):\n             branching = True\n             if isinstance(obj, collections.abc.Mapping):\n                 iter_obj = obj.items()\n-            elif is_iterable_like(obj):\n+            elif is_iterable_like(obj) or isinstance(obj, xml.etree.ElementTree.Element):\n                 iter_obj = enumerate(obj)\n             elif isinstance(obj, re.Match):\n                 iter_obj = itertools.chain(\n@@ -167,7 +169,7 @@ def apply_key(key, obj, is_last):\n                 result = next((v for k, v in obj.groupdict().items() if casefold(k) == key), None)\n \n         elif isinstance(key, (int, slice)):\n-            if is_iterable_like(obj, collections.abc.Sequence):\n+            if is_iterable_like(obj, (collections.abc.Sequence, xml.etree.ElementTree.Element)):\n                 branching = isinstance(key, slice)\n                 with contextlib.suppress(IndexError):\n                     result = obj[key]\n@@ -175,6 +177,34 @@ def apply_key(key, obj, is_last):\n                 with contextlib.suppress(IndexError):\n                     result = str(obj)[key]\n \n+        elif isinstance(obj, xml.etree.ElementTree.Element) and isinstance(key, str):\n+            xpath, _, special = key.rpartition('/')\n+            if not special.startswith('@') and special != 'text()':\n+                xpath = key\n+                special = None\n+\n+            # Allow abbreviations of relative paths, absolute paths error\n+            if xpath.startswith('/'):\n+                xpath = f'.{xpath}'\n+            elif xpath and not xpath.startswith('./'):\n+                xpath = f'./{xpath}'\n+\n+            def apply_specials(element):\n+                if special is None:\n+                    return element\n+                if special == '@':\n+                    return element.attrib\n+                if special.startswith('@'):\n+                    return try_call(element.attrib.get, args=(special[1:],))\n+                if special == 'text()':\n+                    return element.text\n+                assert False, f'apply_specials is missing case for {special!r}'\n+\n+            if xpath:\n+                result = list(map(apply_specials, obj.iterfind(xpath)))\n+            else:\n+                result = apply_specials(obj)\n+\n         return branching, result if branching else (result,)\n \n     def lazy_last(iterable):\n@@ -195,14 +225,6 @@ def apply_path(start_obj, path, test_type):\n \n         key = None\n         for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if is_user_input and isinstance(key, str):\n-                if key == ':':\n-                    key = ...\n-                elif ':' in key:\n-                    key = slice(*map(int_or_none, key.split(':')))\n-                elif int_or_none(key) is not None:\n-                    key = int(key)\n-\n             if not casesense and isinstance(key, str):\n                 key = key.casefold()\n \n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/version.py",
            "diff": "diff --git a/yt_dlp/version.py b/yt_dlp/version.py\nindex 67cfe44e..687ef878 100644\n--- a/yt_dlp/version.py\n+++ b/yt_dlp/version.py\n@@ -1,11 +1,15 @@\n # Autogenerated by devscripts/update-version.py\n \n-__version__ = '2023.07.06'\n+__version__ = '2023.12.30'\n \n-RELEASE_GIT_HEAD = 'b532a3481046e1eabb6232ee8196fb696c356ff6'\n+RELEASE_GIT_HEAD = 'f10589e3453009bb523f55849bba144c9b91cf2a'\n \n VARIANT = None\n \n UPDATE_HINT = None\n \n CHANNEL = 'stable'\n+\n+ORIGIN = 'yt-dlp/yt-dlp'\n+\n+_pkg_version = '2023.12.30'\n"
        },
        {
            "commit": "e29a1f6d5a51a55349b025c23fa01bddb8858a71",
            "file_path": "yt_dlp/webvtt.py",
            "diff": "diff --git a/yt_dlp/webvtt.py b/yt_dlp/webvtt.py\nindex dd729827..c80c5863 100644\n--- a/yt_dlp/webvtt.py\n+++ b/yt_dlp/webvtt.py\n@@ -95,6 +95,7 @@ def __init__(self, parser):\n _REGEX_EOF = re.compile(r'\\Z')\n _REGEX_NL = re.compile(r'(?:\\r\\n|[\\r\\n]|$)')\n _REGEX_BLANK = re.compile(r'(?:\\r\\n|[\\r\\n])+')\n+_REGEX_OPTIONAL_WHITESPACE = re.compile(r'[ \\t]*')\n \n \n def _parse_ts(ts):\n@@ -286,6 +287,7 @@ def parse(cls, parser):\n         if not m1:\n             return None\n         m2 = parser.consume(cls._REGEX_SETTINGS)\n+        parser.consume(_REGEX_OPTIONAL_WHITESPACE)\n         if not parser.consume(_REGEX_NL):\n             return None\n \n"
        }
    ]
}