{
    "sha_fail": "9fa62424e0f94d17a247b63898651667216075a5",
    "changed_files": [
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": ".gitignore",
            "diff": "diff --git a/.gitignore b/.gitignore\nindex f050557..7198a65 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -58,3 +58,7 @@ docsite/\n \n # Jupyter notebook\n .ipynb_checkpoints/\n+\n+# Root build assets\n+packages/*/README.md\n+packages/*/LICENSE\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/__init__.py b/packages/graphrag/graphrag/__init__.py\nnew file mode 100644\nindex 0000000..a1e9b58\n--- /dev/null\n+++ b/packages/graphrag/graphrag/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The GraphRAG package.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/__main__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/__main__.py b/packages/graphrag/graphrag/__main__.py\nnew file mode 100644\nindex 0000000..faafaee\n--- /dev/null\n+++ b/packages/graphrag/graphrag/__main__.py\n@@ -0,0 +1,8 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The GraphRAG package.\"\"\"\n+\n+from graphrag.cli.main import app\n+\n+app(prog_name=\"graphrag\")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/api/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/api/__init__.py b/packages/graphrag/graphrag/api/__init__.py\nnew file mode 100644\nindex 0000000..05692c4\n--- /dev/null\n+++ b/packages/graphrag/graphrag/api/__init__.py\n@@ -0,0 +1,39 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"API for GraphRAG.\n+\n+WARNING: This API is under development and may undergo changes in future releases.\n+Backwards compatibility is not guaranteed at this time.\n+\"\"\"\n+\n+from graphrag.api.index import build_index\n+from graphrag.api.prompt_tune import generate_indexing_prompts\n+from graphrag.api.query import (\n+    basic_search,\n+    basic_search_streaming,\n+    drift_search,\n+    drift_search_streaming,\n+    global_search,\n+    global_search_streaming,\n+    local_search,\n+    local_search_streaming,\n+)\n+from graphrag.prompt_tune.types import DocSelectionType\n+\n+__all__ = [  # noqa: RUF022\n+    # index API\n+    \"build_index\",\n+    # query API\n+    \"global_search\",\n+    \"global_search_streaming\",\n+    \"local_search\",\n+    \"local_search_streaming\",\n+    \"drift_search\",\n+    \"drift_search_streaming\",\n+    \"basic_search\",\n+    \"basic_search_streaming\",\n+    # prompt tuning API\n+    \"DocSelectionType\",\n+    \"generate_indexing_prompts\",\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/api/index.py",
            "diff": "diff --git a/packages/graphrag/graphrag/api/index.py b/packages/graphrag/graphrag/api/index.py\nnew file mode 100644\nindex 0000000..7265e46\n--- /dev/null\n+++ b/packages/graphrag/graphrag/api/index.py\n@@ -0,0 +1,101 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"\n+Indexing API for GraphRAG.\n+\n+WARNING: This API is under development and may undergo changes in future releases.\n+Backwards compatibility is not guaranteed at this time.\n+\"\"\"\n+\n+import logging\n+from typing import Any\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import IndexingMethod\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.run_pipeline import run_pipeline\n+from graphrag.index.run.utils import create_callback_chain\n+from graphrag.index.typing.pipeline_run_result import PipelineRunResult\n+from graphrag.index.workflows.factory import PipelineFactory\n+from graphrag.logger.standard_logging import init_loggers\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def build_index(\n+    config: GraphRagConfig,\n+    method: IndexingMethod | str = IndexingMethod.Standard,\n+    is_update_run: bool = False,\n+    memory_profile: bool = False,\n+    callbacks: list[WorkflowCallbacks] | None = None,\n+    additional_context: dict[str, Any] | None = None,\n+    verbose: bool = False,\n+    input_documents: pd.DataFrame | None = None,\n+) -> list[PipelineRunResult]:\n+    \"\"\"Run the pipeline with the given configuration.\n+\n+    Parameters\n+    ----------\n+    config : GraphRagConfig\n+        The configuration.\n+    method : IndexingMethod default=IndexingMethod.Standard\n+        Styling of indexing to perform (full LLM, NLP + LLM, etc.).\n+    memory_profile : bool\n+        Whether to enable memory profiling.\n+    callbacks : list[WorkflowCallbacks] | None default=None\n+        A list of callbacks to register.\n+    additional_context : dict[str, Any] | None default=None\n+        Additional context to pass to the pipeline run. This can be accessed in the pipeline state under the 'additional_context' key.\n+    input_documents : pd.DataFrame | None default=None.\n+        Override document loading and parsing and supply your own dataframe of documents to index.\n+\n+    Returns\n+    -------\n+    list[PipelineRunResult]\n+        The list of pipeline run results\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose)\n+\n+    # Create callbacks for pipeline lifecycle events if provided\n+    workflow_callbacks = (\n+        create_callback_chain(callbacks) if callbacks else NoopWorkflowCallbacks()\n+    )\n+\n+    outputs: list[PipelineRunResult] = []\n+\n+    if memory_profile:\n+        logger.warning(\"New pipeline does not yet support memory profiling.\")\n+\n+    logger.info(\"Initializing indexing pipeline...\")\n+    # todo: this could propagate out to the cli for better clarity, but will be a breaking api change\n+    method = _get_method(method, is_update_run)\n+    pipeline = PipelineFactory.create_pipeline(config, method)\n+\n+    workflow_callbacks.pipeline_start(pipeline.names())\n+\n+    async for output in run_pipeline(\n+        pipeline,\n+        config,\n+        callbacks=workflow_callbacks,\n+        is_update_run=is_update_run,\n+        additional_context=additional_context,\n+        input_documents=input_documents,\n+    ):\n+        outputs.append(output)\n+        if output.errors and len(output.errors) > 0:\n+            logger.error(\"Workflow %s completed with errors\", output.workflow)\n+        else:\n+            logger.info(\"Workflow %s completed successfully\", output.workflow)\n+        logger.debug(str(output.result))\n+\n+    workflow_callbacks.pipeline_end(outputs)\n+    return outputs\n+\n+\n+def _get_method(method: IndexingMethod | str, is_update_run: bool) -> str:\n+    m = method.value if isinstance(method, IndexingMethod) else method\n+    return f\"{m}-update\" if is_update_run else m\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/api/prompt_tune.py",
            "diff": "diff --git a/packages/graphrag/graphrag/api/prompt_tune.py b/packages/graphrag/graphrag/api/prompt_tune.py\nnew file mode 100644\nindex 0000000..43fb0a5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/api/prompt_tune.py\n@@ -0,0 +1,202 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"\n+Auto Templating API.\n+\n+This API provides access to the auto templating feature of graphrag, allowing external applications\n+to hook into graphrag and generate prompts from private data.\n+\n+WARNING: This API is under development and may undergo changes in future releases.\n+Backwards compatibility is not guaranteed at this time.\n+\"\"\"\n+\n+import logging\n+from typing import Annotated\n+\n+import annotated_types\n+from pydantic import PositiveInt, validate_call\n+\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.logger.standard_logging import init_loggers\n+from graphrag.prompt_tune.defaults import MAX_TOKEN_COUNT, PROMPT_TUNING_MODEL_ID\n+from graphrag.prompt_tune.generator.community_report_rating import (\n+    generate_community_report_rating,\n+)\n+from graphrag.prompt_tune.generator.community_report_summarization import (\n+    create_community_summarization_prompt,\n+)\n+from graphrag.prompt_tune.generator.community_reporter_role import (\n+    generate_community_reporter_role,\n+)\n+from graphrag.prompt_tune.generator.domain import generate_domain\n+from graphrag.prompt_tune.generator.entity_relationship import (\n+    generate_entity_relationship_examples,\n+)\n+from graphrag.prompt_tune.generator.entity_summarization_prompt import (\n+    create_entity_summarization_prompt,\n+)\n+from graphrag.prompt_tune.generator.entity_types import generate_entity_types\n+from graphrag.prompt_tune.generator.extract_graph_prompt import (\n+    create_extract_graph_prompt,\n+)\n+from graphrag.prompt_tune.generator.language import detect_language\n+from graphrag.prompt_tune.generator.persona import generate_persona\n+from graphrag.prompt_tune.loader.input import load_docs_in_chunks\n+from graphrag.prompt_tune.types import DocSelectionType\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+async def generate_indexing_prompts(\n+    config: GraphRagConfig,\n+    chunk_size: PositiveInt = graphrag_config_defaults.chunks.size,\n+    overlap: Annotated[\n+        int, annotated_types.Gt(-1)\n+    ] = graphrag_config_defaults.chunks.overlap,\n+    limit: PositiveInt = 15,\n+    selection_method: DocSelectionType = DocSelectionType.RANDOM,\n+    domain: str | None = None,\n+    language: str | None = None,\n+    max_tokens: int = MAX_TOKEN_COUNT,\n+    discover_entity_types: bool = True,\n+    min_examples_required: PositiveInt = 2,\n+    n_subset_max: PositiveInt = 300,\n+    k: PositiveInt = 15,\n+    verbose: bool = False,\n+) -> tuple[str, str, str]:\n+    \"\"\"Generate indexing prompts.\n+\n+    Parameters\n+    ----------\n+    - config: The GraphRag configuration.\n+    - output_path: The path to store the prompts.\n+    - chunk_size: The chunk token size to use for input text units.\n+    - limit: The limit of chunks to load.\n+    - selection_method: The chunk selection method.\n+    - domain: The domain to map the input documents to.\n+    - language: The language to use for the prompts.\n+    - max_tokens: The maximum number of tokens to use on entity extraction prompts\n+    - discover_entity_types: Generate entity types.\n+    - min_examples_required: The minimum number of examples required for entity extraction prompts.\n+    - n_subset_max: The number of text chunks to embed when using auto selection method.\n+    - k: The number of documents to select when using auto selection method.\n+\n+    Returns\n+    -------\n+    tuple[str, str, str]: entity extraction prompt, entity summarization prompt, community summarization prompt\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"prompt-tuning.log\")\n+\n+    # Retrieve documents\n+    logger.info(\"Chunking documents...\")\n+    doc_list = await load_docs_in_chunks(\n+        config=config,\n+        limit=limit,\n+        select_method=selection_method,\n+        logger=logger,\n+        chunk_size=chunk_size,\n+        overlap=overlap,\n+        n_subset_max=n_subset_max,\n+        k=k,\n+    )\n+\n+    # Create LLM from config\n+    # TODO: Expose a way to specify Prompt Tuning model ID through config\n+    logger.info(\"Retrieving language model configuration...\")\n+    default_llm_settings = config.get_language_model_config(PROMPT_TUNING_MODEL_ID)\n+\n+    logger.info(\"Creating language model...\")\n+    llm = ModelManager().register_chat(\n+        name=\"prompt_tuning\",\n+        model_type=default_llm_settings.type,\n+        config=default_llm_settings,\n+        callbacks=NoopWorkflowCallbacks(),\n+        cache=None,\n+    )\n+\n+    if not domain:\n+        logger.info(\"Generating domain...\")\n+        domain = await generate_domain(llm, doc_list)\n+\n+    if not language:\n+        logger.info(\"Detecting language...\")\n+        language = await detect_language(llm, doc_list)\n+\n+    logger.info(\"Generating persona...\")\n+    persona = await generate_persona(llm, domain)\n+\n+    logger.info(\"Generating community report ranking description...\")\n+    community_report_ranking = await generate_community_report_rating(\n+        llm, domain=domain, persona=persona, docs=doc_list\n+    )\n+\n+    entity_types = None\n+    extract_graph_llm_settings = config.get_language_model_config(\n+        config.extract_graph.model_id\n+    )\n+    if discover_entity_types:\n+        logger.info(\"Generating entity types...\")\n+        entity_types = await generate_entity_types(\n+            llm,\n+            domain=domain,\n+            persona=persona,\n+            docs=doc_list,\n+            json_mode=extract_graph_llm_settings.model_supports_json or False,\n+        )\n+\n+    logger.info(\"Generating entity relationship examples...\")\n+    examples = await generate_entity_relationship_examples(\n+        llm,\n+        persona=persona,\n+        entity_types=entity_types,\n+        docs=doc_list,\n+        language=language,\n+        json_mode=False,  # config.llm.model_supports_json should be used, but these prompts are used in non-json mode by the index engine\n+    )\n+\n+    logger.info(\"Generating entity extraction prompt...\")\n+    extract_graph_prompt = create_extract_graph_prompt(\n+        entity_types=entity_types,\n+        docs=doc_list,\n+        examples=examples,\n+        language=language,\n+        json_mode=False,  # config.llm.model_supports_json should be used, but these prompts are used in non-json mode by the index engine\n+        tokenizer=get_tokenizer(model_config=extract_graph_llm_settings),\n+        max_token_count=max_tokens,\n+        min_examples_required=min_examples_required,\n+    )\n+\n+    logger.info(\"Generating entity summarization prompt...\")\n+    entity_summarization_prompt = create_entity_summarization_prompt(\n+        persona=persona,\n+        language=language,\n+    )\n+\n+    logger.info(\"Generating community reporter role...\")\n+    community_reporter_role = await generate_community_reporter_role(\n+        llm, domain=domain, persona=persona, docs=doc_list\n+    )\n+\n+    logger.info(\"Generating community summarization prompt...\")\n+    community_summarization_prompt = create_community_summarization_prompt(\n+        persona=persona,\n+        role=community_reporter_role,\n+        report_rating_description=community_report_ranking,\n+        language=language,\n+    )\n+\n+    logger.debug(\"Generated domain: %s\", domain)\n+    logger.debug(\"Detected language: %s\", language)\n+    logger.debug(\"Generated persona: %s\", persona)\n+\n+    return (\n+        extract_graph_prompt,\n+        entity_summarization_prompt,\n+        community_summarization_prompt,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/api/query.py",
            "diff": "diff --git a/packages/graphrag/graphrag/api/query.py b/packages/graphrag/graphrag/api/query.py\nnew file mode 100644\nindex 0000000..e49a097\n--- /dev/null\n+++ b/packages/graphrag/graphrag/api/query.py\n@@ -0,0 +1,552 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"\n+Query Engine API.\n+\n+This API provides access to the query engine of graphrag, allowing external applications\n+to hook into graphrag and run queries over a knowledge graph generated by graphrag.\n+\n+Contains the following functions:\n+ - global_search: Perform a global search.\n+ - global_search_streaming: Perform a global search and stream results back.\n+ - local_search: Perform a local search.\n+ - local_search_streaming: Perform a local search and stream results back.\n+\n+WARNING: This API is under development and may undergo changes in future releases.\n+Backwards compatibility is not guaranteed at this time.\n+\"\"\"\n+\n+import logging\n+from collections.abc import AsyncGenerator\n+from typing import Any\n+\n+import pandas as pd\n+from pydantic import validate_call\n+\n+from graphrag.callbacks.noop_query_callbacks import NoopQueryCallbacks\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.config.embeddings import (\n+    community_full_content_embedding,\n+    entity_description_embedding,\n+    text_unit_text_embedding,\n+)\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.logger.standard_logging import init_loggers\n+from graphrag.query.factory import (\n+    get_basic_search_engine,\n+    get_drift_search_engine,\n+    get_global_search_engine,\n+    get_local_search_engine,\n+)\n+from graphrag.query.indexer_adapters import (\n+    read_indexer_communities,\n+    read_indexer_covariates,\n+    read_indexer_entities,\n+    read_indexer_relationships,\n+    read_indexer_report_embeddings,\n+    read_indexer_reports,\n+    read_indexer_text_units,\n+)\n+from graphrag.utils.api import (\n+    get_embedding_store,\n+    load_search_prompt,\n+    truncate,\n+)\n+from graphrag.utils.cli import redact\n+\n+# Initialize standard logger\n+logger = logging.getLogger(__name__)\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+async def global_search(\n+    config: GraphRagConfig,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_reports: pd.DataFrame,\n+    community_level: int | None,\n+    dynamic_community_selection: bool,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> tuple[\n+    str | dict[str, Any] | list[dict[str, Any]],\n+    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n+]:\n+    \"\"\"Perform a global search and return the context data and response.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n+    - communities (pd.DataFrame): A DataFrame containing the final communities (from communities.parquet)\n+    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n+    - community_level (int): The community level to search at.\n+    - dynamic_community_selection (bool): Enable dynamic community selection instead of using all community reports at a fixed level. Note that you can still provide community_level cap the maximum level to search.\n+    - response_type (str): The type of response to return.\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    callbacks = callbacks or []\n+    full_response = \"\"\n+    context_data = {}\n+\n+    def on_context(context: Any) -> None:\n+        nonlocal context_data\n+        context_data = context\n+\n+    local_callbacks = NoopQueryCallbacks()\n+    local_callbacks.on_context = on_context\n+    callbacks.append(local_callbacks)\n+\n+    logger.debug(\"Executing global search query: %s\", query)\n+    async for chunk in global_search_streaming(\n+        config=config,\n+        entities=entities,\n+        communities=communities,\n+        community_reports=community_reports,\n+        community_level=community_level,\n+        dynamic_community_selection=dynamic_community_selection,\n+        response_type=response_type,\n+        query=query,\n+        callbacks=callbacks,\n+    ):\n+        full_response += chunk\n+    logger.debug(\"Query response: %s\", truncate(full_response, 400))\n+    return full_response, context_data\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+def global_search_streaming(\n+    config: GraphRagConfig,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_reports: pd.DataFrame,\n+    community_level: int | None,\n+    dynamic_community_selection: bool,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> AsyncGenerator:\n+    \"\"\"Perform a global search and return the context data and response via a generator.\n+\n+    Context data is returned as a dictionary of lists, with one list entry for each record.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n+    - communities (pd.DataFrame): A DataFrame containing the final communities (from communities.parquet)\n+    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n+    - community_level (int): The community level to search at.\n+    - dynamic_community_selection (bool): Enable dynamic community selection instead of using all community reports at a fixed level. Note that you can still provide community_level cap the maximum level to search.\n+    - response_type (str): The type of response to return.\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    communities_ = read_indexer_communities(communities, community_reports)\n+    reports = read_indexer_reports(\n+        community_reports,\n+        communities,\n+        community_level=community_level,\n+        dynamic_community_selection=dynamic_community_selection,\n+    )\n+    entities_ = read_indexer_entities(\n+        entities, communities, community_level=community_level\n+    )\n+    map_prompt = load_search_prompt(config.root_dir, config.global_search.map_prompt)\n+    reduce_prompt = load_search_prompt(\n+        config.root_dir, config.global_search.reduce_prompt\n+    )\n+    knowledge_prompt = load_search_prompt(\n+        config.root_dir, config.global_search.knowledge_prompt\n+    )\n+\n+    logger.debug(\"Executing streaming global search query: %s\", query)\n+    search_engine = get_global_search_engine(\n+        config,\n+        reports=reports,\n+        entities=entities_,\n+        communities=communities_,\n+        response_type=response_type,\n+        dynamic_community_selection=dynamic_community_selection,\n+        map_system_prompt=map_prompt,\n+        reduce_system_prompt=reduce_prompt,\n+        general_knowledge_inclusion_prompt=knowledge_prompt,\n+        callbacks=callbacks,\n+    )\n+    return search_engine.stream_search(query=query)\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+async def local_search(\n+    config: GraphRagConfig,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_reports: pd.DataFrame,\n+    text_units: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+    covariates: pd.DataFrame | None,\n+    community_level: int,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> tuple[\n+    str | dict[str, Any] | list[dict[str, Any]],\n+    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n+]:\n+    \"\"\"Perform a local search and return the context data and response.\n+\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n+    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n+    - text_units (pd.DataFrame): A DataFrame containing the final text units (from text_units.parquet)\n+    - relationships (pd.DataFrame): A DataFrame containing the final relationships (from relationships.parquet)\n+    - covariates (pd.DataFrame): A DataFrame containing the final covariates (from covariates.parquet)\n+    - community_level (int): The community level to search at.\n+    - response_type (str): The response type to return.\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    callbacks = callbacks or []\n+    full_response = \"\"\n+    context_data = {}\n+\n+    def on_context(context: Any) -> None:\n+        nonlocal context_data\n+        context_data = context\n+\n+    local_callbacks = NoopQueryCallbacks()\n+    local_callbacks.on_context = on_context\n+    callbacks.append(local_callbacks)\n+\n+    logger.debug(\"Executing local search query: %s\", query)\n+    async for chunk in local_search_streaming(\n+        config=config,\n+        entities=entities,\n+        communities=communities,\n+        community_reports=community_reports,\n+        text_units=text_units,\n+        relationships=relationships,\n+        covariates=covariates,\n+        community_level=community_level,\n+        response_type=response_type,\n+        query=query,\n+        callbacks=callbacks,\n+    ):\n+        full_response += chunk\n+    logger.debug(\"Query response: %s\", truncate(full_response, 400))\n+    return full_response, context_data\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+def local_search_streaming(\n+    config: GraphRagConfig,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_reports: pd.DataFrame,\n+    text_units: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+    covariates: pd.DataFrame | None,\n+    community_level: int,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> AsyncGenerator:\n+    \"\"\"Perform a local search and return the context data and response via a generator.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n+    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n+    - text_units (pd.DataFrame): A DataFrame containing the final text units (from text_units.parquet)\n+    - relationships (pd.DataFrame): A DataFrame containing the final relationships (from relationships.parquet)\n+    - covariates (pd.DataFrame): A DataFrame containing the final covariates (from covariates.parquet)\n+    - community_level (int): The community level to search at.\n+    - response_type (str): The response type to return.\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    msg = f\"Vector Store Args: {redact(config.vector_store.model_dump())}\"\n+    logger.debug(msg)\n+\n+    description_embedding_store = get_embedding_store(\n+        store=config.vector_store.model_dump(),\n+        embedding_name=entity_description_embedding,\n+    )\n+\n+    entities_ = read_indexer_entities(entities, communities, community_level)\n+    covariates_ = read_indexer_covariates(covariates) if covariates is not None else []\n+    prompt = load_search_prompt(config.root_dir, config.local_search.prompt)\n+\n+    logger.debug(\"Executing streaming local search query: %s\", query)\n+    search_engine = get_local_search_engine(\n+        config=config,\n+        reports=read_indexer_reports(community_reports, communities, community_level),\n+        text_units=read_indexer_text_units(text_units),\n+        entities=entities_,\n+        relationships=read_indexer_relationships(relationships),\n+        covariates={\"claims\": covariates_},\n+        description_embedding_store=description_embedding_store,\n+        response_type=response_type,\n+        system_prompt=prompt,\n+        callbacks=callbacks,\n+    )\n+    return search_engine.stream_search(query=query)\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+async def drift_search(\n+    config: GraphRagConfig,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_reports: pd.DataFrame,\n+    text_units: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+    community_level: int,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> tuple[\n+    str | dict[str, Any] | list[dict[str, Any]],\n+    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n+]:\n+    \"\"\"Perform a DRIFT search and return the context data and response.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n+    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n+    - text_units (pd.DataFrame): A DataFrame containing the final text units (from text_units.parquet)\n+    - relationships (pd.DataFrame): A DataFrame containing the final relationships (from relationships.parquet)\n+    - community_level (int): The community level to search at.\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    callbacks = callbacks or []\n+    full_response = \"\"\n+    context_data = {}\n+\n+    def on_context(context: Any) -> None:\n+        nonlocal context_data\n+        context_data = context\n+\n+    local_callbacks = NoopQueryCallbacks()\n+    local_callbacks.on_context = on_context\n+    callbacks.append(local_callbacks)\n+\n+    logger.debug(\"Executing drift search query: %s\", query)\n+    async for chunk in drift_search_streaming(\n+        config=config,\n+        entities=entities,\n+        communities=communities,\n+        community_reports=community_reports,\n+        text_units=text_units,\n+        relationships=relationships,\n+        community_level=community_level,\n+        response_type=response_type,\n+        query=query,\n+        callbacks=callbacks,\n+    ):\n+        full_response += chunk\n+    logger.debug(\"Query response: %s\", truncate(full_response, 400))\n+    return full_response, context_data\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+def drift_search_streaming(\n+    config: GraphRagConfig,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_reports: pd.DataFrame,\n+    text_units: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+    community_level: int,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> AsyncGenerator:\n+    \"\"\"Perform a DRIFT search and return the context data and response.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n+    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n+    - text_units (pd.DataFrame): A DataFrame containing the final text units (from text_units.parquet)\n+    - relationships (pd.DataFrame): A DataFrame containing the final relationships (from relationships.parquet)\n+    - community_level (int): The community level to search at.\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    msg = f\"Vector Store Args: {redact(config.vector_store.model_dump())}\"\n+    logger.debug(msg)\n+\n+    description_embedding_store = get_embedding_store(\n+        store=config.vector_store.model_dump(),\n+        embedding_name=entity_description_embedding,\n+    )\n+\n+    full_content_embedding_store = get_embedding_store(\n+        store=config.vector_store.model_dump(),\n+        embedding_name=community_full_content_embedding,\n+    )\n+\n+    entities_ = read_indexer_entities(entities, communities, community_level)\n+    reports = read_indexer_reports(community_reports, communities, community_level)\n+    read_indexer_report_embeddings(reports, full_content_embedding_store)\n+    prompt = load_search_prompt(config.root_dir, config.drift_search.prompt)\n+    reduce_prompt = load_search_prompt(\n+        config.root_dir, config.drift_search.reduce_prompt\n+    )\n+\n+    logger.debug(\"Executing streaming drift search query: %s\", query)\n+    search_engine = get_drift_search_engine(\n+        config=config,\n+        reports=reports,\n+        text_units=read_indexer_text_units(text_units),\n+        entities=entities_,\n+        relationships=read_indexer_relationships(relationships),\n+        description_embedding_store=description_embedding_store,\n+        local_system_prompt=prompt,\n+        reduce_system_prompt=reduce_prompt,\n+        response_type=response_type,\n+        callbacks=callbacks,\n+    )\n+    return search_engine.stream_search(query=query)\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+async def basic_search(\n+    config: GraphRagConfig,\n+    text_units: pd.DataFrame,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> tuple[\n+    str | dict[str, Any] | list[dict[str, Any]],\n+    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n+]:\n+    \"\"\"Perform a basic search and return the context data and response.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - text_units (pd.DataFrame): A DataFrame containing the final text units (from text_units.parquet)\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    callbacks = callbacks or []\n+    full_response = \"\"\n+    context_data = {}\n+\n+    def on_context(context: Any) -> None:\n+        nonlocal context_data\n+        context_data = context\n+\n+    local_callbacks = NoopQueryCallbacks()\n+    local_callbacks.on_context = on_context\n+    callbacks.append(local_callbacks)\n+\n+    logger.debug(\"Executing basic search query: %s\", query)\n+    async for chunk in basic_search_streaming(\n+        config=config,\n+        text_units=text_units,\n+        response_type=response_type,\n+        query=query,\n+        callbacks=callbacks,\n+    ):\n+        full_response += chunk\n+    logger.debug(\"Query response: %s\", truncate(full_response, 400))\n+    return full_response, context_data\n+\n+\n+@validate_call(config={\"arbitrary_types_allowed\": True})\n+def basic_search_streaming(\n+    config: GraphRagConfig,\n+    text_units: pd.DataFrame,\n+    response_type: str,\n+    query: str,\n+    callbacks: list[QueryCallbacks] | None = None,\n+    verbose: bool = False,\n+) -> AsyncGenerator:\n+    \"\"\"Perform a local search and return the context data and response via a generator.\n+\n+    Parameters\n+    ----------\n+    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n+    - text_units (pd.DataFrame): A DataFrame containing the final text units (from text_units.parquet)\n+    - query (str): The user query to search for.\n+\n+    Returns\n+    -------\n+    TODO: Document the search response type and format.\n+    \"\"\"\n+    init_loggers(config=config, verbose=verbose, filename=\"query.log\")\n+\n+    msg = f\"Vector Store Args: {redact(config.vector_store.model_dump())}\"\n+    logger.debug(msg)\n+\n+    embedding_store = get_embedding_store(\n+        store=config.vector_store.model_dump(),\n+        embedding_name=text_unit_text_embedding,\n+    )\n+\n+    prompt = load_search_prompt(config.root_dir, config.basic_search.prompt)\n+\n+    logger.debug(\"Executing streaming basic search query: %s\", query)\n+    search_engine = get_basic_search_engine(\n+        config=config,\n+        text_units=read_indexer_text_units(text_units),\n+        text_unit_embeddings=embedding_store,\n+        response_type=response_type,\n+        system_prompt=prompt,\n+        callbacks=callbacks,\n+    )\n+    return search_engine.stream_search(query=query)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cache/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cache/__init__.py b/packages/graphrag/graphrag/cache/__init__.py\nnew file mode 100644\nindex 0000000..9c4e8be\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cache/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing cache implementations.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cache/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cache/factory.py b/packages/graphrag/graphrag/cache/factory.py\nnew file mode 100644\nindex 0000000..f406fc8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cache/factory.py\n@@ -0,0 +1,66 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Factory functions for creating a cache.\"\"\"\n+\n+from __future__ import annotations\n+\n+from graphrag.cache.json_pipeline_cache import JsonPipelineCache\n+from graphrag.cache.memory_pipeline_cache import InMemoryCache\n+from graphrag.cache.noop_pipeline_cache import NoopPipelineCache\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.config.enums import CacheType\n+from graphrag.factory.factory import Factory\n+from graphrag.storage.blob_pipeline_storage import BlobPipelineStorage\n+from graphrag.storage.cosmosdb_pipeline_storage import CosmosDBPipelineStorage\n+from graphrag.storage.file_pipeline_storage import FilePipelineStorage\n+\n+\n+class CacheFactory(Factory[PipelineCache]):\n+    \"\"\"A factory class for cache implementations.\n+\n+    Includes a method for users to register a custom cache implementation.\n+\n+    Configuration arguments are passed to each cache implementation as kwargs\n+    for individual enforcement of required/optional arguments.\n+    \"\"\"\n+\n+\n+# --- register built-in cache implementations ---\n+def create_file_cache(root_dir: str, base_dir: str, **kwargs) -> PipelineCache:\n+    \"\"\"Create a file-based cache implementation.\"\"\"\n+    # Create storage with base_dir in kwargs since FilePipelineStorage expects it there\n+    storage_kwargs = {\"base_dir\": root_dir, **kwargs}\n+    storage = FilePipelineStorage(**storage_kwargs).child(base_dir)\n+    return JsonPipelineCache(storage)\n+\n+\n+def create_blob_cache(**kwargs) -> PipelineCache:\n+    \"\"\"Create a blob storage-based cache implementation.\"\"\"\n+    storage = BlobPipelineStorage(**kwargs)\n+    return JsonPipelineCache(storage)\n+\n+\n+def create_cosmosdb_cache(**kwargs) -> PipelineCache:\n+    \"\"\"Create a CosmosDB-based cache implementation.\"\"\"\n+    storage = CosmosDBPipelineStorage(**kwargs)\n+    return JsonPipelineCache(storage)\n+\n+\n+def create_noop_cache(**_kwargs) -> PipelineCache:\n+    \"\"\"Create a no-op cache implementation.\"\"\"\n+    return NoopPipelineCache()\n+\n+\n+def create_memory_cache(**kwargs) -> PipelineCache:\n+    \"\"\"Create a memory cache implementation.\"\"\"\n+    return InMemoryCache(**kwargs)\n+\n+\n+# --- register built-in cache implementations ---\n+cache_factory = CacheFactory()\n+cache_factory.register(CacheType.none.value, create_noop_cache)\n+cache_factory.register(CacheType.memory.value, create_memory_cache)\n+cache_factory.register(CacheType.file.value, create_file_cache)\n+cache_factory.register(CacheType.blob.value, create_blob_cache)\n+cache_factory.register(CacheType.cosmosdb.value, create_cosmosdb_cache)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cache/json_pipeline_cache.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cache/json_pipeline_cache.py b/packages/graphrag/graphrag/cache/json_pipeline_cache.py\nnew file mode 100644\nindex 0000000..84cd180\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cache/json_pipeline_cache.py\n@@ -0,0 +1,65 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'JsonPipelineCache' model.\"\"\"\n+\n+import json\n+from typing import Any\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+\n+class JsonPipelineCache(PipelineCache):\n+    \"\"\"File pipeline cache class definition.\"\"\"\n+\n+    _storage: PipelineStorage\n+    _encoding: str\n+\n+    def __init__(self, storage: PipelineStorage, encoding=\"utf-8\"):\n+        \"\"\"Init method definition.\"\"\"\n+        self._storage = storage\n+        self._encoding = encoding\n+\n+    async def get(self, key: str) -> str | None:\n+        \"\"\"Get method definition.\"\"\"\n+        if await self.has(key):\n+            try:\n+                data = await self._storage.get(key, encoding=self._encoding)\n+                data = json.loads(data)\n+            except UnicodeDecodeError:\n+                await self._storage.delete(key)\n+                return None\n+            except json.decoder.JSONDecodeError:\n+                await self._storage.delete(key)\n+                return None\n+            else:\n+                return data.get(\"result\")\n+\n+        return None\n+\n+    async def set(self, key: str, value: Any, debug_data: dict | None = None) -> None:\n+        \"\"\"Set method definition.\"\"\"\n+        if value is None:\n+            return\n+        data = {\"result\": value, **(debug_data or {})}\n+        await self._storage.set(\n+            key, json.dumps(data, ensure_ascii=False), encoding=self._encoding\n+        )\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Has method definition.\"\"\"\n+        return await self._storage.has(key)\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete method definition.\"\"\"\n+        if await self.has(key):\n+            await self._storage.delete(key)\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear method definition.\"\"\"\n+        await self._storage.clear()\n+\n+    def child(self, name: str) -> \"JsonPipelineCache\":\n+        \"\"\"Child method definition.\"\"\"\n+        return JsonPipelineCache(self._storage.child(name), encoding=self._encoding)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cache/memory_pipeline_cache.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cache/memory_pipeline_cache.py b/packages/graphrag/graphrag/cache/memory_pipeline_cache.py\nnew file mode 100644\nindex 0000000..62de552\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cache/memory_pipeline_cache.py\n@@ -0,0 +1,78 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'InMemoryCache' model.\"\"\"\n+\n+from typing import Any\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+\n+\n+class InMemoryCache(PipelineCache):\n+    \"\"\"In memory cache class definition.\"\"\"\n+\n+    _cache: dict[str, Any]\n+    _name: str\n+\n+    def __init__(self, name: str | None = None):\n+        \"\"\"Init method definition.\"\"\"\n+        self._cache = {}\n+        self._name = name or \"\"\n+\n+    async def get(self, key: str) -> Any:\n+        \"\"\"Get the value for the given key.\n+\n+        Args:\n+            - key - The key to get the value for.\n+            - as_bytes - Whether or not to return the value as bytes.\n+\n+        Returns\n+        -------\n+            - output - The value for the given key.\n+        \"\"\"\n+        key = self._create_cache_key(key)\n+        return self._cache.get(key)\n+\n+    async def set(self, key: str, value: Any, debug_data: dict | None = None) -> None:\n+        \"\"\"Set the value for the given key.\n+\n+        Args:\n+            - key - The key to set the value for.\n+            - value - The value to set.\n+        \"\"\"\n+        key = self._create_cache_key(key)\n+        self._cache[key] = value\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Return True if the given key exists in the storage.\n+\n+        Args:\n+            - key - The key to check for.\n+\n+        Returns\n+        -------\n+            - output - True if the key exists in the storage, False otherwise.\n+        \"\"\"\n+        key = self._create_cache_key(key)\n+        return key in self._cache\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete the given key from the storage.\n+\n+        Args:\n+            - key - The key to delete.\n+        \"\"\"\n+        key = self._create_cache_key(key)\n+        del self._cache[key]\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear the storage.\"\"\"\n+        self._cache.clear()\n+\n+    def child(self, name: str) -> PipelineCache:\n+        \"\"\"Create a sub cache with the given name.\"\"\"\n+        return InMemoryCache(name)\n+\n+    def _create_cache_key(self, key: str) -> str:\n+        \"\"\"Create a cache key for the given key.\"\"\"\n+        return f\"{self._name}{key}\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cache/noop_pipeline_cache.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cache/noop_pipeline_cache.py b/packages/graphrag/graphrag/cache/noop_pipeline_cache.py\nnew file mode 100644\nindex 0000000..227ef68\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cache/noop_pipeline_cache.py\n@@ -0,0 +1,65 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Module containing the NoopPipelineCache implementation.\"\"\"\n+\n+from typing import Any\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+\n+\n+class NoopPipelineCache(PipelineCache):\n+    \"\"\"A no-op implementation of the pipeline cache, usually useful for testing.\"\"\"\n+\n+    async def get(self, key: str) -> Any:\n+        \"\"\"Get the value for the given key.\n+\n+        Args:\n+            - key - The key to get the value for.\n+            - as_bytes - Whether or not to return the value as bytes.\n+\n+        Returns\n+        -------\n+            - output - The value for the given key.\n+        \"\"\"\n+        return None\n+\n+    async def set(\n+        self, key: str, value: str | bytes | None, debug_data: dict | None = None\n+    ) -> None:\n+        \"\"\"Set the value for the given key.\n+\n+        Args:\n+            - key - The key to set the value for.\n+            - value - The value to set.\n+        \"\"\"\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Return True if the given key exists in the cache.\n+\n+        Args:\n+            - key - The key to check for.\n+\n+        Returns\n+        -------\n+            - output - True if the key exists in the cache, False otherwise.\n+        \"\"\"\n+        return False\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete the given key from the cache.\n+\n+        Args:\n+            - key - The key to delete.\n+        \"\"\"\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear the cache.\"\"\"\n+\n+    def child(self, name: str) -> PipelineCache:\n+        \"\"\"Create a child cache with the given name.\n+\n+        Args:\n+            - name - The name to create the sub cache with.\n+        \"\"\"\n+        return self\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cache/pipeline_cache.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cache/pipeline_cache.py b/packages/graphrag/graphrag/cache/pipeline_cache.py\nnew file mode 100644\nindex 0000000..c68c5cf\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cache/pipeline_cache.py\n@@ -0,0 +1,67 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'PipelineCache' model.\"\"\"\n+\n+from __future__ import annotations\n+\n+from abc import ABCMeta, abstractmethod\n+from typing import Any\n+\n+\n+class PipelineCache(metaclass=ABCMeta):\n+    \"\"\"Provide a cache interface for the pipeline.\"\"\"\n+\n+    @abstractmethod\n+    async def get(self, key: str) -> Any:\n+        \"\"\"Get the value for the given key.\n+\n+        Args:\n+            - key - The key to get the value for.\n+            - as_bytes - Whether or not to return the value as bytes.\n+\n+        Returns\n+        -------\n+            - output - The value for the given key.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def set(self, key: str, value: Any, debug_data: dict | None = None) -> None:\n+        \"\"\"Set the value for the given key.\n+\n+        Args:\n+            - key - The key to set the value for.\n+            - value - The value to set.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Return True if the given key exists in the cache.\n+\n+        Args:\n+            - key - The key to check for.\n+\n+        Returns\n+        -------\n+            - output - True if the key exists in the cache, False otherwise.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete the given key from the cache.\n+\n+        Args:\n+            - key - The key to delete.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def clear(self) -> None:\n+        \"\"\"Clear the cache.\"\"\"\n+\n+    @abstractmethod\n+    def child(self, name: str) -> PipelineCache:\n+        \"\"\"Create a child cache with the given name.\n+\n+        Args:\n+            - name - The name to create the sub cache with.\n+        \"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/__init__.py b/packages/graphrag/graphrag/callbacks/__init__.py\nnew file mode 100644\nindex 0000000..d623571\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing callback implementations.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/console_workflow_callbacks.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/console_workflow_callbacks.py b/packages/graphrag/graphrag/callbacks/console_workflow_callbacks.py\nnew file mode 100644\nindex 0000000..dbe7e0d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/console_workflow_callbacks.py\n@@ -0,0 +1,46 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A logger that emits updates from the indexing engine to the console.\"\"\"\n+\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.index.typing.pipeline_run_result import PipelineRunResult\n+from graphrag.logger.progress import Progress\n+\n+# ruff: noqa: T201\n+\n+\n+class ConsoleWorkflowCallbacks(NoopWorkflowCallbacks):\n+    \"\"\"A logger that writes to a console.\"\"\"\n+\n+    _verbose = False\n+\n+    def __init__(self, verbose=False):\n+        self._verbose = verbose\n+\n+    def pipeline_start(self, names: list[str]) -> None:\n+        \"\"\"Execute this callback to signal when the entire pipeline starts.\"\"\"\n+        print(\"Starting pipeline with workflows:\", \", \".join(names))\n+\n+    def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n+        \"\"\"Execute this callback to signal when the entire pipeline ends.\"\"\"\n+        print(\"Pipeline complete\")\n+\n+    def workflow_start(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow starts.\"\"\"\n+        print(f\"Starting workflow: {name}\")\n+\n+    def workflow_end(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow ends.\"\"\"\n+        print(\"\")  # account for potential return on prior progress\n+        print(f\"Workflow complete: {name}\")\n+        if self._verbose:\n+            print(instance)\n+\n+    def progress(self, progress: Progress) -> None:\n+        \"\"\"Handle when progress occurs.\"\"\"\n+        complete = progress.completed_items or 0\n+        total = progress.total_items or 1\n+        percent = round((complete / total) * 100)\n+        start = f\"  {complete} / {total} \"\n+        print(f\"{start:{'.'}<{percent}}\", flush=True, end=\"\\r\")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/llm_callbacks.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/llm_callbacks.py b/packages/graphrag/graphrag/callbacks/llm_callbacks.py\nnew file mode 100644\nindex 0000000..3b6579a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/llm_callbacks.py\n@@ -0,0 +1,14 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LLM Callbacks.\"\"\"\n+\n+from typing import Protocol\n+\n+\n+class BaseLLMCallback(Protocol):\n+    \"\"\"Base class for LLM callbacks.\"\"\"\n+\n+    def on_llm_new_token(self, token: str):\n+        \"\"\"Handle when a new token is generated.\"\"\"\n+        ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/noop_query_callbacks.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/noop_query_callbacks.py b/packages/graphrag/graphrag/callbacks/noop_query_callbacks.py\nnew file mode 100644\nindex 0000000..a0434e8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/noop_query_callbacks.py\n@@ -0,0 +1,33 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"No-op Query Callbacks.\"\"\"\n+\n+from typing import Any\n+\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.query.structured_search.base import SearchResult\n+\n+\n+class NoopQueryCallbacks(QueryCallbacks):\n+    \"\"\"A no-op implementation of QueryCallbacks.\"\"\"\n+\n+    def on_context(self, context: Any) -> None:\n+        \"\"\"Handle when context data is constructed.\"\"\"\n+\n+    def on_map_response_start(self, map_response_contexts: list[str]) -> None:\n+        \"\"\"Handle the start of map operation.\"\"\"\n+\n+    def on_map_response_end(self, map_response_outputs: list[SearchResult]) -> None:\n+        \"\"\"Handle the end of map operation.\"\"\"\n+\n+    def on_reduce_response_start(\n+        self, reduce_response_context: str | dict[str, Any]\n+    ) -> None:\n+        \"\"\"Handle the start of reduce operation.\"\"\"\n+\n+    def on_reduce_response_end(self, reduce_response_output: str) -> None:\n+        \"\"\"Handle the end of reduce operation.\"\"\"\n+\n+    def on_llm_new_token(self, token):\n+        \"\"\"Handle when a new token is generated.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/noop_workflow_callbacks.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/noop_workflow_callbacks.py b/packages/graphrag/graphrag/callbacks/noop_workflow_callbacks.py\nnew file mode 100644\nindex 0000000..9f9ac2a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/noop_workflow_callbacks.py\n@@ -0,0 +1,27 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A no-op implementation of WorkflowCallbacks.\"\"\"\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.typing.pipeline_run_result import PipelineRunResult\n+from graphrag.logger.progress import Progress\n+\n+\n+class NoopWorkflowCallbacks(WorkflowCallbacks):\n+    \"\"\"A no-op implementation of WorkflowCallbacks that logs all events to standard logging.\"\"\"\n+\n+    def pipeline_start(self, names: list[str]) -> None:\n+        \"\"\"Execute this callback to signal when the entire pipeline starts.\"\"\"\n+\n+    def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n+        \"\"\"Execute this callback to signal when the entire pipeline ends.\"\"\"\n+\n+    def workflow_start(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow starts.\"\"\"\n+\n+    def workflow_end(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow ends.\"\"\"\n+\n+    def progress(self, progress: Progress) -> None:\n+        \"\"\"Handle when progress occurs.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/query_callbacks.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/query_callbacks.py b/packages/graphrag/graphrag/callbacks/query_callbacks.py\nnew file mode 100644\nindex 0000000..8f24de6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/query_callbacks.py\n@@ -0,0 +1,33 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Query Callbacks.\"\"\"\n+\n+from typing import Any\n+\n+from graphrag.callbacks.llm_callbacks import BaseLLMCallback\n+from graphrag.query.structured_search.base import SearchResult\n+\n+\n+class QueryCallbacks(BaseLLMCallback):\n+    \"\"\"Callbacks used during query execution.\"\"\"\n+\n+    def on_context(self, context: Any) -> None:\n+        \"\"\"Handle when context data is constructed.\"\"\"\n+\n+    def on_map_response_start(self, map_response_contexts: list[str]) -> None:\n+        \"\"\"Handle the start of map operation.\"\"\"\n+\n+    def on_map_response_end(self, map_response_outputs: list[SearchResult]) -> None:\n+        \"\"\"Handle the end of map operation.\"\"\"\n+\n+    def on_reduce_response_start(\n+        self, reduce_response_context: str | dict[str, Any]\n+    ) -> None:\n+        \"\"\"Handle the start of reduce operation.\"\"\"\n+\n+    def on_reduce_response_end(self, reduce_response_output: str) -> None:\n+        \"\"\"Handle the end of reduce operation.\"\"\"\n+\n+    def on_llm_new_token(self, token) -> None:\n+        \"\"\"Handle when a new token is generated.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/workflow_callbacks.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/workflow_callbacks.py b/packages/graphrag/graphrag/callbacks/workflow_callbacks.py\nnew file mode 100644\nindex 0000000..0429cff\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/workflow_callbacks.py\n@@ -0,0 +1,37 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Collection of callbacks that can be used to monitor the workflow execution.\"\"\"\n+\n+from typing import Protocol\n+\n+from graphrag.index.typing.pipeline_run_result import PipelineRunResult\n+from graphrag.logger.progress import Progress\n+\n+\n+class WorkflowCallbacks(Protocol):\n+    \"\"\"\n+    A collection of callbacks that can be used to monitor the workflow execution.\n+\n+    This base class is a \"noop\" implementation so that clients may implement just the callbacks they need.\n+    \"\"\"\n+\n+    def pipeline_start(self, names: list[str]) -> None:\n+        \"\"\"Execute this callback to signal when the entire pipeline starts.\"\"\"\n+        ...\n+\n+    def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n+        \"\"\"Execute this callback to signal when the entire pipeline ends.\"\"\"\n+        ...\n+\n+    def workflow_start(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow starts.\"\"\"\n+        ...\n+\n+    def workflow_end(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow ends.\"\"\"\n+        ...\n+\n+    def progress(self, progress: Progress) -> None:\n+        \"\"\"Handle when progress occurs.\"\"\"\n+        ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/callbacks/workflow_callbacks_manager.py",
            "diff": "diff --git a/packages/graphrag/graphrag/callbacks/workflow_callbacks_manager.py b/packages/graphrag/graphrag/callbacks/workflow_callbacks_manager.py\nnew file mode 100644\nindex 0000000..1ca0c09\n--- /dev/null\n+++ b/packages/graphrag/graphrag/callbacks/workflow_callbacks_manager.py\n@@ -0,0 +1,52 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the WorkflowCallbacks registry.\"\"\"\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.typing.pipeline_run_result import PipelineRunResult\n+from graphrag.logger.progress import Progress\n+\n+\n+class WorkflowCallbacksManager(WorkflowCallbacks):\n+    \"\"\"A registry of WorkflowCallbacks.\"\"\"\n+\n+    _callbacks: list[WorkflowCallbacks]\n+\n+    def __init__(self):\n+        \"\"\"Create a new instance of WorkflowCallbacksRegistry.\"\"\"\n+        self._callbacks = []\n+\n+    def register(self, callbacks: WorkflowCallbacks) -> None:\n+        \"\"\"Register a new WorkflowCallbacks type.\"\"\"\n+        self._callbacks.append(callbacks)\n+\n+    def pipeline_start(self, names: list[str]) -> None:\n+        \"\"\"Execute this callback when a the entire pipeline starts.\"\"\"\n+        for callback in self._callbacks:\n+            if hasattr(callback, \"pipeline_start\"):\n+                callback.pipeline_start(names)\n+\n+    def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n+        \"\"\"Execute this callback when the entire pipeline ends.\"\"\"\n+        for callback in self._callbacks:\n+            if hasattr(callback, \"pipeline_end\"):\n+                callback.pipeline_end(results)\n+\n+    def workflow_start(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow starts.\"\"\"\n+        for callback in self._callbacks:\n+            if hasattr(callback, \"workflow_start\"):\n+                callback.workflow_start(name, instance)\n+\n+    def workflow_end(self, name: str, instance: object) -> None:\n+        \"\"\"Execute this callback when a workflow ends.\"\"\"\n+        for callback in self._callbacks:\n+            if hasattr(callback, \"workflow_end\"):\n+                callback.workflow_end(name, instance)\n+\n+    def progress(self, progress: Progress) -> None:\n+        \"\"\"Handle when progress occurs.\"\"\"\n+        for callback in self._callbacks:\n+            if hasattr(callback, \"progress\"):\n+                callback.progress(progress)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cli/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cli/__init__.py b/packages/graphrag/graphrag/cli/__init__.py\nnew file mode 100644\nindex 0000000..2301782\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cli/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI for GraphRAG.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cli/index.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cli/index.py b/packages/graphrag/graphrag/cli/index.py\nnew file mode 100644\nindex 0000000..b5464d2\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cli/index.py\n@@ -0,0 +1,161 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI implementation of the index subcommand.\"\"\"\n+\n+import asyncio\n+import logging\n+import sys\n+import warnings\n+from pathlib import Path\n+\n+import graphrag.api as api\n+from graphrag.callbacks.console_workflow_callbacks import ConsoleWorkflowCallbacks\n+from graphrag.config.enums import CacheType, IndexingMethod\n+from graphrag.config.load_config import load_config\n+from graphrag.index.validate_config import validate_config_names\n+from graphrag.utils.cli import redact\n+\n+# Ignore warnings from numba\n+warnings.filterwarnings(\"ignore\", message=\".*NumbaDeprecationWarning.*\")\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def _register_signal_handlers():\n+    import signal\n+\n+    def handle_signal(signum, _):\n+        # Handle the signal here\n+        logger.debug(f\"Received signal {signum}, exiting...\")  # noqa: G004\n+        for task in asyncio.all_tasks():\n+            task.cancel()\n+        logger.debug(\"All tasks cancelled. Exiting...\")\n+\n+    # Register signal handlers for SIGINT and SIGHUP\n+    signal.signal(signal.SIGINT, handle_signal)\n+\n+    if sys.platform != \"win32\":\n+        signal.signal(signal.SIGHUP, handle_signal)\n+\n+\n+def index_cli(\n+    root_dir: Path,\n+    method: IndexingMethod,\n+    verbose: bool,\n+    memprofile: bool,\n+    cache: bool,\n+    config_filepath: Path | None,\n+    dry_run: bool,\n+    skip_validation: bool,\n+    output_dir: Path | None,\n+):\n+    \"\"\"Run the pipeline with the given config.\"\"\"\n+    cli_overrides = {}\n+    if output_dir:\n+        cli_overrides[\"output.base_dir\"] = str(output_dir)\n+        cli_overrides[\"reporting.base_dir\"] = str(output_dir)\n+        cli_overrides[\"update_index_output.base_dir\"] = str(output_dir)\n+    config = load_config(root_dir, config_filepath, cli_overrides)\n+    _run_index(\n+        config=config,\n+        method=method,\n+        is_update_run=False,\n+        verbose=verbose,\n+        memprofile=memprofile,\n+        cache=cache,\n+        dry_run=dry_run,\n+        skip_validation=skip_validation,\n+    )\n+\n+\n+def update_cli(\n+    root_dir: Path,\n+    method: IndexingMethod,\n+    verbose: bool,\n+    memprofile: bool,\n+    cache: bool,\n+    config_filepath: Path | None,\n+    skip_validation: bool,\n+    output_dir: Path | None,\n+):\n+    \"\"\"Run the pipeline with the given config.\"\"\"\n+    cli_overrides = {}\n+    if output_dir:\n+        cli_overrides[\"output.base_dir\"] = str(output_dir)\n+        cli_overrides[\"reporting.base_dir\"] = str(output_dir)\n+        cli_overrides[\"update_index_output.base_dir\"] = str(output_dir)\n+\n+    config = load_config(root_dir, config_filepath, cli_overrides)\n+\n+    _run_index(\n+        config=config,\n+        method=method,\n+        is_update_run=True,\n+        verbose=verbose,\n+        memprofile=memprofile,\n+        cache=cache,\n+        dry_run=False,\n+        skip_validation=skip_validation,\n+    )\n+\n+\n+def _run_index(\n+    config,\n+    method,\n+    is_update_run,\n+    verbose,\n+    memprofile,\n+    cache,\n+    dry_run,\n+    skip_validation,\n+):\n+    # Configure the root logger with the specified log level\n+    from graphrag.logger.standard_logging import init_loggers\n+\n+    # Initialize loggers and reporting config\n+    init_loggers(\n+        config=config,\n+        verbose=verbose,\n+    )\n+\n+    if not cache:\n+        config.cache.type = CacheType.none\n+\n+    if not skip_validation:\n+        validate_config_names(config)\n+\n+    logger.info(\"Starting pipeline run. %s\", dry_run)\n+    logger.info(\n+        \"Using default configuration: %s\",\n+        redact(config.model_dump()),\n+    )\n+\n+    if dry_run:\n+        logger.info(\"Dry run complete, exiting...\", True)\n+        sys.exit(0)\n+\n+    _register_signal_handlers()\n+\n+    outputs = asyncio.run(\n+        api.build_index(\n+            config=config,\n+            method=method,\n+            is_update_run=is_update_run,\n+            memory_profile=memprofile,\n+            callbacks=[ConsoleWorkflowCallbacks(verbose=verbose)],\n+            verbose=verbose,\n+        )\n+    )\n+    encountered_errors = any(\n+        output.errors and len(output.errors) > 0 for output in outputs\n+    )\n+\n+    if encountered_errors:\n+        logger.error(\n+            \"Errors occurred during the pipeline run, see logs for more details.\"\n+        )\n+    else:\n+        logger.info(\"All workflows completed successfully.\")\n+\n+    sys.exit(1 if encountered_errors else 0)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cli/initialize.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cli/initialize.py b/packages/graphrag/graphrag/cli/initialize.py\nnew file mode 100644\nindex 0000000..09215f8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cli/initialize.py\n@@ -0,0 +1,95 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI implementation of the initialization subcommand.\"\"\"\n+\n+import logging\n+from pathlib import Path\n+\n+from graphrag.config.init_content import INIT_DOTENV, INIT_YAML\n+from graphrag.prompts.index.community_report import (\n+    COMMUNITY_REPORT_PROMPT,\n+)\n+from graphrag.prompts.index.community_report_text_units import (\n+    COMMUNITY_REPORT_TEXT_PROMPT,\n+)\n+from graphrag.prompts.index.extract_claims import EXTRACT_CLAIMS_PROMPT\n+from graphrag.prompts.index.extract_graph import GRAPH_EXTRACTION_PROMPT\n+from graphrag.prompts.index.summarize_descriptions import SUMMARIZE_PROMPT\n+from graphrag.prompts.query.basic_search_system_prompt import BASIC_SEARCH_SYSTEM_PROMPT\n+from graphrag.prompts.query.drift_search_system_prompt import (\n+    DRIFT_LOCAL_SYSTEM_PROMPT,\n+    DRIFT_REDUCE_PROMPT,\n+)\n+from graphrag.prompts.query.global_search_knowledge_system_prompt import (\n+    GENERAL_KNOWLEDGE_INSTRUCTION,\n+)\n+from graphrag.prompts.query.global_search_map_system_prompt import MAP_SYSTEM_PROMPT\n+from graphrag.prompts.query.global_search_reduce_system_prompt import (\n+    REDUCE_SYSTEM_PROMPT,\n+)\n+from graphrag.prompts.query.local_search_system_prompt import LOCAL_SEARCH_SYSTEM_PROMPT\n+from graphrag.prompts.query.question_gen_system_prompt import QUESTION_SYSTEM_PROMPT\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def initialize_project_at(path: Path, force: bool) -> None:\n+    \"\"\"\n+    Initialize the project at the given path.\n+\n+    Parameters\n+    ----------\n+    path : Path\n+        The path at which to initialize the project.\n+    force : bool\n+        Whether to force initialization even if the project already exists.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If the project already exists and force is False.\n+    \"\"\"\n+    logger.info(\"Initializing project at %s\", path)\n+    root = Path(path)\n+    if not root.exists():\n+        root.mkdir(parents=True, exist_ok=True)\n+\n+    settings_yaml = root / \"settings.yaml\"\n+    if settings_yaml.exists() and not force:\n+        msg = f\"Project already initialized at {root}\"\n+        raise ValueError(msg)\n+\n+    with settings_yaml.open(\"wb\") as file:\n+        file.write(INIT_YAML.encode(encoding=\"utf-8\", errors=\"strict\"))\n+\n+    dotenv = root / \".env\"\n+    if not dotenv.exists() or force:\n+        with dotenv.open(\"wb\") as file:\n+            file.write(INIT_DOTENV.encode(encoding=\"utf-8\", errors=\"strict\"))\n+\n+    prompts_dir = root / \"prompts\"\n+    if not prompts_dir.exists():\n+        prompts_dir.mkdir(parents=True, exist_ok=True)\n+\n+    prompts = {\n+        \"extract_graph\": GRAPH_EXTRACTION_PROMPT,\n+        \"summarize_descriptions\": SUMMARIZE_PROMPT,\n+        \"extract_claims\": EXTRACT_CLAIMS_PROMPT,\n+        \"community_report_graph\": COMMUNITY_REPORT_PROMPT,\n+        \"community_report_text\": COMMUNITY_REPORT_TEXT_PROMPT,\n+        \"drift_search_system_prompt\": DRIFT_LOCAL_SYSTEM_PROMPT,\n+        \"drift_reduce_prompt\": DRIFT_REDUCE_PROMPT,\n+        \"global_search_map_system_prompt\": MAP_SYSTEM_PROMPT,\n+        \"global_search_reduce_system_prompt\": REDUCE_SYSTEM_PROMPT,\n+        \"global_search_knowledge_system_prompt\": GENERAL_KNOWLEDGE_INSTRUCTION,\n+        \"local_search_system_prompt\": LOCAL_SEARCH_SYSTEM_PROMPT,\n+        \"basic_search_system_prompt\": BASIC_SEARCH_SYSTEM_PROMPT,\n+        \"question_gen_system_prompt\": QUESTION_SYSTEM_PROMPT,\n+    }\n+\n+    for name, content in prompts.items():\n+        prompt_file = prompts_dir / f\"{name}.txt\"\n+        if not prompt_file.exists() or force:\n+            with prompt_file.open(\"wb\") as file:\n+                file.write(content.encode(encoding=\"utf-8\", errors=\"strict\"))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cli/main.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cli/main.py b/packages/graphrag/graphrag/cli/main.py\nnew file mode 100644\nindex 0000000..7b4d3bb\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cli/main.py\n@@ -0,0 +1,546 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI entrypoint.\"\"\"\n+\n+import os\n+import re\n+from collections.abc import Callable\n+from pathlib import Path\n+\n+import typer\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import IndexingMethod, SearchMethod\n+from graphrag.prompt_tune.defaults import LIMIT, MAX_TOKEN_COUNT, N_SUBSET_MAX, K\n+from graphrag.prompt_tune.types import DocSelectionType\n+\n+INVALID_METHOD_ERROR = \"Invalid method\"\n+\n+app = typer.Typer(\n+    help=\"GraphRAG: A graph-based retrieval-augmented generation (RAG) system.\",\n+    no_args_is_help=True,\n+)\n+\n+\n+# A workaround for typer's lack of support for proper autocompletion of file/directory paths\n+# For more detail, watch\n+#   https://github.com/fastapi/typer/discussions/682\n+#   https://github.com/fastapi/typer/issues/951\n+def path_autocomplete(\n+    file_okay: bool = True,\n+    dir_okay: bool = True,\n+    readable: bool = True,\n+    writable: bool = False,\n+    match_wildcard: str | None = None,\n+) -> Callable[[str], list[str]]:\n+    \"\"\"Autocomplete file and directory paths.\"\"\"\n+\n+    def wildcard_match(string: str, pattern: str) -> bool:\n+        regex = re.escape(pattern).replace(r\"\\?\", \".\").replace(r\"\\*\", \".*\")\n+        return re.fullmatch(regex, string) is not None\n+\n+    from pathlib import Path\n+\n+    def completer(incomplete: str) -> list[str]:\n+        # List items in the current directory as Path objects\n+        items = Path().iterdir()\n+        completions = []\n+\n+        for item in items:\n+            # Filter based on file/directory properties\n+            if not file_okay and item.is_file():\n+                continue\n+            if not dir_okay and item.is_dir():\n+                continue\n+            if readable and not os.access(item, os.R_OK):\n+                continue\n+            if writable and not os.access(item, os.W_OK):\n+                continue\n+\n+            # Append the name of the matching item\n+            completions.append(item.name)\n+\n+        # Apply wildcard matching if required\n+        if match_wildcard:\n+            completions = filter(\n+                lambda i: wildcard_match(i, match_wildcard)\n+                if match_wildcard\n+                else False,\n+                completions,\n+            )\n+\n+        # Return completions that start with the given incomplete string\n+        return [i for i in completions if i.startswith(incomplete)]\n+\n+    return completer\n+\n+\n+CONFIG_AUTOCOMPLETE = path_autocomplete(\n+    file_okay=True,\n+    dir_okay=False,\n+    match_wildcard=\"*.yaml\",\n+    readable=True,\n+)\n+\n+ROOT_AUTOCOMPLETE = path_autocomplete(\n+    file_okay=False,\n+    dir_okay=True,\n+    writable=True,\n+    match_wildcard=\"*\",\n+)\n+\n+\n+@app.command(\"init\")\n+def _initialize_cli(\n+    root: Path = typer.Option(\n+        Path(),\n+        \"--root\",\n+        \"-r\",\n+        help=\"The project root directory.\",\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+        autocompletion=ROOT_AUTOCOMPLETE,\n+    ),\n+    force: bool = typer.Option(\n+        False,\n+        \"--force\",\n+        \"-f\",\n+        help=\"Force initialization even if the project already exists.\",\n+    ),\n+) -> None:\n+    \"\"\"Generate a default configuration file.\"\"\"\n+    from graphrag.cli.initialize import initialize_project_at\n+\n+    initialize_project_at(path=root, force=force)\n+\n+\n+@app.command(\"index\")\n+def _index_cli(\n+    config: Path | None = typer.Option(\n+        None,\n+        \"--config\",\n+        \"-c\",\n+        help=\"The configuration to use.\",\n+        exists=True,\n+        file_okay=True,\n+        readable=True,\n+        autocompletion=CONFIG_AUTOCOMPLETE,\n+    ),\n+    root: Path = typer.Option(\n+        Path(),\n+        \"--root\",\n+        \"-r\",\n+        help=\"The project root directory.\",\n+        exists=True,\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+        autocompletion=ROOT_AUTOCOMPLETE,\n+    ),\n+    method: IndexingMethod = typer.Option(\n+        IndexingMethod.Standard.value,\n+        \"--method\",\n+        \"-m\",\n+        help=\"The indexing method to use.\",\n+    ),\n+    verbose: bool = typer.Option(\n+        False,\n+        \"--verbose\",\n+        \"-v\",\n+        help=\"Run the indexing pipeline with verbose logging\",\n+    ),\n+    memprofile: bool = typer.Option(\n+        False,\n+        \"--memprofile\",\n+        help=\"Run the indexing pipeline with memory profiling\",\n+    ),\n+    dry_run: bool = typer.Option(\n+        False,\n+        \"--dry-run\",\n+        help=(\n+            \"Run the indexing pipeline without executing any steps \"\n+            \"to inspect and validate the configuration.\"\n+        ),\n+    ),\n+    cache: bool = typer.Option(\n+        True,\n+        \"--cache/--no-cache\",\n+        help=\"Use LLM cache.\",\n+    ),\n+    skip_validation: bool = typer.Option(\n+        False,\n+        \"--skip-validation\",\n+        help=\"Skip any preflight validation. Useful when running no LLM steps.\",\n+    ),\n+    output: Path | None = typer.Option(\n+        None,\n+        \"--output\",\n+        \"-o\",\n+        help=(\n+            \"Indexing pipeline output directory. \"\n+            \"Overrides output.base_dir in the configuration file.\"\n+        ),\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+    ),\n+) -> None:\n+    \"\"\"Build a knowledge graph index.\"\"\"\n+    from graphrag.cli.index import index_cli\n+\n+    index_cli(\n+        root_dir=root,\n+        verbose=verbose,\n+        memprofile=memprofile,\n+        cache=cache,\n+        config_filepath=config,\n+        dry_run=dry_run,\n+        skip_validation=skip_validation,\n+        output_dir=output,\n+        method=method,\n+    )\n+\n+\n+@app.command(\"update\")\n+def _update_cli(\n+    config: Path | None = typer.Option(\n+        None,\n+        \"--config\",\n+        \"-c\",\n+        help=\"The configuration to use.\",\n+        exists=True,\n+        file_okay=True,\n+        readable=True,\n+        autocompletion=CONFIG_AUTOCOMPLETE,\n+    ),\n+    root: Path = typer.Option(\n+        Path(),\n+        \"--root\",\n+        \"-r\",\n+        help=\"The project root directory.\",\n+        exists=True,\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+        autocompletion=ROOT_AUTOCOMPLETE,\n+    ),\n+    method: IndexingMethod = typer.Option(\n+        IndexingMethod.Standard.value,\n+        \"--method\",\n+        \"-m\",\n+        help=\"The indexing method to use.\",\n+    ),\n+    verbose: bool = typer.Option(\n+        False,\n+        \"--verbose\",\n+        \"-v\",\n+        help=\"Run the indexing pipeline with verbose logging.\",\n+    ),\n+    memprofile: bool = typer.Option(\n+        False,\n+        \"--memprofile\",\n+        help=\"Run the indexing pipeline with memory profiling.\",\n+    ),\n+    cache: bool = typer.Option(\n+        True,\n+        \"--cache/--no-cache\",\n+        help=\"Use LLM cache.\",\n+    ),\n+    skip_validation: bool = typer.Option(\n+        False,\n+        \"--skip-validation\",\n+        help=\"Skip any preflight validation. Useful when running no LLM steps.\",\n+    ),\n+    output: Path | None = typer.Option(\n+        None,\n+        \"--output\",\n+        \"-o\",\n+        help=(\n+            \"Indexing pipeline output directory. \"\n+            \"Overrides output.base_dir in the configuration file.\"\n+        ),\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+    ),\n+) -> None:\n+    \"\"\"\n+    Update an existing knowledge graph index.\n+\n+    Applies a default output configuration (if not provided by config), saving the new index to the local file system in the `update_output` folder.\n+    \"\"\"\n+    from graphrag.cli.index import update_cli\n+\n+    update_cli(\n+        root_dir=root,\n+        verbose=verbose,\n+        memprofile=memprofile,\n+        cache=cache,\n+        config_filepath=config,\n+        skip_validation=skip_validation,\n+        output_dir=output,\n+        method=method,\n+    )\n+\n+\n+@app.command(\"prompt-tune\")\n+def _prompt_tune_cli(\n+    root: Path = typer.Option(\n+        Path(),\n+        \"--root\",\n+        \"-r\",\n+        help=\"The project root directory.\",\n+        exists=True,\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+        autocompletion=ROOT_AUTOCOMPLETE,\n+    ),\n+    config: Path | None = typer.Option(\n+        None,\n+        \"--config\",\n+        \"-c\",\n+        help=\"The configuration to use.\",\n+        exists=True,\n+        file_okay=True,\n+        readable=True,\n+        autocompletion=CONFIG_AUTOCOMPLETE,\n+    ),\n+    verbose: bool = typer.Option(\n+        False,\n+        \"--verbose\",\n+        \"-v\",\n+        help=\"Run the prompt tuning pipeline with verbose logging.\",\n+    ),\n+    domain: str | None = typer.Option(\n+        None,\n+        \"--domain\",\n+        help=(\n+            \"The domain your input data is related to. \"\n+            \"For example 'space science', 'microbiology', 'environmental news'. \"\n+            \"If not defined, a domain will be inferred from the input data.\"\n+        ),\n+    ),\n+    selection_method: DocSelectionType = typer.Option(\n+        DocSelectionType.RANDOM.value,\n+        \"--selection-method\",\n+        help=\"The text chunk selection method.\",\n+    ),\n+    n_subset_max: int = typer.Option(\n+        N_SUBSET_MAX,\n+        \"--n-subset-max\",\n+        help=\"The number of text chunks to embed when --selection-method=auto.\",\n+    ),\n+    k: int = typer.Option(\n+        K,\n+        \"--k\",\n+        help=\"The maximum number of documents to select from each centroid when --selection-method=auto.\",\n+    ),\n+    limit: int = typer.Option(\n+        LIMIT,\n+        \"--limit\",\n+        help=\"The number of documents to load when --selection-method={random,top}.\",\n+    ),\n+    max_tokens: int = typer.Option(\n+        MAX_TOKEN_COUNT,\n+        \"--max-tokens\",\n+        help=\"The max token count for prompt generation.\",\n+    ),\n+    min_examples_required: int = typer.Option(\n+        2,\n+        \"--min-examples-required\",\n+        help=\"The minimum number of examples to generate/include in the entity extraction prompt.\",\n+    ),\n+    chunk_size: int = typer.Option(\n+        graphrag_config_defaults.chunks.size,\n+        \"--chunk-size\",\n+        help=\"The size of each example text chunk. Overrides chunks.size in the configuration file.\",\n+    ),\n+    overlap: int = typer.Option(\n+        graphrag_config_defaults.chunks.overlap,\n+        \"--overlap\",\n+        help=\"The overlap size for chunking documents. Overrides chunks.overlap in the configuration file.\",\n+    ),\n+    language: str | None = typer.Option(\n+        None,\n+        \"--language\",\n+        help=\"The primary language used for inputs and outputs in graphrag prompts.\",\n+    ),\n+    discover_entity_types: bool = typer.Option(\n+        True,\n+        \"--discover-entity-types/--no-discover-entity-types\",\n+        help=\"Discover and extract unspecified entity types.\",\n+    ),\n+    output: Path = typer.Option(\n+        Path(\"prompts\"),\n+        \"--output\",\n+        \"-o\",\n+        help=\"The directory to save prompts to, relative to the project root directory.\",\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+    ),\n+) -> None:\n+    \"\"\"Generate custom graphrag prompts with your own data (i.e. auto templating).\"\"\"\n+    import asyncio\n+\n+    from graphrag.cli.prompt_tune import prompt_tune\n+\n+    loop = asyncio.get_event_loop()\n+    loop.run_until_complete(\n+        prompt_tune(\n+            root=root,\n+            config=config,\n+            domain=domain,\n+            verbose=verbose,\n+            selection_method=selection_method,\n+            limit=limit,\n+            max_tokens=max_tokens,\n+            chunk_size=chunk_size,\n+            overlap=overlap,\n+            language=language,\n+            discover_entity_types=discover_entity_types,\n+            output=output,\n+            n_subset_max=n_subset_max,\n+            k=k,\n+            min_examples_required=min_examples_required,\n+        )\n+    )\n+\n+\n+@app.command(\"query\")\n+def _query_cli(\n+    method: SearchMethod = typer.Option(\n+        ...,\n+        \"--method\",\n+        \"-m\",\n+        help=\"The query algorithm to use.\",\n+    ),\n+    query: str = typer.Option(\n+        ...,\n+        \"--query\",\n+        \"-q\",\n+        help=\"The query to execute.\",\n+    ),\n+    config: Path | None = typer.Option(\n+        None,\n+        \"--config\",\n+        \"-c\",\n+        help=\"The configuration to use.\",\n+        exists=True,\n+        file_okay=True,\n+        readable=True,\n+        autocompletion=CONFIG_AUTOCOMPLETE,\n+    ),\n+    verbose: bool = typer.Option(\n+        False,\n+        \"--verbose\",\n+        \"-v\",\n+        help=\"Run the query with verbose logging.\",\n+    ),\n+    data: Path | None = typer.Option(\n+        None,\n+        \"--data\",\n+        \"-d\",\n+        help=\"Index output directory (contains the parquet files).\",\n+        exists=True,\n+        dir_okay=True,\n+        readable=True,\n+        resolve_path=True,\n+        autocompletion=ROOT_AUTOCOMPLETE,\n+    ),\n+    root: Path = typer.Option(\n+        Path(),\n+        \"--root\",\n+        \"-r\",\n+        help=\"The project root directory.\",\n+        exists=True,\n+        dir_okay=True,\n+        writable=True,\n+        resolve_path=True,\n+        autocompletion=ROOT_AUTOCOMPLETE,\n+    ),\n+    community_level: int = typer.Option(\n+        2,\n+        \"--community-level\",\n+        help=(\n+            \"Leiden hierarchy level from which to load community reports. \"\n+            \"Higher values represent smaller communities.\"\n+        ),\n+    ),\n+    dynamic_community_selection: bool = typer.Option(\n+        False,\n+        \"--dynamic-community-selection/--no-dynamic-selection\",\n+        help=\"Use global search with dynamic community selection.\",\n+    ),\n+    response_type: str = typer.Option(\n+        \"Multiple Paragraphs\",\n+        \"--response-type\",\n+        help=(\n+            \"Free-form description of the desired response format \"\n+            \"(e.g. 'Single Sentence', 'List of 3-7 Points', etc.).\"\n+        ),\n+    ),\n+    streaming: bool = typer.Option(\n+        False,\n+        \"--streaming/--no-streaming\",\n+        help=\"Print the response in a streaming manner.\",\n+    ),\n+) -> None:\n+    \"\"\"Query a knowledge graph index.\"\"\"\n+    from graphrag.cli.query import (\n+        run_basic_search,\n+        run_drift_search,\n+        run_global_search,\n+        run_local_search,\n+    )\n+\n+    match method:\n+        case SearchMethod.LOCAL:\n+            run_local_search(\n+                config_filepath=config,\n+                data_dir=data,\n+                root_dir=root,\n+                community_level=community_level,\n+                response_type=response_type,\n+                streaming=streaming,\n+                query=query,\n+                verbose=verbose,\n+            )\n+        case SearchMethod.GLOBAL:\n+            run_global_search(\n+                config_filepath=config,\n+                data_dir=data,\n+                root_dir=root,\n+                community_level=community_level,\n+                dynamic_community_selection=dynamic_community_selection,\n+                response_type=response_type,\n+                streaming=streaming,\n+                query=query,\n+                verbose=verbose,\n+            )\n+        case SearchMethod.DRIFT:\n+            run_drift_search(\n+                config_filepath=config,\n+                data_dir=data,\n+                root_dir=root,\n+                community_level=community_level,\n+                streaming=streaming,\n+                response_type=response_type,\n+                query=query,\n+                verbose=verbose,\n+            )\n+        case SearchMethod.BASIC:\n+            run_basic_search(\n+                config_filepath=config,\n+                data_dir=data,\n+                root_dir=root,\n+                response_type=response_type,\n+                streaming=streaming,\n+                query=query,\n+                verbose=verbose,\n+            )\n+        case _:\n+            raise ValueError(INVALID_METHOD_ERROR)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cli/prompt_tune.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cli/prompt_tune.py b/packages/graphrag/graphrag/cli/prompt_tune.py\nnew file mode 100644\nindex 0000000..2646776\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cli/prompt_tune.py\n@@ -0,0 +1,117 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI implementation of the prompt-tune subcommand.\"\"\"\n+\n+import logging\n+from pathlib import Path\n+\n+import graphrag.api as api\n+from graphrag.config.load_config import load_config\n+from graphrag.prompt_tune.generator.community_report_summarization import (\n+    COMMUNITY_SUMMARIZATION_FILENAME,\n+)\n+from graphrag.prompt_tune.generator.entity_summarization_prompt import (\n+    ENTITY_SUMMARIZATION_FILENAME,\n+)\n+from graphrag.prompt_tune.generator.extract_graph_prompt import (\n+    EXTRACT_GRAPH_FILENAME,\n+)\n+from graphrag.utils.cli import redact\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def prompt_tune(\n+    root: Path,\n+    config: Path | None,\n+    domain: str | None,\n+    verbose: bool,\n+    selection_method: api.DocSelectionType,\n+    limit: int,\n+    max_tokens: int,\n+    chunk_size: int,\n+    overlap: int,\n+    language: str | None,\n+    discover_entity_types: bool,\n+    output: Path,\n+    n_subset_max: int,\n+    k: int,\n+    min_examples_required: int,\n+):\n+    \"\"\"Prompt tune the model.\n+\n+    Parameters\n+    ----------\n+    - config: The configuration file.\n+    - root: The root directory.\n+    - domain: The domain to map the input documents to.\n+    - verbose: Enable verbose logging.\n+    - selection_method: The chunk selection method.\n+    - limit: The limit of chunks to load.\n+    - max_tokens: The maximum number of tokens to use on entity extraction prompts.\n+    - chunk_size: The chunk token size to use.\n+    - language: The language to use for the prompts.\n+    - discover_entity_types: Generate entity types.\n+    - output: The output folder to store the prompts.\n+    - n_subset_max: The number of text chunks to embed when using auto selection method.\n+    - k: The number of documents to select when using auto selection method.\n+    - min_examples_required: The minimum number of examples required for entity extraction prompts.\n+    \"\"\"\n+    root_path = Path(root).resolve()\n+    graph_config = load_config(root_path, config)\n+\n+    # override chunking config in the configuration\n+    if chunk_size != graph_config.chunks.size:\n+        graph_config.chunks.size = chunk_size\n+\n+    if overlap != graph_config.chunks.overlap:\n+        graph_config.chunks.overlap = overlap\n+\n+    # configure the root logger with the specified log level\n+    from graphrag.logger.standard_logging import init_loggers\n+\n+    # initialize loggers with config\n+    init_loggers(config=graph_config, verbose=verbose, filename=\"prompt-tuning.log\")\n+\n+    logger.info(\"Starting prompt tune.\")\n+    logger.info(\n+        \"Using default configuration: %s\",\n+        redact(graph_config.model_dump()),\n+    )\n+\n+    prompts = await api.generate_indexing_prompts(\n+        config=graph_config,\n+        chunk_size=chunk_size,\n+        overlap=overlap,\n+        limit=limit,\n+        selection_method=selection_method,\n+        domain=domain,\n+        language=language,\n+        max_tokens=max_tokens,\n+        discover_entity_types=discover_entity_types,\n+        min_examples_required=min_examples_required,\n+        n_subset_max=n_subset_max,\n+        k=k,\n+        verbose=verbose,\n+    )\n+\n+    output_path = output.resolve()\n+    if output_path:\n+        logger.info(\"Writing prompts to %s\", output_path)\n+        output_path.mkdir(parents=True, exist_ok=True)\n+        extract_graph_prompt_path = output_path / EXTRACT_GRAPH_FILENAME\n+        entity_summarization_prompt_path = output_path / ENTITY_SUMMARIZATION_FILENAME\n+        community_summarization_prompt_path = (\n+            output_path / COMMUNITY_SUMMARIZATION_FILENAME\n+        )\n+        # write files to output path\n+        with extract_graph_prompt_path.open(\"wb\") as file:\n+            file.write(prompts[0].encode(encoding=\"utf-8\", errors=\"strict\"))\n+        with entity_summarization_prompt_path.open(\"wb\") as file:\n+            file.write(prompts[1].encode(encoding=\"utf-8\", errors=\"strict\"))\n+        with community_summarization_prompt_path.open(\"wb\") as file:\n+            file.write(prompts[2].encode(encoding=\"utf-8\", errors=\"strict\"))\n+        logger.info(\"Prompts written to %s\", output_path)\n+    else:\n+        logger.error(\"No output path provided. Skipping writing prompts.\")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/cli/query.py",
            "diff": "diff --git a/packages/graphrag/graphrag/cli/query.py b/packages/graphrag/graphrag/cli/query.py\nnew file mode 100644\nindex 0000000..914196b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/cli/query.py\n@@ -0,0 +1,391 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI implementation of the query subcommand.\"\"\"\n+\n+import asyncio\n+import sys\n+from pathlib import Path\n+from typing import TYPE_CHECKING, Any\n+\n+import graphrag.api as api\n+from graphrag.callbacks.noop_query_callbacks import NoopQueryCallbacks\n+from graphrag.config.load_config import load_config\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.utils.api import create_storage_from_config\n+from graphrag.utils.storage import load_table_from_storage, storage_has_table\n+\n+if TYPE_CHECKING:\n+    import pandas as pd\n+\n+# ruff: noqa: T201\n+\n+\n+def run_global_search(\n+    config_filepath: Path | None,\n+    data_dir: Path | None,\n+    root_dir: Path,\n+    community_level: int | None,\n+    dynamic_community_selection: bool,\n+    response_type: str,\n+    streaming: bool,\n+    query: str,\n+    verbose: bool,\n+):\n+    \"\"\"Perform a global search with a given query.\n+\n+    Loads index files required for global search and calls the Query API.\n+    \"\"\"\n+    root = root_dir.resolve()\n+    cli_overrides = {}\n+    if data_dir:\n+        cli_overrides[\"output.base_dir\"] = str(data_dir)\n+    config = load_config(root, config_filepath, cli_overrides)\n+\n+    dataframe_dict = _resolve_output_files(\n+        config=config,\n+        output_list=[\n+            \"entities\",\n+            \"communities\",\n+            \"community_reports\",\n+        ],\n+        optional_list=[],\n+    )\n+\n+    entities: pd.DataFrame = dataframe_dict[\"entities\"]\n+    communities: pd.DataFrame = dataframe_dict[\"communities\"]\n+    community_reports: pd.DataFrame = dataframe_dict[\"community_reports\"]\n+\n+    if streaming:\n+\n+        async def run_streaming_search():\n+            full_response = \"\"\n+            context_data = {}\n+\n+            def on_context(context: Any) -> None:\n+                nonlocal context_data\n+                context_data = context\n+\n+            callbacks = NoopQueryCallbacks()\n+            callbacks.on_context = on_context\n+\n+            async for stream_chunk in api.global_search_streaming(\n+                config=config,\n+                entities=entities,\n+                communities=communities,\n+                community_reports=community_reports,\n+                community_level=community_level,\n+                dynamic_community_selection=dynamic_community_selection,\n+                response_type=response_type,\n+                query=query,\n+                callbacks=[callbacks],\n+                verbose=verbose,\n+            ):\n+                full_response += stream_chunk\n+                print(stream_chunk, end=\"\")\n+                sys.stdout.flush()\n+            print()\n+            return full_response, context_data\n+\n+        return asyncio.run(run_streaming_search())\n+    # not streaming\n+    response, context_data = asyncio.run(\n+        api.global_search(\n+            config=config,\n+            entities=entities,\n+            communities=communities,\n+            community_reports=community_reports,\n+            community_level=community_level,\n+            dynamic_community_selection=dynamic_community_selection,\n+            response_type=response_type,\n+            query=query,\n+            verbose=verbose,\n+        )\n+    )\n+    print(response)\n+\n+    return response, context_data\n+\n+\n+def run_local_search(\n+    config_filepath: Path | None,\n+    data_dir: Path | None,\n+    root_dir: Path,\n+    community_level: int,\n+    response_type: str,\n+    streaming: bool,\n+    query: str,\n+    verbose: bool,\n+):\n+    \"\"\"Perform a local search with a given query.\n+\n+    Loads index files required for local search and calls the Query API.\n+    \"\"\"\n+    root = root_dir.resolve()\n+    cli_overrides = {}\n+    if data_dir:\n+        cli_overrides[\"output.base_dir\"] = str(data_dir)\n+    config = load_config(root, config_filepath, cli_overrides)\n+\n+    dataframe_dict = _resolve_output_files(\n+        config=config,\n+        output_list=[\n+            \"communities\",\n+            \"community_reports\",\n+            \"text_units\",\n+            \"relationships\",\n+            \"entities\",\n+        ],\n+        optional_list=[\n+            \"covariates\",\n+        ],\n+    )\n+\n+    communities: pd.DataFrame = dataframe_dict[\"communities\"]\n+    community_reports: pd.DataFrame = dataframe_dict[\"community_reports\"]\n+    text_units: pd.DataFrame = dataframe_dict[\"text_units\"]\n+    relationships: pd.DataFrame = dataframe_dict[\"relationships\"]\n+    entities: pd.DataFrame = dataframe_dict[\"entities\"]\n+    covariates: pd.DataFrame | None = dataframe_dict[\"covariates\"]\n+\n+    if streaming:\n+\n+        async def run_streaming_search():\n+            full_response = \"\"\n+            context_data = {}\n+\n+            def on_context(context: Any) -> None:\n+                nonlocal context_data\n+                context_data = context\n+\n+            callbacks = NoopQueryCallbacks()\n+            callbacks.on_context = on_context\n+\n+            async for stream_chunk in api.local_search_streaming(\n+                config=config,\n+                entities=entities,\n+                communities=communities,\n+                community_reports=community_reports,\n+                text_units=text_units,\n+                relationships=relationships,\n+                covariates=covariates,\n+                community_level=community_level,\n+                response_type=response_type,\n+                query=query,\n+                callbacks=[callbacks],\n+                verbose=verbose,\n+            ):\n+                full_response += stream_chunk\n+                print(stream_chunk, end=\"\")\n+                sys.stdout.flush()\n+            print()\n+            return full_response, context_data\n+\n+        return asyncio.run(run_streaming_search())\n+    # not streaming\n+    response, context_data = asyncio.run(\n+        api.local_search(\n+            config=config,\n+            entities=entities,\n+            communities=communities,\n+            community_reports=community_reports,\n+            text_units=text_units,\n+            relationships=relationships,\n+            covariates=covariates,\n+            community_level=community_level,\n+            response_type=response_type,\n+            query=query,\n+            verbose=verbose,\n+        )\n+    )\n+    print(response)\n+\n+    return response, context_data\n+\n+\n+def run_drift_search(\n+    config_filepath: Path | None,\n+    data_dir: Path | None,\n+    root_dir: Path,\n+    community_level: int,\n+    response_type: str,\n+    streaming: bool,\n+    query: str,\n+    verbose: bool,\n+):\n+    \"\"\"Perform a local search with a given query.\n+\n+    Loads index files required for local search and calls the Query API.\n+    \"\"\"\n+    root = root_dir.resolve()\n+    cli_overrides = {}\n+    if data_dir:\n+        cli_overrides[\"output.base_dir\"] = str(data_dir)\n+    config = load_config(root, config_filepath, cli_overrides)\n+\n+    dataframe_dict = _resolve_output_files(\n+        config=config,\n+        output_list=[\n+            \"communities\",\n+            \"community_reports\",\n+            \"text_units\",\n+            \"relationships\",\n+            \"entities\",\n+        ],\n+    )\n+\n+    communities: pd.DataFrame = dataframe_dict[\"communities\"]\n+    community_reports: pd.DataFrame = dataframe_dict[\"community_reports\"]\n+    text_units: pd.DataFrame = dataframe_dict[\"text_units\"]\n+    relationships: pd.DataFrame = dataframe_dict[\"relationships\"]\n+    entities: pd.DataFrame = dataframe_dict[\"entities\"]\n+\n+    if streaming:\n+\n+        async def run_streaming_search():\n+            full_response = \"\"\n+            context_data = {}\n+\n+            def on_context(context: Any) -> None:\n+                nonlocal context_data\n+                context_data = context\n+\n+            callbacks = NoopQueryCallbacks()\n+            callbacks.on_context = on_context\n+\n+            async for stream_chunk in api.drift_search_streaming(\n+                config=config,\n+                entities=entities,\n+                communities=communities,\n+                community_reports=community_reports,\n+                text_units=text_units,\n+                relationships=relationships,\n+                community_level=community_level,\n+                response_type=response_type,\n+                query=query,\n+                callbacks=[callbacks],\n+                verbose=verbose,\n+            ):\n+                full_response += stream_chunk\n+                print(stream_chunk, end=\"\")\n+                sys.stdout.flush()\n+            print()\n+            return full_response, context_data\n+\n+        return asyncio.run(run_streaming_search())\n+\n+    # not streaming\n+    response, context_data = asyncio.run(\n+        api.drift_search(\n+            config=config,\n+            entities=entities,\n+            communities=communities,\n+            community_reports=community_reports,\n+            text_units=text_units,\n+            relationships=relationships,\n+            community_level=community_level,\n+            response_type=response_type,\n+            query=query,\n+            verbose=verbose,\n+        )\n+    )\n+    print(response)\n+\n+    return response, context_data\n+\n+\n+def run_basic_search(\n+    config_filepath: Path | None,\n+    data_dir: Path | None,\n+    root_dir: Path,\n+    response_type: str,\n+    streaming: bool,\n+    query: str,\n+    verbose: bool,\n+):\n+    \"\"\"Perform a basics search with a given query.\n+\n+    Loads index files required for basic search and calls the Query API.\n+    \"\"\"\n+    root = root_dir.resolve()\n+    cli_overrides = {}\n+    if data_dir:\n+        cli_overrides[\"output.base_dir\"] = str(data_dir)\n+    config = load_config(root, config_filepath, cli_overrides)\n+\n+    dataframe_dict = _resolve_output_files(\n+        config=config,\n+        output_list=[\n+            \"text_units\",\n+        ],\n+    )\n+\n+    text_units: pd.DataFrame = dataframe_dict[\"text_units\"]\n+\n+    if streaming:\n+\n+        async def run_streaming_search():\n+            full_response = \"\"\n+            context_data = {}\n+\n+            def on_context(context: Any) -> None:\n+                nonlocal context_data\n+                context_data = context\n+\n+            callbacks = NoopQueryCallbacks()\n+            callbacks.on_context = on_context\n+\n+            async for stream_chunk in api.basic_search_streaming(\n+                config=config,\n+                text_units=text_units,\n+                response_type=response_type,\n+                query=query,\n+                callbacks=[callbacks],\n+                verbose=verbose,\n+            ):\n+                full_response += stream_chunk\n+                print(stream_chunk, end=\"\")\n+                sys.stdout.flush()\n+            print()\n+            return full_response, context_data\n+\n+        return asyncio.run(run_streaming_search())\n+    # not streaming\n+    response, context_data = asyncio.run(\n+        api.basic_search(\n+            config=config,\n+            text_units=text_units,\n+            response_type=response_type,\n+            query=query,\n+            verbose=verbose,\n+        )\n+    )\n+    print(response)\n+\n+    return response, context_data\n+\n+\n+def _resolve_output_files(\n+    config: GraphRagConfig,\n+    output_list: list[str],\n+    optional_list: list[str] | None = None,\n+) -> dict[str, Any]:\n+    \"\"\"Read indexing output files to a dataframe dict.\"\"\"\n+    dataframe_dict = {}\n+    storage_obj = create_storage_from_config(config.output)\n+    for name in output_list:\n+        df_value = asyncio.run(load_table_from_storage(name=name, storage=storage_obj))\n+        dataframe_dict[name] = df_value\n+\n+    # for optional output files, set the dict entry to None instead of erroring out if it does not exist\n+    if optional_list:\n+        for optional_file in optional_list:\n+            file_exists = asyncio.run(storage_has_table(optional_file, storage_obj))\n+            if file_exists:\n+                df_value = asyncio.run(\n+                    load_table_from_storage(name=optional_file, storage=storage_obj)\n+                )\n+                dataframe_dict[optional_file] = df_value\n+            else:\n+                dataframe_dict[optional_file] = None\n+    return dataframe_dict\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/__init__.py b/packages/graphrag/graphrag/config/__init__.py\nnew file mode 100644\nindex 0000000..926d8d9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The config package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/create_graphrag_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/create_graphrag_config.py b/packages/graphrag/graphrag/config/create_graphrag_config.py\nnew file mode 100644\nindex 0000000..59d7699\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/create_graphrag_config.py\n@@ -0,0 +1,43 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration, loaded from environment variables.\"\"\"\n+\n+from pathlib import Path\n+from typing import Any\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+\n+\n+def create_graphrag_config(\n+    values: dict[str, Any] | None = None,\n+    root_dir: str | None = None,\n+) -> GraphRagConfig:\n+    \"\"\"Load Configuration Parameters from a dictionary.\n+\n+    Parameters\n+    ----------\n+    values : dict[str, Any] | None\n+        Dictionary of configuration values to pass into pydantic model.\n+    root_dir : str | None\n+        Root directory for the project.\n+    skip_validation : bool\n+        Skip pydantic model validation of the configuration.\n+        This is useful for testing and mocking purposes but\n+        should not be used in the core code or API.\n+\n+    Returns\n+    -------\n+    GraphRagConfig\n+        The configuration object.\n+\n+    Raises\n+    ------\n+    ValidationError\n+        If the configuration values do not satisfy pydantic validation.\n+    \"\"\"\n+    values = values or {}\n+    if root_dir:\n+        root_path = Path(root_dir).resolve()\n+        values[\"root_dir\"] = str(root_path)\n+    return GraphRagConfig(**values)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/defaults.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/defaults.py b/packages/graphrag/graphrag/config/defaults.py\nnew file mode 100644\nindex 0000000..fdeb241\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/defaults.py\n@@ -0,0 +1,425 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Common default configuration values.\"\"\"\n+\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+from typing import ClassVar\n+\n+from graphrag.config.embeddings import default_embeddings\n+from graphrag.config.enums import (\n+    AsyncType,\n+    AuthType,\n+    CacheType,\n+    ChunkStrategyType,\n+    InputFileType,\n+    ModelType,\n+    NounPhraseExtractorType,\n+    ReportingType,\n+    StorageType,\n+    VectorStoreType,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.stop_words import (\n+    EN_STOP_WORDS,\n+)\n+\n+DEFAULT_OUTPUT_BASE_DIR = \"output\"\n+DEFAULT_CHAT_MODEL_ID = \"default_chat_model\"\n+DEFAULT_CHAT_MODEL_TYPE = ModelType.Chat\n+DEFAULT_CHAT_MODEL_AUTH_TYPE = AuthType.APIKey\n+DEFAULT_CHAT_MODEL = \"gpt-4.1\"\n+DEFAULT_EMBEDDING_MODEL_ID = \"default_embedding_model\"\n+DEFAULT_EMBEDDING_MODEL_TYPE = ModelType.Embedding\n+DEFAULT_EMBEDDING_MODEL_AUTH_TYPE = AuthType.APIKey\n+DEFAULT_EMBEDDING_MODEL = \"text-embedding-3-large\"\n+DEFAULT_MODEL_PROVIDER = \"openai\"\n+\n+ENCODING_MODEL = \"o200k_base\"\n+COGNITIVE_SERVICES_AUDIENCE = \"https://cognitiveservices.azure.com/.default\"\n+\n+DEFAULT_ENTITY_TYPES = [\"organization\", \"person\", \"geo\", \"event\"]\n+\n+\n+@dataclass\n+class BasicSearchDefaults:\n+    \"\"\"Default values for basic search.\"\"\"\n+\n+    prompt: None = None\n+    k: int = 10\n+    max_context_tokens: int = 12_000\n+    chat_model_id: str = DEFAULT_CHAT_MODEL_ID\n+    embedding_model_id: str = DEFAULT_EMBEDDING_MODEL_ID\n+\n+\n+@dataclass\n+class CacheDefaults:\n+    \"\"\"Default values for cache.\"\"\"\n+\n+    type: ClassVar[CacheType] = CacheType.file\n+    base_dir: str = \"cache\"\n+    connection_string: None = None\n+    container_name: None = None\n+    storage_account_blob_url: None = None\n+    cosmosdb_account_url: None = None\n+\n+\n+@dataclass\n+class ChunksDefaults:\n+    \"\"\"Default values for chunks.\"\"\"\n+\n+    size: int = 1200\n+    overlap: int = 100\n+    strategy: ClassVar[ChunkStrategyType] = ChunkStrategyType.tokens\n+    encoding_model: str = ENCODING_MODEL\n+    prepend_metadata: bool = False\n+    chunk_size_includes_metadata: bool = False\n+\n+\n+@dataclass\n+class ClusterGraphDefaults:\n+    \"\"\"Default values for cluster graph.\"\"\"\n+\n+    max_cluster_size: int = 10\n+    use_lcc: bool = True\n+    seed: int = 0xDEADBEEF\n+\n+\n+@dataclass\n+class CommunityReportDefaults:\n+    \"\"\"Default values for community report.\"\"\"\n+\n+    graph_prompt: None = None\n+    text_prompt: None = None\n+    max_length: int = 2000\n+    max_input_length: int = 8000\n+    model_id: str = DEFAULT_CHAT_MODEL_ID\n+    model_instance_name: str = \"community_reporting\"\n+\n+\n+@dataclass\n+class DriftSearchDefaults:\n+    \"\"\"Default values for drift search.\"\"\"\n+\n+    prompt: None = None\n+    reduce_prompt: None = None\n+    data_max_tokens: int = 12_000\n+    reduce_max_tokens: None = None\n+    reduce_temperature: float = 0\n+    reduce_max_completion_tokens: None = None\n+    concurrency: int = 32\n+    drift_k_followups: int = 20\n+    primer_folds: int = 5\n+    primer_llm_max_tokens: int = 12_000\n+    n_depth: int = 3\n+    local_search_text_unit_prop: float = 0.9\n+    local_search_community_prop: float = 0.1\n+    local_search_top_k_mapped_entities: int = 10\n+    local_search_top_k_relationships: int = 10\n+    local_search_max_data_tokens: int = 12_000\n+    local_search_temperature: float = 0\n+    local_search_top_p: float = 1\n+    local_search_n: int = 1\n+    local_search_llm_max_gen_tokens: int | None = None\n+    local_search_llm_max_gen_completion_tokens: int | None = None\n+    chat_model_id: str = DEFAULT_CHAT_MODEL_ID\n+    embedding_model_id: str = DEFAULT_EMBEDDING_MODEL_ID\n+\n+\n+@dataclass\n+class EmbedTextDefaults:\n+    \"\"\"Default values for embedding text.\"\"\"\n+\n+    model_id: str = DEFAULT_EMBEDDING_MODEL_ID\n+    model_instance_name: str = \"text_embedding\"\n+    batch_size: int = 16\n+    batch_max_tokens: int = 8191\n+    names: list[str] = field(default_factory=lambda: default_embeddings)\n+    strategy: None = None\n+\n+\n+@dataclass\n+class ExtractClaimsDefaults:\n+    \"\"\"Default values for claim extraction.\"\"\"\n+\n+    enabled: bool = False\n+    prompt: None = None\n+    description: str = (\n+        \"Any claims or facts that could be relevant to information discovery.\"\n+    )\n+    max_gleanings: int = 1\n+    strategy: None = None\n+    model_id: str = DEFAULT_CHAT_MODEL_ID\n+    model_instance_name: str = \"extract_claims\"\n+\n+\n+@dataclass\n+class ExtractGraphDefaults:\n+    \"\"\"Default values for extracting graph.\"\"\"\n+\n+    prompt: None = None\n+    entity_types: list[str] = field(\n+        default_factory=lambda: [\"organization\", \"person\", \"geo\", \"event\"]\n+    )\n+    max_gleanings: int = 1\n+    strategy: None = None\n+    model_id: str = DEFAULT_CHAT_MODEL_ID\n+    model_instance_name: str = \"extract_graph\"\n+\n+\n+@dataclass\n+class TextAnalyzerDefaults:\n+    \"\"\"Default values for text analyzer.\"\"\"\n+\n+    extractor_type: ClassVar[NounPhraseExtractorType] = (\n+        NounPhraseExtractorType.RegexEnglish\n+    )\n+    model_name: str = \"en_core_web_md\"\n+    max_word_length: int = 15\n+    word_delimiter: str = \" \"\n+    include_named_entities: bool = True\n+    exclude_nouns: list[str] = field(default_factory=lambda: EN_STOP_WORDS)\n+    exclude_entity_tags: list[str] = field(default_factory=lambda: [\"DATE\"])\n+    exclude_pos_tags: list[str] = field(\n+        default_factory=lambda: [\"DET\", \"PRON\", \"INTJ\", \"X\"]\n+    )\n+    noun_phrase_tags: list[str] = field(default_factory=lambda: [\"PROPN\", \"NOUNS\"])\n+    noun_phrase_grammars: dict[str, str] = field(\n+        default_factory=lambda: {\n+            \"PROPN,PROPN\": \"PROPN\",\n+            \"NOUN,NOUN\": \"NOUNS\",\n+            \"NOUNS,NOUN\": \"NOUNS\",\n+            \"ADJ,ADJ\": \"ADJ\",\n+            \"ADJ,NOUN\": \"NOUNS\",\n+        }\n+    )\n+\n+\n+@dataclass\n+class ExtractGraphNLPDefaults:\n+    \"\"\"Default values for NLP graph extraction.\"\"\"\n+\n+    normalize_edge_weights: bool = True\n+    text_analyzer: TextAnalyzerDefaults = field(default_factory=TextAnalyzerDefaults)\n+    concurrent_requests: int = 25\n+    async_mode: AsyncType = AsyncType.Threaded\n+\n+\n+@dataclass\n+class GlobalSearchDefaults:\n+    \"\"\"Default values for global search.\"\"\"\n+\n+    map_prompt: None = None\n+    reduce_prompt: None = None\n+    knowledge_prompt: None = None\n+    max_context_tokens: int = 12_000\n+    data_max_tokens: int = 12_000\n+    map_max_length: int = 1000\n+    reduce_max_length: int = 2000\n+    dynamic_search_threshold: int = 1\n+    dynamic_search_keep_parent: bool = False\n+    dynamic_search_num_repeats: int = 1\n+    dynamic_search_use_summary: bool = False\n+    dynamic_search_max_level: int = 2\n+    chat_model_id: str = DEFAULT_CHAT_MODEL_ID\n+\n+\n+@dataclass\n+class StorageDefaults:\n+    \"\"\"Default values for storage.\"\"\"\n+\n+    type: ClassVar[StorageType] = StorageType.file\n+    base_dir: str = DEFAULT_OUTPUT_BASE_DIR\n+    connection_string: None = None\n+    container_name: None = None\n+    storage_account_blob_url: None = None\n+    cosmosdb_account_url: None = None\n+\n+\n+@dataclass\n+class InputStorageDefaults(StorageDefaults):\n+    \"\"\"Default values for input storage.\"\"\"\n+\n+    base_dir: str = \"input\"\n+\n+\n+@dataclass\n+class InputDefaults:\n+    \"\"\"Default values for input.\"\"\"\n+\n+    storage: InputStorageDefaults = field(default_factory=InputStorageDefaults)\n+    file_type: ClassVar[InputFileType] = InputFileType.text\n+    encoding: str = \"utf-8\"\n+    file_pattern: str = \"\"\n+    text_column: str = \"text\"\n+    title_column: None = None\n+    metadata: None = None\n+\n+\n+@dataclass\n+class LanguageModelDefaults:\n+    \"\"\"Default values for language model.\"\"\"\n+\n+    api_key: None = None\n+    auth_type: ClassVar[AuthType] = AuthType.APIKey\n+    model_provider: str | None = None\n+    encoding_model: str = \"\"\n+    max_tokens: int | None = None\n+    temperature: float = 0\n+    max_completion_tokens: int | None = None\n+    reasoning_effort: str | None = None\n+    top_p: float = 1\n+    n: int = 1\n+    frequency_penalty: float = 0.0\n+    presence_penalty: float = 0.0\n+    request_timeout: float = 180.0\n+    api_base: None = None\n+    api_version: None = None\n+    deployment_name: None = None\n+    organization: None = None\n+    proxy: None = None\n+    audience: None = None\n+    model_supports_json: None = None\n+    tokens_per_minute: None = None\n+    requests_per_minute: None = None\n+    rate_limit_strategy: str | None = \"static\"\n+    retry_strategy: str = \"exponential_backoff\"\n+    max_retries: int = 10\n+    max_retry_wait: float = 10.0\n+    concurrent_requests: int = 25\n+    responses: None = None\n+    async_mode: AsyncType = AsyncType.Threaded\n+\n+\n+@dataclass\n+class LocalSearchDefaults:\n+    \"\"\"Default values for local search.\"\"\"\n+\n+    prompt: None = None\n+    text_unit_prop: float = 0.5\n+    community_prop: float = 0.15\n+    conversation_history_max_turns: int = 5\n+    top_k_entities: int = 10\n+    top_k_relationships: int = 10\n+    max_context_tokens: int = 12_000\n+    chat_model_id: str = DEFAULT_CHAT_MODEL_ID\n+    embedding_model_id: str = DEFAULT_EMBEDDING_MODEL_ID\n+\n+\n+@dataclass\n+class OutputDefaults(StorageDefaults):\n+    \"\"\"Default values for output.\"\"\"\n+\n+    base_dir: str = DEFAULT_OUTPUT_BASE_DIR\n+\n+\n+@dataclass\n+class PruneGraphDefaults:\n+    \"\"\"Default values for pruning graph.\"\"\"\n+\n+    min_node_freq: int = 2\n+    max_node_freq_std: None = None\n+    min_node_degree: int = 1\n+    max_node_degree_std: None = None\n+    min_edge_weight_pct: float = 40.0\n+    remove_ego_nodes: bool = True\n+    lcc_only: bool = False\n+\n+\n+@dataclass\n+class ReportingDefaults:\n+    \"\"\"Default values for reporting.\"\"\"\n+\n+    type: ClassVar[ReportingType] = ReportingType.file\n+    base_dir: str = \"logs\"\n+    connection_string: None = None\n+    container_name: None = None\n+    storage_account_blob_url: None = None\n+\n+\n+@dataclass\n+class SnapshotsDefaults:\n+    \"\"\"Default values for snapshots.\"\"\"\n+\n+    embeddings: bool = False\n+    graphml: bool = False\n+    raw_graph: bool = False\n+\n+\n+@dataclass\n+class SummarizeDescriptionsDefaults:\n+    \"\"\"Default values for summarizing descriptions.\"\"\"\n+\n+    prompt: None = None\n+    max_length: int = 500\n+    max_input_tokens: int = 4_000\n+    strategy: None = None\n+    model_id: str = DEFAULT_CHAT_MODEL_ID\n+    model_instance_name: str = \"summarize_descriptions\"\n+\n+\n+@dataclass\n+class UpdateIndexOutputDefaults(StorageDefaults):\n+    \"\"\"Default values for update index output.\"\"\"\n+\n+    base_dir: str = \"update_output\"\n+\n+\n+@dataclass\n+class VectorStoreDefaults:\n+    \"\"\"Default values for vector stores.\"\"\"\n+\n+    type: ClassVar[str] = VectorStoreType.LanceDB.value\n+    db_uri: str = str(Path(DEFAULT_OUTPUT_BASE_DIR) / \"lancedb\")\n+    overwrite: bool = True\n+    index_prefix: None = None\n+    url: None = None\n+    api_key: None = None\n+    audience: None = None\n+    database_name: None = None\n+    schema: None = None\n+\n+\n+@dataclass\n+class GraphRagConfigDefaults:\n+    \"\"\"Default values for GraphRAG.\"\"\"\n+\n+    root_dir: str = \"\"\n+    models: dict = field(default_factory=dict)\n+    reporting: ReportingDefaults = field(default_factory=ReportingDefaults)\n+    storage: StorageDefaults = field(default_factory=StorageDefaults)\n+    output: OutputDefaults = field(default_factory=OutputDefaults)\n+    update_index_output: UpdateIndexOutputDefaults = field(\n+        default_factory=UpdateIndexOutputDefaults\n+    )\n+    cache: CacheDefaults = field(default_factory=CacheDefaults)\n+    input: InputDefaults = field(default_factory=InputDefaults)\n+    embed_text: EmbedTextDefaults = field(default_factory=EmbedTextDefaults)\n+    chunks: ChunksDefaults = field(default_factory=ChunksDefaults)\n+    snapshots: SnapshotsDefaults = field(default_factory=SnapshotsDefaults)\n+    extract_graph: ExtractGraphDefaults = field(default_factory=ExtractGraphDefaults)\n+    extract_graph_nlp: ExtractGraphNLPDefaults = field(\n+        default_factory=ExtractGraphNLPDefaults\n+    )\n+    summarize_descriptions: SummarizeDescriptionsDefaults = field(\n+        default_factory=SummarizeDescriptionsDefaults\n+    )\n+    community_reports: CommunityReportDefaults = field(\n+        default_factory=CommunityReportDefaults\n+    )\n+    extract_claims: ExtractClaimsDefaults = field(default_factory=ExtractClaimsDefaults)\n+    prune_graph: PruneGraphDefaults = field(default_factory=PruneGraphDefaults)\n+    cluster_graph: ClusterGraphDefaults = field(default_factory=ClusterGraphDefaults)\n+    local_search: LocalSearchDefaults = field(default_factory=LocalSearchDefaults)\n+    global_search: GlobalSearchDefaults = field(default_factory=GlobalSearchDefaults)\n+    drift_search: DriftSearchDefaults = field(default_factory=DriftSearchDefaults)\n+    basic_search: BasicSearchDefaults = field(default_factory=BasicSearchDefaults)\n+    vector_store: VectorStoreDefaults = field(\n+        default_factory=lambda: VectorStoreDefaults()\n+    )\n+    workflows: None = None\n+\n+\n+language_model_defaults = LanguageModelDefaults()\n+vector_store_defaults = VectorStoreDefaults()\n+graphrag_config_defaults = GraphRagConfigDefaults()\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/embeddings.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/embeddings.py b/packages/graphrag/graphrag/config/embeddings.py\nnew file mode 100644\nindex 0000000..60711b8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/embeddings.py\n@@ -0,0 +1,51 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing embeddings values.\"\"\"\n+\n+entity_title_embedding = \"entity.title\"\n+entity_description_embedding = \"entity.description\"\n+relationship_description_embedding = \"relationship.description\"\n+document_text_embedding = \"document.text\"\n+community_title_embedding = \"community.title\"\n+community_summary_embedding = \"community.summary\"\n+community_full_content_embedding = \"community.full_content\"\n+text_unit_text_embedding = \"text_unit.text\"\n+\n+all_embeddings: set[str] = {\n+    entity_title_embedding,\n+    entity_description_embedding,\n+    relationship_description_embedding,\n+    document_text_embedding,\n+    community_title_embedding,\n+    community_summary_embedding,\n+    community_full_content_embedding,\n+    text_unit_text_embedding,\n+}\n+default_embeddings: list[str] = [\n+    entity_description_embedding,\n+    community_full_content_embedding,\n+    text_unit_text_embedding,\n+]\n+\n+\n+def create_index_name(\n+    index_prefix: str, embedding_name: str, validate: bool = True\n+) -> str:\n+    \"\"\"\n+    Create a index name for the embedding store.\n+\n+    Within any given vector store, we can have multiple sets of embeddings organized into projects.\n+    The `container` param is used for this partitioning, and is added as a index_prefix to the index name for differentiation.\n+\n+    The embedding name is fixed, with the available list defined in graphrag.index.config.embeddings\n+\n+    Note that we use dot notation in our names, but many vector stores do not support this - so we convert to dashes.\n+    \"\"\"\n+    if validate and embedding_name not in all_embeddings:\n+        msg = f\"Invalid embedding name: {embedding_name}\"\n+        raise KeyError(msg)\n+\n+    if index_prefix:\n+        return f\"{index_prefix}-{embedding_name}\".replace(\".\", \"-\")\n+    return embedding_name.replace(\".\", \"-\")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/enums.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/enums.py b/packages/graphrag/graphrag/config/enums.py\nnew file mode 100644\nindex 0000000..4835763\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/enums.py\n@@ -0,0 +1,172 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing config enums.\"\"\"\n+\n+from __future__ import annotations\n+\n+from enum import Enum\n+\n+\n+class CacheType(str, Enum):\n+    \"\"\"The cache configuration type for the pipeline.\"\"\"\n+\n+    file = \"file\"\n+    \"\"\"The file cache configuration type.\"\"\"\n+    memory = \"memory\"\n+    \"\"\"The memory cache configuration type.\"\"\"\n+    none = \"none\"\n+    \"\"\"The none cache configuration type.\"\"\"\n+    blob = \"blob\"\n+    \"\"\"The blob cache configuration type.\"\"\"\n+    cosmosdb = \"cosmosdb\"\n+    \"\"\"The cosmosdb cache configuration type\"\"\"\n+\n+    def __repr__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return f'\"{self.value}\"'\n+\n+\n+class InputFileType(str, Enum):\n+    \"\"\"The input file type for the pipeline.\"\"\"\n+\n+    csv = \"csv\"\n+    \"\"\"The CSV input type.\"\"\"\n+    text = \"text\"\n+    \"\"\"The text input type.\"\"\"\n+    json = \"json\"\n+    \"\"\"The JSON input type.\"\"\"\n+\n+    def __repr__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return f'\"{self.value}\"'\n+\n+\n+class StorageType(str, Enum):\n+    \"\"\"The output type for the pipeline.\"\"\"\n+\n+    file = \"file\"\n+    \"\"\"The file output type.\"\"\"\n+    memory = \"memory\"\n+    \"\"\"The memory output type.\"\"\"\n+    blob = \"blob\"\n+    \"\"\"The blob output type.\"\"\"\n+    cosmosdb = \"cosmosdb\"\n+    \"\"\"The cosmosdb output type\"\"\"\n+\n+    def __repr__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return f'\"{self.value}\"'\n+\n+\n+class VectorStoreType(str, Enum):\n+    \"\"\"The supported vector store types.\"\"\"\n+\n+    LanceDB = \"lancedb\"\n+    AzureAISearch = \"azure_ai_search\"\n+    CosmosDB = \"cosmosdb\"\n+\n+\n+class ReportingType(str, Enum):\n+    \"\"\"The reporting configuration type for the pipeline.\"\"\"\n+\n+    file = \"file\"\n+    \"\"\"The file reporting configuration type.\"\"\"\n+    blob = \"blob\"\n+    \"\"\"The blob reporting configuration type.\"\"\"\n+\n+    def __repr__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return f'\"{self.value}\"'\n+\n+\n+class ModelType(str, Enum):\n+    \"\"\"LLMType enum class definition.\"\"\"\n+\n+    # Embeddings\n+    Embedding = \"embedding\"\n+\n+    # Chat Completion\n+    Chat = \"chat\"\n+\n+    # Debug\n+    MockChat = \"mock_chat\"\n+    MockEmbedding = \"mock_embedding\"\n+\n+    def __repr__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return f'\"{self.value}\"'\n+\n+\n+class AuthType(str, Enum):\n+    \"\"\"AuthType enum class definition.\"\"\"\n+\n+    APIKey = \"api_key\"\n+    AzureManagedIdentity = \"azure_managed_identity\"\n+\n+\n+class AsyncType(str, Enum):\n+    \"\"\"Enum for the type of async to use.\"\"\"\n+\n+    AsyncIO = \"asyncio\"\n+    Threaded = \"threaded\"\n+\n+\n+class ChunkStrategyType(str, Enum):\n+    \"\"\"ChunkStrategy class definition.\"\"\"\n+\n+    tokens = \"tokens\"\n+    sentence = \"sentence\"\n+\n+    def __repr__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return f'\"{self.value}\"'\n+\n+\n+class SearchMethod(Enum):\n+    \"\"\"The type of search to run.\"\"\"\n+\n+    LOCAL = \"local\"\n+    GLOBAL = \"global\"\n+    DRIFT = \"drift\"\n+    BASIC = \"basic\"\n+\n+    def __str__(self):\n+        \"\"\"Return the string representation of the enum value.\"\"\"\n+        return self.value\n+\n+\n+class IndexingMethod(str, Enum):\n+    \"\"\"Enum for the type of indexing to perform.\"\"\"\n+\n+    Standard = \"standard\"\n+    \"\"\"Traditional GraphRAG indexing, with all graph construction and summarization performed by a language model.\"\"\"\n+    Fast = \"fast\"\n+    \"\"\"Fast indexing, using NLP for graph construction and language model for summarization.\"\"\"\n+    StandardUpdate = \"standard-update\"\n+    \"\"\"Incremental update with standard indexing.\"\"\"\n+    FastUpdate = \"fast-update\"\n+    \"\"\"Incremental update with fast indexing.\"\"\"\n+\n+\n+class NounPhraseExtractorType(str, Enum):\n+    \"\"\"Enum for the noun phrase extractor options.\"\"\"\n+\n+    RegexEnglish = \"regex_english\"\n+    \"\"\"Standard extractor using regex. Fastest, but limited to English.\"\"\"\n+    Syntactic = \"syntactic_parser\"\n+    \"\"\"Noun phrase extractor based on dependency parsing and NER using SpaCy.\"\"\"\n+    CFG = \"cfg\"\n+    \"\"\"Noun phrase extractor combining CFG-based noun-chunk extraction and NER.\"\"\"\n+\n+\n+class ModularityMetric(str, Enum):\n+    \"\"\"Enum for the modularity metric to use.\"\"\"\n+\n+    Graph = \"graph\"\n+    \"\"\"Graph modularity metric.\"\"\"\n+\n+    LCC = \"lcc\"\n+\n+    WeightedComponents = \"weighted_components\"\n+    \"\"\"Weighted components modularity metric.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/environment_reader.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/environment_reader.py b/packages/graphrag/graphrag/config/environment_reader.py\nnew file mode 100644\nindex 0000000..2584226\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/environment_reader.py\n@@ -0,0 +1,155 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A configuration reader utility class.\"\"\"\n+\n+from collections.abc import Callable\n+from contextlib import contextmanager\n+from enum import Enum\n+from typing import Any, TypeVar\n+\n+from environs import Env\n+\n+T = TypeVar(\"T\")\n+\n+KeyValue = str | Enum\n+EnvKeySet = str | list[str]\n+\n+\n+def read_key(value: KeyValue) -> str:\n+    \"\"\"Read a key value.\"\"\"\n+    if not isinstance(value, str):\n+        return value.value.lower()\n+    return value.lower()\n+\n+\n+class EnvironmentReader:\n+    \"\"\"A configuration reader utility class.\"\"\"\n+\n+    _env: Env\n+    _config_stack: list[dict]\n+\n+    def __init__(self, env: Env):\n+        self._env = env\n+        self._config_stack = []\n+\n+    @property\n+    def env(self):\n+        \"\"\"Get the environment object.\"\"\"\n+        return self._env\n+\n+    def _read_env(\n+        self, env_key: str | list[str], default_value: T, read: Callable[[str, T], T]\n+    ) -> T | None:\n+        if isinstance(env_key, str):\n+            env_key = [env_key]\n+\n+        for k in env_key:\n+            result = read(k.upper(), default_value)\n+            if result is not default_value:\n+                return result\n+\n+        return default_value\n+\n+    def envvar_prefix(self, prefix: KeyValue):\n+        \"\"\"Set the environment variable prefix.\"\"\"\n+        prefix = read_key(prefix)\n+        prefix = f\"{prefix}_\".upper()\n+        return self._env.prefixed(prefix)\n+\n+    def use(self, value: Any | None):\n+        \"\"\"Create a context manager to push the value into the config_stack.\"\"\"\n+\n+        @contextmanager\n+        def config_context():\n+            self._config_stack.append(value or {})\n+            try:\n+                yield\n+            finally:\n+                self._config_stack.pop()\n+\n+        return config_context()\n+\n+    @property\n+    def section(self) -> dict:\n+        \"\"\"Get the current section.\"\"\"\n+        return self._config_stack[-1] if self._config_stack else {}\n+\n+    def str(\n+        self,\n+        key: KeyValue,\n+        env_key: EnvKeySet | None = None,\n+        default_value: str | None = None,\n+    ) -> str | None:\n+        \"\"\"Read a configuration value.\"\"\"\n+        key = read_key(key)\n+        if self.section and key in self.section:\n+            return self.section[key]\n+\n+        return self._read_env(\n+            env_key or key, default_value, (lambda k, dv: self._env(k, dv))\n+        )\n+\n+    def int(\n+        self,\n+        key: KeyValue,\n+        env_key: EnvKeySet | None = None,\n+        default_value: int | None = None,\n+    ) -> int | None:\n+        \"\"\"Read an integer configuration value.\"\"\"\n+        key = read_key(key)\n+        if self.section and key in self.section:\n+            return int(self.section[key])\n+        return self._read_env(\n+            env_key or key, default_value, lambda k, dv: self._env.int(k, dv)\n+        )\n+\n+    def bool(\n+        self,\n+        key: KeyValue,\n+        env_key: EnvKeySet | None = None,\n+        default_value: bool | None = None,\n+    ) -> bool | None:\n+        \"\"\"Read an integer configuration value.\"\"\"\n+        key = read_key(key)\n+        if self.section and key in self.section:\n+            return bool(self.section[key])\n+\n+        return self._read_env(\n+            env_key or key, default_value, lambda k, dv: self._env.bool(k, dv)\n+        )\n+\n+    def float(\n+        self,\n+        key: KeyValue,\n+        env_key: EnvKeySet | None = None,\n+        default_value: float | None = None,\n+    ) -> float | None:\n+        \"\"\"Read a float configuration value.\"\"\"\n+        key = read_key(key)\n+        if self.section and key in self.section:\n+            return float(self.section[key])\n+        return self._read_env(\n+            env_key or key, default_value, lambda k, dv: self._env.float(k, dv)\n+        )\n+\n+    def list(\n+        self,\n+        key: KeyValue,\n+        env_key: EnvKeySet | None = None,\n+        default_value: list | None = None,\n+    ) -> list | None:\n+        \"\"\"Parse an list configuration value.\"\"\"\n+        key = read_key(key)\n+        result = None\n+        if self.section and key in self.section:\n+            result = self.section[key]\n+            if isinstance(result, list):\n+                return result\n+\n+        if result is None:\n+            result = self.str(key, env_key)\n+        if result is not None:\n+            result = [s.strip() for s in result.split(\",\")]\n+            return [s for s in result if s]\n+        return default_value\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/errors.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/errors.py b/packages/graphrag/graphrag/config/errors.py\nnew file mode 100644\nindex 0000000..32a20b8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/errors.py\n@@ -0,0 +1,50 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"Errors for the default configuration.\"\"\"\n+\n+\n+class ApiKeyMissingError(ValueError):\n+    \"\"\"LLM Key missing error.\"\"\"\n+\n+    def __init__(self, llm_type: str, azure_auth_type: str | None = None) -> None:\n+        \"\"\"Init method definition.\"\"\"\n+        msg = f\"API Key is required for {llm_type}\"\n+        if azure_auth_type:\n+            msg += f\" when using {azure_auth_type} authentication\"\n+        msg += \". Please rerun `graphrag init` and set the API_KEY.\"\n+        super().__init__(msg)\n+\n+\n+class AzureApiBaseMissingError(ValueError):\n+    \"\"\"Azure API Base missing error.\"\"\"\n+\n+    def __init__(self, llm_type: str) -> None:\n+        \"\"\"Init method definition.\"\"\"\n+        msg = f\"API Base is required for {llm_type}. Please rerun `graphrag init` and set the api_base.\"\n+        super().__init__(msg)\n+\n+\n+class AzureApiVersionMissingError(ValueError):\n+    \"\"\"Azure API version missing error.\"\"\"\n+\n+    def __init__(self, llm_type: str) -> None:\n+        \"\"\"Init method definition.\"\"\"\n+        msg = f\"API Version is required for {llm_type}. Please rerun `graphrag init` and set the api_version.\"\n+        super().__init__(msg)\n+\n+\n+class LanguageModelConfigMissingError(ValueError):\n+    \"\"\"Missing model configuration error.\"\"\"\n+\n+    def __init__(self, key: str = \"\") -> None:\n+        \"\"\"Init method definition.\"\"\"\n+        msg = f'A {key} model configuration is required. Please rerun `graphrag init` and set models[\"{key}\"] in settings.yaml.'\n+        super().__init__(msg)\n+\n+\n+class ConflictingSettingsError(ValueError):\n+    \"\"\"Missing model configuration error.\"\"\"\n+\n+    def __init__(self, msg: str) -> None:\n+        \"\"\"Init method definition.\"\"\"\n+        super().__init__(msg)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/init_content.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/init_content.py b/packages/graphrag/graphrag/config/init_content.py\nnew file mode 100644\nindex 0000000..1cbccf7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/init_content.py\n@@ -0,0 +1,151 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Content for the init CLI command to generate a default configuration.\"\"\"\n+\n+import graphrag.config.defaults as defs\n+from graphrag.config.defaults import (\n+    graphrag_config_defaults,\n+    language_model_defaults,\n+    vector_store_defaults,\n+)\n+\n+INIT_YAML = f\"\"\"\\\n+### This config file contains required core defaults that must be set, along with a handful of common optional settings.\n+### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/\n+\n+### LLM settings ###\n+## There are a number of settings to tune the threading and token limits for LLM calls - check the docs.\n+\n+models:\n+  {defs.DEFAULT_CHAT_MODEL_ID}:\n+    type: {defs.DEFAULT_CHAT_MODEL_TYPE.value}\n+    model_provider: {defs.DEFAULT_MODEL_PROVIDER}\n+    auth_type: {defs.DEFAULT_CHAT_MODEL_AUTH_TYPE.value} # or azure_managed_identity\n+    api_key: ${{GRAPHRAG_API_KEY}} # set this in the generated .env file, or remove if managed identity\n+    model: {defs.DEFAULT_CHAT_MODEL}\n+    # api_base: https://<instance>.openai.azure.com\n+    # api_version: 2024-05-01-preview\n+    model_supports_json: true # recommended if this is available for your model.\n+    concurrent_requests: {language_model_defaults.concurrent_requests}\n+    retry_strategy: {language_model_defaults.retry_strategy}\n+    max_retries: {language_model_defaults.max_retries}\n+    tokens_per_minute: null\n+    requests_per_minute: null\n+  {defs.DEFAULT_EMBEDDING_MODEL_ID}:\n+    type: {defs.DEFAULT_EMBEDDING_MODEL_TYPE.value}\n+    model_provider: {defs.DEFAULT_MODEL_PROVIDER}\n+    auth_type: {defs.DEFAULT_EMBEDDING_MODEL_AUTH_TYPE.value}\n+    api_key: ${{GRAPHRAG_API_KEY}}\n+    model: {defs.DEFAULT_EMBEDDING_MODEL}\n+    # api_base: https://<instance>.openai.azure.com\n+    # api_version: 2024-05-01-preview\n+    concurrent_requests: {language_model_defaults.concurrent_requests}\n+    retry_strategy: {language_model_defaults.retry_strategy}\n+    max_retries: {language_model_defaults.max_retries}\n+    tokens_per_minute: null\n+    requests_per_minute: null\n+\n+### Input settings ###\n+\n+input:\n+  storage:\n+    type: {graphrag_config_defaults.input.storage.type.value} # or blob\n+    base_dir: \"{graphrag_config_defaults.input.storage.base_dir}\"\n+  file_type: {graphrag_config_defaults.input.file_type.value} # [csv, text, json]\n+\n+chunks:\n+  size: {graphrag_config_defaults.chunks.size}\n+  overlap: {graphrag_config_defaults.chunks.overlap}\n+\n+### Output/storage settings ###\n+## If blob storage is specified in the following four sections,\n+## connection_string and container_name must be provided\n+\n+output:\n+  type: {graphrag_config_defaults.output.type.value} # [file, blob, cosmosdb]\n+  base_dir: \"{graphrag_config_defaults.output.base_dir}\"\n+    \n+cache:\n+  type: {graphrag_config_defaults.cache.type.value} # [file, blob, cosmosdb]\n+  base_dir: \"{graphrag_config_defaults.cache.base_dir}\"\n+\n+reporting:\n+  type: {graphrag_config_defaults.reporting.type.value} # [file, blob]\n+  base_dir: \"{graphrag_config_defaults.reporting.base_dir}\"\n+\n+vector_store:\n+  type: {vector_store_defaults.type}\n+  db_uri: {vector_store_defaults.db_uri}\n+\n+### Workflow settings ###\n+\n+embed_text:\n+  model_id: {graphrag_config_defaults.embed_text.model_id}\n+\n+extract_graph:\n+  model_id: {graphrag_config_defaults.extract_graph.model_id}\n+  prompt: \"prompts/extract_graph.txt\"\n+  entity_types: [{\",\".join(graphrag_config_defaults.extract_graph.entity_types)}]\n+  max_gleanings: {graphrag_config_defaults.extract_graph.max_gleanings}\n+\n+summarize_descriptions:\n+  model_id: {graphrag_config_defaults.summarize_descriptions.model_id}\n+  prompt: \"prompts/summarize_descriptions.txt\"\n+  max_length: {graphrag_config_defaults.summarize_descriptions.max_length}\n+\n+extract_graph_nlp:\n+  text_analyzer:\n+    extractor_type: {graphrag_config_defaults.extract_graph_nlp.text_analyzer.extractor_type.value} # [regex_english, syntactic_parser, cfg]\n+\n+cluster_graph:\n+  max_cluster_size: {graphrag_config_defaults.cluster_graph.max_cluster_size}\n+\n+extract_claims:\n+  enabled: false\n+  model_id: {graphrag_config_defaults.extract_claims.model_id}\n+  prompt: \"prompts/extract_claims.txt\"\n+  description: \"{graphrag_config_defaults.extract_claims.description}\"\n+  max_gleanings: {graphrag_config_defaults.extract_claims.max_gleanings}\n+\n+community_reports:\n+  model_id: {graphrag_config_defaults.community_reports.model_id}\n+  graph_prompt: \"prompts/community_report_graph.txt\"\n+  text_prompt: \"prompts/community_report_text.txt\"\n+  max_length: {graphrag_config_defaults.community_reports.max_length}\n+  max_input_length: {graphrag_config_defaults.community_reports.max_input_length}\n+\n+snapshots:\n+  graphml: false\n+  embeddings: false\n+\n+### Query settings ###\n+## The prompt locations are required here, but each search method has a number of optional knobs that can be tuned.\n+## See the config docs: https://microsoft.github.io/graphrag/config/yaml/#query\n+\n+local_search:\n+  chat_model_id: {graphrag_config_defaults.local_search.chat_model_id}\n+  embedding_model_id: {graphrag_config_defaults.local_search.embedding_model_id}\n+  prompt: \"prompts/local_search_system_prompt.txt\"\n+\n+global_search:\n+  chat_model_id: {graphrag_config_defaults.global_search.chat_model_id}\n+  map_prompt: \"prompts/global_search_map_system_prompt.txt\"\n+  reduce_prompt: \"prompts/global_search_reduce_system_prompt.txt\"\n+  knowledge_prompt: \"prompts/global_search_knowledge_system_prompt.txt\"\n+\n+drift_search:\n+  chat_model_id: {graphrag_config_defaults.drift_search.chat_model_id}\n+  embedding_model_id: {graphrag_config_defaults.drift_search.embedding_model_id}\n+  prompt: \"prompts/drift_search_system_prompt.txt\"\n+  reduce_prompt: \"prompts/drift_search_reduce_prompt.txt\"\n+\n+basic_search:\n+  chat_model_id: {graphrag_config_defaults.basic_search.chat_model_id}\n+  embedding_model_id: {graphrag_config_defaults.basic_search.embedding_model_id}\n+  prompt: \"prompts/basic_search_system_prompt.txt\"\n+\"\"\"\n+\n+INIT_DOTENV = \"\"\"\\\n+GRAPHRAG_API_KEY=<API_KEY>\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/load_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/load_config.py b/packages/graphrag/graphrag/config/load_config.py\nnew file mode 100644\nindex 0000000..de90260\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/load_config.py\n@@ -0,0 +1,191 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Default method for loading config.\"\"\"\n+\n+import json\n+import os\n+from pathlib import Path\n+from string import Template\n+from typing import Any\n+\n+import yaml\n+from dotenv import load_dotenv\n+\n+from graphrag.config.create_graphrag_config import create_graphrag_config\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+\n+_default_config_files = [\"settings.yaml\", \"settings.yml\", \"settings.json\"]\n+\n+\n+def _search_for_config_in_root_dir(root: str | Path) -> Path | None:\n+    \"\"\"Resolve the config path from the given root directory.\n+\n+    Parameters\n+    ----------\n+    root : str | Path\n+        The path to the root directory containing the config file.\n+        Searches for a default config file (settings.{yaml,yml,json}).\n+\n+    Returns\n+    -------\n+    Path | None\n+        returns a Path if there is a config in the root directory\n+        Otherwise returns None.\n+    \"\"\"\n+    root = Path(root)\n+\n+    if not root.is_dir():\n+        msg = f\"Invalid config path: {root} is not a directory\"\n+        raise FileNotFoundError(msg)\n+\n+    for file in _default_config_files:\n+        if (root / file).is_file():\n+            return root / file\n+\n+    return None\n+\n+\n+def _parse_env_variables(text: str) -> str:\n+    \"\"\"Parse environment variables in the configuration text.\n+\n+    Parameters\n+    ----------\n+    text : str\n+        The configuration text.\n+\n+    Returns\n+    -------\n+    str\n+        The configuration text with environment variables parsed.\n+\n+    Raises\n+    ------\n+    KeyError\n+        If an environment variable is not found.\n+    \"\"\"\n+    return Template(text).substitute(os.environ)\n+\n+\n+def _load_dotenv(config_path: Path | str) -> None:\n+    \"\"\"Load the .env file if it exists in the same directory as the config file.\n+\n+    Parameters\n+    ----------\n+    config_path : Path | str\n+        The path to the config file.\n+    \"\"\"\n+    config_path = Path(config_path)\n+    dotenv_path = config_path.parent / \".env\"\n+    if dotenv_path.exists():\n+        load_dotenv(dotenv_path)\n+\n+\n+def _get_config_path(root_dir: Path, config_filepath: Path | None) -> Path:\n+    \"\"\"Get the configuration file path.\n+\n+    Parameters\n+    ----------\n+    root_dir : str | Path\n+        The root directory of the project. Will search for the config file in this directory.\n+    config_filepath : str | None\n+        The path to the config file.\n+        If None, searches for config file in root.\n+\n+    Returns\n+    -------\n+    Path\n+        The configuration file path.\n+    \"\"\"\n+    if config_filepath:\n+        config_path = config_filepath.resolve()\n+        if not config_path.exists():\n+            msg = f\"Specified Config file not found: {config_path}\"\n+            raise FileNotFoundError(msg)\n+    else:\n+        config_path = _search_for_config_in_root_dir(root_dir)\n+\n+    if not config_path:\n+        msg = f\"Config file not found in root directory: {root_dir}\"\n+        raise FileNotFoundError(msg)\n+\n+    return config_path\n+\n+\n+def _apply_overrides(data: dict[str, Any], overrides: dict[str, Any]) -> None:\n+    \"\"\"Apply the overrides to the raw configuration.\"\"\"\n+    for key, value in overrides.items():\n+        keys = key.split(\".\")\n+        target = data\n+        current_path = keys[0]\n+        for k in keys[:-1]:\n+            current_path += f\".{k}\"\n+            target_obj = target.get(k, {})\n+            if not isinstance(target_obj, dict):\n+                msg = f\"Cannot override non-dict value: data[{current_path}] is not a dict.\"\n+                raise TypeError(msg)\n+            target[k] = target_obj\n+            target = target[k]\n+        target[keys[-1]] = value\n+\n+\n+def _parse(file_extension: str, contents: str) -> dict[str, Any]:\n+    \"\"\"Parse configuration.\"\"\"\n+    match file_extension:\n+        case \".yaml\" | \".yml\":\n+            return yaml.safe_load(contents)\n+        case \".json\":\n+            return json.loads(contents)\n+        case _:\n+            msg = (\n+                f\"Unable to parse config. Unsupported file extension: {file_extension}\"\n+            )\n+            raise ValueError(msg)\n+\n+\n+def load_config(\n+    root_dir: Path,\n+    config_filepath: Path | None = None,\n+    cli_overrides: dict[str, Any] | None = None,\n+) -> GraphRagConfig:\n+    \"\"\"Load configuration from a file.\n+\n+    Parameters\n+    ----------\n+    root_dir : str | Path\n+        The root directory of the project. Will search for the config file in this directory.\n+    config_filepath : str | None\n+        The path to the config file.\n+        If None, searches for config file in root.\n+    cli_overrides : dict[str, Any] | None\n+        A flat dictionary of cli overrides.\n+        Example: {'output.base_dir': 'override_value'}\n+\n+    Returns\n+    -------\n+    GraphRagConfig\n+        The loaded configuration.\n+\n+    Raises\n+    ------\n+    FileNotFoundError\n+        If the config file is not found.\n+    ValueError\n+        If the config file extension is not supported.\n+    TypeError\n+        If applying cli overrides to the config fails.\n+    KeyError\n+        If config file references a non-existent environment variable.\n+    ValidationError\n+        If there are pydantic validation errors when instantiating the config.\n+    \"\"\"\n+    root = root_dir.resolve()\n+    config_path = _get_config_path(root, config_filepath)\n+    _load_dotenv(config_path)\n+    config_extension = config_path.suffix\n+    config_text = config_path.read_text(encoding=\"utf-8\")\n+    config_text = _parse_env_variables(config_text)\n+    config_data = _parse(config_extension, config_text)\n+    if cli_overrides:\n+        _apply_overrides(config_data, cli_overrides)\n+    return create_graphrag_config(config_data, root_dir=str(root))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/__init__.py b/packages/graphrag/graphrag/config/models/__init__.py\nnew file mode 100644\nindex 0000000..6c5862a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Interfaces for Default Config parameterization.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/basic_search_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/basic_search_config.py b/packages/graphrag/graphrag/config/models/basic_search_config.py\nnew file mode 100644\nindex 0000000..66a1e68\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/basic_search_config.py\n@@ -0,0 +1,33 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class BasicSearchConfig(BaseModel):\n+    \"\"\"The default configuration section for Cache.\"\"\"\n+\n+    prompt: str | None = Field(\n+        description=\"The basic search prompt to use.\",\n+        default=graphrag_config_defaults.basic_search.prompt,\n+    )\n+    chat_model_id: str = Field(\n+        description=\"The model ID to use for basic search.\",\n+        default=graphrag_config_defaults.basic_search.chat_model_id,\n+    )\n+    embedding_model_id: str = Field(\n+        description=\"The model ID to use for text embeddings.\",\n+        default=graphrag_config_defaults.basic_search.embedding_model_id,\n+    )\n+    k: int = Field(\n+        description=\"The number of text units to include in search context.\",\n+        default=graphrag_config_defaults.basic_search.k,\n+    )\n+    max_context_tokens: int = Field(\n+        description=\"The maximum tokens.\",\n+        default=graphrag_config_defaults.basic_search.max_context_tokens,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/cache_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/cache_config.py b/packages/graphrag/graphrag/config/models/cache_config.py\nnew file mode 100644\nindex 0000000..c301e9d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/cache_config.py\n@@ -0,0 +1,38 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import CacheType\n+\n+\n+class CacheConfig(BaseModel):\n+    \"\"\"The default configuration section for Cache.\"\"\"\n+\n+    type: CacheType | str = Field(\n+        description=\"The cache type to use.\",\n+        default=graphrag_config_defaults.cache.type,\n+    )\n+    base_dir: str = Field(\n+        description=\"The base directory for the cache.\",\n+        default=graphrag_config_defaults.cache.base_dir,\n+    )\n+    connection_string: str | None = Field(\n+        description=\"The cache connection string to use.\",\n+        default=graphrag_config_defaults.cache.connection_string,\n+    )\n+    container_name: str | None = Field(\n+        description=\"The cache container name to use.\",\n+        default=graphrag_config_defaults.cache.container_name,\n+    )\n+    storage_account_blob_url: str | None = Field(\n+        description=\"The storage account blob url to use.\",\n+        default=graphrag_config_defaults.cache.storage_account_blob_url,\n+    )\n+    cosmosdb_account_url: str | None = Field(\n+        description=\"The cosmosdb account url to use.\",\n+        default=graphrag_config_defaults.cache.cosmosdb_account_url,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/chunking_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/chunking_config.py b/packages/graphrag/graphrag/config/models/chunking_config.py\nnew file mode 100644\nindex 0000000..4d12142\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/chunking_config.py\n@@ -0,0 +1,38 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import ChunkStrategyType\n+\n+\n+class ChunkingConfig(BaseModel):\n+    \"\"\"Configuration section for chunking.\"\"\"\n+\n+    size: int = Field(\n+        description=\"The chunk size to use.\",\n+        default=graphrag_config_defaults.chunks.size,\n+    )\n+    overlap: int = Field(\n+        description=\"The chunk overlap to use.\",\n+        default=graphrag_config_defaults.chunks.overlap,\n+    )\n+    strategy: ChunkStrategyType = Field(\n+        description=\"The chunking strategy to use.\",\n+        default=graphrag_config_defaults.chunks.strategy,\n+    )\n+    encoding_model: str = Field(\n+        description=\"The encoding model to use.\",\n+        default=graphrag_config_defaults.chunks.encoding_model,\n+    )\n+    prepend_metadata: bool = Field(\n+        description=\"Prepend metadata into each chunk.\",\n+        default=graphrag_config_defaults.chunks.prepend_metadata,\n+    )\n+    chunk_size_includes_metadata: bool = Field(\n+        description=\"Count metadata in max tokens.\",\n+        default=graphrag_config_defaults.chunks.chunk_size_includes_metadata,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/cluster_graph_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/cluster_graph_config.py b/packages/graphrag/graphrag/config/models/cluster_graph_config.py\nnew file mode 100644\nindex 0000000..7b4e6a4\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/cluster_graph_config.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class ClusterGraphConfig(BaseModel):\n+    \"\"\"Configuration section for clustering graphs.\"\"\"\n+\n+    max_cluster_size: int = Field(\n+        description=\"The maximum cluster size to use.\",\n+        default=graphrag_config_defaults.cluster_graph.max_cluster_size,\n+    )\n+    use_lcc: bool = Field(\n+        description=\"Whether to use the largest connected component.\",\n+        default=graphrag_config_defaults.cluster_graph.use_lcc,\n+    )\n+    seed: int = Field(\n+        description=\"The seed to use for the clustering.\",\n+        default=graphrag_config_defaults.cluster_graph.seed,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/community_reports_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/community_reports_config.py b/packages/graphrag/graphrag/config/models/community_reports_config.py\nnew file mode 100644\nindex 0000000..1257124\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/community_reports_config.py\n@@ -0,0 +1,65 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.prompts.index.community_report import COMMUNITY_REPORT_PROMPT\n+from graphrag.prompts.index.community_report_text_units import (\n+    COMMUNITY_REPORT_TEXT_PROMPT,\n+)\n+\n+\n+@dataclass\n+class CommunityReportPrompts:\n+    \"\"\"Community report prompt templates.\"\"\"\n+\n+    graph_prompt: str\n+    text_prompt: str\n+\n+\n+class CommunityReportsConfig(BaseModel):\n+    \"\"\"Configuration section for community reports.\"\"\"\n+\n+    model_id: str = Field(\n+        description=\"The model ID to use for community reports.\",\n+        default=graphrag_config_defaults.community_reports.model_id,\n+    )\n+    model_instance_name: str = Field(\n+        description=\"The model singleton instance name. This primarily affects the cache storage partitioning.\",\n+        default=graphrag_config_defaults.community_reports.model_instance_name,\n+    )\n+    graph_prompt: str | None = Field(\n+        description=\"The community report extraction prompt to use for graph-based summarization.\",\n+        default=graphrag_config_defaults.community_reports.graph_prompt,\n+    )\n+    text_prompt: str | None = Field(\n+        description=\"The community report extraction prompt to use for text-based summarization.\",\n+        default=graphrag_config_defaults.community_reports.text_prompt,\n+    )\n+    max_length: int = Field(\n+        description=\"The community report maximum length in tokens.\",\n+        default=graphrag_config_defaults.community_reports.max_length,\n+    )\n+    max_input_length: int = Field(\n+        description=\"The maximum input length in tokens to use when generating reports.\",\n+        default=graphrag_config_defaults.community_reports.max_input_length,\n+    )\n+\n+    def resolved_prompts(self, root_dir: str) -> CommunityReportPrompts:\n+        \"\"\"Get the resolved community report extraction prompts.\"\"\"\n+        return CommunityReportPrompts(\n+            graph_prompt=(Path(root_dir) / self.graph_prompt).read_text(\n+                encoding=\"utf-8\"\n+            )\n+            if self.graph_prompt\n+            else COMMUNITY_REPORT_PROMPT,\n+            text_prompt=(Path(root_dir) / self.text_prompt).read_text(encoding=\"utf-8\")\n+            if self.text_prompt\n+            else COMMUNITY_REPORT_TEXT_PROMPT,\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/drift_search_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/drift_search_config.py b/packages/graphrag/graphrag/config/models/drift_search_config.py\nnew file mode 100644\nindex 0000000..a6edf66\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/drift_search_config.py\n@@ -0,0 +1,123 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class DRIFTSearchConfig(BaseModel):\n+    \"\"\"The default configuration section for Cache.\"\"\"\n+\n+    prompt: str | None = Field(\n+        description=\"The drift search prompt to use.\",\n+        default=graphrag_config_defaults.drift_search.prompt,\n+    )\n+    reduce_prompt: str | None = Field(\n+        description=\"The drift search reduce prompt to use.\",\n+        default=graphrag_config_defaults.drift_search.reduce_prompt,\n+    )\n+    chat_model_id: str = Field(\n+        description=\"The model ID to use for drift search.\",\n+        default=graphrag_config_defaults.drift_search.chat_model_id,\n+    )\n+    embedding_model_id: str = Field(\n+        description=\"The model ID to use for drift search.\",\n+        default=graphrag_config_defaults.drift_search.embedding_model_id,\n+    )\n+    data_max_tokens: int = Field(\n+        description=\"The data llm maximum tokens.\",\n+        default=graphrag_config_defaults.drift_search.data_max_tokens,\n+    )\n+\n+    reduce_max_tokens: int | None = Field(\n+        description=\"The reduce llm maximum tokens response to produce.\",\n+        default=graphrag_config_defaults.drift_search.reduce_max_tokens,\n+    )\n+\n+    reduce_temperature: float = Field(\n+        description=\"The temperature to use for token generation in reduce.\",\n+        default=graphrag_config_defaults.drift_search.reduce_temperature,\n+    )\n+\n+    reduce_max_completion_tokens: int | None = Field(\n+        description=\"The reduce llm maximum tokens response to produce.\",\n+        default=graphrag_config_defaults.drift_search.reduce_max_completion_tokens,\n+    )\n+\n+    concurrency: int = Field(\n+        description=\"The number of concurrent requests.\",\n+        default=graphrag_config_defaults.drift_search.concurrency,\n+    )\n+\n+    drift_k_followups: int = Field(\n+        description=\"The number of top global results to retrieve.\",\n+        default=graphrag_config_defaults.drift_search.drift_k_followups,\n+    )\n+\n+    primer_folds: int = Field(\n+        description=\"The number of folds for search priming.\",\n+        default=graphrag_config_defaults.drift_search.primer_folds,\n+    )\n+\n+    primer_llm_max_tokens: int = Field(\n+        description=\"The maximum number of tokens for the LLM in primer.\",\n+        default=graphrag_config_defaults.drift_search.primer_llm_max_tokens,\n+    )\n+\n+    n_depth: int = Field(\n+        description=\"The number of drift search steps to take.\",\n+        default=graphrag_config_defaults.drift_search.n_depth,\n+    )\n+\n+    local_search_text_unit_prop: float = Field(\n+        description=\"The proportion of search dedicated to text units.\",\n+        default=graphrag_config_defaults.drift_search.local_search_text_unit_prop,\n+    )\n+\n+    local_search_community_prop: float = Field(\n+        description=\"The proportion of search dedicated to community properties.\",\n+        default=graphrag_config_defaults.drift_search.local_search_community_prop,\n+    )\n+\n+    local_search_top_k_mapped_entities: int = Field(\n+        description=\"The number of top K entities to map during local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_top_k_mapped_entities,\n+    )\n+\n+    local_search_top_k_relationships: int = Field(\n+        description=\"The number of top K relationships to map during local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_top_k_relationships,\n+    )\n+\n+    local_search_max_data_tokens: int = Field(\n+        description=\"The maximum context size in tokens for local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_max_data_tokens,\n+    )\n+\n+    local_search_temperature: float = Field(\n+        description=\"The temperature to use for token generation in local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_temperature,\n+    )\n+\n+    local_search_top_p: float = Field(\n+        description=\"The top-p value to use for token generation in local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_top_p,\n+    )\n+\n+    local_search_n: int = Field(\n+        description=\"The number of completions to generate in local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_n,\n+    )\n+\n+    local_search_llm_max_gen_tokens: int | None = Field(\n+        description=\"The maximum number of generated tokens for the LLM in local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_llm_max_gen_tokens,\n+    )\n+\n+    local_search_llm_max_gen_completion_tokens: int | None = Field(\n+        description=\"The maximum number of generated tokens for the LLM in local search.\",\n+        default=graphrag_config_defaults.drift_search.local_search_llm_max_gen_completion_tokens,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/embed_text_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/embed_text_config.py b/packages/graphrag/graphrag/config/models/embed_text_config.py\nnew file mode 100644\nindex 0000000..c33409d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/embed_text_config.py\n@@ -0,0 +1,33 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class EmbedTextConfig(BaseModel):\n+    \"\"\"Configuration section for text embeddings.\"\"\"\n+\n+    model_id: str = Field(\n+        description=\"The model ID to use for text embeddings.\",\n+        default=graphrag_config_defaults.embed_text.model_id,\n+    )\n+    model_instance_name: str = Field(\n+        description=\"The model singleton instance name. This primarily affects the cache storage partitioning.\",\n+        default=graphrag_config_defaults.embed_text.model_instance_name,\n+    )\n+    batch_size: int = Field(\n+        description=\"The batch size to use.\",\n+        default=graphrag_config_defaults.embed_text.batch_size,\n+    )\n+    batch_max_tokens: int = Field(\n+        description=\"The batch max tokens to use.\",\n+        default=graphrag_config_defaults.embed_text.batch_max_tokens,\n+    )\n+    names: list[str] = Field(\n+        description=\"The specific embeddings to perform.\",\n+        default=graphrag_config_defaults.embed_text.names,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/extract_claims_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/extract_claims_config.py b/packages/graphrag/graphrag/config/models/extract_claims_config.py\nnew file mode 100644\nindex 0000000..77a633b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/extract_claims_config.py\n@@ -0,0 +1,56 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.prompts.index.extract_claims import EXTRACT_CLAIMS_PROMPT\n+\n+\n+@dataclass\n+class ClaimExtractionPrompts:\n+    \"\"\"Claim extraction prompt templates.\"\"\"\n+\n+    extraction_prompt: str\n+\n+\n+class ExtractClaimsConfig(BaseModel):\n+    \"\"\"Configuration section for claim extraction.\"\"\"\n+\n+    enabled: bool = Field(\n+        description=\"Whether claim extraction is enabled.\",\n+        default=graphrag_config_defaults.extract_claims.enabled,\n+    )\n+    model_id: str = Field(\n+        description=\"The model ID to use for claim extraction.\",\n+        default=graphrag_config_defaults.extract_claims.model_id,\n+    )\n+    model_instance_name: str = Field(\n+        description=\"The model singleton instance name. This primarily affects the cache storage partitioning.\",\n+        default=graphrag_config_defaults.extract_claims.model_instance_name,\n+    )\n+    prompt: str | None = Field(\n+        description=\"The claim extraction prompt to use.\",\n+        default=graphrag_config_defaults.extract_claims.prompt,\n+    )\n+    description: str = Field(\n+        description=\"The claim description to use.\",\n+        default=graphrag_config_defaults.extract_claims.description,\n+    )\n+    max_gleanings: int = Field(\n+        description=\"The maximum number of entity gleanings to use.\",\n+        default=graphrag_config_defaults.extract_claims.max_gleanings,\n+    )\n+\n+    def resolved_prompts(self, root_dir: str) -> ClaimExtractionPrompts:\n+        \"\"\"Get the resolved claim extraction prompts.\"\"\"\n+        return ClaimExtractionPrompts(\n+            extraction_prompt=(Path(root_dir) / self.prompt).read_text(encoding=\"utf-8\")\n+            if self.prompt\n+            else EXTRACT_CLAIMS_PROMPT,\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/extract_graph_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/extract_graph_config.py b/packages/graphrag/graphrag/config/models/extract_graph_config.py\nnew file mode 100644\nindex 0000000..8a61585\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/extract_graph_config.py\n@@ -0,0 +1,52 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.prompts.index.extract_graph import GRAPH_EXTRACTION_PROMPT\n+\n+\n+@dataclass\n+class ExtractGraphPrompts:\n+    \"\"\"Graph extraction prompt templates.\"\"\"\n+\n+    extraction_prompt: str\n+\n+\n+class ExtractGraphConfig(BaseModel):\n+    \"\"\"Configuration section for entity extraction.\"\"\"\n+\n+    model_id: str = Field(\n+        description=\"The model ID to use for text embeddings.\",\n+        default=graphrag_config_defaults.extract_graph.model_id,\n+    )\n+    model_instance_name: str = Field(\n+        description=\"The model singleton instance name. This primarily affects the cache storage partitioning.\",\n+        default=graphrag_config_defaults.extract_graph.model_instance_name,\n+    )\n+    prompt: str | None = Field(\n+        description=\"The entity extraction prompt to use.\",\n+        default=graphrag_config_defaults.extract_graph.prompt,\n+    )\n+    entity_types: list[str] = Field(\n+        description=\"The entity extraction entity types to use.\",\n+        default=graphrag_config_defaults.extract_graph.entity_types,\n+    )\n+    max_gleanings: int = Field(\n+        description=\"The maximum number of entity gleanings to use.\",\n+        default=graphrag_config_defaults.extract_graph.max_gleanings,\n+    )\n+\n+    def resolved_prompts(self, root_dir: str) -> ExtractGraphPrompts:\n+        \"\"\"Get the resolved graph extraction prompts.\"\"\"\n+        return ExtractGraphPrompts(\n+            extraction_prompt=(Path(root_dir) / self.prompt).read_text(encoding=\"utf-8\")\n+            if self.prompt\n+            else GRAPH_EXTRACTION_PROMPT,\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/extract_graph_nlp_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/extract_graph_nlp_config.py b/packages/graphrag/graphrag/config/models/extract_graph_nlp_config.py\nnew file mode 100644\nindex 0000000..5ab587c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/extract_graph_nlp_config.py\n@@ -0,0 +1,74 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import AsyncType, NounPhraseExtractorType\n+\n+\n+class TextAnalyzerConfig(BaseModel):\n+    \"\"\"Configuration section for NLP text analyzer.\"\"\"\n+\n+    extractor_type: NounPhraseExtractorType = Field(\n+        description=\"The noun phrase extractor type.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.extractor_type,\n+    )\n+    model_name: str = Field(\n+        description=\"The SpaCy model name.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.model_name,\n+    )\n+    max_word_length: int = Field(\n+        description=\"The max word length for NLP parsing.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.max_word_length,\n+    )\n+    word_delimiter: str = Field(\n+        description=\"The delimiter for splitting words.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.word_delimiter,\n+    )\n+    include_named_entities: bool = Field(\n+        description=\"Whether to include named entities in noun phrases.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.include_named_entities,\n+    )\n+    exclude_nouns: list[str] | None = Field(\n+        description=\"The list of excluded nouns (i.e., stopwords). If None, will use a default stopword list\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.exclude_nouns,\n+    )\n+    exclude_entity_tags: list[str] = Field(\n+        description=\"The list of named entity tags to exclude in noun phrases.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.exclude_entity_tags,\n+    )\n+    exclude_pos_tags: list[str] = Field(\n+        description=\"The list of part-of-speech tags to remove in noun phrases.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.exclude_pos_tags,\n+    )\n+    noun_phrase_tags: list[str] = Field(\n+        description=\"The list of noun phrase tags.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.noun_phrase_tags,\n+    )\n+    noun_phrase_grammars: dict[str, str] = Field(\n+        description=\"The CFG for matching noun phrases. The key is a tuple of POS tags and the value is the grammar.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.text_analyzer.noun_phrase_grammars,\n+    )\n+\n+\n+class ExtractGraphNLPConfig(BaseModel):\n+    \"\"\"Configuration section for graph extraction via NLP.\"\"\"\n+\n+    normalize_edge_weights: bool = Field(\n+        description=\"Whether to normalize edge weights.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.normalize_edge_weights,\n+    )\n+    text_analyzer: TextAnalyzerConfig = Field(\n+        description=\"The text analyzer configuration.\", default=TextAnalyzerConfig()\n+    )\n+    concurrent_requests: int = Field(\n+        description=\"The number of threads to use for the extraction process.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.concurrent_requests,\n+    )\n+    async_mode: AsyncType = Field(\n+        description=\"The async mode to use.\",\n+        default=graphrag_config_defaults.extract_graph_nlp.async_mode,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/global_search_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/global_search_config.py b/packages/graphrag/graphrag/config/models/global_search_config.py\nnew file mode 100644\nindex 0000000..c350efc\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/global_search_config.py\n@@ -0,0 +1,67 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class GlobalSearchConfig(BaseModel):\n+    \"\"\"The default configuration section for Cache.\"\"\"\n+\n+    map_prompt: str | None = Field(\n+        description=\"The global search mapper prompt to use.\",\n+        default=graphrag_config_defaults.global_search.map_prompt,\n+    )\n+    reduce_prompt: str | None = Field(\n+        description=\"The global search reducer to use.\",\n+        default=graphrag_config_defaults.global_search.reduce_prompt,\n+    )\n+    chat_model_id: str = Field(\n+        description=\"The model ID to use for global search.\",\n+        default=graphrag_config_defaults.global_search.chat_model_id,\n+    )\n+    knowledge_prompt: str | None = Field(\n+        description=\"The global search general prompt to use.\",\n+        default=graphrag_config_defaults.global_search.knowledge_prompt,\n+    )\n+    max_context_tokens: int = Field(\n+        description=\"The maximum context size in tokens.\",\n+        default=graphrag_config_defaults.global_search.max_context_tokens,\n+    )\n+    data_max_tokens: int = Field(\n+        description=\"The data llm maximum tokens.\",\n+        default=graphrag_config_defaults.global_search.data_max_tokens,\n+    )\n+    map_max_length: int = Field(\n+        description=\"The map llm maximum response length in words.\",\n+        default=graphrag_config_defaults.global_search.map_max_length,\n+    )\n+    reduce_max_length: int = Field(\n+        description=\"The reduce llm maximum response length in words.\",\n+        default=graphrag_config_defaults.global_search.reduce_max_length,\n+    )\n+\n+    # configurations for dynamic community selection\n+    dynamic_search_threshold: int = Field(\n+        description=\"Rating threshold in include a community report\",\n+        default=graphrag_config_defaults.global_search.dynamic_search_threshold,\n+    )\n+    dynamic_search_keep_parent: bool = Field(\n+        description=\"Keep parent community if any of the child communities are relevant\",\n+        default=graphrag_config_defaults.global_search.dynamic_search_keep_parent,\n+    )\n+    dynamic_search_num_repeats: int = Field(\n+        description=\"Number of times to rate the same community report\",\n+        default=graphrag_config_defaults.global_search.dynamic_search_num_repeats,\n+    )\n+    dynamic_search_use_summary: bool = Field(\n+        description=\"Use community summary instead of full_context\",\n+        default=graphrag_config_defaults.global_search.dynamic_search_use_summary,\n+    )\n+    dynamic_search_max_level: int = Field(\n+        description=\"The maximum level of community hierarchy to consider if none of the processed communities are relevant\",\n+        default=graphrag_config_defaults.global_search.dynamic_search_max_level,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/graph_rag_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/graph_rag_config.py b/packages/graphrag/graphrag/config/models/graph_rag_config.py\nnew file mode 100644\nindex 0000000..6a6a98e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/graph_rag_config.py\n@@ -0,0 +1,359 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pathlib import Path\n+\n+from devtools import pformat\n+from pydantic import BaseModel, Field, model_validator\n+\n+import graphrag.config.defaults as defs\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import VectorStoreType\n+from graphrag.config.errors import LanguageModelConfigMissingError\n+from graphrag.config.models.basic_search_config import BasicSearchConfig\n+from graphrag.config.models.cache_config import CacheConfig\n+from graphrag.config.models.chunking_config import ChunkingConfig\n+from graphrag.config.models.cluster_graph_config import ClusterGraphConfig\n+from graphrag.config.models.community_reports_config import CommunityReportsConfig\n+from graphrag.config.models.drift_search_config import DRIFTSearchConfig\n+from graphrag.config.models.embed_text_config import EmbedTextConfig\n+from graphrag.config.models.extract_claims_config import ExtractClaimsConfig\n+from graphrag.config.models.extract_graph_config import ExtractGraphConfig\n+from graphrag.config.models.extract_graph_nlp_config import ExtractGraphNLPConfig\n+from graphrag.config.models.global_search_config import GlobalSearchConfig\n+from graphrag.config.models.input_config import InputConfig\n+from graphrag.config.models.language_model_config import LanguageModelConfig\n+from graphrag.config.models.local_search_config import LocalSearchConfig\n+from graphrag.config.models.prune_graph_config import PruneGraphConfig\n+from graphrag.config.models.reporting_config import ReportingConfig\n+from graphrag.config.models.snapshots_config import SnapshotsConfig\n+from graphrag.config.models.storage_config import StorageConfig\n+from graphrag.config.models.summarize_descriptions_config import (\n+    SummarizeDescriptionsConfig,\n+)\n+from graphrag.config.models.vector_store_config import VectorStoreConfig\n+from graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter_factory import (\n+    RateLimiterFactory,\n+)\n+from graphrag.language_model.providers.litellm.services.retry.retry_factory import (\n+    RetryFactory,\n+)\n+\n+\n+class GraphRagConfig(BaseModel):\n+    \"\"\"Base class for the Default-Configuration parameterization settings.\"\"\"\n+\n+    def __repr__(self) -> str:\n+        \"\"\"Get a string representation.\"\"\"\n+        return pformat(self, highlight=False)\n+\n+    def __str__(self):\n+        \"\"\"Get a string representation.\"\"\"\n+        return self.model_dump_json(indent=4)\n+\n+    root_dir: str = Field(\n+        description=\"The root directory for the configuration.\",\n+        default=graphrag_config_defaults.root_dir,\n+    )\n+\n+    def _validate_root_dir(self) -> None:\n+        \"\"\"Validate the root directory.\"\"\"\n+        if self.root_dir.strip() == \"\":\n+            self.root_dir = str(Path.cwd())\n+\n+        root_dir = Path(self.root_dir).resolve()\n+        if not root_dir.is_dir():\n+            msg = f\"Invalid root directory: {self.root_dir} is not a directory.\"\n+            raise FileNotFoundError(msg)\n+        self.root_dir = str(root_dir)\n+\n+    models: dict[str, LanguageModelConfig] = Field(\n+        description=\"Available language model configurations.\",\n+        default=graphrag_config_defaults.models,\n+    )\n+\n+    def _validate_models(self) -> None:\n+        \"\"\"Validate the models configuration.\n+\n+        Ensure both a default chat model and default embedding model\n+        have been defined. Other models may also be defined but\n+        defaults are required for the time being as places of the\n+        code fallback to default model configs instead\n+        of specifying a specific model.\n+\n+        TODO: Don't fallback to default models elsewhere in the code.\n+        Forcing code to specify a model to use and allowing for any\n+        names for model configurations.\n+        \"\"\"\n+        if defs.DEFAULT_CHAT_MODEL_ID not in self.models:\n+            raise LanguageModelConfigMissingError(defs.DEFAULT_CHAT_MODEL_ID)\n+        if defs.DEFAULT_EMBEDDING_MODEL_ID not in self.models:\n+            raise LanguageModelConfigMissingError(defs.DEFAULT_EMBEDDING_MODEL_ID)\n+\n+    def _validate_retry_services(self) -> None:\n+        \"\"\"Validate the retry services configuration.\"\"\"\n+        retry_factory = RetryFactory()\n+\n+        for model_id, model in self.models.items():\n+            if model.retry_strategy != \"none\":\n+                if model.retry_strategy not in retry_factory:\n+                    msg = f\"Retry strategy '{model.retry_strategy}' for model '{model_id}' is not registered. Available strategies: {', '.join(retry_factory.keys())}\"\n+                    raise ValueError(msg)\n+\n+                _ = retry_factory.create(\n+                    strategy=model.retry_strategy,\n+                    init_args={\n+                        \"max_retries\": model.max_retries,\n+                        \"max_retry_wait\": model.max_retry_wait,\n+                    },\n+                )\n+\n+    def _validate_rate_limiter_services(self) -> None:\n+        \"\"\"Validate the rate limiter services configuration.\"\"\"\n+        rate_limiter_factory = RateLimiterFactory()\n+\n+        for model_id, model in self.models.items():\n+            if model.rate_limit_strategy is not None:\n+                if model.rate_limit_strategy not in rate_limiter_factory:\n+                    msg = f\"Rate Limiter strategy '{model.rate_limit_strategy}' for model '{model_id}' is not registered. Available strategies: {', '.join(rate_limiter_factory.keys())}\"\n+                    raise ValueError(msg)\n+\n+                rpm = (\n+                    model.requests_per_minute\n+                    if type(model.requests_per_minute) is int\n+                    else None\n+                )\n+                tpm = (\n+                    model.tokens_per_minute\n+                    if type(model.tokens_per_minute) is int\n+                    else None\n+                )\n+                if rpm is not None or tpm is not None:\n+                    _ = rate_limiter_factory.create(\n+                        strategy=model.rate_limit_strategy,\n+                        init_args={\"rpm\": rpm, \"tpm\": tpm},\n+                    )\n+\n+    input: InputConfig = Field(\n+        description=\"The input configuration.\", default=InputConfig()\n+    )\n+    \"\"\"The input configuration.\"\"\"\n+\n+    def _validate_input_pattern(self) -> None:\n+        \"\"\"Validate the input file pattern based on the specified type.\"\"\"\n+        if len(self.input.file_pattern) == 0:\n+            if self.input.file_type == defs.InputFileType.text:\n+                self.input.file_pattern = \".*\\\\.txt$\"\n+            else:\n+                self.input.file_pattern = f\".*\\\\.{self.input.file_type.value}$\"\n+\n+    def _validate_input_base_dir(self) -> None:\n+        \"\"\"Validate the input base directory.\"\"\"\n+        if self.input.storage.type == defs.StorageType.file:\n+            if self.input.storage.base_dir.strip() == \"\":\n+                msg = \"input storage base directory is required for file input storage. Please rerun `graphrag init` and set the input storage configuration.\"\n+                raise ValueError(msg)\n+            self.input.storage.base_dir = str(\n+                (Path(self.root_dir) / self.input.storage.base_dir).resolve()\n+            )\n+\n+    chunks: ChunkingConfig = Field(\n+        description=\"The chunking configuration to use.\",\n+        default=ChunkingConfig(),\n+    )\n+    \"\"\"The chunking configuration to use.\"\"\"\n+\n+    output: StorageConfig = Field(\n+        description=\"The output configuration.\",\n+        default=StorageConfig(),\n+    )\n+    \"\"\"The output configuration.\"\"\"\n+\n+    def _validate_output_base_dir(self) -> None:\n+        \"\"\"Validate the output base directory.\"\"\"\n+        if self.output.type == defs.StorageType.file:\n+            if self.output.base_dir.strip() == \"\":\n+                msg = \"output base directory is required for file output. Please rerun `graphrag init` and set the output configuration.\"\n+                raise ValueError(msg)\n+            self.output.base_dir = str(\n+                (Path(self.root_dir) / self.output.base_dir).resolve()\n+            )\n+\n+    update_index_output: StorageConfig = Field(\n+        description=\"The output configuration for the updated index.\",\n+        default=StorageConfig(\n+            base_dir=graphrag_config_defaults.update_index_output.base_dir,\n+        ),\n+    )\n+    \"\"\"The output configuration for the updated index.\"\"\"\n+\n+    def _validate_update_index_output_base_dir(self) -> None:\n+        \"\"\"Validate the update index output base directory.\"\"\"\n+        if self.update_index_output.type == defs.StorageType.file:\n+            if self.update_index_output.base_dir.strip() == \"\":\n+                msg = \"update_index_output base directory is required for file output. Please rerun `graphrag init` and set the update_index_output configuration.\"\n+                raise ValueError(msg)\n+            self.update_index_output.base_dir = str(\n+                (Path(self.root_dir) / self.update_index_output.base_dir).resolve()\n+            )\n+\n+    cache: CacheConfig = Field(\n+        description=\"The cache configuration.\", default=CacheConfig()\n+    )\n+    \"\"\"The cache configuration.\"\"\"\n+\n+    reporting: ReportingConfig = Field(\n+        description=\"The reporting configuration.\", default=ReportingConfig()\n+    )\n+    \"\"\"The reporting configuration.\"\"\"\n+\n+    def _validate_reporting_base_dir(self) -> None:\n+        \"\"\"Validate the reporting base directory.\"\"\"\n+        if self.reporting.type == defs.ReportingType.file:\n+            if self.reporting.base_dir.strip() == \"\":\n+                msg = \"Reporting base directory is required for file reporting. Please rerun `graphrag init` and set the reporting configuration.\"\n+                raise ValueError(msg)\n+            self.reporting.base_dir = str(\n+                (Path(self.root_dir) / self.reporting.base_dir).resolve()\n+            )\n+\n+    vector_store: VectorStoreConfig = Field(\n+        description=\"The vector store configuration.\", default=VectorStoreConfig()\n+    )\n+    \"\"\"The vector store configuration.\"\"\"\n+\n+    workflows: list[str] | None = Field(\n+        description=\"List of workflows to run, in execution order. This always overrides any built-in workflow methods.\",\n+        default=graphrag_config_defaults.workflows,\n+    )\n+    \"\"\"List of workflows to run, in execution order.\"\"\"\n+\n+    embed_text: EmbedTextConfig = Field(\n+        description=\"Text embedding configuration.\",\n+        default=EmbedTextConfig(),\n+    )\n+    \"\"\"Text embedding configuration.\"\"\"\n+\n+    extract_graph: ExtractGraphConfig = Field(\n+        description=\"The entity extraction configuration to use.\",\n+        default=ExtractGraphConfig(),\n+    )\n+    \"\"\"The entity extraction configuration to use.\"\"\"\n+\n+    summarize_descriptions: SummarizeDescriptionsConfig = Field(\n+        description=\"The description summarization configuration to use.\",\n+        default=SummarizeDescriptionsConfig(),\n+    )\n+    \"\"\"The description summarization configuration to use.\"\"\"\n+\n+    extract_graph_nlp: ExtractGraphNLPConfig = Field(\n+        description=\"The NLP-based graph extraction configuration to use.\",\n+        default=ExtractGraphNLPConfig(),\n+    )\n+    \"\"\"The NLP-based graph extraction configuration to use.\"\"\"\n+\n+    prune_graph: PruneGraphConfig = Field(\n+        description=\"The graph pruning configuration to use.\",\n+        default=PruneGraphConfig(),\n+    )\n+    \"\"\"The graph pruning configuration to use.\"\"\"\n+\n+    cluster_graph: ClusterGraphConfig = Field(\n+        description=\"The cluster graph configuration to use.\",\n+        default=ClusterGraphConfig(),\n+    )\n+    \"\"\"The cluster graph configuration to use.\"\"\"\n+\n+    extract_claims: ExtractClaimsConfig = Field(\n+        description=\"The claim extraction configuration to use.\",\n+        default=ExtractClaimsConfig(\n+            enabled=graphrag_config_defaults.extract_claims.enabled,\n+        ),\n+    )\n+    \"\"\"The claim extraction configuration to use.\"\"\"\n+\n+    community_reports: CommunityReportsConfig = Field(\n+        description=\"The community reports configuration to use.\",\n+        default=CommunityReportsConfig(),\n+    )\n+    \"\"\"The community reports configuration to use.\"\"\"\n+\n+    snapshots: SnapshotsConfig = Field(\n+        description=\"The snapshots configuration to use.\",\n+        default=SnapshotsConfig(),\n+    )\n+    \"\"\"The snapshots configuration to use.\"\"\"\n+\n+    local_search: LocalSearchConfig = Field(\n+        description=\"The local search configuration.\", default=LocalSearchConfig()\n+    )\n+    \"\"\"The local search configuration.\"\"\"\n+\n+    global_search: GlobalSearchConfig = Field(\n+        description=\"The global search configuration.\", default=GlobalSearchConfig()\n+    )\n+    \"\"\"The global search configuration.\"\"\"\n+\n+    drift_search: DRIFTSearchConfig = Field(\n+        description=\"The drift search configuration.\", default=DRIFTSearchConfig()\n+    )\n+    \"\"\"The drift search configuration.\"\"\"\n+\n+    basic_search: BasicSearchConfig = Field(\n+        description=\"The basic search configuration.\", default=BasicSearchConfig()\n+    )\n+    \"\"\"The basic search configuration.\"\"\"\n+\n+    def _validate_vector_store_db_uri(self) -> None:\n+        \"\"\"Validate the vector store configuration.\"\"\"\n+        store = self.vector_store\n+        if store.type == VectorStoreType.LanceDB:\n+            if not store.db_uri or store.db_uri.strip == \"\":\n+                msg = \"Vector store URI is required for LanceDB. Please rerun `graphrag init` and set the vector store configuration.\"\n+                raise ValueError(msg)\n+            store.db_uri = str((Path(self.root_dir) / store.db_uri).resolve())\n+\n+    def _validate_factories(self) -> None:\n+        \"\"\"Validate the factories used in the configuration.\"\"\"\n+        self._validate_retry_services()\n+        self._validate_rate_limiter_services()\n+\n+    def get_language_model_config(self, model_id: str) -> LanguageModelConfig:\n+        \"\"\"Get a model configuration by ID.\n+\n+        Parameters\n+        ----------\n+        model_id : str\n+            The ID of the model to get. Should match an ID in the models list.\n+\n+        Returns\n+        -------\n+        LanguageModelConfig\n+            The model configuration if found.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the model ID is not found in the configuration.\n+        \"\"\"\n+        if model_id not in self.models:\n+            err_msg = f\"Model ID {model_id} not found in configuration. Please rerun `graphrag init` and set the model configuration.\"\n+            raise ValueError(err_msg)\n+\n+        return self.models[model_id]\n+\n+    @model_validator(mode=\"after\")\n+    def _validate_model(self):\n+        \"\"\"Validate the model configuration.\"\"\"\n+        self._validate_root_dir()\n+        self._validate_models()\n+        self._validate_input_pattern()\n+        self._validate_input_base_dir()\n+        self._validate_reporting_base_dir()\n+        self._validate_output_base_dir()\n+        self._validate_update_index_output_base_dir()\n+        self._validate_vector_store_db_uri()\n+        self._validate_factories()\n+        return self\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/input_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/input_config.py b/packages/graphrag/graphrag/config/models/input_config.py\nnew file mode 100644\nindex 0000000..bc34d94\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/input_config.py\n@@ -0,0 +1,46 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+import graphrag.config.defaults as defs\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import InputFileType\n+from graphrag.config.models.storage_config import StorageConfig\n+\n+\n+class InputConfig(BaseModel):\n+    \"\"\"The default configuration section for Input.\"\"\"\n+\n+    storage: StorageConfig = Field(\n+        description=\"The storage configuration to use for reading input documents.\",\n+        default=StorageConfig(\n+            base_dir=graphrag_config_defaults.input.storage.base_dir,\n+        ),\n+    )\n+    file_type: InputFileType = Field(\n+        description=\"The input file type to use.\",\n+        default=graphrag_config_defaults.input.file_type,\n+    )\n+    encoding: str = Field(\n+        description=\"The input file encoding to use.\",\n+        default=defs.graphrag_config_defaults.input.encoding,\n+    )\n+    file_pattern: str = Field(\n+        description=\"The input file pattern to use.\",\n+        default=graphrag_config_defaults.input.file_pattern,\n+    )\n+    text_column: str = Field(\n+        description=\"The input text column to use.\",\n+        default=graphrag_config_defaults.input.text_column,\n+    )\n+    title_column: str | None = Field(\n+        description=\"The input title column to use.\",\n+        default=graphrag_config_defaults.input.title_column,\n+    )\n+    metadata: list[str] | None = Field(\n+        description=\"The document attribute columns to use.\",\n+        default=graphrag_config_defaults.input.metadata,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/language_model_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/language_model_config.py b/packages/graphrag/graphrag/config/models/language_model_config.py\nnew file mode 100644\nindex 0000000..11c46d7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/language_model_config.py\n@@ -0,0 +1,345 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Language model configuration.\"\"\"\n+\n+import logging\n+\n+from pydantic import BaseModel, Field, model_validator\n+\n+from graphrag.config.defaults import language_model_defaults\n+from graphrag.config.enums import AsyncType, AuthType, ModelType\n+from graphrag.config.errors import (\n+    ApiKeyMissingError,\n+    AzureApiBaseMissingError,\n+    AzureApiVersionMissingError,\n+    ConflictingSettingsError,\n+)\n+from graphrag.language_model.factory import ChatModelFactory, EmbeddingModelFactory\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LanguageModelConfig(BaseModel):\n+    \"\"\"Language model configuration.\"\"\"\n+\n+    api_key: str | None = Field(\n+        description=\"The API key to use for the LLM service.\",\n+        default=language_model_defaults.api_key,\n+    )\n+\n+    def _validate_api_key(self) -> None:\n+        \"\"\"Validate the API key.\n+\n+        API Key is required when using OpenAI API\n+        or when using Azure API with API Key authentication.\n+        For the time being, this check is extra verbose for clarity.\n+        It will also raise an exception if an API Key is provided\n+        when one is not expected such as the case of using Azure\n+        Managed Identity.\n+\n+        Raises\n+        ------\n+        ApiKeyMissingError\n+            If the API key is missing and is required.\n+        \"\"\"\n+        if self.auth_type == AuthType.APIKey and (\n+            self.api_key is None or self.api_key.strip() == \"\"\n+        ):\n+            raise ApiKeyMissingError(\n+                self.type,\n+                self.auth_type.value,\n+            )\n+\n+        if (self.auth_type == AuthType.AzureManagedIdentity) and (\n+            self.api_key is not None and self.api_key.strip() != \"\"\n+        ):\n+            msg = \"API Key should not be provided when using Azure Managed Identity. Please rerun `graphrag init` and remove the api_key when using Azure Managed Identity.\"\n+            raise ConflictingSettingsError(msg)\n+\n+    auth_type: AuthType = Field(\n+        description=\"The authentication type.\",\n+        default=language_model_defaults.auth_type,\n+    )\n+\n+    def _validate_auth_type(self) -> None:\n+        \"\"\"Validate the authentication type.\n+\n+        auth_type must be api_key when using OpenAI and\n+        can be either api_key or azure_managed_identity when using AOI.\n+\n+        Raises\n+        ------\n+        ConflictingSettingsError\n+            If the Azure authentication type conflicts with the model being used.\n+        \"\"\"\n+        if (\n+            self.auth_type == AuthType.AzureManagedIdentity\n+            and self.model_provider != \"azure\"\n+        ):\n+            msg = f\"auth_type of azure_managed_identity is not supported for model type {self.type}. Please rerun `graphrag init` and set the auth_type to api_key.\"\n+            raise ConflictingSettingsError(msg)\n+\n+    type: ModelType | str = Field(description=\"The type of LLM model to use.\")\n+\n+    def _validate_type(self) -> None:\n+        \"\"\"Validate the model type.\n+\n+        Raises\n+        ------\n+        KeyError\n+            If the model name is not recognized.\n+        \"\"\"\n+        # Type should be contained by the registered models\n+        if (\n+            self.type not in ChatModelFactory()\n+            and self.type not in EmbeddingModelFactory()\n+        ):\n+            msg = f\"Model type {self.type} is not recognized, must be one of {ChatModelFactory().keys() + EmbeddingModelFactory().keys()}.\"\n+            raise KeyError(msg)\n+\n+    model_provider: str | None = Field(\n+        description=\"The model provider to use.\",\n+        default=language_model_defaults.model_provider,\n+    )\n+\n+    def _validate_model_provider(self) -> None:\n+        \"\"\"Validate the model provider.\n+\n+        Required when using Litellm.\n+\n+        Raises\n+        ------\n+        KeyError\n+            If the model provider is not recognized.\n+        \"\"\"\n+        if (self.type == ModelType.Chat or self.type == ModelType.Embedding) and (\n+            self.model_provider is None or self.model_provider.strip() == \"\"\n+        ):\n+            msg = f\"Model provider must be specified when using type == {self.type}.\"\n+            raise KeyError(msg)\n+\n+    model: str = Field(description=\"The LLM model to use.\")\n+    encoding_model: str = Field(\n+        description=\"The encoding model to use\",\n+        default=language_model_defaults.encoding_model,\n+    )\n+\n+    api_base: str | None = Field(\n+        description=\"The base URL for the LLM API.\",\n+        default=language_model_defaults.api_base,\n+    )\n+\n+    def _validate_api_base(self) -> None:\n+        \"\"\"Validate the API base.\n+\n+        Required when using AOI.\n+\n+        Raises\n+        ------\n+        AzureApiBaseMissingError\n+            If the API base is missing and is required.\n+        \"\"\"\n+        if (self.model_provider == \"azure\") and (\n+            self.api_base is None or self.api_base.strip() == \"\"\n+        ):\n+            raise AzureApiBaseMissingError(self.type)\n+\n+    api_version: str | None = Field(\n+        description=\"The version of the LLM API to use.\",\n+        default=language_model_defaults.api_version,\n+    )\n+\n+    def _validate_api_version(self) -> None:\n+        \"\"\"Validate the API version.\n+\n+        Required when using AOI.\n+\n+        Raises\n+        ------\n+        AzureApiBaseMissingError\n+            If the API base is missing and is required.\n+        \"\"\"\n+        if (self.model_provider == \"azure\") and (\n+            self.api_version is None or self.api_version.strip() == \"\"\n+        ):\n+            raise AzureApiVersionMissingError(self.type)\n+\n+    deployment_name: str | None = Field(\n+        description=\"The deployment name to use for the LLM service.\",\n+        default=language_model_defaults.deployment_name,\n+    )\n+\n+    def _validate_deployment_name(self) -> None:\n+        \"\"\"Validate the deployment name.\n+\n+        Required when using AOI.\n+\n+        Raises\n+        ------\n+        AzureDeploymentNameMissingError\n+            If the deployment name is missing and is required.\n+        \"\"\"\n+        if (self.model_provider == \"azure\") and (\n+            self.deployment_name is None or self.deployment_name.strip() == \"\"\n+        ):\n+            msg = f\"deployment_name is not set for Azure-hosted model. This will default to your model name ({self.model}). If different, this should be set.\"\n+            logger.debug(msg)\n+\n+    organization: str | None = Field(\n+        description=\"The organization to use for the LLM service.\",\n+        default=language_model_defaults.organization,\n+    )\n+    proxy: str | None = Field(\n+        description=\"The proxy to use for the LLM service.\",\n+        default=language_model_defaults.proxy,\n+    )\n+    audience: str | None = Field(\n+        description=\"Azure resource URI to use with managed identity for the llm connection.\",\n+        default=language_model_defaults.audience,\n+    )\n+    model_supports_json: bool | None = Field(\n+        description=\"Whether the model supports JSON output mode.\",\n+        default=language_model_defaults.model_supports_json,\n+    )\n+    request_timeout: float = Field(\n+        description=\"The request timeout to use.\",\n+        default=language_model_defaults.request_timeout,\n+    )\n+    tokens_per_minute: int | None = Field(\n+        description=\"The number of tokens per minute to use for the LLM service.\",\n+        default=language_model_defaults.tokens_per_minute,\n+    )\n+\n+    def _validate_tokens_per_minute(self) -> None:\n+        \"\"\"Validate the tokens per minute.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the tokens per minute is less than 0.\n+        \"\"\"\n+        # If the value is a number, check if it is less than 1\n+        if isinstance(self.tokens_per_minute, int) and self.tokens_per_minute < 1:\n+            msg = f\"Tokens per minute must be a non zero positive number or null. Suggested value: {language_model_defaults.tokens_per_minute}.\"\n+            raise ValueError(msg)\n+\n+    requests_per_minute: int | None = Field(\n+        description=\"The number of requests per minute to use for the LLM service.\",\n+        default=language_model_defaults.requests_per_minute,\n+    )\n+\n+    def _validate_requests_per_minute(self) -> None:\n+        \"\"\"Validate the requests per minute.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the requests per minute is less than 0.\n+        \"\"\"\n+        # If the value is a number, check if it is less than 1\n+        if isinstance(self.requests_per_minute, int) and self.requests_per_minute < 1:\n+            msg = f\"Requests per minute must be a non zero positive number or null. Suggested value: {language_model_defaults.requests_per_minute}.\"\n+            raise ValueError(msg)\n+\n+    rate_limit_strategy: str | None = Field(\n+        description=\"The rate limit strategy to use for the LLM service.\",\n+        default=language_model_defaults.rate_limit_strategy,\n+    )\n+\n+    retry_strategy: str = Field(\n+        description=\"The retry strategy to use for the LLM service.\",\n+        default=language_model_defaults.retry_strategy,\n+    )\n+    max_retries: int = Field(\n+        description=\"The maximum number of retries to use for the LLM service.\",\n+        default=language_model_defaults.max_retries,\n+    )\n+\n+    def _validate_max_retries(self) -> None:\n+        \"\"\"Validate the maximum retries.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the maximum retries is less than 0.\n+        \"\"\"\n+        if self.max_retries < 1:\n+            msg = f\"Maximum retries must be greater than or equal to 1. Suggested value: {language_model_defaults.max_retries}.\"\n+            raise ValueError(msg)\n+\n+    max_retry_wait: float = Field(\n+        description=\"The maximum retry wait to use for the LLM service.\",\n+        default=language_model_defaults.max_retry_wait,\n+    )\n+    concurrent_requests: int = Field(\n+        description=\"Whether to use concurrent requests for the LLM service.\",\n+        default=language_model_defaults.concurrent_requests,\n+    )\n+    async_mode: AsyncType = Field(\n+        description=\"The async mode to use.\", default=language_model_defaults.async_mode\n+    )\n+    responses: list[str | BaseModel] | None = Field(\n+        default=language_model_defaults.responses,\n+        description=\"Static responses to use in mock mode.\",\n+    )\n+    max_tokens: int | None = Field(\n+        description=\"The maximum number of tokens to generate.\",\n+        default=language_model_defaults.max_tokens,\n+    )\n+    temperature: float = Field(\n+        description=\"The temperature to use for token generation.\",\n+        default=language_model_defaults.temperature,\n+    )\n+    max_completion_tokens: int | None = Field(\n+        description=\"The maximum number of tokens to consume. This includes reasoning tokens for the o* reasoning models.\",\n+        default=language_model_defaults.max_completion_tokens,\n+    )\n+    reasoning_effort: str | None = Field(\n+        description=\"Level of effort OpenAI reasoning models should expend. Supported options are 'low', 'medium', 'high'; and OAI defaults to 'medium'.\",\n+        default=language_model_defaults.reasoning_effort,\n+    )\n+    top_p: float = Field(\n+        description=\"The top-p value to use for token generation.\",\n+        default=language_model_defaults.top_p,\n+    )\n+    n: int = Field(\n+        description=\"The number of completions to generate.\",\n+        default=language_model_defaults.n,\n+    )\n+    frequency_penalty: float = Field(\n+        description=\"The frequency penalty to use for token generation.\",\n+        default=language_model_defaults.frequency_penalty,\n+    )\n+    presence_penalty: float = Field(\n+        description=\"The presence penalty to use for token generation.\",\n+        default=language_model_defaults.presence_penalty,\n+    )\n+\n+    def _validate_azure_settings(self) -> None:\n+        \"\"\"Validate the Azure settings.\n+\n+        Raises\n+        ------\n+        AzureApiBaseMissingError\n+            If the API base is missing and is required.\n+        AzureApiVersionMissingError\n+            If the API version is missing and is required.\n+        AzureDeploymentNameMissingError\n+            If the deployment name is missing and is required.\n+        \"\"\"\n+        self._validate_api_base()\n+        self._validate_api_version()\n+        self._validate_deployment_name()\n+\n+    @model_validator(mode=\"after\")\n+    def _validate_model(self):\n+        self._validate_type()\n+        self._validate_model_provider()\n+        self._validate_auth_type()\n+        self._validate_api_key()\n+        self._validate_tokens_per_minute()\n+        self._validate_requests_per_minute()\n+        self._validate_max_retries()\n+        self._validate_azure_settings()\n+        return self\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/local_search_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/local_search_config.py b/packages/graphrag/graphrag/config/models/local_search_config.py\nnew file mode 100644\nindex 0000000..4cf31ff\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/local_search_config.py\n@@ -0,0 +1,49 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class LocalSearchConfig(BaseModel):\n+    \"\"\"The default configuration section for Cache.\"\"\"\n+\n+    prompt: str | None = Field(\n+        description=\"The local search prompt to use.\",\n+        default=graphrag_config_defaults.local_search.prompt,\n+    )\n+    chat_model_id: str = Field(\n+        description=\"The model ID to use for local search.\",\n+        default=graphrag_config_defaults.local_search.chat_model_id,\n+    )\n+    embedding_model_id: str = Field(\n+        description=\"The model ID to use for text embeddings.\",\n+        default=graphrag_config_defaults.local_search.embedding_model_id,\n+    )\n+    text_unit_prop: float = Field(\n+        description=\"The text unit proportion.\",\n+        default=graphrag_config_defaults.local_search.text_unit_prop,\n+    )\n+    community_prop: float = Field(\n+        description=\"The community proportion.\",\n+        default=graphrag_config_defaults.local_search.community_prop,\n+    )\n+    conversation_history_max_turns: int = Field(\n+        description=\"The conversation history maximum turns.\",\n+        default=graphrag_config_defaults.local_search.conversation_history_max_turns,\n+    )\n+    top_k_entities: int = Field(\n+        description=\"The top k mapped entities.\",\n+        default=graphrag_config_defaults.local_search.top_k_entities,\n+    )\n+    top_k_relationships: int = Field(\n+        description=\"The top k mapped relations.\",\n+        default=graphrag_config_defaults.local_search.top_k_relationships,\n+    )\n+    max_context_tokens: int = Field(\n+        description=\"The maximum tokens.\",\n+        default=graphrag_config_defaults.local_search.max_context_tokens,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/prune_graph_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/prune_graph_config.py b/packages/graphrag/graphrag/config/models/prune_graph_config.py\nnew file mode 100644\nindex 0000000..29d6be9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/prune_graph_config.py\n@@ -0,0 +1,41 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class PruneGraphConfig(BaseModel):\n+    \"\"\"Configuration section for pruning graphs.\"\"\"\n+\n+    min_node_freq: int = Field(\n+        description=\"The minimum node frequency to allow.\",\n+        default=graphrag_config_defaults.prune_graph.min_node_freq,\n+    )\n+    max_node_freq_std: float | None = Field(\n+        description=\"The maximum standard deviation of node frequency to allow.\",\n+        default=graphrag_config_defaults.prune_graph.max_node_freq_std,\n+    )\n+    min_node_degree: int = Field(\n+        description=\"The minimum node degree to allow.\",\n+        default=graphrag_config_defaults.prune_graph.min_node_degree,\n+    )\n+    max_node_degree_std: float | None = Field(\n+        description=\"The maximum standard deviation of node degree to allow.\",\n+        default=graphrag_config_defaults.prune_graph.max_node_degree_std,\n+    )\n+    min_edge_weight_pct: float = Field(\n+        description=\"The minimum edge weight percentile to allow. Use e.g, `40` for 40%.\",\n+        default=graphrag_config_defaults.prune_graph.min_edge_weight_pct,\n+    )\n+    remove_ego_nodes: bool = Field(\n+        description=\"Remove ego nodes.\",\n+        default=graphrag_config_defaults.prune_graph.remove_ego_nodes,\n+    )\n+    lcc_only: bool = Field(\n+        description=\"Only use largest connected component.\",\n+        default=graphrag_config_defaults.prune_graph.lcc_only,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/reporting_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/reporting_config.py b/packages/graphrag/graphrag/config/models/reporting_config.py\nnew file mode 100644\nindex 0000000..0e33736\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/reporting_config.py\n@@ -0,0 +1,34 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import ReportingType\n+\n+\n+class ReportingConfig(BaseModel):\n+    \"\"\"The default configuration section for Reporting.\"\"\"\n+\n+    type: ReportingType | str = Field(\n+        description=\"The reporting type to use.\",\n+        default=graphrag_config_defaults.reporting.type,\n+    )\n+    base_dir: str = Field(\n+        description=\"The base directory for reporting.\",\n+        default=graphrag_config_defaults.reporting.base_dir,\n+    )\n+    connection_string: str | None = Field(\n+        description=\"The reporting connection string to use.\",\n+        default=graphrag_config_defaults.reporting.connection_string,\n+    )\n+    container_name: str | None = Field(\n+        description=\"The reporting container name to use.\",\n+        default=graphrag_config_defaults.reporting.container_name,\n+    )\n+    storage_account_blob_url: str | None = Field(\n+        description=\"The storage account blob url to use.\",\n+        default=graphrag_config_defaults.reporting.storage_account_blob_url,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/snapshots_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/snapshots_config.py b/packages/graphrag/graphrag/config/models/snapshots_config.py\nnew file mode 100644\nindex 0000000..5c0109e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/snapshots_config.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+\n+\n+class SnapshotsConfig(BaseModel):\n+    \"\"\"Configuration section for snapshots.\"\"\"\n+\n+    embeddings: bool = Field(\n+        description=\"A flag indicating whether to take snapshots of embeddings.\",\n+        default=graphrag_config_defaults.snapshots.embeddings,\n+    )\n+    graphml: bool = Field(\n+        description=\"A flag indicating whether to take snapshots of GraphML.\",\n+        default=graphrag_config_defaults.snapshots.graphml,\n+    )\n+    raw_graph: bool = Field(\n+        description=\"A flag indicating whether to take snapshots of the raw extracted graph (entities and relationships) before merging.\",\n+        default=graphrag_config_defaults.snapshots.raw_graph,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/storage_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/storage_config.py b/packages/graphrag/graphrag/config/models/storage_config.py\nnew file mode 100644\nindex 0000000..3f01448\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/storage_config.py\n@@ -0,0 +1,52 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pathlib import Path\n+\n+from pydantic import BaseModel, Field, field_validator\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.config.enums import StorageType\n+\n+\n+class StorageConfig(BaseModel):\n+    \"\"\"The default configuration section for storage.\"\"\"\n+\n+    type: StorageType | str = Field(\n+        description=\"The storage type to use.\",\n+        default=graphrag_config_defaults.storage.type,\n+    )\n+    base_dir: str = Field(\n+        description=\"The base directory for the output.\",\n+        default=graphrag_config_defaults.storage.base_dir,\n+    )\n+\n+    # Validate the base dir for multiple OS (use Path)\n+    # if not using a cloud storage type.\n+    @field_validator(\"base_dir\", mode=\"before\")\n+    @classmethod\n+    def validate_base_dir(cls, value, info):\n+        \"\"\"Ensure that base_dir is a valid filesystem path when using local storage.\"\"\"\n+        # info.data contains other field values, including 'type'\n+        if info.data.get(\"type\") != StorageType.file:\n+            return value\n+        return str(Path(value))\n+\n+    connection_string: str | None = Field(\n+        description=\"The storage connection string to use.\",\n+        default=graphrag_config_defaults.storage.connection_string,\n+    )\n+    container_name: str | None = Field(\n+        description=\"The storage container name to use.\",\n+        default=graphrag_config_defaults.storage.container_name,\n+    )\n+    storage_account_blob_url: str | None = Field(\n+        description=\"The storage account blob url to use.\",\n+        default=graphrag_config_defaults.storage.storage_account_blob_url,\n+    )\n+    cosmosdb_account_url: str | None = Field(\n+        description=\"The cosmosdb account url to use.\",\n+        default=graphrag_config_defaults.storage.cosmosdb_account_url,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/summarize_descriptions_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/summarize_descriptions_config.py b/packages/graphrag/graphrag/config/models/summarize_descriptions_config.py\nnew file mode 100644\nindex 0000000..3414db7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/summarize_descriptions_config.py\n@@ -0,0 +1,52 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.prompts.index.summarize_descriptions import SUMMARIZE_PROMPT\n+\n+\n+@dataclass\n+class SummarizeDescriptionsPrompts:\n+    \"\"\"Description summarization prompt templates.\"\"\"\n+\n+    summarize_prompt: str\n+\n+\n+class SummarizeDescriptionsConfig(BaseModel):\n+    \"\"\"Configuration section for description summarization.\"\"\"\n+\n+    model_id: str = Field(\n+        description=\"The model ID to use for summarization.\",\n+        default=graphrag_config_defaults.summarize_descriptions.model_id,\n+    )\n+    model_instance_name: str = Field(\n+        description=\"The model singleton instance name. This primarily affects the cache storage partitioning.\",\n+        default=graphrag_config_defaults.summarize_descriptions.model_instance_name,\n+    )\n+    prompt: str | None = Field(\n+        description=\"The description summarization prompt to use.\",\n+        default=graphrag_config_defaults.summarize_descriptions.prompt,\n+    )\n+    max_length: int = Field(\n+        description=\"The description summarization maximum length.\",\n+        default=graphrag_config_defaults.summarize_descriptions.max_length,\n+    )\n+    max_input_tokens: int = Field(\n+        description=\"Maximum tokens to submit from the input entity descriptions.\",\n+        default=graphrag_config_defaults.summarize_descriptions.max_input_tokens,\n+    )\n+\n+    def resolved_prompts(self, root_dir: str) -> SummarizeDescriptionsPrompts:\n+        \"\"\"Get the resolved description summarization prompts.\"\"\"\n+        return SummarizeDescriptionsPrompts(\n+            summarize_prompt=(Path(root_dir) / self.prompt).read_text(encoding=\"utf-8\")\n+            if self.prompt\n+            else SUMMARIZE_PROMPT,\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/vector_store_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/vector_store_config.py b/packages/graphrag/graphrag/config/models/vector_store_config.py\nnew file mode 100644\nindex 0000000..c2b3e61\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/vector_store_config.py\n@@ -0,0 +1,106 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+from pydantic import BaseModel, Field, model_validator\n+\n+from graphrag.config.defaults import vector_store_defaults\n+from graphrag.config.embeddings import all_embeddings\n+from graphrag.config.enums import VectorStoreType\n+from graphrag.config.models.vector_store_schema_config import VectorStoreSchemaConfig\n+\n+\n+class VectorStoreConfig(BaseModel):\n+    \"\"\"The default configuration section for Vector Store.\"\"\"\n+\n+    type: str = Field(\n+        description=\"The vector store type to use.\",\n+        default=vector_store_defaults.type,\n+    )\n+\n+    db_uri: str | None = Field(\n+        description=\"The database URI to use.\",\n+        default=None,\n+    )\n+\n+    def _validate_db_uri(self) -> None:\n+        \"\"\"Validate the database URI.\"\"\"\n+        if self.type == VectorStoreType.LanceDB.value and (\n+            self.db_uri is None or self.db_uri.strip() == \"\"\n+        ):\n+            self.db_uri = vector_store_defaults.db_uri\n+\n+        if self.type != VectorStoreType.LanceDB.value and (\n+            self.db_uri is not None and self.db_uri.strip() != \"\"\n+        ):\n+            msg = \"vector_store.db_uri is only used when vector_store.type == lancedb. Please rerun `graphrag init` and select the correct vector store type.\"\n+            raise ValueError(msg)\n+\n+    url: str | None = Field(\n+        description=\"The database URL when type == azure_ai_search.\",\n+        default=vector_store_defaults.url,\n+    )\n+\n+    def _validate_url(self) -> None:\n+        \"\"\"Validate the database URL.\"\"\"\n+        if self.type == VectorStoreType.AzureAISearch and (\n+            self.url is None or self.url.strip() == \"\"\n+        ):\n+            msg = \"vector_store.url is required when vector_store.type == azure_ai_search. Please rerun `graphrag init` and select the correct vector store type.\"\n+            raise ValueError(msg)\n+\n+        if self.type == VectorStoreType.CosmosDB and (\n+            self.url is None or self.url.strip() == \"\"\n+        ):\n+            msg = \"vector_store.url is required when vector_store.type == cosmos_db. Please rerun `graphrag init` and select the correct vector store type.\"\n+            raise ValueError(msg)\n+\n+        if self.type == VectorStoreType.LanceDB and (\n+            self.url is not None and self.url.strip() != \"\"\n+        ):\n+            msg = \"vector_store.url is only used when vector_store.type == azure_ai_search or vector_store.type == cosmos_db. Please rerun `graphrag init` and select the correct vector store type.\"\n+            raise ValueError(msg)\n+\n+    api_key: str | None = Field(\n+        description=\"The database API key when type == azure_ai_search.\",\n+        default=vector_store_defaults.api_key,\n+    )\n+\n+    audience: str | None = Field(\n+        description=\"The database audience when type == azure_ai_search.\",\n+        default=vector_store_defaults.audience,\n+    )\n+\n+    index_prefix: str | None = Field(\n+        description=\"The index prefix to use.\",\n+        default=vector_store_defaults.index_prefix,\n+    )\n+\n+    database_name: str | None = Field(\n+        description=\"The database name to use when type == cosmos_db.\",\n+        default=vector_store_defaults.database_name,\n+    )\n+\n+    embeddings_schema: dict[str, VectorStoreSchemaConfig] = {}\n+\n+    def _validate_embeddings_schema(self) -> None:\n+        \"\"\"Validate the embeddings schema.\"\"\"\n+        for name in self.embeddings_schema:\n+            if name not in all_embeddings:\n+                msg = f\"vector_store.embeddings_schema contains an invalid embedding schema name: {name}. Please update your settings.yaml and select the correct embedding schema names.\"\n+                raise ValueError(msg)\n+\n+        if self.type == VectorStoreType.CosmosDB:\n+            for id_field in self.embeddings_schema:\n+                if id_field != \"id\":\n+                    msg = \"When using CosmosDB, the id_field in embeddings_schema must be 'id'. Please update your settings.yaml and set the id_field to 'id'.\"\n+                    raise ValueError(msg)\n+\n+    @model_validator(mode=\"after\")\n+    def _validate_model(self):\n+        \"\"\"Validate the model.\"\"\"\n+        self._validate_db_uri()\n+        self._validate_url()\n+        self._validate_embeddings_schema()\n+        return self\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/models/vector_store_schema_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/models/vector_store_schema_config.py b/packages/graphrag/graphrag/config/models/vector_store_schema_config.py\nnew file mode 100644\nindex 0000000..ccb91b3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/models/vector_store_schema_config.py\n@@ -0,0 +1,54 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Parameterization settings for the default configuration.\"\"\"\n+\n+import re\n+\n+from pydantic import BaseModel, Field, model_validator\n+\n+DEFAULT_VECTOR_SIZE: int = 3072\n+\n+VALID_IDENTIFIER_REGEX = re.compile(r\"^[A-Za-z_][A-Za-z0-9_]*$\")\n+\n+\n+def is_valid_field_name(field: str) -> bool:\n+    \"\"\"Check if a field name is valid for CosmosDB.\"\"\"\n+    return bool(VALID_IDENTIFIER_REGEX.match(field))\n+\n+\n+class VectorStoreSchemaConfig(BaseModel):\n+    \"\"\"The default configuration section for Vector Store Schema.\"\"\"\n+\n+    id_field: str = Field(\n+        description=\"The ID field to use.\",\n+        default=\"id\",\n+    )\n+\n+    vector_field: str = Field(\n+        description=\"The vector field to use.\",\n+        default=\"vector\",\n+    )\n+\n+    vector_size: int = Field(\n+        description=\"The vector size to use.\",\n+        default=DEFAULT_VECTOR_SIZE,\n+    )\n+\n+    index_name: str | None = Field(description=\"The index name to use.\", default=None)\n+\n+    def _validate_schema(self) -> None:\n+        \"\"\"Validate the schema.\"\"\"\n+        for field in [\n+            self.id_field,\n+            self.vector_field,\n+        ]:\n+            if not is_valid_field_name(field):\n+                msg = f\"Unsafe or invalid field name: {field}\"\n+                raise ValueError(msg)\n+\n+    @model_validator(mode=\"after\")\n+    def _validate_model(self):\n+        \"\"\"Validate the model.\"\"\"\n+        self._validate_schema()\n+        return self\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/config/read_dotenv.py",
            "diff": "diff --git a/packages/graphrag/graphrag/config/read_dotenv.py b/packages/graphrag/graphrag/config/read_dotenv.py\nnew file mode 100644\nindex 0000000..a2da5d4\n--- /dev/null\n+++ b/packages/graphrag/graphrag/config/read_dotenv.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the read_dotenv utility.\"\"\"\n+\n+import logging\n+import os\n+from pathlib import Path\n+\n+from dotenv import dotenv_values\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def read_dotenv(root: str) -> None:\n+    \"\"\"Read a .env file in the given root path.\"\"\"\n+    env_path = Path(root) / \".env\"\n+    if env_path.exists():\n+        logger.info(\"Loading pipeline .env file\")\n+        env_config = dotenv_values(f\"{env_path}\")\n+        for key, value in env_config.items():\n+            if key not in os.environ:\n+                os.environ[key] = value or \"\"\n+    else:\n+        logger.info(\"No .env file found at %s\", root)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/__init__.py b/packages/graphrag/graphrag/data_model/__init__.py\nnew file mode 100644\nindex 0000000..3c0de52\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Knowledge model package.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/community.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/community.py b/packages/graphrag/graphrag/data_model/community.py\nnew file mode 100644\nindex 0000000..b7016cc\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/community.py\n@@ -0,0 +1,79 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Community' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.data_model.named import Named\n+\n+\n+@dataclass\n+class Community(Named):\n+    \"\"\"A protocol for a community in the system.\"\"\"\n+\n+    level: str\n+    \"\"\"Community level.\"\"\"\n+\n+    parent: str\n+    \"\"\"Community ID of the parent node of this community.\"\"\"\n+\n+    children: list[str]\n+    \"\"\"List of community IDs of the child nodes of this community.\"\"\"\n+\n+    entity_ids: list[str] | None = None\n+    \"\"\"List of entity IDs related to the community (optional).\"\"\"\n+\n+    relationship_ids: list[str] | None = None\n+    \"\"\"List of relationship IDs related to the community (optional).\"\"\"\n+\n+    text_unit_ids: list[str] | None = None\n+    \"\"\"List of text unit IDs related to the community (optional).\"\"\"\n+\n+    covariate_ids: dict[str, list[str]] | None = None\n+    \"\"\"Dictionary of different types of covariates related to the community (optional), e.g. claims\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+    \"\"\"A dictionary of additional attributes associated with the community (optional). To be included in the search prompt.\"\"\"\n+\n+    size: int | None = None\n+    \"\"\"The size of the community (Amount of text units).\"\"\"\n+\n+    period: str | None = None\n+    \"\"\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        title_key: str = \"title\",\n+        short_id_key: str = \"human_readable_id\",\n+        level_key: str = \"level\",\n+        entities_key: str = \"entity_ids\",\n+        relationships_key: str = \"relationship_ids\",\n+        text_units_key: str = \"text_unit_ids\",\n+        covariates_key: str = \"covariate_ids\",\n+        parent_key: str = \"parent\",\n+        children_key: str = \"children\",\n+        attributes_key: str = \"attributes\",\n+        size_key: str = \"size\",\n+        period_key: str = \"period\",\n+    ) -> \"Community\":\n+        \"\"\"Create a new community from the dict data.\"\"\"\n+        return Community(\n+            id=d[id_key],\n+            title=d[title_key],\n+            level=d[level_key],\n+            parent=d[parent_key],\n+            children=d[children_key],\n+            short_id=d.get(short_id_key),\n+            entity_ids=d.get(entities_key),\n+            relationship_ids=d.get(relationships_key),\n+            text_unit_ids=d.get(text_units_key),\n+            covariate_ids=d.get(covariates_key),\n+            attributes=d.get(attributes_key),\n+            size=d.get(size_key),\n+            period=d.get(period_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/community_report.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/community_report.py b/packages/graphrag/graphrag/data_model/community_report.py\nnew file mode 100644\nindex 0000000..2ce55f7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/community_report.py\n@@ -0,0 +1,67 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'CommunityReport' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.data_model.named import Named\n+\n+\n+@dataclass\n+class CommunityReport(Named):\n+    \"\"\"Defines an LLM-generated summary report of a community.\"\"\"\n+\n+    community_id: str\n+    \"\"\"The ID of the community this report is associated with.\"\"\"\n+\n+    summary: str = \"\"\n+    \"\"\"Summary of the report.\"\"\"\n+\n+    full_content: str = \"\"\n+    \"\"\"Full content of the report.\"\"\"\n+\n+    rank: float | None = 1.0\n+    \"\"\"Rank of the report, used for sorting (optional). Higher means more important\"\"\"\n+\n+    full_content_embedding: list[float] | None = None\n+    \"\"\"The semantic (i.e. text) embedding of the full report content (optional).\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+    \"\"\"A dictionary of additional attributes associated with the report (optional).\"\"\"\n+\n+    size: int | None = None\n+    \"\"\"The size of the report (Amount of text units).\"\"\"\n+\n+    period: str | None = None\n+    \"\"\"The period of the report (optional).\"\"\"\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        title_key: str = \"title\",\n+        community_id_key: str = \"community\",\n+        short_id_key: str = \"human_readable_id\",\n+        summary_key: str = \"summary\",\n+        full_content_key: str = \"full_content\",\n+        rank_key: str = \"rank\",\n+        attributes_key: str = \"attributes\",\n+        size_key: str = \"size\",\n+        period_key: str = \"period\",\n+    ) -> \"CommunityReport\":\n+        \"\"\"Create a new community report from the dict data.\"\"\"\n+        return CommunityReport(\n+            id=d[id_key],\n+            title=d[title_key],\n+            community_id=d[community_id_key],\n+            short_id=d.get(short_id_key),\n+            summary=d[summary_key],\n+            full_content=d[full_content_key],\n+            rank=d[rank_key],\n+            attributes=d.get(attributes_key),\n+            size=d.get(size_key),\n+            period=d.get(period_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/covariate.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/covariate.py b/packages/graphrag/graphrag/data_model/covariate.py\nnew file mode 100644\nindex 0000000..6fd6b3f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/covariate.py\n@@ -0,0 +1,54 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Covariate' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.data_model.identified import Identified\n+\n+\n+@dataclass\n+class Covariate(Identified):\n+    \"\"\"\n+    A protocol for a covariate in the system.\n+\n+    Covariates are metadata associated with a subject, e.g. entity claims.\n+    Each subject (e.g. entity) may be associated with multiple types of covariates.\n+    \"\"\"\n+\n+    subject_id: str\n+    \"\"\"The subject id.\"\"\"\n+\n+    subject_type: str = \"entity\"\n+    \"\"\"The subject type.\"\"\"\n+\n+    covariate_type: str = \"claim\"\n+    \"\"\"The covariate type.\"\"\"\n+\n+    text_unit_ids: list[str] | None = None\n+    \"\"\"List of text unit IDs in which the covariate info appears (optional).\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        subject_id_key: str = \"subject_id\",\n+        covariate_type_key: str = \"covariate_type\",\n+        short_id_key: str = \"human_readable_id\",\n+        text_unit_ids_key: str = \"text_unit_ids\",\n+        attributes_key: str = \"attributes\",\n+    ) -> \"Covariate\":\n+        \"\"\"Create a new covariate from the dict data.\"\"\"\n+        return Covariate(\n+            id=d[id_key],\n+            short_id=d.get(short_id_key),\n+            subject_id=d[subject_id_key],\n+            covariate_type=d.get(covariate_type_key, \"claim\"),\n+            text_unit_ids=d.get(text_unit_ids_key),\n+            attributes=d.get(attributes_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/document.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/document.py b/packages/graphrag/graphrag/data_model/document.py\nnew file mode 100644\nindex 0000000..80c9854\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/document.py\n@@ -0,0 +1,49 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Document' model.\"\"\"\n+\n+from dataclasses import dataclass, field\n+from typing import Any\n+\n+from graphrag.data_model.named import Named\n+\n+\n+@dataclass\n+class Document(Named):\n+    \"\"\"A protocol for a document in the system.\"\"\"\n+\n+    type: str = \"text\"\n+    \"\"\"Type of the document.\"\"\"\n+\n+    text_unit_ids: list[str] = field(default_factory=list)\n+    \"\"\"list of text units in the document.\"\"\"\n+\n+    text: str = \"\"\n+    \"\"\"The raw text content of the document.\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+    \"\"\"A dictionary of structured attributes such as author, etc (optional).\"\"\"\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        short_id_key: str = \"human_readable_id\",\n+        title_key: str = \"title\",\n+        type_key: str = \"type\",\n+        text_key: str = \"text\",\n+        text_units_key: str = \"text_units\",\n+        attributes_key: str = \"attributes\",\n+    ) -> \"Document\":\n+        \"\"\"Create a new document from the dict data.\"\"\"\n+        return Document(\n+            id=d[id_key],\n+            short_id=d.get(short_id_key),\n+            title=d[title_key],\n+            type=d.get(type_key, \"text\"),\n+            text=d[text_key],\n+            text_unit_ids=d.get(text_units_key, []),\n+            attributes=d.get(attributes_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/entity.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/entity.py b/packages/graphrag/graphrag/data_model/entity.py\nnew file mode 100644\nindex 0000000..0b0470c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/entity.py\n@@ -0,0 +1,69 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Entity' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.data_model.named import Named\n+\n+\n+@dataclass\n+class Entity(Named):\n+    \"\"\"A protocol for an entity in the system.\"\"\"\n+\n+    type: str | None = None\n+    \"\"\"Type of the entity (can be any string, optional).\"\"\"\n+\n+    description: str | None = None\n+    \"\"\"Description of the entity (optional).\"\"\"\n+\n+    description_embedding: list[float] | None = None\n+    \"\"\"The semantic (i.e. text) embedding of the entity (optional).\"\"\"\n+\n+    name_embedding: list[float] | None = None\n+    \"\"\"The semantic (i.e. text) embedding of the entity (optional).\"\"\"\n+\n+    community_ids: list[str] | None = None\n+    \"\"\"The community IDs of the entity (optional).\"\"\"\n+\n+    text_unit_ids: list[str] | None = None\n+    \"\"\"List of text unit IDs in which the entity appears (optional).\"\"\"\n+\n+    rank: int | None = 1\n+    \"\"\"Rank of the entity, used for sorting (optional). Higher rank indicates more important entity. This can be based on centrality or other metrics.\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+    \"\"\"Additional attributes associated with the entity (optional), e.g. start time, end time, etc. To be included in the search prompt.\"\"\"\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        short_id_key: str = \"human_readable_id\",\n+        title_key: str = \"title\",\n+        type_key: str = \"type\",\n+        description_key: str = \"description\",\n+        description_embedding_key: str = \"description_embedding\",\n+        name_embedding_key: str = \"name_embedding\",\n+        community_key: str = \"community\",\n+        text_unit_ids_key: str = \"text_unit_ids\",\n+        rank_key: str = \"degree\",\n+        attributes_key: str = \"attributes\",\n+    ) -> \"Entity\":\n+        \"\"\"Create a new entity from the dict data.\"\"\"\n+        return Entity(\n+            id=d[id_key],\n+            title=d[title_key],\n+            short_id=d.get(short_id_key),\n+            type=d.get(type_key),\n+            description=d.get(description_key),\n+            name_embedding=d.get(name_embedding_key),\n+            description_embedding=d.get(description_embedding_key),\n+            community_ids=d.get(community_key),\n+            rank=d.get(rank_key, 1),\n+            text_unit_ids=d.get(text_unit_ids_key),\n+            attributes=d.get(attributes_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/identified.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/identified.py b/packages/graphrag/graphrag/data_model/identified.py\nnew file mode 100644\nindex 0000000..ca2c939\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/identified.py\n@@ -0,0 +1,17 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Identified' protocol.\"\"\"\n+\n+from dataclasses import dataclass\n+\n+\n+@dataclass\n+class Identified:\n+    \"\"\"A protocol for an item with an ID.\"\"\"\n+\n+    id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    short_id: str | None\n+    \"\"\"Human readable ID used to refer to this community in prompts or texts displayed to users, such as in a report text (optional).\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/named.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/named.py b/packages/graphrag/graphrag/data_model/named.py\nnew file mode 100644\nindex 0000000..d5048c7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/named.py\n@@ -0,0 +1,16 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Named' protocol.\"\"\"\n+\n+from dataclasses import dataclass\n+\n+from graphrag.data_model.identified import Identified\n+\n+\n+@dataclass\n+class Named(Identified):\n+    \"\"\"A protocol for an item with a name/title.\"\"\"\n+\n+    title: str\n+    \"\"\"The name/title of the item.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/relationship.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/relationship.py b/packages/graphrag/graphrag/data_model/relationship.py\nnew file mode 100644\nindex 0000000..89d139b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/relationship.py\n@@ -0,0 +1,65 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'Relationship' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.data_model.identified import Identified\n+\n+\n+@dataclass\n+class Relationship(Identified):\n+    \"\"\"A relationship between two entities. This is a generic relationship, and can be used to represent any type of relationship between any two entities.\"\"\"\n+\n+    source: str\n+    \"\"\"The source entity name.\"\"\"\n+\n+    target: str\n+    \"\"\"The target entity name.\"\"\"\n+\n+    weight: float | None = 1.0\n+    \"\"\"The edge weight.\"\"\"\n+\n+    description: str | None = None\n+    \"\"\"A description of the relationship (optional).\"\"\"\n+\n+    description_embedding: list[float] | None = None\n+    \"\"\"The semantic embedding for the relationship description (optional).\"\"\"\n+\n+    text_unit_ids: list[str] | None = None\n+    \"\"\"List of text unit IDs in which the relationship appears (optional).\"\"\"\n+\n+    rank: int | None = 1\n+    \"\"\"Rank of the relationship, used for sorting (optional). Higher rank indicates more important relationship. This can be based on centrality or other metrics.\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+    \"\"\"Additional attributes associated with the relationship (optional). To be included in the search prompt\"\"\"\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        short_id_key: str = \"human_readable_id\",\n+        source_key: str = \"source\",\n+        target_key: str = \"target\",\n+        description_key: str = \"description\",\n+        rank_key: str = \"rank\",\n+        weight_key: str = \"weight\",\n+        text_unit_ids_key: str = \"text_unit_ids\",\n+        attributes_key: str = \"attributes\",\n+    ) -> \"Relationship\":\n+        \"\"\"Create a new relationship from the dict data.\"\"\"\n+        return Relationship(\n+            id=d[id_key],\n+            short_id=d.get(short_id_key),\n+            source=d[source_key],\n+            target=d[target_key],\n+            rank=d.get(rank_key, 1),\n+            description=d.get(description_key),\n+            weight=d.get(weight_key, 1.0),\n+            text_unit_ids=d.get(text_unit_ids_key),\n+            attributes=d.get(attributes_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/schemas.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/schemas.py b/packages/graphrag/graphrag/data_model/schemas.py\nnew file mode 100644\nindex 0000000..93ede46\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/schemas.py\n@@ -0,0 +1,159 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"Common field name definitions for data frames.\"\"\"\n+\n+ID = \"id\"\n+SHORT_ID = \"human_readable_id\"\n+TITLE = \"title\"\n+DESCRIPTION = \"description\"\n+\n+TYPE = \"type\"\n+\n+# POST-PREP NODE TABLE SCHEMA\n+NODE_DEGREE = \"degree\"\n+NODE_FREQUENCY = \"frequency\"\n+NODE_DETAILS = \"node_details\"\n+\n+# POST-PREP EDGE TABLE SCHEMA\n+EDGE_SOURCE = \"source\"\n+EDGE_TARGET = \"target\"\n+EDGE_DEGREE = \"combined_degree\"\n+EDGE_DETAILS = \"edge_details\"\n+EDGE_WEIGHT = \"weight\"\n+\n+# POST-PREP CLAIM TABLE SCHEMA\n+CLAIM_SUBJECT = \"subject_id\"\n+CLAIM_STATUS = \"status\"\n+CLAIM_DETAILS = \"claim_details\"\n+\n+# COMMUNITY HIERARCHY TABLE SCHEMA\n+SUB_COMMUNITY = \"sub_community\"\n+\n+# COMMUNITY CONTEXT TABLE SCHEMA\n+ALL_CONTEXT = \"all_context\"\n+CONTEXT_STRING = \"context_string\"\n+CONTEXT_SIZE = \"context_size\"\n+CONTEXT_EXCEED_FLAG = \"context_exceed_limit\"\n+\n+# COMMUNITY REPORT TABLE SCHEMA\n+COMMUNITY_ID = \"community\"\n+COMMUNITY_LEVEL = \"level\"\n+COMMUNITY_PARENT = \"parent\"\n+COMMUNITY_CHILDREN = \"children\"\n+TITLE = \"title\"\n+SUMMARY = \"summary\"\n+FINDINGS = \"findings\"\n+RATING = \"rank\"\n+EXPLANATION = \"rating_explanation\"\n+FULL_CONTENT = \"full_content\"\n+FULL_CONTENT_JSON = \"full_content_json\"\n+\n+ENTITY_IDS = \"entity_ids\"\n+RELATIONSHIP_IDS = \"relationship_ids\"\n+TEXT_UNIT_IDS = \"text_unit_ids\"\n+COVARIATE_IDS = \"covariate_ids\"\n+DOCUMENT_ID = \"document_id\"\n+\n+PERIOD = \"period\"\n+SIZE = \"size\"\n+\n+# text units\n+ENTITY_DEGREE = \"entity_degree\"\n+ALL_DETAILS = \"all_details\"\n+TEXT = \"text\"\n+N_TOKENS = \"n_tokens\"\n+\n+CREATION_DATE = \"creation_date\"\n+METADATA = \"metadata\"\n+\n+# the following lists define the final content and ordering of columns in the data model parquet outputs\n+ENTITIES_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    TITLE,\n+    TYPE,\n+    DESCRIPTION,\n+    TEXT_UNIT_IDS,\n+    NODE_FREQUENCY,\n+    NODE_DEGREE,\n+]\n+\n+RELATIONSHIPS_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    EDGE_SOURCE,\n+    EDGE_TARGET,\n+    DESCRIPTION,\n+    EDGE_WEIGHT,\n+    EDGE_DEGREE,\n+    TEXT_UNIT_IDS,\n+]\n+\n+COMMUNITIES_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    COMMUNITY_ID,\n+    COMMUNITY_LEVEL,\n+    COMMUNITY_PARENT,\n+    COMMUNITY_CHILDREN,\n+    TITLE,\n+    ENTITY_IDS,\n+    RELATIONSHIP_IDS,\n+    TEXT_UNIT_IDS,\n+    PERIOD,\n+    SIZE,\n+]\n+\n+COMMUNITY_REPORTS_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    COMMUNITY_ID,\n+    COMMUNITY_LEVEL,\n+    COMMUNITY_PARENT,\n+    COMMUNITY_CHILDREN,\n+    TITLE,\n+    SUMMARY,\n+    FULL_CONTENT,\n+    RATING,\n+    EXPLANATION,\n+    FINDINGS,\n+    FULL_CONTENT_JSON,\n+    PERIOD,\n+    SIZE,\n+]\n+\n+COVARIATES_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    \"covariate_type\",\n+    TYPE,\n+    DESCRIPTION,\n+    \"subject_id\",\n+    \"object_id\",\n+    \"status\",\n+    \"start_date\",\n+    \"end_date\",\n+    \"source_text\",\n+    \"text_unit_id\",\n+]\n+\n+TEXT_UNITS_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    TEXT,\n+    N_TOKENS,\n+    DOCUMENT_ID,\n+    ENTITY_IDS,\n+    RELATIONSHIP_IDS,\n+    COVARIATE_IDS,\n+]\n+\n+DOCUMENTS_FINAL_COLUMNS = [\n+    ID,\n+    SHORT_ID,\n+    TITLE,\n+    TEXT,\n+    TEXT_UNIT_IDS,\n+    CREATION_DATE,\n+    METADATA,\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/text_unit.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/text_unit.py b/packages/graphrag/graphrag/data_model/text_unit.py\nnew file mode 100644\nindex 0000000..55006ab\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/text_unit.py\n@@ -0,0 +1,62 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the 'TextUnit' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.data_model.identified import Identified\n+\n+\n+@dataclass\n+class TextUnit(Identified):\n+    \"\"\"A protocol for a TextUnit item in a Document database.\"\"\"\n+\n+    text: str\n+    \"\"\"The text of the unit.\"\"\"\n+\n+    entity_ids: list[str] | None = None\n+    \"\"\"List of entity IDs related to the text unit (optional).\"\"\"\n+\n+    relationship_ids: list[str] | None = None\n+    \"\"\"List of relationship IDs related to the text unit (optional).\"\"\"\n+\n+    covariate_ids: dict[str, list[str]] | None = None\n+    \"Dictionary of different types of covariates related to the text unit (optional).\"\n+\n+    n_tokens: int | None = None\n+    \"\"\"The number of tokens in the text (optional).\"\"\"\n+\n+    document_id: str | None = None\n+    \"\"\"ID of the document in which the text unit appears (optional).\"\"\"\n+\n+    attributes: dict[str, Any] | None = None\n+    \"\"\"A dictionary of additional attributes associated with the text unit (optional).\"\"\"\n+\n+    @classmethod\n+    def from_dict(\n+        cls,\n+        d: dict[str, Any],\n+        id_key: str = \"id\",\n+        short_id_key: str = \"human_readable_id\",\n+        text_key: str = \"text\",\n+        entities_key: str = \"entity_ids\",\n+        relationships_key: str = \"relationship_ids\",\n+        covariates_key: str = \"covariate_ids\",\n+        n_tokens_key: str = \"n_tokens\",\n+        document_id_key: str = \"document_id\",\n+        attributes_key: str = \"attributes\",\n+    ) -> \"TextUnit\":\n+        \"\"\"Create a new text unit from the dict data.\"\"\"\n+        return TextUnit(\n+            id=d[id_key],\n+            short_id=d.get(short_id_key),\n+            text=d[text_key],\n+            entity_ids=d.get(entities_key),\n+            relationship_ids=d.get(relationships_key),\n+            covariate_ids=d.get(covariates_key),\n+            n_tokens=d.get(n_tokens_key),\n+            document_id=d.get(document_id_key),\n+            attributes=d.get(attributes_key),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/data_model/types.py",
            "diff": "diff --git a/packages/graphrag/graphrag/data_model/types.py b/packages/graphrag/graphrag/data_model/types.py\nnew file mode 100644\nindex 0000000..6156e39\n--- /dev/null\n+++ b/packages/graphrag/graphrag/data_model/types.py\n@@ -0,0 +1,8 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Common types for the GraphRAG knowledge model.\"\"\"\n+\n+from collections.abc import Callable\n+\n+TextEmbedder = Callable[[str], list[float]]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/factory/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/factory/__init__.py b/packages/graphrag/graphrag/factory/__init__.py\nnew file mode 100644\nindex 0000000..55bd738\n--- /dev/null\n+++ b/packages/graphrag/graphrag/factory/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Factory module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/factory/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/factory/factory.py b/packages/graphrag/graphrag/factory/factory.py\nnew file mode 100644\nindex 0000000..3116c53\n--- /dev/null\n+++ b/packages/graphrag/graphrag/factory/factory.py\n@@ -0,0 +1,68 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Factory ABC.\"\"\"\n+\n+from abc import ABC\n+from collections.abc import Callable\n+from typing import Any, ClassVar, Generic, TypeVar\n+\n+T = TypeVar(\"T\", covariant=True)\n+\n+\n+class Factory(ABC, Generic[T]):\n+    \"\"\"Abstract base class for factories.\"\"\"\n+\n+    _instance: ClassVar[\"Factory | None\"] = None\n+\n+    def __new__(cls, *args: Any, **kwargs: Any) -> \"Factory\":\n+        \"\"\"Create a new instance of Factory if it does not exist.\"\"\"\n+        if cls._instance is None:\n+            cls._instance = super().__new__(cls, *args, **kwargs)\n+        return cls._instance\n+\n+    def __init__(self):\n+        if not hasattr(self, \"_initialized\"):\n+            self._services: dict[str, Callable[..., T]] = {}\n+            self._initialized = True\n+\n+    def __contains__(self, strategy: str) -> bool:\n+        \"\"\"Check if a strategy is registered.\"\"\"\n+        return strategy in self._services\n+\n+    def keys(self) -> list[str]:\n+        \"\"\"Get a list of registered strategy names.\"\"\"\n+        return list(self._services.keys())\n+\n+    def register(self, strategy: str, initializer: Callable[..., T]) -> None:\n+        \"\"\"\n+        Register a new service.\n+\n+        Args\n+        ----\n+            strategy: The name of the strategy.\n+            initializer: A callable that creates an instance of T.\n+        \"\"\"\n+        self._services[strategy] = initializer\n+\n+    def create(self, strategy: str, init_args: dict[str, Any] | None = None) -> T:\n+        \"\"\"\n+        Create a service instance based on the strategy.\n+\n+        Args\n+        ----\n+            strategy: The name of the strategy.\n+            init_args: Dict of keyword arguments to pass to the service initializer.\n+\n+        Returns\n+        -------\n+            An instance of T.\n+\n+        Raises\n+        ------\n+            ValueError: If the strategy is not registered.\n+        \"\"\"\n+        if strategy not in self._services:\n+            msg = f\"Strategy '{strategy}' is not registered.\"\n+            raise ValueError(msg)\n+        return self._services[strategy](**(init_args or {}))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/__init__.py b/packages/graphrag/graphrag/index/__init__.py\nnew file mode 100644\nindex 0000000..c5acc43\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The indexing engine package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/__init__.py b/packages/graphrag/graphrag/index/input/__init__.py\nnew file mode 100644\nindex 0000000..15177c9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine input package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/csv.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/csv.py b/packages/graphrag/graphrag/index/input/csv.py\nnew file mode 100644\nindex 0000000..a70863e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/csv.py\n@@ -0,0 +1,35 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing load method definition.\"\"\"\n+\n+import logging\n+from io import BytesIO\n+\n+import pandas as pd\n+\n+from graphrag.index.input.input_reader import InputReader\n+from graphrag.index.input.util import process_data_columns\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class CSVFileReader(InputReader):\n+    \"\"\"Reader implementation for csv files.\"\"\"\n+\n+    async def read_file(self, path: str) -> pd.DataFrame:\n+        \"\"\"Read a csv file into a DataFrame of documents.\n+\n+        Args:\n+            - path - The path to read the file from.\n+\n+        Returns\n+        -------\n+            - output - DataFrame with a row for each document in the file.\n+        \"\"\"\n+        buffer = BytesIO(await self._storage.get(path, as_bytes=True))\n+        data = pd.read_csv(buffer, encoding=self._config.encoding)\n+        data = process_data_columns(data, self._config, path)\n+        creation_date = await self._storage.get_creation_date(path)\n+        data[\"creation_date\"] = data.apply(lambda _: creation_date, axis=1)\n+        return data\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/factory.py b/packages/graphrag/graphrag/index/input/factory.py\nnew file mode 100644\nindex 0000000..bdabd0d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/factory.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing create_input method definition.\"\"\"\n+\n+import logging\n+\n+from graphrag.config.enums import InputFileType\n+from graphrag.factory.factory import Factory\n+from graphrag.index.input.csv import CSVFileReader\n+from graphrag.index.input.input_reader import InputReader\n+from graphrag.index.input.json import JSONFileReader\n+from graphrag.index.input.text import TextFileReader\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class InputReaderFactory(Factory[InputReader]):\n+    \"\"\"Factory for creating Input Reader instances.\"\"\"\n+\n+\n+input_reader_factory = InputReaderFactory()\n+input_reader_factory.register(InputFileType.text, TextFileReader)\n+input_reader_factory.register(InputFileType.csv, CSVFileReader)\n+input_reader_factory.register(InputFileType.json, JSONFileReader)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/input_reader.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/input_reader.py b/packages/graphrag/graphrag/index/input/input_reader.py\nnew file mode 100644\nindex 0000000..ed0add9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/input_reader.py\n@@ -0,0 +1,84 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'PipelineCache' model.\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import re\n+from abc import ABCMeta, abstractmethod\n+from typing import TYPE_CHECKING\n+\n+import pandas as pd\n+\n+if TYPE_CHECKING:\n+    from graphrag.config.models.input_config import InputConfig\n+    from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class InputReader(metaclass=ABCMeta):\n+    \"\"\"Provide a cache interface for the pipeline.\"\"\"\n+\n+    def __init__(self, storage: PipelineStorage, config: InputConfig, **kwargs):\n+        self._storage = storage\n+        self._config = config\n+\n+    async def read_files(self) -> pd.DataFrame:\n+        \"\"\"Load files from storage and apply a loader function based on file type. Process metadata on the results if needed.\"\"\"\n+        files = list(self._storage.find(re.compile(self._config.file_pattern)))\n+\n+        if len(files) == 0:\n+            msg = f\"No {self._config.file_type} files found in {self._config.storage.base_dir}\"\n+            raise ValueError(msg)\n+\n+        files_loaded = []\n+\n+        for file in files:\n+            try:\n+                files_loaded.append(await self.read_file(file))\n+            except Exception as e:  # noqa: BLE001 (catching Exception is fine here)\n+                logger.warning(\"Warning! Error loading file %s. Skipping...\", file)\n+                logger.warning(\"Error: %s\", e)\n+\n+        logger.info(\n+            \"Found %d %s files, loading %d\",\n+            len(files),\n+            self._config.file_type,\n+            len(files_loaded),\n+        )\n+        result = pd.concat(files_loaded)\n+        total_files_log = (\n+            f\"Total number of unfiltered {self._config.file_type} rows: {len(result)}\"\n+        )\n+        logger.info(total_files_log)\n+        # Convert metadata columns to strings and collapse them into a JSON object\n+        if self._config.metadata:\n+            if all(col in result.columns for col in self._config.metadata):\n+                # Collapse the metadata columns into a single JSON object column\n+                result[\"metadata\"] = result[self._config.metadata].apply(\n+                    lambda row: row.to_dict(), axis=1\n+                )\n+            else:\n+                value_error_msg = (\n+                    \"One or more metadata columns not found in the DataFrame.\"\n+                )\n+                raise ValueError(value_error_msg)\n+\n+            result[self._config.metadata] = result[self._config.metadata].astype(str)\n+\n+        return result\n+\n+    @abstractmethod\n+    async def read_file(self, path: str) -> pd.DataFrame:\n+        \"\"\"Read a file into a DataFrame of documents.\n+\n+        Args:\n+            - path - The path to read the file from.\n+\n+        Returns\n+        -------\n+            - output - DataFrame with a row for each document in the file.\n+        \"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/json.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/json.py b/packages/graphrag/graphrag/index/input/json.py\nnew file mode 100644\nindex 0000000..cae7db5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/json.py\n@@ -0,0 +1,39 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing load method definition.\"\"\"\n+\n+import json\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.index.input.input_reader import InputReader\n+from graphrag.index.input.util import process_data_columns\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class JSONFileReader(InputReader):\n+    \"\"\"Reader implementation for json files.\"\"\"\n+\n+    async def read_file(self, path: str) -> pd.DataFrame:\n+        \"\"\"Read a JSON file into a DataFrame of documents.\n+\n+        Args:\n+            - path - The path to read the file from.\n+\n+        Returns\n+        -------\n+            - output - DataFrame with a row for each document in the file.\n+        \"\"\"\n+        text = await self._storage.get(path, encoding=self._config.encoding)\n+        as_json = json.loads(text)\n+        # json file could just be a single object, or an array of objects\n+        rows = as_json if isinstance(as_json, list) else [as_json]\n+        data = pd.DataFrame(rows)\n+        data = process_data_columns(data, self._config, path)\n+        creation_date = await self._storage.get_creation_date(path)\n+        data[\"creation_date\"] = data.apply(lambda _: creation_date, axis=1)\n+\n+        return data\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/text.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/text.py b/packages/graphrag/graphrag/index/input/text.py\nnew file mode 100644\nindex 0000000..dac2c4c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/text.py\n@@ -0,0 +1,35 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing load method definition.\"\"\"\n+\n+import logging\n+from pathlib import Path\n+\n+import pandas as pd\n+\n+from graphrag.index.input.input_reader import InputReader\n+from graphrag.index.utils.hashing import gen_sha512_hash\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class TextFileReader(InputReader):\n+    \"\"\"Reader implementation for text files.\"\"\"\n+\n+    async def read_file(self, path: str) -> pd.DataFrame:\n+        \"\"\"Read a text file into a DataFrame of documents.\n+\n+        Args:\n+            - path - The path to read the file from.\n+\n+        Returns\n+        -------\n+            - output - DataFrame with a row for each document in the file.\n+        \"\"\"\n+        text = await self._storage.get(path, encoding=self._config.encoding)\n+        new_item = {\"text\": text}\n+        new_item[\"id\"] = gen_sha512_hash(new_item, new_item.keys())\n+        new_item[\"title\"] = str(Path(path).name)\n+        new_item[\"creation_date\"] = await self._storage.get_creation_date(path)\n+        return pd.DataFrame([new_item])\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/input/util.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/input/util.py b/packages/graphrag/graphrag/index/input/util.py\nnew file mode 100644\nindex 0000000..2780909\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/input/util.py\n@@ -0,0 +1,46 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Shared column processing for structured input files.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.input_config import InputConfig\n+from graphrag.index.utils.hashing import gen_sha512_hash\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def process_data_columns(\n+    documents: pd.DataFrame, config: InputConfig, path: str\n+) -> pd.DataFrame:\n+    \"\"\"Process configured data columns of a DataFrame.\"\"\"\n+    if \"id\" not in documents.columns:\n+        documents[\"id\"] = documents.apply(\n+            lambda x: gen_sha512_hash(x, x.keys()), axis=1\n+        )\n+    if config.text_column is not None and \"text\" not in documents.columns:\n+        if config.text_column not in documents.columns:\n+            logger.warning(\n+                \"text_column %s not found in csv file %s\",\n+                config.text_column,\n+                path,\n+            )\n+        else:\n+            documents[\"text\"] = documents.apply(lambda x: x[config.text_column], axis=1)\n+    if config.title_column is not None:\n+        if config.title_column not in documents.columns:\n+            logger.warning(\n+                \"title_column %s not found in csv file %s\",\n+                config.title_column,\n+                path,\n+            )\n+        else:\n+            documents[\"title\"] = documents.apply(\n+                lambda x: x[config.title_column], axis=1\n+            )\n+    else:\n+        documents[\"title\"] = documents.apply(lambda _: path, axis=1)\n+    return documents\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/__init__.py b/packages/graphrag/graphrag/index/operations/__init__.py\nnew file mode 100644\nindex 0000000..c5a0f18\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Reusable data frame operations.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/__init__.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/__init__.py\nnew file mode 100644\nindex 0000000..5aa6b17\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine noun graph package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/build_noun_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/build_noun_graph.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/build_noun_graph.py\nnew file mode 100644\nindex 0000000..8d3310e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/build_noun_graph.py\n@@ -0,0 +1,138 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Graph extraction using NLP.\"\"\"\n+\n+from itertools import combinations\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.config.enums import AsyncType\n+from graphrag.index.operations.build_noun_graph.np_extractors.base import (\n+    BaseNounPhraseExtractor,\n+)\n+from graphrag.index.utils.derive_from_rows import derive_from_rows\n+from graphrag.index.utils.graphs import calculate_pmi_edge_weights\n+from graphrag.index.utils.hashing import gen_sha512_hash\n+\n+\n+async def build_noun_graph(\n+    text_unit_df: pd.DataFrame,\n+    text_analyzer: BaseNounPhraseExtractor,\n+    normalize_edge_weights: bool,\n+    num_threads: int,\n+    async_mode: AsyncType,\n+    cache: PipelineCache,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"Build a noun graph from text units.\"\"\"\n+    text_units = text_unit_df.loc[:, [\"id\", \"text\"]]\n+    nodes_df = await _extract_nodes(\n+        text_units,\n+        text_analyzer,\n+        num_threads=num_threads,\n+        async_mode=async_mode,\n+        cache=cache,\n+    )\n+    edges_df = _extract_edges(nodes_df, normalize_edge_weights=normalize_edge_weights)\n+    return (nodes_df, edges_df)\n+\n+\n+async def _extract_nodes(\n+    text_unit_df: pd.DataFrame,\n+    text_analyzer: BaseNounPhraseExtractor,\n+    num_threads: int,\n+    async_mode: AsyncType,\n+    cache: PipelineCache,\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Extract initial nodes and edges from text units.\n+\n+    Input: text unit df with schema [id, text, document_id]\n+    Returns a dataframe with schema [id, title, frequency, text_unit_ids].\n+    \"\"\"\n+    cache = cache.child(\"extract_noun_phrases\")\n+\n+    async def extract(row):\n+        text = row[\"text\"]\n+        attrs = {\"text\": text, \"analyzer\": str(text_analyzer)}\n+        key = gen_sha512_hash(attrs, attrs.keys())\n+        result = await cache.get(key)\n+        if not result:\n+            result = text_analyzer.extract(text)\n+            await cache.set(key, result)\n+        return result\n+\n+    text_unit_df[\"noun_phrases\"] = await derive_from_rows(\n+        text_unit_df,\n+        extract,\n+        num_threads=num_threads,\n+        async_type=async_mode,\n+        progress_msg=\"extract noun phrases progress: \",\n+    )\n+\n+    noun_node_df = text_unit_df.explode(\"noun_phrases\")\n+    noun_node_df = noun_node_df.rename(\n+        columns={\"noun_phrases\": \"title\", \"id\": \"text_unit_id\"}\n+    )\n+\n+    # group by title and count the number of text units\n+    grouped_node_df = (\n+        noun_node_df.groupby(\"title\").agg({\"text_unit_id\": list}).reset_index()\n+    )\n+    grouped_node_df = grouped_node_df.rename(columns={\"text_unit_id\": \"text_unit_ids\"})\n+    grouped_node_df[\"frequency\"] = grouped_node_df[\"text_unit_ids\"].apply(len)\n+    grouped_node_df = grouped_node_df[[\"title\", \"frequency\", \"text_unit_ids\"]]\n+    return grouped_node_df.loc[:, [\"title\", \"frequency\", \"text_unit_ids\"]]\n+\n+\n+def _extract_edges(\n+    nodes_df: pd.DataFrame,\n+    normalize_edge_weights: bool = True,\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Extract edges from nodes.\n+\n+    Nodes appear in the same text unit are connected.\n+    Input: nodes_df with schema [id, title, frequency, text_unit_ids]\n+    Returns: edges_df with schema [source, target, weight, text_unit_ids]\n+    \"\"\"\n+    text_units_df = nodes_df.explode(\"text_unit_ids\")\n+    text_units_df = text_units_df.rename(columns={\"text_unit_ids\": \"text_unit_id\"})\n+\n+    text_units_df = (\n+        text_units_df.groupby(\"text_unit_id\")\n+        .agg({\"title\": lambda x: list(x) if len(x) > 1 else np.nan})\n+        .reset_index()\n+    )\n+    text_units_df = text_units_df.dropna()\n+    titles = text_units_df[\"title\"].tolist()\n+    all_edges: list[list[tuple[str, str]]] = [list(combinations(t, 2)) for t in titles]\n+\n+    text_units_df = text_units_df.assign(edges=all_edges)  # type: ignore\n+    edge_df = text_units_df.explode(\"edges\")[[\"edges\", \"text_unit_id\"]]\n+\n+    edge_df[[\"source\", \"target\"]] = edge_df.loc[:, \"edges\"].to_list()\n+    edge_df[\"min_source\"] = edge_df[[\"source\", \"target\"]].min(axis=1)\n+    edge_df[\"max_target\"] = edge_df[[\"source\", \"target\"]].max(axis=1)\n+    edge_df = edge_df.drop(columns=[\"source\", \"target\"]).rename(\n+        columns={\"min_source\": \"source\", \"max_target\": \"target\"}  # type: ignore\n+    )\n+\n+    edge_df = edge_df[(edge_df.source.notna()) & (edge_df.target.notna())]\n+    edge_df = edge_df.drop(columns=[\"edges\"])\n+    # group by source and target, count the number of text units\n+    grouped_edge_df = (\n+        edge_df.groupby([\"source\", \"target\"]).agg({\"text_unit_id\": list}).reset_index()\n+    )\n+    grouped_edge_df = grouped_edge_df.rename(columns={\"text_unit_id\": \"text_unit_ids\"})\n+    grouped_edge_df[\"weight\"] = grouped_edge_df[\"text_unit_ids\"].apply(len)\n+    grouped_edge_df = grouped_edge_df.loc[\n+        :, [\"source\", \"target\", \"weight\", \"text_unit_ids\"]\n+    ]\n+    if normalize_edge_weights:\n+        # use PMI weight instead of raw weight\n+        grouped_edge_df = calculate_pmi_edge_weights(nodes_df, grouped_edge_df)\n+\n+    return grouped_edge_df\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/__init__.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/__init__.py\nnew file mode 100644\nindex 0000000..116ecc8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"NLP-based graph extractors.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/base.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/base.py\nnew file mode 100644\nindex 0000000..18bb3f8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/base.py\n@@ -0,0 +1,61 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base class for noun phrase extractors.\"\"\"\n+\n+import logging\n+from abc import ABCMeta, abstractmethod\n+\n+import spacy\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BaseNounPhraseExtractor(metaclass=ABCMeta):\n+    \"\"\"Abstract base class for noun phrase extractors.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model_name: str | None,\n+        exclude_nouns: list[str] | None = None,\n+        max_word_length: int = 15,\n+        word_delimiter: str = \" \",\n+    ) -> None:\n+        self.model_name = model_name\n+        self.max_word_length = max_word_length\n+        if exclude_nouns is None:\n+            exclude_nouns = []\n+        self.exclude_nouns = [noun.upper() for noun in exclude_nouns]\n+        self.word_delimiter = word_delimiter\n+\n+    @abstractmethod\n+    def extract(self, text: str) -> list[str]:\n+        \"\"\"\n+        Extract noun phrases from text.\n+\n+        Args:\n+            text: Text.\n+\n+        Returns: List of noun phrases.\n+        \"\"\"\n+\n+    @abstractmethod\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the extractor, used for cache key generation.\"\"\"\n+\n+    @staticmethod\n+    def load_spacy_model(\n+        model_name: str, exclude: list[str] | None = None\n+    ) -> spacy.language.Language:\n+        \"\"\"Load a SpaCy model.\"\"\"\n+        if exclude is None:\n+            exclude = []\n+        try:\n+            return spacy.load(model_name, exclude=exclude)\n+        except OSError:\n+            msg = f\"Model `{model_name}` not found. Attempting to download...\"\n+            logger.info(msg)\n+            from spacy.cli.download import download\n+\n+            download(model_name)\n+            return spacy.load(model_name, exclude=exclude)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py\nnew file mode 100644\nindex 0000000..68c636a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py\n@@ -0,0 +1,181 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CFG-based noun phrase extractor.\"\"\"\n+\n+from typing import Any\n+\n+from spacy.tokens.doc import Doc\n+\n+from graphrag.index.operations.build_noun_graph.np_extractors.base import (\n+    BaseNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.np_validator import (\n+    has_valid_token_length,\n+    is_compound,\n+    is_valid_entity,\n+)\n+\n+\n+class CFGNounPhraseExtractor(BaseNounPhraseExtractor):\n+    \"\"\"CFG-based noun phrase extractor.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model_name: str,\n+        max_word_length: int,\n+        include_named_entities: bool,\n+        exclude_entity_tags: list[str],\n+        exclude_pos_tags: list[str],\n+        exclude_nouns: list[str],\n+        word_delimiter: str,\n+        noun_phrase_grammars: dict[tuple, str],\n+        noun_phrase_tags: list[str],\n+    ):\n+        \"\"\"\n+        Noun phrase extractor combining CFG-based noun-chunk extraction and NER.\n+\n+        CFG-based extraction was based on TextBlob's fast NP extractor implementation:\n+        This extractor tends to be faster than the dependency-parser-based extractors but grammars may need to be changed for different languages.\n+\n+        Args:\n+            model_name: SpaCy model name.\n+            max_word_length: Maximum length (in character) of each extracted word.\n+            include_named_entities: Whether to include named entities in noun phrases\n+            exclude_entity_tags: list of named entity tags to exclude in noun phrases.\n+            exclude_pos_tags: List of POS tags to remove in noun phrases.\n+            word_delimiter: Delimiter for joining words.\n+            noun_phrase_grammars: CFG for matching noun phrases.\n+        \"\"\"\n+        super().__init__(\n+            model_name=model_name,\n+            max_word_length=max_word_length,\n+            exclude_nouns=exclude_nouns,\n+            word_delimiter=word_delimiter,\n+        )\n+        self.include_named_entities = include_named_entities\n+        self.exclude_entity_tags = exclude_entity_tags\n+        if not include_named_entities:\n+            self.nlp = self.load_spacy_model(\n+                model_name, exclude=[\"lemmatizer\", \"parser\", \"ner\"]\n+            )\n+        else:\n+            self.nlp = self.load_spacy_model(\n+                model_name, exclude=[\"lemmatizer\", \"parser\"]\n+            )\n+\n+        self.exclude_pos_tags = exclude_pos_tags\n+        self.noun_phrase_grammars = noun_phrase_grammars\n+        self.noun_phrase_tags = noun_phrase_tags\n+\n+    def extract(\n+        self,\n+        text: str,\n+    ) -> list[str]:\n+        \"\"\"\n+        Extract noun phrases from text. Noun phrases may include named entities and noun chunks, which are filtered based on some heuristics.\n+\n+        Args:\n+            text: Text.\n+\n+        Returns: List of noun phrases.\n+        \"\"\"\n+        doc = self.nlp(text)\n+\n+        filtered_noun_phrases = set()\n+        if self.include_named_entities:\n+            # extract noun chunks + entities then filter overlapping spans\n+            entities = [\n+                (ent.text, ent.label_)\n+                for ent in doc.ents\n+                if ent.label_ not in self.exclude_entity_tags\n+            ]\n+            entity_texts = set({ent[0] for ent in entities})\n+            cfg_matches = self.extract_cfg_matches(doc)\n+            noun_phrases = entities + [\n+                np for np in cfg_matches if np[0] not in entity_texts\n+            ]\n+\n+            # filter noun phrases based on heuristics\n+            tagged_noun_phrases = [\n+                self._tag_noun_phrases(np, entity_texts) for np in noun_phrases\n+            ]\n+            for tagged_np in tagged_noun_phrases:\n+                if (tagged_np[\"is_valid_entity\"]) or (\n+                    (\n+                        len(tagged_np[\"cleaned_tokens\"]) > 1\n+                        or tagged_np[\"has_compound_words\"]\n+                    )\n+                    and tagged_np[\"has_valid_tokens\"]\n+                ):\n+                    filtered_noun_phrases.add(tagged_np[\"cleaned_text\"])\n+        else:\n+            noun_phrases = self.extract_cfg_matches(doc)\n+            tagged_noun_phrases = [self._tag_noun_phrases(np) for np in noun_phrases]\n+            for tagged_np in tagged_noun_phrases:\n+                if (tagged_np[\"has_proper_nouns\"]) or (\n+                    (\n+                        len(tagged_np[\"cleaned_tokens\"]) > 1\n+                        or tagged_np[\"has_compound_words\"]\n+                    )\n+                    and tagged_np[\"has_valid_tokens\"]\n+                ):\n+                    filtered_noun_phrases.add(tagged_np[\"cleaned_text\"])\n+        return list(filtered_noun_phrases)\n+\n+    def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]:\n+        \"\"\"Return noun phrases that match a given context-free grammar.\"\"\"\n+        tagged_tokens = [\n+            (token.text, token.pos_)\n+            for token in doc\n+            if token.pos_ not in self.exclude_pos_tags\n+            and token.is_space is False\n+            and token.text != \"-\"\n+        ]\n+        merge = True\n+        while merge:\n+            merge = False\n+            for index in range(len(tagged_tokens) - 1):\n+                first, second = tagged_tokens[index], tagged_tokens[index + 1]\n+                key = first[1], second[1]\n+                value = self.noun_phrase_grammars.get(key, None)\n+                if value:\n+                    # find a matching pattern, pop the two tokens and insert the merged one\n+                    merge = True\n+                    tagged_tokens.pop(index)\n+                    tagged_tokens.pop(index)\n+                    match = f\"{first[0]}{self.word_delimiter}{second[0]}\"\n+                    pos = value\n+                    tagged_tokens.insert(index, (match, pos))\n+                    break\n+        return [t for t in tagged_tokens if t[1] in self.noun_phrase_tags]\n+\n+    def _tag_noun_phrases(\n+        self, noun_chunk: tuple[str, str], entities: set[str] | None = None\n+    ) -> dict[str, Any]:\n+        \"\"\"Extract attributes of a noun chunk, to be used for filtering.\"\"\"\n+        tokens = noun_chunk[0].split(self.word_delimiter)\n+        cleaned_tokens = [\n+            token for token in tokens if token.upper() not in self.exclude_nouns\n+        ]\n+\n+        has_valid_entity = False\n+        if entities and noun_chunk[0] in entities:\n+            has_valid_entity = is_valid_entity(noun_chunk, cleaned_tokens)\n+\n+        return {\n+            \"cleaned_tokens\": cleaned_tokens,\n+            \"cleaned_text\": self.word_delimiter.join(cleaned_tokens)\n+            .replace(\"\\n\", \"\")\n+            .upper(),\n+            \"is_valid_entity\": has_valid_entity,\n+            \"has_proper_nouns\": (noun_chunk[1] == \"PROPN\"),\n+            \"has_compound_words\": is_compound(cleaned_tokens),\n+            \"has_valid_tokens\": has_valid_token_length(\n+                cleaned_tokens, self.max_word_length\n+            ),\n+        }\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the extractor, used for cache key generation.\"\"\"\n+        return f\"cfg_{self.model_name}_{self.max_word_length}_{self.include_named_entities}_{self.exclude_entity_tags}_{self.exclude_pos_tags}_{self.exclude_nouns}_{self.word_delimiter}_{self.noun_phrase_grammars}_{self.noun_phrase_tags}\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/factory.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/factory.py\nnew file mode 100644\nindex 0000000..b790c5a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/factory.py\n@@ -0,0 +1,82 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Create a noun phrase extractor from a configuration.\"\"\"\n+\n+from typing import ClassVar\n+\n+from graphrag.config.enums import NounPhraseExtractorType\n+from graphrag.config.models.extract_graph_nlp_config import TextAnalyzerConfig\n+from graphrag.index.operations.build_noun_graph.np_extractors.base import (\n+    BaseNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.cfg_extractor import (\n+    CFGNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.regex_extractor import (\n+    RegexENNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.stop_words import (\n+    EN_STOP_WORDS,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.syntactic_parsing_extractor import (\n+    SyntacticNounPhraseExtractor,\n+)\n+\n+\n+class NounPhraseExtractorFactory:\n+    \"\"\"A factory class for creating noun phrase extractor.\"\"\"\n+\n+    np_extractor_types: ClassVar[dict[str, type]] = {}\n+\n+    @classmethod\n+    def register(cls, np_extractor_type: str, np_extractor: type):\n+        \"\"\"Register a vector store type.\"\"\"\n+        cls.np_extractor_types[np_extractor_type] = np_extractor\n+\n+    @classmethod\n+    def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor:\n+        \"\"\"Get the noun phrase extractor type from a string.\"\"\"\n+        np_extractor_type = config.extractor_type\n+        exclude_nouns = config.exclude_nouns\n+        if exclude_nouns is None:\n+            exclude_nouns = EN_STOP_WORDS\n+        match np_extractor_type:\n+            case NounPhraseExtractorType.Syntactic:\n+                return SyntacticNounPhraseExtractor(\n+                    model_name=config.model_name,\n+                    max_word_length=config.max_word_length,\n+                    include_named_entities=config.include_named_entities,\n+                    exclude_entity_tags=config.exclude_entity_tags,\n+                    exclude_pos_tags=config.exclude_pos_tags,\n+                    exclude_nouns=exclude_nouns,\n+                    word_delimiter=config.word_delimiter,\n+                )\n+            case NounPhraseExtractorType.CFG:\n+                grammars = {}\n+                for key, value in config.noun_phrase_grammars.items():\n+                    grammars[tuple(key.split(\",\"))] = value\n+                return CFGNounPhraseExtractor(\n+                    model_name=config.model_name,\n+                    max_word_length=config.max_word_length,\n+                    include_named_entities=config.include_named_entities,\n+                    exclude_entity_tags=config.exclude_entity_tags,\n+                    exclude_pos_tags=config.exclude_pos_tags,\n+                    exclude_nouns=exclude_nouns,\n+                    word_delimiter=config.word_delimiter,\n+                    noun_phrase_grammars=grammars,\n+                    noun_phrase_tags=config.noun_phrase_tags,\n+                )\n+            case NounPhraseExtractorType.RegexEnglish:\n+                return RegexENNounPhraseExtractor(\n+                    exclude_nouns=exclude_nouns,\n+                    max_word_length=config.max_word_length,\n+                    word_delimiter=config.word_delimiter,\n+                )\n+\n+\n+def create_noun_phrase_extractor(\n+    analyzer_config: TextAnalyzerConfig,\n+) -> BaseNounPhraseExtractor:\n+    \"\"\"Create a noun phrase extractor from a configuration.\"\"\"\n+    return NounPhraseExtractorFactory.get_np_extractor(analyzer_config)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py\nnew file mode 100644\nindex 0000000..d24c337\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions to tag noun phrases for filtering.\"\"\"\n+\n+\n+def is_compound(tokens: list[str]) -> bool:\n+    \"\"\"List of tokens forms a compound noun phrase.\"\"\"\n+    return any(\n+        \"-\" in token and len(token.strip()) > 1 and len(token.strip().split(\"-\")) > 1\n+        for token in tokens\n+    )\n+\n+\n+def has_valid_token_length(tokens: list[str], max_length: int) -> bool:\n+    \"\"\"Check if all tokens have valid length.\"\"\"\n+    return all(len(token) <= max_length for token in tokens)\n+\n+\n+def is_valid_entity(entity: tuple[str, str], tokens: list[str]) -> bool:\n+    \"\"\"Check if the entity is valid.\"\"\"\n+    return (entity[1] not in [\"CARDINAL\", \"ORDINAL\"] and len(tokens) > 0) or (\n+        entity[1] in [\"CARDINAL\", \"ORDINAL\"]\n+        and (len(tokens) > 1 or is_compound(tokens))\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py\nnew file mode 100644\nindex 0000000..2f14b68\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py\n@@ -0,0 +1,123 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Functions to analyze text data using SpaCy.\"\"\"\n+\n+import re\n+from typing import Any\n+\n+import nltk\n+from textblob import TextBlob\n+\n+from graphrag.index.operations.build_noun_graph.np_extractors.base import (\n+    BaseNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.resource_loader import (\n+    download_if_not_exists,\n+)\n+\n+\n+class RegexENNounPhraseExtractor(BaseNounPhraseExtractor):\n+    \"\"\"Regular expression-based noun phrase extractor for English.\"\"\"\n+\n+    def __init__(\n+        self,\n+        exclude_nouns: list[str],\n+        max_word_length: int,\n+        word_delimiter: str,\n+    ):\n+        \"\"\"\n+        Noun phrase extractor for English based on TextBlob's fast NP extractor, which uses a regex POS tagger and context-free grammars to detect noun phrases.\n+\n+        NOTE: This is the extractor used in the first bencharmking of LazyGraphRAG but it only works for English.\n+        It is much faster but likely less accurate than the syntactic parser-based extractor.\n+        TODO: Reimplement this using SpaCy to remove TextBlob dependency.\n+\n+        Args:\n+            max_word_length: Maximum length (in character) of each extracted word.\n+            word_delimiter: Delimiter for joining words.\n+        \"\"\"\n+        super().__init__(\n+            model_name=None,\n+            max_word_length=max_word_length,\n+            exclude_nouns=exclude_nouns,\n+            word_delimiter=word_delimiter,\n+        )\n+        # download corpora\n+        download_if_not_exists(\"brown\")\n+        download_if_not_exists(\"treebank\")\n+        download_if_not_exists(\"averaged_perceptron_tagger_eng\")\n+\n+        # download tokenizers\n+        download_if_not_exists(\"punkt\")\n+        download_if_not_exists(\"punkt_tab\")\n+\n+        # Preload the corpora to avoid lazy loading issues due to\n+        # race conditions when running multi-threaded jobs.\n+        nltk.corpus.brown.ensure_loaded()\n+        nltk.corpus.treebank.ensure_loaded()\n+\n+    def extract(\n+        self,\n+        text: str,\n+    ) -> list[str]:\n+        \"\"\"\n+        Extract noun phrases from text using regex patterns.\n+\n+        Args:\n+            text: Text.\n+\n+        Returns: List of noun phrases.\n+        \"\"\"\n+        blob = TextBlob(text)\n+        proper_nouns = [token[0].upper() for token in blob.tags if token[1] == \"NNP\"]  # type: ignore\n+        tagged_noun_phrases = [\n+            self._tag_noun_phrases(chunk, proper_nouns)\n+            for chunk in blob.noun_phrases  # type: ignore\n+        ]\n+\n+        filtered_noun_phrases = set()\n+        for tagged_np in tagged_noun_phrases:\n+            if (\n+                tagged_np[\"has_proper_nouns\"]\n+                or len(tagged_np[\"cleaned_tokens\"]) > 1\n+                or tagged_np[\"has_compound_words\"]\n+            ) and tagged_np[\"has_valid_tokens\"]:\n+                filtered_noun_phrases.add(tagged_np[\"cleaned_text\"])\n+        return list(filtered_noun_phrases)\n+\n+    def _tag_noun_phrases(\n+        self, noun_phrase: str, all_proper_nouns: list[str] | None = None\n+    ) -> dict[str, Any]:\n+        \"\"\"Extract attributes of a noun chunk, to be used for filtering.\"\"\"\n+        if all_proper_nouns is None:\n+            all_proper_nouns = []\n+        tokens = [token for token in re.split(r\"[\\s]+\", noun_phrase) if len(token) > 0]\n+        cleaned_tokens = [\n+            token for token in tokens if token.upper() not in self.exclude_nouns\n+        ]\n+        has_proper_nouns = any(\n+            token.upper() in all_proper_nouns for token in cleaned_tokens\n+        )\n+        has_compound_words = any(\n+            \"-\" in token\n+            and len(token.strip()) > 1\n+            and len(token.strip().split(\"-\")) > 1\n+            for token in cleaned_tokens\n+        )\n+        has_valid_tokens = all(\n+            re.match(r\"^[a-zA-Z0-9\\-]+\\n?$\", token) for token in cleaned_tokens\n+        ) and all(len(token) <= self.max_word_length for token in cleaned_tokens)\n+        return {\n+            \"cleaned_tokens\": cleaned_tokens,\n+            \"cleaned_text\": self.word_delimiter.join(token for token in cleaned_tokens)\n+            .replace(\"\\n\", \"\")\n+            .upper(),\n+            \"has_proper_nouns\": has_proper_nouns,\n+            \"has_compound_words\": has_compound_words,\n+            \"has_valid_tokens\": has_valid_tokens,\n+        }\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the extractor, used for cache key generation.\"\"\"\n+        return f\"regex_en_{self.exclude_nouns}_{self.max_word_length}_{self.word_delimiter}\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py\nnew file mode 100644\nindex 0000000..ed6c5a8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py\n@@ -0,0 +1,38 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions needed for nltk-based noun-phrase extractors (i.e. TextBlob).\"\"\"\n+\n+import nltk\n+\n+\n+def download_if_not_exists(resource_name) -> bool:\n+    \"\"\"Download nltk resources if they haven't been already.\"\"\"\n+    # look under all possible categories\n+    root_categories = [\n+        \"corpora\",\n+        \"tokenizers\",\n+        \"taggers\",\n+        \"chunkers\",\n+        \"classifiers\",\n+        \"stemmers\",\n+        \"stopwords\",\n+        \"languages\",\n+        \"frequent\",\n+        \"gate\",\n+        \"models\",\n+        \"mt\",\n+        \"sentiment\",\n+        \"similarity\",\n+    ]\n+    for category in root_categories:\n+        try:\n+            # if found, stop looking and avoid downloading\n+            nltk.find(f\"{category}/{resource_name}\")\n+            return True  # noqa: TRY300\n+        except LookupError:\n+            continue\n+\n+    # is not found, download\n+    nltk.download(resource_name)\n+    return False\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/stop_words.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/stop_words.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/stop_words.py\nnew file mode 100644\nindex 0000000..2788a62\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/stop_words.py\n@@ -0,0 +1,21 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Custom list of stop words to be excluded by noun phrase extractors.\"\"\"\n+\n+EN_STOP_WORDS = [\n+    \"stuff\",\n+    \"thing\",\n+    \"things\",\n+    \"bunch\",\n+    \"bit\",\n+    \"bits\",\n+    \"people\",\n+    \"person\",\n+    \"okay\",\n+    \"hey\",\n+    \"hi\",\n+    \"hello\",\n+    \"laughter\",\n+    \"oh\",\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py\nnew file mode 100644\nindex 0000000..9a38e41\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py\n@@ -0,0 +1,162 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Noun phrase extractor based on dependency parsing and NER using SpaCy.\"\"\"\n+\n+from typing import Any\n+\n+from spacy.tokens.span import Span\n+from spacy.util import filter_spans\n+\n+from graphrag.index.operations.build_noun_graph.np_extractors.base import (\n+    BaseNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.np_validator import (\n+    has_valid_token_length,\n+    is_compound,\n+    is_valid_entity,\n+)\n+\n+\n+class SyntacticNounPhraseExtractor(BaseNounPhraseExtractor):\n+    \"\"\"Noun phrase extractor based on dependency parsing and NER using SpaCy.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model_name: str,\n+        max_word_length: int,\n+        include_named_entities: bool,\n+        exclude_entity_tags: list[str],\n+        exclude_pos_tags: list[str],\n+        exclude_nouns: list[str],\n+        word_delimiter: str,\n+    ):\n+        \"\"\"\n+        Noun phrase extractor based on dependency parsing and NER using SpaCy.\n+\n+        This extractor tends to produce more accurate results than regex-based extractors but is slower.\n+        Also, it can be used for different languages by using the corresponding models from SpaCy.\n+\n+        Args:\n+            model_name: SpaCy model name.\n+            max_word_length: Maximum length (in character) of each extracted word.\n+            include_named_entities: Whether to include named entities in noun phrases\n+            exclude_entity_tags: list of named entity tags to exclude in noun phrases.\n+            exclude_pos_tags: List of POS tags to remove in noun phrases.\n+            word_delimiter: Delimiter for joining words.\n+        \"\"\"\n+        super().__init__(\n+            model_name=model_name,\n+            max_word_length=max_word_length,\n+            exclude_nouns=exclude_nouns,\n+            word_delimiter=word_delimiter,\n+        )\n+        self.include_named_entities = include_named_entities\n+        self.exclude_entity_tags = exclude_entity_tags\n+        if not include_named_entities:\n+            self.nlp = self.load_spacy_model(model_name, exclude=[\"lemmatizer\", \"ner\"])\n+        else:\n+            self.nlp = self.load_spacy_model(model_name, exclude=[\"lemmatizer\"])\n+\n+        self.exclude_pos_tags = exclude_pos_tags\n+\n+    def extract(\n+        self,\n+        text: str,\n+    ) -> list[str]:\n+        \"\"\"\n+        Extract noun phrases from text. Noun phrases may include named entities and noun chunks, which are filtered based on some heuristics.\n+\n+        Args:\n+            text: Text.\n+\n+        Returns: List of noun phrases.\n+        \"\"\"\n+        doc = self.nlp(text)\n+\n+        filtered_noun_phrases = set()\n+        if self.include_named_entities:\n+            # extract noun chunks + entities then filter overlapping spans\n+            entities = [\n+                ent for ent in doc.ents if ent.label_ not in self.exclude_entity_tags\n+            ]\n+            spans = entities + list(doc.noun_chunks)\n+            spans = filter_spans(spans)\n+\n+            # reading missing entities\n+            missing_entities = [\n+                ent\n+                for ent in entities\n+                if not any(ent.text in span.text for span in spans)\n+            ]\n+            spans.extend(missing_entities)\n+\n+            # filtering noun phrases based on some heuristics\n+            tagged_noun_phrases = [\n+                self._tag_noun_phrases(span, entities) for span in spans\n+            ]\n+            for tagged_np in tagged_noun_phrases:\n+                if (tagged_np[\"is_valid_entity\"]) or (\n+                    (\n+                        len(tagged_np[\"cleaned_tokens\"]) > 1\n+                        or tagged_np[\"has_compound_words\"]\n+                    )\n+                    and tagged_np[\"has_valid_tokens\"]\n+                ):\n+                    filtered_noun_phrases.add(tagged_np[\"cleaned_text\"])\n+        else:\n+            tagged_noun_phrases = [\n+                self._tag_noun_phrases(chunk, []) for chunk in doc.noun_chunks\n+            ]\n+            for tagged_np in tagged_noun_phrases:\n+                if (tagged_np[\"has_proper_noun\"]) or (\n+                    (\n+                        len(tagged_np[\"cleaned_tokens\"]) > 1\n+                        or tagged_np[\"has_compound_words\"]\n+                    )\n+                    and tagged_np[\"has_valid_tokens\"]\n+                ):\n+                    filtered_noun_phrases.add(tagged_np[\"cleaned_text\"])\n+\n+        return list(filtered_noun_phrases)\n+\n+    def _tag_noun_phrases(\n+        self, noun_chunk: Span, entities: list[Span]\n+    ) -> dict[str, Any]:\n+        \"\"\"Extract attributes of a noun chunk, to be used for filtering.\"\"\"\n+        cleaned_tokens = [\n+            token\n+            for token in noun_chunk\n+            if token.pos_ not in self.exclude_pos_tags\n+            and token.text.upper() not in self.exclude_nouns\n+            and token.is_space is False\n+            and not token.is_punct\n+        ]\n+        cleaned_token_texts = [token.text for token in cleaned_tokens]\n+        cleaned_text_string = (\n+            self.word_delimiter.join(cleaned_token_texts).replace(\"\\n\", \"\").upper()\n+        )\n+\n+        noun_chunk_entity_labels = [\n+            (ent.text, ent.label_) for ent in entities if noun_chunk.text == ent.text\n+        ]\n+        if noun_chunk_entity_labels:\n+            noun_chunk_entity_label = noun_chunk_entity_labels[0]\n+            valid_entity = is_valid_entity(noun_chunk_entity_label, cleaned_token_texts)\n+        else:\n+            valid_entity = False\n+\n+        return {\n+            \"cleaned_tokens\": cleaned_tokens,\n+            \"cleaned_text\": cleaned_text_string,\n+            \"is_valid_entity\": valid_entity,\n+            \"has_proper_nouns\": any(token.pos_ == \"PROPN\" for token in cleaned_tokens),\n+            \"has_compound_words\": is_compound(cleaned_token_texts),\n+            \"has_valid_tokens\": has_valid_token_length(\n+                cleaned_token_texts, self.max_word_length\n+            ),\n+        }\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the extractor, used for cache key generation.\"\"\"\n+        return f\"syntactic_{self.model_name}_{self.max_word_length}_{self.include_named_entities}_{self.exclude_entity_tags}_{self.exclude_pos_tags}_{self.exclude_nouns}_{self.word_delimiter}\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/chunk_text/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/chunk_text/__init__.py b/packages/graphrag/graphrag/index/operations/chunk_text/__init__.py\nnew file mode 100644\nindex 0000000..1e000e6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/chunk_text/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine text chunk package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/chunk_text/bootstrap.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/chunk_text/bootstrap.py b/packages/graphrag/graphrag/index/operations/chunk_text/bootstrap.py\nnew file mode 100644\nindex 0000000..222a6be\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/chunk_text/bootstrap.py\n@@ -0,0 +1,31 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Bootstrap definition.\"\"\"\n+\n+import warnings\n+\n+# Ignore warnings from numba\n+warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n+warnings.filterwarnings(\"ignore\", message=\".*Use no seed for parallelism.*\")\n+\n+initialized_nltk = False\n+\n+\n+def bootstrap():\n+    \"\"\"Bootstrap definition.\"\"\"\n+    global initialized_nltk\n+    if not initialized_nltk:\n+        import nltk\n+        from nltk.corpus import wordnet as wn\n+\n+        nltk.download(\"punkt\")\n+        nltk.download(\"punkt_tab\")\n+        nltk.download(\"averaged_perceptron_tagger\")\n+        nltk.download(\"averaged_perceptron_tagger_eng\")\n+        nltk.download(\"maxent_ne_chunker\")\n+        nltk.download(\"maxent_ne_chunker_tab\")\n+        nltk.download(\"words\")\n+        nltk.download(\"wordnet\")\n+        wn.ensure_loaded()\n+        initialized_nltk = True\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/chunk_text/chunk_text.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/chunk_text/chunk_text.py b/packages/graphrag/graphrag/index/operations/chunk_text/chunk_text.py\nnew file mode 100644\nindex 0000000..fc50370\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/chunk_text/chunk_text.py\n@@ -0,0 +1,140 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing _get_num_total, chunk, run_strategy and load_strategy methods definitions.\"\"\"\n+\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.models.chunking_config import ChunkingConfig, ChunkStrategyType\n+from graphrag.index.operations.chunk_text.typing import (\n+    ChunkInput,\n+    ChunkStrategy,\n+)\n+from graphrag.logger.progress import ProgressTicker, progress_ticker\n+\n+\n+def chunk_text(\n+    input: pd.DataFrame,\n+    column: str,\n+    size: int,\n+    overlap: int,\n+    encoding_model: str,\n+    strategy: ChunkStrategyType,\n+    callbacks: WorkflowCallbacks,\n+) -> pd.Series:\n+    \"\"\"\n+    Chunk a piece of text into smaller pieces.\n+\n+    ## Usage\n+    ```yaml\n+    args:\n+        column: <column name> # The name of the column containing the text to chunk, this can either be a column with text, or a column with a list[tuple[doc_id, str]]\n+        strategy: <strategy config> # The strategy to use to chunk the text, see below for more details\n+    ```\n+\n+    ## Strategies\n+    The text chunk verb uses a strategy to chunk the text. The strategy is an object which defines the strategy to use. The following strategies are available:\n+\n+    ### tokens\n+    This strategy uses the [tokens] library to chunk a piece of text. The strategy config is as follows:\n+\n+    ```yaml\n+    strategy: tokens\n+    size: 1200 # Optional, The chunk size to use, default: 1200\n+    overlap: 100 # Optional, The chunk overlap to use, default: 100\n+    ```\n+\n+    ### sentence\n+    This strategy uses the nltk library to chunk a piece of text into sentences. The strategy config is as follows:\n+\n+    ```yaml\n+    strategy: sentence\n+    ```\n+    \"\"\"\n+    strategy_exec = load_strategy(strategy)\n+\n+    num_total = _get_num_total(input, column)\n+    tick = progress_ticker(callbacks.progress, num_total)\n+\n+    # collapse the config back to a single object to support \"polymorphic\" function call\n+    config = ChunkingConfig(size=size, overlap=overlap, encoding_model=encoding_model)\n+\n+    return cast(\n+        \"pd.Series\",\n+        input.apply(\n+            cast(\n+                \"Any\",\n+                lambda x: run_strategy(\n+                    strategy_exec,\n+                    x[column],\n+                    config,\n+                    tick,\n+                ),\n+            ),\n+            axis=1,\n+        ),\n+    )\n+\n+\n+def run_strategy(\n+    strategy_exec: ChunkStrategy,\n+    input: ChunkInput,\n+    config: ChunkingConfig,\n+    tick: ProgressTicker,\n+) -> list[str | tuple[list[str] | None, str, int]]:\n+    \"\"\"Run strategy method definition.\"\"\"\n+    if isinstance(input, str):\n+        return [item.text_chunk for item in strategy_exec([input], config, tick)]\n+\n+    # We can work with both just a list of text content\n+    # or a list of tuples of (document_id, text content)\n+    # text_to_chunk = '''\n+    texts = [item if isinstance(item, str) else item[1] for item in input]\n+\n+    strategy_results = strategy_exec(texts, config, tick)\n+\n+    results = []\n+    for strategy_result in strategy_results:\n+        doc_indices = strategy_result.source_doc_indices\n+        if isinstance(input[doc_indices[0]], str):\n+            results.append(strategy_result.text_chunk)\n+        else:\n+            doc_ids = [input[doc_idx][0] for doc_idx in doc_indices]\n+            results.append((\n+                doc_ids,\n+                strategy_result.text_chunk,\n+                strategy_result.n_tokens,\n+            ))\n+    return results\n+\n+\n+def load_strategy(strategy: ChunkStrategyType) -> ChunkStrategy:\n+    \"\"\"Load strategy method definition.\"\"\"\n+    match strategy:\n+        case ChunkStrategyType.tokens:\n+            from graphrag.index.operations.chunk_text.strategies import run_tokens\n+\n+            return run_tokens\n+        case ChunkStrategyType.sentence:\n+            # NLTK\n+            from graphrag.index.operations.chunk_text.bootstrap import bootstrap\n+            from graphrag.index.operations.chunk_text.strategies import run_sentences\n+\n+            bootstrap()\n+            return run_sentences\n+        case _:\n+            msg = f\"Unknown strategy: {strategy}\"\n+            raise ValueError(msg)\n+\n+\n+def _get_num_total(output: pd.DataFrame, column: str) -> int:\n+    num_total = 0\n+    for row in output[column]:\n+        if isinstance(row, str):\n+            num_total += 1\n+        else:\n+            num_total += len(row)\n+    return num_total\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/chunk_text/strategies.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/chunk_text/strategies.py b/packages/graphrag/graphrag/index/operations/chunk_text/strategies.py\nnew file mode 100644\nindex 0000000..9d48786\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/chunk_text/strategies.py\n@@ -0,0 +1,52 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing chunk strategies.\"\"\"\n+\n+from collections.abc import Iterable\n+\n+import nltk\n+\n+from graphrag.config.models.chunking_config import ChunkingConfig\n+from graphrag.index.operations.chunk_text.typing import TextChunk\n+from graphrag.index.text_splitting.text_splitting import (\n+    TokenChunkerOptions,\n+    split_multiple_texts_on_tokens,\n+)\n+from graphrag.logger.progress import ProgressTicker\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+\n+\n+def run_tokens(\n+    input: list[str],\n+    config: ChunkingConfig,\n+    tick: ProgressTicker,\n+) -> Iterable[TextChunk]:\n+    \"\"\"Chunks text into chunks based on encoding tokens.\"\"\"\n+    tokens_per_chunk = config.size\n+    chunk_overlap = config.overlap\n+    tokenizer = get_tokenizer(encoding_model=config.encoding_model)\n+    return split_multiple_texts_on_tokens(\n+        input,\n+        TokenChunkerOptions(\n+            chunk_overlap=chunk_overlap,\n+            tokens_per_chunk=tokens_per_chunk,\n+            encode=tokenizer.encode,\n+            decode=tokenizer.decode,\n+        ),\n+        tick,\n+    )\n+\n+\n+def run_sentences(\n+    input: list[str], _config: ChunkingConfig, tick: ProgressTicker\n+) -> Iterable[TextChunk]:\n+    \"\"\"Chunks text into multiple parts by sentence.\"\"\"\n+    for doc_idx, text in enumerate(input):\n+        sentences = nltk.sent_tokenize(text)\n+        for sentence in sentences:\n+            yield TextChunk(\n+                text_chunk=sentence,\n+                source_doc_indices=[doc_idx],\n+            )\n+        tick(1)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/chunk_text/typing.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/chunk_text/typing.py b/packages/graphrag/graphrag/index/operations/chunk_text/typing.py\nnew file mode 100644\nindex 0000000..bf58ef5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/chunk_text/typing.py\n@@ -0,0 +1,27 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'TextChunk' model.\"\"\"\n+\n+from collections.abc import Callable, Iterable\n+from dataclasses import dataclass\n+\n+from graphrag.config.models.chunking_config import ChunkingConfig\n+from graphrag.logger.progress import ProgressTicker\n+\n+\n+@dataclass\n+class TextChunk:\n+    \"\"\"Text chunk class definition.\"\"\"\n+\n+    text_chunk: str\n+    source_doc_indices: list[int]\n+    n_tokens: int | None = None\n+\n+\n+ChunkInput = str | list[str] | list[tuple[str, str]]\n+\"\"\"Input to a chunking strategy. Can be a string, a list of strings, or a list of tuples of (id, text).\"\"\"\n+\n+ChunkStrategy = Callable[\n+    [list[str], ChunkingConfig, ProgressTicker], Iterable[TextChunk]\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/cluster_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/cluster_graph.py b/packages/graphrag/graphrag/index/operations/cluster_graph.py\nnew file mode 100644\nindex 0000000..d7e1b0c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/cluster_graph.py\n@@ -0,0 +1,80 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing cluster_graph, apply_clustering methods definition.\"\"\"\n+\n+import logging\n+\n+import networkx as nx\n+\n+from graphrag.index.utils.graphs import hierarchical_leiden\n+from graphrag.index.utils.stable_lcc import stable_largest_connected_component\n+\n+Communities = list[tuple[int, int, int, list[str]]]\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def cluster_graph(\n+    graph: nx.Graph,\n+    max_cluster_size: int,\n+    use_lcc: bool,\n+    seed: int | None = None,\n+) -> Communities:\n+    \"\"\"Apply a hierarchical clustering algorithm to a graph.\"\"\"\n+    if len(graph.nodes) == 0:\n+        logger.warning(\"Graph has no nodes\")\n+        return []\n+\n+    node_id_to_community_map, parent_mapping = _compute_leiden_communities(\n+        graph=graph,\n+        max_cluster_size=max_cluster_size,\n+        use_lcc=use_lcc,\n+        seed=seed,\n+    )\n+\n+    levels = sorted(node_id_to_community_map.keys())\n+\n+    clusters: dict[int, dict[int, list[str]]] = {}\n+    for level in levels:\n+        result = {}\n+        clusters[level] = result\n+        for node_id, raw_community_id in node_id_to_community_map[level].items():\n+            community_id = raw_community_id\n+            if community_id not in result:\n+                result[community_id] = []\n+            result[community_id].append(node_id)\n+\n+    results: Communities = []\n+    for level in clusters:\n+        for cluster_id, nodes in clusters[level].items():\n+            results.append((level, cluster_id, parent_mapping[cluster_id], nodes))\n+    return results\n+\n+\n+# Taken from graph_intelligence & adapted\n+def _compute_leiden_communities(\n+    graph: nx.Graph | nx.DiGraph,\n+    max_cluster_size: int,\n+    use_lcc: bool,\n+    seed: int | None = None,\n+) -> tuple[dict[int, dict[str, int]], dict[int, int]]:\n+    \"\"\"Return Leiden root communities and their hierarchy mapping.\"\"\"\n+    if use_lcc:\n+        graph = stable_largest_connected_component(graph)\n+\n+    community_mapping = hierarchical_leiden(\n+        graph, max_cluster_size=max_cluster_size, random_seed=seed\n+    )\n+    results: dict[int, dict[str, int]] = {}\n+    hierarchy: dict[int, int] = {}\n+    for partition in community_mapping:\n+        results[partition.level] = results.get(partition.level, {})\n+        results[partition.level][partition.node] = partition.cluster\n+\n+        hierarchy[partition.cluster] = (\n+            partition.parent_cluster if partition.parent_cluster is not None else -1\n+        )\n+\n+    return results, hierarchy\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/compute_degree.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/compute_degree.py b/packages/graphrag/graphrag/index/operations/compute_degree.py\nnew file mode 100644\nindex 0000000..b720bf6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/compute_degree.py\n@@ -0,0 +1,15 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing create_graph definition.\"\"\"\n+\n+import networkx as nx\n+import pandas as pd\n+\n+\n+def compute_degree(graph: nx.Graph) -> pd.DataFrame:\n+    \"\"\"Create a new DataFrame with the degree of each node in the graph.\"\"\"\n+    return pd.DataFrame([\n+        {\"title\": node, \"degree\": int(degree)}\n+        for node, degree in graph.degree  # type: ignore\n+    ])\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/compute_edge_combined_degree.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/compute_edge_combined_degree.py b/packages/graphrag/graphrag/index/operations/compute_edge_combined_degree.py\nnew file mode 100644\nindex 0000000..f6c75b0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/compute_edge_combined_degree.py\n@@ -0,0 +1,43 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing compute_edge_combined_degree methods definition.\"\"\"\n+\n+from typing import cast\n+\n+import pandas as pd\n+\n+\n+def compute_edge_combined_degree(\n+    edge_df: pd.DataFrame,\n+    node_degree_df: pd.DataFrame,\n+    node_name_column: str,\n+    node_degree_column: str,\n+    edge_source_column: str,\n+    edge_target_column: str,\n+) -> pd.Series:\n+    \"\"\"Compute the combined degree for each edge in a graph.\"\"\"\n+\n+    def join_to_degree(df: pd.DataFrame, column: str) -> pd.DataFrame:\n+        degree_column = _degree_colname(column)\n+        result = df.merge(\n+            node_degree_df.rename(\n+                columns={node_name_column: column, node_degree_column: degree_column}\n+            ),\n+            on=column,\n+            how=\"left\",\n+        )\n+        result[degree_column] = result[degree_column].fillna(0)\n+        return result\n+\n+    output_df = join_to_degree(edge_df, edge_source_column)\n+    output_df = join_to_degree(output_df, edge_target_column)\n+    output_df[\"combined_degree\"] = (\n+        output_df[_degree_colname(edge_source_column)]\n+        + output_df[_degree_colname(edge_target_column)]\n+    )\n+    return cast(\"pd.Series\", output_df[\"combined_degree\"])\n+\n+\n+def _degree_colname(column: str) -> str:\n+    return f\"{column}_degree\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/create_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/create_graph.py b/packages/graphrag/graphrag/index/operations/create_graph.py\nnew file mode 100644\nindex 0000000..54b63b7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/create_graph.py\n@@ -0,0 +1,23 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing create_graph definition.\"\"\"\n+\n+import networkx as nx\n+import pandas as pd\n+\n+\n+def create_graph(\n+    edges: pd.DataFrame,\n+    edge_attr: list[str | int] | None = None,\n+    nodes: pd.DataFrame | None = None,\n+    node_id: str = \"title\",\n+) -> nx.Graph:\n+    \"\"\"Create a networkx graph from nodes and edges dataframes.\"\"\"\n+    graph = nx.from_pandas_edgelist(edges, edge_attr=edge_attr)\n+\n+    if nodes is not None:\n+        nodes.set_index(node_id, inplace=True)\n+        graph.add_nodes_from((n, dict(d)) for n, d in nodes.iterrows())\n+\n+    return graph\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/embed_text/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/embed_text/__init__.py b/packages/graphrag/graphrag/index/operations/embed_text/__init__.py\nnew file mode 100644\nindex 0000000..39d03c9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/embed_text/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine text embed package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/embed_text/embed_text.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/embed_text/embed_text.py b/packages/graphrag/graphrag/index/operations/embed_text/embed_text.py\nnew file mode 100644\nindex 0000000..d8144f8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/embed_text/embed_text.py\n@@ -0,0 +1,93 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing embed_text, load_strategy and create_row_from_embedding_data methods definition.\"\"\"\n+\n+import logging\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.operations.embed_text.run_embed_text import run_embed_text\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.vector_stores.base import BaseVectorStore, VectorStoreDocument\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def embed_text(\n+    input: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    model: EmbeddingModel,\n+    tokenizer: Tokenizer,\n+    embed_column: str,\n+    batch_size: int,\n+    batch_max_tokens: int,\n+    num_threads: int,\n+    vector_store: BaseVectorStore,\n+    id_column: str = \"id\",\n+    title_column: str | None = None,\n+):\n+    \"\"\"Embed a piece of text into a vector space. The operation outputs a new column containing a mapping between doc_id and vector.\"\"\"\n+    if embed_column not in input.columns:\n+        msg = f\"Column {embed_column} not found in input dataframe with columns {input.columns}\"\n+        raise ValueError(msg)\n+    title = title_column or embed_column\n+    if title not in input.columns:\n+        msg = (\n+            f\"Column {title} not found in input dataframe with columns {input.columns}\"\n+        )\n+        raise ValueError(msg)\n+    if id_column not in input.columns:\n+        msg = f\"Column {id_column} not found in input dataframe with columns {input.columns}\"\n+        raise ValueError(msg)\n+\n+    vector_store.create_index()\n+\n+    index = 0\n+\n+    all_results = []\n+\n+    num_total_batches = (input.shape[0] + batch_size - 1) // batch_size\n+    while batch_size * index < input.shape[0]:\n+        logger.info(\n+            \"uploading text embeddings batch %d/%d of size %d to vector store\",\n+            index + 1,\n+            num_total_batches,\n+            batch_size,\n+        )\n+        batch = input.iloc[batch_size * index : batch_size * (index + 1)]\n+        texts: list[str] = batch[embed_column].tolist()\n+        ids: list[str] = batch[id_column].tolist()\n+        result = await run_embed_text(\n+            texts,\n+            callbacks,\n+            model,\n+            tokenizer,\n+            batch_size,\n+            batch_max_tokens,\n+            num_threads,\n+        )\n+        if result.embeddings:\n+            embeddings = [\n+                embedding for embedding in result.embeddings if embedding is not None\n+            ]\n+            all_results.extend(embeddings)\n+\n+        vectors = result.embeddings or []\n+        documents: list[VectorStoreDocument] = []\n+        for doc_id, doc_vector in zip(ids, vectors, strict=True):\n+            if type(doc_vector) is np.ndarray:\n+                doc_vector = doc_vector.tolist()\n+            document = VectorStoreDocument(\n+                id=doc_id,\n+                vector=doc_vector,\n+            )\n+            documents.append(document)\n+\n+        vector_store.load_documents(documents)\n+        index += 1\n+\n+    return all_results\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/embed_text/run_embed_text.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/embed_text/run_embed_text.py b/packages/graphrag/graphrag/index/operations/embed_text/run_embed_text.py\nnew file mode 100644\nindex 0000000..fc3da5e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/embed_text/run_embed_text.py\n@@ -0,0 +1,171 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run method definition.\"\"\"\n+\n+import asyncio\n+import logging\n+from dataclasses import dataclass\n+\n+import numpy as np\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.text_splitting.text_splitting import TokenTextSplitter\n+from graphrag.index.utils.is_null import is_null\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.logger.progress import ProgressTicker, progress_ticker\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass\n+class TextEmbeddingResult:\n+    \"\"\"Text embedding result class definition.\"\"\"\n+\n+    embeddings: list[list[float] | None] | None\n+\n+\n+async def run_embed_text(\n+    input: list[str],\n+    callbacks: WorkflowCallbacks,\n+    model: EmbeddingModel,\n+    tokenizer: Tokenizer,\n+    batch_size: int,\n+    batch_max_tokens: int,\n+    num_threads: int,\n+) -> TextEmbeddingResult:\n+    \"\"\"Run the Claim extraction chain.\"\"\"\n+    if is_null(input):\n+        return TextEmbeddingResult(embeddings=None)\n+\n+    splitter = _get_splitter(tokenizer, batch_max_tokens)\n+\n+    semaphore: asyncio.Semaphore = asyncio.Semaphore(num_threads)\n+\n+    # Break up the input texts. The sizes here indicate how many snippets are in each input text\n+    texts, input_sizes = _prepare_embed_texts(input, splitter)\n+    text_batches = _create_text_batches(\n+        texts,\n+        batch_size,\n+        batch_max_tokens,\n+        splitter,\n+    )\n+    logger.info(\n+        \"embedding %d inputs via %d snippets using %d batches. max_batch_size=%d, batch_max_tokens=%d\",\n+        len(input),\n+        len(texts),\n+        len(text_batches),\n+        batch_size,\n+        batch_max_tokens,\n+    )\n+    ticker = progress_ticker(\n+        callbacks.progress,\n+        len(text_batches),\n+        description=\"generate embeddings progress: \",\n+    )\n+\n+    # Embed each chunk of snippets\n+    embeddings = await _execute(model, text_batches, ticker, semaphore)\n+    embeddings = _reconstitute_embeddings(embeddings, input_sizes)\n+\n+    return TextEmbeddingResult(embeddings=embeddings)\n+\n+\n+def _get_splitter(tokenizer: Tokenizer, batch_max_tokens: int) -> TokenTextSplitter:\n+    return TokenTextSplitter(\n+        tokenizer=tokenizer,\n+        chunk_size=batch_max_tokens,\n+    )\n+\n+\n+async def _execute(\n+    model: EmbeddingModel,\n+    chunks: list[list[str]],\n+    tick: ProgressTicker,\n+    semaphore: asyncio.Semaphore,\n+) -> list[list[float]]:\n+    async def embed(chunk: list[str]):\n+        async with semaphore:\n+            chunk_embeddings = await model.aembed_batch(chunk)\n+            result = np.array(chunk_embeddings)\n+            tick(1)\n+        return result\n+\n+    futures = [embed(chunk) for chunk in chunks]\n+    results = await asyncio.gather(*futures)\n+    # merge results in a single list of lists (reduce the collect dimension)\n+    return [item for sublist in results for item in sublist]\n+\n+\n+def _create_text_batches(\n+    texts: list[str],\n+    max_batch_size: int,\n+    max_batch_tokens: int,\n+    splitter: TokenTextSplitter,\n+) -> list[list[str]]:\n+    \"\"\"Create batches of texts to embed.\"\"\"\n+    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference\n+    # According to this embeddings reference, Azure limits us to 16 concurrent embeddings and 8191 tokens per request\n+    result = []\n+    current_batch = []\n+    current_batch_tokens = 0\n+\n+    for text in texts:\n+        token_count = splitter.num_tokens(text)\n+        if (\n+            len(current_batch) >= max_batch_size\n+            or current_batch_tokens + token_count > max_batch_tokens\n+        ):\n+            result.append(current_batch)\n+            current_batch = []\n+            current_batch_tokens = 0\n+\n+        current_batch.append(text)\n+        current_batch_tokens += token_count\n+\n+    if len(current_batch) > 0:\n+        result.append(current_batch)\n+\n+    return result\n+\n+\n+def _prepare_embed_texts(\n+    input: list[str], splitter: TokenTextSplitter\n+) -> tuple[list[str], list[int]]:\n+    sizes: list[int] = []\n+    snippets: list[str] = []\n+\n+    for text in input:\n+        # Split the input text and filter out any empty content\n+        split_texts = splitter.split_text(text)\n+        if split_texts is None:\n+            continue\n+        split_texts = [text for text in split_texts if len(text) > 0]\n+\n+        sizes.append(len(split_texts))\n+        snippets.extend(split_texts)\n+\n+    return snippets, sizes\n+\n+\n+def _reconstitute_embeddings(\n+    raw_embeddings: list[list[float]], sizes: list[int]\n+) -> list[list[float] | None]:\n+    \"\"\"Reconstitute the embeddings into the original input texts.\"\"\"\n+    embeddings: list[list[float] | None] = []\n+    cursor = 0\n+    for size in sizes:\n+        if size == 0:\n+            embeddings.append(None)\n+        elif size == 1:\n+            embedding = raw_embeddings[cursor]\n+            embeddings.append(embedding)\n+            cursor += 1\n+        else:\n+            chunk = raw_embeddings[cursor : cursor + size]\n+            average = np.average(chunk, axis=0)\n+            normalized = average / np.linalg.norm(average)\n+            embeddings.append(normalized.tolist())\n+            cursor += size\n+    return embeddings\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_covariates/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_covariates/__init__.py b/packages/graphrag/graphrag/index/operations/extract_covariates/__init__.py\nnew file mode 100644\nindex 0000000..e130668\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_covariates/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine text extract claims package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_covariates/claim_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_covariates/claim_extractor.py b/packages/graphrag/graphrag/index/operations/extract_covariates/claim_extractor.py\nnew file mode 100644\nindex 0000000..99db5d7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_covariates/claim_extractor.py\n@@ -0,0 +1,182 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'ClaimExtractorResult' and 'ClaimExtractor' models.\"\"\"\n+\n+import logging\n+import traceback\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.config.defaults import graphrag_config_defaults\n+from graphrag.index.typing.error_handler import ErrorHandlerFn\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompts.index.extract_claims import (\n+    CONTINUE_PROMPT,\n+    LOOP_PROMPT,\n+)\n+\n+INPUT_TEXT_KEY = \"input_text\"\n+INPUT_ENTITY_SPEC_KEY = \"entity_specs\"\n+INPUT_CLAIM_DESCRIPTION_KEY = \"claim_description\"\n+INPUT_RESOLVED_ENTITIES_KEY = \"resolved_entities\"\n+RECORD_DELIMITER_KEY = \"record_delimiter\"\n+COMPLETION_DELIMITER_KEY = \"completion_delimiter\"\n+TUPLE_DELIMITER = \"<|>\"\n+RECORD_DELIMITER = \"##\"\n+COMPLETION_DELIMITER = \"<|COMPLETE|>\"\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass\n+class ClaimExtractorResult:\n+    \"\"\"Claim extractor result class definition.\"\"\"\n+\n+    output: list[dict]\n+    source_docs: dict[str, Any]\n+\n+\n+class ClaimExtractor:\n+    \"\"\"Claim extractor class definition.\"\"\"\n+\n+    _model: ChatModel\n+    _extraction_prompt: str\n+    _max_gleanings: int\n+    _on_error: ErrorHandlerFn\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        extraction_prompt: str,\n+        max_gleanings: int | None = None,\n+        on_error: ErrorHandlerFn | None = None,\n+    ):\n+        \"\"\"Init method definition.\"\"\"\n+        self._model = model\n+        self._extraction_prompt = extraction_prompt\n+        self._max_gleanings = (\n+            max_gleanings\n+            if max_gleanings is not None\n+            else graphrag_config_defaults.extract_claims.max_gleanings\n+        )\n+        self._on_error = on_error or (lambda _e, _s, _d: None)\n+\n+    async def __call__(\n+        self,\n+        texts,\n+        entity_spec,\n+        resolved_entities,\n+        claim_description,\n+    ) -> ClaimExtractorResult:\n+        \"\"\"Call method definition.\"\"\"\n+        source_doc_map = {}\n+        all_claims: list[dict] = []\n+        for doc_index, text in enumerate(texts):\n+            document_id = f\"d{doc_index}\"\n+            try:\n+                claims = await self._process_document(\n+                    text, claim_description, entity_spec\n+                )\n+                all_claims += [\n+                    self._clean_claim(c, document_id, resolved_entities) for c in claims\n+                ]\n+                source_doc_map[document_id] = text\n+            except Exception as e:\n+                logger.exception(\"error extracting claim\")\n+                self._on_error(\n+                    e,\n+                    traceback.format_exc(),\n+                    {\"doc_index\": doc_index, \"text\": text},\n+                )\n+                continue\n+\n+        return ClaimExtractorResult(\n+            output=all_claims,\n+            source_docs=source_doc_map,\n+        )\n+\n+    def _clean_claim(\n+        self, claim: dict, document_id: str, resolved_entities: dict\n+    ) -> dict:\n+        # clean the parsed claims to remove any claims with status = False\n+        obj = claim.get(\"object_id\", claim.get(\"object\"))\n+        subject = claim.get(\"subject_id\", claim.get(\"subject\"))\n+\n+        # If subject or object in resolved entities, then replace with resolved entity\n+        obj = resolved_entities.get(obj, obj)\n+        subject = resolved_entities.get(subject, subject)\n+        claim[\"object_id\"] = obj\n+        claim[\"subject_id\"] = subject\n+        return claim\n+\n+    async def _process_document(\n+        self, text: str, claim_description: str, entity_spec: dict\n+    ) -> list[dict]:\n+        response = await self._model.achat(\n+            self._extraction_prompt.format(**{\n+                INPUT_TEXT_KEY: text,\n+                INPUT_CLAIM_DESCRIPTION_KEY: claim_description,\n+                INPUT_ENTITY_SPEC_KEY: entity_spec,\n+            })\n+        )\n+        results = response.output.content or \"\"\n+        claims = results.strip().removesuffix(COMPLETION_DELIMITER)\n+\n+        # if gleanings are specified, enter a loop to extract more claims\n+        # there are two exit criteria: (a) we hit the configured max, (b) the model says there are no more claims\n+        if self._max_gleanings > 0:\n+            for i in range(self._max_gleanings):\n+                response = await self._model.achat(\n+                    CONTINUE_PROMPT,\n+                    name=f\"extract-continuation-{i}\",\n+                    history=response.history,\n+                )\n+                extension = response.output.content or \"\"\n+                claims += RECORD_DELIMITER + extension.strip().removesuffix(\n+                    COMPLETION_DELIMITER\n+                )\n+\n+                # If this isn't the last loop, check to see if we should continue\n+                if i >= self._max_gleanings - 1:\n+                    break\n+\n+                response = await self._model.achat(\n+                    LOOP_PROMPT,\n+                    name=f\"extract-loopcheck-{i}\",\n+                    history=response.history,\n+                )\n+\n+                if response.output.content != \"Y\":\n+                    break\n+\n+        return self._parse_claim_tuples(results)\n+\n+    def _parse_claim_tuples(self, claims: str) -> list[dict[str, Any]]:\n+        \"\"\"Parse claim tuples.\"\"\"\n+\n+        def pull_field(index: int, fields: list[str]) -> str | None:\n+            return fields[index].strip() if len(fields) > index else None\n+\n+        result: list[dict[str, Any]] = []\n+        claims_values = (\n+            claims.strip().removesuffix(COMPLETION_DELIMITER).split(RECORD_DELIMITER)\n+        )\n+        for claim in claims_values:\n+            claim = claim.strip().removeprefix(\"(\").removesuffix(\")\")\n+\n+            # Ignore the completion delimiter\n+            if claim == COMPLETION_DELIMITER:\n+                continue\n+\n+            claim_fields = claim.split(TUPLE_DELIMITER)\n+            result.append({\n+                \"subject_id\": pull_field(0, claim_fields),\n+                \"object_id\": pull_field(1, claim_fields),\n+                \"type\": pull_field(2, claim_fields),\n+                \"status\": pull_field(3, claim_fields),\n+                \"start_date\": pull_field(4, claim_fields),\n+                \"end_date\": pull_field(5, claim_fields),\n+                \"description\": pull_field(6, claim_fields),\n+                \"source_text\": pull_field(7, claim_fields),\n+            })\n+        return result\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_covariates/extract_covariates.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_covariates/extract_covariates.py b/packages/graphrag/graphrag/index/operations/extract_covariates/extract_covariates.py\nnew file mode 100644\nindex 0000000..bc2e1fa\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_covariates/extract_covariates.py\n@@ -0,0 +1,119 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the extract_covariates verb definition.\"\"\"\n+\n+import logging\n+from collections.abc import Iterable\n+from dataclasses import asdict\n+from typing import Any\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.index.operations.extract_covariates.claim_extractor import ClaimExtractor\n+from graphrag.index.operations.extract_covariates.typing import (\n+    Covariate,\n+    CovariateExtractionResult,\n+)\n+from graphrag.index.utils.derive_from_rows import derive_from_rows\n+from graphrag.language_model.protocol.base import ChatModel\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def extract_covariates(\n+    input: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    column: str,\n+    covariate_type: str,\n+    max_gleanings: int,\n+    claim_description: str,\n+    prompt: str,\n+    entity_types: list[str],\n+    num_threads: int,\n+    async_type: AsyncType,\n+):\n+    \"\"\"Extract claims from a piece of text.\"\"\"\n+    resolved_entities_map = {}\n+\n+    async def run_strategy(row):\n+        text = row[column]\n+        result = await run_extract_claims(\n+            input=text,\n+            entity_types=entity_types,\n+            resolved_entities_map=resolved_entities_map,\n+            model=model,\n+            max_gleanings=max_gleanings,\n+            claim_description=claim_description,\n+            prompt=prompt,\n+        )\n+        return [\n+            create_row_from_claim_data(row, item, covariate_type)\n+            for item in result.covariate_data\n+        ]\n+\n+    results = await derive_from_rows(\n+        input,\n+        run_strategy,\n+        callbacks,\n+        num_threads=num_threads,\n+        async_type=async_type,\n+        progress_msg=\"extract covariates progress: \",\n+    )\n+    return pd.DataFrame([item for row in results for item in row or []])\n+\n+\n+def create_row_from_claim_data(row, covariate_data: Covariate, covariate_type: str):\n+    \"\"\"Create a row from the claim data and the input row.\"\"\"\n+    return {**row, **asdict(covariate_data), \"covariate_type\": covariate_type}\n+\n+\n+async def run_extract_claims(\n+    input: str | Iterable[str],\n+    entity_types: list[str],\n+    resolved_entities_map: dict[str, str],\n+    model: ChatModel,\n+    max_gleanings: int,\n+    claim_description: str,\n+    prompt: str,\n+) -> CovariateExtractionResult:\n+    \"\"\"Run the Claim extraction chain.\"\"\"\n+    extractor = ClaimExtractor(\n+        model=model,\n+        extraction_prompt=prompt,\n+        max_gleanings=max_gleanings,\n+        on_error=lambda e, s, d: logger.error(\n+            \"Claim Extraction Error\", exc_info=e, extra={\"stack\": s, \"details\": d}\n+        ),\n+    )\n+\n+    input = [input] if isinstance(input, str) else input\n+\n+    results = await extractor(\n+        texts=input,\n+        entity_spec=entity_types,\n+        resolved_entities=resolved_entities_map,\n+        claim_description=claim_description,\n+    )\n+\n+    claim_data = results.output\n+    return CovariateExtractionResult([create_covariate(item) for item in claim_data])\n+\n+\n+def create_covariate(item: dict[str, Any]) -> Covariate:\n+    \"\"\"Create a covariate from the item.\"\"\"\n+    return Covariate(\n+        subject_id=item.get(\"subject_id\"),\n+        object_id=item.get(\"object_id\"),\n+        type=item.get(\"type\"),\n+        status=item.get(\"status\"),\n+        start_date=item.get(\"start_date\"),\n+        end_date=item.get(\"end_date\"),\n+        description=item.get(\"description\"),\n+        source_text=item.get(\"source_text\"),\n+        record_id=item.get(\"record_id\"),\n+        id=item.get(\"id\"),\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_covariates/typing.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_covariates/typing.py b/packages/graphrag/graphrag/index/operations/extract_covariates/typing.py\nnew file mode 100644\nindex 0000000..a524b2b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_covariates/typing.py\n@@ -0,0 +1,49 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'Covariate' and 'CovariateExtractionResult' models.\"\"\"\n+\n+from collections.abc import Awaitable, Callable, Iterable\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+\n+\n+@dataclass\n+class Covariate:\n+    \"\"\"Covariate class definition.\"\"\"\n+\n+    covariate_type: str | None = None\n+    subject_id: str | None = None\n+    object_id: str | None = None\n+    type: str | None = None\n+    status: str | None = None\n+    start_date: str | None = None\n+    end_date: str | None = None\n+    description: str | None = None\n+    source_text: list[str] | None = None\n+    doc_id: str | None = None\n+    record_id: int | None = None\n+    id: str | None = None\n+\n+\n+@dataclass\n+class CovariateExtractionResult:\n+    \"\"\"Covariate extraction result class definition.\"\"\"\n+\n+    covariate_data: list[Covariate]\n+\n+\n+CovariateExtractStrategy = Callable[\n+    [\n+        Iterable[str],\n+        list[str],\n+        dict[str, str],\n+        WorkflowCallbacks,\n+        PipelineCache,\n+        dict[str, Any],\n+    ],\n+    Awaitable[CovariateExtractionResult],\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_graph/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_graph/__init__.py b/packages/graphrag/graphrag/index/operations/extract_graph/__init__.py\nnew file mode 100644\nindex 0000000..52da158\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_graph/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine entities extraction package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_graph/extract_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_graph/extract_graph.py b/packages/graphrag/graphrag/index/operations/extract_graph/extract_graph.py\nnew file mode 100644\nindex 0000000..3aa8740\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_graph/extract_graph.py\n@@ -0,0 +1,147 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing entity_extract methods.\"\"\"\n+\n+import logging\n+\n+import networkx as nx\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.index.operations.extract_graph.graph_extractor import GraphExtractor\n+from graphrag.index.operations.extract_graph.typing import (\n+    Document,\n+    EntityExtractionResult,\n+    EntityTypes,\n+)\n+from graphrag.index.utils.derive_from_rows import derive_from_rows\n+from graphrag.language_model.protocol.base import ChatModel\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def extract_graph(\n+    text_units: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    text_column: str,\n+    id_column: str,\n+    model: ChatModel,\n+    prompt: str,\n+    entity_types: list[str],\n+    max_gleanings: int,\n+    num_threads: int,\n+    async_type: AsyncType,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"Extract a graph from a piece of text using a language model.\"\"\"\n+    num_started = 0\n+\n+    async def run_strategy(row):\n+        nonlocal num_started\n+        text = row[text_column]\n+        id = row[id_column]\n+        result = await run_extract_graph(\n+            [Document(text=text, id=id)],\n+            entity_types,\n+            model,\n+            prompt,\n+            max_gleanings,\n+        )\n+        num_started += 1\n+        return [result.entities, result.relationships, result.graph]\n+\n+    results = await derive_from_rows(\n+        text_units,\n+        run_strategy,\n+        callbacks,\n+        num_threads=num_threads,\n+        async_type=async_type,\n+        progress_msg=\"extract graph progress: \",\n+    )\n+\n+    entity_dfs = []\n+    relationship_dfs = []\n+    for result in results:\n+        if result:\n+            entity_dfs.append(pd.DataFrame(result[0]))\n+            relationship_dfs.append(pd.DataFrame(result[1]))\n+\n+    entities = _merge_entities(entity_dfs)\n+    relationships = _merge_relationships(relationship_dfs)\n+\n+    return (entities, relationships)\n+\n+\n+async def run_extract_graph(\n+    docs: list[Document],\n+    entity_types: EntityTypes,\n+    model: ChatModel,\n+    prompt: str,\n+    max_gleanings: int,\n+) -> EntityExtractionResult:\n+    \"\"\"Run the graph intelligence entity extraction strategy.\"\"\"\n+    extractor = GraphExtractor(\n+        model=model,\n+        prompt=prompt,\n+        max_gleanings=max_gleanings,\n+        on_error=lambda e, s, d: logger.error(\n+            \"Entity Extraction Error\", exc_info=e, extra={\"stack\": s, \"details\": d}\n+        ),\n+    )\n+    text_list = [doc.text.strip() for doc in docs]\n+\n+    results = await extractor(\n+        list(text_list),\n+        entity_types=entity_types,\n+    )\n+\n+    graph = results.output\n+    # Map the \"source_id\" back to the \"id\" field\n+    for _, node in graph.nodes(data=True):  # type: ignore\n+        if node is not None:\n+            node[\"source_id\"] = \",\".join(\n+                docs[int(id)].id for id in node[\"source_id\"].split(\",\")\n+            )\n+\n+    for _, _, edge in graph.edges(data=True):  # type: ignore\n+        if edge is not None:\n+            edge[\"source_id\"] = \",\".join(\n+                docs[int(id)].id for id in edge[\"source_id\"].split(\",\")\n+            )\n+\n+    entities = [\n+        ({\"title\": item[0], **(item[1] or {})})\n+        for item in graph.nodes(data=True)\n+        if item is not None\n+    ]\n+\n+    relationships = nx.to_pandas_edgelist(graph)\n+\n+    return EntityExtractionResult(entities, relationships, graph)\n+\n+\n+def _merge_entities(entity_dfs) -> pd.DataFrame:\n+    all_entities = pd.concat(entity_dfs, ignore_index=True)\n+    return (\n+        all_entities.groupby([\"title\", \"type\"], sort=False)\n+        .agg(\n+            description=(\"description\", list),\n+            text_unit_ids=(\"source_id\", list),\n+            frequency=(\"source_id\", \"count\"),\n+        )\n+        .reset_index()\n+    )\n+\n+\n+def _merge_relationships(relationship_dfs) -> pd.DataFrame:\n+    all_relationships = pd.concat(relationship_dfs, ignore_index=False)\n+    return (\n+        all_relationships.groupby([\"source\", \"target\"], sort=False)\n+        .agg(\n+            description=(\"description\", list),\n+            text_unit_ids=(\"source_id\", list),\n+            weight=(\"weight\", \"sum\"),\n+        )\n+        .reset_index()\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_graph/graph_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_graph/graph_extractor.py b/packages/graphrag/graphrag/index/operations/extract_graph/graph_extractor.py\nnew file mode 100644\nindex 0000000..8e98eb7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_graph/graph_extractor.py\n@@ -0,0 +1,257 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'GraphExtractionResult' and 'GraphExtractor' models.\"\"\"\n+\n+import logging\n+import re\n+import traceback\n+from collections.abc import Mapping\n+from dataclasses import dataclass\n+from typing import Any\n+\n+import networkx as nx\n+\n+from graphrag.index.typing.error_handler import ErrorHandlerFn\n+from graphrag.index.utils.string import clean_str\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompts.index.extract_graph import (\n+    CONTINUE_PROMPT,\n+    LOOP_PROMPT,\n+)\n+\n+INPUT_TEXT_KEY = \"input_text\"\n+RECORD_DELIMITER_KEY = \"record_delimiter\"\n+COMPLETION_DELIMITER_KEY = \"completion_delimiter\"\n+ENTITY_TYPES_KEY = \"entity_types\"\n+TUPLE_DELIMITER = \"<|>\"\n+RECORD_DELIMITER = \"##\"\n+COMPLETION_DELIMITER = \"<|COMPLETE|>\"\n+DEFAULT_ENTITY_TYPES = [\"organization\", \"person\", \"geo\", \"event\"]\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass\n+class GraphExtractionResult:\n+    \"\"\"Unipartite graph extraction result class definition.\"\"\"\n+\n+    output: nx.Graph\n+    source_docs: dict[Any, Any]\n+\n+\n+class GraphExtractor:\n+    \"\"\"Unipartite graph extractor class definition.\"\"\"\n+\n+    _model: ChatModel\n+    _join_descriptions: bool\n+    _extraction_prompt: str\n+    _max_gleanings: int\n+    _on_error: ErrorHandlerFn\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        prompt: str,\n+        max_gleanings: int,\n+        join_descriptions=True,\n+        on_error: ErrorHandlerFn | None = None,\n+    ):\n+        \"\"\"Init method definition.\"\"\"\n+        self._model = model\n+        self._join_descriptions = join_descriptions\n+        self._extraction_prompt = prompt\n+        self._max_gleanings = max_gleanings\n+        self._on_error = on_error or (lambda _e, _s, _d: None)\n+\n+    async def __call__(\n+        self, texts: list[str], entity_types: list[str]\n+    ) -> GraphExtractionResult:\n+        \"\"\"Call method definition.\"\"\"\n+        all_records: dict[int, str] = {}\n+        source_doc_map: dict[int, str] = {}\n+\n+        for doc_index, text in enumerate(texts):\n+            try:\n+                # Invoke the entity extraction\n+                result = await self._process_document(text, entity_types)\n+                source_doc_map[doc_index] = text\n+                all_records[doc_index] = result\n+            except Exception as e:\n+                logger.exception(\"error extracting graph\")\n+                self._on_error(\n+                    e,\n+                    traceback.format_exc(),\n+                    {\n+                        \"doc_index\": doc_index,\n+                        \"text\": text,\n+                    },\n+                )\n+\n+        output = await self._process_results(\n+            all_records,\n+            TUPLE_DELIMITER,\n+            RECORD_DELIMITER,\n+        )\n+\n+        return GraphExtractionResult(\n+            output=output,\n+            source_docs=source_doc_map,\n+        )\n+\n+    async def _process_document(self, text: str, entity_types: list[str]) -> str:\n+        response = await self._model.achat(\n+            self._extraction_prompt.format(**{\n+                INPUT_TEXT_KEY: text,\n+                ENTITY_TYPES_KEY: \",\".join(entity_types),\n+            }),\n+        )\n+        results = response.output.content or \"\"\n+\n+        # if gleanings are specified, enter a loop to extract more entities\n+        # there are two exit criteria: (a) we hit the configured max, (b) the model says there are no more entities\n+        if self._max_gleanings > 0:\n+            for i in range(self._max_gleanings):\n+                response = await self._model.achat(\n+                    CONTINUE_PROMPT,\n+                    name=f\"extract-continuation-{i}\",\n+                    history=response.history,\n+                )\n+                results += response.output.content or \"\"\n+\n+                # if this is the final glean, don't bother updating the continuation flag\n+                if i >= self._max_gleanings - 1:\n+                    break\n+\n+                response = await self._model.achat(\n+                    LOOP_PROMPT,\n+                    name=f\"extract-loopcheck-{i}\",\n+                    history=response.history,\n+                )\n+                if response.output.content != \"Y\":\n+                    break\n+\n+        return results\n+\n+    async def _process_results(\n+        self,\n+        results: dict[int, str],\n+        tuple_delimiter: str,\n+        record_delimiter: str,\n+    ) -> nx.Graph:\n+        \"\"\"Parse the result string to create an undirected unipartite graph.\n+\n+        Args:\n+            - results - dict of results from the extraction chain\n+            - tuple_delimiter - delimiter between tuples in an output record, default is '<|>'\n+            - record_delimiter - delimiter between records, default is '##'\n+        Returns:\n+            - output - unipartite graph in graphML format\n+        \"\"\"\n+        graph = nx.Graph()\n+        for source_doc_id, extracted_data in results.items():\n+            records = [r.strip() for r in extracted_data.split(record_delimiter)]\n+\n+            for record in records:\n+                record = re.sub(r\"^\\(|\\)$\", \"\", record.strip())\n+                record_attributes = record.split(tuple_delimiter)\n+\n+                if record_attributes[0] == '\"entity\"' and len(record_attributes) >= 4:\n+                    # add this record as a node in the G\n+                    entity_name = clean_str(record_attributes[1].upper())\n+                    entity_type = clean_str(record_attributes[2].upper())\n+                    entity_description = clean_str(record_attributes[3])\n+\n+                    if entity_name in graph.nodes():\n+                        node = graph.nodes[entity_name]\n+                        if self._join_descriptions:\n+                            node[\"description\"] = \"\\n\".join(\n+                                list({\n+                                    *_unpack_descriptions(node),\n+                                    entity_description,\n+                                })\n+                            )\n+                        else:\n+                            if len(entity_description) > len(node[\"description\"]):\n+                                node[\"description\"] = entity_description\n+                        node[\"source_id\"] = \", \".join(\n+                            list({\n+                                *_unpack_source_ids(node),\n+                                str(source_doc_id),\n+                            })\n+                        )\n+                        node[\"type\"] = (\n+                            entity_type if entity_type != \"\" else node[\"type\"]\n+                        )\n+                    else:\n+                        graph.add_node(\n+                            entity_name,\n+                            type=entity_type,\n+                            description=entity_description,\n+                            source_id=str(source_doc_id),\n+                        )\n+\n+                if (\n+                    record_attributes[0] == '\"relationship\"'\n+                    and len(record_attributes) >= 5\n+                ):\n+                    # add this record as edge\n+                    source = clean_str(record_attributes[1].upper())\n+                    target = clean_str(record_attributes[2].upper())\n+                    edge_description = clean_str(record_attributes[3])\n+                    edge_source_id = clean_str(str(source_doc_id))\n+                    try:\n+                        weight = float(record_attributes[-1])\n+                    except ValueError:\n+                        weight = 1.0\n+\n+                    if source not in graph.nodes():\n+                        graph.add_node(\n+                            source,\n+                            type=\"\",\n+                            description=\"\",\n+                            source_id=edge_source_id,\n+                        )\n+                    if target not in graph.nodes():\n+                        graph.add_node(\n+                            target,\n+                            type=\"\",\n+                            description=\"\",\n+                            source_id=edge_source_id,\n+                        )\n+                    if graph.has_edge(source, target):\n+                        edge_data = graph.get_edge_data(source, target)\n+                        if edge_data is not None:\n+                            weight += edge_data[\"weight\"]\n+                            if self._join_descriptions:\n+                                edge_description = \"\\n\".join(\n+                                    list({\n+                                        *_unpack_descriptions(edge_data),\n+                                        edge_description,\n+                                    })\n+                                )\n+                            edge_source_id = \", \".join(\n+                                list({\n+                                    *_unpack_source_ids(edge_data),\n+                                    str(source_doc_id),\n+                                })\n+                            )\n+                    graph.add_edge(\n+                        source,\n+                        target,\n+                        weight=weight,\n+                        description=edge_description,\n+                        source_id=edge_source_id,\n+                    )\n+\n+        return graph\n+\n+\n+def _unpack_descriptions(data: Mapping) -> list[str]:\n+    value = data.get(\"description\", None)\n+    return [] if value is None else value.split(\"\\n\")\n+\n+\n+def _unpack_source_ids(data: Mapping) -> list[str]:\n+    value = data.get(\"source_id\", None)\n+    return [] if value is None else value.split(\", \")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/extract_graph/typing.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/extract_graph/typing.py b/packages/graphrag/graphrag/index/operations/extract_graph/typing.py\nnew file mode 100644\nindex 0000000..d74eb9a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/extract_graph/typing.py\n@@ -0,0 +1,45 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'Document' and 'EntityExtractionResult' models.\"\"\"\n+\n+from collections.abc import Awaitable, Callable\n+from dataclasses import dataclass\n+from typing import Any\n+\n+import networkx as nx\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+\n+ExtractedEntity = dict[str, Any]\n+ExtractedRelationship = dict[str, Any]\n+StrategyConfig = dict[str, Any]\n+EntityTypes = list[str]\n+\n+\n+@dataclass\n+class Document:\n+    \"\"\"Document class definition.\"\"\"\n+\n+    text: str\n+    id: str\n+\n+\n+@dataclass\n+class EntityExtractionResult:\n+    \"\"\"Entity extraction result class definition.\"\"\"\n+\n+    entities: list[ExtractedEntity]\n+    relationships: list[ExtractedRelationship]\n+    graph: nx.Graph | None\n+\n+\n+EntityExtractStrategy = Callable[\n+    [\n+        list[Document],\n+        EntityTypes,\n+        PipelineCache,\n+        StrategyConfig,\n+    ],\n+    Awaitable[EntityExtractionResult],\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/finalize_community_reports.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/finalize_community_reports.py b/packages/graphrag/graphrag/index/operations/finalize_community_reports.py\nnew file mode 100644\nindex 0000000..124e17e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/finalize_community_reports.py\n@@ -0,0 +1,33 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"All the steps to transform final entities.\"\"\"\n+\n+from uuid import uuid4\n+\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import COMMUNITY_REPORTS_FINAL_COLUMNS\n+\n+\n+def finalize_community_reports(\n+    reports: pd.DataFrame,\n+    communities: pd.DataFrame,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform final community reports.\"\"\"\n+    # Merge with communities to add shared fields\n+    community_reports = reports.merge(\n+        communities.loc[:, [\"community\", \"parent\", \"children\", \"size\", \"period\"]],\n+        on=\"community\",\n+        how=\"left\",\n+        copy=False,\n+    )\n+\n+    community_reports[\"community\"] = community_reports[\"community\"].astype(int)\n+    community_reports[\"human_readable_id\"] = community_reports[\"community\"]\n+    community_reports[\"id\"] = [uuid4().hex for _ in range(len(community_reports))]\n+\n+    return community_reports.loc[\n+        :,\n+        COMMUNITY_REPORTS_FINAL_COLUMNS,\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/finalize_entities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/finalize_entities.py b/packages/graphrag/graphrag/index/operations/finalize_entities.py\nnew file mode 100644\nindex 0000000..28ac2a5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/finalize_entities.py\n@@ -0,0 +1,36 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"All the steps to transform final entities.\"\"\"\n+\n+from uuid import uuid4\n+\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import ENTITIES_FINAL_COLUMNS\n+from graphrag.index.operations.compute_degree import compute_degree\n+from graphrag.index.operations.create_graph import create_graph\n+\n+\n+def finalize_entities(\n+    entities: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform final entities.\"\"\"\n+    graph = create_graph(relationships, edge_attr=[\"weight\"])\n+    degrees = compute_degree(graph)\n+    final_entities = entities.merge(degrees, on=\"title\", how=\"left\").drop_duplicates(\n+        subset=\"title\"\n+    )\n+    final_entities = final_entities.loc[entities[\"title\"].notna()].reset_index()\n+    # disconnected nodes and those with no community even at level 0 can be missing degree\n+    final_entities[\"degree\"] = final_entities[\"degree\"].fillna(0).astype(int)\n+    final_entities.reset_index(inplace=True)\n+    final_entities[\"human_readable_id\"] = final_entities.index\n+    final_entities[\"id\"] = final_entities[\"human_readable_id\"].apply(\n+        lambda _x: str(uuid4())\n+    )\n+    return final_entities.loc[\n+        :,\n+        ENTITIES_FINAL_COLUMNS,\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/finalize_relationships.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/finalize_relationships.py b/packages/graphrag/graphrag/index/operations/finalize_relationships.py\nnew file mode 100644\nindex 0000000..21ba413\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/finalize_relationships.py\n@@ -0,0 +1,44 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"All the steps to transform final relationships.\"\"\"\n+\n+from uuid import uuid4\n+\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import RELATIONSHIPS_FINAL_COLUMNS\n+from graphrag.index.operations.compute_degree import compute_degree\n+from graphrag.index.operations.compute_edge_combined_degree import (\n+    compute_edge_combined_degree,\n+)\n+from graphrag.index.operations.create_graph import create_graph\n+\n+\n+def finalize_relationships(\n+    relationships: pd.DataFrame,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform final relationships.\"\"\"\n+    graph = create_graph(relationships, edge_attr=[\"weight\"])\n+    degrees = compute_degree(graph)\n+\n+    final_relationships = relationships.drop_duplicates(subset=[\"source\", \"target\"])\n+    final_relationships[\"combined_degree\"] = compute_edge_combined_degree(\n+        final_relationships,\n+        degrees,\n+        node_name_column=\"title\",\n+        node_degree_column=\"degree\",\n+        edge_source_column=\"source\",\n+        edge_target_column=\"target\",\n+    )\n+\n+    final_relationships.reset_index(inplace=True)\n+    final_relationships[\"human_readable_id\"] = final_relationships.index\n+    final_relationships[\"id\"] = final_relationships[\"human_readable_id\"].apply(\n+        lambda _x: str(uuid4())\n+    )\n+\n+    return final_relationships.loc[\n+        :,\n+        RELATIONSHIPS_FINAL_COLUMNS,\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/graph_to_dataframes.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/graph_to_dataframes.py b/packages/graphrag/graphrag/index/operations/graph_to_dataframes.py\nnew file mode 100644\nindex 0000000..dbc608f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/graph_to_dataframes.py\n@@ -0,0 +1,38 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing create_graph definition.\"\"\"\n+\n+import networkx as nx\n+import pandas as pd\n+\n+\n+def graph_to_dataframes(\n+    graph: nx.Graph,\n+    node_columns: list[str] | None = None,\n+    edge_columns: list[str] | None = None,\n+    node_id: str = \"title\",\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"Deconstructs an nx.Graph into nodes and edges dataframes.\"\"\"\n+    # nx graph nodes are a tuple, and creating a df from them results in the id being the index\n+    nodes = pd.DataFrame.from_dict(dict(graph.nodes(data=True)), orient=\"index\")\n+    nodes[node_id] = nodes.index\n+    nodes.reset_index(inplace=True, drop=True)\n+\n+    edges = nx.to_pandas_edgelist(graph)\n+\n+    # we don't deal in directed graphs, but we do need to ensure consistent ordering for df joins\n+    # nx loses the initial ordering\n+    edges[\"min_source\"] = edges[[\"source\", \"target\"]].min(axis=1)\n+    edges[\"max_target\"] = edges[[\"source\", \"target\"]].max(axis=1)\n+    edges = edges.drop(columns=[\"source\", \"target\"]).rename(\n+        columns={\"min_source\": \"source\", \"max_target\": \"target\"}  # type: ignore\n+    )\n+\n+    if node_columns:\n+        nodes = nodes.loc[:, node_columns]\n+\n+    if edge_columns:\n+        edges = edges.loc[:, edge_columns]\n+\n+    return (nodes, edges)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/prune_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/prune_graph.py b/packages/graphrag/graphrag/index/operations/prune_graph.py\nnew file mode 100644\nindex 0000000..b03dfb7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/prune_graph.py\n@@ -0,0 +1,92 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Graph pruning.\"\"\"\n+\n+from typing import TYPE_CHECKING, cast\n+\n+import networkx as nx\n+import numpy as np\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.index.utils.graphs import largest_connected_component\n+\n+if TYPE_CHECKING:\n+    from networkx.classes.reportviews import DegreeView\n+\n+\n+def prune_graph(\n+    graph: nx.Graph,\n+    min_node_freq: int = 1,\n+    max_node_freq_std: float | None = None,\n+    min_node_degree: int = 1,\n+    max_node_degree_std: float | None = None,\n+    min_edge_weight_pct: float = 40,\n+    remove_ego_nodes: bool = False,\n+    lcc_only: bool = False,\n+) -> nx.Graph:\n+    \"\"\"Prune graph by removing nodes that are out of frequency/degree ranges and edges with low weights.\"\"\"\n+    # remove ego nodes if needed\n+    degree = cast(\"DegreeView\", graph.degree)\n+    degrees = list(degree())  # type: ignore\n+    if remove_ego_nodes:\n+        # ego node is one with highest degree\n+        ego_node = max(degrees, key=lambda x: x[1])\n+        graph.remove_nodes_from([ego_node[0]])\n+\n+    # remove nodes that are not within the predefined degree range\n+    graph.remove_nodes_from([\n+        node for node, degree in degrees if degree < min_node_degree\n+    ])\n+    if max_node_degree_std is not None:\n+        upper_threshold = _get_upper_threshold_by_std(\n+            [degree for _, degree in degrees], max_node_degree_std\n+        )\n+        graph.remove_nodes_from([\n+            node for node, degree in degrees if degree > upper_threshold\n+        ])\n+\n+    # remove nodes that are not within the predefined frequency range\n+    graph.remove_nodes_from([\n+        node\n+        for node, data in graph.nodes(data=True)\n+        if data[schemas.NODE_FREQUENCY] < min_node_freq\n+    ])\n+    if max_node_freq_std is not None:\n+        upper_threshold = _get_upper_threshold_by_std(\n+            [data[schemas.NODE_FREQUENCY] for _, data in graph.nodes(data=True)],\n+            max_node_freq_std,\n+        )\n+        graph.remove_nodes_from([\n+            node\n+            for node, data in graph.nodes(data=True)\n+            if data[schemas.NODE_FREQUENCY] > upper_threshold\n+        ])\n+\n+    # remove edges by min weight\n+    if min_edge_weight_pct > 0:\n+        min_edge_weight = np.percentile(\n+            [data[schemas.EDGE_WEIGHT] for _, _, data in graph.edges(data=True)],\n+            min_edge_weight_pct,\n+        )\n+        graph.remove_edges_from([\n+            (source, target)\n+            for source, target, data in graph.edges(data=True)\n+            if source in graph.nodes()\n+            and target in graph.nodes()\n+            and data[schemas.EDGE_WEIGHT] < min_edge_weight\n+        ])\n+\n+    if lcc_only:\n+        return largest_connected_component(graph)\n+\n+    return graph\n+\n+\n+def _get_upper_threshold_by_std(\n+    data: list[float] | list[int], std_trim: float\n+) -> float:\n+    \"\"\"Get upper threshold by standard deviation.\"\"\"\n+    mean = np.mean(data)\n+    std = np.std(data)\n+    return mean + std_trim * std  # type: ignore\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/snapshot_graphml.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/snapshot_graphml.py b/packages/graphrag/graphrag/index/operations/snapshot_graphml.py\nnew file mode 100644\nindex 0000000..c1eb9b0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/snapshot_graphml.py\n@@ -0,0 +1,18 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing snapshot_graphml method definition.\"\"\"\n+\n+import networkx as nx\n+\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+\n+async def snapshot_graphml(\n+    input: str | nx.Graph,\n+    name: str,\n+    storage: PipelineStorage,\n+) -> None:\n+    \"\"\"Take a entire snapshot of a graph to standard graphml format.\"\"\"\n+    graphml = input if isinstance(input, str) else \"\\n\".join(nx.generate_graphml(input))\n+    await storage.set(name + \".graphml\", graphml)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/__init__.py b/packages/graphrag/graphrag/index/operations/summarize_communities/__init__.py\nnew file mode 100644\nindex 0000000..e8745d8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Community summarization modules.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/build_mixed_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/build_mixed_context.py b/packages/graphrag/graphrag/index/operations/summarize_communities/build_mixed_context.py\nnew file mode 100644\nindex 0000000..0e0a4e3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/build_mixed_context.py\n@@ -0,0 +1,73 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"A module containing the build_mixed_context method definition.\"\"\"\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.index.operations.summarize_communities.graph_context.sort_context import (\n+    sort_context,\n+)\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+def build_mixed_context(\n+    context: list[dict], tokenizer: Tokenizer, max_context_tokens: int\n+) -> str:\n+    \"\"\"\n+    Build parent context by concatenating all sub-communities' contexts.\n+\n+    If the context exceeds the limit, we use sub-community reports instead.\n+    \"\"\"\n+    sorted_context = sorted(\n+        context, key=lambda x: x[schemas.CONTEXT_SIZE], reverse=True\n+    )\n+\n+    # replace local context with sub-community reports, starting from the biggest sub-community\n+    substitute_reports = []\n+    final_local_contexts = []\n+    exceeded_limit = True\n+    context_string = \"\"\n+\n+    for idx, sub_community_context in enumerate(sorted_context):\n+        if exceeded_limit:\n+            if sub_community_context[schemas.FULL_CONTENT]:\n+                substitute_reports.append({\n+                    schemas.COMMUNITY_ID: sub_community_context[schemas.SUB_COMMUNITY],\n+                    schemas.FULL_CONTENT: sub_community_context[schemas.FULL_CONTENT],\n+                })\n+            else:\n+                # this sub-community has no report, so we will use its local context\n+                final_local_contexts.extend(sub_community_context[schemas.ALL_CONTEXT])\n+                continue\n+\n+            # add local context for the remaining sub-communities\n+            remaining_local_context = []\n+            for rid in range(idx + 1, len(sorted_context)):\n+                remaining_local_context.extend(sorted_context[rid][schemas.ALL_CONTEXT])\n+            new_context_string = sort_context(\n+                local_context=remaining_local_context + final_local_contexts,\n+                tokenizer=tokenizer,\n+                sub_community_reports=substitute_reports,\n+            )\n+            if tokenizer.num_tokens(new_context_string) <= max_context_tokens:\n+                exceeded_limit = False\n+                context_string = new_context_string\n+                break\n+\n+    if exceeded_limit:\n+        # if all sub-community reports exceed the limit, we add reports until context is full\n+        substitute_reports = []\n+        for sub_community_context in sorted_context:\n+            substitute_reports.append({\n+                schemas.COMMUNITY_ID: sub_community_context[schemas.SUB_COMMUNITY],\n+                schemas.FULL_CONTENT: sub_community_context[schemas.FULL_CONTENT],\n+            })\n+            new_context_string = pd.DataFrame(substitute_reports).to_csv(\n+                index=False, sep=\",\"\n+            )\n+            if tokenizer.num_tokens(new_context_string) > max_context_tokens:\n+                break\n+\n+            context_string = new_context_string\n+    return context_string\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/community_reports_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/community_reports_extractor.py b/packages/graphrag/graphrag/index/operations/summarize_communities/community_reports_extractor.py\nnew file mode 100644\nindex 0000000..3ca2925\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/community_reports_extractor.py\n@@ -0,0 +1,101 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'CommunityReportsResult' and 'CommunityReportsExtractor' models.\"\"\"\n+\n+import logging\n+import traceback\n+from dataclasses import dataclass\n+\n+from pydantic import BaseModel, Field\n+\n+from graphrag.index.typing.error_handler import ErrorHandlerFn\n+from graphrag.language_model.protocol.base import ChatModel\n+\n+logger = logging.getLogger(__name__)\n+\n+# these tokens are used in the prompt\n+INPUT_TEXT_KEY = \"input_text\"\n+MAX_LENGTH_KEY = \"max_report_length\"\n+\n+\n+class FindingModel(BaseModel):\n+    \"\"\"A model for the expected LLM response shape.\"\"\"\n+\n+    summary: str = Field(description=\"The summary of the finding.\")\n+    explanation: str = Field(description=\"An explanation of the finding.\")\n+\n+\n+class CommunityReportResponse(BaseModel):\n+    \"\"\"A model for the expected LLM response shape.\"\"\"\n+\n+    title: str = Field(description=\"The title of the report.\")\n+    summary: str = Field(description=\"A summary of the report.\")\n+    findings: list[FindingModel] = Field(\n+        description=\"A list of findings in the report.\"\n+    )\n+    rating: float = Field(description=\"The rating of the report.\")\n+    rating_explanation: str = Field(description=\"An explanation of the rating.\")\n+\n+\n+@dataclass\n+class CommunityReportsResult:\n+    \"\"\"Community reports result class definition.\"\"\"\n+\n+    output: str\n+    structured_output: CommunityReportResponse | None\n+\n+\n+class CommunityReportsExtractor:\n+    \"\"\"Community reports extractor class definition.\"\"\"\n+\n+    _model: ChatModel\n+    _extraction_prompt: str\n+    _output_formatter_prompt: str\n+    _on_error: ErrorHandlerFn\n+    _max_report_length: int\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        extraction_prompt: str,\n+        max_report_length: int,\n+        on_error: ErrorHandlerFn | None = None,\n+    ):\n+        \"\"\"Init method definition.\"\"\"\n+        self._model = model\n+        self._extraction_prompt = extraction_prompt\n+        self._on_error = on_error or (lambda _e, _s, _d: None)\n+        self._max_report_length = max_report_length\n+\n+    async def __call__(self, input_text: str):\n+        \"\"\"Call method definition.\"\"\"\n+        output = None\n+        try:\n+            prompt = self._extraction_prompt.format(**{\n+                INPUT_TEXT_KEY: input_text,\n+                MAX_LENGTH_KEY: str(self._max_report_length),\n+            })\n+            response = await self._model.achat(\n+                prompt,\n+                json=True,  # Leaving this as True to avoid creating new cache entries\n+                name=\"create_community_report\",\n+                json_model=CommunityReportResponse,  # A model is required when using json mode\n+            )\n+\n+            output = response.parsed_response\n+        except Exception as e:\n+            logger.exception(\"error generating community report\")\n+            self._on_error(e, traceback.format_exc(), None)\n+\n+        text_output = self._get_text_output(output) if output else \"\"\n+        return CommunityReportsResult(\n+            structured_output=output,\n+            output=text_output,\n+        )\n+\n+    def _get_text_output(self, report: CommunityReportResponse) -> str:\n+        report_sections = \"\\n\\n\".join(\n+            f\"## {f.summary}\\n\\n{f.explanation}\" for f in report.findings\n+        )\n+        return f\"# {report.title}\\n\\n{report.summary}\\n\\n{report_sections}\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/explode_communities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/explode_communities.py b/packages/graphrag/graphrag/index/operations/summarize_communities/explode_communities.py\nnew file mode 100644\nindex 0000000..2e788d6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/explode_communities.py\n@@ -0,0 +1,23 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Explode a list of communities into nodes for filtering.\"\"\"\n+\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import (\n+    COMMUNITY_ID,\n+)\n+\n+\n+def explode_communities(\n+    communities: pd.DataFrame, entities: pd.DataFrame\n+) -> pd.DataFrame:\n+    \"\"\"Explode a list of communities into nodes for filtering.\"\"\"\n+    community_join = communities.explode(\"entity_ids\").loc[\n+        :, [\"community\", \"level\", \"entity_ids\"]\n+    ]\n+    nodes = entities.merge(\n+        community_join, left_on=\"id\", right_on=\"entity_ids\", how=\"left\"\n+    )\n+    return nodes.loc[nodes.loc[:, COMMUNITY_ID] != -1]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/__init__.py b/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/__init__.py\nnew file mode 100644\nindex 0000000..d185822\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Package of context builders for graph-based reports.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/context_builder.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/context_builder.py b/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/context_builder.py\nnew file mode 100644\nindex 0000000..34d281b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/context_builder.py\n@@ -0,0 +1,362 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Context builders for graphs.\"\"\"\n+\n+import logging\n+from typing import cast\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.operations.summarize_communities.build_mixed_context import (\n+    build_mixed_context,\n+)\n+from graphrag.index.operations.summarize_communities.graph_context.sort_context import (\n+    parallel_sort_context_batch,\n+    sort_context,\n+)\n+from graphrag.index.operations.summarize_communities.utils import (\n+    get_levels,\n+)\n+from graphrag.index.utils.dataframes import (\n+    antijoin,\n+    drop_columns,\n+    join,\n+    select,\n+    transform_series,\n+    union,\n+    where_column_equals,\n+)\n+from graphrag.logger.progress import progress_iterable\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def build_local_context(\n+    nodes,\n+    edges,\n+    claims,\n+    tokenizer: Tokenizer,\n+    callbacks: WorkflowCallbacks,\n+    max_context_tokens: int = 16_000,\n+):\n+    \"\"\"Prep communities for report generation.\"\"\"\n+    levels = get_levels(nodes, schemas.COMMUNITY_LEVEL)\n+\n+    dfs = []\n+\n+    for level in progress_iterable(levels, callbacks.progress, len(levels)):\n+        communities_at_level_df = _prepare_reports_at_level(\n+            nodes, edges, claims, tokenizer, level, max_context_tokens\n+        )\n+\n+        communities_at_level_df.loc[:, schemas.COMMUNITY_LEVEL] = level\n+        dfs.append(communities_at_level_df)\n+\n+    # build initial local context for all communities\n+    return pd.concat(dfs)\n+\n+\n+def _prepare_reports_at_level(\n+    node_df: pd.DataFrame,\n+    edge_df: pd.DataFrame,\n+    claim_df: pd.DataFrame | None,\n+    tokenizer: Tokenizer,\n+    level: int,\n+    max_context_tokens: int = 16_000,\n+) -> pd.DataFrame:\n+    \"\"\"Prepare reports at a given level.\"\"\"\n+    # Filter and prepare node details\n+    level_node_df = node_df[node_df[schemas.COMMUNITY_LEVEL] == level]\n+    logger.info(\"Number of nodes at level=%s => %s\", level, len(level_node_df))\n+    nodes_set = set(level_node_df[schemas.TITLE])\n+\n+    # Filter and prepare edge details\n+    level_edge_df = edge_df[\n+        edge_df.loc[:, schemas.EDGE_SOURCE].isin(nodes_set)\n+        & edge_df.loc[:, schemas.EDGE_TARGET].isin(nodes_set)\n+    ]\n+    level_edge_df.loc[:, schemas.EDGE_DETAILS] = level_edge_df.loc[\n+        :,\n+        [\n+            schemas.SHORT_ID,\n+            schemas.EDGE_SOURCE,\n+            schemas.EDGE_TARGET,\n+            schemas.DESCRIPTION,\n+            schemas.EDGE_DEGREE,\n+        ],\n+    ].to_dict(orient=\"records\")\n+\n+    level_claim_df = pd.DataFrame()\n+    if claim_df is not None:\n+        level_claim_df = claim_df[\n+            claim_df.loc[:, schemas.CLAIM_SUBJECT].isin(nodes_set)\n+        ]\n+\n+    # Merge node and edge details\n+    # Group edge details by node and aggregate into lists\n+    source_edges = (\n+        level_edge_df.groupby(schemas.EDGE_SOURCE)\n+        .agg({schemas.EDGE_DETAILS: \"first\"})\n+        .reset_index()\n+        .rename(columns={schemas.EDGE_SOURCE: schemas.TITLE})\n+    )\n+\n+    target_edges = (\n+        level_edge_df.groupby(schemas.EDGE_TARGET)\n+        .agg({schemas.EDGE_DETAILS: \"first\"})\n+        .reset_index()\n+        .rename(columns={schemas.EDGE_TARGET: schemas.TITLE})\n+    )\n+\n+    # Merge aggregated edges into the node DataFrame\n+    merged_node_df = level_node_df.merge(\n+        source_edges, on=schemas.TITLE, how=\"left\"\n+    ).merge(target_edges, on=schemas.TITLE, how=\"left\")\n+\n+    # Combine source and target edge details into a single column\n+    merged_node_df.loc[:, schemas.EDGE_DETAILS] = merged_node_df.loc[\n+        :, f\"{schemas.EDGE_DETAILS}_x\"\n+    ].combine_first(merged_node_df.loc[:, f\"{schemas.EDGE_DETAILS}_y\"])\n+\n+    # Drop intermediate columns\n+    merged_node_df.drop(\n+        columns=[f\"{schemas.EDGE_DETAILS}_x\", f\"{schemas.EDGE_DETAILS}_y\"], inplace=True\n+    )\n+\n+    # Aggregate node and edge details\n+    merged_node_df = (\n+        merged_node_df.groupby([\n+            schemas.TITLE,\n+            schemas.COMMUNITY_ID,\n+            schemas.COMMUNITY_LEVEL,\n+            schemas.NODE_DEGREE,\n+        ])\n+        .agg({\n+            schemas.NODE_DETAILS: \"first\",\n+            schemas.EDGE_DETAILS: lambda x: list(x.dropna()),\n+        })\n+        .reset_index()\n+    )\n+\n+    # Add ALL_CONTEXT column\n+    # Ensure schemas.CLAIM_DETAILS exists with the correct length\n+    # Merge claim details if available\n+    if claim_df is not None:\n+        merged_node_df = merged_node_df.merge(\n+            level_claim_df.loc[\n+                :, [schemas.CLAIM_SUBJECT, schemas.CLAIM_DETAILS]\n+            ].rename(columns={schemas.CLAIM_SUBJECT: schemas.TITLE}),\n+            on=schemas.TITLE,\n+            how=\"left\",\n+        )\n+\n+    # Create the ALL_CONTEXT column\n+    merged_node_df[schemas.ALL_CONTEXT] = (\n+        merged_node_df.loc[\n+            :,\n+            [\n+                schemas.TITLE,\n+                schemas.NODE_DEGREE,\n+                schemas.NODE_DETAILS,\n+                schemas.EDGE_DETAILS,\n+            ],\n+        ]\n+        .assign(\n+            **{schemas.CLAIM_DETAILS: merged_node_df[schemas.CLAIM_DETAILS]}\n+            if claim_df is not None\n+            else {}\n+        )\n+        .to_dict(orient=\"records\")\n+    )\n+\n+    # group all node details by community\n+    community_df = (\n+        merged_node_df.groupby(schemas.COMMUNITY_ID)\n+        .agg({schemas.ALL_CONTEXT: list})\n+        .reset_index()\n+    )\n+\n+    # Generate community-level context strings using vectorized batch processing\n+    return parallel_sort_context_batch(\n+        community_df,\n+        tokenizer=tokenizer,\n+        max_context_tokens=max_context_tokens,\n+    )\n+\n+\n+def build_level_context(\n+    report_df: pd.DataFrame | None,\n+    community_hierarchy_df: pd.DataFrame,\n+    local_context_df: pd.DataFrame,\n+    tokenizer: Tokenizer,\n+    level: int,\n+    max_context_tokens: int,\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Prep context for each community in a given level.\n+\n+    For each community:\n+    - Check if local context fits within the limit, if yes use local context\n+    - If local context exceeds the limit, iteratively replace local context with sub-community reports, starting from the biggest sub-community\n+    \"\"\"\n+    # Filter by community level\n+    level_context_df = local_context_df.loc[\n+        local_context_df.loc[:, schemas.COMMUNITY_LEVEL] == level\n+    ]\n+\n+    # Filter valid and invalid contexts using boolean logic\n+    valid_context_df = level_context_df.loc[\n+        ~level_context_df.loc[:, schemas.CONTEXT_EXCEED_FLAG]\n+    ]\n+    invalid_context_df = level_context_df.loc[\n+        level_context_df.loc[:, schemas.CONTEXT_EXCEED_FLAG]\n+    ]\n+\n+    # there is no report to substitute with, so we just trim the local context of the invalid context records\n+    # this case should only happen at the bottom level of the community hierarchy where there are no sub-communities\n+    if invalid_context_df.empty:\n+        return valid_context_df\n+\n+    if report_df is None or report_df.empty:\n+        invalid_context_df.loc[:, schemas.CONTEXT_STRING] = _sort_and_trim_context(\n+            invalid_context_df, tokenizer, max_context_tokens\n+        )\n+        invalid_context_df[schemas.CONTEXT_SIZE] = invalid_context_df.loc[\n+            :, schemas.CONTEXT_STRING\n+        ].map(tokenizer.num_tokens)\n+        invalid_context_df[schemas.CONTEXT_EXCEED_FLAG] = False\n+        return union(valid_context_df, invalid_context_df)\n+\n+    level_context_df = _antijoin_reports(level_context_df, report_df)\n+\n+    # for each invalid context, we will try to substitute with sub-community reports\n+    # first get local context and report (if available) for each sub-community\n+    sub_context_df = _get_subcontext_df(level + 1, report_df, local_context_df)\n+    community_df = _get_community_df(\n+        level,\n+        invalid_context_df,\n+        sub_context_df,\n+        community_hierarchy_df,\n+        tokenizer,\n+        max_context_tokens,\n+    )\n+\n+    # handle any remaining invalid records that can't be subsituted with sub-community reports\n+    # this should be rare, but if it happens, we will just trim the local context to fit the limit\n+    remaining_df = _antijoin_reports(invalid_context_df, community_df)\n+    remaining_df.loc[:, schemas.CONTEXT_STRING] = _sort_and_trim_context(\n+        remaining_df, tokenizer, max_context_tokens\n+    )\n+\n+    result = union(valid_context_df, community_df, remaining_df)\n+    result[schemas.CONTEXT_SIZE] = result.loc[:, schemas.CONTEXT_STRING].map(\n+        tokenizer.num_tokens\n+    )\n+\n+    result[schemas.CONTEXT_EXCEED_FLAG] = False\n+    return result\n+\n+\n+def _drop_community_level(df: pd.DataFrame) -> pd.DataFrame:\n+    \"\"\"Drop the community level column from the dataframe.\"\"\"\n+    return drop_columns(df, schemas.COMMUNITY_LEVEL)\n+\n+\n+def _at_level(level: int, df: pd.DataFrame) -> pd.DataFrame:\n+    \"\"\"Return records at the given level.\"\"\"\n+    return where_column_equals(df, schemas.COMMUNITY_LEVEL, level)\n+\n+\n+def _antijoin_reports(df: pd.DataFrame, reports: pd.DataFrame) -> pd.DataFrame:\n+    \"\"\"Return records in df that are not in reports.\"\"\"\n+    return antijoin(df, reports, schemas.COMMUNITY_ID)\n+\n+\n+def _sort_and_trim_context(\n+    df: pd.DataFrame, tokenizer: Tokenizer, max_context_tokens: int\n+) -> pd.Series:\n+    \"\"\"Sort and trim context to fit the limit.\"\"\"\n+    series = cast(\"pd.Series\", df[schemas.ALL_CONTEXT])\n+    return transform_series(\n+        series,\n+        lambda x: sort_context(\n+            x, tokenizer=tokenizer, max_context_tokens=max_context_tokens\n+        ),\n+    )\n+\n+\n+def _build_mixed_context(\n+    df: pd.DataFrame, tokenizer: Tokenizer, max_context_tokens: int\n+) -> pd.Series:\n+    \"\"\"Sort and trim context to fit the limit.\"\"\"\n+    series = cast(\"pd.Series\", df[schemas.ALL_CONTEXT])\n+    return transform_series(\n+        series,\n+        lambda x: build_mixed_context(\n+            x, tokenizer, max_context_tokens=max_context_tokens\n+        ),\n+    )\n+\n+\n+def _get_subcontext_df(\n+    level: int, report_df: pd.DataFrame, local_context_df: pd.DataFrame\n+) -> pd.DataFrame:\n+    \"\"\"Get sub-community context for each community.\"\"\"\n+    sub_report_df = _drop_community_level(_at_level(level, report_df))\n+    sub_context_df = _at_level(level, local_context_df)\n+    sub_context_df = join(sub_context_df, sub_report_df, schemas.COMMUNITY_ID)\n+    sub_context_df.rename(\n+        columns={schemas.COMMUNITY_ID: schemas.SUB_COMMUNITY}, inplace=True\n+    )\n+    return sub_context_df\n+\n+\n+def _get_community_df(\n+    level: int,\n+    invalid_context_df: pd.DataFrame,\n+    sub_context_df: pd.DataFrame,\n+    community_hierarchy_df: pd.DataFrame,\n+    tokenizer: Tokenizer,\n+    max_context_tokens: int,\n+) -> pd.DataFrame:\n+    \"\"\"Get community context for each community.\"\"\"\n+    # collect all sub communities' contexts for each community\n+    community_df = _drop_community_level(_at_level(level, community_hierarchy_df))\n+    invalid_community_ids = select(invalid_context_df, schemas.COMMUNITY_ID)\n+    subcontext_selection = select(\n+        sub_context_df,\n+        schemas.SUB_COMMUNITY,\n+        schemas.FULL_CONTENT,\n+        schemas.ALL_CONTEXT,\n+        schemas.CONTEXT_SIZE,\n+    )\n+\n+    invalid_communities = join(\n+        community_df, invalid_community_ids, schemas.COMMUNITY_ID, \"inner\"\n+    )\n+    community_df = join(\n+        invalid_communities, subcontext_selection, schemas.SUB_COMMUNITY\n+    )\n+    community_df[schemas.ALL_CONTEXT] = community_df.apply(\n+        lambda x: {\n+            schemas.SUB_COMMUNITY: x[schemas.SUB_COMMUNITY],\n+            schemas.ALL_CONTEXT: x[schemas.ALL_CONTEXT],\n+            schemas.FULL_CONTENT: x[schemas.FULL_CONTENT],\n+            schemas.CONTEXT_SIZE: x[schemas.CONTEXT_SIZE],\n+        },\n+        axis=1,\n+    )\n+    community_df = (\n+        community_df.groupby(schemas.COMMUNITY_ID)\n+        .agg({schemas.ALL_CONTEXT: list})\n+        .reset_index()\n+    )\n+    community_df[schemas.CONTEXT_STRING] = _build_mixed_context(\n+        community_df, tokenizer, max_context_tokens\n+    )\n+    community_df[schemas.COMMUNITY_LEVEL] = level\n+    return community_df\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/sort_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/sort_context.py b/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/sort_context.py\nnew file mode 100644\nindex 0000000..3b7863f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/graph_context/sort_context.py\n@@ -0,0 +1,164 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"Sort context by degree in descending order.\"\"\"\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+def sort_context(\n+    local_context: list[dict],\n+    tokenizer: Tokenizer,\n+    sub_community_reports: list[dict] | None = None,\n+    max_context_tokens: int | None = None,\n+    node_name_column: str = schemas.TITLE,\n+    node_details_column: str = schemas.NODE_DETAILS,\n+    edge_id_column: str = schemas.SHORT_ID,\n+    edge_details_column: str = schemas.EDGE_DETAILS,\n+    edge_degree_column: str = schemas.EDGE_DEGREE,\n+    edge_source_column: str = schemas.EDGE_SOURCE,\n+    edge_target_column: str = schemas.EDGE_TARGET,\n+    claim_details_column: str = schemas.CLAIM_DETAILS,\n+) -> str:\n+    \"\"\"Sort context by degree in descending order, optimizing for performance.\"\"\"\n+\n+    def _get_context_string(\n+        entities: list[dict],\n+        edges: list[dict],\n+        claims: list[dict],\n+        sub_community_reports: list[dict] | None = None,\n+    ) -> str:\n+        \"\"\"Concatenate structured data into a context string.\"\"\"\n+        contexts = []\n+        if sub_community_reports:\n+            report_df = pd.DataFrame(sub_community_reports)\n+            if not report_df.empty:\n+                contexts.append(\n+                    f\"----Reports-----\\n{report_df.to_csv(index=False, sep=',')}\"\n+                )\n+\n+        for label, data in [\n+            (\"Entities\", entities),\n+            (\"Claims\", claims),\n+            (\"Relationships\", edges),\n+        ]:\n+            if data:\n+                data_df = pd.DataFrame(data)\n+                if not data_df.empty:\n+                    contexts.append(\n+                        f\"-----{label}-----\\n{data_df.to_csv(index=False, sep=',')}\"\n+                    )\n+\n+        return \"\\n\\n\".join(contexts)\n+\n+    # Preprocess local context\n+    edges = [\n+        {**e, schemas.SHORT_ID: int(e[schemas.SHORT_ID])}\n+        for record in local_context\n+        for e in record.get(edge_details_column, [])\n+        if isinstance(e, dict)\n+    ]\n+\n+    node_details = {\n+        record[node_name_column]: {\n+            **record[node_details_column],\n+            schemas.SHORT_ID: int(record[node_details_column][schemas.SHORT_ID]),\n+        }\n+        for record in local_context\n+    }\n+\n+    claim_details = {\n+        record[node_name_column]: [\n+            {**c, schemas.SHORT_ID: int(c[schemas.SHORT_ID])}\n+            for c in record.get(claim_details_column, [])\n+            if isinstance(c, dict) and c.get(schemas.SHORT_ID) is not None\n+        ]\n+        for record in local_context\n+        if isinstance(record.get(claim_details_column), list)\n+    }\n+\n+    # Sort edges by degree (desc) and ID (asc)\n+    edges.sort(key=lambda x: (-x.get(edge_degree_column, 0), x.get(edge_id_column, \"\")))\n+\n+    # Deduplicate and build context incrementally\n+    edge_ids, nodes_ids, claims_ids = set(), set(), set()\n+    sorted_edges, sorted_nodes, sorted_claims = [], [], []\n+    context_string = \"\"\n+\n+    for edge in edges:\n+        source, target = edge[edge_source_column], edge[edge_target_column]\n+\n+        # Add source and target node details\n+        for node in [node_details.get(source), node_details.get(target)]:\n+            if node and node[schemas.SHORT_ID] not in nodes_ids:\n+                nodes_ids.add(node[schemas.SHORT_ID])\n+                sorted_nodes.append(node)\n+\n+        # Add claims related to source and target\n+        for claims in [claim_details.get(source), claim_details.get(target)]:\n+            if claims:\n+                for claim in claims:\n+                    if claim[schemas.SHORT_ID] not in claims_ids:\n+                        claims_ids.add(claim[schemas.SHORT_ID])\n+                        sorted_claims.append(claim)\n+\n+        # Add the edge\n+        if edge[schemas.SHORT_ID] not in edge_ids:\n+            edge_ids.add(edge[schemas.SHORT_ID])\n+            sorted_edges.append(edge)\n+\n+        # Generate new context string\n+        new_context_string = _get_context_string(\n+            sorted_nodes, sorted_edges, sorted_claims, sub_community_reports\n+        )\n+        if (\n+            max_context_tokens\n+            and tokenizer.num_tokens(new_context_string) > max_context_tokens\n+        ):\n+            break\n+        context_string = new_context_string\n+\n+    # Return the final context string\n+    return context_string or _get_context_string(\n+        sorted_nodes, sorted_edges, sorted_claims, sub_community_reports\n+    )\n+\n+\n+def parallel_sort_context_batch(\n+    community_df, tokenizer: Tokenizer, max_context_tokens, parallel=False\n+):\n+    \"\"\"Calculate context using parallelization if enabled.\"\"\"\n+    if parallel:\n+        # Use ThreadPoolExecutor for parallel execution\n+        from concurrent.futures import ThreadPoolExecutor\n+\n+        with ThreadPoolExecutor(max_workers=None) as executor:\n+            context_strings = list(\n+                executor.map(\n+                    lambda x: sort_context(\n+                        x, tokenizer, max_context_tokens=max_context_tokens\n+                    ),\n+                    community_df[schemas.ALL_CONTEXT],\n+                )\n+            )\n+        community_df[schemas.CONTEXT_STRING] = context_strings\n+\n+    else:\n+        # Assign context strings directly to the DataFrame\n+        community_df[schemas.CONTEXT_STRING] = community_df[schemas.ALL_CONTEXT].apply(\n+            lambda context_list: sort_context(\n+                context_list, tokenizer, max_context_tokens=max_context_tokens\n+            )\n+        )\n+\n+    # Calculate other columns\n+    community_df[schemas.CONTEXT_SIZE] = community_df[schemas.CONTEXT_STRING].apply(\n+        tokenizer.num_tokens\n+    )\n+    community_df[schemas.CONTEXT_EXCEED_FLAG] = (\n+        community_df[schemas.CONTEXT_SIZE] > max_context_tokens\n+    )\n+\n+    return community_df\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/summarize_communities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/summarize_communities.py b/packages/graphrag/graphrag/index/operations/summarize_communities/summarize_communities.py\nnew file mode 100644\nindex 0000000..f602833\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/summarize_communities.py\n@@ -0,0 +1,160 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing create_community_reports and load_strategy methods definition.\"\"\"\n+\n+import logging\n+from collections.abc import Callable\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.index.operations.summarize_communities.community_reports_extractor import (\n+    CommunityReportsExtractor,\n+)\n+from graphrag.index.operations.summarize_communities.typing import (\n+    CommunityReport,\n+    CommunityReportsStrategy,\n+    Finding,\n+)\n+from graphrag.index.operations.summarize_communities.utils import (\n+    get_levels,\n+)\n+from graphrag.index.utils.derive_from_rows import derive_from_rows\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.logger.progress import progress_ticker\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def summarize_communities(\n+    nodes: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    local_contexts,\n+    level_context_builder: Callable,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    prompt: str,\n+    tokenizer: Tokenizer,\n+    max_input_length: int,\n+    max_report_length: int,\n+    num_threads: int,\n+    async_type: AsyncType,\n+):\n+    \"\"\"Generate community summaries.\"\"\"\n+    reports: list[CommunityReport | None] = []\n+    tick = progress_ticker(callbacks.progress, len(local_contexts))\n+    community_hierarchy = (\n+        communities.explode(\"children\")\n+        .rename({\"children\": \"sub_community\"}, axis=1)\n+        .loc[:, [\"community\", \"level\", \"sub_community\"]]\n+    ).dropna()\n+\n+    levels = get_levels(nodes)\n+\n+    level_contexts = []\n+    for level in levels:\n+        level_context = level_context_builder(\n+            pd.DataFrame(reports),\n+            community_hierarchy_df=community_hierarchy,\n+            local_context_df=local_contexts,\n+            level=level,\n+            tokenizer=tokenizer,\n+            max_context_tokens=max_input_length,\n+        )\n+        level_contexts.append(level_context)\n+\n+    for i, level_context in enumerate(level_contexts):\n+\n+        async def run_generate(record):\n+            result = await _generate_report(\n+                run_extractor,\n+                community_id=record[schemas.COMMUNITY_ID],\n+                community_level=record[schemas.COMMUNITY_LEVEL],\n+                community_context=record[schemas.CONTEXT_STRING],\n+                model=model,\n+                extraction_prompt=prompt,\n+                max_report_length=max_report_length,\n+            )\n+            tick()\n+            return result\n+\n+        local_reports = await derive_from_rows(\n+            level_context,\n+            run_generate,\n+            callbacks=NoopWorkflowCallbacks(),\n+            num_threads=num_threads,\n+            async_type=async_type,\n+            progress_msg=f\"level {levels[i]} summarize communities progress: \",\n+        )\n+        reports.extend([lr for lr in local_reports if lr is not None])\n+\n+    return pd.DataFrame(reports)\n+\n+\n+async def _generate_report(\n+    runner: CommunityReportsStrategy,\n+    model: ChatModel,\n+    extraction_prompt: str,\n+    community_id: int,\n+    community_level: int,\n+    community_context: str,\n+    max_report_length: int,\n+) -> CommunityReport | None:\n+    \"\"\"Generate a report for a single community.\"\"\"\n+    return await runner(\n+        community_id,\n+        community_context,\n+        community_level,\n+        model,\n+        extraction_prompt,\n+        max_report_length,\n+    )\n+\n+\n+async def run_extractor(\n+    community: str | int,\n+    input: str,\n+    level: int,\n+    model: ChatModel,\n+    extraction_prompt: str,\n+    max_report_length: int,\n+) -> CommunityReport | None:\n+    \"\"\"Run the graph intelligence entity extraction strategy.\"\"\"\n+    extractor = CommunityReportsExtractor(\n+        model,\n+        extraction_prompt=extraction_prompt,\n+        max_report_length=max_report_length,\n+        on_error=lambda e, stack, _data: logger.error(\n+            \"Community Report Extraction Error\", exc_info=e, extra={\"stack\": stack}\n+        ),\n+    )\n+\n+    try:\n+        results = await extractor(input)\n+        report = results.structured_output\n+        if report is None:\n+            logger.warning(\"No report found for community: %s\", community)\n+            return None\n+\n+        return CommunityReport(\n+            community=community,\n+            full_content=results.output,\n+            level=level,\n+            rank=report.rating,\n+            title=report.title,\n+            rating_explanation=report.rating_explanation,\n+            summary=report.summary,\n+            findings=[\n+                Finding(explanation=f.explanation, summary=f.summary)\n+                for f in report.findings\n+            ],\n+            full_content_json=report.model_dump_json(indent=4),\n+        )\n+    except Exception:\n+        logger.exception(\"Error processing community: %s\", community)\n+        return None\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/__init__.py b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/__init__.py\nnew file mode 100644\nindex 0000000..716aa6b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Package of context builders for text unit-based reports.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py\nnew file mode 100644\nindex 0000000..8d6e0ae\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py\n@@ -0,0 +1,235 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Context builders for text units.\"\"\"\n+\n+import logging\n+from typing import cast\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.index.operations.summarize_communities.build_mixed_context import (\n+    build_mixed_context,\n+)\n+from graphrag.index.operations.summarize_communities.text_unit_context.prep_text_units import (\n+    prep_text_units,\n+)\n+from graphrag.index.operations.summarize_communities.text_unit_context.sort_context import (\n+    sort_context,\n+)\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def build_local_context(\n+    community_membership_df: pd.DataFrame,\n+    text_units_df: pd.DataFrame,\n+    node_df: pd.DataFrame,\n+    tokenizer: Tokenizer,\n+    max_context_tokens: int = 16000,\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Prep context data for community report generation using text unit data.\n+\n+    Community membership has columns [COMMUNITY_ID, COMMUNITY_LEVEL, ENTITY_IDS, RELATIONSHIP_IDS, TEXT_UNIT_IDS]\n+    \"\"\"\n+    # get text unit details, include short_id, text, and entity degree (sum of degrees of the text unit's nodes that belong to a community)\n+    prepped_text_units_df = prep_text_units(text_units_df, node_df)\n+    prepped_text_units_df = prepped_text_units_df.rename(\n+        columns={\n+            schemas.ID: schemas.TEXT_UNIT_IDS,\n+            schemas.COMMUNITY_ID: schemas.COMMUNITY_ID,\n+        }\n+    )\n+\n+    # merge text unit details with community membership\n+    context_df = community_membership_df.loc[\n+        :, [schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL, schemas.TEXT_UNIT_IDS]\n+    ]\n+    context_df = context_df.explode(schemas.TEXT_UNIT_IDS)\n+    context_df = context_df.merge(\n+        prepped_text_units_df,\n+        on=[schemas.TEXT_UNIT_IDS, schemas.COMMUNITY_ID],\n+        how=\"left\",\n+    )\n+\n+    context_df[schemas.ALL_CONTEXT] = context_df.apply(\n+        lambda x: {\n+            \"id\": x[schemas.ALL_DETAILS][schemas.SHORT_ID],\n+            \"text\": x[schemas.ALL_DETAILS][schemas.TEXT],\n+            \"entity_degree\": x[schemas.ALL_DETAILS][schemas.ENTITY_DEGREE],\n+        },\n+        axis=1,\n+    )\n+\n+    context_df = (\n+        context_df.groupby([schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL])\n+        .agg({schemas.ALL_CONTEXT: list})\n+        .reset_index()\n+    )\n+    context_df[schemas.CONTEXT_STRING] = context_df[schemas.ALL_CONTEXT].apply(\n+        lambda x: sort_context(x, tokenizer)\n+    )\n+    context_df[schemas.CONTEXT_SIZE] = context_df[schemas.CONTEXT_STRING].apply(\n+        lambda x: tokenizer.num_tokens(x)\n+    )\n+    context_df[schemas.CONTEXT_EXCEED_FLAG] = context_df[schemas.CONTEXT_SIZE].apply(\n+        lambda x: x > max_context_tokens\n+    )\n+\n+    return context_df\n+\n+\n+def build_level_context(\n+    report_df: pd.DataFrame | None,\n+    community_hierarchy_df: pd.DataFrame,\n+    local_context_df: pd.DataFrame,\n+    level: int,\n+    tokenizer: Tokenizer,\n+    max_context_tokens: int = 16000,\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Prep context for each community in a given level.\n+\n+    For each community:\n+    - Check if local context fits within the limit, if yes use local context\n+    - If local context exceeds the limit, iteratively replace local context with sub-community reports, starting from the biggest sub-community\n+    \"\"\"\n+    if report_df is None or report_df.empty:\n+        # there is no report to substitute with, so we just trim the local context of the invalid context records\n+        # this case should only happen at the bottom level of the community hierarchy where there are no sub-communities\n+        level_context_df = local_context_df[\n+            local_context_df[schemas.COMMUNITY_LEVEL] == level\n+        ]\n+\n+        valid_context_df = cast(\n+            \"pd.DataFrame\",\n+            level_context_df[~level_context_df[schemas.CONTEXT_EXCEED_FLAG]],\n+        )\n+        invalid_context_df = cast(\n+            \"pd.DataFrame\",\n+            level_context_df[level_context_df[schemas.CONTEXT_EXCEED_FLAG]],\n+        )\n+\n+        if invalid_context_df.empty:\n+            return valid_context_df\n+\n+        invalid_context_df.loc[:, [schemas.CONTEXT_STRING]] = invalid_context_df[\n+            schemas.ALL_CONTEXT\n+        ].apply(\n+            lambda x: sort_context(x, tokenizer, max_context_tokens=max_context_tokens)\n+        )\n+        invalid_context_df.loc[:, [schemas.CONTEXT_SIZE]] = invalid_context_df[\n+            schemas.CONTEXT_STRING\n+        ].apply(lambda x: tokenizer.num_tokens(x))\n+        invalid_context_df.loc[:, [schemas.CONTEXT_EXCEED_FLAG]] = False\n+\n+        return pd.concat([valid_context_df, invalid_context_df])\n+\n+    level_context_df = local_context_df[\n+        local_context_df[schemas.COMMUNITY_LEVEL] == level\n+    ]\n+\n+    # exclude those that already have reports\n+    level_context_df = level_context_df.merge(\n+        report_df[[schemas.COMMUNITY_ID]],\n+        on=schemas.COMMUNITY_ID,\n+        how=\"outer\",\n+        indicator=True,\n+    )\n+    level_context_df = level_context_df[level_context_df[\"_merge\"] == \"left_only\"].drop(\n+        \"_merge\", axis=1\n+    )\n+    valid_context_df = cast(\n+        \"pd.DataFrame\",\n+        level_context_df[level_context_df[schemas.CONTEXT_EXCEED_FLAG] is False],\n+    )\n+    invalid_context_df = cast(\n+        \"pd.DataFrame\",\n+        level_context_df[level_context_df[schemas.CONTEXT_EXCEED_FLAG] is True],\n+    )\n+\n+    if invalid_context_df.empty:\n+        return valid_context_df\n+\n+    # for each invalid context, we will try to substitute with sub-community reports\n+    # first get local context and report (if available) for each sub-community\n+    sub_report_df = report_df[report_df[schemas.COMMUNITY_LEVEL] == level + 1].drop(\n+        [schemas.COMMUNITY_LEVEL], axis=1\n+    )\n+    sub_context_df = local_context_df[\n+        local_context_df[schemas.COMMUNITY_LEVEL] == level + 1\n+    ]\n+    sub_context_df = sub_context_df.merge(\n+        sub_report_df, on=schemas.COMMUNITY_ID, how=\"left\"\n+    )\n+    sub_context_df.rename(\n+        columns={schemas.COMMUNITY_ID: schemas.SUB_COMMUNITY}, inplace=True\n+    )\n+\n+    # collect all sub communities' contexts for each community\n+    community_df = community_hierarchy_df[\n+        community_hierarchy_df[schemas.COMMUNITY_LEVEL] == level\n+    ].drop([schemas.COMMUNITY_LEVEL], axis=1)\n+    community_df = community_df.merge(\n+        invalid_context_df[[schemas.COMMUNITY_ID]], on=schemas.COMMUNITY_ID, how=\"inner\"\n+    )\n+    community_df = community_df.merge(\n+        sub_context_df[\n+            [\n+                schemas.SUB_COMMUNITY,\n+                schemas.FULL_CONTENT,\n+                schemas.ALL_CONTEXT,\n+                schemas.CONTEXT_SIZE,\n+            ]\n+        ],\n+        on=schemas.SUB_COMMUNITY,\n+        how=\"left\",\n+    )\n+    community_df[schemas.ALL_CONTEXT] = community_df.apply(\n+        lambda x: {\n+            schemas.SUB_COMMUNITY: x[schemas.SUB_COMMUNITY],\n+            schemas.ALL_CONTEXT: x[schemas.ALL_CONTEXT],\n+            schemas.FULL_CONTENT: x[schemas.FULL_CONTENT],\n+            schemas.CONTEXT_SIZE: x[schemas.CONTEXT_SIZE],\n+        },\n+        axis=1,\n+    )\n+    community_df = (\n+        community_df.groupby(schemas.COMMUNITY_ID)\n+        .agg({schemas.ALL_CONTEXT: list})\n+        .reset_index()\n+    )\n+    community_df[schemas.CONTEXT_STRING] = community_df[schemas.ALL_CONTEXT].apply(\n+        lambda x: build_mixed_context(x, tokenizer, max_context_tokens)\n+    )\n+    community_df[schemas.CONTEXT_SIZE] = community_df[schemas.CONTEXT_STRING].apply(\n+        lambda x: tokenizer.num_tokens(x)\n+    )\n+    community_df[schemas.CONTEXT_EXCEED_FLAG] = False\n+    community_df[schemas.COMMUNITY_LEVEL] = level\n+\n+    # handle any remaining invalid records that can't be subsituted with sub-community reports\n+    # this should be rare, but if it happens, we will just trim the local context to fit the limit\n+    remaining_df = invalid_context_df.merge(\n+        community_df[[schemas.COMMUNITY_ID]],\n+        on=schemas.COMMUNITY_ID,\n+        how=\"outer\",\n+        indicator=True,\n+    )\n+    remaining_df = remaining_df[remaining_df[\"_merge\"] == \"left_only\"].drop(\n+        \"_merge\", axis=1\n+    )\n+    remaining_df[schemas.CONTEXT_STRING] = cast(\n+        \"pd.DataFrame\", remaining_df[schemas.ALL_CONTEXT]\n+    ).apply(lambda x: sort_context(x, tokenizer, max_context_tokens=max_context_tokens))\n+    remaining_df[schemas.CONTEXT_SIZE] = cast(\n+        \"pd.DataFrame\", remaining_df[schemas.CONTEXT_STRING]\n+    ).apply(lambda x: tokenizer.num_tokens(x))\n+    remaining_df[schemas.CONTEXT_EXCEED_FLAG] = False\n+\n+    return cast(\n+        \"pd.DataFrame\", pd.concat([valid_context_df, community_df, remaining_df])\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py\nnew file mode 100644\nindex 0000000..af31ee7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py\n@@ -0,0 +1,45 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Prepare text units for community reports.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def prep_text_units(\n+    text_unit_df: pd.DataFrame,\n+    node_df: pd.DataFrame,\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Calculate text unit degree  and concatenate text unit details.\n+\n+    Returns : dataframe with columns [COMMUNITY_ID, TEXT_UNIT_ID, ALL_DETAILS]\n+    \"\"\"\n+    node_df.drop(columns=[\"id\"], inplace=True)\n+    node_to_text_ids = node_df.explode(schemas.TEXT_UNIT_IDS).rename(\n+        columns={schemas.TEXT_UNIT_IDS: schemas.ID}\n+    )\n+    node_to_text_ids = node_to_text_ids[\n+        [schemas.TITLE, schemas.COMMUNITY_ID, schemas.NODE_DEGREE, schemas.ID]\n+    ]\n+    text_unit_degrees = (\n+        node_to_text_ids.groupby([schemas.COMMUNITY_ID, schemas.ID])\n+        .agg({schemas.NODE_DEGREE: \"sum\"})\n+        .reset_index()\n+    )\n+    result_df = text_unit_df.merge(text_unit_degrees, on=schemas.ID, how=\"left\")\n+    result_df[schemas.ALL_DETAILS] = result_df.apply(\n+        lambda x: {\n+            schemas.SHORT_ID: x[schemas.SHORT_ID],\n+            schemas.TEXT: x[schemas.TEXT],\n+            schemas.ENTITY_DEGREE: x[schemas.NODE_DEGREE],\n+        },\n+        axis=1,\n+    )\n+    return result_df.loc[:, [schemas.COMMUNITY_ID, schemas.ID, schemas.ALL_DETAILS]]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py\nnew file mode 100644\nindex 0000000..7bad931\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py\n@@ -0,0 +1,85 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Sort local context by total degree of associated nodes in descending order.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def get_context_string(\n+    text_units: list[dict],\n+    sub_community_reports: list[dict] | None = None,\n+) -> str:\n+    \"\"\"Concatenate structured data into a context string.\"\"\"\n+    contexts = []\n+    if sub_community_reports:\n+        sub_community_reports = [\n+            report\n+            for report in sub_community_reports\n+            if schemas.COMMUNITY_ID in report\n+            and report[schemas.COMMUNITY_ID]\n+            and str(report[schemas.COMMUNITY_ID]).strip() != \"\"\n+        ]\n+        report_df = pd.DataFrame(sub_community_reports).drop_duplicates()\n+        if not report_df.empty:\n+            if report_df[schemas.COMMUNITY_ID].dtype == float:\n+                report_df[schemas.COMMUNITY_ID] = report_df[\n+                    schemas.COMMUNITY_ID\n+                ].astype(int)\n+            report_string = (\n+                f\"----REPORTS-----\\n{report_df.to_csv(index=False, sep=',')}\"\n+            )\n+            contexts.append(report_string)\n+\n+    text_units = [\n+        unit\n+        for unit in text_units\n+        if \"id\" in unit and unit[\"id\"] and str(unit[\"id\"]).strip() != \"\"\n+    ]\n+    text_units_df = pd.DataFrame(text_units).drop_duplicates()\n+    if not text_units_df.empty:\n+        if text_units_df[\"id\"].dtype == float:\n+            text_units_df[\"id\"] = text_units_df[\"id\"].astype(int)\n+        text_unit_string = (\n+            f\"-----SOURCES-----\\n{text_units_df.to_csv(index=False, sep=',')}\"\n+        )\n+        contexts.append(text_unit_string)\n+\n+    return \"\\n\\n\".join(contexts)\n+\n+\n+def sort_context(\n+    local_context: list[dict],\n+    tokenizer: Tokenizer,\n+    sub_community_reports: list[dict] | None = None,\n+    max_context_tokens: int | None = None,\n+) -> str:\n+    \"\"\"Sort local context (list of text units) by total degree of associated nodes in descending order.\"\"\"\n+    sorted_text_units = sorted(\n+        local_context, key=lambda x: x[schemas.ENTITY_DEGREE], reverse=True\n+    )\n+\n+    current_text_units = []\n+    context_string = \"\"\n+    for record in sorted_text_units:\n+        current_text_units.append(record)\n+        if max_context_tokens:\n+            new_context_string = get_context_string(\n+                current_text_units, sub_community_reports\n+            )\n+            if tokenizer.num_tokens(new_context_string) > max_context_tokens:\n+                break\n+\n+            context_string = new_context_string\n+\n+    if context_string == \"\":\n+        return get_context_string(sorted_text_units, sub_community_reports)\n+\n+    return context_string\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/typing.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/typing.py b/packages/graphrag/graphrag/index/operations/summarize_communities/typing.py\nnew file mode 100644\nindex 0000000..709c5cc\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/typing.py\n@@ -0,0 +1,50 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'Finding' and 'CommunityReport' models.\"\"\"\n+\n+from collections.abc import Awaitable, Callable\n+from typing import Any\n+\n+from typing_extensions import TypedDict\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+\n+ExtractedEntity = dict[str, Any]\n+RowContext = dict[str, Any]\n+EntityTypes = list[str]\n+Claim = dict[str, Any]\n+\n+\n+class Finding(TypedDict):\n+    \"\"\"Finding class definition.\"\"\"\n+\n+    summary: str\n+    explanation: str\n+\n+\n+class CommunityReport(TypedDict):\n+    \"\"\"Community report class definition.\"\"\"\n+\n+    community: str | int\n+    title: str\n+    summary: str\n+    full_content: str\n+    full_content_json: str\n+    rank: float\n+    level: int\n+    rating_explanation: str\n+    findings: list[Finding]\n+\n+\n+CommunityReportsStrategy = Callable[\n+    [\n+        str | int,\n+        str,\n+        int,\n+        ChatModel,\n+        str,\n+        int,\n+    ],\n+    Awaitable[CommunityReport | None],\n+]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_communities/utils.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_communities/utils.py b/packages/graphrag/graphrag/index/operations/summarize_communities/utils.py\nnew file mode 100644\nindex 0000000..6261b1b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_communities/utils.py\n@@ -0,0 +1,17 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing community report generation utilities.\"\"\"\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+\n+\n+def get_levels(\n+    df: pd.DataFrame, level_column: str = schemas.COMMUNITY_LEVEL\n+) -> list[int]:\n+    \"\"\"Get the levels of the communities.\"\"\"\n+    levels = df[level_column].dropna().unique()\n+    levels = [int(lvl) for lvl in levels if lvl != -1]\n+    return sorted(levels, reverse=True)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_descriptions/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_descriptions/__init__.py b/packages/graphrag/graphrag/index/operations/summarize_descriptions/__init__.py\nnew file mode 100644\nindex 0000000..8b70026\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_descriptions/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Root package for description summarization.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_descriptions/description_summary_extractor.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_descriptions/description_summary_extractor.py b/packages/graphrag/graphrag/index/operations/summarize_descriptions/description_summary_extractor.py\nnew file mode 100644\nindex 0000000..4a999fd\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_descriptions/description_summary_extractor.py\n@@ -0,0 +1,132 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'GraphExtractionResult' and 'GraphExtractor' models.\"\"\"\n+\n+import json\n+from dataclasses import dataclass\n+\n+from graphrag.index.typing.error_handler import ErrorHandlerFn\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+\n+# these tokens are used in the prompt\n+ENTITY_NAME_KEY = \"entity_name\"\n+DESCRIPTION_LIST_KEY = \"description_list\"\n+MAX_LENGTH_KEY = \"max_length\"\n+\n+\n+@dataclass\n+class SummarizationResult:\n+    \"\"\"Unipartite graph extraction result class definition.\"\"\"\n+\n+    id: str | tuple[str, str]\n+    description: str\n+\n+\n+class SummarizeExtractor:\n+    \"\"\"Unipartite graph extractor class definition.\"\"\"\n+\n+    _model: ChatModel\n+    _summarization_prompt: str\n+    _on_error: ErrorHandlerFn\n+    _max_summary_length: int\n+    _max_input_tokens: int\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        max_summary_length: int,\n+        max_input_tokens: int,\n+        summarization_prompt: str,\n+        on_error: ErrorHandlerFn | None = None,\n+    ):\n+        \"\"\"Init method definition.\"\"\"\n+        # TODO: streamline construction\n+        self._model = model\n+        self._tokenizer = get_tokenizer(model.config)\n+        self._summarization_prompt = summarization_prompt\n+        self._on_error = on_error or (lambda _e, _s, _d: None)\n+        self._max_summary_length = max_summary_length\n+        self._max_input_tokens = max_input_tokens\n+\n+    async def __call__(\n+        self,\n+        id: str | tuple[str, str],\n+        descriptions: list[str],\n+    ) -> SummarizationResult:\n+        \"\"\"Call method definition.\"\"\"\n+        result = \"\"\n+        if len(descriptions) == 0:\n+            result = \"\"\n+        elif len(descriptions) == 1:\n+            result = descriptions[0]\n+        else:\n+            result = await self._summarize_descriptions(id, descriptions)\n+\n+        return SummarizationResult(\n+            id=id,\n+            description=result or \"\",\n+        )\n+\n+    async def _summarize_descriptions(\n+        self, id: str | tuple[str, str], descriptions: list[str]\n+    ) -> str:\n+        \"\"\"Summarize descriptions into a single description.\"\"\"\n+        sorted_id = sorted(id) if isinstance(id, list) else id\n+\n+        # Safety check, should always be a list\n+        if not isinstance(descriptions, list):\n+            descriptions = [descriptions]\n+\n+        # Sort description lists\n+        if len(descriptions) > 1:\n+            descriptions = sorted(descriptions)\n+\n+        # Iterate over descriptions, adding all until the max input tokens is reached\n+        usable_tokens = self._max_input_tokens - self._tokenizer.num_tokens(\n+            self._summarization_prompt\n+        )\n+        descriptions_collected = []\n+        result = \"\"\n+\n+        for i, description in enumerate(descriptions):\n+            usable_tokens -= self._tokenizer.num_tokens(description)\n+            descriptions_collected.append(description)\n+\n+            # If buffer is full, or all descriptions have been added, summarize\n+            if (usable_tokens < 0 and len(descriptions_collected) > 1) or (\n+                i == len(descriptions) - 1\n+            ):\n+                # Calculate result (final or partial)\n+                result = await self._summarize_descriptions_with_llm(\n+                    sorted_id, descriptions_collected\n+                )\n+\n+                # If we go for another loop, reset values to new\n+                if i != len(descriptions) - 1:\n+                    descriptions_collected = [result]\n+                    usable_tokens = (\n+                        self._max_input_tokens\n+                        - self._tokenizer.num_tokens(self._summarization_prompt)\n+                        - self._tokenizer.num_tokens(result)\n+                    )\n+\n+        return result\n+\n+    async def _summarize_descriptions_with_llm(\n+        self, id: str | tuple[str, str] | list[str], descriptions: list[str]\n+    ):\n+        \"\"\"Summarize descriptions using the LLM.\"\"\"\n+        response = await self._model.achat(\n+            self._summarization_prompt.format(**{\n+                ENTITY_NAME_KEY: json.dumps(id, ensure_ascii=False),\n+                DESCRIPTION_LIST_KEY: json.dumps(\n+                    sorted(descriptions), ensure_ascii=False\n+                ),\n+                MAX_LENGTH_KEY: self._max_summary_length,\n+            }),\n+            name=\"summarize\",\n+        )\n+        # Calculate result\n+        return str(response.output.content)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py b/packages/graphrag/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py\nnew file mode 100644\nindex 0000000..48aaf37\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py\n@@ -0,0 +1,137 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the summarize_descriptions verb.\"\"\"\n+\n+import asyncio\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.operations.summarize_descriptions.description_summary_extractor import (\n+    SummarizeExtractor,\n+)\n+from graphrag.index.operations.summarize_descriptions.typing import (\n+    SummarizedDescriptionResult,\n+)\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.logger.progress import ProgressTicker, progress_ticker\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def summarize_descriptions(\n+    entities_df: pd.DataFrame,\n+    relationships_df: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    max_summary_length: int,\n+    max_input_tokens: int,\n+    prompt: str,\n+    num_threads: int,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"Summarize entity and relationship descriptions from an entity graph, using a language model.\"\"\"\n+\n+    async def get_summarized(\n+        nodes: pd.DataFrame, edges: pd.DataFrame, semaphore: asyncio.Semaphore\n+    ):\n+        ticker_length = len(nodes) + len(edges)\n+\n+        ticker = progress_ticker(\n+            callbacks.progress,\n+            ticker_length,\n+            description=\"Summarize entity/relationship description progress: \",\n+        )\n+\n+        node_futures = [\n+            do_summarize_descriptions(\n+                str(row.title),  # type: ignore\n+                sorted(set(row.description)),  # type: ignore\n+                ticker,\n+                semaphore,\n+            )\n+            for row in nodes.itertuples(index=False)\n+        ]\n+\n+        node_results = await asyncio.gather(*node_futures)\n+\n+        node_descriptions = [\n+            {\n+                \"title\": result.id,\n+                \"description\": result.description,\n+            }\n+            for result in node_results\n+        ]\n+\n+        edge_futures = [\n+            do_summarize_descriptions(\n+                (str(row.source), str(row.target)),  # type: ignore\n+                sorted(set(row.description)),  # type: ignore\n+                ticker,\n+                semaphore,\n+            )\n+            for row in edges.itertuples(index=False)\n+        ]\n+\n+        edge_results = await asyncio.gather(*edge_futures)\n+\n+        edge_descriptions = [\n+            {\n+                \"source\": result.id[0],\n+                \"target\": result.id[1],\n+                \"description\": result.description,\n+            }\n+            for result in edge_results\n+        ]\n+\n+        entity_descriptions = pd.DataFrame(node_descriptions)\n+        relationship_descriptions = pd.DataFrame(edge_descriptions)\n+        return entity_descriptions, relationship_descriptions\n+\n+    async def do_summarize_descriptions(\n+        id: str | tuple[str, str],\n+        descriptions: list[str],\n+        ticker: ProgressTicker,\n+        semaphore: asyncio.Semaphore,\n+    ):\n+        async with semaphore:\n+            results = await run_summarize_descriptions(\n+                id,\n+                descriptions,\n+                model,\n+                max_summary_length,\n+                max_input_tokens,\n+                prompt,\n+            )\n+            ticker(1)\n+        return results\n+\n+    semaphore = asyncio.Semaphore(num_threads)\n+\n+    return await get_summarized(entities_df, relationships_df, semaphore)\n+\n+\n+async def run_summarize_descriptions(\n+    id: str | tuple[str, str],\n+    descriptions: list[str],\n+    model: ChatModel,\n+    max_summary_length: int,\n+    max_input_tokens: int,\n+    prompt: str,\n+) -> SummarizedDescriptionResult:\n+    \"\"\"Run the graph intelligence entity extraction strategy.\"\"\"\n+    extractor = SummarizeExtractor(\n+        model=model,\n+        summarization_prompt=prompt,\n+        on_error=lambda e, stack, details: logger.error(\n+            \"Entity Extraction Error\",\n+            exc_info=e,\n+            extra={\"stack\": stack, \"details\": details},\n+        ),\n+        max_summary_length=max_summary_length,\n+        max_input_tokens=max_input_tokens,\n+    )\n+\n+    result = await extractor(id=id, descriptions=descriptions)\n+    return SummarizedDescriptionResult(id=result.id, description=result.description)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/operations/summarize_descriptions/typing.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/operations/summarize_descriptions/typing.py b/packages/graphrag/graphrag/index/operations/summarize_descriptions/typing.py\nnew file mode 100644\nindex 0000000..5a912ca\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/operations/summarize_descriptions/typing.py\n@@ -0,0 +1,21 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'SummarizedDescriptionResult' model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any, NamedTuple\n+\n+\n+@dataclass\n+class SummarizedDescriptionResult:\n+    \"\"\"Entity summarization result class definition.\"\"\"\n+\n+    id: str | tuple[str, str]\n+    description: str\n+\n+\n+class DescriptionSummarizeRow(NamedTuple):\n+    \"\"\"DescriptionSummarizeRow class definition.\"\"\"\n+\n+    graph: Any\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/run/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/run/__init__.py b/packages/graphrag/graphrag/index/run/__init__.py\nnew file mode 100644\nindex 0000000..d5e41d6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/run/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Run module for GraphRAG.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/run/run_pipeline.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/run/run_pipeline.py b/packages/graphrag/graphrag/index/run/run_pipeline.py\nnew file mode 100644\nindex 0000000..f652db7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/run/run_pipeline.py\n@@ -0,0 +1,167 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Different methods to run the pipeline.\"\"\"\n+\n+import json\n+import logging\n+import re\n+import time\n+from collections.abc import AsyncIterable\n+from dataclasses import asdict\n+from typing import Any\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import create_run_context\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.pipeline import Pipeline\n+from graphrag.index.typing.pipeline_run_result import PipelineRunResult\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.api import create_cache_from_config, create_storage_from_config\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_pipeline(\n+    pipeline: Pipeline,\n+    config: GraphRagConfig,\n+    callbacks: WorkflowCallbacks,\n+    is_update_run: bool = False,\n+    additional_context: dict[str, Any] | None = None,\n+    input_documents: pd.DataFrame | None = None,\n+) -> AsyncIterable[PipelineRunResult]:\n+    \"\"\"Run all workflows using a simplified pipeline.\"\"\"\n+    root_dir = config.root_dir\n+\n+    input_storage = create_storage_from_config(config.input.storage)\n+    output_storage = create_storage_from_config(config.output)\n+    cache = create_cache_from_config(config.cache, root_dir)\n+\n+    # load existing state in case any workflows are stateful\n+    state_json = await output_storage.get(\"context.json\")\n+    state = json.loads(state_json) if state_json else {}\n+\n+    if additional_context:\n+        state.setdefault(\"additional_context\", {}).update(additional_context)\n+\n+    if is_update_run:\n+        logger.info(\"Running incremental indexing.\")\n+\n+        update_storage = create_storage_from_config(config.update_index_output)\n+        # we use this to store the new subset index, and will merge its content with the previous index\n+        update_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n+        timestamped_storage = update_storage.child(update_timestamp)\n+        delta_storage = timestamped_storage.child(\"delta\")\n+        # copy the previous output to a backup folder, so we can replace it with the update\n+        # we'll read from this later when we merge the old and new indexes\n+        previous_storage = timestamped_storage.child(\"previous\")\n+        await _copy_previous_output(output_storage, previous_storage)\n+\n+        state[\"update_timestamp\"] = update_timestamp\n+\n+        # if the user passes in a df directly, write directly to storage so we can skip finding/parsing later\n+        if input_documents is not None:\n+            await write_table_to_storage(input_documents, \"documents\", delta_storage)\n+            pipeline.remove(\"load_update_documents\")\n+\n+        context = create_run_context(\n+            input_storage=input_storage,\n+            output_storage=delta_storage,\n+            previous_storage=previous_storage,\n+            cache=cache,\n+            callbacks=callbacks,\n+            state=state,\n+        )\n+\n+    else:\n+        logger.info(\"Running standard indexing.\")\n+\n+        # if the user passes in a df directly, write directly to storage so we can skip finding/parsing later\n+        if input_documents is not None:\n+            await write_table_to_storage(input_documents, \"documents\", output_storage)\n+            pipeline.remove(\"load_input_documents\")\n+\n+        context = create_run_context(\n+            input_storage=input_storage,\n+            output_storage=output_storage,\n+            cache=cache,\n+            callbacks=callbacks,\n+            state=state,\n+        )\n+\n+    async for table in _run_pipeline(\n+        pipeline=pipeline,\n+        config=config,\n+        context=context,\n+    ):\n+        yield table\n+\n+\n+async def _run_pipeline(\n+    pipeline: Pipeline,\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> AsyncIterable[PipelineRunResult]:\n+    start_time = time.time()\n+\n+    last_workflow = \"<startup>\"\n+\n+    try:\n+        await _dump_json(context)\n+\n+        logger.info(\"Executing pipeline...\")\n+        for name, workflow_function in pipeline.run():\n+            last_workflow = name\n+            context.callbacks.workflow_start(name, None)\n+            work_time = time.time()\n+            result = await workflow_function(config, context)\n+            context.callbacks.workflow_end(name, result)\n+            yield PipelineRunResult(\n+                workflow=name, result=result.result, state=context.state, errors=None\n+            )\n+            context.stats.workflows[name] = {\"overall\": time.time() - work_time}\n+            if result.stop:\n+                logger.info(\"Halting pipeline at workflow request\")\n+                break\n+\n+        context.stats.total_runtime = time.time() - start_time\n+        logger.info(\"Indexing pipeline complete.\")\n+        await _dump_json(context)\n+\n+    except Exception as e:\n+        logger.exception(\"error running workflow %s\", last_workflow)\n+        yield PipelineRunResult(\n+            workflow=last_workflow, result=None, state=context.state, errors=[e]\n+        )\n+\n+\n+async def _dump_json(context: PipelineRunContext) -> None:\n+    \"\"\"Dump the stats and context state to the storage.\"\"\"\n+    await context.output_storage.set(\n+        \"stats.json\", json.dumps(asdict(context.stats), indent=4, ensure_ascii=False)\n+    )\n+    # Dump context state, excluding additional_context\n+    temp_context = context.state.pop(\n+        \"additional_context\", None\n+    )  # Remove reference only, as object size is uncertain\n+    try:\n+        state_blob = json.dumps(context.state, indent=4, ensure_ascii=False)\n+    finally:\n+        if temp_context:\n+            context.state[\"additional_context\"] = temp_context\n+\n+    await context.output_storage.set(\"context.json\", state_blob)\n+\n+\n+async def _copy_previous_output(\n+    storage: PipelineStorage,\n+    copy_storage: PipelineStorage,\n+):\n+    for file in storage.find(re.compile(r\"\\.parquet$\")):\n+        base_name = file[0].replace(\".parquet\", \"\")\n+        table = await load_table_from_storage(base_name, storage)\n+        await write_table_to_storage(table, base_name, copy_storage)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/run/utils.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/run/utils.py b/packages/graphrag/graphrag/index/run/utils.py\nnew file mode 100644\nindex 0000000..52b1f0b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/run/utils.py\n@@ -0,0 +1,61 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Utility functions for the GraphRAG run module.\"\"\"\n+\n+from graphrag.cache.memory_pipeline_cache import InMemoryCache\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.callbacks.workflow_callbacks_manager import WorkflowCallbacksManager\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.state import PipelineState\n+from graphrag.index.typing.stats import PipelineRunStats\n+from graphrag.storage.memory_pipeline_storage import MemoryPipelineStorage\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.api import create_storage_from_config\n+\n+\n+def create_run_context(\n+    input_storage: PipelineStorage | None = None,\n+    output_storage: PipelineStorage | None = None,\n+    previous_storage: PipelineStorage | None = None,\n+    cache: PipelineCache | None = None,\n+    callbacks: WorkflowCallbacks | None = None,\n+    stats: PipelineRunStats | None = None,\n+    state: PipelineState | None = None,\n+) -> PipelineRunContext:\n+    \"\"\"Create the run context for the pipeline.\"\"\"\n+    return PipelineRunContext(\n+        input_storage=input_storage or MemoryPipelineStorage(),\n+        output_storage=output_storage or MemoryPipelineStorage(),\n+        previous_storage=previous_storage or MemoryPipelineStorage(),\n+        cache=cache or InMemoryCache(),\n+        callbacks=callbacks or NoopWorkflowCallbacks(),\n+        stats=stats or PipelineRunStats(),\n+        state=state or {},\n+    )\n+\n+\n+def create_callback_chain(\n+    callbacks: list[WorkflowCallbacks] | None,\n+) -> WorkflowCallbacks:\n+    \"\"\"Create a callback manager that encompasses multiple callbacks.\"\"\"\n+    manager = WorkflowCallbacksManager()\n+    for callback in callbacks or []:\n+        manager.register(callback)\n+    return manager\n+\n+\n+def get_update_storages(\n+    config: GraphRagConfig, timestamp: str\n+) -> tuple[PipelineStorage, PipelineStorage, PipelineStorage]:\n+    \"\"\"Get storage objects for the update index run.\"\"\"\n+    output_storage = create_storage_from_config(config.output)\n+    update_storage = create_storage_from_config(config.update_index_output)\n+    timestamped_storage = update_storage.child(timestamp)\n+    delta_storage = timestamped_storage.child(\"delta\")\n+    previous_storage = timestamped_storage.child(\"previous\")\n+\n+    return output_storage, previous_storage, delta_storage\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/text_splitting/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/text_splitting/__init__.py b/packages/graphrag/graphrag/index/text_splitting/__init__.py\nnew file mode 100644\nindex 0000000..e6f3b31\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/text_splitting/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The Indexing Engine Text Splitting package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/text_splitting/check_token_limit.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/text_splitting/check_token_limit.py b/packages/graphrag/graphrag/index/text_splitting/check_token_limit.py\nnew file mode 100644\nindex 0000000..7b6a139\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/text_splitting/check_token_limit.py\n@@ -0,0 +1,15 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Token limit method definition.\"\"\"\n+\n+from graphrag.index.text_splitting.text_splitting import TokenTextSplitter\n+\n+\n+def check_token_limit(text, max_token):\n+    \"\"\"Check token limit.\"\"\"\n+    text_splitter = TokenTextSplitter(chunk_size=max_token, chunk_overlap=0)\n+    docs = text_splitter.split_text(text)\n+    if len(docs) > 1:\n+        return 0\n+    return 1\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/text_splitting/text_splitting.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/text_splitting/text_splitting.py b/packages/graphrag/graphrag/index/text_splitting/text_splitting.py\nnew file mode 100644\nindex 0000000..caa9153\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/text_splitting/text_splitting.py\n@@ -0,0 +1,173 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the 'Tokenizer', 'TextSplitter', 'NoopTextSplitter' and 'TokenTextSplitter' models.\"\"\"\n+\n+import logging\n+from abc import ABC, abstractmethod\n+from collections.abc import Callable, Iterable\n+from dataclasses import dataclass\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.index.operations.chunk_text.typing import TextChunk\n+from graphrag.logger.progress import ProgressTicker\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+EncodedText = list[int]\n+DecodeFn = Callable[[EncodedText], str]\n+EncodeFn = Callable[[str], EncodedText]\n+LengthFn = Callable[[str], int]\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass(frozen=True)\n+class TokenChunkerOptions:\n+    \"\"\"TokenChunkerOptions data class.\"\"\"\n+\n+    chunk_overlap: int\n+    \"\"\"Overlap in tokens between chunks\"\"\"\n+    tokens_per_chunk: int\n+    \"\"\"Maximum number of tokens per chunk\"\"\"\n+    decode: DecodeFn\n+    \"\"\" Function to decode a list of token ids to a string\"\"\"\n+    encode: EncodeFn\n+    \"\"\" Function to encode a string to a list of token ids\"\"\"\n+\n+\n+class TextSplitter(ABC):\n+    \"\"\"Text splitter class definition.\"\"\"\n+\n+    _chunk_size: int\n+    _chunk_overlap: int\n+    _length_function: LengthFn\n+    _keep_separator: bool\n+    _add_start_index: bool\n+    _strip_whitespace: bool\n+\n+    def __init__(\n+        self,\n+        # based on OpenAI embedding chunk size limits\n+        # https://devblogs.microsoft.com/azure-sql/embedding-models-and-dimensions-optimizing-the-performance-resource-usage-ratio/\n+        chunk_size: int = 8191,\n+        chunk_overlap: int = 100,\n+        length_function: LengthFn = len,\n+        keep_separator: bool = False,\n+        add_start_index: bool = False,\n+        strip_whitespace: bool = True,\n+    ):\n+        \"\"\"Init method definition.\"\"\"\n+        self._chunk_size = chunk_size\n+        self._chunk_overlap = chunk_overlap\n+        self._length_function = length_function\n+        self._keep_separator = keep_separator\n+        self._add_start_index = add_start_index\n+        self._strip_whitespace = strip_whitespace\n+\n+    @abstractmethod\n+    def split_text(self, text: str | list[str]) -> Iterable[str]:\n+        \"\"\"Split text method definition.\"\"\"\n+\n+\n+class NoopTextSplitter(TextSplitter):\n+    \"\"\"Noop text splitter class definition.\"\"\"\n+\n+    def split_text(self, text: str | list[str]) -> Iterable[str]:\n+        \"\"\"Split text method definition.\"\"\"\n+        return [text] if isinstance(text, str) else text\n+\n+\n+class TokenTextSplitter(TextSplitter):\n+    \"\"\"Token text splitter class definition.\"\"\"\n+\n+    def __init__(\n+        self,\n+        tokenizer: Tokenizer | None = None,\n+        **kwargs: Any,\n+    ):\n+        \"\"\"Init method definition.\"\"\"\n+        super().__init__(**kwargs)\n+        self._tokenizer = tokenizer or get_tokenizer()\n+\n+    def num_tokens(self, text: str) -> int:\n+        \"\"\"Return the number of tokens in a string.\"\"\"\n+        return self._tokenizer.num_tokens(text)\n+\n+    def split_text(self, text: str | list[str]) -> list[str]:\n+        \"\"\"Split text method.\"\"\"\n+        if isinstance(text, list):\n+            text = \" \".join(text)\n+        elif cast(\"bool\", pd.isna(text)) or text == \"\":\n+            return []\n+        if not isinstance(text, str):\n+            msg = f\"Attempting to split a non-string value, actual is {type(text)}\"\n+            raise TypeError(msg)\n+\n+        token_chunker_options = TokenChunkerOptions(\n+            chunk_overlap=self._chunk_overlap,\n+            tokens_per_chunk=self._chunk_size,\n+            decode=self._tokenizer.decode,\n+            encode=self._tokenizer.encode,\n+        )\n+\n+        return split_single_text_on_tokens(text=text, tokenizer=token_chunker_options)\n+\n+\n+def split_single_text_on_tokens(text: str, tokenizer: TokenChunkerOptions) -> list[str]:\n+    \"\"\"Split a single text and return chunks using the tokenizer.\"\"\"\n+    result = []\n+    input_ids = tokenizer.encode(text)\n+\n+    start_idx = 0\n+    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n+    chunk_ids = input_ids[start_idx:cur_idx]\n+\n+    while start_idx < len(input_ids):\n+        chunk_text = tokenizer.decode(list(chunk_ids))\n+        result.append(chunk_text)  # Append chunked text as string\n+        if cur_idx == len(input_ids):\n+            break\n+        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n+        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n+        chunk_ids = input_ids[start_idx:cur_idx]\n+\n+    return result\n+\n+\n+# Adapted from - https://github.com/langchain-ai/langchain/blob/77b359edf5df0d37ef0d539f678cf64f5557cb54/libs/langchain/langchain/text_splitter.py#L471\n+# So we could have better control over the chunking process\n+def split_multiple_texts_on_tokens(\n+    texts: list[str], tokenizer: TokenChunkerOptions, tick: ProgressTicker\n+) -> list[TextChunk]:\n+    \"\"\"Split multiple texts and return chunks with metadata using the tokenizer.\"\"\"\n+    result = []\n+    mapped_ids = []\n+\n+    for source_doc_idx, text in enumerate(texts):\n+        encoded = tokenizer.encode(text)\n+        if tick:\n+            tick(1)  # Track progress if tick callback is provided\n+        mapped_ids.append((source_doc_idx, encoded))\n+\n+    input_ids = [\n+        (source_doc_idx, id) for source_doc_idx, ids in mapped_ids for id in ids\n+    ]\n+\n+    start_idx = 0\n+    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n+    chunk_ids = input_ids[start_idx:cur_idx]\n+\n+    while start_idx < len(input_ids):\n+        chunk_text = tokenizer.decode([id for _, id in chunk_ids])\n+        doc_indices = list({doc_idx for doc_idx, _ in chunk_ids})\n+        result.append(TextChunk(chunk_text, doc_indices, len(chunk_ids)))\n+        if cur_idx == len(input_ids):\n+            break\n+        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n+        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n+        chunk_ids = input_ids[start_idx:cur_idx]\n+\n+    return result\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/__init__.py b/packages/graphrag/graphrag/index/typing/__init__.py\nnew file mode 100644\nindex 0000000..a1b33f3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Root typings for GraphRAG.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/context.py b/packages/graphrag/graphrag/index/typing/context.py\nnew file mode 100644\nindex 0000000..ef2e1f7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/context.py\n@@ -0,0 +1,32 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+# isort: skip_file\n+\"\"\"A module containing the 'PipelineRunContext' models.\"\"\"\n+\n+from dataclasses import dataclass\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.index.typing.state import PipelineState\n+from graphrag.index.typing.stats import PipelineRunStats\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+\n+@dataclass\n+class PipelineRunContext:\n+    \"\"\"Provides the context for the current pipeline run.\"\"\"\n+\n+    stats: PipelineRunStats\n+    input_storage: PipelineStorage\n+    \"Storage for input documents.\"\n+    output_storage: PipelineStorage\n+    \"Long-term storage for pipeline verbs to use. Items written here will be written to the storage provider.\"\n+    previous_storage: PipelineStorage\n+    \"Storage for previous pipeline run when running in update mode.\"\n+    cache: PipelineCache\n+    \"Cache instance for reading previous LLM responses.\"\n+    callbacks: WorkflowCallbacks\n+    \"Callbacks to be called during the pipeline run.\"\n+    state: PipelineState\n+    \"Arbitrary property bag for runtime state, persistent pre-computes, or experimental features.\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/error_handler.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/error_handler.py b/packages/graphrag/graphrag/index/typing/error_handler.py\nnew file mode 100644\nindex 0000000..a564a79\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/error_handler.py\n@@ -0,0 +1,8 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Shared error handler types.\"\"\"\n+\n+from collections.abc import Callable\n+\n+ErrorHandlerFn = Callable[[BaseException | None, str | None, dict | None], None]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/pipeline.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/pipeline.py b/packages/graphrag/graphrag/index/typing/pipeline.py\nnew file mode 100644\nindex 0000000..4e755ba\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/pipeline.py\n@@ -0,0 +1,27 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the Pipeline class.\"\"\"\n+\n+from collections.abc import Generator\n+\n+from graphrag.index.typing.workflow import Workflow\n+\n+\n+class Pipeline:\n+    \"\"\"Encapsulates running workflows.\"\"\"\n+\n+    def __init__(self, workflows: list[Workflow]):\n+        self.workflows = workflows\n+\n+    def run(self) -> Generator[Workflow]:\n+        \"\"\"Return a Generator over the pipeline workflows.\"\"\"\n+        yield from self.workflows\n+\n+    def names(self) -> list[str]:\n+        \"\"\"Return the names of the workflows in the pipeline.\"\"\"\n+        return [name for name, _ in self.workflows]\n+\n+    def remove(self, name: str) -> None:\n+        \"\"\"Remove a workflow from the pipeline by name.\"\"\"\n+        self.workflows = [w for w in self.workflows if w[0] != name]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/pipeline_run_result.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/pipeline_run_result.py b/packages/graphrag/graphrag/index/typing/pipeline_run_result.py\nnew file mode 100644\nindex 0000000..f6a68d8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/pipeline_run_result.py\n@@ -0,0 +1,22 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing the PipelineRunResult class.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.index.typing.state import PipelineState\n+\n+\n+@dataclass\n+class PipelineRunResult:\n+    \"\"\"Pipeline run result class definition.\"\"\"\n+\n+    workflow: str\n+    \"\"\"The name of the workflow that was executed.\"\"\"\n+    result: Any | None\n+    \"\"\"The result of the workflow function. This can be anything - we use it only for logging downstream, and expect each workflow function to write official outputs to the provided storage.\"\"\"\n+    state: PipelineState\n+    \"\"\"Ongoing pipeline context state object.\"\"\"\n+    errors: list[BaseException] | None\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/state.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/state.py b/packages/graphrag/graphrag/index/typing/state.py\nnew file mode 100644\nindex 0000000..b9990c6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/state.py\n@@ -0,0 +1,8 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Pipeline state types.\"\"\"\n+\n+from typing import Any\n+\n+PipelineState = dict[Any, Any]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/stats.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/stats.py b/packages/graphrag/graphrag/index/typing/stats.py\nnew file mode 100644\nindex 0000000..2717736\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/stats.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Pipeline stats types.\"\"\"\n+\n+from dataclasses import dataclass, field\n+\n+\n+@dataclass\n+class PipelineRunStats:\n+    \"\"\"Pipeline running stats.\"\"\"\n+\n+    total_runtime: float = field(default=0)\n+    \"\"\"Float representing the total runtime.\"\"\"\n+\n+    num_documents: int = field(default=0)\n+    \"\"\"Number of documents.\"\"\"\n+    update_documents: int = field(default=0)\n+    \"\"\"Number of update documents.\"\"\"\n+\n+    input_load_time: float = field(default=0)\n+    \"\"\"Float representing the input load time.\"\"\"\n+\n+    workflows: dict[str, dict[str, float]] = field(default_factory=dict)\n+    \"\"\"A dictionary of workflows.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/typing/workflow.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/typing/workflow.py b/packages/graphrag/graphrag/index/typing/workflow.py\nnew file mode 100644\nindex 0000000..89538b0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/typing/workflow.py\n@@ -0,0 +1,28 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Pipeline workflow types.\"\"\"\n+\n+from collections.abc import Awaitable, Callable\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.typing.context import PipelineRunContext\n+\n+\n+@dataclass\n+class WorkflowFunctionOutput:\n+    \"\"\"Data container for Workflow function results.\"\"\"\n+\n+    result: Any | None\n+    \"\"\"The result of the workflow function. This can be anything - we use it only for logging downstream, and expect each workflow function to write official outputs to the provided storage.\"\"\"\n+    stop: bool = False\n+    \"\"\"Flag to indicate if the workflow should stop after this function. This should only be used when continuation could cause an unstable failure.\"\"\"\n+\n+\n+WorkflowFunction = Callable[\n+    [GraphRagConfig, PipelineRunContext],\n+    Awaitable[WorkflowFunctionOutput],\n+]\n+Workflow = tuple[str, WorkflowFunction]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/update/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/update/__init__.py b/packages/graphrag/graphrag/index/update/__init__.py\nnew file mode 100644\nindex 0000000..e696640\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/update/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Incremental Indexing main module definition.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/update/communities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/update/communities.py b/packages/graphrag/graphrag/index/update/communities.py\nnew file mode 100644\nindex 0000000..70418e0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/update/communities.py\n@@ -0,0 +1,151 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Dataframe operations and utils for Incremental Indexing.\"\"\"\n+\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import (\n+    COMMUNITIES_FINAL_COLUMNS,\n+    COMMUNITY_REPORTS_FINAL_COLUMNS,\n+)\n+\n+\n+def _update_and_merge_communities(\n+    old_communities: pd.DataFrame,\n+    delta_communities: pd.DataFrame,\n+) -> tuple[pd.DataFrame, dict]:\n+    \"\"\"Update and merge communities.\n+\n+    Parameters\n+    ----------\n+    old_communities : pd.DataFrame\n+        The old communities.\n+    delta_communities : pd.DataFrame\n+        The delta communities.\n+    community_id_mapping : dict\n+        The community id mapping.\n+\n+    Returns\n+    -------\n+    pd.DataFrame\n+        The updated communities.\n+    \"\"\"\n+    # Check if size and period columns exist in the old_communities. If not, add them\n+    if \"size\" not in old_communities.columns:\n+        old_communities[\"size\"] = None\n+    if \"period\" not in old_communities.columns:\n+        old_communities[\"period\"] = None\n+\n+    # Same for delta_communities\n+    if \"size\" not in delta_communities.columns:\n+        delta_communities[\"size\"] = None\n+    if \"period\" not in delta_communities.columns:\n+        delta_communities[\"period\"] = None\n+\n+    # Increment all community ids by the max of the old communities\n+    old_max_community_id = old_communities[\"community\"].fillna(0).astype(int).max()\n+    # Increment only the non-NaN values in delta_communities[\"community\"]\n+    community_id_mapping = {\n+        v: v + old_max_community_id + 1\n+        for k, v in delta_communities[\"community\"].dropna().astype(int).items()\n+    }\n+    community_id_mapping.update({-1: -1})\n+\n+    # Look for community ids in community and replace them with the corresponding id in the mapping\n+    delta_communities[\"community\"] = (\n+        delta_communities[\"community\"]\n+        .astype(int)\n+        .apply(lambda x: community_id_mapping.get(x, x))\n+    )\n+\n+    delta_communities[\"parent\"] = (\n+        delta_communities[\"parent\"]\n+        .astype(int)\n+        .apply(lambda x: community_id_mapping.get(x, x))\n+    )\n+\n+    old_communities[\"community\"] = old_communities[\"community\"].astype(int)\n+\n+    # Merge the final communities\n+    merged_communities = pd.concat(\n+        [old_communities, delta_communities], ignore_index=True, copy=False\n+    )\n+\n+    # Rename title\n+    merged_communities[\"title\"] = \"Community \" + merged_communities[\"community\"].astype(\n+        str\n+    )\n+    # Re-assign the human_readable_id\n+    merged_communities[\"human_readable_id\"] = merged_communities[\"community\"]\n+\n+    merged_communities = merged_communities.loc[\n+        :,\n+        COMMUNITIES_FINAL_COLUMNS,\n+    ]\n+    return merged_communities, community_id_mapping\n+\n+\n+def _update_and_merge_community_reports(\n+    old_community_reports: pd.DataFrame,\n+    delta_community_reports: pd.DataFrame,\n+    community_id_mapping: dict,\n+) -> pd.DataFrame:\n+    \"\"\"Update and merge community reports.\n+\n+    Parameters\n+    ----------\n+    old_community_reports : pd.DataFrame\n+        The old community reports.\n+    delta_community_reports : pd.DataFrame\n+        The delta community reports.\n+    community_id_mapping : dict\n+        The community id mapping.\n+\n+    Returns\n+    -------\n+    pd.DataFrame\n+        The updated community reports.\n+    \"\"\"\n+    # Check if size and period columns exist in the old_community_reports. If not, add them\n+    if \"size\" not in old_community_reports.columns:\n+        old_community_reports[\"size\"] = None\n+    if \"period\" not in old_community_reports.columns:\n+        old_community_reports[\"period\"] = None\n+\n+    # Same for delta_community_reports\n+    if \"size\" not in delta_community_reports.columns:\n+        delta_community_reports[\"size\"] = None\n+    if \"period\" not in delta_community_reports.columns:\n+        delta_community_reports[\"period\"] = None\n+\n+    # Look for community ids in community and replace them with the corresponding id in the mapping\n+    delta_community_reports[\"community\"] = (\n+        delta_community_reports[\"community\"]\n+        .astype(int)\n+        .apply(lambda x: community_id_mapping.get(x, x))\n+    )\n+\n+    delta_community_reports[\"parent\"] = (\n+        delta_community_reports[\"parent\"]\n+        .astype(int)\n+        .apply(lambda x: community_id_mapping.get(x, x))\n+    )\n+\n+    old_community_reports[\"community\"] = old_community_reports[\"community\"].astype(int)\n+\n+    # Merge the final community reports\n+    merged_community_reports = pd.concat(\n+        [old_community_reports, delta_community_reports], ignore_index=True, copy=False\n+    )\n+\n+    # Maintain type compat with query\n+    merged_community_reports[\"community\"] = merged_community_reports[\n+        \"community\"\n+    ].astype(int)\n+    # Re-assign the human_readable_id\n+    merged_community_reports[\"human_readable_id\"] = merged_community_reports[\n+        \"community\"\n+    ]\n+\n+    return merged_community_reports.loc[:, COMMUNITY_REPORTS_FINAL_COLUMNS]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/update/entities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/update/entities.py b/packages/graphrag/graphrag/index/update/entities.py\nnew file mode 100644\nindex 0000000..56707af\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/update/entities.py\n@@ -0,0 +1,76 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Entity related operations and utils for Incremental Indexing.\"\"\"\n+\n+import itertools\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import ENTITIES_FINAL_COLUMNS\n+\n+\n+def _group_and_resolve_entities(\n+    old_entities_df: pd.DataFrame, delta_entities_df: pd.DataFrame\n+) -> tuple[pd.DataFrame, dict]:\n+    \"\"\"Group and resolve entities.\n+\n+    Parameters\n+    ----------\n+    old_entities_df : pd.DataFrame\n+        The first dataframe.\n+    delta_entities_df : pd.DataFrame\n+        The delta dataframe.\n+\n+    Returns\n+    -------\n+    pd.DataFrame\n+        The resolved dataframe.\n+    dict\n+        The id mapping for existing entities. In the form of {df_b.id: df_a.id}.\n+    \"\"\"\n+    # If a title exists in A and B, make a dictionary for {B.id : A.id}\n+    merged = delta_entities_df[[\"id\", \"title\"]].merge(\n+        old_entities_df[[\"id\", \"title\"]],\n+        on=\"title\",\n+        suffixes=(\"_B\", \"_A\"),\n+        copy=False,\n+    )\n+    id_mapping = dict(zip(merged[\"id_B\"], merged[\"id_A\"], strict=True))\n+\n+    # Increment human readable id in b by the max of a\n+    initial_id = old_entities_df[\"human_readable_id\"].max() + 1\n+    delta_entities_df[\"human_readable_id\"] = np.arange(\n+        initial_id, initial_id + len(delta_entities_df)\n+    )\n+    # Concat A and B\n+    combined = pd.concat(\n+        [old_entities_df, delta_entities_df], ignore_index=True, copy=False\n+    )\n+\n+    # Group by title and resolve conflicts\n+    aggregated = (\n+        combined.groupby(\"title\")\n+        .agg({\n+            \"id\": \"first\",\n+            \"type\": \"first\",\n+            \"human_readable_id\": \"first\",\n+            \"description\": lambda x: list(x.astype(str)),  # Ensure str\n+            # Concatenate nd.array into a single list\n+            \"text_unit_ids\": lambda x: list(itertools.chain(*x.tolist())),\n+            \"degree\": \"first\",  # todo: we could probably re-compute this with the entire new graph\n+        })\n+        .reset_index()\n+    )\n+\n+    # recompute frequency to include new text units\n+    aggregated[\"frequency\"] = aggregated[\"text_unit_ids\"].apply(len)\n+\n+    # Force the result into a DataFrame\n+    resolved: pd.DataFrame = pd.DataFrame(aggregated)\n+\n+    # Modify column order to keep consistency\n+    resolved = resolved.loc[:, ENTITIES_FINAL_COLUMNS]\n+\n+    return resolved, id_mapping\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/update/incremental_index.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/update/incremental_index.py b/packages/graphrag/graphrag/index/update/incremental_index.py\nnew file mode 100644\nindex 0000000..ac56e30\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/update/incremental_index.py\n@@ -0,0 +1,83 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Dataframe operations and utils for Incremental Indexing.\"\"\"\n+\n+from dataclasses import dataclass\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import (\n+    load_table_from_storage,\n+    write_table_to_storage,\n+)\n+\n+\n+@dataclass\n+class InputDelta:\n+    \"\"\"Dataclass to hold the input delta.\n+\n+    Attributes\n+    ----------\n+    new_inputs : pd.DataFrame\n+        The new inputs.\n+    deleted_inputs : pd.DataFrame\n+        The deleted inputs.\n+    \"\"\"\n+\n+    new_inputs: pd.DataFrame\n+    deleted_inputs: pd.DataFrame\n+\n+\n+async def get_delta_docs(\n+    input_dataset: pd.DataFrame, storage: PipelineStorage\n+) -> InputDelta:\n+    \"\"\"Get the delta between the input dataset and the final documents.\n+\n+    Parameters\n+    ----------\n+    input_dataset : pd.DataFrame\n+        The input dataset.\n+    storage : PipelineStorage\n+        The Pipeline storage.\n+\n+    Returns\n+    -------\n+    InputDelta\n+        The input delta. With new inputs and deleted inputs.\n+    \"\"\"\n+    final_docs = await load_table_from_storage(\"documents\", storage)\n+\n+    # Select distinct title from final docs and from dataset\n+    previous_docs: list[str] = final_docs[\"title\"].unique().tolist()\n+    dataset_docs: list[str] = input_dataset[\"title\"].unique().tolist()\n+\n+    # Get the new documents (using loc to ensure DataFrame)\n+    new_docs = input_dataset.loc[~input_dataset[\"title\"].isin(previous_docs)]\n+\n+    # Get the deleted documents (again using loc to ensure DataFrame)\n+    deleted_docs = final_docs.loc[~final_docs[\"title\"].isin(dataset_docs)]\n+\n+    return InputDelta(new_docs, deleted_docs)\n+\n+\n+async def concat_dataframes(\n+    name: str,\n+    previous_storage: PipelineStorage,\n+    delta_storage: PipelineStorage,\n+    output_storage: PipelineStorage,\n+) -> pd.DataFrame:\n+    \"\"\"Concatenate dataframes.\"\"\"\n+    old_df = await load_table_from_storage(name, previous_storage)\n+    delta_df = await load_table_from_storage(name, delta_storage)\n+\n+    # Merge the final documents\n+    initial_id = old_df[\"human_readable_id\"].max() + 1\n+    delta_df[\"human_readable_id\"] = np.arange(initial_id, initial_id + len(delta_df))\n+    final_df = pd.concat([old_df, delta_df], ignore_index=True, copy=False)\n+\n+    await write_table_to_storage(final_df, name, output_storage)\n+\n+    return final_df\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/update/relationships.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/update/relationships.py b/packages/graphrag/graphrag/index/update/relationships.py\nnew file mode 100644\nindex 0000000..8174d9c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/update/relationships.py\n@@ -0,0 +1,85 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Relationship related operations and utils for Incremental Indexing.\"\"\"\n+\n+import itertools\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.data_model.schemas import RELATIONSHIPS_FINAL_COLUMNS\n+\n+\n+def _update_and_merge_relationships(\n+    old_relationships: pd.DataFrame, delta_relationships: pd.DataFrame\n+) -> pd.DataFrame:\n+    \"\"\"Update and merge relationships.\n+\n+    Parameters\n+    ----------\n+    old_relationships : pd.DataFrame\n+        The old relationships.\n+    delta_relationships : pd.DataFrame\n+        The delta relationships.\n+\n+    Returns\n+    -------\n+    pd.DataFrame\n+        The updated relationships.\n+    \"\"\"\n+    # Increment the human readable id in b by the max of a\n+    # Ensure both columns are integers\n+    delta_relationships[\"human_readable_id\"] = delta_relationships[\n+        \"human_readable_id\"\n+    ].astype(int)\n+    old_relationships[\"human_readable_id\"] = old_relationships[\n+        \"human_readable_id\"\n+    ].astype(int)\n+\n+    # Adjust delta_relationships IDs to be greater than any in old_relationships\n+    initial_id = old_relationships[\"human_readable_id\"].max() + 1\n+    delta_relationships[\"human_readable_id\"] = np.arange(\n+        initial_id, initial_id + len(delta_relationships)\n+    )\n+\n+    # Merge the DataFrames without copying if possible\n+    merged_relationships = pd.concat(\n+        [old_relationships, delta_relationships], ignore_index=True, copy=False\n+    )\n+\n+    # Group by title and resolve conflicts\n+    aggregated = (\n+        merged_relationships.groupby([\"source\", \"target\"])\n+        .agg({\n+            \"id\": \"first\",\n+            \"human_readable_id\": \"first\",\n+            \"description\": lambda x: list(x.astype(str)),  # Ensure str\n+            # Concatenate nd.array into a single list\n+            \"text_unit_ids\": lambda x: list(itertools.chain(*x.tolist())),\n+            \"weight\": \"mean\",\n+            \"combined_degree\": \"sum\",\n+        })\n+        .reset_index()\n+    )\n+\n+    # Force the result into a DataFrame\n+    final_relationships: pd.DataFrame = pd.DataFrame(aggregated)\n+\n+    # Recalculate target and source degrees\n+    final_relationships[\"source_degree\"] = final_relationships.groupby(\"source\")[\n+        \"target\"\n+    ].transform(\"count\")\n+    final_relationships[\"target_degree\"] = final_relationships.groupby(\"target\")[\n+        \"source\"\n+    ].transform(\"count\")\n+\n+    # Recalculate the combined_degree of the relationships (source degree + target degree)\n+    final_relationships[\"combined_degree\"] = (\n+        final_relationships[\"source_degree\"] + final_relationships[\"target_degree\"]\n+    )\n+\n+    return final_relationships.loc[\n+        :,\n+        RELATIONSHIPS_FINAL_COLUMNS,\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/__init__.py b/packages/graphrag/graphrag/index/utils/__init__.py\nnew file mode 100644\nindex 0000000..d1737fc\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Utils methods definition.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/dataframes.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/dataframes.py b/packages/graphrag/graphrag/index/utils/dataframes.py\nnew file mode 100644\nindex 0000000..949f40f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/dataframes.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing DataFrame utilities.\"\"\"\n+\n+from collections.abc import Callable\n+from typing import Any, cast\n+\n+import pandas as pd\n+from pandas._typing import MergeHow\n+\n+\n+def drop_columns(df: pd.DataFrame, *column: str) -> pd.DataFrame:\n+    \"\"\"Drop columns from a dataframe.\"\"\"\n+    return df.drop(list(column), axis=1)\n+\n+\n+def where_column_equals(df: pd.DataFrame, column: str, value: Any) -> pd.DataFrame:\n+    \"\"\"Return a filtered DataFrame where a column equals a value.\"\"\"\n+    return cast(\"pd.DataFrame\", df[df[column] == value])\n+\n+\n+def antijoin(df: pd.DataFrame, exclude: pd.DataFrame, column: str) -> pd.DataFrame:\n+    \"\"\"Return an anti-joined dataframe.\n+\n+    Arguments:\n+    * df: The DataFrame to apply the exclusion to\n+    * exclude: The DataFrame containing rows to remove.\n+    * column: The join-on column.\n+    \"\"\"\n+    return df.loc[~df.loc[:, column].isin(exclude.loc[:, column])]\n+\n+\n+def transform_series(series: pd.Series, fn: Callable[[Any], Any]) -> pd.Series:\n+    \"\"\"Apply a transformation function to a series.\"\"\"\n+    return cast(\"pd.Series\", series.apply(fn))\n+\n+\n+def join(\n+    left: pd.DataFrame, right: pd.DataFrame, key: str, strategy: MergeHow = \"left\"\n+) -> pd.DataFrame:\n+    \"\"\"Perform a table join.\"\"\"\n+    return left.merge(right, on=key, how=strategy)\n+\n+\n+def union(*frames: pd.DataFrame) -> pd.DataFrame:\n+    \"\"\"Perform a union operation on the given set of dataframes.\"\"\"\n+    return pd.concat(list(frames))\n+\n+\n+def select(df: pd.DataFrame, *columns: str) -> pd.DataFrame:\n+    \"\"\"Select columns from a dataframe.\"\"\"\n+    return cast(\"pd.DataFrame\", df[list(columns)])\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/derive_from_rows.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/derive_from_rows.py b/packages/graphrag/graphrag/index/utils/derive_from_rows.py\nnew file mode 100644\nindex 0000000..663d78c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/derive_from_rows.py\n@@ -0,0 +1,173 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Apply a generic transform function to each row in a table.\"\"\"\n+\n+import asyncio\n+import inspect\n+import logging\n+import traceback\n+from collections.abc import Awaitable, Callable, Coroutine, Hashable\n+from typing import Any, TypeVar, cast\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.logger.progress import progress_ticker\n+\n+logger = logging.getLogger(__name__)\n+ItemType = TypeVar(\"ItemType\")\n+\n+\n+class ParallelizationError(ValueError):\n+    \"\"\"Exception for invalid parallel processing.\"\"\"\n+\n+    def __init__(self, num_errors: int, example: str | None = None):\n+        msg = f\"{num_errors} Errors occurred while running parallel transformation, could not complete!\"\n+        if example:\n+            msg += f\"\\nExample error: {example}\"\n+        super().__init__(msg)\n+\n+\n+async def derive_from_rows(\n+    input: pd.DataFrame,\n+    transform: Callable[[pd.Series], Awaitable[ItemType]],\n+    callbacks: WorkflowCallbacks | None = None,\n+    num_threads: int = 4,\n+    async_type: AsyncType = AsyncType.AsyncIO,\n+    progress_msg: str = \"\",\n+) -> list[ItemType | None]:\n+    \"\"\"Apply a generic transform function to each row. Any errors will be reported and thrown.\"\"\"\n+    callbacks = callbacks or NoopWorkflowCallbacks()\n+    match async_type:\n+        case AsyncType.AsyncIO:\n+            return await derive_from_rows_asyncio(\n+                input, transform, callbacks, num_threads, progress_msg\n+            )\n+        case AsyncType.Threaded:\n+            return await derive_from_rows_asyncio_threads(\n+                input, transform, callbacks, num_threads, progress_msg\n+            )\n+        case _:\n+            msg = f\"Unsupported scheduling type {async_type}\"\n+            raise ValueError(msg)\n+\n+\n+\"\"\"A module containing the derive_from_rows_async method.\"\"\"\n+\n+\n+async def derive_from_rows_asyncio_threads(\n+    input: pd.DataFrame,\n+    transform: Callable[[pd.Series], Awaitable[ItemType]],\n+    callbacks: WorkflowCallbacks,\n+    num_threads: int | None = 4,\n+    progress_msg: str = \"\",\n+) -> list[ItemType | None]:\n+    \"\"\"\n+    Derive from rows asynchronously.\n+\n+    This is useful for IO bound operations.\n+    \"\"\"\n+    semaphore = asyncio.Semaphore(num_threads or 4)\n+\n+    async def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]:\n+        tasks = [asyncio.to_thread(execute, row) for row in input.iterrows()]\n+\n+        async def execute_task(task: Coroutine) -> ItemType | None:\n+            async with semaphore:\n+                # fire off the thread\n+                thread = await task\n+                return await thread\n+\n+        return await asyncio.gather(*[execute_task(task) for task in tasks])\n+\n+    return await _derive_from_rows_base(\n+        input, transform, callbacks, gather, progress_msg\n+    )\n+\n+\n+\"\"\"A module containing the derive_from_rows_async method.\"\"\"\n+\n+\n+async def derive_from_rows_asyncio(\n+    input: pd.DataFrame,\n+    transform: Callable[[pd.Series], Awaitable[ItemType]],\n+    callbacks: WorkflowCallbacks,\n+    num_threads: int = 4,\n+    progress_msg: str = \"\",\n+) -> list[ItemType | None]:\n+    \"\"\"\n+    Derive from rows asynchronously.\n+\n+    This is useful for IO bound operations.\n+    \"\"\"\n+    semaphore = asyncio.Semaphore(num_threads or 4)\n+\n+    async def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]:\n+        async def execute_row_protected(\n+            row: tuple[Hashable, pd.Series],\n+        ) -> ItemType | None:\n+            async with semaphore:\n+                return await execute(row)\n+\n+        tasks = [\n+            asyncio.create_task(execute_row_protected(row)) for row in input.iterrows()\n+        ]\n+        return await asyncio.gather(*tasks)\n+\n+    return await _derive_from_rows_base(\n+        input, transform, callbacks, gather, progress_msg\n+    )\n+\n+\n+ItemType = TypeVar(\"ItemType\")\n+\n+ExecuteFn = Callable[[tuple[Hashable, pd.Series]], Awaitable[ItemType | None]]\n+GatherFn = Callable[[ExecuteFn], Awaitable[list[ItemType | None]]]\n+\n+\n+async def _derive_from_rows_base(\n+    input: pd.DataFrame,\n+    transform: Callable[[pd.Series], Awaitable[ItemType]],\n+    callbacks: WorkflowCallbacks,\n+    gather: GatherFn[ItemType],\n+    progress_msg: str = \"\",\n+) -> list[ItemType | None]:\n+    \"\"\"\n+    Derive from rows asynchronously.\n+\n+    This is useful for IO bound operations.\n+    \"\"\"\n+    tick = progress_ticker(\n+        callbacks.progress, num_total=len(input), description=progress_msg\n+    )\n+    errors: list[tuple[BaseException, str]] = []\n+\n+    async def execute(row: tuple[Any, pd.Series]) -> ItemType | None:\n+        try:\n+            result = transform(row[1])\n+            if inspect.iscoroutine(result):\n+                result = await result\n+        except Exception as e:  # noqa: BLE001\n+            errors.append((e, traceback.format_exc()))\n+            return None\n+        else:\n+            return cast(\"ItemType\", result)\n+        finally:\n+            tick(1)\n+\n+    result = await gather(execute)\n+\n+    tick.done()\n+\n+    for error, stack in errors:\n+        logger.error(\n+            \"parallel transformation error\", exc_info=error, extra={\"stack\": stack}\n+        )\n+\n+    if len(errors) > 0:\n+        raise ParallelizationError(len(errors), errors[0][1])\n+\n+    return result\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/dicts.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/dicts.py b/packages/graphrag/graphrag/index/utils/dicts.py\nnew file mode 100644\nindex 0000000..e7ee746\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/dicts.py\n@@ -0,0 +1,22 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A utility module containing methods for inspecting and verifying dictionary types.\"\"\"\n+\n+\n+def dict_has_keys_with_types(\n+    data: dict, expected_fields: list[tuple[str, type]], inplace: bool = False\n+) -> bool:\n+    \"\"\"Return True if the given dictionary has the given keys with the given types.\"\"\"\n+    for field, field_type in expected_fields:\n+        if field not in data:\n+            return False\n+\n+        value = data[field]\n+        try:\n+            cast_value = field_type(value)\n+            if inplace:\n+                data[field] = cast_value\n+        except (TypeError, ValueError):\n+            return False\n+    return True\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/graphs.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/graphs.py b/packages/graphrag/graphrag/index/utils/graphs.py\nnew file mode 100644\nindex 0000000..028f36c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/graphs.py\n@@ -0,0 +1,382 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"\n+Collection of graph utility functions.\n+\n+These are largely copies/re-implementations of graspologic methods to avoid dependency issues.\n+\"\"\"\n+\n+import logging\n+import math\n+from collections import defaultdict\n+from typing import Any, cast\n+\n+import graspologic_native as gn\n+import networkx as nx\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.config.enums import ModularityMetric\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def largest_connected_component(graph: nx.Graph) -> nx.Graph:\n+    \"\"\"Return the largest connected component of the graph.\"\"\"\n+    graph = graph.copy()\n+    lcc_nodes = max(nx.connected_components(graph), key=len)\n+    lcc = graph.subgraph(lcc_nodes).copy()\n+    lcc.remove_nodes_from([n for n in lcc if n not in lcc_nodes])\n+    return cast(\"nx.Graph\", lcc)\n+\n+\n+def _nx_to_edge_list(\n+    graph: nx.Graph,\n+    weight_attribute: str = \"weight\",\n+    weight_default: float = 1.0,\n+) -> list[tuple[str, str, float]]:\n+    \"\"\"\n+    Convert an undirected, non-multigraph networkx graph to a list of edges.\n+\n+    Each edge is represented as a tuple of (source_str, target_str, weight).\n+    \"\"\"\n+    edge_list: list[tuple[str, str, float]] = []\n+\n+    # Decide how to retrieve the weight data\n+    edge_iter = graph.edges(data=weight_attribute, default=weight_default)  # type: ignore\n+\n+    for source, target, weight in edge_iter:\n+        source_str = str(source)\n+        target_str = str(target)\n+        edge_list.append((source_str, target_str, float(weight)))\n+\n+    return edge_list\n+\n+\n+def hierarchical_leiden(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int | None = 0xDEADBEEF,\n+) -> list[gn.HierarchicalCluster]:\n+    \"\"\"Run hierarchical leiden on the graph.\"\"\"\n+    return gn.hierarchical_leiden(\n+        edges=_nx_to_edge_list(graph),\n+        max_cluster_size=max_cluster_size,\n+        seed=random_seed,\n+        starting_communities=None,\n+        resolution=1.0,\n+        randomness=0.001,\n+        use_modularity=True,\n+        iterations=1,\n+    )\n+\n+\n+def modularity(\n+    graph: nx.Graph,\n+    partitions: dict[Any, int],\n+    weight_attribute: str = \"weight\",\n+    resolution: float = 1.0,\n+) -> float:\n+    \"\"\"Given an undirected graph and a dictionary of vertices to community ids, calculate the modularity.\"\"\"\n+    components = _modularity_components(graph, partitions, weight_attribute, resolution)\n+    return sum(components.values())\n+\n+\n+def _modularity_component(\n+    intra_community_degree: float,\n+    total_community_degree: float,\n+    network_degree_sum: float,\n+    resolution: float,\n+) -> float:\n+    community_degree_ratio = math.pow(total_community_degree, 2.0) / (\n+        2.0 * network_degree_sum\n+    )\n+    return (intra_community_degree - resolution * community_degree_ratio) / (\n+        2.0 * network_degree_sum\n+    )\n+\n+\n+def _modularity_components(\n+    graph: nx.Graph,\n+    partitions: dict[Any, int],\n+    weight_attribute: str = \"weight\",\n+    resolution: float = 1.0,\n+) -> dict[int, float]:\n+    total_edge_weight = 0.0\n+    communities = set(partitions.values())\n+\n+    degree_sums_within_community: dict[int, float] = defaultdict(lambda: 0.0)\n+    degree_sums_for_community: dict[int, float] = defaultdict(lambda: 0.0)\n+    for vertex, neighbor_vertex, weight in graph.edges(data=weight_attribute):\n+        vertex_community = partitions[vertex]\n+        neighbor_community = partitions[neighbor_vertex]\n+        if vertex_community == neighbor_community:\n+            if vertex == neighbor_vertex:\n+                degree_sums_within_community[vertex_community] += weight\n+            else:\n+                degree_sums_within_community[vertex_community] += weight * 2.0\n+        degree_sums_for_community[vertex_community] += weight\n+        degree_sums_for_community[neighbor_community] += weight\n+        total_edge_weight += weight\n+\n+    return {\n+        comm: _modularity_component(\n+            degree_sums_within_community[comm],\n+            degree_sums_for_community[comm],\n+            total_edge_weight,\n+            resolution,\n+        )\n+        for comm in communities\n+    }\n+\n+\n+def calculate_root_modularity(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int = 0xDEADBEEF,\n+) -> float:\n+    \"\"\"Calculate distance between the modularity of the graph's root clusters and the target modularity.\"\"\"\n+    hcs = hierarchical_leiden(\n+        graph, max_cluster_size=max_cluster_size, random_seed=random_seed\n+    )\n+    root_clusters = first_level_hierarchical_clustering(hcs)\n+    return modularity(graph, root_clusters)\n+\n+\n+def calculate_leaf_modularity(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int = 0xDEADBEEF,\n+) -> float:\n+    \"\"\"Calculate distance between the modularity of the graph's leaf clusters and the target modularity.\"\"\"\n+    hcs = hierarchical_leiden(\n+        graph, max_cluster_size=max_cluster_size, random_seed=random_seed\n+    )\n+    leaf_clusters = final_level_hierarchical_clustering(hcs)\n+    return modularity(graph, leaf_clusters)\n+\n+\n+def calculate_graph_modularity(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int = 0xDEADBEEF,\n+    use_root_modularity: bool = True,\n+) -> float:\n+    \"\"\"Calculate modularity of the whole graph.\"\"\"\n+    if use_root_modularity:\n+        return calculate_root_modularity(\n+            graph, max_cluster_size=max_cluster_size, random_seed=random_seed\n+        )\n+    return calculate_leaf_modularity(\n+        graph, max_cluster_size=max_cluster_size, random_seed=random_seed\n+    )\n+\n+\n+def calculate_lcc_modularity(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int = 0xDEADBEEF,\n+    use_root_modularity: bool = True,\n+) -> float:\n+    \"\"\"Calculate modularity of the largest connected component of the graph.\"\"\"\n+    lcc = cast(\"nx.Graph\", largest_connected_component(graph))\n+    if use_root_modularity:\n+        return calculate_root_modularity(\n+            lcc, max_cluster_size=max_cluster_size, random_seed=random_seed\n+        )\n+    return calculate_leaf_modularity(\n+        lcc, max_cluster_size=max_cluster_size, random_seed=random_seed\n+    )\n+\n+\n+def calculate_weighted_modularity(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int = 0xDEADBEEF,\n+    min_connected_component_size: int = 10,\n+    use_root_modularity: bool = True,\n+) -> float:\n+    \"\"\"\n+    Calculate weighted modularity of all connected components with size greater than min_connected_component_size.\n+\n+    Modularity = sum(component_modularity * component_size) / total_nodes.\n+    \"\"\"\n+    connected_components: list[set] = list(nx.connected_components(graph))\n+    filtered_components = [\n+        component\n+        for component in connected_components\n+        if len(component) > min_connected_component_size\n+    ]\n+    if len(filtered_components) == 0:\n+        filtered_components = [graph]\n+\n+    total_nodes = sum(len(component) for component in filtered_components)\n+    total_modularity = 0\n+    for component in filtered_components:\n+        if len(component) > min_connected_component_size:\n+            subgraph = graph.subgraph(component)\n+            if use_root_modularity:\n+                modularity = calculate_root_modularity(\n+                    subgraph, max_cluster_size=max_cluster_size, random_seed=random_seed\n+                )\n+            else:\n+                modularity = calculate_leaf_modularity(\n+                    subgraph, max_cluster_size=max_cluster_size, random_seed=random_seed\n+                )\n+            total_modularity += modularity * len(component) / total_nodes\n+    return total_modularity\n+\n+\n+def calculate_modularity(\n+    graph: nx.Graph,\n+    max_cluster_size: int = 10,\n+    random_seed: int = 0xDEADBEEF,\n+    use_root_modularity: bool = True,\n+    modularity_metric: ModularityMetric = ModularityMetric.WeightedComponents,\n+) -> float:\n+    \"\"\"Calculate modularity of the graph based on the modularity metric type.\"\"\"\n+    match modularity_metric:\n+        case ModularityMetric.Graph:\n+            logger.info(\"Calculating graph modularity\")\n+            return calculate_graph_modularity(\n+                graph,\n+                max_cluster_size=max_cluster_size,\n+                random_seed=random_seed,\n+                use_root_modularity=use_root_modularity,\n+            )\n+        case ModularityMetric.LCC:\n+            logger.info(\"Calculating LCC modularity\")\n+            return calculate_lcc_modularity(\n+                graph,\n+                max_cluster_size=max_cluster_size,\n+                random_seed=random_seed,\n+                use_root_modularity=use_root_modularity,\n+            )\n+        case ModularityMetric.WeightedComponents:\n+            logger.info(\"Calculating weighted-components modularity\")\n+            return calculate_weighted_modularity(\n+                graph,\n+                max_cluster_size=max_cluster_size,\n+                random_seed=random_seed,\n+                use_root_modularity=use_root_modularity,\n+            )\n+\n+\n+def calculate_pmi_edge_weights(\n+    nodes_df: pd.DataFrame,\n+    edges_df: pd.DataFrame,\n+    node_name_col: str = \"title\",\n+    node_freq_col: str = \"frequency\",\n+    edge_weight_col: str = \"weight\",\n+    edge_source_col: str = \"source\",\n+    edge_target_col: str = \"target\",\n+) -> pd.DataFrame:\n+    \"\"\"\n+    Calculate pointwise mutual information (PMI) edge weights.\n+\n+    Uses a variant of PMI that accounts for bias towards low-frequency events.\n+    pmi(x,y) = p(x,y) * log2(p(x,y)/ (p(x)*p(y))\n+    p(x,y) = edge_weight(x,y) / total_edge_weights\n+    p(x) = freq_occurrence(x) / total_freq_occurrences.\n+\n+    \"\"\"\n+    copied_nodes_df = nodes_df[[node_name_col, node_freq_col]]\n+\n+    total_edge_weights = edges_df[edge_weight_col].sum()\n+    total_freq_occurrences = nodes_df[node_freq_col].sum()\n+    copied_nodes_df[\"prop_occurrence\"] = (\n+        copied_nodes_df[node_freq_col] / total_freq_occurrences\n+    )\n+    copied_nodes_df = copied_nodes_df.loc[:, [node_name_col, \"prop_occurrence\"]]\n+\n+    edges_df[\"prop_weight\"] = edges_df[edge_weight_col] / total_edge_weights\n+    edges_df = (\n+        edges_df.merge(\n+            copied_nodes_df, left_on=edge_source_col, right_on=node_name_col, how=\"left\"\n+        )\n+        .drop(columns=[node_name_col])\n+        .rename(columns={\"prop_occurrence\": \"source_prop\"})\n+    )\n+    edges_df = (\n+        edges_df.merge(\n+            copied_nodes_df, left_on=edge_target_col, right_on=node_name_col, how=\"left\"\n+        )\n+        .drop(columns=[node_name_col])\n+        .rename(columns={\"prop_occurrence\": \"target_prop\"})\n+    )\n+    edges_df[edge_weight_col] = edges_df[\"prop_weight\"] * np.log2(\n+        edges_df[\"prop_weight\"] / (edges_df[\"source_prop\"] * edges_df[\"target_prop\"])\n+    )\n+\n+    return edges_df.drop(columns=[\"prop_weight\", \"source_prop\", \"target_prop\"])\n+\n+\n+def calculate_rrf_edge_weights(\n+    nodes_df: pd.DataFrame,\n+    edges_df: pd.DataFrame,\n+    node_name_col=\"title\",\n+    node_freq_col=\"freq\",\n+    edge_weight_col=\"weight\",\n+    edge_source_col=\"source\",\n+    edge_target_col=\"target\",\n+    rrf_smoothing_factor: int = 60,\n+) -> pd.DataFrame:\n+    \"\"\"Calculate reciprocal rank fusion (RRF) edge weights as a combination of PMI weight and combined freq of source and target.\"\"\"\n+    edges_df = calculate_pmi_edge_weights(\n+        nodes_df,\n+        edges_df,\n+        node_name_col,\n+        node_freq_col,\n+        edge_weight_col,\n+        edge_source_col,\n+        edge_target_col,\n+    )\n+\n+    edges_df[\"pmi_rank\"] = edges_df[edge_weight_col].rank(method=\"min\", ascending=False)\n+    edges_df[\"raw_weight_rank\"] = edges_df[edge_weight_col].rank(\n+        method=\"min\", ascending=False\n+    )\n+    edges_df[edge_weight_col] = edges_df.apply(\n+        lambda x: (1 / (rrf_smoothing_factor + x[\"pmi_rank\"]))\n+        + (1 / (rrf_smoothing_factor + x[\"raw_weight_rank\"])),\n+        axis=1,\n+    )\n+\n+    return edges_df.drop(columns=[\"pmi_rank\", \"raw_weight_rank\"])\n+\n+\n+def get_upper_threshold_by_std(data: list[float] | list[int], std_trim: float) -> float:\n+    \"\"\"Get upper threshold by standard deviation.\"\"\"\n+    mean = np.mean(data)\n+    std = np.std(data)\n+    return cast(\"float\", mean + std_trim * std)\n+\n+\n+def first_level_hierarchical_clustering(\n+    hcs: list[gn.HierarchicalCluster],\n+) -> dict[Any, int]:\n+    \"\"\"first_level_hierarchical_clustering.\n+\n+    Returns\n+    -------\n+    dict[Any, int]\n+        The initial leiden algorithm clustering results as a dictionary\n+        of node id to community id.\n+    \"\"\"\n+    return {entry.node: entry.cluster for entry in hcs if entry.level == 0}\n+\n+\n+def final_level_hierarchical_clustering(\n+    hcs: list[gn.HierarchicalCluster],\n+) -> dict[Any, int]:\n+    \"\"\"\n+    final_level_hierarchical_clustering.\n+\n+    Returns\n+    -------\n+    dict[Any, int]\n+        The last leiden algorithm clustering results as a dictionary\n+        of node id to community id.\n+    \"\"\"\n+    return {entry.node: entry.cluster for entry in hcs if entry.is_final_cluster}\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/hashing.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/hashing.py b/packages/graphrag/graphrag/index/utils/hashing.py\nnew file mode 100644\nindex 0000000..4ee6a98\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/hashing.py\n@@ -0,0 +1,14 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Hashing utilities.\"\"\"\n+\n+from collections.abc import Iterable\n+from hashlib import sha512\n+from typing import Any\n+\n+\n+def gen_sha512_hash(item: dict[str, Any], hashcode: Iterable[str]):\n+    \"\"\"Generate a SHA512 hash.\"\"\"\n+    hashed = \"\".join([str(item[column]) for column in hashcode])\n+    return f\"{sha512(hashed.encode('utf-8'), usedforsecurity=False).hexdigest()}\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/is_null.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/is_null.py b/packages/graphrag/graphrag/index/utils/is_null.py\nnew file mode 100644\nindex 0000000..f5df195\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/is_null.py\n@@ -0,0 +1,19 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Defines the is_null utility.\"\"\"\n+\n+import math\n+from typing import Any\n+\n+\n+def is_null(value: Any) -> bool:\n+    \"\"\"Check if value is null or is nan.\"\"\"\n+\n+    def is_none() -> bool:\n+        return value is None\n+\n+    def is_nan() -> bool:\n+        return isinstance(value, float) and math.isnan(value)\n+\n+    return is_none() or is_nan()\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/stable_lcc.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/stable_lcc.py b/packages/graphrag/graphrag/index/utils/stable_lcc.py\nnew file mode 100644\nindex 0000000..806c0fd\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/stable_lcc.py\n@@ -0,0 +1,66 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module for producing a stable largest connected component, i.e. same input graph == same output lcc.\"\"\"\n+\n+import html\n+from typing import Any, cast\n+\n+import networkx as nx\n+\n+from graphrag.index.utils.graphs import largest_connected_component\n+\n+\n+def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph:\n+    \"\"\"Return the largest connected component of the graph, with nodes and edges sorted in a stable way.\"\"\"\n+    graph = graph.copy()\n+    graph = cast(\"nx.Graph\", largest_connected_component(graph))\n+    graph = normalize_node_names(graph)\n+    return _stabilize_graph(graph)\n+\n+\n+def _stabilize_graph(graph: nx.Graph) -> nx.Graph:\n+    \"\"\"Ensure an undirected graph with the same relationships will always be read the same way.\"\"\"\n+    fixed_graph = nx.DiGraph() if graph.is_directed() else nx.Graph()\n+\n+    sorted_nodes = graph.nodes(data=True)\n+    sorted_nodes = sorted(sorted_nodes, key=lambda x: x[0])\n+\n+    fixed_graph.add_nodes_from(sorted_nodes)\n+    edges = list(graph.edges(data=True))\n+\n+    # If the graph is undirected, we create the edges in a stable way, so we get the same results\n+    # for example:\n+    # A -> B\n+    # in graph theory is the same as\n+    # B -> A\n+    # in an undirected graph\n+    # however, this can lead to downstream issues because sometimes\n+    # consumers read graph.nodes() which ends up being [A, B] and sometimes it's [B, A]\n+    # but they base some of their logic on the order of the nodes, so the order ends up being important\n+    # so we sort the nodes in the edge in a stable way, so that we always get the same order\n+    if not graph.is_directed():\n+\n+        def _sort_source_target(edge):\n+            source, target, edge_data = edge\n+            if source > target:\n+                temp = source\n+                source = target\n+                target = temp\n+            return source, target, edge_data\n+\n+        edges = [_sort_source_target(edge) for edge in edges]\n+\n+    def _get_edge_key(source: Any, target: Any) -> str:\n+        return f\"{source} -> {target}\"\n+\n+    edges = sorted(edges, key=lambda x: _get_edge_key(x[0], x[1]))\n+\n+    fixed_graph.add_edges_from(edges)\n+    return fixed_graph\n+\n+\n+def normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph | nx.DiGraph:\n+    \"\"\"Normalize node names.\"\"\"\n+    node_mapping = {node: html.unescape(node.upper().strip()) for node in graph.nodes()}  # type: ignore\n+    return nx.relabel_nodes(graph, node_mapping)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/string.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/string.py b/packages/graphrag/graphrag/index/utils/string.py\nnew file mode 100644\nindex 0000000..7e1654b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/string.py\n@@ -0,0 +1,19 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"String utilities.\"\"\"\n+\n+import html\n+import re\n+from typing import Any\n+\n+\n+def clean_str(input: Any) -> str:\n+    \"\"\"Clean an input string by removing HTML escapes, control characters, and other unwanted characters.\"\"\"\n+    # If we get non-string input, just give it back\n+    if not isinstance(input, str):\n+        return input\n+\n+    result = html.unescape(input.strip())\n+    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python\n+    return re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", result)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/utils/uuid.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/utils/uuid.py b/packages/graphrag/graphrag/index/utils/uuid.py\nnew file mode 100644\nindex 0000000..0671fb0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/utils/uuid.py\n@@ -0,0 +1,14 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"UUID utilities.\"\"\"\n+\n+import uuid\n+from random import Random, getrandbits\n+\n+\n+def gen_uuid(rd: Random | None = None):\n+    \"\"\"Generate a random UUID v4.\"\"\"\n+    return uuid.UUID(\n+        int=rd.getrandbits(128) if rd is not None else getrandbits(128), version=4\n+    ).hex\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/validate_config.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/validate_config.py b/packages/graphrag/graphrag/index/validate_config.py\nnew file mode 100644\nindex 0000000..014592d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/validate_config.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing validate_config_names definition.\"\"\"\n+\n+import asyncio\n+import logging\n+import sys\n+\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.language_model.manager import ModelManager\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def validate_config_names(parameters: GraphRagConfig) -> None:\n+    \"\"\"Validate config file for model deployment name typos, by running a quick test message for each.\"\"\"\n+    for id, config in parameters.models.items():\n+        if config.type == \"chat\":\n+            llm = ModelManager().register_chat(\n+                name=\"test-llm\",\n+                model_type=config.type,\n+                config=config,\n+                callbacks=NoopWorkflowCallbacks(),\n+                cache=None,\n+            )\n+            try:\n+                asyncio.run(\n+                    llm.achat(\"This is an LLM connectivity test. Say Hello World\")\n+                )\n+                logger.info(\"LLM Config Params Validated\")\n+            except Exception as e:  # noqa: BLE001\n+                logger.error(f\"LLM configuration error detected.\\n{e}\")  # noqa\n+                print(f\"Failed to validate language model ({id}) params\", e)  # noqa: T201\n+                sys.exit(1)\n+        elif config.type == \"embedding\":\n+            embed_llm = ModelManager().register_embedding(\n+                name=\"test-embed-llm\",\n+                model_type=config.type,\n+                config=config,\n+                callbacks=NoopWorkflowCallbacks(),\n+                cache=None,\n+            )\n+            try:\n+                asyncio.run(\n+                    embed_llm.aembed_batch([\"This is an LLM Embedding Test String\"])\n+                )\n+                logger.info(\"Embedding LLM Config Params Validated\")\n+            except Exception as e:  # noqa: BLE001\n+                logger.error(f\"Embedding configuration error detected.\\n{e}\")  # noqa\n+                print(f\"Failed to validate embedding model ({id}) params\", e)  # noqa: T201\n+                sys.exit(1)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/__init__.py b/packages/graphrag/graphrag/index/workflows/__init__.py\nnew file mode 100644\nindex 0000000..5567bac\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/__init__.py\n@@ -0,0 +1,100 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\n+\"\"\"A package containing all built-in workflow definitions.\"\"\"\n+\n+from graphrag.index.workflows.factory import PipelineFactory\n+\n+from .create_base_text_units import (\n+    run_workflow as run_create_base_text_units,\n+)\n+from .create_communities import (\n+    run_workflow as run_create_communities,\n+)\n+from .create_community_reports import (\n+    run_workflow as run_create_community_reports,\n+)\n+from .create_community_reports_text import (\n+    run_workflow as run_create_community_reports_text,\n+)\n+from .create_final_documents import (\n+    run_workflow as run_create_final_documents,\n+)\n+from .create_final_text_units import (\n+    run_workflow as run_create_final_text_units,\n+)\n+from .extract_covariates import (\n+    run_workflow as run_extract_covariates,\n+)\n+from .extract_graph import (\n+    run_workflow as run_extract_graph,\n+)\n+from .extract_graph_nlp import (\n+    run_workflow as run_extract_graph_nlp,\n+)\n+from .finalize_graph import (\n+    run_workflow as run_finalize_graph,\n+)\n+from .generate_text_embeddings import (\n+    run_workflow as run_generate_text_embeddings,\n+)\n+from .load_input_documents import (\n+    run_workflow as run_load_input_documents,\n+)\n+from .load_update_documents import (\n+    run_workflow as run_load_update_documents,\n+)\n+from .prune_graph import (\n+    run_workflow as run_prune_graph,\n+)\n+from .update_clean_state import (\n+    run_workflow as run_update_clean_state,\n+)\n+from .update_communities import (\n+    run_workflow as run_update_communities,\n+)\n+from .update_community_reports import (\n+    run_workflow as run_update_community_reports,\n+)\n+from .update_covariates import (\n+    run_workflow as run_update_covariates,\n+)\n+from .update_entities_relationships import (\n+    run_workflow as run_update_entities_relationships,\n+)\n+from .update_final_documents import (\n+    run_workflow as run_update_final_documents,\n+)\n+from .update_text_embeddings import (\n+    run_workflow as run_update_text_embeddings,\n+)\n+from .update_text_units import (\n+    run_workflow as run_update_text_units,\n+)\n+\n+# register all of our built-in workflows at once\n+PipelineFactory.register_all({\n+    \"load_input_documents\": run_load_input_documents,\n+    \"load_update_documents\": run_load_update_documents,\n+    \"create_base_text_units\": run_create_base_text_units,\n+    \"create_communities\": run_create_communities,\n+    \"create_community_reports_text\": run_create_community_reports_text,\n+    \"create_community_reports\": run_create_community_reports,\n+    \"extract_covariates\": run_extract_covariates,\n+    \"create_final_documents\": run_create_final_documents,\n+    \"create_final_text_units\": run_create_final_text_units,\n+    \"extract_graph_nlp\": run_extract_graph_nlp,\n+    \"extract_graph\": run_extract_graph,\n+    \"finalize_graph\": run_finalize_graph,\n+    \"generate_text_embeddings\": run_generate_text_embeddings,\n+    \"prune_graph\": run_prune_graph,\n+    \"update_final_documents\": run_update_final_documents,\n+    \"update_text_embeddings\": run_update_text_embeddings,\n+    \"update_community_reports\": run_update_community_reports,\n+    \"update_entities_relationships\": run_update_entities_relationships,\n+    \"update_communities\": run_update_communities,\n+    \"update_covariates\": run_update_covariates,\n+    \"update_text_units\": run_update_text_units,\n+    \"update_clean_state\": run_update_clean_state,\n+})\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/create_base_text_units.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/create_base_text_units.py b/packages/graphrag/graphrag/index/workflows/create_base_text_units.py\nnew file mode 100644\nindex 0000000..b80cf6d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/create_base_text_units.py\n@@ -0,0 +1,144 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import json\n+import logging\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.models.chunking_config import ChunkStrategyType\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.operations.chunk_text.chunk_text import chunk_text\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.utils.hashing import gen_sha512_hash\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform base text_units.\"\"\"\n+    logger.info(\"Workflow started: create_base_text_units\")\n+    documents = await load_table_from_storage(\"documents\", context.output_storage)\n+\n+    chunks = config.chunks\n+\n+    output = create_base_text_units(\n+        documents,\n+        context.callbacks,\n+        chunks.size,\n+        chunks.overlap,\n+        chunks.encoding_model,\n+        strategy=chunks.strategy,\n+        prepend_metadata=chunks.prepend_metadata,\n+        chunk_size_includes_metadata=chunks.chunk_size_includes_metadata,\n+    )\n+\n+    await write_table_to_storage(output, \"text_units\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: create_base_text_units\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+def create_base_text_units(\n+    documents: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    size: int,\n+    overlap: int,\n+    encoding_model: str,\n+    strategy: ChunkStrategyType,\n+    prepend_metadata: bool,\n+    chunk_size_includes_metadata: bool,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform base text_units.\"\"\"\n+    documents.sort_values(by=[\"id\"], ascending=[True], inplace=True)\n+\n+    tokenizer = get_tokenizer(encoding_model=encoding_model)\n+\n+    def chunker(row: pd.Series) -> Any:\n+        line_delimiter = \".\\n\"\n+        metadata_str = \"\"\n+        metadata_tokens = 0\n+\n+        if prepend_metadata and \"metadata\" in row:\n+            metadata = row[\"metadata\"]\n+            if isinstance(metadata, str):\n+                metadata = json.loads(metadata)\n+            if isinstance(metadata, dict):\n+                metadata_str = (\n+                    line_delimiter.join(f\"{k}: {v}\" for k, v in metadata.items())\n+                    + line_delimiter\n+                )\n+\n+            if chunk_size_includes_metadata:\n+                metadata_tokens = len(tokenizer.encode(metadata_str))\n+                if metadata_tokens >= size:\n+                    message = \"Metadata tokens exceeds the maximum tokens per chunk. Please increase the tokens per chunk.\"\n+                    raise ValueError(message)\n+\n+        chunked = chunk_text(\n+            pd.DataFrame([row]).reset_index(drop=True),\n+            column=\"text\",\n+            size=size - metadata_tokens,\n+            overlap=overlap,\n+            encoding_model=encoding_model,\n+            strategy=strategy,\n+            callbacks=callbacks,\n+        )[0]\n+\n+        if prepend_metadata:\n+            for index, chunk in enumerate(chunked):\n+                if isinstance(chunk, str):\n+                    chunked[index] = metadata_str + chunk\n+                else:\n+                    chunked[index] = (\n+                        (chunk[0], metadata_str + chunk[1], chunk[2]) if chunk else None\n+                    )\n+\n+        row[\"chunks\"] = chunked\n+        return row\n+\n+    # Track progress of row-wise apply operation\n+    total_rows = len(documents)\n+    logger.info(\"Starting chunking process for %d documents\", total_rows)\n+\n+    def chunker_with_logging(row: pd.Series, row_index: int) -> Any:\n+        \"\"\"Add logging to chunker execution.\"\"\"\n+        result = chunker(row)\n+        logger.info(\"chunker progress:  %d/%d\", row_index + 1, total_rows)\n+        return result\n+\n+    text_units = documents.apply(\n+        lambda row: chunker_with_logging(row, row.name), axis=1\n+    )\n+\n+    text_units = cast(\"pd.DataFrame\", text_units[[\"id\", \"chunks\"]])\n+    text_units = text_units.explode(\"chunks\")\n+    text_units.rename(\n+        columns={\n+            \"id\": \"document_id\",\n+            \"chunks\": \"text\",\n+        },\n+        inplace=True,\n+    )\n+\n+    text_units[\"id\"] = text_units.apply(\n+        lambda row: gen_sha512_hash(row, [\"text\"]), axis=1\n+    )\n+    # get a final token measurement\n+    text_units[\"n_tokens\"] = text_units[\"text\"].apply(\n+        lambda x: len(tokenizer.encode(x))\n+    )\n+\n+    return cast(\n+        \"pd.DataFrame\", text_units[text_units[\"text\"].notna()].reset_index(drop=True)\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/create_communities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/create_communities.py b/packages/graphrag/graphrag/index/workflows/create_communities.py\nnew file mode 100644\nindex 0000000..c06d5f4\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/create_communities.py\n@@ -0,0 +1,156 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+from datetime import datetime, timezone\n+from typing import cast\n+from uuid import uuid4\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.data_model.schemas import COMMUNITIES_FINAL_COLUMNS\n+from graphrag.index.operations.cluster_graph import cluster_graph\n+from graphrag.index.operations.create_graph import create_graph\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform final communities.\"\"\"\n+    logger.info(\"Workflow started: create_communities\")\n+    entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    relationships = await load_table_from_storage(\n+        \"relationships\", context.output_storage\n+    )\n+\n+    max_cluster_size = config.cluster_graph.max_cluster_size\n+    use_lcc = config.cluster_graph.use_lcc\n+    seed = config.cluster_graph.seed\n+\n+    output = create_communities(\n+        entities,\n+        relationships,\n+        max_cluster_size=max_cluster_size,\n+        use_lcc=use_lcc,\n+        seed=seed,\n+    )\n+\n+    await write_table_to_storage(output, \"communities\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: create_communities\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+def create_communities(\n+    entities: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+    max_cluster_size: int,\n+    use_lcc: bool,\n+    seed: int | None = None,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform final communities.\"\"\"\n+    graph = create_graph(relationships, edge_attr=[\"weight\"])\n+\n+    clusters = cluster_graph(\n+        graph,\n+        max_cluster_size,\n+        use_lcc,\n+        seed=seed,\n+    )\n+\n+    communities = pd.DataFrame(\n+        clusters, columns=pd.Index([\"level\", \"community\", \"parent\", \"title\"])\n+    ).explode(\"title\")\n+    communities[\"community\"] = communities[\"community\"].astype(int)\n+\n+    # aggregate entity ids for each community\n+    entity_ids = communities.merge(entities, on=\"title\", how=\"inner\")\n+    entity_ids = (\n+        entity_ids.groupby(\"community\").agg(entity_ids=(\"id\", list)).reset_index()\n+    )\n+\n+    # aggregate relationships ids for each community\n+    # these are limited to only those where the source and target are in the same community\n+    max_level = communities[\"level\"].max()\n+    all_grouped = pd.DataFrame(\n+        columns=[\"community\", \"level\", \"relationship_ids\", \"text_unit_ids\"]  # type: ignore\n+    )\n+    for level in range(max_level + 1):\n+        communities_at_level = communities.loc[communities[\"level\"] == level]\n+        sources = relationships.merge(\n+            communities_at_level, left_on=\"source\", right_on=\"title\", how=\"inner\"\n+        )\n+        targets = sources.merge(\n+            communities_at_level, left_on=\"target\", right_on=\"title\", how=\"inner\"\n+        )\n+        matched = targets.loc[targets[\"community_x\"] == targets[\"community_y\"]]\n+        text_units = matched.explode(\"text_unit_ids\")\n+        grouped = (\n+            text_units.groupby([\"community_x\", \"level_x\", \"parent_x\"])\n+            .agg(relationship_ids=(\"id\", list), text_unit_ids=(\"text_unit_ids\", list))\n+            .reset_index()\n+        )\n+        grouped.rename(\n+            columns={\n+                \"community_x\": \"community\",\n+                \"level_x\": \"level\",\n+                \"parent_x\": \"parent\",\n+            },\n+            inplace=True,\n+        )\n+        all_grouped = pd.concat([\n+            all_grouped,\n+            grouped.loc[\n+                :, [\"community\", \"level\", \"parent\", \"relationship_ids\", \"text_unit_ids\"]\n+            ],\n+        ])\n+\n+    # deduplicate the lists\n+    all_grouped[\"relationship_ids\"] = all_grouped[\"relationship_ids\"].apply(\n+        lambda x: sorted(set(x))\n+    )\n+    all_grouped[\"text_unit_ids\"] = all_grouped[\"text_unit_ids\"].apply(\n+        lambda x: sorted(set(x))\n+    )\n+\n+    # join it all up and add some new fields\n+    final_communities = all_grouped.merge(entity_ids, on=\"community\", how=\"inner\")\n+    final_communities[\"id\"] = [str(uuid4()) for _ in range(len(final_communities))]\n+    final_communities[\"human_readable_id\"] = final_communities[\"community\"]\n+    final_communities[\"title\"] = \"Community \" + final_communities[\"community\"].astype(\n+        str\n+    )\n+    final_communities[\"parent\"] = final_communities[\"parent\"].astype(int)\n+    # collect the children so we have a tree going both ways\n+    parent_grouped = cast(\n+        \"pd.DataFrame\",\n+        final_communities.groupby(\"parent\").agg(children=(\"community\", \"unique\")),\n+    )\n+    final_communities = final_communities.merge(\n+        parent_grouped,\n+        left_on=\"community\",\n+        right_on=\"parent\",\n+        how=\"left\",\n+    )\n+    # replace NaN children with empty list\n+    final_communities[\"children\"] = final_communities[\"children\"].apply(\n+        lambda x: x if isinstance(x, np.ndarray) else []  # type: ignore\n+    )\n+    # add fields for incremental update tracking\n+    final_communities[\"period\"] = datetime.now(timezone.utc).date().isoformat()\n+    final_communities[\"size\"] = final_communities.loc[:, \"entity_ids\"].apply(len)\n+\n+    return final_communities.loc[\n+        :,\n+        COMMUNITIES_FINAL_COLUMNS,\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/create_community_reports.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/create_community_reports.py b/packages/graphrag/graphrag/index/workflows/create_community_reports.py\nnew file mode 100644\nindex 0000000..0415cb3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/create_community_reports.py\n@@ -0,0 +1,198 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+import graphrag.data_model.schemas as schemas\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.operations.finalize_community_reports import (\n+    finalize_community_reports,\n+)\n+from graphrag.index.operations.summarize_communities.explode_communities import (\n+    explode_communities,\n+)\n+from graphrag.index.operations.summarize_communities.graph_context.context_builder import (\n+    build_level_context,\n+    build_local_context,\n+)\n+from graphrag.index.operations.summarize_communities.summarize_communities import (\n+    summarize_communities,\n+)\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.utils.storage import (\n+    load_table_from_storage,\n+    storage_has_table,\n+    write_table_to_storage,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform community reports.\"\"\"\n+    logger.info(\"Workflow started: create_community_reports\")\n+    edges = await load_table_from_storage(\"relationships\", context.output_storage)\n+    entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    communities = await load_table_from_storage(\"communities\", context.output_storage)\n+    claims = None\n+    if config.extract_claims.enabled and await storage_has_table(\n+        \"covariates\", context.output_storage\n+    ):\n+        claims = await load_table_from_storage(\"covariates\", context.output_storage)\n+\n+    model_config = config.get_language_model_config(config.community_reports.model_id)\n+    prompts = config.community_reports.resolved_prompts(config.root_dir)\n+\n+    model = ModelManager().get_or_create_chat_model(\n+        name=config.community_reports.model_instance_name,\n+        model_type=model_config.type,\n+        config=model_config,\n+        callbacks=context.callbacks,\n+        cache=context.cache,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config)\n+\n+    output = await create_community_reports(\n+        edges_input=edges,\n+        entities=entities,\n+        communities=communities,\n+        claims_input=claims,\n+        callbacks=context.callbacks,\n+        model=model,\n+        tokenizer=tokenizer,\n+        prompt=prompts.graph_prompt,\n+        max_input_length=config.community_reports.max_input_length,\n+        max_report_length=config.community_reports.max_length,\n+        num_threads=model_config.concurrent_requests,\n+        async_type=model_config.async_mode,\n+    )\n+\n+    await write_table_to_storage(output, \"community_reports\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: create_community_reports\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+async def create_community_reports(\n+    edges_input: pd.DataFrame,\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    claims_input: pd.DataFrame | None,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    tokenizer: Tokenizer,\n+    prompt: str,\n+    max_input_length: int,\n+    max_report_length: int,\n+    num_threads: int,\n+    async_type: AsyncType,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform community reports.\"\"\"\n+    nodes = explode_communities(communities, entities)\n+\n+    nodes = _prep_nodes(nodes)\n+    edges = _prep_edges(edges_input)\n+\n+    claims = None\n+    if claims_input is not None:\n+        claims = _prep_claims(claims_input)\n+\n+    local_contexts = build_local_context(\n+        nodes,\n+        edges,\n+        claims,\n+        tokenizer,\n+        callbacks,\n+        max_input_length,\n+    )\n+\n+    community_reports = await summarize_communities(\n+        nodes,\n+        communities,\n+        local_contexts,\n+        build_level_context,\n+        callbacks,\n+        model=model,\n+        prompt=prompt,\n+        tokenizer=tokenizer,\n+        max_input_length=max_input_length,\n+        max_report_length=max_report_length,\n+        num_threads=num_threads,\n+        async_type=async_type,\n+    )\n+\n+    return finalize_community_reports(community_reports, communities)\n+\n+\n+def _prep_nodes(input: pd.DataFrame) -> pd.DataFrame:\n+    \"\"\"Prepare nodes by filtering, filling missing descriptions, and creating NODE_DETAILS.\"\"\"\n+    # Fill missing values in DESCRIPTION\n+    input.loc[:, schemas.DESCRIPTION] = input.loc[:, schemas.DESCRIPTION].fillna(\n+        \"No Description\"\n+    )\n+\n+    # Create NODE_DETAILS column\n+    input.loc[:, schemas.NODE_DETAILS] = input.loc[\n+        :,\n+        [\n+            schemas.SHORT_ID,\n+            schemas.TITLE,\n+            schemas.DESCRIPTION,\n+            schemas.NODE_DEGREE,\n+        ],\n+    ].to_dict(orient=\"records\")\n+\n+    return input\n+\n+\n+def _prep_edges(input: pd.DataFrame) -> pd.DataFrame:\n+    # Fill missing DESCRIPTION\n+    input.fillna(value={schemas.DESCRIPTION: \"No Description\"}, inplace=True)\n+\n+    # Create EDGE_DETAILS column\n+    input.loc[:, schemas.EDGE_DETAILS] = input.loc[\n+        :,\n+        [\n+            schemas.SHORT_ID,\n+            schemas.EDGE_SOURCE,\n+            schemas.EDGE_TARGET,\n+            schemas.DESCRIPTION,\n+            schemas.EDGE_DEGREE,\n+        ],\n+    ].to_dict(orient=\"records\")\n+\n+    return input\n+\n+\n+def _prep_claims(input: pd.DataFrame) -> pd.DataFrame:\n+    # Fill missing DESCRIPTION\n+    input.fillna(value={schemas.DESCRIPTION: \"No Description\"}, inplace=True)\n+\n+    # Create CLAIM_DETAILS column\n+    input.loc[:, schemas.CLAIM_DETAILS] = input.loc[\n+        :,\n+        [\n+            schemas.SHORT_ID,\n+            schemas.CLAIM_SUBJECT,\n+            schemas.TYPE,\n+            schemas.CLAIM_STATUS,\n+            schemas.DESCRIPTION,\n+        ],\n+    ].to_dict(orient=\"records\")\n+\n+    return input\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/create_community_reports_text.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/create_community_reports_text.py b/packages/graphrag/graphrag/index/workflows/create_community_reports_text.py\nnew file mode 100644\nindex 0000000..94d79ca\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/create_community_reports_text.py\n@@ -0,0 +1,116 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.operations.finalize_community_reports import (\n+    finalize_community_reports,\n+)\n+from graphrag.index.operations.summarize_communities.explode_communities import (\n+    explode_communities,\n+)\n+from graphrag.index.operations.summarize_communities.summarize_communities import (\n+    summarize_communities,\n+)\n+from graphrag.index.operations.summarize_communities.text_unit_context.context_builder import (\n+    build_level_context,\n+    build_local_context,\n+)\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform community reports.\"\"\"\n+    logger.info(\"Workflow started: create_community_reports_text\")\n+    entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    communities = await load_table_from_storage(\"communities\", context.output_storage)\n+\n+    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+\n+    model_config = config.get_language_model_config(config.community_reports.model_id)\n+    model = ModelManager().get_or_create_chat_model(\n+        name=config.community_reports.model_instance_name,\n+        model_type=model_config.type,\n+        config=model_config,\n+        callbacks=context.callbacks,\n+        cache=context.cache,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config)\n+\n+    prompts = config.community_reports.resolved_prompts(config.root_dir)\n+\n+    output = await create_community_reports_text(\n+        entities,\n+        communities,\n+        text_units,\n+        context.callbacks,\n+        model=model,\n+        tokenizer=tokenizer,\n+        prompt=prompts.text_prompt,\n+        max_input_length=config.community_reports.max_input_length,\n+        max_report_length=config.community_reports.max_length,\n+        num_threads=model_config.concurrent_requests,\n+        async_type=model_config.async_mode,\n+    )\n+\n+    await write_table_to_storage(output, \"community_reports\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: create_community_reports_text\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+async def create_community_reports_text(\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    text_units: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    tokenizer: Tokenizer,\n+    prompt: str,\n+    max_input_length: int,\n+    max_report_length: int,\n+    num_threads: int,\n+    async_type: AsyncType,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform community reports.\"\"\"\n+    nodes = explode_communities(communities, entities)\n+\n+    local_contexts = build_local_context(\n+        communities, text_units, nodes, tokenizer, max_input_length\n+    )\n+\n+    community_reports = await summarize_communities(\n+        nodes,\n+        communities,\n+        local_contexts,\n+        build_level_context,\n+        callbacks,\n+        model=model,\n+        prompt=prompt,\n+        tokenizer=tokenizer,\n+        max_input_length=max_input_length,\n+        max_report_length=max_report_length,\n+        num_threads=num_threads,\n+        async_type=async_type,\n+    )\n+\n+    return finalize_community_reports(community_reports, communities)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/create_final_documents.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/create_final_documents.py b/packages/graphrag/graphrag/index/workflows/create_final_documents.py\nnew file mode 100644\nindex 0000000..f12560b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/create_final_documents.py\n@@ -0,0 +1,73 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.data_model.schemas import DOCUMENTS_FINAL_COLUMNS\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    _config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform final documents.\"\"\"\n+    logger.info(\"Workflow started: create_final_documents\")\n+    documents = await load_table_from_storage(\"documents\", context.output_storage)\n+    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+\n+    output = create_final_documents(documents, text_units)\n+\n+    await write_table_to_storage(output, \"documents\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: create_final_documents\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+def create_final_documents(\n+    documents: pd.DataFrame, text_units: pd.DataFrame\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform final documents.\"\"\"\n+    renamed = text_units.loc[:, [\"id\", \"document_id\", \"text\"]].rename(\n+        columns={\n+            \"document_id\": \"chunk_doc_id\",\n+            \"id\": \"chunk_id\",\n+            \"text\": \"chunk_text\",\n+        }\n+    )\n+\n+    joined = renamed.merge(\n+        documents,\n+        left_on=\"chunk_doc_id\",\n+        right_on=\"id\",\n+        how=\"inner\",\n+        copy=False,\n+    )\n+\n+    docs_with_text_units = joined.groupby(\"id\", sort=False).agg(\n+        text_unit_ids=(\"chunk_id\", list)\n+    )\n+\n+    rejoined = docs_with_text_units.merge(\n+        documents,\n+        on=\"id\",\n+        how=\"right\",\n+        copy=False,\n+    ).reset_index(drop=True)\n+\n+    rejoined[\"id\"] = rejoined[\"id\"].astype(str)\n+    rejoined[\"human_readable_id\"] = rejoined.index\n+\n+    if \"metadata\" not in rejoined.columns:\n+        rejoined[\"metadata\"] = pd.Series(dtype=\"object\")\n+\n+    return rejoined.loc[:, DOCUMENTS_FINAL_COLUMNS]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/create_final_text_units.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/create_final_text_units.py b/packages/graphrag/graphrag/index/workflows/create_final_text_units.py\nnew file mode 100644\nindex 0000000..fbcf0af\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/create_final_text_units.py\n@@ -0,0 +1,127 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.data_model.schemas import TEXT_UNITS_FINAL_COLUMNS\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import (\n+    load_table_from_storage,\n+    storage_has_table,\n+    write_table_to_storage,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform the text units.\"\"\"\n+    logger.info(\"Workflow started: create_final_text_units\")\n+    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+    final_entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    final_relationships = await load_table_from_storage(\n+        \"relationships\", context.output_storage\n+    )\n+    final_covariates = None\n+    if config.extract_claims.enabled and await storage_has_table(\n+        \"covariates\", context.output_storage\n+    ):\n+        final_covariates = await load_table_from_storage(\n+            \"covariates\", context.output_storage\n+        )\n+\n+    output = create_final_text_units(\n+        text_units,\n+        final_entities,\n+        final_relationships,\n+        final_covariates,\n+    )\n+\n+    await write_table_to_storage(output, \"text_units\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: create_final_text_units\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+def create_final_text_units(\n+    text_units: pd.DataFrame,\n+    final_entities: pd.DataFrame,\n+    final_relationships: pd.DataFrame,\n+    final_covariates: pd.DataFrame | None,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to transform the text units.\"\"\"\n+    selected = text_units.loc[:, [\"id\", \"text\", \"document_id\", \"n_tokens\"]]\n+    selected[\"human_readable_id\"] = selected.index\n+\n+    entity_join = _entities(final_entities)\n+    relationship_join = _relationships(final_relationships)\n+\n+    entity_joined = _join(selected, entity_join)\n+    relationship_joined = _join(entity_joined, relationship_join)\n+    final_joined = relationship_joined\n+\n+    if final_covariates is not None:\n+        covariate_join = _covariates(final_covariates)\n+        final_joined = _join(relationship_joined, covariate_join)\n+    else:\n+        final_joined[\"covariate_ids\"] = [[] for i in range(len(final_joined))]\n+\n+    aggregated = final_joined.groupby(\"id\", sort=False).agg(\"first\").reset_index()\n+\n+    return aggregated.loc[\n+        :,\n+        TEXT_UNITS_FINAL_COLUMNS,\n+    ]\n+\n+\n+def _entities(df: pd.DataFrame) -> pd.DataFrame:\n+    selected = df.loc[:, [\"id\", \"text_unit_ids\"]]\n+    unrolled = selected.explode([\"text_unit_ids\"]).reset_index(drop=True)\n+\n+    return (\n+        unrolled.groupby(\"text_unit_ids\", sort=False)\n+        .agg(entity_ids=(\"id\", \"unique\"))\n+        .reset_index()\n+        .rename(columns={\"text_unit_ids\": \"id\"})\n+    )\n+\n+\n+def _relationships(df: pd.DataFrame) -> pd.DataFrame:\n+    selected = df.loc[:, [\"id\", \"text_unit_ids\"]]\n+    unrolled = selected.explode([\"text_unit_ids\"]).reset_index(drop=True)\n+\n+    return (\n+        unrolled.groupby(\"text_unit_ids\", sort=False)\n+        .agg(relationship_ids=(\"id\", \"unique\"))\n+        .reset_index()\n+        .rename(columns={\"text_unit_ids\": \"id\"})\n+    )\n+\n+\n+def _covariates(df: pd.DataFrame) -> pd.DataFrame:\n+    selected = df.loc[:, [\"id\", \"text_unit_id\"]]\n+\n+    return (\n+        selected.groupby(\"text_unit_id\", sort=False)\n+        .agg(covariate_ids=(\"id\", \"unique\"))\n+        .reset_index()\n+        .rename(columns={\"text_unit_id\": \"id\"})\n+    )\n+\n+\n+def _join(left, right):\n+    return left.merge(\n+        right,\n+        on=\"id\",\n+        how=\"left\",\n+        suffixes=[\"_1\", \"_2\"],\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/extract_covariates.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/extract_covariates.py b/packages/graphrag/graphrag/index/workflows/extract_covariates.py\nnew file mode 100644\nindex 0000000..52da074\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/extract_covariates.py\n@@ -0,0 +1,103 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+from uuid import uuid4\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.defaults import DEFAULT_ENTITY_TYPES\n+from graphrag.config.enums import AsyncType\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.data_model.schemas import COVARIATES_FINAL_COLUMNS\n+from graphrag.index.operations.extract_covariates.extract_covariates import (\n+    extract_covariates as extractor,\n+)\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to extract and format covariates.\"\"\"\n+    logger.info(\"Workflow started: extract_covariates\")\n+    output = None\n+    if config.extract_claims.enabled:\n+        text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+\n+        model_config = config.get_language_model_config(config.extract_claims.model_id)\n+\n+        model = ModelManager().get_or_create_chat_model(\n+            name=config.extract_claims.model_instance_name,\n+            model_type=model_config.type,\n+            config=model_config,\n+            callbacks=context.callbacks,\n+            cache=context.cache,\n+        )\n+\n+        prompts = config.extract_claims.resolved_prompts(config.root_dir)\n+\n+        output = await extract_covariates(\n+            text_units=text_units,\n+            callbacks=context.callbacks,\n+            model=model,\n+            covariate_type=\"claim\",\n+            max_gleanings=config.extract_claims.max_gleanings,\n+            claim_description=config.extract_claims.description,\n+            prompt=prompts.extraction_prompt,\n+            entity_types=DEFAULT_ENTITY_TYPES,\n+            num_threads=model_config.concurrent_requests,\n+            async_type=model_config.async_mode,\n+        )\n+\n+        await write_table_to_storage(output, \"covariates\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: extract_covariates\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+async def extract_covariates(\n+    text_units: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    covariate_type: str,\n+    max_gleanings: int,\n+    claim_description: str,\n+    prompt: str,\n+    entity_types: list[str],\n+    num_threads: int,\n+    async_type: AsyncType,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to extract and format covariates.\"\"\"\n+    # reassign the id because it will be overwritten in the output by a covariate one\n+    # this also results in text_unit_id being copied to the output covariate table\n+    text_units[\"text_unit_id\"] = text_units[\"id\"]\n+\n+    covariates = await extractor(\n+        input=text_units,\n+        callbacks=callbacks,\n+        model=model,\n+        column=\"text\",\n+        covariate_type=covariate_type,\n+        max_gleanings=max_gleanings,\n+        claim_description=claim_description,\n+        prompt=prompt,\n+        entity_types=entity_types,\n+        num_threads=num_threads,\n+        async_type=async_type,\n+    )\n+    text_units.drop(columns=[\"text_unit_id\"], inplace=True)  # don't pollute the global\n+    covariates[\"id\"] = covariates[\"covariate_type\"].apply(lambda _x: str(uuid4()))\n+    covariates[\"human_readable_id\"] = covariates.index\n+\n+    return covariates.loc[:, COVARIATES_FINAL_COLUMNS]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/extract_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/extract_graph.py b/packages/graphrag/graphrag/index/workflows/extract_graph.py\nnew file mode 100644\nindex 0000000..1a28a39\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/extract_graph.py\n@@ -0,0 +1,189 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.enums import AsyncType\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.operations.extract_graph.extract_graph import (\n+    extract_graph as extractor,\n+)\n+from graphrag.index.operations.summarize_descriptions.summarize_descriptions import (\n+    summarize_descriptions,\n+)\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to create the base entity graph.\"\"\"\n+    logger.info(\"Workflow started: extract_graph\")\n+    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+\n+    extraction_model_config = config.get_language_model_config(\n+        config.extract_graph.model_id\n+    )\n+    extraction_prompts = config.extract_graph.resolved_prompts(config.root_dir)\n+    extraction_model = ModelManager().get_or_create_chat_model(\n+        name=config.extract_graph.model_instance_name,\n+        model_type=extraction_model_config.type,\n+        config=extraction_model_config,\n+        cache=context.cache,\n+    )\n+\n+    summarization_model_config = config.get_language_model_config(\n+        config.summarize_descriptions.model_id\n+    )\n+    summarization_prompts = config.summarize_descriptions.resolved_prompts(\n+        config.root_dir\n+    )\n+    summarization_model = ModelManager().get_or_create_chat_model(\n+        name=config.summarize_descriptions.model_instance_name,\n+        model_type=summarization_model_config.type,\n+        config=summarization_model_config,\n+        cache=context.cache,\n+    )\n+\n+    entities, relationships, raw_entities, raw_relationships = await extract_graph(\n+        text_units=text_units,\n+        callbacks=context.callbacks,\n+        extraction_model=extraction_model,\n+        extraction_prompt=extraction_prompts.extraction_prompt,\n+        entity_types=config.extract_graph.entity_types,\n+        max_gleanings=config.extract_graph.max_gleanings,\n+        extraction_num_threads=extraction_model_config.concurrent_requests,\n+        extraction_async_type=extraction_model_config.async_mode,\n+        summarization_model=summarization_model,\n+        max_summary_length=config.summarize_descriptions.max_length,\n+        max_input_tokens=config.summarize_descriptions.max_input_tokens,\n+        summarization_prompt=summarization_prompts.summarize_prompt,\n+        summarization_num_threads=summarization_model_config.concurrent_requests,\n+    )\n+\n+    await write_table_to_storage(entities, \"entities\", context.output_storage)\n+    await write_table_to_storage(relationships, \"relationships\", context.output_storage)\n+\n+    if config.snapshots.raw_graph:\n+        await write_table_to_storage(\n+            raw_entities, \"raw_entities\", context.output_storage\n+        )\n+        await write_table_to_storage(\n+            raw_relationships, \"raw_relationships\", context.output_storage\n+        )\n+\n+    logger.info(\"Workflow completed: extract_graph\")\n+    return WorkflowFunctionOutput(\n+        result={\n+            \"entities\": entities,\n+            \"relationships\": relationships,\n+        }\n+    )\n+\n+\n+async def extract_graph(\n+    text_units: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    extraction_model: ChatModel,\n+    extraction_prompt: str,\n+    entity_types: list[str],\n+    max_gleanings: int,\n+    extraction_num_threads: int,\n+    extraction_async_type: AsyncType,\n+    summarization_model: ChatModel,\n+    max_summary_length: int,\n+    max_input_tokens: int,\n+    summarization_prompt: str,\n+    summarization_num_threads: int,\n+) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n+    \"\"\"All the steps to create the base entity graph.\"\"\"\n+    # this returns a graph for each text unit, to be merged later\n+    extracted_entities, extracted_relationships = await extractor(\n+        text_units=text_units,\n+        callbacks=callbacks,\n+        text_column=\"text\",\n+        id_column=\"id\",\n+        model=extraction_model,\n+        prompt=extraction_prompt,\n+        entity_types=entity_types,\n+        max_gleanings=max_gleanings,\n+        num_threads=extraction_num_threads,\n+        async_type=extraction_async_type,\n+    )\n+\n+    if not _validate_data(extracted_entities):\n+        error_msg = \"Entity Extraction failed. No entities detected during extraction.\"\n+        logger.error(error_msg)\n+        raise ValueError(error_msg)\n+\n+    if not _validate_data(extracted_relationships):\n+        error_msg = (\n+            \"Entity Extraction failed. No relationships detected during extraction.\"\n+        )\n+        logger.error(error_msg)\n+        raise ValueError(error_msg)\n+\n+    # copy these as is before any summarization\n+    raw_entities = extracted_entities.copy()\n+    raw_relationships = extracted_relationships.copy()\n+\n+    entities, relationships = await get_summarized_entities_relationships(\n+        extracted_entities=extracted_entities,\n+        extracted_relationships=extracted_relationships,\n+        callbacks=callbacks,\n+        model=summarization_model,\n+        max_summary_length=max_summary_length,\n+        max_input_tokens=max_input_tokens,\n+        summarization_prompt=summarization_prompt,\n+        num_threads=summarization_num_threads,\n+    )\n+\n+    return (entities, relationships, raw_entities, raw_relationships)\n+\n+\n+async def get_summarized_entities_relationships(\n+    extracted_entities: pd.DataFrame,\n+    extracted_relationships: pd.DataFrame,\n+    callbacks: WorkflowCallbacks,\n+    model: ChatModel,\n+    max_summary_length: int,\n+    max_input_tokens: int,\n+    summarization_prompt: str,\n+    num_threads: int,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"Summarize the entities and relationships.\"\"\"\n+    entity_summaries, relationship_summaries = await summarize_descriptions(\n+        entities_df=extracted_entities,\n+        relationships_df=extracted_relationships,\n+        callbacks=callbacks,\n+        model=model,\n+        max_summary_length=max_summary_length,\n+        max_input_tokens=max_input_tokens,\n+        prompt=summarization_prompt,\n+        num_threads=num_threads,\n+    )\n+\n+    relationships = extracted_relationships.drop(columns=[\"description\"]).merge(\n+        relationship_summaries, on=[\"source\", \"target\"], how=\"left\"\n+    )\n+\n+    extracted_entities.drop(columns=[\"description\"], inplace=True)\n+    entities = extracted_entities.merge(entity_summaries, on=\"title\", how=\"left\")\n+    return entities, relationships\n+\n+\n+def _validate_data(df: pd.DataFrame) -> bool:\n+    \"\"\"Validate that the dataframe has data.\"\"\"\n+    return len(df) > 0\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/extract_graph_nlp.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/extract_graph_nlp.py b/packages/graphrag/graphrag/index/workflows/extract_graph_nlp.py\nnew file mode 100644\nindex 0000000..b49f3e7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/extract_graph_nlp.py\n@@ -0,0 +1,83 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.config.enums import AsyncType\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.operations.build_noun_graph.build_noun_graph import build_noun_graph\n+from graphrag.index.operations.build_noun_graph.np_extractors.base import (\n+    BaseNounPhraseExtractor,\n+)\n+from graphrag.index.operations.build_noun_graph.np_extractors.factory import (\n+    create_noun_phrase_extractor,\n+)\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to create the base entity graph.\"\"\"\n+    logger.info(\"Workflow started: extract_graph_nlp\")\n+    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+\n+    text_analyzer_config = config.extract_graph_nlp.text_analyzer\n+    text_analyzer = create_noun_phrase_extractor(text_analyzer_config)\n+\n+    entities, relationships = await extract_graph_nlp(\n+        text_units,\n+        context.cache,\n+        text_analyzer=text_analyzer,\n+        normalize_edge_weights=config.extract_graph_nlp.normalize_edge_weights,\n+        num_threads=config.extract_graph_nlp.concurrent_requests,\n+        async_type=config.extract_graph_nlp.async_mode,\n+    )\n+\n+    await write_table_to_storage(entities, \"entities\", context.output_storage)\n+    await write_table_to_storage(relationships, \"relationships\", context.output_storage)\n+\n+    logger.info(\"Workflow completed: extract_graph_nlp\")\n+\n+    return WorkflowFunctionOutput(\n+        result={\n+            \"entities\": entities,\n+            \"relationships\": relationships,\n+        }\n+    )\n+\n+\n+async def extract_graph_nlp(\n+    text_units: pd.DataFrame,\n+    cache: PipelineCache,\n+    text_analyzer: BaseNounPhraseExtractor,\n+    normalize_edge_weights: bool,\n+    num_threads: int,\n+    async_type: AsyncType,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"All the steps to create the base entity graph.\"\"\"\n+    extracted_nodes, extracted_edges = await build_noun_graph(\n+        text_units,\n+        text_analyzer=text_analyzer,\n+        normalize_edge_weights=normalize_edge_weights,\n+        num_threads=num_threads,\n+        async_mode=async_type,\n+        cache=cache,\n+    )\n+\n+    # add in any other columns required by downstream workflows\n+    extracted_nodes[\"type\"] = \"NOUN PHRASE\"\n+    extracted_nodes[\"description\"] = \"\"\n+    extracted_edges[\"description\"] = \"\"\n+\n+    return (extracted_nodes, extracted_edges)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/factory.py b/packages/graphrag/graphrag/index/workflows/factory.py\nnew file mode 100644\nindex 0000000..585ecfa\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/factory.py\n@@ -0,0 +1,97 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Encapsulates pipeline construction and selection.\"\"\"\n+\n+import logging\n+from typing import ClassVar\n+\n+from graphrag.config.enums import IndexingMethod\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.typing.pipeline import Pipeline\n+from graphrag.index.typing.workflow import WorkflowFunction\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class PipelineFactory:\n+    \"\"\"A factory class for workflow pipelines.\"\"\"\n+\n+    workflows: ClassVar[dict[str, WorkflowFunction]] = {}\n+    pipelines: ClassVar[dict[str, list[str]]] = {}\n+\n+    @classmethod\n+    def register(cls, name: str, workflow: WorkflowFunction):\n+        \"\"\"Register a custom workflow function.\"\"\"\n+        cls.workflows[name] = workflow\n+\n+    @classmethod\n+    def register_all(cls, workflows: dict[str, WorkflowFunction]):\n+        \"\"\"Register a dict of custom workflow functions.\"\"\"\n+        for name, workflow in workflows.items():\n+            cls.register(name, workflow)\n+\n+    @classmethod\n+    def register_pipeline(cls, name: str, workflows: list[str]):\n+        \"\"\"Register a new pipeline method as a list of workflow names.\"\"\"\n+        cls.pipelines[name] = workflows\n+\n+    @classmethod\n+    def create_pipeline(\n+        cls,\n+        config: GraphRagConfig,\n+        method: IndexingMethod | str = IndexingMethod.Standard,\n+    ) -> Pipeline:\n+        \"\"\"Create a pipeline generator.\"\"\"\n+        workflows = config.workflows or cls.pipelines.get(method, [])\n+        logger.info(\"Creating pipeline with workflows: %s\", workflows)\n+        return Pipeline([(name, cls.workflows[name]) for name in workflows])\n+\n+\n+# --- Register default implementations ---\n+_standard_workflows = [\n+    \"create_base_text_units\",\n+    \"create_final_documents\",\n+    \"extract_graph\",\n+    \"finalize_graph\",\n+    \"extract_covariates\",\n+    \"create_communities\",\n+    \"create_final_text_units\",\n+    \"create_community_reports\",\n+    \"generate_text_embeddings\",\n+]\n+_fast_workflows = [\n+    \"create_base_text_units\",\n+    \"create_final_documents\",\n+    \"extract_graph_nlp\",\n+    \"prune_graph\",\n+    \"finalize_graph\",\n+    \"create_communities\",\n+    \"create_final_text_units\",\n+    \"create_community_reports_text\",\n+    \"generate_text_embeddings\",\n+]\n+_update_workflows = [\n+    \"update_final_documents\",\n+    \"update_entities_relationships\",\n+    \"update_text_units\",\n+    \"update_covariates\",\n+    \"update_communities\",\n+    \"update_community_reports\",\n+    \"update_text_embeddings\",\n+    \"update_clean_state\",\n+]\n+PipelineFactory.register_pipeline(\n+    IndexingMethod.Standard, [\"load_input_documents\", *_standard_workflows]\n+)\n+PipelineFactory.register_pipeline(\n+    IndexingMethod.Fast, [\"load_input_documents\", *_fast_workflows]\n+)\n+PipelineFactory.register_pipeline(\n+    IndexingMethod.StandardUpdate,\n+    [\"load_update_documents\", *_standard_workflows, *_update_workflows],\n+)\n+PipelineFactory.register_pipeline(\n+    IndexingMethod.FastUpdate,\n+    [\"load_update_documents\", *_fast_workflows, *_update_workflows],\n+)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/finalize_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/finalize_graph.py b/packages/graphrag/graphrag/index/workflows/finalize_graph.py\nnew file mode 100644\nindex 0000000..49529ae\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/finalize_graph.py\n@@ -0,0 +1,68 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.operations.create_graph import create_graph\n+from graphrag.index.operations.finalize_entities import finalize_entities\n+from graphrag.index.operations.finalize_relationships import finalize_relationships\n+from graphrag.index.operations.snapshot_graphml import snapshot_graphml\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to create the base entity graph.\"\"\"\n+    logger.info(\"Workflow started: finalize_graph\")\n+    entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    relationships = await load_table_from_storage(\n+        \"relationships\", context.output_storage\n+    )\n+\n+    final_entities, final_relationships = finalize_graph(\n+        entities,\n+        relationships,\n+    )\n+\n+    await write_table_to_storage(final_entities, \"entities\", context.output_storage)\n+    await write_table_to_storage(\n+        final_relationships, \"relationships\", context.output_storage\n+    )\n+\n+    if config.snapshots.graphml:\n+        graph = create_graph(final_relationships, edge_attr=[\"weight\"])\n+\n+        await snapshot_graphml(\n+            graph,\n+            name=\"graph\",\n+            storage=context.output_storage,\n+        )\n+\n+    logger.info(\"Workflow completed: finalize_graph\")\n+    return WorkflowFunctionOutput(\n+        result={\n+            \"entities\": entities,\n+            \"relationships\": relationships,\n+        }\n+    )\n+\n+\n+def finalize_graph(\n+    entities: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"All the steps to finalize the entity and relationship formats.\"\"\"\n+    final_entities = finalize_entities(entities, relationships)\n+    final_relationships = finalize_relationships(relationships)\n+    return (final_entities, final_relationships)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/generate_text_embeddings.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/generate_text_embeddings.py b/packages/graphrag/graphrag/index/workflows/generate_text_embeddings.py\nnew file mode 100644\nindex 0000000..6e47443\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/generate_text_embeddings.py\n@@ -0,0 +1,284 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.embeddings import (\n+    community_full_content_embedding,\n+    community_summary_embedding,\n+    community_title_embedding,\n+    create_index_name,\n+    document_text_embedding,\n+    entity_description_embedding,\n+    entity_title_embedding,\n+    relationship_description_embedding,\n+    text_unit_text_embedding,\n+)\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.config.models.vector_store_config import VectorStoreConfig\n+from graphrag.config.models.vector_store_schema_config import VectorStoreSchemaConfig\n+from graphrag.index.operations.embed_text.embed_text import embed_text\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.utils.storage import (\n+    load_table_from_storage,\n+    write_table_to_storage,\n+)\n+from graphrag.vector_stores.base import BaseVectorStore\n+from graphrag.vector_stores.factory import VectorStoreFactory\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to transform community reports.\"\"\"\n+    logger.info(\"Workflow started: generate_text_embeddings\")\n+    embedded_fields = config.embed_text.names\n+    logger.info(\"Embedding the following fields: %s\", embedded_fields)\n+    documents = None\n+    relationships = None\n+    text_units = None\n+    entities = None\n+    community_reports = None\n+    if document_text_embedding in embedded_fields:\n+        documents = await load_table_from_storage(\"documents\", context.output_storage)\n+    if relationship_description_embedding in embedded_fields:\n+        relationships = await load_table_from_storage(\n+            \"relationships\", context.output_storage\n+        )\n+    if text_unit_text_embedding in embedded_fields:\n+        text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n+    if (\n+        entity_title_embedding in embedded_fields\n+        or entity_description_embedding in embedded_fields\n+    ):\n+        entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    if (\n+        community_title_embedding in embedded_fields\n+        or community_summary_embedding in embedded_fields\n+        or community_full_content_embedding in embedded_fields\n+    ):\n+        community_reports = await load_table_from_storage(\n+            \"community_reports\", context.output_storage\n+        )\n+\n+    model_config = config.get_language_model_config(config.embed_text.model_id)\n+\n+    model = ModelManager().get_or_create_embedding_model(\n+        name=config.embed_text.model_instance_name,\n+        model_type=model_config.type,\n+        config=model_config,\n+        callbacks=context.callbacks,\n+        cache=context.cache,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config)\n+\n+    output = await generate_text_embeddings(\n+        documents=documents,\n+        relationships=relationships,\n+        text_units=text_units,\n+        entities=entities,\n+        community_reports=community_reports,\n+        callbacks=context.callbacks,\n+        model=model,\n+        tokenizer=tokenizer,\n+        batch_size=config.embed_text.batch_size,\n+        batch_max_tokens=config.embed_text.batch_max_tokens,\n+        num_threads=model_config.concurrent_requests,\n+        vector_store_config=config.vector_store,\n+        embedded_fields=embedded_fields,\n+    )\n+\n+    if config.snapshots.embeddings:\n+        for name, table in output.items():\n+            await write_table_to_storage(\n+                table,\n+                f\"embeddings.{name}\",\n+                context.output_storage,\n+            )\n+\n+    logger.info(\"Workflow completed: generate_text_embeddings\")\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+async def generate_text_embeddings(\n+    documents: pd.DataFrame | None,\n+    relationships: pd.DataFrame | None,\n+    text_units: pd.DataFrame | None,\n+    entities: pd.DataFrame | None,\n+    community_reports: pd.DataFrame | None,\n+    callbacks: WorkflowCallbacks,\n+    model: EmbeddingModel,\n+    tokenizer: Tokenizer,\n+    batch_size: int,\n+    batch_max_tokens: int,\n+    num_threads: int,\n+    vector_store_config: VectorStoreConfig,\n+    embedded_fields: list[str],\n+) -> dict[str, pd.DataFrame]:\n+    \"\"\"All the steps to generate all embeddings.\"\"\"\n+    embedding_param_map = {\n+        document_text_embedding: {\n+            \"data\": documents.loc[:, [\"id\", \"text\"]] if documents is not None else None,\n+            \"embed_column\": \"text\",\n+        },\n+        relationship_description_embedding: {\n+            \"data\": relationships.loc[:, [\"id\", \"description\"]]\n+            if relationships is not None\n+            else None,\n+            \"embed_column\": \"description\",\n+        },\n+        text_unit_text_embedding: {\n+            \"data\": text_units.loc[:, [\"id\", \"text\"]]\n+            if text_units is not None\n+            else None,\n+            \"embed_column\": \"text\",\n+        },\n+        entity_title_embedding: {\n+            \"data\": entities.loc[:, [\"id\", \"title\"]] if entities is not None else None,\n+            \"embed_column\": \"title\",\n+        },\n+        entity_description_embedding: {\n+            \"data\": entities.loc[:, [\"id\", \"title\", \"description\"]].assign(\n+                title_description=lambda df: df[\"title\"] + \":\" + df[\"description\"]\n+            )\n+            if entities is not None\n+            else None,\n+            \"embed_column\": \"title_description\",\n+        },\n+        community_title_embedding: {\n+            \"data\": community_reports.loc[:, [\"id\", \"title\"]]\n+            if community_reports is not None\n+            else None,\n+            \"embed_column\": \"title\",\n+        },\n+        community_summary_embedding: {\n+            \"data\": community_reports.loc[:, [\"id\", \"summary\"]]\n+            if community_reports is not None\n+            else None,\n+            \"embed_column\": \"summary\",\n+        },\n+        community_full_content_embedding: {\n+            \"data\": community_reports.loc[:, [\"id\", \"full_content\"]]\n+            if community_reports is not None\n+            else None,\n+            \"embed_column\": \"full_content\",\n+        },\n+    }\n+\n+    logger.info(\"Creating embeddings\")\n+    outputs = {}\n+    for field in embedded_fields:\n+        if embedding_param_map[field][\"data\"] is None:\n+            msg = f\"Embedding {field} is specified but data table is not in storage. This may or may not be intentional - if you expect it to me here, please check for errors earlier in the logs.\"\n+            logger.warning(msg)\n+        else:\n+            outputs[field] = await _run_embeddings(\n+                name=field,\n+                callbacks=callbacks,\n+                model=model,\n+                tokenizer=tokenizer,\n+                vector_store_config=vector_store_config,\n+                batch_size=batch_size,\n+                batch_max_tokens=batch_max_tokens,\n+                num_threads=num_threads,\n+                **embedding_param_map[field],\n+            )\n+    return outputs\n+\n+\n+async def _run_embeddings(\n+    name: str,\n+    data: pd.DataFrame,\n+    embed_column: str,\n+    callbacks: WorkflowCallbacks,\n+    model: EmbeddingModel,\n+    tokenizer: Tokenizer,\n+    batch_size: int,\n+    batch_max_tokens: int,\n+    num_threads: int,\n+    vector_store_config: VectorStoreConfig,\n+) -> pd.DataFrame:\n+    \"\"\"All the steps to generate single embedding.\"\"\"\n+    index_name = _get_index_name(vector_store_config, name)\n+    vector_store = _create_vector_store(vector_store_config, index_name, name)\n+\n+    data[\"embedding\"] = await embed_text(\n+        input=data,\n+        callbacks=callbacks,\n+        model=model,\n+        tokenizer=tokenizer,\n+        embed_column=embed_column,\n+        batch_size=batch_size,\n+        batch_max_tokens=batch_max_tokens,\n+        num_threads=num_threads,\n+        vector_store=vector_store,\n+    )\n+\n+    return data.loc[:, [\"id\", \"embedding\"]]\n+\n+\n+def _create_vector_store(\n+    vector_store_config: VectorStoreConfig,\n+    index_name: str,\n+    embedding_name: str | None = None,\n+) -> BaseVectorStore:\n+    embeddings_schema: dict[str, VectorStoreSchemaConfig] = (\n+        vector_store_config.embeddings_schema\n+    )\n+\n+    single_embedding_config: VectorStoreSchemaConfig = VectorStoreSchemaConfig()\n+\n+    if (\n+        embeddings_schema is not None\n+        and embedding_name is not None\n+        and embedding_name in embeddings_schema\n+    ):\n+        raw_config = embeddings_schema[embedding_name]\n+        if isinstance(raw_config, dict):\n+            single_embedding_config = VectorStoreSchemaConfig(**raw_config)\n+        else:\n+            single_embedding_config = raw_config\n+\n+    if (\n+        single_embedding_config.index_name is not None\n+        and vector_store_config.index_prefix\n+    ):\n+        single_embedding_config.index_name = (\n+            f\"{vector_store_config.index_prefix}-{single_embedding_config.index_name}\"\n+        )\n+\n+    if single_embedding_config.index_name is None:\n+        single_embedding_config.index_name = index_name\n+\n+    args = vector_store_config.model_dump()\n+    args[\"vector_store_schema_config\"] = single_embedding_config\n+    vector_store = VectorStoreFactory().create(\n+        vector_store_config.type,\n+        args,\n+    )\n+\n+    vector_store.connect(**args)\n+    return vector_store\n+\n+\n+def _get_index_name(vector_store_config: VectorStoreConfig, embedding_name: str) -> str:\n+    index_prefix = vector_store_config.index_prefix or \"\"\n+    index_name = create_index_name(index_prefix, embedding_name)\n+\n+    msg = f\"using vector store {vector_store_config.type} with index prefix {index_prefix} for embedding {embedding_name}: {index_name}\"\n+    logger.info(msg)\n+    return index_name\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/load_input_documents.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/load_input_documents.py b/packages/graphrag/graphrag/index/workflows/load_input_documents.py\nnew file mode 100644\nindex 0000000..228ca4d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/load_input_documents.py\n@@ -0,0 +1,42 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.input.factory import InputReaderFactory\n+from graphrag.index.input.input_reader import InputReader\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Load and parse input documents into a standard format.\"\"\"\n+    input_reader = InputReaderFactory().create(\n+        config.input.file_type,\n+        {\"storage\": context.input_storage, \"config\": config.input},\n+    )\n+\n+    output = await load_input_documents(input_reader)\n+\n+    logger.info(\"Final # of rows loaded: %s\", len(output))\n+    context.stats.num_documents = len(output)\n+\n+    await write_table_to_storage(output, \"documents\", context.output_storage)\n+\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+async def load_input_documents(input_reader: InputReader) -> pd.DataFrame:\n+    \"\"\"Load and parse input documents into a standard format.\"\"\"\n+    return await input_reader.read_files()\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/load_update_documents.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/load_update_documents.py b/packages/graphrag/graphrag/index/workflows/load_update_documents.py\nnew file mode 100644\nindex 0000000..7755091\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/load_update_documents.py\n@@ -0,0 +1,57 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.input.factory import InputReaderFactory\n+from graphrag.index.input.input_reader import InputReader\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.update.incremental_index import get_delta_docs\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Load and parse update-only input documents into a standard format.\"\"\"\n+    input_reader = InputReaderFactory().create(\n+        config.input.file_type,\n+        {\"storage\": context.input_storage, \"config\": config.input},\n+    )\n+    output = await load_update_documents(\n+        input_reader,\n+        context.previous_storage,\n+    )\n+\n+    logger.info(\"Final # of update rows loaded: %s\", len(output))\n+    context.stats.update_documents = len(output)\n+\n+    if len(output) == 0:\n+        logger.warning(\"No new update documents found.\")\n+        return WorkflowFunctionOutput(result=None, stop=True)\n+\n+    await write_table_to_storage(output, \"documents\", context.output_storage)\n+\n+    return WorkflowFunctionOutput(result=output)\n+\n+\n+async def load_update_documents(\n+    input_reader: InputReader,\n+    previous_storage: PipelineStorage,\n+) -> pd.DataFrame:\n+    \"\"\"Load and parse update-only input documents into a standard format.\"\"\"\n+    input_documents = await input_reader.read_files()\n+    # previous storage is the output of the previous run\n+    # we'll use this to diff the input from the prior\n+    delta_documents = await get_delta_docs(input_documents, previous_storage)\n+    return delta_documents.new_inputs\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/prune_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/prune_graph.py b/packages/graphrag/graphrag/index/workflows/prune_graph.py\nnew file mode 100644\nindex 0000000..8bb48df\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/prune_graph.py\n@@ -0,0 +1,82 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.config.models.prune_graph_config import PruneGraphConfig\n+from graphrag.index.operations.create_graph import create_graph\n+from graphrag.index.operations.graph_to_dataframes import graph_to_dataframes\n+from graphrag.index.operations.prune_graph import prune_graph as prune_graph_operation\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"All the steps to create the base entity graph.\"\"\"\n+    logger.info(\"Workflow started: prune_graph\")\n+    entities = await load_table_from_storage(\"entities\", context.output_storage)\n+    relationships = await load_table_from_storage(\n+        \"relationships\", context.output_storage\n+    )\n+\n+    pruned_entities, pruned_relationships = prune_graph(\n+        entities,\n+        relationships,\n+        pruning_config=config.prune_graph,\n+    )\n+\n+    await write_table_to_storage(pruned_entities, \"entities\", context.output_storage)\n+    await write_table_to_storage(\n+        pruned_relationships, \"relationships\", context.output_storage\n+    )\n+\n+    logger.info(\"Workflow completed: prune_graph\")\n+    return WorkflowFunctionOutput(\n+        result={\n+            \"entities\": pruned_entities,\n+            \"relationships\": pruned_relationships,\n+        }\n+    )\n+\n+\n+def prune_graph(\n+    entities: pd.DataFrame,\n+    relationships: pd.DataFrame,\n+    pruning_config: PruneGraphConfig,\n+) -> tuple[pd.DataFrame, pd.DataFrame]:\n+    \"\"\"Prune a full graph based on graph statistics.\"\"\"\n+    # create a temporary graph to prune, then turn it back into dataframes\n+    graph = create_graph(relationships, edge_attr=[\"weight\"], nodes=entities)\n+    pruned = prune_graph_operation(\n+        graph,\n+        min_node_freq=pruning_config.min_node_freq,\n+        max_node_freq_std=pruning_config.max_node_freq_std,\n+        min_node_degree=pruning_config.min_node_degree,\n+        max_node_degree_std=pruning_config.max_node_degree_std,\n+        min_edge_weight_pct=pruning_config.min_edge_weight_pct,\n+        remove_ego_nodes=pruning_config.remove_ego_nodes,\n+        lcc_only=pruning_config.lcc_only,\n+    )\n+\n+    pruned_nodes, pruned_edges = graph_to_dataframes(\n+        pruned, node_columns=[\"title\"], edge_columns=[\"source\", \"target\"]\n+    )\n+\n+    # subset the full nodes and edges to only include the pruned remainders\n+    subset_entities = pruned_nodes.merge(entities, on=\"title\", how=\"inner\")\n+    subset_relationships = pruned_edges.merge(\n+        relationships, on=[\"source\", \"target\"], how=\"inner\"\n+    )\n+\n+    return (subset_entities, subset_relationships)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_clean_state.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_clean_state.py b/packages/graphrag/graphrag/index/workflows/update_clean_state.py\nnew file mode 100644\nindex 0000000..3803be4\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_clean_state.py\n@@ -0,0 +1,31 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(  # noqa: RUF029\n+    _config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Clean the state after the update.\"\"\"\n+    logger.info(\"Workflow started: update_clean_state\")\n+    keys_to_delete = [\n+        key_name\n+        for key_name in context.state\n+        if key_name.startswith(\"incremental_update_\")\n+    ]\n+\n+    for key_name in keys_to_delete:\n+        del context.state[key_name]\n+\n+    logger.info(\"Workflow completed: update_clean_state\")\n+    return WorkflowFunctionOutput(result=None)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_communities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_communities.py b/packages/graphrag/graphrag/index/workflows/update_communities.py\nnew file mode 100644\nindex 0000000..b7e3e6a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_communities.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.update.communities import _update_and_merge_communities\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the communities from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_communities\")\n+    output_storage, previous_storage, delta_storage = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+\n+    community_id_mapping = await _update_communities(\n+        previous_storage, delta_storage, output_storage\n+    )\n+\n+    context.state[\"incremental_update_community_id_mapping\"] = community_id_mapping\n+\n+    logger.info(\"Workflow completed: update_communities\")\n+    return WorkflowFunctionOutput(result=None)\n+\n+\n+async def _update_communities(\n+    previous_storage: PipelineStorage,\n+    delta_storage: PipelineStorage,\n+    output_storage: PipelineStorage,\n+) -> dict:\n+    \"\"\"Update the communities output.\"\"\"\n+    old_communities = await load_table_from_storage(\"communities\", previous_storage)\n+    delta_communities = await load_table_from_storage(\"communities\", delta_storage)\n+    merged_communities, community_id_mapping = _update_and_merge_communities(\n+        old_communities, delta_communities\n+    )\n+\n+    await write_table_to_storage(merged_communities, \"communities\", output_storage)\n+\n+    return community_id_mapping\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_community_reports.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_community_reports.py b/packages/graphrag/graphrag/index/workflows/update_community_reports.py\nnew file mode 100644\nindex 0000000..42576ac\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_community_reports.py\n@@ -0,0 +1,66 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.update.communities import _update_and_merge_community_reports\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the community reports from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_community_reports\")\n+    output_storage, previous_storage, delta_storage = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+\n+    community_id_mapping = context.state[\"incremental_update_community_id_mapping\"]\n+\n+    merged_community_reports = await _update_community_reports(\n+        previous_storage, delta_storage, output_storage, community_id_mapping\n+    )\n+\n+    context.state[\"incremental_update_merged_community_reports\"] = (\n+        merged_community_reports\n+    )\n+\n+    logger.info(\"Workflow completed: update_community_reports\")\n+    return WorkflowFunctionOutput(result=None)\n+\n+\n+async def _update_community_reports(\n+    previous_storage: PipelineStorage,\n+    delta_storage: PipelineStorage,\n+    output_storage: PipelineStorage,\n+    community_id_mapping: dict,\n+) -> pd.DataFrame:\n+    \"\"\"Update the community reports output.\"\"\"\n+    old_community_reports = await load_table_from_storage(\n+        \"community_reports\", previous_storage\n+    )\n+    delta_community_reports = await load_table_from_storage(\n+        \"community_reports\", delta_storage\n+    )\n+    merged_community_reports = _update_and_merge_community_reports(\n+        old_community_reports, delta_community_reports, community_id_mapping\n+    )\n+\n+    await write_table_to_storage(\n+        merged_community_reports, \"community_reports\", output_storage\n+    )\n+\n+    return merged_community_reports\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_covariates.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_covariates.py b/packages/graphrag/graphrag/index/workflows/update_covariates.py\nnew file mode 100644\nindex 0000000..f0bf29a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_covariates.py\n@@ -0,0 +1,82 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import (\n+    load_table_from_storage,\n+    storage_has_table,\n+    write_table_to_storage,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the covariates from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_covariates\")\n+    output_storage, previous_storage, delta_storage = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+\n+    if await storage_has_table(\n+        \"covariates\", previous_storage\n+    ) and await storage_has_table(\"covariates\", delta_storage):\n+        logger.info(\"Updating Covariates\")\n+        await _update_covariates(previous_storage, delta_storage, output_storage)\n+\n+    logger.info(\"Workflow completed: update_covariates\")\n+    return WorkflowFunctionOutput(result=None)\n+\n+\n+async def _update_covariates(\n+    previous_storage: PipelineStorage,\n+    delta_storage: PipelineStorage,\n+    output_storage: PipelineStorage,\n+) -> None:\n+    \"\"\"Update the covariates output.\"\"\"\n+    old_covariates = await load_table_from_storage(\"covariates\", previous_storage)\n+    delta_covariates = await load_table_from_storage(\"covariates\", delta_storage)\n+    merged_covariates = _merge_covariates(old_covariates, delta_covariates)\n+\n+    await write_table_to_storage(merged_covariates, \"covariates\", output_storage)\n+\n+\n+def _merge_covariates(\n+    old_covariates: pd.DataFrame, delta_covariates: pd.DataFrame\n+) -> pd.DataFrame:\n+    \"\"\"Merge the covariates.\n+\n+    Parameters\n+    ----------\n+    old_covariates : pd.DataFrame\n+        The old covariates.\n+    delta_covariates : pd.DataFrame\n+        The delta covariates.\n+\n+    Returns\n+    -------\n+    pd.DataFrame\n+        The merged covariates.\n+    \"\"\"\n+    # Get the max human readable id from the old covariates and update the delta covariates\n+    initial_id = old_covariates[\"human_readable_id\"].max() + 1\n+    delta_covariates[\"human_readable_id\"] = np.arange(\n+        initial_id, initial_id + len(delta_covariates)\n+    )\n+\n+    # Concatenate the old and delta covariates\n+    return pd.concat([old_covariates, delta_covariates], ignore_index=True, copy=False)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_entities_relationships.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_entities_relationships.py b/packages/graphrag/graphrag/index/workflows/update_entities_relationships.py\nnew file mode 100644\nindex 0000000..69bdefa\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_entities_relationships.py\n@@ -0,0 +1,113 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import pandas as pd\n+\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.callbacks.workflow_callbacks import WorkflowCallbacks\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.update.entities import _group_and_resolve_entities\n+from graphrag.index.update.relationships import _update_and_merge_relationships\n+from graphrag.index.workflows.extract_graph import get_summarized_entities_relationships\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the entities and relationships from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_entities_relationships\")\n+    output_storage, previous_storage, delta_storage = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+\n+    (\n+        merged_entities_df,\n+        merged_relationships_df,\n+        entity_id_mapping,\n+    ) = await _update_entities_and_relationships(\n+        previous_storage,\n+        delta_storage,\n+        output_storage,\n+        config,\n+        context.cache,\n+        context.callbacks,\n+    )\n+\n+    context.state[\"incremental_update_merged_entities\"] = merged_entities_df\n+    context.state[\"incremental_update_merged_relationships\"] = merged_relationships_df\n+    context.state[\"incremental_update_entity_id_mapping\"] = entity_id_mapping\n+\n+    logger.info(\"Workflow completed: update_entities_relationships\")\n+    return WorkflowFunctionOutput(result=None)\n+\n+\n+async def _update_entities_and_relationships(\n+    previous_storage: PipelineStorage,\n+    delta_storage: PipelineStorage,\n+    output_storage: PipelineStorage,\n+    config: GraphRagConfig,\n+    cache: PipelineCache,\n+    callbacks: WorkflowCallbacks,\n+) -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n+    \"\"\"Update Final Entities  and Relationships output.\"\"\"\n+    old_entities = await load_table_from_storage(\"entities\", previous_storage)\n+    delta_entities = await load_table_from_storage(\"entities\", delta_storage)\n+\n+    merged_entities_df, entity_id_mapping = _group_and_resolve_entities(\n+        old_entities, delta_entities\n+    )\n+\n+    # Update Relationships\n+    old_relationships = await load_table_from_storage(\"relationships\", previous_storage)\n+    delta_relationships = await load_table_from_storage(\"relationships\", delta_storage)\n+    merged_relationships_df = _update_and_merge_relationships(\n+        old_relationships,\n+        delta_relationships,\n+    )\n+\n+    summarization_model_config = config.get_language_model_config(\n+        config.summarize_descriptions.model_id\n+    )\n+    prompts = config.summarize_descriptions.resolved_prompts(config.root_dir)\n+    model = ModelManager().get_or_create_chat_model(\n+        name=\"summarize_descriptions\",\n+        model_type=summarization_model_config.type,\n+        config=summarization_model_config,\n+        cache=cache,\n+    )\n+\n+    (\n+        merged_entities_df,\n+        merged_relationships_df,\n+    ) = await get_summarized_entities_relationships(\n+        extracted_entities=merged_entities_df,\n+        extracted_relationships=merged_relationships_df,\n+        callbacks=callbacks,\n+        model=model,\n+        max_summary_length=config.summarize_descriptions.max_length,\n+        max_input_tokens=config.summarize_descriptions.max_input_tokens,\n+        summarization_prompt=prompts.summarize_prompt,\n+        num_threads=summarization_model_config.concurrent_requests,\n+    )\n+\n+    # Save the updated entities back to storage\n+    await write_table_to_storage(merged_entities_df, \"entities\", output_storage)\n+\n+    await write_table_to_storage(\n+        merged_relationships_df, \"relationships\", output_storage\n+    )\n+\n+    return merged_entities_df, merged_relationships_df, entity_id_mapping\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_final_documents.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_final_documents.py b/packages/graphrag/graphrag/index/workflows/update_final_documents.py\nnew file mode 100644\nindex 0000000..b684beb\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_final_documents.py\n@@ -0,0 +1,34 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.update.incremental_index import concat_dataframes\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the documents from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_final_documents\")\n+    output_storage, previous_storage, delta_storage = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+\n+    final_documents = await concat_dataframes(\n+        \"documents\", previous_storage, delta_storage, output_storage\n+    )\n+\n+    context.state[\"incremental_update_final_documents\"] = final_documents\n+\n+    logger.info(\"Workflow completed: update_final_documents\")\n+    return WorkflowFunctionOutput(result=None)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_text_embeddings.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_text_embeddings.py b/packages/graphrag/graphrag/index/workflows/update_text_embeddings.py\nnew file mode 100644\nindex 0000000..0fc5f1f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_text_embeddings.py\n@@ -0,0 +1,76 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.index.workflows.generate_text_embeddings import generate_text_embeddings\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.utils.storage import write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the text embeddings from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_text_embeddings\")\n+    output_storage, _, _ = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+\n+    final_documents_df = context.state[\"incremental_update_final_documents\"]\n+    merged_relationships_df = context.state[\"incremental_update_merged_relationships\"]\n+    merged_text_units = context.state[\"incremental_update_merged_text_units\"]\n+    merged_entities_df = context.state[\"incremental_update_merged_entities\"]\n+    merged_community_reports = context.state[\n+        \"incremental_update_merged_community_reports\"\n+    ]\n+\n+    embedded_fields = config.embed_text.names\n+\n+    model_config = config.get_language_model_config(config.embed_text.model_id)\n+\n+    model = ModelManager().get_or_create_embedding_model(\n+        name=\"text_embedding\",\n+        model_type=model_config.type,\n+        config=model_config,\n+        callbacks=context.callbacks,\n+        cache=context.cache,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config)\n+\n+    result = await generate_text_embeddings(\n+        documents=final_documents_df,\n+        relationships=merged_relationships_df,\n+        text_units=merged_text_units,\n+        entities=merged_entities_df,\n+        community_reports=merged_community_reports,\n+        callbacks=context.callbacks,\n+        model=model,\n+        tokenizer=tokenizer,\n+        batch_size=config.embed_text.batch_size,\n+        batch_max_tokens=config.embed_text.batch_max_tokens,\n+        num_threads=model_config.concurrent_requests,\n+        vector_store_config=config.vector_store,\n+        embedded_fields=embedded_fields,\n+    )\n+    if config.snapshots.embeddings:\n+        for name, table in result.items():\n+            await write_table_to_storage(\n+                table,\n+                f\"embeddings.{name}\",\n+                output_storage,\n+            )\n+\n+    logger.info(\"Workflow completed: update_text_embeddings\")\n+    return WorkflowFunctionOutput(result=None)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/index/workflows/update_text_units.py",
            "diff": "diff --git a/packages/graphrag/graphrag/index/workflows/update_text_units.py b/packages/graphrag/graphrag/index/workflows/update_text_units.py\nnew file mode 100644\nindex 0000000..392533f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/index/workflows/update_text_units.py\n@@ -0,0 +1,92 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing run_workflow method definition.\"\"\"\n+\n+import logging\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.run.utils import get_update_storages\n+from graphrag.index.typing.context import PipelineRunContext\n+from graphrag.index.typing.workflow import WorkflowFunctionOutput\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.utils.storage import load_table_from_storage, write_table_to_storage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def run_workflow(\n+    config: GraphRagConfig,\n+    context: PipelineRunContext,\n+) -> WorkflowFunctionOutput:\n+    \"\"\"Update the text units from a incremental index run.\"\"\"\n+    logger.info(\"Workflow started: update_text_units\")\n+    output_storage, previous_storage, delta_storage = get_update_storages(\n+        config, context.state[\"update_timestamp\"]\n+    )\n+    entity_id_mapping = context.state[\"incremental_update_entity_id_mapping\"]\n+\n+    merged_text_units = await _update_text_units(\n+        previous_storage, delta_storage, output_storage, entity_id_mapping\n+    )\n+\n+    context.state[\"incremental_update_merged_text_units\"] = merged_text_units\n+\n+    logger.info(\"Workflow completed: update_text_units\")\n+    return WorkflowFunctionOutput(result=None)\n+\n+\n+async def _update_text_units(\n+    previous_storage: PipelineStorage,\n+    delta_storage: PipelineStorage,\n+    output_storage: PipelineStorage,\n+    entity_id_mapping: dict,\n+) -> pd.DataFrame:\n+    \"\"\"Update the text units output.\"\"\"\n+    old_text_units = await load_table_from_storage(\"text_units\", previous_storage)\n+    delta_text_units = await load_table_from_storage(\"text_units\", delta_storage)\n+    merged_text_units = _update_and_merge_text_units(\n+        old_text_units, delta_text_units, entity_id_mapping\n+    )\n+\n+    await write_table_to_storage(merged_text_units, \"text_units\", output_storage)\n+\n+    return merged_text_units\n+\n+\n+def _update_and_merge_text_units(\n+    old_text_units: pd.DataFrame,\n+    delta_text_units: pd.DataFrame,\n+    entity_id_mapping: dict,\n+) -> pd.DataFrame:\n+    \"\"\"Update and merge text units.\n+\n+    Parameters\n+    ----------\n+    old_text_units : pd.DataFrame\n+        The old text units.\n+    delta_text_units : pd.DataFrame\n+        The delta text units.\n+    entity_id_mapping : dict\n+        The entity id mapping.\n+\n+    Returns\n+    -------\n+    pd.DataFrame\n+        The updated text units.\n+    \"\"\"\n+    # Look for entity ids in entity_ids and replace them with the corresponding id in the mapping\n+    if entity_id_mapping:\n+        delta_text_units[\"entity_ids\"] = delta_text_units[\"entity_ids\"].apply(\n+            lambda x: [entity_id_mapping.get(i, i) for i in x] if x is not None else x\n+        )\n+\n+    initial_id = old_text_units[\"human_readable_id\"].max() + 1\n+    delta_text_units[\"human_readable_id\"] = np.arange(\n+        initial_id, initial_id + len(delta_text_units)\n+    )\n+    # Merge the final text units\n+    return pd.concat([old_text_units, delta_text_units], ignore_index=True, copy=False)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/__init__.py b/packages/graphrag/graphrag/language_model/__init__.py\nnew file mode 100644\nindex 0000000..1c84bfd\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GraphRAG Language Models module. Allows for provider registrations while providing some out-of-the-box solutions.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/cache/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/cache/__init__.py b/packages/graphrag/graphrag/language_model/cache/__init__.py\nnew file mode 100644\nindex 0000000..41cca79\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/cache/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Cache provider definitions for Language Models.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/cache/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/cache/base.py b/packages/graphrag/graphrag/language_model/cache/base.py\nnew file mode 100644\nindex 0000000..554d02c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/cache/base.py\n@@ -0,0 +1,36 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base cache protocol definition.\"\"\"\n+\n+from typing import Any, Protocol\n+\n+\n+class ModelCache(Protocol):\n+    \"\"\"Base cache protocol.\"\"\"\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Check if the cache has a value.\"\"\"\n+        ...\n+\n+    async def get(self, key: str) -> Any | None:\n+        \"\"\"Retrieve a value from the cache.\"\"\"\n+        ...\n+\n+    async def set(\n+        self, key: str, value: Any, metadata: dict[str, Any] | None = None\n+    ) -> None:\n+        \"\"\"Write a value into the cache.\"\"\"\n+        ...\n+\n+    async def remove(self, key: str) -> None:\n+        \"\"\"Remove a value from the cache.\"\"\"\n+        ...\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear the cache.\"\"\"\n+        ...\n+\n+    def child(self, key: str) -> Any:\n+        \"\"\"Create a child cache.\"\"\"\n+        ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/events/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/events/__init__.py b/packages/graphrag/graphrag/language_model/events/__init__.py\nnew file mode 100644\nindex 0000000..c6abec3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/events/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Model Event handler modules.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/events/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/events/base.py b/packages/graphrag/graphrag/language_model/events/base.py\nnew file mode 100644\nindex 0000000..940da3f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/events/base.py\n@@ -0,0 +1,19 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base model events protocol.\"\"\"\n+\n+from typing import Any, Protocol\n+\n+\n+class ModelEventHandler(Protocol):\n+    \"\"\"Protocol for Model event handling.\"\"\"\n+\n+    async def on_error(\n+        self,\n+        error: BaseException | None,\n+        traceback: str | None = None,\n+        arguments: dict[str, Any] | None = None,\n+    ) -> None:\n+        \"\"\"Handle an model error.\"\"\"\n+        ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/factory.py b/packages/graphrag/graphrag/language_model/factory.py\nnew file mode 100644\nindex 0000000..6ff32b5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/factory.py\n@@ -0,0 +1,25 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing a factory for supported llm types.\"\"\"\n+\n+from graphrag.config.enums import ModelType\n+from graphrag.factory.factory import Factory\n+from graphrag.language_model.protocol.base import ChatModel, EmbeddingModel\n+from graphrag.language_model.providers.litellm.chat_model import LitellmChatModel\n+from graphrag.language_model.providers.litellm.embedding_model import (\n+    LitellmEmbeddingModel,\n+)\n+\n+\n+class ChatModelFactory(Factory[ChatModel]):\n+    \"\"\"Singleton factory for creating ChatModel instances.\"\"\"\n+\n+\n+class EmbeddingModelFactory(Factory[EmbeddingModel]):\n+    \"\"\"Singleton factory for creating EmbeddingModel instances.\"\"\"\n+\n+\n+# --- Register default implementations ---\n+ChatModelFactory().register(ModelType.Chat, LitellmChatModel)\n+EmbeddingModelFactory().register(ModelType.Embedding, LitellmEmbeddingModel)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/manager.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/manager.py b/packages/graphrag/graphrag/language_model/manager.py\nnew file mode 100644\nindex 0000000..29349cb\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/manager.py\n@@ -0,0 +1,151 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Singleton LLM Manager for ChatLLM and EmbeddingsLLM instances.\n+\n+This manager lets you register chat and embeddings LLMs independently.\n+It leverages the LLMFactory for instantiation.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, ClassVar\n+\n+from typing_extensions import Self\n+\n+from graphrag.language_model.factory import ChatModelFactory, EmbeddingModelFactory\n+\n+if TYPE_CHECKING:\n+    from graphrag.language_model.protocol.base import ChatModel, EmbeddingModel\n+\n+\n+class ModelManager:\n+    \"\"\"Singleton manager for LLM instances.\"\"\"\n+\n+    _instance: ClassVar[ModelManager | None] = None\n+\n+    def __new__(cls) -> Self:\n+        \"\"\"Create a new instance of LLMManager if it does not exist.\"\"\"\n+        if cls._instance is None:\n+            cls._instance = super().__new__(cls)\n+        return cls._instance  # type: ignore[return-value]\n+\n+    def __init__(self) -> None:\n+        # Avoid reinitialization in the singleton.\n+        if not hasattr(self, \"_initialized\"):\n+            self.chat_models: dict[str, ChatModel] = {}\n+            self.embedding_models: dict[str, EmbeddingModel] = {}\n+            self._initialized = True\n+\n+    @classmethod\n+    def get_instance(cls) -> ModelManager:\n+        \"\"\"Return the singleton instance of LLMManager.\"\"\"\n+        return cls.__new__(cls)\n+\n+    def register_chat(\n+        self, name: str, model_type: str, **chat_kwargs: Any\n+    ) -> ChatModel:\n+        \"\"\"\n+        Register a ChatLLM instance under a unique name.\n+\n+        Args:\n+            name: Unique identifier for the ChatLLM instance.\n+            model_type: Key for the ChatLLM implementation in LLMFactory.\n+            **chat_kwargs: Additional parameters for instantiation.\n+        \"\"\"\n+        chat_kwargs[\"name\"] = name\n+        self.chat_models[name] = ChatModelFactory().create(model_type, chat_kwargs)\n+        return self.chat_models[name]\n+\n+    def register_embedding(\n+        self, name: str, model_type: str, **embedding_kwargs: Any\n+    ) -> EmbeddingModel:\n+        \"\"\"\n+        Register an EmbeddingsLLM instance under a unique name.\n+\n+        Args:\n+            name: Unique identifier for the EmbeddingsLLM instance.\n+            embedding_key: Key for the EmbeddingsLLM implementation in LLMFactory.\n+            **embedding_kwargs: Additional parameters for instantiation.\n+        \"\"\"\n+        embedding_kwargs[\"name\"] = name\n+        self.embedding_models[name] = EmbeddingModelFactory().create(\n+            model_type, embedding_kwargs\n+        )\n+        return self.embedding_models[name]\n+\n+    def get_chat_model(self, name: str) -> ChatModel | None:\n+        \"\"\"\n+        Retrieve the ChatLLM instance registered under the given name.\n+\n+        Raises\n+        ------\n+            ValueError: If no ChatLLM is registered under the name.\n+        \"\"\"\n+        if name not in self.chat_models:\n+            msg = f\"No ChatLLM registered under the name '{name}'.\"\n+            raise ValueError(msg)\n+        return self.chat_models[name]\n+\n+    def get_embedding_model(self, name: str) -> EmbeddingModel | None:\n+        \"\"\"\n+        Retrieve the EmbeddingsLLM instance registered under the given name.\n+\n+        Raises\n+        ------\n+            ValueError: If no EmbeddingsLLM is registered under the name.\n+        \"\"\"\n+        if name not in self.embedding_models:\n+            msg = f\"No EmbeddingsLLM registered under the name '{name}'.\"\n+            raise ValueError(msg)\n+        return self.embedding_models[name]\n+\n+    def get_or_create_chat_model(\n+        self, name: str, model_type: str, **chat_kwargs: Any\n+    ) -> ChatModel:\n+        \"\"\"\n+        Retrieve the ChatLLM instance registered under the given name.\n+\n+        If the ChatLLM does not exist, it is created and registered.\n+\n+        Args:\n+            name: Unique identifier for the ChatLLM instance.\n+            model_type: Key for the ChatModel implementation in LLMFactory.\n+            **chat_kwargs: Additional parameters for instantiation.\n+        \"\"\"\n+        if name not in self.chat_models:\n+            return self.register_chat(name, model_type, **chat_kwargs)\n+        return self.chat_models[name]\n+\n+    def get_or_create_embedding_model(\n+        self, name: str, model_type: str, **embedding_kwargs: Any\n+    ) -> EmbeddingModel:\n+        \"\"\"\n+        Retrieve the EmbeddingsLLM instance registered under the given name.\n+\n+        If the EmbeddingsLLM does not exist, it is created and registered.\n+\n+        Args:\n+            name: Unique identifier for the EmbeddingsLLM instance.\n+            model_type: Key for the EmbeddingsLLM implementation in LLMFactory.\n+            **embedding_kwargs: Additional parameters for instantiation.\n+        \"\"\"\n+        if name not in self.embedding_models:\n+            return self.register_embedding(name, model_type, **embedding_kwargs)\n+        return self.embedding_models[name]\n+\n+    def remove_chat(self, name: str) -> None:\n+        \"\"\"Remove the ChatLLM instance registered under the given name.\"\"\"\n+        self.chat_models.pop(name, None)\n+\n+    def remove_embedding(self, name: str) -> None:\n+        \"\"\"Remove the EmbeddingsLLM instance registered under the given name.\"\"\"\n+        self.embedding_models.pop(name, None)\n+\n+    def list_chat_models(self) -> dict[str, ChatModel]:\n+        \"\"\"Return a copy of all registered ChatLLM instances.\"\"\"\n+        return dict(self.chat_models)\n+\n+    def list_embedding_models(self) -> dict[str, EmbeddingModel]:\n+        \"\"\"Return a copy of all registered EmbeddingsLLM instances.\"\"\"\n+        return dict(self.embedding_models)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/protocol/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/protocol/__init__.py b/packages/graphrag/graphrag/language_model/protocol/__init__.py\nnew file mode 100644\nindex 0000000..12432bd\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/protocol/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base protocol definitions for LLMs.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/protocol/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/protocol/base.py b/packages/graphrag/graphrag/language_model/protocol/base.py\nnew file mode 100644\nindex 0000000..74cd387\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/protocol/base.py\n@@ -0,0 +1,166 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base llm protocol definitions.\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Protocol\n+\n+if TYPE_CHECKING:\n+    from collections.abc import AsyncGenerator, Generator\n+\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+    from graphrag.language_model.response.base import ModelResponse\n+\n+\n+class EmbeddingModel(Protocol):\n+    \"\"\"\n+    Protocol for an embedding-based Language Model (LM).\n+\n+    This protocol defines the methods required for an embedding-based LM.\n+    \"\"\"\n+\n+    config: LanguageModelConfig\n+    \"\"\"Passthrough of the config used to create the model instance.\"\"\"\n+\n+    async def aembed_batch(\n+        self, text_list: list[str], **kwargs: Any\n+    ) -> list[list[float]]:\n+        \"\"\"\n+        Generate an embedding vector for the given list of strings.\n+\n+        Args:\n+            text: The text to generate an embedding for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A collections of list of floats representing the embedding vector for each item in the batch.\n+        \"\"\"\n+        ...\n+\n+    async def aembed(self, text: str, **kwargs: Any) -> list[float]:\n+        \"\"\"\n+        Generate an embedding vector for the given text.\n+\n+        Args:\n+            text: The text to generate an embedding for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A list of floats representing the embedding vector.\n+        \"\"\"\n+        ...\n+\n+    def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]:\n+        \"\"\"\n+        Generate an embedding vector for the given list of strings.\n+\n+        Args:\n+            text: The text to generate an embedding for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A collections of list of floats representing the embedding vector for each item in the batch.\n+        \"\"\"\n+        ...\n+\n+    def embed(self, text: str, **kwargs: Any) -> list[float]:\n+        \"\"\"\n+        Generate an embedding vector for the given text.\n+\n+        Args:\n+            text: The text to generate an embedding for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A list of floats representing the embedding vector.\n+        \"\"\"\n+        ...\n+\n+\n+class ChatModel(Protocol):\n+    \"\"\"\n+    Protocol for a chat-based Language Model (LM).\n+\n+    This protocol defines the methods required for a chat-based LM.\n+    Prompt is always required for the chat method, and any other keyword arguments are forwarded to the Model provider.\n+    \"\"\"\n+\n+    config: LanguageModelConfig\n+    \"\"\"Passthrough of the config used to create the model instance.\"\"\"\n+\n+    async def achat(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> ModelResponse:\n+        \"\"\"\n+        Generate a response for the given text.\n+\n+        Args:\n+            prompt: The text to generate a response for.\n+            history: The conversation history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A string representing the response.\n+\n+        \"\"\"\n+        ...\n+\n+    async def achat_stream(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"\n+        Generate a response for the given text using a streaming interface.\n+\n+        Args:\n+            prompt: The text to generate a response for.\n+            history: The conversation history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A generator that yields strings representing the response.\n+        \"\"\"\n+        yield \"\"  # Yield an empty string so that the function is recognized as a generator\n+        ...\n+\n+    def chat(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> ModelResponse:\n+        \"\"\"\n+        Generate a response for the given text.\n+\n+        Args:\n+            prompt: The text to generate a response for.\n+            history: The conversation history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A string representing the response.\n+\n+        \"\"\"\n+        ...\n+\n+    def chat_stream(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> Generator[str, None]:\n+        \"\"\"\n+        Generate a response for the given text using a streaming interface.\n+\n+        Args:\n+            prompt: The text to generate a response for.\n+            history: The conversation history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A generator that yields strings representing the response.\n+        \"\"\"\n+        ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/__init__.py b/packages/graphrag/graphrag/language_model/providers/__init__.py\nnew file mode 100644\nindex 0000000..d635f89\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Model Providers module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/__init__.py b/packages/graphrag/graphrag/language_model/providers/litellm/__init__.py\nnew file mode 100644\nindex 0000000..a1f948a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GraphRAG LiteLLM module. Provides LiteLLM-based implementations of chat and embedding models.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/chat_model.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/chat_model.py b/packages/graphrag/graphrag/language_model/providers/litellm/chat_model.py\nnew file mode 100644\nindex 0000000..bf4af8a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/chat_model.py\n@@ -0,0 +1,402 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Chat model implementation using Litellm.\"\"\"\n+\n+import inspect\n+import json\n+from collections.abc import AsyncGenerator, Generator\n+from typing import TYPE_CHECKING, Any, cast\n+\n+import litellm\n+from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n+from litellm import (\n+    CustomStreamWrapper,\n+    ModelResponse,  # type: ignore\n+    acompletion,\n+    completion,\n+)\n+from pydantic import BaseModel, Field\n+\n+from graphrag.config.defaults import COGNITIVE_SERVICES_AUDIENCE\n+from graphrag.config.enums import AuthType\n+from graphrag.language_model.providers.litellm.request_wrappers.with_cache import (\n+    with_cache,\n+)\n+from graphrag.language_model.providers.litellm.request_wrappers.with_logging import (\n+    with_logging,\n+)\n+from graphrag.language_model.providers.litellm.request_wrappers.with_rate_limiter import (\n+    with_rate_limiter,\n+)\n+from graphrag.language_model.providers.litellm.request_wrappers.with_retries import (\n+    with_retries,\n+)\n+from graphrag.language_model.providers.litellm.types import (\n+    AFixedModelCompletion,\n+    FixedModelCompletion,\n+)\n+\n+if TYPE_CHECKING:\n+    from graphrag.cache.pipeline_cache import PipelineCache\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+    from graphrag.language_model.response.base import ModelResponse as MR  # noqa: N817\n+\n+litellm.suppress_debug_info = True\n+\n+\n+def _create_base_completions(\n+    model_config: \"LanguageModelConfig\",\n+) -> tuple[FixedModelCompletion, AFixedModelCompletion]:\n+    \"\"\"Wrap the base litellm completion function with the model configuration.\n+\n+    Args\n+    ----\n+        model_config: The configuration for the language model.\n+\n+    Returns\n+    -------\n+        A tuple containing the synchronous and asynchronous completion functions.\n+    \"\"\"\n+    model_provider = model_config.model_provider\n+    model = model_config.deployment_name or model_config.model\n+\n+    base_args: dict[str, Any] = {\n+        \"drop_params\": True,  # LiteLLM drop unsupported params for selected model.\n+        \"model\": f\"{model_provider}/{model}\",\n+        \"timeout\": model_config.request_timeout,\n+        \"top_p\": model_config.top_p,\n+        \"n\": model_config.n,\n+        \"temperature\": model_config.temperature,\n+        \"frequency_penalty\": model_config.frequency_penalty,\n+        \"presence_penalty\": model_config.presence_penalty,\n+        \"api_base\": model_config.api_base,\n+        \"api_version\": model_config.api_version,\n+        \"api_key\": model_config.api_key,\n+        \"organization\": model_config.organization,\n+        \"proxy\": model_config.proxy,\n+        \"audience\": model_config.audience,\n+        \"max_tokens\": model_config.max_tokens,\n+        \"max_completion_tokens\": model_config.max_completion_tokens,\n+        \"reasoning_effort\": model_config.reasoning_effort,\n+    }\n+\n+    if model_config.auth_type == AuthType.AzureManagedIdentity:\n+        if model_config.model_provider != \"azure\":\n+            msg = \"Azure Managed Identity authentication is only supported for Azure models.\"\n+            raise ValueError(msg)\n+\n+        base_args[\"azure_scope\"] = base_args.pop(\"audience\")\n+        base_args[\"azure_ad_token_provider\"] = get_bearer_token_provider(\n+            DefaultAzureCredential(),\n+            model_config.audience or COGNITIVE_SERVICES_AUDIENCE,\n+        )\n+\n+    def _base_completion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper:\n+        new_args = {**base_args, **kwargs}\n+\n+        if \"name\" in new_args:\n+            new_args.pop(\"name\")\n+\n+        return completion(**new_args)\n+\n+    async def _base_acompletion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper:\n+        new_args = {**base_args, **kwargs}\n+\n+        if \"name\" in new_args:\n+            new_args.pop(\"name\")\n+\n+        return await acompletion(**new_args)\n+\n+    return (_base_completion, _base_acompletion)\n+\n+\n+def _create_completions(\n+    model_config: \"LanguageModelConfig\",\n+    cache: \"PipelineCache | None\",\n+    cache_key_prefix: str,\n+) -> tuple[FixedModelCompletion, AFixedModelCompletion]:\n+    \"\"\"Wrap the base litellm completion function with the model configuration and additional features.\n+\n+    Wrap the base litellm completion function with instance variables based on the model configuration.\n+    Then wrap additional features such as rate limiting, retries, and caching, if enabled.\n+\n+    Final function composition order:\n+    - Logging(Cache(Retries(RateLimiter(ModelCompletion()))))\n+\n+    Args\n+    ----\n+        model_config: The configuration for the language model.\n+        cache: Optional cache for storing responses.\n+        cache_key_prefix: Prefix for cache keys.\n+\n+    Returns\n+    -------\n+        A tuple containing the synchronous and asynchronous completion functions.\n+\n+    \"\"\"\n+    completion, acompletion = _create_base_completions(model_config)\n+\n+    if model_config.rate_limit_strategy is not None and (\n+        model_config.requests_per_minute is not None\n+        or model_config.tokens_per_minute is not None\n+    ):\n+        completion, acompletion = with_rate_limiter(\n+            sync_fn=completion,\n+            async_fn=acompletion,\n+            model_config=model_config,\n+            rpm=model_config.requests_per_minute,\n+            tpm=model_config.tokens_per_minute,\n+        )\n+\n+    if model_config.retry_strategy != \"none\":\n+        completion, acompletion = with_retries(\n+            sync_fn=completion,\n+            async_fn=acompletion,\n+            model_config=model_config,\n+        )\n+\n+    if cache is not None:\n+        completion, acompletion = with_cache(\n+            sync_fn=completion,\n+            async_fn=acompletion,\n+            model_config=model_config,\n+            cache=cache,\n+            request_type=\"chat\",\n+            cache_key_prefix=cache_key_prefix,\n+        )\n+\n+    completion, acompletion = with_logging(\n+        sync_fn=completion,\n+        async_fn=acompletion,\n+    )\n+\n+    return (completion, acompletion)\n+\n+\n+class LitellmModelOutput(BaseModel):\n+    \"\"\"A model representing the output from a language model.\"\"\"\n+\n+    content: str = Field(description=\"The generated text content\")\n+    full_response: None = Field(\n+        default=None, description=\"The full response from the model, if available\"\n+    )\n+\n+\n+class LitellmModelResponse(BaseModel):\n+    \"\"\"A model representing the response from a language model.\"\"\"\n+\n+    output: LitellmModelOutput = Field(description=\"The output from the model\")\n+    parsed_response: BaseModel | None = Field(\n+        default=None, description=\"Parsed response from the model\"\n+    )\n+    history: list = Field(\n+        default_factory=list,\n+        description=\"Conversation history including the prompt and response\",\n+    )\n+\n+\n+class LitellmChatModel:\n+    \"\"\"LiteLLM-based Chat Model.\"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        config: \"LanguageModelConfig\",\n+        cache: \"PipelineCache | None\" = None,\n+        **kwargs: Any,\n+    ):\n+        self.name = name\n+        self.config = config\n+        self.cache = cache.child(self.name) if cache else None\n+        self.completion, self.acompletion = _create_completions(\n+            config, self.cache, \"chat\"\n+        )\n+\n+    def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]:\n+        \"\"\"Get model arguments supported by litellm.\"\"\"\n+        args_to_include = [\n+            \"name\",\n+            \"modalities\",\n+            \"prediction\",\n+            \"audio\",\n+            \"logit_bias\",\n+            \"metadata\",\n+            \"user\",\n+            \"response_format\",\n+            \"seed\",\n+            \"tools\",\n+            \"tool_choice\",\n+            \"logprobs\",\n+            \"top_logprobs\",\n+            \"parallel_tool_calls\",\n+            \"web_search_options\",\n+            \"extra_headers\",\n+            \"functions\",\n+            \"function_call\",\n+            \"thinking\",\n+        ]\n+        new_args = {k: v for k, v in kwargs.items() if k in args_to_include}\n+\n+        # If using JSON, check if response_format should be a Pydantic model or just a general JSON object\n+        if kwargs.get(\"json\"):\n+            new_args[\"response_format\"] = {\"type\": \"json_object\"}\n+\n+            if (\n+                \"json_model\" in kwargs\n+                and inspect.isclass(kwargs[\"json_model\"])\n+                and issubclass(kwargs[\"json_model\"], BaseModel)\n+            ):\n+                new_args[\"response_format\"] = kwargs[\"json_model\"]\n+\n+        return new_args\n+\n+    async def achat(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> \"MR\":\n+        \"\"\"\n+        Generate a response for the given prompt and history.\n+\n+        Args\n+        ----\n+            prompt: The prompt to generate a response for.\n+            history: Optional chat history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            LitellmModelResponse: The generated model response.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        messages: list[dict[str, str]] = history or []\n+        messages.append({\"role\": \"user\", \"content\": prompt})\n+\n+        response = await self.acompletion(messages=messages, stream=False, **new_kwargs)  # type: ignore\n+\n+        messages.append({\n+            \"role\": \"assistant\",\n+            \"content\": response.choices[0].message.content or \"\",  # type: ignore\n+        })\n+\n+        parsed_response: BaseModel | None = None\n+        if \"response_format\" in new_kwargs:\n+            parsed_dict: dict[str, Any] = json.loads(\n+                response.choices[0].message.content or \"{}\"  # type: ignore\n+            )\n+            parsed_response = parsed_dict  # type: ignore\n+            if inspect.isclass(new_kwargs[\"response_format\"]) and issubclass(\n+                new_kwargs[\"response_format\"], BaseModel\n+            ):\n+                # If response_format is a pydantic model, instantiate it\n+                model_initializer = cast(\n+                    \"type[BaseModel]\", new_kwargs[\"response_format\"]\n+                )\n+                parsed_response = model_initializer(**parsed_dict)\n+\n+        return LitellmModelResponse(\n+            output=LitellmModelOutput(\n+                content=response.choices[0].message.content or \"\"  # type: ignore\n+            ),\n+            parsed_response=parsed_response,\n+            history=messages,\n+        )\n+\n+    async def achat_stream(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"\n+        Generate a response for the given prompt and history.\n+\n+        Args\n+        ----\n+            prompt: The prompt to generate a response for.\n+            history: Optional chat history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            AsyncGenerator[str, None]: The generated response as a stream of strings.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        messages: list[dict[str, str]] = history or []\n+        messages.append({\"role\": \"user\", \"content\": prompt})\n+\n+        response = await self.acompletion(messages=messages, stream=True, **new_kwargs)  # type: ignore\n+\n+        async for chunk in response:  # type: ignore\n+            if chunk.choices and chunk.choices[0].delta.content:\n+                yield chunk.choices[0].delta.content\n+\n+    def chat(self, prompt: str, history: list | None = None, **kwargs: Any) -> \"MR\":\n+        \"\"\"\n+        Generate a response for the given prompt and history.\n+\n+        Args\n+        ----\n+            prompt: The prompt to generate a response for.\n+            history: Optional chat history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            LitellmModelResponse: The generated model response.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        messages: list[dict[str, str]] = history or []\n+        messages.append({\"role\": \"user\", \"content\": prompt})\n+\n+        response = self.completion(messages=messages, stream=False, **new_kwargs)  # type: ignore\n+\n+        messages.append({\n+            \"role\": \"assistant\",\n+            \"content\": response.choices[0].message.content or \"\",  # type: ignore\n+        })\n+\n+        parsed_response: BaseModel | None = None\n+        if \"response_format\" in new_kwargs:\n+            parsed_dict: dict[str, Any] = json.loads(\n+                response.choices[0].message.content or \"{}\"  # type: ignore\n+            )\n+            parsed_response = parsed_dict  # type: ignore\n+            if inspect.isclass(new_kwargs[\"response_format\"]) and issubclass(\n+                new_kwargs[\"response_format\"], BaseModel\n+            ):\n+                # If response_format is a pydantic model, instantiate it\n+                model_initializer = cast(\n+                    \"type[BaseModel]\", new_kwargs[\"response_format\"]\n+                )\n+                parsed_response = model_initializer(**parsed_dict)\n+\n+        return LitellmModelResponse(\n+            output=LitellmModelOutput(\n+                content=response.choices[0].message.content or \"\"  # type: ignore\n+            ),\n+            parsed_response=parsed_response,\n+            history=messages,\n+        )\n+\n+    def chat_stream(\n+        self, prompt: str, history: list | None = None, **kwargs: Any\n+    ) -> Generator[str, None]:\n+        \"\"\"\n+        Generate a response for the given prompt and history.\n+\n+        Args\n+        ----\n+            prompt: The prompt to generate a response for.\n+            history: Optional chat history.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            Generator[str, None]: The generated response as a stream of strings.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        messages: list[dict[str, str]] = history or []\n+        messages.append({\"role\": \"user\", \"content\": prompt})\n+\n+        response = self.completion(messages=messages, stream=True, **new_kwargs)  # type: ignore\n+\n+        for chunk in response:\n+            if chunk.choices and chunk.choices[0].delta.content:  # type: ignore\n+                yield chunk.choices[0].delta.content  # type: ignore\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/embedding_model.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/embedding_model.py b/packages/graphrag/graphrag/language_model/providers/litellm/embedding_model.py\nnew file mode 100644\nindex 0000000..17eea1e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/embedding_model.py\n@@ -0,0 +1,268 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Embedding model implementation using Litellm.\"\"\"\n+\n+from typing import TYPE_CHECKING, Any\n+\n+import litellm\n+from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n+from litellm import (\n+    EmbeddingResponse,  # type: ignore\n+    aembedding,\n+    embedding,\n+)\n+\n+from graphrag.config.defaults import COGNITIVE_SERVICES_AUDIENCE\n+from graphrag.config.enums import AuthType\n+from graphrag.language_model.providers.litellm.request_wrappers.with_cache import (\n+    with_cache,\n+)\n+from graphrag.language_model.providers.litellm.request_wrappers.with_logging import (\n+    with_logging,\n+)\n+from graphrag.language_model.providers.litellm.request_wrappers.with_rate_limiter import (\n+    with_rate_limiter,\n+)\n+from graphrag.language_model.providers.litellm.request_wrappers.with_retries import (\n+    with_retries,\n+)\n+from graphrag.language_model.providers.litellm.types import (\n+    AFixedModelEmbedding,\n+    FixedModelEmbedding,\n+)\n+\n+if TYPE_CHECKING:\n+    from graphrag.cache.pipeline_cache import PipelineCache\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+\n+litellm.suppress_debug_info = True\n+\n+\n+def _create_base_embeddings(\n+    model_config: \"LanguageModelConfig\",\n+) -> tuple[FixedModelEmbedding, AFixedModelEmbedding]:\n+    \"\"\"Wrap the base litellm embedding function with the model configuration.\n+\n+    Args\n+    ----\n+        model_config: The configuration for the language model.\n+\n+    Returns\n+    -------\n+        A tuple containing the synchronous and asynchronous embedding functions.\n+    \"\"\"\n+    model_provider = model_config.model_provider\n+    model = model_config.deployment_name or model_config.model\n+\n+    base_args: dict[str, Any] = {\n+        \"drop_params\": True,  # LiteLLM drop unsupported params for selected model.\n+        \"model\": f\"{model_provider}/{model}\",\n+        \"timeout\": model_config.request_timeout,\n+        \"api_base\": model_config.api_base,\n+        \"api_version\": model_config.api_version,\n+        \"api_key\": model_config.api_key,\n+        \"organization\": model_config.organization,\n+        \"proxy\": model_config.proxy,\n+        \"audience\": model_config.audience,\n+    }\n+\n+    if model_config.auth_type == AuthType.AzureManagedIdentity:\n+        if model_config.model_provider != \"azure\":\n+            msg = \"Azure Managed Identity authentication is only supported for Azure models.\"\n+            raise ValueError(msg)\n+\n+        base_args[\"azure_scope\"] = base_args.pop(\"audience\")\n+        base_args[\"azure_ad_token_provider\"] = get_bearer_token_provider(\n+            DefaultAzureCredential(),\n+            model_config.audience or COGNITIVE_SERVICES_AUDIENCE,\n+        )\n+\n+    def _base_embedding(**kwargs: Any) -> EmbeddingResponse:\n+        new_args = {**base_args, **kwargs}\n+\n+        if \"name\" in new_args:\n+            new_args.pop(\"name\")\n+\n+        return embedding(**new_args)\n+\n+    async def _base_aembedding(**kwargs: Any) -> EmbeddingResponse:\n+        new_args = {**base_args, **kwargs}\n+\n+        if \"name\" in new_args:\n+            new_args.pop(\"name\")\n+\n+        return await aembedding(**new_args)\n+\n+    return (_base_embedding, _base_aembedding)\n+\n+\n+def _create_embeddings(\n+    model_config: \"LanguageModelConfig\",\n+    cache: \"PipelineCache | None\",\n+    cache_key_prefix: str,\n+) -> tuple[FixedModelEmbedding, AFixedModelEmbedding]:\n+    \"\"\"Wrap the base litellm embedding function with the model configuration and additional features.\n+\n+    Wrap the base litellm embedding function with instance variables based on the model configuration.\n+    Then wrap additional features such as rate limiting, retries, and caching, if enabled.\n+\n+    Final function composition order:\n+    - Logging(Cache(Retries(RateLimiter(ModelEmbedding()))))\n+\n+    Args\n+    ----\n+        model_config: The configuration for the language model.\n+        cache: Optional cache for storing responses.\n+        cache_key_prefix: Prefix for cache keys.\n+\n+    Returns\n+    -------\n+        A tuple containing the synchronous and asynchronous embedding functions.\n+\n+    \"\"\"\n+    embedding, aembedding = _create_base_embeddings(model_config)\n+\n+    if model_config.rate_limit_strategy is not None and (\n+        model_config.requests_per_minute is not None\n+        or model_config.tokens_per_minute is not None\n+    ):\n+        embedding, aembedding = with_rate_limiter(\n+            sync_fn=embedding,\n+            async_fn=aembedding,\n+            model_config=model_config,\n+            rpm=model_config.requests_per_minute,\n+            tpm=model_config.tokens_per_minute,\n+        )\n+\n+    if model_config.retry_strategy != \"none\":\n+        embedding, aembedding = with_retries(\n+            sync_fn=embedding,\n+            async_fn=aembedding,\n+            model_config=model_config,\n+        )\n+\n+    if cache is not None:\n+        embedding, aembedding = with_cache(\n+            sync_fn=embedding,\n+            async_fn=aembedding,\n+            model_config=model_config,\n+            cache=cache,\n+            request_type=\"embedding\",\n+            cache_key_prefix=cache_key_prefix,\n+        )\n+\n+    embedding, aembedding = with_logging(\n+        sync_fn=embedding,\n+        async_fn=aembedding,\n+    )\n+\n+    return (embedding, aembedding)\n+\n+\n+class LitellmEmbeddingModel:\n+    \"\"\"LiteLLM-based Embedding Model.\"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        config: \"LanguageModelConfig\",\n+        cache: \"PipelineCache | None\" = None,\n+        **kwargs: Any,\n+    ):\n+        self.name = name\n+        self.config = config\n+        self.cache = cache.child(self.name) if cache else None\n+        self.embedding, self.aembedding = _create_embeddings(\n+            config, self.cache, \"embeddings\"\n+        )\n+\n+    def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]:\n+        \"\"\"Get model arguments supported by litellm.\"\"\"\n+        args_to_include = [\n+            \"name\",\n+            \"dimensions\",\n+            \"encoding_format\",\n+            \"timeout\",\n+            \"user\",\n+        ]\n+        return {k: v for k, v in kwargs.items() if k in args_to_include}\n+\n+    async def aembed_batch(\n+        self, text_list: list[str], **kwargs: Any\n+    ) -> list[list[float]]:\n+        \"\"\"\n+        Batch generate embeddings.\n+\n+        Args\n+        ----\n+            text_list: A batch of text inputs to generate embeddings for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A Batch of embeddings.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        response = await self.aembedding(input=text_list, **new_kwargs)\n+\n+        return [emb.get(\"embedding\", []) for emb in response.data]\n+\n+    async def aembed(self, text: str, **kwargs: Any) -> list[float]:\n+        \"\"\"\n+        Async embed.\n+\n+        Args:\n+            text: The text to generate an embedding for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            An embedding.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        response = await self.aembedding(input=[text], **new_kwargs)\n+\n+        return (\n+            response.data[0].get(\"embedding\", [])\n+            if response.data and response.data[0]\n+            else []\n+        )\n+\n+    def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]:\n+        \"\"\"\n+        Batch generate embeddings.\n+\n+        Args:\n+            text_list: A batch of text inputs to generate embeddings for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            A Batch of embeddings.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        response = self.embedding(input=text_list, **new_kwargs)\n+\n+        return [emb.get(\"embedding\", []) for emb in response.data]\n+\n+    def embed(self, text: str, **kwargs: Any) -> list[float]:\n+        \"\"\"\n+        Embed a single text input.\n+\n+        Args:\n+            text: The text to generate an embedding for.\n+            **kwargs: Additional keyword arguments (e.g., model parameters).\n+\n+        Returns\n+        -------\n+            An embedding.\n+        \"\"\"\n+        new_kwargs = self._get_kwargs(**kwargs)\n+        response = self.embedding(input=[text], **new_kwargs)\n+\n+        return (\n+            response.data[0].get(\"embedding\", [])\n+            if response.data and response.data[0]\n+            else []\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/get_cache_key.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/get_cache_key.py b/packages/graphrag/graphrag/language_model/providers/litellm/get_cache_key.py\nnew file mode 100644\nindex 0000000..0d6938d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/get_cache_key.py\n@@ -0,0 +1,140 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"\n+LiteLLM cache key generation.\n+\n+Modeled after the fnllm cache key generation.\n+https://github.com/microsoft/essex-toolkit/blob/23d3077b65c0e8f1d89c397a2968fe570a25f790/python/fnllm/fnllm/caching/base.py#L50\n+\"\"\"\n+\n+import hashlib\n+import inspect\n+import json\n+from typing import TYPE_CHECKING, Any\n+\n+from pydantic import BaseModel\n+\n+if TYPE_CHECKING:\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+\n+\n+_CACHE_VERSION = 3\n+\"\"\"\n+If there's a breaking change in what we cache, we should increment this version number to invalidate existing caches.\n+\n+fnllm was on cache version 2 and though we generate\n+similar cache keys, the objects stored in cache by fnllm and litellm are different.\n+Using litellm model providers will not be able to reuse caches generated by fnllm\n+thus we start with version 3 for litellm.\n+\"\"\"\n+\n+\n+def get_cache_key(\n+    model_config: \"LanguageModelConfig\",\n+    prefix: str,\n+    messages: str | None = None,\n+    input: str | None = None,\n+    **kwargs: Any,\n+) -> str:\n+    \"\"\"Generate a cache key based on the model configuration and input arguments.\n+\n+    Modeled after the fnllm cache key generation.\n+    https://github.com/microsoft/essex-toolkit/blob/23d3077b65c0e8f1d89c397a2968fe570a25f790/python/fnllm/fnllm/caching/base.py#L50\n+\n+    Args\n+    ____\n+        model_config: The configuration of the language model.\n+        prefix: A prefix for the cache key.\n+        **kwargs: Additional model input parameters.\n+\n+    Returns\n+    -------\n+        `{prefix}_{data_hash}_v{version}` if prefix is provided.\n+    \"\"\"\n+    cache_key: dict[str, Any] = {\n+        \"parameters\": _get_parameters(model_config, **kwargs),\n+    }\n+\n+    if messages is not None and input is not None:\n+        msg = \"Only one of 'messages' or 'input' should be provided.\"\n+        raise ValueError(msg)\n+\n+    if messages is not None:\n+        cache_key[\"messages\"] = messages\n+    elif input is not None:\n+        cache_key[\"input\"] = input\n+    else:\n+        msg = \"Either 'messages' or 'input' must be provided.\"\n+        raise ValueError(msg)\n+\n+    data_hash = _hash(json.dumps(cache_key, sort_keys=True))\n+\n+    name = kwargs.get(\"name\")\n+\n+    if name:\n+        prefix += f\"_{name}\"\n+\n+    return f\"{prefix}_{data_hash}_v{_CACHE_VERSION}\"\n+\n+\n+def _get_parameters(\n+    model_config: \"LanguageModelConfig\",\n+    **kwargs: Any,\n+) -> dict[str, Any]:\n+    \"\"\"Pluck out the parameters that define a cache key.\n+\n+    Use the same parameters as fnllm except request timeout.\n+    - embeddings: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/embeddings/parameters.py#L12\n+    - chat: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/chat/parameters.py#L25\n+\n+    Args\n+    ____\n+        model_config: The configuration of the language model.\n+        **kwargs: Additional model input parameters.\n+\n+    Returns\n+    -------\n+        dict[str, Any]: A dictionary of parameters that define the cache key.\n+    \"\"\"\n+    parameters = {\n+        \"model\": model_config.deployment_name or model_config.model,\n+        \"frequency_penalty\": model_config.frequency_penalty,\n+        \"max_tokens\": model_config.max_tokens,\n+        \"max_completion_tokens\": model_config.max_completion_tokens,\n+        \"n\": model_config.n,\n+        \"presence_penalty\": model_config.presence_penalty,\n+        \"temperature\": model_config.temperature,\n+        \"top_p\": model_config.top_p,\n+        \"reasoning_effort\": model_config.reasoning_effort,\n+    }\n+    keys_to_cache = [\n+        \"function_call\",\n+        \"functions\",\n+        \"logit_bias\",\n+        \"logprobs\",\n+        \"parallel_tool_calls\",\n+        \"seed\",\n+        \"service_tier\",\n+        \"stop\",\n+        \"tool_choice\",\n+        \"tools\",\n+        \"top_logprobs\",\n+        \"user\",\n+        \"dimensions\",\n+        \"encoding_format\",\n+    ]\n+    parameters.update({key: kwargs.get(key) for key in keys_to_cache if key in kwargs})\n+\n+    response_format = kwargs.get(\"response_format\")\n+    if inspect.isclass(response_format) and issubclass(response_format, BaseModel):\n+        parameters[\"response_format\"] = str(response_format)\n+    elif response_format is not None:\n+        parameters[\"response_format\"] = response_format\n+\n+    return parameters\n+\n+\n+def _hash(input: str) -> str:\n+    \"\"\"Generate a hash for the input string.\"\"\"\n+    return hashlib.sha256(input.encode()).hexdigest()\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/__init__.py b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/__init__.py\nnew file mode 100644\nindex 0000000..b1ba631\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM completion/embedding function wrappers.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_cache.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_cache.py b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_cache.py\nnew file mode 100644\nindex 0000000..d14c972\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_cache.py\n@@ -0,0 +1,107 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM completion/embedding cache wrapper.\"\"\"\n+\n+import asyncio\n+from typing import TYPE_CHECKING, Any, Literal\n+\n+from litellm import EmbeddingResponse, ModelResponse  # type: ignore\n+\n+from graphrag.language_model.providers.litellm.get_cache_key import get_cache_key\n+from graphrag.language_model.providers.litellm.types import (\n+    AsyncLitellmRequestFunc,\n+    LitellmRequestFunc,\n+)\n+\n+if TYPE_CHECKING:\n+    from graphrag.cache.pipeline_cache import PipelineCache\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+\n+\n+def with_cache(\n+    *,\n+    sync_fn: LitellmRequestFunc,\n+    async_fn: AsyncLitellmRequestFunc,\n+    model_config: \"LanguageModelConfig\",\n+    cache: \"PipelineCache\",\n+    request_type: Literal[\"chat\", \"embedding\"],\n+    cache_key_prefix: str,\n+) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n+    \"\"\"\n+    Wrap the synchronous and asynchronous request functions with caching.\n+\n+    Args\n+    ----\n+        sync_fn: The synchronous chat/embedding request function to wrap.\n+        async_fn: The asynchronous chat/embedding request function to wrap.\n+        model_config: The configuration for the language model.\n+        cache: The cache to use for storing responses.\n+        request_type: The type of request being made, either \"chat\" or \"embedding\".\n+        cache_key_prefix: The prefix to use for cache keys.\n+\n+    Returns\n+    -------\n+        A tuple containing the wrapped synchronous and asynchronous chat/embedding request functions.\n+    \"\"\"\n+\n+    def _wrapped_with_cache(**kwargs: Any) -> Any:\n+        is_streaming = kwargs.get(\"stream\", False)\n+        if is_streaming:\n+            return sync_fn(**kwargs)\n+        cache_key = get_cache_key(\n+            model_config=model_config, prefix=cache_key_prefix, **kwargs\n+        )\n+        event_loop = asyncio.get_event_loop()\n+        cached_response = event_loop.run_until_complete(cache.get(cache_key))\n+        if (\n+            cached_response is not None\n+            and isinstance(cached_response, dict)\n+            and \"response\" in cached_response\n+            and cached_response[\"response\"] is not None\n+            and isinstance(cached_response[\"response\"], dict)\n+        ):\n+            try:\n+                if request_type == \"chat\":\n+                    return ModelResponse(**cached_response[\"response\"])\n+                return EmbeddingResponse(**cached_response[\"response\"])\n+            except Exception:  # noqa: BLE001\n+                # Try to retrieve value from cache but if it fails, continue\n+                # to make the request.\n+                ...\n+        response = sync_fn(**kwargs)\n+        event_loop.run_until_complete(\n+            cache.set(cache_key, {\"response\": response.model_dump()})\n+        )\n+        return response\n+\n+    async def _wrapped_with_cache_async(\n+        **kwargs: Any,\n+    ) -> Any:\n+        is_streaming = kwargs.get(\"stream\", False)\n+        if is_streaming:\n+            return await async_fn(**kwargs)\n+        cache_key = get_cache_key(\n+            model_config=model_config, prefix=cache_key_prefix, **kwargs\n+        )\n+        cached_response = await cache.get(cache_key)\n+        if (\n+            cached_response is not None\n+            and isinstance(cached_response, dict)\n+            and \"response\" in cached_response\n+            and cached_response[\"response\"] is not None\n+            and isinstance(cached_response[\"response\"], dict)\n+        ):\n+            try:\n+                if request_type == \"chat\":\n+                    return ModelResponse(**cached_response[\"response\"])\n+                return EmbeddingResponse(**cached_response[\"response\"])\n+            except Exception:  # noqa: BLE001\n+                # Try to retrieve value from cache but if it fails, continue\n+                # to make the request.\n+                ...\n+        response = await async_fn(**kwargs)\n+        await cache.set(cache_key, {\"response\": response.model_dump()})\n+        return response\n+\n+    return (_wrapped_with_cache, _wrapped_with_cache_async)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_logging.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_logging.py b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_logging.py\nnew file mode 100644\nindex 0000000..a353f45\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_logging.py\n@@ -0,0 +1,56 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM completion/embedding logging wrapper.\"\"\"\n+\n+import logging\n+from typing import Any\n+\n+from graphrag.language_model.providers.litellm.types import (\n+    AsyncLitellmRequestFunc,\n+    LitellmRequestFunc,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def with_logging(\n+    *,\n+    sync_fn: LitellmRequestFunc,\n+    async_fn: AsyncLitellmRequestFunc,\n+) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n+    \"\"\"\n+    Wrap the synchronous and asynchronous request functions with retries.\n+\n+    Args\n+    ----\n+        sync_fn: The synchronous chat/embedding request function to wrap.\n+        async_fn: The asynchronous chat/embedding request function to wrap.\n+        model_config: The configuration for the language model.\n+\n+    Returns\n+    -------\n+        A tuple containing the wrapped synchronous and asynchronous chat/embedding request functions.\n+    \"\"\"\n+\n+    def _wrapped_with_logging(**kwargs: Any) -> Any:\n+        try:\n+            return sync_fn(**kwargs)\n+        except Exception as e:\n+            logger.exception(\n+                f\"with_logging: Request failed with exception={e}\",  # noqa: G004, TRY401\n+            )\n+            raise\n+\n+    async def _wrapped_with_logging_async(\n+        **kwargs: Any,\n+    ) -> Any:\n+        try:\n+            return await async_fn(**kwargs)\n+        except Exception as e:\n+            logger.exception(\n+                f\"with_logging: Async request failed with exception={e}\",  # noqa: G004, TRY401\n+            )\n+            raise\n+\n+    return (_wrapped_with_logging, _wrapped_with_logging_async)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py\nnew file mode 100644\nindex 0000000..1083694\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py\n@@ -0,0 +1,97 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM completion/embedding rate limiter wrapper.\"\"\"\n+\n+from typing import TYPE_CHECKING, Any\n+\n+from litellm import token_counter  # type: ignore\n+\n+from graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter_factory import (\n+    RateLimiterFactory,\n+)\n+from graphrag.language_model.providers.litellm.types import (\n+    AsyncLitellmRequestFunc,\n+    LitellmRequestFunc,\n+)\n+\n+if TYPE_CHECKING:\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+\n+\n+def with_rate_limiter(\n+    *,\n+    sync_fn: LitellmRequestFunc,\n+    async_fn: AsyncLitellmRequestFunc,\n+    model_config: \"LanguageModelConfig\",\n+    rpm: int | None = None,\n+    tpm: int | None = None,\n+) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n+    \"\"\"\n+    Wrap the synchronous and asynchronous request functions with rate limiting.\n+\n+    Args\n+    ----\n+        sync_fn: The synchronous chat/embedding request function to wrap.\n+        async_fn: The asynchronous chat/embedding request function to wrap.\n+        model_config: The configuration for the language model.\n+        processing_event: A threading event that can be used to pause the rate limiter.\n+        rpm: An optional requests per minute limit.\n+        tpm: An optional tokens per minute limit.\n+\n+    If `rpm` and `tpm` is set to 0 or None, rate limiting is disabled.\n+\n+    Returns\n+    -------\n+        A tuple containing the wrapped synchronous and asynchronous chat/embedding request functions.\n+    \"\"\"\n+    rate_limiter_factory = RateLimiterFactory()\n+\n+    if (\n+        model_config.rate_limit_strategy is None\n+        or model_config.rate_limit_strategy not in rate_limiter_factory\n+    ):\n+        msg = f\"Rate Limiter strategy '{model_config.rate_limit_strategy}' is none or not registered. Available strategies: {', '.join(rate_limiter_factory.keys())}\"\n+        raise ValueError(msg)\n+\n+    rate_limiter_service = rate_limiter_factory.create(\n+        strategy=model_config.rate_limit_strategy, init_args={\"rpm\": rpm, \"tpm\": tpm}\n+    )\n+\n+    max_tokens = model_config.max_completion_tokens or model_config.max_tokens or 0\n+\n+    def _wrapped_with_rate_limiter(**kwargs: Any) -> Any:\n+        token_count = max_tokens\n+        if \"messages\" in kwargs:\n+            token_count += token_counter(\n+                model=model_config.model,\n+                messages=kwargs[\"messages\"],\n+            )\n+        elif \"input\" in kwargs:\n+            token_count += token_counter(\n+                model=model_config.model,\n+                text=kwargs[\"input\"],\n+            )\n+\n+        with rate_limiter_service.acquire(token_count=token_count):\n+            return sync_fn(**kwargs)\n+\n+    async def _wrapped_with_rate_limiter_async(\n+        **kwargs: Any,\n+    ) -> Any:\n+        token_count = max_tokens\n+        if \"messages\" in kwargs:\n+            token_count += token_counter(\n+                model=model_config.model,\n+                messages=kwargs[\"messages\"],\n+            )\n+        elif \"input\" in kwargs:\n+            token_count += token_counter(\n+                model=model_config.model,\n+                text=kwargs[\"input\"],\n+            )\n+\n+        with rate_limiter_service.acquire(token_count=token_count):\n+            return await async_fn(**kwargs)\n+\n+    return (_wrapped_with_rate_limiter, _wrapped_with_rate_limiter_async)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_retries.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_retries.py b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_retries.py\nnew file mode 100644\nindex 0000000..53e13f3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/request_wrappers/with_retries.py\n@@ -0,0 +1,56 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM completion/embedding retries wrapper.\"\"\"\n+\n+from typing import TYPE_CHECKING, Any\n+\n+from graphrag.language_model.providers.litellm.services.retry.retry_factory import (\n+    RetryFactory,\n+)\n+from graphrag.language_model.providers.litellm.types import (\n+    AsyncLitellmRequestFunc,\n+    LitellmRequestFunc,\n+)\n+\n+if TYPE_CHECKING:\n+    from graphrag.config.models.language_model_config import LanguageModelConfig\n+\n+\n+def with_retries(\n+    *,\n+    sync_fn: LitellmRequestFunc,\n+    async_fn: AsyncLitellmRequestFunc,\n+    model_config: \"LanguageModelConfig\",\n+) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n+    \"\"\"\n+    Wrap the synchronous and asynchronous request functions with retries.\n+\n+    Args\n+    ----\n+        sync_fn: The synchronous chat/embedding request function to wrap.\n+        async_fn: The asynchronous chat/embedding request function to wrap.\n+        model_config: The configuration for the language model.\n+\n+    Returns\n+    -------\n+        A tuple containing the wrapped synchronous and asynchronous chat/embedding request functions.\n+    \"\"\"\n+    retry_factory = RetryFactory()\n+    retry_service = retry_factory.create(\n+        strategy=model_config.retry_strategy,\n+        init_args={\n+            \"max_retries\": model_config.max_retries,\n+            \"max_retry_wait\": model_config.max_retry_wait,\n+        },\n+    )\n+\n+    def _wrapped_with_retries(**kwargs: Any) -> Any:\n+        return retry_service.retry(func=sync_fn, **kwargs)\n+\n+    async def _wrapped_with_retries_async(\n+        **kwargs: Any,\n+    ) -> Any:\n+        return await retry_service.aretry(func=async_fn, **kwargs)\n+\n+    return (_wrapped_with_retries, _wrapped_with_retries_async)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/__init__.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/__init__.py\nnew file mode 100644\nindex 0000000..dd0cf9f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Services.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/__init__.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/__init__.py\nnew file mode 100644\nindex 0000000..3c80d9f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Rate Limiter.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py\nnew file mode 100644\nindex 0000000..24a01a4\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py\n@@ -0,0 +1,37 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Rate Limiter.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from collections.abc import Iterator\n+from contextlib import contextmanager\n+from typing import Any\n+\n+\n+class RateLimiter(ABC):\n+    \"\"\"Abstract base class for rate limiters.\"\"\"\n+\n+    @abstractmethod\n+    def __init__(\n+        self,\n+        /,\n+        **kwargs: Any,\n+    ) -> None: ...\n+\n+    @abstractmethod\n+    @contextmanager\n+    def acquire(self, *, token_count: int) -> Iterator[None]:\n+        \"\"\"\n+        Acquire Rate Limiter.\n+\n+        Args\n+        ----\n+            token_count: The estimated number of tokens for the current request.\n+\n+        Yields\n+        ------\n+            None: This context manager does not return any value.\n+        \"\"\"\n+        msg = \"RateLimiter subclasses must implement the acquire method.\"\n+        raise NotImplementedError(msg)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py\nnew file mode 100644\nindex 0000000..5904be0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py\n@@ -0,0 +1,20 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Rate Limiter Factory.\"\"\"\n+\n+from graphrag.factory.factory import Factory\n+from graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter import (\n+    RateLimiter,\n+)\n+from graphrag.language_model.providers.litellm.services.rate_limiter.static_rate_limiter import (\n+    StaticRateLimiter,\n+)\n+\n+\n+class RateLimiterFactory(Factory[RateLimiter]):\n+    \"\"\"Singleton factory for creating rate limiter services.\"\"\"\n+\n+\n+rate_limiter_factory = RateLimiterFactory()\n+rate_limiter_factory.register(\"static\", StaticRateLimiter)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py\nnew file mode 100644\nindex 0000000..43681ce\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py\n@@ -0,0 +1,133 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Static Rate Limiter.\"\"\"\n+\n+import threading\n+import time\n+from collections import deque\n+from collections.abc import Iterator\n+from contextlib import contextmanager\n+from typing import Any\n+\n+from graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter import (\n+    RateLimiter,\n+)\n+\n+\n+class StaticRateLimiter(RateLimiter):\n+    \"\"\"Static Rate Limiter implementation.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        rpm: int | None = None,\n+        tpm: int | None = None,\n+        default_stagger: float = 0.0,\n+        period_in_seconds: int = 60,\n+        **kwargs: Any,\n+    ):\n+        if rpm is None and tpm is None:\n+            msg = \"Both TPM and RPM cannot be None (disabled), one or both must be set to a positive integer.\"\n+            raise ValueError(msg)\n+        if (rpm is not None and rpm <= 0) or (tpm is not None and tpm <= 0):\n+            msg = \"RPM and TPM must be either None (disabled) or positive integers.\"\n+            raise ValueError(msg)\n+        if default_stagger < 0:\n+            msg = \"Default stagger must be a >= 0.\"\n+            raise ValueError(msg)\n+        if period_in_seconds <= 0:\n+            msg = \"Period in seconds must be a positive integer.\"\n+            raise ValueError(msg)\n+        self.rpm = rpm\n+        self.tpm = tpm\n+        self._lock = threading.Lock()\n+        self.rate_queue: deque[float] = deque()\n+        self.token_queue: deque[int] = deque()\n+        self.period_in_seconds = period_in_seconds\n+        self._last_time: float | None = None\n+\n+        self.stagger = default_stagger\n+        if self.rpm is not None and self.rpm > 0:\n+            self.stagger = self.period_in_seconds / self.rpm\n+\n+    @contextmanager\n+    def acquire(self, *, token_count: int) -> Iterator[None]:\n+        \"\"\"\n+        Acquire Rate Limiter.\n+\n+        Args\n+        ----\n+            token_count: The estimated number of tokens for the current request.\n+\n+        Yields\n+        ------\n+            None: This context manager does not return any value.\n+        \"\"\"\n+        while True:\n+            with self._lock:\n+                current_time = time.time()\n+\n+                # Use two sliding windows to keep track of #requests and tokens per period\n+                # Drop old requests and tokens out of the sliding windows\n+                while (\n+                    len(self.rate_queue) > 0\n+                    and self.rate_queue[0] < current_time - self.period_in_seconds\n+                ):\n+                    self.rate_queue.popleft()\n+                    self.token_queue.popleft()\n+\n+                # If sliding window still exceed request limit, wait again\n+                # Waiting requires reacquiring the lock, allowing other threads\n+                # to see if their request fits within the rate limiting windows\n+                # Makes more sense for token limit than request limit\n+                if (\n+                    self.rpm is not None\n+                    and self.rpm > 0\n+                    and len(self.rate_queue) >= self.rpm\n+                ):\n+                    continue\n+\n+                # Check if current token window exceeds token limit\n+                # If it does, wait again\n+                # This does not account for the tokens from the current request\n+                # This is intentional, as we want to allow the current request\n+                # to be processed if it is larger than the tpm but smaller than context window.\n+                # tpm is a rate/soft limit and not the hard limit of context window limits.\n+                if (\n+                    self.tpm is not None\n+                    and self.tpm > 0\n+                    and sum(self.token_queue) >= self.tpm\n+                ):\n+                    continue\n+\n+                # This check accounts for the current request token usage\n+                # is within the token limits bound.\n+                # If the current requests token limit exceeds the token limit,\n+                # Then let it be processed.\n+                if (\n+                    self.tpm is not None\n+                    and self.tpm > 0\n+                    and token_count <= self.tpm\n+                    and sum(self.token_queue) + token_count > self.tpm\n+                ):\n+                    continue\n+\n+                # If there was a previous call, check if we need to stagger\n+                if (\n+                    self.stagger > 0\n+                    and (\n+                        self._last_time  # is None if this is the first hit to the rate limiter\n+                        and current_time - self._last_time\n+                        < self.stagger  # If more time has passed than the stagger time, we can proceed\n+                    )\n+                ):\n+                    time.sleep(self.stagger - (current_time - self._last_time))\n+                    current_time = time.time()\n+\n+                # Add the current request to the sliding window\n+                self.rate_queue.append(current_time)\n+                self.token_queue.append(token_count)\n+                self._last_time = current_time\n+                break\n+        yield\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/__init__.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/__init__.py\nnew file mode 100644\nindex 0000000..f01e002\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Retry Services.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/exponential_retry.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/exponential_retry.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/exponential_retry.py\nnew file mode 100644\nindex 0000000..e008322\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/exponential_retry.py\n@@ -0,0 +1,83 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Exponential Retry Service.\"\"\"\n+\n+import asyncio\n+import logging\n+import random\n+import time\n+from collections.abc import Awaitable, Callable\n+from typing import Any\n+\n+from graphrag.language_model.providers.litellm.services.retry.retry import Retry\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class ExponentialRetry(Retry):\n+    \"\"\"LiteLLM Exponential Retry Service.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        max_retries: int = 5,\n+        base_delay: float = 2.0,\n+        jitter: bool = True,\n+        **kwargs: Any,\n+    ):\n+        if max_retries <= 0:\n+            msg = \"max_retries must be greater than 0.\"\n+            raise ValueError(msg)\n+\n+        if base_delay <= 1.0:\n+            msg = \"base_delay must be greater than 1.0.\"\n+            raise ValueError(msg)\n+\n+        self._max_retries = max_retries\n+        self._base_delay = base_delay\n+        self._jitter = jitter\n+\n+    def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n+        \"\"\"Retry a synchronous function.\"\"\"\n+        retries = 0\n+        delay = 1.0  # Initial delay in seconds\n+        while True:\n+            try:\n+                return func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"ExponentialRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                delay *= self._base_delay\n+                logger.exception(\n+                    f\"ExponentialRetry: Request failed, retrying, retries={retries}, delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+                time.sleep(delay + (self._jitter * random.uniform(0, 1)))  # noqa: S311\n+\n+    async def aretry(\n+        self,\n+        func: Callable[..., Awaitable[Any]],\n+        **kwargs: Any,\n+    ) -> Any:\n+        \"\"\"Retry an asynchronous function.\"\"\"\n+        retries = 0\n+        delay = 1.0  # Initial delay in seconds\n+        while True:\n+            try:\n+                return await func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"ExponentialRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                delay *= self._base_delay\n+                logger.exception(\n+                    f\"ExponentialRetry: Request failed, retrying, retries={retries}, delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+                await asyncio.sleep(delay + (self._jitter * random.uniform(0, 1)))  # noqa: S311\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py\nnew file mode 100644\nindex 0000000..97fbdbf\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py\n@@ -0,0 +1,81 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Incremental Wait Retry Service.\"\"\"\n+\n+import asyncio\n+import logging\n+import time\n+from collections.abc import Awaitable, Callable\n+from typing import Any\n+\n+from graphrag.language_model.providers.litellm.services.retry.retry import Retry\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class IncrementalWaitRetry(Retry):\n+    \"\"\"LiteLLM Incremental Wait Retry Service.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        max_retry_wait: float,\n+        max_retries: int = 5,\n+        **kwargs: Any,\n+    ):\n+        if max_retries <= 0:\n+            msg = \"max_retries must be greater than 0.\"\n+            raise ValueError(msg)\n+\n+        if max_retry_wait <= 0:\n+            msg = \"max_retry_wait must be greater than 0.\"\n+            raise ValueError(msg)\n+\n+        self._max_retries = max_retries\n+        self._max_retry_wait = max_retry_wait\n+        self._increment = max_retry_wait / max_retries\n+\n+    def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n+        \"\"\"Retry a synchronous function.\"\"\"\n+        retries = 0\n+        delay = 0.0\n+        while True:\n+            try:\n+                return func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"IncrementalWaitRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                delay += self._increment\n+                logger.exception(\n+                    f\"IncrementalWaitRetry: Request failed, retrying after incremental delay, retries={retries}, delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+                time.sleep(delay)\n+\n+    async def aretry(\n+        self,\n+        func: Callable[..., Awaitable[Any]],\n+        **kwargs: Any,\n+    ) -> Any:\n+        \"\"\"Retry an asynchronous function.\"\"\"\n+        retries = 0\n+        delay = 0.0\n+        while True:\n+            try:\n+                return await func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"IncrementalWaitRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                delay += self._increment\n+                logger.exception(\n+                    f\"IncrementalWaitRetry: Request failed, retrying after incremental delay, retries={retries}, delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+                await asyncio.sleep(delay)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py\nnew file mode 100644\nindex 0000000..088f454\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py\n@@ -0,0 +1,66 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Native Retry Service.\"\"\"\n+\n+import logging\n+from collections.abc import Awaitable, Callable\n+from typing import Any\n+\n+from graphrag.language_model.providers.litellm.services.retry.retry import Retry\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class NativeRetry(Retry):\n+    \"\"\"LiteLLM Native Retry Service.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        max_retries: int = 5,\n+        **kwargs: Any,\n+    ):\n+        if max_retries <= 0:\n+            msg = \"max_retries must be greater than 0.\"\n+            raise ValueError(msg)\n+\n+        self._max_retries = max_retries\n+\n+    def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n+        \"\"\"Retry a synchronous function.\"\"\"\n+        retries = 0\n+        while True:\n+            try:\n+                return func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"NativeRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                logger.exception(\n+                    f\"NativeRetry: Request failed, immediately retrying, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+\n+    async def aretry(\n+        self,\n+        func: Callable[..., Awaitable[Any]],\n+        **kwargs: Any,\n+    ) -> Any:\n+        \"\"\"Retry an asynchronous function.\"\"\"\n+        retries = 0\n+        while True:\n+            try:\n+                return await func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"NativeRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                logger.exception(\n+                    f\"NativeRetry: Request failed, immediately retrying, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py\nnew file mode 100644\nindex 0000000..603f439\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py\n@@ -0,0 +1,79 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Random Wait Retry Service.\"\"\"\n+\n+import asyncio\n+import logging\n+import random\n+import time\n+from collections.abc import Awaitable, Callable\n+from typing import Any\n+\n+from graphrag.language_model.providers.litellm.services.retry.retry import Retry\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class RandomWaitRetry(Retry):\n+    \"\"\"LiteLLM Random Wait Retry Service.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        max_retry_wait: float,\n+        max_retries: int = 5,\n+        **kwargs: Any,\n+    ):\n+        if max_retries <= 0:\n+            msg = \"max_retries must be greater than 0.\"\n+            raise ValueError(msg)\n+\n+        if max_retry_wait <= 0:\n+            msg = \"max_retry_wait must be greater than 0.\"\n+            raise ValueError(msg)\n+\n+        self._max_retries = max_retries\n+        self._max_retry_wait = max_retry_wait\n+\n+    def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n+        \"\"\"Retry a synchronous function.\"\"\"\n+        retries = 0\n+        while True:\n+            try:\n+                return func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"RandomWaitRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                delay = random.uniform(0, self._max_retry_wait)  # noqa: S311\n+                logger.exception(\n+                    f\"RandomWaitRetry: Request failed, retrying after random delay, retries={retries}, delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+                time.sleep(delay)\n+\n+    async def aretry(\n+        self,\n+        func: Callable[..., Awaitable[Any]],\n+        **kwargs: Any,\n+    ) -> Any:\n+        \"\"\"Retry an asynchronous function.\"\"\"\n+        retries = 0\n+        while True:\n+            try:\n+                return await func(**kwargs)\n+            except Exception as e:\n+                if retries >= self._max_retries:\n+                    logger.exception(\n+                        f\"RandomWaitRetry: Max retries exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                    )\n+                    raise\n+                retries += 1\n+                delay = random.uniform(0, self._max_retry_wait)  # noqa: S311\n+                logger.exception(\n+                    f\"RandomWaitRetry: Request failed, retrying after random delay, retries={retries}, delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n+                )\n+                await asyncio.sleep(delay)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry.py\nnew file mode 100644\nindex 0000000..4f53e59\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry.py\n@@ -0,0 +1,33 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Retry Abstract Base Class.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from collections.abc import Awaitable, Callable\n+from typing import Any\n+\n+\n+class Retry(ABC):\n+    \"\"\"LiteLLM Retry Abstract Base Class.\"\"\"\n+\n+    @abstractmethod\n+    def __init__(self, /, **kwargs: Any):\n+        msg = \"Retry subclasses must implement the __init__ method.\"\n+        raise NotImplementedError(msg)\n+\n+    @abstractmethod\n+    def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n+        \"\"\"Retry a synchronous function.\"\"\"\n+        msg = \"Subclasses must implement this method\"\n+        raise NotImplementedError(msg)\n+\n+    @abstractmethod\n+    async def aretry(\n+        self,\n+        func: Callable[..., Awaitable[Any]],\n+        **kwargs: Any,\n+    ) -> Any:\n+        \"\"\"Retry an asynchronous function.\"\"\"\n+        msg = \"Subclasses must implement this method\"\n+        raise NotImplementedError(msg)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry_factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry_factory.py b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry_factory.py\nnew file mode 100644\nindex 0000000..9acdf21\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/services/retry/retry_factory.py\n@@ -0,0 +1,31 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Retry Factory.\"\"\"\n+\n+from graphrag.factory.factory import Factory\n+from graphrag.language_model.providers.litellm.services.retry.exponential_retry import (\n+    ExponentialRetry,\n+)\n+from graphrag.language_model.providers.litellm.services.retry.incremental_wait_retry import (\n+    IncrementalWaitRetry,\n+)\n+from graphrag.language_model.providers.litellm.services.retry.native_wait_retry import (\n+    NativeRetry,\n+)\n+from graphrag.language_model.providers.litellm.services.retry.random_wait_retry import (\n+    RandomWaitRetry,\n+)\n+from graphrag.language_model.providers.litellm.services.retry.retry import Retry\n+\n+\n+class RetryFactory(Factory[Retry]):\n+    \"\"\"Singleton factory for creating retry services.\"\"\"\n+\n+\n+retry_factory = RetryFactory()\n+\n+retry_factory.register(\"native\", NativeRetry)\n+retry_factory.register(\"exponential_backoff\", ExponentialRetry)\n+retry_factory.register(\"random_wait\", RandomWaitRetry)\n+retry_factory.register(\"incremental_wait\", IncrementalWaitRetry)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/providers/litellm/types.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/providers/litellm/types.py b/packages/graphrag/graphrag/language_model/providers/litellm/types.py\nnew file mode 100644\nindex 0000000..cec39b1\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/providers/litellm/types.py\n@@ -0,0 +1,235 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM types.\"\"\"\n+\n+from typing import (\n+    Any,\n+    Protocol,\n+    runtime_checkable,\n+)\n+\n+from litellm import (\n+    AnthropicThinkingParam,\n+    BaseModel,\n+    ChatCompletionAudioParam,\n+    ChatCompletionModality,\n+    ChatCompletionPredictionContentParam,\n+    CustomStreamWrapper,\n+    EmbeddingResponse,  # type: ignore\n+    ModelResponse,  # type: ignore\n+    OpenAIWebSearchOptions,\n+)\n+from openai.types.chat.chat_completion import (\n+    ChatCompletion,\n+    Choice,\n+)\n+from openai.types.chat.chat_completion_chunk import ChatCompletionChunk, ChoiceDelta\n+from openai.types.chat.chat_completion_chunk import Choice as ChunkChoice\n+from openai.types.chat.chat_completion_message import ChatCompletionMessage\n+from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\n+from openai.types.completion_usage import (\n+    CompletionTokensDetails,\n+    CompletionUsage,\n+    PromptTokensDetails,\n+)\n+from openai.types.create_embedding_response import CreateEmbeddingResponse, Usage\n+from openai.types.embedding import Embedding\n+\n+LMChatCompletionMessageParam = ChatCompletionMessageParam | dict[str, str]\n+\n+LMChatCompletion = ChatCompletion\n+LMChoice = Choice\n+LMChatCompletionMessage = ChatCompletionMessage\n+\n+LMChatCompletionChunk = ChatCompletionChunk\n+LMChoiceChunk = ChunkChoice\n+LMChoiceDelta = ChoiceDelta\n+\n+LMCompletionUsage = CompletionUsage\n+LMPromptTokensDetails = PromptTokensDetails\n+LMCompletionTokensDetails = CompletionTokensDetails\n+\n+\n+LMEmbeddingResponse = CreateEmbeddingResponse\n+LMEmbedding = Embedding\n+LMEmbeddingUsage = Usage\n+\n+\n+@runtime_checkable\n+class FixedModelCompletion(Protocol):\n+    \"\"\"\n+    Synchronous chat completion function.\n+\n+    Same signature as litellm.completion but without the `model` parameter\n+    as this is already set in the model configuration.\n+    \"\"\"\n+\n+    def __call__(\n+        self,\n+        *,\n+        messages: list = [],  # type: ignore  # noqa: B006\n+        stream: bool | None = None,\n+        stream_options: dict | None = None,  # type: ignore\n+        stop=None,  # type: ignore\n+        max_completion_tokens: int | None = None,\n+        max_tokens: int | None = None,\n+        modalities: list[ChatCompletionModality] | None = None,\n+        prediction: ChatCompletionPredictionContentParam | None = None,\n+        audio: ChatCompletionAudioParam | None = None,\n+        logit_bias: dict | None = None,  # type: ignore\n+        user: str | None = None,\n+        # openai v1.0+ new params\n+        response_format: dict | type[BaseModel] | None = None,  # type: ignore\n+        seed: int | None = None,\n+        tools: list | None = None,  # type: ignore\n+        tool_choice: str | dict | None = None,  # type: ignore\n+        logprobs: bool | None = None,\n+        top_logprobs: int | None = None,\n+        parallel_tool_calls: bool | None = None,\n+        web_search_options: OpenAIWebSearchOptions | None = None,\n+        deployment_id=None,  # type: ignore\n+        extra_headers: dict | None = None,  # type: ignore\n+        # soon to be deprecated params by OpenAI\n+        functions: list | None = None,  # type: ignore\n+        function_call: str | None = None,\n+        # Optional liteLLM function params\n+        thinking: AnthropicThinkingParam | None = None,\n+        **kwargs: Any,\n+    ) -> ModelResponse | CustomStreamWrapper:\n+        \"\"\"Chat completion function.\"\"\"\n+        ...\n+\n+\n+@runtime_checkable\n+class AFixedModelCompletion(Protocol):\n+    \"\"\"\n+    Asynchronous chat completion function.\n+\n+    Same signature as litellm.acompletion but without the `model` parameter\n+    as this is already set in the model configuration.\n+    \"\"\"\n+\n+    async def __call__(\n+        self,\n+        *,\n+        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n+        messages: list = [],  # type: ignore  # noqa: B006\n+        stream: bool | None = None,\n+        stream_options: dict | None = None,  # type: ignore\n+        stop=None,  # type: ignore\n+        max_completion_tokens: int | None = None,\n+        max_tokens: int | None = None,\n+        modalities: list[ChatCompletionModality] | None = None,\n+        prediction: ChatCompletionPredictionContentParam | None = None,\n+        audio: ChatCompletionAudioParam | None = None,\n+        logit_bias: dict | None = None,  # type: ignore\n+        user: str | None = None,\n+        # openai v1.0+ new params\n+        response_format: dict | type[BaseModel] | None = None,  # type: ignore\n+        seed: int | None = None,\n+        tools: list | None = None,  # type: ignore\n+        tool_choice: str | dict | None = None,  # type: ignore\n+        logprobs: bool | None = None,\n+        top_logprobs: int | None = None,\n+        parallel_tool_calls: bool | None = None,\n+        web_search_options: OpenAIWebSearchOptions | None = None,\n+        deployment_id=None,  # type: ignore\n+        extra_headers: dict | None = None,  # type: ignore\n+        # soon to be deprecated params by OpenAI\n+        functions: list | None = None,  # type: ignore\n+        function_call: str | None = None,\n+        # Optional liteLLM function params\n+        thinking: AnthropicThinkingParam | None = None,\n+        **kwargs: Any,\n+    ) -> ModelResponse | CustomStreamWrapper:\n+        \"\"\"Chat completion function.\"\"\"\n+        ...\n+\n+\n+@runtime_checkable\n+class FixedModelEmbedding(Protocol):\n+    \"\"\"\n+    Synchronous embedding function.\n+\n+    Same signature as litellm.embedding but without the `model` parameter\n+    as this is already set in the model configuration.\n+    \"\"\"\n+\n+    def __call__(\n+        self,\n+        *,\n+        request_id: str | None = None,\n+        input: list = [],  # type: ignore  # noqa: B006\n+        # Optional params\n+        dimensions: int | None = None,\n+        encoding_format: str | None = None,\n+        timeout: int = 600,  # default to 10 minutes\n+        # set api_base, api_version, api_key\n+        api_base: str | None = None,\n+        api_version: str | None = None,\n+        api_key: str | None = None,\n+        api_type: str | None = None,\n+        caching: bool = False,\n+        user: str | None = None,\n+        **kwargs: Any,\n+    ) -> EmbeddingResponse:\n+        \"\"\"Embedding function.\"\"\"\n+        ...\n+\n+\n+@runtime_checkable\n+class AFixedModelEmbedding(Protocol):\n+    \"\"\"\n+    Asynchronous embedding function.\n+\n+    Same signature as litellm.embedding but without the `model` parameter\n+    as this is already set in the model configuration.\n+    \"\"\"\n+\n+    async def __call__(\n+        self,\n+        *,\n+        request_id: str | None = None,\n+        input: list = [],  # type: ignore  # noqa: B006\n+        # Optional params\n+        dimensions: int | None = None,\n+        encoding_format: str | None = None,\n+        timeout: int = 600,  # default to 10 minutes\n+        # set api_base, api_version, api_key\n+        api_base: str | None = None,\n+        api_version: str | None = None,\n+        api_key: str | None = None,\n+        api_type: str | None = None,\n+        caching: bool = False,\n+        user: str | None = None,\n+        **kwargs: Any,\n+    ) -> EmbeddingResponse:\n+        \"\"\"Embedding function.\"\"\"\n+        ...\n+\n+\n+@runtime_checkable\n+class LitellmRequestFunc(Protocol):\n+    \"\"\"\n+    Synchronous request function.\n+\n+    Represents either a chat completion or embedding function.\n+    \"\"\"\n+\n+    def __call__(self, /, **kwargs: Any) -> Any:\n+        \"\"\"Request function.\"\"\"\n+        ...\n+\n+\n+@runtime_checkable\n+class AsyncLitellmRequestFunc(Protocol):\n+    \"\"\"\n+    Asynchronous request function.\n+\n+    Represents either a chat completion or embedding function.\n+    \"\"\"\n+\n+    async def __call__(self, /, **kwargs: Any) -> Any:\n+        \"\"\"Request function.\"\"\"\n+        ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/response/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/response/__init__.py b/packages/graphrag/graphrag/language_model/response/__init__.py\nnew file mode 100644\nindex 0000000..3c4721c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/response/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing Model response definitions.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/response/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/response/base.py b/packages/graphrag/graphrag/language_model/response/base.py\nnew file mode 100644\nindex 0000000..178259c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/response/base.py\n@@ -0,0 +1,71 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base llm response protocol.\"\"\"\n+\n+from typing import Any, Generic, Protocol, TypeVar\n+\n+from pydantic import BaseModel, Field\n+\n+T = TypeVar(\"T\", bound=BaseModel, covariant=True)\n+\n+\n+class ModelOutput(Protocol):\n+    \"\"\"Protocol for Model response's output object.\"\"\"\n+\n+    @property\n+    def content(self) -> str:\n+        \"\"\"Return the textual content of the output.\"\"\"\n+        ...\n+\n+    @property\n+    def full_response(self) -> dict[str, Any] | None:\n+        \"\"\"Return the complete JSON response returned by the model.\"\"\"\n+        ...\n+\n+\n+class ModelResponse(Protocol, Generic[T]):\n+    \"\"\"Protocol for LLM response.\"\"\"\n+\n+    @property\n+    def output(self) -> ModelOutput:\n+        \"\"\"Return the output of the response.\"\"\"\n+        ...\n+\n+    @property\n+    def parsed_response(self) -> T | None:\n+        \"\"\"Return the parsed response.\"\"\"\n+        ...\n+\n+    @property\n+    def history(self) -> list:\n+        \"\"\"Return the history of the response.\"\"\"\n+        ...\n+\n+\n+class BaseModelOutput(BaseModel):\n+    \"\"\"Base class for LLM output.\"\"\"\n+\n+    content: str = Field(..., description=\"The textual content of the output.\")\n+    \"\"\"The textual content of the output.\"\"\"\n+    full_response: dict[str, Any] | None = Field(\n+        None, description=\"The complete JSON response returned by the LLM provider.\"\n+    )\n+    \"\"\"The complete JSON response returned by the LLM provider.\"\"\"\n+\n+\n+class BaseModelResponse(BaseModel, Generic[T]):\n+    \"\"\"Base class for a Model response.\"\"\"\n+\n+    output: BaseModelOutput\n+    \"\"\"\"\"\"\n+    parsed_response: T | None = None\n+    \"\"\"Parsed response.\"\"\"\n+    history: list[Any] = Field(default_factory=list)\n+    \"\"\"History of the response.\"\"\"\n+    tool_calls: list = Field(default_factory=list)\n+    \"\"\"Tool calls required by the Model. These will be instances of the LLM tools (with filled parameters).\"\"\"\n+    metrics: Any | None = None\n+    \"\"\"Request/response metrics.\"\"\"\n+    cache_hit: bool | None = None\n+    \"\"\"Whether the response was a cache hit.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/response/base.pyi",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/response/base.pyi b/packages/graphrag/graphrag/language_model/response/base.pyi\nnew file mode 100644\nindex 0000000..7a33b0a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/response/base.pyi\n@@ -0,0 +1,50 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+from typing import Any, Generic, Protocol, TypeVar\n+\n+from pydantic import BaseModel\n+\n+_T = TypeVar(\"_T\", bound=BaseModel, covariant=True)\n+\n+class ModelOutput(Protocol):\n+    @property\n+    def content(self) -> str: ...\n+    @property\n+    def full_response(self) -> dict[str, Any] | None: ...\n+\n+class ModelResponse(Protocol, Generic[_T]):\n+    @property\n+    def output(self) -> ModelOutput: ...\n+    @property\n+    def parsed_response(self) -> _T | None: ...\n+    @property\n+    def history(self) -> list[Any]: ...\n+\n+class BaseModelOutput(BaseModel):\n+    content: str\n+    full_response: dict[str, Any] | None\n+\n+    def __init__(\n+        self,\n+        content: str,\n+        full_response: dict[str, Any] | None = None,\n+    ) -> None: ...\n+\n+class BaseModelResponse(BaseModel, Generic[_T]):\n+    output: BaseModelOutput\n+    parsed_response: _T | None\n+    history: list[Any]\n+    tool_calls: list[Any]\n+    metrics: Any | None\n+    cache_hit: bool | None\n+\n+    def __init__(\n+        self,\n+        output: BaseModelOutput,\n+        parsed_response: _T | None = None,\n+        history: list[Any] = ...,  # default provided by Pydantic\n+        tool_calls: list[Any] = ...,  # default provided by Pydantic\n+        metrics: Any | None = None,\n+        cache_hit: bool | None = None,\n+    ) -> None: ...\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/language_model/util.py",
            "diff": "diff --git a/packages/graphrag/graphrag/language_model/util.py b/packages/graphrag/graphrag/language_model/util.py\nnew file mode 100644\nindex 0000000..fd33077\n--- /dev/null\n+++ b/packages/graphrag/graphrag/language_model/util.py\n@@ -0,0 +1,41 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Utility functions for language models.\"\"\"\n+\n+from typing import Any\n+\n+from graphrag.config.models.language_model_config import LanguageModelConfig\n+\n+\n+def is_reasoning_model(model: str) -> bool:\n+    \"\"\"Return whether the model uses a known OpenAI reasoning model.\"\"\"\n+    return model.lower() in {\"o1\", \"o1-mini\", \"o3-mini\"}\n+\n+\n+def get_openai_model_parameters_from_config(\n+    config: LanguageModelConfig,\n+) -> dict[str, Any]:\n+    \"\"\"Get the model parameters for a given config, adjusting for reasoning API differences.\"\"\"\n+    return get_openai_model_parameters_from_dict(config.model_dump())\n+\n+\n+def get_openai_model_parameters_from_dict(config: dict[str, Any]) -> dict[str, Any]:\n+    \"\"\"Get the model parameters for a given config, adjusting for reasoning API differences.\"\"\"\n+    params = {\n+        \"n\": config.get(\"n\"),\n+    }\n+    if is_reasoning_model(config[\"model\"]):\n+        params[\"max_completion_tokens\"] = config.get(\"max_completion_tokens\")\n+        params[\"reasoning_effort\"] = config.get(\"reasoning_effort\")\n+    else:\n+        params[\"max_tokens\"] = config.get(\"max_tokens\")\n+        params[\"temperature\"] = config.get(\"temperature\")\n+        params[\"frequency_penalty\"] = config.get(\"frequency_penalty\")\n+        params[\"presence_penalty\"] = config.get(\"presence_penalty\")\n+        params[\"top_p\"] = config.get(\"top_p\")\n+\n+    if config.get(\"response_format\"):\n+        params[\"response_format\"] = config[\"response_format\"]\n+\n+    return params\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/logger/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/logger/__init__.py b/packages/graphrag/graphrag/logger/__init__.py\nnew file mode 100644\nindex 0000000..d209b11\n--- /dev/null\n+++ b/packages/graphrag/graphrag/logger/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Logger utilities and implementations.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/logger/blob_workflow_logger.py",
            "diff": "diff --git a/packages/graphrag/graphrag/logger/blob_workflow_logger.py b/packages/graphrag/graphrag/logger/blob_workflow_logger.py\nnew file mode 100644\nindex 0000000..ae4893c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/logger/blob_workflow_logger.py\n@@ -0,0 +1,119 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A logger that emits updates from the indexing engine to a blob in Azure Storage.\"\"\"\n+\n+import json\n+import logging\n+from datetime import datetime, timezone\n+from pathlib import Path\n+from typing import Any\n+\n+from azure.identity import DefaultAzureCredential\n+from azure.storage.blob import BlobServiceClient\n+\n+\n+class BlobWorkflowLogger(logging.Handler):\n+    \"\"\"A logging handler that writes to a blob storage account.\"\"\"\n+\n+    _blob_service_client: BlobServiceClient\n+    _container_name: str\n+    _max_block_count: int = 25000  # 25k blocks per blob\n+\n+    def __init__(\n+        self,\n+        connection_string: str | None,\n+        container_name: str | None,\n+        blob_name: str = \"\",\n+        base_dir: str | None = None,\n+        storage_account_blob_url: str | None = None,\n+        level: int = logging.NOTSET,\n+    ):\n+        \"\"\"Create a new instance of the BlobWorkflowLogger class.\"\"\"\n+        super().__init__(level)\n+\n+        if container_name is None:\n+            msg = \"No container name provided for blob storage.\"\n+            raise ValueError(msg)\n+        if connection_string is None and storage_account_blob_url is None:\n+            msg = \"No storage account blob url provided for blob storage.\"\n+            raise ValueError(msg)\n+\n+        self._connection_string = connection_string\n+        self._storage_account_blob_url = storage_account_blob_url\n+\n+        if self._connection_string:\n+            self._blob_service_client = BlobServiceClient.from_connection_string(\n+                self._connection_string\n+            )\n+        else:\n+            if storage_account_blob_url is None:\n+                msg = \"Either connection_string or storage_account_blob_url must be provided.\"\n+                raise ValueError(msg)\n+\n+            self._blob_service_client = BlobServiceClient(\n+                storage_account_blob_url,\n+                credential=DefaultAzureCredential(),\n+            )\n+\n+        if blob_name == \"\":\n+            blob_name = f\"report/{datetime.now(tz=timezone.utc).strftime('%Y-%m-%d-%H:%M:%S:%f')}.logs.json\"\n+\n+        self._blob_name = str(Path(base_dir or \"\") / blob_name)\n+        self._container_name = container_name\n+        self._blob_client = self._blob_service_client.get_blob_client(\n+            self._container_name, self._blob_name\n+        )\n+        if not self._blob_client.exists():\n+            self._blob_client.create_append_blob()\n+\n+        self._num_blocks = 0  # refresh block counter\n+\n+    def emit(self, record) -> None:\n+        \"\"\"Emit a log record to blob storage.\"\"\"\n+        try:\n+            # Create JSON structure based on record\n+            log_data = {\n+                \"type\": self._get_log_type(record.levelno),\n+                \"data\": record.getMessage(),\n+            }\n+\n+            # Add additional fields if they exist\n+            if hasattr(record, \"details\") and record.details:  # type: ignore[reportAttributeAccessIssue]\n+                log_data[\"details\"] = record.details  # type: ignore[reportAttributeAccessIssue]\n+            if record.exc_info and record.exc_info[1]:\n+                log_data[\"cause\"] = str(record.exc_info[1])\n+            if hasattr(record, \"stack\") and record.stack:  # type: ignore[reportAttributeAccessIssue]\n+                log_data[\"stack\"] = record.stack  # type: ignore[reportAttributeAccessIssue]\n+\n+            self._write_log(log_data)\n+        except (OSError, ValueError):\n+            self.handleError(record)\n+\n+    def _get_log_type(self, level: int) -> str:\n+        \"\"\"Get log type string based on log level.\"\"\"\n+        if level >= logging.ERROR:\n+            return \"error\"\n+        if level >= logging.WARNING:\n+            return \"warning\"\n+        return \"log\"\n+\n+    def _write_log(self, log: dict[str, Any]):\n+        \"\"\"Write log data to blob storage.\"\"\"\n+        # create a new file when block count hits close 25k\n+        if (\n+            self._num_blocks >= self._max_block_count\n+        ):  # Check if block count exceeds 25k\n+            self.__init__(\n+                self._connection_string,\n+                self._container_name,\n+                storage_account_blob_url=self._storage_account_blob_url,\n+            )\n+\n+        blob_client = self._blob_service_client.get_blob_client(\n+            self._container_name, self._blob_name\n+        )\n+        blob_client.append_block(json.dumps(log, indent=4, ensure_ascii=False) + \"\\n\")\n+\n+        # update the blob's block count\n+        self._num_blocks += 1\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/logger/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/logger/factory.py b/packages/graphrag/graphrag/logger/factory.py\nnew file mode 100644\nindex 0000000..5680073\n--- /dev/null\n+++ b/packages/graphrag/graphrag/logger/factory.py\n@@ -0,0 +1,64 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Factory functions for creating a logger.\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+from pathlib import Path\n+\n+from graphrag.config.enums import ReportingType\n+from graphrag.factory.factory import Factory\n+\n+LOG_FORMAT = \"%(asctime)s.%(msecs)04d - %(levelname)s - %(name)s - %(message)s\"\n+DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n+\n+\n+class LoggerFactory(Factory[logging.Handler]):\n+    \"\"\"A factory class for logger implementations.\n+\n+    Includes a method for users to register a custom logger implementation.\n+\n+    Configuration arguments are passed to each logger implementation as kwargs\n+    for individual enforcement of required/optional arguments.\n+\n+    Note that because we rely on the built-in Python logging architecture, this factory does not return an instance,\n+    it merely configures the logger to your specified storage location.\n+    \"\"\"\n+\n+\n+# --- register built-in logger implementations ---\n+def create_file_logger(**kwargs) -> logging.Handler:\n+    \"\"\"Create a file-based logger.\"\"\"\n+    root_dir = kwargs[\"root_dir\"]\n+    base_dir = kwargs[\"base_dir\"]\n+    filename = kwargs[\"filename\"]\n+    log_dir = Path(root_dir) / base_dir\n+    log_dir.mkdir(parents=True, exist_ok=True)\n+    log_file_path = log_dir / filename\n+\n+    handler = logging.FileHandler(str(log_file_path), mode=\"a\")\n+\n+    formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=DATE_FORMAT)\n+    handler.setFormatter(formatter)\n+\n+    return handler\n+\n+\n+def create_blob_logger(**kwargs) -> logging.Handler:\n+    \"\"\"Create a blob storage-based logger.\"\"\"\n+    from graphrag.logger.blob_workflow_logger import BlobWorkflowLogger\n+\n+    return BlobWorkflowLogger(\n+        connection_string=kwargs[\"connection_string\"],\n+        container_name=kwargs[\"container_name\"],\n+        base_dir=kwargs[\"base_dir\"],\n+        storage_account_blob_url=kwargs[\"storage_account_blob_url\"],\n+    )\n+\n+\n+# --- register built-in implementations ---\n+logger_factory = LoggerFactory()\n+logger_factory.register(ReportingType.file.value, create_file_logger)\n+logger_factory.register(ReportingType.blob.value, create_blob_logger)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/logger/progress.py",
            "diff": "diff --git a/packages/graphrag/graphrag/logger/progress.py b/packages/graphrag/graphrag/logger/progress.py\nnew file mode 100644\nindex 0000000..20617a6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/logger/progress.py\n@@ -0,0 +1,95 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Progress Logging Utilities.\"\"\"\n+\n+import logging\n+from collections.abc import Callable, Iterable\n+from dataclasses import dataclass\n+from typing import TypeVar\n+\n+T = TypeVar(\"T\")\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass\n+class Progress:\n+    \"\"\"A class representing the progress of a task.\"\"\"\n+\n+    description: str | None = None\n+    \"\"\"Description of the progress\"\"\"\n+\n+    total_items: int | None = None\n+    \"\"\"Total number of items\"\"\"\n+\n+    completed_items: int | None = None\n+    \"\"\"Number of items completed\"\"\"\n+\n+\n+ProgressHandler = Callable[[Progress], None]\n+\"\"\"A function to handle progress reports.\"\"\"\n+\n+\n+class ProgressTicker:\n+    \"\"\"A class that emits progress reports incrementally.\"\"\"\n+\n+    _callback: ProgressHandler | None\n+    _description: str\n+    _num_total: int\n+    _num_complete: int\n+\n+    def __init__(\n+        self, callback: ProgressHandler | None, num_total: int, description: str = \"\"\n+    ):\n+        self._callback = callback\n+        self._description = description\n+        self._num_total = num_total\n+        self._num_complete = 0\n+\n+    def __call__(self, num_ticks: int = 1) -> None:\n+        \"\"\"Emit progress.\"\"\"\n+        self._num_complete += num_ticks\n+        if self._callback is not None:\n+            p = Progress(\n+                total_items=self._num_total,\n+                completed_items=self._num_complete,\n+                description=self._description,\n+            )\n+            if p.description:\n+                logger.info(\"%s%s/%s\", p.description, p.completed_items, p.total_items)\n+            self._callback(p)\n+\n+    def done(self) -> None:\n+        \"\"\"Mark the progress as done.\"\"\"\n+        if self._callback is not None:\n+            self._callback(\n+                Progress(\n+                    total_items=self._num_total,\n+                    completed_items=self._num_total,\n+                    description=self._description,\n+                )\n+            )\n+\n+\n+def progress_ticker(\n+    callback: ProgressHandler | None, num_total: int, description: str = \"\"\n+) -> ProgressTicker:\n+    \"\"\"Create a progress ticker.\"\"\"\n+    return ProgressTicker(callback, num_total, description=description)\n+\n+\n+def progress_iterable(\n+    iterable: Iterable[T],\n+    progress: ProgressHandler | None,\n+    num_total: int | None = None,\n+    description: str = \"\",\n+) -> Iterable[T]:\n+    \"\"\"Wrap an iterable with a progress handler. Every time an item is yielded, the progress handler will be called with the current progress.\"\"\"\n+    if num_total is None:\n+        num_total = len(list(iterable))\n+\n+    tick = ProgressTicker(progress, num_total, description=description)\n+\n+    for item in iterable:\n+        tick(1)\n+        yield item\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/logger/standard_logging.py",
            "diff": "diff --git a/packages/graphrag/graphrag/logger/standard_logging.py b/packages/graphrag/graphrag/logger/standard_logging.py\nnew file mode 100644\nindex 0000000..31296e1\n--- /dev/null\n+++ b/packages/graphrag/graphrag/logger/standard_logging.py\n@@ -0,0 +1,83 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Standard logging configuration for the graphrag package.\n+\n+This module provides a standardized way to configure Python's built-in\n+logging system for use within the graphrag package.\n+\n+Usage:\n+    # Configuration should be done once at the start of your application:\n+    from graphrag.logger.standard_logging import init_loggers\n+    init_loggers(config)\n+\n+    # Then throughout your code:\n+    import logging\n+    logger = logging.getLogger(__name__)  # Use standard logging\n+\n+    # Use standard logging methods:\n+    logger.debug(\"Debug message\")\n+    logger.info(\"Info message\")\n+    logger.warning(\"Warning message\")\n+    logger.error(\"Error message\")\n+    logger.critical(\"Critical error message\")\n+\n+Notes\n+-----\n+    The logging system is hierarchical. Loggers are organized in a tree structure,\n+    with the root logger named 'graphrag'. All loggers created with names starting\n+    with 'graphrag.' will be children of this root logger. This allows for consistent\n+    configuration of all graphrag-related logs throughout the application.\n+\n+    All progress logging now uses this standard logging system for consistency.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+from typing import TYPE_CHECKING\n+\n+from graphrag.logger.factory import (\n+    LoggerFactory,\n+)\n+\n+if TYPE_CHECKING:\n+    from graphrag.config.models.graph_rag_config import GraphRagConfig\n+\n+DEFAULT_LOG_FILENAME = \"indexing-engine.log\"\n+\n+\n+def init_loggers(\n+    config: GraphRagConfig,\n+    verbose: bool = False,\n+    filename: str = DEFAULT_LOG_FILENAME,\n+) -> None:\n+    \"\"\"Initialize logging handlers for graphrag based on configuration.\n+\n+    Parameters\n+    ----------\n+    config : GraphRagConfig | None, default=None\n+        The GraphRAG configuration. If None, defaults to file-based reporting.\n+    verbose : bool, default=False\n+        Whether to enable verbose (DEBUG) logging.\n+    filename : Optional[str]\n+        Log filename on disk. If unset, will use a default name.\n+    \"\"\"\n+    logger = logging.getLogger(\"graphrag\")\n+    log_level = logging.DEBUG if verbose else logging.INFO\n+    logger.setLevel(log_level)\n+\n+    # clear any existing handlers to avoid duplicate logs\n+    if logger.hasHandlers():\n+        # Close file handlers properly before removing them\n+        for handler in logger.handlers:\n+            if isinstance(handler, logging.FileHandler):\n+                handler.close()\n+        logger.handlers.clear()\n+\n+    reporting_config = config.reporting\n+    config_dict = reporting_config.model_dump()\n+    args = {**config_dict, \"root_dir\": config.root_dir, \"filename\": filename}\n+\n+    handler = LoggerFactory().create(reporting_config.type, args)\n+    logger.addHandler(handler)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/__init__.py b/packages/graphrag/graphrag/prompt_tune/__init__.py\nnew file mode 100644\nindex 0000000..6997787\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The prompt-tuning package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/defaults.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/defaults.py b/packages/graphrag/graphrag/prompt_tune/defaults.py\nnew file mode 100644\nindex 0000000..6fb84f1\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/defaults.py\n@@ -0,0 +1,20 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Default values for the prompt-tuning module.\n+\n+Note: These values get accessed from the CLI to set default behavior.\n+To maintain fast responsiveness from the CLI, do not add long-running code in this file and be mindful of imports.\n+\"\"\"\n+\n+DEFAULT_TASK = \"\"\"\n+Identify the relations and structure of the community of interest, specifically within the {domain} domain.\n+\"\"\"\n+\n+K = 15\n+LIMIT = 15\n+MAX_TOKEN_COUNT = 2000\n+MIN_CHUNK_SIZE = 200\n+N_SUBSET_MAX = 300\n+MIN_CHUNK_OVERLAP = 0\n+PROMPT_TUNING_MODEL_ID = \"default_chat_model\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/__init__.py b/packages/graphrag/graphrag/prompt_tune/generator/__init__.py\nnew file mode 100644\nindex 0000000..8f14405\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Prompt generation module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/community_report_rating.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/community_report_rating.py b/packages/graphrag/graphrag/prompt_tune/generator/community_report_rating.py\nnew file mode 100644\nindex 0000000..22cf731\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/community_report_rating.py\n@@ -0,0 +1,35 @@\n+\"\"\"Generate a rating description for community report rating.\"\"\"\n+\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.prompt.community_report_rating import (\n+    GENERATE_REPORT_RATING_PROMPT,\n+)\n+\n+\n+async def generate_community_report_rating(\n+    model: ChatModel, domain: str, persona: str, docs: str | list[str]\n+) -> str:\n+    \"\"\"Generate an LLM persona to use for GraphRAG prompts.\n+\n+    Parameters\n+    ----------\n+    - llm (CompletionLLM): The LLM to use for generation\n+    - domain (str): The domain to generate a rating for\n+    - persona (str): The persona to generate a rating for for\n+    - docs (str | list[str]): Documents used to contextualize the rating\n+\n+    Returns\n+    -------\n+    - str: The generated rating description prompt response.\n+    \"\"\"\n+    docs_str = \" \".join(docs) if isinstance(docs, list) else docs\n+    domain_prompt = GENERATE_REPORT_RATING_PROMPT.format(\n+        domain=domain, persona=persona, input_text=docs_str\n+    )\n+\n+    response = await model.achat(domain_prompt)\n+\n+    return str(response.output.content).strip()\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/community_report_summarization.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/community_report_summarization.py b/packages/graphrag/graphrag/prompt_tune/generator/community_report_summarization.py\nnew file mode 100644\nindex 0000000..d8a2505\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/community_report_summarization.py\n@@ -0,0 +1,50 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Module for generating prompts for community report summarization.\"\"\"\n+\n+from pathlib import Path\n+\n+from graphrag.prompt_tune.template.community_report_summarization import (\n+    COMMUNITY_REPORT_SUMMARIZATION_PROMPT,\n+)\n+\n+COMMUNITY_SUMMARIZATION_FILENAME = \"community_report_graph.txt\"\n+\n+\n+def create_community_summarization_prompt(\n+    persona: str,\n+    role: str,\n+    report_rating_description: str,\n+    language: str,\n+    output_path: Path | None = None,\n+) -> str:\n+    \"\"\"Create a prompt for community summarization. If output_path is provided, write the prompt to a file.\n+\n+    Parameters\n+    ----------\n+    - persona (str): The persona to use for the community summarization prompt\n+    - role (str): The role to use for the community summarization prompt\n+    - language (str): The language to use for the community summarization prompt\n+    - output_path (Path | None): The path to write the prompt to. Default is None. If None, the prompt is not written to a file. Default is None.\n+\n+    Returns\n+    -------\n+    - str: The community summarization prompt\n+    \"\"\"\n+    prompt = COMMUNITY_REPORT_SUMMARIZATION_PROMPT.format(\n+        persona=persona,\n+        role=role,\n+        report_rating_description=report_rating_description,\n+        language=language,\n+    )\n+\n+    if output_path:\n+        output_path.mkdir(parents=True, exist_ok=True)\n+\n+        output_path = output_path / COMMUNITY_SUMMARIZATION_FILENAME\n+        # Write file to output path\n+        with output_path.open(\"wb\") as file:\n+            file.write(prompt.encode(encoding=\"utf-8\", errors=\"strict\"))\n+\n+    return prompt\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/community_reporter_role.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/community_reporter_role.py b/packages/graphrag/graphrag/prompt_tune/generator/community_reporter_role.py\nnew file mode 100644\nindex 0000000..d3c90d1\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/community_reporter_role.py\n@@ -0,0 +1,35 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Generate a community reporter role for community summarization.\"\"\"\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.prompt.community_reporter_role import (\n+    GENERATE_COMMUNITY_REPORTER_ROLE_PROMPT,\n+)\n+\n+\n+async def generate_community_reporter_role(\n+    model: ChatModel, domain: str, persona: str, docs: str | list[str]\n+) -> str:\n+    \"\"\"Generate an LLM persona to use for GraphRAG prompts.\n+\n+    Parameters\n+    ----------\n+    - llm (CompletionLLM): The LLM to use for generation\n+    - domain (str): The domain to generate a persona for\n+    - persona (str): The persona to generate a role for\n+    - docs (str | list[str]): The domain to generate a persona for\n+\n+    Returns\n+    -------\n+    - str: The generated domain prompt response.\n+    \"\"\"\n+    docs_str = \" \".join(docs) if isinstance(docs, list) else docs\n+    domain_prompt = GENERATE_COMMUNITY_REPORTER_ROLE_PROMPT.format(\n+        domain=domain, persona=persona, input_text=docs_str\n+    )\n+\n+    response = await model.achat(domain_prompt)\n+\n+    return str(response.output.content)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/domain.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/domain.py b/packages/graphrag/graphrag/prompt_tune/generator/domain.py\nnew file mode 100644\nindex 0000000..7838594\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/domain.py\n@@ -0,0 +1,27 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Domain generation for GraphRAG prompts.\"\"\"\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.prompt.domain import GENERATE_DOMAIN_PROMPT\n+\n+\n+async def generate_domain(model: ChatModel, docs: str | list[str]) -> str:\n+    \"\"\"Generate an LLM persona to use for GraphRAG prompts.\n+\n+    Parameters\n+    ----------\n+    - llm (CompletionLLM): The LLM to use for generation\n+    - docs (str | list[str]): The domain to generate a persona for\n+\n+    Returns\n+    -------\n+    - str: The generated domain prompt response.\n+    \"\"\"\n+    docs_str = \" \".join(docs) if isinstance(docs, list) else docs\n+    domain_prompt = GENERATE_DOMAIN_PROMPT.format(input_text=docs_str)\n+\n+    response = await model.achat(domain_prompt)\n+\n+    return str(response.output.content)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/entity_relationship.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/entity_relationship.py b/packages/graphrag/graphrag/prompt_tune/generator/entity_relationship.py\nnew file mode 100644\nindex 0000000..70225cb\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/entity_relationship.py\n@@ -0,0 +1,65 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Entity relationship example generation module.\"\"\"\n+\n+import asyncio\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.prompt.entity_relationship import (\n+    ENTITY_RELATIONSHIPS_GENERATION_JSON_PROMPT,\n+    ENTITY_RELATIONSHIPS_GENERATION_PROMPT,\n+    UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT,\n+)\n+\n+MAX_EXAMPLES = 5\n+\n+\n+async def generate_entity_relationship_examples(\n+    model: ChatModel,\n+    persona: str,\n+    entity_types: str | list[str] | None,\n+    docs: str | list[str],\n+    language: str,\n+    json_mode: bool = False,\n+) -> list[str]:\n+    \"\"\"Generate a list of entity/relationships examples for use in generating an entity configuration.\n+\n+    Will return entity/relationships examples as either JSON or in tuple_delimiter format depending\n+    on the json_mode parameter.\n+    \"\"\"\n+    docs_list = [docs] if isinstance(docs, str) else docs\n+    history = [{\"content\": persona, \"role\": \"system\"}]\n+\n+    if entity_types:\n+        entity_types_str = (\n+            entity_types\n+            if isinstance(entity_types, str)\n+            else \", \".join(map(str, entity_types))\n+        )\n+\n+        messages = [\n+            (\n+                ENTITY_RELATIONSHIPS_GENERATION_JSON_PROMPT\n+                if json_mode\n+                else ENTITY_RELATIONSHIPS_GENERATION_PROMPT\n+            ).format(entity_types=entity_types_str, input_text=doc, language=language)\n+            for doc in docs_list\n+        ]\n+    else:\n+        messages = [\n+            UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT.format(\n+                input_text=doc, language=language\n+            )\n+            for doc in docs_list\n+        ]\n+\n+    messages = messages[:MAX_EXAMPLES]\n+\n+    tasks = [\n+        model.achat(message, history=history, json=json_mode) for message in messages\n+    ]\n+\n+    responses = await asyncio.gather(*tasks)\n+\n+    return [str(response.output.content) for response in responses]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/entity_summarization_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/entity_summarization_prompt.py b/packages/graphrag/graphrag/prompt_tune/generator/entity_summarization_prompt.py\nnew file mode 100644\nindex 0000000..979e5b0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/entity_summarization_prompt.py\n@@ -0,0 +1,39 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Entity summarization prompt generation module.\"\"\"\n+\n+from pathlib import Path\n+\n+from graphrag.prompt_tune.template.entity_summarization import (\n+    ENTITY_SUMMARIZATION_PROMPT,\n+)\n+\n+ENTITY_SUMMARIZATION_FILENAME = \"summarize_descriptions.txt\"\n+\n+\n+def create_entity_summarization_prompt(\n+    persona: str,\n+    language: str,\n+    output_path: Path | None = None,\n+) -> str:\n+    \"\"\"\n+    Create a prompt for entity summarization.\n+\n+    Parameters\n+    ----------\n+    - persona (str): The persona to use for the entity summarization prompt\n+    - language (str): The language to use for the entity summarization prompt\n+    - output_path (Path | None): The path to write the prompt to. Default is None.\n+    \"\"\"\n+    prompt = ENTITY_SUMMARIZATION_PROMPT.format(persona=persona, language=language)\n+\n+    if output_path:\n+        output_path.mkdir(parents=True, exist_ok=True)\n+\n+        output_path = output_path / ENTITY_SUMMARIZATION_FILENAME\n+        # Write file to output path\n+        with output_path.open(\"wb\") as file:\n+            file.write(prompt.encode(encoding=\"utf-8\", errors=\"strict\"))\n+\n+    return prompt\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/entity_types.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/entity_types.py b/packages/graphrag/graphrag/prompt_tune/generator/entity_types.py\nnew file mode 100644\nindex 0000000..d68ab52\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/entity_types.py\n@@ -0,0 +1,59 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Entity type generation module for fine-tuning.\"\"\"\n+\n+from pydantic import BaseModel\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.defaults import DEFAULT_TASK\n+from graphrag.prompt_tune.prompt.entity_types import (\n+    ENTITY_TYPE_GENERATION_JSON_PROMPT,\n+    ENTITY_TYPE_GENERATION_PROMPT,\n+)\n+\n+\n+class EntityTypesResponse(BaseModel):\n+    \"\"\"Entity types response model.\"\"\"\n+\n+    entity_types: list[str]\n+\n+\n+async def generate_entity_types(\n+    model: ChatModel,\n+    domain: str,\n+    persona: str,\n+    docs: str | list[str],\n+    task: str = DEFAULT_TASK,\n+    json_mode: bool = False,\n+) -> str | list[str]:\n+    \"\"\"\n+    Generate entity type categories from a given set of (small) documents.\n+\n+    Example Output:\n+    \"entity_types\": ['military unit', 'organization', 'person', 'location', 'event', 'date', 'equipment']\n+    \"\"\"\n+    formatted_task = task.format(domain=domain)\n+\n+    docs_str = \"\\n\".join(docs) if isinstance(docs, list) else docs\n+\n+    entity_types_prompt = (\n+        ENTITY_TYPE_GENERATION_JSON_PROMPT\n+        if json_mode\n+        else ENTITY_TYPE_GENERATION_PROMPT\n+    ).format(task=formatted_task, input_text=docs_str)\n+\n+    history = [{\"role\": \"system\", \"content\": persona}]\n+\n+    if json_mode:\n+        response = await model.achat(\n+            entity_types_prompt,\n+            history=history,\n+            json=json_mode,\n+            json_model=EntityTypesResponse,\n+        )\n+        parsed_model = response.parsed_response\n+        return parsed_model.entity_types if parsed_model else []\n+\n+    response = await model.achat(entity_types_prompt, history=history, json=json_mode)\n+    return str(response.output.content)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/extract_graph_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/extract_graph_prompt.py b/packages/graphrag/graphrag/prompt_tune/generator/extract_graph_prompt.py\nnew file mode 100644\nindex 0000000..db0f87c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/extract_graph_prompt.py\n@@ -0,0 +1,109 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Entity Extraction prompt generator module.\"\"\"\n+\n+from pathlib import Path\n+\n+from graphrag.prompt_tune.template.extract_graph import (\n+    EXAMPLE_EXTRACTION_TEMPLATE,\n+    GRAPH_EXTRACTION_JSON_PROMPT,\n+    GRAPH_EXTRACTION_PROMPT,\n+    UNTYPED_EXAMPLE_EXTRACTION_TEMPLATE,\n+    UNTYPED_GRAPH_EXTRACTION_PROMPT,\n+)\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+EXTRACT_GRAPH_FILENAME = \"extract_graph.txt\"\n+\n+\n+def create_extract_graph_prompt(\n+    entity_types: str | list[str] | None,\n+    docs: list[str],\n+    examples: list[str],\n+    language: str,\n+    max_token_count: int,\n+    tokenizer: Tokenizer | None = None,\n+    json_mode: bool = False,\n+    output_path: Path | None = None,\n+    min_examples_required: int = 2,\n+) -> str:\n+    \"\"\"\n+    Create a prompt for entity extraction.\n+\n+    Parameters\n+    ----------\n+    - entity_types (str | list[str]): The entity types to extract\n+    - docs (list[str]): The list of documents to extract entities from\n+    - examples (list[str]): The list of examples to use for entity extraction\n+    - language (str): The language of the inputs and outputs\n+    - tokenizer (Tokenizer): The tokenizer to use for encoding and decoding text.\n+    - max_token_count (int): The maximum number of tokens to use for the prompt\n+    - json_mode (bool): Whether to use JSON mode for the prompt. Default is False\n+    - output_path (Path | None): The path to write the prompt to. Default is None.\n+        - min_examples_required (int): The minimum number of examples required. Default is 2.\n+\n+    Returns\n+    -------\n+    - str: The entity extraction prompt\n+    \"\"\"\n+    prompt = (\n+        (GRAPH_EXTRACTION_JSON_PROMPT if json_mode else GRAPH_EXTRACTION_PROMPT)\n+        if entity_types\n+        else UNTYPED_GRAPH_EXTRACTION_PROMPT\n+    )\n+    if isinstance(entity_types, list):\n+        entity_types = \", \".join(map(str, entity_types))\n+\n+    tokenizer = tokenizer or get_tokenizer()\n+\n+    tokens_left = (\n+        max_token_count\n+        - tokenizer.num_tokens(prompt)\n+        - tokenizer.num_tokens(entity_types)\n+        if entity_types\n+        else 0\n+    )\n+\n+    examples_prompt = \"\"\n+\n+    # Iterate over examples, while we have tokens left or examples left\n+    for i, output in enumerate(examples):\n+        input = docs[i]\n+        example_formatted = (\n+            EXAMPLE_EXTRACTION_TEMPLATE.format(\n+                n=i + 1, input_text=input, entity_types=entity_types, output=output\n+            )\n+            if entity_types\n+            else UNTYPED_EXAMPLE_EXTRACTION_TEMPLATE.format(\n+                n=i + 1, input_text=input, output=output\n+            )\n+        )\n+\n+        example_tokens = tokenizer.num_tokens(example_formatted)\n+\n+        # Ensure at least three examples are included\n+        if i >= min_examples_required and example_tokens > tokens_left:\n+            break\n+\n+        examples_prompt += example_formatted\n+        tokens_left -= example_tokens\n+\n+    prompt = (\n+        prompt.format(\n+            entity_types=entity_types, examples=examples_prompt, language=language\n+        )\n+        if entity_types\n+        else prompt.format(examples=examples_prompt, language=language)\n+    )\n+\n+    if output_path:\n+        output_path.mkdir(parents=True, exist_ok=True)\n+\n+        output_path = output_path / EXTRACT_GRAPH_FILENAME\n+        # Write file to output path\n+        with output_path.open(\"wb\") as file:\n+            file.write(prompt.encode(encoding=\"utf-8\", errors=\"strict\"))\n+\n+    return prompt\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/language.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/language.py b/packages/graphrag/graphrag/prompt_tune/generator/language.py\nnew file mode 100644\nindex 0000000..5c00fd6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/language.py\n@@ -0,0 +1,27 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Language detection for GraphRAG prompts.\"\"\"\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.prompt.language import DETECT_LANGUAGE_PROMPT\n+\n+\n+async def detect_language(model: ChatModel, docs: str | list[str]) -> str:\n+    \"\"\"Detect input language to use for GraphRAG prompts.\n+\n+    Parameters\n+    ----------\n+    - llm (CompletionLLM): The LLM to use for generation\n+    - docs (str | list[str]): The docs to detect language from\n+\n+    Returns\n+    -------\n+    - str: The detected language.\n+    \"\"\"\n+    docs_str = \" \".join(docs) if isinstance(docs, list) else docs\n+    language_prompt = DETECT_LANGUAGE_PROMPT.format(input_text=docs_str)\n+\n+    response = await model.achat(language_prompt)\n+\n+    return str(response.output.content)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/generator/persona.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/generator/persona.py b/packages/graphrag/graphrag/prompt_tune/generator/persona.py\nnew file mode 100644\nindex 0000000..b9bf485\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/generator/persona.py\n@@ -0,0 +1,27 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Persona generating module for fine-tuning GraphRAG prompts.\"\"\"\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompt_tune.defaults import DEFAULT_TASK\n+from graphrag.prompt_tune.prompt.persona import GENERATE_PERSONA_PROMPT\n+\n+\n+async def generate_persona(\n+    model: ChatModel, domain: str, task: str = DEFAULT_TASK\n+) -> str:\n+    \"\"\"Generate an LLM persona to use for GraphRAG prompts.\n+\n+    Parameters\n+    ----------\n+    - llm (CompletionLLM): The LLM to use for generation\n+    - domain (str): The domain to generate a persona for\n+    - task (str): The task to generate a persona for. Default is DEFAULT_TASK\n+    \"\"\"\n+    formatted_task = task.format(domain=domain)\n+    persona_prompt = GENERATE_PERSONA_PROMPT.format(sample_task=formatted_task)\n+\n+    response = await model.achat(persona_prompt)\n+\n+    return str(response.output.content)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/loader/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/loader/__init__.py b/packages/graphrag/graphrag/prompt_tune/loader/__init__.py\nnew file mode 100644\nindex 0000000..7c7a6f8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/loader/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning config and data loader module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/loader/input.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/loader/input.py b/packages/graphrag/graphrag/prompt_tune/loader/input.py\nnew file mode 100644\nindex 0000000..5e9fccb\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/loader/input.py\n@@ -0,0 +1,120 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Input loading module.\"\"\"\n+\n+import logging\n+from typing import Any\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.cache.noop_pipeline_cache import NoopPipelineCache\n+from graphrag.callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.index.input.factory import InputReaderFactory\n+from graphrag.index.operations.embed_text.run_embed_text import (\n+    run_embed_text,\n+)\n+from graphrag.index.workflows.create_base_text_units import create_base_text_units\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.prompt_tune.defaults import (\n+    LIMIT,\n+    N_SUBSET_MAX,\n+    K,\n+)\n+from graphrag.prompt_tune.types import DocSelectionType\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.utils.api import create_storage_from_config\n+\n+\n+def _sample_chunks_from_embeddings(\n+    text_chunks: pd.DataFrame,\n+    embeddings: np.ndarray[Any, np.dtype[np.float64]],\n+    k: int = K,\n+) -> pd.DataFrame:\n+    \"\"\"Sample text chunks from embeddings.\"\"\"\n+    center = np.mean(embeddings, axis=0)\n+    distances = np.linalg.norm(embeddings - center, axis=1)\n+    nearest_indices = np.argsort(distances)[:k]\n+\n+    return text_chunks.iloc[nearest_indices]\n+\n+\n+async def load_docs_in_chunks(\n+    config: GraphRagConfig,\n+    select_method: DocSelectionType,\n+    limit: int,\n+    logger: logging.Logger,\n+    chunk_size: int,\n+    overlap: int,\n+    n_subset_max: int = N_SUBSET_MAX,\n+    k: int = K,\n+) -> list[str]:\n+    \"\"\"Load docs into chunks for generating prompts.\"\"\"\n+    embeddings_llm_settings = config.get_language_model_config(\n+        config.embed_text.model_id\n+    )\n+    model = ModelManager().get_or_create_embedding_model(\n+        name=\"text_embedding\",\n+        model_type=embeddings_llm_settings.type,\n+        config=embeddings_llm_settings,\n+        callbacks=NoopWorkflowCallbacks(),\n+        cache=NoopPipelineCache(),\n+    )\n+    tokenizer = get_tokenizer(embeddings_llm_settings)\n+    input_storage = create_storage_from_config(config.input.storage)\n+    input_reader = InputReaderFactory().create(\n+        config.input.file_type,\n+        {\"storage\": input_storage, \"config\": config.input},\n+    )\n+    dataset = await input_reader.read_files()\n+    chunk_config = config.chunks\n+    chunks_df = create_base_text_units(\n+        documents=dataset,\n+        callbacks=NoopWorkflowCallbacks(),\n+        size=chunk_size,\n+        overlap=overlap,\n+        encoding_model=chunk_config.encoding_model,\n+        strategy=chunk_config.strategy,\n+        prepend_metadata=chunk_config.prepend_metadata,\n+        chunk_size_includes_metadata=chunk_config.chunk_size_includes_metadata,\n+    )\n+\n+    # Depending on the select method, build the dataset\n+    if limit <= 0 or limit > len(chunks_df):\n+        logger.warning(f\"Limit out of range, using default number of chunks: {LIMIT}\")  # noqa: G004\n+        limit = LIMIT\n+\n+    if select_method == DocSelectionType.TOP:\n+        chunks_df = chunks_df[:limit]\n+    elif select_method == DocSelectionType.RANDOM:\n+        chunks_df = chunks_df.sample(n=limit)\n+    elif select_method == DocSelectionType.AUTO:\n+        if k is None or k <= 0:\n+            msg = \"k must be an integer > 0\"\n+            raise ValueError(msg)\n+\n+        \"\"\"Convert text chunks into dense text embeddings.\"\"\"\n+        sampled_text_chunks = chunks_df.sample(n=min(n_subset_max, len(chunks_df)))[\n+            \"text\"\n+        ].tolist()\n+\n+        embedding_results = await run_embed_text(\n+            sampled_text_chunks,\n+            callbacks=NoopWorkflowCallbacks(),\n+            model=model,\n+            tokenizer=tokenizer,\n+            batch_size=config.embed_text.batch_size,\n+            batch_max_tokens=config.embed_text.batch_max_tokens,\n+            num_threads=embeddings_llm_settings.concurrent_requests,\n+        )\n+        embeddings = np.array(embedding_results.embeddings)\n+        chunks_df = _sample_chunks_from_embeddings(chunks_df, embeddings, k=k)\n+\n+    # Convert the dataset to list form, so we have a list of documents\n+    return [\n+        # need this to prevent the str.format() function from breaking when parsing LaTeX from markdown files\n+        i.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n+        for i in chunks_df[\"text\"]\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/__init__.py b/packages/graphrag/graphrag/prompt_tune/prompt/__init__.py\nnew file mode 100644\nindex 0000000..497c56d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Persona, entity type, relationships and domain generation prompts module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/community_report_rating.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/community_report_rating.py b/packages/graphrag/graphrag/prompt_tune/prompt/community_report_rating.py\nnew file mode 100644\nindex 0000000..b061645\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/community_report_rating.py\n@@ -0,0 +1,132 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine tuning prompts for Community Reports Rating.\"\"\"\n+\n+GENERATE_REPORT_RATING_PROMPT = \"\"\"\n+\n+You are a helpful agent tasked with rating the importance of a given text in the context of the provided domain and persona. Your goal is to provide a rating that reflects the relevance and significance of the text to the specified domain and persona. Use your expertise to evaluate the text based on the importance criteria and assign a float score between 0-10. Only respond with the text description of the importance criteria. Use the provided example data format to guide your response. Ignore the content of the example data and focus on the structure.\n+\n+######################\n+-Examples-\n+######################\n+\n+### Example 1\n+\n+# Domain\n+\n+Personal and Family Communication\n+\n+# Persona\n+\n+You are an expert in Social Network Analysis with a focus on the Personal and Family Communication domain. You are skilled at mapping and interpreting complex social networks, understanding the dynamics of interpersonal relationships, and identifying patterns of communication within communities. You are adept at helping people understand the structure and relations within their personal and family networks, providing insights into how information flows, how strong various connections are, and how these networks influence individual and group behavior.\n+\n+# Data\n+\n+\n+Subject: Re: Event\n+From: Alice Brown alice.brown@example.com\n+Date: 2012-11-14, 9:52 a.m.\n+To: John Smith john.smith@example.com\n+CC: Jane Doe jane.doe@example.com, Bob Johnson bob.johnson@example.com, Emma Davis emma.davis@example.com\n+\n+The event is at 6pm at City Hall (Queen street) event chamber. We\n+just need to get there by 5:45pm. It is 30-minute long so we will be\n+done by 6:30pm. We'll then head over to New Sky on Spadina for some\n+unique cuisine!\n+\n+Guests are you and Emma, and my uncle and auntie from London\n+who my folks have designated to act as their reps. Jane and Joe are\n+witnesses.\n+\n+Be there or be square!\n+Alice\n+\n+On Wed, Nov 14, 2012 at 9:40 AM, John Smith john.smith@example.com wrote:\n+\n+Thats the day after Bob's event!\n+Any more details on the event schedule? ITS NEXT WEEK!\n+On Tue, Nov 13, 2012 at 7:51 PM, Jane Doe\n+jane.doe@example.com wrote:\n+I am supposed to forward you the invitation to this year's celebration.\n+Date: Saturday, Nov. 24, 6 pm starting\n+Place as usual: Dean's house, 6 Cardish, Kleinburg L0J 1C0\n+Jane Doe\n+jane.doe@example.com\n+\n+# Importance Criteria\n+\n+A float score between 0-10 that represents the relevance of the email's content to family communication, health concerns, travel plans, and interpersonal dynamics, with 1 being trivial or spam and 10 being highly relevant, urgent, and impactful to family cohesion or well-being.\n+#############################\n+\n+### Example 2\n+\n+# Domain\n+\n+Literary Analysis\n+\n+# Persona\n+\n+You are a literary scholar with a focus on works from the 19th century. You are skilled at analyzing and interpreting texts, identifying themes and motifs, and understanding the historical and cultural contexts in which these works were written. You are adept at helping people understand the deeper meanings and significance of literary works, providing insights into the author's intentions, the social issues addressed in the text, and the impact of these works on contemporary society.\n+\n+# Data\n+\n+Had she found Jane in any apparent danger, Mrs. Bennet would have been very miserable; but being satisfied on seeing her that her illness was not alarming, she had no wish of her recovering immediately, as her restoration to health would probably remove her from Netherfield. She would not listen, therefore, to her daughter's proposal of being carried home; neither did the apothecary, who arrived about the same time, think it at all advisable. After sitting a little with Jane, on Miss Bingley's appearance and invitation, the mother and three daughters all attended her into the breakfast parlor. Bingley met them with hopes that Mrs. Bennet had not found Miss Bennet worse than she expected.\n+\n+\"Indeed I have, Sir,\" was her answer. \"She is a great deal too ill to be moved. Mr. Jones says we must not think of moving her. We must trespass a little longer on your kindness.\"\n+\n+\"Removed!\" cried Bingley. \"It must not be thought of. My sister, I am sure, will not hear of her removal.\"\n+\n+# Importance Criteria\n+\n+A float score between 0-10 that represents the relevance of the text to literary analysis, historical context, thematic interpretation, and cultural significance, with 1 being trivial or irrelevant and 10 being highly significant, profound, and impactful to the understanding of the text and its implications.\n+#############################\n+\n+### Example 3\n+\n+# Domain\n+\n+Environmental Science\n+\n+# Persona\n+\n+You are an environmental scientist with a focus on climate change and sustainability. You are skilled at analyzing data, interpreting social commentary and recommending policy changes. You are adept at helping people understand the causes and consequences of climate change, providing insights into how they can reduce their carbon footprint, adopt sustainable practices, and contribute to a healthier planet.\n+\n+# Data\n+\n+Host 1 (Anna): Welcome to \"Green Living Today,\" the podcast where we explore practical tips and inspiring stories about sustainable living. I'm your host, Anna Green.\n+\n+Host 2 (Mark): And I'm Mark Smith. Today, we have a special episode focused on reducing plastic waste in our daily lives. We'll be talking to a special guest who has made significant strides in living a plastic-free lifestyle.\n+\n+Anna: That's right, Mark. Our guest today is Laura Thompson, the founder of \"Plastic-Free Living,\" a blog dedicated to sharing tips and resources for reducing plastic use. Welcome to the show, Laura!\n+\n+Guest (Laura): Thanks, Anna and Mark. It's great to be here.\n+\n+Mark: Laura, let's start by talking about your journey. What inspired you to start living a plastic-free lifestyle?\n+\n+# Importance Criteria\n+\n+A float score between 0-10 that represents the relevance of the text to sustainability, plastic waste reduction, and environmental policies, with 1 being trivial or irrelevant and 10 being highly significant, impactful, and actionable in promoting environmental awareness.\n+#############################\n+\n+\n+#############################\n+-Real Data-\n+#############################\n+\n+# Domain\n+\n+{domain}\n+\n+# Persona\n+\n+{persona}\n+\n+# Data\n+\n+{input_text}\n+\n+# Importance Criteria\n+\n+\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/community_reporter_role.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/community_reporter_role.py b/packages/graphrag/graphrag/prompt_tune/prompt/community_reporter_role.py\nnew file mode 100644\nindex 0000000..b667bc2\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/community_reporter_role.py\n@@ -0,0 +1,20 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for community reporter role generation.\"\"\"\n+\n+GENERATE_COMMUNITY_REPORTER_ROLE_PROMPT = \"\"\"\n+{persona}\n+Given a sample text, help the user by creating a role definition that will be tasked with community analysis.\n+Take a look at this example, determine its key parts, and using the domain provided and your expertise, create a new role definition for the provided inputs that follows the same pattern as the example.\n+Remember, your output should look just like the provided example in structure and content.\n+\n+Example:\n+A technologist reporter that is analyzing Kevin Scott's \"Behind the Tech Podcast\", given a list of entities\n+that belong to the community as well as their relationships and optional associated claims.\n+The report will be used to inform decision-makers about significant developments associated with the community and their potential impact.\n+\n+\n+Domain: {domain}\n+Text: {input_text}\n+Role:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/domain.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/domain.py b/packages/graphrag/graphrag/prompt_tune/prompt/domain.py\nnew file mode 100644\nindex 0000000..4b4587f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/domain.py\n@@ -0,0 +1,12 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for domain generation.\"\"\"\n+\n+GENERATE_DOMAIN_PROMPT = \"\"\"\n+You are an intelligent assistant that helps a human to analyze the information in a text document.\n+Given a sample text, help the user by assigning a descriptive domain that summarizes what the text is about.\n+Example domains are: \"Social studies\", \"Algorithmic analysis\", \"Medical science\", among others.\n+\n+Text: {input_text}\n+Domain:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/entity_relationship.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/entity_relationship.py b/packages/graphrag/graphrag/prompt_tune/prompt/entity_relationship.py\nnew file mode 100644\nindex 0000000..ec7dca5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/entity_relationship.py\n@@ -0,0 +1,355 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for entity relationship generation.\"\"\"\n+\n+ENTITY_RELATIONSHIPS_GENERATION_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n+\n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: One of the following types: [{entity_types}]\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+Format each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n+\n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\n+Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n+\n+3. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use ## as the list delimiter.\n+\n+4. If you have to translate into {language}, just translate the descriptions, nothing else!\n+\n+5. When finished, output <|COMPLETE|>.\n+\n+######################\n+-Examples-\n+######################\n+Example 1:\n+Entity_types: ORGANIZATION,PERSON\n+Text:\n+The Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n+######################\n+Output:\n+(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n+##\n+(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n+##\n+(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n+##\n+(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n+<|COMPLETE|>\n+\n+######################\n+Example 2:\n+Entity_types: ORGANIZATION\n+Text:\n+TechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n+\n+TechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n+######################\n+Output:\n+(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n+##\n+(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n+##\n+(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n+<|COMPLETE|>\n+\n+######################\n+Example 3:\n+Entity_types: ORGANIZATION,GEO,PERSON\n+Text:\n+Five Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n+\n+The swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n+\n+The exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n+\n+They were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n+\n+The Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n+######################\n+Output:\n+(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n+##\n+(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n+##\n+(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n+##\n+##\n+(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n+##\n+(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n+##\n+(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n+##\n+(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n+##\n+(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n+##\n+(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n+##\n+(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n+##\n+(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n+##\n+(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n+##\n+(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n+##\n+(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n+##\n+(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n+<|COMPLETE|>\n+\n+-Real Data-\n+######################\n+entity_types: {entity_types}\n+text: {input_text}\n+######################\n+output:\n+\"\"\"\n+\n+ENTITY_RELATIONSHIPS_GENERATION_JSON_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n+\n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: One of the following types: [{entity_types}]\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+\n+Format each entity output as a JSON entry with the following format:\n+\n+{{\"name\": <entity name>, \"type\": <type>, \"description\": <entity description>}}\n+\n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\n+\n+Format each relationship as a JSON entry with the following format:\n+\n+{{\"source\": <source_entity>, \"target\": <target_entity>, \"relationship\": <relationship_description>, \"relationship_strength\": <relationship_strength>}}\n+\n+3. Return output in {language} as a single list of all JSON entities and relationships identified in steps 1 and 2.\n+\n+4. If you have to translate into {language}, just translate the descriptions, nothing else!\n+\n+######################\n+-Examples-\n+######################\n+Example 1:\n+Text:\n+The Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n+######################\n+Output:\n+[\n+  {{\"name\": \"CENTRAL INSTITUTION\", \"type\": \"ORGANIZATION\", \"description\": \"The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday\"}},\n+  {{\"name\": \"MARTIN SMITH\", \"type\": \"PERSON\", \"description\": \"Martin Smith is the chair of the Central Institution\"}},\n+  {{\"name\": \"MARKET STRATEGY COMMITTEE\", \"type\": \"ORGANIZATION\", \"description\": \"The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply\"}},\n+  {{\"source\": \"MARTIN SMITH\", \"target\": \"CENTRAL INSTITUTION\", \"relationship\": \"Martin Smith is the Chair of the Central Institution and will answer questions at a press conference\", \"relationship_strength\": 9}}\n+]\n+\n+######################\n+Example 2:\n+Text:\n+TechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n+\n+TechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n+######################\n+Output:\n+[\n+  {{\"name\": \"TECHGLOBAL\", \"type\": \"ORGANIZATION\", \"description\": \"TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones\"}},\n+  {{\"name\": \"VISION HOLDINGS\", \"type\": \"ORGANIZATION\", \"description\": \"Vision Holdings is a firm that previously owned TechGlobal\"}},\n+  {{\"source\": \"TECHGLOBAL\", \"target\": \"VISION HOLDINGS\", \"relationship\": \"Vision Holdings formerly owned TechGlobal from 2014 until present\", \"relationship_strength\": 5}}\n+]\n+\n+######################\n+Example 3:\n+Text:\n+Five Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n+\n+The swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n+\n+The exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n+\n+They were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n+\n+The Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n+######################\n+Output:\n+[\n+  {{\"name\": \"FIRUZABAD\", \"type\": \"GEO\", \"description\": \"Firuzabad held Aurelians as hostages\"}},\n+  {{\"name\": \"AURELIA\", \"type\": \"GEO\", \"description\": \"Country seeking to release hostages\"}},\n+  {{\"name\": \"QUINTARA\", \"type\": \"GEO\", \"description\": \"Country that negotiated a swap of money in exchange for hostages\"}},\n+  {{\"name\": \"TIRUZIA\", \"type\": \"GEO\", \"description\": \"Capital of Firuzabad where the Aurelians were being held\"}},\n+  {{\"name\": \"KROHAARA\", \"type\": \"GEO\", \"description\": \"Capital city in Quintara\"}},\n+  {{\"name\": \"CASHION\", \"type\": \"GEO\", \"description\": \"Capital city in Aurelia\"}},\n+  {{\"name\": \"SAMUEL NAMARA\", \"type\": \"PERSON\", \"description\": \"Aurelian who spent time in Tiruzia's Alhamia Prison\"}},\n+  {{\"name\": \"ALHAMIA PRISON\", \"type\": \"GEO\", \"description\": \"Prison in Tiruzia\"}},\n+  {{\"name\": \"DURKE BATAGLANI\", \"type\": \"PERSON\", \"description\": \"Aurelian journalist who was held hostage\"}},\n+  {{\"name\": \"MEGGIE TAZBAH\", \"type\": \"PERSON\", \"description\": \"Bratinas national and environmentalist who was held hostage\"}},\n+  {{\"source\": \"FIRUZABAD\", \"target\": \"AURELIA\", \"relationship\": \"Firuzabad negotiated a hostage exchange with Aurelia\", \"relationship_strength\": 2}},\n+  {{\"source\": \"QUINTARA\", \"target\": \"AURELIA\", \"relationship\": \"Quintara brokered the hostage exchange between Firuzabad and Aurelia\", \"relationship_strength\": 2}},\n+  {{\"source\": \"QUINTARA\", \"target\": \"FIRUZABAD\", \"relationship\": \"Quintara brokered the hostage exchange between Firuzabad and Aurelia\", \"relationship_strength\": 2}},\n+  {{\"source\": \"SAMUEL NAMARA\", \"target\": \"ALHAMIA PRISON\", \"relationship\": \"Samuel Namara was a prisoner at Alhamia prison\", \"relationship_strength\": 8}},\n+  {{\"source\": \"SAMUEL NAMARA\", \"target\": \"MEGGIE TAZBAH\", \"relationship\": \"Samuel Namara and Meggie Tazbah were exchanged in the same hostage release\", \"relationship_strength\": 2}},\n+  {{\"source\": \"SAMUEL NAMARA\", \"target\": \"DURKE BATAGLANI\", \"relationship\": \"Samuel Namara and Durke Bataglani were exchanged in the same hostage release\", \"relationship_strength\": 2}},\n+  {{\"source\": \"MEGGIE TAZBAH\", \"target\": \"DURKE BATAGLANI\", \"relationship\": \"Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release\", \"relationship_strength\": 2}},\n+  {{\"source\": \"SAMUEL NAMARA\", \"target\": \"FIRUZABAD\", \"relationship\": \"Samuel Namara was a hostage in Firuzabad\", \"relationship_strength\": 2}},\n+  {{\"source\": \"MEGGIE TAZBAH\", \"target\": \"FIRUZABAD\", \"relationship\": \"Meggie Tazbah was a hostage in Firuzabad\", \"relationship_strength\": 2}},\n+  {{\"source\": \"DURKE BATAGLANI\", \"target\": \"FIRUZABAD\", \"relationship\": \"Durke Bataglani was a hostage in Firuzabad\", \"relationship_strength\": 2}}\n+]\n+\n+\n+\n+-Real Data-\n+######################\n+entity_types: {entity_types}\n+text: {input_text}\n+######################\n+output:\n+\"\"\"\n+\n+UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.\n+Next, report all relationships among the identified entities.\n+\n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+Format each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n+\n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n+Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n+\n+3. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n+\n+4. If you have to translate into {language}, just translate the descriptions, nothing else!\n+\n+5. When finished, output <|COMPLETE|>.\n+\n+######################\n+-Examples-\n+######################\n+Example 1:\n+Text:\n+The Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n+######################\n+Output:\n+(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n+##\n+(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n+##\n+(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n+##\n+(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n+<|COMPLETE|>\n+\n+######################\n+Example 2:\n+Text:\n+TechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n+\n+TechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n+######################\n+Output:\n+(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n+##\n+(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n+##\n+(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n+<|COMPLETE|>\n+\n+######################\n+Example 3:\n+Text:\n+Five Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n+\n+The swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n+\n+The exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n+\n+They were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n+\n+The Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n+######################\n+Output:\n+(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n+##\n+(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n+##\n+(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n+##\n+##\n+(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n+##\n+(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n+##\n+(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n+##\n+(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n+##\n+(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n+##\n+(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n+##\n+(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n+##\n+(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n+##\n+(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n+##\n+(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n+##\n+(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n+##\n+(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n+<|COMPLETE|>\n+\n+######################\n+-Real Data-\n+######################\n+Text: {input_text}\n+######################\n+Output:\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/entity_types.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/entity_types.py b/packages/graphrag/graphrag/prompt_tune/prompt/entity_types.py\nnew file mode 100644\nindex 0000000..ff3e799\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/entity_types.py\n@@ -0,0 +1,89 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for entity types generation.\"\"\"\n+\n+ENTITY_TYPE_GENERATION_PROMPT = \"\"\"\n+The goal is to study the connections and relations between the entity types and their features in order to understand all available information from the text.\n+The user's task is to {task}.\n+As part of the analysis, you want to identify the entity types present in the following text.\n+The entity types must be relevant to the user task.\n+Avoid general entity types such as \"other\" or \"unknown\".\n+This is VERY IMPORTANT: Do not generate redundant or overlapping entity types. For example, if the text contains \"company\" and \"organization\" entity types, you should return only one of them.\n+Don't worry about quantity, always choose quality over quantity. And make sure EVERYTHING in your answer is relevant to the context of entity extraction.\n+And remember, it is ENTITY TYPES what we need.\n+Return the entity types in as a list of comma sepparated of strings.\n+=====================================================================\n+EXAMPLE SECTION: The following section includes example output. These examples **must be excluded from your answer**.\n+\n+EXAMPLE 1\n+Task: Determine the connections and organizational hierarchy within the specified community.\n+Text: Example_Org_A is a company in Sweden. Example_Org_A's director is Example_Individual_B.\n+RESPONSE:\n+organization, person\n+END OF EXAMPLE 1\n+\n+EXAMPLE 2\n+Task: Identify the key concepts, principles, and arguments shared among different philosophical schools of thought, and trace the historical or ideological influences they have on each other.\n+Text: Rationalism, epitomized by thinkers such as Ren\u00e9 Descartes, holds that reason is the primary source of knowledge. Key concepts within this school include the emphasis on the deductive method of reasoning.\n+RESPONSE:\n+concept, person, school of thought\n+END OF EXAMPLE 2\n+\n+EXAMPLE 3\n+Task: Identify the full range of basic forces, factors, and trends that would indirectly shape an issue.\n+Text: Industry leaders such as Panasonic are vying for supremacy in the battery production sector. They are investing heavily in research and development and are exploring new technologies to gain a competitive edge.\n+RESPONSE:\n+organization, technology, sectors, investment strategies\n+END OF EXAMPLE 3\n+======================================================================\n+\n+======================================================================\n+REAL DATA: The following section is the real data. You should use only this real data to prepare your answer. Generate Entity Types only.\n+Task: {task}\n+Text: {input_text}\n+RESPONSE:\n+{{<entity_types>}}\n+\"\"\"\n+\n+ENTITY_TYPE_GENERATION_JSON_PROMPT = \"\"\"\n+The goal is to study the connections and relations between the entity types and their features in order to understand all available information from the text.\n+The user's task is to {task}.\n+As part of the analysis, you want to identify the entity types present in the following text.\n+The entity types must be relevant to the user task.\n+Avoid general entity types such as \"other\" or \"unknown\".\n+This is VERY IMPORTANT: Do not generate redundant or overlapping entity types. For example, if the text contains \"company\" and \"organization\" entity types, you should return only one of them.\n+Don't worry about quantity, always choose quality over quantity. And make sure EVERYTHING in your answer is relevant to the context of entity extraction.\n+Return the entity types in JSON format with \"entities\" as the key and the entity types as an array of strings.\n+=====================================================================\n+EXAMPLE SECTION: The following section includes example output. These examples **must be excluded from your answer**.\n+\n+EXAMPLE 1\n+Task: Determine the connections and organizational hierarchy within the specified community.\n+Text: Example_Org_A is a company in Sweden. Example_Org_A's director is Example_Individual_B.\n+JSON RESPONSE:\n+{{\"entity_types\": [organization, person] }}\n+END OF EXAMPLE 1\n+\n+EXAMPLE 2\n+Task: Identify the key concepts, principles, and arguments shared among different philosophical schools of thought, and trace the historical or ideological influences they have on each other.\n+Text: Rationalism, epitomized by thinkers such as Ren\u00e9 Descartes, holds that reason is the primary source of knowledge. Key concepts within this school include the emphasis on the deductive method of reasoning.\n+JSON RESPONSE:\n+{{\"entity_types\": [concept, person, school of thought] }}\n+END OF EXAMPLE 2\n+\n+EXAMPLE 3\n+Task: Identify the full range of basic forces, factors, and trends that would indirectly shape an issue.\n+Text: Industry leaders such as Panasonic are vying for supremacy in the battery production sector. They are investing heavily in research and development and are exploring new technologies to gain a competitive edge.\n+JSON RESPONSE:\n+{{\"entity_types\": [organization, technology, sectors, investment strategies] }}\n+END OF EXAMPLE 3\n+======================================================================\n+\n+======================================================================\n+REAL DATA: The following section is the real data. You should use only this real data to prepare your answer. Generate Entity Types only.\n+Task: {task}\n+Text: {input_text}\n+JSON response format:\n+{{\"entity_types\": [<entity_types>] }}\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/language.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/language.py b/packages/graphrag/graphrag/prompt_tune/prompt/language.py\nnew file mode 100644\nindex 0000000..b8425e6\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/language.py\n@@ -0,0 +1,12 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for language detection.\"\"\"\n+\n+DETECT_LANGUAGE_PROMPT = \"\"\"\n+You are an intelligent assistant that helps a human to analyze the information in a text document.\n+Given a sample text, help the user by determining what's the primary language of the provided texts.\n+Examples are: \"English\", \"Spanish\", \"Japanese\", \"Portuguese\" among others. Reply ONLY with the language name.\n+\n+Text: {input_text}\n+Language:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/prompt/persona.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/prompt/persona.py b/packages/graphrag/graphrag/prompt_tune/prompt/persona.py\nnew file mode 100644\nindex 0000000..58515fd\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/prompt/persona.py\n@@ -0,0 +1,13 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for persona generation.\"\"\"\n+\n+GENERATE_PERSONA_PROMPT = \"\"\"\n+You are an intelligent assistant that helps a human to analyze the information in a text document.\n+Given a specific type of task and sample text, help the user by generating a 3 to 4 sentence description of an expert who could help solve the problem.\n+Use a format similar to the following:\n+You are an expert {{role}}. You are skilled at {{relevant skills}}. You are adept at helping people with {{specific task}}.\n+\n+task: {sample_task}\n+persona description:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/template/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/template/__init__.py b/packages/graphrag/graphrag/prompt_tune/template/__init__.py\nnew file mode 100644\nindex 0000000..f830ce2\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/template/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for entity extraction, entity summarization, and community report summarization.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/template/community_report_summarization.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/template/community_report_summarization.py b/packages/graphrag/graphrag/prompt_tune/template/community_report_summarization.py\nnew file mode 100644\nindex 0000000..b0d037a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/template/community_report_summarization.py\n@@ -0,0 +1,105 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for community report summarization.\"\"\"\n+\n+COMMUNITY_REPORT_SUMMARIZATION_PROMPT = \"\"\"\n+{persona}\n+\n+# Goal\n+Write a comprehensive assessment report of a community taking on the role of a {role}. The content of this report includes an overview of the community's key entities and relationships.\n+\n+# Report Structure\n+The report should include the following sections:\n+- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n+- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n+- REPORT RATING: {report_rating_description}\n+- RATING EXPLANATION: Give a single sentence explanation of the rating.\n+- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n+\n+Return output as a well-formed JSON-formatted string with the following format. Don't use any unnecessary escape sequences. The output should be a single JSON object that can be parsed by json.loads.\n+    {{{{\n+        \"title\": <report_title>,\n+        \"summary\": <executive_summary>,\n+        \"rating\": <impact_severity_rating>,\n+        \"rating_explanation\": <rating_explanation>,\n+        \"findings\": [\n+            {{{{\n+                \"summary\":<insight_1_summary>,\n+                \"explanation\": <insight_1_explanation>\n+            }}}},\n+            {{{{\n+                \"summary\":<insight_2_summary>,\n+                \"explanation\": <insight_2_explanation>\n+            }}}}\n+        ]\n+    }}}}\n+\n+# Grounding Rules\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n+\n+where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+Your answers should be in {language}.\n+\n+# Example Input\n+-----------\n+Text:\n+\n+Entities\n+\n+id,entity,description\n+5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n+6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n+\n+Relationships\n+\n+id,source,target,description\n+37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n+38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n+39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n+40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n+41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n+43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n+\n+Output:\n+{{{{\n+    \"title\": \"Verdant Oasis Plaza and Unity March\",\n+    \"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.\",\n+    \"rating\": 5.0,\n+    \"rating_explanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.\",\n+    \"findings\": [\n+        {{{{\n+            \"summary\": \"Verdant Oasis Plaza as the central location\",\n+            \"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza's association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]\"\n+        }}}},\n+        {{{{\n+            \"summary\": \"Harmony Assembly's role in the community\",\n+            \"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]\"\n+        }}}},\n+        {{{{\n+            \"summary\": \"Unity March as a significant event\",\n+            \"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community's dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n+        }}}},\n+        {{{{\n+            \"summary\": \"Role of Tribune Spotlight\",\n+            \"explanation\": \"Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]\"\n+        }}}}\n+    ]\n+}}}}\n+\n+# Real Data\n+\n+Use the following text for your answer. Do not make anything up in your answer.\n+\n+Text:\n+{{input_text}}\n+Output:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/template/entity_summarization.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/template/entity_summarization.py b/packages/graphrag/graphrag/prompt_tune/template/entity_summarization.py\nnew file mode 100644\nindex 0000000..7710ba0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/template/entity_summarization.py\n@@ -0,0 +1,22 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for entity summarization.\"\"\"\n+\n+ENTITY_SUMMARIZATION_PROMPT = \"\"\"\n+{persona}\n+Using your expertise, you're asked to generate a comprehensive summary of the data provided below.\n+Given one or two entities, and a list of descriptions, all related to the same entity or group of entities.\n+Please concatenate all of these into a single, concise description in {language}. Make sure to include information collected from all the descriptions.\n+If the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\n+Make sure it is written in third person, and include the entity names so we have the full context.\n+\n+Enrich it as much as you can with relevant information from the nearby text, this is very important.\n+\n+If no answer is possible, or the description is empty, only convey information that is provided within the text.\n+#######\n+-Data-\n+Entities: {{entity_name}}\n+Description List: {{description_list}}\n+#######\n+Output:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/template/extract_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/template/extract_graph.py b/packages/graphrag/graphrag/prompt_tune/template/extract_graph.py\nnew file mode 100644\nindex 0000000..58a095c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/template/extract_graph.py\n@@ -0,0 +1,141 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Fine-tuning prompts for entity extraction.\"\"\"\n+\n+GRAPH_EXTRACTION_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n+\n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: One of the following types: [{entity_types}]\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+Format each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n+\n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\n+Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n+\n+3. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n+\n+4. If you have to translate into {language}, just translate the descriptions, nothing else!\n+\n+5. When finished, output <|COMPLETE|>.\n+\n+-Examples-\n+######################\n+{examples}\n+\n+-Real Data-\n+######################\n+entity_types: [{entity_types}]\n+text: {{input_text}}\n+######################\n+output:\"\"\"\n+\n+GRAPH_EXTRACTION_JSON_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n+\n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: One of the following types: [{entity_types}]\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+Format each entity output as a JSON entry with the following format:\n+\n+{{\"name\": <entity name>, \"type\": <type>, \"description\": <entity description>}}\n+\n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\n+Format each relationship as a JSON entry with the following format:\n+\n+{{\"source\": <source_entity>, \"target\": <target_entity>, \"relationship\": <relationship_description>, \"relationship_strength\": <relationship_strength>}}\n+\n+3. Return output in {language} as a single list of all JSON entities and relationships identified in steps 1 and 2.\n+\n+4. If you have to translate into {language}, just translate the descriptions, nothing else!\n+\n+-Examples-\n+######################\n+{examples}\n+\n+-Real Data-\n+######################\n+entity_types: {entity_types}\n+text: {{input_text}}\n+######################\n+output:\"\"\"\n+\n+EXAMPLE_EXTRACTION_TEMPLATE = \"\"\"\n+Example {n}:\n+\n+entity_types: [{entity_types}]\n+text:\n+{input_text}\n+------------------------\n+output:\n+{output}\n+#############################\n+\n+\"\"\"\n+\n+UNTYPED_EXAMPLE_EXTRACTION_TEMPLATE = \"\"\"\n+Example {n}:\n+\n+text:\n+{input_text}\n+------------------------\n+output:\n+{output}\n+#############################\n+\n+\"\"\"\n+\n+\n+UNTYPED_GRAPH_EXTRACTION_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.\n+Next, report all relationships among the identified entities.\n+\n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: Suggest several labels or categories for the entity. The categories should not be specific, but should be as general as possible.\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+Format each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n+\n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n+Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n+\n+3. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n+\n+4. If you have to translate into {language}, just translate the descriptions, nothing else!\n+\n+5. When finished, output <|COMPLETE|>.\n+\n+-Examples-\n+######################\n+{examples}\n+\n+-Real Data-\n+######################\n+text: {{input_text}}\n+######################\n+output:\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompt_tune/types.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompt_tune/types.py b/packages/graphrag/graphrag/prompt_tune/types.py\nnew file mode 100644\nindex 0000000..f3df632\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompt_tune/types.py\n@@ -0,0 +1,19 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Types for prompt tuning.\"\"\"\n+\n+from enum import Enum\n+\n+\n+class DocSelectionType(str, Enum):\n+    \"\"\"The type of document selection to use.\"\"\"\n+\n+    ALL = \"all\"\n+    RANDOM = \"random\"\n+    TOP = \"top\"\n+    AUTO = \"auto\"\n+\n+    def __str__(self):\n+        \"\"\"Return the string representation of the enum value.\"\"\"\n+        return self.value\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/__init__.py b/packages/graphrag/graphrag/prompts/__init__.py\nnew file mode 100644\nindex 0000000..3bb0594\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"All prompts for the GraphRAG system.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/index/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/index/__init__.py b/packages/graphrag/graphrag/prompts/index/__init__.py\nnew file mode 100644\nindex 0000000..f7216c0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/index/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"All prompts for the indexing engine.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/index/community_report.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/index/community_report.py b/packages/graphrag/graphrag/prompts/index/community_report.py\nnew file mode 100644\nindex 0000000..c3a7702\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/index/community_report.py\n@@ -0,0 +1,153 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"A file containing prompts definition.\"\"\"\n+\n+COMMUNITY_REPORT_PROMPT = \"\"\"\n+You are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n+\n+# Goal\n+Write a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community's key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n+\n+# Report Structure\n+\n+The report should include the following sections:\n+\n+- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n+- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n+- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n+- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n+- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n+\n+Return output as a well-formed JSON-formatted string with the following format:\n+    {{\n+        \"title\": <report_title>,\n+        \"summary\": <executive_summary>,\n+        \"rating\": <impact_severity_rating>,\n+        \"rating_explanation\": <rating_explanation>,\n+        \"findings\": [\n+            {{\n+                \"summary\":<insight_1_summary>,\n+                \"explanation\": <insight_1_explanation>\n+            }},\n+            {{\n+                \"summary\":<insight_2_summary>,\n+                \"explanation\": <insight_2_explanation>\n+            }}\n+        ]\n+    }}\n+\n+# Grounding Rules\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n+\n+where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+Limit the total report length to {max_report_length} words.\n+\n+# Example Input\n+-----------\n+Text:\n+\n+Entities\n+\n+id,entity,description\n+5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n+6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n+\n+Relationships\n+\n+id,source,target,description\n+37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n+38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n+39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n+40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n+41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n+43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n+\n+Output:\n+{{\n+    \"title\": \"Verdant Oasis Plaza and Unity March\",\n+    \"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.\",\n+    \"rating\": 5.0,\n+    \"rating_explanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.\",\n+    \"findings\": [\n+        {{\n+            \"summary\": \"Verdant Oasis Plaza as the central location\",\n+            \"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza's association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]\"\n+        }},\n+        {{\n+            \"summary\": \"Harmony Assembly's role in the community\",\n+            \"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]\"\n+        }},\n+        {{\n+            \"summary\": \"Unity March as a significant event\",\n+            \"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community's dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n+        }},\n+        {{\n+            \"summary\": \"Role of Tribune Spotlight\",\n+            \"explanation\": \"Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]\"\n+        }}\n+    ]\n+}}\n+\n+\n+# Real Data\n+\n+Use the following text for your answer. Do not make anything up in your answer.\n+\n+Text:\n+{input_text}\n+\n+The report should include the following sections:\n+\n+- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n+- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n+- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n+- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n+- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n+\n+Return output as a well-formed JSON-formatted string with the following format:\n+    {{\n+        \"title\": <report_title>,\n+        \"summary\": <executive_summary>,\n+        \"rating\": <impact_severity_rating>,\n+        \"rating_explanation\": <rating_explanation>,\n+        \"findings\": [\n+            {{\n+                \"summary\":<insight_1_summary>,\n+                \"explanation\": <insight_1_explanation>\n+            }},\n+            {{\n+                \"summary\":<insight_2_summary>,\n+                \"explanation\": <insight_2_explanation>\n+            }}\n+        ]\n+    }}\n+\n+# Grounding Rules\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n+\n+where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+Limit the total report length to {max_report_length} words.\n+\n+Output:\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/index/community_report_text_units.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/index/community_report_text_units.py b/packages/graphrag/graphrag/prompts/index/community_report_text_units.py\nnew file mode 100644\nindex 0000000..47fcd29\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/index/community_report_text_units.py\n@@ -0,0 +1,95 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A file containing prompts definition.\"\"\"\n+\n+COMMUNITY_REPORT_TEXT_PROMPT = \"\"\"\n+You are an AI assistant that helps a human analyst to perform general information discovery.\n+Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n+\n+# Goal\n+Write a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims.\n+The report will be used to inform decision-makers about information associated with the community and their potential impact.\n+The content of this report includes an overview of the community's key entities, their core attributes or capabilities, their connections, and noteworthy claims.\n+Retain as much time specific information as possible so your end user can build a timeline of events.\n+\n+# Report Structure\n+The report should include the following sections:\n+- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title. Avoid including phrases like 'eligibility assessment' or 'eligibility assessment report' in the title.\n+- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant program-specific or eligibility-related insights.\n+- IMPORTANCE RATING: A float score between 0-10 that represents the importance of entities within the community..\n+- RATING EXPLANATION: Give a single sentence explanation of the importance rating.\n+- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n+- DATE RANGE: A range of dates (YYYY-MM-DD) with the format [START, END] which corresponds to the date range of text units and intermediate reports used to build the report.\n+\n+Return output as a well-formed JSON-formatted string with the following format. Don't use any unnecessary escape sequences. The output should be a single JSON object that can be parsed by json.loads.\n+    {{\n+        \"title\": \"<report_title>\",\n+        \"summary\": \"<executive_summary>\",\n+        \"rating\": <importance_rating>,\n+        \"rating_explanation\": \"<rating_explanation>\",\n+        \"findings\": [{{\"summary\":\"<insight_1_summary>\", \"explanation\": \"<insight_1_explanation\"}}, {{\"summary\":\"<insight_2_summary>\", \"explanation\": \"<insight_2_explanation\"}}],\n+\t\t\"date_range\": [\"<date range start>\", \"<date range end>\"],\n+\n+    }}\n+\n+# Grounding Rules\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids), <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\"Person X resolved a major issue with project Y [Data: Sources (1, 5),  Date_Range ((2001, 05, 12), (2001, 05, 14))]. He also made major updates to the database of app Y [Data: Reports (2, 4), Sources (7, 23, 2, 34, 46, +more), Date_Range ((2001, 05, 15), (2001, 05, 18))\"\"\n+\n+where 1, 2, 4, 5, 7, 23, 2, 34, and 46 represent the id (not the index) of the relevant data record.\n+\n+Limit the total report length to {max_report_length} words.\n+\n+# Example Input\n+-----------\n+SOURCES\n+id, text\n+1, Text: From: compliance.office@enron.com To: management.team@enron.com Cc: legal.team@enron.com, risk@enron.com Date: Wed, 12 Jul 2000 08:30:00 -0600 (CST) Subject: Quick Update on Compliance & Risk Efforts\n+2, Quick update on what's been cooking in Compliance and Risk Management. Risk Management is stepping up \u2014 They've been tightening up on our financial risk assessments and mitigation strategies since early this year.\n+3, Their efforts are key to keeping us on solid ground financially and in compliance with the latest market regulations as of mid-2000. It's crucial for our strategic planning and helps us stay ahead.\n+5, Legal's keeping us in check \u2014 The Legal Compliance team is on top of ensuring all our operations are up to scratch with legal standards. They're especially focused on improving our corporate governance and contract management as of the second quarter of 2000. This is critical for keeping our operations smooth and legally sound.\n+9, Working together \u2014 Risk Management and Legal Compliance have been syncing up better than ever since the start of Q2 2000. They're making sure our strategies are not just effective but also fully compliant. This coordination is essential for our integrated governance approach.\n+10, Your thoughts? \u2014 How do these updates impact your area? Got ideas on how we can do better? Give your department heads a shout.\n+11, Thanks for staying engaged. Let's keep pushing for better and smarter ways to work. Cheers, Jane Doe\n+\n+Output:\n+\n+{{\n+    \"title\": \"Enron Compliance and Risk Management Overview as of July 2000\",\n+    \"summary\": \"This report delves into Enron's key departments focusing on compliance and risk management, illustrating how these entities interact within the organizational framework to uphold regulatory standards and manage financial risks effectively. The information is relevant to the company's operations around mid-2000.\",\n+    \"rating\": 9.2,\n+    \"rating_explanation\": \"The high importance rating reflects the critical roles that the Risk Management and Legal Compliance Departments play in ensuring Enron's adherence to financial and legal regulations, crucial for maintaining the company's integrity and operational stability.\",\n+    \"findings\": [\n+        {{\n+            \"summary\": \"Risk Management Operational Scope\",\n+            \"explanation\": \"The Risk Management Department at Enron plays a pivotal role in identifying, assessing, and mitigating financial risks. Their proactive approach, highlighted from the beginning of 2000, helps safeguard Enron against potential financial pitfalls and ensures continuous compliance with evolving market regulations. Effective risk management not only prevents financial anomalies but also supports the company's strategic decision-making processes.\\n\\n[Data: Sources (2, 3), Date_Range ((2000, 01, 01), (2000, 07, 12))]\"\n+        }},\n+        {{\n+            \"summary\": \"Legal Compliance and Governance\",\n+            \"explanation\": \"The Legal Compliance Department ensures that all Enron's operations adhere to the legal standards set by regulatory bodies. Their focus on corporate governance and contract management, noted starting Q2 2000, is crucial in maintaining Enron's reputation and operational legality, especially in managing complex contracts and corporate agreements. Their efforts underscore the commitment to upholding high legal standards and ethical practices.\\n\\n[Data: Source (5), Date_Range ((2000, 04, 01), (2000, 07, 12))]\"\n+        }},\n+        {{\n+            \"summary\": \"Interdepartmental Collaboration for Compliance\",\n+            \"explanation\": \"Collaboration between the Risk Management and Legal Compliance Departments, established in Q2 2000, ensures that risk mitigation strategies are legally sound and that compliance measures consider financial risks. This synergy is vital for holistic governance and has been instrumental in integrating risk management with legal compliance strategies at Enron. Enhanced interdepartmental cooperation during this period plays a crucial role in aligning the company's strategies with regulatory requirements.\\n\\n[Data: Sources (9), Date_Range ((2000, 04, 01), (2000, 07, 12))]\"\n+        }}\n+    ],\n+    \"date_range\": [\"2000-01-01\", \"2000-07-12\"]\n+}}\n+\n+\n+# Real Data\n+\n+Use the following text for your answer. Do not make anything up in your answer.\n+\n+Text:\n+{input_text}\n+\n+Output:\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/index/extract_claims.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/index/extract_claims.py b/packages/graphrag/graphrag/prompts/index/extract_claims.py\nnew file mode 100644\nindex 0000000..59b19c9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/index/extract_claims.py\n@@ -0,0 +1,61 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A file containing prompts definition.\"\"\"\n+\n+EXTRACT_CLAIMS_PROMPT = \"\"\"\n+-Target activity-\n+You are an intelligent assistant that helps a human analyst to analyze claims against certain entities presented in a text document.\n+\n+-Goal-\n+Given a text document that is potentially relevant to this activity, an entity specification, and a claim description, extract all entities that match the entity specification and all claims against those entities.\n+\n+-Steps-\n+1. Extract all named entities that match the predefined entity specification. Entity specification can either be a list of entity names or a list of entity types.\n+2. For each entity identified in step 1, extract all claims associated with the entity. Claims need to match the specified claim description, and the entity should be the subject of the claim.\n+For each claim, extract the following information:\n+- Subject: name of the entity that is subject of the claim, capitalized. The subject entity is one that committed the action described in the claim. Subject needs to be one of the named entities identified in step 1.\n+- Object: name of the entity that is object of the claim, capitalized. The object entity is one that either reports/handles or is affected by the action described in the claim. If object entity is unknown, use **NONE**.\n+- Claim Type: overall category of the claim, capitalized. Name it in a way that can be repeated across multiple text inputs, so that similar claims share the same claim type\n+- Claim Status: **TRUE**, **FALSE**, or **SUSPECTED**. TRUE means the claim is confirmed, FALSE means the claim is found to be False, SUSPECTED means the claim is not verified.\n+- Claim Description: Detailed description explaining the reasoning behind the claim, together with all the related evidence and references.\n+- Claim Date: Period (start_date, end_date) when the claim was made. Both start_date and end_date should be in ISO-8601 format. If the claim was made on a single date rather than a date range, set the same date for both start_date and end_date. If date is unknown, return **NONE**.\n+- Claim Source Text: List of **all** quotes from the original text that are relevant to the claim.\n+\n+Format each claim as (<subject_entity><|><object_entity><|><claim_type><|><claim_status><|><claim_start_date><|><claim_end_date><|><claim_description><|><claim_source>)\n+\n+3. Return output in English as a single list of all the claims identified in steps 1 and 2. Use **##** as the list delimiter.\n+\n+4. When finished, output <|COMPLETE|>\n+\n+-Examples-\n+Example 1:\n+Entity specification: organization\n+Claim description: red flags associated with an entity\n+Text: According to an article on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B. The company is owned by Person C who was suspected of engaging in corruption activities in 2015.\n+Output:\n+\n+(COMPANY A<|>GOVERNMENT AGENCY B<|>ANTI-COMPETITIVE PRACTICES<|>TRUE<|>2022-01-10T00:00:00<|>2022-01-10T00:00:00<|>Company A was found to engage in anti-competitive practices because it was fined for bid rigging in multiple public tenders published by Government Agency B according to an article published on 2022/01/10<|>According to an article published on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B.)\n+<|COMPLETE|>\n+\n+Example 2:\n+Entity specification: Company A, Person C\n+Claim description: red flags associated with an entity\n+Text: According to an article on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B. The company is owned by Person C who was suspected of engaging in corruption activities in 2015.\n+Output:\n+\n+(COMPANY A<|>GOVERNMENT AGENCY B<|>ANTI-COMPETITIVE PRACTICES<|>TRUE<|>2022-01-10T00:00:00<|>2022-01-10T00:00:00<|>Company A was found to engage in anti-competitive practices because it was fined for bid rigging in multiple public tenders published by Government Agency B according to an article published on 2022/01/10<|>According to an article published on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B.)\n+##\n+(PERSON C<|>NONE<|>CORRUPTION<|>SUSPECTED<|>2015-01-01T00:00:00<|>2015-12-30T00:00:00<|>Person C was suspected of engaging in corruption activities in 2015<|>The company is owned by Person C who was suspected of engaging in corruption activities in 2015)\n+<|COMPLETE|>\n+\n+-Real Data-\n+Use the following input for your answer.\n+Entity specification: {entity_specs}\n+Claim description: {claim_description}\n+Text: {input_text}\n+Output:\"\"\"\n+\n+\n+CONTINUE_PROMPT = \"MANY entities were missed in the last extraction.  Add them below using the same format:\\n\"\n+LOOP_PROMPT = \"It appears some entities may have still been missed. Answer Y if there are still entities that need to be added, or N if there are none. Please answer with a single letter Y or N.\\n\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/index/extract_graph.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/index/extract_graph.py b/packages/graphrag/graphrag/prompts/index/extract_graph.py\nnew file mode 100644\nindex 0000000..9115793\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/index/extract_graph.py\n@@ -0,0 +1,129 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A file containing prompts definition.\"\"\"\n+\n+GRAPH_EXTRACTION_PROMPT = \"\"\"\n+-Goal-\n+Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n+ \n+-Steps-\n+1. Identify all entities. For each identified entity, extract the following information:\n+- entity_name: Name of the entity, capitalized\n+- entity_type: One of the following types: [{entity_types}]\n+- entity_description: Comprehensive description of the entity's attributes and activities\n+Format each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n+ \n+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n+For each pair of related entities, extract the following information:\n+- source_entity: name of the source entity, as identified in step 1\n+- target_entity: name of the target entity, as identified in step 1\n+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n+- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n+ Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n+ \n+3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n+ \n+4. When finished, output <|COMPLETE|>\n+ \n+######################\n+-Examples-\n+######################\n+Example 1:\n+Entity_types: ORGANIZATION,PERSON\n+Text:\n+The Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n+######################\n+Output:\n+(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n+##\n+(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n+##\n+(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n+##\n+(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n+<|COMPLETE|>\n+\n+######################\n+Example 2:\n+Entity_types: ORGANIZATION\n+Text:\n+TechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n+\n+TechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n+######################\n+Output:\n+(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n+##\n+(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n+##\n+(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n+<|COMPLETE|>\n+\n+######################\n+Example 3:\n+Entity_types: ORGANIZATION,GEO,PERSON\n+Text:\n+Five Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n+\n+The swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n+\n+The exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n+\n+They were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n+\n+The Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n+######################\n+Output:\n+(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n+##\n+(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n+##\n+(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n+##\n+##\n+(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n+##\n+(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n+##\n+(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n+##\n+(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n+##\n+(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n+##\n+(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n+##\n+(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n+##\n+(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n+##\n+(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n+##\n+(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n+##\n+(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n+##\n+(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n+##\n+(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n+<|COMPLETE|>\n+\n+######################\n+-Real Data-\n+######################\n+Entity_types: {entity_types}\n+Text: {input_text}\n+######################\n+Output:\"\"\"\n+\n+CONTINUE_PROMPT = \"MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\\n\"\n+LOOP_PROMPT = \"It appears some entities and relationships may have still been missed. Answer Y if there are still entities or relationships that need to be added, or N if there are none. Please answer with a single letter Y or N.\\n\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/index/summarize_descriptions.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/index/summarize_descriptions.py b/packages/graphrag/graphrag/prompts/index/summarize_descriptions.py\nnew file mode 100644\nindex 0000000..4a91619\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/index/summarize_descriptions.py\n@@ -0,0 +1,20 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A file containing prompts definition.\"\"\"\n+\n+SUMMARIZE_PROMPT = \"\"\"\n+You are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\n+Given one or more entities, and a list of descriptions, all related to the same entity or group of entities.\n+Please concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\n+If the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\n+Make sure it is written in third person, and include the entity names so we have the full context.\n+Limit the final description length to {max_length} words.\n+\n+#######\n+-Data-\n+Entities: {entity_name}\n+Description List: {description_list}\n+#######\n+Output:\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/__init__.py b/packages/graphrag/graphrag/prompts/query/__init__.py\nnew file mode 100644\nindex 0000000..8e8ef1e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"All prompts for the query engine.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/basic_search_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/basic_search_system_prompt.py b/packages/graphrag/graphrag/prompts/query/basic_search_system_prompt.py\nnew file mode 100644\nindex 0000000..bc37c70\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/basic_search_system_prompt.py\n@@ -0,0 +1,73 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Basic Search prompts.\"\"\"\n+\n+BASIC_SEARCH_SYSTEM_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant responding to questions about data in the tables provided.\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all relevant information in the input data tables appropriate for the response length and format.\n+\n+You should use the data provided in the data tables below as the primary context for generating the response.\n+\n+If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: Sources (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Sources (1, 3)]\"\n+\n+where 1, 2, 3, 7, 34, 46, and 64 represent the source id taken from the \"id\" column in the provided tables.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+\n+---Data tables---\n+\n+{context_data}\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all relevant information in the input data appropriate for the response length and format.\n+\n+You should use the data provided in the data tables below as the primary context for generating the response.\n+\n+If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: Sources (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Sources (1, 3)]\"\n+\n+where 1, 2, 3, 7, 34, 46, and 64 represent the source id taken from the \"id\" column in the provided tables.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/drift_search_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/drift_search_system_prompt.py b/packages/graphrag/graphrag/prompts/query/drift_search_system_prompt.py\nnew file mode 100644\nindex 0000000..3faae89\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/drift_search_system_prompt.py\n@@ -0,0 +1,167 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"DRIFT Search prompts.\"\"\"\n+\n+DRIFT_LOCAL_SYSTEM_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant responding to questions about data in the tables provided.\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n+\n+If you don't know the answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16)].\"\n+\n+where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Pay close attention specifically to the Sources tables as it contains the most relevant information for the user query. You will be rewarded for preserving the context of the sources in your response.\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+\n+---Data tables---\n+\n+{context_data}\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n+\n+If you don't know the answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16)].\"\n+\n+where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Pay close attention specifically to the Sources tables as it contains the most relevant information for the user query. You will be rewarded for preserving the context of the sources in your response.\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+Add sections and commentary to the response as appropriate for the length and format.\n+\n+Additionally provide a score between 0 and 100 representing how well the response addresses the overall research question: {global_query}. Based on your response, suggest up to five follow-up questions that could be asked to further explore the topic as it relates to the overall research question. Do not include scores or follow up questions in the 'response' field of the JSON, add them to the respective 'score' and 'follow_up_queries' keys of the JSON output. Format your response in JSON with the following keys and values:\n+\n+{{'response': str, Put your answer, formatted in markdown, here. Do not answer the global query in this section.\n+'score': int,\n+'follow_up_queries': List[str]}}\n+\"\"\"\n+\n+\n+DRIFT_REDUCE_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant responding to questions about data in the reports provided.\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input reports appropriate for the response length and format, and incorporating any relevant general knowledge while being as specific, accurate and concise as possible.\n+\n+If you don't know the answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (1, 5, 15)].\"\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+If you decide to use general knowledge, you should add a delimiter stating that the information is not supported by the data tables. For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing. [Data: General Knowledge (href)]\"\n+\n+---Data Reports---\n+\n+{context_data}\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input reports appropriate for the response length and format, and incorporating any relevant general knowledge while being as specific, accurate and concise as possible.\n+\n+If you don't know the answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (1, 5, 15)].\"\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+If you decide to use general knowledge, you should add a delimiter stating that the information is not supported by the data tables. For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing. [Data: General Knowledge (href)]\".\n+\n+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown. Now answer the following query using the data above:\n+\n+\"\"\"\n+\n+\n+DRIFT_PRIMER_PROMPT = \"\"\"You are a helpful agent designed to reason over a knowledge graph in response to a user query.\n+This is a unique knowledge graph where edges are freeform text rather than verb operators. You will begin your reasoning looking at a summary of the content of the most relevant communites and will provide:\n+\n+1. score: How well the intermediate answer addresses the query. A score of 0 indicates a poor, unfocused answer, while a score of 100 indicates a highly focused, relevant answer that addresses the query in its entirety.\n+\n+2. intermediate_answer: This answer should match the level of detail and length found in the community summaries. The intermediate answer should be exactly 2000 characters long. This must be formatted in markdown and must begin with a header that explains how the following text is related to the query.\n+\n+3. follow_up_queries: A list of follow-up queries that could be asked to further explore the topic. These should be formatted as a list of strings. Generate at least five good follow-up queries.\n+\n+Use this information to help you decide whether or not you need more information about the entities mentioned in the report. You may also use your general knowledge to think of entities which may help enrich your answer.\n+\n+You will also provide a full answer from the content you have available. Use the data provided to generate follow-up queries to help refine your search. Do not ask compound questions, for example: \"What is the market cap of Apple and Microsoft?\". Use your knowledge of the entity distribution to focus on entity types that will be useful for searching a broad area of the knowledge graph.\n+\n+For the query:\n+\n+{query}\n+\n+The top-ranked community summaries:\n+\n+{community_reports}\n+\n+Provide the intermediate answer, and all scores in JSON format following:\n+\n+{{'intermediate_answer': str,\n+'score': int,\n+'follow_up_queries': List[str]}}\n+\n+Begin:\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/global_search_knowledge_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/global_search_knowledge_system_prompt.py b/packages/graphrag/graphrag/prompts/query/global_search_knowledge_system_prompt.py\nnew file mode 100644\nindex 0000000..9125ef3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/global_search_knowledge_system_prompt.py\n@@ -0,0 +1,9 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Global Search system prompts.\"\"\"\n+\n+GENERAL_KNOWLEDGE_INSTRUCTION = \"\"\"\n+The response may also include relevant real-world knowledge outside the dataset, but it must be explicitly annotated with a verification tag [LLM: verify]. For example:\n+\"This is an example sentence supported by real-world knowledge [LLM: verify].\"\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/global_search_map_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/global_search_map_system_prompt.py b/packages/graphrag/graphrag/prompts/query/global_search_map_system_prompt.py\nnew file mode 100644\nindex 0000000..02e98f9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/global_search_map_system_prompt.py\n@@ -0,0 +1,85 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"System prompts for global search.\"\"\"\n+\n+MAP_SYSTEM_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant responding to questions about data in the tables provided.\n+\n+\n+---Goal---\n+\n+Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the input data tables.\n+\n+You should use the data provided in the data tables below as the primary context for generating the response.\n+If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n+\n+Each key point in the response should have the following element:\n+- Description: A comprehensive description of the point.\n+- Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.\n+\n+The response should be JSON formatted as follows:\n+{{\n+    \"points\": [\n+        {{\"description\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}},\n+        {{\"description\": \"Description of point 2 [Data: Reports (report ids)]\", \"score\": score_value}}\n+    ]\n+}}\n+\n+The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n+\n+Points supported by data should list the relevant reports as references as follows:\n+\"This is an example sentence supported by data references [Data: Reports (report ids)]\"\n+\n+**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n+\n+where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the provided tables.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+Limit your response length to {max_length} words.\n+\n+---Data tables---\n+\n+{context_data}\n+\n+---Goal---\n+\n+Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the input data tables.\n+\n+You should use the data provided in the data tables below as the primary context for generating the response.\n+If you don't know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n+\n+Each key point in the response should have the following element:\n+- Description: A comprehensive description of the point.\n+- Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.\n+\n+The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n+\n+Points supported by data should list the relevant reports as references as follows:\n+\"This is an example sentence supported by data references [Data: Reports (report ids)]\"\n+\n+**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n+\n+where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the provided tables.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+Limit your response length to {max_length} words.\n+\n+The response should be JSON formatted as follows:\n+{{\n+    \"points\": [\n+        {{\"description\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}},\n+        {{\"description\": \"Description of point 2 [Data: Reports (report ids)]\", \"score\": score_value}}\n+    ]\n+}}\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/global_search_reduce_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/global_search_reduce_system_prompt.py b/packages/graphrag/graphrag/prompts/query/global_search_reduce_system_prompt.py\nnew file mode 100644\nindex 0000000..01bf455\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/global_search_reduce_system_prompt.py\n@@ -0,0 +1,85 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Global Search system prompts.\"\"\"\n+\n+REDUCE_SYSTEM_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant responding to questions about a dataset by synthesizing perspectives from multiple analysts.\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarize all the reports from multiple analysts who focused on different parts of the dataset.\n+\n+Note that the analysts' reports provided below are ranked in the **descending order of importance**.\n+\n+If you don't know the answer or if the provided reports do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n+\n+The final response should remove all irrelevant information from the analysts' reports and merge the cleaned information into a comprehensive answer that provides explanations of all the key points and implications appropriate for the response length and format.\n+\n+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n+\n+The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n+\n+The response should also preserve all the data references previously included in the analysts' reports, but do not mention the roles of multiple analysts in the analysis process.\n+\n+**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 34, 46, 64, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n+\n+where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+Limit your response length to {max_length} words.\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+\n+---Analyst Reports---\n+\n+{report_data}\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarize all the reports from multiple analysts who focused on different parts of the dataset.\n+\n+Note that the analysts' reports provided below are ranked in the **descending order of importance**.\n+\n+If you don't know the answer or if the provided reports do not contain sufficient information to provide an answer, just say so. Do not make anything up.\n+\n+The final response should remove all irrelevant information from the analysts' reports and merge the cleaned information into a comprehensive answer that provides explanations of all the key points and implications appropriate for the response length and format.\n+\n+The response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n+\n+The response should also preserve all the data references previously included in the analysts' reports, but do not mention the roles of multiple analysts in the analysis process.\n+\n+**Do not list more than 5 record ids in a single reference**. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2, 7, 34, 46, 64, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n+\n+where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+Limit your response length to {max_length} words.\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n+\"\"\"\n+\n+NO_DATA_ANSWER = (\n+    \"I am sorry but I am unable to answer this question given the provided data.\"\n+)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/local_search_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/local_search_system_prompt.py b/packages/graphrag/graphrag/prompts/query/local_search_system_prompt.py\nnew file mode 100644\nindex 0000000..70b1d12\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/local_search_system_prompt.py\n@@ -0,0 +1,69 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Local search system prompts.\"\"\"\n+\n+LOCAL_SEARCH_SYSTEM_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant responding to questions about data in the tables provided.\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n+\n+If you don't know the answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\n+\n+where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+\n+---Data tables---\n+\n+{context_data}\n+\n+\n+---Goal---\n+\n+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n+\n+If you don't know the answer, just say so. Do not make anything up.\n+\n+Points supported by data should list their data references as follows:\n+\n+\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n+\n+Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n+\n+For example:\n+\n+\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Sources (15, 16), Reports (1), Entities (5, 7); Relationships (23); Claims (2, 7, 34, 46, 64, +more)].\"\n+\n+where 15, 16, 1, 5, 7, 23, 2, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n+\n+Do not include information where the supporting evidence for it is not provided.\n+\n+\n+---Target response length and format---\n+\n+{response_type}\n+\n+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/prompts/query/question_gen_system_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/prompts/query/question_gen_system_prompt.py b/packages/graphrag/graphrag/prompts/query/question_gen_system_prompt.py\nnew file mode 100644\nindex 0000000..904ede2\n--- /dev/null\n+++ b/packages/graphrag/graphrag/prompts/query/question_gen_system_prompt.py\n@@ -0,0 +1,28 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Question Generation system prompts.\"\"\"\n+\n+QUESTION_SYSTEM_PROMPT = \"\"\"\n+---Role---\n+\n+You are a helpful assistant generating a bulleted list of {question_count} questions about data in the tables provided.\n+\n+\n+---Data tables---\n+\n+{context_data}\n+\n+\n+---Goal---\n+\n+Given a series of example questions provided by the user, generate a bulleted list of {question_count} candidates for the next question. Use - marks as bullet points.\n+\n+These candidate questions should represent the most important or urgent information content or themes in the data tables.\n+\n+The candidate questions should be answerable using the data tables provided, but should not mention any specific data fields or data tables in the question text.\n+\n+If the user's questions reference several named entities, then each candidate question should reference all named entities.\n+\n+---Example questions---\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/py.typed",
            "diff": "diff --git a/packages/graphrag/graphrag/py.typed b/packages/graphrag/graphrag/py.typed\nnew file mode 100644\nindex 0000000..f4bd298\n--- /dev/null\n+++ b/packages/graphrag/graphrag/py.typed\n@@ -0,0 +1,2 @@\n+# This package supports type hinting,\n+# see https://www.python.org/dev/peps/pep-0561/#packaging-type-information\n\\ No newline at end of file\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/__init__.py b/packages/graphrag/graphrag/query/__init__.py\nnew file mode 100644\nindex 0000000..effd81e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The query engine package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/__init__.py b/packages/graphrag/graphrag/query/context_builder/__init__.py\nnew file mode 100644\nindex 0000000..7e27364\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Functions to build context for system prompt to generate responses for a user query.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/builders.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/builders.py b/packages/graphrag/graphrag/query/context_builder/builders.py\nnew file mode 100644\nindex 0000000..736c187\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/builders.py\n@@ -0,0 +1,75 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base classes for global and local context builders.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+\n+import pandas as pd\n+\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+\n+\n+@dataclass\n+class ContextBuilderResult:\n+    \"\"\"A class to hold the results of the build_context.\"\"\"\n+\n+    context_chunks: str | list[str]\n+    context_records: dict[str, pd.DataFrame]\n+    llm_calls: int = 0\n+    prompt_tokens: int = 0\n+    output_tokens: int = 0\n+\n+\n+class GlobalContextBuilder(ABC):\n+    \"\"\"Base class for global-search context builders.\"\"\"\n+\n+    @abstractmethod\n+    async def build_context(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs,\n+    ) -> ContextBuilderResult:\n+        \"\"\"Build the context for the global search mode.\"\"\"\n+\n+\n+class LocalContextBuilder(ABC):\n+    \"\"\"Base class for local-search context builders.\"\"\"\n+\n+    @abstractmethod\n+    def build_context(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs,\n+    ) -> ContextBuilderResult:\n+        \"\"\"Build the context for the local search mode.\"\"\"\n+\n+\n+class DRIFTContextBuilder(ABC):\n+    \"\"\"Base class for DRIFT-search context builders.\"\"\"\n+\n+    @abstractmethod\n+    async def build_context(\n+        self,\n+        query: str,\n+        **kwargs,\n+    ) -> tuple[pd.DataFrame, dict[str, int]]:\n+        \"\"\"Build the context for the primer search actions.\"\"\"\n+\n+\n+class BasicContextBuilder(ABC):\n+    \"\"\"Base class for basic-search context builders.\"\"\"\n+\n+    @abstractmethod\n+    def build_context(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs,\n+    ) -> ContextBuilderResult:\n+        \"\"\"Build the context for the basic search mode.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/community_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/community_context.py b/packages/graphrag/graphrag/query/context_builder/community_context.py\nnew file mode 100644\nindex 0000000..4917be0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/community_context.py\n@@ -0,0 +1,264 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Community Context.\"\"\"\n+\n+import logging\n+import random\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.entity import Entity\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+NO_COMMUNITY_RECORDS_WARNING: str = (\n+    \"Warning: No community records added when building community context.\"\n+)\n+\n+\n+def build_community_context(\n+    community_reports: list[CommunityReport],\n+    entities: list[Entity] | None = None,\n+    tokenizer: Tokenizer | None = None,\n+    use_community_summary: bool = True,\n+    column_delimiter: str = \"|\",\n+    shuffle_data: bool = True,\n+    include_community_rank: bool = False,\n+    min_community_rank: int = 0,\n+    community_rank_name: str = \"rank\",\n+    include_community_weight: bool = True,\n+    community_weight_name: str = \"occurrence weight\",\n+    normalize_community_weight: bool = True,\n+    max_context_tokens: int = 8000,\n+    single_batch: bool = True,\n+    context_name: str = \"Reports\",\n+    random_state: int = 86,\n+) -> tuple[str | list[str], dict[str, pd.DataFrame]]:\n+    \"\"\"\n+    Prepare community report data table as context data for system prompt.\n+\n+    If entities are provided, the community weight is calculated as the count of text units associated with entities within the community.\n+\n+    The calculated weight is added as an attribute to the community reports and added to the context data table.\n+    \"\"\"\n+    tokenizer = tokenizer or get_tokenizer()\n+\n+    def _is_included(report: CommunityReport) -> bool:\n+        return report.rank is not None and report.rank >= min_community_rank\n+\n+    def _get_header(attributes: list[str]) -> list[str]:\n+        header = [\"id\", \"title\"]\n+        attributes = [col for col in attributes if col not in header]\n+        if not include_community_weight:\n+            attributes = [col for col in attributes if col != community_weight_name]\n+        header.extend(attributes)\n+        header.append(\"summary\" if use_community_summary else \"content\")\n+        if include_community_rank:\n+            header.append(community_rank_name)\n+        return header\n+\n+    def _report_context_text(\n+        report: CommunityReport, attributes: list[str]\n+    ) -> tuple[str, list[str]]:\n+        context: list[str] = [\n+            report.short_id if report.short_id else \"\",\n+            report.title,\n+            *[\n+                str(report.attributes.get(field, \"\")) if report.attributes else \"\"\n+                for field in attributes\n+            ],\n+        ]\n+        context.append(report.summary if use_community_summary else report.full_content)\n+        if include_community_rank:\n+            context.append(str(report.rank))\n+        result = column_delimiter.join(context) + \"\\n\"\n+        return result, context\n+\n+    compute_community_weights = (\n+        entities\n+        and len(community_reports) > 0\n+        and include_community_weight\n+        and (\n+            community_reports[0].attributes is None\n+            or community_weight_name not in community_reports[0].attributes\n+        )\n+    )\n+    if compute_community_weights:\n+        logger.debug(\"Computing community weights...\")\n+        community_reports = _compute_community_weights(\n+            community_reports=community_reports,\n+            entities=entities,\n+            weight_attribute=community_weight_name,\n+            normalize=normalize_community_weight,\n+        )\n+\n+    selected_reports = [report for report in community_reports if _is_included(report)]\n+\n+    if selected_reports is None or len(selected_reports) == 0:\n+        return ([], {})\n+\n+    if shuffle_data:\n+        random.seed(random_state)\n+        random.shuffle(selected_reports)\n+\n+    # \"global\" variables\n+    attributes = (\n+        list(community_reports[0].attributes.keys())\n+        if community_reports[0].attributes\n+        else []\n+    )\n+    header = _get_header(attributes)\n+    all_context_text: list[str] = []\n+    all_context_records: list[pd.DataFrame] = []\n+\n+    # batch variables\n+    batch_text: str = \"\"\n+    batch_tokens: int = 0\n+    batch_records: list[list[str]] = []\n+\n+    def _init_batch() -> None:\n+        nonlocal batch_text, batch_tokens, batch_records\n+        batch_text = (\n+            f\"-----{context_name}-----\" + \"\\n\" + column_delimiter.join(header) + \"\\n\"\n+        )\n+        batch_tokens = tokenizer.num_tokens(batch_text)\n+        batch_records = []\n+\n+    def _cut_batch() -> None:\n+        # convert the current context records to pandas dataframe and sort by weight and rank if exist\n+        record_df = _convert_report_context_to_df(\n+            context_records=batch_records,\n+            header=header,\n+            weight_column=(\n+                community_weight_name if entities and include_community_weight else None\n+            ),\n+            rank_column=community_rank_name if include_community_rank else None,\n+        )\n+        if len(record_df) == 0:\n+            return\n+        current_context_text = record_df.to_csv(index=False, sep=column_delimiter)\n+        if not all_context_text and single_batch:\n+            current_context_text = f\"-----{context_name}-----\\n{current_context_text}\"\n+\n+        all_context_text.append(current_context_text)\n+        all_context_records.append(record_df)\n+\n+    # initialize the first batch\n+    _init_batch()\n+\n+    for report in selected_reports:\n+        new_context_text, new_context = _report_context_text(report, attributes)\n+        new_tokens = tokenizer.num_tokens(new_context_text)\n+\n+        if batch_tokens + new_tokens > max_context_tokens:\n+            # add the current batch to the context data and start a new batch if we are in multi-batch mode\n+            _cut_batch()\n+            if single_batch:\n+                break\n+            _init_batch()\n+\n+        # add current report to the current batch\n+        batch_text += new_context_text\n+        batch_tokens += new_tokens\n+        batch_records.append(new_context)\n+\n+    # Extract the IDs from the current batch\n+    current_batch_ids = {record[0] for record in batch_records}\n+\n+    # Extract the IDs from all previous batches in all_context_records\n+    existing_ids_sets = [set(record[\"id\"].to_list()) for record in all_context_records]\n+\n+    # Check if the current batch has been added\n+    if current_batch_ids not in existing_ids_sets:\n+        _cut_batch()\n+\n+    if len(all_context_records) == 0:\n+        logger.warning(NO_COMMUNITY_RECORDS_WARNING)\n+        return ([], {})\n+\n+    return all_context_text, {\n+        context_name.lower(): pd.concat(all_context_records, ignore_index=True)\n+    }\n+\n+\n+def _compute_community_weights(\n+    community_reports: list[CommunityReport],\n+    entities: list[Entity] | None,\n+    weight_attribute: str = \"occurrence\",\n+    normalize: bool = True,\n+) -> list[CommunityReport]:\n+    \"\"\"Calculate a community's weight as count of text units associated with entities within the community.\"\"\"\n+    if not entities:\n+        return community_reports\n+\n+    community_text_units = {}\n+    for entity in entities:\n+        if entity.community_ids:\n+            for community_id in entity.community_ids:\n+                if community_id not in community_text_units:\n+                    community_text_units[community_id] = []\n+                community_text_units[community_id].extend(entity.text_unit_ids)\n+    for report in community_reports:\n+        if not report.attributes:\n+            report.attributes = {}\n+        report.attributes[weight_attribute] = len(\n+            set(community_text_units.get(report.community_id, []))\n+        )\n+    if normalize:\n+        # normalize by max weight\n+        all_weights = [\n+            report.attributes[weight_attribute]\n+            for report in community_reports\n+            if report.attributes\n+        ]\n+        max_weight = max(all_weights)\n+        for report in community_reports:\n+            if report.attributes:\n+                report.attributes[weight_attribute] = (\n+                    report.attributes[weight_attribute] / max_weight\n+                )\n+    return community_reports\n+\n+\n+def _rank_report_context(\n+    report_df: pd.DataFrame,\n+    weight_column: str | None = \"occurrence weight\",\n+    rank_column: str | None = \"rank\",\n+) -> pd.DataFrame:\n+    \"\"\"Sort report context by community weight and rank if exist.\"\"\"\n+    rank_attributes: list[str] = []\n+    if weight_column:\n+        rank_attributes.append(weight_column)\n+        report_df[weight_column] = report_df[weight_column].astype(float)\n+    if rank_column:\n+        rank_attributes.append(rank_column)\n+        report_df[rank_column] = report_df[rank_column].astype(float)\n+    if len(rank_attributes) > 0:\n+        report_df.sort_values(by=rank_attributes, ascending=False, inplace=True)\n+    return report_df\n+\n+\n+def _convert_report_context_to_df(\n+    context_records: list[list[str]],\n+    header: list[str],\n+    weight_column: str | None = None,\n+    rank_column: str | None = None,\n+) -> pd.DataFrame:\n+    \"\"\"Convert report context records to pandas dataframe and sort by weight and rank if exist.\"\"\"\n+    if len(context_records) == 0:\n+        return pd.DataFrame()\n+\n+    record_df = pd.DataFrame(\n+        context_records,\n+        columns=cast(\"Any\", header),\n+    )\n+    return _rank_report_context(\n+        report_df=record_df,\n+        weight_column=weight_column,\n+        rank_column=rank_column,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/conversation_history.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/conversation_history.py b/packages/graphrag/graphrag/query/context_builder/conversation_history.py\nnew file mode 100644\nindex 0000000..c209981\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/conversation_history.py\n@@ -0,0 +1,213 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Classes for storing and managing conversation history.\"\"\"\n+\n+from dataclasses import dataclass\n+from enum import Enum\n+\n+import pandas as pd\n+\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\"\"\"\n+Enum for conversation roles\n+\"\"\"\n+\n+\n+class ConversationRole(str, Enum):\n+    \"\"\"Enum for conversation roles.\"\"\"\n+\n+    SYSTEM = \"system\"\n+    USER = \"user\"\n+    ASSISTANT = \"assistant\"\n+\n+    @staticmethod\n+    def from_string(value: str) -> \"ConversationRole\":\n+        \"\"\"Convert string to ConversationRole.\"\"\"\n+        if value == \"system\":\n+            return ConversationRole.SYSTEM\n+        if value == \"user\":\n+            return ConversationRole.USER\n+        if value == \"assistant\":\n+            return ConversationRole.ASSISTANT\n+\n+        msg = f\"Invalid Role: {value}\"\n+        raise ValueError(msg)\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the enum value.\"\"\"\n+        return self.value\n+\n+\n+\"\"\"\n+Data class for storing a single conversation turn\n+\"\"\"\n+\n+\n+@dataclass\n+class ConversationTurn:\n+    \"\"\"Data class for storing a single conversation turn.\"\"\"\n+\n+    role: ConversationRole\n+    content: str\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the conversation turn.\"\"\"\n+        return f\"{self.role}: {self.content}\"\n+\n+\n+@dataclass\n+class QATurn:\n+    \"\"\"\n+    Data class for storing a QA turn.\n+\n+    A QA turn contains a user question and one more multiple assistant answers.\n+    \"\"\"\n+\n+    user_query: ConversationTurn\n+    assistant_answers: list[ConversationTurn] | None = None\n+\n+    def get_answer_text(self) -> str | None:\n+        \"\"\"Get the text of the assistant answers.\"\"\"\n+        return (\n+            \"\\n\".join([answer.content for answer in self.assistant_answers])\n+            if self.assistant_answers\n+            else None\n+        )\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return string representation of the QA turn.\"\"\"\n+        answers = self.get_answer_text()\n+        return (\n+            f\"Question: {self.user_query.content}\\nAnswer: {answers}\"\n+            if answers\n+            else f\"Question: {self.user_query.content}\"\n+        )\n+\n+\n+class ConversationHistory:\n+    \"\"\"Class for storing a conversation history.\"\"\"\n+\n+    turns: list[ConversationTurn]\n+\n+    def __init__(self):\n+        self.turns = []\n+\n+    @classmethod\n+    def from_list(\n+        cls, conversation_turns: list[dict[str, str]]\n+    ) -> \"ConversationHistory\":\n+        \"\"\"\n+        Create a conversation history from a list of conversation turns.\n+\n+        Each turn is a dictionary in the form of {\"role\": \"<conversation_role>\", \"content\": \"<turn content>\"}\n+        \"\"\"\n+        history = cls()\n+        for turn in conversation_turns:\n+            history.turns.append(\n+                ConversationTurn(\n+                    role=ConversationRole.from_string(\n+                        turn.get(\"role\", ConversationRole.USER)\n+                    ),\n+                    content=turn.get(\"content\", \"\"),\n+                )\n+            )\n+        return history\n+\n+    def add_turn(self, role: ConversationRole, content: str):\n+        \"\"\"Add a new turn to the conversation history.\"\"\"\n+        self.turns.append(ConversationTurn(role=role, content=content))\n+\n+    def to_qa_turns(self) -> list[QATurn]:\n+        \"\"\"Convert conversation history to a list of QA turns.\"\"\"\n+        qa_turns = list[QATurn]()\n+        current_qa_turn = None\n+        for turn in self.turns:\n+            if turn.role == ConversationRole.USER:\n+                if current_qa_turn:\n+                    qa_turns.append(current_qa_turn)\n+                current_qa_turn = QATurn(user_query=turn, assistant_answers=[])\n+            else:\n+                if current_qa_turn:\n+                    current_qa_turn.assistant_answers.append(turn)  # type: ignore\n+        if current_qa_turn:\n+            qa_turns.append(current_qa_turn)\n+        return qa_turns\n+\n+    def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]:\n+        \"\"\"Get the last user turns in the conversation history.\"\"\"\n+        user_turns = []\n+        for turn in self.turns[::-1]:\n+            if turn.role == ConversationRole.USER:\n+                user_turns.append(turn.content)\n+                if max_user_turns and len(user_turns) >= max_user_turns:\n+                    break\n+        return user_turns\n+\n+    def build_context(\n+        self,\n+        tokenizer: Tokenizer | None = None,\n+        include_user_turns_only: bool = True,\n+        max_qa_turns: int | None = 5,\n+        max_context_tokens: int = 8000,\n+        recency_bias: bool = True,\n+        column_delimiter: str = \"|\",\n+        context_name: str = \"Conversation History\",\n+    ) -> tuple[str, dict[str, pd.DataFrame]]:\n+        \"\"\"\n+        Prepare conversation history as context data for system prompt.\n+\n+        Parameters\n+        ----------\n+            user_queries_only: If True, only user queries (not assistant responses) will be included in the context, default is True.\n+            max_qa_turns: Maximum number of QA turns to include in the context, default is 1.\n+            recency_bias: If True, reverse the order of the conversation history to ensure last QA got prioritized.\n+            column_delimiter: Delimiter to use for separating columns in the context data, default is \"|\".\n+            context_name: Name of the context, default is \"Conversation History\".\n+\n+        \"\"\"\n+        tokenizer = tokenizer or get_tokenizer()\n+        qa_turns = self.to_qa_turns()\n+        if include_user_turns_only:\n+            qa_turns = [\n+                QATurn(user_query=qa_turn.user_query, assistant_answers=None)\n+                for qa_turn in qa_turns\n+            ]\n+        if recency_bias:\n+            qa_turns = qa_turns[::-1]\n+        if max_qa_turns and len(qa_turns) > max_qa_turns:\n+            qa_turns = qa_turns[:max_qa_turns]\n+\n+        # build context for qa turns\n+        # add context header\n+        if len(qa_turns) == 0 or not qa_turns:\n+            return (\"\", {context_name: pd.DataFrame()})\n+\n+        # add table header\n+        header = f\"-----{context_name}-----\" + \"\\n\"\n+\n+        turn_list = []\n+        current_context_df = pd.DataFrame()\n+        for turn in qa_turns:\n+            turn_list.append({\n+                \"turn\": ConversationRole.USER.__str__(),\n+                \"content\": turn.user_query.content,\n+            })\n+            if turn.assistant_answers:\n+                turn_list.append({\n+                    \"turn\": ConversationRole.ASSISTANT.__str__(),\n+                    \"content\": turn.get_answer_text(),\n+                })\n+\n+            context_df = pd.DataFrame(turn_list)\n+            context_text = header + context_df.to_csv(sep=column_delimiter, index=False)\n+            if tokenizer.num_tokens(context_text) > max_context_tokens:\n+                break\n+\n+            current_context_df = context_df\n+        context_text = header + current_context_df.to_csv(\n+            sep=column_delimiter, index=False\n+        )\n+        return (context_text, {context_name.lower(): current_context_df})\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/dynamic_community_selection.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/dynamic_community_selection.py b/packages/graphrag/graphrag/query/context_builder/dynamic_community_selection.py\nnew file mode 100644\nindex 0000000..904478f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/dynamic_community_selection.py\n@@ -0,0 +1,171 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Algorithm to dynamically select relevant communities with respect to a query.\"\"\"\n+\n+import asyncio\n+import logging\n+from collections import Counter\n+from copy import deepcopy\n+from time import time\n+from typing import Any\n+\n+from graphrag.data_model.community import Community\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.query.context_builder.rate_prompt import RATE_QUERY\n+from graphrag.query.context_builder.rate_relevancy import rate_relevancy\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class DynamicCommunitySelection:\n+    \"\"\"Dynamic community selection to select community reports that are relevant to the query.\n+\n+    Any community report with a rating EQUAL or ABOVE the rating_threshold is considered relevant.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        community_reports: list[CommunityReport],\n+        communities: list[Community],\n+        model: ChatModel,\n+        tokenizer: Tokenizer,\n+        rate_query: str = RATE_QUERY,\n+        use_summary: bool = False,\n+        threshold: int = 1,\n+        keep_parent: bool = False,\n+        num_repeats: int = 1,\n+        max_level: int = 2,\n+        concurrent_coroutines: int = 8,\n+        model_params: dict[str, Any] | None = None,\n+    ):\n+        self.model = model\n+        self.tokenizer = tokenizer\n+        self.rate_query = rate_query\n+        self.num_repeats = num_repeats\n+        self.use_summary = use_summary\n+        self.threshold = threshold\n+        self.keep_parent = keep_parent\n+        self.max_level = max_level\n+        self.semaphore = asyncio.Semaphore(concurrent_coroutines)\n+        self.model_params = model_params if model_params else {}\n+\n+        self.reports = {report.community_id: report for report in community_reports}\n+        self.communities = {community.short_id: community for community in communities}\n+\n+        # mapping from level to communities\n+        self.levels: dict[str, list[str]] = {}\n+\n+        for community in communities:\n+            if community.level not in self.levels:\n+                self.levels[community.level] = []\n+            if community.short_id in self.reports:\n+                self.levels[community.level].append(community.short_id)\n+\n+        # start from root communities (level 0)\n+        self.starting_communities = self.levels[\"0\"]\n+\n+    async def select(self, query: str) -> tuple[list[CommunityReport], dict[str, Any]]:\n+        \"\"\"\n+        Select relevant communities with respect to the query.\n+\n+        Args:\n+            query: the query to rate against\n+        \"\"\"\n+        start = time()\n+        queue = deepcopy(self.starting_communities)\n+        level = 0\n+\n+        ratings = {}  # store the ratings for each community\n+        llm_info: dict[str, Any] = {\n+            \"llm_calls\": 0,\n+            \"prompt_tokens\": 0,\n+            \"output_tokens\": 0,\n+        }\n+        relevant_communities = set()\n+\n+        while queue:\n+            gather_results = await asyncio.gather(*[\n+                rate_relevancy(\n+                    query=query,\n+                    description=(\n+                        self.reports[community].summary\n+                        if self.use_summary\n+                        else self.reports[community].full_content\n+                    ),\n+                    model=self.model,\n+                    tokenizer=self.tokenizer,\n+                    rate_query=self.rate_query,\n+                    num_repeats=self.num_repeats,\n+                    semaphore=self.semaphore,\n+                    **self.model_params,\n+                )\n+                for community in queue\n+            ])\n+\n+            communities_to_rate = []\n+            for community, result in zip(queue, gather_results, strict=True):\n+                rating = result[\"rating\"]\n+                logger.debug(\n+                    \"dynamic community selection: community %s rating %s\",\n+                    community,\n+                    rating,\n+                )\n+                ratings[community] = rating\n+                llm_info[\"llm_calls\"] += result[\"llm_calls\"]\n+                llm_info[\"prompt_tokens\"] += result[\"prompt_tokens\"]\n+                llm_info[\"output_tokens\"] += result[\"output_tokens\"]\n+                if rating >= self.threshold:\n+                    relevant_communities.add(community)\n+                    # find children nodes of the current node and append them to the queue\n+                    # TODO check why some sub_communities are NOT in report_df\n+                    if community in self.communities:\n+                        for child in self.communities[community].children:\n+                            if child in self.reports:\n+                                communities_to_rate.append(child)\n+                            else:\n+                                logger.debug(\n+                                    \"dynamic community selection: cannot find community %s in reports\",\n+                                    child,\n+                                )\n+                    # remove parent node if the current node is deemed relevant\n+                    if not self.keep_parent and community in self.communities:\n+                        relevant_communities.discard(self.communities[community].parent)\n+            queue = communities_to_rate\n+            level += 1\n+            if (\n+                (len(queue) == 0)\n+                and (len(relevant_communities) == 0)\n+                and (str(level) in self.levels)\n+                and (level <= self.max_level)\n+            ):\n+                logger.debug(\n+                    \"dynamic community selection: no relevant community \"\n+                    \"reports, adding all reports at level %s to rate.\",\n+                    level,\n+                )\n+                # append all communities at the next level to queue\n+                queue = self.levels[str(level)]\n+\n+        community_reports = [\n+            self.reports[community] for community in relevant_communities\n+        ]\n+        end = time()\n+\n+        logger.debug(\n+            \"dynamic community selection (took: %ss)\\n\"\n+            \"\\trating distribution %s\\n\"\n+            \"\\t%s out of %s community reports are relevant\\n\"\n+            \"\\tprompt tokens: %s, output tokens: %s\",\n+            int(end - start),\n+            dict(sorted(Counter(ratings.values()).items())),\n+            len(relevant_communities),\n+            len(self.reports),\n+            llm_info[\"prompt_tokens\"],\n+            llm_info[\"output_tokens\"],\n+        )\n+\n+        llm_info[\"ratings\"] = ratings\n+        return community_reports, llm_info\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/entity_extraction.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/entity_extraction.py b/packages/graphrag/graphrag/query/context_builder/entity_extraction.py\nnew file mode 100644\nindex 0000000..0dd03ba\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/entity_extraction.py\n@@ -0,0 +1,121 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Orchestration Context Builders.\"\"\"\n+\n+from enum import Enum\n+\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.query.input.retrieval.entities import (\n+    get_entity_by_id,\n+    get_entity_by_key,\n+    get_entity_by_name,\n+)\n+from graphrag.vector_stores.base import BaseVectorStore\n+\n+\n+class EntityVectorStoreKey(str, Enum):\n+    \"\"\"Keys used as ids in the entity embedding vectorstores.\"\"\"\n+\n+    ID = \"id\"\n+    TITLE = \"title\"\n+\n+    @staticmethod\n+    def from_string(value: str) -> \"EntityVectorStoreKey\":\n+        \"\"\"Convert string to EntityVectorStoreKey.\"\"\"\n+        if value == \"id\":\n+            return EntityVectorStoreKey.ID\n+        if value == \"title\":\n+            return EntityVectorStoreKey.TITLE\n+\n+        msg = f\"Invalid EntityVectorStoreKey: {value}\"\n+        raise ValueError(msg)\n+\n+\n+def map_query_to_entities(\n+    query: str,\n+    text_embedding_vectorstore: BaseVectorStore,\n+    text_embedder: EmbeddingModel,\n+    all_entities_dict: dict[str, Entity],\n+    embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n+    include_entity_names: list[str] | None = None,\n+    exclude_entity_names: list[str] | None = None,\n+    k: int = 10,\n+    oversample_scaler: int = 2,\n+) -> list[Entity]:\n+    \"\"\"Extract entities that match a given query using semantic similarity of text embeddings of query and entity descriptions.\"\"\"\n+    if include_entity_names is None:\n+        include_entity_names = []\n+    if exclude_entity_names is None:\n+        exclude_entity_names = []\n+    all_entities = list(all_entities_dict.values())\n+    matched_entities = []\n+    if query != \"\":\n+        # get entities with highest semantic similarity to query\n+        # oversample to account for excluded entities\n+        search_results = text_embedding_vectorstore.similarity_search_by_text(\n+            text=query,\n+            text_embedder=lambda t: text_embedder.embed(t),\n+            k=k * oversample_scaler,\n+        )\n+        for result in search_results:\n+            if embedding_vectorstore_key == EntityVectorStoreKey.ID and isinstance(\n+                result.document.id, str\n+            ):\n+                matched = get_entity_by_id(all_entities_dict, result.document.id)\n+            else:\n+                matched = get_entity_by_key(\n+                    entities=all_entities,\n+                    key=embedding_vectorstore_key,\n+                    value=result.document.id,\n+                )\n+            if matched:\n+                matched_entities.append(matched)\n+    else:\n+        all_entities.sort(key=lambda x: x.rank if x.rank else 0, reverse=True)\n+        matched_entities = all_entities[:k]\n+\n+    # filter out excluded entities\n+    if exclude_entity_names:\n+        matched_entities = [\n+            entity\n+            for entity in matched_entities\n+            if entity.title not in exclude_entity_names\n+        ]\n+\n+    # add entities in the include_entity list\n+    included_entities = []\n+    for entity_name in include_entity_names:\n+        included_entities.extend(get_entity_by_name(all_entities, entity_name))\n+    return included_entities + matched_entities\n+\n+\n+def find_nearest_neighbors_by_entity_rank(\n+    entity_name: str,\n+    all_entities: list[Entity],\n+    all_relationships: list[Relationship],\n+    exclude_entity_names: list[str] | None = None,\n+    k: int | None = 10,\n+) -> list[Entity]:\n+    \"\"\"Retrieve entities that have direct connections with the target entity, sorted by entity rank.\"\"\"\n+    if exclude_entity_names is None:\n+        exclude_entity_names = []\n+    entity_relationships = [\n+        rel\n+        for rel in all_relationships\n+        if rel.source == entity_name or rel.target == entity_name\n+    ]\n+    source_entity_names = {rel.source for rel in entity_relationships}\n+    target_entity_names = {rel.target for rel in entity_relationships}\n+    related_entity_names = (source_entity_names.union(target_entity_names)).difference(\n+        set(exclude_entity_names)\n+    )\n+    top_relations = [\n+        entity for entity in all_entities if entity.title in related_entity_names\n+    ]\n+    top_relations.sort(key=lambda x: x.rank if x.rank else 0, reverse=True)\n+    if k:\n+        return top_relations[:k]\n+    return top_relations\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/local_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/local_context.py b/packages/graphrag/graphrag/query/context_builder/local_context.py\nnew file mode 100644\nindex 0000000..a2d8a54\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/local_context.py\n@@ -0,0 +1,357 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Local Context Builder.\"\"\"\n+\n+from collections import defaultdict\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.query.input.retrieval.covariates import (\n+    get_candidate_covariates,\n+    to_covariate_dataframe,\n+)\n+from graphrag.query.input.retrieval.entities import to_entity_dataframe\n+from graphrag.query.input.retrieval.relationships import (\n+    get_candidate_relationships,\n+    get_entities_from_relationships,\n+    get_in_network_relationships,\n+    get_out_network_relationships,\n+    to_relationship_dataframe,\n+)\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+def build_entity_context(\n+    selected_entities: list[Entity],\n+    tokenizer: Tokenizer | None = None,\n+    max_context_tokens: int = 8000,\n+    include_entity_rank: bool = True,\n+    rank_description: str = \"number of relationships\",\n+    column_delimiter: str = \"|\",\n+    context_name=\"Entities\",\n+) -> tuple[str, pd.DataFrame]:\n+    \"\"\"Prepare entity data table as context data for system prompt.\"\"\"\n+    tokenizer = tokenizer or get_tokenizer()\n+\n+    if len(selected_entities) == 0:\n+        return \"\", pd.DataFrame()\n+\n+    # add headers\n+    current_context_text = f\"-----{context_name}-----\" + \"\\n\"\n+    header = [\"id\", \"entity\", \"description\"]\n+    if include_entity_rank:\n+        header.append(rank_description)\n+    attribute_cols = (\n+        list(selected_entities[0].attributes.keys())\n+        if selected_entities[0].attributes\n+        else []\n+    )\n+    header.extend(attribute_cols)\n+    current_context_text += column_delimiter.join(header) + \"\\n\"\n+    current_tokens = tokenizer.num_tokens(current_context_text)\n+\n+    all_context_records = [header]\n+    for entity in selected_entities:\n+        new_context = [\n+            entity.short_id if entity.short_id else \"\",\n+            entity.title,\n+            entity.description if entity.description else \"\",\n+        ]\n+        if include_entity_rank:\n+            new_context.append(str(entity.rank))\n+        for field in attribute_cols:\n+            field_value = (\n+                str(entity.attributes.get(field))\n+                if entity.attributes and entity.attributes.get(field)\n+                else \"\"\n+            )\n+            new_context.append(field_value)\n+        new_context_text = column_delimiter.join(new_context) + \"\\n\"\n+        new_tokens = tokenizer.num_tokens(new_context_text)\n+        if current_tokens + new_tokens > max_context_tokens:\n+            break\n+        current_context_text += new_context_text\n+        all_context_records.append(new_context)\n+        current_tokens += new_tokens\n+\n+    if len(all_context_records) > 1:\n+        record_df = pd.DataFrame(\n+            all_context_records[1:], columns=cast(\"Any\", all_context_records[0])\n+        )\n+    else:\n+        record_df = pd.DataFrame()\n+\n+    return current_context_text, record_df\n+\n+\n+def build_covariates_context(\n+    selected_entities: list[Entity],\n+    covariates: list[Covariate],\n+    tokenizer: Tokenizer | None = None,\n+    max_context_tokens: int = 8000,\n+    column_delimiter: str = \"|\",\n+    context_name: str = \"Covariates\",\n+) -> tuple[str, pd.DataFrame]:\n+    \"\"\"Prepare covariate data tables as context data for system prompt.\"\"\"\n+    tokenizer = tokenizer or get_tokenizer()\n+    # create an empty list of covariates\n+    if len(selected_entities) == 0 or len(covariates) == 0:\n+        return \"\", pd.DataFrame()\n+\n+    selected_covariates = list[Covariate]()\n+    record_df = pd.DataFrame()\n+\n+    # add context header\n+    current_context_text = f\"-----{context_name}-----\" + \"\\n\"\n+\n+    # add header\n+    header = [\"id\", \"entity\"]\n+    attributes = covariates[0].attributes or {} if len(covariates) > 0 else {}\n+    attribute_cols = list(attributes.keys()) if len(covariates) > 0 else []\n+    header.extend(attribute_cols)\n+    current_context_text += column_delimiter.join(header) + \"\\n\"\n+    current_tokens = tokenizer.num_tokens(current_context_text)\n+\n+    all_context_records = [header]\n+    for entity in selected_entities:\n+        selected_covariates.extend([\n+            cov for cov in covariates if cov.subject_id == entity.title\n+        ])\n+\n+    for covariate in selected_covariates:\n+        new_context = [\n+            covariate.short_id if covariate.short_id else \"\",\n+            covariate.subject_id,\n+        ]\n+        for field in attribute_cols:\n+            field_value = (\n+                str(covariate.attributes.get(field))\n+                if covariate.attributes and covariate.attributes.get(field)\n+                else \"\"\n+            )\n+            new_context.append(field_value)\n+\n+        new_context_text = column_delimiter.join(new_context) + \"\\n\"\n+        new_tokens = tokenizer.num_tokens(new_context_text)\n+        if current_tokens + new_tokens > max_context_tokens:\n+            break\n+        current_context_text += new_context_text\n+        all_context_records.append(new_context)\n+        current_tokens += new_tokens\n+\n+        if len(all_context_records) > 1:\n+            record_df = pd.DataFrame(\n+                all_context_records[1:], columns=cast(\"Any\", all_context_records[0])\n+            )\n+        else:\n+            record_df = pd.DataFrame()\n+\n+    return current_context_text, record_df\n+\n+\n+def build_relationship_context(\n+    selected_entities: list[Entity],\n+    relationships: list[Relationship],\n+    tokenizer: Tokenizer | None = None,\n+    include_relationship_weight: bool = False,\n+    max_context_tokens: int = 8000,\n+    top_k_relationships: int = 10,\n+    relationship_ranking_attribute: str = \"rank\",\n+    column_delimiter: str = \"|\",\n+    context_name: str = \"Relationships\",\n+) -> tuple[str, pd.DataFrame]:\n+    \"\"\"Prepare relationship data tables as context data for system prompt.\"\"\"\n+    tokenizer = tokenizer or get_tokenizer()\n+    selected_relationships = _filter_relationships(\n+        selected_entities=selected_entities,\n+        relationships=relationships,\n+        top_k_relationships=top_k_relationships,\n+        relationship_ranking_attribute=relationship_ranking_attribute,\n+    )\n+\n+    if len(selected_entities) == 0 or len(selected_relationships) == 0:\n+        return \"\", pd.DataFrame()\n+\n+    # add headers\n+    current_context_text = f\"-----{context_name}-----\" + \"\\n\"\n+    header = [\"id\", \"source\", \"target\", \"description\"]\n+    if include_relationship_weight:\n+        header.append(\"weight\")\n+    attribute_cols = (\n+        list(selected_relationships[0].attributes.keys())\n+        if selected_relationships[0].attributes\n+        else []\n+    )\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+\n+    current_context_text += column_delimiter.join(header) + \"\\n\"\n+    current_tokens = tokenizer.num_tokens(current_context_text)\n+\n+    all_context_records = [header]\n+    for rel in selected_relationships:\n+        new_context = [\n+            rel.short_id if rel.short_id else \"\",\n+            rel.source,\n+            rel.target,\n+            rel.description if rel.description else \"\",\n+        ]\n+        if include_relationship_weight:\n+            new_context.append(str(rel.weight if rel.weight else \"\"))\n+        for field in attribute_cols:\n+            field_value = (\n+                str(rel.attributes.get(field))\n+                if rel.attributes and rel.attributes.get(field)\n+                else \"\"\n+            )\n+            new_context.append(field_value)\n+        new_context_text = column_delimiter.join(new_context) + \"\\n\"\n+        new_tokens = tokenizer.num_tokens(new_context_text)\n+        if current_tokens + new_tokens > max_context_tokens:\n+            break\n+        current_context_text += new_context_text\n+        all_context_records.append(new_context)\n+        current_tokens += new_tokens\n+\n+    if len(all_context_records) > 1:\n+        record_df = pd.DataFrame(\n+            all_context_records[1:], columns=cast(\"Any\", all_context_records[0])\n+        )\n+    else:\n+        record_df = pd.DataFrame()\n+\n+    return current_context_text, record_df\n+\n+\n+def _filter_relationships(\n+    selected_entities: list[Entity],\n+    relationships: list[Relationship],\n+    top_k_relationships: int = 10,\n+    relationship_ranking_attribute: str = \"rank\",\n+) -> list[Relationship]:\n+    \"\"\"Filter and sort relationships based on a set of selected entities and a ranking attribute.\"\"\"\n+    # First priority: in-network relationships (i.e. relationships between selected entities)\n+    in_network_relationships = get_in_network_relationships(\n+        selected_entities=selected_entities,\n+        relationships=relationships,\n+        ranking_attribute=relationship_ranking_attribute,\n+    )\n+\n+    # Second priority -  out-of-network relationships\n+    # (i.e. relationships between selected entities and other entities that are not within the selected entities)\n+    out_network_relationships = get_out_network_relationships(\n+        selected_entities=selected_entities,\n+        relationships=relationships,\n+        ranking_attribute=relationship_ranking_attribute,\n+    )\n+    if len(out_network_relationships) <= 1:\n+        return in_network_relationships + out_network_relationships\n+\n+    # within out-of-network relationships, prioritize mutual relationships\n+    # (i.e. relationships with out-network entities that are shared with multiple selected entities)\n+    selected_entity_names = [entity.title for entity in selected_entities]\n+    out_network_source_names = [\n+        relationship.source\n+        for relationship in out_network_relationships\n+        if relationship.source not in selected_entity_names\n+    ]\n+    out_network_target_names = [\n+        relationship.target\n+        for relationship in out_network_relationships\n+        if relationship.target not in selected_entity_names\n+    ]\n+    out_network_entity_names = list(\n+        set(out_network_source_names + out_network_target_names)\n+    )\n+    out_network_entity_links = defaultdict(int)\n+    for entity_name in out_network_entity_names:\n+        targets = [\n+            relationship.target\n+            for relationship in out_network_relationships\n+            if relationship.source == entity_name\n+        ]\n+        sources = [\n+            relationship.source\n+            for relationship in out_network_relationships\n+            if relationship.target == entity_name\n+        ]\n+        out_network_entity_links[entity_name] = len(set(targets + sources))\n+\n+    # sort out-network relationships by number of links and rank_attributes\n+    for rel in out_network_relationships:\n+        if rel.attributes is None:\n+            rel.attributes = {}\n+        rel.attributes[\"links\"] = (\n+            out_network_entity_links[rel.source]\n+            if rel.source in out_network_entity_links\n+            else out_network_entity_links[rel.target]\n+        )\n+\n+    # sort by attributes[links] first, then by ranking_attribute\n+    if relationship_ranking_attribute == \"rank\":\n+        out_network_relationships.sort(\n+            key=lambda x: (x.attributes[\"links\"], x.rank),  # type: ignore\n+            reverse=True,  # type: ignore\n+        )\n+    elif relationship_ranking_attribute == \"weight\":\n+        out_network_relationships.sort(\n+            key=lambda x: (x.attributes[\"links\"], x.weight),  # type: ignore\n+            reverse=True,  # type: ignore\n+        )\n+    else:\n+        out_network_relationships.sort(\n+            key=lambda x: (\n+                x.attributes[\"links\"],  # type: ignore\n+                x.attributes[relationship_ranking_attribute],  # type: ignore\n+            ),  # type: ignore\n+            reverse=True,\n+        )\n+\n+    relationship_budget = top_k_relationships * len(selected_entities)\n+    return in_network_relationships + out_network_relationships[:relationship_budget]\n+\n+\n+def get_candidate_context(\n+    selected_entities: list[Entity],\n+    entities: list[Entity],\n+    relationships: list[Relationship],\n+    covariates: dict[str, list[Covariate]],\n+    include_entity_rank: bool = True,\n+    entity_rank_description: str = \"number of relationships\",\n+    include_relationship_weight: bool = False,\n+) -> dict[str, pd.DataFrame]:\n+    \"\"\"Prepare entity, relationship, and covariate data tables as context data for system prompt.\"\"\"\n+    candidate_context = {}\n+    candidate_relationships = get_candidate_relationships(\n+        selected_entities=selected_entities,\n+        relationships=relationships,\n+    )\n+    candidate_context[\"relationships\"] = to_relationship_dataframe(\n+        relationships=candidate_relationships,\n+        include_relationship_weight=include_relationship_weight,\n+    )\n+    candidate_entities = get_entities_from_relationships(\n+        relationships=candidate_relationships, entities=entities\n+    )\n+    candidate_context[\"entities\"] = to_entity_dataframe(\n+        entities=candidate_entities,\n+        include_entity_rank=include_entity_rank,\n+        rank_description=entity_rank_description,\n+    )\n+\n+    for covariate in covariates:\n+        candidate_covariates = get_candidate_covariates(\n+            selected_entities=selected_entities,\n+            covariates=covariates[covariate],\n+        )\n+        candidate_context[covariate.lower()] = to_covariate_dataframe(\n+            candidate_covariates\n+        )\n+\n+    return candidate_context\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/rate_prompt.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/rate_prompt.py b/packages/graphrag/graphrag/query/context_builder/rate_prompt.py\nnew file mode 100644\nindex 0000000..6a5bcaf\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/rate_prompt.py\n@@ -0,0 +1,23 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Global search with dynamic community selection prompt.\"\"\"\n+\n+RATE_QUERY = \"\"\"\n+---Role---\n+You are a helpful assistant responsible for deciding whether the provided information is useful in answering a given question, even if it is only partially relevant.\n+---Goal---\n+On a scale from 0 to 5, please rate how relevant or helpful is the provided information in answering the question.\n+---Information---\n+{description}\n+---Question---\n+{question}\n+---Target response length and format---\n+Please response in the following JSON format with two entries:\n+- \"reason\": the reasoning of your rating, please include information that you have considered.\n+- \"rating\": the relevancy rating from 0 to 5, where 0 is the least relevant and 5 is the most relevant.\n+{{\n+    \"reason\": str,\n+    \"rating\": int.\n+}}\n+\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/rate_relevancy.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/rate_relevancy.py b/packages/graphrag/graphrag/query/context_builder/rate_relevancy.py\nnew file mode 100644\nindex 0000000..6fa1289\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/rate_relevancy.py\n@@ -0,0 +1,77 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Algorithm to rate the relevancy between a query and description text.\"\"\"\n+\n+import asyncio\n+import logging\n+from contextlib import nullcontext\n+from typing import Any\n+\n+import numpy as np\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.query.context_builder.rate_prompt import RATE_QUERY\n+from graphrag.query.llm.text_utils import try_parse_json_object\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def rate_relevancy(\n+    query: str,\n+    description: str,\n+    model: ChatModel,\n+    tokenizer: Tokenizer,\n+    rate_query: str = RATE_QUERY,\n+    num_repeats: int = 1,\n+    semaphore: asyncio.Semaphore | None = None,\n+    **model_params: Any,\n+) -> dict[str, Any]:\n+    \"\"\"\n+    Rate the relevancy between the query and description on a scale of 0 to 10.\n+\n+    Args:\n+        query: the query (or question) to rate against\n+        description: the community description to rate, it can be the community\n+            title, summary, or the full content.\n+        llm: LLM model to use for rating\n+        tokenizer: tokenizer\n+        num_repeats: number of times to repeat the rating process for the same community (default: 1)\n+        model_params: additional arguments to pass to the LLM model\n+        semaphore: asyncio.Semaphore to limit the number of concurrent LLM calls (default: None)\n+    \"\"\"\n+    llm_calls, prompt_tokens, output_tokens, ratings = 0, 0, 0, []\n+    messages = [\n+        {\n+            \"role\": \"system\",\n+            \"content\": rate_query.format(description=description, question=query),\n+        },\n+    ]\n+    for _ in range(num_repeats):\n+        async with semaphore if semaphore is not None else nullcontext():\n+            model_response = await model.achat(\n+                prompt=query, history=messages, model_parameters=model_params, json=True\n+            )\n+            response = model_response.output.content\n+        try:\n+            _, parsed_response = try_parse_json_object(response)\n+            ratings.append(parsed_response[\"rating\"])\n+        except KeyError:\n+            # in case of json parsing error, default to rating 1 so the report is kept.\n+            # json parsing error should rarely happen.\n+            logger.warning(\"Error parsing json response, defaulting to rating 1\")\n+            ratings.append(1)\n+        llm_calls += 1\n+        prompt_tokens += tokenizer.num_tokens(messages[0][\"content\"])\n+        output_tokens += tokenizer.num_tokens(response)\n+    # select the decision with the most votes\n+    options, counts = np.unique(ratings, return_counts=True)\n+    rating = int(options[np.argmax(counts)])\n+    return {\n+        \"rating\": rating,\n+        \"ratings\": ratings,\n+        \"llm_calls\": llm_calls,\n+        \"prompt_tokens\": prompt_tokens,\n+        \"output_tokens\": output_tokens,\n+    }\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/context_builder/source_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/context_builder/source_context.py b/packages/graphrag/graphrag/query/context_builder/source_context.py\nnew file mode 100644\nindex 0000000..eaf308f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/context_builder/source_context.py\n@@ -0,0 +1,100 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Context Build utility methods.\"\"\"\n+\n+import random\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\"\"\"\n+Contain util functions to build text unit context for the search's system prompt\n+\"\"\"\n+\n+\n+def build_text_unit_context(\n+    text_units: list[TextUnit],\n+    tokenizer: Tokenizer | None = None,\n+    column_delimiter: str = \"|\",\n+    shuffle_data: bool = True,\n+    max_context_tokens: int = 8000,\n+    context_name: str = \"Sources\",\n+    random_state: int = 86,\n+) -> tuple[str, dict[str, pd.DataFrame]]:\n+    \"\"\"Prepare text-unit data table as context data for system prompt.\"\"\"\n+    tokenizer = tokenizer or get_tokenizer()\n+    if text_units is None or len(text_units) == 0:\n+        return (\"\", {})\n+\n+    if shuffle_data:\n+        random.seed(random_state)\n+        random.shuffle(text_units)\n+\n+    # add context header\n+    current_context_text = f\"-----{context_name}-----\" + \"\\n\"\n+\n+    # add header\n+    header = [\"id\", \"text\"]\n+    attribute_cols = (\n+        list(text_units[0].attributes.keys()) if text_units[0].attributes else []\n+    )\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+\n+    current_context_text += column_delimiter.join(header) + \"\\n\"\n+    current_tokens = tokenizer.num_tokens(current_context_text)\n+    all_context_records = [header]\n+\n+    for unit in text_units:\n+        new_context = [\n+            unit.short_id,\n+            unit.text,\n+            *[\n+                str(unit.attributes.get(field, \"\")) if unit.attributes else \"\"\n+                for field in attribute_cols\n+            ],\n+        ]\n+        new_context_text = column_delimiter.join(new_context) + \"\\n\"\n+        new_tokens = tokenizer.num_tokens(new_context_text)\n+\n+        if current_tokens + new_tokens > max_context_tokens:\n+            break\n+\n+        current_context_text += new_context_text\n+        all_context_records.append(new_context)\n+        current_tokens += new_tokens\n+\n+    if len(all_context_records) > 1:\n+        record_df = pd.DataFrame(\n+            all_context_records[1:], columns=cast(\"Any\", all_context_records[0])\n+        )\n+    else:\n+        record_df = pd.DataFrame()\n+    return current_context_text, {context_name.lower(): record_df}\n+\n+\n+def count_relationships(\n+    entity_relationships: list[Relationship], text_unit: TextUnit\n+) -> int:\n+    \"\"\"Count the number of relationships of the selected entity that are associated with the text unit.\"\"\"\n+    if not text_unit.relationship_ids:\n+        # Use list comprehension to count relationships where the text_unit.id is in rel.text_unit_ids\n+        return sum(\n+            1\n+            for rel in entity_relationships\n+            if rel.text_unit_ids and text_unit.id in rel.text_unit_ids\n+        )\n+\n+    # Use a set for faster lookups if entity_relationships is large\n+    entity_relationship_ids = {rel.id for rel in entity_relationships}\n+\n+    # Count matching relationship ids efficiently\n+    return sum(\n+        1 for rel_id in text_unit.relationship_ids if rel_id in entity_relationship_ids\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/factory.py b/packages/graphrag/graphrag/query/factory.py\nnew file mode 100644\nindex 0000000..3f73c8c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/factory.py\n@@ -0,0 +1,303 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Query Factory methods to support CLI.\"\"\"\n+\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.data_model.community import Community\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.util import (\n+    get_openai_model_parameters_from_config,\n+)\n+from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n+from graphrag.query.structured_search.basic_search.basic_context import (\n+    BasicSearchContext,\n+)\n+from graphrag.query.structured_search.basic_search.search import BasicSearch\n+from graphrag.query.structured_search.drift_search.drift_context import (\n+    DRIFTSearchContextBuilder,\n+)\n+from graphrag.query.structured_search.drift_search.search import DRIFTSearch\n+from graphrag.query.structured_search.global_search.community_context import (\n+    GlobalCommunityContext,\n+)\n+from graphrag.query.structured_search.global_search.search import GlobalSearch\n+from graphrag.query.structured_search.local_search.mixed_context import (\n+    LocalSearchMixedContext,\n+)\n+from graphrag.query.structured_search.local_search.search import LocalSearch\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.vector_stores.base import BaseVectorStore\n+\n+\n+def get_local_search_engine(\n+    config: GraphRagConfig,\n+    reports: list[CommunityReport],\n+    text_units: list[TextUnit],\n+    entities: list[Entity],\n+    relationships: list[Relationship],\n+    covariates: dict[str, list[Covariate]],\n+    response_type: str,\n+    description_embedding_store: BaseVectorStore,\n+    system_prompt: str | None = None,\n+    callbacks: list[QueryCallbacks] | None = None,\n+) -> LocalSearch:\n+    \"\"\"Create a local search engine based on data + configuration.\"\"\"\n+    model_settings = config.get_language_model_config(config.local_search.chat_model_id)\n+\n+    chat_model = ModelManager().get_or_create_chat_model(\n+        name=\"local_search_chat\",\n+        model_type=model_settings.type,\n+        config=model_settings,\n+    )\n+\n+    embedding_settings = config.get_language_model_config(\n+        config.local_search.embedding_model_id\n+    )\n+\n+    embedding_model = ModelManager().get_or_create_embedding_model(\n+        name=\"local_search_embedding\",\n+        model_type=embedding_settings.type,\n+        config=embedding_settings,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config=model_settings)\n+\n+    ls_config = config.local_search\n+\n+    model_params = get_openai_model_parameters_from_config(model_settings)\n+\n+    return LocalSearch(\n+        model=chat_model,\n+        system_prompt=system_prompt,\n+        context_builder=LocalSearchMixedContext(\n+            community_reports=reports,\n+            text_units=text_units,\n+            entities=entities,\n+            relationships=relationships,\n+            covariates=covariates,\n+            entity_text_embeddings=description_embedding_store,\n+            embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n+            text_embedder=embedding_model,\n+            tokenizer=tokenizer,\n+        ),\n+        tokenizer=tokenizer,\n+        model_params=model_params,\n+        context_builder_params={\n+            \"text_unit_prop\": ls_config.text_unit_prop,\n+            \"community_prop\": ls_config.community_prop,\n+            \"conversation_history_max_turns\": ls_config.conversation_history_max_turns,\n+            \"conversation_history_user_turns_only\": True,\n+            \"top_k_mapped_entities\": ls_config.top_k_entities,\n+            \"top_k_relationships\": ls_config.top_k_relationships,\n+            \"include_entity_rank\": True,\n+            \"include_relationship_weight\": True,\n+            \"include_community_rank\": False,\n+            \"return_candidate_context\": False,\n+            \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n+            \"max_context_tokens\": ls_config.max_context_tokens,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n+        },\n+        response_type=response_type,\n+        callbacks=callbacks,\n+    )\n+\n+\n+def get_global_search_engine(\n+    config: GraphRagConfig,\n+    reports: list[CommunityReport],\n+    entities: list[Entity],\n+    communities: list[Community],\n+    response_type: str,\n+    dynamic_community_selection: bool = False,\n+    map_system_prompt: str | None = None,\n+    reduce_system_prompt: str | None = None,\n+    general_knowledge_inclusion_prompt: str | None = None,\n+    callbacks: list[QueryCallbacks] | None = None,\n+) -> GlobalSearch:\n+    \"\"\"Create a global search engine based on data + configuration.\"\"\"\n+    model_settings = config.get_language_model_config(\n+        config.global_search.chat_model_id\n+    )\n+\n+    model = ModelManager().get_or_create_chat_model(\n+        name=\"global_search\",\n+        model_type=model_settings.type,\n+        config=model_settings,\n+    )\n+\n+    model_params = get_openai_model_parameters_from_config(model_settings)\n+\n+    # Here we get encoding based on specified encoding name\n+    tokenizer = get_tokenizer(model_config=model_settings)\n+    gs_config = config.global_search\n+\n+    dynamic_community_selection_kwargs = {}\n+    if dynamic_community_selection:\n+        # TODO: Allow for another llm definition only for Global Search to leverage -mini models\n+\n+        dynamic_community_selection_kwargs.update({\n+            \"model\": model,\n+            \"tokenizer\": tokenizer,\n+            \"keep_parent\": gs_config.dynamic_search_keep_parent,\n+            \"num_repeats\": gs_config.dynamic_search_num_repeats,\n+            \"use_summary\": gs_config.dynamic_search_use_summary,\n+            \"concurrent_coroutines\": model_settings.concurrent_requests,\n+            \"threshold\": gs_config.dynamic_search_threshold,\n+            \"max_level\": gs_config.dynamic_search_max_level,\n+            \"model_params\": {**model_params},\n+        })\n+\n+    return GlobalSearch(\n+        model=model,\n+        map_system_prompt=map_system_prompt,\n+        reduce_system_prompt=reduce_system_prompt,\n+        general_knowledge_inclusion_prompt=general_knowledge_inclusion_prompt,\n+        context_builder=GlobalCommunityContext(\n+            community_reports=reports,\n+            communities=communities,\n+            entities=entities,\n+            tokenizer=tokenizer,\n+            dynamic_community_selection=dynamic_community_selection,\n+            dynamic_community_selection_kwargs=dynamic_community_selection_kwargs,\n+        ),\n+        tokenizer=tokenizer,\n+        max_data_tokens=gs_config.data_max_tokens,\n+        map_llm_params={**model_params},\n+        reduce_llm_params={**model_params},\n+        map_max_length=gs_config.map_max_length,\n+        reduce_max_length=gs_config.reduce_max_length,\n+        allow_general_knowledge=False,\n+        json_mode=False,\n+        context_builder_params={\n+            \"use_community_summary\": False,\n+            \"shuffle_data\": True,\n+            \"include_community_rank\": True,\n+            \"min_community_rank\": 0,\n+            \"community_rank_name\": \"rank\",\n+            \"include_community_weight\": True,\n+            \"community_weight_name\": \"occurrence weight\",\n+            \"normalize_community_weight\": True,\n+            \"max_context_tokens\": gs_config.max_context_tokens,\n+            \"context_name\": \"Reports\",\n+        },\n+        concurrent_coroutines=model_settings.concurrent_requests,\n+        response_type=response_type,\n+        callbacks=callbacks,\n+    )\n+\n+\n+def get_drift_search_engine(\n+    config: GraphRagConfig,\n+    reports: list[CommunityReport],\n+    text_units: list[TextUnit],\n+    entities: list[Entity],\n+    relationships: list[Relationship],\n+    description_embedding_store: BaseVectorStore,\n+    response_type: str,\n+    local_system_prompt: str | None = None,\n+    reduce_system_prompt: str | None = None,\n+    callbacks: list[QueryCallbacks] | None = None,\n+) -> DRIFTSearch:\n+    \"\"\"Create a local search engine based on data + configuration.\"\"\"\n+    chat_model_settings = config.get_language_model_config(\n+        config.drift_search.chat_model_id\n+    )\n+\n+    chat_model = ModelManager().get_or_create_chat_model(\n+        name=\"drift_search_chat\",\n+        model_type=chat_model_settings.type,\n+        config=chat_model_settings,\n+    )\n+\n+    embedding_model_settings = config.get_language_model_config(\n+        config.drift_search.embedding_model_id\n+    )\n+\n+    embedding_model = ModelManager().get_or_create_embedding_model(\n+        name=\"drift_search_embedding\",\n+        model_type=embedding_model_settings.type,\n+        config=embedding_model_settings,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config=chat_model_settings)\n+\n+    return DRIFTSearch(\n+        model=chat_model,\n+        context_builder=DRIFTSearchContextBuilder(\n+            model=chat_model,\n+            text_embedder=embedding_model,\n+            entities=entities,\n+            relationships=relationships,\n+            reports=reports,\n+            entity_text_embeddings=description_embedding_store,\n+            text_units=text_units,\n+            local_system_prompt=local_system_prompt,\n+            reduce_system_prompt=reduce_system_prompt,\n+            config=config.drift_search,\n+            response_type=response_type,\n+        ),\n+        tokenizer=tokenizer,\n+        callbacks=callbacks,\n+    )\n+\n+\n+def get_basic_search_engine(\n+    text_units: list[TextUnit],\n+    text_unit_embeddings: BaseVectorStore,\n+    config: GraphRagConfig,\n+    response_type: str,\n+    system_prompt: str | None = None,\n+    callbacks: list[QueryCallbacks] | None = None,\n+) -> BasicSearch:\n+    \"\"\"Create a basic search engine based on data + configuration.\"\"\"\n+    chat_model_settings = config.get_language_model_config(\n+        config.basic_search.chat_model_id\n+    )\n+\n+    chat_model = ModelManager().get_or_create_chat_model(\n+        name=\"basic_search_chat\",\n+        model_type=chat_model_settings.type,\n+        config=chat_model_settings,\n+    )\n+\n+    embedding_model_settings = config.get_language_model_config(\n+        config.basic_search.embedding_model_id\n+    )\n+\n+    embedding_model = ModelManager().get_or_create_embedding_model(\n+        name=\"basic_search_embedding\",\n+        model_type=embedding_model_settings.type,\n+        config=embedding_model_settings,\n+    )\n+\n+    tokenizer = get_tokenizer(model_config=chat_model_settings)\n+\n+    bs_config = config.basic_search\n+\n+    model_params = get_openai_model_parameters_from_config(chat_model_settings)\n+\n+    return BasicSearch(\n+        model=chat_model,\n+        system_prompt=system_prompt,\n+        response_type=response_type,\n+        context_builder=BasicSearchContext(\n+            text_embedder=embedding_model,\n+            text_unit_embeddings=text_unit_embeddings,\n+            text_units=text_units,\n+            tokenizer=tokenizer,\n+        ),\n+        tokenizer=tokenizer,\n+        model_params=model_params,\n+        context_builder_params={\n+            \"embedding_vectorstore_key\": \"id\",\n+            \"k\": bs_config.k,\n+            \"max_context_tokens\": bs_config.max_context_tokens,\n+        },\n+        callbacks=callbacks,\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/indexer_adapters.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/indexer_adapters.py b/packages/graphrag/graphrag/query/indexer_adapters.py\nnew file mode 100644\nindex 0000000..0c6e54a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/indexer_adapters.py\n@@ -0,0 +1,244 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"Indexing-Engine to Query Read Adapters.\n+\n+The parts of these functions that do type adaptation, renaming, collating, etc. should eventually go away.\n+Ideally this is just a straight read-through into the object model.\n+\"\"\"\n+\n+import logging\n+from typing import cast\n+\n+import pandas as pd\n+\n+from graphrag.config.models.graph_rag_config import GraphRagConfig\n+from graphrag.data_model.community import Community\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.language_model.manager import ModelManager\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.query.input.loaders.dfs import (\n+    read_communities,\n+    read_community_reports,\n+    read_covariates,\n+    read_entities,\n+    read_relationships,\n+    read_text_units,\n+)\n+from graphrag.vector_stores.base import BaseVectorStore\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def read_indexer_text_units(final_text_units: pd.DataFrame) -> list[TextUnit]:\n+    \"\"\"Read in the Text Units from the raw indexing outputs.\"\"\"\n+    return read_text_units(\n+        df=final_text_units,\n+        # expects a covariate map of type -> ids\n+        covariates_col=None,\n+    )\n+\n+\n+def read_indexer_covariates(final_covariates: pd.DataFrame) -> list[Covariate]:\n+    \"\"\"Read in the Claims from the raw indexing outputs.\"\"\"\n+    covariate_df = final_covariates\n+    covariate_df[\"id\"] = covariate_df[\"id\"].astype(str)\n+    return read_covariates(\n+        df=covariate_df,\n+        short_id_col=\"human_readable_id\",\n+        attributes_cols=[\n+            \"object_id\",\n+            \"status\",\n+            \"start_date\",\n+            \"end_date\",\n+            \"description\",\n+        ],\n+        text_unit_ids_col=None,\n+    )\n+\n+\n+def read_indexer_relationships(final_relationships: pd.DataFrame) -> list[Relationship]:\n+    \"\"\"Read in the Relationships from the raw indexing outputs.\"\"\"\n+    return read_relationships(\n+        df=final_relationships,\n+        short_id_col=\"human_readable_id\",\n+        rank_col=\"combined_degree\",\n+        description_embedding_col=None,\n+        attributes_cols=None,\n+    )\n+\n+\n+def read_indexer_reports(\n+    final_community_reports: pd.DataFrame,\n+    final_communities: pd.DataFrame,\n+    community_level: int | None,\n+    dynamic_community_selection: bool = False,\n+    content_embedding_col: str = \"full_content_embedding\",\n+    config: GraphRagConfig | None = None,\n+) -> list[CommunityReport]:\n+    \"\"\"Read in the Community Reports from the raw indexing outputs.\n+\n+    If not dynamic_community_selection, then select reports with the max community level that an entity belongs to.\n+    \"\"\"\n+    reports_df = final_community_reports\n+    nodes_df = final_communities.explode(\"entity_ids\")\n+\n+    if community_level is not None:\n+        nodes_df = _filter_under_community_level(nodes_df, community_level)\n+        reports_df = _filter_under_community_level(reports_df, community_level)\n+\n+    if not dynamic_community_selection:\n+        # perform community level roll up\n+        nodes_df.loc[:, \"community\"] = nodes_df[\"community\"].fillna(-1)\n+        nodes_df.loc[:, \"community\"] = nodes_df[\"community\"].astype(int)\n+\n+        nodes_df = nodes_df.groupby([\"title\"]).agg({\"community\": \"max\"}).reset_index()\n+        filtered_community_df = nodes_df[\"community\"].drop_duplicates()\n+\n+        reports_df = reports_df.merge(\n+            filtered_community_df, on=\"community\", how=\"inner\"\n+        )\n+\n+    if config and (\n+        content_embedding_col not in reports_df.columns\n+        or reports_df.loc[:, content_embedding_col].isna().any()\n+    ):\n+        # TODO: Find a way to retrieve the right embedding model id.\n+        embedding_model_settings = config.get_language_model_config(\n+            \"default_embedding_model\"\n+        )\n+        embedder = ModelManager().get_or_create_embedding_model(\n+            name=\"default_embedding\",\n+            model_type=embedding_model_settings.type,\n+            config=embedding_model_settings,\n+        )\n+        reports_df = embed_community_reports(\n+            reports_df, embedder, embedding_col=content_embedding_col\n+        )\n+\n+    return read_community_reports(\n+        df=reports_df,\n+        id_col=\"id\",\n+        short_id_col=\"community\",\n+        content_embedding_col=content_embedding_col,\n+    )\n+\n+\n+def read_indexer_report_embeddings(\n+    community_reports: list[CommunityReport],\n+    embeddings_store: BaseVectorStore,\n+):\n+    \"\"\"Read in the Community Reports from the raw indexing outputs.\"\"\"\n+    for report in community_reports:\n+        report.full_content_embedding = embeddings_store.search_by_id(report.id).vector\n+\n+\n+def read_indexer_entities(\n+    entities: pd.DataFrame,\n+    communities: pd.DataFrame,\n+    community_level: int | None,\n+) -> list[Entity]:\n+    \"\"\"Read in the Entities from the raw indexing outputs.\"\"\"\n+    community_join = communities.explode(\"entity_ids\").loc[\n+        :, [\"community\", \"level\", \"entity_ids\"]\n+    ]\n+    nodes_df = entities.merge(\n+        community_join, left_on=\"id\", right_on=\"entity_ids\", how=\"left\"\n+    )\n+\n+    if community_level is not None:\n+        nodes_df = _filter_under_community_level(nodes_df, community_level)\n+\n+    nodes_df = nodes_df.loc[:, [\"id\", \"community\"]]\n+    nodes_df[\"community\"] = nodes_df[\"community\"].fillna(-1)\n+    # group entities by id and degree and remove duplicated community IDs\n+    nodes_df = nodes_df.groupby([\"id\"]).agg({\"community\": set}).reset_index()\n+    nodes_df[\"community\"] = nodes_df[\"community\"].apply(\n+        lambda x: [str(int(i)) for i in x]\n+    )\n+    final_df = nodes_df.merge(entities, on=\"id\", how=\"inner\").drop_duplicates(\n+        subset=[\"id\"]\n+    )\n+    # read entity dataframe to knowledge model objects\n+    return read_entities(\n+        df=final_df,\n+        id_col=\"id\",\n+        title_col=\"title\",\n+        type_col=\"type\",\n+        short_id_col=\"human_readable_id\",\n+        description_col=\"description\",\n+        community_col=\"community\",\n+        rank_col=\"degree\",\n+        name_embedding_col=None,\n+        description_embedding_col=\"description_embedding\",\n+        text_unit_ids_col=\"text_unit_ids\",\n+    )\n+\n+\n+def read_indexer_communities(\n+    final_communities: pd.DataFrame,\n+    final_community_reports: pd.DataFrame,\n+) -> list[Community]:\n+    \"\"\"Read in the Communities from the raw indexing outputs.\n+\n+    Reconstruct the community hierarchy information and add to the sub-community field.\n+    \"\"\"\n+    communities_df = final_communities\n+    nodes_df = communities_df.explode(\"entity_ids\")\n+    reports_df = final_community_reports\n+\n+    # ensure communities matches community reports\n+    missing_reports = communities_df[\n+        ~communities_df.community.isin(reports_df.community.unique())\n+    ].community.to_list()\n+    if len(missing_reports):\n+        logger.warning(\"Missing reports for communities: %s\", missing_reports)\n+        communities_df = communities_df.loc[\n+            communities_df.community.isin(reports_df.community.unique())\n+        ]\n+        nodes_df = nodes_df.loc[nodes_df.community.isin(reports_df.community.unique())]\n+\n+    return read_communities(\n+        communities_df,\n+        id_col=\"id\",\n+        short_id_col=\"community\",\n+        title_col=\"title\",\n+        level_col=\"level\",\n+        entities_col=None,\n+        relationships_col=None,\n+        covariates_col=None,\n+        parent_col=\"parent\",\n+        children_col=\"children\",\n+        attributes_cols=None,\n+    )\n+\n+\n+def embed_community_reports(\n+    reports_df: pd.DataFrame,\n+    embedder: EmbeddingModel,\n+    source_col: str = \"full_content\",\n+    embedding_col: str = \"full_content_embedding\",\n+) -> pd.DataFrame:\n+    \"\"\"Embed a source column of the reports dataframe using the given embedder.\"\"\"\n+    if source_col not in reports_df.columns:\n+        error_msg = f\"Reports missing {source_col} column\"\n+        raise ValueError(error_msg)\n+\n+    if embedding_col not in reports_df.columns:\n+        reports_df[embedding_col] = reports_df.loc[:, source_col].apply(\n+            lambda x: embedder.embed(x)\n+        )\n+\n+    return reports_df\n+\n+\n+def _filter_under_community_level(\n+    df: pd.DataFrame, community_level: int\n+) -> pd.DataFrame:\n+    return cast(\n+        \"pd.DataFrame\",\n+        df[df.level <= community_level],\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/__init__.py b/packages/graphrag/graphrag/query/input/__init__.py\nnew file mode 100644\nindex 0000000..94ae973\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GraphRAG Orchestration Inputs.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/loaders/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/loaders/__init__.py b/packages/graphrag/graphrag/query/input/loaders/__init__.py\nnew file mode 100644\nindex 0000000..8f19dac\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/loaders/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GraphRAG Orchestartion Input Loaders.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/loaders/dfs.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/loaders/dfs.py b/packages/graphrag/graphrag/query/input/loaders/dfs.py\nnew file mode 100644\nindex 0000000..17aeb60\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/loaders/dfs.py\n@@ -0,0 +1,261 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Load data from dataframes into collections of data objects.\"\"\"\n+\n+import pandas as pd\n+\n+from graphrag.data_model.community import Community\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.query.input.loaders.utils import (\n+    to_list,\n+    to_optional_dict,\n+    to_optional_float,\n+    to_optional_int,\n+    to_optional_list,\n+    to_optional_str,\n+    to_str,\n+)\n+\n+\n+def _prepare_records(df: pd.DataFrame) -> list[dict]:\n+    \"\"\"\n+    Reset index and convert the DataFrame to a list of dictionaries.\n+\n+    We rename the reset index column to 'Index' for consistency.\n+    \"\"\"\n+    df_reset = df.reset_index().rename(columns={\"index\": \"Index\"})\n+    return df_reset.to_dict(\"records\")\n+\n+\n+def read_entities(\n+    df: pd.DataFrame,\n+    id_col: str = \"id\",\n+    short_id_col: str | None = \"human_readable_id\",\n+    title_col: str = \"title\",\n+    type_col: str | None = \"type\",\n+    description_col: str | None = \"description\",\n+    name_embedding_col: str | None = \"name_embedding\",\n+    description_embedding_col: str | None = \"description_embedding\",\n+    community_col: str | None = \"community_ids\",\n+    text_unit_ids_col: str | None = \"text_unit_ids\",\n+    rank_col: str | None = \"degree\",\n+    attributes_cols: list[str] | None = None,\n+) -> list[Entity]:\n+    \"\"\"Read entities from a dataframe using pre-converted records.\"\"\"\n+    records = _prepare_records(df)\n+    return [\n+        Entity(\n+            id=to_str(row, id_col),\n+            short_id=to_optional_str(row, short_id_col)\n+            if short_id_col\n+            else str(row[\"Index\"]),\n+            title=to_str(row, title_col),\n+            type=to_optional_str(row, type_col),\n+            description=to_optional_str(row, description_col),\n+            name_embedding=to_optional_list(row, name_embedding_col, item_type=float),\n+            description_embedding=to_optional_list(\n+                row, description_embedding_col, item_type=float\n+            ),\n+            community_ids=to_optional_list(row, community_col, item_type=str),\n+            text_unit_ids=to_optional_list(row, text_unit_ids_col),\n+            rank=to_optional_int(row, rank_col),\n+            attributes=(\n+                {col: row.get(col) for col in attributes_cols}\n+                if attributes_cols\n+                else None\n+            ),\n+        )\n+        for row in records\n+    ]\n+\n+\n+def read_relationships(\n+    df: pd.DataFrame,\n+    id_col: str = \"id\",\n+    short_id_col: str | None = \"human_readable_id\",\n+    source_col: str = \"source\",\n+    target_col: str = \"target\",\n+    description_col: str | None = \"description\",\n+    rank_col: str | None = \"combined_degree\",\n+    description_embedding_col: str | None = \"description_embedding\",\n+    weight_col: str | None = \"weight\",\n+    text_unit_ids_col: str | None = \"text_unit_ids\",\n+    attributes_cols: list[str] | None = None,\n+) -> list[Relationship]:\n+    \"\"\"Read relationships from a dataframe using pre-converted records.\"\"\"\n+    records = _prepare_records(df)\n+    return [\n+        Relationship(\n+            id=to_str(row, id_col),\n+            short_id=to_optional_str(row, short_id_col)\n+            if short_id_col\n+            else str(row[\"Index\"]),\n+            source=to_str(row, source_col),\n+            target=to_str(row, target_col),\n+            description=to_optional_str(row, description_col),\n+            description_embedding=to_optional_list(\n+                row, description_embedding_col, item_type=float\n+            ),\n+            weight=to_optional_float(row, weight_col),\n+            text_unit_ids=to_optional_list(row, text_unit_ids_col, item_type=str),\n+            rank=to_optional_int(row, rank_col),\n+            attributes=(\n+                {col: row.get(col) for col in attributes_cols}\n+                if attributes_cols\n+                else None\n+            ),\n+        )\n+        for row in records\n+    ]\n+\n+\n+def read_covariates(\n+    df: pd.DataFrame,\n+    id_col: str = \"id\",\n+    short_id_col: str | None = \"human_readable_id\",\n+    subject_col: str = \"subject_id\",\n+    covariate_type_col: str | None = \"type\",\n+    text_unit_ids_col: str | None = \"text_unit_ids\",\n+    attributes_cols: list[str] | None = None,\n+) -> list[Covariate]:\n+    \"\"\"Read covariates from a dataframe using pre-converted records.\"\"\"\n+    records = _prepare_records(df)\n+    return [\n+        Covariate(\n+            id=to_str(row, id_col),\n+            short_id=to_optional_str(row, short_id_col)\n+            if short_id_col\n+            else str(row[\"Index\"]),\n+            subject_id=to_str(row, subject_col),\n+            covariate_type=(\n+                to_str(row, covariate_type_col) if covariate_type_col else \"claim\"\n+            ),\n+            text_unit_ids=to_optional_list(row, text_unit_ids_col, item_type=str),\n+            attributes=(\n+                {col: row.get(col) for col in attributes_cols}\n+                if attributes_cols\n+                else None\n+            ),\n+        )\n+        for row in records\n+    ]\n+\n+\n+def read_communities(\n+    df: pd.DataFrame,\n+    id_col: str = \"id\",\n+    short_id_col: str | None = \"community\",\n+    title_col: str = \"title\",\n+    level_col: str = \"level\",\n+    entities_col: str | None = \"entity_ids\",\n+    relationships_col: str | None = \"relationship_ids\",\n+    text_units_col: str | None = \"text_unit_ids\",\n+    covariates_col: str | None = \"covariate_ids\",\n+    parent_col: str | None = \"parent\",\n+    children_col: str | None = \"children\",\n+    attributes_cols: list[str] | None = None,\n+) -> list[Community]:\n+    \"\"\"Read communities from a dataframe using pre-converted records.\"\"\"\n+    records = _prepare_records(df)\n+    return [\n+        Community(\n+            id=to_str(row, id_col),\n+            short_id=to_optional_str(row, short_id_col)\n+            if short_id_col\n+            else str(row[\"Index\"]),\n+            title=to_str(row, title_col),\n+            level=to_str(row, level_col),\n+            entity_ids=to_optional_list(row, entities_col, item_type=str),\n+            relationship_ids=to_optional_list(row, relationships_col, item_type=str),\n+            text_unit_ids=to_optional_list(row, text_units_col, item_type=str),\n+            covariate_ids=to_optional_dict(\n+                row, covariates_col, key_type=str, value_type=str\n+            ),\n+            parent=to_str(row, parent_col),\n+            children=to_list(row, children_col),\n+            attributes=(\n+                {col: row.get(col) for col in attributes_cols}\n+                if attributes_cols\n+                else None\n+            ),\n+        )\n+        for row in records\n+    ]\n+\n+\n+def read_community_reports(\n+    df: pd.DataFrame,\n+    id_col: str = \"id\",\n+    short_id_col: str | None = \"community\",\n+    title_col: str = \"title\",\n+    community_col: str = \"community\",\n+    summary_col: str = \"summary\",\n+    content_col: str = \"full_content\",\n+    rank_col: str | None = \"rank\",\n+    content_embedding_col: str | None = \"full_content_embedding\",\n+    attributes_cols: list[str] | None = None,\n+) -> list[CommunityReport]:\n+    \"\"\"Read community reports from a dataframe using pre-converted records.\"\"\"\n+    records = _prepare_records(df)\n+    return [\n+        CommunityReport(\n+            id=to_str(row, id_col),\n+            short_id=to_optional_str(row, short_id_col)\n+            if short_id_col\n+            else str(row[\"Index\"]),\n+            title=to_str(row, title_col),\n+            community_id=to_str(row, community_col),\n+            summary=to_str(row, summary_col),\n+            full_content=to_str(row, content_col),\n+            rank=to_optional_float(row, rank_col),\n+            full_content_embedding=to_optional_list(\n+                row, content_embedding_col, item_type=float\n+            ),\n+            attributes=(\n+                {col: row.get(col) for col in attributes_cols}\n+                if attributes_cols\n+                else None\n+            ),\n+        )\n+        for row in records\n+    ]\n+\n+\n+def read_text_units(\n+    df: pd.DataFrame,\n+    id_col: str = \"id\",\n+    text_col: str = \"text\",\n+    entities_col: str | None = \"entity_ids\",\n+    relationships_col: str | None = \"relationship_ids\",\n+    covariates_col: str | None = \"covariate_ids\",\n+    tokens_col: str | None = \"n_tokens\",\n+    document_id_col: str | None = \"document_id\",\n+    attributes_cols: list[str] | None = None,\n+) -> list[TextUnit]:\n+    \"\"\"Read text units from a dataframe using pre-converted records.\"\"\"\n+    records = _prepare_records(df)\n+    return [\n+        TextUnit(\n+            id=to_str(row, id_col),\n+            short_id=str(row[\"Index\"]),\n+            text=to_str(row, text_col),\n+            entity_ids=to_optional_list(row, entities_col, item_type=str),\n+            relationship_ids=to_optional_list(row, relationships_col, item_type=str),\n+            covariate_ids=to_optional_dict(\n+                row, covariates_col, key_type=str, value_type=str\n+            ),\n+            n_tokens=to_optional_int(row, tokens_col),\n+            document_id=to_optional_str(row, document_id_col),\n+            attributes=(\n+                {col: row.get(col) for col in attributes_cols}\n+                if attributes_cols\n+                else None\n+            ),\n+        )\n+        for row in records\n+    ]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/loaders/utils.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/loaders/utils.py b/packages/graphrag/graphrag/query/input/loaders/utils.py\nnew file mode 100644\nindex 0000000..40fe2cf\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/loaders/utils.py\n@@ -0,0 +1,187 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Data load utils.\"\"\"\n+\n+from collections.abc import Mapping\n+from typing import Any\n+\n+import numpy as np\n+\n+\n+def _get_value(\n+    data: Mapping[str, Any], column_name: str | None, required: bool = True\n+) -> Any:\n+    \"\"\"\n+    Retrieve a column value from data.\n+\n+    If `required` is True, raises a ValueError when:\n+      - column_name is None, or\n+      - column_name is not in data.\n+\n+    For optional columns (required=False), returns None if column_name is None.\n+    \"\"\"\n+    if column_name is None:\n+        if required:\n+            msg = \"Column name is None\"\n+            raise ValueError(msg)\n+        return None\n+    if column_name in data:\n+        return data[column_name]\n+    if required:\n+        msg = f\"Column [{column_name}] not found in data\"\n+        raise ValueError(msg)\n+    return None\n+\n+\n+def to_str(data: Mapping[str, Any], column_name: str | None) -> str:\n+    \"\"\"Convert and validate a value to a string.\"\"\"\n+    value = _get_value(data, column_name, required=True)\n+    return str(value)\n+\n+\n+def to_optional_str(data: Mapping[str, Any], column_name: str | None) -> str | None:\n+    \"\"\"Convert and validate a value to an optional string.\"\"\"\n+    value = _get_value(data, column_name, required=True)\n+    return None if value is None else str(value)\n+\n+\n+def to_list(\n+    data: Mapping[str, Any], column_name: str | None, item_type: type | None = None\n+) -> list:\n+    \"\"\"Convert and validate a value to a list.\"\"\"\n+    value = _get_value(data, column_name, required=True)\n+    if isinstance(value, np.ndarray):\n+        value = value.tolist()\n+    if not isinstance(value, list):\n+        msg = f\"value is not a list: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    if item_type is not None:\n+        for v in value:\n+            if not isinstance(v, item_type):\n+                msg = f\"list item is not [{item_type}]: {v} ({type(v)})\"\n+                raise TypeError(msg)\n+    return value\n+\n+\n+def to_optional_list(\n+    data: Mapping[str, Any], column_name: str | None, item_type: type | None = None\n+) -> list | None:\n+    \"\"\"Convert and validate a value to an optional list.\"\"\"\n+    if column_name is None or column_name not in data:\n+        return None\n+    value = data[column_name]\n+    if value is None:\n+        return None\n+    if isinstance(value, np.ndarray):\n+        value = value.tolist()\n+    if isinstance(value, str):\n+        value = [value]\n+    if not isinstance(value, list):\n+        msg = f\"value is not a list: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    if item_type is not None:\n+        for v in value:\n+            if not isinstance(v, item_type):\n+                msg = f\"list item is not [{item_type}]: {v} ({type(v)})\"\n+                raise TypeError(msg)\n+    return value\n+\n+\n+def to_int(data: Mapping[str, Any], column_name: str | None) -> int:\n+    \"\"\"Convert and validate a value to an int.\"\"\"\n+    value = _get_value(data, column_name, required=True)\n+    if isinstance(value, float):\n+        value = int(value)\n+    if not isinstance(value, int):\n+        msg = f\"value is not an int: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    return int(value)\n+\n+\n+def to_optional_int(data: Mapping[str, Any], column_name: str | None) -> int | None:\n+    \"\"\"Convert and validate a value to an optional int.\"\"\"\n+    if column_name is None or column_name not in data:\n+        return None\n+    value = data[column_name]\n+    if value is None:\n+        return None\n+    if isinstance(value, float):\n+        value = int(value)\n+    if not isinstance(value, int):\n+        msg = f\"value is not an int: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    return int(value)\n+\n+\n+def to_float(data: Mapping[str, Any], column_name: str | None) -> float:\n+    \"\"\"Convert and validate a value to a float.\"\"\"\n+    value = _get_value(data, column_name, required=True)\n+    if not isinstance(value, float):\n+        msg = f\"value is not a float: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    return float(value)\n+\n+\n+def to_optional_float(data: Mapping[str, Any], column_name: str | None) -> float | None:\n+    \"\"\"Convert and validate a value to an optional float.\"\"\"\n+    if column_name is None or column_name not in data:\n+        return None\n+    value = data[column_name]\n+    if value is None:\n+        return None\n+    if not isinstance(value, float):\n+        return float(value)\n+    return float(value)\n+\n+\n+def to_dict(\n+    data: Mapping[str, Any],\n+    column_name: str | None,\n+    key_type: type | None = None,\n+    value_type: type | None = None,\n+) -> dict:\n+    \"\"\"Convert and validate a value to a dict.\"\"\"\n+    value = _get_value(data, column_name, required=True)\n+    if not isinstance(value, dict):\n+        msg = f\"value is not a dict: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    if key_type is not None:\n+        for k in value:\n+            if not isinstance(k, key_type):\n+                msg = f\"dict key is not [{key_type}]: {k} ({type(k)})\"\n+                raise TypeError(msg)\n+    if value_type is not None:\n+        for v in value.values():\n+            if not isinstance(v, value_type):\n+                msg = f\"dict value is not [{value_type}]: {v} ({type(v)})\"\n+                raise TypeError(msg)\n+    return value\n+\n+\n+def to_optional_dict(\n+    data: Mapping[str, Any],\n+    column_name: str | None,\n+    key_type: type | None = None,\n+    value_type: type | None = None,\n+) -> dict | None:\n+    \"\"\"Convert and validate a value to an optional dict.\"\"\"\n+    if column_name is None or column_name not in data:\n+        return None\n+    value = data[column_name]\n+    if value is None:\n+        return None\n+    if not isinstance(value, dict):\n+        msg = f\"value is not a dict: {value} ({type(value)})\"\n+        raise TypeError(msg)\n+    if key_type is not None:\n+        for k in value:\n+            if not isinstance(k, key_type):\n+                msg = f\"dict key is not [{key_type}]: {k} ({type(k)})\"\n+                raise TypeError(msg)\n+    if value_type is not None:\n+        for v in value.values():\n+            if not isinstance(v, value_type):\n+                msg = f\"dict value is not [{value_type}]: {v} ({type(v)})\"\n+                raise TypeError(msg)\n+    return value\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/retrieval/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/retrieval/__init__.py b/packages/graphrag/graphrag/query/input/retrieval/__init__.py\nnew file mode 100644\nindex 0000000..75c2f9f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/retrieval/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GraphRAG Orchestration Input Retrieval.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/retrieval/community_reports.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/retrieval/community_reports.py b/packages/graphrag/graphrag/query/input/retrieval/community_reports.py\nnew file mode 100644\nindex 0000000..c10e410\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/retrieval/community_reports.py\n@@ -0,0 +1,75 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions to retrieve community reports from a collection.\"\"\"\n+\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.entity import Entity\n+\n+\n+def get_candidate_communities(\n+    selected_entities: list[Entity],\n+    community_reports: list[CommunityReport],\n+    include_community_rank: bool = False,\n+    use_community_summary: bool = False,\n+) -> pd.DataFrame:\n+    \"\"\"Get all communities that are related to selected entities.\"\"\"\n+    selected_community_ids = [\n+        entity.community_ids for entity in selected_entities if entity.community_ids\n+    ]\n+    selected_community_ids = [\n+        item for sublist in selected_community_ids for item in sublist\n+    ]\n+    selected_reports = [\n+        community\n+        for community in community_reports\n+        if community.id in selected_community_ids\n+    ]\n+    return to_community_report_dataframe(\n+        reports=selected_reports,\n+        include_community_rank=include_community_rank,\n+        use_community_summary=use_community_summary,\n+    )\n+\n+\n+def to_community_report_dataframe(\n+    reports: list[CommunityReport],\n+    include_community_rank: bool = False,\n+    use_community_summary: bool = False,\n+) -> pd.DataFrame:\n+    \"\"\"Convert a list of communities to a pandas dataframe.\"\"\"\n+    if len(reports) == 0:\n+        return pd.DataFrame()\n+\n+    # add header\n+    header = [\"id\", \"title\"]\n+    attribute_cols = list(reports[0].attributes.keys()) if reports[0].attributes else []\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+    header.append(\"summary\" if use_community_summary else \"content\")\n+    if include_community_rank:\n+        header.append(\"rank\")\n+\n+    records = []\n+    for report in reports:\n+        new_record = [\n+            report.short_id if report.short_id else \"\",\n+            report.title,\n+            *[\n+                str(report.attributes.get(field, \"\"))\n+                if report.attributes and report.attributes.get(field)\n+                else \"\"\n+                for field in attribute_cols\n+            ],\n+        ]\n+        new_record.append(\n+            report.summary if use_community_summary else report.full_content\n+        )\n+        if include_community_rank:\n+            new_record.append(str(report.rank))\n+        records.append(new_record)\n+    return pd.DataFrame(records, columns=cast(\"Any\", header))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/retrieval/covariates.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/retrieval/covariates.py b/packages/graphrag/graphrag/query/input/retrieval/covariates.py\nnew file mode 100644\nindex 0000000..3aaf96f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/retrieval/covariates.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions to retrieve covariates from a collection.\"\"\"\n+\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+\n+\n+def get_candidate_covariates(\n+    selected_entities: list[Entity],\n+    covariates: list[Covariate],\n+) -> list[Covariate]:\n+    \"\"\"Get all covariates that are related to selected entities.\"\"\"\n+    selected_entity_names = [entity.title for entity in selected_entities]\n+    return [\n+        covariate\n+        for covariate in covariates\n+        if covariate.subject_id in selected_entity_names\n+    ]\n+\n+\n+def to_covariate_dataframe(covariates: list[Covariate]) -> pd.DataFrame:\n+    \"\"\"Convert a list of covariates to a pandas dataframe.\"\"\"\n+    if len(covariates) == 0:\n+        return pd.DataFrame()\n+\n+    # add header\n+    header = [\"id\", \"entity\"]\n+    attributes = covariates[0].attributes or {} if len(covariates) > 0 else {}\n+    attribute_cols = list(attributes.keys()) if len(covariates) > 0 else []\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+\n+    records = []\n+    for covariate in covariates:\n+        new_record = [\n+            covariate.short_id if covariate.short_id else \"\",\n+            covariate.subject_id,\n+        ]\n+        for field in attribute_cols:\n+            field_value = (\n+                str(covariate.attributes.get(field))\n+                if covariate.attributes and covariate.attributes.get(field)\n+                else \"\"\n+            )\n+            new_record.append(field_value)\n+        records.append(new_record)\n+    return pd.DataFrame(records, columns=cast(\"Any\", header))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/retrieval/entities.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/retrieval/entities.py b/packages/graphrag/graphrag/query/input/retrieval/entities.py\nnew file mode 100644\nindex 0000000..b8b5eea\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/retrieval/entities.py\n@@ -0,0 +1,102 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions to get entities from a collection.\"\"\"\n+\n+import uuid\n+from collections.abc import Iterable\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.entity import Entity\n+\n+\n+def get_entity_by_id(entities: dict[str, Entity], value: str) -> Entity | None:\n+    \"\"\"Get entity by id.\"\"\"\n+    entity = entities.get(value)\n+    if entity is None and is_valid_uuid(value):\n+        entity = entities.get(value.replace(\"-\", \"\"))\n+    return entity\n+\n+\n+def get_entity_by_key(\n+    entities: Iterable[Entity], key: str, value: str | int\n+) -> Entity | None:\n+    \"\"\"Get entity by key.\"\"\"\n+    if isinstance(value, str) and is_valid_uuid(value):\n+        value_no_dashes = value.replace(\"-\", \"\")\n+        for entity in entities:\n+            entity_value = getattr(entity, key)\n+            if entity_value in (value, value_no_dashes):\n+                return entity\n+    else:\n+        for entity in entities:\n+            if getattr(entity, key) == value:\n+                return entity\n+    return None\n+\n+\n+def get_entity_by_name(entities: Iterable[Entity], entity_name: str) -> list[Entity]:\n+    \"\"\"Get entities by name.\"\"\"\n+    return [entity for entity in entities if entity.title == entity_name]\n+\n+\n+def get_entity_by_attribute(\n+    entities: Iterable[Entity], attribute_name: str, attribute_value: Any\n+) -> list[Entity]:\n+    \"\"\"Get entities by attribute.\"\"\"\n+    return [\n+        entity\n+        for entity in entities\n+        if entity.attributes\n+        and entity.attributes.get(attribute_name) == attribute_value\n+    ]\n+\n+\n+def to_entity_dataframe(\n+    entities: list[Entity],\n+    include_entity_rank: bool = True,\n+    rank_description: str = \"number of relationships\",\n+) -> pd.DataFrame:\n+    \"\"\"Convert a list of entities to a pandas dataframe.\"\"\"\n+    if len(entities) == 0:\n+        return pd.DataFrame()\n+    header = [\"id\", \"entity\", \"description\"]\n+    if include_entity_rank:\n+        header.append(rank_description)\n+    attribute_cols = (\n+        list(entities[0].attributes.keys()) if entities[0].attributes else []\n+    )\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+\n+    records = []\n+    for entity in entities:\n+        new_record = [\n+            entity.short_id if entity.short_id else \"\",\n+            entity.title,\n+            entity.description if entity.description else \"\",\n+        ]\n+        if include_entity_rank:\n+            new_record.append(str(entity.rank))\n+\n+        for field in attribute_cols:\n+            field_value = (\n+                str(entity.attributes.get(field))\n+                if entity.attributes and entity.attributes.get(field)\n+                else \"\"\n+            )\n+            new_record.append(field_value)\n+        records.append(new_record)\n+    return pd.DataFrame(records, columns=cast(\"Any\", header))\n+\n+\n+def is_valid_uuid(value: str) -> bool:\n+    \"\"\"Determine if a string is a valid UUID.\"\"\"\n+    try:\n+        uuid.UUID(str(value))\n+    except ValueError:\n+        return False\n+    else:\n+        return True\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/retrieval/relationships.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/retrieval/relationships.py b/packages/graphrag/graphrag/query/input/retrieval/relationships.py\nnew file mode 100644\nindex 0000000..fdb3f81\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/retrieval/relationships.py\n@@ -0,0 +1,139 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions to retrieve relationships from a collection.\"\"\"\n+\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+\n+\n+def get_in_network_relationships(\n+    selected_entities: list[Entity],\n+    relationships: list[Relationship],\n+    ranking_attribute: str = \"rank\",\n+) -> list[Relationship]:\n+    \"\"\"Get all directed relationships between selected entities, sorted by ranking_attribute.\"\"\"\n+    selected_entity_names = [entity.title for entity in selected_entities]\n+    selected_relationships = [\n+        relationship\n+        for relationship in relationships\n+        if relationship.source in selected_entity_names\n+        and relationship.target in selected_entity_names\n+    ]\n+    if len(selected_relationships) <= 1:\n+        return selected_relationships\n+\n+    # sort by ranking attribute\n+    return sort_relationships_by_rank(selected_relationships, ranking_attribute)\n+\n+\n+def get_out_network_relationships(\n+    selected_entities: list[Entity],\n+    relationships: list[Relationship],\n+    ranking_attribute: str = \"rank\",\n+) -> list[Relationship]:\n+    \"\"\"Get relationships from selected entities to other entities that are not within the selected entities, sorted by ranking_attribute.\"\"\"\n+    selected_entity_names = [entity.title for entity in selected_entities]\n+    source_relationships = [\n+        relationship\n+        for relationship in relationships\n+        if relationship.source in selected_entity_names\n+        and relationship.target not in selected_entity_names\n+    ]\n+    target_relationships = [\n+        relationship\n+        for relationship in relationships\n+        if relationship.target in selected_entity_names\n+        and relationship.source not in selected_entity_names\n+    ]\n+    selected_relationships = source_relationships + target_relationships\n+    return sort_relationships_by_rank(selected_relationships, ranking_attribute)\n+\n+\n+def get_candidate_relationships(\n+    selected_entities: list[Entity],\n+    relationships: list[Relationship],\n+) -> list[Relationship]:\n+    \"\"\"Get all relationships that are associated with the selected entities.\"\"\"\n+    selected_entity_names = [entity.title for entity in selected_entities]\n+    return [\n+        relationship\n+        for relationship in relationships\n+        if relationship.source in selected_entity_names\n+        or relationship.target in selected_entity_names\n+    ]\n+\n+\n+def get_entities_from_relationships(\n+    relationships: list[Relationship], entities: list[Entity]\n+) -> list[Entity]:\n+    \"\"\"Get all entities that are associated with the selected relationships.\"\"\"\n+    selected_entity_names = [relationship.source for relationship in relationships] + [\n+        relationship.target for relationship in relationships\n+    ]\n+    return [entity for entity in entities if entity.title in selected_entity_names]\n+\n+\n+def sort_relationships_by_rank(\n+    relationships: list[Relationship],\n+    ranking_attribute: str = \"rank\",\n+) -> list[Relationship]:\n+    \"\"\"Sort relationships by a ranking_attribute.\"\"\"\n+    if len(relationships) == 0:\n+        return relationships\n+\n+    # sort by ranking attribute\n+    attribute_names = (\n+        list(relationships[0].attributes.keys()) if relationships[0].attributes else []\n+    )\n+    if ranking_attribute in attribute_names:\n+        relationships.sort(\n+            key=lambda x: int(x.attributes[ranking_attribute]) if x.attributes else 0,\n+            reverse=True,\n+        )\n+    elif ranking_attribute == \"rank\":\n+        relationships.sort(key=lambda x: x.rank if x.rank else 0.0, reverse=True)\n+    elif ranking_attribute == \"weight\":\n+        relationships.sort(key=lambda x: x.weight if x.weight else 0.0, reverse=True)\n+    return relationships\n+\n+\n+def to_relationship_dataframe(\n+    relationships: list[Relationship], include_relationship_weight: bool = True\n+) -> pd.DataFrame:\n+    \"\"\"Convert a list of relationships to a pandas dataframe.\"\"\"\n+    if len(relationships) == 0:\n+        return pd.DataFrame()\n+\n+    header = [\"id\", \"source\", \"target\", \"description\"]\n+    if include_relationship_weight:\n+        header.append(\"weight\")\n+    attribute_cols = (\n+        list(relationships[0].attributes.keys()) if relationships[0].attributes else []\n+    )\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+\n+    records = []\n+    for rel in relationships:\n+        new_record = [\n+            rel.short_id if rel.short_id else \"\",\n+            rel.source,\n+            rel.target,\n+            rel.description if rel.description else \"\",\n+        ]\n+        if include_relationship_weight:\n+            new_record.append(str(rel.weight if rel.weight else \"\"))\n+        for field in attribute_cols:\n+            field_value = (\n+                str(rel.attributes.get(field))\n+                if rel.attributes and rel.attributes.get(field)\n+                else \"\"\n+            )\n+            new_record.append(field_value)\n+        records.append(new_record)\n+    return pd.DataFrame(records, columns=cast(\"Any\", header))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/input/retrieval/text_units.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/input/retrieval/text_units.py b/packages/graphrag/graphrag/query/input/retrieval/text_units.py\nnew file mode 100644\nindex 0000000..b57a2fc\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/input/retrieval/text_units.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions to retrieve text units from a collection.\"\"\"\n+\n+from typing import Any, cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.text_unit import TextUnit\n+\n+\n+def get_candidate_text_units(\n+    selected_entities: list[Entity],\n+    text_units: list[TextUnit],\n+) -> pd.DataFrame:\n+    \"\"\"Get all text units that are associated to selected entities.\"\"\"\n+    selected_text_ids = [\n+        entity.text_unit_ids for entity in selected_entities if entity.text_unit_ids\n+    ]\n+    selected_text_ids = [item for sublist in selected_text_ids for item in sublist]\n+    selected_text_units = [unit for unit in text_units if unit.id in selected_text_ids]\n+    return to_text_unit_dataframe(selected_text_units)\n+\n+\n+def to_text_unit_dataframe(text_units: list[TextUnit]) -> pd.DataFrame:\n+    \"\"\"Convert a list of text units to a pandas dataframe.\"\"\"\n+    if len(text_units) == 0:\n+        return pd.DataFrame()\n+\n+    # add header\n+    header = [\"id\", \"text\"]\n+    attribute_cols = (\n+        list(text_units[0].attributes.keys()) if text_units[0].attributes else []\n+    )\n+    attribute_cols = [col for col in attribute_cols if col not in header]\n+    header.extend(attribute_cols)\n+\n+    records = []\n+    for unit in text_units:\n+        new_record = [\n+            unit.short_id,\n+            unit.text,\n+            *[\n+                str(unit.attributes.get(field, \"\"))\n+                if unit.attributes and unit.attributes.get(field)\n+                else \"\"\n+                for field in attribute_cols\n+            ],\n+        ]\n+        records.append(new_record)\n+    return pd.DataFrame(records, columns=cast(\"Any\", header))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/llm/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/llm/__init__.py b/packages/graphrag/graphrag/query/llm/__init__.py\nnew file mode 100644\nindex 0000000..b8f507b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/llm/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Orchestration LLM utilities.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/llm/text_utils.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/llm/text_utils.py b/packages/graphrag/graphrag/query/llm/text_utils.py\nnew file mode 100644\nindex 0000000..ddd6abe\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/llm/text_utils.py\n@@ -0,0 +1,102 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Text Utilities for LLM.\"\"\"\n+\n+import json\n+import logging\n+import re\n+from collections.abc import Iterator\n+from itertools import islice\n+\n+from json_repair import repair_json\n+\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def batched(iterable: Iterator, n: int):\n+    \"\"\"\n+    Batch data into tuples of length n. The last batch may be shorter.\n+\n+    Taken from Python's cookbook: https://docs.python.org/3/library/itertools.html#itertools.batched\n+    \"\"\"\n+    # batched('ABCDEFG', 3) --> ABC DEF G\n+    if n < 1:\n+        value_error = \"n must be at least one\"\n+        raise ValueError(value_error)\n+    it = iter(iterable)\n+    while batch := tuple(islice(it, n)):\n+        yield batch\n+\n+\n+def chunk_text(text: str, max_tokens: int, tokenizer: Tokenizer | None = None):\n+    \"\"\"Chunk text by token length.\"\"\"\n+    if tokenizer is None:\n+        tokenizer = get_tokenizer()\n+    tokens = tokenizer.encode(text)  # type: ignore\n+    chunk_iterator = batched(iter(tokens), max_tokens)\n+    yield from (tokenizer.decode(list(chunk)) for chunk in chunk_iterator)\n+\n+\n+def try_parse_json_object(input: str, verbose: bool = True) -> tuple[str, dict]:\n+    \"\"\"JSON cleaning and formatting utilities.\"\"\"\n+    # Sometimes, the LLM returns a json string with some extra description, this function will clean it up.\n+\n+    result = None\n+    try:\n+        # Try parse first\n+        result = json.loads(input)\n+    except json.JSONDecodeError:\n+        if verbose:\n+            logger.warning(\"Error decoding faulty json, attempting repair\")\n+\n+    if result:\n+        return input, result\n+\n+    pattern = r\"\\{(.*)\\}\"\n+    match = re.search(pattern, input, re.DOTALL)\n+    input = \"{\" + match.group(1) + \"}\" if match else input\n+\n+    # Clean up json string.\n+    input = (\n+        input.replace(\"{{\", \"{\")\n+        .replace(\"}}\", \"}\")\n+        .replace('\"[{', \"[{\")\n+        .replace('}]\"', \"}]\")\n+        .replace(\"\\\\\", \" \")\n+        .replace(\"\\\\n\", \" \")\n+        .replace(\"\\n\", \" \")\n+        .replace(\"\\r\", \"\")\n+        .strip()\n+    )\n+\n+    # Remove JSON Markdown Frame\n+    if input.startswith(\"```json\"):\n+        input = input[len(\"```json\") :]\n+    if input.endswith(\"```\"):\n+        input = input[: len(input) - len(\"```\")]\n+\n+    try:\n+        result = json.loads(input)\n+    except json.JSONDecodeError:\n+        # Fixup potentially malformed json string using json_repair.\n+        input = str(repair_json(json_str=input, return_objects=False))\n+\n+        # Generate JSON-string output using best-attempt prompting & parsing techniques.\n+        try:\n+            result = json.loads(input)\n+        except json.JSONDecodeError:\n+            if verbose:\n+                logger.exception(\"error loading json, json=%s\", input)\n+            return input, {}\n+        else:\n+            if not isinstance(result, dict):\n+                if verbose:\n+                    logger.exception(\"not expected dict type. type=%s:\", type(result))\n+                return input, {}\n+            return input, result\n+    else:\n+        return input, result\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/question_gen/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/question_gen/__init__.py b/packages/graphrag/graphrag/query/question_gen/__init__.py\nnew file mode 100644\nindex 0000000..d732927\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/question_gen/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Question Generation Module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/question_gen/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/question_gen/base.py b/packages/graphrag/graphrag/query/question_gen/base.py\nnew file mode 100644\nindex 0000000..2195aee\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/question_gen/base.py\n@@ -0,0 +1,65 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base classes for generating questions based on previously asked questions and most recent context data.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.query.context_builder.builders import (\n+    GlobalContextBuilder,\n+    LocalContextBuilder,\n+)\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+@dataclass\n+class QuestionResult:\n+    \"\"\"A Structured Question Result.\"\"\"\n+\n+    response: list[str]\n+    context_data: str | dict[str, Any]\n+    completion_time: float\n+    llm_calls: int\n+    prompt_tokens: int\n+\n+\n+class BaseQuestionGen(ABC):\n+    \"\"\"The Base Question Gen implementation.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: GlobalContextBuilder | LocalContextBuilder,\n+        tokenizer: Tokenizer | None = None,\n+        model_params: dict[str, Any] | None = None,\n+        context_builder_params: dict[str, Any] | None = None,\n+    ):\n+        self.model = model\n+        self.context_builder = context_builder\n+        self.tokenizer = tokenizer or get_tokenizer(model.config)\n+        self.model_params = model_params or {}\n+        self.context_builder_params = context_builder_params or {}\n+\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        question_history: list[str],\n+        context_data: str | None,\n+        question_count: int,\n+        **kwargs,\n+    ) -> QuestionResult:\n+        \"\"\"Generate questions.\"\"\"\n+\n+    @abstractmethod\n+    async def agenerate(\n+        self,\n+        question_history: list[str],\n+        context_data: str | None,\n+        question_count: int,\n+        **kwargs,\n+    ) -> QuestionResult:\n+        \"\"\"Generate questions asynchronously.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/question_gen/local_gen.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/question_gen/local_gen.py b/packages/graphrag/graphrag/query/question_gen/local_gen.py\nnew file mode 100644\nindex 0000000..82795e9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/question_gen/local_gen.py\n@@ -0,0 +1,213 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Local question generation.\"\"\"\n+\n+import logging\n+import time\n+from typing import Any, cast\n+\n+from graphrag.callbacks.llm_callbacks import BaseLLMCallback\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompts.query.question_gen_system_prompt import QUESTION_SYSTEM_PROMPT\n+from graphrag.query.context_builder.builders import (\n+    ContextBuilderResult,\n+    LocalContextBuilder,\n+)\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+from graphrag.query.question_gen.base import BaseQuestionGen, QuestionResult\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LocalQuestionGen(BaseQuestionGen):\n+    \"\"\"Search orchestration for global search mode.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: LocalContextBuilder,\n+        tokenizer: Tokenizer | None = None,\n+        system_prompt: str = QUESTION_SYSTEM_PROMPT,\n+        callbacks: list[BaseLLMCallback] | None = None,\n+        model_params: dict[str, Any] | None = None,\n+        context_builder_params: dict[str, Any] | None = None,\n+    ):\n+        super().__init__(\n+            model=model,\n+            context_builder=context_builder,\n+            tokenizer=tokenizer,\n+            model_params=model_params,\n+            context_builder_params=context_builder_params,\n+        )\n+        self.system_prompt = system_prompt\n+        self.callbacks = callbacks or []\n+\n+    async def agenerate(\n+        self,\n+        question_history: list[str],\n+        context_data: str | None,\n+        question_count: int,\n+        **kwargs,\n+    ) -> QuestionResult:\n+        \"\"\"\n+        Generate a question based on the question history and context data.\n+\n+        If context data is not provided, it will be generated by the local context builder\n+        \"\"\"\n+        start_time = time.time()\n+\n+        if len(question_history) == 0:\n+            question_text = \"\"\n+            conversation_history = None\n+        else:\n+            # construct current query and conversation history\n+            question_text = question_history[-1]\n+            history = [\n+                {\"role\": \"user\", \"content\": query} for query in question_history[:-1]\n+            ]\n+            conversation_history = ConversationHistory.from_list(history)\n+\n+        if context_data is None:\n+            # generate context data based on the question history\n+            result = cast(\n+                \"ContextBuilderResult\",\n+                self.context_builder.build_context(\n+                    query=question_text,\n+                    conversation_history=conversation_history,\n+                    **kwargs,\n+                    **self.context_builder_params,\n+                ),\n+            )\n+            context_data = cast(\"str\", result.context_chunks)\n+            context_records = result.context_records\n+        else:\n+            context_records = {\"context_data\": context_data}\n+        logger.debug(\n+            \"GENERATE QUESTION: %s. LAST QUESTION: %s\", start_time, question_text\n+        )\n+        system_prompt = \"\"\n+        try:\n+            system_prompt = self.system_prompt.format(\n+                context_data=context_data, question_count=question_count\n+            )\n+            question_messages = [\n+                {\"role\": \"system\", \"content\": system_prompt},\n+            ]\n+\n+            response = \"\"\n+            async for chunk in self.model.achat_stream(\n+                prompt=question_text,\n+                history=question_messages,\n+                model_parameters=self.model_params,\n+            ):\n+                response += chunk\n+                for callback in self.callbacks:\n+                    callback.on_llm_new_token(chunk)\n+\n+            return QuestionResult(\n+                response=response.split(\"\\n\"),\n+                context_data={\n+                    \"question_context\": question_text,\n+                    **context_records,\n+                },\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n+            )\n+\n+        except Exception:\n+            logger.exception(\"Exception in generating question\")\n+            return QuestionResult(\n+                response=[],\n+                context_data=context_records,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n+            )\n+\n+    async def generate(\n+        self,\n+        question_history: list[str],\n+        context_data: str | None,\n+        question_count: int,\n+        **kwargs,\n+    ) -> QuestionResult:\n+        \"\"\"\n+        Generate a question based on the question history and context data.\n+\n+        If context data is not provided, it will be generated by the local context builder\n+        \"\"\"\n+        start_time = time.time()\n+        if len(question_history) == 0:\n+            question_text = \"\"\n+            conversation_history = None\n+        else:\n+            # construct current query and conversation history\n+            question_text = question_history[-1]\n+            history = [\n+                {\"role\": \"user\", \"content\": query} for query in question_history[:-1]\n+            ]\n+            conversation_history = ConversationHistory.from_list(history)\n+\n+        if context_data is None:\n+            # generate context data based on the question history\n+            result = cast(\n+                \"ContextBuilderResult\",\n+                self.context_builder.build_context(\n+                    query=question_text,\n+                    conversation_history=conversation_history,\n+                    **kwargs,\n+                    **self.context_builder_params,\n+                ),\n+            )\n+            context_data = cast(\"str\", result.context_chunks)\n+            context_records = result.context_records\n+        else:\n+            context_records = {\"context_data\": context_data}\n+        logger.debug(\n+            \"GENERATE QUESTION: %s. QUESTION HISTORY: %s\", start_time, question_text\n+        )\n+        system_prompt = \"\"\n+        try:\n+            system_prompt = self.system_prompt.format(\n+                context_data=context_data, question_count=question_count\n+            )\n+            question_messages = [\n+                {\"role\": \"system\", \"content\": system_prompt},\n+                {\"role\": \"user\", \"content\": question_text},\n+            ]\n+\n+            response = \"\"\n+            async for chunk in self.model.achat_stream(\n+                prompt=question_text,\n+                history=question_messages,\n+                model_parameters=self.model_params,\n+            ):\n+                response += chunk\n+                for callback in self.callbacks:\n+                    callback.on_llm_new_token(chunk)\n+\n+            return QuestionResult(\n+                response=response.split(\"\\n\"),\n+                context_data={\n+                    \"question_context\": question_text,\n+                    **context_records,\n+                },\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n+            )\n+\n+        except Exception:\n+            logger.exception(\"Exception in generating questions\")\n+            return QuestionResult(\n+                response=[],\n+                context_data=context_records,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n+            )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/__init__.py b/packages/graphrag/graphrag/query/structured_search/__init__.py\nnew file mode 100644\nindex 0000000..b41baaf\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Structured Search package.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/base.py b/packages/graphrag/graphrag/query/structured_search/base.py\nnew file mode 100644\nindex 0000000..753b419\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/base.py\n@@ -0,0 +1,92 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base classes for search algos.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from collections.abc import AsyncGenerator\n+from dataclasses import dataclass\n+from typing import Any, Generic, TypeVar\n+\n+import pandas as pd\n+\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.query.context_builder.builders import (\n+    BasicContextBuilder,\n+    DRIFTContextBuilder,\n+    GlobalContextBuilder,\n+    LocalContextBuilder,\n+)\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+@dataclass\n+class SearchResult:\n+    \"\"\"A Structured Search Result.\"\"\"\n+\n+    response: str | dict[str, Any] | list[dict[str, Any]]\n+    context_data: str | list[pd.DataFrame] | dict[str, pd.DataFrame]\n+    # actual text strings that are in the context window, built from context_data\n+    context_text: str | list[str] | dict[str, str]\n+    completion_time: float\n+    # total LLM calls and token usage\n+    llm_calls: int\n+    prompt_tokens: int\n+    output_tokens: int\n+    # breakdown of LLM calls and token usage\n+    llm_calls_categories: dict[str, int] | None = None\n+    prompt_tokens_categories: dict[str, int] | None = None\n+    output_tokens_categories: dict[str, int] | None = None\n+\n+\n+T = TypeVar(\n+    \"T\",\n+    GlobalContextBuilder,\n+    LocalContextBuilder,\n+    DRIFTContextBuilder,\n+    BasicContextBuilder,\n+)\n+\n+\n+class BaseSearch(ABC, Generic[T]):\n+    \"\"\"The Base Search implementation.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: T,\n+        tokenizer: Tokenizer | None = None,\n+        model_params: dict[str, Any] | None = None,\n+        context_builder_params: dict[str, Any] | None = None,\n+    ):\n+        self.model = model\n+        self.context_builder = context_builder\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.model_params = model_params or {}\n+        self.context_builder_params = context_builder_params or {}\n+\n+    @abstractmethod\n+    async def search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs,\n+    ) -> SearchResult:\n+        \"\"\"Search for the given query asynchronously.\"\"\"\n+        msg = \"Subclasses must implement this method\"\n+        raise NotImplementedError(msg)\n+\n+    @abstractmethod\n+    async def stream_search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"Stream search for the given query.\"\"\"\n+        yield \"\"  # This makes it an async generator.\n+        msg = \"Subclasses must implement this method\"\n+        raise NotImplementedError(msg)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/basic_search/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/basic_search/__init__.py b/packages/graphrag/graphrag/query/structured_search/basic_search/__init__.py\nnew file mode 100644\nindex 0000000..804a5d2\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/basic_search/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The BasicSearch package.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/basic_search/basic_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/basic_search/basic_context.py b/packages/graphrag/graphrag/query/structured_search/basic_search/basic_context.py\nnew file mode 100644\nindex 0000000..b739001\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/basic_search/basic_context.py\n@@ -0,0 +1,105 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Basic Context Builder implementation.\"\"\"\n+\n+import logging\n+from typing import cast\n+\n+import pandas as pd\n+\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.query.context_builder.builders import (\n+    BasicContextBuilder,\n+    ContextBuilderResult,\n+)\n+from graphrag.query.context_builder.conversation_history import ConversationHistory\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.vector_stores.base import BaseVectorStore\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BasicSearchContext(BasicContextBuilder):\n+    \"\"\"Class representing the Basic Search Context Builder.\"\"\"\n+\n+    def __init__(\n+        self,\n+        text_embedder: EmbeddingModel,\n+        text_unit_embeddings: BaseVectorStore,\n+        text_units: list[TextUnit] | None = None,\n+        tokenizer: Tokenizer | None = None,\n+        embedding_vectorstore_key: str = \"id\",\n+    ):\n+        self.text_embedder = text_embedder\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.text_units = text_units\n+        self.text_unit_embeddings = text_unit_embeddings\n+        self.embedding_vectorstore_key = embedding_vectorstore_key\n+\n+    def build_context(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        k: int = 10,\n+        max_context_tokens: int = 12_000,\n+        context_name: str = \"Sources\",\n+        column_delimiter: str = \"|\",\n+        text_id_col: str = \"id\",\n+        text_col: str = \"text\",\n+        **kwargs,\n+    ) -> ContextBuilderResult:\n+        \"\"\"Build the context for the basic search mode.\"\"\"\n+        if query != \"\":\n+            related_texts = self.text_unit_embeddings.similarity_search_by_text(\n+                text=query,\n+                text_embedder=lambda t: self.text_embedder.embed(t),\n+                k=k,\n+            )\n+\n+            text_unit_ids = {t.document.id for t in related_texts}\n+            text_units_filtered = []\n+            text_units_filtered = [\n+                {text_id_col: t.short_id, text_col: t.text}\n+                for t in self.text_units or []\n+                if t.id in text_unit_ids\n+            ]\n+            related_text_df = pd.DataFrame(text_units_filtered)\n+        else:\n+            related_text_df = pd.DataFrame({\n+                text_id_col: [],\n+                text_col: [],\n+            })\n+\n+        # add these related text chunks into context until we fill up the context window\n+        current_tokens = 0\n+        text_ids = []\n+        current_tokens = len(\n+            self.tokenizer.encode(text_id_col + column_delimiter + text_col + \"\\n\")\n+        )\n+        for i, row in related_text_df.iterrows():\n+            text = row[text_id_col] + column_delimiter + row[text_col] + \"\\n\"\n+            tokens = len(self.tokenizer.encode(text))\n+            if current_tokens + tokens > max_context_tokens:\n+                msg = f\"Reached token limit: {current_tokens + tokens}. Reverting to previous context state\"\n+                logger.warning(msg)\n+                break\n+\n+            current_tokens += tokens\n+            text_ids.append(i)\n+        final_text_df = cast(\n+            \"pd.DataFrame\",\n+            related_text_df[related_text_df.index.isin(text_ids)].reset_index(\n+                drop=True\n+            ),\n+        )\n+        final_text = final_text_df.to_csv(\n+            index=False, escapechar=\"\\\\\", sep=column_delimiter\n+        )\n+\n+        return ContextBuilderResult(\n+            context_chunks=final_text,\n+            context_records={context_name.lower(): final_text_df},\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/basic_search/search.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/basic_search/search.py b/packages/graphrag/graphrag/query/structured_search/basic_search/search.py\nnew file mode 100644\nindex 0000000..ce5f656\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/basic_search/search.py\n@@ -0,0 +1,160 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"BasicSearch implementation.\"\"\"\n+\n+import logging\n+import time\n+from collections.abc import AsyncGenerator\n+from typing import Any\n+\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompts.query.basic_search_system_prompt import (\n+    BASIC_SEARCH_SYSTEM_PROMPT,\n+)\n+from graphrag.query.context_builder.builders import BasicContextBuilder\n+from graphrag.query.context_builder.conversation_history import ConversationHistory\n+from graphrag.query.structured_search.base import BaseSearch, SearchResult\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\"\"\"\n+Implementation of a generic RAG algorithm (vector search on raw text chunks)\n+\"\"\"\n+\n+\n+class BasicSearch(BaseSearch[BasicContextBuilder]):\n+    \"\"\"Search orchestration for basic search mode.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: BasicContextBuilder,\n+        tokenizer: Tokenizer | None = None,\n+        system_prompt: str | None = None,\n+        response_type: str = \"multiple paragraphs\",\n+        callbacks: list[QueryCallbacks] | None = None,\n+        model_params: dict[str, Any] | None = None,\n+        context_builder_params: dict | None = None,\n+    ):\n+        super().__init__(\n+            model=model,\n+            context_builder=context_builder,\n+            tokenizer=tokenizer,\n+            model_params=model_params,\n+            context_builder_params=context_builder_params or {},\n+        )\n+        self.system_prompt = system_prompt or BASIC_SEARCH_SYSTEM_PROMPT\n+        self.callbacks = callbacks or []\n+        self.response_type = response_type\n+\n+    async def search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs,\n+    ) -> SearchResult:\n+        \"\"\"Build rag search context that fits a single context window and generate answer for the user query.\"\"\"\n+        start_time = time.time()\n+        search_prompt = \"\"\n+        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n+\n+        context_result = self.context_builder.build_context(\n+            query=query,\n+            conversation_history=conversation_history,\n+            **kwargs,\n+            **self.context_builder_params,\n+        )\n+\n+        llm_calls[\"build_context\"] = context_result.llm_calls\n+        prompt_tokens[\"build_context\"] = context_result.prompt_tokens\n+        output_tokens[\"build_context\"] = context_result.output_tokens\n+\n+        logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n+        try:\n+            search_prompt = self.system_prompt.format(\n+                context_data=context_result.context_chunks,\n+                response_type=self.response_type,\n+            )\n+            search_messages = [\n+                {\"role\": \"system\", \"content\": search_prompt},\n+            ]\n+\n+            response = \"\"\n+            async for chunk in self.model.achat_stream(\n+                prompt=query,\n+                history=search_messages,\n+                model_parameters=self.model_params,\n+            ):\n+                for callback in self.callbacks:\n+                    callback.on_llm_new_token(chunk)\n+                response += chunk\n+\n+            llm_calls[\"response\"] = 1\n+            prompt_tokens[\"response\"] = len(self.tokenizer.encode(search_prompt))\n+            output_tokens[\"response\"] = len(self.tokenizer.encode(response))\n+\n+            for callback in self.callbacks:\n+                callback.on_context(context_result.context_records)\n+\n+            return SearchResult(\n+                response=response,\n+                context_data=context_result.context_records,\n+                context_text=context_result.context_chunks,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=sum(output_tokens.values()),\n+                llm_calls_categories=llm_calls,\n+                prompt_tokens_categories=prompt_tokens,\n+                output_tokens_categories=output_tokens,\n+            )\n+\n+        except Exception:\n+            logger.exception(\"Exception in _asearch\")\n+            return SearchResult(\n+                response=\"\",\n+                context_data=context_result.context_records,\n+                context_text=context_result.context_chunks,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=0,\n+                llm_calls_categories=llm_calls,\n+                prompt_tokens_categories=prompt_tokens,\n+                output_tokens_categories=output_tokens,\n+            )\n+\n+    async def stream_search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"Build basic search context that fits a single context window and generate answer for the user query.\"\"\"\n+        start_time = time.time()\n+\n+        context_result = self.context_builder.build_context(\n+            query=query,\n+            conversation_history=conversation_history,\n+            **self.context_builder_params,\n+        )\n+        logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n+        search_prompt = self.system_prompt.format(\n+            context_data=context_result.context_chunks, response_type=self.response_type\n+        )\n+        search_messages = [\n+            {\"role\": \"system\", \"content\": search_prompt},\n+        ]\n+\n+        for callback in self.callbacks:\n+            callback.on_context(context_result.context_records)\n+\n+        async for chunk_response in self.model.achat_stream(\n+            prompt=query,\n+            history=search_messages,\n+            model_parameters=self.model_params,\n+        ):\n+            for callback in self.callbacks:\n+                callback.on_llm_new_token(chunk_response)\n+            yield chunk_response\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/drift_search/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/drift_search/__init__.py b/packages/graphrag/graphrag/query/structured_search/drift_search/__init__.py\nnew file mode 100644\nindex 0000000..fe58251\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/drift_search/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"DriftSearch module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/drift_search/action.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/drift_search/action.py b/packages/graphrag/graphrag/query/structured_search/drift_search/action.py\nnew file mode 100644\nindex 0000000..8f1da3d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/drift_search/action.py\n@@ -0,0 +1,237 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"DRIFT Search Query State.\"\"\"\n+\n+import json\n+import logging\n+from typing import Any\n+\n+from graphrag.query.llm.text_utils import try_parse_json_object\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class DriftAction:\n+    \"\"\"\n+    Represent an action containing a query, answer, score, and follow-up actions.\n+\n+    This class encapsulates action strings produced by the LLM in a structured way.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        query: str,\n+        answer: str | None = None,\n+        follow_ups: list[\"DriftAction\"] | None = None,\n+    ):\n+        \"\"\"\n+        Initialize the DriftAction with a query, optional answer, and follow-up actions.\n+\n+        Args:\n+            query (str): The query for the action.\n+            answer (Optional[str]): The answer to the query, if available.\n+            follow_ups (Optional[list[DriftAction]]): A list of follow-up actions.\n+        \"\"\"\n+        self.query = query\n+        self.answer: str | None = answer  # Corresponds to an 'intermediate_answer'\n+        self.score: float | None = None\n+        self.follow_ups: list[DriftAction] = (\n+            follow_ups if follow_ups is not None else []\n+        )\n+        self.metadata: dict[str, Any] = {\n+            \"llm_calls\": 0,\n+            \"prompt_tokens\": 0,\n+            \"output_tokens\": 0,\n+        }\n+\n+    @property\n+    def is_complete(self) -> bool:\n+        \"\"\"Check if the action is complete (i.e., an answer is available).\"\"\"\n+        return self.answer is not None\n+\n+    async def search(self, search_engine: Any, global_query: str, scorer: Any = None):\n+        \"\"\"\n+        Execute an asynchronous search using the search engine, and update the action with the results.\n+\n+        If a scorer is provided, compute the score for the action.\n+\n+        Args:\n+            search_engine (Any): The search engine to execute the query.\n+            global_query (str): The global query string.\n+            scorer (Any, optional): Scorer to compute scores for the action.\n+\n+        Returns\n+        -------\n+        self : DriftAction\n+            Updated action with search results.\n+        \"\"\"\n+        if self.is_complete:\n+            logger.warning(\"Action already complete. Skipping search.\")\n+            return self\n+\n+        search_result = await search_engine.search(\n+            drift_query=global_query, query=self.query\n+        )\n+\n+        # Do not launch exception as it will roll up with other steps\n+        # Instead return an empty response and let score -inf handle it\n+        _, response = try_parse_json_object(search_result.response, verbose=False)\n+\n+        self.answer = response.pop(\"response\", None)\n+        self.score = float(response.pop(\"score\", \"-inf\"))\n+        self.metadata.update({\"context_data\": search_result.context_data})\n+\n+        if self.answer is None:\n+            logger.warning(\"No answer found for query: %s\", self.query)\n+\n+        self.metadata[\"llm_calls\"] += 1\n+        self.metadata[\"prompt_tokens\"] += search_result.prompt_tokens\n+        self.metadata[\"output_tokens\"] += search_result.output_tokens\n+\n+        self.follow_ups = response.pop(\"follow_up_queries\", [])\n+        if not self.follow_ups:\n+            logger.warning(\"No follow-up actions found for response: %s\", response)\n+\n+        if scorer:\n+            self.compute_score(scorer)\n+\n+        return self\n+\n+    def compute_score(self, scorer: Any):\n+        \"\"\"\n+        Compute the score for the action using the provided scorer.\n+\n+        Args:\n+            scorer (Any): The scorer to compute the score.\n+        \"\"\"\n+        score = scorer.compute_score(self.query, self.answer)\n+        self.score = (\n+            score if score is not None else float(\"-inf\")\n+        )  # Default to -inf for sorting\n+\n+    def serialize(self, include_follow_ups: bool = True) -> dict[str, Any]:\n+        \"\"\"\n+        Serialize the action to a dictionary.\n+\n+        Args:\n+            include_follow_ups (bool): Whether to include follow-up actions in the serialization.\n+\n+        Returns\n+        -------\n+        dict[str, Any]\n+            Serialized action as a dictionary.\n+        \"\"\"\n+        data = {\n+            \"query\": self.query,\n+            \"answer\": self.answer,\n+            \"score\": self.score,\n+            \"metadata\": self.metadata,\n+        }\n+        if include_follow_ups:\n+            data[\"follow_ups\"] = [action.serialize() for action in self.follow_ups]\n+        return data\n+\n+    @classmethod\n+    def deserialize(cls, data: dict[str, Any]) -> \"DriftAction\":\n+        \"\"\"\n+        Deserialize the action from a dictionary.\n+\n+        Args:\n+            data (dict[str, Any]): Serialized action data.\n+\n+        Returns\n+        -------\n+        DriftAction\n+            A deserialized instance of DriftAction.\n+        \"\"\"\n+        # Ensure 'query' exists in the data, raise a ValueError if missing\n+        query = data.get(\"query\")\n+        if query is None:\n+            error_message = \"Missing 'query' key in serialized data\"\n+            raise ValueError(error_message)\n+\n+        # Initialize the DriftAction\n+        action = cls(query)\n+        action.answer = data.get(\"answer\")\n+        action.score = data.get(\"score\")\n+        action.metadata = data.get(\"metadata\", {})\n+\n+        action.follow_ups = [\n+            cls.deserialize(fu_data) for fu_data in data.get(\"follow_up_queries\", [])\n+        ]\n+        return action\n+\n+    @classmethod\n+    def from_primer_response(\n+        cls, query: str, response: str | dict[str, Any] | list[dict[str, Any]]\n+    ) -> \"DriftAction\":\n+        \"\"\"\n+        Create a DriftAction from a DRIFTPrimer response.\n+\n+        Args:\n+            query (str): The query string.\n+            response (str | dict[str, Any] | list[dict[str, Any]]): Primer response data.\n+\n+        Returns\n+        -------\n+        DriftAction\n+            A new instance of DriftAction based on the response.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the response is not a dictionary or expected format.\n+        \"\"\"\n+        if isinstance(response, dict):\n+            action = cls(\n+                query,\n+                follow_ups=response.get(\"follow_up_queries\", []),\n+                answer=response.get(\"intermediate_answer\"),\n+            )\n+            action.score = response.get(\"score\")\n+            return action\n+\n+        # If response is a string, attempt to parse as JSON\n+        if isinstance(response, str):\n+            try:\n+                parsed_response = json.loads(response)\n+                if isinstance(parsed_response, dict):\n+                    return cls.from_primer_response(query, parsed_response)\n+                error_message = \"Parsed response must be a dictionary.\"\n+                raise ValueError(error_message)\n+            except json.JSONDecodeError as e:\n+                error_message = f\"Failed to parse response string: {e}. Parsed response must be a dictionary.\"\n+                raise ValueError(error_message) from e\n+\n+        error_message = f\"Unsupported response type: {type(response).__name__}. Expected a dictionary or JSON string.\"\n+        raise ValueError(error_message)\n+\n+    def __hash__(self) -> int:\n+        \"\"\"\n+        Allow DriftAction objects to be hashable for use in networkx.MultiDiGraph.\n+\n+        Assumes queries are unique.\n+\n+        Returns\n+        -------\n+        int\n+            Hash based on the query.\n+        \"\"\"\n+        return hash(self.query)\n+\n+    def __eq__(self, other: object) -> bool:\n+        \"\"\"\n+        Check equality based on the query string.\n+\n+        Args:\n+            other (Any): Another object to compare with.\n+\n+        Returns\n+        -------\n+        bool\n+            True if the other object is a DriftAction with the same query, False otherwise.\n+        \"\"\"\n+        if not isinstance(other, DriftAction):\n+            return False\n+        return self.query == other.query\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/drift_search/drift_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/drift_search/drift_context.py b/packages/graphrag/graphrag/query/structured_search/drift_search/drift_context.py\nnew file mode 100644\nindex 0000000..4b4325a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/drift_search/drift_context.py\n@@ -0,0 +1,227 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"DRIFT Context Builder implementation.\"\"\"\n+\n+import logging\n+from dataclasses import asdict\n+from typing import Any\n+\n+import numpy as np\n+import pandas as pd\n+\n+from graphrag.config.models.drift_search_config import DRIFTSearchConfig\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.language_model.protocol.base import ChatModel, EmbeddingModel\n+from graphrag.prompts.query.drift_search_system_prompt import (\n+    DRIFT_LOCAL_SYSTEM_PROMPT,\n+    DRIFT_REDUCE_PROMPT,\n+)\n+from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n+from graphrag.query.structured_search.base import DRIFTContextBuilder\n+from graphrag.query.structured_search.drift_search.primer import PrimerQueryProcessor\n+from graphrag.query.structured_search.local_search.mixed_context import (\n+    LocalSearchMixedContext,\n+)\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.vector_stores.base import BaseVectorStore\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class DRIFTSearchContextBuilder(DRIFTContextBuilder):\n+    \"\"\"Class representing the DRIFT Search Context Builder.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        text_embedder: EmbeddingModel,\n+        entities: list[Entity],\n+        entity_text_embeddings: BaseVectorStore,\n+        text_units: list[TextUnit] | None = None,\n+        reports: list[CommunityReport] | None = None,\n+        relationships: list[Relationship] | None = None,\n+        covariates: dict[str, list[Covariate]] | None = None,\n+        tokenizer: Tokenizer | None = None,\n+        embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n+        config: DRIFTSearchConfig | None = None,\n+        local_system_prompt: str | None = None,\n+        local_mixed_context: LocalSearchMixedContext | None = None,\n+        reduce_system_prompt: str | None = None,\n+        response_type: str | None = None,\n+    ):\n+        \"\"\"Initialize the DRIFT search context builder with necessary components.\"\"\"\n+        self.config = config or DRIFTSearchConfig()\n+        self.model = model\n+        self.text_embedder = text_embedder\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.local_system_prompt = local_system_prompt or DRIFT_LOCAL_SYSTEM_PROMPT\n+        self.reduce_system_prompt = reduce_system_prompt or DRIFT_REDUCE_PROMPT\n+\n+        self.entities = entities\n+        self.entity_text_embeddings = entity_text_embeddings\n+        self.reports = reports\n+        self.text_units = text_units\n+        self.relationships = relationships\n+        self.covariates = covariates\n+        self.embedding_vectorstore_key = embedding_vectorstore_key\n+\n+        self.response_type = response_type\n+\n+        self.local_mixed_context = (\n+            local_mixed_context or self.init_local_context_builder()\n+        )\n+\n+    def init_local_context_builder(self) -> LocalSearchMixedContext:\n+        \"\"\"\n+        Initialize the local search mixed context builder.\n+\n+        Returns\n+        -------\n+        LocalSearchMixedContext: Initialized local context.\n+        \"\"\"\n+        return LocalSearchMixedContext(\n+            community_reports=self.reports,\n+            text_units=self.text_units,\n+            entities=self.entities,\n+            relationships=self.relationships,\n+            covariates=self.covariates,\n+            entity_text_embeddings=self.entity_text_embeddings,\n+            embedding_vectorstore_key=self.embedding_vectorstore_key,\n+            text_embedder=self.text_embedder,\n+            tokenizer=self.tokenizer,\n+        )\n+\n+    @staticmethod\n+    def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame:\n+        \"\"\"\n+        Convert a list of CommunityReport objects to a pandas DataFrame.\n+\n+        Args\n+        ----\n+        reports : list[CommunityReport]\n+            List of CommunityReport objects.\n+\n+        Returns\n+        -------\n+        pd.DataFrame: DataFrame with report data.\n+\n+        Raises\n+        ------\n+        ValueError: If some reports are missing full content or full content embeddings.\n+        \"\"\"\n+        report_df = pd.DataFrame([asdict(report) for report in reports])\n+        missing_content_error = \"Some reports are missing full content.\"\n+        missing_embedding_error = (\n+            \"Some reports are missing full content embeddings. {missing} out of {total}\"\n+        )\n+\n+        if (\n+            \"full_content\" not in report_df.columns\n+            or report_df[\"full_content\"].isna().sum() > 0\n+        ):\n+            raise ValueError(missing_content_error)\n+\n+        if (\n+            \"full_content_embedding\" not in report_df.columns\n+            or report_df[\"full_content_embedding\"].isna().sum() > 0\n+        ):\n+            raise ValueError(\n+                missing_embedding_error.format(\n+                    missing=report_df[\"full_content_embedding\"].isna().sum(),\n+                    total=len(report_df),\n+                )\n+            )\n+        return report_df\n+\n+    @staticmethod\n+    def check_query_doc_encodings(query_embedding: Any, embedding: Any) -> bool:\n+        \"\"\"\n+        Check if the embeddings are compatible.\n+\n+        Args\n+        ----\n+        query_embedding : Any\n+            Embedding of the query.\n+        embedding : Any\n+            Embedding to compare against.\n+\n+        Returns\n+        -------\n+        bool: True if embeddings match, otherwise False.\n+        \"\"\"\n+        return (\n+            query_embedding is not None\n+            and embedding is not None\n+            and isinstance(query_embedding, type(embedding))\n+            and len(query_embedding) == len(embedding)\n+            and isinstance(query_embedding[0], type(embedding[0]))\n+        )\n+\n+    async def build_context(\n+        self, query: str, **kwargs\n+    ) -> tuple[pd.DataFrame, dict[str, int]]:\n+        \"\"\"\n+        Build DRIFT search context.\n+\n+        Args\n+        ----\n+        query : str\n+            Search query string.\n+\n+        Returns\n+        -------\n+        pd.DataFrame: Top-k most similar documents.\n+        dict[str, int]: Number of LLM calls, and prompts and output tokens.\n+\n+        Raises\n+        ------\n+        ValueError: If no community reports are available, or embeddings\n+        are incompatible.\n+        \"\"\"\n+        if self.reports is None:\n+            missing_reports_error = (\n+                \"No community reports available. Please provide a list of reports.\"\n+            )\n+            raise ValueError(missing_reports_error)\n+\n+        query_processor = PrimerQueryProcessor(\n+            chat_model=self.model,\n+            text_embedder=self.text_embedder,\n+            tokenizer=self.tokenizer,\n+            reports=self.reports,\n+        )\n+\n+        query_embedding, token_ct = await query_processor(query)\n+\n+        report_df = self.convert_reports_to_df(self.reports)\n+\n+        # Check compatibility between query embedding and document embeddings\n+        if not self.check_query_doc_encodings(\n+            query_embedding, report_df[\"full_content_embedding\"].iloc[0]\n+        ):\n+            error_message = (\n+                \"Query and document embeddings are not compatible. \"\n+                \"Please ensure that the embeddings are of the same type and length.\"\n+            )\n+            raise ValueError(error_message)\n+\n+        # Vectorized cosine similarity computation\n+        query_norm = np.linalg.norm(query_embedding)\n+        document_norms = np.linalg.norm(\n+            report_df[\"full_content_embedding\"].to_list(), axis=1\n+        )\n+        dot_products = np.dot(\n+            np.vstack(report_df[\"full_content_embedding\"].to_list()), query_embedding\n+        )\n+        report_df[\"similarity\"] = dot_products / (document_norms * query_norm)\n+\n+        # Sort by similarity and select top-k\n+        top_k = report_df.nlargest(self.config.drift_k_followups, \"similarity\")\n+\n+        return top_k.loc[:, [\"short_id\", \"community_id\", \"full_content\"]], token_ct\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/drift_search/primer.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/drift_search/primer.py b/packages/graphrag/graphrag/query/structured_search/drift_search/primer.py\nnew file mode 100644\nindex 0000000..2a7f145\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/drift_search/primer.py\n@@ -0,0 +1,201 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Primer for DRIFT search.\"\"\"\n+\n+import json\n+import logging\n+import secrets\n+import time\n+\n+import numpy as np\n+import pandas as pd\n+from tqdm.asyncio import tqdm_asyncio\n+\n+from graphrag.config.models.drift_search_config import DRIFTSearchConfig\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.language_model.protocol.base import ChatModel, EmbeddingModel\n+from graphrag.prompts.query.drift_search_system_prompt import (\n+    DRIFT_PRIMER_PROMPT,\n+)\n+from graphrag.query.structured_search.base import SearchResult\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class PrimerQueryProcessor:\n+    \"\"\"Process the query by expanding it using community reports and generate follow-up actions.\"\"\"\n+\n+    def __init__(\n+        self,\n+        chat_model: ChatModel,\n+        text_embedder: EmbeddingModel,\n+        reports: list[CommunityReport],\n+        tokenizer: Tokenizer | None = None,\n+    ):\n+        \"\"\"\n+        Initialize the PrimerQueryProcessor.\n+\n+        Args:\n+            chat_llm (ChatOpenAI): The language model used to process the query.\n+            text_embedder (BaseTextEmbedding): The text embedding model.\n+            reports (list[CommunityReport]): List of community reports.\n+            tokenizer (Tokenizer, optional): Token encoder for token counting.\n+        \"\"\"\n+        self.chat_model = chat_model\n+        self.text_embedder = text_embedder\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.reports = reports\n+\n+    async def expand_query(self, query: str) -> tuple[str, dict[str, int]]:\n+        \"\"\"\n+        Expand the query using a random community report template.\n+\n+        Args:\n+            query (str): The original search query.\n+\n+        Returns\n+        -------\n+        tuple[str, dict[str, int]]: Expanded query text and the number of tokens used.\n+        \"\"\"\n+        template = secrets.choice(self.reports).full_content  # nosec S311\n+\n+        prompt = f\"\"\"Create a hypothetical answer to the following query: {query}\\n\\n\n+                  Format it to follow the structure of the template below:\\n\\n\n+                  {template}\\n\"\n+                  Ensure that the hypothetical answer does not reference new named entities that are not present in the original query.\"\"\"\n+\n+        model_response = await self.chat_model.achat(prompt)\n+        text = model_response.output.content\n+\n+        prompt_tokens = len(self.tokenizer.encode(prompt))\n+        output_tokens = len(self.tokenizer.encode(text))\n+        token_ct = {\n+            \"llm_calls\": 1,\n+            \"prompt_tokens\": prompt_tokens,\n+            \"output_tokens\": output_tokens,\n+        }\n+        if text == \"\":\n+            logger.warning(\"Failed to generate expansion for query: %s\", query)\n+            return query, token_ct\n+        return text, token_ct\n+\n+    async def __call__(self, query: str) -> tuple[list[float], dict[str, int]]:\n+        \"\"\"\n+        Call method to process the query, expand it, and embed the result.\n+\n+        Args:\n+            query (str): The search query.\n+\n+        Returns\n+        -------\n+        tuple[list[float], int]: List of embeddings for the expanded query and the token count.\n+        \"\"\"\n+        hyde_query, token_ct = await self.expand_query(query)\n+        logger.debug(\"Expanded query: %s\", hyde_query)\n+        return self.text_embedder.embed(hyde_query), token_ct\n+\n+\n+class DRIFTPrimer:\n+    \"\"\"Perform initial query decomposition using global guidance from information in community reports.\"\"\"\n+\n+    def __init__(\n+        self,\n+        config: DRIFTSearchConfig,\n+        chat_model: ChatModel,\n+        tokenizer: Tokenizer | None = None,\n+    ):\n+        \"\"\"\n+        Initialize the DRIFTPrimer.\n+\n+        Args:\n+            config (DRIFTSearchConfig): Configuration settings for DRIFT search.\n+            chat_llm (ChatOpenAI): The language model used for searching.\n+            tokenizer (Tokenizer, optional): Tokenizer for managing tokens.\n+        \"\"\"\n+        self.chat_model = chat_model\n+        self.config = config\n+        self.tokenizer = tokenizer or get_tokenizer()\n+\n+    async def decompose_query(\n+        self, query: str, reports: pd.DataFrame\n+    ) -> tuple[dict, dict[str, int]]:\n+        \"\"\"\n+        Decompose the query into subqueries based on the fetched global structures.\n+\n+        Args:\n+            query (str): The original search query.\n+            reports (pd.DataFrame): DataFrame containing community reports.\n+\n+        Returns\n+        -------\n+        tuple[dict, int, int]: Parsed response and the number of prompt and output tokens used.\n+        \"\"\"\n+        community_reports = \"\\n\\n\".join(reports[\"full_content\"].tolist())\n+        prompt = DRIFT_PRIMER_PROMPT.format(\n+            query=query, community_reports=community_reports\n+        )\n+        model_response = await self.chat_model.achat(prompt, json=True)\n+        response = model_response.output.content\n+\n+        parsed_response = json.loads(response)\n+\n+        token_ct = {\n+            \"llm_calls\": 1,\n+            \"prompt_tokens\": len(self.tokenizer.encode(prompt)),\n+            \"output_tokens\": len(self.tokenizer.encode(response)),\n+        }\n+\n+        return parsed_response, token_ct\n+\n+    async def search(\n+        self,\n+        query: str,\n+        top_k_reports: pd.DataFrame,\n+    ) -> SearchResult:\n+        \"\"\"\n+        Asynchronous search method that processes the query and returns a SearchResult.\n+\n+        Args:\n+            query (str): The search query.\n+            top_k_reports (pd.DataFrame): DataFrame containing the top-k reports.\n+\n+        Returns\n+        -------\n+        SearchResult: The search result containing the response and context data.\n+        \"\"\"\n+        start_time = time.perf_counter()\n+        report_folds = self.split_reports(top_k_reports)\n+        tasks = [self.decompose_query(query, fold) for fold in report_folds]\n+\n+        results_with_tokens = await tqdm_asyncio.gather(*tasks, leave=False)\n+\n+        completion_time = time.perf_counter() - start_time\n+\n+        return SearchResult(\n+            response=[response for response, _ in results_with_tokens],\n+            context_data={\"top_k_reports\": top_k_reports},\n+            context_text=top_k_reports.to_json() or \"\",\n+            completion_time=completion_time,\n+            llm_calls=len(results_with_tokens),\n+            prompt_tokens=sum(ct[\"prompt_tokens\"] for _, ct in results_with_tokens),\n+            output_tokens=sum(ct[\"output_tokens\"] for _, ct in results_with_tokens),\n+        )\n+\n+    def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]:\n+        \"\"\"\n+        Split the reports into folds, allowing for parallel processing.\n+\n+        Args:\n+            reports (pd.DataFrame): DataFrame of community reports.\n+\n+        Returns\n+        -------\n+        list[pd.DataFrame]: List of report folds.\n+        \"\"\"\n+        primer_folds = self.config.primer_folds or 1  # Ensure at least one fold\n+        if primer_folds == 1:\n+            return [reports]\n+        return [pd.DataFrame(fold) for fold in np.array_split(reports, primer_folds)]\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/drift_search/search.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/drift_search/search.py b/packages/graphrag/graphrag/query/structured_search/drift_search/search.py\nnew file mode 100644\nindex 0000000..64a8e52\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/drift_search/search.py\n@@ -0,0 +1,448 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"DRIFT Search implementation.\"\"\"\n+\n+import logging\n+import time\n+from collections.abc import AsyncGenerator\n+from typing import Any\n+\n+from tqdm.asyncio import tqdm_asyncio\n+\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.language_model.util import (\n+    get_openai_model_parameters_from_dict,\n+)\n+from graphrag.query.context_builder.conversation_history import ConversationHistory\n+from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n+from graphrag.query.structured_search.base import BaseSearch, SearchResult\n+from graphrag.query.structured_search.drift_search.action import DriftAction\n+from graphrag.query.structured_search.drift_search.drift_context import (\n+    DRIFTSearchContextBuilder,\n+)\n+from graphrag.query.structured_search.drift_search.primer import DRIFTPrimer\n+from graphrag.query.structured_search.drift_search.state import QueryState\n+from graphrag.query.structured_search.local_search.search import LocalSearch\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class DRIFTSearch(BaseSearch[DRIFTSearchContextBuilder]):\n+    \"\"\"Class representing a DRIFT Search.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: DRIFTSearchContextBuilder,\n+        tokenizer: Tokenizer | None = None,\n+        query_state: QueryState | None = None,\n+        callbacks: list[QueryCallbacks] | None = None,\n+    ):\n+        \"\"\"\n+        Initialize the DRIFTSearch class.\n+\n+        Args:\n+            llm (ChatOpenAI): The language model used for searching.\n+            context_builder (DRIFTSearchContextBuilder): Builder for search context.\n+            config (DRIFTSearchConfig, optional): Configuration settings for DRIFTSearch.\n+            tokenizer (Tokenizer, optional): Token encoder for managing tokens.\n+            query_state (QueryState, optional): State of the current search query.\n+        \"\"\"\n+        super().__init__(model, context_builder, tokenizer)\n+\n+        self.context_builder = context_builder\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.query_state = query_state or QueryState()\n+        self.primer = DRIFTPrimer(\n+            config=self.context_builder.config,\n+            chat_model=model,\n+            tokenizer=self.tokenizer,\n+        )\n+        self.callbacks = callbacks or []\n+        self.local_search = self.init_local_search()\n+\n+    def init_local_search(self) -> LocalSearch:\n+        \"\"\"\n+        Initialize the LocalSearch object with parameters based on the DRIFT search configuration.\n+\n+        Returns\n+        -------\n+        LocalSearch: An instance of the LocalSearch class with the configured parameters.\n+        \"\"\"\n+        local_context_params = {\n+            \"text_unit_prop\": self.context_builder.config.local_search_text_unit_prop,\n+            \"community_prop\": self.context_builder.config.local_search_community_prop,\n+            \"top_k_mapped_entities\": self.context_builder.config.local_search_top_k_mapped_entities,\n+            \"top_k_relationships\": self.context_builder.config.local_search_top_k_relationships,\n+            \"include_entity_rank\": True,\n+            \"include_relationship_weight\": True,\n+            \"include_community_rank\": False,\n+            \"return_candidate_context\": False,\n+            \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n+            \"max_context_tokens\": self.context_builder.config.local_search_max_data_tokens,\n+        }\n+\n+        model_params = get_openai_model_parameters_from_dict({\n+            \"model\": self.model.config.model,\n+            \"max_tokens\": self.context_builder.config.local_search_llm_max_gen_tokens,\n+            \"temperature\": self.context_builder.config.local_search_temperature,\n+            \"n\": self.context_builder.config.local_search_n,\n+            \"top_p\": self.context_builder.config.local_search_top_p,\n+            \"max_completion_tokens\": self.context_builder.config.local_search_llm_max_gen_completion_tokens,\n+            \"response_format\": {\"type\": \"json_object\"},\n+        })\n+\n+        return LocalSearch(\n+            model=self.model,\n+            system_prompt=self.context_builder.local_system_prompt,\n+            context_builder=self.context_builder.local_mixed_context,\n+            tokenizer=self.tokenizer,\n+            model_params=model_params,\n+            context_builder_params=local_context_params,\n+            response_type=\"multiple paragraphs\",\n+            callbacks=self.callbacks,\n+        )\n+\n+    def _process_primer_results(\n+        self, query: str, search_results: SearchResult\n+    ) -> DriftAction:\n+        \"\"\"\n+        Process the results from the primer search to extract intermediate answers and follow-up queries.\n+\n+        Args:\n+            query (str): The original search query.\n+            search_results (SearchResult): The results from the primer search.\n+\n+        Returns\n+        -------\n+        DriftAction: Action generated from the primer response.\n+\n+        Raises\n+        ------\n+        RuntimeError: If no intermediate answers or follow-up queries are found in the primer response.\n+        \"\"\"\n+        response = search_results.response\n+        if isinstance(response, list) and isinstance(response[0], dict):\n+            intermediate_answers = [\n+                i[\"intermediate_answer\"] for i in response if \"intermediate_answer\" in i\n+            ]\n+\n+            if not intermediate_answers:\n+                error_msg = \"No intermediate answers found in primer response. Ensure that the primer response includes intermediate answers.\"\n+                raise RuntimeError(error_msg)\n+\n+            intermediate_answer = \"\\n\\n\".join([\n+                i[\"intermediate_answer\"] for i in response if \"intermediate_answer\" in i\n+            ])\n+\n+            follow_ups = [fu for i in response for fu in i.get(\"follow_up_queries\", [])]\n+\n+            if not follow_ups:\n+                error_msg = \"No follow-up queries found in primer response. Ensure that the primer response includes follow-up queries.\"\n+                raise RuntimeError(error_msg)\n+\n+            score = sum(i.get(\"score\", float(\"-inf\")) for i in response) / len(response)\n+            response_data = {\n+                \"intermediate_answer\": intermediate_answer,\n+                \"follow_up_queries\": follow_ups,\n+                \"score\": score,\n+            }\n+            return DriftAction.from_primer_response(query, response_data)\n+        error_msg = \"Response must be a list of dictionaries.\"\n+        raise ValueError(error_msg)\n+\n+    async def _search_step(\n+        self, global_query: str, search_engine: LocalSearch, actions: list[DriftAction]\n+    ) -> list[DriftAction]:\n+        \"\"\"\n+        Perform an asynchronous search step by executing each DriftAction asynchronously.\n+\n+        Args:\n+            global_query (str): The global query for the search.\n+            search_engine (LocalSearch): The local search engine instance.\n+            actions (list[DriftAction]): A list of actions to perform.\n+\n+        Returns\n+        -------\n+        list[DriftAction]: The results from executing the search actions asynchronously.\n+        \"\"\"\n+        tasks = [\n+            action.search(search_engine=search_engine, global_query=global_query)\n+            for action in actions\n+        ]\n+        return await tqdm_asyncio.gather(*tasks, leave=False)\n+\n+    async def search(\n+        self,\n+        query: str,\n+        conversation_history: Any = None,\n+        reduce: bool = True,\n+        **kwargs,\n+    ) -> SearchResult:\n+        \"\"\"\n+        Perform an asynchronous DRIFT search.\n+\n+        Args:\n+            query (str): The query to search for.\n+            conversation_history (Any, optional): The conversation history, if any.\n+            reduce (bool, optional): Whether to reduce the response to a single comprehensive response.\n+\n+        Returns\n+        -------\n+        SearchResult: The search result containing the response and context data.\n+\n+        Raises\n+        ------\n+        ValueError: If the query is empty.\n+        \"\"\"\n+        if query == \"\":\n+            error_msg = \"DRIFT Search query cannot be empty.\"\n+            raise ValueError(error_msg)\n+\n+        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n+\n+        start_time = time.perf_counter()\n+\n+        # Check if query state is empty\n+        if not self.query_state.graph:\n+            # Prime the search with the primer\n+            primer_context, token_ct = await self.context_builder.build_context(query)\n+            llm_calls[\"build_context\"] = token_ct[\"llm_calls\"]\n+            prompt_tokens[\"build_context\"] = token_ct[\"prompt_tokens\"]\n+            output_tokens[\"build_context\"] = token_ct[\"output_tokens\"]\n+\n+            primer_response = await self.primer.search(\n+                query=query, top_k_reports=primer_context\n+            )\n+            llm_calls[\"primer\"] = primer_response.llm_calls\n+            prompt_tokens[\"primer\"] = primer_response.prompt_tokens\n+            output_tokens[\"primer\"] = primer_response.output_tokens\n+\n+            # Package response into DriftAction\n+            init_action = self._process_primer_results(query, primer_response)\n+            self.query_state.add_action(init_action)\n+            self.query_state.add_all_follow_ups(init_action, init_action.follow_ups)\n+\n+        # Main loop\n+        epochs = 0\n+        llm_call_offset = 0\n+        while epochs < self.context_builder.config.n_depth:\n+            actions = self.query_state.rank_incomplete_actions()\n+            if len(actions) == 0:\n+                logger.debug(\"No more actions to take. Exiting DRIFT loop.\")\n+                break\n+            actions = actions[: self.context_builder.config.drift_k_followups]\n+            llm_call_offset += (\n+                len(actions) - self.context_builder.config.drift_k_followups\n+            )\n+            # Process actions\n+            results = await self._search_step(\n+                global_query=query, search_engine=self.local_search, actions=actions\n+            )\n+\n+            # Update query state\n+            for action in results:\n+                self.query_state.add_action(action)\n+                self.query_state.add_all_follow_ups(action, action.follow_ups)\n+            epochs += 1\n+\n+        t_elapsed = time.perf_counter() - start_time\n+\n+        # Calculate token usage\n+        token_ct = self.query_state.action_token_ct()\n+        llm_calls[\"action\"] = token_ct[\"llm_calls\"]\n+        prompt_tokens[\"action\"] = token_ct[\"prompt_tokens\"]\n+        output_tokens[\"action\"] = token_ct[\"output_tokens\"]\n+\n+        # Package up context data\n+        response_state, context_data, context_text = self.query_state.serialize(\n+            include_context=True\n+        )\n+\n+        reduced_response = response_state\n+        if reduce:\n+            # Reduce response_state to a single comprehensive response\n+            for callback in self.callbacks:\n+                callback.on_reduce_response_start(response_state)\n+\n+            model_params = get_openai_model_parameters_from_dict({\n+                \"model\": self.model.config.model,\n+                \"max_tokens\": self.context_builder.config.reduce_max_tokens,\n+                \"temperature\": self.context_builder.config.reduce_temperature,\n+                \"max_completion_tokens\": self.context_builder.config.reduce_max_completion_tokens,\n+            })\n+\n+            reduced_response = await self._reduce_response(\n+                responses=response_state,\n+                query=query,\n+                llm_calls=llm_calls,\n+                prompt_tokens=prompt_tokens,\n+                output_tokens=output_tokens,\n+                model_params=model_params,\n+            )\n+\n+            for callback in self.callbacks:\n+                callback.on_reduce_response_end(reduced_response)\n+        return SearchResult(\n+            response=reduced_response,\n+            context_data=context_data,\n+            context_text=context_text,\n+            completion_time=t_elapsed,\n+            llm_calls=sum(llm_calls.values()),\n+            prompt_tokens=sum(prompt_tokens.values()),\n+            output_tokens=sum(output_tokens.values()),\n+            llm_calls_categories=llm_calls,\n+            prompt_tokens_categories=prompt_tokens,\n+            output_tokens_categories=output_tokens,\n+        )\n+\n+    async def stream_search(\n+        self, query: str, conversation_history: ConversationHistory | None = None\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"\n+        Perform a streaming DRIFT search asynchronously.\n+\n+        Args:\n+            query (str): The query to search for.\n+            conversation_history (ConversationHistory, optional): The conversation history.\n+        \"\"\"\n+        result = await self.search(\n+            query=query, conversation_history=conversation_history, reduce=False\n+        )\n+\n+        if isinstance(result.response, list):\n+            result.response = result.response[0]\n+\n+        for callback in self.callbacks:\n+            callback.on_reduce_response_start(result.response)\n+\n+        model_params = get_openai_model_parameters_from_dict({\n+            \"model\": self.model.config.model,\n+            \"max_tokens\": self.context_builder.config.reduce_max_tokens,\n+            \"temperature\": self.context_builder.config.reduce_temperature,\n+            \"max_completion_tokens\": self.context_builder.config.reduce_max_completion_tokens,\n+        })\n+\n+        full_response = \"\"\n+        async for resp in self._reduce_response_streaming(\n+            responses=result.response,\n+            query=query,\n+            model_params=model_params,\n+        ):\n+            full_response += resp\n+            yield resp\n+\n+        for callback in self.callbacks:\n+            callback.on_reduce_response_end(full_response)\n+\n+    async def _reduce_response(\n+        self,\n+        responses: str | dict[str, Any],\n+        query: str,\n+        llm_calls: dict[str, int],\n+        prompt_tokens: dict[str, int],\n+        output_tokens: dict[str, int],\n+        **llm_kwargs,\n+    ) -> str:\n+        \"\"\"Reduce the response to a single comprehensive response.\n+\n+        Parameters\n+        ----------\n+        responses : str|dict[str, Any]\n+            The responses to reduce.\n+        query : str\n+            The original query.\n+        llm_kwargs : dict[str, Any]\n+            Additional keyword arguments to pass to the LLM.\n+\n+        Returns\n+        -------\n+        str\n+            The reduced response.\n+        \"\"\"\n+        reduce_responses = []\n+\n+        if isinstance(responses, str):\n+            reduce_responses = [responses]\n+        else:\n+            reduce_responses = [\n+                response[\"answer\"]\n+                for response in responses.get(\"nodes\", [])\n+                if response.get(\"answer\")\n+            ]\n+\n+        search_prompt = self.context_builder.reduce_system_prompt.format(\n+            context_data=reduce_responses,\n+            response_type=self.context_builder.response_type,\n+        )\n+        search_messages = [\n+            {\"role\": \"system\", \"content\": search_prompt},\n+        ]\n+\n+        model_response = await self.model.achat(\n+            prompt=query,\n+            history=search_messages,\n+            model_parameters=llm_kwargs.get(\"model_params\", {}),\n+        )\n+\n+        reduced_response = model_response.output.content\n+\n+        llm_calls[\"reduce\"] = 1\n+        prompt_tokens[\"reduce\"] = len(self.tokenizer.encode(search_prompt)) + len(\n+            self.tokenizer.encode(query)\n+        )\n+        output_tokens[\"reduce\"] = len(self.tokenizer.encode(reduced_response))\n+\n+        return reduced_response\n+\n+    async def _reduce_response_streaming(\n+        self,\n+        responses: str | dict[str, Any],\n+        query: str,\n+        model_params: dict[str, Any],\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"Reduce the response to a single comprehensive response.\n+\n+        Parameters\n+        ----------\n+        responses : str|dict[str, Any]\n+            The responses to reduce.\n+        query : str\n+            The original query.\n+\n+        Returns\n+        -------\n+        str\n+            The reduced response.\n+        \"\"\"\n+        reduce_responses = []\n+\n+        if isinstance(responses, str):\n+            reduce_responses = [responses]\n+        else:\n+            reduce_responses = [\n+                response[\"answer\"]\n+                for response in responses.get(\"nodes\", [])\n+                if response.get(\"answer\")\n+            ]\n+\n+        search_prompt = self.context_builder.reduce_system_prompt.format(\n+            context_data=reduce_responses,\n+            response_type=self.context_builder.response_type,\n+        )\n+        search_messages = [\n+            {\"role\": \"system\", \"content\": search_prompt},\n+        ]\n+\n+        async for response in self.model.achat_stream(\n+            prompt=query,\n+            history=search_messages,\n+            model_parameters=model_params,\n+        ):\n+            for callback in self.callbacks:\n+                callback.on_llm_new_token(response)\n+            yield response\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/drift_search/state.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/drift_search/state.py b/packages/graphrag/graphrag/query/structured_search/drift_search/state.py\nnew file mode 100644\nindex 0000000..82453b3\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/drift_search/state.py\n@@ -0,0 +1,150 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Manage the state of the DRIFT query, including a graph of actions.\"\"\"\n+\n+import logging\n+import random\n+from collections.abc import Callable\n+from typing import Any\n+\n+import networkx as nx\n+\n+from graphrag.query.structured_search.drift_search.action import DriftAction\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class QueryState:\n+    \"\"\"Manage the state of the query, including a graph of actions.\"\"\"\n+\n+    def __init__(self):\n+        self.graph = nx.MultiDiGraph()\n+\n+    def add_action(self, action: DriftAction, metadata: dict[str, Any] | None = None):\n+        \"\"\"Add an action to the graph with optional metadata.\"\"\"\n+        self.graph.add_node(action, **(metadata or {}))\n+\n+    def relate_actions(\n+        self, parent: DriftAction, child: DriftAction, weight: float = 1.0\n+    ):\n+        \"\"\"Relate two actions in the graph.\"\"\"\n+        self.graph.add_edge(parent, child, weight=weight)\n+\n+    def add_all_follow_ups(\n+        self,\n+        action: DriftAction,\n+        follow_ups: list[DriftAction] | list[str],\n+        weight: float = 1.0,\n+    ):\n+        \"\"\"Add all follow-up actions and links them to the given action.\"\"\"\n+        if len(follow_ups) == 0:\n+            logger.warning(\"No follow-up actions for action: %s\", action.query)\n+\n+        for follow_up in follow_ups:\n+            if isinstance(follow_up, str):\n+                follow_up = DriftAction(query=follow_up)\n+            elif not isinstance(follow_up, DriftAction):\n+                logger.warning(\n+                    \"Follow-up action is not a string, found type: %s\", type(follow_up)\n+                )\n+\n+            self.add_action(follow_up)\n+            self.relate_actions(action, follow_up, weight)\n+\n+    def find_incomplete_actions(self) -> list[DriftAction]:\n+        \"\"\"Find all unanswered actions in the graph.\"\"\"\n+        return [node for node in self.graph.nodes if not node.is_complete]\n+\n+    def rank_incomplete_actions(\n+        self, scorer: Callable[[DriftAction], float] | None = None\n+    ) -> list[DriftAction]:\n+        \"\"\"Rank all unanswered actions in the graph if scorer available.\"\"\"\n+        unanswered = self.find_incomplete_actions()\n+        if scorer:\n+            for node in unanswered:\n+                node.compute_score(scorer)\n+            return sorted(\n+                unanswered,\n+                key=lambda node: (\n+                    node.score if node.score is not None else float(\"-inf\")\n+                ),\n+                reverse=True,\n+            )\n+\n+        # shuffle the list if no scorer\n+        random.shuffle(unanswered)\n+        return list(unanswered)\n+\n+    def serialize(\n+        self, include_context: bool = True\n+    ) -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]:\n+        \"\"\"Serialize the graph to a dictionary, including nodes and edges.\"\"\"\n+        # Create a mapping from nodes to unique IDs\n+        node_to_id = {node: idx for idx, node in enumerate(self.graph.nodes())}\n+\n+        # Serialize nodes\n+        nodes: list[dict[str, Any]] = [\n+            {\n+                **node.serialize(include_follow_ups=False),\n+                \"id\": node_to_id[node],\n+                **self.graph.nodes[node],\n+            }\n+            for node in self.graph.nodes()\n+        ]\n+\n+        # Serialize edges\n+        edges: list[dict[str, Any]] = [\n+            {\n+                \"source\": node_to_id[u],\n+                \"target\": node_to_id[v],\n+                \"weight\": edge_data.get(\"weight\", 1.0),\n+            }\n+            for u, v, edge_data in self.graph.edges(data=True)\n+        ]\n+\n+        if include_context:\n+            context_data = {\n+                node[\"query\"]: node[\"metadata\"][\"context_data\"]\n+                for node in nodes\n+                if node[\"metadata\"].get(\"context_data\") and node.get(\"query\")\n+            }\n+\n+            context_text = str(context_data)\n+\n+            return {\"nodes\": nodes, \"edges\": edges}, context_data, context_text\n+\n+        return {\"nodes\": nodes, \"edges\": edges}\n+\n+    def deserialize(self, data: dict[str, Any]):\n+        \"\"\"Deserialize the dictionary back to a graph.\"\"\"\n+        self.graph.clear()\n+        id_to_action = {}\n+\n+        for node_data in data.get(\"nodes\", []):\n+            node_id = node_data.pop(\"id\")\n+            action = DriftAction.deserialize(node_data)\n+            self.add_action(action)\n+            id_to_action[node_id] = action\n+\n+        for edge_data in data.get(\"edges\", []):\n+            source_id = edge_data[\"source\"]\n+            target_id = edge_data[\"target\"]\n+            weight = edge_data.get(\"weight\", 1.0)\n+            source_action = id_to_action.get(source_id)\n+            target_action = id_to_action.get(target_id)\n+            if source_action and target_action:\n+                self.relate_actions(source_action, target_action, weight)\n+\n+    def action_token_ct(self) -> dict[str, int]:\n+        \"\"\"Return the token count of the action.\"\"\"\n+        llm_calls, prompt_tokens, output_tokens = 0, 0, 0\n+        for action in self.graph.nodes:\n+            llm_calls += action.metadata[\"llm_calls\"]\n+            prompt_tokens += action.metadata[\"prompt_tokens\"]\n+            output_tokens += action.metadata[\"output_tokens\"]\n+        return {\n+            \"llm_calls\": llm_calls,\n+            \"prompt_tokens\": prompt_tokens,\n+            \"output_tokens\": output_tokens,\n+        }\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/global_search/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/global_search/__init__.py b/packages/graphrag/graphrag/query/structured_search/global_search/__init__.py\nnew file mode 100644\nindex 0000000..ba73b60\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/global_search/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GlobalSearch module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/global_search/community_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/global_search/community_context.py b/packages/graphrag/graphrag/query/structured_search/global_search/community_context.py\nnew file mode 100644\nindex 0000000..1709aab\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/global_search/community_context.py\n@@ -0,0 +1,144 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Contains algorithms to build context data for global search prompt.\"\"\"\n+\n+from typing import Any\n+\n+from graphrag.data_model.community import Community\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.entity import Entity\n+from graphrag.query.context_builder.builders import ContextBuilderResult\n+from graphrag.query.context_builder.community_context import (\n+    build_community_context,\n+)\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+from graphrag.query.context_builder.dynamic_community_selection import (\n+    DynamicCommunitySelection,\n+)\n+from graphrag.query.structured_search.base import GlobalContextBuilder\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+class GlobalCommunityContext(GlobalContextBuilder):\n+    \"\"\"GlobalSearch community context builder.\"\"\"\n+\n+    def __init__(\n+        self,\n+        community_reports: list[CommunityReport],\n+        communities: list[Community],\n+        entities: list[Entity] | None = None,\n+        tokenizer: Tokenizer | None = None,\n+        dynamic_community_selection: bool = False,\n+        dynamic_community_selection_kwargs: dict[str, Any] | None = None,\n+        random_state: int = 86,\n+    ):\n+        self.community_reports = community_reports\n+        self.entities = entities\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.dynamic_community_selection = None\n+        if dynamic_community_selection and isinstance(\n+            dynamic_community_selection_kwargs, dict\n+        ):\n+            self.dynamic_community_selection = DynamicCommunitySelection(\n+                community_reports=community_reports,\n+                communities=communities,\n+                model=dynamic_community_selection_kwargs.pop(\"model\"),\n+                tokenizer=dynamic_community_selection_kwargs.pop(\"tokenizer\"),\n+                **dynamic_community_selection_kwargs,\n+            )\n+        self.random_state = random_state\n+\n+    async def build_context(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        use_community_summary: bool = True,\n+        column_delimiter: str = \"|\",\n+        shuffle_data: bool = True,\n+        include_community_rank: bool = False,\n+        min_community_rank: int = 0,\n+        community_rank_name: str = \"rank\",\n+        include_community_weight: bool = True,\n+        community_weight_name: str = \"occurrence\",\n+        normalize_community_weight: bool = True,\n+        max_context_tokens: int = 8000,\n+        context_name: str = \"Reports\",\n+        conversation_history_user_turns_only: bool = True,\n+        conversation_history_max_turns: int | None = 5,\n+        **kwargs: Any,\n+    ) -> ContextBuilderResult:\n+        \"\"\"Prepare batches of community report data table as context data for global search.\"\"\"\n+        conversation_history_context = \"\"\n+        final_context_data = {}\n+        llm_calls, prompt_tokens, output_tokens = 0, 0, 0\n+        if conversation_history:\n+            # build conversation history context\n+            (\n+                conversation_history_context,\n+                conversation_history_context_data,\n+            ) = conversation_history.build_context(\n+                include_user_turns_only=conversation_history_user_turns_only,\n+                max_qa_turns=conversation_history_max_turns,\n+                column_delimiter=column_delimiter,\n+                max_context_tokens=max_context_tokens,\n+                recency_bias=False,\n+            )\n+            if conversation_history_context != \"\":\n+                final_context_data = conversation_history_context_data\n+\n+        community_reports = self.community_reports\n+        if self.dynamic_community_selection is not None:\n+            (\n+                community_reports,\n+                dynamic_info,\n+            ) = await self.dynamic_community_selection.select(query)\n+            llm_calls += dynamic_info[\"llm_calls\"]\n+            prompt_tokens += dynamic_info[\"prompt_tokens\"]\n+            output_tokens += dynamic_info[\"output_tokens\"]\n+\n+        community_context, community_context_data = build_community_context(\n+            community_reports=community_reports,\n+            entities=self.entities,\n+            tokenizer=self.tokenizer,\n+            use_community_summary=use_community_summary,\n+            column_delimiter=column_delimiter,\n+            shuffle_data=shuffle_data,\n+            include_community_rank=include_community_rank,\n+            min_community_rank=min_community_rank,\n+            community_rank_name=community_rank_name,\n+            include_community_weight=include_community_weight,\n+            community_weight_name=community_weight_name,\n+            normalize_community_weight=normalize_community_weight,\n+            max_context_tokens=max_context_tokens,\n+            single_batch=False,\n+            context_name=context_name,\n+            random_state=self.random_state,\n+        )\n+\n+        # Prepare context_prefix based on whether conversation_history_context exists\n+        context_prefix = (\n+            f\"{conversation_history_context}\\n\\n\"\n+            if conversation_history_context\n+            else \"\"\n+        )\n+\n+        final_context = (\n+            [f\"{context_prefix}{context}\" for context in community_context]\n+            if isinstance(community_context, list)\n+            else f\"{context_prefix}{community_context}\"\n+        )\n+\n+        # Update the final context data with the provided community_context_data\n+        final_context_data.update(community_context_data)\n+\n+        return ContextBuilderResult(\n+            context_chunks=final_context,\n+            context_records=final_context_data,\n+            llm_calls=llm_calls,\n+            prompt_tokens=prompt_tokens,\n+            output_tokens=output_tokens,\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/global_search/search.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/global_search/search.py b/packages/graphrag/graphrag/query/structured_search/global_search/search.py\nnew file mode 100644\nindex 0000000..86b95d0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/global_search/search.py\n@@ -0,0 +1,495 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The GlobalSearch Implementation.\"\"\"\n+\n+import asyncio\n+import json\n+import logging\n+import time\n+from collections.abc import AsyncGenerator\n+from dataclasses import dataclass\n+from typing import Any\n+\n+import pandas as pd\n+\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompts.query.global_search_knowledge_system_prompt import (\n+    GENERAL_KNOWLEDGE_INSTRUCTION,\n+)\n+from graphrag.prompts.query.global_search_map_system_prompt import (\n+    MAP_SYSTEM_PROMPT,\n+)\n+from graphrag.prompts.query.global_search_reduce_system_prompt import (\n+    NO_DATA_ANSWER,\n+    REDUCE_SYSTEM_PROMPT,\n+)\n+from graphrag.query.context_builder.builders import GlobalContextBuilder\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+from graphrag.query.llm.text_utils import try_parse_json_object\n+from graphrag.query.structured_search.base import BaseSearch, SearchResult\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass(kw_only=True)\n+class GlobalSearchResult(SearchResult):\n+    \"\"\"A GlobalSearch result.\"\"\"\n+\n+    map_responses: list[SearchResult]\n+    reduce_context_data: str | list[pd.DataFrame] | dict[str, pd.DataFrame]\n+    reduce_context_text: str | list[str] | dict[str, str]\n+\n+\n+class GlobalSearch(BaseSearch[GlobalContextBuilder]):\n+    \"\"\"Search orchestration for global search mode.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: GlobalContextBuilder,\n+        tokenizer: Tokenizer | None = None,\n+        map_system_prompt: str | None = None,\n+        reduce_system_prompt: str | None = None,\n+        response_type: str = \"multiple paragraphs\",\n+        allow_general_knowledge: bool = False,\n+        general_knowledge_inclusion_prompt: str | None = None,\n+        json_mode: bool = True,\n+        callbacks: list[QueryCallbacks] | None = None,\n+        max_data_tokens: int = 8000,\n+        map_llm_params: dict[str, Any] | None = None,\n+        reduce_llm_params: dict[str, Any] | None = None,\n+        map_max_length: int = 1000,\n+        reduce_max_length: int = 2000,\n+        context_builder_params: dict[str, Any] | None = None,\n+        concurrent_coroutines: int = 32,\n+    ):\n+        super().__init__(\n+            model=model,\n+            context_builder=context_builder,\n+            tokenizer=tokenizer,\n+            context_builder_params=context_builder_params,\n+        )\n+        self.map_system_prompt = map_system_prompt or MAP_SYSTEM_PROMPT\n+        self.reduce_system_prompt = reduce_system_prompt or REDUCE_SYSTEM_PROMPT\n+        self.response_type = response_type\n+        self.allow_general_knowledge = allow_general_knowledge\n+        self.general_knowledge_inclusion_prompt = (\n+            general_knowledge_inclusion_prompt or GENERAL_KNOWLEDGE_INSTRUCTION\n+        )\n+        self.callbacks = callbacks or []\n+        self.max_data_tokens = max_data_tokens\n+\n+        self.map_llm_params = map_llm_params if map_llm_params else {}\n+        self.reduce_llm_params = reduce_llm_params if reduce_llm_params else {}\n+        if json_mode:\n+            self.map_llm_params[\"response_format\"] = {\"type\": \"json_object\"}\n+        else:\n+            # remove response_format key if json_mode is False\n+            self.map_llm_params.pop(\"response_format\", None)\n+        self.map_max_length = map_max_length\n+        self.reduce_max_length = reduce_max_length\n+\n+        self.semaphore = asyncio.Semaphore(concurrent_coroutines)\n+\n+    async def stream_search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+    ) -> AsyncGenerator[str, None]:\n+        \"\"\"Stream the global search response.\"\"\"\n+        context_result = await self.context_builder.build_context(\n+            query=query,\n+            conversation_history=conversation_history,\n+            **self.context_builder_params,\n+        )\n+        for callback in self.callbacks:\n+            callback.on_map_response_start(context_result.context_chunks)  # type: ignore\n+\n+        map_responses = await asyncio.gather(*[\n+            self._map_response_single_batch(\n+                context_data=data,\n+                query=query,\n+                max_length=self.map_max_length,\n+                **self.map_llm_params,\n+            )\n+            for data in context_result.context_chunks\n+        ])\n+\n+        for callback in self.callbacks:\n+            callback.on_map_response_end(map_responses)  # type: ignore\n+            callback.on_context(context_result.context_records)\n+\n+        async for response in self._stream_reduce_response(\n+            map_responses=map_responses,  # type: ignore\n+            query=query,\n+            max_length=self.reduce_max_length,\n+            model_parameters=self.reduce_llm_params,\n+        ):\n+            yield response\n+\n+    async def search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs: Any,\n+    ) -> GlobalSearchResult:\n+        \"\"\"\n+        Perform a global search.\n+\n+        Global search mode includes two steps:\n+\n+        - Step 1: Run parallel LLM calls on communities' short summaries to generate answer for each batch\n+        - Step 2: Combine the answers from step 2 to generate the final answer\n+        \"\"\"\n+        # Step 1: Generate answers for each batch of community short summaries\n+        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n+\n+        start_time = time.time()\n+        context_result = await self.context_builder.build_context(\n+            query=query,\n+            conversation_history=conversation_history,\n+            **self.context_builder_params,\n+        )\n+        llm_calls[\"build_context\"] = context_result.llm_calls\n+        prompt_tokens[\"build_context\"] = context_result.prompt_tokens\n+        output_tokens[\"build_context\"] = context_result.output_tokens\n+\n+        for callback in self.callbacks:\n+            callback.on_map_response_start(context_result.context_chunks)  # type: ignore\n+\n+        map_responses = await asyncio.gather(*[\n+            self._map_response_single_batch(\n+                context_data=data,\n+                query=query,\n+                max_length=self.map_max_length,\n+                **self.map_llm_params,\n+            )\n+            for data in context_result.context_chunks\n+        ])\n+\n+        for callback in self.callbacks:\n+            callback.on_map_response_end(map_responses)\n+            callback.on_context(context_result.context_records)\n+\n+        llm_calls[\"map\"] = sum(response.llm_calls for response in map_responses)\n+        prompt_tokens[\"map\"] = sum(response.prompt_tokens for response in map_responses)\n+        output_tokens[\"map\"] = sum(response.output_tokens for response in map_responses)\n+\n+        # Step 2: Combine the intermediate answers from step 2 to generate the final answer\n+        reduce_response = await self._reduce_response(\n+            map_responses=map_responses,\n+            query=query,\n+            **self.reduce_llm_params,\n+        )\n+        llm_calls[\"reduce\"] = reduce_response.llm_calls\n+        prompt_tokens[\"reduce\"] = reduce_response.prompt_tokens\n+        output_tokens[\"reduce\"] = reduce_response.output_tokens\n+\n+        return GlobalSearchResult(\n+            response=reduce_response.response,\n+            context_data=context_result.context_records,\n+            context_text=context_result.context_chunks,\n+            map_responses=map_responses,\n+            reduce_context_data=reduce_response.context_data,\n+            reduce_context_text=reduce_response.context_text,\n+            completion_time=time.time() - start_time,\n+            llm_calls=sum(llm_calls.values()),\n+            prompt_tokens=sum(prompt_tokens.values()),\n+            output_tokens=sum(output_tokens.values()),\n+            llm_calls_categories=llm_calls,\n+            prompt_tokens_categories=prompt_tokens,\n+            output_tokens_categories=output_tokens,\n+        )\n+\n+    async def _map_response_single_batch(\n+        self,\n+        context_data: str,\n+        query: str,\n+        max_length: int,\n+        **llm_kwargs,\n+    ) -> SearchResult:\n+        \"\"\"Generate answer for a single chunk of community reports.\"\"\"\n+        start_time = time.time()\n+        search_prompt = \"\"\n+        try:\n+            search_prompt = self.map_system_prompt.format(\n+                context_data=context_data, max_length=max_length\n+            )\n+            search_messages = [\n+                {\"role\": \"system\", \"content\": search_prompt},\n+            ]\n+            async with self.semaphore:\n+                model_response = await self.model.achat(\n+                    prompt=query,\n+                    history=search_messages,\n+                    model_parameters=llm_kwargs,\n+                    json=True,\n+                )\n+                search_response = model_response.output.content\n+                logger.debug(\"Map response: %s\", search_response)\n+            try:\n+                # parse search response json\n+                processed_response = self._parse_search_response(search_response)\n+            except ValueError:\n+                logger.warning(\n+                    \"Warning: Error parsing search response json - skipping this batch\"\n+                )\n+                processed_response = []\n+\n+            return SearchResult(\n+                response=processed_response,\n+                context_data=context_data,\n+                context_text=context_data,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=len(self.tokenizer.encode(search_response)),\n+            )\n+\n+        except Exception:\n+            logger.exception(\"Exception in _map_response_single_batch\")\n+            return SearchResult(\n+                response=[{\"answer\": \"\", \"score\": 0}],\n+                context_data=context_data,\n+                context_text=context_data,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=0,\n+            )\n+\n+    def _parse_search_response(self, search_response: str) -> list[dict[str, Any]]:\n+        \"\"\"Parse the search response json and return a list of key points.\n+\n+        Parameters\n+        ----------\n+        search_response: str\n+            The search response json string\n+\n+        Returns\n+        -------\n+        list[dict[str, Any]]\n+            A list of key points, each key point is a dictionary with \"answer\" and \"score\" keys\n+        \"\"\"\n+        search_response, j = try_parse_json_object(search_response)\n+        if j == {}:\n+            return [{\"answer\": \"\", \"score\": 0}]\n+\n+        parsed_elements = json.loads(search_response).get(\"points\")\n+        if not parsed_elements or not isinstance(parsed_elements, list):\n+            return [{\"answer\": \"\", \"score\": 0}]\n+\n+        return [\n+            {\n+                \"answer\": element[\"description\"],\n+                \"score\": int(element[\"score\"]),\n+            }\n+            for element in parsed_elements\n+            if \"description\" in element and \"score\" in element\n+        ]\n+\n+    async def _reduce_response(\n+        self,\n+        map_responses: list[SearchResult],\n+        query: str,\n+        **llm_kwargs,\n+    ) -> SearchResult:\n+        \"\"\"Combine all intermediate responses from single batches into a final answer to the user query.\"\"\"\n+        text_data = \"\"\n+        search_prompt = \"\"\n+        start_time = time.time()\n+        try:\n+            # collect all key points into a single list to prepare for sorting\n+            key_points = []\n+            for index, response in enumerate(map_responses):\n+                if not isinstance(response.response, list):\n+                    continue\n+                for element in response.response:\n+                    if not isinstance(element, dict):\n+                        continue\n+                    if \"answer\" not in element or \"score\" not in element:\n+                        continue\n+                    key_points.append({\n+                        \"analyst\": index,\n+                        \"answer\": element[\"answer\"],\n+                        \"score\": element[\"score\"],\n+                    })\n+\n+            # filter response with score = 0 and rank responses by descending order of score\n+            filtered_key_points = [\n+                point\n+                for point in key_points\n+                if point[\"score\"] > 0  # type: ignore\n+            ]\n+\n+            if len(filtered_key_points) == 0 and not self.allow_general_knowledge:\n+                # return no data answer if no key points are found\n+                logger.warning(\n+                    \"Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.\"\n+                )\n+                return SearchResult(\n+                    response=NO_DATA_ANSWER,\n+                    context_data=\"\",\n+                    context_text=\"\",\n+                    completion_time=time.time() - start_time,\n+                    llm_calls=0,\n+                    prompt_tokens=0,\n+                    output_tokens=0,\n+                )\n+\n+            filtered_key_points = sorted(\n+                filtered_key_points,\n+                key=lambda x: x[\"score\"],  # type: ignore\n+                reverse=True,  # type: ignore\n+            )\n+\n+            data = []\n+            total_tokens = 0\n+            for point in filtered_key_points:\n+                formatted_response_data = []\n+                formatted_response_data.append(\n+                    f\"----Analyst {point['analyst'] + 1}----\"\n+                )\n+                formatted_response_data.append(\n+                    f\"Importance Score: {point['score']}\"  # type: ignore\n+                )\n+                formatted_response_data.append(point[\"answer\"])  # type: ignore\n+                formatted_response_text = \"\\n\".join(formatted_response_data)\n+                if (\n+                    total_tokens + len(self.tokenizer.encode(formatted_response_text))\n+                    > self.max_data_tokens\n+                ):\n+                    break\n+                data.append(formatted_response_text)\n+                total_tokens += len(self.tokenizer.encode(formatted_response_text))\n+            text_data = \"\\n\\n\".join(data)\n+\n+            search_prompt = self.reduce_system_prompt.format(\n+                report_data=text_data,\n+                response_type=self.response_type,\n+                max_length=self.reduce_max_length,\n+            )\n+            if self.allow_general_knowledge:\n+                search_prompt += \"\\n\" + self.general_knowledge_inclusion_prompt\n+            search_messages = [\n+                {\"role\": \"system\", \"content\": search_prompt},\n+                {\"role\": \"user\", \"content\": query},\n+            ]\n+\n+            search_response = \"\"\n+            async for chunk_response in self.model.achat_stream(\n+                prompt=query,\n+                history=search_messages,\n+                model_parameters=llm_kwargs,\n+            ):\n+                search_response += chunk_response\n+                for callback in self.callbacks:\n+                    callback.on_llm_new_token(chunk_response)\n+\n+            return SearchResult(\n+                response=search_response,\n+                context_data=text_data,\n+                context_text=text_data,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=len(self.tokenizer.encode(search_response)),\n+            )\n+        except Exception:\n+            logger.exception(\"Exception in reduce_response\")\n+            return SearchResult(\n+                response=\"\",\n+                context_data=text_data,\n+                context_text=text_data,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=0,\n+            )\n+\n+    async def _stream_reduce_response(\n+        self,\n+        map_responses: list[SearchResult],\n+        query: str,\n+        max_length: int,\n+        **llm_kwargs,\n+    ) -> AsyncGenerator[str, None]:\n+        # collect all key points into a single list to prepare for sorting\n+        key_points = []\n+        for index, response in enumerate(map_responses):\n+            if not isinstance(response.response, list):\n+                continue\n+            for element in response.response:\n+                if not isinstance(element, dict):\n+                    continue\n+                if \"answer\" not in element or \"score\" not in element:\n+                    continue\n+                key_points.append({\n+                    \"analyst\": index,\n+                    \"answer\": element[\"answer\"],\n+                    \"score\": element[\"score\"],\n+                })\n+\n+        # filter response with score = 0 and rank responses by descending order of score\n+        filtered_key_points = [\n+            point\n+            for point in key_points\n+            if point[\"score\"] > 0  # type: ignore\n+        ]\n+\n+        if len(filtered_key_points) == 0 and not self.allow_general_knowledge:\n+            # return no data answer if no key points are found\n+            logger.warning(\n+                \"Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.\"\n+            )\n+            yield NO_DATA_ANSWER\n+            return\n+\n+        filtered_key_points = sorted(\n+            filtered_key_points,\n+            key=lambda x: x[\"score\"],  # type: ignore\n+            reverse=True,  # type: ignore\n+        )\n+\n+        data = []\n+        total_tokens = 0\n+        for point in filtered_key_points:\n+            formatted_response_data = [\n+                f\"----Analyst {point['analyst'] + 1}----\",\n+                f\"Importance Score: {point['score']}\",\n+                point[\"answer\"],\n+            ]\n+            formatted_response_text = \"\\n\".join(formatted_response_data)\n+            if (\n+                total_tokens + len(self.tokenizer.encode(formatted_response_text))\n+                > self.max_data_tokens\n+            ):\n+                break\n+            data.append(formatted_response_text)\n+            total_tokens += len(self.tokenizer.encode(formatted_response_text))\n+        text_data = \"\\n\\n\".join(data)\n+\n+        search_prompt = self.reduce_system_prompt.format(\n+            report_data=text_data,\n+            response_type=self.response_type,\n+            max_length=max_length,\n+        )\n+        if self.allow_general_knowledge:\n+            search_prompt += \"\\n\" + self.general_knowledge_inclusion_prompt\n+        search_messages = [\n+            {\"role\": \"system\", \"content\": search_prompt},\n+        ]\n+\n+        async for chunk_response in self.model.achat_stream(\n+            prompt=query,\n+            history=search_messages,\n+            **llm_kwargs,\n+        ):\n+            for callback in self.callbacks:\n+                callback.on_llm_new_token(chunk_response)\n+            yield chunk_response\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/local_search/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/local_search/__init__.py b/packages/graphrag/graphrag/query/structured_search/local_search/__init__.py\nnew file mode 100644\nindex 0000000..8b8b1e7\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/local_search/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The LocalSearch package.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/local_search/mixed_context.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/local_search/mixed_context.py b/packages/graphrag/graphrag/query/structured_search/local_search/mixed_context.py\nnew file mode 100644\nindex 0000000..b91272d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/local_search/mixed_context.py\n@@ -0,0 +1,489 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\"\"\"Algorithms to build context data for local search prompt.\"\"\"\n+\n+import logging\n+from copy import deepcopy\n+from typing import Any\n+\n+import pandas as pd\n+\n+from graphrag.data_model.community_report import CommunityReport\n+from graphrag.data_model.covariate import Covariate\n+from graphrag.data_model.entity import Entity\n+from graphrag.data_model.relationship import Relationship\n+from graphrag.data_model.text_unit import TextUnit\n+from graphrag.language_model.protocol.base import EmbeddingModel\n+from graphrag.query.context_builder.builders import ContextBuilderResult\n+from graphrag.query.context_builder.community_context import (\n+    build_community_context,\n+)\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+from graphrag.query.context_builder.entity_extraction import (\n+    EntityVectorStoreKey,\n+    map_query_to_entities,\n+)\n+from graphrag.query.context_builder.local_context import (\n+    build_covariates_context,\n+    build_entity_context,\n+    build_relationship_context,\n+    get_candidate_context,\n+)\n+from graphrag.query.context_builder.source_context import (\n+    build_text_unit_context,\n+    count_relationships,\n+)\n+from graphrag.query.input.retrieval.community_reports import (\n+    get_candidate_communities,\n+)\n+from graphrag.query.input.retrieval.text_units import get_candidate_text_units\n+from graphrag.query.structured_search.base import LocalContextBuilder\n+from graphrag.tokenizer.get_tokenizer import get_tokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+from graphrag.vector_stores.base import BaseVectorStore\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LocalSearchMixedContext(LocalContextBuilder):\n+    \"\"\"Build data context for local search prompt combining community reports and entity/relationship/covariate tables.\"\"\"\n+\n+    def __init__(\n+        self,\n+        entities: list[Entity],\n+        entity_text_embeddings: BaseVectorStore,\n+        text_embedder: EmbeddingModel,\n+        text_units: list[TextUnit] | None = None,\n+        community_reports: list[CommunityReport] | None = None,\n+        relationships: list[Relationship] | None = None,\n+        covariates: dict[str, list[Covariate]] | None = None,\n+        tokenizer: Tokenizer | None = None,\n+        embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n+    ):\n+        if community_reports is None:\n+            community_reports = []\n+        if relationships is None:\n+            relationships = []\n+        if covariates is None:\n+            covariates = {}\n+        if text_units is None:\n+            text_units = []\n+        self.entities = {entity.id: entity for entity in entities}\n+        self.community_reports = {\n+            community.community_id: community for community in community_reports\n+        }\n+        self.text_units = {unit.id: unit for unit in text_units}\n+        self.relationships = {\n+            relationship.id: relationship for relationship in relationships\n+        }\n+        self.covariates = covariates\n+        self.entity_text_embeddings = entity_text_embeddings\n+        self.text_embedder = text_embedder\n+        self.tokenizer = tokenizer or get_tokenizer()\n+        self.embedding_vectorstore_key = embedding_vectorstore_key\n+\n+    def build_context(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        include_entity_names: list[str] | None = None,\n+        exclude_entity_names: list[str] | None = None,\n+        conversation_history_max_turns: int | None = 5,\n+        conversation_history_user_turns_only: bool = True,\n+        max_context_tokens: int = 8000,\n+        text_unit_prop: float = 0.5,\n+        community_prop: float = 0.25,\n+        top_k_mapped_entities: int = 10,\n+        top_k_relationships: int = 10,\n+        include_community_rank: bool = False,\n+        include_entity_rank: bool = False,\n+        rank_description: str = \"number of relationships\",\n+        include_relationship_weight: bool = False,\n+        relationship_ranking_attribute: str = \"rank\",\n+        return_candidate_context: bool = False,\n+        use_community_summary: bool = False,\n+        min_community_rank: int = 0,\n+        community_context_name: str = \"Reports\",\n+        column_delimiter: str = \"|\",\n+        **kwargs: dict[str, Any],\n+    ) -> ContextBuilderResult:\n+        \"\"\"\n+        Build data context for local search prompt.\n+\n+        Build a context by combining community reports and entity/relationship/covariate tables, and text units using a predefined ratio set by summary_prop.\n+        \"\"\"\n+        if include_entity_names is None:\n+            include_entity_names = []\n+        if exclude_entity_names is None:\n+            exclude_entity_names = []\n+        if community_prop + text_unit_prop > 1:\n+            value_error = (\n+                \"The sum of community_prop and text_unit_prop should not exceed 1.\"\n+            )\n+            raise ValueError(value_error)\n+\n+        # map user query to entities\n+        # if there is conversation history, attached the previous user questions to the current query\n+        if conversation_history:\n+            pre_user_questions = \"\\n\".join(\n+                conversation_history.get_user_turns(conversation_history_max_turns)\n+            )\n+            query = f\"{query}\\n{pre_user_questions}\"\n+\n+        selected_entities = map_query_to_entities(\n+            query=query,\n+            text_embedding_vectorstore=self.entity_text_embeddings,\n+            text_embedder=self.text_embedder,\n+            all_entities_dict=self.entities,\n+            embedding_vectorstore_key=self.embedding_vectorstore_key,\n+            include_entity_names=include_entity_names,\n+            exclude_entity_names=exclude_entity_names,\n+            k=top_k_mapped_entities,\n+            oversample_scaler=2,\n+        )\n+\n+        # build context\n+        final_context = list[str]()\n+        final_context_data = dict[str, pd.DataFrame]()\n+\n+        if conversation_history:\n+            # build conversation history context\n+            (\n+                conversation_history_context,\n+                conversation_history_context_data,\n+            ) = conversation_history.build_context(\n+                include_user_turns_only=conversation_history_user_turns_only,\n+                max_qa_turns=conversation_history_max_turns,\n+                column_delimiter=column_delimiter,\n+                max_context_tokens=max_context_tokens,\n+                recency_bias=False,\n+            )\n+            if conversation_history_context.strip() != \"\":\n+                final_context.append(conversation_history_context)\n+                final_context_data = conversation_history_context_data\n+                max_context_tokens = max_context_tokens - len(\n+                    self.tokenizer.encode(conversation_history_context)\n+                )\n+\n+        # build community context\n+        community_tokens = max(int(max_context_tokens * community_prop), 0)\n+        community_context, community_context_data = self._build_community_context(\n+            selected_entities=selected_entities,\n+            max_context_tokens=community_tokens,\n+            use_community_summary=use_community_summary,\n+            column_delimiter=column_delimiter,\n+            include_community_rank=include_community_rank,\n+            min_community_rank=min_community_rank,\n+            return_candidate_context=return_candidate_context,\n+            context_name=community_context_name,\n+        )\n+        if community_context.strip() != \"\":\n+            final_context.append(community_context)\n+            final_context_data = {**final_context_data, **community_context_data}\n+\n+        # build local (i.e. entity-relationship-covariate) context\n+        local_prop = 1 - community_prop - text_unit_prop\n+        local_tokens = max(int(max_context_tokens * local_prop), 0)\n+        local_context, local_context_data = self._build_local_context(\n+            selected_entities=selected_entities,\n+            max_context_tokens=local_tokens,\n+            include_entity_rank=include_entity_rank,\n+            rank_description=rank_description,\n+            include_relationship_weight=include_relationship_weight,\n+            top_k_relationships=top_k_relationships,\n+            relationship_ranking_attribute=relationship_ranking_attribute,\n+            return_candidate_context=return_candidate_context,\n+            column_delimiter=column_delimiter,\n+        )\n+        if local_context.strip() != \"\":\n+            final_context.append(str(local_context))\n+            final_context_data = {**final_context_data, **local_context_data}\n+\n+        text_unit_tokens = max(int(max_context_tokens * text_unit_prop), 0)\n+        text_unit_context, text_unit_context_data = self._build_text_unit_context(\n+            selected_entities=selected_entities,\n+            max_context_tokens=text_unit_tokens,\n+            return_candidate_context=return_candidate_context,\n+        )\n+\n+        if text_unit_context.strip() != \"\":\n+            final_context.append(text_unit_context)\n+            final_context_data = {**final_context_data, **text_unit_context_data}\n+\n+        return ContextBuilderResult(\n+            context_chunks=\"\\n\\n\".join(final_context),\n+            context_records=final_context_data,\n+        )\n+\n+    def _build_community_context(\n+        self,\n+        selected_entities: list[Entity],\n+        max_context_tokens: int = 4000,\n+        use_community_summary: bool = False,\n+        column_delimiter: str = \"|\",\n+        include_community_rank: bool = False,\n+        min_community_rank: int = 0,\n+        return_candidate_context: bool = False,\n+        context_name: str = \"Reports\",\n+    ) -> tuple[str, dict[str, pd.DataFrame]]:\n+        \"\"\"Add community data to the context window until it hits the max_context_tokens limit.\"\"\"\n+        if len(selected_entities) == 0 or len(self.community_reports) == 0:\n+            return (\"\", {context_name.lower(): pd.DataFrame()})\n+\n+        community_matches = {}\n+        for entity in selected_entities:\n+            # increase count of the community that this entity belongs to\n+            if entity.community_ids:\n+                for community_id in entity.community_ids:\n+                    community_matches[community_id] = (\n+                        community_matches.get(community_id, 0) + 1\n+                    )\n+\n+        # sort communities by number of matched entities and rank\n+        selected_communities = [\n+            self.community_reports[community_id]\n+            for community_id in community_matches\n+            if community_id in self.community_reports\n+        ]\n+        for community in selected_communities:\n+            if community.attributes is None:\n+                community.attributes = {}\n+            community.attributes[\"matches\"] = community_matches[community.community_id]\n+        selected_communities.sort(\n+            key=lambda x: (x.attributes[\"matches\"], x.rank),  # type: ignore\n+            reverse=True,  # type: ignore\n+        )\n+        for community in selected_communities:\n+            del community.attributes[\"matches\"]  # type: ignore\n+\n+        context_text, context_data = build_community_context(\n+            community_reports=selected_communities,\n+            tokenizer=self.tokenizer,\n+            use_community_summary=use_community_summary,\n+            column_delimiter=column_delimiter,\n+            shuffle_data=False,\n+            include_community_rank=include_community_rank,\n+            min_community_rank=min_community_rank,\n+            max_context_tokens=max_context_tokens,\n+            single_batch=True,\n+            context_name=context_name,\n+        )\n+        if isinstance(context_text, list) and len(context_text) > 0:\n+            context_text = \"\\n\\n\".join(context_text)\n+\n+        if return_candidate_context:\n+            candidate_context_data = get_candidate_communities(\n+                selected_entities=selected_entities,\n+                community_reports=list(self.community_reports.values()),\n+                use_community_summary=use_community_summary,\n+                include_community_rank=include_community_rank,\n+            )\n+            context_key = context_name.lower()\n+            if context_key not in context_data:\n+                context_data[context_key] = candidate_context_data\n+                context_data[context_key][\"in_context\"] = False\n+            else:\n+                if (\n+                    \"id\" in candidate_context_data.columns\n+                    and \"id\" in context_data[context_key].columns\n+                ):\n+                    candidate_context_data[\"in_context\"] = candidate_context_data[\n+                        \"id\"\n+                    ].isin(  # cspell:disable-line\n+                        context_data[context_key][\"id\"]\n+                    )\n+                    context_data[context_key] = candidate_context_data\n+                else:\n+                    context_data[context_key][\"in_context\"] = True\n+        return (str(context_text), context_data)\n+\n+    def _build_text_unit_context(\n+        self,\n+        selected_entities: list[Entity],\n+        max_context_tokens: int = 8000,\n+        return_candidate_context: bool = False,\n+        column_delimiter: str = \"|\",\n+        context_name: str = \"Sources\",\n+    ) -> tuple[str, dict[str, pd.DataFrame]]:\n+        \"\"\"Rank matching text units and add them to the context window until it hits the max_context_tokens limit.\"\"\"\n+        if not selected_entities or not self.text_units:\n+            return (\"\", {context_name.lower(): pd.DataFrame()})\n+        selected_text_units = []\n+        text_unit_ids_set = set()\n+\n+        unit_info_list = []\n+        relationship_values = list(self.relationships.values())\n+\n+        for index, entity in enumerate(selected_entities):\n+            # get matching relationships\n+            entity_relationships = [\n+                rel\n+                for rel in relationship_values\n+                if rel.source == entity.title or rel.target == entity.title\n+            ]\n+\n+            for text_id in entity.text_unit_ids or []:\n+                if text_id not in text_unit_ids_set and text_id in self.text_units:\n+                    selected_unit = deepcopy(self.text_units[text_id])\n+                    num_relationships = count_relationships(\n+                        entity_relationships, selected_unit\n+                    )\n+                    text_unit_ids_set.add(text_id)\n+                    unit_info_list.append((selected_unit, index, num_relationships))\n+\n+        # sort by entity_order and the number of relationships desc\n+        unit_info_list.sort(key=lambda x: (x[1], -x[2]))\n+\n+        selected_text_units = [unit[0] for unit in unit_info_list]\n+\n+        context_text, context_data = build_text_unit_context(\n+            text_units=selected_text_units,\n+            tokenizer=self.tokenizer,\n+            max_context_tokens=max_context_tokens,\n+            shuffle_data=False,\n+            context_name=context_name,\n+            column_delimiter=column_delimiter,\n+        )\n+\n+        if return_candidate_context:\n+            candidate_context_data = get_candidate_text_units(\n+                selected_entities=selected_entities,\n+                text_units=list(self.text_units.values()),\n+            )\n+            context_key = context_name.lower()\n+            if context_key not in context_data:\n+                candidate_context_data[\"in_context\"] = False\n+                context_data[context_key] = candidate_context_data\n+            else:\n+                if (\n+                    \"id\" in candidate_context_data.columns\n+                    and \"id\" in context_data[context_key].columns\n+                ):\n+                    candidate_context_data[\"in_context\"] = candidate_context_data[\n+                        \"id\"\n+                    ].isin(context_data[context_key][\"id\"])\n+                    context_data[context_key] = candidate_context_data\n+                else:\n+                    context_data[context_key][\"in_context\"] = True\n+\n+        return (str(context_text), context_data)\n+\n+    def _build_local_context(\n+        self,\n+        selected_entities: list[Entity],\n+        max_context_tokens: int = 8000,\n+        include_entity_rank: bool = False,\n+        rank_description: str = \"relationship count\",\n+        include_relationship_weight: bool = False,\n+        top_k_relationships: int = 10,\n+        relationship_ranking_attribute: str = \"rank\",\n+        return_candidate_context: bool = False,\n+        column_delimiter: str = \"|\",\n+    ) -> tuple[str, dict[str, pd.DataFrame]]:\n+        \"\"\"Build data context for local search prompt combining entity/relationship/covariate tables.\"\"\"\n+        # build entity context\n+        entity_context, entity_context_data = build_entity_context(\n+            selected_entities=selected_entities,\n+            tokenizer=self.tokenizer,\n+            max_context_tokens=max_context_tokens,\n+            column_delimiter=column_delimiter,\n+            include_entity_rank=include_entity_rank,\n+            rank_description=rank_description,\n+            context_name=\"Entities\",\n+        )\n+        entity_tokens = len(self.tokenizer.encode(entity_context))\n+\n+        # build relationship-covariate context\n+        added_entities = []\n+        final_context = []\n+        final_context_data = {}\n+\n+        # gradually add entities and associated metadata to the context until we reach limit\n+        for entity in selected_entities:\n+            current_context = []\n+            current_context_data = {}\n+            added_entities.append(entity)\n+\n+            # build relationship context\n+            (\n+                relationship_context,\n+                relationship_context_data,\n+            ) = build_relationship_context(\n+                selected_entities=added_entities,\n+                relationships=list(self.relationships.values()),\n+                tokenizer=self.tokenizer,\n+                max_context_tokens=max_context_tokens,\n+                column_delimiter=column_delimiter,\n+                top_k_relationships=top_k_relationships,\n+                include_relationship_weight=include_relationship_weight,\n+                relationship_ranking_attribute=relationship_ranking_attribute,\n+                context_name=\"Relationships\",\n+            )\n+            current_context.append(relationship_context)\n+            current_context_data[\"relationships\"] = relationship_context_data\n+            total_tokens = entity_tokens + len(\n+                self.tokenizer.encode(relationship_context)\n+            )\n+\n+            # build covariate context\n+            for covariate in self.covariates:\n+                covariate_context, covariate_context_data = build_covariates_context(\n+                    selected_entities=added_entities,\n+                    covariates=self.covariates[covariate],\n+                    tokenizer=self.tokenizer,\n+                    max_context_tokens=max_context_tokens,\n+                    column_delimiter=column_delimiter,\n+                    context_name=covariate,\n+                )\n+                total_tokens += len(self.tokenizer.encode(covariate_context))\n+                current_context.append(covariate_context)\n+                current_context_data[covariate.lower()] = covariate_context_data\n+\n+            if total_tokens > max_context_tokens:\n+                logger.warning(\n+                    \"Reached token limit - reverting to previous context state\"\n+                )\n+                break\n+\n+            final_context = current_context\n+            final_context_data = current_context_data\n+\n+        # attach entity context to final context\n+        final_context_text = entity_context + \"\\n\\n\" + \"\\n\\n\".join(final_context)\n+        final_context_data[\"entities\"] = entity_context_data\n+\n+        if return_candidate_context:\n+            # we return all the candidate entities/relationships/covariates (not only those that were fitted into the context window)\n+            # and add a tag to indicate which records were included in the context window\n+            candidate_context_data = get_candidate_context(\n+                selected_entities=selected_entities,\n+                entities=list(self.entities.values()),\n+                relationships=list(self.relationships.values()),\n+                covariates=self.covariates,\n+                include_entity_rank=include_entity_rank,\n+                entity_rank_description=rank_description,\n+                include_relationship_weight=include_relationship_weight,\n+            )\n+            for key in candidate_context_data:\n+                candidate_df = candidate_context_data[key]\n+                if key not in final_context_data:\n+                    final_context_data[key] = candidate_df\n+                    final_context_data[key][\"in_context\"] = False\n+                else:\n+                    in_context_df = final_context_data[key]\n+\n+                    if \"id\" in in_context_df.columns and \"id\" in candidate_df.columns:\n+                        candidate_df[\"in_context\"] = candidate_df[\n+                            \"id\"\n+                        ].isin(  # cspell:disable-line\n+                            in_context_df[\"id\"]\n+                        )\n+                        final_context_data[key] = candidate_df\n+                    else:\n+                        final_context_data[key][\"in_context\"] = True\n+        else:\n+            for key in final_context_data:\n+                final_context_data[key][\"in_context\"] = True\n+        return (final_context_text, final_context_data)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/query/structured_search/local_search/search.py",
            "diff": "diff --git a/packages/graphrag/graphrag/query/structured_search/local_search/search.py b/packages/graphrag/graphrag/query/structured_search/local_search/search.py\nnew file mode 100644\nindex 0000000..fdd7294\n--- /dev/null\n+++ b/packages/graphrag/graphrag/query/structured_search/local_search/search.py\n@@ -0,0 +1,163 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LocalSearch implementation.\"\"\"\n+\n+import logging\n+import time\n+from collections.abc import AsyncGenerator\n+from typing import Any\n+\n+from graphrag.callbacks.query_callbacks import QueryCallbacks\n+from graphrag.language_model.protocol.base import ChatModel\n+from graphrag.prompts.query.local_search_system_prompt import (\n+    LOCAL_SEARCH_SYSTEM_PROMPT,\n+)\n+from graphrag.query.context_builder.builders import LocalContextBuilder\n+from graphrag.query.context_builder.conversation_history import (\n+    ConversationHistory,\n+)\n+from graphrag.query.structured_search.base import BaseSearch, SearchResult\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LocalSearch(BaseSearch[LocalContextBuilder]):\n+    \"\"\"Search orchestration for local search mode.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model: ChatModel,\n+        context_builder: LocalContextBuilder,\n+        tokenizer: Tokenizer | None = None,\n+        system_prompt: str | None = None,\n+        response_type: str = \"multiple paragraphs\",\n+        callbacks: list[QueryCallbacks] | None = None,\n+        model_params: dict[str, Any] | None = None,\n+        context_builder_params: dict | None = None,\n+    ):\n+        super().__init__(\n+            model=model,\n+            context_builder=context_builder,\n+            tokenizer=tokenizer,\n+            model_params=model_params,\n+            context_builder_params=context_builder_params or {},\n+        )\n+        self.system_prompt = system_prompt or LOCAL_SEARCH_SYSTEM_PROMPT\n+        self.callbacks = callbacks or []\n+        self.response_type = response_type\n+\n+    async def search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+        **kwargs,\n+    ) -> SearchResult:\n+        \"\"\"Build local search context that fits a single context window and generate answer for the user query.\"\"\"\n+        start_time = time.time()\n+        search_prompt = \"\"\n+        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n+        context_result = self.context_builder.build_context(\n+            query=query,\n+            conversation_history=conversation_history,\n+            **kwargs,\n+            **self.context_builder_params,\n+        )\n+        llm_calls[\"build_context\"] = context_result.llm_calls\n+        prompt_tokens[\"build_context\"] = context_result.prompt_tokens\n+        output_tokens[\"build_context\"] = context_result.output_tokens\n+\n+        logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n+        try:\n+            if \"drift_query\" in kwargs:\n+                drift_query = kwargs[\"drift_query\"]\n+                search_prompt = self.system_prompt.format(\n+                    context_data=context_result.context_chunks,\n+                    response_type=self.response_type,\n+                    global_query=drift_query,\n+                )\n+            else:\n+                search_prompt = self.system_prompt.format(\n+                    context_data=context_result.context_chunks,\n+                    response_type=self.response_type,\n+                )\n+            history_messages = [\n+                {\"role\": \"system\", \"content\": search_prompt},\n+            ]\n+\n+            full_response = \"\"\n+\n+            async for response in self.model.achat_stream(\n+                prompt=query,\n+                history=history_messages,\n+                model_parameters=self.model_params,\n+            ):\n+                full_response += response\n+                for callback in self.callbacks:\n+                    callback.on_llm_new_token(response)\n+\n+            llm_calls[\"response\"] = 1\n+            prompt_tokens[\"response\"] = len(self.tokenizer.encode(search_prompt))\n+            output_tokens[\"response\"] = len(self.tokenizer.encode(full_response))\n+\n+            for callback in self.callbacks:\n+                callback.on_context(context_result.context_records)\n+\n+            return SearchResult(\n+                response=full_response,\n+                context_data=context_result.context_records,\n+                context_text=context_result.context_chunks,\n+                completion_time=time.time() - start_time,\n+                llm_calls=sum(llm_calls.values()),\n+                prompt_tokens=sum(prompt_tokens.values()),\n+                output_tokens=sum(output_tokens.values()),\n+                llm_calls_categories=llm_calls,\n+                prompt_tokens_categories=prompt_tokens,\n+                output_tokens_categories=output_tokens,\n+            )\n+\n+        except Exception:\n+            logger.exception(\"Exception in _asearch\")\n+            return SearchResult(\n+                response=\"\",\n+                context_data=context_result.context_records,\n+                context_text=context_result.context_chunks,\n+                completion_time=time.time() - start_time,\n+                llm_calls=1,\n+                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n+                output_tokens=0,\n+            )\n+\n+    async def stream_search(\n+        self,\n+        query: str,\n+        conversation_history: ConversationHistory | None = None,\n+    ) -> AsyncGenerator:\n+        \"\"\"Build local search context that fits a single context window and generate answer for the user query.\"\"\"\n+        start_time = time.time()\n+\n+        context_result = self.context_builder.build_context(\n+            query=query,\n+            conversation_history=conversation_history,\n+            **self.context_builder_params,\n+        )\n+        logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n+        search_prompt = self.system_prompt.format(\n+            context_data=context_result.context_chunks, response_type=self.response_type\n+        )\n+        history_messages = [\n+            {\"role\": \"system\", \"content\": search_prompt},\n+        ]\n+\n+        for callback in self.callbacks:\n+            callback.on_context(context_result.context_records)\n+\n+        async for response in self.model.achat_stream(\n+            prompt=query,\n+            history=history_messages,\n+            model_parameters=self.model_params,\n+        ):\n+            for callback in self.callbacks:\n+                callback.on_llm_new_token(response)\n+            yield response\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/__init__.py b/packages/graphrag/graphrag/storage/__init__.py\nnew file mode 100644\nindex 0000000..b21f077\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The storage package root.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/blob_pipeline_storage.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/blob_pipeline_storage.py b/packages/graphrag/graphrag/storage/blob_pipeline_storage.py\nnew file mode 100644\nindex 0000000..5a00af8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/blob_pipeline_storage.py\n@@ -0,0 +1,350 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Azure Blob Storage implementation of PipelineStorage.\"\"\"\n+\n+import logging\n+import re\n+from collections.abc import Iterator\n+from pathlib import Path\n+from typing import Any\n+\n+from azure.identity import DefaultAzureCredential\n+from azure.storage.blob import BlobServiceClient\n+\n+from graphrag.storage.pipeline_storage import (\n+    PipelineStorage,\n+    get_timestamp_formatted_with_local_tz,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BlobPipelineStorage(PipelineStorage):\n+    \"\"\"The Blob-Storage implementation.\"\"\"\n+\n+    _connection_string: str | None\n+    _container_name: str\n+    _path_prefix: str\n+    _encoding: str\n+    _storage_account_blob_url: str | None\n+\n+    def __init__(self, **kwargs: Any) -> None:\n+        \"\"\"Create a new BlobStorage instance.\"\"\"\n+        connection_string = kwargs.get(\"connection_string\")\n+        storage_account_blob_url = kwargs.get(\"storage_account_blob_url\")\n+        path_prefix = kwargs.get(\"base_dir\")\n+        container_name = kwargs[\"container_name\"]\n+        if container_name is None:\n+            msg = \"No container name provided for blob storage.\"\n+            raise ValueError(msg)\n+        if connection_string is None and storage_account_blob_url is None:\n+            msg = \"No storage account blob url provided for blob storage.\"\n+            raise ValueError(msg)\n+\n+        logger.info(\"Creating blob storage at %s\", container_name)\n+        if connection_string:\n+            self._blob_service_client = BlobServiceClient.from_connection_string(\n+                connection_string\n+            )\n+        else:\n+            if storage_account_blob_url is None:\n+                msg = \"Either connection_string or storage_account_blob_url must be provided.\"\n+                raise ValueError(msg)\n+\n+            self._blob_service_client = BlobServiceClient(\n+                account_url=storage_account_blob_url,\n+                credential=DefaultAzureCredential(),\n+            )\n+        self._encoding = kwargs.get(\"encoding\", \"utf-8\")\n+        self._container_name = container_name\n+        self._connection_string = connection_string\n+        self._path_prefix = path_prefix or \"\"\n+        self._storage_account_blob_url = storage_account_blob_url\n+        self._storage_account_name = (\n+            storage_account_blob_url.split(\"//\")[1].split(\".\")[0]\n+            if storage_account_blob_url\n+            else None\n+        )\n+        logger.debug(\n+            \"creating blob storage at container=%s, path=%s\",\n+            self._container_name,\n+            self._path_prefix,\n+        )\n+        self._create_container()\n+\n+    def _create_container(self) -> None:\n+        \"\"\"Create the container if it does not exist.\"\"\"\n+        if not self._container_exists():\n+            container_name = self._container_name\n+            container_names = [\n+                container.name\n+                for container in self._blob_service_client.list_containers()\n+            ]\n+            if container_name not in container_names:\n+                self._blob_service_client.create_container(container_name)\n+\n+    def _delete_container(self) -> None:\n+        \"\"\"Delete the container.\"\"\"\n+        if self._container_exists():\n+            self._blob_service_client.delete_container(self._container_name)\n+\n+    def _container_exists(self) -> bool:\n+        \"\"\"Check if the container exists.\"\"\"\n+        container_name = self._container_name\n+        container_names = [\n+            container.name for container in self._blob_service_client.list_containers()\n+        ]\n+        return container_name in container_names\n+\n+    def find(\n+        self,\n+        file_pattern: re.Pattern[str],\n+        base_dir: str | None = None,\n+        max_count=-1,\n+    ) -> Iterator[str]:\n+        \"\"\"Find blobs in a container using a file pattern.\n+\n+        Params:\n+            base_dir: The name of the base container.\n+            file_pattern: The file pattern to use.\n+            max_count: The maximum number of blobs to return. If -1, all blobs are returned.\n+\n+        Returns\n+        -------\n+                An iterator of blob names and their corresponding regex matches.\n+        \"\"\"\n+        base_dir = base_dir or \"\"\n+\n+        logger.info(\n+            \"search container %s for files matching %s\",\n+            self._container_name,\n+            file_pattern.pattern,\n+        )\n+\n+        def _blobname(blob_name: str) -> str:\n+            if blob_name.startswith(self._path_prefix):\n+                blob_name = blob_name.replace(self._path_prefix, \"\", 1)\n+            if blob_name.startswith(\"/\"):\n+                blob_name = blob_name[1:]\n+            return blob_name\n+\n+        try:\n+            container_client = self._blob_service_client.get_container_client(\n+                self._container_name\n+            )\n+            all_blobs = list(container_client.list_blobs())\n+\n+            num_loaded = 0\n+            num_total = len(list(all_blobs))\n+            num_filtered = 0\n+            for blob in all_blobs:\n+                match = file_pattern.search(blob.name)\n+                if match and blob.name.startswith(base_dir):\n+                    yield _blobname(blob.name)\n+                    num_loaded += 1\n+                    if max_count > 0 and num_loaded >= max_count:\n+                        break\n+                else:\n+                    num_filtered += 1\n+                logger.debug(\n+                    \"Blobs loaded: %d, filtered: %d, total: %d\",\n+                    num_loaded,\n+                    num_filtered,\n+                    num_total,\n+                )\n+        except Exception:  # noqa: BLE001\n+            logger.warning(\n+                \"Error finding blobs: base_dir=%s, file_pattern=%s\",\n+                base_dir,\n+                file_pattern,\n+            )\n+\n+    async def get(\n+        self, key: str, as_bytes: bool | None = False, encoding: str | None = None\n+    ) -> Any:\n+        \"\"\"Get a value from the cache.\"\"\"\n+        try:\n+            key = self._keyname(key)\n+            container_client = self._blob_service_client.get_container_client(\n+                self._container_name\n+            )\n+            blob_client = container_client.get_blob_client(key)\n+            blob_data = blob_client.download_blob().readall()\n+            if not as_bytes:\n+                coding = encoding or self._encoding\n+                blob_data = blob_data.decode(coding)\n+        except Exception:  # noqa: BLE001\n+            logger.warning(\"Error getting key %s\", key)\n+            return None\n+        else:\n+            return blob_data\n+\n+    async def set(self, key: str, value: Any, encoding: str | None = None) -> None:\n+        \"\"\"Set a value in the cache.\"\"\"\n+        try:\n+            key = self._keyname(key)\n+            container_client = self._blob_service_client.get_container_client(\n+                self._container_name\n+            )\n+            blob_client = container_client.get_blob_client(key)\n+            if isinstance(value, bytes):\n+                blob_client.upload_blob(value, overwrite=True)\n+            else:\n+                coding = encoding or self._encoding\n+                blob_client.upload_blob(value.encode(coding), overwrite=True)\n+        except Exception:\n+            logger.exception(\"Error setting key %s: %s\", key)\n+\n+    def _set_df_json(self, key: str, dataframe: Any) -> None:\n+        \"\"\"Set a json dataframe.\"\"\"\n+        if self._connection_string is None and self._storage_account_name:\n+            dataframe.to_json(\n+                self._abfs_url(key),\n+                storage_options={\n+                    \"account_name\": self._storage_account_name,\n+                    \"credential\": DefaultAzureCredential(),\n+                },\n+                orient=\"records\",\n+                lines=True,\n+                force_ascii=False,\n+            )\n+        else:\n+            dataframe.to_json(\n+                self._abfs_url(key),\n+                storage_options={\"connection_string\": self._connection_string},\n+                orient=\"records\",\n+                lines=True,\n+                force_ascii=False,\n+            )\n+\n+    def _set_df_parquet(self, key: str, dataframe: Any) -> None:\n+        \"\"\"Set a parquet dataframe.\"\"\"\n+        if self._connection_string is None and self._storage_account_name:\n+            dataframe.to_parquet(\n+                self._abfs_url(key),\n+                storage_options={\n+                    \"account_name\": self._storage_account_name,\n+                    \"credential\": DefaultAzureCredential(),\n+                },\n+            )\n+        else:\n+            dataframe.to_parquet(\n+                self._abfs_url(key),\n+                storage_options={\"connection_string\": self._connection_string},\n+            )\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Check if a key exists in the cache.\"\"\"\n+        key = self._keyname(key)\n+        container_client = self._blob_service_client.get_container_client(\n+            self._container_name\n+        )\n+        blob_client = container_client.get_blob_client(key)\n+        return blob_client.exists()\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete a key from the cache.\"\"\"\n+        key = self._keyname(key)\n+        container_client = self._blob_service_client.get_container_client(\n+            self._container_name\n+        )\n+        blob_client = container_client.get_blob_client(key)\n+        blob_client.delete_blob()\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear the cache.\"\"\"\n+\n+    def child(self, name: str | None) -> \"PipelineStorage\":\n+        \"\"\"Create a child storage instance.\"\"\"\n+        if name is None:\n+            return self\n+        path = str(Path(self._path_prefix) / name)\n+        return BlobPipelineStorage(\n+            connection_string=self._connection_string,\n+            container_name=self._container_name,\n+            encoding=self._encoding,\n+            base_dir=path,\n+            storage_account_blob_url=self._storage_account_blob_url,\n+        )\n+\n+    def keys(self) -> list[str]:\n+        \"\"\"Return the keys in the storage.\"\"\"\n+        msg = \"Blob storage does yet not support listing keys.\"\n+        raise NotImplementedError(msg)\n+\n+    def _keyname(self, key: str) -> str:\n+        \"\"\"Get the key name.\"\"\"\n+        return str(Path(self._path_prefix) / key)\n+\n+    def _abfs_url(self, key: str) -> str:\n+        \"\"\"Get the ABFS URL.\"\"\"\n+        path = str(Path(self._container_name) / self._path_prefix / key)\n+        return f\"abfs://{path}\"\n+\n+    async def get_creation_date(self, key: str) -> str:\n+        \"\"\"Get a value from the cache.\"\"\"\n+        try:\n+            key = self._keyname(key)\n+            container_client = self._blob_service_client.get_container_client(\n+                self._container_name\n+            )\n+            blob_client = container_client.get_blob_client(key)\n+            timestamp = blob_client.download_blob().properties.creation_time\n+            return get_timestamp_formatted_with_local_tz(timestamp)\n+        except Exception:  # noqa: BLE001\n+            logger.warning(\"Error getting key %s\", key)\n+            return \"\"\n+\n+\n+def validate_blob_container_name(container_name: str):\n+    \"\"\"\n+    Check if the provided blob container name is valid based on Azure rules.\n+\n+        - A blob container name must be between 3 and 63 characters in length.\n+        - Start with a letter or number\n+        - All letters used in blob container names must be lowercase.\n+        - Contain only letters, numbers, or the hyphen.\n+        - Consecutive hyphens are not permitted.\n+        - Cannot end with a hyphen.\n+\n+    Args:\n+    -----\n+    container_name (str)\n+        The blob container name to be validated.\n+\n+    Returns\n+    -------\n+        bool: True if valid, False otherwise.\n+    \"\"\"\n+    # Check the length of the name\n+    if len(container_name) < 3 or len(container_name) > 63:\n+        return ValueError(\n+            f\"Container name must be between 3 and 63 characters in length. Name provided was {len(container_name)} characters long.\"\n+        )\n+\n+    # Check if the name starts with a letter or number\n+    if not container_name[0].isalnum():\n+        return ValueError(\n+            f\"Container name must start with a letter or number. Starting character was {container_name[0]}.\"\n+        )\n+\n+    # Check for valid characters (letters, numbers, hyphen) and lowercase letters\n+    if not re.match(r\"^[a-z0-9-]+$\", container_name):\n+        return ValueError(\n+            f\"Container name must only contain:\\n- lowercase letters\\n- numbers\\n- or hyphens\\nName provided was {container_name}.\"\n+        )\n+\n+    # Check for consecutive hyphens\n+    if \"--\" in container_name:\n+        return ValueError(\n+            f\"Container name cannot contain consecutive hyphens. Name provided was {container_name}.\"\n+        )\n+\n+    # Check for hyphens at the end of the name\n+    if container_name[-1] == \"-\":\n+        return ValueError(\n+            f\"Container name cannot end with a hyphen. Name provided was {container_name}.\"\n+        )\n+\n+    return True\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/cosmosdb_pipeline_storage.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/cosmosdb_pipeline_storage.py b/packages/graphrag/graphrag/storage/cosmosdb_pipeline_storage.py\nnew file mode 100644\nindex 0000000..8d2673e\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/cosmosdb_pipeline_storage.py\n@@ -0,0 +1,346 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Azure CosmosDB Storage implementation of PipelineStorage.\"\"\"\n+\n+import json\n+import logging\n+import re\n+from collections.abc import Iterator\n+from datetime import datetime, timezone\n+from io import BytesIO, StringIO\n+from typing import Any\n+\n+import pandas as pd\n+from azure.cosmos import ContainerProxy, CosmosClient, DatabaseProxy\n+from azure.cosmos.exceptions import CosmosResourceNotFoundError\n+from azure.cosmos.partition_key import PartitionKey\n+from azure.identity import DefaultAzureCredential\n+\n+from graphrag.logger.progress import Progress\n+from graphrag.storage.pipeline_storage import (\n+    PipelineStorage,\n+    get_timestamp_formatted_with_local_tz,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class CosmosDBPipelineStorage(PipelineStorage):\n+    \"\"\"The CosmosDB-Storage Implementation.\"\"\"\n+\n+    _cosmos_client: CosmosClient\n+    _database_client: DatabaseProxy | None\n+    _container_client: ContainerProxy | None\n+    _cosmosdb_account_url: str | None\n+    _connection_string: str | None\n+    _database_name: str\n+    _container_name: str\n+    _encoding: str\n+    _no_id_prefixes: list[str]\n+\n+    def __init__(self, **kwargs: Any) -> None:\n+        \"\"\"Create a CosmosDB storage instance.\"\"\"\n+        logger.info(\"Creating cosmosdb storage\")\n+        cosmosdb_account_url = kwargs.get(\"cosmosdb_account_url\")\n+        connection_string = kwargs.get(\"connection_string\")\n+        database_name = kwargs[\"base_dir\"]\n+        container_name = kwargs[\"container_name\"]\n+        if not database_name:\n+            msg = \"No base_dir provided for database name\"\n+            raise ValueError(msg)\n+        if connection_string is None and cosmosdb_account_url is None:\n+            msg = \"connection_string or cosmosdb_account_url is required.\"\n+            raise ValueError(msg)\n+\n+        if connection_string:\n+            self._cosmos_client = CosmosClient.from_connection_string(connection_string)\n+        else:\n+            if cosmosdb_account_url is None:\n+                msg = (\n+                    \"Either connection_string or cosmosdb_account_url must be provided.\"\n+                )\n+                raise ValueError(msg)\n+            self._cosmos_client = CosmosClient(\n+                url=cosmosdb_account_url,\n+                credential=DefaultAzureCredential(),\n+            )\n+        self._encoding = kwargs.get(\"encoding\", \"utf-8\")\n+        self._database_name = database_name\n+        self._connection_string = connection_string\n+        self._cosmosdb_account_url = cosmosdb_account_url\n+        self._container_name = container_name\n+        self._cosmosdb_account_name = (\n+            cosmosdb_account_url.split(\"//\")[1].split(\".\")[0]\n+            if cosmosdb_account_url\n+            else None\n+        )\n+        self._no_id_prefixes = []\n+        logger.debug(\n+            \"creating cosmosdb storage with account: %s and database: %s and container: %s\",\n+            self._cosmosdb_account_name,\n+            self._database_name,\n+            self._container_name,\n+        )\n+        self._create_database()\n+        self._create_container()\n+\n+    def _create_database(self) -> None:\n+        \"\"\"Create the database if it doesn't exist.\"\"\"\n+        self._database_client = self._cosmos_client.create_database_if_not_exists(\n+            id=self._database_name\n+        )\n+\n+    def _delete_database(self) -> None:\n+        \"\"\"Delete the database if it exists.\"\"\"\n+        if self._database_client:\n+            self._database_client = self._cosmos_client.delete_database(\n+                self._database_client\n+            )\n+        self._container_client = None\n+\n+    def _create_container(self) -> None:\n+        \"\"\"Create a container for the current container name if it doesn't exist.\"\"\"\n+        partition_key = PartitionKey(path=\"/id\", kind=\"Hash\")\n+        if self._database_client:\n+            self._container_client = (\n+                self._database_client.create_container_if_not_exists(\n+                    id=self._container_name,\n+                    partition_key=partition_key,\n+                )\n+            )\n+\n+    def _delete_container(self) -> None:\n+        \"\"\"Delete the container with the current container name if it exists.\"\"\"\n+        if self._database_client and self._container_client:\n+            self._container_client = self._database_client.delete_container(\n+                self._container_client\n+            )\n+\n+    def find(\n+        self,\n+        file_pattern: re.Pattern[str],\n+        base_dir: str | None = None,\n+        max_count=-1,\n+    ) -> Iterator[str]:\n+        \"\"\"Find documents in a Cosmos DB container using a file pattern regex.\n+\n+        Params:\n+            base_dir: The name of the base directory (not used in Cosmos DB context).\n+            file_pattern: The file pattern to use.\n+            max_count: The maximum number of documents to return. If -1, all documents are returned.\n+\n+        Returns\n+        -------\n+            An iterator of document IDs and their corresponding regex matches.\n+        \"\"\"\n+        base_dir = base_dir or \"\"\n+        logger.info(\n+            \"search container %s for documents matching %s\",\n+            self._container_name,\n+            file_pattern.pattern,\n+        )\n+        if not self._database_client or not self._container_client:\n+            return\n+\n+        try:\n+            query = \"SELECT * FROM c WHERE RegexMatch(c.id, @pattern)\"\n+            parameters: list[dict[str, Any]] = [\n+                {\"name\": \"@pattern\", \"value\": file_pattern.pattern}\n+            ]\n+\n+            items = list(\n+                self._container_client.query_items(\n+                    query=query,\n+                    parameters=parameters,\n+                    enable_cross_partition_query=True,\n+                )\n+            )\n+            num_loaded = 0\n+            num_total = len(items)\n+            if num_total == 0:\n+                return\n+            num_filtered = 0\n+            for item in items:\n+                match = file_pattern.search(item[\"id\"])\n+                if match:\n+                    yield item[\"id\"]\n+                    num_loaded += 1\n+                    if max_count > 0 and num_loaded >= max_count:\n+                        break\n+                else:\n+                    num_filtered += 1\n+\n+                progress_status = _create_progress_status(\n+                    num_loaded, num_filtered, num_total\n+                )\n+                logger.debug(\n+                    \"Progress: %s (%d/%d completed)\",\n+                    progress_status.description,\n+                    progress_status.completed_items,\n+                    progress_status.total_items,\n+                )\n+        except Exception:  # noqa: BLE001\n+            logger.warning(\n+                \"An error occurred while searching for documents in Cosmos DB.\"\n+            )\n+\n+    async def get(\n+        self, key: str, as_bytes: bool | None = None, encoding: str | None = None\n+    ) -> Any:\n+        \"\"\"Fetch all items in a container that match the given key.\"\"\"\n+        try:\n+            if not self._database_client or not self._container_client:\n+                return None\n+            if as_bytes:\n+                prefix = self._get_prefix(key)\n+                query = f\"SELECT * FROM c WHERE STARTSWITH(c.id, '{prefix}')\"  # noqa: S608\n+                queried_items = self._container_client.query_items(\n+                    query=query, enable_cross_partition_query=True\n+                )\n+                items_list = list(queried_items)\n+                for item in items_list:\n+                    item[\"id\"] = item[\"id\"].split(\":\")[1]\n+\n+                items_json_str = json.dumps(items_list)\n+\n+                items_df = pd.read_json(\n+                    StringIO(items_json_str), orient=\"records\", lines=False\n+                )\n+\n+                # Drop the \"id\" column if the original dataframe does not include it\n+                # TODO: Figure out optimal way to handle missing id keys in input dataframes\n+                if prefix in self._no_id_prefixes:\n+                    items_df.drop(columns=[\"id\"], axis=1, inplace=True)\n+\n+                return items_df.to_parquet()\n+            item = self._container_client.read_item(item=key, partition_key=key)\n+            item_body = item.get(\"body\")\n+            return json.dumps(item_body)\n+        except Exception:  # noqa: BLE001\n+            logger.warning(\"Error reading item %s\", key)\n+            return None\n+\n+    async def set(self, key: str, value: Any, encoding: str | None = None) -> None:\n+        \"\"\"Insert the contents of a file into a cosmosdb container for the given filename key.\n+\n+        For better optimization, the file is destructured such that each row is a unique cosmosdb item.\n+        \"\"\"\n+        try:\n+            if not self._database_client or not self._container_client:\n+                msg = \"Database or container not initialized\"\n+                raise ValueError(msg)  # noqa: TRY301\n+            # value represents a parquet file\n+            if isinstance(value, bytes):\n+                prefix = self._get_prefix(key)\n+                value_df = pd.read_parquet(BytesIO(value))\n+                value_json = value_df.to_json(\n+                    orient=\"records\", lines=False, force_ascii=False\n+                )\n+                if value_json is None:\n+                    logger.error(\"Error converting output %s to json\", key)\n+                else:\n+                    cosmosdb_item_list = json.loads(value_json)\n+                    for index, cosmosdb_item in enumerate(cosmosdb_item_list):\n+                        # If the id key does not exist in the input dataframe json, create a unique id using the prefix and item index\n+                        # TODO: Figure out optimal way to handle missing id keys in input dataframes\n+                        if \"id\" not in cosmosdb_item:\n+                            prefixed_id = f\"{prefix}:{index}\"\n+                            self._no_id_prefixes.append(prefix)\n+                        else:\n+                            prefixed_id = f\"{prefix}:{cosmosdb_item['id']}\"\n+                        cosmosdb_item[\"id\"] = prefixed_id\n+                        self._container_client.upsert_item(body=cosmosdb_item)\n+            # value represents a cache output or stats.json\n+            else:\n+                cosmosdb_item = {\n+                    \"id\": key,\n+                    \"body\": json.loads(value),\n+                }\n+                self._container_client.upsert_item(body=cosmosdb_item)\n+        except Exception:\n+            logger.exception(\"Error writing item %s\", key)\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Check if the contents of the given filename key exist in the cosmosdb storage.\"\"\"\n+        if not self._database_client or not self._container_client:\n+            return False\n+        if \".parquet\" in key:\n+            prefix = self._get_prefix(key)\n+            query = f\"SELECT * FROM c WHERE STARTSWITH(c.id, '{prefix}')\"  # noqa: S608\n+            queried_items = self._container_client.query_items(\n+                query=query, enable_cross_partition_query=True\n+            )\n+            return len(list(queried_items)) > 0\n+        query = f\"SELECT * FROM c WHERE c.id = '{key}'\"  # noqa: S608\n+        queried_items = self._container_client.query_items(\n+            query=query, enable_cross_partition_query=True\n+        )\n+        return len(list(queried_items)) == 1\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete all cosmosdb items belonging to the given filename key.\"\"\"\n+        if not self._database_client or not self._container_client:\n+            return\n+        try:\n+            if \".parquet\" in key:\n+                prefix = self._get_prefix(key)\n+                query = f\"SELECT * FROM c WHERE STARTSWITH(c.id, '{prefix}')\"  # noqa: S608\n+                queried_items = self._container_client.query_items(\n+                    query=query, enable_cross_partition_query=True\n+                )\n+                for item in queried_items:\n+                    self._container_client.delete_item(\n+                        item=item[\"id\"], partition_key=item[\"id\"]\n+                    )\n+            else:\n+                self._container_client.delete_item(item=key, partition_key=key)\n+        except CosmosResourceNotFoundError:\n+            return\n+        except Exception:\n+            logger.exception(\"Error deleting item %s\", key)\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear all contents from storage.\n+\n+        # This currently deletes the database, including all containers and data within it.\n+        # TODO: We should decide what granularity of deletion is the ideal behavior (e.g. delete all items within a container, delete the current container, delete the current database)\n+        \"\"\"\n+        self._delete_database()\n+\n+    def keys(self) -> list[str]:\n+        \"\"\"Return the keys in the storage.\"\"\"\n+        msg = \"CosmosDB storage does yet not support listing keys.\"\n+        raise NotImplementedError(msg)\n+\n+    def child(self, name: str | None) -> PipelineStorage:\n+        \"\"\"Create a child storage instance.\"\"\"\n+        return self\n+\n+    def _get_prefix(self, key: str) -> str:\n+        \"\"\"Get the prefix of the filename key.\"\"\"\n+        return key.split(\".\")[0]\n+\n+    async def get_creation_date(self, key: str) -> str:\n+        \"\"\"Get a value from the cache.\"\"\"\n+        try:\n+            if not self._database_client or not self._container_client:\n+                return \"\"\n+            item = self._container_client.read_item(item=key, partition_key=key)\n+            return get_timestamp_formatted_with_local_tz(\n+                datetime.fromtimestamp(item[\"_ts\"], tz=timezone.utc)\n+            )\n+\n+        except Exception:  # noqa: BLE001\n+            logger.warning(\"Error getting key %s\", key)\n+            return \"\"\n+\n+\n+def _create_progress_status(\n+    num_loaded: int, num_filtered: int, num_total: int\n+) -> Progress:\n+    return Progress(\n+        total_items=num_total,\n+        completed_items=num_loaded + num_filtered,\n+        description=f\"{num_loaded} files loaded ({num_filtered} filtered)\",\n+    )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/factory.py b/packages/graphrag/graphrag/storage/factory.py\nnew file mode 100644\nindex 0000000..d2dfa66\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/factory.py\n@@ -0,0 +1,32 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Factory functions for creating storage.\"\"\"\n+\n+from __future__ import annotations\n+\n+from graphrag.config.enums import StorageType\n+from graphrag.factory.factory import Factory\n+from graphrag.storage.blob_pipeline_storage import BlobPipelineStorage\n+from graphrag.storage.cosmosdb_pipeline_storage import CosmosDBPipelineStorage\n+from graphrag.storage.file_pipeline_storage import FilePipelineStorage\n+from graphrag.storage.memory_pipeline_storage import MemoryPipelineStorage\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+\n+class StorageFactory(Factory[PipelineStorage]):\n+    \"\"\"A factory class for storage implementations.\n+\n+    Includes a method for users to register a custom storage implementation.\n+\n+    Configuration arguments are passed to each storage implementation as kwargs\n+    for individual enforcement of required/optional arguments.\n+    \"\"\"\n+\n+\n+# --- register built-in storage implementations ---\n+storage_factory = StorageFactory()\n+storage_factory.register(StorageType.blob.value, BlobPipelineStorage)\n+storage_factory.register(StorageType.cosmosdb.value, CosmosDBPipelineStorage)\n+storage_factory.register(StorageType.file.value, FilePipelineStorage)\n+storage_factory.register(StorageType.memory.value, MemoryPipelineStorage)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/file_pipeline_storage.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/file_pipeline_storage.py b/packages/graphrag/graphrag/storage/file_pipeline_storage.py\nnew file mode 100644\nindex 0000000..15445b0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/file_pipeline_storage.py\n@@ -0,0 +1,158 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"File-based Storage implementation of PipelineStorage.\"\"\"\n+\n+import logging\n+import os\n+import re\n+import shutil\n+from collections.abc import Iterator\n+from datetime import datetime, timezone\n+from pathlib import Path\n+from typing import Any, cast\n+\n+import aiofiles\n+from aiofiles.os import remove\n+from aiofiles.ospath import exists\n+\n+from graphrag.storage.pipeline_storage import (\n+    PipelineStorage,\n+    get_timestamp_formatted_with_local_tz,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class FilePipelineStorage(PipelineStorage):\n+    \"\"\"File storage class definition.\"\"\"\n+\n+    _root_dir: str\n+    _encoding: str\n+\n+    def __init__(self, **kwargs: Any) -> None:\n+        \"\"\"Create a file based storage.\"\"\"\n+        self._root_dir = kwargs.get(\"base_dir\", \"\")\n+        self._encoding = kwargs.get(\"encoding\", \"utf-8\")\n+        logger.info(\"Creating file storage at %s\", self._root_dir)\n+        Path(self._root_dir).mkdir(parents=True, exist_ok=True)\n+\n+    def find(\n+        self,\n+        file_pattern: re.Pattern[str],\n+        base_dir: str | None = None,\n+        max_count=-1,\n+    ) -> Iterator[str]:\n+        \"\"\"Find files in the storage using a file pattern.\"\"\"\n+        search_path = Path(self._root_dir) / (base_dir or \"\")\n+        logger.info(\n+            \"search %s for files matching %s\", search_path, file_pattern.pattern\n+        )\n+        all_files = list(search_path.rglob(\"**/*\"))\n+        num_loaded = 0\n+        num_total = len(all_files)\n+        num_filtered = 0\n+        for file in all_files:\n+            match = file_pattern.search(f\"{file}\")\n+            if match:\n+                filename = f\"{file}\".replace(self._root_dir, \"\")\n+                if filename.startswith(os.sep):\n+                    filename = filename[1:]\n+                yield filename\n+                num_loaded += 1\n+                if max_count > 0 and num_loaded >= max_count:\n+                    break\n+            else:\n+                num_filtered += 1\n+            logger.debug(\n+                \"Files loaded: %d, filtered: %d, total: %d\",\n+                num_loaded,\n+                num_filtered,\n+                num_total,\n+            )\n+\n+    async def get(\n+        self, key: str, as_bytes: bool | None = False, encoding: str | None = None\n+    ) -> Any:\n+        \"\"\"Get method definition.\"\"\"\n+        file_path = join_path(self._root_dir, key)\n+\n+        if await self.has(key):\n+            return await self._read_file(file_path, as_bytes, encoding)\n+        if await exists(key):\n+            # Lookup for key, as it is pressumably a new file loaded from inputs\n+            # and not yet written to storage\n+            return await self._read_file(key, as_bytes, encoding)\n+\n+        return None\n+\n+    async def _read_file(\n+        self,\n+        path: str | Path,\n+        as_bytes: bool | None = False,\n+        encoding: str | None = None,\n+    ) -> Any:\n+        \"\"\"Read the contents of a file.\"\"\"\n+        read_type = \"rb\" if as_bytes else \"r\"\n+        encoding = None if as_bytes else (encoding or self._encoding)\n+\n+        async with aiofiles.open(\n+            path,\n+            cast(\"Any\", read_type),\n+            encoding=encoding,\n+        ) as f:\n+            return await f.read()\n+\n+    async def set(self, key: str, value: Any, encoding: str | None = None) -> None:\n+        \"\"\"Set method definition.\"\"\"\n+        is_bytes = isinstance(value, bytes)\n+        write_type = \"wb\" if is_bytes else \"w\"\n+        encoding = None if is_bytes else encoding or self._encoding\n+        async with aiofiles.open(\n+            join_path(self._root_dir, key),\n+            cast(\"Any\", write_type),\n+            encoding=encoding,\n+        ) as f:\n+            await f.write(value)\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Has method definition.\"\"\"\n+        return await exists(join_path(self._root_dir, key))\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete method definition.\"\"\"\n+        if await self.has(key):\n+            await remove(join_path(self._root_dir, key))\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear method definition.\"\"\"\n+        for file in Path(self._root_dir).glob(\"*\"):\n+            if file.is_dir():\n+                shutil.rmtree(file)\n+            else:\n+                file.unlink()\n+\n+    def child(self, name: str | None) -> \"PipelineStorage\":\n+        \"\"\"Create a child storage instance.\"\"\"\n+        if name is None:\n+            return self\n+        child_path = str(Path(self._root_dir) / Path(name))\n+        return FilePipelineStorage(base_dir=child_path, encoding=self._encoding)\n+\n+    def keys(self) -> list[str]:\n+        \"\"\"Return the keys in the storage.\"\"\"\n+        return [item.name for item in Path(self._root_dir).iterdir() if item.is_file()]\n+\n+    async def get_creation_date(self, key: str) -> str:\n+        \"\"\"Get the creation date of a file.\"\"\"\n+        file_path = Path(join_path(self._root_dir, key))\n+\n+        creation_timestamp = file_path.stat().st_ctime\n+        creation_time_utc = datetime.fromtimestamp(creation_timestamp, tz=timezone.utc)\n+\n+        return get_timestamp_formatted_with_local_tz(creation_time_utc)\n+\n+\n+def join_path(file_path: str, file_name: str) -> Path:\n+    \"\"\"Join a path and a file. Independent of the OS.\"\"\"\n+    return Path(file_path) / Path(file_name).parent / Path(file_name).name\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/memory_pipeline_storage.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/memory_pipeline_storage.py b/packages/graphrag/graphrag/storage/memory_pipeline_storage.py\nnew file mode 100644\nindex 0000000..3567e3d\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/memory_pipeline_storage.py\n@@ -0,0 +1,78 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'InMemoryStorage' model.\"\"\"\n+\n+from typing import TYPE_CHECKING, Any\n+\n+from graphrag.storage.file_pipeline_storage import FilePipelineStorage\n+\n+if TYPE_CHECKING:\n+    from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+\n+class MemoryPipelineStorage(FilePipelineStorage):\n+    \"\"\"In memory storage class definition.\"\"\"\n+\n+    _storage: dict[str, Any]\n+\n+    def __init__(self):\n+        \"\"\"Init method definition.\"\"\"\n+        super().__init__()\n+        self._storage = {}\n+\n+    async def get(\n+        self, key: str, as_bytes: bool | None = None, encoding: str | None = None\n+    ) -> Any:\n+        \"\"\"Get the value for the given key.\n+\n+        Args:\n+            - key - The key to get the value for.\n+            - as_bytes - Whether or not to return the value as bytes.\n+\n+        Returns\n+        -------\n+            - output - The value for the given key.\n+        \"\"\"\n+        return self._storage.get(key)\n+\n+    async def set(self, key: str, value: Any, encoding: str | None = None) -> None:\n+        \"\"\"Set the value for the given key.\n+\n+        Args:\n+            - key - The key to set the value for.\n+            - value - The value to set.\n+        \"\"\"\n+        self._storage[key] = value\n+\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Return True if the given key exists in the storage.\n+\n+        Args:\n+            - key - The key to check for.\n+\n+        Returns\n+        -------\n+            - output - True if the key exists in the storage, False otherwise.\n+        \"\"\"\n+        return key in self._storage\n+\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete the given key from the storage.\n+\n+        Args:\n+            - key - The key to delete.\n+        \"\"\"\n+        del self._storage[key]\n+\n+    async def clear(self) -> None:\n+        \"\"\"Clear the storage.\"\"\"\n+        self._storage.clear()\n+\n+    def child(self, name: str | None) -> \"PipelineStorage\":\n+        \"\"\"Create a child storage instance.\"\"\"\n+        return MemoryPipelineStorage()\n+\n+    def keys(self) -> list[str]:\n+        \"\"\"Return the keys in the storage.\"\"\"\n+        return list(self._storage.keys())\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/storage/pipeline_storage.py",
            "diff": "diff --git a/packages/graphrag/graphrag/storage/pipeline_storage.py b/packages/graphrag/graphrag/storage/pipeline_storage.py\nnew file mode 100644\nindex 0000000..ba3ab86\n--- /dev/null\n+++ b/packages/graphrag/graphrag/storage/pipeline_storage.py\n@@ -0,0 +1,98 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A module containing 'PipelineStorage' model.\"\"\"\n+\n+import re\n+from abc import ABCMeta, abstractmethod\n+from collections.abc import Iterator\n+from datetime import datetime\n+from typing import Any\n+\n+\n+class PipelineStorage(metaclass=ABCMeta):\n+    \"\"\"Provide a storage interface for the pipeline. This is where the pipeline will store its output data.\"\"\"\n+\n+    @abstractmethod\n+    def find(\n+        self,\n+        file_pattern: re.Pattern[str],\n+        base_dir: str | None = None,\n+        max_count=-1,\n+    ) -> Iterator[str]:\n+        \"\"\"Find files in the storage using a file pattern.\"\"\"\n+\n+    @abstractmethod\n+    async def get(\n+        self, key: str, as_bytes: bool | None = None, encoding: str | None = None\n+    ) -> Any:\n+        \"\"\"Get the value for the given key.\n+\n+        Args:\n+            - key - The key to get the value for.\n+            - as_bytes - Whether or not to return the value as bytes.\n+\n+        Returns\n+        -------\n+            - output - The value for the given key.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def set(self, key: str, value: Any, encoding: str | None = None) -> None:\n+        \"\"\"Set the value for the given key.\n+\n+        Args:\n+            - key - The key to set the value for.\n+            - value - The value to set.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def has(self, key: str) -> bool:\n+        \"\"\"Return True if the given key exists in the storage.\n+\n+        Args:\n+            - key - The key to check for.\n+\n+        Returns\n+        -------\n+            - output - True if the key exists in the storage, False otherwise.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def delete(self, key: str) -> None:\n+        \"\"\"Delete the given key from the storage.\n+\n+        Args:\n+            - key - The key to delete.\n+        \"\"\"\n+\n+    @abstractmethod\n+    async def clear(self) -> None:\n+        \"\"\"Clear the storage.\"\"\"\n+\n+    @abstractmethod\n+    def child(self, name: str | None) -> \"PipelineStorage\":\n+        \"\"\"Create a child storage instance.\"\"\"\n+\n+    @abstractmethod\n+    def keys(self) -> list[str]:\n+        \"\"\"List all keys in the storage.\"\"\"\n+\n+    @abstractmethod\n+    async def get_creation_date(self, key: str) -> str:\n+        \"\"\"Get the creation date for the given key.\n+\n+        Args:\n+            - key - The key to get the creation date for.\n+\n+        Returns\n+        -------\n+            - output - The creation date for the given key.\n+        \"\"\"\n+\n+\n+def get_timestamp_formatted_with_local_tz(timestamp: datetime) -> str:\n+    \"\"\"Get the formatted timestamp with the local time zone.\"\"\"\n+    creation_time_local = timestamp.astimezone()\n+\n+    return creation_time_local.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/tokenizer/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/tokenizer/__init__.py b/packages/graphrag/graphrag/tokenizer/__init__.py\nnew file mode 100644\nindex 0000000..caa9cb8\n--- /dev/null\n+++ b/packages/graphrag/graphrag/tokenizer/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"GraphRAG tokenizer.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/tokenizer/get_tokenizer.py",
            "diff": "diff --git a/packages/graphrag/graphrag/tokenizer/get_tokenizer.py b/packages/graphrag/graphrag/tokenizer/get_tokenizer.py\nnew file mode 100644\nindex 0000000..5d1ef40\n--- /dev/null\n+++ b/packages/graphrag/graphrag/tokenizer/get_tokenizer.py\n@@ -0,0 +1,41 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Get Tokenizer.\"\"\"\n+\n+from graphrag.config.defaults import ENCODING_MODEL\n+from graphrag.config.models.language_model_config import LanguageModelConfig\n+from graphrag.tokenizer.litellm_tokenizer import LitellmTokenizer\n+from graphrag.tokenizer.tiktoken_tokenizer import TiktokenTokenizer\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+def get_tokenizer(\n+    model_config: LanguageModelConfig | None = None,\n+    encoding_model: str = ENCODING_MODEL,\n+) -> Tokenizer:\n+    \"\"\"\n+    Get the tokenizer for the given model configuration or fallback to a tiktoken based tokenizer.\n+\n+    Args\n+    ----\n+        model_config: LanguageModelConfig, optional\n+            The model configuration. If not provided or model_config.encoding_model is manually set,\n+            use a tiktoken based tokenizer. Otherwise, use a LitellmTokenizer based on the model name.\n+            LiteLLM supports token encoding/decoding for the range of models it supports.\n+        encoding_model: str, optional\n+            A tiktoken encoding model to use if no model configuration is provided. Only used if a\n+            model configuration is not provided.\n+\n+    Returns\n+    -------\n+        An instance of a Tokenizer.\n+    \"\"\"\n+    if model_config is not None:\n+        if model_config.encoding_model.strip() != \"\":\n+            # User has manually specified a tiktoken encoding model to use for the provided model configuration.\n+            return TiktokenTokenizer(encoding_name=model_config.encoding_model)\n+\n+        return LitellmTokenizer(model_name=model_config.model)\n+\n+    return TiktokenTokenizer(encoding_name=encoding_model)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/tokenizer/litellm_tokenizer.py",
            "diff": "diff --git a/packages/graphrag/graphrag/tokenizer/litellm_tokenizer.py b/packages/graphrag/graphrag/tokenizer/litellm_tokenizer.py\nnew file mode 100644\nindex 0000000..1a85f56\n--- /dev/null\n+++ b/packages/graphrag/graphrag/tokenizer/litellm_tokenizer.py\n@@ -0,0 +1,47 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"LiteLLM Tokenizer.\"\"\"\n+\n+from litellm import decode, encode  # type: ignore\n+\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+class LitellmTokenizer(Tokenizer):\n+    \"\"\"LiteLLM Tokenizer.\"\"\"\n+\n+    def __init__(self, model_name: str) -> None:\n+        \"\"\"Initialize the LiteLLM Tokenizer.\n+\n+        Args\n+        ----\n+            model_name (str): The name of the LiteLLM model to use for tokenization.\n+        \"\"\"\n+        self.model_name = model_name\n+\n+    def encode(self, text: str) -> list[int]:\n+        \"\"\"Encode the given text into a list of tokens.\n+\n+        Args\n+        ----\n+            text (str): The input text to encode.\n+\n+        Returns\n+        -------\n+            list[int]: A list of tokens representing the encoded text.\n+        \"\"\"\n+        return encode(model=self.model_name, text=text)\n+\n+    def decode(self, tokens: list[int]) -> str:\n+        \"\"\"Decode a list of tokens back into a string.\n+\n+        Args\n+        ----\n+            tokens (list[int]): A list of tokens to decode.\n+\n+        Returns\n+        -------\n+            str: The decoded string from the list of tokens.\n+        \"\"\"\n+        return decode(model=self.model_name, tokens=tokens)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/tokenizer/tiktoken_tokenizer.py",
            "diff": "diff --git a/packages/graphrag/graphrag/tokenizer/tiktoken_tokenizer.py b/packages/graphrag/graphrag/tokenizer/tiktoken_tokenizer.py\nnew file mode 100644\nindex 0000000..fa6c6e9\n--- /dev/null\n+++ b/packages/graphrag/graphrag/tokenizer/tiktoken_tokenizer.py\n@@ -0,0 +1,47 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Tiktoken Tokenizer.\"\"\"\n+\n+import tiktoken\n+\n+from graphrag.tokenizer.tokenizer import Tokenizer\n+\n+\n+class TiktokenTokenizer(Tokenizer):\n+    \"\"\"Tiktoken Tokenizer.\"\"\"\n+\n+    def __init__(self, encoding_name: str) -> None:\n+        \"\"\"Initialize the Tiktoken Tokenizer.\n+\n+        Args\n+        ----\n+            encoding_name (str): The name of the Tiktoken encoding to use for tokenization.\n+        \"\"\"\n+        self.encoding = tiktoken.get_encoding(encoding_name)\n+\n+    def encode(self, text: str) -> list[int]:\n+        \"\"\"Encode the given text into a list of tokens.\n+\n+        Args\n+        ----\n+            text (str): The input text to encode.\n+\n+        Returns\n+        -------\n+            list[int]: A list of tokens representing the encoded text.\n+        \"\"\"\n+        return self.encoding.encode(text)\n+\n+    def decode(self, tokens: list[int]) -> str:\n+        \"\"\"Decode a list of tokens back into a string.\n+\n+        Args\n+        ----\n+            tokens (list[int]): A list of tokens to decode.\n+\n+        Returns\n+        -------\n+            str: The decoded string from the list of tokens.\n+        \"\"\"\n+        return self.encoding.decode(tokens)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/tokenizer/tokenizer.py",
            "diff": "diff --git a/packages/graphrag/graphrag/tokenizer/tokenizer.py b/packages/graphrag/graphrag/tokenizer/tokenizer.py\nnew file mode 100644\nindex 0000000..32c0b2b\n--- /dev/null\n+++ b/packages/graphrag/graphrag/tokenizer/tokenizer.py\n@@ -0,0 +1,53 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Tokenizer Abstract Base Class.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+\n+\n+class Tokenizer(ABC):\n+    \"\"\"Tokenizer Abstract Base Class.\"\"\"\n+\n+    @abstractmethod\n+    def encode(self, text: str) -> list[int]:\n+        \"\"\"Encode the given text into a list of tokens.\n+\n+        Args\n+        ----\n+            text (str): The input text to encode.\n+\n+        Returns\n+        -------\n+            list[int]: A list of tokens representing the encoded text.\n+        \"\"\"\n+        msg = \"The encode method must be implemented by subclasses.\"\n+        raise NotImplementedError(msg)\n+\n+    @abstractmethod\n+    def decode(self, tokens: list[int]) -> str:\n+        \"\"\"Decode a list of tokens back into a string.\n+\n+        Args\n+        ----\n+            tokens (list[int]): A list of tokens to decode.\n+\n+        Returns\n+        -------\n+            str: The decoded string from the list of tokens.\n+        \"\"\"\n+        msg = \"The decode method must be implemented by subclasses.\"\n+        raise NotImplementedError(msg)\n+\n+    def num_tokens(self, text: str) -> int:\n+        \"\"\"Return the number of tokens in the given text.\n+\n+        Args\n+        ----\n+            text (str): The input text to analyze.\n+\n+        Returns\n+        -------\n+            int: The number of tokens in the input text.\n+        \"\"\"\n+        return len(self.encode(text))\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/utils/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/utils/__init__.py b/packages/graphrag/graphrag/utils/__init__.py\nnew file mode 100644\nindex 0000000..4296613\n--- /dev/null\n+++ b/packages/graphrag/graphrag/utils/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Util functions for the GraphRAG package.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/utils/api.py",
            "diff": "diff --git a/packages/graphrag/graphrag/utils/api.py b/packages/graphrag/graphrag/utils/api.py\nnew file mode 100644\nindex 0000000..db4a90a\n--- /dev/null\n+++ b/packages/graphrag/graphrag/utils/api.py\n@@ -0,0 +1,129 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"API functions for the GraphRAG module.\"\"\"\n+\n+from pathlib import Path\n+from typing import Any\n+\n+from graphrag.cache.factory import CacheFactory\n+from graphrag.cache.pipeline_cache import PipelineCache\n+from graphrag.config.embeddings import create_index_name\n+from graphrag.config.models.cache_config import CacheConfig\n+from graphrag.config.models.storage_config import StorageConfig\n+from graphrag.config.models.vector_store_schema_config import VectorStoreSchemaConfig\n+from graphrag.storage.factory import StorageFactory\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+from graphrag.vector_stores.base import (\n+    BaseVectorStore,\n+)\n+from graphrag.vector_stores.factory import VectorStoreFactory\n+\n+\n+def get_embedding_store(\n+    store: dict[str, Any],\n+    embedding_name: str,\n+) -> BaseVectorStore:\n+    \"\"\"Get the embedding description store.\"\"\"\n+    vector_store_type = store[\"type\"]\n+    index_name = create_index_name(\n+        store.get(\"container_name\", \"default\"), embedding_name\n+    )\n+\n+    embeddings_schema: dict[str, VectorStoreSchemaConfig] = store.get(\n+        \"embeddings_schema\", {}\n+    )\n+    embedding_config: VectorStoreSchemaConfig = VectorStoreSchemaConfig()\n+\n+    if (\n+        embeddings_schema is not None\n+        and embedding_name is not None\n+        and embedding_name in embeddings_schema\n+    ):\n+        raw_config = embeddings_schema[embedding_name]\n+        if isinstance(raw_config, dict):\n+            embedding_config = VectorStoreSchemaConfig(**raw_config)\n+        else:\n+            embedding_config = raw_config\n+\n+    if embedding_config.index_name is None:\n+        embedding_config.index_name = index_name\n+\n+    embedding_store = VectorStoreFactory().create(\n+        vector_store_type,\n+        {**store, \"vector_store_schema_config\": embedding_config},\n+    )\n+    embedding_store.connect(**store)\n+\n+    return embedding_store\n+\n+\n+def reformat_context_data(context_data: dict) -> dict:\n+    \"\"\"\n+    Reformats context_data for all query responses.\n+\n+    Reformats a dictionary of dataframes into a dictionary of lists.\n+    One list entry for each record. Records are grouped by original\n+    dictionary keys.\n+\n+    Note: depending on which query algorithm is used, the context_data may not\n+          contain the same information (keys). In this case, the default behavior will be to\n+          set these keys as empty lists to preserve a standard output format.\n+    \"\"\"\n+    final_format = {\n+        \"reports\": [],\n+        \"entities\": [],\n+        \"relationships\": [],\n+        \"claims\": [],\n+        \"sources\": [],\n+    }\n+    for key in context_data:\n+        records = (\n+            context_data[key].to_dict(orient=\"records\")\n+            if context_data[key] is not None and not isinstance(context_data[key], dict)\n+            else context_data[key]\n+        )\n+        if len(records) < 1:\n+            continue\n+        final_format[key] = records\n+    return final_format\n+\n+\n+def load_search_prompt(root_dir: str, prompt_config: str | None) -> str | None:\n+    \"\"\"\n+    Load the search prompt from disk if configured.\n+\n+    If not, leave it empty - the search functions will load their defaults.\n+\n+    \"\"\"\n+    if prompt_config:\n+        prompt_file = Path(root_dir) / prompt_config\n+        if prompt_file.exists():\n+            return prompt_file.read_bytes().decode(encoding=\"utf-8\")\n+    return None\n+\n+\n+def create_storage_from_config(output: StorageConfig) -> PipelineStorage:\n+    \"\"\"Create a storage object from the config.\"\"\"\n+    storage_config = output.model_dump()\n+    return StorageFactory().create(\n+        storage_config[\"type\"],\n+        storage_config,\n+    )\n+\n+\n+def create_cache_from_config(cache: CacheConfig, root_dir: str) -> PipelineCache:\n+    \"\"\"Create a cache object from the config.\"\"\"\n+    cache_config = cache.model_dump()\n+    args = {**cache_config, \"root_dir\": root_dir}\n+    return CacheFactory().create(\n+        strategy=cache_config[\"type\"],\n+        init_args=args,\n+    )\n+\n+\n+def truncate(text: str, max_length: int) -> str:\n+    \"\"\"Truncate a string to a maximum length.\"\"\"\n+    if len(text) <= max_length:\n+        return text\n+    return text[:max_length] + \"...[truncated]\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/utils/cli.py",
            "diff": "diff --git a/packages/graphrag/graphrag/utils/cli.py b/packages/graphrag/graphrag/utils/cli.py\nnew file mode 100644\nindex 0000000..5a9c166\n--- /dev/null\n+++ b/packages/graphrag/graphrag/utils/cli.py\n@@ -0,0 +1,54 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"CLI functions for the GraphRAG module.\"\"\"\n+\n+import argparse\n+import json\n+from pathlib import Path\n+\n+\n+def file_exist(path):\n+    \"\"\"Check for file existence.\"\"\"\n+    if not Path(path).is_file():\n+        msg = f\"File not found: {path}\"\n+        raise argparse.ArgumentTypeError(msg)\n+    return path\n+\n+\n+def dir_exist(path):\n+    \"\"\"Check for directory existence.\"\"\"\n+    if not Path(path).is_dir():\n+        msg = f\"Directory not found: {path}\"\n+        raise argparse.ArgumentTypeError(msg)\n+    return path\n+\n+\n+def redact(config: dict) -> str:\n+    \"\"\"Sanitize secrets in a config object.\"\"\"\n+\n+    # Redact any sensitive configuration\n+    def redact_dict(config: dict) -> dict:\n+        if not isinstance(config, dict):\n+            return config\n+\n+        result = {}\n+        for key, value in config.items():\n+            if key in {\n+                \"api_key\",\n+                \"connection_string\",\n+                \"container_name\",\n+                \"organization\",\n+            }:\n+                if value is not None:\n+                    result[key] = \"==== REDACTED ====\"\n+            elif isinstance(value, dict):\n+                result[key] = redact_dict(value)\n+            elif isinstance(value, list):\n+                result[key] = [redact_dict(i) for i in value]\n+            else:\n+                result[key] = value\n+        return result\n+\n+    redacted_dict = redact_dict(config)\n+    return json.dumps(redacted_dict, indent=4)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/utils/storage.py",
            "diff": "diff --git a/packages/graphrag/graphrag/utils/storage.py b/packages/graphrag/graphrag/utils/storage.py\nnew file mode 100644\nindex 0000000..8534330\n--- /dev/null\n+++ b/packages/graphrag/graphrag/utils/storage.py\n@@ -0,0 +1,44 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Storage functions for the GraphRAG run module.\"\"\"\n+\n+import logging\n+from io import BytesIO\n+\n+import pandas as pd\n+\n+from graphrag.storage.pipeline_storage import PipelineStorage\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def load_table_from_storage(name: str, storage: PipelineStorage) -> pd.DataFrame:\n+    \"\"\"Load a parquet from the storage instance.\"\"\"\n+    filename = f\"{name}.parquet\"\n+    if not await storage.has(filename):\n+        msg = f\"Could not find {filename} in storage!\"\n+        raise ValueError(msg)\n+    try:\n+        logger.info(\"reading table from storage: %s\", filename)\n+        return pd.read_parquet(BytesIO(await storage.get(filename, as_bytes=True)))\n+    except Exception:\n+        logger.exception(\"error loading table from storage: %s\", filename)\n+        raise\n+\n+\n+async def write_table_to_storage(\n+    table: pd.DataFrame, name: str, storage: PipelineStorage\n+) -> None:\n+    \"\"\"Write a table to storage.\"\"\"\n+    await storage.set(f\"{name}.parquet\", table.to_parquet())\n+\n+\n+async def delete_table_from_storage(name: str, storage: PipelineStorage) -> None:\n+    \"\"\"Delete a table to storage.\"\"\"\n+    await storage.delete(f\"{name}.parquet\")\n+\n+\n+async def storage_has_table(name: str, storage: PipelineStorage) -> bool:\n+    \"\"\"Check if a table exists in storage.\"\"\"\n+    return await storage.has(f\"{name}.parquet\")\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/vector_stores/__init__.py",
            "diff": "diff --git a/packages/graphrag/graphrag/vector_stores/__init__.py b/packages/graphrag/graphrag/vector_stores/__init__.py\nnew file mode 100644\nindex 0000000..4f137d0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/vector_stores/__init__.py\n@@ -0,0 +1,4 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing vector store implementations.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/vector_stores/azure_ai_search.py",
            "diff": "diff --git a/packages/graphrag/graphrag/vector_stores/azure_ai_search.py b/packages/graphrag/graphrag/vector_stores/azure_ai_search.py\nnew file mode 100644\nindex 0000000..e6193e0\n--- /dev/null\n+++ b/packages/graphrag/graphrag/vector_stores/azure_ai_search.py\n@@ -0,0 +1,174 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the Azure AI Search  vector store implementation.\"\"\"\n+\n+from typing import Any\n+\n+from azure.core.credentials import AzureKeyCredential\n+from azure.identity import DefaultAzureCredential\n+from azure.search.documents import SearchClient\n+from azure.search.documents.indexes import SearchIndexClient\n+from azure.search.documents.indexes.models import (\n+    HnswAlgorithmConfiguration,\n+    HnswParameters,\n+    SearchField,\n+    SearchFieldDataType,\n+    SearchIndex,\n+    SimpleField,\n+    VectorSearch,\n+    VectorSearchAlgorithmMetric,\n+    VectorSearchProfile,\n+)\n+from azure.search.documents.models import VectorizedQuery\n+\n+from graphrag.data_model.types import TextEmbedder\n+from graphrag.vector_stores.base import (\n+    BaseVectorStore,\n+    VectorStoreDocument,\n+    VectorStoreSearchResult,\n+)\n+\n+\n+class AzureAISearchVectorStore(BaseVectorStore):\n+    \"\"\"Azure AI Search vector storage implementation.\"\"\"\n+\n+    index_client: SearchIndexClient\n+\n+    def connect(self, **kwargs: Any) -> Any:\n+        \"\"\"Connect to AI search vector storage.\"\"\"\n+        url = kwargs[\"url\"]\n+        api_key = kwargs.get(\"api_key\")\n+        audience = kwargs.get(\"audience\")\n+\n+        self.vector_search_profile_name = kwargs.get(\n+            \"vector_search_profile_name\", \"vectorSearchProfile\"\n+        )\n+\n+        if url:\n+            audience_arg = {\"audience\": audience} if audience and not api_key else {}\n+            self.db_connection = SearchClient(\n+                endpoint=url,\n+                index_name=self.index_name if self.index_name else \"\",\n+                credential=(\n+                    AzureKeyCredential(api_key) if api_key else DefaultAzureCredential()\n+                ),\n+                **audience_arg,\n+            )\n+            self.index_client = SearchIndexClient(\n+                endpoint=url,\n+                credential=(\n+                    AzureKeyCredential(api_key) if api_key else DefaultAzureCredential()\n+                ),\n+                **audience_arg,\n+            )\n+        else:\n+            not_supported_error = \"Azure AI Search expects `url`.\"\n+            raise ValueError(not_supported_error)\n+\n+    def create_index(self) -> None:\n+        \"\"\"Load documents into an Azure AI Search index.\"\"\"\n+        if (\n+            self.index_name is not None\n+            and self.index_name in self.index_client.list_index_names()\n+        ):\n+            self.index_client.delete_index(self.index_name)\n+\n+        # Configure vector search profile\n+        vector_search = VectorSearch(\n+            algorithms=[\n+                HnswAlgorithmConfiguration(\n+                    name=\"HnswAlg\",\n+                    parameters=HnswParameters(\n+                        metric=VectorSearchAlgorithmMetric.COSINE\n+                    ),\n+                )\n+            ],\n+            profiles=[\n+                VectorSearchProfile(\n+                    name=self.vector_search_profile_name,\n+                    algorithm_configuration_name=\"HnswAlg\",\n+                )\n+            ],\n+        )\n+        # Configure the index\n+        index = SearchIndex(\n+            name=self.index_name if self.index_name else \"\",\n+            fields=[\n+                SimpleField(\n+                    name=self.id_field,\n+                    type=SearchFieldDataType.String,\n+                    key=True,\n+                ),\n+                SearchField(\n+                    name=self.vector_field,\n+                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n+                    searchable=True,\n+                    hidden=False,  # DRIFT needs to return the vector for client-side similarity\n+                    vector_search_dimensions=self.vector_size,\n+                    vector_search_profile_name=self.vector_search_profile_name,\n+                ),\n+            ],\n+            vector_search=vector_search,\n+        )\n+        self.index_client.create_or_update_index(\n+            index,\n+        )\n+\n+    def load_documents(self, documents: list[VectorStoreDocument]) -> None:\n+        \"\"\"Load documents into an Azure AI Search index.\"\"\"\n+        batch = [\n+            {\n+                self.id_field: doc.id,\n+                self.vector_field: doc.vector,\n+            }\n+            for doc in documents\n+            if doc.vector is not None\n+        ]\n+\n+        if len(batch) > 0:\n+            self.db_connection.upload_documents(batch)\n+\n+    def similarity_search_by_vector(\n+        self, query_embedding: list[float], k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform a vector-based similarity search.\"\"\"\n+        vectorized_query = VectorizedQuery(\n+            vector=query_embedding, k_nearest_neighbors=k, fields=self.vector_field\n+        )\n+\n+        response = self.db_connection.search(\n+            vector_queries=[vectorized_query],\n+        )\n+\n+        return [\n+            VectorStoreSearchResult(\n+                document=VectorStoreDocument(\n+                    id=doc.get(self.id_field, \"\"),\n+                    vector=doc.get(self.vector_field, []),\n+                ),\n+                # Cosine similarity between 0.333 and 1.000\n+                # https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking#scores-in-a-hybrid-search-results\n+                score=doc[\"@search.score\"],\n+            )\n+            for doc in response\n+        ]\n+\n+    def similarity_search_by_text(\n+        self, text: str, text_embedder: TextEmbedder, k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform a text-based similarity search.\"\"\"\n+        query_embedding = text_embedder(text)\n+        if query_embedding:\n+            return self.similarity_search_by_vector(\n+                query_embedding=query_embedding, k=k\n+            )\n+        return []\n+\n+    def search_by_id(self, id: str) -> VectorStoreDocument:\n+        \"\"\"Search for a document by id.\"\"\"\n+        response = self.db_connection.get_document(id)\n+        return VectorStoreDocument(\n+            id=response.get(self.id_field, \"\"),\n+            vector=response.get(self.vector_field, []),\n+        )\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/vector_stores/base.py",
            "diff": "diff --git a/packages/graphrag/graphrag/vector_stores/base.py b/packages/graphrag/graphrag/vector_stores/base.py\nnew file mode 100644\nindex 0000000..eda7e18\n--- /dev/null\n+++ b/packages/graphrag/graphrag/vector_stores/base.py\n@@ -0,0 +1,82 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Base classes for vector stores.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+from typing import Any\n+\n+from graphrag.config.models.vector_store_schema_config import VectorStoreSchemaConfig\n+from graphrag.data_model.types import TextEmbedder\n+\n+\n+@dataclass\n+class VectorStoreDocument:\n+    \"\"\"A document that is stored in vector storage.\"\"\"\n+\n+    id: str | int\n+    \"\"\"unique id for the document\"\"\"\n+\n+    vector: list[float] | None\n+\n+\n+@dataclass\n+class VectorStoreSearchResult:\n+    \"\"\"A vector storage search result.\"\"\"\n+\n+    document: VectorStoreDocument\n+    \"\"\"Document that was found.\"\"\"\n+\n+    score: float\n+    \"\"\"Similarity score between -1 and 1. Higher is more similar.\"\"\"\n+\n+\n+class BaseVectorStore(ABC):\n+    \"\"\"The base class for vector storage data-access classes.\"\"\"\n+\n+    def __init__(\n+        self,\n+        vector_store_schema_config: VectorStoreSchemaConfig,\n+        db_connection: Any | None = None,\n+        document_collection: Any | None = None,\n+        query_filter: Any | None = None,\n+        **kwargs: Any,\n+    ):\n+        self.db_connection = db_connection\n+        self.document_collection = document_collection\n+        self.query_filter = query_filter\n+        self.kwargs = kwargs\n+\n+        self.index_name = vector_store_schema_config.index_name\n+        self.id_field = vector_store_schema_config.id_field\n+        self.vector_field = vector_store_schema_config.vector_field\n+        self.vector_size = vector_store_schema_config.vector_size\n+\n+    @abstractmethod\n+    def connect(self, **kwargs: Any) -> None:\n+        \"\"\"Connect to vector storage.\"\"\"\n+\n+    @abstractmethod\n+    def create_index(self) -> None:\n+        \"\"\"Create index.\"\"\"\n+\n+    @abstractmethod\n+    def load_documents(self, documents: list[VectorStoreDocument]) -> None:\n+        \"\"\"Load documents into the vector-store.\"\"\"\n+\n+    @abstractmethod\n+    def similarity_search_by_vector(\n+        self, query_embedding: list[float], k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform ANN search by vector.\"\"\"\n+\n+    @abstractmethod\n+    def similarity_search_by_text(\n+        self, text: str, text_embedder: TextEmbedder, k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform ANN search by text.\"\"\"\n+\n+    @abstractmethod\n+    def search_by_id(self, id: str) -> VectorStoreDocument:\n+        \"\"\"Search for a document by id.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/vector_stores/cosmosdb.py",
            "diff": "diff --git a/packages/graphrag/graphrag/vector_stores/cosmosdb.py b/packages/graphrag/graphrag/vector_stores/cosmosdb.py\nnew file mode 100644\nindex 0000000..23b2c8f\n--- /dev/null\n+++ b/packages/graphrag/graphrag/vector_stores/cosmosdb.py\n@@ -0,0 +1,255 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"A package containing the CosmosDB vector store implementation.\"\"\"\n+\n+from typing import Any\n+\n+from azure.cosmos import ContainerProxy, CosmosClient, DatabaseProxy\n+from azure.cosmos.exceptions import CosmosHttpResponseError\n+from azure.cosmos.partition_key import PartitionKey\n+from azure.identity import DefaultAzureCredential\n+\n+from graphrag.data_model.types import TextEmbedder\n+from graphrag.vector_stores.base import (\n+    BaseVectorStore,\n+    VectorStoreDocument,\n+    VectorStoreSearchResult,\n+)\n+\n+\n+class CosmosDBVectorStore(BaseVectorStore):\n+    \"\"\"Azure CosmosDB vector storage implementation.\"\"\"\n+\n+    _cosmos_client: CosmosClient\n+    _database_client: DatabaseProxy\n+    _container_client: ContainerProxy\n+\n+    def connect(self, **kwargs: Any) -> Any:\n+        \"\"\"Connect to CosmosDB vector storage.\"\"\"\n+        connection_string = kwargs.get(\"connection_string\")\n+        if connection_string:\n+            self._cosmos_client = CosmosClient.from_connection_string(connection_string)\n+        else:\n+            url = kwargs.get(\"url\")\n+            if not url:\n+                msg = \"Either connection_string or url must be provided.\"\n+                raise ValueError(msg)\n+            self._cosmos_client = CosmosClient(\n+                url=url, credential=DefaultAzureCredential()\n+            )\n+\n+        database_name = kwargs.get(\"database_name\")\n+        if database_name is None:\n+            msg = \"Database name must be provided.\"\n+            raise ValueError(msg)\n+        self._database_name = database_name\n+        if self.index_name is None:\n+            msg = \"Index name is empty or not provided.\"\n+            raise ValueError(msg)\n+        self._container_name = self.index_name\n+\n+        self.vector_size = self.vector_size\n+        self._create_database()\n+        self._create_container()\n+\n+    def _create_database(self) -> None:\n+        \"\"\"Create the database if it doesn't exist.\"\"\"\n+        self._cosmos_client.create_database_if_not_exists(id=self._database_name)\n+        self._database_client = self._cosmos_client.get_database_client(\n+            self._database_name\n+        )\n+\n+    def _delete_database(self) -> None:\n+        \"\"\"Delete the database if it exists.\"\"\"\n+        if self._database_exists():\n+            self._cosmos_client.delete_database(self._database_name)\n+\n+    def _database_exists(self) -> bool:\n+        \"\"\"Check if the database exists.\"\"\"\n+        existing_database_names = [\n+            database[\"id\"] for database in self._cosmos_client.list_databases()\n+        ]\n+        return self._database_name in existing_database_names\n+\n+    def _create_container(self) -> None:\n+        \"\"\"Create the container if it doesn't exist.\"\"\"\n+        partition_key = PartitionKey(path=f\"/{self.id_field}\", kind=\"Hash\")\n+\n+        # Define the container vector policy\n+        vector_embedding_policy = {\n+            \"vectorEmbeddings\": [\n+                {\n+                    \"path\": f\"/{self.vector_field}\",\n+                    \"dataType\": \"float32\",\n+                    \"distanceFunction\": \"cosine\",\n+                    \"dimensions\": self.vector_size,\n+                }\n+            ]\n+        }\n+\n+        # Define the vector indexing policy\n+        indexing_policy = {\n+            \"indexingMode\": \"consistent\",\n+            \"automatic\": True,\n+            \"includedPaths\": [{\"path\": \"/*\"}],\n+            \"excludedPaths\": [\n+                {\"path\": \"/_etag/?\"},\n+                {\"path\": f\"/{self.vector_field}/*\"},\n+            ],\n+        }\n+\n+        # Currently, the CosmosDB emulator does not support the diskANN policy.\n+        try:\n+            # First try with the standard diskANN policy\n+            indexing_policy[\"vectorIndexes\"] = [\n+                {\"path\": f\"/{self.vector_field}\", \"type\": \"diskANN\"}\n+            ]\n+\n+            # Create the container and container client\n+            self._database_client.create_container_if_not_exists(\n+                id=self._container_name,\n+                partition_key=partition_key,\n+                indexing_policy=indexing_policy,\n+                vector_embedding_policy=vector_embedding_policy,\n+            )\n+        except CosmosHttpResponseError:\n+            # If diskANN fails (likely in emulator), retry without vector indexes\n+            indexing_policy.pop(\"vectorIndexes\", None)\n+\n+            # Create the container with compatible indexing policy\n+            self._database_client.create_container_if_not_exists(\n+                id=self._container_name,\n+                partition_key=partition_key,\n+                indexing_policy=indexing_policy,\n+                vector_embedding_policy=vector_embedding_policy,\n+            )\n+\n+        self._container_client = self._database_client.get_container_client(\n+            self._container_name\n+        )\n+\n+    def _delete_container(self) -> None:\n+        \"\"\"Delete the vector store container in the database if it exists.\"\"\"\n+        if self._container_exists():\n+            self._database_client.delete_container(self._container_name)\n+\n+    def _container_exists(self) -> bool:\n+        \"\"\"Check if the container name exists in the database.\"\"\"\n+        existing_container_names = [\n+            container[\"id\"] for container in self._database_client.list_containers()\n+        ]\n+        return self._container_name in existing_container_names\n+\n+    def create_index(self) -> None:\n+        \"\"\"Load documents into CosmosDB.\"\"\"\n+        # Create a CosmosDB container on overwrite\n+        self._delete_container()\n+        self._create_container()\n+\n+        if self._container_client is None:\n+            msg = \"Container client is not initialized.\"\n+            raise ValueError(msg)\n+\n+    def load_documents(self, documents: list[VectorStoreDocument]) -> None:\n+        \"\"\"Load documents into CosmosDB.\"\"\"\n+        # Upload documents to CosmosDB\n+        for doc in documents:\n+            if doc.vector is not None:\n+                print(\"Document to store:\")  # noqa: T201\n+                print(doc)  # noqa: T201\n+                doc_json = {\n+                    self.id_field: doc.id,\n+                    self.vector_field: doc.vector,\n+                }\n+                print(\"Storing document in CosmosDB:\")  # noqa: T201\n+                print(doc_json)  # noqa: T201\n+                self._container_client.upsert_item(doc_json)\n+\n+    def similarity_search_by_vector(\n+        self, query_embedding: list[float], k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform a vector-based similarity search.\"\"\"\n+        if self._container_client is None:\n+            msg = \"Container client is not initialized.\"\n+            raise ValueError(msg)\n+\n+        try:\n+            query = f\"SELECT TOP {k} c.{self.id_field}, c.{self.vector_field}, VectorDistance(c.{self.vector_field}, @embedding) AS SimilarityScore FROM c ORDER BY VectorDistance(c.{self.vector_field}, @embedding)\"  # noqa: S608\n+            query_params = [{\"name\": \"@embedding\", \"value\": query_embedding}]\n+            items = list(\n+                self._container_client.query_items(\n+                    query=query,\n+                    parameters=query_params,\n+                    enable_cross_partition_query=True,\n+                )\n+            )\n+        except (CosmosHttpResponseError, ValueError):\n+            # Currently, the CosmosDB emulator does not support the VectorDistance function.\n+            # For emulator or test environments - fetch all items and calculate distance locally\n+            query = f\"SELECT c.{self.id_field}, c.{self.vector_field} FROM c\"  # noqa: S608\n+            items = list(\n+                self._container_client.query_items(\n+                    query=query,\n+                    enable_cross_partition_query=True,\n+                )\n+            )\n+\n+            # Calculate cosine similarity locally (1 - cosine distance)\n+            from numpy import dot\n+            from numpy.linalg import norm\n+\n+            def cosine_similarity(a, b):\n+                if norm(a) * norm(b) == 0:\n+                    return 0.0\n+                return dot(a, b) / (norm(a) * norm(b))\n+\n+            # Calculate scores for all items\n+            for item in items:\n+                item_vector = item.get(self.vector_field, [])\n+                similarity = cosine_similarity(query_embedding, item_vector)\n+                item[\"SimilarityScore\"] = similarity\n+\n+            # Sort by similarity score (higher is better) and take top k\n+            items = sorted(\n+                items, key=lambda x: x.get(\"SimilarityScore\", 0.0), reverse=True\n+            )[:k]\n+\n+        return [\n+            VectorStoreSearchResult(\n+                document=VectorStoreDocument(\n+                    id=item.get(self.id_field, \"\"),\n+                    vector=item.get(self.vector_field, []),\n+                ),\n+                score=item.get(\"SimilarityScore\", 0.0),\n+            )\n+            for item in items\n+        ]\n+\n+    def similarity_search_by_text(\n+        self, text: str, text_embedder: TextEmbedder, k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform a text-based similarity search.\"\"\"\n+        query_embedding = text_embedder(text)\n+        if query_embedding:\n+            return self.similarity_search_by_vector(\n+                query_embedding=query_embedding, k=k\n+            )\n+        return []\n+\n+    def search_by_id(self, id: str) -> VectorStoreDocument:\n+        \"\"\"Search for a document by id.\"\"\"\n+        if self._container_client is None:\n+            msg = \"Container client is not initialized.\"\n+            raise ValueError(msg)\n+\n+        item = self._container_client.read_item(item=id, partition_key=id)\n+        return VectorStoreDocument(\n+            id=item.get(self.id_field, \"\"),\n+            vector=item.get(self.vector_field, []),\n+        )\n+\n+    def clear(self) -> None:\n+        \"\"\"Clear the vector store.\"\"\"\n+        self._delete_container()\n+        self._delete_database()\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/vector_stores/factory.py",
            "diff": "diff --git a/packages/graphrag/graphrag/vector_stores/factory.py b/packages/graphrag/graphrag/vector_stores/factory.py\nnew file mode 100644\nindex 0000000..90b004c\n--- /dev/null\n+++ b/packages/graphrag/graphrag/vector_stores/factory.py\n@@ -0,0 +1,32 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Factory functions for creating a vector store.\"\"\"\n+\n+from __future__ import annotations\n+\n+from graphrag.config.enums import VectorStoreType\n+from graphrag.factory.factory import Factory\n+from graphrag.vector_stores.azure_ai_search import AzureAISearchVectorStore\n+from graphrag.vector_stores.base import BaseVectorStore\n+from graphrag.vector_stores.cosmosdb import CosmosDBVectorStore\n+from graphrag.vector_stores.lancedb import LanceDBVectorStore\n+\n+\n+class VectorStoreFactory(Factory[BaseVectorStore]):\n+    \"\"\"A factory for vector stores.\n+\n+    Includes a method for users to register a custom vector store implementation.\n+\n+    Configuration arguments are passed to each vector store implementation as kwargs\n+    for individual enforcement of required/optional arguments.\n+    \"\"\"\n+\n+\n+# --- register built-in vector store implementations ---\n+vector_store_factory = VectorStoreFactory()\n+vector_store_factory.register(VectorStoreType.LanceDB.value, LanceDBVectorStore)\n+vector_store_factory.register(\n+    VectorStoreType.AzureAISearch.value, AzureAISearchVectorStore\n+)\n+vector_store_factory.register(VectorStoreType.CosmosDB.value, CosmosDBVectorStore)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "packages/graphrag/graphrag/vector_stores/lancedb.py",
            "diff": "diff --git a/packages/graphrag/graphrag/vector_stores/lancedb.py b/packages/graphrag/graphrag/vector_stores/lancedb.py\nnew file mode 100644\nindex 0000000..2b589d5\n--- /dev/null\n+++ b/packages/graphrag/graphrag/vector_stores/lancedb.py\n@@ -0,0 +1,149 @@\n+# Copyright (c) 2024 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"The LanceDB vector storage implementation package.\"\"\"\n+\n+from typing import Any\n+\n+import lancedb\n+import numpy as np\n+import pyarrow as pa\n+\n+from graphrag.data_model.types import TextEmbedder\n+from graphrag.vector_stores.base import (\n+    BaseVectorStore,\n+    VectorStoreDocument,\n+    VectorStoreSearchResult,\n+)\n+\n+\n+class LanceDBVectorStore(BaseVectorStore):\n+    \"\"\"LanceDB vector storage implementation.\"\"\"\n+\n+    def connect(self, **kwargs: Any) -> Any:\n+        \"\"\"Connect to the vector storage.\"\"\"\n+        self.db_connection = lancedb.connect(kwargs[\"db_uri\"])\n+\n+        if self.index_name and self.index_name in self.db_connection.table_names():\n+            self.document_collection = self.db_connection.open_table(self.index_name)\n+\n+    def create_index(self) -> None:\n+        \"\"\"Create index.\"\"\"\n+        dummy_vector = np.zeros(self.vector_size, dtype=np.float32)\n+        flat_array = pa.array(dummy_vector, type=pa.float32())\n+        vector_column = pa.FixedSizeListArray.from_arrays(flat_array, self.vector_size)\n+\n+        data = pa.table({\n+            self.id_field: pa.array([\"__DUMMY__\"], type=pa.string()),\n+            self.vector_field: vector_column,\n+        })\n+\n+        self.document_collection = self.db_connection.create_table(\n+            self.index_name if self.index_name else \"\",\n+            data=data,\n+            mode=\"overwrite\",\n+            schema=data.schema,\n+        )\n+\n+        # Step 5: Create index now that schema exists\n+        self.document_collection.create_index(\n+            vector_column_name=self.vector_field, index_type=\"IVF_FLAT\"\n+        )\n+\n+    def load_documents(self, documents: list[VectorStoreDocument]) -> None:\n+        \"\"\"Load documents into vector storage.\"\"\"\n+        self.document_collection.delete(f\"{self.id_field} = '__DUMMY__'\")\n+\n+        # Step 1: Prepare data columns manually\n+        ids = []\n+        vectors = []\n+\n+        for document in documents:\n+            self.vector_size = (\n+                len(document.vector) if document.vector else self.vector_size\n+            )\n+            if document.vector is not None and len(document.vector) == self.vector_size:\n+                ids.append(document.id)\n+                vectors.append(np.array(document.vector, dtype=np.float32))\n+\n+        # Step 2: Handle empty case\n+        if len(ids) == 0:\n+            data = None\n+        else:\n+            # Step 3: Flatten the vectors and build FixedSizeListArray manually\n+            flat_vector = np.concatenate(vectors).astype(np.float32)\n+            flat_array = pa.array(flat_vector, type=pa.float32())\n+            vector_column = pa.FixedSizeListArray.from_arrays(\n+                flat_array, self.vector_size\n+            )\n+\n+            # Step 4: Create PyArrow table (let schema be inferred)\n+            data = pa.table({\n+                self.id_field: pa.array(ids, type=pa.string()),\n+                self.vector_field: vector_column,\n+            })\n+\n+            if data:\n+                self.document_collection = self.db_connection.create_table(\n+                    self.index_name if self.index_name else \"\",\n+                    data=data,\n+                    mode=\"overwrite\",\n+                    schema=data.schema,\n+                )\n+\n+    def similarity_search_by_vector(\n+        self, query_embedding: list[float] | np.ndarray, k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform a vector-based similarity search.\"\"\"\n+        if self.query_filter:\n+            docs = (\n+                self.document_collection.search(\n+                    query=query_embedding, vector_column_name=self.vector_field\n+                )\n+                .where(self.query_filter, prefilter=True)\n+                .limit(k)\n+                .to_list()\n+            )\n+        else:\n+            query_embedding = np.array(query_embedding, dtype=np.float32)\n+\n+            docs = (\n+                self.document_collection.search(\n+                    query=query_embedding, vector_column_name=self.vector_field\n+                )\n+                .limit(k)\n+                .to_list()\n+            )\n+        return [\n+            VectorStoreSearchResult(\n+                document=VectorStoreDocument(\n+                    id=doc[self.id_field],\n+                    vector=doc[self.vector_field],\n+                ),\n+                score=1 - abs(float(doc[\"_distance\"])),\n+            )\n+            for doc in docs\n+        ]\n+\n+    def similarity_search_by_text(\n+        self, text: str, text_embedder: TextEmbedder, k: int = 10\n+    ) -> list[VectorStoreSearchResult]:\n+        \"\"\"Perform a similarity search using a given input text.\"\"\"\n+        query_embedding = text_embedder(text)\n+        if query_embedding:\n+            return self.similarity_search_by_vector(query_embedding, k)\n+        return []\n+\n+    def search_by_id(self, id: str) -> VectorStoreDocument:\n+        \"\"\"Search for a document by id.\"\"\"\n+        doc = (\n+            self.document_collection.search()\n+            .where(f\"{self.id_field} == '{id}'\", prefilter=True)\n+            .to_list()\n+        )\n+        if doc:\n+            return VectorStoreDocument(\n+                id=doc[0][self.id_field],\n+                vector=doc[0][self.vector_field],\n+            )\n+        return VectorStoreDocument(id=id, vector=None)\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "scripts/__init__.py",
            "diff": "diff --git a/scripts/__init__.py b/scripts/__init__.py\nnew file mode 100644\nindex 0000000..4adce34\n--- /dev/null\n+++ b/scripts/__init__.py\n@@ -0,0 +1,5 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\n+\"\"\"GraphRAG Scripts module.\"\"\"\n"
        },
        {
            "commit": "9fa62424e0f94d17a247b63898651667216075a5",
            "file_path": "scripts/copy_build_assets.py",
            "diff": "diff --git a/scripts/copy_build_assets.py b/scripts/copy_build_assets.py\nnew file mode 100644\nindex 0000000..cb1878d\n--- /dev/null\n+++ b/scripts/copy_build_assets.py\n@@ -0,0 +1,26 @@\n+# Copyright (c) 2025 Microsoft Corporation.\n+# Licensed under the MIT License\n+\n+\"\"\"Copy root build assets to package directories.\"\"\"\n+\n+import shutil\n+from pathlib import Path\n+\n+\n+def copy_build_assets():\n+    \"\"\"Copy root build assets to package build directories so files are included in pypi distributions.\"\"\"\n+    root_dir = Path(__file__).parent.parent\n+    build_assets = [\"README.md\", \"LICENSE\"]\n+\n+    for package_dir in root_dir.glob(\"packages/*\"):\n+        if package_dir.is_dir():\n+            for asset in build_assets:\n+                src = root_dir / asset\n+                dest = package_dir / asset\n+                if src.exists():\n+                    shutil.copy(src, dest)\n+                    print(f\"Copied {asset} to {package_dir}\")\n+\n+\n+if __name__ == \"__main__\":\n+    copy_build_assets()\n"
        }
    ]
}