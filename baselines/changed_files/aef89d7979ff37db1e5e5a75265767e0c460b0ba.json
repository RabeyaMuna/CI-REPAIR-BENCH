{
    "sha_fail": "aef89d7979ff37db1e5e5a75265767e0c460b0ba",
    "changed_files": [
        {
            "commit": "aef89d7979ff37db1e5e5a75265767e0c460b0ba",
            "file_path": "cookbook/observability/langtrace_op.py",
            "diff": "diff --git a/cookbook/observability/langtrace_op.py b/cookbook/observability/langtrace_op.py\nindex c88f256a2..ac6b34035 100644\n--- a/cookbook/observability/langtrace_op.py\n+++ b/cookbook/observability/langtrace_op.py\n@@ -6,13 +6,15 @@ This example shows how to instrument your agno agent with Langtrace.\n 3. Set your Langtrace API key as an environment variables:\n   - export LANGTRACE_API_KEY=<your-key>\n \"\"\"\n-# Must precede other imports\n-from langtrace_python_sdk import langtrace   # type: ignore\n-from langtrace_python_sdk.utils.with_root_span import with_langtrace_root_span   # type: ignore\n \n+# Must precede other imports\n from agno.agent import Agent\n from agno.models.openai import OpenAIChat\n from agno.tools.yfinance import YFinanceTools\n+from langtrace_python_sdk import langtrace  # type: ignore\n+from langtrace_python_sdk.utils.with_root_span import (\n+    with_langtrace_root_span,  # type: ignore\n+)\n \n langtrace.init()\n \n"
        },
        {
            "commit": "aef89d7979ff37db1e5e5a75265767e0c460b0ba",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex 1d34b3ddf..a9b6e1466 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -576,18 +576,16 @@ class Agent:\n         response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n         message: Optional[Union[str, List, Dict, Message]] = None,\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n-    ) -> Iterator[RunResponse]:\n+    ) -> RunResponse:\n         \"\"\"Run the Agent and yield the RunResponse.\n \n         Steps:\n         1. Reason about the task if reasoning is enabled\n-        2. Start the Run by yielding a RunStarted event\n-        3. Generate a response from the Model (includes running function calls)\n-        4. Update RunResponse\n-        5. Update Agent Memory\n-        6. Calculate session metrics\n-        7. Save session to storage\n-        12. Save output to file if save_response_to_file is set\n+        2. Generate a response from the Model (includes running function calls)\n+        3. Update Agent Memory\n+        4. Calculate session metrics\n+        5. Save session to storage\n+        6. Save output to file if save_response_to_file is set\n         \"\"\"\n         if isinstance(self.memory, AgentMemory):\n             self.memory = cast(AgentMemory, self.memory)\n@@ -595,263 +593,402 @@ class Agent:\n             self.memory = cast(Memory, self.memory)\n         self.model = cast(Model, self.model)\n \n-        # Reason about the task if reasoning is enabled\n+        # 1. Reason about the task if reasoning is enabled\n         if self.reasoning or self.reasoning_model is not None:\n             reasoning_generator = self.reason(run_messages=run_messages, session_id=session_id)\n \n-            if self.stream:\n-                yield from reasoning_generator\n-            else:\n-                # Consume the generator without yielding\n-                deque(reasoning_generator, maxlen=0)\n+            # Consume the generator without yielding\n+            deque(reasoning_generator, maxlen=0)\n \n         # Get the index of the last \"user\" message in messages_for_run\n-        # We track this, so we can add messages after this index to the RunResponse and Memory\n+        # We track this so we can add messages after this index to the RunResponse and Memory\n         index_of_last_user_message = len(run_messages.messages)\n \n-        # 6. Start the Run by yielding a RunStarted event\n-        if self.stream_intermediate_steps:\n-            yield self.create_run_response(\"Run started\", session_id=session_id, event=RunEvent.run_started)\n-\n-        # 7. Generate a response from the Model (includes running function calls)\n+        # 2. Generate a response from the Model (includes running function calls)\n         model_response: ModelResponse\n-        self.model = cast(Model, self.model)\n-        reasoning_started = False\n-        reasoning_time_taken = 0.0\n-        if self.stream:\n-            model_response = ModelResponse()\n-            for model_response_chunk in self.model.response_stream(\n-                messages=run_messages.messages,\n-                response_format=response_format,\n-                tools=self._tools_for_model,\n-                functions=self._functions_for_model,\n-                tool_choice=self.tool_choice,\n-                tool_call_limit=self.tool_call_limit,\n+\n+        # Get the model response\n+        model_response = self.model.response(\n+            messages=run_messages.messages,\n+            response_format=response_format,\n+            tools=self._tools_for_model,\n+            functions=self._functions_for_model,\n+            tool_choice=self.tool_choice,\n+            tool_call_limit=self.tool_call_limit,\n+        )\n+        # Format tool calls if they exist\n+        if model_response.tool_calls:\n+            run_response.formatted_tool_calls = format_tool_calls(model_response.tool_calls)\n+\n+        # Handle structured outputs\n+        if self.response_model is not None and model_response.parsed is not None:\n+            # We get native structured outputs from the model\n+            if self._model_should_return_structured_output():\n+                # Update the run_response content with the structured output\n+                run_response.content = model_response.parsed\n+                # Update the run_response content_type with the structured output class name\n+                run_response.content_type = self.response_model.__name__\n+        else:\n+            # Update the run_response content with the model response content\n+            run_response.content = model_response.content\n+\n+        # Update the run_response thinking with the model response thinking\n+        if model_response.thinking is not None:\n+            run_response.thinking = model_response.thinking\n+        if model_response.redacted_thinking is not None:\n+            if run_response.thinking is None:\n+                run_response.thinking = model_response.redacted_thinking\n+            else:\n+                run_response.thinking += model_response.redacted_thinking\n+\n+        # Update the run_response citations with the model response citations\n+        if model_response.citations is not None:\n+            run_response.citations = model_response.citations\n+\n+        # Update the run_response tools with the model response tools\n+        if model_response.tool_calls is not None:\n+            if run_response.tools is None:\n+                run_response.tools = model_response.tool_calls\n+            else:\n+                run_response.tools.extend(model_response.tool_calls)\n+\n+            # For Reasoning/Thinking/Knowledge Tools update reasoning_content in RunResponse\n+            for tool_call in model_response.tool_calls:\n+                tool_name = tool_call.get(\"tool_name\", \"\")\n+                if tool_name.lower() in [\"think\", \"analyze\"]:\n+                    tool_args = tool_call.get(\"tool_args\", {})\n+                    self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n+\n+        # Update the run_response audio with the model response audio\n+        if model_response.audio is not None:\n+            run_response.response_audio = model_response.audio\n+\n+        if model_response.image is not None:\n+            self.add_image(model_response.image)\n+\n+        # Update the run_response messages with the messages\n+        run_response.messages = run_messages.messages\n+        # Update the run_response created_at with the model response created_at\n+        run_response.created_at = model_response.created_at\n+\n+        # Update RunResponse\n+        # Build a list of messages that should be added to the RunResponse\n+        messages_for_run_response = [m for m in run_messages.messages if m.add_to_agent_memory]\n+        # Update the RunResponse messages\n+        run_response.messages = messages_for_run_response\n+        # Update the RunResponse metrics\n+        run_response.metrics = self.aggregate_metrics_from_messages(messages_for_run_response)\n+\n+        # 3. Update Agent Memory\n+        if isinstance(self.memory, AgentMemory):\n+            # Add the system message to the memory\n+            if run_messages.system_message is not None:\n+                self.memory.add_system_message(\n+                    run_messages.system_message, system_message_role=self.system_message_role\n+                )\n+\n+            # Build a list of messages that should be added to the AgentMemory\n+            messages_for_memory: List[Message] = (\n+                [run_messages.user_message] if run_messages.user_message is not None else []\n+            )\n+            # Add messages from messages_for_run after the last user message\n+            for _rm in run_messages.messages[index_of_last_user_message:]:\n+                if _rm.add_to_agent_memory:\n+                    messages_for_memory.append(_rm)\n+            if len(messages_for_memory) > 0:\n+                self.memory.add_messages(messages=messages_for_memory)\n+\n+            # Create an AgentRun object to add to memory\n+            agent_run = AgentRun(response=run_response)\n+            agent_run.message = run_messages.user_message\n+\n+            # Update the memories with the user message if needed\n+            if (\n+                self.memory.create_user_memories\n+                and self.memory.update_user_memories_after_run\n+                and run_messages.user_message is not None\n             ):\n-                # If the model response is an assistant_response, yield a RunResponse\n-                if model_response_chunk.event == ModelResponseEvent.assistant_response.value:\n-                    # Process content and thinking\n-                    if model_response_chunk.content is not None:\n-                        model_response.content = (model_response.content or \"\") + model_response_chunk.content\n-                        run_response.content = model_response.content\n-\n-                    if model_response_chunk.thinking is not None:\n-                        model_response.thinking = (model_response.thinking or \"\") + model_response_chunk.thinking\n-                        run_response.thinking = model_response.thinking\n-\n-                    if model_response_chunk.redacted_thinking is not None:\n-                        model_response.redacted_thinking = (\n-                            model_response.redacted_thinking or \"\"\n-                        ) + model_response_chunk.redacted_thinking\n-\n-                        # We only have thinking on response\n-                        run_response.thinking = model_response.redacted_thinking\n-\n-                    if model_response_chunk.citations is not None:\n-                        # We get citations in one chunk\n-                        run_response.citations = model_response_chunk.citations\n-\n-                    # Only yield if we have content or thinking to show\n-                    if (\n-                        model_response_chunk.content is not None\n-                        or model_response_chunk.thinking is not None\n-                        or model_response_chunk.redacted_thinking is not None\n-                        or model_response_chunk.citations is not None\n-                    ):\n-                        yield self.create_run_response(\n-                            content=model_response_chunk.content,\n-                            thinking=model_response_chunk.thinking,\n-                            redacted_thinking=model_response_chunk.redacted_thinking,\n-                            citations=model_response_chunk.citations,\n-                            created_at=model_response_chunk.created_at,\n-                            session_id=session_id,\n-                        )\n+                self.memory.update_memory(input=run_messages.user_message.get_content_string())\n \n-                    # Process audio\n-                    if model_response_chunk.audio is not None:\n-                        if model_response.audio is None:\n-                            model_response.audio = AudioResponse(id=str(uuid4()), content=\"\", transcript=\"\")\n-\n-                        if model_response_chunk.audio.id is not None:\n-                            model_response.audio.id = model_response_chunk.audio.id  # type: ignore\n-                        if model_response_chunk.audio.content is not None:\n-                            model_response.audio.content += model_response_chunk.audio.content  # type: ignore\n-                        if model_response_chunk.audio.transcript is not None:\n-                            model_response.audio.transcript += model_response_chunk.audio.transcript  # type: ignore\n-                        if model_response_chunk.audio.expires_at is not None:\n-                            model_response.audio.expires_at = model_response_chunk.audio.expires_at  # type: ignore\n-                        if model_response_chunk.audio.mime_type is not None:\n-                            model_response.audio.mime_type = model_response_chunk.audio.mime_type  # type: ignore\n-                        model_response.audio.sample_rate = model_response_chunk.audio.sample_rate\n-                        model_response.audio.channels = model_response_chunk.audio.channels\n-\n-                        # Yield the audio and transcript bit by bit\n-                        run_response.response_audio = AudioResponse(\n-                            id=model_response_chunk.audio.id,\n-                            content=model_response_chunk.audio.content,\n-                            transcript=model_response_chunk.audio.transcript,\n-                            sample_rate=model_response_chunk.audio.sample_rate,\n-                            channels=model_response_chunk.audio.channels,\n-                        )\n-                        run_response.created_at = model_response_chunk.created_at\n+            if messages is not None and len(messages) > 0:\n+                for _im in messages:\n+                    # Parse the message and convert to a Message object if possible\n+                    mp = None\n+                    if isinstance(_im, Message):\n+                        mp = _im\n+                    elif isinstance(_im, dict):\n+                        try:\n+                            mp = Message(**_im)\n+                        except Exception as e:\n+                            log_warning(f\"Failed to validate message: {e}\")\n+                    else:\n+                        log_warning(f\"Unsupported message type: {type(_im)}\")\n+                        continue\n+\n+                    # Add the message to the AgentRun\n+                    if mp:\n+                        if agent_run.messages is None:\n+                            agent_run.messages = []\n+                        agent_run.messages.append(mp)\n+                        if self.memory.create_user_memories and self.memory.update_user_memories_after_run:\n+                            self.memory.update_memory(input=mp.get_content_string())\n+                    else:\n+                        log_warning(\"Unable to add message to memory\")\n+            # Add AgentRun to memory\n+            self.memory.add_run(agent_run)\n+            # Update the session summary if needed\n+            if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n+                self.memory.update_summary()\n+\n+            # 4. Calculate session metrics\n+            self.session_metrics = self.calculate_metrics(self.memory.messages)\n+        elif isinstance(self.memory, Memory):\n+            # Add AgentRun to memory\n+            self.memory.add_run(session_id=session_id, run=run_response)\n \n-                        yield run_response\n+            self._make_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n \n-                    if model_response_chunk.image is not None:\n-                        self.add_image(model_response_chunk.image)\n+            # 4. Calculate session metrics\n+            if self.session_metrics is None:\n+                self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n+            else:\n+                self.session_metrics += self.calculate_metrics(\n+                    run_messages.messages\n+                )  # Calculate metrics for the session\n \n-                        yield run_response\n+        # 5. Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-                # If the model response is a tool_call_started, add the tool call to the run_response\n-                elif (\n-                    model_response_chunk.event == ModelResponseEvent.tool_call_started.value\n-                ):  # Add tool calls to the run_response\n-                    new_tool_calls_list = model_response_chunk.tool_calls\n-                    if new_tool_calls_list is not None:\n-                        # Add tool calls to the agent.run_response\n-                        if run_response.tools is None:\n-                            run_response.tools = new_tool_calls_list\n-                        else:\n-                            run_response.tools.extend(new_tool_calls_list)\n+        # 6. Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n \n-                        # Format tool calls whenever new ones are added during streaming\n-                        run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n+        # Log Agent Run\n+        self._log_agent_run(user_id=user_id, session_id=session_id)\n \n-                    # Yield a RunResponse with the tool_call_started event\n-                    yield self.create_run_response(\n-                        content=model_response_chunk.content,\n-                        created_at=model_response_chunk.created_at,\n-                        event=RunEvent.tool_call_started,\n-                        session_id=session_id,\n-                        run_response=run_response,\n-                        )\n+        # Otherwise convert the response to the structured format\n+        if self.response_model is not None and not isinstance(run_response.content, self.response_model):\n+            if isinstance(run_response.content, str) and self.parse_response:\n+                try:\n+                    structured_output = parse_response_model_str(run_response.content, self.response_model)\n \n-                # If the model response is a tool_call_completed, update the existing tool call in the run_response\n-                elif model_response_chunk.event == ModelResponseEvent.tool_call_completed.value:\n-                    reasoning_step: Optional[ReasoningStep] = None\n-\n-                    new_tool_calls_list = model_response_chunk.tool_calls\n-                    if new_tool_calls_list is not None:\n-                        # Update the existing tool call in the run_response\n-                        if run_response.tools:\n-                            # Create a mapping of tool_call_id to index\n-                            tool_call_index_map = {\n-                                tc[\"tool_call_id\"]: i\n-                                for i, tc in enumerate(run_response.tools)\n-                                if tc.get(\"tool_call_id\") is not None\n-                            }\n-                            # Process tool calls\n-                            for tool_call_dict in new_tool_calls_list:\n-                                tool_call_id = tool_call_dict.get(\"tool_call_id\")\n-                                index = tool_call_index_map.get(tool_call_id)\n-                                if index is not None:\n-                                    run_response.tools[index] = tool_call_dict\n-                        else:\n-                            run_response.tools = new_tool_calls_list\n+                    # Update RunResponse\n+                    if structured_output is not None:\n+                        run_response.content = structured_output\n+                        run_response.content_type = self.response_model.__name__\n+                    else:\n+                        log_warning(\"Failed to convert response to response_model\")\n+                except Exception as e:\n+                    log_warning(f\"Failed to convert response to output model: {e}\")\n+            else:\n+                log_warning(\"Something went wrong. Run response content is not a string\")\n \n-                        # Only iterate through new tool calls\n-                        for tool_call in new_tool_calls_list:\n-                            tool_name = tool_call.get(\"tool_name\", \"\")\n-                            if tool_name.lower() in [\"think\", \"analyze\"]:\n-                                tool_args = tool_call.get(\"tool_args\", {})\n+        log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n \n-                                reasoning_step = self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n+        return run_response\n \n-                                metrics = tool_call.get(\"metrics\")\n-                                if metrics is not None and metrics.time is not None:\n-                                    reasoning_time_taken = reasoning_time_taken + float(metrics.time)\n+    def _run_stream(\n+        self,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+        stream_intermediate_steps: bool = False,\n+    ) -> Iterator[RunResponse]:\n+        if isinstance(self.memory, AgentMemory):\n+            self.memory = cast(AgentMemory, self.memory)\n+        else:\n+            self.memory = cast(Memory, self.memory)\n+        self.model = cast(Model, self.model)\n \n-                    if self.stream_intermediate_steps:\n-                        if reasoning_step is not None:\n-                            if not reasoning_started:\n-                                yield self.create_run_response(\n-                                    content=\"Reasoning started\",\n-                                    event=RunEvent.reasoning_started,\n-                                )\n-                                reasoning_started = True\n+        # 1. Reason about the task if reasoning is enabled\n+        if self.reasoning or self.reasoning_model is not None:\n+            reasoning_generator = self.reason(run_messages=run_messages, session_id=session_id)\n+            yield from reasoning_generator\n \n-                            yield self.create_run_response(\n-                                content=reasoning_step,\n-                                content_type=reasoning_step.__class__.__name__,\n-                                event=RunEvent.reasoning_step,\n-                                reasoning_content=run_response.reasoning_content,\n-                            )\n+        # Get the index of the last \"user\" message in messages_for_run\n+        # We track this, so we can add messages after this index to the RunResponse and Memory\n+        index_of_last_user_message = len(run_messages.messages)\n+\n+        # 6. Start the Run by yielding a RunStarted event\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\"Run started\", session_id=session_id, event=RunEvent.run_started)\n+\n+        model_response = ModelResponse()\n+        reasoning_started = False\n+        reasoning_time_taken = 0.0\n+\n+        # 2. Process model response chunks\n+        for model_response_chunk in self.model.response_stream(\n+            messages=run_messages.messages,\n+            response_format=response_format,\n+            tools=self._tools_for_model,\n+            functions=self._functions_for_model,\n+            tool_choice=self.tool_choice,\n+            tool_call_limit=self.tool_call_limit,\n+        ):\n+            # If the model response is an assistant_response, yield a RunResponse\n+            if model_response_chunk.event == ModelResponseEvent.assistant_response.value:\n+                # Process content and thinking\n+                if model_response_chunk.content is not None:\n+                    model_response.content = (model_response.content or \"\") + model_response_chunk.content\n+                    run_response.content = model_response.content\n+\n+                if model_response_chunk.thinking is not None:\n+                    model_response.thinking = (model_response.thinking or \"\") + model_response_chunk.thinking\n+                    run_response.thinking = model_response.thinking\n+\n+                if model_response_chunk.redacted_thinking is not None:\n+                    model_response.redacted_thinking = (\n+                        model_response.redacted_thinking or \"\"\n+                    ) + model_response_chunk.redacted_thinking\n+\n+                    # We only have thinking on response\n+                    run_response.thinking = model_response.redacted_thinking\n+\n+                if model_response_chunk.citations is not None:\n+                    # We get citations in one chunk\n+                    run_response.citations = model_response_chunk.citations\n \n-                    # Yield a RunResponse with the tool_call_completed event\n+                # Only yield if we have content or thinking to show\n+                if (\n+                    model_response_chunk.content is not None\n+                    or model_response_chunk.thinking is not None\n+                    or model_response_chunk.redacted_thinking is not None\n+                    or model_response_chunk.citations is not None\n+                ):\n                     yield self.create_run_response(\n                         content=model_response_chunk.content,\n-                        event=RunEvent.tool_call_completed,\n+                        thinking=model_response_chunk.thinking,\n+                        redacted_thinking=model_response_chunk.redacted_thinking,\n+                        citations=model_response_chunk.citations,\n                         created_at=model_response_chunk.created_at,\n                         session_id=session_id,\n-                        run_response=run_response,\n                     )\n \n-        else:\n-            # Get the model response\n-            model_response = self.model.response(\n-                messages=run_messages.messages,\n-                response_format=response_format,\n-                tools=self._tools_for_model,\n-                functions=self._functions_for_model,\n-                tool_choice=self.tool_choice,\n-                tool_call_limit=self.tool_call_limit,\n-            )\n-            # Format tool calls if they exist\n-            if model_response.tool_calls:\n-                run_response.formatted_tool_calls = format_tool_calls(model_response.tool_calls)\n-\n-            # Handle structured outputs\n-            if self.response_model is not None and model_response.parsed is not None:\n-                # We get native structured outputs from the model\n-                if self._model_should_return_structured_output():\n-                    # Update the run_response content with the structured output\n-                    run_response.content = model_response.parsed\n-                    # Update the run_response content_type with the structured output class name\n-                    run_response.content_type = self.response_model.__name__\n-            else:\n-                # Update the run_response content with the model response content\n-                run_response.content = model_response.content\n-\n-            # Update the run_response thinking with the model response thinking\n-            if model_response.thinking is not None:\n-                run_response.thinking = model_response.thinking\n-            if model_response.redacted_thinking is not None:\n-                if run_response.thinking is None:\n-                    run_response.thinking = model_response.redacted_thinking\n-                else:\n-                    run_response.thinking += model_response.redacted_thinking\n+                # Process audio\n+                if model_response_chunk.audio is not None:\n+                    if model_response.audio is None:\n+                        model_response.audio = AudioResponse(id=str(uuid4()), content=\"\", transcript=\"\")\n+\n+                    if model_response_chunk.audio.id is not None:\n+                        model_response.audio.id = model_response_chunk.audio.id  # type: ignore\n+                    if model_response_chunk.audio.content is not None:\n+                        model_response.audio.content += model_response_chunk.audio.content  # type: ignore\n+                    if model_response_chunk.audio.transcript is not None:\n+                        model_response.audio.transcript += model_response_chunk.audio.transcript  # type: ignore\n+                    if model_response_chunk.audio.expires_at is not None:\n+                        model_response.audio.expires_at = model_response_chunk.audio.expires_at  # type: ignore\n+                    if model_response_chunk.audio.mime_type is not None:\n+                        model_response.audio.mime_type = model_response_chunk.audio.mime_type  # type: ignore\n+                    model_response.audio.sample_rate = model_response_chunk.audio.sample_rate\n+                    model_response.audio.channels = model_response_chunk.audio.channels\n+\n+                    # Yield the audio and transcript bit by bit\n+                    run_response.response_audio = AudioResponse(\n+                        id=model_response_chunk.audio.id,\n+                        content=model_response_chunk.audio.content,\n+                        transcript=model_response_chunk.audio.transcript,\n+                        sample_rate=model_response_chunk.audio.sample_rate,\n+                        channels=model_response_chunk.audio.channels,\n+                    )\n+                    run_response.created_at = model_response_chunk.created_at\n \n-            # Update the run_response citations with the model response citations\n-            if model_response.citations is not None:\n-                run_response.citations = model_response.citations\n+                    yield run_response\n \n-            # Update the run_response tools with the model response tools\n-            if model_response.tool_calls is not None:\n-                if run_response.tools is None:\n-                    run_response.tools = model_response.tool_calls\n-                else:\n-                    run_response.tools.extend(model_response.tool_calls)\n+                if model_response_chunk.image is not None:\n+                    self.add_image(model_response_chunk.image)\n+\n+                    yield run_response\n+\n+            # If the model response is a tool_call_started, add the tool call to the run_response\n+            elif (\n+                model_response_chunk.event == ModelResponseEvent.tool_call_started.value\n+            ):  # Add tool calls to the run_response\n+                new_tool_calls_list = model_response_chunk.tool_calls\n+                if new_tool_calls_list is not None:\n+                    # Add tool calls to the agent.run_response\n+                    if run_response.tools is None:\n+                        run_response.tools = new_tool_calls_list\n+                    else:\n+                        run_response.tools.extend(new_tool_calls_list)\n+\n+                    # Format tool calls whenever new ones are added during streaming\n+                    run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n+\n+                # Yield a RunResponse with the tool_call_started event\n+                yield self.create_run_response(\n+                    content=model_response_chunk.content,\n+                    created_at=model_response_chunk.created_at,\n+                    event=RunEvent.tool_call_started,\n+                    session_id=session_id,\n+                    run_response=run_response,\n+                )\n+\n+            # If the model response is a tool_call_completed, update the existing tool call in the run_response\n+            elif model_response_chunk.event == ModelResponseEvent.tool_call_completed.value:\n+                reasoning_step: Optional[ReasoningStep] = None\n+\n+                new_tool_calls_list = model_response_chunk.tool_calls\n+                if new_tool_calls_list is not None:\n+                    # Update the existing tool call in the run_response\n+                    if run_response.tools:\n+                        # Create a mapping of tool_call_id to index\n+                        tool_call_index_map = {\n+                            tc[\"tool_call_id\"]: i\n+                            for i, tc in enumerate(run_response.tools)\n+                            if tc.get(\"tool_call_id\") is not None\n+                        }\n+                        # Process tool calls\n+                        for tool_call_dict in new_tool_calls_list:\n+                            tool_call_id = tool_call_dict.get(\"tool_call_id\")\n+                            index = tool_call_index_map.get(tool_call_id)\n+                            if index is not None:\n+                                run_response.tools[index] = tool_call_dict\n+                    else:\n+                        run_response.tools = new_tool_calls_list\n+\n+                    # Only iterate through new tool calls\n+                    for tool_call in new_tool_calls_list:\n+                        tool_name = tool_call.get(\"tool_name\", \"\")\n+                        if tool_name.lower() in [\"think\", \"analyze\"]:\n+                            tool_args = tool_call.get(\"tool_args\", {})\n \n-                # For Reasoning/Thinking/Knowledge Tools update reasoning_content in RunResponse\n-                for tool_call in model_response.tool_calls:\n-                    tool_name = tool_call.get(\"tool_name\", \"\")\n-                    if tool_name.lower() in [\"think\", \"analyze\"]:\n-                        tool_args = tool_call.get(\"tool_args\", {})\n-                        self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n+                            reasoning_step = self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n \n-            # Update the run_response audio with the model response audio\n-            if model_response.audio is not None:\n-                run_response.response_audio = model_response.audio\n+                            metrics = tool_call.get(\"metrics\")\n+                            if metrics is not None and metrics.time is not None:\n+                                reasoning_time_taken = reasoning_time_taken + float(metrics.time)\n+\n+                if stream_intermediate_steps:\n+                    if reasoning_step is not None:\n+                        if not reasoning_started:\n+                            yield self.create_run_response(\n+                                content=\"Reasoning started\",\n+                                event=RunEvent.reasoning_started,\n+                            )\n+                            reasoning_started = True\n \n-            if model_response.image is not None:\n-                self.add_image(model_response.image)\n+                        yield self.create_run_response(\n+                            content=reasoning_step,\n+                            content_type=reasoning_step.__class__.__name__,\n+                            event=RunEvent.reasoning_step,\n+                            reasoning_content=run_response.reasoning_content,\n+                        )\n \n-            # Update the run_response messages with the messages\n-            run_response.messages = run_messages.messages\n-            # Update the run_response created_at with the model response created_at\n-            run_response.created_at = model_response.created_at\n+                # Yield a RunResponse with the tool_call_completed event\n+                yield self.create_run_response(\n+                    content=model_response_chunk.content,\n+                    event=RunEvent.tool_call_completed,\n+                    created_at=model_response_chunk.created_at,\n+                    session_id=session_id,\n+                    run_response=run_response,\n+                )\n \n-        if self.stream_intermediate_steps and reasoning_started:\n+        # Determine reasoning completed\n+        if stream_intermediate_steps and reasoning_started:\n             all_reasoning_steps: List[ReasoningStep] = []\n             if run_response and run_response.extra_data and hasattr(run_response.extra_data, \"reasoning_steps\"):\n                 all_reasoning_steps = cast(List[ReasoningStep], run_response.extra_data.reasoning_steps)\n@@ -864,7 +1001,7 @@ class Agent:\n                     event=RunEvent.reasoning_completed,\n                 )\n \n-        # 8. Update RunResponse\n+        # Update RunResponse\n         # Build a list of messages that should be added to the RunResponse\n         messages_for_run_response = [m for m in run_messages.messages if m.add_to_agent_memory]\n         # Update the RunResponse messages\n@@ -873,10 +1010,10 @@ class Agent:\n         run_response.metrics = self.aggregate_metrics_from_messages(messages_for_run_response)\n \n         # Update the run_response audio if streaming\n-        if self.stream and model_response.audio is not None:\n+        if model_response.audio is not None:\n             run_response.response_audio = model_response.audio\n \n-        # 9. Update Agent Memory\n+        # 3. Update Agent Memory\n         if isinstance(self.memory, AgentMemory):\n             # Add the system message to the memory\n             if run_messages.system_message is not None:\n@@ -937,7 +1074,7 @@ class Agent:\n             if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n                 self.memory.update_summary()\n \n-            # 10. Calculate session metrics\n+            # 4. Calculate session metrics\n             self.session_metrics = self.calculate_metrics(self.memory.messages)\n         elif isinstance(self.memory, Memory):\n             # Add AgentRun to memory\n@@ -945,6 +1082,7 @@ class Agent:\n \n             self._make_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n \n+            # 4. Calculate session metrics\n             if self.session_metrics is None:\n                 self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n             else:\n@@ -953,25 +1091,25 @@ class Agent:\n                 )  # Calculate metrics for the session\n \n         # Yield UpdatingMemory event\n-        if self.stream_intermediate_steps:\n+        if stream_intermediate_steps:\n             yield self.create_run_response(\n                 content=\"Memory updated\",\n                 session_id=session_id,\n                 event=RunEvent.updating_memory,\n             )\n \n-        # 11. Save session to storage\n+        # 5. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 12. Save output to file if save_response_to_file is set\n+        # 6. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n-\n         # Log Agent Run\n         self._log_agent_run(user_id=user_id, session_id=session_id)\n \n         log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n-        if self.stream_intermediate_steps:\n+\n+        if stream_intermediate_steps:\n             yield self.create_run_response(\n                 content=run_response.content,\n                 reasoning_content=run_response.reasoning_content,\n@@ -980,21 +1118,6 @@ class Agent:\n                 run_response=run_response,\n             )\n \n-        # Yield final response if not streaming so that run() can get the response\n-        if not self.stream:\n-            yield run_response\n-    \n-    def _run_stream(self, \n-        run_response: RunResponse,\n-        run_messages: RunMessages,\n-        session_id: str,\n-        user_id: Optional[str] = None,\n-        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n-        message: Optional[Union[str, List, Dict, Message]] = None,\n-        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n-        ) -> Iterator[RunResponse]:\n-        pass\n-    \n     @overload\n     def run(\n         self,\n@@ -1137,14 +1260,14 @@ class Agent:\n             session_id=session_id,\n             user_id=user_id,\n             async_mode=False,\n-            knowledge_filters=knowledge_filters,\n+            knowledge_filters=effective_filters,\n         )\n \n+        # Create a run_id for this specific run\n+        run_id = str(uuid4())\n+\n         for attempt in range(num_attempts):\n             try:\n-                # Create a run_id for this specific run\n-                run_id = str(uuid4())\n-\n                 # Create a new run_response for this attempt\n                 run_response = RunResponse(run_id=run_id, session_id=session_id, agent_id=self.agent_id)\n \n@@ -1165,460 +1288,513 @@ class Agent:\n                 elif messages is not None:\n                     self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n \n-                log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+                log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+\n+                # Prepare run messages\n+                run_messages: RunMessages = self.get_run_messages(\n+                    message=message,\n+                    session_id=session_id,\n+                    user_id=user_id,\n+                    audio=audio,\n+                    images=images,\n+                    videos=videos,\n+                    files=files,\n+                    messages=messages,\n+                    **kwargs,\n+                )\n+                if len(run_messages.messages) == 0:\n+                    log_error(\"No messages to be sent to the model.\")\n+\n+                self.run_messages = run_messages\n+\n+                if stream and self.is_streamable:\n+                    resp = self._run_stream(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    )\n+                    return resp\n+                else:\n+                    resp = self._run(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                    )\n+                    return resp\n+            except ModelProviderError as e:\n+                log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n+                if isinstance(e, StopAgentRun):\n+                    raise e\n+                last_exception = e\n+                if attempt < num_attempts - 1:  # Don't sleep on the last attempt\n+                    if self.exponential_backoff:\n+                        delay = 2**attempt * self.delay_between_retries\n+                    else:\n+                        delay = self.delay_between_retries\n+                    import time\n+\n+                    time.sleep(delay)\n+            except KeyboardInterrupt:\n+                # Create a cancelled response\n+                cancelled_response = RunResponse(\n+                    run_id=self.run_id or str(uuid4()),\n+                    session_id=session_id,\n+                    agent_id=self.agent_id,\n+                    content=\"Operation cancelled by user\",\n+                    event=RunEvent.run_cancelled,\n+                )\n+                return cancelled_response\n+\n+        # If we get here, all retries failed\n+        if last_exception is not None:\n+            log_error(\n+                f\"Failed after {num_attempts} attempts. Last error using {last_exception.model_name}({last_exception.model_id})\"\n+            )\n+            raise last_exception\n+        else:\n+            raise Exception(f\"Failed after {num_attempts} attempts.\")\n+\n+    async def _arun(\n+        self,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+    ) -> RunResponse:\n+        \"\"\"Run the Agent and yield the RunResponse.\n+\n+        Steps:\n+        1. Reason about the task if reasoning is enabled\n+        2. Generate a response from the Model (includes running function calls)\n+        3. Update Agent Memory\n+        4. Calculate session metrics\n+        5. Save session to storage\n+        6. Save output to file if save_response_to_file is set\n+        \"\"\"\n+\n+        if isinstance(self.memory, AgentMemory):\n+            self.memory = cast(AgentMemory, self.memory)\n+        else:\n+            self.memory = cast(Memory, self.memory)\n+        self.model = cast(Model, self.model)\n+\n+        # 1. Reason about the task if reasoning is enabled\n+        if self.reasoning or self.reasoning_model is not None:\n+            areason_generator = self.areason(run_messages=run_messages, session_id=session_id)\n+            # Consume the generator without yielding\n+            async for _ in areason_generator:\n+                pass\n+\n+        # Get the index of the last \"user\" message in messages_for_run\n+        # We track this so we can add messages after this index to the RunResponse and Memory\n+        index_of_last_user_message = len(run_messages.messages)\n+\n+        # 2. Generate a response from the Model (includes running function calls)\n+        model_response: ModelResponse\n+        self.model = cast(Model, self.model)\n+\n+        # Get the model response\n+        model_response = await self.model.aresponse(\n+            messages=run_messages.messages,\n+            response_format=response_format,\n+            tools=self._tools_for_model,\n+            functions=self._functions_for_model,\n+            tool_choice=self.tool_choice,\n+            tool_call_limit=self.tool_call_limit,\n+        )\n+        # Format tool calls if they exist\n+        if model_response.tool_calls:\n+            run_response.formatted_tool_calls = format_tool_calls(model_response.tool_calls)\n+\n+        # Handle structured outputs\n+        if self.response_model is not None and model_response.parsed is not None:\n+            # We get native structured outputs from the model\n+            if self._model_should_return_structured_output():\n+                # Update the run_response content with the structured output\n+                run_response.content = model_response.parsed\n+                # Update the run_response content_type with the structured output class name\n+                run_response.content_type = self.response_model.__name__\n+        else:\n+            # Update the run_response content with the model response content\n+            run_response.content = model_response.content\n+\n+        # Update the run_response thinking with the model response thinking\n+        if model_response.thinking is not None:\n+            run_response.thinking = model_response.thinking\n+        if model_response.redacted_thinking is not None:\n+            if run_response.thinking is None:\n+                run_response.thinking = model_response.redacted_thinking\n+            else:\n+                run_response.thinking += model_response.redacted_thinking\n+\n+        if model_response.citations is not None:\n+            run_response.citations = model_response.citations\n+\n+        # Update the run_response tools with the model response tools\n+        if model_response.tool_calls is not None:\n+            if run_response.tools is None:\n+                run_response.tools = model_response.tool_calls\n+            else:\n+                run_response.tools.extend(model_response.tool_calls)\n+\n+            # For Reasoning/Thinking/Knowledge Tools update reasoning_content in RunResponse\n+            for tool_call in model_response.tool_calls:\n+                tool_name = tool_call.get(\"tool_name\", \"\")\n+                if tool_name.lower() in [\"think\", \"analyze\"]:\n+                    tool_args = tool_call.get(\"tool_args\", {})\n+                    self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n+\n+        # Update the run_response audio with the model response audio\n+        if model_response.audio is not None:\n+            run_response.response_audio = model_response.audio\n+\n+        if model_response.image is not None:\n+            self.add_image(model_response.image)\n+\n+        # Update the run_response messages with the messages\n+        run_response.messages = run_messages.messages\n+        # Update the run_response created_at with the model response created_at\n+        run_response.created_at = model_response.created_at\n+\n+        # Update RunResponse\n+        # Build a list of messages that should be added to the RunResponse\n+        messages_for_run_response = [m for m in run_messages.messages if m.add_to_agent_memory]\n+        # Update the RunResponse messages\n+        run_response.messages = messages_for_run_response\n+        # Update the RunResponse metrics\n+        run_response.metrics = self.aggregate_metrics_from_messages(messages_for_run_response)\n+\n+        # 3. Update Agent Memory\n+        if isinstance(self.memory, AgentMemory):\n+            # Add the system message to the memory\n+            if run_messages.system_message is not None:\n+                self.memory.add_system_message(\n+                    run_messages.system_message, system_message_role=self.system_message_role\n+                )\n+\n+            # Build a list of messages that should be added to the AgentMemory\n+            messages_for_memory: List[Message] = (\n+                [run_messages.user_message] if run_messages.user_message is not None else []\n+            )\n+            # Add messages from messages_for_run after the last user message\n+            for _rm in run_messages.messages[index_of_last_user_message:]:\n+                if _rm.add_to_agent_memory:\n+                    messages_for_memory.append(_rm)\n+            if len(messages_for_memory) > 0:\n+                self.memory.add_messages(messages=messages_for_memory)\n+\n+            # Create an AgentRun object to add to memory\n+            agent_run = AgentRun(response=run_response)\n+            agent_run.message = run_messages.user_message\n+\n+            # Update the memories with the user message if needed\n+            if (\n+                self.memory.create_user_memories\n+                and self.memory.update_user_memories_after_run\n+                and run_messages.user_message is not None\n+            ):\n+                await self.memory.aupdate_memory(input=run_messages.user_message.get_content_string())\n+\n+            if messages is not None and len(messages) > 0:\n+                for _im in messages:\n+                    # Parse the message and convert to a Message object if possible\n+                    mp = None\n+                    if isinstance(_im, Message):\n+                        mp = _im\n+                    elif isinstance(_im, dict):\n+                        try:\n+                            mp = Message(**_im)\n+                        except Exception as e:\n+                            log_warning(f\"Failed to validate message: {e}\")\n+                    else:\n+                        log_warning(f\"Unsupported message type: {type(_im)}\")\n+                        continue\n+\n+                    # Add the message to the AgentRun\n+                    if mp:\n+                        if agent_run.messages is None:\n+                            agent_run.messages = []\n+                        agent_run.messages.append(mp)\n+                        if self.memory.create_user_memories and self.memory.update_user_memories_after_run:\n+                            await self.memory.aupdate_memory(input=mp.get_content_string())\n+                    else:\n+                        log_warning(\"Unable to add message to memory\")\n+            # Add AgentRun to memory\n+            self.memory.add_run(agent_run)\n+            # Update the session summary if needed\n+            if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n+                await self.memory.aupdate_summary()\n+\n+            # 4. Calculate metrics for the run\n+            self.session_metrics = self.calculate_metrics(self.memory.messages)\n+        elif isinstance(self.memory, Memory):\n+            # Add AgentRun to memory\n+            self.memory.add_run(session_id=session_id, run=run_response)\n \n-                # Prepare run messages\n-                run_messages: RunMessages = self.get_run_messages(\n-                    message=message,\n-                    session_id=session_id,\n-                    user_id=user_id,\n-                    audio=audio,\n-                    images=images,\n-                    videos=videos,\n-                    files=files,\n-                    messages=messages,\n-                    **kwargs,\n-                )\n-                if len(run_messages.messages) == 0:\n-                    log_error(\"No messages to be sent to the model.\")\n+            await self._amake_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n \n-                self.run_messages = run_messages\n+            # 4. Calculate metrics for the run\n+            if self.session_metrics is None:\n+                self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n+            else:\n+                self.session_metrics += self.calculate_metrics(\n+                    run_messages.messages\n+                )  # Calculate metrics for the session\n \n+        # 5. Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-                # If a response_model is set, return the response as a structured output\n-                if self.response_model is not None and self.parse_response:\n-                    # Set stream=False and run the agent\n-                    if self.stream and self.stream is True:\n-                        log_debug(\"Setting stream=False as response_model is set\")\n-                        self.stream = False\n-                    rr: RunResponse = next(\n-                        self._run(\n-                            run_response=run_response,\n-                            run_messages=run_messages,\n-                            message=message,\n-                            user_id=user_id,\n-                            session_id=session_id,\n-                            response_format=response_format,\n-                            messages=messages,\n-                        )\n-                    )\n+        # 6. Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n \n-                    # Do a final check confirming the content is in the response_model format\n-                    if isinstance(rr.content, self.response_model):\n-                        return rr\n+        # Log Agent Run\n+        await self._alog_agent_run(user_id=user_id, session_id=session_id)\n \n-                    # Otherwise convert the response to the structured format\n-                    if isinstance(rr.content, str):\n-                        try:\n-                            structured_output = parse_response_model_str(rr.content, self.response_model)\n+        # Otherwise convert the response to the structured format\n+        if self.response_model is not None and not isinstance(run_response.content, self.response_model):\n+            if isinstance(run_response.content, str) and self.parse_response:\n+                try:\n+                    structured_output = parse_response_model_str(run_response.content, self.response_model)\n \n-                            # Update RunResponse\n-                            if structured_output is not None:\n-                                rr.content = structured_output\n-                                rr.content_type = self.response_model.__name__\n-                            else:\n-                                log_warning(\"Failed to convert response to response_model\")\n-                        except Exception as e:\n-                            log_warning(f\"Failed to convert response to output model: {e}\")\n-                    else:\n-                        log_warning(\"Something went wrong. Run response content is not a string\")\n-                    return rr\n-                else:\n-                    if stream and self.is_streamable:\n-                        resp = self._run(\n-                            run_response=run_response,\n-                            run_messages=run_messages,\n-                            message=message,\n-                            user_id=user_id,\n-                            session_id=session_id,\n-                            response_format=response_format,\n-                            messages=messages,\n-                        )\n-                        return resp\n-                    else:\n-                        resp = self._run(\n-                            run_response=run_response,\n-                            run_messages=run_messages,\n-                            message=message,\n-                            user_id=user_id,\n-                            session_id=session_id,\n-                            response_format=response_format,\n-                            messages=messages,\n-                        )\n-                        return next(resp)\n-            except ModelProviderError as e:\n-                log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n-                if isinstance(e, StopAgentRun):\n-                    raise e\n-                last_exception = e\n-                if attempt < num_attempts - 1:  # Don't sleep on the last attempt\n-                    if self.exponential_backoff:\n-                        delay = 2**attempt * self.delay_between_retries\n+                    # Update RunResponse\n+                    if structured_output is not None:\n+                        run_response.content = structured_output\n+                        run_response.content_type = self.response_model.__name__\n                     else:\n-                        delay = self.delay_between_retries\n-                    import time\n+                        log_warning(\"Failed to convert response to response_model\")\n+                except Exception as e:\n+                    log_warning(f\"Failed to convert response to output model: {e}\")\n+            else:\n+                log_warning(\"Something went wrong. Run response content is not a string\")\n \n-                    time.sleep(delay)\n-            except KeyboardInterrupt:\n-                # Create a cancelled response\n-                cancelled_response = RunResponse(\n-                    run_id=self.run_id or str(uuid4()),\n-                    session_id=session_id,\n-                    agent_id=self.agent_id,\n-                    content=\"Operation cancelled by user\",\n-                    event=RunEvent.run_cancelled,\n-                )\n-                return cancelled_response\n+        log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n \n-        # If we get here, all retries failed\n-        if last_exception is not None:\n-            log_error(\n-                f\"Failed after {num_attempts} attempts. Last error using {last_exception.model_name}({last_exception.model_id})\"\n-            )\n-            raise last_exception\n-        else:\n-            raise Exception(f\"Failed after {num_attempts} attempts.\")\n+        return run_response\n \n-    async def _arun(\n+    async def _arun_stream(\n         self,\n-        message: Optional[Union[str, List, Dict, Message]] = None,\n-        *,\n-        stream: bool = False,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n         session_id: str,\n         user_id: Optional[str] = None,\n-        audio: Optional[Sequence[Audio]] = None,\n-        images: Optional[Sequence[Image]] = None,\n-        videos: Optional[Sequence[Video]] = None,\n-        files: Optional[Sequence[File]] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n-        stream_intermediate_steps: bool = False,\n-        knowledge_filters: Optional[Dict[str, Any]] = None,\n-        run_response: RunResponse,\n-        **kwargs: Any,\n+        stream_intermediate_steps: Optional[bool] = None,\n     ) -> AsyncIterator[RunResponse]:\n         \"\"\"Run the Agent and yield the RunResponse.\n \n         Steps:\n-        1. Prepare the Agent for the run\n-        2. Update the Model and resolve context\n-        3. Read existing session from storage\n-        4. Prepare run messages\n-        5. Reason about the task if reasoning is enabled\n-        6. Start the Run by yielding a RunStarted event\n-        7. Generate a response from the Model (includes running function calls)\n-        8. Update RunResponse\n-        9. Update Agent Memory\n-        10. Calculate session metrics\n-        11. Save session to storage\n-        12. Save output to file if save_response_to_file is set\n+        1. Reason about the task if reasoning is enabled\n+        2. Generate a response from the Model (includes running function calls)\n+        3. Update Agent Memory\n+        4. Calculate session metrics\n+        5. Save session to storage\n+        6. Save output to file if save_response_to_file is set\n         \"\"\"\n \n-        # 1. Prepare the Agent for the run\n         if isinstance(self.memory, AgentMemory):\n             self.memory = cast(AgentMemory, self.memory)\n         else:\n             self.memory = cast(Memory, self.memory)\n-        # 1.2 Set streaming and stream intermediate steps\n-        self.stream = self.stream or (stream and self.is_streamable)\n-        self.stream_intermediate_steps = self.stream_intermediate_steps or (stream_intermediate_steps and self.stream)\n-        # 1.3 Create a run_id and RunResponse\n-        self.run_id = str(uuid4())\n-\n-        log_debug(f\"Async Agent Run Start: {run_response.run_id}\", center=True, symbol=\"*\")\n-\n-        # 2.1 Prepare arguments for the model\n-        self.set_default_model()\n-        response_format = self._get_response_format()\n         self.model = cast(Model, self.model)\n \n-        self.determine_tools_for_model(\n-            model=self.model,\n-            session_id=session_id,\n-            user_id=user_id,\n-            async_mode=True,\n-            knowledge_filters=knowledge_filters,\n-        )\n-\n-        run_response.model = self.model.id if self.model is not None else None\n-\n-        # 2.2 Resolve context\n-        if self.context is not None and self.resolve_context:\n-            await self.aresolve_run_context()\n-\n-        # 3. Read existing session from storage\n-        self.read_from_storage(session_id=session_id, user_id=user_id)\n-\n-        # 4. Prepare run messages\n-        run_messages: RunMessages = self.get_run_messages(\n-            message=message,\n-            session_id=session_id,\n-            user_id=user_id,\n-            audio=audio,\n-            images=images,\n-            videos=videos,\n-            files=files,\n-            messages=messages,\n-            **kwargs,\n-        )\n-        if len(run_messages.messages) == 0:\n-            log_error(\"No messages to be sent to the model.\")\n-        self.run_messages = run_messages\n-\n-        # 5. Reason about the task if reasoning is enabled\n+        # 1. Reason about the task if reasoning is enabled\n         if self.reasoning or self.reasoning_model is not None:\n             areason_generator = self.areason(run_messages=run_messages, session_id=session_id)\n-            if self.stream:\n-                async for item in areason_generator:\n-                    yield item\n-            else:\n-                # Consume the generator without yielding\n-                async for _ in areason_generator:\n-                    pass\n+            async for item in areason_generator:\n+                yield item\n \n         # Get the index of the last \"user\" message in messages_for_run\n         # We track this so we can add messages after this index to the RunResponse and Memory\n         index_of_last_user_message = len(run_messages.messages)\n \n-        # 6. Start the Run by yielding a RunStarted event\n-        if self.stream_intermediate_steps:\n+        # Start the Run by yielding a RunStarted event\n+        if stream_intermediate_steps:\n             yield self.create_run_response(\"Run started\", session_id=session_id, event=RunEvent.run_started)\n \n-        # 7. Generate a response from the Model (includes running function calls)\n+        # 2. Generate a response from the Model (includes running function calls)\n         reasoning_started = False\n         reasoning_time_taken = 0.0\n+        model_response = ModelResponse(content=\"\")\n+\n+        model_response_stream = self.model.aresponse_stream(\n+            messages=run_messages.messages,\n+            response_format=response_format,\n+            tools=self._tools_for_model,\n+            functions=self._functions_for_model,\n+            tool_choice=self.tool_choice,\n+            tool_call_limit=self.tool_call_limit,\n+        )  # type: ignore\n+\n+        async for model_response_chunk in model_response_stream:  # type: ignore\n+            # If the model response is an assistant_response, yield a RunResponse\n+            if model_response_chunk.event == ModelResponseEvent.assistant_response.value:\n+                # Process content and thinking\n+                if model_response_chunk.content is not None:\n+                    model_response.content = (model_response.content or \"\") + model_response_chunk.content\n+                    run_response.content = model_response.content\n+\n+                if model_response_chunk.thinking is not None:\n+                    model_response.thinking = (model_response.thinking or \"\") + model_response_chunk.thinking\n+                    run_response.thinking = model_response.thinking\n+\n+                if model_response_chunk.redacted_thinking is not None:\n+                    model_response.redacted_thinking = (\n+                        model_response.redacted_thinking or \"\"\n+                    ) + model_response_chunk.redacted_thinking\n+                    # We only have thinking on response\n+                    run_response.thinking = model_response.redacted_thinking\n \n-        model_response: ModelResponse\n-        self.model = cast(Model, self.model)\n-        if stream and self.is_streamable:\n-            model_response = ModelResponse(content=\"\")\n-            model_response_stream = self.model.aresponse_stream(\n-                messages=run_messages.messages,\n-                response_format=response_format,\n-                tools=self._tools_for_model,\n-                functions=self._functions_for_model,\n-                tool_choice=self.tool_choice,\n-                tool_call_limit=self.tool_call_limit,\n-            )  # type: ignore\n-            async for model_response_chunk in model_response_stream:  # type: ignore\n-                # If the model response is an assistant_response, yield a RunResponse\n-                if model_response_chunk.event == ModelResponseEvent.assistant_response.value:\n-                    # Process content and thinking\n-                    if model_response_chunk.content is not None:\n-                        model_response.content = (model_response.content or \"\") + model_response_chunk.content\n-                        run_response.content = model_response.content\n-\n-                    if model_response_chunk.thinking is not None:\n-                        model_response.thinking = (model_response.thinking or \"\") + model_response_chunk.thinking\n-                        run_response.thinking = model_response.thinking\n-\n-                    if model_response_chunk.redacted_thinking is not None:\n-                        model_response.redacted_thinking = (\n-                            model_response.redacted_thinking or \"\"\n-                        ) + model_response_chunk.redacted_thinking\n-                        # We only have thinking on response\n-                        run_response.thinking = model_response.redacted_thinking\n-\n-                    if model_response_chunk.citations is not None:\n-                        run_response.citations = model_response_chunk.citations\n-\n-                    # Only yield if we have content or thinking to show\n-                    if (\n-                        model_response_chunk.content is not None\n-                        or model_response_chunk.thinking is not None\n-                        or model_response_chunk.redacted_thinking is not None\n-                        or model_response_chunk.citations is not None\n-                    ):\n-                        yield self.create_run_response(\n-                            content=model_response_chunk.content,\n-                            thinking=model_response_chunk.thinking,\n-                            redacted_thinking=model_response_chunk.redacted_thinking,\n-                            citations=model_response_chunk.citations,\n-                            created_at=model_response_chunk.created_at,\n-                            session_id=session_id,\n-                        )\n-\n-                    # Process audio\n-                    if model_response_chunk.audio is not None:\n-                        if model_response.audio is None:\n-                            model_response.audio = AudioResponse(id=str(uuid4()), content=\"\", transcript=\"\")\n-\n-                        if model_response_chunk.audio.id is not None:\n-                            model_response.audio.id = model_response_chunk.audio.id  # type: ignore\n-                        if model_response_chunk.audio.content is not None:\n-                            model_response.audio.content += model_response_chunk.audio.content  # type: ignore\n-                        if model_response_chunk.audio.transcript is not None:\n-                            model_response.audio.transcript += model_response_chunk.audio.transcript  # type: ignore\n-                        if model_response_chunk.audio.expires_at is not None:\n-                            model_response.audio.expires_at = model_response_chunk.audio.expires_at  # type: ignore\n-                        if model_response_chunk.audio.mime_type is not None:\n-                            model_response.audio.mime_type = model_response_chunk.audio.mime_type  # type: ignore\n-                        model_response.audio.sample_rate = model_response_chunk.audio.sample_rate\n-                        model_response.audio.channels = model_response_chunk.audio.channels\n-\n-                        # Yield the audio and transcript bit by bit\n-                        run_response.response_audio = AudioResponse(\n-                            id=model_response_chunk.audio.id,\n-                            content=model_response_chunk.audio.content,\n-                            transcript=model_response_chunk.audio.transcript,\n-                            sample_rate=model_response_chunk.audio.sample_rate,\n-                            channels=model_response_chunk.audio.channels,\n-                        )\n-                        run_response.created_at = model_response_chunk.created_at\n-\n-                        yield run_response\n-\n-                    if model_response_chunk.image is not None:\n-                        self.add_image(model_response_chunk.image)\n-\n-                        yield run_response\n-\n-                # If the model response is a tool_call_started, add the tool call to the run_response\n-                elif model_response_chunk.event == ModelResponseEvent.tool_call_started.value:\n-                    # Add tool calls to the run_response\n-                    new_tool_calls_list = model_response_chunk.tool_calls\n-                    if new_tool_calls_list is not None:\n-                        # Add tool calls to the agent.run_response\n-                        if run_response.tools is None:\n-                            run_response.tools = new_tool_calls_list\n-                        else:\n-                            run_response.tools.extend(new_tool_calls_list)\n-\n-                        # Format tool calls whenever new ones are added during streaming\n-                        run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n+                if model_response_chunk.citations is not None:\n+                    run_response.citations = model_response_chunk.citations\n \n-                    # Yield a RunResponse with the tool_call_started event\n+                # Only yield if we have content or thinking to show\n+                if (\n+                    model_response_chunk.content is not None\n+                    or model_response_chunk.thinking is not None\n+                    or model_response_chunk.redacted_thinking is not None\n+                    or model_response_chunk.citations is not None\n+                ):\n                     yield self.create_run_response(\n                         content=model_response_chunk.content,\n-                        event=RunEvent.tool_call_started,\n+                        thinking=model_response_chunk.thinking,\n+                        redacted_thinking=model_response_chunk.redacted_thinking,\n+                        citations=model_response_chunk.citations,\n                         created_at=model_response_chunk.created_at,\n                         session_id=session_id,\n-                        run_response=run_response,\n                     )\n \n-                # If the model response is a tool_call_completed, update the existing tool call in the run_response\n-                elif model_response_chunk.event == ModelResponseEvent.tool_call_completed.value:\n-                    reasoning_step: Optional[ReasoningStep] = None\n-                    new_tool_calls_list = model_response_chunk.tool_calls\n-                    if new_tool_calls_list is not None:\n-                        # Update the existing tool call in the run_response\n-                        if run_response.tools:\n-                            # Create a mapping of tool_call_id to index\n-                            tool_call_index_map = {\n-                                tc[\"tool_call_id\"]: i\n-                                for i, tc in enumerate(run_response.tools)\n-                                if tc.get(\"tool_call_id\") is not None\n-                            }\n-                            # Process tool calls\n-                            for tool_call_dict in new_tool_calls_list:\n-                                tool_call_id = tool_call_dict.get(\"tool_call_id\")\n-                                index = tool_call_index_map.get(tool_call_id)\n-                                if index is not None:\n-                                    run_response.tools[index] = tool_call_dict\n-                        else:\n-                            run_response.tools = new_tool_calls_list\n+                # Process audio\n+                if model_response_chunk.audio is not None:\n+                    if model_response.audio is None:\n+                        model_response.audio = AudioResponse(id=str(uuid4()), content=\"\", transcript=\"\")\n+\n+                    if model_response_chunk.audio.id is not None:\n+                        model_response.audio.id = model_response_chunk.audio.id  # type: ignore\n+                    if model_response_chunk.audio.content is not None:\n+                        model_response.audio.content += model_response_chunk.audio.content  # type: ignore\n+                    if model_response_chunk.audio.transcript is not None:\n+                        model_response.audio.transcript += model_response_chunk.audio.transcript  # type: ignore\n+                    if model_response_chunk.audio.expires_at is not None:\n+                        model_response.audio.expires_at = model_response_chunk.audio.expires_at  # type: ignore\n+                    if model_response_chunk.audio.mime_type is not None:\n+                        model_response.audio.mime_type = model_response_chunk.audio.mime_type  # type: ignore\n+                    model_response.audio.sample_rate = model_response_chunk.audio.sample_rate\n+                    model_response.audio.channels = model_response_chunk.audio.channels\n+\n+                    # Yield the audio and transcript bit by bit\n+                    run_response.response_audio = AudioResponse(\n+                        id=model_response_chunk.audio.id,\n+                        content=model_response_chunk.audio.content,\n+                        transcript=model_response_chunk.audio.transcript,\n+                        sample_rate=model_response_chunk.audio.sample_rate,\n+                        channels=model_response_chunk.audio.channels,\n+                    )\n+                    run_response.created_at = model_response_chunk.created_at\n \n-                        # Only iterate through new tool calls\n-                        for tool_call in new_tool_calls_list:\n-                            tool_name = tool_call.get(\"tool_name\", \"\")\n-                            if tool_name.lower() in [\"think\", \"analyze\"]:\n-                                tool_args = tool_call.get(\"tool_args\", {})\n+                    yield run_response\n \n-                                reasoning_step = self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n+                if model_response_chunk.image is not None:\n+                    self.add_image(model_response_chunk.image)\n \n-                                metrics = tool_call.get(\"metrics\")\n-                                if metrics is not None and metrics.time is not None:\n-                                    reasoning_time_taken = reasoning_time_taken + float(metrics.time)\n+                    yield run_response\n \n-                    if self.stream_intermediate_steps:\n-                        if reasoning_step is not None:\n-                            if not reasoning_started:\n-                                yield self.create_run_response(\n-                                    content=\"Reasoning started\",\n-                                    event=RunEvent.reasoning_started,\n-                                )\n-                                reasoning_started = True\n+            # If the model response is a tool_call_started, add the tool call to the run_response\n+            elif model_response_chunk.event == ModelResponseEvent.tool_call_started.value:\n+                # Add tool calls to the run_response\n+                new_tool_calls_list = model_response_chunk.tool_calls\n+                if new_tool_calls_list is not None:\n+                    # Add tool calls to the agent.run_response\n+                    if run_response.tools is None:\n+                        run_response.tools = new_tool_calls_list\n+                    else:\n+                        run_response.tools.extend(new_tool_calls_list)\n \n-                            yield self.create_run_response(\n-                                content=reasoning_step,\n-                                content_type=reasoning_step.__class__.__name__,\n-                                event=RunEvent.reasoning_step,\n-                                reasoning_content=run_response.reasoning_content,\n-                            )\n+                    # Format tool calls whenever new ones are added during streaming\n+                    run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n \n-                    # Yield a RunResponse with the tool_call_completed event\n-                    yield self.create_run_response(\n-                        content=model_response_chunk.content,\n-                        event=RunEvent.tool_call_completed,\n-                        created_at=model_response_chunk.created_at,\n-                        session_id=session_id,\n-                        run_response=run_response,\n-                    )\n+                # Yield a RunResponse with the tool_call_started event\n+                yield self.create_run_response(\n+                    content=model_response_chunk.content,\n+                    event=RunEvent.tool_call_started,\n+                    created_at=model_response_chunk.created_at,\n+                    session_id=session_id,\n+                    run_response=run_response,\n+                )\n \n-        else:\n-            # Get the model response\n-            model_response = await self.model.aresponse(\n-                messages=run_messages.messages,\n-                response_format=response_format,\n-                tools=self._tools_for_model,\n-                functions=self._functions_for_model,\n-                tool_choice=self.tool_choice,\n-                tool_call_limit=self.tool_call_limit,\n-            )\n-            # Format tool calls if they exist\n-            if model_response.tool_calls:\n-                run_response.formatted_tool_calls = format_tool_calls(model_response.tool_calls)\n-\n-            # Handle structured outputs\n-            if self.response_model is not None and model_response.parsed is not None:\n-                # We get native structured outputs from the model\n-                if self._model_should_return_structured_output():\n-                    # Update the run_response content with the structured output\n-                    run_response.content = model_response.parsed\n-                    # Update the run_response content_type with the structured output class name\n-                    run_response.content_type = self.response_model.__name__\n-            else:\n-                # Update the run_response content with the model response content\n-                run_response.content = model_response.content\n-\n-            # Update the run_response thinking with the model response thinking\n-            if model_response.thinking is not None:\n-                run_response.thinking = model_response.thinking\n-            if model_response.redacted_thinking is not None:\n-                if run_response.thinking is None:\n-                    run_response.thinking = model_response.redacted_thinking\n-                else:\n-                    run_response.thinking += model_response.redacted_thinking\n+            # If the model response is a tool_call_completed, update the existing tool call in the run_response\n+            elif model_response_chunk.event == ModelResponseEvent.tool_call_completed.value:\n+                reasoning_step: Optional[ReasoningStep] = None\n+                new_tool_calls_list = model_response_chunk.tool_calls\n+                if new_tool_calls_list is not None:\n+                    # Update the existing tool call in the run_response\n+                    if run_response.tools:\n+                        # Create a mapping of tool_call_id to index\n+                        tool_call_index_map = {\n+                            tc[\"tool_call_id\"]: i\n+                            for i, tc in enumerate(run_response.tools)\n+                            if tc.get(\"tool_call_id\") is not None\n+                        }\n+                        # Process tool calls\n+                        for tool_call_dict in new_tool_calls_list:\n+                            tool_call_id = tool_call_dict.get(\"tool_call_id\")\n+                            index = tool_call_index_map.get(tool_call_id)\n+                            if index is not None:\n+                                run_response.tools[index] = tool_call_dict\n+                    else:\n+                        run_response.tools = new_tool_calls_list\n \n-            if model_response.citations is not None:\n-                run_response.citations = model_response.citations\n+                    # Only iterate through new tool calls\n+                    for tool_call in new_tool_calls_list:\n+                        tool_name = tool_call.get(\"tool_name\", \"\")\n+                        if tool_name.lower() in [\"think\", \"analyze\"]:\n+                            tool_args = tool_call.get(\"tool_args\", {})\n \n-            # Update the run_response tools with the model response tools\n-            if model_response.tool_calls is not None:\n-                if run_response.tools is None:\n-                    run_response.tools = model_response.tool_calls\n-                else:\n-                    run_response.tools.extend(model_response.tool_calls)\n+                            reasoning_step = self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n \n-                # For Reasoning/Thinking/Knowledge Tools update reasoning_content in RunResponse\n-                for tool_call in model_response.tool_calls:\n-                    tool_name = tool_call.get(\"tool_name\", \"\")\n-                    if tool_name.lower() in [\"think\", \"analyze\"]:\n-                        tool_args = tool_call.get(\"tool_args\", {})\n-                        self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n+                            metrics = tool_call.get(\"metrics\")\n+                            if metrics is not None and metrics.time is not None:\n+                                reasoning_time_taken = reasoning_time_taken + float(metrics.time)\n \n-            # Update the run_response audio with the model response audio\n-            if model_response.audio is not None:\n-                run_response.response_audio = model_response.audio\n+                if stream_intermediate_steps:\n+                    if reasoning_step is not None:\n+                        if not reasoning_started:\n+                            yield self.create_run_response(\n+                                content=\"Reasoning started\",\n+                                event=RunEvent.reasoning_started,\n+                            )\n+                            reasoning_started = True\n \n-            if model_response.image is not None:\n-                self.add_image(model_response.image)\n+                        yield self.create_run_response(\n+                            content=reasoning_step,\n+                            content_type=reasoning_step.__class__.__name__,\n+                            event=RunEvent.reasoning_step,\n+                            reasoning_content=run_response.reasoning_content,\n+                        )\n \n-            # Update the run_response messages with the messages\n-            run_response.messages = run_messages.messages\n-            # Update the run_response created_at with the model response created_at\n-            run_response.created_at = model_response.created_at\n+                # Yield a RunResponse with the tool_call_completed event\n+                yield self.create_run_response(\n+                    content=model_response_chunk.content,\n+                    event=RunEvent.tool_call_completed,\n+                    created_at=model_response_chunk.created_at,\n+                    session_id=session_id,\n+                    run_response=run_response,\n+                )\n \n-        if self.stream_intermediate_steps and reasoning_started:\n+        if stream_intermediate_steps and reasoning_started:\n             all_reasoning_steps: List[ReasoningStep] = []\n             if run_response and run_response.extra_data and hasattr(run_response.extra_data, \"reasoning_steps\"):\n                 all_reasoning_steps = cast(List[ReasoningStep], run_response.extra_data.reasoning_steps)\n@@ -1631,7 +1807,7 @@ class Agent:\n                     event=RunEvent.reasoning_completed,\n                 )\n \n-        # 8. Update RunResponse\n+        # Update RunResponse\n         # Build a list of messages that should be added to the RunResponse\n         messages_for_run_response = [m for m in run_messages.messages if m.add_to_agent_memory]\n         # Update the RunResponse messages\n@@ -1640,10 +1816,10 @@ class Agent:\n         run_response.metrics = self.aggregate_metrics_from_messages(messages_for_run_response)\n \n         # Update the run_response audio if streaming\n-        if self.stream and model_response.audio is not None:\n+        if model_response.audio is not None:\n             run_response.response_audio = model_response.audio\n \n-        # 9. Update Agent Memory\n+        # 3. Update Agent Memory\n         if isinstance(self.memory, AgentMemory):\n             # Add the system message to the memory\n             if run_messages.system_message is not None:\n@@ -1704,6 +1880,7 @@ class Agent:\n             if self.memory.create_session_summary and self.memory.update_session_summary_after_run:\n                 await self.memory.aupdate_summary()\n \n+            # 4. Calculate metrics for the run\n             self.session_metrics = self.calculate_metrics(self.memory.messages)\n         elif isinstance(self.memory, Memory):\n             # Add AgentRun to memory\n@@ -1711,6 +1888,7 @@ class Agent:\n \n             await self._amake_memories_and_summaries(run_messages, session_id, user_id, messages)  # type: ignore\n \n+            # 4. Calculate metrics for the run\n             if self.session_metrics is None:\n                 self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n             else:\n@@ -1719,35 +1897,24 @@ class Agent:\n                 )  # Calculate metrics for the session\n \n         # Yield UpdatingMemory event\n-        if self.stream_intermediate_steps:\n+        if stream_intermediate_steps:\n             yield self.create_run_response(\n                 content=\"Memory updated\",\n                 session_id=session_id,\n                 event=RunEvent.updating_memory,\n             )\n \n-        # 11. Save session to storage\n+        # 5. Save session to storage\n         self.write_to_storage(user_id=user_id, session_id=session_id)\n \n-        # 12. Save output to file if save_response_to_file is set\n+        # 6. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=message, session_id=session_id)\n \n-        # Set run_input\n-        if message is not None:\n-            if isinstance(message, str):\n-                self.run_input = message\n-            elif isinstance(message, Message):\n-                self.run_input = message.to_dict()\n-            else:\n-                self.run_input = message\n-        elif messages is not None:\n-            self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n-\n         # Log Agent Run\n         await self._alog_agent_run(user_id=user_id, session_id=session_id)\n \n         log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n-        if self.stream_intermediate_steps:\n+        if stream_intermediate_steps:\n             yield self.create_run_response(\n                 content=run_response.content,\n                 reasoning_content=run_response.reasoning_content,\n@@ -1756,10 +1923,6 @@ class Agent:\n                 run_response=run_response,\n             )\n \n-        # Yield final response if not streaming so that run() can get the response\n-        if not self.stream:\n-            yield run_response\n-\n     async def arun(\n         self,\n         message: Optional[Union[str, List, Dict, Message]] = None,\n@@ -1818,6 +1981,9 @@ class Agent:\n         if stream is False:\n             stream_intermediate_steps = False\n \n+        self.stream = self.stream or (stream and self.is_streamable)\n+        self.stream_intermediate_steps = self.stream_intermediate_steps or (stream_intermediate_steps and self.stream)\n+\n         # Use the default user_id and session_id when necessary\n         if user_id is None:\n             user_id = self.user_id\n@@ -1836,9 +2002,34 @@ class Agent:\n \n         log_debug(f\"Session ID: {session_id}\", center=True)\n \n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id, user_id=user_id)\n+\n+        # Read existing session from storage\n+        if self.context is not None:\n+            self.resolve_run_context()\n+\n+        if self.response_model is not None and self.parse_response and stream is True:\n+            # Disable stream if response_model is set\n+            stream = False\n+            log_debug(\"Disabling stream as response_model is set\")\n+\n         last_exception = None\n         num_attempts = retries + 1\n \n+        # Prepare arguments for the model\n+        self.set_default_model()\n+        response_format = self._get_response_format()\n+        self.model = cast(Model, self.model)\n+\n+        self.determine_tools_for_model(\n+            model=self.model,\n+            session_id=session_id,\n+            user_id=user_id,\n+            async_mode=False,\n+            knowledge_filters=effective_filters,\n+        )\n+\n         # Create a run_id for this specific run\n         run_id = str(uuid4())\n \n@@ -1847,87 +2038,65 @@ class Agent:\n                 # Create a new run_response for this attempt\n                 run_response = RunResponse(run_id=run_id, session_id=session_id, agent_id=self.agent_id)\n \n+                run_response.model = self.model.id if self.model is not None else None\n+\n                 # for backward compatibility, set self.run_response\n                 self.run_response = run_response\n+                self.run_id = run_id\n+\n+                # Set run_input\n+                if message is not None:\n+                    if isinstance(message, str):\n+                        self.run_input = message\n+                    elif isinstance(message, Message):\n+                        self.run_input = message.to_dict()\n+                    else:\n+                        self.run_input = message\n+                elif messages is not None:\n+                    self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n+\n+                log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+\n+                # Prepare run messages\n+                run_messages: RunMessages = self.get_run_messages(\n+                    message=message,\n+                    session_id=session_id,\n+                    user_id=user_id,\n+                    audio=audio,\n+                    images=images,\n+                    videos=videos,\n+                    files=files,\n+                    messages=messages,\n+                    **kwargs,\n+                )\n+                if len(run_messages.messages) == 0:\n+                    log_error(\"No messages to be sent to the model.\")\n \n-                # If a response_model is set, return the response as a structured output\n-                if self.response_model is not None and self.parse_response:\n-                    # Set stream=False and run the agent\n-                    if self.stream and self.stream is True:\n-                        log_debug(\"Setting stream=False as response_model is set\")\n-                        self.stream = False\n+                self.run_messages = run_messages\n \n-                    resp = await self._arun(\n+                # Pass the new run_response to _arun\n+                if stream and self.is_streamable:\n+                    resp = self._arun_stream(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n                         message=message,\n-                        stream=False,\n                         user_id=user_id,\n                         session_id=session_id,\n-                        audio=audio,\n-                        images=images,\n-                        videos=videos,\n-                        files=files,\n+                        response_format=response_format,\n                         messages=messages,\n                         stream_intermediate_steps=stream_intermediate_steps,\n-                        knowledge_filters=effective_filters,\n-                        run_response=run_response,\n-                        **kwargs,\n-                    ).__anext__()\n-\n-                    # Do a final check confirming the content is in the response_model format\n-                    if isinstance(resp.content, self.response_model):\n-                        return resp\n-\n-                    # Otherwise convert the response to the structured format\n-                    if isinstance(resp.content, str):\n-                        try:\n-                            structured_output = parse_response_model_str(resp.content, self.response_model)\n-\n-                            # Update RunResponse\n-                            if structured_output is not None:\n-                                resp.content = structured_output\n-                                resp.content_type = self.response_model.__name__\n-                            else:\n-                                log_warning(\"Failed to convert response to response_model\")\n-                        except Exception as e:\n-                            log_warning(f\"Failed to convert response to output model: {e}\")\n-                    else:\n-                        log_warning(\"Something went wrong. Run response content is not a string\")\n+                    )  # type: ignore[assignment]\n                     return resp\n                 else:\n-                    # Pass the new run_response to _arun\n-                    if stream and self.is_streamable:\n-                        resp = self._arun(\n-                            message=message,\n-                            stream=True,\n-                            user_id=user_id,\n-                            session_id=session_id,\n-                            audio=audio,\n-                            images=images,\n-                            videos=videos,\n-                            files=files,\n-                            messages=messages,\n-                            stream_intermediate_steps=stream_intermediate_steps,\n-                            knowledge_filters=effective_filters,\n-                            run_response=run_response,\n-                            **kwargs,\n-                        )  # type: ignore[assignment]\n-                        return resp\n-                    else:\n-                        return await self._arun(\n-                            message=message,\n-                            stream=False,\n-                            user_id=user_id,\n-                            session_id=session_id,\n-                            audio=audio,\n-                            images=images,\n-                            videos=videos,\n-                            files=files,\n-                            messages=messages,\n-                            stream_intermediate_steps=stream_intermediate_steps,\n-                            knowledge_filters=effective_filters,\n-                            run_response=run_response,\n-                            **kwargs,\n-                        ).__anext__()  # type: ignore[assignment]\n+                    return await self._arun(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                    )\n             except ModelProviderError as e:\n                 log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n                 if isinstance(e, StopAgentRun):\n@@ -3630,7 +3799,7 @@ class Agent:\n \n             log_debug(f\"Searching knowledge base with filters: {filters}\")\n             relevant_docs: List[Document] = await self.knowledge.async_search(\n-                query=query, num_documents=num_documents, filters=filters, **kwargs\n+                query=query, num_documents=num_documents, filters=filters\n             )\n \n             if not relevant_docs or len(relevant_docs) == 0:\n"
        },
        {
            "commit": "aef89d7979ff37db1e5e5a75265767e0c460b0ba",
            "file_path": "libs/agno/agno/playground/async_router.py",
            "diff": "diff --git a/libs/agno/agno/playground/async_router.py b/libs/agno/agno/playground/async_router.py\nindex e67cf9ab9..4e54b86f6 100644\n--- a/libs/agno/agno/playground/async_router.py\n+++ b/libs/agno/agno/playground/async_router.py\n@@ -72,6 +72,7 @@ async def chat_response_streamer(\n             yield run_response_chunk.to_json()\n     except Exception as e:\n         import traceback\n+\n         traceback.print_exc(limit=3)\n         error_response = RunResponse(\n             content=str(e),\n"
        }
    ]
}