{
    "sha_fail": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
    "changed_files": [
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "examples/realtime/audio_util.py",
            "diff": "diff --git a/examples/realtime/audio_util.py b/examples/realtime/audio_util.py\nindex b073cc4..954a508 100644\n--- a/examples/realtime/audio_util.py\n+++ b/examples/realtime/audio_util.py\n@@ -11,7 +11,7 @@ import pyaudio\n import sounddevice as sd\n from pydub import AudioSegment\n \n-from openai.resources.beta.realtime.realtime import AsyncRealtimeConnection\n+from openai.resources.realtime.realtime import AsyncRealtimeConnection\n \n CHUNK_LENGTH_S = 0.05  # 100ms\n SAMPLE_RATE = 24000\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "examples/realtime/azure_realtime.py",
            "diff": "diff --git a/examples/realtime/azure_realtime.py b/examples/realtime/azure_realtime.py\nindex de88d47..3cf64b8 100644\n--- a/examples/realtime/azure_realtime.py\n+++ b/examples/realtime/azure_realtime.py\n@@ -26,10 +26,16 @@ async def main() -> None:\n         azure_ad_token_provider=get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\"),\n         api_version=\"2024-10-01-preview\",\n     )\n-    async with client.beta.realtime.connect(\n-        model=\"gpt-4o-realtime-preview\",  # deployment name for your model\n+    async with client.realtime.connect(\n+        model=\"gpt-realtime\",  # deployment name for your model\n     ) as connection:\n-        await connection.session.update(session={\"modalities\": [\"text\"]})  # type: ignore\n+        await connection.session.update(\n+            session={\n+                \"output_modalities\": [\"text\"],\n+                \"model\": \"gpt-realtime\",\n+                \"type\": \"realtime\",\n+            }\n+        )\n         while True:\n             user_input = input(\"Enter a message: \")\n             if user_input == \"q\":\n@@ -44,9 +50,9 @@ async def main() -> None:\n             )\n             await connection.response.create()\n             async for event in connection:\n-                if event.type == \"response.text.delta\":\n+                if event.type == \"response.output_text.delta\":\n                     print(event.delta, flush=True, end=\"\")\n-                elif event.type == \"response.text.done\":\n+                elif event.type == \"response.output_text.done\":\n                     print()\n                 elif event.type == \"response.done\":\n                     break\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "examples/realtime/push_to_talk_app.py",
            "diff": "diff --git a/examples/realtime/push_to_talk_app.py b/examples/realtime/push_to_talk_app.py\nindex 02d3f76..acf3899 100755\n--- a/examples/realtime/push_to_talk_app.py\n+++ b/examples/realtime/push_to_talk_app.py\n@@ -38,8 +38,8 @@ from textual.reactive import reactive\n from textual.containers import Container\n \n from openai import AsyncOpenAI\n-from openai.types.beta.realtime.session import Session\n-from openai.resources.beta.realtime.realtime import AsyncRealtimeConnection\n+from openai.types.realtime.session import Session\n+from openai.resources.realtime.realtime import AsyncRealtimeConnection\n \n \n class SessionDisplay(Static):\n@@ -154,13 +154,21 @@ class RealtimeApp(App[None]):\n         self.run_worker(self.send_mic_audio())\n \n     async def handle_realtime_connection(self) -> None:\n-        async with self.client.beta.realtime.connect(model=\"gpt-4o-realtime-preview\") as conn:\n+        async with self.client.realtime.connect(model=\"gpt-realtime\") as conn:\n             self.connection = conn\n             self.connected.set()\n \n             # note: this is the default and can be omitted\n             # if you want to manually handle VAD yourself, then set `'turn_detection': None`\n-            await conn.session.update(session={\"turn_detection\": {\"type\": \"server_vad\"}})\n+            await conn.session.update(\n+                session={\n+                    \"audio\": {\n+                        \"input\": {\"turn_detection\": {\"type\": \"server_vad\"}},\n+                    },\n+                    \"model\": \"gpt-realtime\",\n+                    \"type\": \"realtime\",\n+                }\n+            )\n \n             acc_items: dict[str, Any] = {}\n \n@@ -176,7 +184,7 @@ class RealtimeApp(App[None]):\n                     self.session = event.session\n                     continue\n \n-                if event.type == \"response.audio.delta\":\n+                if event.type == \"response.output_audio.delta\":\n                     if event.item_id != self.last_audio_item_id:\n                         self.audio_player.reset_frame_count()\n                         self.last_audio_item_id = event.item_id\n@@ -185,7 +193,7 @@ class RealtimeApp(App[None]):\n                     self.audio_player.add_data(bytes_data)\n                     continue\n \n-                if event.type == \"response.audio_transcript.delta\":\n+                if event.type == \"response.output_audio_transcript.delta\":\n                     try:\n                         text = acc_items[event.item_id]\n                     except KeyError:\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "examples/realtime/realtime.py",
            "diff": "diff --git a/examples/realtime/realtime.py b/examples/realtime/realtime.py\nnew file mode 100755\nindex 0000000..214961e\n--- /dev/null\n+++ b/examples/realtime/realtime.py\n@@ -0,0 +1,54 @@\n+#!/usr/bin/env rye run python\n+import asyncio\n+\n+from openai import AsyncOpenAI\n+\n+# Azure OpenAI Realtime Docs\n+\n+# How-to: https://learn.microsoft.com/azure/ai-services/openai/how-to/realtime-audio\n+# Supported models and API versions: https://learn.microsoft.com/azure/ai-services/openai/how-to/realtime-audio#supported-models\n+# Entra ID auth: https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity\n+\n+\n+async def main() -> None:\n+    \"\"\"The following example demonstrates how to configure OpenAI to use the Realtime API.\n+    For an audio example, see push_to_talk_app.py and update the client and model parameter accordingly.\n+\n+    When prompted for user input, type a message and hit enter to send it to the model.\n+    Enter \"q\" to quit the conversation.\n+    \"\"\"\n+\n+    client = AsyncOpenAI()\n+    async with client.realtime.connect(\n+        model=\"gpt-realtime\",\n+    ) as connection:\n+        await connection.session.update(\n+            session={\n+                \"output_modalities\": [\"text\"],\n+                \"model\": \"gpt-realtime\",\n+                \"type\": \"realtime\",\n+            }\n+        )\n+        while True:\n+            user_input = input(\"Enter a message: \")\n+            if user_input == \"q\":\n+                break\n+\n+            await connection.conversation.item.create(\n+                item={\n+                    \"type\": \"message\",\n+                    \"role\": \"user\",\n+                    \"content\": [{\"type\": \"input_text\", \"text\": user_input}],\n+                }\n+            )\n+            await connection.response.create()\n+            async for event in connection:\n+                if event.type == \"response.output_text.delta\":\n+                    print(event.delta, flush=True, end=\"\")\n+                elif event.type == \"response.output_text.done\":\n+                    print()\n+                elif event.type == \"response.done\":\n+                    break\n+\n+\n+asyncio.run(main())\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/__init__.py",
            "diff": "diff --git a/src/openai/__init__.py b/src/openai/__init__.py\nindex b944fbe..a03b49e 100644\n--- a/src/openai/__init__.py\n+++ b/src/openai/__init__.py\n@@ -379,6 +379,7 @@ from ._module_client import (\n     models as models,\n     batches as batches,\n     uploads as uploads,\n+    realtime as realtime,\n     webhooks as webhooks,\n     responses as responses,\n     containers as containers,\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/_client.py",
            "diff": "diff --git a/src/openai/_client.py b/src/openai/_client.py\nindex b99db78..fe5ebac 100644\n--- a/src/openai/_client.py\n+++ b/src/openai/_client.py\n@@ -45,6 +45,7 @@ if TYPE_CHECKING:\n         models,\n         batches,\n         uploads,\n+        realtime,\n         responses,\n         containers,\n         embeddings,\n@@ -67,6 +68,7 @@ if TYPE_CHECKING:\n     from .resources.evals.evals import Evals, AsyncEvals\n     from .resources.moderations import Moderations, AsyncModerations\n     from .resources.uploads.uploads import Uploads, AsyncUploads\n+    from .resources.realtime.realtime import Realtime, AsyncRealtime\n     from .resources.responses.responses import Responses, AsyncResponses\n     from .resources.containers.containers import Containers, AsyncContainers\n     from .resources.fine_tuning.fine_tuning import FineTuning, AsyncFineTuning\n@@ -256,6 +258,12 @@ class OpenAI(SyncAPIClient):\n \n         return Responses(self)\n \n+    @cached_property\n+    def realtime(self) -> Realtime:\n+        from .resources.realtime import Realtime\n+\n+        return Realtime(self)\n+\n     @cached_property\n     def conversations(self) -> Conversations:\n         from .resources.conversations import Conversations\n@@ -581,6 +589,12 @@ class AsyncOpenAI(AsyncAPIClient):\n \n         return AsyncResponses(self)\n \n+    @cached_property\n+    def realtime(self) -> AsyncRealtime:\n+        from .resources.realtime import AsyncRealtime\n+\n+        return AsyncRealtime(self)\n+\n     @cached_property\n     def conversations(self) -> AsyncConversations:\n         from .resources.conversations import AsyncConversations\n@@ -816,6 +830,12 @@ class OpenAIWithRawResponse:\n \n         return ResponsesWithRawResponse(self._client.responses)\n \n+    @cached_property\n+    def realtime(self) -> realtime.RealtimeWithRawResponse:\n+        from .resources.realtime import RealtimeWithRawResponse\n+\n+        return RealtimeWithRawResponse(self._client.realtime)\n+\n     @cached_property\n     def conversations(self) -> conversations.ConversationsWithRawResponse:\n         from .resources.conversations import ConversationsWithRawResponse\n@@ -925,6 +945,12 @@ class AsyncOpenAIWithRawResponse:\n \n         return AsyncResponsesWithRawResponse(self._client.responses)\n \n+    @cached_property\n+    def realtime(self) -> realtime.AsyncRealtimeWithRawResponse:\n+        from .resources.realtime import AsyncRealtimeWithRawResponse\n+\n+        return AsyncRealtimeWithRawResponse(self._client.realtime)\n+\n     @cached_property\n     def conversations(self) -> conversations.AsyncConversationsWithRawResponse:\n         from .resources.conversations import AsyncConversationsWithRawResponse\n@@ -1034,6 +1060,12 @@ class OpenAIWithStreamedResponse:\n \n         return ResponsesWithStreamingResponse(self._client.responses)\n \n+    @cached_property\n+    def realtime(self) -> realtime.RealtimeWithStreamingResponse:\n+        from .resources.realtime import RealtimeWithStreamingResponse\n+\n+        return RealtimeWithStreamingResponse(self._client.realtime)\n+\n     @cached_property\n     def conversations(self) -> conversations.ConversationsWithStreamingResponse:\n         from .resources.conversations import ConversationsWithStreamingResponse\n@@ -1143,6 +1175,12 @@ class AsyncOpenAIWithStreamedResponse:\n \n         return AsyncResponsesWithStreamingResponse(self._client.responses)\n \n+    @cached_property\n+    def realtime(self) -> realtime.AsyncRealtimeWithStreamingResponse:\n+        from .resources.realtime import AsyncRealtimeWithStreamingResponse\n+\n+        return AsyncRealtimeWithStreamingResponse(self._client.realtime)\n+\n     @cached_property\n     def conversations(self) -> conversations.AsyncConversationsWithStreamingResponse:\n         from .resources.conversations import AsyncConversationsWithStreamingResponse\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/_module_client.py",
            "diff": "diff --git a/src/openai/_module_client.py b/src/openai/_module_client.py\nindex 5c8df24..4ecc284 100644\n--- a/src/openai/_module_client.py\n+++ b/src/openai/_module_client.py\n@@ -19,6 +19,7 @@ if TYPE_CHECKING:\n     from .resources.evals.evals import Evals\n     from .resources.moderations import Moderations\n     from .resources.uploads.uploads import Uploads\n+    from .resources.realtime.realtime import Realtime\n     from .resources.responses.responses import Responses\n     from .resources.containers.containers import Containers\n     from .resources.fine_tuning.fine_tuning import FineTuning\n@@ -89,6 +90,12 @@ class WebhooksProxy(LazyProxy[\"Webhooks\"]):\n         return _load_client().webhooks\n \n \n+class RealtimeProxy(LazyProxy[\"Realtime\"]):\n+    @override\n+    def __load__(self) -> Realtime:\n+        return _load_client().realtime\n+\n+\n class ResponsesProxy(LazyProxy[\"Responses\"]):\n     @override\n     def __load__(self) -> Responses:\n@@ -147,6 +154,7 @@ models: Models = ModelsProxy().__as_proxied__()\n batches: Batches = BatchesProxy().__as_proxied__()\n uploads: Uploads = UploadsProxy().__as_proxied__()\n webhooks: Webhooks = WebhooksProxy().__as_proxied__()\n+realtime: Realtime = RealtimeProxy().__as_proxied__()\n responses: Responses = ResponsesProxy().__as_proxied__()\n embeddings: Embeddings = EmbeddingsProxy().__as_proxied__()\n containers: Containers = ContainersProxy().__as_proxied__()\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/resources/audio/speech.py",
            "diff": "diff --git a/src/openai/resources/audio/speech.py b/src/openai/resources/audio/speech.py\nindex 6251cfe..64ce5ee 100644\n--- a/src/openai/resources/audio/speech.py\n+++ b/src/openai/resources/audio/speech.py\n@@ -50,7 +50,9 @@ class Speech(SyncAPIResource):\n         *,\n         input: str,\n         model: Union[str, SpeechModel],\n-        voice: Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\"]],\n+        voice: Union[\n+            str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"]\n+        ],\n         instructions: str | NotGiven = NOT_GIVEN,\n         response_format: Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"] | NotGiven = NOT_GIVEN,\n         speed: float | NotGiven = NOT_GIVEN,\n@@ -144,7 +146,9 @@ class AsyncSpeech(AsyncAPIResource):\n         *,\n         input: str,\n         model: Union[str, SpeechModel],\n-        voice: Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\"]],\n+        voice: Union[\n+            str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"]\n+        ],\n         instructions: str | NotGiven = NOT_GIVEN,\n         response_format: Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"] | NotGiven = NOT_GIVEN,\n         speed: float | NotGiven = NOT_GIVEN,\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/resources/beta/beta.py",
            "diff": "diff --git a/src/openai/resources/beta/beta.py b/src/openai/resources/beta/beta.py\nindex 4feaaab..9084c47 100644\n--- a/src/openai/resources/beta/beta.py\n+++ b/src/openai/resources/beta/beta.py\n@@ -24,10 +24,6 @@ from ...resources.chat import Chat, AsyncChat\n from .realtime.realtime import (\n     Realtime,\n     AsyncRealtime,\n-    RealtimeWithRawResponse,\n-    AsyncRealtimeWithRawResponse,\n-    RealtimeWithStreamingResponse,\n-    AsyncRealtimeWithStreamingResponse,\n )\n \n __all__ = [\"Beta\", \"AsyncBeta\"]\n@@ -111,10 +107,6 @@ class BetaWithRawResponse:\n     def __init__(self, beta: Beta) -> None:\n         self._beta = beta\n \n-    @cached_property\n-    def realtime(self) -> RealtimeWithRawResponse:\n-        return RealtimeWithRawResponse(self._beta.realtime)\n-\n     @cached_property\n     def assistants(self) -> AssistantsWithRawResponse:\n         return AssistantsWithRawResponse(self._beta.assistants)\n@@ -128,10 +120,6 @@ class AsyncBetaWithRawResponse:\n     def __init__(self, beta: AsyncBeta) -> None:\n         self._beta = beta\n \n-    @cached_property\n-    def realtime(self) -> AsyncRealtimeWithRawResponse:\n-        return AsyncRealtimeWithRawResponse(self._beta.realtime)\n-\n     @cached_property\n     def assistants(self) -> AsyncAssistantsWithRawResponse:\n         return AsyncAssistantsWithRawResponse(self._beta.assistants)\n@@ -145,10 +133,6 @@ class BetaWithStreamingResponse:\n     def __init__(self, beta: Beta) -> None:\n         self._beta = beta\n \n-    @cached_property\n-    def realtime(self) -> RealtimeWithStreamingResponse:\n-        return RealtimeWithStreamingResponse(self._beta.realtime)\n-\n     @cached_property\n     def assistants(self) -> AssistantsWithStreamingResponse:\n         return AssistantsWithStreamingResponse(self._beta.assistants)\n@@ -162,10 +146,6 @@ class AsyncBetaWithStreamingResponse:\n     def __init__(self, beta: AsyncBeta) -> None:\n         self._beta = beta\n \n-    @cached_property\n-    def realtime(self) -> AsyncRealtimeWithStreamingResponse:\n-        return AsyncRealtimeWithStreamingResponse(self._beta.realtime)\n-\n     @cached_property\n     def assistants(self) -> AsyncAssistantsWithStreamingResponse:\n         return AsyncAssistantsWithStreamingResponse(self._beta.assistants)\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/resources/realtime/__init__.py",
            "diff": "diff --git a/src/openai/resources/realtime/__init__.py b/src/openai/resources/realtime/__init__.py\nnew file mode 100644\nindex 0000000..7a41de8\n--- /dev/null\n+++ b/src/openai/resources/realtime/__init__.py\n@@ -0,0 +1,33 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from .realtime import (\n+    Realtime,\n+    AsyncRealtime,\n+    RealtimeWithRawResponse,\n+    AsyncRealtimeWithRawResponse,\n+    RealtimeWithStreamingResponse,\n+    AsyncRealtimeWithStreamingResponse,\n+)\n+from .client_secrets import (\n+    ClientSecrets,\n+    AsyncClientSecrets,\n+    ClientSecretsWithRawResponse,\n+    AsyncClientSecretsWithRawResponse,\n+    ClientSecretsWithStreamingResponse,\n+    AsyncClientSecretsWithStreamingResponse,\n+)\n+\n+__all__ = [\n+    \"ClientSecrets\",\n+    \"AsyncClientSecrets\",\n+    \"ClientSecretsWithRawResponse\",\n+    \"AsyncClientSecretsWithRawResponse\",\n+    \"ClientSecretsWithStreamingResponse\",\n+    \"AsyncClientSecretsWithStreamingResponse\",\n+    \"Realtime\",\n+    \"AsyncRealtime\",\n+    \"RealtimeWithRawResponse\",\n+    \"AsyncRealtimeWithRawResponse\",\n+    \"RealtimeWithStreamingResponse\",\n+    \"AsyncRealtimeWithStreamingResponse\",\n+]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/resources/realtime/client_secrets.py",
            "diff": "diff --git a/src/openai/resources/realtime/client_secrets.py b/src/openai/resources/realtime/client_secrets.py\nnew file mode 100644\nindex 0000000..ba0f9ee\n--- /dev/null\n+++ b/src/openai/resources/realtime/client_secrets.py\n@@ -0,0 +1,185 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+import httpx\n+\n+from ... import _legacy_response\n+from ..._types import NOT_GIVEN, Body, Query, Headers, NotGiven\n+from ..._utils import maybe_transform, async_maybe_transform\n+from ..._compat import cached_property\n+from ..._resource import SyncAPIResource, AsyncAPIResource\n+from ..._response import to_streamed_response_wrapper, async_to_streamed_response_wrapper\n+from ..._base_client import make_request_options\n+from ...types.realtime import client_secret_create_params\n+from ...types.realtime.client_secret_create_response import ClientSecretCreateResponse\n+\n+__all__ = [\"ClientSecrets\", \"AsyncClientSecrets\"]\n+\n+\n+class ClientSecrets(SyncAPIResource):\n+    @cached_property\n+    def with_raw_response(self) -> ClientSecretsWithRawResponse:\n+        \"\"\"\n+        This property can be used as a prefix for any HTTP method call to return\n+        the raw response object instead of the parsed content.\n+\n+        For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers\n+        \"\"\"\n+        return ClientSecretsWithRawResponse(self)\n+\n+    @cached_property\n+    def with_streaming_response(self) -> ClientSecretsWithStreamingResponse:\n+        \"\"\"\n+        An alternative to `.with_raw_response` that doesn't eagerly read the response body.\n+\n+        For more information, see https://www.github.com/openai/openai-python#with_streaming_response\n+        \"\"\"\n+        return ClientSecretsWithStreamingResponse(self)\n+\n+    def create(\n+        self,\n+        *,\n+        expires_after: client_secret_create_params.ExpiresAfter | NotGiven = NOT_GIVEN,\n+        session: client_secret_create_params.Session | NotGiven = NOT_GIVEN,\n+        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n+        # The extra values given here take precedence over values defined on the client or passed to this method.\n+        extra_headers: Headers | None = None,\n+        extra_query: Query | None = None,\n+        extra_body: Body | None = None,\n+        timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n+    ) -> ClientSecretCreateResponse:\n+        \"\"\"\n+        Create a Realtime session and client secret for either realtime or\n+        transcription.\n+\n+        Args:\n+          expires_after: Configuration for the ephemeral token expiration.\n+\n+          session: Session configuration to use for the client secret. Choose either a realtime\n+              session or a transcription session.\n+\n+          extra_headers: Send extra headers\n+\n+          extra_query: Add additional query parameters to the request\n+\n+          extra_body: Add additional JSON properties to the request\n+\n+          timeout: Override the client-level default timeout for this request, in seconds\n+        \"\"\"\n+        return self._post(\n+            \"/realtime/client_secrets\",\n+            body=maybe_transform(\n+                {\n+                    \"expires_after\": expires_after,\n+                    \"session\": session,\n+                },\n+                client_secret_create_params.ClientSecretCreateParams,\n+            ),\n+            options=make_request_options(\n+                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n+            ),\n+            cast_to=ClientSecretCreateResponse,\n+        )\n+\n+\n+class AsyncClientSecrets(AsyncAPIResource):\n+    @cached_property\n+    def with_raw_response(self) -> AsyncClientSecretsWithRawResponse:\n+        \"\"\"\n+        This property can be used as a prefix for any HTTP method call to return\n+        the raw response object instead of the parsed content.\n+\n+        For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers\n+        \"\"\"\n+        return AsyncClientSecretsWithRawResponse(self)\n+\n+    @cached_property\n+    def with_streaming_response(self) -> AsyncClientSecretsWithStreamingResponse:\n+        \"\"\"\n+        An alternative to `.with_raw_response` that doesn't eagerly read the response body.\n+\n+        For more information, see https://www.github.com/openai/openai-python#with_streaming_response\n+        \"\"\"\n+        return AsyncClientSecretsWithStreamingResponse(self)\n+\n+    async def create(\n+        self,\n+        *,\n+        expires_after: client_secret_create_params.ExpiresAfter | NotGiven = NOT_GIVEN,\n+        session: client_secret_create_params.Session | NotGiven = NOT_GIVEN,\n+        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n+        # The extra values given here take precedence over values defined on the client or passed to this method.\n+        extra_headers: Headers | None = None,\n+        extra_query: Query | None = None,\n+        extra_body: Body | None = None,\n+        timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n+    ) -> ClientSecretCreateResponse:\n+        \"\"\"\n+        Create a Realtime session and client secret for either realtime or\n+        transcription.\n+\n+        Args:\n+          expires_after: Configuration for the ephemeral token expiration.\n+\n+          session: Session configuration to use for the client secret. Choose either a realtime\n+              session or a transcription session.\n+\n+          extra_headers: Send extra headers\n+\n+          extra_query: Add additional query parameters to the request\n+\n+          extra_body: Add additional JSON properties to the request\n+\n+          timeout: Override the client-level default timeout for this request, in seconds\n+        \"\"\"\n+        return await self._post(\n+            \"/realtime/client_secrets\",\n+            body=await async_maybe_transform(\n+                {\n+                    \"expires_after\": expires_after,\n+                    \"session\": session,\n+                },\n+                client_secret_create_params.ClientSecretCreateParams,\n+            ),\n+            options=make_request_options(\n+                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n+            ),\n+            cast_to=ClientSecretCreateResponse,\n+        )\n+\n+\n+class ClientSecretsWithRawResponse:\n+    def __init__(self, client_secrets: ClientSecrets) -> None:\n+        self._client_secrets = client_secrets\n+\n+        self.create = _legacy_response.to_raw_response_wrapper(\n+            client_secrets.create,\n+        )\n+\n+\n+class AsyncClientSecretsWithRawResponse:\n+    def __init__(self, client_secrets: AsyncClientSecrets) -> None:\n+        self._client_secrets = client_secrets\n+\n+        self.create = _legacy_response.async_to_raw_response_wrapper(\n+            client_secrets.create,\n+        )\n+\n+\n+class ClientSecretsWithStreamingResponse:\n+    def __init__(self, client_secrets: ClientSecrets) -> None:\n+        self._client_secrets = client_secrets\n+\n+        self.create = to_streamed_response_wrapper(\n+            client_secrets.create,\n+        )\n+\n+\n+class AsyncClientSecretsWithStreamingResponse:\n+    def __init__(self, client_secrets: AsyncClientSecrets) -> None:\n+        self._client_secrets = client_secrets\n+\n+        self.create = async_to_streamed_response_wrapper(\n+            client_secrets.create,\n+        )\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/resources/realtime/realtime.py",
            "diff": "diff --git a/src/openai/resources/realtime/realtime.py b/src/openai/resources/realtime/realtime.py\nnew file mode 100644\nindex 0000000..ebdfce8\n--- /dev/null\n+++ b/src/openai/resources/realtime/realtime.py\n@@ -0,0 +1,1056 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+import json\n+import logging\n+from types import TracebackType\n+from typing import TYPE_CHECKING, Any, Iterator, cast\n+from typing_extensions import AsyncIterator\n+\n+import httpx\n+from pydantic import BaseModel\n+\n+from ..._types import NOT_GIVEN, Query, Headers, NotGiven\n+from ..._utils import (\n+    is_azure_client,\n+    maybe_transform,\n+    strip_not_given,\n+    async_maybe_transform,\n+    is_async_azure_client,\n+)\n+from ..._compat import cached_property\n+from ..._models import construct_type_unchecked\n+from ..._resource import SyncAPIResource, AsyncAPIResource\n+from ..._exceptions import OpenAIError\n+from ..._base_client import _merge_mappings\n+from .client_secrets import (\n+    ClientSecrets,\n+    AsyncClientSecrets,\n+    ClientSecretsWithRawResponse,\n+    AsyncClientSecretsWithRawResponse,\n+    ClientSecretsWithStreamingResponse,\n+    AsyncClientSecretsWithStreamingResponse,\n+)\n+from ...types.realtime import response_create_event_param\n+from ...types.websocket_connection_options import WebsocketConnectionOptions\n+from ...types.realtime.realtime_client_event import RealtimeClientEvent\n+from ...types.realtime.realtime_server_event import RealtimeServerEvent\n+from ...types.realtime.conversation_item_param import ConversationItemParam\n+from ...types.realtime.realtime_client_event_param import RealtimeClientEventParam\n+from ...types.realtime.realtime_session_create_request_param import RealtimeSessionCreateRequestParam\n+from ...types.realtime.realtime_transcription_session_create_request_param import (\n+    RealtimeTranscriptionSessionCreateRequestParam,\n+)\n+\n+if TYPE_CHECKING:\n+    from websockets.sync.client import ClientConnection as WebsocketConnection\n+    from websockets.asyncio.client import ClientConnection as AsyncWebsocketConnection\n+\n+    from ..._client import OpenAI, AsyncOpenAI\n+\n+__all__ = [\"Realtime\", \"AsyncRealtime\"]\n+\n+log: logging.Logger = logging.getLogger(__name__)\n+\n+\n+class Realtime(SyncAPIResource):\n+    @cached_property\n+    def client_secrets(self) -> ClientSecrets:\n+        return ClientSecrets(self._client)\n+\n+    @cached_property\n+    def with_raw_response(self) -> RealtimeWithRawResponse:\n+        \"\"\"\n+        This property can be used as a prefix for any HTTP method call to return\n+        the raw response object instead of the parsed content.\n+\n+        For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers\n+        \"\"\"\n+        return RealtimeWithRawResponse(self)\n+\n+    @cached_property\n+    def with_streaming_response(self) -> RealtimeWithStreamingResponse:\n+        \"\"\"\n+        An alternative to `.with_raw_response` that doesn't eagerly read the response body.\n+\n+        For more information, see https://www.github.com/openai/openai-python#with_streaming_response\n+        \"\"\"\n+        return RealtimeWithStreamingResponse(self)\n+\n+    def connect(\n+        self,\n+        *,\n+        model: str,\n+        extra_query: Query = {},\n+        extra_headers: Headers = {},\n+        websocket_connection_options: WebsocketConnectionOptions = {},\n+    ) -> RealtimeConnectionManager:\n+        \"\"\"\n+        The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as function calling.\n+\n+        Some notable benefits of the API include:\n+\n+        - Native speech-to-speech: Skipping an intermediate text format means low latency and nuanced output.\n+        - Natural, steerable voices: The models have natural inflection and can laugh, whisper, and adhere to tone direction.\n+        - Simultaneous multimodal output: Text is useful for moderation; faster-than-realtime audio ensures stable playback.\n+\n+        The Realtime API is a stateful, event-based API that communicates over a WebSocket.\n+        \"\"\"\n+        return RealtimeConnectionManager(\n+            client=self._client,\n+            extra_query=extra_query,\n+            extra_headers=extra_headers,\n+            websocket_connection_options=websocket_connection_options,\n+            model=model,\n+        )\n+\n+\n+class AsyncRealtime(AsyncAPIResource):\n+    @cached_property\n+    def client_secrets(self) -> AsyncClientSecrets:\n+        return AsyncClientSecrets(self._client)\n+\n+    @cached_property\n+    def with_raw_response(self) -> AsyncRealtimeWithRawResponse:\n+        \"\"\"\n+        This property can be used as a prefix for any HTTP method call to return\n+        the raw response object instead of the parsed content.\n+\n+        For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers\n+        \"\"\"\n+        return AsyncRealtimeWithRawResponse(self)\n+\n+    @cached_property\n+    def with_streaming_response(self) -> AsyncRealtimeWithStreamingResponse:\n+        \"\"\"\n+        An alternative to `.with_raw_response` that doesn't eagerly read the response body.\n+\n+        For more information, see https://www.github.com/openai/openai-python#with_streaming_response\n+        \"\"\"\n+        return AsyncRealtimeWithStreamingResponse(self)\n+\n+    def connect(\n+        self,\n+        *,\n+        model: str,\n+        extra_query: Query = {},\n+        extra_headers: Headers = {},\n+        websocket_connection_options: WebsocketConnectionOptions = {},\n+    ) -> AsyncRealtimeConnectionManager:\n+        \"\"\"\n+        The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as function calling.\n+\n+        Some notable benefits of the API include:\n+\n+        - Native speech-to-speech: Skipping an intermediate text format means low latency and nuanced output.\n+        - Natural, steerable voices: The models have natural inflection and can laugh, whisper, and adhere to tone direction.\n+        - Simultaneous multimodal output: Text is useful for moderation; faster-than-realtime audio ensures stable playback.\n+\n+        The Realtime API is a stateful, event-based API that communicates over a WebSocket.\n+        \"\"\"\n+        return AsyncRealtimeConnectionManager(\n+            client=self._client,\n+            extra_query=extra_query,\n+            extra_headers=extra_headers,\n+            websocket_connection_options=websocket_connection_options,\n+            model=model,\n+        )\n+\n+\n+class RealtimeWithRawResponse:\n+    def __init__(self, realtime: Realtime) -> None:\n+        self._realtime = realtime\n+\n+    @cached_property\n+    def client_secrets(self) -> ClientSecretsWithRawResponse:\n+        return ClientSecretsWithRawResponse(self._realtime.client_secrets)\n+\n+\n+class AsyncRealtimeWithRawResponse:\n+    def __init__(self, realtime: AsyncRealtime) -> None:\n+        self._realtime = realtime\n+\n+    @cached_property\n+    def client_secrets(self) -> AsyncClientSecretsWithRawResponse:\n+        return AsyncClientSecretsWithRawResponse(self._realtime.client_secrets)\n+\n+\n+class RealtimeWithStreamingResponse:\n+    def __init__(self, realtime: Realtime) -> None:\n+        self._realtime = realtime\n+\n+    @cached_property\n+    def client_secrets(self) -> ClientSecretsWithStreamingResponse:\n+        return ClientSecretsWithStreamingResponse(self._realtime.client_secrets)\n+\n+\n+class AsyncRealtimeWithStreamingResponse:\n+    def __init__(self, realtime: AsyncRealtime) -> None:\n+        self._realtime = realtime\n+\n+    @cached_property\n+    def client_secrets(self) -> AsyncClientSecretsWithStreamingResponse:\n+        return AsyncClientSecretsWithStreamingResponse(self._realtime.client_secrets)\n+\n+\n+class AsyncRealtimeConnection:\n+    \"\"\"Represents a live websocket connection to the Realtime API\"\"\"\n+\n+    session: AsyncRealtimeSessionResource\n+    response: AsyncRealtimeResponseResource\n+    input_audio_buffer: AsyncRealtimeInputAudioBufferResource\n+    conversation: AsyncRealtimeConversationResource\n+    output_audio_buffer: AsyncRealtimeOutputAudioBufferResource\n+    transcription_session: AsyncRealtimeTranscriptionSessionResource\n+\n+    _connection: AsyncWebsocketConnection\n+\n+    def __init__(self, connection: AsyncWebsocketConnection) -> None:\n+        self._connection = connection\n+\n+        self.session = AsyncRealtimeSessionResource(self)\n+        self.response = AsyncRealtimeResponseResource(self)\n+        self.input_audio_buffer = AsyncRealtimeInputAudioBufferResource(self)\n+        self.conversation = AsyncRealtimeConversationResource(self)\n+        self.output_audio_buffer = AsyncRealtimeOutputAudioBufferResource(self)\n+        self.transcription_session = AsyncRealtimeTranscriptionSessionResource(self)\n+\n+    async def __aiter__(self) -> AsyncIterator[RealtimeServerEvent]:\n+        \"\"\"\n+        An infinite-iterator that will continue to yield events until\n+        the connection is closed.\n+        \"\"\"\n+        from websockets.exceptions import ConnectionClosedOK\n+\n+        try:\n+            while True:\n+                yield await self.recv()\n+        except ConnectionClosedOK:\n+            return\n+\n+    async def recv(self) -> RealtimeServerEvent:\n+        \"\"\"\n+        Receive the next message from the connection and parses it into a `RealtimeServerEvent` object.\n+\n+        Canceling this method is safe. There's no risk of losing data.\n+        \"\"\"\n+        return self.parse_event(await self.recv_bytes())\n+\n+    async def recv_bytes(self) -> bytes:\n+        \"\"\"Receive the next message from the connection as raw bytes.\n+\n+        Canceling this method is safe. There's no risk of losing data.\n+\n+        If you want to parse the message into a `RealtimeServerEvent` object like `.recv()` does,\n+        then you can call `.parse_event(data)`.\n+        \"\"\"\n+        message = await self._connection.recv(decode=False)\n+        log.debug(f\"Received websocket message: %s\", message)\n+        return message\n+\n+    async def send(self, event: RealtimeClientEvent | RealtimeClientEventParam) -> None:\n+        data = (\n+            event.to_json(use_api_names=True, exclude_defaults=True, exclude_unset=True)\n+            if isinstance(event, BaseModel)\n+            else json.dumps(await async_maybe_transform(event, RealtimeClientEventParam))\n+        )\n+        await self._connection.send(data)\n+\n+    async def close(self, *, code: int = 1000, reason: str = \"\") -> None:\n+        await self._connection.close(code=code, reason=reason)\n+\n+    def parse_event(self, data: str | bytes) -> RealtimeServerEvent:\n+        \"\"\"\n+        Converts a raw `str` or `bytes` message into a `RealtimeServerEvent` object.\n+\n+        This is helpful if you're using `.recv_bytes()`.\n+        \"\"\"\n+        return cast(\n+            RealtimeServerEvent, construct_type_unchecked(value=json.loads(data), type_=cast(Any, RealtimeServerEvent))\n+        )\n+\n+\n+class AsyncRealtimeConnectionManager:\n+    \"\"\"\n+    Context manager over a `AsyncRealtimeConnection` that is returned by `realtime.connect()`\n+\n+    This context manager ensures that the connection will be closed when it exits.\n+\n+    ---\n+\n+    Note that if your application doesn't work well with the context manager approach then you\n+    can call the `.enter()` method directly to initiate a connection.\n+\n+    **Warning**: You must remember to close the connection with `.close()`.\n+\n+    ```py\n+    connection = await client.realtime.connect(...).enter()\n+    # ...\n+    await connection.close()\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        client: AsyncOpenAI,\n+        model: str,\n+        extra_query: Query,\n+        extra_headers: Headers,\n+        websocket_connection_options: WebsocketConnectionOptions,\n+    ) -> None:\n+        self.__client = client\n+        self.__model = model\n+        self.__connection: AsyncRealtimeConnection | None = None\n+        self.__extra_query = extra_query\n+        self.__extra_headers = extra_headers\n+        self.__websocket_connection_options = websocket_connection_options\n+\n+    async def __aenter__(self) -> AsyncRealtimeConnection:\n+        \"\"\"\n+        \ud83d\udc4b If your application doesn't work well with the context manager approach then you\n+        can call this method directly to initiate a connection.\n+\n+        **Warning**: You must remember to close the connection with `.close()`.\n+\n+        ```py\n+        connection = await client.realtime.connect(...).enter()\n+        # ...\n+        await connection.close()\n+        ```\n+        \"\"\"\n+        try:\n+            from websockets.asyncio.client import connect\n+        except ImportError as exc:\n+            raise OpenAIError(\"You need to install `openai[realtime]` to use this method\") from exc\n+\n+        extra_query = self.__extra_query\n+        auth_headers = self.__client.auth_headers\n+        if is_async_azure_client(self.__client):\n+            url, auth_headers = await self.__client._configure_realtime(self.__model, extra_query)\n+        else:\n+            url = self._prepare_url().copy_with(\n+                params={\n+                    **self.__client.base_url.params,\n+                    \"model\": self.__model,\n+                    **extra_query,\n+                },\n+            )\n+        log.debug(\"Connecting to %s\", url)\n+        if self.__websocket_connection_options:\n+            log.debug(\"Connection options: %s\", self.__websocket_connection_options)\n+\n+        self.__connection = AsyncRealtimeConnection(\n+            await connect(\n+                str(url),\n+                user_agent_header=self.__client.user_agent,\n+                additional_headers=_merge_mappings(\n+                    {\n+                        **auth_headers,\n+                    },\n+                    self.__extra_headers,\n+                ),\n+                **self.__websocket_connection_options,\n+            )\n+        )\n+\n+        return self.__connection\n+\n+    enter = __aenter__\n+\n+    def _prepare_url(self) -> httpx.URL:\n+        if self.__client.websocket_base_url is not None:\n+            base_url = httpx.URL(self.__client.websocket_base_url)\n+        else:\n+            base_url = self.__client._base_url.copy_with(scheme=\"wss\")\n+\n+        merge_raw_path = base_url.raw_path.rstrip(b\"/\") + b\"/realtime\"\n+        return base_url.copy_with(raw_path=merge_raw_path)\n+\n+    async def __aexit__(\n+        self, exc_type: type[BaseException] | None, exc: BaseException | None, exc_tb: TracebackType | None\n+    ) -> None:\n+        if self.__connection is not None:\n+            await self.__connection.close()\n+\n+\n+class RealtimeConnection:\n+    \"\"\"Represents a live websocket connection to the Realtime API\"\"\"\n+\n+    session: RealtimeSessionResource\n+    response: RealtimeResponseResource\n+    input_audio_buffer: RealtimeInputAudioBufferResource\n+    conversation: RealtimeConversationResource\n+    output_audio_buffer: RealtimeOutputAudioBufferResource\n+    transcription_session: RealtimeTranscriptionSessionResource\n+\n+    _connection: WebsocketConnection\n+\n+    def __init__(self, connection: WebsocketConnection) -> None:\n+        self._connection = connection\n+\n+        self.session = RealtimeSessionResource(self)\n+        self.response = RealtimeResponseResource(self)\n+        self.input_audio_buffer = RealtimeInputAudioBufferResource(self)\n+        self.conversation = RealtimeConversationResource(self)\n+        self.output_audio_buffer = RealtimeOutputAudioBufferResource(self)\n+        self.transcription_session = RealtimeTranscriptionSessionResource(self)\n+\n+    def __iter__(self) -> Iterator[RealtimeServerEvent]:\n+        \"\"\"\n+        An infinite-iterator that will continue to yield events until\n+        the connection is closed.\n+        \"\"\"\n+        from websockets.exceptions import ConnectionClosedOK\n+\n+        try:\n+            while True:\n+                yield self.recv()\n+        except ConnectionClosedOK:\n+            return\n+\n+    def recv(self) -> RealtimeServerEvent:\n+        \"\"\"\n+        Receive the next message from the connection and parses it into a `RealtimeServerEvent` object.\n+\n+        Canceling this method is safe. There's no risk of losing data.\n+        \"\"\"\n+        return self.parse_event(self.recv_bytes())\n+\n+    def recv_bytes(self) -> bytes:\n+        \"\"\"Receive the next message from the connection as raw bytes.\n+\n+        Canceling this method is safe. There's no risk of losing data.\n+\n+        If you want to parse the message into a `RealtimeServerEvent` object like `.recv()` does,\n+        then you can call `.parse_event(data)`.\n+        \"\"\"\n+        message = self._connection.recv(decode=False)\n+        log.debug(f\"Received websocket message: %s\", message)\n+        return message\n+\n+    def send(self, event: RealtimeClientEvent | RealtimeClientEventParam) -> None:\n+        data = (\n+            event.to_json(use_api_names=True, exclude_defaults=True, exclude_unset=True)\n+            if isinstance(event, BaseModel)\n+            else json.dumps(maybe_transform(event, RealtimeClientEventParam))\n+        )\n+        self._connection.send(data)\n+\n+    def close(self, *, code: int = 1000, reason: str = \"\") -> None:\n+        self._connection.close(code=code, reason=reason)\n+\n+    def parse_event(self, data: str | bytes) -> RealtimeServerEvent:\n+        \"\"\"\n+        Converts a raw `str` or `bytes` message into a `RealtimeServerEvent` object.\n+\n+        This is helpful if you're using `.recv_bytes()`.\n+        \"\"\"\n+        return cast(\n+            RealtimeServerEvent, construct_type_unchecked(value=json.loads(data), type_=cast(Any, RealtimeServerEvent))\n+        )\n+\n+\n+class RealtimeConnectionManager:\n+    \"\"\"\n+    Context manager over a `RealtimeConnection` that is returned by `realtime.connect()`\n+\n+    This context manager ensures that the connection will be closed when it exits.\n+\n+    ---\n+\n+    Note that if your application doesn't work well with the context manager approach then you\n+    can call the `.enter()` method directly to initiate a connection.\n+\n+    **Warning**: You must remember to close the connection with `.close()`.\n+\n+    ```py\n+    connection = client.realtime.connect(...).enter()\n+    # ...\n+    connection.close()\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        client: OpenAI,\n+        model: str,\n+        extra_query: Query,\n+        extra_headers: Headers,\n+        websocket_connection_options: WebsocketConnectionOptions,\n+    ) -> None:\n+        self.__client = client\n+        self.__model = model\n+        self.__connection: RealtimeConnection | None = None\n+        self.__extra_query = extra_query\n+        self.__extra_headers = extra_headers\n+        self.__websocket_connection_options = websocket_connection_options\n+\n+    def __enter__(self) -> RealtimeConnection:\n+        \"\"\"\n+        \ud83d\udc4b If your application doesn't work well with the context manager approach then you\n+        can call this method directly to initiate a connection.\n+\n+        **Warning**: You must remember to close the connection with `.close()`.\n+\n+        ```py\n+        connection = client.realtime.connect(...).enter()\n+        # ...\n+        connection.close()\n+        ```\n+        \"\"\"\n+        try:\n+            from websockets.sync.client import connect\n+        except ImportError as exc:\n+            raise OpenAIError(\"You need to install `openai[realtime]` to use this method\") from exc\n+\n+        extra_query = self.__extra_query\n+        auth_headers = self.__client.auth_headers\n+        if is_azure_client(self.__client):\n+            url, auth_headers = self.__client._configure_realtime(self.__model, extra_query)\n+        else:\n+            url = self._prepare_url().copy_with(\n+                params={\n+                    **self.__client.base_url.params,\n+                    \"model\": self.__model,\n+                    **extra_query,\n+                },\n+            )\n+        log.debug(\"Connecting to %s\", url)\n+        if self.__websocket_connection_options:\n+            log.debug(\"Connection options: %s\", self.__websocket_connection_options)\n+\n+        self.__connection = RealtimeConnection(\n+            connect(\n+                str(url),\n+                user_agent_header=self.__client.user_agent,\n+                additional_headers=_merge_mappings(\n+                    {\n+                        **auth_headers,\n+                    },\n+                    self.__extra_headers,\n+                ),\n+                **self.__websocket_connection_options,\n+            )\n+        )\n+\n+        return self.__connection\n+\n+    enter = __enter__\n+\n+    def _prepare_url(self) -> httpx.URL:\n+        if self.__client.websocket_base_url is not None:\n+            base_url = httpx.URL(self.__client.websocket_base_url)\n+        else:\n+            base_url = self.__client._base_url.copy_with(scheme=\"wss\")\n+\n+        merge_raw_path = base_url.raw_path.rstrip(b\"/\") + b\"/realtime\"\n+        return base_url.copy_with(raw_path=merge_raw_path)\n+\n+    def __exit__(\n+        self, exc_type: type[BaseException] | None, exc: BaseException | None, exc_tb: TracebackType | None\n+    ) -> None:\n+        if self.__connection is not None:\n+            self.__connection.close()\n+\n+\n+class BaseRealtimeConnectionResource:\n+    def __init__(self, connection: RealtimeConnection) -> None:\n+        self._connection = connection\n+\n+\n+class RealtimeSessionResource(BaseRealtimeConnectionResource):\n+    def update(self, *, session: RealtimeSessionCreateRequestParam, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"\n+        Send this event to update the session\u2019s default configuration.\n+        The client may send this event at any time to update any field,\n+        except for `voice`. However, note that once a session has been\n+        initialized with a particular `model`, it can\u2019t be changed to\n+        another model using `session.update`.\n+\n+        When the server receives a `session.update`, it will respond\n+        with a `session.updated` event showing the full, effective configuration.\n+        Only the fields that are present are updated. To clear a field like\n+        `instructions`, pass an empty string.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"session.update\", \"session\": session, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class RealtimeResponseResource(BaseRealtimeConnectionResource):\n+    def create(\n+        self,\n+        *,\n+        event_id: str | NotGiven = NOT_GIVEN,\n+        response: response_create_event_param.Response | NotGiven = NOT_GIVEN,\n+    ) -> None:\n+        \"\"\"\n+        This event instructs the server to create a Response, which means triggering\n+        model inference. When in Server VAD mode, the server will create Responses\n+        automatically.\n+\n+        A Response will include at least one Item, and may have two, in which case\n+        the second will be a function call. These Items will be appended to the\n+        conversation history.\n+\n+        The server will respond with a `response.created` event, events for Items\n+        and content created, and finally a `response.done` event to indicate the\n+        Response is complete.\n+\n+        The `response.create` event includes inference configuration like\n+        `instructions`, and `temperature`. These fields will override the Session's\n+        configuration for this Response only.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"response.create\", \"event_id\": event_id, \"response\": response}),\n+            )\n+        )\n+\n+    def cancel(self, *, event_id: str | NotGiven = NOT_GIVEN, response_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event to cancel an in-progress response.\n+\n+        The server will respond\n+        with a `response.done` event with a status of `response.status=cancelled`. If\n+        there is no response to cancel, the server will respond with an error.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"response.cancel\", \"event_id\": event_id, \"response_id\": response_id}),\n+            )\n+        )\n+\n+\n+class RealtimeInputAudioBufferResource(BaseRealtimeConnectionResource):\n+    def clear(self, *, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event to clear the audio bytes in the buffer.\n+\n+        The server will\n+        respond with an `input_audio_buffer.cleared` event.\n+        \"\"\"\n+        self._connection.send(\n+            cast(RealtimeClientEventParam, strip_not_given({\"type\": \"input_audio_buffer.clear\", \"event_id\": event_id}))\n+        )\n+\n+    def commit(self, *, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"\n+        Send this event to commit the user input audio buffer, which will create a\n+        new user message item in the conversation. This event will produce an error\n+        if the input audio buffer is empty. When in Server VAD mode, the client does\n+        not need to send this event, the server will commit the audio buffer\n+        automatically.\n+\n+        Committing the input audio buffer will trigger input audio transcription\n+        (if enabled in session configuration), but it will not create a response\n+        from the model. The server will respond with an `input_audio_buffer.committed`\n+        event.\n+        \"\"\"\n+        self._connection.send(\n+            cast(RealtimeClientEventParam, strip_not_given({\"type\": \"input_audio_buffer.commit\", \"event_id\": event_id}))\n+        )\n+\n+    def append(self, *, audio: str, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event to append audio bytes to the input audio buffer.\n+\n+        The audio\n+        buffer is temporary storage you can write to and later commit. In Server VAD\n+        mode, the audio buffer is used to detect speech and the server will decide\n+        when to commit. When Server VAD is disabled, you must commit the audio buffer\n+        manually.\n+\n+        The client may choose how much audio to place in each event up to a maximum\n+        of 15 MiB, for example streaming smaller chunks from the client may allow the\n+        VAD to be more responsive. Unlike made other client events, the server will\n+        not send a confirmation response to this event.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"input_audio_buffer.append\", \"audio\": audio, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class RealtimeConversationResource(BaseRealtimeConnectionResource):\n+    @cached_property\n+    def item(self) -> RealtimeConversationItemResource:\n+        return RealtimeConversationItemResource(self._connection)\n+\n+\n+class RealtimeConversationItemResource(BaseRealtimeConnectionResource):\n+    def delete(self, *, item_id: str, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event when you want to remove any item from the conversation\n+        history.\n+\n+        The server will respond with a `conversation.item.deleted` event,\n+        unless the item does not exist in the conversation history, in which case the\n+        server will respond with an error.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"conversation.item.delete\", \"item_id\": item_id, \"event_id\": event_id}),\n+            )\n+        )\n+\n+    def create(\n+        self,\n+        *,\n+        item: ConversationItemParam,\n+        event_id: str | NotGiven = NOT_GIVEN,\n+        previous_item_id: str | NotGiven = NOT_GIVEN,\n+    ) -> None:\n+        \"\"\"\n+        Add a new Item to the Conversation's context, including messages, function\n+        calls, and function call responses. This event can be used both to populate a\n+        \"history\" of the conversation and to add new items mid-stream, but has the\n+        current limitation that it cannot populate assistant audio messages.\n+\n+        If successful, the server will respond with a `conversation.item.created`\n+        event, otherwise an `error` event will be sent.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given(\n+                    {\n+                        \"type\": \"conversation.item.create\",\n+                        \"item\": item,\n+                        \"event_id\": event_id,\n+                        \"previous_item_id\": previous_item_id,\n+                    }\n+                ),\n+            )\n+        )\n+\n+    def truncate(\n+        self, *, audio_end_ms: int, content_index: int, item_id: str, event_id: str | NotGiven = NOT_GIVEN\n+    ) -> None:\n+        \"\"\"Send this event to truncate a previous assistant message\u2019s audio.\n+\n+        The server\n+        will produce audio faster than realtime, so this event is useful when the user\n+        interrupts to truncate audio that has already been sent to the client but not\n+        yet played. This will synchronize the server's understanding of the audio with\n+        the client's playback.\n+\n+        Truncating audio will delete the server-side text transcript to ensure there\n+        is not text in the context that hasn't been heard by the user.\n+\n+        If successful, the server will respond with a `conversation.item.truncated`\n+        event.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given(\n+                    {\n+                        \"type\": \"conversation.item.truncate\",\n+                        \"audio_end_ms\": audio_end_ms,\n+                        \"content_index\": content_index,\n+                        \"item_id\": item_id,\n+                        \"event_id\": event_id,\n+                    }\n+                ),\n+            )\n+        )\n+\n+    def retrieve(self, *, item_id: str, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"\n+        Send this event when you want to retrieve the server's representation of a specific item in the conversation history. This is useful, for example, to inspect user audio after noise cancellation and VAD.\n+        The server will respond with a `conversation.item.retrieved` event,\n+        unless the item does not exist in the conversation history, in which case the\n+        server will respond with an error.\n+        \"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"conversation.item.retrieve\", \"item_id\": item_id, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class RealtimeOutputAudioBufferResource(BaseRealtimeConnectionResource):\n+    def clear(self, *, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"**WebRTC Only:** Emit to cut off the current audio response.\n+\n+        This will trigger the server to\n+        stop generating audio and emit a `output_audio_buffer.cleared` event. This\n+        event should be preceded by a `response.cancel` client event to stop the\n+        generation of the current response.\n+        [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n+        \"\"\"\n+        self._connection.send(\n+            cast(RealtimeClientEventParam, strip_not_given({\"type\": \"output_audio_buffer.clear\", \"event_id\": event_id}))\n+        )\n+\n+\n+class RealtimeTranscriptionSessionResource(BaseRealtimeConnectionResource):\n+    def update(\n+        self, *, session: RealtimeTranscriptionSessionCreateRequestParam, event_id: str | NotGiven = NOT_GIVEN\n+    ) -> None:\n+        \"\"\"Send this event to update a transcription session.\"\"\"\n+        self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"transcription_session.update\", \"session\": session, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class BaseAsyncRealtimeConnectionResource:\n+    def __init__(self, connection: AsyncRealtimeConnection) -> None:\n+        self._connection = connection\n+\n+\n+class AsyncRealtimeSessionResource(BaseAsyncRealtimeConnectionResource):\n+    async def update(self, *, session: RealtimeSessionCreateRequestParam, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"\n+        Send this event to update the session\u2019s default configuration.\n+        The client may send this event at any time to update any field,\n+        except for `voice`. However, note that once a session has been\n+        initialized with a particular `model`, it can\u2019t be changed to\n+        another model using `session.update`.\n+\n+        When the server receives a `session.update`, it will respond\n+        with a `session.updated` event showing the full, effective configuration.\n+        Only the fields that are present are updated. To clear a field like\n+        `instructions`, pass an empty string.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"session.update\", \"session\": session, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class AsyncRealtimeResponseResource(BaseAsyncRealtimeConnectionResource):\n+    async def create(\n+        self,\n+        *,\n+        event_id: str | NotGiven = NOT_GIVEN,\n+        response: response_create_event_param.Response | NotGiven = NOT_GIVEN,\n+    ) -> None:\n+        \"\"\"\n+        This event instructs the server to create a Response, which means triggering\n+        model inference. When in Server VAD mode, the server will create Responses\n+        automatically.\n+\n+        A Response will include at least one Item, and may have two, in which case\n+        the second will be a function call. These Items will be appended to the\n+        conversation history.\n+\n+        The server will respond with a `response.created` event, events for Items\n+        and content created, and finally a `response.done` event to indicate the\n+        Response is complete.\n+\n+        The `response.create` event includes inference configuration like\n+        `instructions`, and `temperature`. These fields will override the Session's\n+        configuration for this Response only.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"response.create\", \"event_id\": event_id, \"response\": response}),\n+            )\n+        )\n+\n+    async def cancel(self, *, event_id: str | NotGiven = NOT_GIVEN, response_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event to cancel an in-progress response.\n+\n+        The server will respond\n+        with a `response.done` event with a status of `response.status=cancelled`. If\n+        there is no response to cancel, the server will respond with an error.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"response.cancel\", \"event_id\": event_id, \"response_id\": response_id}),\n+            )\n+        )\n+\n+\n+class AsyncRealtimeInputAudioBufferResource(BaseAsyncRealtimeConnectionResource):\n+    async def clear(self, *, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event to clear the audio bytes in the buffer.\n+\n+        The server will\n+        respond with an `input_audio_buffer.cleared` event.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(RealtimeClientEventParam, strip_not_given({\"type\": \"input_audio_buffer.clear\", \"event_id\": event_id}))\n+        )\n+\n+    async def commit(self, *, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"\n+        Send this event to commit the user input audio buffer, which will create a\n+        new user message item in the conversation. This event will produce an error\n+        if the input audio buffer is empty. When in Server VAD mode, the client does\n+        not need to send this event, the server will commit the audio buffer\n+        automatically.\n+\n+        Committing the input audio buffer will trigger input audio transcription\n+        (if enabled in session configuration), but it will not create a response\n+        from the model. The server will respond with an `input_audio_buffer.committed`\n+        event.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(RealtimeClientEventParam, strip_not_given({\"type\": \"input_audio_buffer.commit\", \"event_id\": event_id}))\n+        )\n+\n+    async def append(self, *, audio: str, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event to append audio bytes to the input audio buffer.\n+\n+        The audio\n+        buffer is temporary storage you can write to and later commit. In Server VAD\n+        mode, the audio buffer is used to detect speech and the server will decide\n+        when to commit. When Server VAD is disabled, you must commit the audio buffer\n+        manually.\n+\n+        The client may choose how much audio to place in each event up to a maximum\n+        of 15 MiB, for example streaming smaller chunks from the client may allow the\n+        VAD to be more responsive. Unlike made other client events, the server will\n+        not send a confirmation response to this event.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"input_audio_buffer.append\", \"audio\": audio, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class AsyncRealtimeConversationResource(BaseAsyncRealtimeConnectionResource):\n+    @cached_property\n+    def item(self) -> AsyncRealtimeConversationItemResource:\n+        return AsyncRealtimeConversationItemResource(self._connection)\n+\n+\n+class AsyncRealtimeConversationItemResource(BaseAsyncRealtimeConnectionResource):\n+    async def delete(self, *, item_id: str, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"Send this event when you want to remove any item from the conversation\n+        history.\n+\n+        The server will respond with a `conversation.item.deleted` event,\n+        unless the item does not exist in the conversation history, in which case the\n+        server will respond with an error.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"conversation.item.delete\", \"item_id\": item_id, \"event_id\": event_id}),\n+            )\n+        )\n+\n+    async def create(\n+        self,\n+        *,\n+        item: ConversationItemParam,\n+        event_id: str | NotGiven = NOT_GIVEN,\n+        previous_item_id: str | NotGiven = NOT_GIVEN,\n+    ) -> None:\n+        \"\"\"\n+        Add a new Item to the Conversation's context, including messages, function\n+        calls, and function call responses. This event can be used both to populate a\n+        \"history\" of the conversation and to add new items mid-stream, but has the\n+        current limitation that it cannot populate assistant audio messages.\n+\n+        If successful, the server will respond with a `conversation.item.created`\n+        event, otherwise an `error` event will be sent.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given(\n+                    {\n+                        \"type\": \"conversation.item.create\",\n+                        \"item\": item,\n+                        \"event_id\": event_id,\n+                        \"previous_item_id\": previous_item_id,\n+                    }\n+                ),\n+            )\n+        )\n+\n+    async def truncate(\n+        self, *, audio_end_ms: int, content_index: int, item_id: str, event_id: str | NotGiven = NOT_GIVEN\n+    ) -> None:\n+        \"\"\"Send this event to truncate a previous assistant message\u2019s audio.\n+\n+        The server\n+        will produce audio faster than realtime, so this event is useful when the user\n+        interrupts to truncate audio that has already been sent to the client but not\n+        yet played. This will synchronize the server's understanding of the audio with\n+        the client's playback.\n+\n+        Truncating audio will delete the server-side text transcript to ensure there\n+        is not text in the context that hasn't been heard by the user.\n+\n+        If successful, the server will respond with a `conversation.item.truncated`\n+        event.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given(\n+                    {\n+                        \"type\": \"conversation.item.truncate\",\n+                        \"audio_end_ms\": audio_end_ms,\n+                        \"content_index\": content_index,\n+                        \"item_id\": item_id,\n+                        \"event_id\": event_id,\n+                    }\n+                ),\n+            )\n+        )\n+\n+    async def retrieve(self, *, item_id: str, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"\n+        Send this event when you want to retrieve the server's representation of a specific item in the conversation history. This is useful, for example, to inspect user audio after noise cancellation and VAD.\n+        The server will respond with a `conversation.item.retrieved` event,\n+        unless the item does not exist in the conversation history, in which case the\n+        server will respond with an error.\n+        \"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"conversation.item.retrieve\", \"item_id\": item_id, \"event_id\": event_id}),\n+            )\n+        )\n+\n+\n+class AsyncRealtimeOutputAudioBufferResource(BaseAsyncRealtimeConnectionResource):\n+    async def clear(self, *, event_id: str | NotGiven = NOT_GIVEN) -> None:\n+        \"\"\"**WebRTC Only:** Emit to cut off the current audio response.\n+\n+        This will trigger the server to\n+        stop generating audio and emit a `output_audio_buffer.cleared` event. This\n+        event should be preceded by a `response.cancel` client event to stop the\n+        generation of the current response.\n+        [Learn more](https://platform.openai.com/docs/guides/realtime-conversations#client-and-server-events-for-audio-in-webrtc).\n+        \"\"\"\n+        await self._connection.send(\n+            cast(RealtimeClientEventParam, strip_not_given({\"type\": \"output_audio_buffer.clear\", \"event_id\": event_id}))\n+        )\n+\n+\n+class AsyncRealtimeTranscriptionSessionResource(BaseAsyncRealtimeConnectionResource):\n+    async def update(\n+        self, *, session: RealtimeTranscriptionSessionCreateRequestParam, event_id: str | NotGiven = NOT_GIVEN\n+    ) -> None:\n+        \"\"\"Send this event to update a transcription session.\"\"\"\n+        await self._connection.send(\n+            cast(\n+                RealtimeClientEventParam,\n+                strip_not_given({\"type\": \"transcription_session.update\", \"session\": session, \"event_id\": event_id}),\n+            )\n+        )\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/resources/responses/responses.py",
            "diff": "diff --git a/src/openai/resources/responses/responses.py b/src/openai/resources/responses/responses.py\nindex e04382a..e459f55 100644\n--- a/src/openai/resources/responses/responses.py\n+++ b/src/openai/resources/responses/responses.py\n@@ -260,7 +260,7 @@ class Responses(SyncAPIResource):\n           tools: An array of tools the model may call while generating a response. You can\n               specify which tool to use by setting the `tool_choice` parameter.\n \n-              The two categories of tools you can provide the model are:\n+              We support the following categories of tools:\n \n               - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n                 capabilities, like\n@@ -268,6 +268,9 @@ class Responses(SyncAPIResource):\n                 [file search](https://platform.openai.com/docs/guides/tools-file-search).\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n+              - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+                predefined connectors such as Google Drive and Notion. Learn more about\n+                [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n                 the model to call your own code with strongly typed arguments and outputs.\n                 Learn more about\n@@ -496,7 +499,7 @@ class Responses(SyncAPIResource):\n           tools: An array of tools the model may call while generating a response. You can\n               specify which tool to use by setting the `tool_choice` parameter.\n \n-              The two categories of tools you can provide the model are:\n+              We support the following categories of tools:\n \n               - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n                 capabilities, like\n@@ -504,6 +507,9 @@ class Responses(SyncAPIResource):\n                 [file search](https://platform.openai.com/docs/guides/tools-file-search).\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n+              - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+                predefined connectors such as Google Drive and Notion. Learn more about\n+                [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n                 the model to call your own code with strongly typed arguments and outputs.\n                 Learn more about\n@@ -732,7 +738,7 @@ class Responses(SyncAPIResource):\n           tools: An array of tools the model may call while generating a response. You can\n               specify which tool to use by setting the `tool_choice` parameter.\n \n-              The two categories of tools you can provide the model are:\n+              We support the following categories of tools:\n \n               - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n                 capabilities, like\n@@ -740,6 +746,9 @@ class Responses(SyncAPIResource):\n                 [file search](https://platform.openai.com/docs/guides/tools-file-search).\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n+              - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+                predefined connectors such as Google Drive and Notion. Learn more about\n+                [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n                 the model to call your own code with strongly typed arguments and outputs.\n                 Learn more about\n@@ -1682,7 +1691,7 @@ class AsyncResponses(AsyncAPIResource):\n           tools: An array of tools the model may call while generating a response. You can\n               specify which tool to use by setting the `tool_choice` parameter.\n \n-              The two categories of tools you can provide the model are:\n+              We support the following categories of tools:\n \n               - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n                 capabilities, like\n@@ -1690,6 +1699,9 @@ class AsyncResponses(AsyncAPIResource):\n                 [file search](https://platform.openai.com/docs/guides/tools-file-search).\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n+              - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+                predefined connectors such as Google Drive and Notion. Learn more about\n+                [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n                 the model to call your own code with strongly typed arguments and outputs.\n                 Learn more about\n@@ -1918,7 +1930,7 @@ class AsyncResponses(AsyncAPIResource):\n           tools: An array of tools the model may call while generating a response. You can\n               specify which tool to use by setting the `tool_choice` parameter.\n \n-              The two categories of tools you can provide the model are:\n+              We support the following categories of tools:\n \n               - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n                 capabilities, like\n@@ -1926,6 +1938,9 @@ class AsyncResponses(AsyncAPIResource):\n                 [file search](https://platform.openai.com/docs/guides/tools-file-search).\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n+              - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+                predefined connectors such as Google Drive and Notion. Learn more about\n+                [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n                 the model to call your own code with strongly typed arguments and outputs.\n                 Learn more about\n@@ -2154,7 +2169,7 @@ class AsyncResponses(AsyncAPIResource):\n           tools: An array of tools the model may call while generating a response. You can\n               specify which tool to use by setting the `tool_choice` parameter.\n \n-              The two categories of tools you can provide the model are:\n+              We support the following categories of tools:\n \n               - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n                 capabilities, like\n@@ -2162,6 +2177,9 @@ class AsyncResponses(AsyncAPIResource):\n                 [file search](https://platform.openai.com/docs/guides/tools-file-search).\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n+              - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+                predefined connectors such as Google Drive and Notion. Learn more about\n+                [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n                 the model to call your own code with strongly typed arguments and outputs.\n                 Learn more about\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/audio/speech_create_params.py",
            "diff": "diff --git a/src/openai/types/audio/speech_create_params.py b/src/openai/types/audio/speech_create_params.py\nindex feeb68c..634d788 100644\n--- a/src/openai/types/audio/speech_create_params.py\n+++ b/src/openai/types/audio/speech_create_params.py\n@@ -20,7 +20,9 @@ class SpeechCreateParams(TypedDict, total=False):\n     `tts-1`, `tts-1-hd` or `gpt-4o-mini-tts`.\n     \"\"\"\n \n-    voice: Required[Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\"]]]\n+    voice: Required[\n+        Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"]]\n+    ]\n     \"\"\"The voice to use when generating the audio.\n \n     Supported voices are `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `onyx`,\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/chat/chat_completion_audio_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_audio_param.py b/src/openai/types/chat/chat_completion_audio_param.py\nindex dc68159..b1576b4 100644\n--- a/src/openai/types/chat/chat_completion_audio_param.py\n+++ b/src/openai/types/chat/chat_completion_audio_param.py\n@@ -15,7 +15,9 @@ class ChatCompletionAudioParam(TypedDict, total=False):\n     Must be one of `wav`, `mp3`, `flac`, `opus`, or `pcm16`.\n     \"\"\"\n \n-    voice: Required[Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\"]]]\n+    voice: Required[\n+        Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"]]\n+    ]\n     \"\"\"The voice the model uses to respond.\n \n     Supported voices are `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`,\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/__init__.py",
            "diff": "diff --git a/src/openai/types/realtime/__init__.py b/src/openai/types/realtime/__init__.py\nnew file mode 100644\nindex 0000000..b05f620\n--- /dev/null\n+++ b/src/openai/types/realtime/__init__.py\n@@ -0,0 +1,184 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from .realtime_error import RealtimeError as RealtimeError\n+from .realtime_session import RealtimeSession as RealtimeSession\n+from .conversation_item import ConversationItem as ConversationItem\n+from .realtime_response import RealtimeResponse as RealtimeResponse\n+from .log_prob_properties import LogProbProperties as LogProbProperties\n+from .realtime_truncation import RealtimeTruncation as RealtimeTruncation\n+from .response_done_event import ResponseDoneEvent as ResponseDoneEvent\n+from .realtime_error_event import RealtimeErrorEvent as RealtimeErrorEvent\n+from .session_update_event import SessionUpdateEvent as SessionUpdateEvent\n+from .mcp_list_tools_failed import McpListToolsFailed as McpListToolsFailed\n+from .realtime_audio_config import RealtimeAudioConfig as RealtimeAudioConfig\n+from .realtime_client_event import RealtimeClientEvent as RealtimeClientEvent\n+from .realtime_server_event import RealtimeServerEvent as RealtimeServerEvent\n+from .realtime_tools_config import RealtimeToolsConfig as RealtimeToolsConfig\n+from .response_cancel_event import ResponseCancelEvent as ResponseCancelEvent\n+from .response_create_event import ResponseCreateEvent as ResponseCreateEvent\n+from .session_created_event import SessionCreatedEvent as SessionCreatedEvent\n+from .session_updated_event import SessionUpdatedEvent as SessionUpdatedEvent\n+from .conversation_item_done import ConversationItemDone as ConversationItemDone\n+from .realtime_mcp_tool_call import RealtimeMcpToolCall as RealtimeMcpToolCall\n+from .realtime_mcphttp_error import RealtimeMcphttpError as RealtimeMcphttpError\n+from .response_created_event import ResponseCreatedEvent as ResponseCreatedEvent\n+from .conversation_item_added import ConversationItemAdded as ConversationItemAdded\n+from .conversation_item_param import ConversationItemParam as ConversationItemParam\n+from .realtime_connect_params import RealtimeConnectParams as RealtimeConnectParams\n+from .realtime_mcp_list_tools import RealtimeMcpListTools as RealtimeMcpListTools\n+from .realtime_response_usage import RealtimeResponseUsage as RealtimeResponseUsage\n+from .realtime_tracing_config import RealtimeTracingConfig as RealtimeTracingConfig\n+from .mcp_list_tools_completed import McpListToolsCompleted as McpListToolsCompleted\n+from .realtime_response_status import RealtimeResponseStatus as RealtimeResponseStatus\n+from .response_mcp_call_failed import ResponseMcpCallFailed as ResponseMcpCallFailed\n+from .response_text_done_event import ResponseTextDoneEvent as ResponseTextDoneEvent\n+from .rate_limits_updated_event import RateLimitsUpdatedEvent as RateLimitsUpdatedEvent\n+from .realtime_truncation_param import RealtimeTruncationParam as RealtimeTruncationParam\n+from .response_audio_done_event import ResponseAudioDoneEvent as ResponseAudioDoneEvent\n+from .response_text_delta_event import ResponseTextDeltaEvent as ResponseTextDeltaEvent\n+from .conversation_created_event import ConversationCreatedEvent as ConversationCreatedEvent\n+from .mcp_list_tools_in_progress import McpListToolsInProgress as McpListToolsInProgress\n+from .response_audio_delta_event import ResponseAudioDeltaEvent as ResponseAudioDeltaEvent\n+from .session_update_event_param import SessionUpdateEventParam as SessionUpdateEventParam\n+from .client_secret_create_params import ClientSecretCreateParams as ClientSecretCreateParams\n+from .realtime_audio_config_param import RealtimeAudioConfigParam as RealtimeAudioConfigParam\n+from .realtime_client_event_param import RealtimeClientEventParam as RealtimeClientEventParam\n+from .realtime_mcp_protocol_error import RealtimeMcpProtocolError as RealtimeMcpProtocolError\n+from .realtime_tool_choice_config import RealtimeToolChoiceConfig as RealtimeToolChoiceConfig\n+from .realtime_tools_config_param import RealtimeToolsConfigParam as RealtimeToolsConfigParam\n+from .realtime_tools_config_union import RealtimeToolsConfigUnion as RealtimeToolsConfigUnion\n+from .response_cancel_event_param import ResponseCancelEventParam as ResponseCancelEventParam\n+from .response_create_event_param import ResponseCreateEventParam as ResponseCreateEventParam\n+from .response_mcp_call_completed import ResponseMcpCallCompleted as ResponseMcpCallCompleted\n+from .realtime_mcp_tool_call_param import RealtimeMcpToolCallParam as RealtimeMcpToolCallParam\n+from .realtime_mcphttp_error_param import RealtimeMcphttpErrorParam as RealtimeMcphttpErrorParam\n+from .transcription_session_update import TranscriptionSessionUpdate as TranscriptionSessionUpdate\n+from .client_secret_create_response import ClientSecretCreateResponse as ClientSecretCreateResponse\n+from .realtime_client_secret_config import RealtimeClientSecretConfig as RealtimeClientSecretConfig\n+from .realtime_mcp_approval_request import RealtimeMcpApprovalRequest as RealtimeMcpApprovalRequest\n+from .realtime_mcp_list_tools_param import RealtimeMcpListToolsParam as RealtimeMcpListToolsParam\n+from .realtime_tracing_config_param import RealtimeTracingConfigParam as RealtimeTracingConfigParam\n+from .response_mcp_call_in_progress import ResponseMcpCallInProgress as ResponseMcpCallInProgress\n+from .transcription_session_created import TranscriptionSessionCreated as TranscriptionSessionCreated\n+from .conversation_item_create_event import ConversationItemCreateEvent as ConversationItemCreateEvent\n+from .conversation_item_delete_event import ConversationItemDeleteEvent as ConversationItemDeleteEvent\n+from .input_audio_buffer_clear_event import InputAudioBufferClearEvent as InputAudioBufferClearEvent\n+from .realtime_mcp_approval_response import RealtimeMcpApprovalResponse as RealtimeMcpApprovalResponse\n+from .conversation_item_created_event import ConversationItemCreatedEvent as ConversationItemCreatedEvent\n+from .conversation_item_deleted_event import ConversationItemDeletedEvent as ConversationItemDeletedEvent\n+from .input_audio_buffer_append_event import InputAudioBufferAppendEvent as InputAudioBufferAppendEvent\n+from .input_audio_buffer_commit_event import InputAudioBufferCommitEvent as InputAudioBufferCommitEvent\n+from .output_audio_buffer_clear_event import OutputAudioBufferClearEvent as OutputAudioBufferClearEvent\n+from .realtime_session_create_request import RealtimeSessionCreateRequest as RealtimeSessionCreateRequest\n+from .response_output_item_done_event import ResponseOutputItemDoneEvent as ResponseOutputItemDoneEvent\n+from .conversation_item_retrieve_event import ConversationItemRetrieveEvent as ConversationItemRetrieveEvent\n+from .conversation_item_truncate_event import ConversationItemTruncateEvent as ConversationItemTruncateEvent\n+from .input_audio_buffer_cleared_event import InputAudioBufferClearedEvent as InputAudioBufferClearedEvent\n+from .realtime_session_create_response import RealtimeSessionCreateResponse as RealtimeSessionCreateResponse\n+from .response_content_part_done_event import ResponseContentPartDoneEvent as ResponseContentPartDoneEvent\n+from .response_mcp_call_arguments_done import ResponseMcpCallArgumentsDone as ResponseMcpCallArgumentsDone\n+from .response_output_item_added_event import ResponseOutputItemAddedEvent as ResponseOutputItemAddedEvent\n+from .conversation_item_truncated_event import ConversationItemTruncatedEvent as ConversationItemTruncatedEvent\n+from .realtime_mcp_protocol_error_param import RealtimeMcpProtocolErrorParam as RealtimeMcpProtocolErrorParam\n+from .realtime_mcp_tool_execution_error import RealtimeMcpToolExecutionError as RealtimeMcpToolExecutionError\n+from .realtime_tool_choice_config_param import RealtimeToolChoiceConfigParam as RealtimeToolChoiceConfigParam\n+from .realtime_tools_config_union_param import RealtimeToolsConfigUnionParam as RealtimeToolsConfigUnionParam\n+from .response_content_part_added_event import ResponseContentPartAddedEvent as ResponseContentPartAddedEvent\n+from .response_mcp_call_arguments_delta import ResponseMcpCallArgumentsDelta as ResponseMcpCallArgumentsDelta\n+from .input_audio_buffer_committed_event import InputAudioBufferCommittedEvent as InputAudioBufferCommittedEvent\n+from .transcription_session_update_param import TranscriptionSessionUpdateParam as TranscriptionSessionUpdateParam\n+from .realtime_client_secret_config_param import RealtimeClientSecretConfigParam as RealtimeClientSecretConfigParam\n+from .realtime_mcp_approval_request_param import RealtimeMcpApprovalRequestParam as RealtimeMcpApprovalRequestParam\n+from .transcription_session_updated_event import TranscriptionSessionUpdatedEvent as TranscriptionSessionUpdatedEvent\n+from .conversation_item_create_event_param import ConversationItemCreateEventParam as ConversationItemCreateEventParam\n+from .conversation_item_delete_event_param import ConversationItemDeleteEventParam as ConversationItemDeleteEventParam\n+from .input_audio_buffer_clear_event_param import InputAudioBufferClearEventParam as InputAudioBufferClearEventParam\n+from .input_audio_buffer_timeout_triggered import InputAudioBufferTimeoutTriggered as InputAudioBufferTimeoutTriggered\n+from .realtime_mcp_approval_response_param import RealtimeMcpApprovalResponseParam as RealtimeMcpApprovalResponseParam\n+from .response_audio_transcript_done_event import ResponseAudioTranscriptDoneEvent as ResponseAudioTranscriptDoneEvent\n+from .input_audio_buffer_append_event_param import InputAudioBufferAppendEventParam as InputAudioBufferAppendEventParam\n+from .input_audio_buffer_commit_event_param import InputAudioBufferCommitEventParam as InputAudioBufferCommitEventParam\n+from .output_audio_buffer_clear_event_param import OutputAudioBufferClearEventParam as OutputAudioBufferClearEventParam\n+from .realtime_session_create_request_param import (\n+    RealtimeSessionCreateRequestParam as RealtimeSessionCreateRequestParam,\n+)\n+from .response_audio_transcript_delta_event import (\n+    ResponseAudioTranscriptDeltaEvent as ResponseAudioTranscriptDeltaEvent,\n+)\n+from .conversation_item_retrieve_event_param import (\n+    ConversationItemRetrieveEventParam as ConversationItemRetrieveEventParam,\n+)\n+from .conversation_item_truncate_event_param import (\n+    ConversationItemTruncateEventParam as ConversationItemTruncateEventParam,\n+)\n+from .input_audio_buffer_speech_started_event import (\n+    InputAudioBufferSpeechStartedEvent as InputAudioBufferSpeechStartedEvent,\n+)\n+from .input_audio_buffer_speech_stopped_event import (\n+    InputAudioBufferSpeechStoppedEvent as InputAudioBufferSpeechStoppedEvent,\n+)\n+from .realtime_conversation_item_user_message import (\n+    RealtimeConversationItemUserMessage as RealtimeConversationItemUserMessage,\n+)\n+from .realtime_mcp_tool_execution_error_param import (\n+    RealtimeMcpToolExecutionErrorParam as RealtimeMcpToolExecutionErrorParam,\n+)\n+from .realtime_conversation_item_function_call import (\n+    RealtimeConversationItemFunctionCall as RealtimeConversationItemFunctionCall,\n+)\n+from .realtime_conversation_item_system_message import (\n+    RealtimeConversationItemSystemMessage as RealtimeConversationItemSystemMessage,\n+)\n+from .realtime_response_usage_input_token_details import (\n+    RealtimeResponseUsageInputTokenDetails as RealtimeResponseUsageInputTokenDetails,\n+)\n+from .response_function_call_arguments_done_event import (\n+    ResponseFunctionCallArgumentsDoneEvent as ResponseFunctionCallArgumentsDoneEvent,\n+)\n+from .realtime_conversation_item_assistant_message import (\n+    RealtimeConversationItemAssistantMessage as RealtimeConversationItemAssistantMessage,\n+)\n+from .realtime_response_usage_output_token_details import (\n+    RealtimeResponseUsageOutputTokenDetails as RealtimeResponseUsageOutputTokenDetails,\n+)\n+from .response_function_call_arguments_delta_event import (\n+    ResponseFunctionCallArgumentsDeltaEvent as ResponseFunctionCallArgumentsDeltaEvent,\n+)\n+from .realtime_conversation_item_user_message_param import (\n+    RealtimeConversationItemUserMessageParam as RealtimeConversationItemUserMessageParam,\n+)\n+from .realtime_transcription_session_create_request import (\n+    RealtimeTranscriptionSessionCreateRequest as RealtimeTranscriptionSessionCreateRequest,\n+)\n+from .realtime_conversation_item_function_call_param import (\n+    RealtimeConversationItemFunctionCallParam as RealtimeConversationItemFunctionCallParam,\n+)\n+from .realtime_conversation_item_function_call_output import (\n+    RealtimeConversationItemFunctionCallOutput as RealtimeConversationItemFunctionCallOutput,\n+)\n+from .realtime_conversation_item_system_message_param import (\n+    RealtimeConversationItemSystemMessageParam as RealtimeConversationItemSystemMessageParam,\n+)\n+from .realtime_conversation_item_assistant_message_param import (\n+    RealtimeConversationItemAssistantMessageParam as RealtimeConversationItemAssistantMessageParam,\n+)\n+from .conversation_item_input_audio_transcription_segment import (\n+    ConversationItemInputAudioTranscriptionSegment as ConversationItemInputAudioTranscriptionSegment,\n+)\n+from .realtime_transcription_session_create_request_param import (\n+    RealtimeTranscriptionSessionCreateRequestParam as RealtimeTranscriptionSessionCreateRequestParam,\n+)\n+from .realtime_conversation_item_function_call_output_param import (\n+    RealtimeConversationItemFunctionCallOutputParam as RealtimeConversationItemFunctionCallOutputParam,\n+)\n+from .conversation_item_input_audio_transcription_delta_event import (\n+    ConversationItemInputAudioTranscriptionDeltaEvent as ConversationItemInputAudioTranscriptionDeltaEvent,\n+)\n+from .conversation_item_input_audio_transcription_failed_event import (\n+    ConversationItemInputAudioTranscriptionFailedEvent as ConversationItemInputAudioTranscriptionFailedEvent,\n+)\n+from .conversation_item_input_audio_transcription_completed_event import (\n+    ConversationItemInputAudioTranscriptionCompletedEvent as ConversationItemInputAudioTranscriptionCompletedEvent,\n+)\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/client_secret_create_params.py",
            "diff": "diff --git a/src/openai/types/realtime/client_secret_create_params.py b/src/openai/types/realtime/client_secret_create_params.py\nnew file mode 100644\nindex 0000000..696176e\n--- /dev/null\n+++ b/src/openai/types/realtime/client_secret_create_params.py\n@@ -0,0 +1,39 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import Literal, TypeAlias, TypedDict\n+\n+from .realtime_session_create_request_param import RealtimeSessionCreateRequestParam\n+from .realtime_transcription_session_create_request_param import RealtimeTranscriptionSessionCreateRequestParam\n+\n+__all__ = [\"ClientSecretCreateParams\", \"ExpiresAfter\", \"Session\"]\n+\n+\n+class ClientSecretCreateParams(TypedDict, total=False):\n+    expires_after: ExpiresAfter\n+    \"\"\"Configuration for the ephemeral token expiration.\"\"\"\n+\n+    session: Session\n+    \"\"\"Session configuration to use for the client secret.\n+\n+    Choose either a realtime session or a transcription session.\n+    \"\"\"\n+\n+\n+class ExpiresAfter(TypedDict, total=False):\n+    anchor: Literal[\"created_at\"]\n+    \"\"\"The anchor point for the ephemeral token expiration.\n+\n+    Only `created_at` is currently supported.\n+    \"\"\"\n+\n+    seconds: int\n+    \"\"\"The number of seconds from the anchor point to the expiration.\n+\n+    Select a value between `10` and `7200`.\n+    \"\"\"\n+\n+\n+Session: TypeAlias = Union[RealtimeSessionCreateRequestParam, RealtimeTranscriptionSessionCreateRequestParam]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/client_secret_create_response.py",
            "diff": "diff --git a/src/openai/types/realtime/client_secret_create_response.py b/src/openai/types/realtime/client_secret_create_response.py\nnew file mode 100644\nindex 0000000..ea8b9f9\n--- /dev/null\n+++ b/src/openai/types/realtime/client_secret_create_response.py\n@@ -0,0 +1,110 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+from .realtime_session_create_response import RealtimeSessionCreateResponse\n+\n+__all__ = [\n+    \"ClientSecretCreateResponse\",\n+    \"Session\",\n+    \"SessionRealtimeTranscriptionSessionCreateResponse\",\n+    \"SessionRealtimeTranscriptionSessionCreateResponseAudio\",\n+    \"SessionRealtimeTranscriptionSessionCreateResponseAudioInput\",\n+    \"SessionRealtimeTranscriptionSessionCreateResponseAudioInputNoiseReduction\",\n+    \"SessionRealtimeTranscriptionSessionCreateResponseAudioInputTranscription\",\n+    \"SessionRealtimeTranscriptionSessionCreateResponseAudioInputTurnDetection\",\n+]\n+\n+\n+class SessionRealtimeTranscriptionSessionCreateResponseAudioInputNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+\n+\n+class SessionRealtimeTranscriptionSessionCreateResponseAudioInputTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Optional[Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"]] = None\n+    \"\"\"The model to use for transcription.\n+\n+    Can be `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, or `whisper-1`.\n+    \"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"An optional text to guide the model's style or continue a previous audio\n+    segment.\n+\n+    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n+    should match the audio language.\n+    \"\"\"\n+\n+\n+class SessionRealtimeTranscriptionSessionCreateResponseAudioInputTurnDetection(BaseModel):\n+    prefix_padding_ms: Optional[int] = None\n+\n+    silence_duration_ms: Optional[int] = None\n+\n+    threshold: Optional[float] = None\n+\n+    type: Optional[str] = None\n+    \"\"\"Type of turn detection, only `server_vad` is currently supported.\"\"\"\n+\n+\n+class SessionRealtimeTranscriptionSessionCreateResponseAudioInput(BaseModel):\n+    format: Optional[str] = None\n+    \"\"\"The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    noise_reduction: Optional[SessionRealtimeTranscriptionSessionCreateResponseAudioInputNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\"\"\"\n+\n+    transcription: Optional[SessionRealtimeTranscriptionSessionCreateResponseAudioInputTranscription] = None\n+    \"\"\"Configuration of the transcription model.\"\"\"\n+\n+    turn_detection: Optional[SessionRealtimeTranscriptionSessionCreateResponseAudioInputTurnDetection] = None\n+    \"\"\"Configuration for turn detection.\"\"\"\n+\n+\n+class SessionRealtimeTranscriptionSessionCreateResponseAudio(BaseModel):\n+    input: Optional[SessionRealtimeTranscriptionSessionCreateResponseAudioInput] = None\n+\n+\n+class SessionRealtimeTranscriptionSessionCreateResponse(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"Unique identifier for the session that looks like `sess_1234567890abcdef`.\"\"\"\n+\n+    audio: Optional[SessionRealtimeTranscriptionSessionCreateResponseAudio] = None\n+    \"\"\"Configuration for input audio for the session.\"\"\"\n+\n+    expires_at: Optional[int] = None\n+    \"\"\"Expiration timestamp for the session, in seconds since epoch.\"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    object: Optional[str] = None\n+    \"\"\"The object type. Always `realtime.transcription_session`.\"\"\"\n+\n+\n+Session: TypeAlias = Union[RealtimeSessionCreateResponse, SessionRealtimeTranscriptionSessionCreateResponse]\n+\n+\n+class ClientSecretCreateResponse(BaseModel):\n+    expires_at: int\n+    \"\"\"Expiration timestamp for the client secret, in seconds since epoch.\"\"\"\n+\n+    session: Session\n+    \"\"\"The session configuration for either a realtime or transcription session.\"\"\"\n+\n+    value: str\n+    \"\"\"The generated client secret value.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_created_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_created_event.py b/src/openai/types/realtime/conversation_created_event.py\nnew file mode 100644\nindex 0000000..6ec1dc8\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_created_event.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationCreatedEvent\", \"Conversation\"]\n+\n+\n+class Conversation(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the conversation.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.conversation\"]] = None\n+    \"\"\"The object type, must be `realtime.conversation`.\"\"\"\n+\n+\n+class ConversationCreatedEvent(BaseModel):\n+    conversation: Conversation\n+    \"\"\"The conversation resource.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    type: Literal[\"conversation.created\"]\n+    \"\"\"The event type, must be `conversation.created`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item.py b/src/openai/types/realtime/conversation_item.py\nnew file mode 100644\nindex 0000000..be02152\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item.py\n@@ -0,0 +1,32 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union\n+from typing_extensions import Annotated, TypeAlias\n+\n+from ..._utils import PropertyInfo\n+from .realtime_mcp_tool_call import RealtimeMcpToolCall\n+from .realtime_mcp_list_tools import RealtimeMcpListTools\n+from .realtime_mcp_approval_request import RealtimeMcpApprovalRequest\n+from .realtime_mcp_approval_response import RealtimeMcpApprovalResponse\n+from .realtime_conversation_item_user_message import RealtimeConversationItemUserMessage\n+from .realtime_conversation_item_function_call import RealtimeConversationItemFunctionCall\n+from .realtime_conversation_item_system_message import RealtimeConversationItemSystemMessage\n+from .realtime_conversation_item_assistant_message import RealtimeConversationItemAssistantMessage\n+from .realtime_conversation_item_function_call_output import RealtimeConversationItemFunctionCallOutput\n+\n+__all__ = [\"ConversationItem\"]\n+\n+ConversationItem: TypeAlias = Annotated[\n+    Union[\n+        RealtimeConversationItemSystemMessage,\n+        RealtimeConversationItemUserMessage,\n+        RealtimeConversationItemAssistantMessage,\n+        RealtimeConversationItemFunctionCall,\n+        RealtimeConversationItemFunctionCallOutput,\n+        RealtimeMcpApprovalResponse,\n+        RealtimeMcpListTools,\n+        RealtimeMcpToolCall,\n+        RealtimeMcpApprovalRequest,\n+    ],\n+    PropertyInfo(discriminator=\"type\"),\n+]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_added.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_added.py b/src/openai/types/realtime/conversation_item_added.py\nnew file mode 100644\nindex 0000000..ae9f680\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_added.py\n@@ -0,0 +1,26 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+\n+__all__ = [\"ConversationItemAdded\"]\n+\n+\n+class ConversationItemAdded(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    type: Literal[\"conversation.item.added\"]\n+    \"\"\"The event type, must be `conversation.item.added`.\"\"\"\n+\n+    previous_item_id: Optional[str] = None\n+    \"\"\"The ID of the item that precedes this one, if any.\n+\n+    This is used to maintain ordering when items are inserted.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_create_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_create_event.py b/src/openai/types/realtime/conversation_item_create_event.py\nnew file mode 100644\nindex 0000000..8fa2dfe\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_create_event.py\n@@ -0,0 +1,29 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+\n+__all__ = [\"ConversationItemCreateEvent\"]\n+\n+\n+class ConversationItemCreateEvent(BaseModel):\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    type: Literal[\"conversation.item.create\"]\n+    \"\"\"The event type, must be `conversation.item.create`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n+\n+    previous_item_id: Optional[str] = None\n+    \"\"\"The ID of the preceding item after which the new item will be inserted.\n+\n+    If not set, the new item will be appended to the end of the conversation. If set\n+    to `root`, the new item will be added to the beginning of the conversation. If\n+    set to an existing ID, it allows an item to be inserted mid-conversation. If the\n+    ID cannot be found, an error will be returned and the item will not be added.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_create_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_create_event_param.py b/src/openai/types/realtime/conversation_item_create_event_param.py\nnew file mode 100644\nindex 0000000..8530dc7\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_create_event_param.py\n@@ -0,0 +1,29 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from .conversation_item_param import ConversationItemParam\n+\n+__all__ = [\"ConversationItemCreateEventParam\"]\n+\n+\n+class ConversationItemCreateEventParam(TypedDict, total=False):\n+    item: Required[ConversationItemParam]\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    type: Required[Literal[\"conversation.item.create\"]]\n+    \"\"\"The event type, must be `conversation.item.create`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n+\n+    previous_item_id: str\n+    \"\"\"The ID of the preceding item after which the new item will be inserted.\n+\n+    If not set, the new item will be appended to the end of the conversation. If set\n+    to `root`, the new item will be added to the beginning of the conversation. If\n+    set to an existing ID, it allows an item to be inserted mid-conversation. If the\n+    ID cannot be found, an error will be returned and the item will not be added.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_created_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_created_event.py b/src/openai/types/realtime/conversation_item_created_event.py\nnew file mode 100644\nindex 0000000..13f24ad\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_created_event.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+\n+__all__ = [\"ConversationItemCreatedEvent\"]\n+\n+\n+class ConversationItemCreatedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    type: Literal[\"conversation.item.created\"]\n+    \"\"\"The event type, must be `conversation.item.created`.\"\"\"\n+\n+    previous_item_id: Optional[str] = None\n+    \"\"\"\n+    The ID of the preceding item in the Conversation context, allows the client to\n+    understand the order of the conversation. Can be `null` if the item has no\n+    predecessor.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_delete_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_delete_event.py b/src/openai/types/realtime/conversation_item_delete_event.py\nnew file mode 100644\nindex 0000000..3734f72\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_delete_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemDeleteEvent\"]\n+\n+\n+class ConversationItemDeleteEvent(BaseModel):\n+    item_id: str\n+    \"\"\"The ID of the item to delete.\"\"\"\n+\n+    type: Literal[\"conversation.item.delete\"]\n+    \"\"\"The event type, must be `conversation.item.delete`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_delete_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_delete_event_param.py b/src/openai/types/realtime/conversation_item_delete_event_param.py\nnew file mode 100644\nindex 0000000..c3f88d6\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_delete_event_param.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ConversationItemDeleteEventParam\"]\n+\n+\n+class ConversationItemDeleteEventParam(TypedDict, total=False):\n+    item_id: Required[str]\n+    \"\"\"The ID of the item to delete.\"\"\"\n+\n+    type: Required[Literal[\"conversation.item.delete\"]]\n+    \"\"\"The event type, must be `conversation.item.delete`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_deleted_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_deleted_event.py b/src/openai/types/realtime/conversation_item_deleted_event.py\nnew file mode 100644\nindex 0000000..cfe6fe8\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_deleted_event.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemDeletedEvent\"]\n+\n+\n+class ConversationItemDeletedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item that was deleted.\"\"\"\n+\n+    type: Literal[\"conversation.item.deleted\"]\n+    \"\"\"The event type, must be `conversation.item.deleted`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_done.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_done.py b/src/openai/types/realtime/conversation_item_done.py\nnew file mode 100644\nindex 0000000..a4c9b8a\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_done.py\n@@ -0,0 +1,26 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+\n+__all__ = [\"ConversationItemDone\"]\n+\n+\n+class ConversationItemDone(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    type: Literal[\"conversation.item.done\"]\n+    \"\"\"The event type, must be `conversation.item.done`.\"\"\"\n+\n+    previous_item_id: Optional[str] = None\n+    \"\"\"The ID of the item that precedes this one, if any.\n+\n+    This is used to maintain ordering when items are inserted.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_input_audio_transcription_completed_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_input_audio_transcription_completed_event.py b/src/openai/types/realtime/conversation_item_input_audio_transcription_completed_event.py\nnew file mode 100644\nindex 0000000..eda3f3b\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_input_audio_transcription_completed_event.py\n@@ -0,0 +1,76 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+from .log_prob_properties import LogProbProperties\n+\n+__all__ = [\n+    \"ConversationItemInputAudioTranscriptionCompletedEvent\",\n+    \"Usage\",\n+    \"UsageTranscriptTextUsageTokens\",\n+    \"UsageTranscriptTextUsageTokensInputTokenDetails\",\n+    \"UsageTranscriptTextUsageDuration\",\n+]\n+\n+\n+class UsageTranscriptTextUsageTokensInputTokenDetails(BaseModel):\n+    audio_tokens: Optional[int] = None\n+    \"\"\"Number of audio tokens billed for this request.\"\"\"\n+\n+    text_tokens: Optional[int] = None\n+    \"\"\"Number of text tokens billed for this request.\"\"\"\n+\n+\n+class UsageTranscriptTextUsageTokens(BaseModel):\n+    input_tokens: int\n+    \"\"\"Number of input tokens billed for this request.\"\"\"\n+\n+    output_tokens: int\n+    \"\"\"Number of output tokens generated.\"\"\"\n+\n+    total_tokens: int\n+    \"\"\"Total number of tokens used (input + output).\"\"\"\n+\n+    type: Literal[\"tokens\"]\n+    \"\"\"The type of the usage object. Always `tokens` for this variant.\"\"\"\n+\n+    input_token_details: Optional[UsageTranscriptTextUsageTokensInputTokenDetails] = None\n+    \"\"\"Details about the input tokens billed for this request.\"\"\"\n+\n+\n+class UsageTranscriptTextUsageDuration(BaseModel):\n+    seconds: float\n+    \"\"\"Duration of the input audio in seconds.\"\"\"\n+\n+    type: Literal[\"duration\"]\n+    \"\"\"The type of the usage object. Always `duration` for this variant.\"\"\"\n+\n+\n+Usage: TypeAlias = Union[UsageTranscriptTextUsageTokens, UsageTranscriptTextUsageDuration]\n+\n+\n+class ConversationItemInputAudioTranscriptionCompletedEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part containing the audio.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the user message item containing the audio.\"\"\"\n+\n+    transcript: str\n+    \"\"\"The transcribed text.\"\"\"\n+\n+    type: Literal[\"conversation.item.input_audio_transcription.completed\"]\n+    \"\"\"\n+    The event type, must be `conversation.item.input_audio_transcription.completed`.\n+    \"\"\"\n+\n+    usage: Usage\n+    \"\"\"Usage statistics for the transcription.\"\"\"\n+\n+    logprobs: Optional[List[LogProbProperties]] = None\n+    \"\"\"The log probabilities of the transcription.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_input_audio_transcription_delta_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_input_audio_transcription_delta_event.py b/src/openai/types/realtime/conversation_item_input_audio_transcription_delta_event.py\nnew file mode 100644\nindex 0000000..4e9528c\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_input_audio_transcription_delta_event.py\n@@ -0,0 +1,29 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .log_prob_properties import LogProbProperties\n+\n+__all__ = [\"ConversationItemInputAudioTranscriptionDeltaEvent\"]\n+\n+\n+class ConversationItemInputAudioTranscriptionDeltaEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    type: Literal[\"conversation.item.input_audio_transcription.delta\"]\n+    \"\"\"The event type, must be `conversation.item.input_audio_transcription.delta`.\"\"\"\n+\n+    content_index: Optional[int] = None\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    delta: Optional[str] = None\n+    \"\"\"The text delta.\"\"\"\n+\n+    logprobs: Optional[List[LogProbProperties]] = None\n+    \"\"\"The log probabilities of the transcription.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_input_audio_transcription_failed_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_input_audio_transcription_failed_event.py b/src/openai/types/realtime/conversation_item_input_audio_transcription_failed_event.py\nnew file mode 100644\nindex 0000000..edb97bb\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_input_audio_transcription_failed_event.py\n@@ -0,0 +1,39 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemInputAudioTranscriptionFailedEvent\", \"Error\"]\n+\n+\n+class Error(BaseModel):\n+    code: Optional[str] = None\n+    \"\"\"Error code, if any.\"\"\"\n+\n+    message: Optional[str] = None\n+    \"\"\"A human-readable error message.\"\"\"\n+\n+    param: Optional[str] = None\n+    \"\"\"Parameter related to the error, if any.\"\"\"\n+\n+    type: Optional[str] = None\n+    \"\"\"The type of error.\"\"\"\n+\n+\n+class ConversationItemInputAudioTranscriptionFailedEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part containing the audio.\"\"\"\n+\n+    error: Error\n+    \"\"\"Details of the transcription error.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the user message item.\"\"\"\n+\n+    type: Literal[\"conversation.item.input_audio_transcription.failed\"]\n+    \"\"\"The event type, must be `conversation.item.input_audio_transcription.failed`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_input_audio_transcription_segment.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_input_audio_transcription_segment.py b/src/openai/types/realtime/conversation_item_input_audio_transcription_segment.py\nnew file mode 100644\nindex 0000000..e2cbc9d\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_input_audio_transcription_segment.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemInputAudioTranscriptionSegment\"]\n+\n+\n+class ConversationItemInputAudioTranscriptionSegment(BaseModel):\n+    id: str\n+    \"\"\"The segment identifier.\"\"\"\n+\n+    content_index: int\n+    \"\"\"The index of the input audio content part within the item.\"\"\"\n+\n+    end: float\n+    \"\"\"End time of the segment in seconds.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item containing the input audio content.\"\"\"\n+\n+    speaker: str\n+    \"\"\"The detected speaker label for this segment.\"\"\"\n+\n+    start: float\n+    \"\"\"Start time of the segment in seconds.\"\"\"\n+\n+    text: str\n+    \"\"\"The text for this segment.\"\"\"\n+\n+    type: Literal[\"conversation.item.input_audio_transcription.segment\"]\n+    \"\"\"The event type, must be `conversation.item.input_audio_transcription.segment`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_param.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_param.py b/src/openai/types/realtime/conversation_item_param.py\nnew file mode 100644\nindex 0000000..c8b442e\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_param.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import TypeAlias\n+\n+from .realtime_mcp_tool_call_param import RealtimeMcpToolCallParam\n+from .realtime_mcp_list_tools_param import RealtimeMcpListToolsParam\n+from .realtime_mcp_approval_request_param import RealtimeMcpApprovalRequestParam\n+from .realtime_mcp_approval_response_param import RealtimeMcpApprovalResponseParam\n+from .realtime_conversation_item_user_message_param import RealtimeConversationItemUserMessageParam\n+from .realtime_conversation_item_function_call_param import RealtimeConversationItemFunctionCallParam\n+from .realtime_conversation_item_system_message_param import RealtimeConversationItemSystemMessageParam\n+from .realtime_conversation_item_assistant_message_param import RealtimeConversationItemAssistantMessageParam\n+from .realtime_conversation_item_function_call_output_param import RealtimeConversationItemFunctionCallOutputParam\n+\n+__all__ = [\"ConversationItemParam\"]\n+\n+ConversationItemParam: TypeAlias = Union[\n+    RealtimeConversationItemSystemMessageParam,\n+    RealtimeConversationItemUserMessageParam,\n+    RealtimeConversationItemAssistantMessageParam,\n+    RealtimeConversationItemFunctionCallParam,\n+    RealtimeConversationItemFunctionCallOutputParam,\n+    RealtimeMcpApprovalResponseParam,\n+    RealtimeMcpListToolsParam,\n+    RealtimeMcpToolCallParam,\n+    RealtimeMcpApprovalRequestParam,\n+]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_retrieve_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_retrieve_event.py b/src/openai/types/realtime/conversation_item_retrieve_event.py\nnew file mode 100644\nindex 0000000..018c2cc\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_retrieve_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemRetrieveEvent\"]\n+\n+\n+class ConversationItemRetrieveEvent(BaseModel):\n+    item_id: str\n+    \"\"\"The ID of the item to retrieve.\"\"\"\n+\n+    type: Literal[\"conversation.item.retrieve\"]\n+    \"\"\"The event type, must be `conversation.item.retrieve`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_retrieve_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_retrieve_event_param.py b/src/openai/types/realtime/conversation_item_retrieve_event_param.py\nnew file mode 100644\nindex 0000000..71b3ffa\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_retrieve_event_param.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ConversationItemRetrieveEventParam\"]\n+\n+\n+class ConversationItemRetrieveEventParam(TypedDict, total=False):\n+    item_id: Required[str]\n+    \"\"\"The ID of the item to retrieve.\"\"\"\n+\n+    type: Required[Literal[\"conversation.item.retrieve\"]]\n+    \"\"\"The event type, must be `conversation.item.retrieve`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_truncate_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_truncate_event.py b/src/openai/types/realtime/conversation_item_truncate_event.py\nnew file mode 100644\nindex 0000000..63b591b\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_truncate_event.py\n@@ -0,0 +1,32 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemTruncateEvent\"]\n+\n+\n+class ConversationItemTruncateEvent(BaseModel):\n+    audio_end_ms: int\n+    \"\"\"Inclusive duration up to which audio is truncated, in milliseconds.\n+\n+    If the audio_end_ms is greater than the actual audio duration, the server will\n+    respond with an error.\n+    \"\"\"\n+\n+    content_index: int\n+    \"\"\"The index of the content part to truncate. Set this to 0.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the assistant message item to truncate.\n+\n+    Only assistant message items can be truncated.\n+    \"\"\"\n+\n+    type: Literal[\"conversation.item.truncate\"]\n+    \"\"\"The event type, must be `conversation.item.truncate`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_truncate_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_truncate_event_param.py b/src/openai/types/realtime/conversation_item_truncate_event_param.py\nnew file mode 100644\nindex 0000000..d3ad1e1\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_truncate_event_param.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ConversationItemTruncateEventParam\"]\n+\n+\n+class ConversationItemTruncateEventParam(TypedDict, total=False):\n+    audio_end_ms: Required[int]\n+    \"\"\"Inclusive duration up to which audio is truncated, in milliseconds.\n+\n+    If the audio_end_ms is greater than the actual audio duration, the server will\n+    respond with an error.\n+    \"\"\"\n+\n+    content_index: Required[int]\n+    \"\"\"The index of the content part to truncate. Set this to 0.\"\"\"\n+\n+    item_id: Required[str]\n+    \"\"\"The ID of the assistant message item to truncate.\n+\n+    Only assistant message items can be truncated.\n+    \"\"\"\n+\n+    type: Required[Literal[\"conversation.item.truncate\"]]\n+    \"\"\"The event type, must be `conversation.item.truncate`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/conversation_item_truncated_event.py",
            "diff": "diff --git a/src/openai/types/realtime/conversation_item_truncated_event.py b/src/openai/types/realtime/conversation_item_truncated_event.py\nnew file mode 100644\nindex 0000000..f56cabc\n--- /dev/null\n+++ b/src/openai/types/realtime/conversation_item_truncated_event.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ConversationItemTruncatedEvent\"]\n+\n+\n+class ConversationItemTruncatedEvent(BaseModel):\n+    audio_end_ms: int\n+    \"\"\"The duration up to which the audio was truncated, in milliseconds.\"\"\"\n+\n+    content_index: int\n+    \"\"\"The index of the content part that was truncated.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the assistant message item that was truncated.\"\"\"\n+\n+    type: Literal[\"conversation.item.truncated\"]\n+    \"\"\"The event type, must be `conversation.item.truncated`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_append_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_append_event.py b/src/openai/types/realtime/input_audio_buffer_append_event.py\nnew file mode 100644\nindex 0000000..8562cf0\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_append_event.py\n@@ -0,0 +1,23 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferAppendEvent\"]\n+\n+\n+class InputAudioBufferAppendEvent(BaseModel):\n+    audio: str\n+    \"\"\"Base64-encoded audio bytes.\n+\n+    This must be in the format specified by the `input_audio_format` field in the\n+    session configuration.\n+    \"\"\"\n+\n+    type: Literal[\"input_audio_buffer.append\"]\n+    \"\"\"The event type, must be `input_audio_buffer.append`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_append_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_append_event_param.py b/src/openai/types/realtime/input_audio_buffer_append_event_param.py\nnew file mode 100644\nindex 0000000..3ad0bc7\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_append_event_param.py\n@@ -0,0 +1,22 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"InputAudioBufferAppendEventParam\"]\n+\n+\n+class InputAudioBufferAppendEventParam(TypedDict, total=False):\n+    audio: Required[str]\n+    \"\"\"Base64-encoded audio bytes.\n+\n+    This must be in the format specified by the `input_audio_format` field in the\n+    session configuration.\n+    \"\"\"\n+\n+    type: Required[Literal[\"input_audio_buffer.append\"]]\n+    \"\"\"The event type, must be `input_audio_buffer.append`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_clear_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_clear_event.py b/src/openai/types/realtime/input_audio_buffer_clear_event.py\nnew file mode 100644\nindex 0000000..9922ff3\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_clear_event.py\n@@ -0,0 +1,16 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferClearEvent\"]\n+\n+\n+class InputAudioBufferClearEvent(BaseModel):\n+    type: Literal[\"input_audio_buffer.clear\"]\n+    \"\"\"The event type, must be `input_audio_buffer.clear`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_clear_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_clear_event_param.py b/src/openai/types/realtime/input_audio_buffer_clear_event_param.py\nnew file mode 100644\nindex 0000000..2bd6bc5\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_clear_event_param.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"InputAudioBufferClearEventParam\"]\n+\n+\n+class InputAudioBufferClearEventParam(TypedDict, total=False):\n+    type: Required[Literal[\"input_audio_buffer.clear\"]]\n+    \"\"\"The event type, must be `input_audio_buffer.clear`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_cleared_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_cleared_event.py b/src/openai/types/realtime/input_audio_buffer_cleared_event.py\nnew file mode 100644\nindex 0000000..af71844\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_cleared_event.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferClearedEvent\"]\n+\n+\n+class InputAudioBufferClearedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    type: Literal[\"input_audio_buffer.cleared\"]\n+    \"\"\"The event type, must be `input_audio_buffer.cleared`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_commit_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_commit_event.py b/src/openai/types/realtime/input_audio_buffer_commit_event.py\nnew file mode 100644\nindex 0000000..125c3ba\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_commit_event.py\n@@ -0,0 +1,16 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferCommitEvent\"]\n+\n+\n+class InputAudioBufferCommitEvent(BaseModel):\n+    type: Literal[\"input_audio_buffer.commit\"]\n+    \"\"\"The event type, must be `input_audio_buffer.commit`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_commit_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_commit_event_param.py b/src/openai/types/realtime/input_audio_buffer_commit_event_param.py\nnew file mode 100644\nindex 0000000..c9c927a\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_commit_event_param.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"InputAudioBufferCommitEventParam\"]\n+\n+\n+class InputAudioBufferCommitEventParam(TypedDict, total=False):\n+    type: Required[Literal[\"input_audio_buffer.commit\"]]\n+    \"\"\"The event type, must be `input_audio_buffer.commit`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_committed_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_committed_event.py b/src/openai/types/realtime/input_audio_buffer_committed_event.py\nnew file mode 100644\nindex 0000000..5ed1b4c\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_committed_event.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferCommittedEvent\"]\n+\n+\n+class InputAudioBufferCommittedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the user message item that will be created.\"\"\"\n+\n+    type: Literal[\"input_audio_buffer.committed\"]\n+    \"\"\"The event type, must be `input_audio_buffer.committed`.\"\"\"\n+\n+    previous_item_id: Optional[str] = None\n+    \"\"\"\n+    The ID of the preceding item after which the new item will be inserted. Can be\n+    `null` if the item has no predecessor.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_speech_started_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_speech_started_event.py b/src/openai/types/realtime/input_audio_buffer_speech_started_event.py\nnew file mode 100644\nindex 0000000..865205d\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_speech_started_event.py\n@@ -0,0 +1,26 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferSpeechStartedEvent\"]\n+\n+\n+class InputAudioBufferSpeechStartedEvent(BaseModel):\n+    audio_start_ms: int\n+    \"\"\"\n+    Milliseconds from the start of all audio written to the buffer during the\n+    session when speech was first detected. This will correspond to the beginning of\n+    audio sent to the model, and thus includes the `prefix_padding_ms` configured in\n+    the Session.\n+    \"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the user message item that will be created when speech stops.\"\"\"\n+\n+    type: Literal[\"input_audio_buffer.speech_started\"]\n+    \"\"\"The event type, must be `input_audio_buffer.speech_started`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_speech_stopped_event.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_speech_stopped_event.py b/src/openai/types/realtime/input_audio_buffer_speech_stopped_event.py\nnew file mode 100644\nindex 0000000..6cb7845\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_speech_stopped_event.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferSpeechStoppedEvent\"]\n+\n+\n+class InputAudioBufferSpeechStoppedEvent(BaseModel):\n+    audio_end_ms: int\n+    \"\"\"Milliseconds since the session started when speech stopped.\n+\n+    This will correspond to the end of audio sent to the model, and thus includes\n+    the `min_silence_duration_ms` configured in the Session.\n+    \"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the user message item that will be created.\"\"\"\n+\n+    type: Literal[\"input_audio_buffer.speech_stopped\"]\n+    \"\"\"The event type, must be `input_audio_buffer.speech_stopped`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/input_audio_buffer_timeout_triggered.py",
            "diff": "diff --git a/src/openai/types/realtime/input_audio_buffer_timeout_triggered.py b/src/openai/types/realtime/input_audio_buffer_timeout_triggered.py\nnew file mode 100644\nindex 0000000..ed592ac\n--- /dev/null\n+++ b/src/openai/types/realtime/input_audio_buffer_timeout_triggered.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"InputAudioBufferTimeoutTriggered\"]\n+\n+\n+class InputAudioBufferTimeoutTriggered(BaseModel):\n+    audio_end_ms: int\n+    \"\"\"Millisecond offset where speech ended within the buffered audio.\"\"\"\n+\n+    audio_start_ms: int\n+    \"\"\"Millisecond offset where speech started within the buffered audio.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item associated with this segment.\"\"\"\n+\n+    type: Literal[\"input_audio_buffer.timeout_triggered\"]\n+    \"\"\"The event type, must be `input_audio_buffer.timeout_triggered`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/log_prob_properties.py",
            "diff": "diff --git a/src/openai/types/realtime/log_prob_properties.py b/src/openai/types/realtime/log_prob_properties.py\nnew file mode 100644\nindex 0000000..92477d6\n--- /dev/null\n+++ b/src/openai/types/realtime/log_prob_properties.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"LogProbProperties\"]\n+\n+\n+class LogProbProperties(BaseModel):\n+    token: str\n+    \"\"\"The token that was used to generate the log probability.\"\"\"\n+\n+    bytes: List[int]\n+    \"\"\"The bytes that were used to generate the log probability.\"\"\"\n+\n+    logprob: float\n+    \"\"\"The log probability of the token.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/mcp_list_tools_completed.py",
            "diff": "diff --git a/src/openai/types/realtime/mcp_list_tools_completed.py b/src/openai/types/realtime/mcp_list_tools_completed.py\nnew file mode 100644\nindex 0000000..941280f\n--- /dev/null\n+++ b/src/openai/types/realtime/mcp_list_tools_completed.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"McpListToolsCompleted\"]\n+\n+\n+class McpListToolsCompleted(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP list tools item.\"\"\"\n+\n+    type: Literal[\"mcp_list_tools.completed\"]\n+    \"\"\"The event type, must be `mcp_list_tools.completed`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/mcp_list_tools_failed.py",
            "diff": "diff --git a/src/openai/types/realtime/mcp_list_tools_failed.py b/src/openai/types/realtime/mcp_list_tools_failed.py\nnew file mode 100644\nindex 0000000..892eda2\n--- /dev/null\n+++ b/src/openai/types/realtime/mcp_list_tools_failed.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"McpListToolsFailed\"]\n+\n+\n+class McpListToolsFailed(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP list tools item.\"\"\"\n+\n+    type: Literal[\"mcp_list_tools.failed\"]\n+    \"\"\"The event type, must be `mcp_list_tools.failed`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/mcp_list_tools_in_progress.py",
            "diff": "diff --git a/src/openai/types/realtime/mcp_list_tools_in_progress.py b/src/openai/types/realtime/mcp_list_tools_in_progress.py\nnew file mode 100644\nindex 0000000..4254b5f\n--- /dev/null\n+++ b/src/openai/types/realtime/mcp_list_tools_in_progress.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"McpListToolsInProgress\"]\n+\n+\n+class McpListToolsInProgress(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP list tools item.\"\"\"\n+\n+    type: Literal[\"mcp_list_tools.in_progress\"]\n+    \"\"\"The event type, must be `mcp_list_tools.in_progress`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/output_audio_buffer_clear_event.py",
            "diff": "diff --git a/src/openai/types/realtime/output_audio_buffer_clear_event.py b/src/openai/types/realtime/output_audio_buffer_clear_event.py\nnew file mode 100644\nindex 0000000..b4c9503\n--- /dev/null\n+++ b/src/openai/types/realtime/output_audio_buffer_clear_event.py\n@@ -0,0 +1,16 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"OutputAudioBufferClearEvent\"]\n+\n+\n+class OutputAudioBufferClearEvent(BaseModel):\n+    type: Literal[\"output_audio_buffer.clear\"]\n+    \"\"\"The event type, must be `output_audio_buffer.clear`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"The unique ID of the client event used for error handling.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/output_audio_buffer_clear_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/output_audio_buffer_clear_event_param.py b/src/openai/types/realtime/output_audio_buffer_clear_event_param.py\nnew file mode 100644\nindex 0000000..a3205eb\n--- /dev/null\n+++ b/src/openai/types/realtime/output_audio_buffer_clear_event_param.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"OutputAudioBufferClearEventParam\"]\n+\n+\n+class OutputAudioBufferClearEventParam(TypedDict, total=False):\n+    type: Required[Literal[\"output_audio_buffer.clear\"]]\n+    \"\"\"The event type, must be `output_audio_buffer.clear`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the client event used for error handling.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/rate_limits_updated_event.py",
            "diff": "diff --git a/src/openai/types/realtime/rate_limits_updated_event.py b/src/openai/types/realtime/rate_limits_updated_event.py\nnew file mode 100644\nindex 0000000..048a402\n--- /dev/null\n+++ b/src/openai/types/realtime/rate_limits_updated_event.py\n@@ -0,0 +1,33 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RateLimitsUpdatedEvent\", \"RateLimit\"]\n+\n+\n+class RateLimit(BaseModel):\n+    limit: Optional[int] = None\n+    \"\"\"The maximum allowed value for the rate limit.\"\"\"\n+\n+    name: Optional[Literal[\"requests\", \"tokens\"]] = None\n+    \"\"\"The name of the rate limit (`requests`, `tokens`).\"\"\"\n+\n+    remaining: Optional[int] = None\n+    \"\"\"The remaining value before the limit is reached.\"\"\"\n+\n+    reset_seconds: Optional[float] = None\n+    \"\"\"Seconds until the rate limit resets.\"\"\"\n+\n+\n+class RateLimitsUpdatedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    rate_limits: List[RateLimit]\n+    \"\"\"List of rate limit information.\"\"\"\n+\n+    type: Literal[\"rate_limits.updated\"]\n+    \"\"\"The event type, must be `rate_limits.updated`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_audio_config.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_audio_config.py b/src/openai/types/realtime/realtime_audio_config.py\nnew file mode 100644\nindex 0000000..7463c70\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_audio_config.py\n@@ -0,0 +1,184 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeAudioConfig\", \"Input\", \"InputNoiseReduction\", \"InputTranscription\", \"InputTurnDetection\", \"Output\"]\n+\n+\n+class InputNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+    \"\"\"Type of noise reduction.\n+\n+    `near_field` is for close-talking microphones such as headphones, `far_field` is\n+    for far-field microphones such as laptop or conference room microphones.\n+    \"\"\"\n+\n+\n+class InputTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Optional[\n+        Literal[\n+            \"whisper-1\",\n+            \"gpt-4o-transcribe-latest\",\n+            \"gpt-4o-mini-transcribe\",\n+            \"gpt-4o-transcribe\",\n+            \"gpt-4o-transcribe-diarize\",\n+        ]\n+    ] = None\n+    \"\"\"The model to use for transcription.\n+\n+    Current options are `whisper-1`, `gpt-4o-transcribe-latest`,\n+    `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`.\n+    \"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"\n+    An optional text to guide the model's style or continue a previous audio\n+    segment. For `whisper-1`, the\n+    [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n+    For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n+    \"expect words related to technology\".\n+    \"\"\"\n+\n+\n+class InputTurnDetection(BaseModel):\n+    create_response: Optional[bool] = None\n+    \"\"\"\n+    Whether or not to automatically generate a response when a VAD stop event\n+    occurs.\n+    \"\"\"\n+\n+    eagerness: Optional[Literal[\"low\", \"medium\", \"high\", \"auto\"]] = None\n+    \"\"\"Used only for `semantic_vad` mode.\n+\n+    The eagerness of the model to respond. `low` will wait longer for the user to\n+    continue speaking, `high` will respond more quickly. `auto` is the default and\n+    is equivalent to `medium`.\n+    \"\"\"\n+\n+    idle_timeout_ms: Optional[int] = None\n+    \"\"\"\n+    Optional idle timeout after which turn detection will auto-timeout when no\n+    additional audio is received.\n+    \"\"\"\n+\n+    interrupt_response: Optional[bool] = None\n+    \"\"\"\n+    Whether or not to automatically interrupt any ongoing response with output to\n+    the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n+    occurs.\n+    \"\"\"\n+\n+    prefix_padding_ms: Optional[int] = None\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Amount of audio to include before the VAD detected speech (in milliseconds).\n+    Defaults to 300ms.\n+    \"\"\"\n+\n+    silence_duration_ms: Optional[int] = None\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n+    With shorter values the model will respond more quickly, but may jump in on\n+    short pauses from the user.\n+    \"\"\"\n+\n+    threshold: Optional[float] = None\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n+    threshold will require louder audio to activate the model, and thus might\n+    perform better in noisy environments.\n+    \"\"\"\n+\n+    type: Optional[Literal[\"server_vad\", \"semantic_vad\"]] = None\n+    \"\"\"Type of turn detection.\"\"\"\n+\n+\n+class Input(BaseModel):\n+    format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of input audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, input audio must\n+    be 16-bit PCM at a 24kHz sample rate, single channel (mono), and little-endian\n+    byte order.\n+    \"\"\"\n+\n+    noise_reduction: Optional[InputNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\n+\n+    This can be set to `null` to turn off. Noise reduction filters audio added to\n+    the input audio buffer before it is sent to VAD and the model. Filtering the\n+    audio can improve VAD and turn detection accuracy (reducing false positives) and\n+    model performance by improving perception of the input audio.\n+    \"\"\"\n+\n+    transcription: Optional[InputTranscription] = None\n+    \"\"\"\n+    Configuration for input audio transcription, defaults to off and can be set to\n+    `null` to turn off once on. Input audio transcription is not native to the\n+    model, since the model consumes audio directly. Transcription runs\n+    asynchronously through\n+    [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n+    and should be treated as guidance of input audio content rather than precisely\n+    what the model heard. The client can optionally set the language and prompt for\n+    transcription, these offer additional guidance to the transcription service.\n+    \"\"\"\n+\n+    turn_detection: Optional[InputTurnDetection] = None\n+    \"\"\"Configuration for turn detection, ether Server VAD or Semantic VAD.\n+\n+    This can be set to `null` to turn off, in which case the client must manually\n+    trigger model response. Server VAD means that the model will detect the start\n+    and end of speech based on audio volume and respond at the end of user speech.\n+    Semantic VAD is more advanced and uses a turn detection model (in conjunction\n+    with VAD) to semantically estimate whether the user has finished speaking, then\n+    dynamically sets a timeout based on this probability. For example, if user audio\n+    trails off with \"uhhm\", the model will score a low probability of turn end and\n+    wait longer for the user to continue speaking. This can be useful for more\n+    natural conversations, but may have a higher latency.\n+    \"\"\"\n+\n+\n+class Output(BaseModel):\n+    format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of output audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, output audio is\n+    sampled at a rate of 24kHz.\n+    \"\"\"\n+\n+    speed: Optional[float] = None\n+    \"\"\"The speed of the model's spoken response.\n+\n+    1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed.\n+    This value can only be changed in between model turns, not while a response is\n+    in progress.\n+    \"\"\"\n+\n+    voice: Union[\n+        str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"], None\n+    ] = None\n+    \"\"\"The voice the model uses to respond.\n+\n+    Voice cannot be changed during the session once the model has responded with\n+    audio at least once. Current voice options are `alloy`, `ash`, `ballad`,\n+    `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`, and `cedar`.\n+    \"\"\"\n+\n+\n+class RealtimeAudioConfig(BaseModel):\n+    input: Optional[Input] = None\n+\n+    output: Optional[Output] = None\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_audio_config_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_audio_config_param.py b/src/openai/types/realtime/realtime_audio_config_param.py\nnew file mode 100644\nindex 0000000..9f2e12e\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_audio_config_param.py\n@@ -0,0 +1,187 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal, TypedDict\n+\n+__all__ = [\n+    \"RealtimeAudioConfigParam\",\n+    \"Input\",\n+    \"InputNoiseReduction\",\n+    \"InputTranscription\",\n+    \"InputTurnDetection\",\n+    \"Output\",\n+]\n+\n+\n+class InputNoiseReduction(TypedDict, total=False):\n+    type: Literal[\"near_field\", \"far_field\"]\n+    \"\"\"Type of noise reduction.\n+\n+    `near_field` is for close-talking microphones such as headphones, `far_field` is\n+    for far-field microphones such as laptop or conference room microphones.\n+    \"\"\"\n+\n+\n+class InputTranscription(TypedDict, total=False):\n+    language: str\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Literal[\n+        \"whisper-1\",\n+        \"gpt-4o-transcribe-latest\",\n+        \"gpt-4o-mini-transcribe\",\n+        \"gpt-4o-transcribe\",\n+        \"gpt-4o-transcribe-diarize\",\n+    ]\n+    \"\"\"The model to use for transcription.\n+\n+    Current options are `whisper-1`, `gpt-4o-transcribe-latest`,\n+    `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`.\n+    \"\"\"\n+\n+    prompt: str\n+    \"\"\"\n+    An optional text to guide the model's style or continue a previous audio\n+    segment. For `whisper-1`, the\n+    [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n+    For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n+    \"expect words related to technology\".\n+    \"\"\"\n+\n+\n+class InputTurnDetection(TypedDict, total=False):\n+    create_response: bool\n+    \"\"\"\n+    Whether or not to automatically generate a response when a VAD stop event\n+    occurs.\n+    \"\"\"\n+\n+    eagerness: Literal[\"low\", \"medium\", \"high\", \"auto\"]\n+    \"\"\"Used only for `semantic_vad` mode.\n+\n+    The eagerness of the model to respond. `low` will wait longer for the user to\n+    continue speaking, `high` will respond more quickly. `auto` is the default and\n+    is equivalent to `medium`.\n+    \"\"\"\n+\n+    idle_timeout_ms: Optional[int]\n+    \"\"\"\n+    Optional idle timeout after which turn detection will auto-timeout when no\n+    additional audio is received.\n+    \"\"\"\n+\n+    interrupt_response: bool\n+    \"\"\"\n+    Whether or not to automatically interrupt any ongoing response with output to\n+    the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n+    occurs.\n+    \"\"\"\n+\n+    prefix_padding_ms: int\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Amount of audio to include before the VAD detected speech (in milliseconds).\n+    Defaults to 300ms.\n+    \"\"\"\n+\n+    silence_duration_ms: int\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n+    With shorter values the model will respond more quickly, but may jump in on\n+    short pauses from the user.\n+    \"\"\"\n+\n+    threshold: float\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n+    threshold will require louder audio to activate the model, and thus might\n+    perform better in noisy environments.\n+    \"\"\"\n+\n+    type: Literal[\"server_vad\", \"semantic_vad\"]\n+    \"\"\"Type of turn detection.\"\"\"\n+\n+\n+class Input(TypedDict, total=False):\n+    format: Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]\n+    \"\"\"The format of input audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, input audio must\n+    be 16-bit PCM at a 24kHz sample rate, single channel (mono), and little-endian\n+    byte order.\n+    \"\"\"\n+\n+    noise_reduction: InputNoiseReduction\n+    \"\"\"Configuration for input audio noise reduction.\n+\n+    This can be set to `null` to turn off. Noise reduction filters audio added to\n+    the input audio buffer before it is sent to VAD and the model. Filtering the\n+    audio can improve VAD and turn detection accuracy (reducing false positives) and\n+    model performance by improving perception of the input audio.\n+    \"\"\"\n+\n+    transcription: InputTranscription\n+    \"\"\"\n+    Configuration for input audio transcription, defaults to off and can be set to\n+    `null` to turn off once on. Input audio transcription is not native to the\n+    model, since the model consumes audio directly. Transcription runs\n+    asynchronously through\n+    [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n+    and should be treated as guidance of input audio content rather than precisely\n+    what the model heard. The client can optionally set the language and prompt for\n+    transcription, these offer additional guidance to the transcription service.\n+    \"\"\"\n+\n+    turn_detection: InputTurnDetection\n+    \"\"\"Configuration for turn detection, ether Server VAD or Semantic VAD.\n+\n+    This can be set to `null` to turn off, in which case the client must manually\n+    trigger model response. Server VAD means that the model will detect the start\n+    and end of speech based on audio volume and respond at the end of user speech.\n+    Semantic VAD is more advanced and uses a turn detection model (in conjunction\n+    with VAD) to semantically estimate whether the user has finished speaking, then\n+    dynamically sets a timeout based on this probability. For example, if user audio\n+    trails off with \"uhhm\", the model will score a low probability of turn end and\n+    wait longer for the user to continue speaking. This can be useful for more\n+    natural conversations, but may have a higher latency.\n+    \"\"\"\n+\n+\n+class Output(TypedDict, total=False):\n+    format: Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]\n+    \"\"\"The format of output audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, output audio is\n+    sampled at a rate of 24kHz.\n+    \"\"\"\n+\n+    speed: float\n+    \"\"\"The speed of the model's spoken response.\n+\n+    1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed.\n+    This value can only be changed in between model turns, not while a response is\n+    in progress.\n+    \"\"\"\n+\n+    voice: Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"]]\n+    \"\"\"The voice the model uses to respond.\n+\n+    Voice cannot be changed during the session once the model has responded with\n+    audio at least once. Current voice options are `alloy`, `ash`, `ballad`,\n+    `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`, and `cedar`.\n+    \"\"\"\n+\n+\n+class RealtimeAudioConfigParam(TypedDict, total=False):\n+    input: Input\n+\n+    output: Output\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_client_event.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_client_event.py b/src/openai/types/realtime/realtime_client_event.py\nnew file mode 100644\nindex 0000000..8c2c95e\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_client_event.py\n@@ -0,0 +1,38 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union\n+from typing_extensions import Annotated, TypeAlias\n+\n+from ..._utils import PropertyInfo\n+from .session_update_event import SessionUpdateEvent\n+from .response_cancel_event import ResponseCancelEvent\n+from .response_create_event import ResponseCreateEvent\n+from .transcription_session_update import TranscriptionSessionUpdate\n+from .conversation_item_create_event import ConversationItemCreateEvent\n+from .conversation_item_delete_event import ConversationItemDeleteEvent\n+from .input_audio_buffer_clear_event import InputAudioBufferClearEvent\n+from .input_audio_buffer_append_event import InputAudioBufferAppendEvent\n+from .input_audio_buffer_commit_event import InputAudioBufferCommitEvent\n+from .output_audio_buffer_clear_event import OutputAudioBufferClearEvent\n+from .conversation_item_retrieve_event import ConversationItemRetrieveEvent\n+from .conversation_item_truncate_event import ConversationItemTruncateEvent\n+\n+__all__ = [\"RealtimeClientEvent\"]\n+\n+RealtimeClientEvent: TypeAlias = Annotated[\n+    Union[\n+        ConversationItemCreateEvent,\n+        ConversationItemDeleteEvent,\n+        ConversationItemRetrieveEvent,\n+        ConversationItemTruncateEvent,\n+        InputAudioBufferAppendEvent,\n+        InputAudioBufferClearEvent,\n+        OutputAudioBufferClearEvent,\n+        InputAudioBufferCommitEvent,\n+        ResponseCancelEvent,\n+        ResponseCreateEvent,\n+        SessionUpdateEvent,\n+        TranscriptionSessionUpdate,\n+    ],\n+    PropertyInfo(discriminator=\"type\"),\n+]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_client_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_client_event_param.py b/src/openai/types/realtime/realtime_client_event_param.py\nnew file mode 100644\nindex 0000000..8e042dd\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_client_event_param.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import TypeAlias\n+\n+from .session_update_event_param import SessionUpdateEventParam\n+from .response_cancel_event_param import ResponseCancelEventParam\n+from .response_create_event_param import ResponseCreateEventParam\n+from .transcription_session_update_param import TranscriptionSessionUpdateParam\n+from .conversation_item_create_event_param import ConversationItemCreateEventParam\n+from .conversation_item_delete_event_param import ConversationItemDeleteEventParam\n+from .input_audio_buffer_clear_event_param import InputAudioBufferClearEventParam\n+from .input_audio_buffer_append_event_param import InputAudioBufferAppendEventParam\n+from .input_audio_buffer_commit_event_param import InputAudioBufferCommitEventParam\n+from .output_audio_buffer_clear_event_param import OutputAudioBufferClearEventParam\n+from .conversation_item_retrieve_event_param import ConversationItemRetrieveEventParam\n+from .conversation_item_truncate_event_param import ConversationItemTruncateEventParam\n+\n+__all__ = [\"RealtimeClientEventParam\"]\n+\n+RealtimeClientEventParam: TypeAlias = Union[\n+    ConversationItemCreateEventParam,\n+    ConversationItemDeleteEventParam,\n+    ConversationItemRetrieveEventParam,\n+    ConversationItemTruncateEventParam,\n+    InputAudioBufferAppendEventParam,\n+    InputAudioBufferClearEventParam,\n+    OutputAudioBufferClearEventParam,\n+    InputAudioBufferCommitEventParam,\n+    ResponseCancelEventParam,\n+    ResponseCreateEventParam,\n+    SessionUpdateEventParam,\n+    TranscriptionSessionUpdateParam,\n+]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_client_secret_config.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_client_secret_config.py b/src/openai/types/realtime/realtime_client_secret_config.py\nnew file mode 100644\nindex 0000000..29f8f57\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_client_secret_config.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeClientSecretConfig\", \"ExpiresAfter\"]\n+\n+\n+class ExpiresAfter(BaseModel):\n+    anchor: Literal[\"created_at\"]\n+    \"\"\"The anchor point for the ephemeral token expiration.\n+\n+    Only `created_at` is currently supported.\n+    \"\"\"\n+\n+    seconds: Optional[int] = None\n+    \"\"\"The number of seconds from the anchor point to the expiration.\n+\n+    Select a value between `10` and `7200`.\n+    \"\"\"\n+\n+\n+class RealtimeClientSecretConfig(BaseModel):\n+    expires_after: Optional[ExpiresAfter] = None\n+    \"\"\"Configuration for the ephemeral token expiration.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_client_secret_config_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_client_secret_config_param.py b/src/openai/types/realtime/realtime_client_secret_config_param.py\nnew file mode 100644\nindex 0000000..30a8013\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_client_secret_config_param.py\n@@ -0,0 +1,26 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeClientSecretConfigParam\", \"ExpiresAfter\"]\n+\n+\n+class ExpiresAfter(TypedDict, total=False):\n+    anchor: Required[Literal[\"created_at\"]]\n+    \"\"\"The anchor point for the ephemeral token expiration.\n+\n+    Only `created_at` is currently supported.\n+    \"\"\"\n+\n+    seconds: int\n+    \"\"\"The number of seconds from the anchor point to the expiration.\n+\n+    Select a value between `10` and `7200`.\n+    \"\"\"\n+\n+\n+class RealtimeClientSecretConfigParam(TypedDict, total=False):\n+    expires_after: ExpiresAfter\n+    \"\"\"Configuration for the ephemeral token expiration.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_connect_params.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_connect_params.py b/src/openai/types/realtime/realtime_connect_params.py\nnew file mode 100644\nindex 0000000..76474f3\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_connect_params.py\n@@ -0,0 +1,11 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Required, TypedDict\n+\n+__all__ = [\"RealtimeConnectParams\"]\n+\n+\n+class RealtimeConnectParams(TypedDict, total=False):\n+    model: Required[str]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_assistant_message.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_assistant_message.py b/src/openai/types/realtime/realtime_conversation_item_assistant_message.py\nnew file mode 100644\nindex 0000000..d0f3774\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_assistant_message.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeConversationItemAssistantMessage\", \"Content\"]\n+\n+\n+class Content(BaseModel):\n+    text: Optional[str] = None\n+    \"\"\"The text content.\"\"\"\n+\n+    type: Optional[Literal[\"text\"]] = None\n+    \"\"\"The content type. Always `text` for assistant messages.\"\"\"\n+\n+\n+class RealtimeConversationItemAssistantMessage(BaseModel):\n+    content: List[Content]\n+    \"\"\"The content of the message.\"\"\"\n+\n+    role: Literal[\"assistant\"]\n+    \"\"\"The role of the message sender. Always `assistant`.\"\"\"\n+\n+    type: Literal[\"message\"]\n+    \"\"\"The type of the item. Always `message`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.item\"]] = None\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Optional[Literal[\"completed\", \"incomplete\", \"in_progress\"]] = None\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_assistant_message_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_assistant_message_param.py b/src/openai/types/realtime/realtime_conversation_item_assistant_message_param.py\nnew file mode 100644\nindex 0000000..cfbd9cd\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_assistant_message_param.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Iterable\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeConversationItemAssistantMessageParam\", \"Content\"]\n+\n+\n+class Content(TypedDict, total=False):\n+    text: str\n+    \"\"\"The text content.\"\"\"\n+\n+    type: Literal[\"text\"]\n+    \"\"\"The content type. Always `text` for assistant messages.\"\"\"\n+\n+\n+class RealtimeConversationItemAssistantMessageParam(TypedDict, total=False):\n+    content: Required[Iterable[Content]]\n+    \"\"\"The content of the message.\"\"\"\n+\n+    role: Required[Literal[\"assistant\"]]\n+    \"\"\"The role of the message sender. Always `assistant`.\"\"\"\n+\n+    type: Required[Literal[\"message\"]]\n+    \"\"\"The type of the item. Always `message`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Literal[\"realtime.item\"]\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Literal[\"completed\", \"incomplete\", \"in_progress\"]\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_function_call.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_function_call.py b/src/openai/types/realtime/realtime_conversation_item_function_call.py\nnew file mode 100644\nindex 0000000..ce1c6d4\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_function_call.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeConversationItemFunctionCall\"]\n+\n+\n+class RealtimeConversationItemFunctionCall(BaseModel):\n+    arguments: str\n+    \"\"\"The arguments of the function call.\"\"\"\n+\n+    name: str\n+    \"\"\"The name of the function being called.\"\"\"\n+\n+    type: Literal[\"function_call\"]\n+    \"\"\"The type of the item. Always `function_call`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    call_id: Optional[str] = None\n+    \"\"\"The ID of the function call.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.item\"]] = None\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Optional[Literal[\"completed\", \"incomplete\", \"in_progress\"]] = None\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_function_call_output.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_function_call_output.py b/src/openai/types/realtime/realtime_conversation_item_function_call_output.py\nnew file mode 100644\nindex 0000000..cea840f\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_function_call_output.py\n@@ -0,0 +1,28 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeConversationItemFunctionCallOutput\"]\n+\n+\n+class RealtimeConversationItemFunctionCallOutput(BaseModel):\n+    call_id: str\n+    \"\"\"The ID of the function call this output is for.\"\"\"\n+\n+    output: str\n+    \"\"\"The output of the function call.\"\"\"\n+\n+    type: Literal[\"function_call_output\"]\n+    \"\"\"The type of the item. Always `function_call_output`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.item\"]] = None\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Optional[Literal[\"completed\", \"incomplete\", \"in_progress\"]] = None\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_function_call_output_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_function_call_output_param.py b/src/openai/types/realtime/realtime_conversation_item_function_call_output_param.py\nnew file mode 100644\nindex 0000000..a66c587\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_function_call_output_param.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeConversationItemFunctionCallOutputParam\"]\n+\n+\n+class RealtimeConversationItemFunctionCallOutputParam(TypedDict, total=False):\n+    call_id: Required[str]\n+    \"\"\"The ID of the function call this output is for.\"\"\"\n+\n+    output: Required[str]\n+    \"\"\"The output of the function call.\"\"\"\n+\n+    type: Required[Literal[\"function_call_output\"]]\n+    \"\"\"The type of the item. Always `function_call_output`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Literal[\"realtime.item\"]\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Literal[\"completed\", \"incomplete\", \"in_progress\"]\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_function_call_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_function_call_param.py b/src/openai/types/realtime/realtime_conversation_item_function_call_param.py\nnew file mode 100644\nindex 0000000..a4d6fb8\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_function_call_param.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeConversationItemFunctionCallParam\"]\n+\n+\n+class RealtimeConversationItemFunctionCallParam(TypedDict, total=False):\n+    arguments: Required[str]\n+    \"\"\"The arguments of the function call.\"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the function being called.\"\"\"\n+\n+    type: Required[Literal[\"function_call\"]]\n+    \"\"\"The type of the item. Always `function_call`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    call_id: str\n+    \"\"\"The ID of the function call.\"\"\"\n+\n+    object: Literal[\"realtime.item\"]\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Literal[\"completed\", \"incomplete\", \"in_progress\"]\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_system_message.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_system_message.py b/src/openai/types/realtime/realtime_conversation_item_system_message.py\nnew file mode 100644\nindex 0000000..abc67f6\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_system_message.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeConversationItemSystemMessage\", \"Content\"]\n+\n+\n+class Content(BaseModel):\n+    text: Optional[str] = None\n+    \"\"\"The text content.\"\"\"\n+\n+    type: Optional[Literal[\"input_text\"]] = None\n+    \"\"\"The content type. Always `input_text` for system messages.\"\"\"\n+\n+\n+class RealtimeConversationItemSystemMessage(BaseModel):\n+    content: List[Content]\n+    \"\"\"The content of the message.\"\"\"\n+\n+    role: Literal[\"system\"]\n+    \"\"\"The role of the message sender. Always `system`.\"\"\"\n+\n+    type: Literal[\"message\"]\n+    \"\"\"The type of the item. Always `message`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.item\"]] = None\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Optional[Literal[\"completed\", \"incomplete\", \"in_progress\"]] = None\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_system_message_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_system_message_param.py b/src/openai/types/realtime/realtime_conversation_item_system_message_param.py\nnew file mode 100644\nindex 0000000..2a1c442\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_system_message_param.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Iterable\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeConversationItemSystemMessageParam\", \"Content\"]\n+\n+\n+class Content(TypedDict, total=False):\n+    text: str\n+    \"\"\"The text content.\"\"\"\n+\n+    type: Literal[\"input_text\"]\n+    \"\"\"The content type. Always `input_text` for system messages.\"\"\"\n+\n+\n+class RealtimeConversationItemSystemMessageParam(TypedDict, total=False):\n+    content: Required[Iterable[Content]]\n+    \"\"\"The content of the message.\"\"\"\n+\n+    role: Required[Literal[\"system\"]]\n+    \"\"\"The role of the message sender. Always `system`.\"\"\"\n+\n+    type: Required[Literal[\"message\"]]\n+    \"\"\"The type of the item. Always `message`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Literal[\"realtime.item\"]\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Literal[\"completed\", \"incomplete\", \"in_progress\"]\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_user_message.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_user_message.py b/src/openai/types/realtime/realtime_conversation_item_user_message.py\nnew file mode 100644\nindex 0000000..48a6c6e\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_user_message.py\n@@ -0,0 +1,42 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeConversationItemUserMessage\", \"Content\"]\n+\n+\n+class Content(BaseModel):\n+    audio: Optional[str] = None\n+    \"\"\"Base64-encoded audio bytes (for `input_audio`).\"\"\"\n+\n+    text: Optional[str] = None\n+    \"\"\"The text content (for `input_text`).\"\"\"\n+\n+    transcript: Optional[str] = None\n+    \"\"\"Transcript of the audio (for `input_audio`).\"\"\"\n+\n+    type: Optional[Literal[\"input_text\", \"input_audio\"]] = None\n+    \"\"\"The content type (`input_text` or `input_audio`).\"\"\"\n+\n+\n+class RealtimeConversationItemUserMessage(BaseModel):\n+    content: List[Content]\n+    \"\"\"The content of the message.\"\"\"\n+\n+    role: Literal[\"user\"]\n+    \"\"\"The role of the message sender. Always `user`.\"\"\"\n+\n+    type: Literal[\"message\"]\n+    \"\"\"The type of the item. Always `message`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.item\"]] = None\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Optional[Literal[\"completed\", \"incomplete\", \"in_progress\"]] = None\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_conversation_item_user_message_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_conversation_item_user_message_param.py b/src/openai/types/realtime/realtime_conversation_item_user_message_param.py\nnew file mode 100644\nindex 0000000..cff64a6\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_conversation_item_user_message_param.py\n@@ -0,0 +1,42 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Iterable\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeConversationItemUserMessageParam\", \"Content\"]\n+\n+\n+class Content(TypedDict, total=False):\n+    audio: str\n+    \"\"\"Base64-encoded audio bytes (for `input_audio`).\"\"\"\n+\n+    text: str\n+    \"\"\"The text content (for `input_text`).\"\"\"\n+\n+    transcript: str\n+    \"\"\"Transcript of the audio (for `input_audio`).\"\"\"\n+\n+    type: Literal[\"input_text\", \"input_audio\"]\n+    \"\"\"The content type (`input_text` or `input_audio`).\"\"\"\n+\n+\n+class RealtimeConversationItemUserMessageParam(TypedDict, total=False):\n+    content: Required[Iterable[Content]]\n+    \"\"\"The content of the message.\"\"\"\n+\n+    role: Required[Literal[\"user\"]]\n+    \"\"\"The role of the message sender. Always `user`.\"\"\"\n+\n+    type: Required[Literal[\"message\"]]\n+    \"\"\"The type of the item. Always `message`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the item.\"\"\"\n+\n+    object: Literal[\"realtime.item\"]\n+    \"\"\"Identifier for the API object being returned - always `realtime.item`.\"\"\"\n+\n+    status: Literal[\"completed\", \"incomplete\", \"in_progress\"]\n+    \"\"\"The status of the item. Has no effect on the conversation.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_error.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_error.py b/src/openai/types/realtime/realtime_error.py\nnew file mode 100644\nindex 0000000..f1017d0\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_error.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeError\"]\n+\n+\n+class RealtimeError(BaseModel):\n+    message: str\n+    \"\"\"A human-readable error message.\"\"\"\n+\n+    type: str\n+    \"\"\"The type of error (e.g., \"invalid_request_error\", \"server_error\").\"\"\"\n+\n+    code: Optional[str] = None\n+    \"\"\"Error code, if any.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"The event_id of the client event that caused the error, if applicable.\"\"\"\n+\n+    param: Optional[str] = None\n+    \"\"\"Parameter related to the error, if any.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_error_event.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_error_event.py b/src/openai/types/realtime/realtime_error_event.py\nnew file mode 100644\nindex 0000000..8b501d6\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_error_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_error import RealtimeError\n+\n+__all__ = [\"RealtimeErrorEvent\"]\n+\n+\n+class RealtimeErrorEvent(BaseModel):\n+    error: RealtimeError\n+    \"\"\"Details of the error.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    type: Literal[\"error\"]\n+    \"\"\"The event type, must be `error`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_approval_request.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_approval_request.py b/src/openai/types/realtime/realtime_mcp_approval_request.py\nnew file mode 100644\nindex 0000000..bafc8d8\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_approval_request.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeMcpApprovalRequest\"]\n+\n+\n+class RealtimeMcpApprovalRequest(BaseModel):\n+    id: str\n+    \"\"\"The unique ID of the approval request.\"\"\"\n+\n+    arguments: str\n+    \"\"\"A JSON string of arguments for the tool.\"\"\"\n+\n+    name: str\n+    \"\"\"The name of the tool to run.\"\"\"\n+\n+    server_label: str\n+    \"\"\"The label of the MCP server making the request.\"\"\"\n+\n+    type: Literal[\"mcp_approval_request\"]\n+    \"\"\"The type of the item. Always `mcp_approval_request`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_approval_request_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_approval_request_param.py b/src/openai/types/realtime/realtime_mcp_approval_request_param.py\nnew file mode 100644\nindex 0000000..57c21a4\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_approval_request_param.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeMcpApprovalRequestParam\"]\n+\n+\n+class RealtimeMcpApprovalRequestParam(TypedDict, total=False):\n+    id: Required[str]\n+    \"\"\"The unique ID of the approval request.\"\"\"\n+\n+    arguments: Required[str]\n+    \"\"\"A JSON string of arguments for the tool.\"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the tool to run.\"\"\"\n+\n+    server_label: Required[str]\n+    \"\"\"The label of the MCP server making the request.\"\"\"\n+\n+    type: Required[Literal[\"mcp_approval_request\"]]\n+    \"\"\"The type of the item. Always `mcp_approval_request`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_approval_response.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_approval_response.py b/src/openai/types/realtime/realtime_mcp_approval_response.py\nnew file mode 100644\nindex 0000000..2cb03bc\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_approval_response.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeMcpApprovalResponse\"]\n+\n+\n+class RealtimeMcpApprovalResponse(BaseModel):\n+    id: str\n+    \"\"\"The unique ID of the approval response.\"\"\"\n+\n+    approval_request_id: str\n+    \"\"\"The ID of the approval request being answered.\"\"\"\n+\n+    approve: bool\n+    \"\"\"Whether the request was approved.\"\"\"\n+\n+    type: Literal[\"mcp_approval_response\"]\n+    \"\"\"The type of the item. Always `mcp_approval_response`.\"\"\"\n+\n+    reason: Optional[str] = None\n+    \"\"\"Optional reason for the decision.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_approval_response_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_approval_response_param.py b/src/openai/types/realtime/realtime_mcp_approval_response_param.py\nnew file mode 100644\nindex 0000000..19b6337\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_approval_response_param.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Optional\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeMcpApprovalResponseParam\"]\n+\n+\n+class RealtimeMcpApprovalResponseParam(TypedDict, total=False):\n+    id: Required[str]\n+    \"\"\"The unique ID of the approval response.\"\"\"\n+\n+    approval_request_id: Required[str]\n+    \"\"\"The ID of the approval request being answered.\"\"\"\n+\n+    approve: Required[bool]\n+    \"\"\"Whether the request was approved.\"\"\"\n+\n+    type: Required[Literal[\"mcp_approval_response\"]]\n+    \"\"\"The type of the item. Always `mcp_approval_response`.\"\"\"\n+\n+    reason: Optional[str]\n+    \"\"\"Optional reason for the decision.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_list_tools.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_list_tools.py b/src/openai/types/realtime/realtime_mcp_list_tools.py\nnew file mode 100644\nindex 0000000..aeb58a1\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_list_tools.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeMcpListTools\", \"Tool\"]\n+\n+\n+class Tool(BaseModel):\n+    input_schema: object\n+    \"\"\"The JSON schema describing the tool's input.\"\"\"\n+\n+    name: str\n+    \"\"\"The name of the tool.\"\"\"\n+\n+    annotations: Optional[object] = None\n+    \"\"\"Additional annotations about the tool.\"\"\"\n+\n+    description: Optional[str] = None\n+    \"\"\"The description of the tool.\"\"\"\n+\n+\n+class RealtimeMcpListTools(BaseModel):\n+    server_label: str\n+    \"\"\"The label of the MCP server.\"\"\"\n+\n+    tools: List[Tool]\n+    \"\"\"The tools available on the server.\"\"\"\n+\n+    type: Literal[\"mcp_list_tools\"]\n+    \"\"\"The type of the item. Always `mcp_list_tools`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the list.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_list_tools_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_list_tools_param.py b/src/openai/types/realtime/realtime_mcp_list_tools_param.py\nnew file mode 100644\nindex 0000000..eb8605a\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_list_tools_param.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Iterable, Optional\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeMcpListToolsParam\", \"Tool\"]\n+\n+\n+class Tool(TypedDict, total=False):\n+    input_schema: Required[object]\n+    \"\"\"The JSON schema describing the tool's input.\"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the tool.\"\"\"\n+\n+    annotations: Optional[object]\n+    \"\"\"Additional annotations about the tool.\"\"\"\n+\n+    description: Optional[str]\n+    \"\"\"The description of the tool.\"\"\"\n+\n+\n+class RealtimeMcpListToolsParam(TypedDict, total=False):\n+    server_label: Required[str]\n+    \"\"\"The label of the MCP server.\"\"\"\n+\n+    tools: Required[Iterable[Tool]]\n+    \"\"\"The tools available on the server.\"\"\"\n+\n+    type: Required[Literal[\"mcp_list_tools\"]]\n+    \"\"\"The type of the item. Always `mcp_list_tools`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the list.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_protocol_error.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_protocol_error.py b/src/openai/types/realtime/realtime_mcp_protocol_error.py\nnew file mode 100644\nindex 0000000..2e7cfdf\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_protocol_error.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeMcpProtocolError\"]\n+\n+\n+class RealtimeMcpProtocolError(BaseModel):\n+    code: int\n+\n+    message: str\n+\n+    type: Literal[\"protocol_error\"]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_protocol_error_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_protocol_error_param.py b/src/openai/types/realtime/realtime_mcp_protocol_error_param.py\nnew file mode 100644\nindex 0000000..bebe3d3\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_protocol_error_param.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeMcpProtocolErrorParam\"]\n+\n+\n+class RealtimeMcpProtocolErrorParam(TypedDict, total=False):\n+    code: Required[int]\n+\n+    message: Required[str]\n+\n+    type: Required[Literal[\"protocol_error\"]]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_tool_call.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_tool_call.py b/src/openai/types/realtime/realtime_mcp_tool_call.py\nnew file mode 100644\nindex 0000000..533175e\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_tool_call.py\n@@ -0,0 +1,43 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal, Annotated, TypeAlias\n+\n+from ..._utils import PropertyInfo\n+from ..._models import BaseModel\n+from .realtime_mcphttp_error import RealtimeMcphttpError\n+from .realtime_mcp_protocol_error import RealtimeMcpProtocolError\n+from .realtime_mcp_tool_execution_error import RealtimeMcpToolExecutionError\n+\n+__all__ = [\"RealtimeMcpToolCall\", \"Error\"]\n+\n+Error: TypeAlias = Annotated[\n+    Union[RealtimeMcpProtocolError, RealtimeMcpToolExecutionError, RealtimeMcphttpError, None],\n+    PropertyInfo(discriminator=\"type\"),\n+]\n+\n+\n+class RealtimeMcpToolCall(BaseModel):\n+    id: str\n+    \"\"\"The unique ID of the tool call.\"\"\"\n+\n+    arguments: str\n+    \"\"\"A JSON string of the arguments passed to the tool.\"\"\"\n+\n+    name: str\n+    \"\"\"The name of the tool that was run.\"\"\"\n+\n+    server_label: str\n+    \"\"\"The label of the MCP server running the tool.\"\"\"\n+\n+    type: Literal[\"mcp_tool_call\"]\n+    \"\"\"The type of the item. Always `mcp_tool_call`.\"\"\"\n+\n+    approval_request_id: Optional[str] = None\n+    \"\"\"The ID of an associated approval request, if any.\"\"\"\n+\n+    error: Optional[Error] = None\n+    \"\"\"The error from the tool call, if any.\"\"\"\n+\n+    output: Optional[str] = None\n+    \"\"\"The output from the tool call.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_tool_call_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_tool_call_param.py b/src/openai/types/realtime/realtime_mcp_tool_call_param.py\nnew file mode 100644\nindex 0000000..afdc9d1\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_tool_call_param.py\n@@ -0,0 +1,40 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+from .realtime_mcphttp_error_param import RealtimeMcphttpErrorParam\n+from .realtime_mcp_protocol_error_param import RealtimeMcpProtocolErrorParam\n+from .realtime_mcp_tool_execution_error_param import RealtimeMcpToolExecutionErrorParam\n+\n+__all__ = [\"RealtimeMcpToolCallParam\", \"Error\"]\n+\n+Error: TypeAlias = Union[RealtimeMcpProtocolErrorParam, RealtimeMcpToolExecutionErrorParam, RealtimeMcphttpErrorParam]\n+\n+\n+class RealtimeMcpToolCallParam(TypedDict, total=False):\n+    id: Required[str]\n+    \"\"\"The unique ID of the tool call.\"\"\"\n+\n+    arguments: Required[str]\n+    \"\"\"A JSON string of the arguments passed to the tool.\"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the tool that was run.\"\"\"\n+\n+    server_label: Required[str]\n+    \"\"\"The label of the MCP server running the tool.\"\"\"\n+\n+    type: Required[Literal[\"mcp_tool_call\"]]\n+    \"\"\"The type of the item. Always `mcp_tool_call`.\"\"\"\n+\n+    approval_request_id: Optional[str]\n+    \"\"\"The ID of an associated approval request, if any.\"\"\"\n+\n+    error: Optional[Error]\n+    \"\"\"The error from the tool call, if any.\"\"\"\n+\n+    output: Optional[str]\n+    \"\"\"The output from the tool call.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_tool_execution_error.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_tool_execution_error.py b/src/openai/types/realtime/realtime_mcp_tool_execution_error.py\nnew file mode 100644\nindex 0000000..a2ed063\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_tool_execution_error.py\n@@ -0,0 +1,13 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeMcpToolExecutionError\"]\n+\n+\n+class RealtimeMcpToolExecutionError(BaseModel):\n+    message: str\n+\n+    type: Literal[\"tool_execution_error\"]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcp_tool_execution_error_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcp_tool_execution_error_param.py b/src/openai/types/realtime/realtime_mcp_tool_execution_error_param.py\nnew file mode 100644\nindex 0000000..619e11c\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcp_tool_execution_error_param.py\n@@ -0,0 +1,13 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeMcpToolExecutionErrorParam\"]\n+\n+\n+class RealtimeMcpToolExecutionErrorParam(TypedDict, total=False):\n+    message: Required[str]\n+\n+    type: Required[Literal[\"tool_execution_error\"]]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcphttp_error.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcphttp_error.py b/src/openai/types/realtime/realtime_mcphttp_error.py\nnew file mode 100644\nindex 0000000..53cff91\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcphttp_error.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeMcphttpError\"]\n+\n+\n+class RealtimeMcphttpError(BaseModel):\n+    code: int\n+\n+    message: str\n+\n+    type: Literal[\"http_error\"]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_mcphttp_error_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_mcphttp_error_param.py b/src/openai/types/realtime/realtime_mcphttp_error_param.py\nnew file mode 100644\nindex 0000000..2b80a6f\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_mcphttp_error_param.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"RealtimeMcphttpErrorParam\"]\n+\n+\n+class RealtimeMcphttpErrorParam(TypedDict, total=False):\n+    code: Required[int]\n+\n+    message: Required[str]\n+\n+    type: Required[Literal[\"http_error\"]]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_response.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_response.py b/src/openai/types/realtime/realtime_response.py\nnew file mode 100644\nindex 0000000..54f5999\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_response.py\n@@ -0,0 +1,89 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from ..shared.metadata import Metadata\n+from .conversation_item import ConversationItem\n+from .realtime_response_usage import RealtimeResponseUsage\n+from .realtime_response_status import RealtimeResponseStatus\n+\n+__all__ = [\"RealtimeResponse\"]\n+\n+\n+class RealtimeResponse(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the response.\"\"\"\n+\n+    conversation_id: Optional[str] = None\n+    \"\"\"\n+    Which conversation the response is added to, determined by the `conversation`\n+    field in the `response.create` event. If `auto`, the response will be added to\n+    the default conversation and the value of `conversation_id` will be an id like\n+    `conv_1234`. If `none`, the response will not be added to any conversation and\n+    the value of `conversation_id` will be `null`. If responses are being triggered\n+    by server VAD, the response will be added to the default conversation, thus the\n+    `conversation_id` will be an id like `conv_1234`.\n+    \"\"\"\n+\n+    max_output_tokens: Union[int, Literal[\"inf\"], None] = None\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls, that was used in this response.\n+    \"\"\"\n+\n+    metadata: Optional[Metadata] = None\n+    \"\"\"Set of 16 key-value pairs that can be attached to an object.\n+\n+    This can be useful for storing additional information about the object in a\n+    structured format, and querying for objects via API or the dashboard.\n+\n+    Keys are strings with a maximum length of 64 characters. Values are strings with\n+    a maximum length of 512 characters.\n+    \"\"\"\n+\n+    modalities: Optional[List[Literal[\"text\", \"audio\"]]] = None\n+    \"\"\"The set of modalities the model used to respond.\n+\n+    If there are multiple modalities, the model will pick one, for example if\n+    `modalities` is `[\"text\", \"audio\"]`, the model could be responding in either\n+    text or audio.\n+    \"\"\"\n+\n+    object: Optional[Literal[\"realtime.response\"]] = None\n+    \"\"\"The object type, must be `realtime.response`.\"\"\"\n+\n+    output: Optional[List[ConversationItem]] = None\n+    \"\"\"The list of output items generated by the response.\"\"\"\n+\n+    output_audio_format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    status: Optional[Literal[\"completed\", \"cancelled\", \"failed\", \"incomplete\", \"in_progress\"]] = None\n+    \"\"\"\n+    The final status of the response (`completed`, `cancelled`, `failed`, or\n+    `incomplete`, `in_progress`).\n+    \"\"\"\n+\n+    status_details: Optional[RealtimeResponseStatus] = None\n+    \"\"\"Additional details about the status.\"\"\"\n+\n+    temperature: Optional[float] = None\n+    \"\"\"Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\"\"\"\n+\n+    usage: Optional[RealtimeResponseUsage] = None\n+    \"\"\"Usage statistics for the Response, this will correspond to billing.\n+\n+    A Realtime API session will maintain a conversation context and append new Items\n+    to the Conversation, thus output from previous turns (text and audio tokens)\n+    will become the input for later turns.\n+    \"\"\"\n+\n+    voice: Union[\n+        str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"], None\n+    ] = None\n+    \"\"\"\n+    The voice the model used to respond. Current voice options are `alloy`, `ash`,\n+    `ballad`, `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_response_status.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_response_status.py b/src/openai/types/realtime/realtime_response_status.py\nnew file mode 100644\nindex 0000000..12999f6\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_response_status.py\n@@ -0,0 +1,39 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeResponseStatus\", \"Error\"]\n+\n+\n+class Error(BaseModel):\n+    code: Optional[str] = None\n+    \"\"\"Error code, if any.\"\"\"\n+\n+    type: Optional[str] = None\n+    \"\"\"The type of error.\"\"\"\n+\n+\n+class RealtimeResponseStatus(BaseModel):\n+    error: Optional[Error] = None\n+    \"\"\"\n+    A description of the error that caused the response to fail, populated when the\n+    `status` is `failed`.\n+    \"\"\"\n+\n+    reason: Optional[Literal[\"turn_detected\", \"client_cancelled\", \"max_output_tokens\", \"content_filter\"]] = None\n+    \"\"\"The reason the Response did not complete.\n+\n+    For a `cancelled` Response, one of `turn_detected` (the server VAD detected a\n+    new start of speech) or `client_cancelled` (the client sent a cancel event). For\n+    an `incomplete` Response, one of `max_output_tokens` or `content_filter` (the\n+    server-side safety filter activated and cut off the response).\n+    \"\"\"\n+\n+    type: Optional[Literal[\"completed\", \"cancelled\", \"incomplete\", \"failed\"]] = None\n+    \"\"\"\n+    The type of error that caused the response to fail, corresponding with the\n+    `status` field (`completed`, `cancelled`, `incomplete`, `failed`).\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_response_usage.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_response_usage.py b/src/openai/types/realtime/realtime_response_usage.py\nnew file mode 100644\nindex 0000000..dbce5f2\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_response_usage.py\n@@ -0,0 +1,35 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+\n+from ..._models import BaseModel\n+from .realtime_response_usage_input_token_details import RealtimeResponseUsageInputTokenDetails\n+from .realtime_response_usage_output_token_details import RealtimeResponseUsageOutputTokenDetails\n+\n+__all__ = [\"RealtimeResponseUsage\"]\n+\n+\n+class RealtimeResponseUsage(BaseModel):\n+    input_token_details: Optional[RealtimeResponseUsageInputTokenDetails] = None\n+    \"\"\"Details about the input tokens used in the Response.\"\"\"\n+\n+    input_tokens: Optional[int] = None\n+    \"\"\"\n+    The number of input tokens used in the Response, including text and audio\n+    tokens.\n+    \"\"\"\n+\n+    output_token_details: Optional[RealtimeResponseUsageOutputTokenDetails] = None\n+    \"\"\"Details about the output tokens used in the Response.\"\"\"\n+\n+    output_tokens: Optional[int] = None\n+    \"\"\"\n+    The number of output tokens sent in the Response, including text and audio\n+    tokens.\n+    \"\"\"\n+\n+    total_tokens: Optional[int] = None\n+    \"\"\"\n+    The total number of tokens in the Response including input and output text and\n+    audio tokens.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_response_usage_input_token_details.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_response_usage_input_token_details.py b/src/openai/types/realtime/realtime_response_usage_input_token_details.py\nnew file mode 100644\nindex 0000000..dfeead9\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_response_usage_input_token_details.py\n@@ -0,0 +1,18 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeResponseUsageInputTokenDetails\"]\n+\n+\n+class RealtimeResponseUsageInputTokenDetails(BaseModel):\n+    audio_tokens: Optional[int] = None\n+    \"\"\"The number of audio tokens used in the Response.\"\"\"\n+\n+    cached_tokens: Optional[int] = None\n+    \"\"\"The number of cached tokens used in the Response.\"\"\"\n+\n+    text_tokens: Optional[int] = None\n+    \"\"\"The number of text tokens used in the Response.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_response_usage_output_token_details.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_response_usage_output_token_details.py b/src/openai/types/realtime/realtime_response_usage_output_token_details.py\nnew file mode 100644\nindex 0000000..dfa97a1\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_response_usage_output_token_details.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeResponseUsageOutputTokenDetails\"]\n+\n+\n+class RealtimeResponseUsageOutputTokenDetails(BaseModel):\n+    audio_tokens: Optional[int] = None\n+    \"\"\"The number of audio tokens used in the Response.\"\"\"\n+\n+    text_tokens: Optional[int] = None\n+    \"\"\"The number of text tokens used in the Response.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_server_event.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_server_event.py b/src/openai/types/realtime/realtime_server_event.py\nnew file mode 100644\nindex 0000000..8094bcf\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_server_event.py\n@@ -0,0 +1,159 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union\n+from typing_extensions import Literal, Annotated, TypeAlias\n+\n+from ..._utils import PropertyInfo\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+from .response_done_event import ResponseDoneEvent\n+from .realtime_error_event import RealtimeErrorEvent\n+from .mcp_list_tools_failed import McpListToolsFailed\n+from .session_created_event import SessionCreatedEvent\n+from .session_updated_event import SessionUpdatedEvent\n+from .conversation_item_done import ConversationItemDone\n+from .response_created_event import ResponseCreatedEvent\n+from .conversation_item_added import ConversationItemAdded\n+from .mcp_list_tools_completed import McpListToolsCompleted\n+from .response_mcp_call_failed import ResponseMcpCallFailed\n+from .response_text_done_event import ResponseTextDoneEvent\n+from .rate_limits_updated_event import RateLimitsUpdatedEvent\n+from .response_audio_done_event import ResponseAudioDoneEvent\n+from .response_text_delta_event import ResponseTextDeltaEvent\n+from .conversation_created_event import ConversationCreatedEvent\n+from .mcp_list_tools_in_progress import McpListToolsInProgress\n+from .response_audio_delta_event import ResponseAudioDeltaEvent\n+from .response_mcp_call_completed import ResponseMcpCallCompleted\n+from .response_mcp_call_in_progress import ResponseMcpCallInProgress\n+from .transcription_session_created import TranscriptionSessionCreated\n+from .conversation_item_created_event import ConversationItemCreatedEvent\n+from .conversation_item_deleted_event import ConversationItemDeletedEvent\n+from .response_output_item_done_event import ResponseOutputItemDoneEvent\n+from .input_audio_buffer_cleared_event import InputAudioBufferClearedEvent\n+from .response_content_part_done_event import ResponseContentPartDoneEvent\n+from .response_mcp_call_arguments_done import ResponseMcpCallArgumentsDone\n+from .response_output_item_added_event import ResponseOutputItemAddedEvent\n+from .conversation_item_truncated_event import ConversationItemTruncatedEvent\n+from .response_content_part_added_event import ResponseContentPartAddedEvent\n+from .response_mcp_call_arguments_delta import ResponseMcpCallArgumentsDelta\n+from .input_audio_buffer_committed_event import InputAudioBufferCommittedEvent\n+from .transcription_session_updated_event import TranscriptionSessionUpdatedEvent\n+from .input_audio_buffer_timeout_triggered import InputAudioBufferTimeoutTriggered\n+from .response_audio_transcript_done_event import ResponseAudioTranscriptDoneEvent\n+from .response_audio_transcript_delta_event import ResponseAudioTranscriptDeltaEvent\n+from .input_audio_buffer_speech_started_event import InputAudioBufferSpeechStartedEvent\n+from .input_audio_buffer_speech_stopped_event import InputAudioBufferSpeechStoppedEvent\n+from .response_function_call_arguments_done_event import ResponseFunctionCallArgumentsDoneEvent\n+from .response_function_call_arguments_delta_event import ResponseFunctionCallArgumentsDeltaEvent\n+from .conversation_item_input_audio_transcription_segment import ConversationItemInputAudioTranscriptionSegment\n+from .conversation_item_input_audio_transcription_delta_event import ConversationItemInputAudioTranscriptionDeltaEvent\n+from .conversation_item_input_audio_transcription_failed_event import ConversationItemInputAudioTranscriptionFailedEvent\n+from .conversation_item_input_audio_transcription_completed_event import (\n+    ConversationItemInputAudioTranscriptionCompletedEvent,\n+)\n+\n+__all__ = [\n+    \"RealtimeServerEvent\",\n+    \"ConversationItemRetrieved\",\n+    \"OutputAudioBufferStarted\",\n+    \"OutputAudioBufferStopped\",\n+    \"OutputAudioBufferCleared\",\n+]\n+\n+\n+class ConversationItemRetrieved(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    type: Literal[\"conversation.item.retrieved\"]\n+    \"\"\"The event type, must be `conversation.item.retrieved`.\"\"\"\n+\n+\n+class OutputAudioBufferStarted(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The unique ID of the response that produced the audio.\"\"\"\n+\n+    type: Literal[\"output_audio_buffer.started\"]\n+    \"\"\"The event type, must be `output_audio_buffer.started`.\"\"\"\n+\n+\n+class OutputAudioBufferStopped(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The unique ID of the response that produced the audio.\"\"\"\n+\n+    type: Literal[\"output_audio_buffer.stopped\"]\n+    \"\"\"The event type, must be `output_audio_buffer.stopped`.\"\"\"\n+\n+\n+class OutputAudioBufferCleared(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The unique ID of the response that produced the audio.\"\"\"\n+\n+    type: Literal[\"output_audio_buffer.cleared\"]\n+    \"\"\"The event type, must be `output_audio_buffer.cleared`.\"\"\"\n+\n+\n+RealtimeServerEvent: TypeAlias = Annotated[\n+    Union[\n+        ConversationCreatedEvent,\n+        ConversationItemCreatedEvent,\n+        ConversationItemDeletedEvent,\n+        ConversationItemInputAudioTranscriptionCompletedEvent,\n+        ConversationItemInputAudioTranscriptionDeltaEvent,\n+        ConversationItemInputAudioTranscriptionFailedEvent,\n+        ConversationItemRetrieved,\n+        ConversationItemTruncatedEvent,\n+        RealtimeErrorEvent,\n+        InputAudioBufferClearedEvent,\n+        InputAudioBufferCommittedEvent,\n+        InputAudioBufferSpeechStartedEvent,\n+        InputAudioBufferSpeechStoppedEvent,\n+        RateLimitsUpdatedEvent,\n+        ResponseAudioDeltaEvent,\n+        ResponseAudioDoneEvent,\n+        ResponseAudioTranscriptDeltaEvent,\n+        ResponseAudioTranscriptDoneEvent,\n+        ResponseContentPartAddedEvent,\n+        ResponseContentPartDoneEvent,\n+        ResponseCreatedEvent,\n+        ResponseDoneEvent,\n+        ResponseFunctionCallArgumentsDeltaEvent,\n+        ResponseFunctionCallArgumentsDoneEvent,\n+        ResponseOutputItemAddedEvent,\n+        ResponseOutputItemDoneEvent,\n+        ResponseTextDeltaEvent,\n+        ResponseTextDoneEvent,\n+        SessionCreatedEvent,\n+        SessionUpdatedEvent,\n+        TranscriptionSessionUpdatedEvent,\n+        TranscriptionSessionCreated,\n+        OutputAudioBufferStarted,\n+        OutputAudioBufferStopped,\n+        OutputAudioBufferCleared,\n+        ConversationItemAdded,\n+        ConversationItemDone,\n+        InputAudioBufferTimeoutTriggered,\n+        ConversationItemInputAudioTranscriptionSegment,\n+        McpListToolsInProgress,\n+        McpListToolsCompleted,\n+        McpListToolsFailed,\n+        ResponseMcpCallArgumentsDelta,\n+        ResponseMcpCallArgumentsDone,\n+        ResponseMcpCallInProgress,\n+        ResponseMcpCallCompleted,\n+        ResponseMcpCallFailed,\n+    ],\n+    PropertyInfo(discriminator=\"type\"),\n+]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_session.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_session.py b/src/openai/types/realtime/realtime_session.py\nnew file mode 100644\nindex 0000000..43576ea\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_session.py\n@@ -0,0 +1,305 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+from ..responses.response_prompt import ResponsePrompt\n+\n+__all__ = [\n+    \"RealtimeSession\",\n+    \"InputAudioNoiseReduction\",\n+    \"InputAudioTranscription\",\n+    \"Tool\",\n+    \"Tracing\",\n+    \"TracingTracingConfiguration\",\n+    \"TurnDetection\",\n+]\n+\n+\n+class InputAudioNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+    \"\"\"Type of noise reduction.\n+\n+    `near_field` is for close-talking microphones such as headphones, `far_field` is\n+    for far-field microphones such as laptop or conference room microphones.\n+    \"\"\"\n+\n+\n+class InputAudioTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Optional[str] = None\n+    \"\"\"\n+    The model to use for transcription, current options are `gpt-4o-transcribe`,\n+    `gpt-4o-mini-transcribe`, and `whisper-1`.\n+    \"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"\n+    An optional text to guide the model's style or continue a previous audio\n+    segment. For `whisper-1`, the\n+    [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n+    For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n+    \"expect words related to technology\".\n+    \"\"\"\n+\n+\n+class Tool(BaseModel):\n+    description: Optional[str] = None\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: Optional[str] = None\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: Optional[object] = None\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Optional[Literal[\"function\"]] = None\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class TracingTracingConfiguration(BaseModel):\n+    group_id: Optional[str] = None\n+    \"\"\"\n+    The group id to attach to this trace to enable filtering and grouping in the\n+    traces dashboard.\n+    \"\"\"\n+\n+    metadata: Optional[object] = None\n+    \"\"\"\n+    The arbitrary metadata to attach to this trace to enable filtering in the traces\n+    dashboard.\n+    \"\"\"\n+\n+    workflow_name: Optional[str] = None\n+    \"\"\"The name of the workflow to attach to this trace.\n+\n+    This is used to name the trace in the traces dashboard.\n+    \"\"\"\n+\n+\n+Tracing: TypeAlias = Union[Literal[\"auto\"], TracingTracingConfiguration, None]\n+\n+\n+class TurnDetection(BaseModel):\n+    create_response: Optional[bool] = None\n+    \"\"\"\n+    Whether or not to automatically generate a response when a VAD stop event\n+    occurs.\n+    \"\"\"\n+\n+    eagerness: Optional[Literal[\"low\", \"medium\", \"high\", \"auto\"]] = None\n+    \"\"\"Used only for `semantic_vad` mode.\n+\n+    The eagerness of the model to respond. `low` will wait longer for the user to\n+    continue speaking, `high` will respond more quickly. `auto` is the default and\n+    is equivalent to `medium`.\n+    \"\"\"\n+\n+    idle_timeout_ms: Optional[int] = None\n+    \"\"\"\n+    Optional idle timeout after which turn detection will auto-timeout when no\n+    additional audio is received.\n+    \"\"\"\n+\n+    interrupt_response: Optional[bool] = None\n+    \"\"\"\n+    Whether or not to automatically interrupt any ongoing response with output to\n+    the default conversation (i.e. `conversation` of `auto`) when a VAD start event\n+    occurs.\n+    \"\"\"\n+\n+    prefix_padding_ms: Optional[int] = None\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Amount of audio to include before the VAD detected speech (in milliseconds).\n+    Defaults to 300ms.\n+    \"\"\"\n+\n+    silence_duration_ms: Optional[int] = None\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\n+    With shorter values the model will respond more quickly, but may jump in on\n+    short pauses from the user.\n+    \"\"\"\n+\n+    threshold: Optional[float] = None\n+    \"\"\"Used only for `server_vad` mode.\n+\n+    Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\n+    threshold will require louder audio to activate the model, and thus might\n+    perform better in noisy environments.\n+    \"\"\"\n+\n+    type: Optional[Literal[\"server_vad\", \"semantic_vad\"]] = None\n+    \"\"\"Type of turn detection.\"\"\"\n+\n+\n+class RealtimeSession(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"Unique identifier for the session that looks like `sess_1234567890abcdef`.\"\"\"\n+\n+    expires_at: Optional[int] = None\n+    \"\"\"Expiration timestamp for the session, in seconds since epoch.\"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    input_audio_format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of input audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, input audio must\n+    be 16-bit PCM at a 24kHz sample rate, single channel (mono), and little-endian\n+    byte order.\n+    \"\"\"\n+\n+    input_audio_noise_reduction: Optional[InputAudioNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\n+\n+    This can be set to `null` to turn off. Noise reduction filters audio added to\n+    the input audio buffer before it is sent to VAD and the model. Filtering the\n+    audio can improve VAD and turn detection accuracy (reducing false positives) and\n+    model performance by improving perception of the input audio.\n+    \"\"\"\n+\n+    input_audio_transcription: Optional[InputAudioTranscription] = None\n+    \"\"\"\n+    Configuration for input audio transcription, defaults to off and can be set to\n+    `null` to turn off once on. Input audio transcription is not native to the\n+    model, since the model consumes audio directly. Transcription runs\n+    asynchronously through\n+    [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\n+    and should be treated as guidance of input audio content rather than precisely\n+    what the model heard. The client can optionally set the language and prompt for\n+    transcription, these offer additional guidance to the transcription service.\n+    \"\"\"\n+\n+    instructions: Optional[str] = None\n+    \"\"\"The default system instructions (i.e.\n+\n+    system message) prepended to model calls. This field allows the client to guide\n+    the model on desired responses. The model can be instructed on response content\n+    and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of\n+    good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\n+    into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be\n+    followed by the model, but they provide guidance to the model on the desired\n+    behavior.\n+\n+    Note that the server sets default instructions which will be used if this field\n+    is not set and are visible in the `session.created` event at the start of the\n+    session.\n+    \"\"\"\n+\n+    max_response_output_tokens: Union[int, Literal[\"inf\"], None] = None\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n+    `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n+    \"\"\"\n+\n+    modalities: Optional[List[Literal[\"text\", \"audio\"]]] = None\n+    \"\"\"The set of modalities the model can respond with.\n+\n+    To disable audio, set this to [\"text\"].\n+    \"\"\"\n+\n+    model: Optional[\n+        Literal[\n+            \"gpt-4o-realtime-preview\",\n+            \"gpt-4o-realtime-preview-2024-10-01\",\n+            \"gpt-4o-realtime-preview-2024-12-17\",\n+            \"gpt-4o-realtime-preview-2025-06-03\",\n+            \"gpt-4o-mini-realtime-preview\",\n+            \"gpt-4o-mini-realtime-preview-2024-12-17\",\n+        ]\n+    ] = None\n+    \"\"\"The Realtime model used for this session.\"\"\"\n+\n+    object: Optional[Literal[\"realtime.session\"]] = None\n+    \"\"\"The object type. Always `realtime.session`.\"\"\"\n+\n+    output_audio_format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of output audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, output audio is\n+    sampled at a rate of 24kHz.\n+    \"\"\"\n+\n+    prompt: Optional[ResponsePrompt] = None\n+    \"\"\"Reference to a prompt template and its variables.\n+\n+    [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n+    \"\"\"\n+\n+    speed: Optional[float] = None\n+    \"\"\"The speed of the model's spoken response.\n+\n+    1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed.\n+    This value can only be changed in between model turns, not while a response is\n+    in progress.\n+    \"\"\"\n+\n+    temperature: Optional[float] = None\n+    \"\"\"Sampling temperature for the model, limited to [0.6, 1.2].\n+\n+    For audio models a temperature of 0.8 is highly recommended for best\n+    performance.\n+    \"\"\"\n+\n+    tool_choice: Optional[str] = None\n+    \"\"\"How the model chooses tools.\n+\n+    Options are `auto`, `none`, `required`, or specify a function.\n+    \"\"\"\n+\n+    tools: Optional[List[Tool]] = None\n+    \"\"\"Tools (functions) available to the model.\"\"\"\n+\n+    tracing: Optional[Tracing] = None\n+    \"\"\"Configuration options for tracing.\n+\n+    Set to null to disable tracing. Once tracing is enabled for a session, the\n+    configuration cannot be modified.\n+\n+    `auto` will create a trace for the session with default values for the workflow\n+    name, group id, and metadata.\n+    \"\"\"\n+\n+    turn_detection: Optional[TurnDetection] = None\n+    \"\"\"Configuration for turn detection, ether Server VAD or Semantic VAD.\n+\n+    This can be set to `null` to turn off, in which case the client must manually\n+    trigger model response. Server VAD means that the model will detect the start\n+    and end of speech based on audio volume and respond at the end of user speech.\n+    Semantic VAD is more advanced and uses a turn detection model (in conjunction\n+    with VAD) to semantically estimate whether the user has finished speaking, then\n+    dynamically sets a timeout based on this probability. For example, if user audio\n+    trails off with \"uhhm\", the model will score a low probability of turn end and\n+    wait longer for the user to continue speaking. This can be useful for more\n+    natural conversations, but may have a higher latency.\n+    \"\"\"\n+\n+    voice: Union[\n+        str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"], None\n+    ] = None\n+    \"\"\"The voice the model uses to respond.\n+\n+    Voice cannot be changed during the session once the model has responded with\n+    audio at least once. Current voice options are `alloy`, `ash`, `ballad`,\n+    `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_session_create_request.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_session_create_request.py b/src/openai/types/realtime/realtime_session_create_request.py\nnew file mode 100644\nindex 0000000..a8d0f99\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_session_create_request.py\n@@ -0,0 +1,116 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_truncation import RealtimeTruncation\n+from .realtime_audio_config import RealtimeAudioConfig\n+from .realtime_tools_config import RealtimeToolsConfig\n+from .realtime_tracing_config import RealtimeTracingConfig\n+from ..responses.response_prompt import ResponsePrompt\n+from .realtime_tool_choice_config import RealtimeToolChoiceConfig\n+from .realtime_client_secret_config import RealtimeClientSecretConfig\n+\n+__all__ = [\"RealtimeSessionCreateRequest\"]\n+\n+\n+class RealtimeSessionCreateRequest(BaseModel):\n+    model: Union[\n+        str,\n+        Literal[\n+            \"gpt-4o-realtime\",\n+            \"gpt-4o-mini-realtime\",\n+            \"gpt-4o-realtime-preview\",\n+            \"gpt-4o-realtime-preview-2024-10-01\",\n+            \"gpt-4o-realtime-preview-2024-12-17\",\n+            \"gpt-4o-realtime-preview-2025-06-03\",\n+            \"gpt-4o-mini-realtime-preview\",\n+            \"gpt-4o-mini-realtime-preview-2024-12-17\",\n+        ],\n+    ]\n+    \"\"\"The Realtime model used for this session.\"\"\"\n+\n+    type: Literal[\"realtime\"]\n+    \"\"\"The type of session to create. Always `realtime` for the Realtime API.\"\"\"\n+\n+    audio: Optional[RealtimeAudioConfig] = None\n+    \"\"\"Configuration for input and output audio.\"\"\"\n+\n+    client_secret: Optional[RealtimeClientSecretConfig] = None\n+    \"\"\"Configuration options for the generated client secret.\"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    instructions: Optional[str] = None\n+    \"\"\"The default system instructions (i.e.\n+\n+    system message) prepended to model calls. This field allows the client to guide\n+    the model on desired responses. The model can be instructed on response content\n+    and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of\n+    good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\n+    into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be\n+    followed by the model, but they provide guidance to the model on the desired\n+    behavior.\n+\n+    Note that the server sets default instructions which will be used if this field\n+    is not set and are visible in the `session.created` event at the start of the\n+    session.\n+    \"\"\"\n+\n+    max_output_tokens: Union[int, Literal[\"inf\"], None] = None\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n+    `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n+    \"\"\"\n+\n+    output_modalities: Optional[List[Literal[\"text\", \"audio\"]]] = None\n+    \"\"\"The set of modalities the model can respond with.\n+\n+    To disable audio, set this to [\"text\"].\n+    \"\"\"\n+\n+    prompt: Optional[ResponsePrompt] = None\n+    \"\"\"Reference to a prompt template and its variables.\n+\n+    [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n+    \"\"\"\n+\n+    temperature: Optional[float] = None\n+    \"\"\"Sampling temperature for the model, limited to [0.6, 1.2].\n+\n+    For audio models a temperature of 0.8 is highly recommended for best\n+    performance.\n+    \"\"\"\n+\n+    tool_choice: Optional[RealtimeToolChoiceConfig] = None\n+    \"\"\"How the model chooses tools.\n+\n+    Provide one of the string modes or force a specific function/MCP tool.\n+    \"\"\"\n+\n+    tools: Optional[RealtimeToolsConfig] = None\n+    \"\"\"Tools available to the model.\"\"\"\n+\n+    tracing: Optional[RealtimeTracingConfig] = None\n+    \"\"\"Configuration options for tracing.\n+\n+    Set to null to disable tracing. Once tracing is enabled for a session, the\n+    configuration cannot be modified.\n+\n+    `auto` will create a trace for the session with default values for the workflow\n+    name, group id, and metadata.\n+    \"\"\"\n+\n+    truncation: Optional[RealtimeTruncation] = None\n+    \"\"\"\n+    Controls how the realtime conversation is truncated prior to model inference.\n+    The default is `auto`. When set to `retention_ratio`, the server retains a\n+    fraction of the conversation tokens prior to the instructions.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_session_create_request_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_session_create_request_param.py b/src/openai/types/realtime/realtime_session_create_request_param.py\nnew file mode 100644\nindex 0000000..2c5d1e0\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_session_create_request_param.py\n@@ -0,0 +1,119 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from .realtime_truncation_param import RealtimeTruncationParam\n+from .realtime_audio_config_param import RealtimeAudioConfigParam\n+from .realtime_tools_config_param import RealtimeToolsConfigParam\n+from .realtime_tracing_config_param import RealtimeTracingConfigParam\n+from ..responses.response_prompt_param import ResponsePromptParam\n+from .realtime_tool_choice_config_param import RealtimeToolChoiceConfigParam\n+from .realtime_client_secret_config_param import RealtimeClientSecretConfigParam\n+\n+__all__ = [\"RealtimeSessionCreateRequestParam\"]\n+\n+\n+class RealtimeSessionCreateRequestParam(TypedDict, total=False):\n+    model: Required[\n+        Union[\n+            str,\n+            Literal[\n+                \"gpt-4o-realtime\",\n+                \"gpt-4o-mini-realtime\",\n+                \"gpt-4o-realtime-preview\",\n+                \"gpt-4o-realtime-preview-2024-10-01\",\n+                \"gpt-4o-realtime-preview-2024-12-17\",\n+                \"gpt-4o-realtime-preview-2025-06-03\",\n+                \"gpt-4o-mini-realtime-preview\",\n+                \"gpt-4o-mini-realtime-preview-2024-12-17\",\n+            ],\n+        ]\n+    ]\n+    \"\"\"The Realtime model used for this session.\"\"\"\n+\n+    type: Required[Literal[\"realtime\"]]\n+    \"\"\"The type of session to create. Always `realtime` for the Realtime API.\"\"\"\n+\n+    audio: RealtimeAudioConfigParam\n+    \"\"\"Configuration for input and output audio.\"\"\"\n+\n+    client_secret: RealtimeClientSecretConfigParam\n+    \"\"\"Configuration options for the generated client secret.\"\"\"\n+\n+    include: List[Literal[\"item.input_audio_transcription.logprobs\"]]\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    instructions: str\n+    \"\"\"The default system instructions (i.e.\n+\n+    system message) prepended to model calls. This field allows the client to guide\n+    the model on desired responses. The model can be instructed on response content\n+    and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of\n+    good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\n+    into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be\n+    followed by the model, but they provide guidance to the model on the desired\n+    behavior.\n+\n+    Note that the server sets default instructions which will be used if this field\n+    is not set and are visible in the `session.created` event at the start of the\n+    session.\n+    \"\"\"\n+\n+    max_output_tokens: Union[int, Literal[\"inf\"]]\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n+    `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n+    \"\"\"\n+\n+    output_modalities: List[Literal[\"text\", \"audio\"]]\n+    \"\"\"The set of modalities the model can respond with.\n+\n+    To disable audio, set this to [\"text\"].\n+    \"\"\"\n+\n+    prompt: Optional[ResponsePromptParam]\n+    \"\"\"Reference to a prompt template and its variables.\n+\n+    [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n+    \"\"\"\n+\n+    temperature: float\n+    \"\"\"Sampling temperature for the model, limited to [0.6, 1.2].\n+\n+    For audio models a temperature of 0.8 is highly recommended for best\n+    performance.\n+    \"\"\"\n+\n+    tool_choice: RealtimeToolChoiceConfigParam\n+    \"\"\"How the model chooses tools.\n+\n+    Provide one of the string modes or force a specific function/MCP tool.\n+    \"\"\"\n+\n+    tools: RealtimeToolsConfigParam\n+    \"\"\"Tools available to the model.\"\"\"\n+\n+    tracing: Optional[RealtimeTracingConfigParam]\n+    \"\"\"Configuration options for tracing.\n+\n+    Set to null to disable tracing. Once tracing is enabled for a session, the\n+    configuration cannot be modified.\n+\n+    `auto` will create a trace for the session with default values for the workflow\n+    name, group id, and metadata.\n+    \"\"\"\n+\n+    truncation: RealtimeTruncationParam\n+    \"\"\"\n+    Controls how the realtime conversation is truncated prior to model inference.\n+    The default is `auto`. When set to `retention_ratio`, the server retains a\n+    fraction of the conversation tokens prior to the instructions.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_session_create_response.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_session_create_response.py b/src/openai/types/realtime/realtime_session_create_response.py\nnew file mode 100644\nindex 0000000..82fa426\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_session_create_response.py\n@@ -0,0 +1,222 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\n+    \"RealtimeSessionCreateResponse\",\n+    \"Audio\",\n+    \"AudioInput\",\n+    \"AudioInputNoiseReduction\",\n+    \"AudioInputTranscription\",\n+    \"AudioInputTurnDetection\",\n+    \"AudioOutput\",\n+    \"Tool\",\n+    \"Tracing\",\n+    \"TracingTracingConfiguration\",\n+    \"TurnDetection\",\n+]\n+\n+\n+class AudioInputNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+\n+\n+class AudioInputTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\"\"\"\n+\n+    model: Optional[str] = None\n+    \"\"\"The model to use for transcription.\"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"Optional text to guide the model's style or continue a previous audio segment.\"\"\"\n+\n+\n+class AudioInputTurnDetection(BaseModel):\n+    prefix_padding_ms: Optional[int] = None\n+\n+    silence_duration_ms: Optional[int] = None\n+\n+    threshold: Optional[float] = None\n+\n+    type: Optional[str] = None\n+    \"\"\"Type of turn detection, only `server_vad` is currently supported.\"\"\"\n+\n+\n+class AudioInput(BaseModel):\n+    format: Optional[str] = None\n+    \"\"\"The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    noise_reduction: Optional[AudioInputNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\"\"\"\n+\n+    transcription: Optional[AudioInputTranscription] = None\n+    \"\"\"Configuration for input audio transcription.\"\"\"\n+\n+    turn_detection: Optional[AudioInputTurnDetection] = None\n+    \"\"\"Configuration for turn detection.\"\"\"\n+\n+\n+class AudioOutput(BaseModel):\n+    format: Optional[str] = None\n+    \"\"\"The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    speed: Optional[float] = None\n+\n+    voice: Union[\n+        str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"], None\n+    ] = None\n+\n+\n+class Audio(BaseModel):\n+    input: Optional[AudioInput] = None\n+\n+    output: Optional[AudioOutput] = None\n+\n+\n+class Tool(BaseModel):\n+    description: Optional[str] = None\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: Optional[str] = None\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: Optional[object] = None\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Optional[Literal[\"function\"]] = None\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class TracingTracingConfiguration(BaseModel):\n+    group_id: Optional[str] = None\n+    \"\"\"\n+    The group id to attach to this trace to enable filtering and grouping in the\n+    traces dashboard.\n+    \"\"\"\n+\n+    metadata: Optional[object] = None\n+    \"\"\"\n+    The arbitrary metadata to attach to this trace to enable filtering in the traces\n+    dashboard.\n+    \"\"\"\n+\n+    workflow_name: Optional[str] = None\n+    \"\"\"The name of the workflow to attach to this trace.\n+\n+    This is used to name the trace in the traces dashboard.\n+    \"\"\"\n+\n+\n+Tracing: TypeAlias = Union[Literal[\"auto\"], TracingTracingConfiguration]\n+\n+\n+class TurnDetection(BaseModel):\n+    prefix_padding_ms: Optional[int] = None\n+    \"\"\"Amount of audio to include before the VAD detected speech (in milliseconds).\n+\n+    Defaults to 300ms.\n+    \"\"\"\n+\n+    silence_duration_ms: Optional[int] = None\n+    \"\"\"Duration of silence to detect speech stop (in milliseconds).\n+\n+    Defaults to 500ms. With shorter values the model will respond more quickly, but\n+    may jump in on short pauses from the user.\n+    \"\"\"\n+\n+    threshold: Optional[float] = None\n+    \"\"\"Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5.\n+\n+    A higher threshold will require louder audio to activate the model, and thus\n+    might perform better in noisy environments.\n+    \"\"\"\n+\n+    type: Optional[str] = None\n+    \"\"\"Type of turn detection, only `server_vad` is currently supported.\"\"\"\n+\n+\n+class RealtimeSessionCreateResponse(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"Unique identifier for the session that looks like `sess_1234567890abcdef`.\"\"\"\n+\n+    audio: Optional[Audio] = None\n+    \"\"\"Configuration for input and output audio for the session.\"\"\"\n+\n+    expires_at: Optional[int] = None\n+    \"\"\"Expiration timestamp for the session, in seconds since epoch.\"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    instructions: Optional[str] = None\n+    \"\"\"The default system instructions (i.e.\n+\n+    system message) prepended to model calls. This field allows the client to guide\n+    the model on desired responses. The model can be instructed on response content\n+    and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of\n+    good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\n+    into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be\n+    followed by the model, but they provide guidance to the model on the desired\n+    behavior.\n+\n+    Note that the server sets default instructions which will be used if this field\n+    is not set and are visible in the `session.created` event at the start of the\n+    session.\n+    \"\"\"\n+\n+    max_output_tokens: Union[int, Literal[\"inf\"], None] = None\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n+    `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n+    \"\"\"\n+\n+    model: Optional[str] = None\n+    \"\"\"The Realtime model used for this session.\"\"\"\n+\n+    object: Optional[str] = None\n+    \"\"\"The object type. Always `realtime.session`.\"\"\"\n+\n+    output_modalities: Optional[List[Literal[\"text\", \"audio\"]]] = None\n+    \"\"\"The set of modalities the model can respond with.\n+\n+    To disable audio, set this to [\"text\"].\n+    \"\"\"\n+\n+    tool_choice: Optional[str] = None\n+    \"\"\"How the model chooses tools.\n+\n+    Options are `auto`, `none`, `required`, or specify a function.\n+    \"\"\"\n+\n+    tools: Optional[List[Tool]] = None\n+    \"\"\"Tools (functions) available to the model.\"\"\"\n+\n+    tracing: Optional[Tracing] = None\n+    \"\"\"Configuration options for tracing.\n+\n+    Set to null to disable tracing. Once tracing is enabled for a session, the\n+    configuration cannot be modified.\n+\n+    `auto` will create a trace for the session with default values for the workflow\n+    name, group id, and metadata.\n+    \"\"\"\n+\n+    turn_detection: Optional[TurnDetection] = None\n+    \"\"\"Configuration for turn detection.\n+\n+    Can be set to `null` to turn off. Server VAD means that the model will detect\n+    the start and end of speech based on audio volume and respond at the end of user\n+    speech.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tool_choice_config.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tool_choice_config.py b/src/openai/types/realtime/realtime_tool_choice_config.py\nnew file mode 100644\nindex 0000000..f93c490\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tool_choice_config.py\n@@ -0,0 +1,12 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union\n+from typing_extensions import TypeAlias\n+\n+from ..responses.tool_choice_mcp import ToolChoiceMcp\n+from ..responses.tool_choice_options import ToolChoiceOptions\n+from ..responses.tool_choice_function import ToolChoiceFunction\n+\n+__all__ = [\"RealtimeToolChoiceConfig\"]\n+\n+RealtimeToolChoiceConfig: TypeAlias = Union[ToolChoiceOptions, ToolChoiceFunction, ToolChoiceMcp]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tool_choice_config_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tool_choice_config_param.py b/src/openai/types/realtime/realtime_tool_choice_config_param.py\nnew file mode 100644\nindex 0000000..af92f24\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tool_choice_config_param.py\n@@ -0,0 +1,14 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import TypeAlias\n+\n+from ..responses.tool_choice_options import ToolChoiceOptions\n+from ..responses.tool_choice_mcp_param import ToolChoiceMcpParam\n+from ..responses.tool_choice_function_param import ToolChoiceFunctionParam\n+\n+__all__ = [\"RealtimeToolChoiceConfigParam\"]\n+\n+RealtimeToolChoiceConfigParam: TypeAlias = Union[ToolChoiceOptions, ToolChoiceFunctionParam, ToolChoiceMcpParam]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tools_config.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tools_config.py b/src/openai/types/realtime/realtime_tools_config.py\nnew file mode 100644\nindex 0000000..b97599a\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tools_config.py\n@@ -0,0 +1,10 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List\n+from typing_extensions import TypeAlias\n+\n+from .realtime_tools_config_union import RealtimeToolsConfigUnion\n+\n+__all__ = [\"RealtimeToolsConfig\"]\n+\n+RealtimeToolsConfig: TypeAlias = List[RealtimeToolsConfigUnion]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tools_config_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tools_config_param.py b/src/openai/types/realtime/realtime_tools_config_param.py\nnew file mode 100644\nindex 0000000..12af65c\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tools_config_param.py\n@@ -0,0 +1,158 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Dict, List, Union, Optional\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+__all__ = [\n+    \"RealtimeToolsConfigParam\",\n+    \"RealtimeToolsConfigUnionParam\",\n+    \"Function\",\n+    \"Mcp\",\n+    \"McpAllowedTools\",\n+    \"McpAllowedToolsMcpToolFilter\",\n+    \"McpRequireApproval\",\n+    \"McpRequireApprovalMcpToolApprovalFilter\",\n+    \"McpRequireApprovalMcpToolApprovalFilterAlways\",\n+    \"McpRequireApprovalMcpToolApprovalFilterNever\",\n+]\n+\n+\n+class Function(TypedDict, total=False):\n+    description: str\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: str\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: object\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Literal[\"function\"]\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class McpAllowedToolsMcpToolFilter(TypedDict, total=False):\n+    read_only: bool\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: List[str]\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+McpAllowedTools: TypeAlias = Union[List[str], McpAllowedToolsMcpToolFilter]\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilterAlways(TypedDict, total=False):\n+    read_only: bool\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: List[str]\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilterNever(TypedDict, total=False):\n+    read_only: bool\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: List[str]\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilter(TypedDict, total=False):\n+    always: McpRequireApprovalMcpToolApprovalFilterAlways\n+    \"\"\"A filter object to specify which tools are allowed.\"\"\"\n+\n+    never: McpRequireApprovalMcpToolApprovalFilterNever\n+    \"\"\"A filter object to specify which tools are allowed.\"\"\"\n+\n+\n+McpRequireApproval: TypeAlias = Union[McpRequireApprovalMcpToolApprovalFilter, Literal[\"always\", \"never\"]]\n+\n+\n+class Mcp(TypedDict, total=False):\n+    server_label: Required[str]\n+    \"\"\"A label for this MCP server, used to identify it in tool calls.\"\"\"\n+\n+    type: Required[Literal[\"mcp\"]]\n+    \"\"\"The type of the MCP tool. Always `mcp`.\"\"\"\n+\n+    allowed_tools: Optional[McpAllowedTools]\n+    \"\"\"List of allowed tool names or a filter object.\"\"\"\n+\n+    authorization: str\n+    \"\"\"\n+    An OAuth access token that can be used with a remote MCP server, either with a\n+    custom MCP server URL or a service connector. Your application must handle the\n+    OAuth authorization flow and provide the token here.\n+    \"\"\"\n+\n+    connector_id: Literal[\n+        \"connector_dropbox\",\n+        \"connector_gmail\",\n+        \"connector_googlecalendar\",\n+        \"connector_googledrive\",\n+        \"connector_microsoftteams\",\n+        \"connector_outlookcalendar\",\n+        \"connector_outlookemail\",\n+        \"connector_sharepoint\",\n+    ]\n+    \"\"\"Identifier for service connectors, like those available in ChatGPT.\n+\n+    One of `server_url` or `connector_id` must be provided. Learn more about service\n+    connectors\n+    [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n+\n+    Currently supported `connector_id` values are:\n+\n+    - Dropbox: `connector_dropbox`\n+    - Gmail: `connector_gmail`\n+    - Google Calendar: `connector_googlecalendar`\n+    - Google Drive: `connector_googledrive`\n+    - Microsoft Teams: `connector_microsoftteams`\n+    - Outlook Calendar: `connector_outlookcalendar`\n+    - Outlook Email: `connector_outlookemail`\n+    - SharePoint: `connector_sharepoint`\n+    \"\"\"\n+\n+    headers: Optional[Dict[str, str]]\n+    \"\"\"Optional HTTP headers to send to the MCP server.\n+\n+    Use for authentication or other purposes.\n+    \"\"\"\n+\n+    require_approval: Optional[McpRequireApproval]\n+    \"\"\"Specify which of the MCP server's tools require approval.\"\"\"\n+\n+    server_description: str\n+    \"\"\"Optional description of the MCP server, used to provide more context.\"\"\"\n+\n+    server_url: str\n+    \"\"\"The URL for the MCP server.\n+\n+    One of `server_url` or `connector_id` must be provided.\n+    \"\"\"\n+\n+\n+RealtimeToolsConfigUnionParam: TypeAlias = Union[Function, Mcp]\n+\n+RealtimeToolsConfigParam: TypeAlias = List[RealtimeToolsConfigUnionParam]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tools_config_union.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tools_config_union.py b/src/openai/types/realtime/realtime_tools_config_union.py\nnew file mode 100644\nindex 0000000..16b1557\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tools_config_union.py\n@@ -0,0 +1,158 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Dict, List, Union, Optional\n+from typing_extensions import Literal, Annotated, TypeAlias\n+\n+from ..._utils import PropertyInfo\n+from ..._models import BaseModel\n+\n+__all__ = [\n+    \"RealtimeToolsConfigUnion\",\n+    \"Function\",\n+    \"Mcp\",\n+    \"McpAllowedTools\",\n+    \"McpAllowedToolsMcpToolFilter\",\n+    \"McpRequireApproval\",\n+    \"McpRequireApprovalMcpToolApprovalFilter\",\n+    \"McpRequireApprovalMcpToolApprovalFilterAlways\",\n+    \"McpRequireApprovalMcpToolApprovalFilterNever\",\n+]\n+\n+\n+class Function(BaseModel):\n+    description: Optional[str] = None\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: Optional[str] = None\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: Optional[object] = None\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Optional[Literal[\"function\"]] = None\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class McpAllowedToolsMcpToolFilter(BaseModel):\n+    read_only: Optional[bool] = None\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: Optional[List[str]] = None\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+McpAllowedTools: TypeAlias = Union[List[str], McpAllowedToolsMcpToolFilter, None]\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilterAlways(BaseModel):\n+    read_only: Optional[bool] = None\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: Optional[List[str]] = None\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilterNever(BaseModel):\n+    read_only: Optional[bool] = None\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: Optional[List[str]] = None\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilter(BaseModel):\n+    always: Optional[McpRequireApprovalMcpToolApprovalFilterAlways] = None\n+    \"\"\"A filter object to specify which tools are allowed.\"\"\"\n+\n+    never: Optional[McpRequireApprovalMcpToolApprovalFilterNever] = None\n+    \"\"\"A filter object to specify which tools are allowed.\"\"\"\n+\n+\n+McpRequireApproval: TypeAlias = Union[McpRequireApprovalMcpToolApprovalFilter, Literal[\"always\", \"never\"], None]\n+\n+\n+class Mcp(BaseModel):\n+    server_label: str\n+    \"\"\"A label for this MCP server, used to identify it in tool calls.\"\"\"\n+\n+    type: Literal[\"mcp\"]\n+    \"\"\"The type of the MCP tool. Always `mcp`.\"\"\"\n+\n+    allowed_tools: Optional[McpAllowedTools] = None\n+    \"\"\"List of allowed tool names or a filter object.\"\"\"\n+\n+    authorization: Optional[str] = None\n+    \"\"\"\n+    An OAuth access token that can be used with a remote MCP server, either with a\n+    custom MCP server URL or a service connector. Your application must handle the\n+    OAuth authorization flow and provide the token here.\n+    \"\"\"\n+\n+    connector_id: Optional[\n+        Literal[\n+            \"connector_dropbox\",\n+            \"connector_gmail\",\n+            \"connector_googlecalendar\",\n+            \"connector_googledrive\",\n+            \"connector_microsoftteams\",\n+            \"connector_outlookcalendar\",\n+            \"connector_outlookemail\",\n+            \"connector_sharepoint\",\n+        ]\n+    ] = None\n+    \"\"\"Identifier for service connectors, like those available in ChatGPT.\n+\n+    One of `server_url` or `connector_id` must be provided. Learn more about service\n+    connectors\n+    [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n+\n+    Currently supported `connector_id` values are:\n+\n+    - Dropbox: `connector_dropbox`\n+    - Gmail: `connector_gmail`\n+    - Google Calendar: `connector_googlecalendar`\n+    - Google Drive: `connector_googledrive`\n+    - Microsoft Teams: `connector_microsoftteams`\n+    - Outlook Calendar: `connector_outlookcalendar`\n+    - Outlook Email: `connector_outlookemail`\n+    - SharePoint: `connector_sharepoint`\n+    \"\"\"\n+\n+    headers: Optional[Dict[str, str]] = None\n+    \"\"\"Optional HTTP headers to send to the MCP server.\n+\n+    Use for authentication or other purposes.\n+    \"\"\"\n+\n+    require_approval: Optional[McpRequireApproval] = None\n+    \"\"\"Specify which of the MCP server's tools require approval.\"\"\"\n+\n+    server_description: Optional[str] = None\n+    \"\"\"Optional description of the MCP server, used to provide more context.\"\"\"\n+\n+    server_url: Optional[str] = None\n+    \"\"\"The URL for the MCP server.\n+\n+    One of `server_url` or `connector_id` must be provided.\n+    \"\"\"\n+\n+\n+RealtimeToolsConfigUnion: TypeAlias = Annotated[Union[Function, Mcp], PropertyInfo(discriminator=\"type\")]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tools_config_union_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tools_config_union_param.py b/src/openai/types/realtime/realtime_tools_config_union_param.py\nnew file mode 100644\nindex 0000000..1b9f185\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tools_config_union_param.py\n@@ -0,0 +1,155 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Dict, List, Union, Optional\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+__all__ = [\n+    \"RealtimeToolsConfigUnionParam\",\n+    \"Function\",\n+    \"Mcp\",\n+    \"McpAllowedTools\",\n+    \"McpAllowedToolsMcpToolFilter\",\n+    \"McpRequireApproval\",\n+    \"McpRequireApprovalMcpToolApprovalFilter\",\n+    \"McpRequireApprovalMcpToolApprovalFilterAlways\",\n+    \"McpRequireApprovalMcpToolApprovalFilterNever\",\n+]\n+\n+\n+class Function(TypedDict, total=False):\n+    description: str\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: str\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: object\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Literal[\"function\"]\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class McpAllowedToolsMcpToolFilter(TypedDict, total=False):\n+    read_only: bool\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: List[str]\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+McpAllowedTools: TypeAlias = Union[List[str], McpAllowedToolsMcpToolFilter]\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilterAlways(TypedDict, total=False):\n+    read_only: bool\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: List[str]\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilterNever(TypedDict, total=False):\n+    read_only: bool\n+    \"\"\"Indicates whether or not a tool modifies data or is read-only.\n+\n+    If an MCP server is\n+    [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\n+    it will match this filter.\n+    \"\"\"\n+\n+    tool_names: List[str]\n+    \"\"\"List of allowed tool names.\"\"\"\n+\n+\n+class McpRequireApprovalMcpToolApprovalFilter(TypedDict, total=False):\n+    always: McpRequireApprovalMcpToolApprovalFilterAlways\n+    \"\"\"A filter object to specify which tools are allowed.\"\"\"\n+\n+    never: McpRequireApprovalMcpToolApprovalFilterNever\n+    \"\"\"A filter object to specify which tools are allowed.\"\"\"\n+\n+\n+McpRequireApproval: TypeAlias = Union[McpRequireApprovalMcpToolApprovalFilter, Literal[\"always\", \"never\"]]\n+\n+\n+class Mcp(TypedDict, total=False):\n+    server_label: Required[str]\n+    \"\"\"A label for this MCP server, used to identify it in tool calls.\"\"\"\n+\n+    type: Required[Literal[\"mcp\"]]\n+    \"\"\"The type of the MCP tool. Always `mcp`.\"\"\"\n+\n+    allowed_tools: Optional[McpAllowedTools]\n+    \"\"\"List of allowed tool names or a filter object.\"\"\"\n+\n+    authorization: str\n+    \"\"\"\n+    An OAuth access token that can be used with a remote MCP server, either with a\n+    custom MCP server URL or a service connector. Your application must handle the\n+    OAuth authorization flow and provide the token here.\n+    \"\"\"\n+\n+    connector_id: Literal[\n+        \"connector_dropbox\",\n+        \"connector_gmail\",\n+        \"connector_googlecalendar\",\n+        \"connector_googledrive\",\n+        \"connector_microsoftteams\",\n+        \"connector_outlookcalendar\",\n+        \"connector_outlookemail\",\n+        \"connector_sharepoint\",\n+    ]\n+    \"\"\"Identifier for service connectors, like those available in ChatGPT.\n+\n+    One of `server_url` or `connector_id` must be provided. Learn more about service\n+    connectors\n+    [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n+\n+    Currently supported `connector_id` values are:\n+\n+    - Dropbox: `connector_dropbox`\n+    - Gmail: `connector_gmail`\n+    - Google Calendar: `connector_googlecalendar`\n+    - Google Drive: `connector_googledrive`\n+    - Microsoft Teams: `connector_microsoftteams`\n+    - Outlook Calendar: `connector_outlookcalendar`\n+    - Outlook Email: `connector_outlookemail`\n+    - SharePoint: `connector_sharepoint`\n+    \"\"\"\n+\n+    headers: Optional[Dict[str, str]]\n+    \"\"\"Optional HTTP headers to send to the MCP server.\n+\n+    Use for authentication or other purposes.\n+    \"\"\"\n+\n+    require_approval: Optional[McpRequireApproval]\n+    \"\"\"Specify which of the MCP server's tools require approval.\"\"\"\n+\n+    server_description: str\n+    \"\"\"Optional description of the MCP server, used to provide more context.\"\"\"\n+\n+    server_url: str\n+    \"\"\"The URL for the MCP server.\n+\n+    One of `server_url` or `connector_id` must be provided.\n+    \"\"\"\n+\n+\n+RealtimeToolsConfigUnionParam: TypeAlias = Union[Function, Mcp]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tracing_config.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tracing_config.py b/src/openai/types/realtime/realtime_tracing_config.py\nnew file mode 100644\nindex 0000000..1de24d6\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tracing_config.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeTracingConfig\", \"TracingConfiguration\"]\n+\n+\n+class TracingConfiguration(BaseModel):\n+    group_id: Optional[str] = None\n+    \"\"\"\n+    The group id to attach to this trace to enable filtering and grouping in the\n+    traces dashboard.\n+    \"\"\"\n+\n+    metadata: Optional[object] = None\n+    \"\"\"\n+    The arbitrary metadata to attach to this trace to enable filtering in the traces\n+    dashboard.\n+    \"\"\"\n+\n+    workflow_name: Optional[str] = None\n+    \"\"\"The name of the workflow to attach to this trace.\n+\n+    This is used to name the trace in the traces dashboard.\n+    \"\"\"\n+\n+\n+RealtimeTracingConfig: TypeAlias = Union[Literal[\"auto\"], TracingConfiguration, None]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_tracing_config_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_tracing_config_param.py b/src/openai/types/realtime/realtime_tracing_config_param.py\nnew file mode 100644\nindex 0000000..3a35c6f\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_tracing_config_param.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import Literal, TypeAlias, TypedDict\n+\n+__all__ = [\"RealtimeTracingConfigParam\", \"TracingConfiguration\"]\n+\n+\n+class TracingConfiguration(TypedDict, total=False):\n+    group_id: str\n+    \"\"\"\n+    The group id to attach to this trace to enable filtering and grouping in the\n+    traces dashboard.\n+    \"\"\"\n+\n+    metadata: object\n+    \"\"\"\n+    The arbitrary metadata to attach to this trace to enable filtering in the traces\n+    dashboard.\n+    \"\"\"\n+\n+    workflow_name: str\n+    \"\"\"The name of the workflow to attach to this trace.\n+\n+    This is used to name the trace in the traces dashboard.\n+    \"\"\"\n+\n+\n+RealtimeTracingConfigParam: TypeAlias = Union[Literal[\"auto\"], TracingConfiguration]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_transcription_session_create_request.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_transcription_session_create_request.py b/src/openai/types/realtime/realtime_transcription_session_create_request.py\nnew file mode 100644\nindex 0000000..d67bc92\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_transcription_session_create_request.py\n@@ -0,0 +1,128 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\n+    \"RealtimeTranscriptionSessionCreateRequest\",\n+    \"InputAudioNoiseReduction\",\n+    \"InputAudioTranscription\",\n+    \"TurnDetection\",\n+]\n+\n+\n+class InputAudioNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+    \"\"\"Type of noise reduction.\n+\n+    `near_field` is for close-talking microphones such as headphones, `far_field` is\n+    for far-field microphones such as laptop or conference room microphones.\n+    \"\"\"\n+\n+\n+class InputAudioTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Optional[Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"]] = None\n+    \"\"\"\n+    The model to use for transcription, current options are `gpt-4o-transcribe`,\n+    `gpt-4o-mini-transcribe`, and `whisper-1`.\n+    \"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"\n+    An optional text to guide the model's style or continue a previous audio\n+    segment. For `whisper-1`, the\n+    [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n+    For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n+    \"expect words related to technology\".\n+    \"\"\"\n+\n+\n+class TurnDetection(BaseModel):\n+    prefix_padding_ms: Optional[int] = None\n+    \"\"\"Amount of audio to include before the VAD detected speech (in milliseconds).\n+\n+    Defaults to 300ms.\n+    \"\"\"\n+\n+    silence_duration_ms: Optional[int] = None\n+    \"\"\"Duration of silence to detect speech stop (in milliseconds).\n+\n+    Defaults to 500ms. With shorter values the model will respond more quickly, but\n+    may jump in on short pauses from the user.\n+    \"\"\"\n+\n+    threshold: Optional[float] = None\n+    \"\"\"Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5.\n+\n+    A higher threshold will require louder audio to activate the model, and thus\n+    might perform better in noisy environments.\n+    \"\"\"\n+\n+    type: Optional[Literal[\"server_vad\"]] = None\n+    \"\"\"Type of turn detection.\n+\n+    Only `server_vad` is currently supported for transcription sessions.\n+    \"\"\"\n+\n+\n+class RealtimeTranscriptionSessionCreateRequest(BaseModel):\n+    model: Union[str, Literal[\"whisper-1\", \"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\"]]\n+    \"\"\"ID of the model to use.\n+\n+    The options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `whisper-1`\n+    (which is powered by our open source Whisper V2 model).\n+    \"\"\"\n+\n+    type: Literal[\"transcription\"]\n+    \"\"\"The type of session to create.\n+\n+    Always `transcription` for transcription sessions.\n+    \"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"The set of items to include in the transcription. Current available items are:\n+\n+    - `item.input_audio_transcription.logprobs`\n+    \"\"\"\n+\n+    input_audio_format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of input audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, input audio must\n+    be 16-bit PCM at a 24kHz sample rate, single channel (mono), and little-endian\n+    byte order.\n+    \"\"\"\n+\n+    input_audio_noise_reduction: Optional[InputAudioNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\n+\n+    This can be set to `null` to turn off. Noise reduction filters audio added to\n+    the input audio buffer before it is sent to VAD and the model. Filtering the\n+    audio can improve VAD and turn detection accuracy (reducing false positives) and\n+    model performance by improving perception of the input audio.\n+    \"\"\"\n+\n+    input_audio_transcription: Optional[InputAudioTranscription] = None\n+    \"\"\"Configuration for input audio transcription.\n+\n+    The client can optionally set the language and prompt for transcription, these\n+    offer additional guidance to the transcription service.\n+    \"\"\"\n+\n+    turn_detection: Optional[TurnDetection] = None\n+    \"\"\"Configuration for turn detection.\n+\n+    Can be set to `null` to turn off. Server VAD means that the model will detect\n+    the start and end of speech based on audio volume and respond at the end of user\n+    speech.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_transcription_session_create_request_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_transcription_session_create_request_param.py b/src/openai/types/realtime/realtime_transcription_session_create_request_param.py\nnew file mode 100644\nindex 0000000..405f0c5\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_transcription_session_create_request_param.py\n@@ -0,0 +1,128 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import List, Union\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\n+    \"RealtimeTranscriptionSessionCreateRequestParam\",\n+    \"InputAudioNoiseReduction\",\n+    \"InputAudioTranscription\",\n+    \"TurnDetection\",\n+]\n+\n+\n+class InputAudioNoiseReduction(TypedDict, total=False):\n+    type: Literal[\"near_field\", \"far_field\"]\n+    \"\"\"Type of noise reduction.\n+\n+    `near_field` is for close-talking microphones such as headphones, `far_field` is\n+    for far-field microphones such as laptop or conference room microphones.\n+    \"\"\"\n+\n+\n+class InputAudioTranscription(TypedDict, total=False):\n+    language: str\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"]\n+    \"\"\"\n+    The model to use for transcription, current options are `gpt-4o-transcribe`,\n+    `gpt-4o-mini-transcribe`, and `whisper-1`.\n+    \"\"\"\n+\n+    prompt: str\n+    \"\"\"\n+    An optional text to guide the model's style or continue a previous audio\n+    segment. For `whisper-1`, the\n+    [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\n+    For `gpt-4o-transcribe` models, the prompt is a free text string, for example\n+    \"expect words related to technology\".\n+    \"\"\"\n+\n+\n+class TurnDetection(TypedDict, total=False):\n+    prefix_padding_ms: int\n+    \"\"\"Amount of audio to include before the VAD detected speech (in milliseconds).\n+\n+    Defaults to 300ms.\n+    \"\"\"\n+\n+    silence_duration_ms: int\n+    \"\"\"Duration of silence to detect speech stop (in milliseconds).\n+\n+    Defaults to 500ms. With shorter values the model will respond more quickly, but\n+    may jump in on short pauses from the user.\n+    \"\"\"\n+\n+    threshold: float\n+    \"\"\"Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5.\n+\n+    A higher threshold will require louder audio to activate the model, and thus\n+    might perform better in noisy environments.\n+    \"\"\"\n+\n+    type: Literal[\"server_vad\"]\n+    \"\"\"Type of turn detection.\n+\n+    Only `server_vad` is currently supported for transcription sessions.\n+    \"\"\"\n+\n+\n+class RealtimeTranscriptionSessionCreateRequestParam(TypedDict, total=False):\n+    model: Required[Union[str, Literal[\"whisper-1\", \"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\"]]]\n+    \"\"\"ID of the model to use.\n+\n+    The options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `whisper-1`\n+    (which is powered by our open source Whisper V2 model).\n+    \"\"\"\n+\n+    type: Required[Literal[\"transcription\"]]\n+    \"\"\"The type of session to create.\n+\n+    Always `transcription` for transcription sessions.\n+    \"\"\"\n+\n+    include: List[Literal[\"item.input_audio_transcription.logprobs\"]]\n+    \"\"\"The set of items to include in the transcription. Current available items are:\n+\n+    - `item.input_audio_transcription.logprobs`\n+    \"\"\"\n+\n+    input_audio_format: Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]\n+    \"\"\"The format of input audio.\n+\n+    Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For `pcm16`, input audio must\n+    be 16-bit PCM at a 24kHz sample rate, single channel (mono), and little-endian\n+    byte order.\n+    \"\"\"\n+\n+    input_audio_noise_reduction: InputAudioNoiseReduction\n+    \"\"\"Configuration for input audio noise reduction.\n+\n+    This can be set to `null` to turn off. Noise reduction filters audio added to\n+    the input audio buffer before it is sent to VAD and the model. Filtering the\n+    audio can improve VAD and turn detection accuracy (reducing false positives) and\n+    model performance by improving perception of the input audio.\n+    \"\"\"\n+\n+    input_audio_transcription: InputAudioTranscription\n+    \"\"\"Configuration for input audio transcription.\n+\n+    The client can optionally set the language and prompt for transcription, these\n+    offer additional guidance to the transcription service.\n+    \"\"\"\n+\n+    turn_detection: TurnDetection\n+    \"\"\"Configuration for turn detection.\n+\n+    Can be set to `null` to turn off. Server VAD means that the model will detect\n+    the start and end of speech based on audio volume and respond at the end of user\n+    speech.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_truncation.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_truncation.py b/src/openai/types/realtime/realtime_truncation.py\nnew file mode 100644\nindex 0000000..4687e3d\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_truncation.py\n@@ -0,0 +1,22 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeTruncation\", \"RetentionRatioTruncation\"]\n+\n+\n+class RetentionRatioTruncation(BaseModel):\n+    retention_ratio: float\n+    \"\"\"Fraction of pre-instruction conversation tokens to retain (0.0 - 1.0).\"\"\"\n+\n+    type: Literal[\"retention_ratio\"]\n+    \"\"\"Use retention ratio truncation.\"\"\"\n+\n+    post_instructions_token_limit: Optional[int] = None\n+    \"\"\"Optional cap on tokens allowed after the instructions.\"\"\"\n+\n+\n+RealtimeTruncation: TypeAlias = Union[Literal[\"auto\", \"disabled\"], RetentionRatioTruncation]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/realtime_truncation_param.py",
            "diff": "diff --git a/src/openai/types/realtime/realtime_truncation_param.py b/src/openai/types/realtime/realtime_truncation_param.py\nnew file mode 100644\nindex 0000000..edc88ea\n--- /dev/null\n+++ b/src/openai/types/realtime/realtime_truncation_param.py\n@@ -0,0 +1,22 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union, Optional\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+__all__ = [\"RealtimeTruncationParam\", \"RetentionRatioTruncation\"]\n+\n+\n+class RetentionRatioTruncation(TypedDict, total=False):\n+    retention_ratio: Required[float]\n+    \"\"\"Fraction of pre-instruction conversation tokens to retain (0.0 - 1.0).\"\"\"\n+\n+    type: Required[Literal[\"retention_ratio\"]]\n+    \"\"\"Use retention ratio truncation.\"\"\"\n+\n+    post_instructions_token_limit: Optional[int]\n+    \"\"\"Optional cap on tokens allowed after the instructions.\"\"\"\n+\n+\n+RealtimeTruncationParam: TypeAlias = Union[Literal[\"auto\", \"disabled\"], RetentionRatioTruncation]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_audio_delta_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_audio_delta_event.py b/src/openai/types/realtime/response_audio_delta_event.py\nnew file mode 100644\nindex 0000000..d92c546\n--- /dev/null\n+++ b/src/openai/types/realtime/response_audio_delta_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseAudioDeltaEvent\"]\n+\n+\n+class ResponseAudioDeltaEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    delta: str\n+    \"\"\"Base64-encoded audio data delta.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.output_audio.delta\"]\n+    \"\"\"The event type, must be `response.output_audio.delta`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_audio_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_audio_done_event.py b/src/openai/types/realtime/response_audio_done_event.py\nnew file mode 100644\nindex 0000000..5ea0f07\n--- /dev/null\n+++ b/src/openai/types/realtime/response_audio_done_event.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseAudioDoneEvent\"]\n+\n+\n+class ResponseAudioDoneEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.output_audio.done\"]\n+    \"\"\"The event type, must be `response.output_audio.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_audio_transcript_delta_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_audio_transcript_delta_event.py b/src/openai/types/realtime/response_audio_transcript_delta_event.py\nnew file mode 100644\nindex 0000000..4dd5fec\n--- /dev/null\n+++ b/src/openai/types/realtime/response_audio_transcript_delta_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseAudioTranscriptDeltaEvent\"]\n+\n+\n+class ResponseAudioTranscriptDeltaEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    delta: str\n+    \"\"\"The transcript delta.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.output_audio_transcript.delta\"]\n+    \"\"\"The event type, must be `response.output_audio_transcript.delta`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_audio_transcript_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_audio_transcript_done_event.py b/src/openai/types/realtime/response_audio_transcript_done_event.py\nnew file mode 100644\nindex 0000000..2de913d\n--- /dev/null\n+++ b/src/openai/types/realtime/response_audio_transcript_done_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseAudioTranscriptDoneEvent\"]\n+\n+\n+class ResponseAudioTranscriptDoneEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    transcript: str\n+    \"\"\"The final transcript of the audio.\"\"\"\n+\n+    type: Literal[\"response.output_audio_transcript.done\"]\n+    \"\"\"The event type, must be `response.output_audio_transcript.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_cancel_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_cancel_event.py b/src/openai/types/realtime/response_cancel_event.py\nnew file mode 100644\nindex 0000000..15dc141\n--- /dev/null\n+++ b/src/openai/types/realtime/response_cancel_event.py\n@@ -0,0 +1,22 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseCancelEvent\"]\n+\n+\n+class ResponseCancelEvent(BaseModel):\n+    type: Literal[\"response.cancel\"]\n+    \"\"\"The event type, must be `response.cancel`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n+\n+    response_id: Optional[str] = None\n+    \"\"\"\n+    A specific response ID to cancel - if not provided, will cancel an in-progress\n+    response in the default conversation.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_cancel_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/response_cancel_event_param.py b/src/openai/types/realtime/response_cancel_event_param.py\nnew file mode 100644\nindex 0000000..f337407\n--- /dev/null\n+++ b/src/openai/types/realtime/response_cancel_event_param.py\n@@ -0,0 +1,21 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ResponseCancelEventParam\"]\n+\n+\n+class ResponseCancelEventParam(TypedDict, total=False):\n+    type: Required[Literal[\"response.cancel\"]]\n+    \"\"\"The event type, must be `response.cancel`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n+\n+    response_id: str\n+    \"\"\"\n+    A specific response ID to cancel - if not provided, will cancel an in-progress\n+    response in the default conversation.\n+    \"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_content_part_added_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_content_part_added_event.py b/src/openai/types/realtime/response_content_part_added_event.py\nnew file mode 100644\nindex 0000000..aca965c\n--- /dev/null\n+++ b/src/openai/types/realtime/response_content_part_added_event.py\n@@ -0,0 +1,45 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseContentPartAddedEvent\", \"Part\"]\n+\n+\n+class Part(BaseModel):\n+    audio: Optional[str] = None\n+    \"\"\"Base64-encoded audio data (if type is \"audio\").\"\"\"\n+\n+    text: Optional[str] = None\n+    \"\"\"The text content (if type is \"text\").\"\"\"\n+\n+    transcript: Optional[str] = None\n+    \"\"\"The transcript of the audio (if type is \"audio\").\"\"\"\n+\n+    type: Optional[Literal[\"text\", \"audio\"]] = None\n+    \"\"\"The content type (\"text\", \"audio\").\"\"\"\n+\n+\n+class ResponseContentPartAddedEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item to which the content part was added.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    part: Part\n+    \"\"\"The content part that was added.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.content_part.added\"]\n+    \"\"\"The event type, must be `response.content_part.added`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_content_part_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_content_part_done_event.py b/src/openai/types/realtime/response_content_part_done_event.py\nnew file mode 100644\nindex 0000000..59af808\n--- /dev/null\n+++ b/src/openai/types/realtime/response_content_part_done_event.py\n@@ -0,0 +1,45 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseContentPartDoneEvent\", \"Part\"]\n+\n+\n+class Part(BaseModel):\n+    audio: Optional[str] = None\n+    \"\"\"Base64-encoded audio data (if type is \"audio\").\"\"\"\n+\n+    text: Optional[str] = None\n+    \"\"\"The text content (if type is \"text\").\"\"\"\n+\n+    transcript: Optional[str] = None\n+    \"\"\"The transcript of the audio (if type is \"audio\").\"\"\"\n+\n+    type: Optional[Literal[\"text\", \"audio\"]] = None\n+    \"\"\"The content type (\"text\", \"audio\").\"\"\"\n+\n+\n+class ResponseContentPartDoneEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    part: Part\n+    \"\"\"The content part that is done.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.content_part.done\"]\n+    \"\"\"The event type, must be `response.content_part.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_create_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_create_event.py b/src/openai/types/realtime/response_create_event.py\nnew file mode 100644\nindex 0000000..a37045e\n--- /dev/null\n+++ b/src/openai/types/realtime/response_create_event.py\n@@ -0,0 +1,134 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Union, Optional\n+from typing_extensions import Literal, TypeAlias\n+\n+from ..._models import BaseModel\n+from ..shared.metadata import Metadata\n+from .conversation_item import ConversationItem\n+from ..responses.response_prompt import ResponsePrompt\n+from ..responses.tool_choice_mcp import ToolChoiceMcp\n+from ..responses.tool_choice_options import ToolChoiceOptions\n+from ..responses.tool_choice_function import ToolChoiceFunction\n+\n+__all__ = [\"ResponseCreateEvent\", \"Response\", \"ResponseToolChoice\", \"ResponseTool\"]\n+\n+ResponseToolChoice: TypeAlias = Union[ToolChoiceOptions, ToolChoiceFunction, ToolChoiceMcp]\n+\n+\n+class ResponseTool(BaseModel):\n+    description: Optional[str] = None\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: Optional[str] = None\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: Optional[object] = None\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Optional[Literal[\"function\"]] = None\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class Response(BaseModel):\n+    conversation: Union[str, Literal[\"auto\", \"none\"], None] = None\n+    \"\"\"Controls which conversation the response is added to.\n+\n+    Currently supports `auto` and `none`, with `auto` as the default value. The\n+    `auto` value means that the contents of the response will be added to the\n+    default conversation. Set this to `none` to create an out-of-band response which\n+    will not add items to default conversation.\n+    \"\"\"\n+\n+    input: Optional[List[ConversationItem]] = None\n+    \"\"\"Input items to include in the prompt for the model.\n+\n+    Using this field creates a new context for this Response instead of using the\n+    default conversation. An empty array `[]` will clear the context for this\n+    Response. Note that this can include references to items from the default\n+    conversation.\n+    \"\"\"\n+\n+    instructions: Optional[str] = None\n+    \"\"\"The default system instructions (i.e.\n+\n+    system message) prepended to model calls. This field allows the client to guide\n+    the model on desired responses. The model can be instructed on response content\n+    and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of\n+    good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\n+    into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be\n+    followed by the model, but they provide guidance to the model on the desired\n+    behavior.\n+\n+    Note that the server sets default instructions which will be used if this field\n+    is not set and are visible in the `session.created` event at the start of the\n+    session.\n+    \"\"\"\n+\n+    max_output_tokens: Union[int, Literal[\"inf\"], None] = None\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n+    `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n+    \"\"\"\n+\n+    metadata: Optional[Metadata] = None\n+    \"\"\"Set of 16 key-value pairs that can be attached to an object.\n+\n+    This can be useful for storing additional information about the object in a\n+    structured format, and querying for objects via API or the dashboard.\n+\n+    Keys are strings with a maximum length of 64 characters. Values are strings with\n+    a maximum length of 512 characters.\n+    \"\"\"\n+\n+    modalities: Optional[List[Literal[\"text\", \"audio\"]]] = None\n+    \"\"\"The set of modalities the model can respond with.\n+\n+    To disable audio, set this to [\"text\"].\n+    \"\"\"\n+\n+    output_audio_format: Optional[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]] = None\n+    \"\"\"The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    prompt: Optional[ResponsePrompt] = None\n+    \"\"\"Reference to a prompt template and its variables.\n+\n+    [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n+    \"\"\"\n+\n+    temperature: Optional[float] = None\n+    \"\"\"Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\"\"\"\n+\n+    tool_choice: Optional[ResponseToolChoice] = None\n+    \"\"\"How the model chooses tools.\n+\n+    Provide one of the string modes or force a specific function/MCP tool.\n+    \"\"\"\n+\n+    tools: Optional[List[ResponseTool]] = None\n+    \"\"\"Tools (functions) available to the model.\"\"\"\n+\n+    voice: Union[\n+        str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"], None\n+    ] = None\n+    \"\"\"The voice the model uses to respond.\n+\n+    Voice cannot be changed during the session once the model has responded with\n+    audio at least once. Current voice options are `alloy`, `ash`, `ballad`,\n+    `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n+    \"\"\"\n+\n+\n+class ResponseCreateEvent(BaseModel):\n+    type: Literal[\"response.create\"]\n+    \"\"\"The event type, must be `response.create`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n+\n+    response: Optional[Response] = None\n+    \"\"\"Create a new Realtime response with these parameters\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_create_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/response_create_event_param.py b/src/openai/types/realtime/response_create_event_param.py\nnew file mode 100644\nindex 0000000..f941c4c\n--- /dev/null\n+++ b/src/openai/types/realtime/response_create_event_param.py\n@@ -0,0 +1,133 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import List, Union, Iterable, Optional\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+from ..shared_params.metadata import Metadata\n+from .conversation_item_param import ConversationItemParam\n+from ..responses.tool_choice_options import ToolChoiceOptions\n+from ..responses.response_prompt_param import ResponsePromptParam\n+from ..responses.tool_choice_mcp_param import ToolChoiceMcpParam\n+from ..responses.tool_choice_function_param import ToolChoiceFunctionParam\n+\n+__all__ = [\"ResponseCreateEventParam\", \"Response\", \"ResponseToolChoice\", \"ResponseTool\"]\n+\n+ResponseToolChoice: TypeAlias = Union[ToolChoiceOptions, ToolChoiceFunctionParam, ToolChoiceMcpParam]\n+\n+\n+class ResponseTool(TypedDict, total=False):\n+    description: str\n+    \"\"\"\n+    The description of the function, including guidance on when and how to call it,\n+    and guidance about what to tell the user when calling (if anything).\n+    \"\"\"\n+\n+    name: str\n+    \"\"\"The name of the function.\"\"\"\n+\n+    parameters: object\n+    \"\"\"Parameters of the function in JSON Schema.\"\"\"\n+\n+    type: Literal[\"function\"]\n+    \"\"\"The type of the tool, i.e. `function`.\"\"\"\n+\n+\n+class Response(TypedDict, total=False):\n+    conversation: Union[str, Literal[\"auto\", \"none\"]]\n+    \"\"\"Controls which conversation the response is added to.\n+\n+    Currently supports `auto` and `none`, with `auto` as the default value. The\n+    `auto` value means that the contents of the response will be added to the\n+    default conversation. Set this to `none` to create an out-of-band response which\n+    will not add items to default conversation.\n+    \"\"\"\n+\n+    input: Iterable[ConversationItemParam]\n+    \"\"\"Input items to include in the prompt for the model.\n+\n+    Using this field creates a new context for this Response instead of using the\n+    default conversation. An empty array `[]` will clear the context for this\n+    Response. Note that this can include references to items from the default\n+    conversation.\n+    \"\"\"\n+\n+    instructions: str\n+    \"\"\"The default system instructions (i.e.\n+\n+    system message) prepended to model calls. This field allows the client to guide\n+    the model on desired responses. The model can be instructed on response content\n+    and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of\n+    good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\n+    into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be\n+    followed by the model, but they provide guidance to the model on the desired\n+    behavior.\n+\n+    Note that the server sets default instructions which will be used if this field\n+    is not set and are visible in the `session.created` event at the start of the\n+    session.\n+    \"\"\"\n+\n+    max_output_tokens: Union[int, Literal[\"inf\"]]\n+    \"\"\"\n+    Maximum number of output tokens for a single assistant response, inclusive of\n+    tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\n+    `inf` for the maximum available tokens for a given model. Defaults to `inf`.\n+    \"\"\"\n+\n+    metadata: Optional[Metadata]\n+    \"\"\"Set of 16 key-value pairs that can be attached to an object.\n+\n+    This can be useful for storing additional information about the object in a\n+    structured format, and querying for objects via API or the dashboard.\n+\n+    Keys are strings with a maximum length of 64 characters. Values are strings with\n+    a maximum length of 512 characters.\n+    \"\"\"\n+\n+    modalities: List[Literal[\"text\", \"audio\"]]\n+    \"\"\"The set of modalities the model can respond with.\n+\n+    To disable audio, set this to [\"text\"].\n+    \"\"\"\n+\n+    output_audio_format: Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"]\n+    \"\"\"The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    prompt: Optional[ResponsePromptParam]\n+    \"\"\"Reference to a prompt template and its variables.\n+\n+    [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n+    \"\"\"\n+\n+    temperature: float\n+    \"\"\"Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\"\"\"\n+\n+    tool_choice: ResponseToolChoice\n+    \"\"\"How the model chooses tools.\n+\n+    Provide one of the string modes or force a specific function/MCP tool.\n+    \"\"\"\n+\n+    tools: Iterable[ResponseTool]\n+    \"\"\"Tools (functions) available to the model.\"\"\"\n+\n+    voice: Union[str, Literal[\"alloy\", \"ash\", \"ballad\", \"coral\", \"echo\", \"sage\", \"shimmer\", \"verse\", \"marin\", \"cedar\"]]\n+    \"\"\"The voice the model uses to respond.\n+\n+    Voice cannot be changed during the session once the model has responded with\n+    audio at least once. Current voice options are `alloy`, `ash`, `ballad`,\n+    `coral`, `echo`, `sage`, `shimmer`, and `verse`.\n+    \"\"\"\n+\n+\n+class ResponseCreateEventParam(TypedDict, total=False):\n+    type: Required[Literal[\"response.create\"]]\n+    \"\"\"The event type, must be `response.create`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n+\n+    response: Response\n+    \"\"\"Create a new Realtime response with these parameters\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_created_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_created_event.py b/src/openai/types/realtime/response_created_event.py\nnew file mode 100644\nindex 0000000..996bf26\n--- /dev/null\n+++ b/src/openai/types/realtime/response_created_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_response import RealtimeResponse\n+\n+__all__ = [\"ResponseCreatedEvent\"]\n+\n+\n+class ResponseCreatedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    response: RealtimeResponse\n+    \"\"\"The response resource.\"\"\"\n+\n+    type: Literal[\"response.created\"]\n+    \"\"\"The event type, must be `response.created`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_done_event.py b/src/openai/types/realtime/response_done_event.py\nnew file mode 100644\nindex 0000000..ce9a4b9\n--- /dev/null\n+++ b/src/openai/types/realtime/response_done_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_response import RealtimeResponse\n+\n+__all__ = [\"ResponseDoneEvent\"]\n+\n+\n+class ResponseDoneEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    response: RealtimeResponse\n+    \"\"\"The response resource.\"\"\"\n+\n+    type: Literal[\"response.done\"]\n+    \"\"\"The event type, must be `response.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_function_call_arguments_delta_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_function_call_arguments_delta_event.py b/src/openai/types/realtime/response_function_call_arguments_delta_event.py\nnew file mode 100644\nindex 0000000..6d96e78\n--- /dev/null\n+++ b/src/openai/types/realtime/response_function_call_arguments_delta_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseFunctionCallArgumentsDeltaEvent\"]\n+\n+\n+class ResponseFunctionCallArgumentsDeltaEvent(BaseModel):\n+    call_id: str\n+    \"\"\"The ID of the function call.\"\"\"\n+\n+    delta: str\n+    \"\"\"The arguments delta as a JSON string.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the function call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.function_call_arguments.delta\"]\n+    \"\"\"The event type, must be `response.function_call_arguments.delta`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_function_call_arguments_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_function_call_arguments_done_event.py b/src/openai/types/realtime/response_function_call_arguments_done_event.py\nnew file mode 100644\nindex 0000000..be7fae9\n--- /dev/null\n+++ b/src/openai/types/realtime/response_function_call_arguments_done_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseFunctionCallArgumentsDoneEvent\"]\n+\n+\n+class ResponseFunctionCallArgumentsDoneEvent(BaseModel):\n+    arguments: str\n+    \"\"\"The final arguments as a JSON string.\"\"\"\n+\n+    call_id: str\n+    \"\"\"The ID of the function call.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the function call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.function_call_arguments.done\"]\n+    \"\"\"The event type, must be `response.function_call_arguments.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_mcp_call_arguments_delta.py",
            "diff": "diff --git a/src/openai/types/realtime/response_mcp_call_arguments_delta.py b/src/openai/types/realtime/response_mcp_call_arguments_delta.py\nnew file mode 100644\nindex 0000000..0a02a1a\n--- /dev/null\n+++ b/src/openai/types/realtime/response_mcp_call_arguments_delta.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseMcpCallArgumentsDelta\"]\n+\n+\n+class ResponseMcpCallArgumentsDelta(BaseModel):\n+    delta: str\n+    \"\"\"The JSON-encoded arguments delta.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP tool call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.mcp_call_arguments.delta\"]\n+    \"\"\"The event type, must be `response.mcp_call_arguments.delta`.\"\"\"\n+\n+    obfuscation: Optional[str] = None\n+    \"\"\"If present, indicates the delta text was obfuscated.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_mcp_call_arguments_done.py",
            "diff": "diff --git a/src/openai/types/realtime/response_mcp_call_arguments_done.py b/src/openai/types/realtime/response_mcp_call_arguments_done.py\nnew file mode 100644\nindex 0000000..5ec95f1\n--- /dev/null\n+++ b/src/openai/types/realtime/response_mcp_call_arguments_done.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseMcpCallArgumentsDone\"]\n+\n+\n+class ResponseMcpCallArgumentsDone(BaseModel):\n+    arguments: str\n+    \"\"\"The final JSON-encoded arguments string.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP tool call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.mcp_call_arguments.done\"]\n+    \"\"\"The event type, must be `response.mcp_call_arguments.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_mcp_call_completed.py",
            "diff": "diff --git a/src/openai/types/realtime/response_mcp_call_completed.py b/src/openai/types/realtime/response_mcp_call_completed.py\nnew file mode 100644\nindex 0000000..e3fcec2\n--- /dev/null\n+++ b/src/openai/types/realtime/response_mcp_call_completed.py\n@@ -0,0 +1,21 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseMcpCallCompleted\"]\n+\n+\n+class ResponseMcpCallCompleted(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP tool call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    type: Literal[\"response.mcp_call.completed\"]\n+    \"\"\"The event type, must be `response.mcp_call.completed`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_mcp_call_failed.py",
            "diff": "diff --git a/src/openai/types/realtime/response_mcp_call_failed.py b/src/openai/types/realtime/response_mcp_call_failed.py\nnew file mode 100644\nindex 0000000..b7adc8c\n--- /dev/null\n+++ b/src/openai/types/realtime/response_mcp_call_failed.py\n@@ -0,0 +1,21 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseMcpCallFailed\"]\n+\n+\n+class ResponseMcpCallFailed(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP tool call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    type: Literal[\"response.mcp_call.failed\"]\n+    \"\"\"The event type, must be `response.mcp_call.failed`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_mcp_call_in_progress.py",
            "diff": "diff --git a/src/openai/types/realtime/response_mcp_call_in_progress.py b/src/openai/types/realtime/response_mcp_call_in_progress.py\nnew file mode 100644\nindex 0000000..d0fcc76\n--- /dev/null\n+++ b/src/openai/types/realtime/response_mcp_call_in_progress.py\n@@ -0,0 +1,21 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseMcpCallInProgress\"]\n+\n+\n+class ResponseMcpCallInProgress(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the MCP tool call item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    type: Literal[\"response.mcp_call.in_progress\"]\n+    \"\"\"The event type, must be `response.mcp_call.in_progress`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_output_item_added_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_output_item_added_event.py b/src/openai/types/realtime/response_output_item_added_event.py\nnew file mode 100644\nindex 0000000..509dfca\n--- /dev/null\n+++ b/src/openai/types/realtime/response_output_item_added_event.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+\n+__all__ = [\"ResponseOutputItemAddedEvent\"]\n+\n+\n+class ResponseOutputItemAddedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the Response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the Response to which the item belongs.\"\"\"\n+\n+    type: Literal[\"response.output_item.added\"]\n+    \"\"\"The event type, must be `response.output_item.added`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_output_item_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_output_item_done_event.py b/src/openai/types/realtime/response_output_item_done_event.py\nnew file mode 100644\nindex 0000000..800e4ae\n--- /dev/null\n+++ b/src/openai/types/realtime/response_output_item_done_event.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .conversation_item import ConversationItem\n+\n+__all__ = [\"ResponseOutputItemDoneEvent\"]\n+\n+\n+class ResponseOutputItemDoneEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item: ConversationItem\n+    \"\"\"A single item within a Realtime conversation.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the Response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the Response to which the item belongs.\"\"\"\n+\n+    type: Literal[\"response.output_item.done\"]\n+    \"\"\"The event type, must be `response.output_item.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_text_delta_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_text_delta_event.py b/src/openai/types/realtime/response_text_delta_event.py\nnew file mode 100644\nindex 0000000..493348a\n--- /dev/null\n+++ b/src/openai/types/realtime/response_text_delta_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseTextDeltaEvent\"]\n+\n+\n+class ResponseTextDeltaEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    delta: str\n+    \"\"\"The text delta.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    type: Literal[\"response.output_text.delta\"]\n+    \"\"\"The event type, must be `response.output_text.delta`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/response_text_done_event.py",
            "diff": "diff --git a/src/openai/types/realtime/response_text_done_event.py b/src/openai/types/realtime/response_text_done_event.py\nnew file mode 100644\nindex 0000000..83c6cf0\n--- /dev/null\n+++ b/src/openai/types/realtime/response_text_done_event.py\n@@ -0,0 +1,30 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseTextDoneEvent\"]\n+\n+\n+class ResponseTextDoneEvent(BaseModel):\n+    content_index: int\n+    \"\"\"The index of the content part in the item's content array.\"\"\"\n+\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    item_id: str\n+    \"\"\"The ID of the item.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output item in the response.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the response.\"\"\"\n+\n+    text: str\n+    \"\"\"The final text content.\"\"\"\n+\n+    type: Literal[\"response.output_text.done\"]\n+    \"\"\"The event type, must be `response.output_text.done`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/session_created_event.py",
            "diff": "diff --git a/src/openai/types/realtime/session_created_event.py b/src/openai/types/realtime/session_created_event.py\nnew file mode 100644\nindex 0000000..51f7570\n--- /dev/null\n+++ b/src/openai/types/realtime/session_created_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_session import RealtimeSession\n+\n+__all__ = [\"SessionCreatedEvent\"]\n+\n+\n+class SessionCreatedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    session: RealtimeSession\n+    \"\"\"Realtime session object.\"\"\"\n+\n+    type: Literal[\"session.created\"]\n+    \"\"\"The event type, must be `session.created`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/session_update_event.py",
            "diff": "diff --git a/src/openai/types/realtime/session_update_event.py b/src/openai/types/realtime/session_update_event.py\nnew file mode 100644\nindex 0000000..00a4377\n--- /dev/null\n+++ b/src/openai/types/realtime/session_update_event.py\n@@ -0,0 +1,20 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_session_create_request import RealtimeSessionCreateRequest\n+\n+__all__ = [\"SessionUpdateEvent\"]\n+\n+\n+class SessionUpdateEvent(BaseModel):\n+    session: RealtimeSessionCreateRequest\n+    \"\"\"Realtime session object configuration.\"\"\"\n+\n+    type: Literal[\"session.update\"]\n+    \"\"\"The event type, must be `session.update`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/session_update_event_param.py",
            "diff": "diff --git a/src/openai/types/realtime/session_update_event_param.py b/src/openai/types/realtime/session_update_event_param.py\nnew file mode 100644\nindex 0000000..79ff05f\n--- /dev/null\n+++ b/src/openai/types/realtime/session_update_event_param.py\n@@ -0,0 +1,20 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from .realtime_session_create_request_param import RealtimeSessionCreateRequestParam\n+\n+__all__ = [\"SessionUpdateEventParam\"]\n+\n+\n+class SessionUpdateEventParam(TypedDict, total=False):\n+    session: Required[RealtimeSessionCreateRequestParam]\n+    \"\"\"Realtime session object configuration.\"\"\"\n+\n+    type: Required[Literal[\"session.update\"]]\n+    \"\"\"The event type, must be `session.update`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/session_updated_event.py",
            "diff": "diff --git a/src/openai/types/realtime/session_updated_event.py b/src/openai/types/realtime/session_updated_event.py\nnew file mode 100644\nindex 0000000..b8a5972\n--- /dev/null\n+++ b/src/openai/types/realtime/session_updated_event.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_session import RealtimeSession\n+\n+__all__ = [\"SessionUpdatedEvent\"]\n+\n+\n+class SessionUpdatedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    session: RealtimeSession\n+    \"\"\"Realtime session object.\"\"\"\n+\n+    type: Literal[\"session.updated\"]\n+    \"\"\"The event type, must be `session.updated`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/transcription_session_created.py",
            "diff": "diff --git a/src/openai/types/realtime/transcription_session_created.py b/src/openai/types/realtime/transcription_session_created.py\nnew file mode 100644\nindex 0000000..1d34d15\n--- /dev/null\n+++ b/src/openai/types/realtime/transcription_session_created.py\n@@ -0,0 +1,105 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\n+    \"TranscriptionSessionCreated\",\n+    \"Session\",\n+    \"SessionAudio\",\n+    \"SessionAudioInput\",\n+    \"SessionAudioInputNoiseReduction\",\n+    \"SessionAudioInputTranscription\",\n+    \"SessionAudioInputTurnDetection\",\n+]\n+\n+\n+class SessionAudioInputNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+\n+\n+class SessionAudioInputTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Optional[Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"]] = None\n+    \"\"\"The model to use for transcription.\n+\n+    Can be `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, or `whisper-1`.\n+    \"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"An optional text to guide the model's style or continue a previous audio\n+    segment.\n+\n+    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n+    should match the audio language.\n+    \"\"\"\n+\n+\n+class SessionAudioInputTurnDetection(BaseModel):\n+    prefix_padding_ms: Optional[int] = None\n+\n+    silence_duration_ms: Optional[int] = None\n+\n+    threshold: Optional[float] = None\n+\n+    type: Optional[str] = None\n+    \"\"\"Type of turn detection, only `server_vad` is currently supported.\"\"\"\n+\n+\n+class SessionAudioInput(BaseModel):\n+    format: Optional[str] = None\n+    \"\"\"The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    noise_reduction: Optional[SessionAudioInputNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\"\"\"\n+\n+    transcription: Optional[SessionAudioInputTranscription] = None\n+    \"\"\"Configuration of the transcription model.\"\"\"\n+\n+    turn_detection: Optional[SessionAudioInputTurnDetection] = None\n+    \"\"\"Configuration for turn detection.\"\"\"\n+\n+\n+class SessionAudio(BaseModel):\n+    input: Optional[SessionAudioInput] = None\n+\n+\n+class Session(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"Unique identifier for the session that looks like `sess_1234567890abcdef`.\"\"\"\n+\n+    audio: Optional[SessionAudio] = None\n+    \"\"\"Configuration for input audio for the session.\"\"\"\n+\n+    expires_at: Optional[int] = None\n+    \"\"\"Expiration timestamp for the session, in seconds since epoch.\"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    object: Optional[str] = None\n+    \"\"\"The object type. Always `realtime.transcription_session`.\"\"\"\n+\n+\n+class TranscriptionSessionCreated(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    session: Session\n+    \"\"\"A Realtime transcription session configuration object.\"\"\"\n+\n+    type: Literal[\"transcription_session.created\"]\n+    \"\"\"The event type, must be `transcription_session.created`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/transcription_session_update.py",
            "diff": "diff --git a/src/openai/types/realtime/transcription_session_update.py b/src/openai/types/realtime/transcription_session_update.py\nnew file mode 100644\nindex 0000000..c8f5b9e\n--- /dev/null\n+++ b/src/openai/types/realtime/transcription_session_update.py\n@@ -0,0 +1,20 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from .realtime_transcription_session_create_request import RealtimeTranscriptionSessionCreateRequest\n+\n+__all__ = [\"TranscriptionSessionUpdate\"]\n+\n+\n+class TranscriptionSessionUpdate(BaseModel):\n+    session: RealtimeTranscriptionSessionCreateRequest\n+    \"\"\"Realtime transcription session object configuration.\"\"\"\n+\n+    type: Literal[\"transcription_session.update\"]\n+    \"\"\"The event type, must be `transcription_session.update`.\"\"\"\n+\n+    event_id: Optional[str] = None\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/transcription_session_update_param.py",
            "diff": "diff --git a/src/openai/types/realtime/transcription_session_update_param.py b/src/openai/types/realtime/transcription_session_update_param.py\nnew file mode 100644\nindex 0000000..f2e66ef\n--- /dev/null\n+++ b/src/openai/types/realtime/transcription_session_update_param.py\n@@ -0,0 +1,20 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from .realtime_transcription_session_create_request_param import RealtimeTranscriptionSessionCreateRequestParam\n+\n+__all__ = [\"TranscriptionSessionUpdateParam\"]\n+\n+\n+class TranscriptionSessionUpdateParam(TypedDict, total=False):\n+    session: Required[RealtimeTranscriptionSessionCreateRequestParam]\n+    \"\"\"Realtime transcription session object configuration.\"\"\"\n+\n+    type: Required[Literal[\"transcription_session.update\"]]\n+    \"\"\"The event type, must be `transcription_session.update`.\"\"\"\n+\n+    event_id: str\n+    \"\"\"Optional client-generated ID used to identify this event.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/realtime/transcription_session_updated_event.py",
            "diff": "diff --git a/src/openai/types/realtime/transcription_session_updated_event.py b/src/openai/types/realtime/transcription_session_updated_event.py\nnew file mode 100644\nindex 0000000..9abd1d2\n--- /dev/null\n+++ b/src/openai/types/realtime/transcription_session_updated_event.py\n@@ -0,0 +1,105 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\n+    \"TranscriptionSessionUpdatedEvent\",\n+    \"Session\",\n+    \"SessionAudio\",\n+    \"SessionAudioInput\",\n+    \"SessionAudioInputNoiseReduction\",\n+    \"SessionAudioInputTranscription\",\n+    \"SessionAudioInputTurnDetection\",\n+]\n+\n+\n+class SessionAudioInputNoiseReduction(BaseModel):\n+    type: Optional[Literal[\"near_field\", \"far_field\"]] = None\n+\n+\n+class SessionAudioInputTranscription(BaseModel):\n+    language: Optional[str] = None\n+    \"\"\"The language of the input audio.\n+\n+    Supplying the input language in\n+    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n+    format will improve accuracy and latency.\n+    \"\"\"\n+\n+    model: Optional[Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"]] = None\n+    \"\"\"The model to use for transcription.\n+\n+    Can be `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, or `whisper-1`.\n+    \"\"\"\n+\n+    prompt: Optional[str] = None\n+    \"\"\"An optional text to guide the model's style or continue a previous audio\n+    segment.\n+\n+    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n+    should match the audio language.\n+    \"\"\"\n+\n+\n+class SessionAudioInputTurnDetection(BaseModel):\n+    prefix_padding_ms: Optional[int] = None\n+\n+    silence_duration_ms: Optional[int] = None\n+\n+    threshold: Optional[float] = None\n+\n+    type: Optional[str] = None\n+    \"\"\"Type of turn detection, only `server_vad` is currently supported.\"\"\"\n+\n+\n+class SessionAudioInput(BaseModel):\n+    format: Optional[str] = None\n+    \"\"\"The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\"\"\"\n+\n+    noise_reduction: Optional[SessionAudioInputNoiseReduction] = None\n+    \"\"\"Configuration for input audio noise reduction.\"\"\"\n+\n+    transcription: Optional[SessionAudioInputTranscription] = None\n+    \"\"\"Configuration of the transcription model.\"\"\"\n+\n+    turn_detection: Optional[SessionAudioInputTurnDetection] = None\n+    \"\"\"Configuration for turn detection.\"\"\"\n+\n+\n+class SessionAudio(BaseModel):\n+    input: Optional[SessionAudioInput] = None\n+\n+\n+class Session(BaseModel):\n+    id: Optional[str] = None\n+    \"\"\"Unique identifier for the session that looks like `sess_1234567890abcdef`.\"\"\"\n+\n+    audio: Optional[SessionAudio] = None\n+    \"\"\"Configuration for input audio for the session.\"\"\"\n+\n+    expires_at: Optional[int] = None\n+    \"\"\"Expiration timestamp for the session, in seconds since epoch.\"\"\"\n+\n+    include: Optional[List[Literal[\"item.input_audio_transcription.logprobs\"]]] = None\n+    \"\"\"Additional fields to include in server outputs.\n+\n+    - `item.input_audio_transcription.logprobs`: Include logprobs for input audio\n+      transcription.\n+    \"\"\"\n+\n+    object: Optional[str] = None\n+    \"\"\"The object type. Always `realtime.transcription_session`.\"\"\"\n+\n+\n+class TranscriptionSessionUpdatedEvent(BaseModel):\n+    event_id: str\n+    \"\"\"The unique ID of the server event.\"\"\"\n+\n+    session: Session\n+    \"\"\"A Realtime transcription session configuration object.\"\"\"\n+\n+    type: Literal[\"transcription_session.updated\"]\n+    \"\"\"The event type, must be `transcription_session.updated`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/__init__.py",
            "diff": "diff --git a/src/openai/types/responses/__init__.py b/src/openai/types/responses/__init__.py\nindex 7c574ed..8047f3c 100644\n--- a/src/openai/types/responses/__init__.py\n+++ b/src/openai/types/responses/__init__.py\n@@ -59,6 +59,7 @@ from .response_output_message import ResponseOutputMessage as ResponseOutputMess\n from .response_output_refusal import ResponseOutputRefusal as ResponseOutputRefusal\n from .response_reasoning_item import ResponseReasoningItem as ResponseReasoningItem\n from .tool_choice_types_param import ToolChoiceTypesParam as ToolChoiceTypesParam\n+from .web_search_preview_tool import WebSearchPreviewTool as WebSearchPreviewTool\n from .easy_input_message_param import EasyInputMessageParam as EasyInputMessageParam\n from .response_completed_event import ResponseCompletedEvent as ResponseCompletedEvent\n from .response_retrieve_params import ResponseRetrieveParams as ResponseRetrieveParams\n@@ -90,6 +91,7 @@ from .response_refusal_delta_event import ResponseRefusalDeltaEvent as ResponseR\n from .response_output_message_param import ResponseOutputMessageParam as ResponseOutputMessageParam\n from .response_output_refusal_param import ResponseOutputRefusalParam as ResponseOutputRefusalParam\n from .response_reasoning_item_param import ResponseReasoningItemParam as ResponseReasoningItemParam\n+from .web_search_preview_tool_param import WebSearchPreviewToolParam as WebSearchPreviewToolParam\n from .response_file_search_tool_call import ResponseFileSearchToolCall as ResponseFileSearchToolCall\n from .response_mcp_call_failed_event import ResponseMcpCallFailedEvent as ResponseMcpCallFailedEvent\n from .response_custom_tool_call_param import ResponseCustomToolCallParam as ResponseCustomToolCallParam\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/response.py",
            "diff": "diff --git a/src/openai/types/responses/response.py b/src/openai/types/responses/response.py\nindex ce9effd..9f6fd3e 100644\n--- a/src/openai/types/responses/response.py\n+++ b/src/openai/types/responses/response.py\n@@ -116,7 +116,7 @@ class Response(BaseModel):\n \n     You can specify which tool to use by setting the `tool_choice` parameter.\n \n-    The two categories of tools you can provide the model are:\n+    We support the following categories of tools:\n \n     - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       capabilities, like\n@@ -124,6 +124,9 @@ class Response(BaseModel):\n       [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       Learn more about\n       [built-in tools](https://platform.openai.com/docs/guides/tools).\n+    - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+      predefined connectors such as Google Drive and Notion. Learn more about\n+      [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n     - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       the model to call your own code with strongly typed arguments and outputs.\n       Learn more about\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/response_create_params.py",
            "diff": "diff --git a/src/openai/types/responses/response_create_params.py b/src/openai/types/responses/response_create_params.py\nindex ff28c05..eac2494 100644\n--- a/src/openai/types/responses/response_create_params.py\n+++ b/src/openai/types/responses/response_create_params.py\n@@ -216,7 +216,7 @@ class ResponseCreateParamsBase(TypedDict, total=False):\n \n     You can specify which tool to use by setting the `tool_choice` parameter.\n \n-    The two categories of tools you can provide the model are:\n+    We support the following categories of tools:\n \n     - **Built-in tools**: Tools that are provided by OpenAI that extend the model's\n       capabilities, like\n@@ -224,6 +224,9 @@ class ResponseCreateParamsBase(TypedDict, total=False):\n       [file search](https://platform.openai.com/docs/guides/tools-file-search).\n       Learn more about\n       [built-in tools](https://platform.openai.com/docs/guides/tools).\n+    - **MCP Tools**: Integrations with third-party systems via custom MCP servers or\n+      predefined connectors such as Google Drive and Notion. Learn more about\n+      [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).\n     - **Function calls (custom tools)**: Functions that are defined by you, enabling\n       the model to call your own code with strongly typed arguments and outputs.\n       Learn more about\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/tool.py",
            "diff": "diff --git a/src/openai/types/responses/tool.py b/src/openai/types/responses/tool.py\nindex 0fe7133..594e09d 100644\n--- a/src/openai/types/responses/tool.py\n+++ b/src/openai/types/responses/tool.py\n@@ -3,19 +3,18 @@\n from typing import Dict, List, Union, Optional\n from typing_extensions import Literal, Annotated, TypeAlias\n \n-from . import web_search_tool\n from ..._utils import PropertyInfo\n from ..._models import BaseModel\n from .custom_tool import CustomTool\n from .computer_tool import ComputerTool\n from .function_tool import FunctionTool\n+from .web_search_tool import WebSearchTool\n from .file_search_tool import FileSearchTool\n+from .web_search_preview_tool import WebSearchPreviewTool\n \n __all__ = [\n     \"Tool\",\n     \"WebSearchTool\",\n-    \"WebSearchToolFilters\",\n-    \"WebSearchToolUserLocation\",\n     \"Mcp\",\n     \"McpAllowedTools\",\n     \"McpAllowedToolsMcpToolFilter\",\n@@ -32,61 +31,6 @@ __all__ = [\n ]\n \n \n-class WebSearchToolFilters(BaseModel):\n-    allowed_domains: Optional[List[str]] = None\n-    \"\"\"Allowed domains for the search.\n-\n-    If not provided, all domains are allowed. Subdomains of the provided domains are\n-    allowed as well.\n-\n-    Example: `[\"pubmed.ncbi.nlm.nih.gov\"]`\n-    \"\"\"\n-\n-\n-class WebSearchToolUserLocation(BaseModel):\n-    city: Optional[str] = None\n-    \"\"\"Free text input for the city of the user, e.g. `San Francisco`.\"\"\"\n-\n-    country: Optional[str] = None\n-    \"\"\"\n-    The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n-    the user, e.g. `US`.\n-    \"\"\"\n-\n-    region: Optional[str] = None\n-    \"\"\"Free text input for the region of the user, e.g. `California`.\"\"\"\n-\n-    timezone: Optional[str] = None\n-    \"\"\"\n-    The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n-    user, e.g. `America/Los_Angeles`.\n-    \"\"\"\n-\n-    type: Optional[Literal[\"approximate\"]] = None\n-    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n-\n-\n-class WebSearchTool(BaseModel):\n-    type: Literal[\"web_search\", \"web_search_2025_08_26\"]\n-    \"\"\"The type of the web search tool.\n-\n-    One of `web_search` or `web_search_2025_08_26`.\n-    \"\"\"\n-\n-    filters: Optional[WebSearchToolFilters] = None\n-    \"\"\"Filters for the search.\"\"\"\n-\n-    search_context_size: Optional[Literal[\"low\", \"medium\", \"high\"]] = None\n-    \"\"\"High level guidance for the amount of context window space to use for the\n-    search.\n-\n-    One of `low`, `medium`, or `high`. `medium` is the default.\n-    \"\"\"\n-\n-    user_location: Optional[WebSearchToolUserLocation] = None\n-    \"\"\"The approximate location of the user.\"\"\"\n-\n-\n class McpAllowedToolsMcpToolFilter(BaseModel):\n     read_only: Optional[bool] = None\n     \"\"\"Indicates whether or not a tool modifies data or is read-only.\n@@ -310,7 +254,7 @@ Tool: TypeAlias = Annotated[\n         ImageGeneration,\n         LocalShell,\n         CustomTool,\n-        web_search_tool.WebSearchTool,\n+        WebSearchPreviewTool,\n     ],\n     PropertyInfo(discriminator=\"type\"),\n ]\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/tool_param.py",
            "diff": "diff --git a/src/openai/types/responses/tool_param.py b/src/openai/types/responses/tool_param.py\nindex aff9359..def1f08 100644\n--- a/src/openai/types/responses/tool_param.py\n+++ b/src/openai/types/responses/tool_param.py\n@@ -11,12 +11,10 @@ from .computer_tool_param import ComputerToolParam\n from .function_tool_param import FunctionToolParam\n from .web_search_tool_param import WebSearchToolParam\n from .file_search_tool_param import FileSearchToolParam\n+from .web_search_preview_tool_param import WebSearchPreviewToolParam\n \n __all__ = [\n     \"ToolParam\",\n-    \"WebSearchTool\",\n-    \"WebSearchToolFilters\",\n-    \"WebSearchToolUserLocation\",\n     \"Mcp\",\n     \"McpAllowedTools\",\n     \"McpAllowedToolsMcpToolFilter\",\n@@ -33,61 +31,6 @@ __all__ = [\n ]\n \n \n-class WebSearchToolFilters(TypedDict, total=False):\n-    allowed_domains: Optional[List[str]]\n-    \"\"\"Allowed domains for the search.\n-\n-    If not provided, all domains are allowed. Subdomains of the provided domains are\n-    allowed as well.\n-\n-    Example: `[\"pubmed.ncbi.nlm.nih.gov\"]`\n-    \"\"\"\n-\n-\n-class WebSearchToolUserLocation(TypedDict, total=False):\n-    city: Optional[str]\n-    \"\"\"Free text input for the city of the user, e.g. `San Francisco`.\"\"\"\n-\n-    country: Optional[str]\n-    \"\"\"\n-    The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n-    the user, e.g. `US`.\n-    \"\"\"\n-\n-    region: Optional[str]\n-    \"\"\"Free text input for the region of the user, e.g. `California`.\"\"\"\n-\n-    timezone: Optional[str]\n-    \"\"\"\n-    The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n-    user, e.g. `America/Los_Angeles`.\n-    \"\"\"\n-\n-    type: Literal[\"approximate\"]\n-    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n-\n-\n-class WebSearchTool(TypedDict, total=False):\n-    type: Required[Literal[\"web_search\", \"web_search_2025_08_26\"]]\n-    \"\"\"The type of the web search tool.\n-\n-    One of `web_search` or `web_search_2025_08_26`.\n-    \"\"\"\n-\n-    filters: Optional[WebSearchToolFilters]\n-    \"\"\"Filters for the search.\"\"\"\n-\n-    search_context_size: Literal[\"low\", \"medium\", \"high\"]\n-    \"\"\"High level guidance for the amount of context window space to use for the\n-    search.\n-\n-    One of `low`, `medium`, or `high`. `medium` is the default.\n-    \"\"\"\n-\n-    user_location: Optional[WebSearchToolUserLocation]\n-    \"\"\"The approximate location of the user.\"\"\"\n-\n-\n class McpAllowedToolsMcpToolFilter(TypedDict, total=False):\n     read_only: bool\n     \"\"\"Indicates whether or not a tool modifies data or is read-only.\n@@ -302,13 +245,13 @@ ToolParam: TypeAlias = Union[\n     FunctionToolParam,\n     FileSearchToolParam,\n     ComputerToolParam,\n-    WebSearchTool,\n+    WebSearchToolParam,\n     Mcp,\n     CodeInterpreter,\n     ImageGeneration,\n     LocalShell,\n     CustomToolParam,\n-    WebSearchToolParam,\n+    WebSearchPreviewToolParam,\n ]\n \n \n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/web_search_preview_tool.py",
            "diff": "diff --git a/src/openai/types/responses/web_search_preview_tool.py b/src/openai/types/responses/web_search_preview_tool.py\nnew file mode 100644\nindex 0000000..66d6a24\n--- /dev/null\n+++ b/src/openai/types/responses/web_search_preview_tool.py\n@@ -0,0 +1,49 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"WebSearchPreviewTool\", \"UserLocation\"]\n+\n+\n+class UserLocation(BaseModel):\n+    type: Literal[\"approximate\"]\n+    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n+\n+    city: Optional[str] = None\n+    \"\"\"Free text input for the city of the user, e.g. `San Francisco`.\"\"\"\n+\n+    country: Optional[str] = None\n+    \"\"\"\n+    The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n+    the user, e.g. `US`.\n+    \"\"\"\n+\n+    region: Optional[str] = None\n+    \"\"\"Free text input for the region of the user, e.g. `California`.\"\"\"\n+\n+    timezone: Optional[str] = None\n+    \"\"\"\n+    The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n+    user, e.g. `America/Los_Angeles`.\n+    \"\"\"\n+\n+\n+class WebSearchPreviewTool(BaseModel):\n+    type: Literal[\"web_search_preview\", \"web_search_preview_2025_03_11\"]\n+    \"\"\"The type of the web search tool.\n+\n+    One of `web_search_preview` or `web_search_preview_2025_03_11`.\n+    \"\"\"\n+\n+    search_context_size: Optional[Literal[\"low\", \"medium\", \"high\"]] = None\n+    \"\"\"High level guidance for the amount of context window space to use for the\n+    search.\n+\n+    One of `low`, `medium`, or `high`. `medium` is the default.\n+    \"\"\"\n+\n+    user_location: Optional[UserLocation] = None\n+    \"\"\"The user's location.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/web_search_preview_tool_param.py",
            "diff": "diff --git a/src/openai/types/responses/web_search_preview_tool_param.py b/src/openai/types/responses/web_search_preview_tool_param.py\nnew file mode 100644\nindex 0000000..ec2173f\n--- /dev/null\n+++ b/src/openai/types/responses/web_search_preview_tool_param.py\n@@ -0,0 +1,49 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Optional\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"WebSearchPreviewToolParam\", \"UserLocation\"]\n+\n+\n+class UserLocation(TypedDict, total=False):\n+    type: Required[Literal[\"approximate\"]]\n+    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n+\n+    city: Optional[str]\n+    \"\"\"Free text input for the city of the user, e.g. `San Francisco`.\"\"\"\n+\n+    country: Optional[str]\n+    \"\"\"\n+    The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of\n+    the user, e.g. `US`.\n+    \"\"\"\n+\n+    region: Optional[str]\n+    \"\"\"Free text input for the region of the user, e.g. `California`.\"\"\"\n+\n+    timezone: Optional[str]\n+    \"\"\"\n+    The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the\n+    user, e.g. `America/Los_Angeles`.\n+    \"\"\"\n+\n+\n+class WebSearchPreviewToolParam(TypedDict, total=False):\n+    type: Required[Literal[\"web_search_preview\", \"web_search_preview_2025_03_11\"]]\n+    \"\"\"The type of the web search tool.\n+\n+    One of `web_search_preview` or `web_search_preview_2025_03_11`.\n+    \"\"\"\n+\n+    search_context_size: Literal[\"low\", \"medium\", \"high\"]\n+    \"\"\"High level guidance for the amount of context window space to use for the\n+    search.\n+\n+    One of `low`, `medium`, or `high`. `medium` is the default.\n+    \"\"\"\n+\n+    user_location: Optional[UserLocation]\n+    \"\"\"The user's location.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/web_search_tool.py",
            "diff": "diff --git a/src/openai/types/responses/web_search_tool.py b/src/openai/types/responses/web_search_tool.py\nindex a6bf951..bde9600 100644\n--- a/src/openai/types/responses/web_search_tool.py\n+++ b/src/openai/types/responses/web_search_tool.py\n@@ -1,17 +1,25 @@\n # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n \n-from typing import Optional\n+from typing import List, Optional\n from typing_extensions import Literal\n \n from ..._models import BaseModel\n \n-__all__ = [\"WebSearchTool\", \"UserLocation\"]\n+__all__ = [\"WebSearchTool\", \"Filters\", \"UserLocation\"]\n \n \n-class UserLocation(BaseModel):\n-    type: Literal[\"approximate\"]\n-    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n+class Filters(BaseModel):\n+    allowed_domains: Optional[List[str]] = None\n+    \"\"\"Allowed domains for the search.\n+\n+    If not provided, all domains are allowed. Subdomains of the provided domains are\n+    allowed as well.\n+\n+    Example: `[\"pubmed.ncbi.nlm.nih.gov\"]`\n+    \"\"\"\n \n+\n+class UserLocation(BaseModel):\n     city: Optional[str] = None\n     \"\"\"Free text input for the city of the user, e.g. `San Francisco`.\"\"\"\n \n@@ -30,14 +38,20 @@ class UserLocation(BaseModel):\n     user, e.g. `America/Los_Angeles`.\n     \"\"\"\n \n+    type: Optional[Literal[\"approximate\"]] = None\n+    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n+\n \n class WebSearchTool(BaseModel):\n-    type: Literal[\"web_search_preview\", \"web_search_preview_2025_03_11\"]\n+    type: Literal[\"web_search\", \"web_search_2025_08_26\"]\n     \"\"\"The type of the web search tool.\n \n-    One of `web_search_preview` or `web_search_preview_2025_03_11`.\n+    One of `web_search` or `web_search_2025_08_26`.\n     \"\"\"\n \n+    filters: Optional[Filters] = None\n+    \"\"\"Filters for the search.\"\"\"\n+\n     search_context_size: Optional[Literal[\"low\", \"medium\", \"high\"]] = None\n     \"\"\"High level guidance for the amount of context window space to use for the\n     search.\n@@ -46,4 +60,4 @@ class WebSearchTool(BaseModel):\n     \"\"\"\n \n     user_location: Optional[UserLocation] = None\n-    \"\"\"The user's location.\"\"\"\n+    \"\"\"The approximate location of the user.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/responses/web_search_tool_param.py",
            "diff": "diff --git a/src/openai/types/responses/web_search_tool_param.py b/src/openai/types/responses/web_search_tool_param.py\nindex d0335c0..17a3824 100644\n--- a/src/openai/types/responses/web_search_tool_param.py\n+++ b/src/openai/types/responses/web_search_tool_param.py\n@@ -2,16 +2,24 @@\n \n from __future__ import annotations\n \n-from typing import Optional\n+from typing import List, Optional\n from typing_extensions import Literal, Required, TypedDict\n \n-__all__ = [\"WebSearchToolParam\", \"UserLocation\"]\n+__all__ = [\"WebSearchToolParam\", \"Filters\", \"UserLocation\"]\n \n \n-class UserLocation(TypedDict, total=False):\n-    type: Required[Literal[\"approximate\"]]\n-    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n+class Filters(TypedDict, total=False):\n+    allowed_domains: Optional[List[str]]\n+    \"\"\"Allowed domains for the search.\n+\n+    If not provided, all domains are allowed. Subdomains of the provided domains are\n+    allowed as well.\n+\n+    Example: `[\"pubmed.ncbi.nlm.nih.gov\"]`\n+    \"\"\"\n \n+\n+class UserLocation(TypedDict, total=False):\n     city: Optional[str]\n     \"\"\"Free text input for the city of the user, e.g. `San Francisco`.\"\"\"\n \n@@ -30,14 +38,20 @@ class UserLocation(TypedDict, total=False):\n     user, e.g. `America/Los_Angeles`.\n     \"\"\"\n \n+    type: Literal[\"approximate\"]\n+    \"\"\"The type of location approximation. Always `approximate`.\"\"\"\n+\n \n class WebSearchToolParam(TypedDict, total=False):\n-    type: Required[Literal[\"web_search_preview\", \"web_search_preview_2025_03_11\"]]\n+    type: Required[Literal[\"web_search\", \"web_search_2025_08_26\"]]\n     \"\"\"The type of the web search tool.\n \n-    One of `web_search_preview` or `web_search_preview_2025_03_11`.\n+    One of `web_search` or `web_search_2025_08_26`.\n     \"\"\"\n \n+    filters: Optional[Filters]\n+    \"\"\"Filters for the search.\"\"\"\n+\n     search_context_size: Literal[\"low\", \"medium\", \"high\"]\n     \"\"\"High level guidance for the amount of context window space to use for the\n     search.\n@@ -46,4 +60,4 @@ class WebSearchToolParam(TypedDict, total=False):\n     \"\"\"\n \n     user_location: Optional[UserLocation]\n-    \"\"\"The user's location.\"\"\"\n+    \"\"\"The approximate location of the user.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/webhooks/__init__.py",
            "diff": "diff --git a/src/openai/types/webhooks/__init__.py b/src/openai/types/webhooks/__init__.py\nindex 9caad38..8b9e556 100644\n--- a/src/openai/types/webhooks/__init__.py\n+++ b/src/openai/types/webhooks/__init__.py\n@@ -15,6 +15,7 @@ from .response_cancelled_webhook_event import ResponseCancelledWebhookEvent as R\n from .response_completed_webhook_event import ResponseCompletedWebhookEvent as ResponseCompletedWebhookEvent\n from .response_incomplete_webhook_event import ResponseIncompleteWebhookEvent as ResponseIncompleteWebhookEvent\n from .fine_tuning_job_failed_webhook_event import FineTuningJobFailedWebhookEvent as FineTuningJobFailedWebhookEvent\n+from .realtime_call_incoming_webhook_event import RealtimeCallIncomingWebhookEvent as RealtimeCallIncomingWebhookEvent\n from .fine_tuning_job_cancelled_webhook_event import (\n     FineTuningJobCancelledWebhookEvent as FineTuningJobCancelledWebhookEvent,\n )\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/webhooks/realtime_call_incoming_webhook_event.py",
            "diff": "diff --git a/src/openai/types/webhooks/realtime_call_incoming_webhook_event.py b/src/openai/types/webhooks/realtime_call_incoming_webhook_event.py\nnew file mode 100644\nindex 0000000..a166a34\n--- /dev/null\n+++ b/src/openai/types/webhooks/realtime_call_incoming_webhook_event.py\n@@ -0,0 +1,41 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import List, Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"RealtimeCallIncomingWebhookEvent\", \"Data\", \"DataSipHeader\"]\n+\n+\n+class DataSipHeader(BaseModel):\n+    name: str\n+    \"\"\"Name of the SIP Header.\"\"\"\n+\n+    value: str\n+    \"\"\"Value of the SIP Header.\"\"\"\n+\n+\n+class Data(BaseModel):\n+    call_id: str\n+    \"\"\"The unique ID of this call.\"\"\"\n+\n+    sip_headers: List[DataSipHeader]\n+    \"\"\"Headers from the SIP Invite.\"\"\"\n+\n+\n+class RealtimeCallIncomingWebhookEvent(BaseModel):\n+    id: str\n+    \"\"\"The unique ID of the event.\"\"\"\n+\n+    created_at: int\n+    \"\"\"The Unix timestamp (in seconds) of when the model response was completed.\"\"\"\n+\n+    data: Data\n+    \"\"\"Event data payload.\"\"\"\n+\n+    type: Literal[\"realtime.call.incoming\"]\n+    \"\"\"The type of the event. Always `realtime.call.incoming`.\"\"\"\n+\n+    object: Optional[Literal[\"event\"]] = None\n+    \"\"\"The object of the event. Always `event`.\"\"\"\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "src/openai/types/webhooks/unwrap_webhook_event.py",
            "diff": "diff --git a/src/openai/types/webhooks/unwrap_webhook_event.py b/src/openai/types/webhooks/unwrap_webhook_event.py\nindex 91091af..952383c 100644\n--- a/src/openai/types/webhooks/unwrap_webhook_event.py\n+++ b/src/openai/types/webhooks/unwrap_webhook_event.py\n@@ -16,6 +16,7 @@ from .response_cancelled_webhook_event import ResponseCancelledWebhookEvent\n from .response_completed_webhook_event import ResponseCompletedWebhookEvent\n from .response_incomplete_webhook_event import ResponseIncompleteWebhookEvent\n from .fine_tuning_job_failed_webhook_event import FineTuningJobFailedWebhookEvent\n+from .realtime_call_incoming_webhook_event import RealtimeCallIncomingWebhookEvent\n from .fine_tuning_job_cancelled_webhook_event import FineTuningJobCancelledWebhookEvent\n from .fine_tuning_job_succeeded_webhook_event import FineTuningJobSucceededWebhookEvent\n \n@@ -33,6 +34,7 @@ UnwrapWebhookEvent: TypeAlias = Annotated[\n         FineTuningJobCancelledWebhookEvent,\n         FineTuningJobFailedWebhookEvent,\n         FineTuningJobSucceededWebhookEvent,\n+        RealtimeCallIncomingWebhookEvent,\n         ResponseCancelledWebhookEvent,\n         ResponseCompletedWebhookEvent,\n         ResponseFailedWebhookEvent,\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "tests/api_resources/beta/realtime/test_sessions.py",
            "diff": "diff --git a/tests/api_resources/beta/realtime/test_sessions.py b/tests/api_resources/beta/realtime/test_sessions.py\ndeleted file mode 100644\nindex 3c55abf..0000000\n--- a/tests/api_resources/beta/realtime/test_sessions.py\n+++ /dev/null\n@@ -1,166 +0,0 @@\n-# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n-\n-from __future__ import annotations\n-\n-import os\n-from typing import Any, cast\n-\n-import pytest\n-\n-from openai import OpenAI, AsyncOpenAI\n-from tests.utils import assert_matches_type\n-from openai.types.beta.realtime import SessionCreateResponse\n-\n-base_url = os.environ.get(\"TEST_API_BASE_URL\", \"http://127.0.0.1:4010\")\n-\n-\n-class TestSessions:\n-    parametrize = pytest.mark.parametrize(\"client\", [False, True], indirect=True, ids=[\"loose\", \"strict\"])\n-\n-    @parametrize\n-    def test_method_create(self, client: OpenAI) -> None:\n-        session = client.beta.realtime.sessions.create()\n-        assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-    @parametrize\n-    def test_method_create_with_all_params(self, client: OpenAI) -> None:\n-        session = client.beta.realtime.sessions.create(\n-            client_secret={\n-                \"expires_after\": {\n-                    \"anchor\": \"created_at\",\n-                    \"seconds\": 0,\n-                }\n-            },\n-            input_audio_format=\"pcm16\",\n-            input_audio_noise_reduction={\"type\": \"near_field\"},\n-            input_audio_transcription={\n-                \"language\": \"language\",\n-                \"model\": \"model\",\n-                \"prompt\": \"prompt\",\n-            },\n-            instructions=\"instructions\",\n-            max_response_output_tokens=0,\n-            modalities=[\"text\"],\n-            model=\"gpt-4o-realtime-preview\",\n-            output_audio_format=\"pcm16\",\n-            speed=0.25,\n-            temperature=0,\n-            tool_choice=\"tool_choice\",\n-            tools=[\n-                {\n-                    \"description\": \"description\",\n-                    \"name\": \"name\",\n-                    \"parameters\": {},\n-                    \"type\": \"function\",\n-                }\n-            ],\n-            tracing=\"auto\",\n-            turn_detection={\n-                \"create_response\": True,\n-                \"eagerness\": \"low\",\n-                \"interrupt_response\": True,\n-                \"prefix_padding_ms\": 0,\n-                \"silence_duration_ms\": 0,\n-                \"threshold\": 0,\n-                \"type\": \"server_vad\",\n-            },\n-            voice=\"ash\",\n-        )\n-        assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-    @parametrize\n-    def test_raw_response_create(self, client: OpenAI) -> None:\n-        response = client.beta.realtime.sessions.with_raw_response.create()\n-\n-        assert response.is_closed is True\n-        assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-        session = response.parse()\n-        assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-    @parametrize\n-    def test_streaming_response_create(self, client: OpenAI) -> None:\n-        with client.beta.realtime.sessions.with_streaming_response.create() as response:\n-            assert not response.is_closed\n-            assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-\n-            session = response.parse()\n-            assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-        assert cast(Any, response.is_closed) is True\n-\n-\n-class TestAsyncSessions:\n-    parametrize = pytest.mark.parametrize(\n-        \"async_client\", [False, True, {\"http_client\": \"aiohttp\"}], indirect=True, ids=[\"loose\", \"strict\", \"aiohttp\"]\n-    )\n-\n-    @parametrize\n-    async def test_method_create(self, async_client: AsyncOpenAI) -> None:\n-        session = await async_client.beta.realtime.sessions.create()\n-        assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-    @parametrize\n-    async def test_method_create_with_all_params(self, async_client: AsyncOpenAI) -> None:\n-        session = await async_client.beta.realtime.sessions.create(\n-            client_secret={\n-                \"expires_after\": {\n-                    \"anchor\": \"created_at\",\n-                    \"seconds\": 0,\n-                }\n-            },\n-            input_audio_format=\"pcm16\",\n-            input_audio_noise_reduction={\"type\": \"near_field\"},\n-            input_audio_transcription={\n-                \"language\": \"language\",\n-                \"model\": \"model\",\n-                \"prompt\": \"prompt\",\n-            },\n-            instructions=\"instructions\",\n-            max_response_output_tokens=0,\n-            modalities=[\"text\"],\n-            model=\"gpt-4o-realtime-preview\",\n-            output_audio_format=\"pcm16\",\n-            speed=0.25,\n-            temperature=0,\n-            tool_choice=\"tool_choice\",\n-            tools=[\n-                {\n-                    \"description\": \"description\",\n-                    \"name\": \"name\",\n-                    \"parameters\": {},\n-                    \"type\": \"function\",\n-                }\n-            ],\n-            tracing=\"auto\",\n-            turn_detection={\n-                \"create_response\": True,\n-                \"eagerness\": \"low\",\n-                \"interrupt_response\": True,\n-                \"prefix_padding_ms\": 0,\n-                \"silence_duration_ms\": 0,\n-                \"threshold\": 0,\n-                \"type\": \"server_vad\",\n-            },\n-            voice=\"ash\",\n-        )\n-        assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-    @parametrize\n-    async def test_raw_response_create(self, async_client: AsyncOpenAI) -> None:\n-        response = await async_client.beta.realtime.sessions.with_raw_response.create()\n-\n-        assert response.is_closed is True\n-        assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-        session = response.parse()\n-        assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-    @parametrize\n-    async def test_streaming_response_create(self, async_client: AsyncOpenAI) -> None:\n-        async with async_client.beta.realtime.sessions.with_streaming_response.create() as response:\n-            assert not response.is_closed\n-            assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-\n-            session = await response.parse()\n-            assert_matches_type(SessionCreateResponse, session, path=[\"response\"])\n-\n-        assert cast(Any, response.is_closed) is True\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "tests/api_resources/beta/realtime/test_transcription_sessions.py",
            "diff": "diff --git a/tests/api_resources/beta/realtime/test_transcription_sessions.py b/tests/api_resources/beta/realtime/test_transcription_sessions.py\ndeleted file mode 100644\nindex ac52489..0000000\n--- a/tests/api_resources/beta/realtime/test_transcription_sessions.py\n+++ /dev/null\n@@ -1,134 +0,0 @@\n-# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n-\n-from __future__ import annotations\n-\n-import os\n-from typing import Any, cast\n-\n-import pytest\n-\n-from openai import OpenAI, AsyncOpenAI\n-from tests.utils import assert_matches_type\n-from openai.types.beta.realtime import TranscriptionSession\n-\n-base_url = os.environ.get(\"TEST_API_BASE_URL\", \"http://127.0.0.1:4010\")\n-\n-\n-class TestTranscriptionSessions:\n-    parametrize = pytest.mark.parametrize(\"client\", [False, True], indirect=True, ids=[\"loose\", \"strict\"])\n-\n-    @parametrize\n-    def test_method_create(self, client: OpenAI) -> None:\n-        transcription_session = client.beta.realtime.transcription_sessions.create()\n-        assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-    @parametrize\n-    def test_method_create_with_all_params(self, client: OpenAI) -> None:\n-        transcription_session = client.beta.realtime.transcription_sessions.create(\n-            client_secret={\n-                \"expires_at\": {\n-                    \"anchor\": \"created_at\",\n-                    \"seconds\": 0,\n-                }\n-            },\n-            include=[\"string\"],\n-            input_audio_format=\"pcm16\",\n-            input_audio_noise_reduction={\"type\": \"near_field\"},\n-            input_audio_transcription={\n-                \"language\": \"language\",\n-                \"model\": \"gpt-4o-transcribe\",\n-                \"prompt\": \"prompt\",\n-            },\n-            modalities=[\"text\"],\n-            turn_detection={\n-                \"create_response\": True,\n-                \"eagerness\": \"low\",\n-                \"interrupt_response\": True,\n-                \"prefix_padding_ms\": 0,\n-                \"silence_duration_ms\": 0,\n-                \"threshold\": 0,\n-                \"type\": \"server_vad\",\n-            },\n-        )\n-        assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-    @parametrize\n-    def test_raw_response_create(self, client: OpenAI) -> None:\n-        response = client.beta.realtime.transcription_sessions.with_raw_response.create()\n-\n-        assert response.is_closed is True\n-        assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-        transcription_session = response.parse()\n-        assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-    @parametrize\n-    def test_streaming_response_create(self, client: OpenAI) -> None:\n-        with client.beta.realtime.transcription_sessions.with_streaming_response.create() as response:\n-            assert not response.is_closed\n-            assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-\n-            transcription_session = response.parse()\n-            assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-        assert cast(Any, response.is_closed) is True\n-\n-\n-class TestAsyncTranscriptionSessions:\n-    parametrize = pytest.mark.parametrize(\n-        \"async_client\", [False, True, {\"http_client\": \"aiohttp\"}], indirect=True, ids=[\"loose\", \"strict\", \"aiohttp\"]\n-    )\n-\n-    @parametrize\n-    async def test_method_create(self, async_client: AsyncOpenAI) -> None:\n-        transcription_session = await async_client.beta.realtime.transcription_sessions.create()\n-        assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-    @parametrize\n-    async def test_method_create_with_all_params(self, async_client: AsyncOpenAI) -> None:\n-        transcription_session = await async_client.beta.realtime.transcription_sessions.create(\n-            client_secret={\n-                \"expires_at\": {\n-                    \"anchor\": \"created_at\",\n-                    \"seconds\": 0,\n-                }\n-            },\n-            include=[\"string\"],\n-            input_audio_format=\"pcm16\",\n-            input_audio_noise_reduction={\"type\": \"near_field\"},\n-            input_audio_transcription={\n-                \"language\": \"language\",\n-                \"model\": \"gpt-4o-transcribe\",\n-                \"prompt\": \"prompt\",\n-            },\n-            modalities=[\"text\"],\n-            turn_detection={\n-                \"create_response\": True,\n-                \"eagerness\": \"low\",\n-                \"interrupt_response\": True,\n-                \"prefix_padding_ms\": 0,\n-                \"silence_duration_ms\": 0,\n-                \"threshold\": 0,\n-                \"type\": \"server_vad\",\n-            },\n-        )\n-        assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-    @parametrize\n-    async def test_raw_response_create(self, async_client: AsyncOpenAI) -> None:\n-        response = await async_client.beta.realtime.transcription_sessions.with_raw_response.create()\n-\n-        assert response.is_closed is True\n-        assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-        transcription_session = response.parse()\n-        assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-    @parametrize\n-    async def test_streaming_response_create(self, async_client: AsyncOpenAI) -> None:\n-        async with async_client.beta.realtime.transcription_sessions.with_streaming_response.create() as response:\n-            assert not response.is_closed\n-            assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n-\n-            transcription_session = await response.parse()\n-            assert_matches_type(TranscriptionSession, transcription_session, path=[\"response\"])\n-\n-        assert cast(Any, response.is_closed) is True\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "tests/api_resources/beta/test_realtime.py",
            "diff": "diff --git a/tests/api_resources/beta/test_realtime.py b/tests/api_resources/beta/test_realtime.py\nindex 2b0c7f7..8f752a0 100644\n--- a/tests/api_resources/beta/test_realtime.py\n+++ b/tests/api_resources/beta/test_realtime.py\n@@ -6,6 +6,8 @@ import os\n \n import pytest\n \n+# pyright: reportDeprecated=false\n+\n base_url = os.environ.get(\"TEST_API_BASE_URL\", \"http://127.0.0.1:4010\")\n \n \n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "tests/api_resources/realtime/__init__.py",
            "diff": "diff --git a/tests/api_resources/realtime/__init__.py b/tests/api_resources/realtime/__init__.py\nnew file mode 100644\nindex 0000000..fd8019a\n--- /dev/null\n+++ b/tests/api_resources/realtime/__init__.py\n@@ -0,0 +1 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "tests/api_resources/realtime/test_client_secrets.py",
            "diff": "diff --git a/tests/api_resources/realtime/test_client_secrets.py b/tests/api_resources/realtime/test_client_secrets.py\nnew file mode 100644\nindex 0000000..c477268\n--- /dev/null\n+++ b/tests/api_resources/realtime/test_client_secrets.py\n@@ -0,0 +1,208 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+import os\n+from typing import Any, cast\n+\n+import pytest\n+\n+from openai import OpenAI, AsyncOpenAI\n+from tests.utils import assert_matches_type\n+from openai.types.realtime import ClientSecretCreateResponse\n+\n+base_url = os.environ.get(\"TEST_API_BASE_URL\", \"http://127.0.0.1:4010\")\n+\n+\n+class TestClientSecrets:\n+    parametrize = pytest.mark.parametrize(\"client\", [False, True], indirect=True, ids=[\"loose\", \"strict\"])\n+\n+    @parametrize\n+    def test_method_create(self, client: OpenAI) -> None:\n+        client_secret = client.realtime.client_secrets.create()\n+        assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+    @parametrize\n+    def test_method_create_with_all_params(self, client: OpenAI) -> None:\n+        client_secret = client.realtime.client_secrets.create(\n+            expires_after={\n+                \"anchor\": \"created_at\",\n+                \"seconds\": 10,\n+            },\n+            session={\n+                \"model\": \"string\",\n+                \"type\": \"realtime\",\n+                \"audio\": {\n+                    \"input\": {\n+                        \"format\": \"pcm16\",\n+                        \"noise_reduction\": {\"type\": \"near_field\"},\n+                        \"transcription\": {\n+                            \"language\": \"language\",\n+                            \"model\": \"whisper-1\",\n+                            \"prompt\": \"prompt\",\n+                        },\n+                        \"turn_detection\": {\n+                            \"create_response\": True,\n+                            \"eagerness\": \"low\",\n+                            \"idle_timeout_ms\": 0,\n+                            \"interrupt_response\": True,\n+                            \"prefix_padding_ms\": 0,\n+                            \"silence_duration_ms\": 0,\n+                            \"threshold\": 0,\n+                            \"type\": \"server_vad\",\n+                        },\n+                    },\n+                    \"output\": {\n+                        \"format\": \"pcm16\",\n+                        \"speed\": 0.25,\n+                        \"voice\": \"ash\",\n+                    },\n+                },\n+                \"client_secret\": {\n+                    \"expires_after\": {\n+                        \"anchor\": \"created_at\",\n+                        \"seconds\": 0,\n+                    }\n+                },\n+                \"include\": [\"item.input_audio_transcription.logprobs\"],\n+                \"instructions\": \"instructions\",\n+                \"max_output_tokens\": 0,\n+                \"output_modalities\": [\"text\"],\n+                \"prompt\": {\n+                    \"id\": \"id\",\n+                    \"variables\": {\"foo\": \"string\"},\n+                    \"version\": \"version\",\n+                },\n+                \"temperature\": 0,\n+                \"tool_choice\": \"none\",\n+                \"tools\": [\n+                    {\n+                        \"description\": \"description\",\n+                        \"name\": \"name\",\n+                        \"parameters\": {},\n+                        \"type\": \"function\",\n+                    }\n+                ],\n+                \"tracing\": \"auto\",\n+                \"truncation\": \"auto\",\n+            },\n+        )\n+        assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+    @parametrize\n+    def test_raw_response_create(self, client: OpenAI) -> None:\n+        response = client.realtime.client_secrets.with_raw_response.create()\n+\n+        assert response.is_closed is True\n+        assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n+        client_secret = response.parse()\n+        assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+    @parametrize\n+    def test_streaming_response_create(self, client: OpenAI) -> None:\n+        with client.realtime.client_secrets.with_streaming_response.create() as response:\n+            assert not response.is_closed\n+            assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n+\n+            client_secret = response.parse()\n+            assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+        assert cast(Any, response.is_closed) is True\n+\n+\n+class TestAsyncClientSecrets:\n+    parametrize = pytest.mark.parametrize(\n+        \"async_client\", [False, True, {\"http_client\": \"aiohttp\"}], indirect=True, ids=[\"loose\", \"strict\", \"aiohttp\"]\n+    )\n+\n+    @parametrize\n+    async def test_method_create(self, async_client: AsyncOpenAI) -> None:\n+        client_secret = await async_client.realtime.client_secrets.create()\n+        assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+    @parametrize\n+    async def test_method_create_with_all_params(self, async_client: AsyncOpenAI) -> None:\n+        client_secret = await async_client.realtime.client_secrets.create(\n+            expires_after={\n+                \"anchor\": \"created_at\",\n+                \"seconds\": 10,\n+            },\n+            session={\n+                \"model\": \"string\",\n+                \"type\": \"realtime\",\n+                \"audio\": {\n+                    \"input\": {\n+                        \"format\": \"pcm16\",\n+                        \"noise_reduction\": {\"type\": \"near_field\"},\n+                        \"transcription\": {\n+                            \"language\": \"language\",\n+                            \"model\": \"whisper-1\",\n+                            \"prompt\": \"prompt\",\n+                        },\n+                        \"turn_detection\": {\n+                            \"create_response\": True,\n+                            \"eagerness\": \"low\",\n+                            \"idle_timeout_ms\": 0,\n+                            \"interrupt_response\": True,\n+                            \"prefix_padding_ms\": 0,\n+                            \"silence_duration_ms\": 0,\n+                            \"threshold\": 0,\n+                            \"type\": \"server_vad\",\n+                        },\n+                    },\n+                    \"output\": {\n+                        \"format\": \"pcm16\",\n+                        \"speed\": 0.25,\n+                        \"voice\": \"ash\",\n+                    },\n+                },\n+                \"client_secret\": {\n+                    \"expires_after\": {\n+                        \"anchor\": \"created_at\",\n+                        \"seconds\": 0,\n+                    }\n+                },\n+                \"include\": [\"item.input_audio_transcription.logprobs\"],\n+                \"instructions\": \"instructions\",\n+                \"max_output_tokens\": 0,\n+                \"output_modalities\": [\"text\"],\n+                \"prompt\": {\n+                    \"id\": \"id\",\n+                    \"variables\": {\"foo\": \"string\"},\n+                    \"version\": \"version\",\n+                },\n+                \"temperature\": 0,\n+                \"tool_choice\": \"none\",\n+                \"tools\": [\n+                    {\n+                        \"description\": \"description\",\n+                        \"name\": \"name\",\n+                        \"parameters\": {},\n+                        \"type\": \"function\",\n+                    }\n+                ],\n+                \"tracing\": \"auto\",\n+                \"truncation\": \"auto\",\n+            },\n+        )\n+        assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+    @parametrize\n+    async def test_raw_response_create(self, async_client: AsyncOpenAI) -> None:\n+        response = await async_client.realtime.client_secrets.with_raw_response.create()\n+\n+        assert response.is_closed is True\n+        assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n+        client_secret = response.parse()\n+        assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+    @parametrize\n+    async def test_streaming_response_create(self, async_client: AsyncOpenAI) -> None:\n+        async with async_client.realtime.client_secrets.with_streaming_response.create() as response:\n+            assert not response.is_closed\n+            assert response.http_request.headers.get(\"X-Stainless-Lang\") == \"python\"\n+\n+            client_secret = await response.parse()\n+            assert_matches_type(ClientSecretCreateResponse, client_secret, path=[\"response\"])\n+\n+        assert cast(Any, response.is_closed) is True\n"
        },
        {
            "commit": "b7c2ddc5e5dedda01015b3d0e14ea6eb68c282d3",
            "file_path": "tests/api_resources/test_realtime.py",
            "diff": "diff --git a/tests/api_resources/test_realtime.py b/tests/api_resources/test_realtime.py\nnew file mode 100644\nindex 0000000..2b0c7f7\n--- /dev/null\n+++ b/tests/api_resources/test_realtime.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+import os\n+\n+import pytest\n+\n+base_url = os.environ.get(\"TEST_API_BASE_URL\", \"http://127.0.0.1:4010\")\n+\n+\n+class TestRealtime:\n+    parametrize = pytest.mark.parametrize(\"client\", [False, True], indirect=True, ids=[\"loose\", \"strict\"])\n+\n+\n+class TestAsyncRealtime:\n+    parametrize = pytest.mark.parametrize(\n+        \"async_client\", [False, True, {\"http_client\": \"aiohttp\"}], indirect=True, ids=[\"loose\", \"strict\", \"aiohttp\"]\n+    )\n"
        }
    ]
}