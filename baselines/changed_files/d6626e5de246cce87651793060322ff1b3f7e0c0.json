{
    "sha_fail": "d6626e5de246cce87651793060322ff1b3f7e0c0",
    "changed_files": [
        {
            "commit": "d6626e5de246cce87651793060322ff1b3f7e0c0",
            "file_path": "cookbook/models/openai/chat/basic.py",
            "diff": "diff --git a/cookbook/models/openai/chat/basic.py b/cookbook/models/openai/chat/basic.py\nindex 487b248a9..0d461a16f 100644\n--- a/cookbook/models/openai/chat/basic.py\n+++ b/cookbook/models/openai/chat/basic.py\n@@ -2,7 +2,7 @@ from agno.agent import Agent, RunResponse  # noqa\n from agno.models.openai import OpenAIChat\n \n agent = Agent(\n-    model=OpenAIChat(id=\"gpt-4o\", temperature=0.5), debug_mode=True, markdown=True\n+    model=OpenAIChat(id=\"gpt-4o\", temperature=0.5), markdown=True\n )\n \n # Get the response in a variable\n"
        },
        {
            "commit": "d6626e5de246cce87651793060322ff1b3f7e0c0",
            "file_path": "cookbook/models/openai/chat/reasoning_o3_mini.py",
            "diff": "diff --git a/cookbook/models/openai/chat/reasoning_o3_mini.py b/cookbook/models/openai/chat/reasoning_o3_mini.py\nnew file mode 100644\nindex 000000000..1d09346a7\n--- /dev/null\n+++ b/cookbook/models/openai/chat/reasoning_o3_mini.py\n@@ -0,0 +1,13 @@\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIResponses\n+from agno.tools.yfinance import YFinanceTools\n+\n+agent = Agent(\n+    model=OpenAIResponses(id=\"o3-mini\", reasoning_effort=\"high\"),\n+    tools=[YFinanceTools(enable_all=True)],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Write a report on the NVDA, is it a good buy?\", stream=True)\n"
        },
        {
            "commit": "d6626e5de246cce87651793060322ff1b3f7e0c0",
            "file_path": "cookbook/models/openai/chat/verbosity_control.py",
            "diff": "diff --git a/cookbook/models/openai/chat/verbosity_control.py b/cookbook/models/openai/chat/verbosity_control.py\nnew file mode 100644\nindex 000000000..d9a6333b3\n--- /dev/null\n+++ b/cookbook/models/openai/chat/verbosity_control.py\n@@ -0,0 +1,19 @@\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.yfinance import YFinanceTools\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-5\", verbosity=\"high\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=\"Use tables to display data.\",\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+agent.print_response(\"Write a report comparing NVDA to TSLA\", stream=True)\n"
        },
        {
            "commit": "d6626e5de246cce87651793060322ff1b3f7e0c0",
            "file_path": "cookbook/models/openai/responses/verbosity_control.py",
            "diff": "diff --git a/cookbook/models/openai/responses/verbosity_control.py b/cookbook/models/openai/responses/verbosity_control.py\nnew file mode 100644\nindex 000000000..bd85427c3\n--- /dev/null\n+++ b/cookbook/models/openai/responses/verbosity_control.py\n@@ -0,0 +1,18 @@\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.yfinance import YFinanceTools\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-5\", verbosity=\"high\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=\"Use tables to display data.\",\n+    markdown=True,\n+)\n+agent.print_response(\"Write a report comparing NVDA to TSLA\", stream=True)\n"
        },
        {
            "commit": "d6626e5de246cce87651793060322ff1b3f7e0c0",
            "file_path": "libs/agno/agno/models/openai/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/chat.py b/libs/agno/agno/models/openai/chat.py\nindex 804850175..65f3fecc8 100644\n--- a/libs/agno/agno/models/openai/chat.py\n+++ b/libs/agno/agno/models/openai/chat.py\n@@ -1,7 +1,7 @@\n from collections.abc import AsyncIterator\n from dataclasses import dataclass\n from os import getenv\n-from typing import Any, Dict, Iterator, List, Optional, Type, Union\n+from typing import Any, Dict, Iterator, List, Literal, Optional, Type, Union\n \n import httpx\n from pydantic import BaseModel\n@@ -45,6 +45,7 @@ class OpenAIChat(Model):\n     # Request parameters\n     store: Optional[bool] = None\n     reasoning_effort: Optional[str] = None\n+    verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] = None\n     metadata: Optional[Dict[str, Any]] = None\n     frequency_penalty: Optional[float] = None\n     logit_bias: Optional[Any] = None\n@@ -159,6 +160,7 @@ class OpenAIChat(Model):\n         base_params = {\n             \"store\": self.store,\n             \"reasoning_effort\": self.reasoning_effort,\n+            \"verbosity\": self.verbosity,\n             \"frequency_penalty\": self.frequency_penalty,\n             \"logit_bias\": self.logit_bias,\n             \"logprobs\": self.logprobs,\n@@ -227,6 +229,8 @@ class OpenAIChat(Model):\n         model_dict.update(\n             {\n                 \"store\": self.store,\n+                \"reasoning_effort\": self.reasoning_effort,\n+                \"verbosity\": self.verbosity,\n                 \"frequency_penalty\": self.frequency_penalty,\n                 \"logit_bias\": self.logit_bias,\n                 \"logprobs\": self.logprobs,\n"
        },
        {
            "commit": "d6626e5de246cce87651793060322ff1b3f7e0c0",
            "file_path": "libs/agno/agno/models/openai/responses.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/responses.py b/libs/agno/agno/models/openai/responses.py\nindex c982dc3bb..704bbe825 100644\n--- a/libs/agno/agno/models/openai/responses.py\n+++ b/libs/agno/agno/models/openai/responses.py\n@@ -42,6 +42,8 @@ class OpenAIResponses(Model):\n     metadata: Optional[Dict[str, Any]] = None\n     parallel_tool_calls: Optional[bool] = None\n     reasoning: Optional[Dict[str, Any]] = None\n+    verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] = None\n+    reasoning_effort: Optional[Literal[\"minimal\", \"medium\", \"high\"]] = None\n     store: Optional[bool] = None\n     temperature: Optional[float] = None\n     top_p: Optional[float] = None\n@@ -176,7 +178,6 @@ class OpenAIResponses(Model):\n             \"max_tool_calls\": self.max_tool_calls,\n             \"metadata\": self.metadata,\n             \"parallel_tool_calls\": self.parallel_tool_calls,\n-            \"reasoning\": self.reasoning,\n             \"store\": self.store,\n             \"temperature\": self.temperature,\n             \"top_p\": self.top_p,\n@@ -184,21 +185,37 @@ class OpenAIResponses(Model):\n             \"user\": self.user,\n             \"service_tier\": self.service_tier,\n         }\n+\n+        # Handle reasoning parameter - convert reasoning_effort to reasoning format\n+        if self.reasoning is not None:\n+            base_params[\"reasoning\"] = self.reasoning\n+        elif self.reasoning_effort is not None:\n+            base_params[\"reasoning\"] = {\"effort\": self.reasoning_effort}\n+\n+        # Build text parameter\n+        text_params: Dict[str, Any] = {}\n+\n+        # Add verbosity if specified\n+        if self.verbosity is not None:\n+            text_params[\"verbosity\"] = self.verbosity\n+\n         # Set the response format\n         if response_format is not None:\n             if isinstance(response_format, type) and issubclass(response_format, BaseModel):\n                 schema = get_response_schema_for_provider(response_format, \"openai\")\n-                base_params[\"text\"] = {\n-                    \"format\": {\n-                        \"type\": \"json_schema\",\n-                        \"name\": response_format.__name__,\n-                        \"schema\": schema,\n-                        \"strict\": True,\n-                    }\n+                text_params[\"format\"] = {\n+                    \"type\": \"json_schema\",\n+                    \"name\": response_format.__name__,\n+                    \"schema\": schema,\n+                    \"strict\": True,\n                 }\n             else:\n                 # JSON mode\n-                base_params[\"text\"] = {\"format\": {\"type\": \"json_object\"}}\n+                text_params[\"format\"] = {\"type\": \"json_object\"}\n+\n+        # Add text parameter if there are any text-level params\n+        if text_params:\n+            base_params[\"text\"] = text_params\n \n         # Filter out None values\n         request_params: Dict[str, Any] = {k: v for k, v in base_params.items() if v is not None}\n"
        }
    ]
}