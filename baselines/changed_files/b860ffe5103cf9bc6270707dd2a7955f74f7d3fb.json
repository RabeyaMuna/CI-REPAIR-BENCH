{
    "sha_fail": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
    "changed_files": [
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "Dockerfile",
            "diff": "diff --git a/Dockerfile b/Dockerfile\nindex 619ec1ad..e25b4f99 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -1,8 +1,11 @@\n # Build stage\n-FROM python:3.11-slim AS builder\n+FROM python:3.12-slim AS builder\n \n WORKDIR /app\n \n+# Upgrade pip\u3001setuptools and wheel to the latest version\n+RUN pip install --upgrade pip setuptools wheel\n+\n # Install Rust and required build dependencies\n RUN apt-get update && apt-get install -y \\\n     curl \\\n@@ -19,8 +22,8 @@ COPY lightrag/ ./lightrag/\n \n # Install dependencies\n ENV PATH=\"/root/.cargo/bin:${PATH}\"\n-RUN pip install --user --no-cache-dir .\n-RUN pip install --user --no-cache-dir .[api]\n+RUN pip install --user --no-cache-dir --use-pep517 .\n+RUN pip install --user --no-cache-dir --use-pep517 .[api]\n \n # Install depndencies for default storage\n RUN pip install --user --no-cache-dir nano-vectordb networkx\n@@ -30,16 +33,19 @@ RUN pip install --user --no-cache-dir openai ollama tiktoken\n RUN pip install --user --no-cache-dir pypdf2 python-docx python-pptx openpyxl\n \n # Final stage\n-FROM python:3.11-slim\n+FROM python:3.12-slim\n \n WORKDIR /app\n \n+# Upgrade pip and setuptools\n+RUN pip install --upgrade pip setuptools wheel\n+\n # Copy only necessary files from builder\n COPY --from=builder /root/.local /root/.local\n COPY ./lightrag ./lightrag\n COPY setup.py .\n \n-RUN pip install \".[api]\"\n+RUN pip install --use-pep517 \".[api]\"\n # Make sure scripts in .local are usable\n ENV PATH=/root/.local/bin:$PATH\n \n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "env.example",
            "diff": "diff --git a/env.example b/env.example\nindex 5d71b9b6..6a18a68c 100644\n--- a/env.example\n+++ b/env.example\n@@ -8,6 +8,8 @@ PORT=9621\n WEBUI_TITLE='My Graph KB'\n WEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\n # WORKERS=2\n+### gunicorn worker timeout(as default LLM request timeout if LLM_TIMEOUT is not set)\n+# TIMEOUT=150\n # CORS_ORIGINS=http://localhost:3000,http://localhost:8080\n \n ### Optional SSL Configuration\n@@ -83,31 +85,60 @@ ENABLE_LLM_CACHE=true\n ###     If reranking is enabled, the impact of chunk selection strategies will be diminished.\n # KG_CHUNK_PICK_METHOD=VECTOR\n \n+#########################################################\n ### Reranking configuration\n-###     Reranker Set ENABLE_RERANK to true in reranking model is configed\n-# ENABLE_RERANK=True\n-### Minimum rerank score for document chunk exclusion (set to 0.0 to keep all chunks, 0.6 or above if LLM is not strong enought)\n+### RERANK_BINDING type:  null, cohere, jina, aliyun\n+### For rerank model deployed by vLLM use cohere binding\n+#########################################################\n+RERANK_BINDING=null\n+### Enable rerank by default in query params when RERANK_BINDING is not null\n+# RERANK_BY_DEFAULT=True\n+### rerank score chunk filter(set to 0.0 to keep all chunks, 0.6 or above if LLM is not strong enought)\n # MIN_RERANK_SCORE=0.0\n-### Rerank model configuration (required when ENABLE_RERANK=True)\n-# RERANK_MODEL=jina-reranker-v2-base-multilingual\n+\n+### For local deployment with vLLM\n+# RERANK_MODEL=BAAI/bge-reranker-v2-m3\n+# RERANK_BINDING_HOST=http://localhost:8000/v1/rerank\n+# RERANK_BINDING_API_KEY=your_rerank_api_key_here\n+\n+### Default value for Cohere AI\n+# RERANK_MODEL=rerank-v3.5\n+# RERANK_BINDING_HOST=https://api.cohere.com/v2/rerank\n+# RERANK_BINDING_API_KEY=your_rerank_api_key_here\n+\n+### Default value for Jina AI\n+# RERANK_MODELjina-reranker-v2-base-multilingual\n # RERANK_BINDING_HOST=https://api.jina.ai/v1/rerank\n # RERANK_BINDING_API_KEY=your_rerank_api_key_here\n \n+### Default value for Aliyun\n+# RERANK_MODEL=gte-rerank-v2\n+# RERANK_BINDING_HOST=https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank\n+# RERANK_BINDING_API_KEY=your_rerank_api_key_here\n+\n ########################################\n ### Document processing configuration\n ########################################\n-### Language: English, Chinese, French, German ...\n-SUMMARY_LANGUAGE=English\n ENABLE_LLM_CACHE_FOR_EXTRACT=true\n+\n+### Document processing output language: English, Chinese, French, German ...\n+SUMMARY_LANGUAGE=English\n+\n+### Entity types that the LLM will attempt to recognize\n+# ENTITY_TYPES=[\"person\", \"organization\", \"location\", \"event\", \"concept\"]\n+\n ### Chunk size for document splitting, 500~1500 is recommended\n # CHUNK_SIZE=1200\n # CHUNK_OVERLAP_SIZE=100\n-### Entity and relation summarization configuration\n-### Number of duplicated entities/edges to trigger LLM re-summary on merge (at least 3 is recommented)\uff0c and max tokens send to LLM\n-# FORCE_LLM_SUMMARY_ON_MERGE=4\n-# MAX_TOKENS=10000\n-### Maximum number of entity extraction attempts for ambiguous content\n-# MAX_GLEANING=1\n+\n+### Number of summary semgments or tokens to trigger LLM summary on entity/relation merge (at least 3 is recommented)\n+# FORCE_LLM_SUMMARY_ON_MERGE=8\n+### Max description token size to trigger LLM summary\n+# SUMMARY_MAX_TOKENS = 1200\n+### Recommended LLM summary output length in tokens\n+# SUMMARY_LENGTH_RECOMMENDED_=600\n+### Maximum context size sent to LLM for description summary\n+# SUMMARY_CONTEXT_SIZE=12000\n \n ###############################\n ### Concurrency Configuration\n@@ -125,9 +156,8 @@ MAX_PARALLEL_INSERT=2\n ### LLM Configuration\n ### LLM_BINDING type: openai, ollama, lollms, azure_openai, aws_bedrock\n ###########################################################\n-### LLM temperature setting for all llm binding (openai, azure_openai, ollama)\n-# TEMPERATURE=1.0\n-### Some models like o1-mini require temperature to be set to 1, some LLM can fall into output loops with low temperature\n+### LLM request timeout setting for all llm (0 means no timeout for Ollma)\n+# LLM_TIMEOUT=180\n \n LLM_BINDING=openai\n LLM_MODEL=gpt-4o\n@@ -145,29 +175,37 @@ LLM_BINDING_API_KEY=your_api_key\n # LLM_BINDING=openai\n \n ### OpenAI Specific Parameters\n-### Apply frequency penalty to prevent the LLM from generating repetitive or looping outputs\n-# OPENAI_LLM_FREQUENCY_PENALTY=1.1\n-### use the following command to see all support options for openai and azure_openai\n+### To mitigate endless output loops and prevent greedy decoding for Qwen3, set the temperature parameter to a value between 0.8 and 1.0\n+# OPENAI_LLM_TEMPERATURE=1.0\n+# OPENAI_LLM_REASONING_EFFORT=low\n+### If the presence penalty still can not stop the model from generates repetitive or unconstrained output\n+# OPENAI_LLM_MAX_COMPLETION_TOKENS=16384\n+\n+### OpenRouter Specific Parameters\n+# OPENAI_LLM_EXTRA_BODY='{\"reasoning\": {\"enabled\": false}}'\n+### Qwen3 Specific Parameters depoly by vLLM\n+# OPENAI_LLM_EXTRA_BODY='{\"chat_template_kwargs\": {\"enable_thinking\": false}}'\n+\n+### use the following command to see all support options for OpenAI, azure_openai or OpenRouter\n ### lightrag-server --llm-binding openai --help\n \n ### Ollama Server Specific Parameters\n-### Time out in seconds, None for infinite timeout\n-TIMEOUT=240\n-### OLLAMA_LLM_NUM_CTX must be larger than MAX_TOTAL_TOKENS + 2000\n+### OLLAMA_LLM_NUM_CTX must be provided, and should at least larger than MAX_TOTAL_TOKENS + 2000\n OLLAMA_LLM_NUM_CTX=32768\n+# OLLAMA_LLM_TEMPERATURE=1.0\n ### Stop sequences for Ollama LLM\n # OLLAMA_LLM_STOP='[\"</s>\", \"Assistant:\", \"\\n\\n\"]'\n-### If OLLAMA_LLM_TEMPERATURE is not specified, the system will default to the value defined by TEMPERATURE\n-# OLLAMA_LLM_TEMPERATURE=0.85\n ### use the following command to see all support options for Ollama LLM\n ### lightrag-server --llm-binding ollama --help\n \n+### Bedrock Specific Parameters\n+# BEDROCK_LLM_TEMPERATURE=1.0\n+\n ####################################################################################\n ### Embedding Configuration (Should not be changed after the first file processed)\n+### EMBEDDING_BINDING: ollama, openai, azure_openai, jina, lollms, aws_bedrock\n ####################################################################################\n-### Embedding Binding type: ollama, openai, azure_openai, jina,  lollms\n-\n-### see also env.ollama-binding-options.example for fine tuning ollama\n+# EMBEDDING_TIMEOUT=30\n EMBEDDING_BINDING=ollama\n EMBEDDING_MODEL=bge-m3:latest\n EMBEDDING_DIM=1024\n@@ -179,7 +217,7 @@ EMBEDDING_BINDING_HOST=http://localhost:11434\n # EMBEDDING_BINDING=openai\n # EMBEDDING_MODEL=text-embedding-3-large\n # EMBEDDING_DIM=3072\n-# EMBEDDING_BINDING_HOST=https://api.openai.com\n+# EMBEDDING_BINDING_HOST=https://api.openai.com/v1\n # EMBEDDING_BINDING_API_KEY=your_api_key\n \n ### Optional for Azure\n@@ -268,7 +306,7 @@ POSTGRES_IVFFLAT_LISTS=100\n NEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io\n NEO4J_USERNAME=neo4j\n NEO4J_PASSWORD='your_password'\n-# NEO4J_DATABASE=chunk-entity-relation\n+NEO4J_DATABASE=noe4j\n NEO4J_MAX_CONNECTION_POOL_SIZE=100\n NEO4J_CONNECTION_TIMEOUT=30\n NEO4J_CONNECTION_ACQUISITION_TIMEOUT=30\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "examples/rerank_example.py",
            "diff": "diff --git a/examples/rerank_example.py b/examples/rerank_example.py\nindex 5754a750..c7db6656 100644\n--- a/examples/rerank_example.py\n+++ b/examples/rerank_example.py\n@@ -5,15 +5,21 @@ This example demonstrates how to use rerank functionality with LightRAG\n to improve retrieval quality across different query modes.\n \n Configuration Required:\n-1. Set your LLM API key and base URL in llm_model_func()\n-2. Set your embedding API key and base URL in embedding_func()\n-3. Set your rerank API key and base URL in the rerank configuration\n-4. Or use environment variables (.env file):\n-   - RERANK_MODEL=your_rerank_model\n-   - RERANK_BINDING_HOST=your_rerank_endpoint\n-   - RERANK_BINDING_API_KEY=your_rerank_api_key\n-\n-Note: Rerank is now controlled per query via the 'enable_rerank' parameter (default: True)\n+1. Set your OpenAI LLM API key and base URL with env vars\n+    LLM_MODEL\n+    LLM_BINDING_HOST\n+    LLM_BINDING_API_KEY\n+2. Set your OpenAI embedding API key and base URL with env vars:\n+    EMBEDDING_MODEL\n+    EMBEDDING_DIM\n+    EMBEDDING_BINDING_HOST\n+    EMBEDDING_BINDING_API_KEY\n+3. Set your vLLM deployed AI rerank model setting with env vars:\n+    RERANK_MODEL\n+    RERANK_BINDING_HOST\n+    RERANK_BINDING_API_KEY\n+\n+Note: Rerank is controlled per query via the 'enable_rerank' parameter (default: True)\n \"\"\"\n \n import asyncio\n@@ -21,11 +27,13 @@ import os\n import numpy as np\n \n from lightrag import LightRAG, QueryParam\n-from lightrag.rerank import custom_rerank, RerankModel\n from lightrag.llm.openai import openai_complete_if_cache, openai_embed\n from lightrag.utils import EmbeddingFunc, setup_logger\n from lightrag.kg.shared_storage import initialize_pipeline_status\n \n+from functools import partial\n+from lightrag.rerank import cohere_rerank\n+\n # Set up your working directory\n WORKING_DIR = \"./test_rerank\"\n setup_logger(\"test_rerank\")\n@@ -38,12 +46,12 @@ async def llm_model_func(\n     prompt, system_prompt=None, history_messages=[], **kwargs\n ) -> str:\n     return await openai_complete_if_cache(\n-        \"gpt-4o-mini\",\n+        os.getenv(\"LLM_MODEL\"),\n         prompt,\n         system_prompt=system_prompt,\n         history_messages=history_messages,\n-        api_key=\"your_llm_api_key_here\",\n-        base_url=\"https://api.your-llm-provider.com/v1\",\n+        api_key=os.getenv(\"LLM_BINDING_API_KEY\"),\n+        base_url=os.getenv(\"LLM_BINDING_HOST\"),\n         **kwargs,\n     )\n \n@@ -51,23 +59,18 @@ async def llm_model_func(\n async def embedding_func(texts: list[str]) -> np.ndarray:\n     return await openai_embed(\n         texts,\n-        model=\"text-embedding-3-large\",\n-        api_key=\"your_embedding_api_key_here\",\n-        base_url=\"https://api.your-embedding-provider.com/v1\",\n+        model=os.getenv(\"EMBEDDING_MODEL\"),\n+        api_key=os.getenv(\"EMBEDDING_BINDING_API_KEY\"),\n+        base_url=os.getenv(\"EMBEDDING_BINDING_HOST\"),\n     )\n \n \n-async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):\n-    \"\"\"Custom rerank function with all settings included\"\"\"\n-    return await custom_rerank(\n-        query=query,\n-        documents=documents,\n-        model=\"BAAI/bge-reranker-v2-m3\",\n-        base_url=\"https://api.your-rerank-provider.com/v1/rerank\",\n-        api_key=\"your_rerank_api_key_here\",\n-        top_n=top_n or 10,\n-        **kwargs,\n-    )\n+rerank_model_func = partial(\n+    cohere_rerank,\n+    model=os.getenv(\"RERANK_MODEL\"),\n+    api_key=os.getenv(\"RERANK_BINDING_API_KEY\"),\n+    base_url=os.getenv(\"RERANK_BINDING_HOST\"),\n+)\n \n \n async def create_rag_with_rerank():\n@@ -88,42 +91,7 @@ async def create_rag_with_rerank():\n             func=embedding_func,\n         ),\n         # Rerank Configuration - provide the rerank function\n-        rerank_model_func=my_rerank_func,\n-    )\n-\n-    await rag.initialize_storages()\n-    await initialize_pipeline_status()\n-\n-    return rag\n-\n-\n-async def create_rag_with_rerank_model():\n-    \"\"\"Alternative: Create LightRAG instance using RerankModel wrapper\"\"\"\n-\n-    # Get embedding dimension\n-    test_embedding = await embedding_func([\"test\"])\n-    embedding_dim = test_embedding.shape[1]\n-    print(f\"Detected embedding dimension: {embedding_dim}\")\n-\n-    # Method 2: Using RerankModel wrapper\n-    rerank_model = RerankModel(\n-        rerank_func=custom_rerank,\n-        kwargs={\n-            \"model\": \"BAAI/bge-reranker-v2-m3\",\n-            \"base_url\": \"https://api.your-rerank-provider.com/v1/rerank\",\n-            \"api_key\": \"your_rerank_api_key_here\",\n-        },\n-    )\n-\n-    rag = LightRAG(\n-        working_dir=WORKING_DIR,\n-        llm_model_func=llm_model_func,\n-        embedding_func=EmbeddingFunc(\n-            embedding_dim=embedding_dim,\n-            max_token_size=8192,\n-            func=embedding_func,\n-        ),\n-        rerank_model_func=rerank_model.rerank,\n+        rerank_model_func=rerank_model_func,\n     )\n \n     await rag.initialize_storages()\n@@ -136,7 +104,7 @@ async def test_rerank_with_different_settings():\n     \"\"\"\n     Test rerank functionality with different enable_rerank settings\n     \"\"\"\n-    print(\"\ud83d\ude80 Setting up LightRAG with Rerank functionality...\")\n+    print(\"\\n\\n\ud83d\ude80 Setting up LightRAG with Rerank functionality...\")\n \n     rag = await create_rag_with_rerank()\n \n@@ -199,11 +167,11 @@ async def test_direct_rerank():\n     print(\"=\" * 40)\n \n     documents = [\n-        {\"content\": \"Reranking significantly improves retrieval quality\"},\n-        {\"content\": \"LightRAG supports advanced reranking capabilities\"},\n-        {\"content\": \"Vector search finds semantically similar documents\"},\n-        {\"content\": \"Natural language processing with modern transformers\"},\n-        {\"content\": \"The quick brown fox jumps over the lazy dog\"},\n+        \"Vector search finds semantically similar documents\",\n+        \"LightRAG supports advanced reranking capabilities\",\n+        \"Reranking significantly improves retrieval quality\",\n+        \"Natural language processing with modern transformers\",\n+        \"The quick brown fox jumps over the lazy dog\",\n     ]\n \n     query = \"rerank improve quality\"\n@@ -211,20 +179,20 @@ async def test_direct_rerank():\n     print(f\"Documents: {len(documents)}\")\n \n     try:\n-        reranked_docs = await custom_rerank(\n+        reranked_results = await rerank_model_func(\n             query=query,\n             documents=documents,\n-            model=\"BAAI/bge-reranker-v2-m3\",\n-            base_url=\"https://api.your-rerank-provider.com/v1/rerank\",\n-            api_key=\"your_rerank_api_key_here\",\n-            top_n=3,\n+            top_n=4,\n         )\n \n         print(\"\\n\u2705 Rerank Results:\")\n-        for i, doc in enumerate(reranked_docs):\n-            score = doc.get(\"rerank_score\", \"N/A\")\n-            content = doc.get(\"content\", \"\")[:60]\n-            print(f\"  {i+1}. Score: {score:.4f} | {content}...\")\n+        i = 0\n+        for result in reranked_results:\n+            index = result[\"index\"]\n+            score = result[\"relevance_score\"]\n+            content = documents[index]\n+            print(f\"  {index}. Score: {score:.4f} | {content}...\")\n+            i += 1\n \n     except Exception as e:\n         print(f\"\u274c Rerank failed: {e}\")\n@@ -236,12 +204,12 @@ async def main():\n     print(\"=\" * 60)\n \n     try:\n-        # Test rerank with different enable_rerank settings\n-        await test_rerank_with_different_settings()\n-\n         # Test direct rerank\n         await test_direct_rerank()\n \n+        # Test rerank with different enable_rerank settings\n+        await test_rerank_with_different_settings()\n+\n         print(\"\\n\u2705 Example completed successfully!\")\n         print(\"\\n\ud83d\udca1 Key Points:\")\n         print(\"   \u2713 Rerank is now controlled per query via 'enable_rerank' parameter\")\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/__init__.py",
            "diff": "diff --git a/lightrag/api/__init__.py b/lightrag/api/__init__.py\nindex 39c729cd..5a6845a8 100644\n--- a/lightrag/api/__init__.py\n+++ b/lightrag/api/__init__.py\n@@ -1 +1 @@\n-__api_version__ = \"0205\"\n+__api_version__ = \"0209\"\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/config.py",
            "diff": "diff --git a/lightrag/api/config.py b/lightrag/api/config.py\nindex 01d0dd75..eae2f45b 100644\n--- a/lightrag/api/config.py\n+++ b/lightrag/api/config.py\n@@ -30,12 +30,15 @@ from lightrag.constants import (\n     DEFAULT_FORCE_LLM_SUMMARY_ON_MERGE,\n     DEFAULT_MAX_ASYNC,\n     DEFAULT_SUMMARY_MAX_TOKENS,\n+    DEFAULT_SUMMARY_LENGTH_RECOMMENDED,\n+    DEFAULT_SUMMARY_CONTEXT_SIZE,\n     DEFAULT_SUMMARY_LANGUAGE,\n     DEFAULT_EMBEDDING_FUNC_MAX_ASYNC,\n     DEFAULT_EMBEDDING_BATCH_NUM,\n     DEFAULT_OLLAMA_MODEL_NAME,\n     DEFAULT_OLLAMA_MODEL_TAG,\n-    DEFAULT_TEMPERATURE,\n+    DEFAULT_RERANK_BINDING,\n+    DEFAULT_ENTITY_TYPES,\n )\n \n # use the .env that is inside the current folder\n@@ -77,9 +80,7 @@ def parse_args() -> argparse.Namespace:\n         argparse.Namespace: Parsed arguments\n     \"\"\"\n \n-    parser = argparse.ArgumentParser(\n-        description=\"LightRAG FastAPI Server with separate working and input directories\"\n-    )\n+    parser = argparse.ArgumentParser(description=\"LightRAG API Server\")\n \n     # Server configuration\n     parser.add_argument(\n@@ -121,10 +122,26 @@ def parse_args() -> argparse.Namespace:\n         help=f\"Maximum async operations (default: from env or {DEFAULT_MAX_ASYNC})\",\n     )\n     parser.add_argument(\n-        \"--max-tokens\",\n+        \"--summary-max-tokens\",\n+        type=int,\n+        default=get_env_value(\"SUMMARY_MAX_TOKENS\", DEFAULT_SUMMARY_MAX_TOKENS, int),\n+        help=f\"Maximum token size for entity/relation summary(default: from env or {DEFAULT_SUMMARY_MAX_TOKENS})\",\n+    )\n+    parser.add_argument(\n+        \"--summary-context-size\",\n+        type=int,\n+        default=get_env_value(\n+            \"SUMMARY_CONTEXT_SIZE\", DEFAULT_SUMMARY_CONTEXT_SIZE, int\n+        ),\n+        help=f\"LLM Summary Context size (default: from env or {DEFAULT_SUMMARY_CONTEXT_SIZE})\",\n+    )\n+    parser.add_argument(\n+        \"--summary-length-recommended\",\n         type=int,\n-        default=get_env_value(\"MAX_TOKENS\", DEFAULT_SUMMARY_MAX_TOKENS, int),\n-        help=f\"Maximum token size (default: from env or {DEFAULT_SUMMARY_MAX_TOKENS})\",\n+        default=get_env_value(\n+            \"SUMMARY_LENGTH_RECOMMENDED\", DEFAULT_SUMMARY_LENGTH_RECOMMENDED, int\n+        ),\n+        help=f\"LLM Summary Context size (default: from env or {DEFAULT_SUMMARY_LENGTH_RECOMMENDED})\",\n     )\n \n     # Logging configuration\n@@ -226,6 +243,13 @@ def parse_args() -> argparse.Namespace:\n         choices=[\"lollms\", \"ollama\", \"openai\", \"azure_openai\", \"aws_bedrock\", \"jina\"],\n         help=\"Embedding binding type (default: from env or ollama)\",\n     )\n+    parser.add_argument(\n+        \"--rerank-binding\",\n+        type=str,\n+        default=get_env_value(\"RERANK_BINDING\", DEFAULT_RERANK_BINDING),\n+        choices=[\"null\", \"cohere\", \"jina\", \"aliyun\"],\n+        help=f\"Rerank binding type (default: from env or {DEFAULT_RERANK_BINDING})\",\n+    )\n \n     # Conditionally add binding options defined in binding_options module\n     # This will add command line arguments for all binding options (e.g., --ollama-embedding-num_ctx)\n@@ -264,14 +288,6 @@ def parse_args() -> argparse.Namespace:\n     elif os.environ.get(\"LLM_BINDING\") in [\"openai\", \"azure_openai\"]:\n         OpenAILLMOptions.add_args(parser)\n \n-    # Add global temperature command line argument\n-    parser.add_argument(\n-        \"--temperature\",\n-        type=float,\n-        default=get_env_value(\"TEMPERATURE\", DEFAULT_TEMPERATURE, float),\n-        help=\"Global temperature setting for LLM (default: from env TEMPERATURE or 0.1)\",\n-    )\n-\n     args = parser.parse_args()\n \n     # convert relative path to absolute path\n@@ -330,38 +346,13 @@ def parse_args() -> argparse.Namespace:\n     )\n     args.enable_llm_cache = get_env_value(\"ENABLE_LLM_CACHE\", True, bool)\n \n-    # Handle Ollama LLM temperature with priority cascade when llm-binding is ollama\n-    if args.llm_binding == \"ollama\":\n-        # Priority order (highest to lowest):\n-        # 1. --ollama-llm-temperature command argument\n-        # 2. OLLAMA_LLM_TEMPERATURE environment variable\n-        # 3. --temperature command argument\n-        # 4. TEMPERATURE environment variable\n-\n-        # Check if --ollama-llm-temperature was explicitly provided in command line\n-        if \"--ollama-llm-temperature\" not in sys.argv:\n-            # Use args.temperature which handles --temperature command arg and TEMPERATURE env var priority\n-            args.ollama_llm_temperature = args.temperature\n-\n-    # Handle OpenAI LLM temperature with priority cascade when llm-binding is openai or azure_openai\n-    if args.llm_binding in [\"openai\", \"azure_openai\"]:\n-        # Priority order (highest to lowest):\n-        # 1. --openai-llm-temperature command argument\n-        # 2. OPENAI_LLM_TEMPERATURE environment variable\n-        # 3. --temperature command argument\n-        # 4. TEMPERATURE environment variable\n-\n-        # Check if --openai-llm-temperature was explicitly provided in command line\n-        if \"--openai-llm-temperature\" not in sys.argv:\n-            # Use args.temperature which handles --temperature command arg and TEMPERATURE env var priority\n-            args.openai_llm_temperature = args.temperature\n-\n     # Select Document loading tool (DOCLING, DEFAULT)\n     args.document_loading_engine = get_env_value(\"DOCUMENT_LOADING_ENGINE\", \"DEFAULT\")\n \n     # Add environment variables that were previously read directly\n     args.cors_origins = get_env_value(\"CORS_ORIGINS\", \"*\")\n     args.summary_language = get_env_value(\"SUMMARY_LANGUAGE\", DEFAULT_SUMMARY_LANGUAGE)\n+    args.entity_types = get_env_value(\"ENTITY_TYPES\", DEFAULT_ENTITY_TYPES)\n     args.whitelist_paths = get_env_value(\"WHITELIST_PATHS\", \"/health,/api/*\")\n \n     # For JWT Auth\n@@ -372,9 +363,10 @@ def parse_args() -> argparse.Namespace:\n     args.jwt_algorithm = get_env_value(\"JWT_ALGORITHM\", \"HS256\")\n \n     # Rerank model configuration\n-    args.rerank_model = get_env_value(\"RERANK_MODEL\", \"BAAI/bge-reranker-v2-m3\")\n+    args.rerank_model = get_env_value(\"RERANK_MODEL\", None)\n     args.rerank_binding_host = get_env_value(\"RERANK_BINDING_HOST\", None)\n     args.rerank_binding_api_key = get_env_value(\"RERANK_BINDING_API_KEY\", None)\n+    # Note: rerank_binding is already set by argparse, no need to override from env\n \n     # Min rerank score configuration\n     args.min_rerank_score = get_env_value(\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/lightrag_server.py",
            "diff": "diff --git a/lightrag/api/lightrag_server.py b/lightrag/api/lightrag_server.py\nindex c3384181..41ede089 100644\n--- a/lightrag/api/lightrag_server.py\n+++ b/lightrag/api/lightrag_server.py\n@@ -2,7 +2,7 @@\n LightRAG FastAPI Server\n \"\"\"\n \n-from fastapi import FastAPI, Depends, HTTPException, status\n+from fastapi import FastAPI, Depends, HTTPException\n import asyncio\n import os\n import logging\n@@ -11,6 +11,7 @@ import signal\n import sys\n import uvicorn\n import pipmaster as pm\n+import inspect\n from fastapi.staticfiles import StaticFiles\n from fastapi.responses import RedirectResponse\n from pathlib import Path\n@@ -38,6 +39,8 @@ from lightrag.constants import (\n     DEFAULT_LOG_MAX_BYTES,\n     DEFAULT_LOG_BACKUP_COUNT,\n     DEFAULT_LOG_FILENAME,\n+    DEFAULT_LLM_TIMEOUT,\n+    DEFAULT_EMBEDDING_TIMEOUT,\n )\n from lightrag.api.routers.document_routes import (\n     DocumentManager,\n@@ -247,6 +250,7 @@ def create_app(args):\n             azure_openai_complete_if_cache,\n             azure_openai_embed,\n         )\n+        from lightrag.llm.binding_options import OpenAILLMOptions\n     if args.llm_binding == \"aws_bedrock\" or args.embedding_binding == \"aws_bedrock\":\n         from lightrag.llm.bedrock import bedrock_complete_if_cache, bedrock_embed\n     if args.embedding_binding == \"ollama\":\n@@ -254,6 +258,11 @@ def create_app(args):\n     if args.embedding_binding == \"jina\":\n         from lightrag.llm.jina import jina_embed\n \n+    llm_timeout = get_env_value(\"LLM_TIMEOUT\", DEFAULT_LLM_TIMEOUT, int)\n+    embedding_timeout = get_env_value(\n+        \"EMBEDDING_TIMEOUT\", DEFAULT_EMBEDDING_TIMEOUT, int\n+    )\n+\n     async def openai_alike_model_complete(\n         prompt,\n         system_prompt=None,\n@@ -267,12 +276,10 @@ def create_app(args):\n         if history_messages is None:\n             history_messages = []\n \n-        # Use OpenAI LLM options if available, otherwise fallback to global temperature\n-        if args.llm_binding == \"openai\":\n-            openai_options = OpenAILLMOptions.options_dict(args)\n-            kwargs.update(openai_options)\n-        else:\n-            kwargs[\"temperature\"] = args.temperature\n+        # Use OpenAI LLM options if available\n+        openai_options = OpenAILLMOptions.options_dict(args)\n+        kwargs[\"timeout\"] = llm_timeout\n+        kwargs.update(openai_options)\n \n         return await openai_complete_if_cache(\n             args.llm_model,\n@@ -297,12 +304,10 @@ def create_app(args):\n         if history_messages is None:\n             history_messages = []\n \n-        # Use OpenAI LLM options if available, otherwise fallback to global temperature\n-        if args.llm_binding == \"azure_openai\":\n-            openai_options = OpenAILLMOptions.options_dict(args)\n-            kwargs.update(openai_options)\n-        else:\n-            kwargs[\"temperature\"] = args.temperature\n+        # Use OpenAI LLM options\n+        openai_options = OpenAILLMOptions.options_dict(args)\n+        kwargs[\"timeout\"] = llm_timeout\n+        kwargs.update(openai_options)\n \n         return await azure_openai_complete_if_cache(\n             args.llm_model,\n@@ -329,7 +334,7 @@ def create_app(args):\n             history_messages = []\n \n         # Use global temperature for Bedrock\n-        kwargs[\"temperature\"] = args.temperature\n+        kwargs[\"temperature\"] = get_env_value(\"BEDROCK_LLM_TEMPERATURE\", 1.0, float)\n \n         return await bedrock_complete_if_cache(\n             args.llm_model,\n@@ -392,33 +397,60 @@ def create_app(args):\n         ),\n     )\n \n-    # Configure rerank function if model and API are configured\n+    # Configure rerank function based on args.rerank_bindingparameter\n     rerank_model_func = None\n-    if args.rerank_binding_api_key and args.rerank_binding_host:\n-        from lightrag.rerank import custom_rerank\n+    if args.rerank_binding != \"null\":\n+        from lightrag.rerank import cohere_rerank, jina_rerank, ali_rerank\n+\n+        # Map rerank binding to corresponding function\n+        rerank_functions = {\n+            \"cohere\": cohere_rerank,\n+            \"jina\": jina_rerank,\n+            \"aliyun\": ali_rerank,\n+        }\n+\n+        # Select the appropriate rerank function based on binding\n+        selected_rerank_func = rerank_functions.get(args.rerank_binding)\n+        if not selected_rerank_func:\n+            logger.error(f\"Unsupported rerank binding: {args.rerank_binding}\")\n+            raise ValueError(f\"Unsupported rerank binding: {args.rerank_binding}\")\n+\n+        # Get default values from selected_rerank_func if args values are None\n+        if args.rerank_model is None or args.rerank_binding_host is None:\n+            sig = inspect.signature(selected_rerank_func)\n+\n+            # Set default model if args.rerank_model is None\n+            if args.rerank_model is None and \"model\" in sig.parameters:\n+                default_model = sig.parameters[\"model\"].default\n+                if default_model != inspect.Parameter.empty:\n+                    args.rerank_model = default_model\n+\n+            # Set default base_url if args.rerank_binding_host is None\n+            if args.rerank_binding_host is None and \"base_url\" in sig.parameters:\n+                default_base_url = sig.parameters[\"base_url\"].default\n+                if default_base_url != inspect.Parameter.empty:\n+                    args.rerank_binding_host = default_base_url\n \n         async def server_rerank_func(\n-            query: str, documents: list, top_n: int = None, **kwargs\n+            query: str, documents: list, top_n: int = None, extra_body: dict = None\n         ):\n             \"\"\"Server rerank function with configuration from environment variables\"\"\"\n-            return await custom_rerank(\n+            return await selected_rerank_func(\n                 query=query,\n                 documents=documents,\n+                top_n=top_n,\n+                api_key=args.rerank_binding_api_key,\n                 model=args.rerank_model,\n                 base_url=args.rerank_binding_host,\n-                api_key=args.rerank_binding_api_key,\n-                top_n=top_n,\n-                **kwargs,\n+                extra_body=extra_body,\n             )\n \n         rerank_model_func = server_rerank_func\n         logger.info(\n-            f\"Rerank model configured: {args.rerank_model} (can be enabled per query)\"\n+            f\"Reranking is enabled: {args.rerank_model or 'default model'} using {args.rerank_binding} provider\"\n         )\n     else:\n-        logger.info(\n-            \"Rerank model not configured. Set RERANK_BINDING_API_KEY and RERANK_BINDING_HOST to enable reranking.\"\n-        )\n+        logger.info(\"Reranking is disabled\")\n \n     # Create ollama_server_infos from command line arguments\n     from lightrag.api.config import OllamaServerInfos\n@@ -445,13 +477,14 @@ def create_app(args):\n             ),\n             llm_model_name=args.llm_model,\n             llm_model_max_async=args.max_async,\n-            summary_max_tokens=args.max_tokens,\n+            summary_max_tokens=args.summary_max_tokens,\n+            summary_context_size=args.summary_context_size,\n             chunk_token_size=int(args.chunk_size),\n             chunk_overlap_token_size=int(args.chunk_overlap_size),\n             llm_model_kwargs=(\n                 {\n                     \"host\": args.llm_binding_host,\n-                    \"timeout\": args.timeout,\n+                    \"timeout\": llm_timeout,\n                     \"options\": OllamaLLMOptions.options_dict(args),\n                     \"api_key\": args.llm_binding_api_key,\n                 }\n@@ -459,6 +492,8 @@ def create_app(args):\n                 else {}\n             ),\n             embedding_func=embedding_func,\n+            default_llm_timeout=llm_timeout,\n+            default_embedding_timeout=embedding_timeout,\n             kv_storage=args.kv_storage,\n             graph_storage=args.graph_storage,\n             vector_storage=args.vector_storage,\n@@ -471,7 +506,10 @@ def create_app(args):\n             rerank_model_func=rerank_model_func,\n             max_parallel_insert=args.max_parallel_insert,\n             max_graph_nodes=args.max_graph_nodes,\n-            addon_params={\"language\": args.summary_language},\n+            addon_params={\n+                \"language\": args.summary_language,\n+                \"entity_types\": args.entity_types,\n+            },\n             ollama_server_infos=ollama_server_infos,\n         )\n     else:  # azure_openai\n@@ -481,13 +519,13 @@ def create_app(args):\n             llm_model_func=azure_openai_model_complete,\n             chunk_token_size=int(args.chunk_size),\n             chunk_overlap_token_size=int(args.chunk_overlap_size),\n-            llm_model_kwargs={\n-                \"timeout\": args.timeout,\n-            },\n             llm_model_name=args.llm_model,\n             llm_model_max_async=args.max_async,\n-            summary_max_tokens=args.max_tokens,\n+            summary_max_tokens=args.summary_max_tokens,\n+            summary_context_size=args.summary_context_size,\n             embedding_func=embedding_func,\n+            default_llm_timeout=llm_timeout,\n+            default_embedding_timeout=embedding_timeout,\n             kv_storage=args.kv_storage,\n             graph_storage=args.graph_storage,\n             vector_storage=args.vector_storage,\n@@ -500,7 +538,10 @@ def create_app(args):\n             rerank_model_func=rerank_model_func,\n             max_parallel_insert=args.max_parallel_insert,\n             max_graph_nodes=args.max_graph_nodes,\n-            addon_params={\"language\": args.summary_language},\n+            addon_params={\n+                \"language\": args.summary_language,\n+                \"entity_types\": args.entity_types,\n+            },\n             ollama_server_infos=ollama_server_infos,\n         )\n \n@@ -573,9 +614,7 @@ def create_app(args):\n             }\n         username = form_data.username\n         if auth_handler.accounts.get(username) != form_data.password:\n-            raise HTTPException(\n-                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Incorrect credentials\"\n-            )\n+            raise HTTPException(status_code=401, detail=\"Incorrect credentials\")\n \n         # Regular user login\n         user_token = auth_handler.create_token(\n@@ -618,7 +657,8 @@ def create_app(args):\n                     \"embedding_binding\": args.embedding_binding,\n                     \"embedding_binding_host\": args.embedding_binding_host,\n                     \"embedding_model\": args.embedding_model,\n-                    \"max_tokens\": args.max_tokens,\n+                    \"summary_max_tokens\": args.summary_max_tokens,\n+                    \"summary_context_size\": args.summary_context_size,\n                     \"kv_storage\": args.kv_storage,\n                     \"doc_status_storage\": args.doc_status_storage,\n                     \"graph_storage\": args.graph_storage,\n@@ -627,13 +667,12 @@ def create_app(args):\n                     \"enable_llm_cache\": args.enable_llm_cache,\n                     \"workspace\": args.workspace,\n                     \"max_graph_nodes\": args.max_graph_nodes,\n-                    # Rerank configuration (based on whether rerank model is configured)\n+                    # Rerank configuration\n                     \"enable_rerank\": rerank_model_func is not None,\n-                    \"rerank_model\": args.rerank_model\n-                    if rerank_model_func is not None\n-                    else None,\n+                    \"rerank_binding\": args.rerank_binding,\n+                    \"rerank_model\": args.rerank_model if rerank_model_func else None,\n                     \"rerank_binding_host\": args.rerank_binding_host\n-                    if rerank_model_func is not None\n+                    if rerank_model_func\n                     else None,\n                     # Environment variable status (requested configuration)\n                     \"summary_language\": args.summary_language,\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/routers/ollama_api.py",
            "diff": "diff --git a/lightrag/api/routers/ollama_api.py b/lightrag/api/routers/ollama_api.py\nindex c38018f6..9ef7c55f 100644\n--- a/lightrag/api/routers/ollama_api.py\n+++ b/lightrag/api/routers/ollama_api.py\n@@ -469,8 +469,8 @@ class OllamaAPI:\n             \"/chat\", dependencies=[Depends(combined_auth)], include_in_schema=True\n         )\n         async def chat(raw_request: Request):\n-            \"\"\"Process chat completion requests acting as an Ollama model\n-            Routes user queries through LightRAG by selecting query mode based on prefix indicators.\n+            \"\"\"Process chat completion requests by acting as an Ollama model.\n+            Routes user queries through LightRAG by selecting query mode based on query prefix.\n             Detects and forwards OpenWebUI session-related requests (for meta data generation task) directly to LLM.\n             Supports both application/json and application/octet-stream Content-Types.\n             \"\"\"\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/run_with_gunicorn.py",
            "diff": "diff --git a/lightrag/api/run_with_gunicorn.py b/lightrag/api/run_with_gunicorn.py\nindex 8c8a029d..929db019 100644\n--- a/lightrag/api/run_with_gunicorn.py\n+++ b/lightrag/api/run_with_gunicorn.py\n@@ -153,7 +153,7 @@ def main():\n \n             # Timeout configuration prioritizes command line arguments\n             gunicorn_config.timeout = (\n-                global_args.timeout * 2\n+                global_args.timeout + 30\n                 if global_args.timeout is not None\n                 else get_env_value(\n                     \"TIMEOUT\", DEFAULT_TIMEOUT + 30, int, special_none=True\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/utils_api.py",
            "diff": "diff --git a/lightrag/api/utils_api.py b/lightrag/api/utils_api.py\nindex 90a1eb96..563cc2e4 100644\n--- a/lightrag/api/utils_api.py\n+++ b/lightrag/api/utils_api.py\n@@ -201,6 +201,8 @@ def display_splash_screen(args: argparse.Namespace) -> None:\n     ASCIIColors.yellow(f\"{args.port}\")\n     ASCIIColors.white(\"    \u251c\u2500 Workers: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.workers}\")\n+    ASCIIColors.white(\"    \u251c\u2500 Timeout: \", end=\"\")\n+    ASCIIColors.yellow(f\"{args.timeout}\")\n     ASCIIColors.white(\"    \u251c\u2500 CORS Origins: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.cors_origins}\")\n     ASCIIColors.white(\"    \u251c\u2500 SSL Enabled: \", end=\"\")\n@@ -238,14 +240,10 @@ def display_splash_screen(args: argparse.Namespace) -> None:\n     ASCIIColors.yellow(f\"{args.llm_binding_host}\")\n     ASCIIColors.white(\"    \u251c\u2500 Model: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.llm_model}\")\n-    ASCIIColors.white(\"    \u251c\u2500 Temperature: \", end=\"\")\n-    ASCIIColors.yellow(f\"{args.temperature}\")\n     ASCIIColors.white(\"    \u251c\u2500 Max Async for LLM: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.max_async}\")\n-    ASCIIColors.white(\"    \u251c\u2500 Max Tokens: \", end=\"\")\n-    ASCIIColors.yellow(f\"{args.max_tokens}\")\n-    ASCIIColors.white(\"    \u251c\u2500 Timeout: \", end=\"\")\n-    ASCIIColors.yellow(f\"{args.timeout if args.timeout else 'None (infinite)'}\")\n+    ASCIIColors.white(\"    \u251c\u2500 Summary Context Size: \", end=\"\")\n+    ASCIIColors.yellow(f\"{args.summary_context_size}\")\n     ASCIIColors.white(\"    \u251c\u2500 LLM Cache Enabled: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.enable_llm_cache}\")\n     ASCIIColors.white(\"    \u2514\u2500 LLM Cache for Extraction Enabled: \", end=\"\")\n@@ -266,6 +264,8 @@ def display_splash_screen(args: argparse.Namespace) -> None:\n     ASCIIColors.magenta(\"\\n\u2699\ufe0f RAG Configuration:\")\n     ASCIIColors.white(\"    \u251c\u2500 Summary Language: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.summary_language}\")\n+    ASCIIColors.white(\"    \u251c\u2500 Entity Types: \", end=\"\")\n+    ASCIIColors.yellow(f\"{args.entity_types}\")\n     ASCIIColors.white(\"    \u251c\u2500 Max Parallel Insert: \", end=\"\")\n     ASCIIColors.yellow(f\"{args.max_parallel_insert}\")\n     ASCIIColors.white(\"    \u251c\u2500 Chunk Size: \", end=\"\")\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/webui/assets/feature-documents-BSJWpkhB.js",
            "diff": "diff --git a/lightrag/api/webui/assets/feature-documents-BSJWpkhB.js b/lightrag/api/webui/assets/feature-documents-BSJWpkhB.js\ndeleted file mode 100644\nindex 1c2c5bcf..00000000\nBinary files a/lightrag/api/webui/assets/feature-documents-BSJWpkhB.js and /dev/null differ\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/webui/assets/feature-documents-DLarjU2a.js",
            "diff": "diff --git a/lightrag/api/webui/assets/feature-documents-DLarjU2a.js b/lightrag/api/webui/assets/feature-documents-DLarjU2a.js\nnew file mode 100644\nindex 00000000..69306b08\nBinary files /dev/null and b/lightrag/api/webui/assets/feature-documents-DLarjU2a.js differ\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/webui/assets/feature-retrieval-P5Qspbob.js",
            "diff": "diff --git a/lightrag/api/webui/assets/feature-retrieval-P5Qspbob.js b/lightrag/api/webui/assets/feature-retrieval-P5Qspbob.js\nnew file mode 100644\nindex 00000000..02c9f918\nBinary files /dev/null and b/lightrag/api/webui/assets/feature-retrieval-P5Qspbob.js differ\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/webui/assets/index-DI6XUmEl.js",
            "diff": "diff --git a/lightrag/api/webui/assets/index-DI6XUmEl.js b/lightrag/api/webui/assets/index-DI6XUmEl.js\nnew file mode 100644\nindex 00000000..c47c89c5\nBinary files /dev/null and b/lightrag/api/webui/assets/index-DI6XUmEl.js differ\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/api/webui/index.html",
            "diff": "diff --git a/lightrag/api/webui/index.html b/lightrag/api/webui/index.html\nindex a1a08a0a..dc486f54 100644\nBinary files a/lightrag/api/webui/index.html and b/lightrag/api/webui/index.html differ\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/base.py",
            "diff": "diff --git a/lightrag/base.py b/lightrag/base.py\nindex dacfbd90..c5518d23 100644\n--- a/lightrag/base.py\n+++ b/lightrag/base.py\n@@ -22,7 +22,6 @@ from .constants import (\n     DEFAULT_MAX_RELATION_TOKENS,\n     DEFAULT_MAX_TOTAL_TOKENS,\n     DEFAULT_HISTORY_TURNS,\n-    DEFAULT_ENABLE_RERANK,\n     DEFAULT_OLLAMA_MODEL_NAME,\n     DEFAULT_OLLAMA_MODEL_TAG,\n     DEFAULT_OLLAMA_MODEL_SIZE,\n@@ -143,10 +142,6 @@ class QueryParam:\n     history_turns: int = int(os.getenv(\"HISTORY_TURNS\", str(DEFAULT_HISTORY_TURNS)))\n     \"\"\"Number of complete conversation turns (user-assistant pairs) to consider in the response context.\"\"\"\n \n-    # TODO: TODO: Deprecated - ID-based filtering only applies to chunks, not entities or relations, and implemented only in PostgreSQL storage\n-    ids: list[str] | None = None\n-    \"\"\"List of doc ids to filter the results.\"\"\"\n-\n     model_func: Callable[..., object] | None = None\n     \"\"\"Optional override for the LLM model function to use for this specific query.\n     If provided, this will be used instead of the global model function.\n@@ -158,9 +153,7 @@ class QueryParam:\n     If proivded, this will be use instead of the default vaulue from prompt template.\n     \"\"\"\n \n-    enable_rerank: bool = (\n-        os.getenv(\"ENABLE_RERANK\", str(DEFAULT_ENABLE_RERANK).lower()).lower() == \"true\"\n-    )\n+    enable_rerank: bool = os.getenv(\"RERANK_BY_DEFAULT\", \"true\").lower() == \"true\"\n     \"\"\"Enable reranking for retrieved text chunks. If True but no rerank model is configured, a warning will be issued.\n     Default is True to enable reranking when rerank model is available.\n     \"\"\"\n@@ -219,9 +212,16 @@ class BaseVectorStorage(StorageNameSpace, ABC):\n \n     @abstractmethod\n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n-        \"\"\"Query the vector storage and retrieve top_k results.\"\"\"\n+        \"\"\"Query the vector storage and retrieve top_k results.\n+\n+        Args:\n+            query: The query string to search for\n+            top_k: Number of top results to return\n+            query_embedding: Optional pre-computed embedding for the query.\n+                           If provided, skips embedding computation for better performance.\n+        \"\"\"\n \n     @abstractmethod\n     async def upsert(self, data: dict[str, dict[str, Any]]) -> None:\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/constants.py",
            "diff": "diff --git a/lightrag/constants.py b/lightrag/constants.py\nindex e3ed9d7f..d78d869c 100644\n--- a/lightrag/constants.py\n+++ b/lightrag/constants.py\n@@ -11,10 +11,25 @@ DEFAULT_WOKERS = 2\n DEFAULT_MAX_GRAPH_NODES = 1000\n \n # Default values for extraction settings\n-DEFAULT_SUMMARY_LANGUAGE = \"English\"  # Default language for summaries\n-DEFAULT_FORCE_LLM_SUMMARY_ON_MERGE = 4\n+DEFAULT_SUMMARY_LANGUAGE = \"English\"  # Default language for document processing\n DEFAULT_MAX_GLEANING = 1\n-DEFAULT_SUMMARY_MAX_TOKENS = 10000  # Default maximum token size\n+\n+# Number of description fragments to trigger LLM summary\n+DEFAULT_FORCE_LLM_SUMMARY_ON_MERGE = 8\n+# Max description token size to trigger LLM summary\n+DEFAULT_SUMMARY_MAX_TOKENS = 1200\n+# Recommended LLM summary output length in tokens\n+DEFAULT_SUMMARY_LENGTH_RECOMMENDED = 600\n+# Maximum token size sent to LLM for summary\n+DEFAULT_SUMMARY_CONTEXT_SIZE = 12000\n+# Default entities to extract if ENTITY_TYPES is not specified in .env\n+DEFAULT_ENTITY_TYPES = [\n+    \"organization\",\n+    \"person\",\n+    \"geo\",\n+    \"event\",\n+    \"category\",\n+]\n \n # Separator for graph fields\n GRAPH_FIELD_SEP = \"<SEP>\"\n@@ -32,8 +47,8 @@ DEFAULT_KG_CHUNK_PICK_METHOD = \"VECTOR\"\n DEFAULT_HISTORY_TURNS = 0\n \n # Rerank configuration defaults\n-DEFAULT_ENABLE_RERANK = True\n DEFAULT_MIN_RERANK_SCORE = 0.0\n+DEFAULT_RERANK_BINDING = \"null\"\n \n # File path configuration for vector and graph database(Should not be changed, used in Milvus Schema)\n DEFAULT_MAX_FILE_PATH_LENGTH = 32768\n@@ -49,8 +64,12 @@ DEFAULT_MAX_PARALLEL_INSERT = 2  # Default maximum parallel insert operations\n DEFAULT_EMBEDDING_FUNC_MAX_ASYNC = 8  # Default max async for embedding functions\n DEFAULT_EMBEDDING_BATCH_NUM = 10  # Default batch size for embedding computations\n \n-# Ollama Server Timetout in seconds\n-DEFAULT_TIMEOUT = 150\n+# Gunicorn worker timeout\n+DEFAULT_TIMEOUT = 210\n+\n+# Default llm and embedding timeout\n+DEFAULT_LLM_TIMEOUT = 180\n+DEFAULT_EMBEDDING_TIMEOUT = 30\n \n # Logging configuration defaults\n DEFAULT_LOG_MAX_BYTES = 10485760  # Default 10MB\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/exceptions.py",
            "diff": "diff --git a/lightrag/exceptions.py b/lightrag/exceptions.py\nindex ae756f85..d57df1ac 100644\n--- a/lightrag/exceptions.py\n+++ b/lightrag/exceptions.py\n@@ -58,3 +58,41 @@ class RateLimitError(APIStatusError):\n class APITimeoutError(APIConnectionError):\n     def __init__(self, request: httpx.Request) -> None:\n         super().__init__(message=\"Request timed out.\", request=request)\n+\n+\n+class StorageNotInitializedError(RuntimeError):\n+    \"\"\"Raised when storage operations are attempted before initialization.\"\"\"\n+\n+    def __init__(self, storage_type: str = \"Storage\"):\n+        super().__init__(\n+            f\"{storage_type} not initialized. Please ensure proper initialization:\\n\"\n+            f\"\\n\"\n+            f\"  rag = LightRAG(...)\\n\"\n+            f\"  await rag.initialize_storages()  # Required\\n\"\n+            f\"  \\n\"\n+            f\"  from lightrag.kg.shared_storage import initialize_pipeline_status\\n\"\n+            f\"  await initialize_pipeline_status()  # Required for pipeline operations\\n\"\n+            f\"\\n\"\n+            f\"See: https://github.com/HKUDS/LightRAG#important-initialization-requirements\"\n+        )\n+\n+\n+class PipelineNotInitializedError(KeyError):\n+    \"\"\"Raised when pipeline status is accessed before initialization.\"\"\"\n+\n+    def __init__(self, namespace: str = \"\"):\n+        msg = (\n+            f\"Pipeline namespace '{namespace}' not found. \"\n+            f\"This usually means pipeline status was not initialized.\\n\"\n+            f\"\\n\"\n+            f\"Please call 'await initialize_pipeline_status()' after initializing storages:\\n\"\n+            f\"\\n\"\n+            f\"  from lightrag.kg.shared_storage import initialize_pipeline_status\\n\"\n+            f\"  await initialize_pipeline_status()\\n\"\n+            f\"\\n\"\n+            f\"Full initialization sequence:\\n\"\n+            f\"  rag = LightRAG(...)\\n\"\n+            f\"  await rag.initialize_storages()\\n\"\n+            f\"  await initialize_pipeline_status()\"\n+        )\n+        super().__init__(msg)\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/faiss_impl.py",
            "diff": "diff --git a/lightrag/kg/faiss_impl.py b/lightrag/kg/faiss_impl.py\nindex 5687834d..7d6a6dac 100644\n--- a/lightrag/kg/faiss_impl.py\n+++ b/lightrag/kg/faiss_impl.py\n@@ -180,16 +180,20 @@ class FaissVectorDBStorage(BaseVectorStorage):\n         return [m[\"__id__\"] for m in list_data]\n \n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n         \"\"\"\n         Search by a textual query; returns top_k results with their metadata + similarity distance.\n         \"\"\"\n-        embedding = await self.embedding_func(\n-            [query], _priority=5\n-        )  # higher priority for query\n-        # embedding is shape (1, dim)\n-        embedding = np.array(embedding, dtype=np.float32)\n+        if query_embedding is not None:\n+            embedding = np.array([query_embedding], dtype=np.float32)\n+        else:\n+            embedding = await self.embedding_func(\n+                [query], _priority=5\n+            )  # higher priority for query\n+            # embedding is shape (1, dim)\n+            embedding = np.array(embedding, dtype=np.float32)\n+\n         faiss.normalize_L2(embedding)  # we do in-place normalization\n \n         # Perform the similarity search\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/json_doc_status_impl.py",
            "diff": "diff --git a/lightrag/kg/json_doc_status_impl.py b/lightrag/kg/json_doc_status_impl.py\nindex 13054cde..5464d0c3 100644\n--- a/lightrag/kg/json_doc_status_impl.py\n+++ b/lightrag/kg/json_doc_status_impl.py\n@@ -13,6 +13,7 @@ from lightrag.utils import (\n     write_json,\n     get_pinyin_sort_key,\n )\n+from lightrag.exceptions import StorageNotInitializedError\n from .shared_storage import (\n     get_namespace_data,\n     get_storage_lock,\n@@ -65,11 +66,15 @@ class JsonDocStatusStorage(DocStatusStorage):\n \n     async def filter_keys(self, keys: set[str]) -> set[str]:\n         \"\"\"Return keys that should be processed (not in storage or not successfully processed)\"\"\"\n+        if self._storage_lock is None:\n+            raise StorageNotInitializedError(\"JsonDocStatusStorage\")\n         async with self._storage_lock:\n             return set(keys) - set(self._data.keys())\n \n     async def get_by_ids(self, ids: list[str]) -> list[dict[str, Any]]:\n         result: list[dict[str, Any]] = []\n+        if self._storage_lock is None:\n+            raise StorageNotInitializedError(\"JsonDocStatusStorage\")\n         async with self._storage_lock:\n             for id in ids:\n                 data = self._data.get(id, None)\n@@ -80,6 +85,8 @@ class JsonDocStatusStorage(DocStatusStorage):\n     async def get_status_counts(self) -> dict[str, int]:\n         \"\"\"Get counts of documents in each status\"\"\"\n         counts = {status.value: 0 for status in DocStatus}\n+        if self._storage_lock is None:\n+            raise StorageNotInitializedError(\"JsonDocStatusStorage\")\n         async with self._storage_lock:\n             for doc in self._data.values():\n                 counts[doc[\"status\"]] += 1\n@@ -166,6 +173,8 @@ class JsonDocStatusStorage(DocStatusStorage):\n         logger.debug(\n             f\"[{self.workspace}] Inserting {len(data)} records to {self.namespace}\"\n         )\n+        if self._storage_lock is None:\n+            raise StorageNotInitializedError(\"JsonDocStatusStorage\")\n         async with self._storage_lock:\n             # Ensure chunks_list field exists for new documents\n             for doc_id, doc_data in data.items():\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/json_kv_impl.py",
            "diff": "diff --git a/lightrag/kg/json_kv_impl.py b/lightrag/kg/json_kv_impl.py\nindex ca3aa453..553ba417 100644\n--- a/lightrag/kg/json_kv_impl.py\n+++ b/lightrag/kg/json_kv_impl.py\n@@ -10,6 +10,7 @@ from lightrag.utils import (\n     logger,\n     write_json,\n )\n+from lightrag.exceptions import StorageNotInitializedError\n from .shared_storage import (\n     get_namespace_data,\n     get_storage_lock,\n@@ -154,6 +155,8 @@ class JsonKVStorage(BaseKVStorage):\n         logger.debug(\n             f\"[{self.workspace}] Inserting {len(data)} records to {self.namespace}\"\n         )\n+        if self._storage_lock is None:\n+            raise StorageNotInitializedError(\"JsonKVStorage\")\n         async with self._storage_lock:\n             # Add timestamps to data based on whether key exists\n             for k, v in data.items():\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/milvus_impl.py",
            "diff": "diff --git a/lightrag/kg/milvus_impl.py b/lightrag/kg/milvus_impl.py\nindex 07ce1148..f2368afe 100644\n--- a/lightrag/kg/milvus_impl.py\n+++ b/lightrag/kg/milvus_impl.py\n@@ -1047,14 +1047,18 @@ class MilvusVectorDBStorage(BaseVectorStorage):\n         return results\n \n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n         # Ensure collection is loaded before querying\n         self._ensure_collection_loaded()\n \n-        embedding = await self.embedding_func(\n-            [query], _priority=5\n-        )  # higher priority for query\n+        # Use provided embedding or compute it\n+        if query_embedding is not None:\n+            embedding = [query_embedding]  # Milvus expects a list of embeddings\n+        else:\n+            embedding = await self.embedding_func(\n+                [query], _priority=5\n+            )  # higher priority for query\n \n         # Include all meta_fields (created_at is now always included)\n         output_fields = list(self.meta_fields)\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/mongo_impl.py",
            "diff": "diff --git a/lightrag/kg/mongo_impl.py b/lightrag/kg/mongo_impl.py\nindex dd60251b..9e4d7e67 100644\n--- a/lightrag/kg/mongo_impl.py\n+++ b/lightrag/kg/mongo_impl.py\n@@ -280,6 +280,30 @@ class MongoDocStatusStorage(DocStatusStorage):\n     db: AsyncDatabase = field(default=None)\n     _data: AsyncCollection = field(default=None)\n \n+    def _prepare_doc_status_data(self, doc: dict[str, Any]) -> dict[str, Any]:\n+        \"\"\"Normalize and migrate a raw Mongo document to DocProcessingStatus-compatible dict.\"\"\"\n+        # Make a copy of the data to avoid modifying the original\n+        data = doc.copy()\n+        # Remove deprecated content field if it exists\n+        data.pop(\"content\", None)\n+        # Remove MongoDB _id field if it exists\n+        data.pop(\"_id\", None)\n+        # If file_path is not in data, use document id as file path\n+        if \"file_path\" not in data:\n+            data[\"file_path\"] = \"no-file-path\"\n+        # Ensure new fields exist with default values\n+        if \"metadata\" not in data:\n+            data[\"metadata\"] = {}\n+        if \"error_msg\" not in data:\n+            data[\"error_msg\"] = None\n+        # Backward compatibility: migrate legacy 'error' field to 'error_msg'\n+        if \"error\" in data:\n+            if \"error_msg\" not in data or data[\"error_msg\"] in (None, \"\"):\n+                data[\"error_msg\"] = data.pop(\"error\")\n+            else:\n+                data.pop(\"error\", None)\n+        return data\n+\n     def __init__(self, namespace, global_config, embedding_func, workspace=None):\n         super().__init__(\n             namespace=namespace,\n@@ -389,20 +413,7 @@ class MongoDocStatusStorage(DocStatusStorage):\n         processed_result = {}\n         for doc in result:\n             try:\n-                # Make a copy of the data to avoid modifying the original\n-                data = doc.copy()\n-                # Remove deprecated content field if it exists\n-                data.pop(\"content\", None)\n-                # Remove MongoDB _id field if it exists\n-                data.pop(\"_id\", None)\n-                # If file_path is not in data, use document id as file path\n-                if \"file_path\" not in data:\n-                    data[\"file_path\"] = \"no-file-path\"\n-                # Ensure new fields exist with default values\n-                if \"metadata\" not in data:\n-                    data[\"metadata\"] = {}\n-                if \"error_msg\" not in data:\n-                    data[\"error_msg\"] = None\n+                data = self._prepare_doc_status_data(doc)\n                 processed_result[doc[\"_id\"]] = DocProcessingStatus(**data)\n             except KeyError as e:\n                 logger.error(\n@@ -420,20 +431,7 @@ class MongoDocStatusStorage(DocStatusStorage):\n         processed_result = {}\n         for doc in result:\n             try:\n-                # Make a copy of the data to avoid modifying the original\n-                data = doc.copy()\n-                # Remove deprecated content field if it exists\n-                data.pop(\"content\", None)\n-                # Remove MongoDB _id field if it exists\n-                data.pop(\"_id\", None)\n-                # If file_path is not in data, use document id as file path\n-                if \"file_path\" not in data:\n-                    data[\"file_path\"] = \"no-file-path\"\n-                # Ensure new fields exist with default values\n-                if \"metadata\" not in data:\n-                    data[\"metadata\"] = {}\n-                if \"error_msg\" not in data:\n-                    data[\"error_msg\"] = None\n+                data = self._prepare_doc_status_data(doc)\n                 processed_result[doc[\"_id\"]] = DocProcessingStatus(**data)\n             except KeyError as e:\n                 logger.error(\n@@ -661,20 +659,7 @@ class MongoDocStatusStorage(DocStatusStorage):\n             try:\n                 doc_id = doc[\"_id\"]\n \n-                # Make a copy of the data to avoid modifying the original\n-                data = doc.copy()\n-                # Remove deprecated content field if it exists\n-                data.pop(\"content\", None)\n-                # Remove MongoDB _id field if it exists\n-                data.pop(\"_id\", None)\n-                # If file_path is not in data, use document id as file path\n-                if \"file_path\" not in data:\n-                    data[\"file_path\"] = \"no-file-path\"\n-                # Ensure new fields exist with default values\n-                if \"metadata\" not in data:\n-                    data[\"metadata\"] = {}\n-                if \"error_msg\" not in data:\n-                    data[\"error_msg\"] = None\n+                data = self._prepare_doc_status_data(doc)\n \n                 doc_status = DocProcessingStatus(**data)\n                 documents.append((doc_id, doc_status))\n@@ -1825,16 +1810,22 @@ class MongoVectorDBStorage(BaseVectorStorage):\n         return list_data\n \n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n         \"\"\"Queries the vector database using Atlas Vector Search.\"\"\"\n-        # Generate the embedding\n-        embedding = await self.embedding_func(\n-            [query], _priority=5\n-        )  # higher priority for query\n-\n-        # Convert numpy array to a list to ensure compatibility with MongoDB\n-        query_vector = embedding[0].tolist()\n+        if query_embedding is not None:\n+            # Convert numpy array to list if needed for MongoDB compatibility\n+            if hasattr(query_embedding, \"tolist\"):\n+                query_vector = query_embedding.tolist()\n+            else:\n+                query_vector = list(query_embedding)\n+        else:\n+            # Generate the embedding\n+            embedding = await self.embedding_func(\n+                [query], _priority=5\n+            )  # higher priority for query\n+            # Convert numpy array to a list to ensure compatibility with MongoDB\n+            query_vector = embedding[0].tolist()\n \n         # Define the aggregation pipeline with the converted query vector\n         pipeline = [\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/nano_vector_db_impl.py",
            "diff": "diff --git a/lightrag/kg/nano_vector_db_impl.py b/lightrag/kg/nano_vector_db_impl.py\nindex 19352a4a..def5a83d 100644\n--- a/lightrag/kg/nano_vector_db_impl.py\n+++ b/lightrag/kg/nano_vector_db_impl.py\n@@ -137,13 +137,17 @@ class NanoVectorDBStorage(BaseVectorStorage):\n             )\n \n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n-        # Execute embedding outside of lock to avoid improve cocurrent\n-        embedding = await self.embedding_func(\n-            [query], _priority=5\n-        )  # higher priority for query\n-        embedding = embedding[0]\n+        # Use provided embedding or compute it\n+        if query_embedding is not None:\n+            embedding = query_embedding\n+        else:\n+            # Execute embedding outside of lock to avoid improve cocurrent\n+            embedding = await self.embedding_func(\n+                [query], _priority=5\n+            )  # higher priority for query\n+            embedding = embedding[0]\n \n         client = await self._get_client()\n         results = client.query(\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/postgres_impl.py",
            "diff": "diff --git a/lightrag/kg/postgres_impl.py b/lightrag/kg/postgres_impl.py\nindex dd1ff8ae..55cc6e06 100644\n--- a/lightrag/kg/postgres_impl.py\n+++ b/lightrag/kg/postgres_impl.py\n@@ -2006,14 +2006,18 @@ class PGVectorStorage(BaseVectorStorage):\n \n     #################### query method ###############\n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n-        embeddings = await self.embedding_func(\n-            [query], _priority=5\n-        )  # higher priority for query\n-        embedding = embeddings[0]\n+        if query_embedding is not None:\n+            embedding = query_embedding\n+        else:\n+            embeddings = await self.embedding_func(\n+                [query], _priority=5\n+            )  # higher priority for query\n+            embedding = embeddings[0]\n+\n         embedding_string = \",\".join(map(str, embedding))\n-        # Use parameterized document IDs (None means search across all documents)\n+\n         sql = SQL_TEMPLATES[self.namespace].format(embedding_string=embedding_string)\n         params = {\n             \"workspace\": self.workspace,\n@@ -4562,31 +4566,34 @@ SQL_TEMPLATES = {\n                       update_time = EXCLUDED.update_time\n                      \"\"\",\n     \"relationships\": \"\"\"\n-                SELECT r.source_id as src_id, r.target_id as tgt_id,\n-                       EXTRACT(EPOCH FROM r.create_time)::BIGINT as created_at\n-                FROM LIGHTRAG_VDB_RELATION r\n-                WHERE r.workspace = $1\n-                  AND r.content_vector <=> '[{embedding_string}]'::vector < $2\n-                ORDER BY r.content_vector <=> '[{embedding_string}]'::vector\n-                LIMIT $3\n+                     SELECT r.source_id AS src_id,\n+                            r.target_id AS tgt_id,\n+                            EXTRACT(EPOCH FROM r.create_time)::BIGINT AS created_at\n+                     FROM LIGHTRAG_VDB_RELATION r\n+                     WHERE r.workspace = $1\n+                       AND r.content_vector <=> '[{embedding_string}]'::vector < $2\n+                     ORDER BY r.content_vector <=> '[{embedding_string}]'::vector\n+                     LIMIT $3;\n                      \"\"\",\n     \"entities\": \"\"\"\n                 SELECT e.entity_name,\n-                       EXTRACT(EPOCH FROM e.create_time)::BIGINT as created_at\n+                       EXTRACT(EPOCH FROM e.create_time)::BIGINT AS created_at\n                 FROM LIGHTRAG_VDB_ENTITY e\n                 WHERE e.workspace = $1\n                   AND e.content_vector <=> '[{embedding_string}]'::vector < $2\n                 ORDER BY e.content_vector <=> '[{embedding_string}]'::vector\n-                LIMIT $3\n+                LIMIT $3;\n                 \"\"\",\n     \"chunks\": \"\"\"\n-                SELECT c.id, c.content, c.file_path,\n-                       EXTRACT(EPOCH FROM c.create_time)::BIGINT as c.created_at\n-                FROM LIGHTRAG_VDB_CHUNKS c\n-                WHERE c.workspace = $1\n-                  AND c.content_vector <=> '[{embedding_string}]'::vector < $2\n-                ORDER BY c.content_vector <=> '[{embedding_string}]'::vector\n-                LIMIT $3\n+              SELECT c.id,\n+                     c.content,\n+                     c.file_path,\n+                     EXTRACT(EPOCH FROM c.create_time)::BIGINT AS created_at\n+              FROM LIGHTRAG_VDB_CHUNKS c\n+              WHERE c.workspace = $1\n+                AND c.content_vector <=> '[{embedding_string}]'::vector < $2\n+              ORDER BY c.content_vector <=> '[{embedding_string}]'::vector\n+              LIMIT $3;\n               \"\"\",\n     # DROP tables\n     \"drop_specifiy_table_workspace\": \"\"\"\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/qdrant_impl.py",
            "diff": "diff --git a/lightrag/kg/qdrant_impl.py b/lightrag/kg/qdrant_impl.py\nindex e8565ac7..dad95bbc 100644\n--- a/lightrag/kg/qdrant_impl.py\n+++ b/lightrag/kg/qdrant_impl.py\n@@ -200,14 +200,19 @@ class QdrantVectorDBStorage(BaseVectorStorage):\n         return results\n \n     async def query(\n-        self, query: str, top_k: int\n+        self, query: str, top_k: int, query_embedding: list[float] = None\n     ) -> list[dict[str, Any]]:\n-        embedding = await self.embedding_func(\n-            [query], _priority=5\n-        )  # higher priority for query\n+        if query_embedding is not None:\n+            embedding = query_embedding\n+        else:\n+            embedding_result = await self.embedding_func(\n+                [query], _priority=5\n+            )  # higher priority for query\n+            embedding = embedding_result[0]\n+\n         results = self._client.search(\n             collection_name=self.final_namespace,\n-            query_vector=embedding[0],\n+            query_vector=embedding,\n             limit=top_k,\n             with_payload=True,\n             score_threshold=self.cosine_better_than_threshold,\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/kg/shared_storage.py",
            "diff": "diff --git a/lightrag/kg/shared_storage.py b/lightrag/kg/shared_storage.py\nindex 228bf272..e20dce52 100644\n--- a/lightrag/kg/shared_storage.py\n+++ b/lightrag/kg/shared_storage.py\n@@ -8,6 +8,8 @@ import time\n import logging\n from typing import Any, Dict, List, Optional, Union, TypeVar, Generic\n \n+from lightrag.exceptions import PipelineNotInitializedError\n+\n \n # Define a direct print function for critical logs that must be visible in all processes\n def direct_log(message, enable_output: bool = False, level: str = \"DEBUG\"):\n@@ -1057,7 +1059,7 @@ async def initialize_pipeline_status():\n     Initialize pipeline namespace with default values.\n     This function is called during FASTAPI lifespan for each worker.\n     \"\"\"\n-    pipeline_namespace = await get_namespace_data(\"pipeline_status\")\n+    pipeline_namespace = await get_namespace_data(\"pipeline_status\", first_init=True)\n \n     async with get_internal_lock():\n         # Check if already initialized by checking for required fields\n@@ -1192,8 +1194,16 @@ async def try_initialize_namespace(namespace: str) -> bool:\n     return False\n \n \n-async def get_namespace_data(namespace: str) -> Dict[str, Any]:\n-    \"\"\"get the shared data reference for specific namespace\"\"\"\n+async def get_namespace_data(\n+    namespace: str, first_init: bool = False\n+) -> Dict[str, Any]:\n+    \"\"\"get the shared data reference for specific namespace\n+\n+    Args:\n+        namespace: The namespace to retrieve\n+        allow_create: If True, allows creation of the namespace if it doesn't exist.\n+                     Used internally by initialize_pipeline_status().\n+    \"\"\"\n     if _shared_dicts is None:\n         direct_log(\n             f\"Error: try to getnanmespace before it is initialized, pid={os.getpid()}\",\n@@ -1203,6 +1213,13 @@ async def get_namespace_data(namespace: str) -> Dict[str, Any]:\n \n     async with get_internal_lock():\n         if namespace not in _shared_dicts:\n+            # Special handling for pipeline_status namespace\n+            if namespace == \"pipeline_status\" and not first_init:\n+                # Check if pipeline_status should have been initialized but wasn't\n+                # This helps users understand they need to call initialize_pipeline_status()\n+                raise PipelineNotInitializedError(namespace)\n+\n+            # For other namespaces or when allow_create=True, create them dynamically\n             if _is_multiprocess and _manager is not None:\n                 _shared_dicts[namespace] = _manager.dict()\n             else:\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/lightrag.py",
            "diff": "diff --git a/lightrag/lightrag.py b/lightrag/lightrag.py\nindex 924c423e..23e6f575 100644\n--- a/lightrag/lightrag.py\n+++ b/lightrag/lightrag.py\n@@ -34,9 +34,15 @@ from lightrag.constants import (\n     DEFAULT_KG_CHUNK_PICK_METHOD,\n     DEFAULT_MIN_RERANK_SCORE,\n     DEFAULT_SUMMARY_MAX_TOKENS,\n+    DEFAULT_SUMMARY_CONTEXT_SIZE,\n+    DEFAULT_SUMMARY_LENGTH_RECOMMENDED,\n     DEFAULT_MAX_ASYNC,\n     DEFAULT_MAX_PARALLEL_INSERT,\n     DEFAULT_MAX_GRAPH_NODES,\n+    DEFAULT_ENTITY_TYPES,\n+    DEFAULT_SUMMARY_LANGUAGE,\n+    DEFAULT_LLM_TIMEOUT,\n+    DEFAULT_EMBEDDING_TIMEOUT,\n )\n from lightrag.utils import get_env_value\n \n@@ -273,6 +279,10 @@ class LightRAG:\n     - use_llm_check: If True, validates cached embeddings using an LLM.\n     \"\"\"\n \n+    default_embedding_timeout: int = field(\n+        default=int(os.getenv(\"EMBEDDING_TIMEOUT\", DEFAULT_EMBEDDING_TIMEOUT))\n+    )\n+\n     # LLM Configuration\n     # ---\n \n@@ -283,10 +293,22 @@ class LightRAG:\n     \"\"\"Name of the LLM model used for generating responses.\"\"\"\n \n     summary_max_tokens: int = field(\n-        default=int(os.getenv(\"MAX_TOKENS\", DEFAULT_SUMMARY_MAX_TOKENS))\n+        default=int(os.getenv(\"SUMMARY_MAX_TOKENS\", DEFAULT_SUMMARY_MAX_TOKENS))\n+    )\n+    \"\"\"Maximum tokens allowed for entity/relation description.\"\"\"\n+\n+    summary_context_size: int = field(\n+        default=int(os.getenv(\"SUMMARY_CONTEXT_SIZE\", DEFAULT_SUMMARY_CONTEXT_SIZE))\n     )\n     \"\"\"Maximum number of tokens allowed per LLM response.\"\"\"\n \n+    summary_length_recommended: int = field(\n+        default=int(\n+            os.getenv(\"SUMMARY_LENGTH_RECOMMENDED\", DEFAULT_SUMMARY_LENGTH_RECOMMENDED)\n+        )\n+    )\n+    \"\"\"Recommended length of LLM summary output.\"\"\"\n+\n     llm_model_max_async: int = field(\n         default=int(os.getenv(\"MAX_ASYNC\", DEFAULT_MAX_ASYNC))\n     )\n@@ -295,6 +317,10 @@ class LightRAG:\n     llm_model_kwargs: dict[str, Any] = field(default_factory=dict)\n     \"\"\"Additional keyword arguments passed to the LLM model function.\"\"\"\n \n+    default_llm_timeout: int = field(\n+        default=int(os.getenv(\"LLM_TIMEOUT\", DEFAULT_LLM_TIMEOUT))\n+    )\n+\n     # Rerank Configuration\n     # ---\n \n@@ -333,7 +359,10 @@ class LightRAG:\n \n     addon_params: dict[str, Any] = field(\n         default_factory=lambda: {\n-            \"language\": get_env_value(\"SUMMARY_LANGUAGE\", \"English\", str)\n+            \"language\": get_env_value(\n+                \"SUMMARY_LANGUAGE\", DEFAULT_SUMMARY_LANGUAGE, str\n+            ),\n+            \"entity_types\": get_env_value(\"ENTITY_TYPES\", DEFAULT_ENTITY_TYPES, list),\n         }\n     )\n \n@@ -416,6 +445,20 @@ class LightRAG:\n         if self.ollama_server_infos is None:\n             self.ollama_server_infos = OllamaServerInfos()\n \n+        # Validate config\n+        if self.force_llm_summary_on_merge < 3:\n+            logger.warning(\n+                f\"force_llm_summary_on_merge should be at least 3, got {self.force_llm_summary_on_merge}\"\n+            )\n+        if self.summary_context_size > self.max_total_tokens:\n+            logger.warning(\n+                f\"summary_context_size({self.summary_context_size}) should no greater than max_total_tokens({self.max_total_tokens})\"\n+            )\n+        if self.summary_length_recommended > self.summary_max_tokens:\n+            logger.warning(\n+                f\"max_total_tokens({self.summary_max_tokens}) should greater than summary_length_recommended({self.summary_length_recommended})\"\n+            )\n+\n         # Fix global_config now\n         global_config = asdict(self)\n \n@@ -424,7 +467,8 @@ class LightRAG:\n \n         # Init Embedding\n         self.embedding_func = priority_limit_async_func_call(\n-            self.embedding_func_max_async\n+            self.embedding_func_max_async,\n+            llm_timeout=self.default_embedding_timeout,\n         )(self.embedding_func)\n \n         # Initialize all storages\n@@ -517,7 +561,11 @@ class LightRAG:\n         # Directly use llm_response_cache, don't create a new object\n         hashing_kv = self.llm_response_cache\n \n-        self.llm_model_func = priority_limit_async_func_call(self.llm_model_max_async)(\n+        # Get timeout from LLM model kwargs for dynamic timeout calculation\n+        self.llm_model_func = priority_limit_async_func_call(\n+            self.llm_model_max_async,\n+            llm_timeout=self.default_llm_timeout,\n+        )(\n             partial(\n                 self.llm_model_func,  # type: ignore\n                 hashing_kv=hashing_kv,\n@@ -525,14 +573,6 @@ class LightRAG:\n             )\n         )\n \n-        # Init Rerank\n-        if self.rerank_model_func:\n-            logger.info(\"Rerank model initialized for improved retrieval quality\")\n-        else:\n-            logger.warning(\n-                \"Rerank is enabled but no rerank_model_func provided. Reranking will be skipped.\"\n-            )\n-\n         self._storages_status = StoragesStatus.CREATED\n \n     async def initialize_storages(self):\n@@ -2280,117 +2320,111 @@ class LightRAG:\n             relationships_to_delete = set()\n             relationships_to_rebuild = {}  # (src, tgt) -> remaining_chunk_ids\n \n-            # Use graph database lock to ensure atomic merges and updates\n-            graph_db_lock = get_graph_db_lock(enable_logging=False)\n-            async with graph_db_lock:\n-                try:\n-                    # Get affected entities and relations from full_entities and full_relations storage\n-                    doc_entities_data = await self.full_entities.get_by_id(doc_id)\n-                    doc_relations_data = await self.full_relations.get_by_id(doc_id)\n-\n-                    affected_nodes = []\n-                    affected_edges = []\n-\n-                    # Get entity data from graph storage using entity names from full_entities\n-                    if doc_entities_data and \"entity_names\" in doc_entities_data:\n-                        entity_names = doc_entities_data[\"entity_names\"]\n-                        # get_nodes_batch returns dict[str, dict], need to convert to list[dict]\n-                        nodes_dict = (\n-                            await self.chunk_entity_relation_graph.get_nodes_batch(\n-                                entity_names\n-                            )\n-                        )\n-                        for entity_name in entity_names:\n-                            node_data = nodes_dict.get(entity_name)\n-                            if node_data:\n-                                # Ensure compatibility with existing logic that expects \"id\" field\n-                                if \"id\" not in node_data:\n-                                    node_data[\"id\"] = entity_name\n-                                affected_nodes.append(node_data)\n-\n-                    # Get relation data from graph storage using relation pairs from full_relations\n-                    if doc_relations_data and \"relation_pairs\" in doc_relations_data:\n-                        relation_pairs = doc_relations_data[\"relation_pairs\"]\n-                        edge_pairs_dicts = [\n-                            {\"src\": pair[0], \"tgt\": pair[1]} for pair in relation_pairs\n-                        ]\n-                        # get_edges_batch returns dict[tuple[str, str], dict], need to convert to list[dict]\n-                        edges_dict = (\n-                            await self.chunk_entity_relation_graph.get_edges_batch(\n-                                edge_pairs_dicts\n-                            )\n-                        )\n+            try:\n+                # Get affected entities and relations from full_entities and full_relations storage\n+                doc_entities_data = await self.full_entities.get_by_id(doc_id)\n+                doc_relations_data = await self.full_relations.get_by_id(doc_id)\n+\n+                affected_nodes = []\n+                affected_edges = []\n+\n+                # Get entity data from graph storage using entity names from full_entities\n+                if doc_entities_data and \"entity_names\" in doc_entities_data:\n+                    entity_names = doc_entities_data[\"entity_names\"]\n+                    # get_nodes_batch returns dict[str, dict], need to convert to list[dict]\n+                    nodes_dict = await self.chunk_entity_relation_graph.get_nodes_batch(\n+                        entity_names\n+                    )\n+                    for entity_name in entity_names:\n+                        node_data = nodes_dict.get(entity_name)\n+                        if node_data:\n+                            # Ensure compatibility with existing logic that expects \"id\" field\n+                            if \"id\" not in node_data:\n+                                node_data[\"id\"] = entity_name\n+                            affected_nodes.append(node_data)\n+\n+                # Get relation data from graph storage using relation pairs from full_relations\n+                if doc_relations_data and \"relation_pairs\" in doc_relations_data:\n+                    relation_pairs = doc_relations_data[\"relation_pairs\"]\n+                    edge_pairs_dicts = [\n+                        {\"src\": pair[0], \"tgt\": pair[1]} for pair in relation_pairs\n+                    ]\n+                    # get_edges_batch returns dict[tuple[str, str], dict], need to convert to list[dict]\n+                    edges_dict = await self.chunk_entity_relation_graph.get_edges_batch(\n+                        edge_pairs_dicts\n+                    )\n \n-                        for pair in relation_pairs:\n-                            src, tgt = pair[0], pair[1]\n-                            edge_key = (src, tgt)\n-                            edge_data = edges_dict.get(edge_key)\n-                            if edge_data:\n-                                # Ensure compatibility with existing logic that expects \"source\" and \"target\" fields\n-                                if \"source\" not in edge_data:\n-                                    edge_data[\"source\"] = src\n-                                if \"target\" not in edge_data:\n-                                    edge_data[\"target\"] = tgt\n-                                affected_edges.append(edge_data)\n+                    for pair in relation_pairs:\n+                        src, tgt = pair[0], pair[1]\n+                        edge_key = (src, tgt)\n+                        edge_data = edges_dict.get(edge_key)\n+                        if edge_data:\n+                            # Ensure compatibility with existing logic that expects \"source\" and \"target\" fields\n+                            if \"source\" not in edge_data:\n+                                edge_data[\"source\"] = src\n+                            if \"target\" not in edge_data:\n+                                edge_data[\"target\"] = tgt\n+                            affected_edges.append(edge_data)\n \n-                except Exception as e:\n-                    logger.error(f\"Failed to analyze affected graph elements: {e}\")\n-                    raise Exception(f\"Failed to analyze graph dependencies: {e}\") from e\n+            except Exception as e:\n+                logger.error(f\"Failed to analyze affected graph elements: {e}\")\n+                raise Exception(f\"Failed to analyze graph dependencies: {e}\") from e\n \n-                try:\n-                    # Process entities\n-                    for node_data in affected_nodes:\n-                        node_label = node_data.get(\"entity_id\")\n-                        if node_label and \"source_id\" in node_data:\n-                            sources = set(node_data[\"source_id\"].split(GRAPH_FIELD_SEP))\n-                            remaining_sources = sources - chunk_ids\n-\n-                            if not remaining_sources:\n-                                entities_to_delete.add(node_label)\n-                            elif remaining_sources != sources:\n-                                entities_to_rebuild[node_label] = remaining_sources\n+            try:\n+                # Process entities\n+                for node_data in affected_nodes:\n+                    node_label = node_data.get(\"entity_id\")\n+                    if node_label and \"source_id\" in node_data:\n+                        sources = set(node_data[\"source_id\"].split(GRAPH_FIELD_SEP))\n+                        remaining_sources = sources - chunk_ids\n+\n+                        if not remaining_sources:\n+                            entities_to_delete.add(node_label)\n+                        elif remaining_sources != sources:\n+                            entities_to_rebuild[node_label] = remaining_sources\n \n-                    async with pipeline_status_lock:\n-                        log_message = (\n-                            f\"Found {len(entities_to_rebuild)} affected entities\"\n-                        )\n-                        logger.info(log_message)\n-                        pipeline_status[\"latest_message\"] = log_message\n-                        pipeline_status[\"history_messages\"].append(log_message)\n+                async with pipeline_status_lock:\n+                    log_message = f\"Found {len(entities_to_rebuild)} affected entities\"\n+                    logger.info(log_message)\n+                    pipeline_status[\"latest_message\"] = log_message\n+                    pipeline_status[\"history_messages\"].append(log_message)\n \n-                    # Process relationships\n-                    for edge_data in affected_edges:\n-                        src = edge_data.get(\"source\")\n-                        tgt = edge_data.get(\"target\")\n+                # Process relationships\n+                for edge_data in affected_edges:\n+                    src = edge_data.get(\"source\")\n+                    tgt = edge_data.get(\"target\")\n \n-                        if src and tgt and \"source_id\" in edge_data:\n-                            edge_tuple = tuple(sorted((src, tgt)))\n-                            if (\n-                                edge_tuple in relationships_to_delete\n-                                or edge_tuple in relationships_to_rebuild\n-                            ):\n-                                continue\n+                    if src and tgt and \"source_id\" in edge_data:\n+                        edge_tuple = tuple(sorted((src, tgt)))\n+                        if (\n+                            edge_tuple in relationships_to_delete\n+                            or edge_tuple in relationships_to_rebuild\n+                        ):\n+                            continue\n \n-                            sources = set(edge_data[\"source_id\"].split(GRAPH_FIELD_SEP))\n-                            remaining_sources = sources - chunk_ids\n+                        sources = set(edge_data[\"source_id\"].split(GRAPH_FIELD_SEP))\n+                        remaining_sources = sources - chunk_ids\n \n-                            if not remaining_sources:\n-                                relationships_to_delete.add(edge_tuple)\n-                            elif remaining_sources != sources:\n-                                relationships_to_rebuild[edge_tuple] = remaining_sources\n+                        if not remaining_sources:\n+                            relationships_to_delete.add(edge_tuple)\n+                        elif remaining_sources != sources:\n+                            relationships_to_rebuild[edge_tuple] = remaining_sources\n \n-                    async with pipeline_status_lock:\n-                        log_message = (\n-                            f\"Found {len(relationships_to_rebuild)} affected relations\"\n-                        )\n-                        logger.info(log_message)\n-                        pipeline_status[\"latest_message\"] = log_message\n-                        pipeline_status[\"history_messages\"].append(log_message)\n+                async with pipeline_status_lock:\n+                    log_message = (\n+                        f\"Found {len(relationships_to_rebuild)} affected relations\"\n+                    )\n+                    logger.info(log_message)\n+                    pipeline_status[\"latest_message\"] = log_message\n+                    pipeline_status[\"history_messages\"].append(log_message)\n \n-                except Exception as e:\n-                    logger.error(f\"Failed to process graph analysis results: {e}\")\n-                    raise Exception(f\"Failed to process graph dependencies: {e}\") from e\n+            except Exception as e:\n+                logger.error(f\"Failed to process graph analysis results: {e}\")\n+                raise Exception(f\"Failed to process graph dependencies: {e}\") from e\n \n+            # Use graph database lock to prevent dirty read\n+            graph_db_lock = get_graph_db_lock(enable_logging=False)\n+            async with graph_db_lock:\n                 # 5. Delete chunks from storage\n                 if chunk_ids:\n                     try:\n@@ -2461,27 +2495,28 @@ class LightRAG:\n                         logger.error(f\"Failed to delete relationships: {e}\")\n                         raise Exception(f\"Failed to delete relationships: {e}\") from e\n \n-                # 8. Rebuild entities and relationships from remaining chunks\n-                if entities_to_rebuild or relationships_to_rebuild:\n-                    try:\n-                        await _rebuild_knowledge_from_chunks(\n-                            entities_to_rebuild=entities_to_rebuild,\n-                            relationships_to_rebuild=relationships_to_rebuild,\n-                            knowledge_graph_inst=self.chunk_entity_relation_graph,\n-                            entities_vdb=self.entities_vdb,\n-                            relationships_vdb=self.relationships_vdb,\n-                            text_chunks_storage=self.text_chunks,\n-                            llm_response_cache=self.llm_response_cache,\n-                            global_config=asdict(self),\n-                            pipeline_status=pipeline_status,\n-                            pipeline_status_lock=pipeline_status_lock,\n-                        )\n+                # Persist changes to graph database before releasing graph database lock\n+                await self._insert_done()\n \n-                    except Exception as e:\n-                        logger.error(f\"Failed to rebuild knowledge from chunks: {e}\")\n-                        raise Exception(\n-                            f\"Failed to rebuild knowledge graph: {e}\"\n-                        ) from e\n+            # 8. Rebuild entities and relationships from remaining chunks\n+            if entities_to_rebuild or relationships_to_rebuild:\n+                try:\n+                    await _rebuild_knowledge_from_chunks(\n+                        entities_to_rebuild=entities_to_rebuild,\n+                        relationships_to_rebuild=relationships_to_rebuild,\n+                        knowledge_graph_inst=self.chunk_entity_relation_graph,\n+                        entities_vdb=self.entities_vdb,\n+                        relationships_vdb=self.relationships_vdb,\n+                        text_chunks_storage=self.text_chunks,\n+                        llm_response_cache=self.llm_response_cache,\n+                        global_config=asdict(self),\n+                        pipeline_status=pipeline_status,\n+                        pipeline_status_lock=pipeline_status_lock,\n+                    )\n+\n+                except Exception as e:\n+                    logger.error(f\"Failed to rebuild knowledge from chunks: {e}\")\n+                    raise Exception(f\"Failed to rebuild knowledge graph: {e}\") from e\n \n             # 9. Delete from full_entities and full_relations storage\n             try:\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm.py",
            "diff": "diff --git a/lightrag/llm.py b/lightrag/llm.py\ndeleted file mode 100644\nindex e5f98cf8..00000000\n--- a/lightrag/llm.py\n+++ /dev/null\n@@ -1,101 +0,0 @@\n-from __future__ import annotations\n-\n-from typing import Callable, Any\n-from pydantic import BaseModel, Field\n-\n-\n-class Model(BaseModel):\n-    \"\"\"\n-    This is a Pydantic model class named 'Model' that is used to define a custom language model.\n-\n-    Attributes:\n-        gen_func (Callable[[Any], str]): A callable function that generates the response from the language model.\n-            The function should take any argument and return a string.\n-        kwargs (Dict[str, Any]): A dictionary that contains the arguments to pass to the callable function.\n-            This could include parameters such as the model name, API key, etc.\n-\n-    Example usage:\n-        Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY_1\"]})\n-\n-    In this example, 'openai_complete_if_cache' is the callable function that generates the response from the OpenAI model.\n-    The 'kwargs' dictionary contains the model name and API key to be passed to the function.\n-    \"\"\"\n-\n-    gen_func: Callable[[Any], str] = Field(\n-        ...,\n-        description=\"A function that generates the response from the llm. The response must be a string\",\n-    )\n-    kwargs: dict[str, Any] = Field(\n-        ...,\n-        description=\"The arguments to pass to the callable function. Eg. the api key, model name, etc\",\n-    )\n-\n-    class Config:\n-        arbitrary_types_allowed = True\n-\n-\n-class MultiModel:\n-    \"\"\"\n-    Distributes the load across multiple language models. Useful for circumventing low rate limits with certain api providers especially if you are on the free tier.\n-    Could also be used for spliting across diffrent models or providers.\n-\n-    Attributes:\n-        models (List[Model]): A list of language models to be used.\n-\n-    Usage example:\n-        ```python\n-        models = [\n-            Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY_1\"]}),\n-            Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY_2\"]}),\n-            Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY_3\"]}),\n-            Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY_4\"]}),\n-            Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY_5\"]}),\n-        ]\n-        multi_model = MultiModel(models)\n-        rag = LightRAG(\n-            llm_model_func=multi_model.llm_model_func\n-            / ..other args\n-            )\n-        ```\n-    \"\"\"\n-\n-    def __init__(self, models: list[Model]):\n-        self._models = models\n-        self._current_model = 0\n-\n-    def _next_model(self):\n-        self._current_model = (self._current_model + 1) % len(self._models)\n-        return self._models[self._current_model]\n-\n-    async def llm_model_func(\n-        self,\n-        prompt: str,\n-        system_prompt: str | None = None,\n-        history_messages: list[dict[str, Any]] = [],\n-        **kwargs: Any,\n-    ) -> str:\n-        kwargs.pop(\"model\", None)  # stop from overwriting the custom model name\n-        kwargs.pop(\"keyword_extraction\", None)\n-        kwargs.pop(\"mode\", None)\n-        next_model = self._next_model()\n-        args = dict(\n-            prompt=prompt,\n-            system_prompt=system_prompt,\n-            history_messages=history_messages,\n-            **kwargs,\n-            **next_model.kwargs,\n-        )\n-\n-        return await next_model.gen_func(**args)\n-\n-\n-if __name__ == \"__main__\":\n-    import asyncio\n-\n-    async def main():\n-        from lightrag.llm.openai import gpt_4o_mini_complete\n-\n-        result = await gpt_4o_mini_complete(\"How are you?\")\n-        print(result)\n-\n-    asyncio.run(main())\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm/anthropic.py",
            "diff": "diff --git a/lightrag/llm/anthropic.py b/lightrag/llm/anthropic.py\nindex 7878c8f0..98a997d5 100644\n--- a/lightrag/llm/anthropic.py\n+++ b/lightrag/llm/anthropic.py\n@@ -77,14 +77,23 @@ async def anthropic_complete_if_cache(\n     if not VERBOSE_DEBUG and logger.level == logging.DEBUG:\n         logging.getLogger(\"anthropic\").setLevel(logging.INFO)\n \n+    kwargs.pop(\"hashing_kv\", None)\n+    kwargs.pop(\"keyword_extraction\", None)\n+    timeout = kwargs.pop(\"timeout\", None)\n+\n     anthropic_async_client = (\n-        AsyncAnthropic(default_headers=default_headers, api_key=api_key)\n+        AsyncAnthropic(\n+            default_headers=default_headers, api_key=api_key, timeout=timeout\n+        )\n         if base_url is None\n         else AsyncAnthropic(\n-            base_url=base_url, default_headers=default_headers, api_key=api_key\n+            base_url=base_url,\n+            default_headers=default_headers,\n+            api_key=api_key,\n+            timeout=timeout,\n         )\n     )\n-    kwargs.pop(\"hashing_kv\", None)\n+\n     messages: list[dict[str, Any]] = []\n     if system_prompt:\n         messages.append({\"role\": \"system\", \"content\": system_prompt})\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm/azure_openai.py",
            "diff": "diff --git a/lightrag/llm/azure_openai.py b/lightrag/llm/azure_openai.py\nindex 60d2c18e..0ede0824 100644\n--- a/lightrag/llm/azure_openai.py\n+++ b/lightrag/llm/azure_openai.py\n@@ -59,13 +59,17 @@ async def azure_openai_complete_if_cache(\n         or os.getenv(\"OPENAI_API_VERSION\")\n     )\n \n+    kwargs.pop(\"hashing_kv\", None)\n+    kwargs.pop(\"keyword_extraction\", None)\n+    timeout = kwargs.pop(\"timeout\", None)\n+\n     openai_async_client = AsyncAzureOpenAI(\n         azure_endpoint=base_url,\n         azure_deployment=deployment,\n         api_key=api_key,\n         api_version=api_version,\n+        timeout=timeout,\n     )\n-    kwargs.pop(\"hashing_kv\", None)\n     messages = []\n     if system_prompt:\n         messages.append({\"role\": \"system\", \"content\": system_prompt})\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm/binding_options.py",
            "diff": "diff --git a/lightrag/llm/binding_options.py b/lightrag/llm/binding_options.py\nindex 827620ee..c2f2c9d7 100644\n--- a/lightrag/llm/binding_options.py\n+++ b/lightrag/llm/binding_options.py\n@@ -99,7 +99,7 @@ class BindingOptions:\n         group = parser.add_argument_group(f\"{cls._binding_name} binding options\")\n         for arg_item in cls.args_env_name_type_value():\n             # Handle JSON parsing for list types\n-            if arg_item[\"type\"] == List[str]:\n+            if arg_item[\"type\"] is List[str]:\n \n                 def json_list_parser(value):\n                     try:\n@@ -126,6 +126,34 @@ class BindingOptions:\n                     default=env_value,\n                     help=arg_item[\"help\"],\n                 )\n+            # Handle JSON parsing for dict types\n+            elif arg_item[\"type\"] is dict:\n+\n+                def json_dict_parser(value):\n+                    try:\n+                        parsed = json.loads(value)\n+                        if not isinstance(parsed, dict):\n+                            raise argparse.ArgumentTypeError(\n+                                f\"Expected JSON object, got {type(parsed).__name__}\"\n+                            )\n+                        return parsed\n+                    except json.JSONDecodeError as e:\n+                        raise argparse.ArgumentTypeError(f\"Invalid JSON: {e}\")\n+\n+                # Get environment variable with JSON parsing\n+                env_value = get_env_value(f\"{arg_item['env_name']}\", argparse.SUPPRESS)\n+                if env_value is not argparse.SUPPRESS:\n+                    try:\n+                        env_value = json_dict_parser(env_value)\n+                    except argparse.ArgumentTypeError:\n+                        env_value = argparse.SUPPRESS\n+\n+                group.add_argument(\n+                    f\"--{arg_item['argname']}\",\n+                    type=json_dict_parser,\n+                    default=env_value,\n+                    help=arg_item[\"help\"],\n+                )\n             else:\n                 group.add_argument(\n                     f\"--{arg_item['argname']}\",\n@@ -234,8 +262,8 @@ class BindingOptions:\n                 if arg_item[\"help\"]:\n                     sample_stream.write(f\"# {arg_item['help']}\\n\")\n \n-                # Handle JSON formatting for list types\n-                if arg_item[\"type\"] == List[str]:\n+                # Handle JSON formatting for list and dict types\n+                if arg_item[\"type\"] is List[str] or arg_item[\"type\"] is dict:\n                     default_value = json.dumps(arg_item[\"default\"])\n                 else:\n                     default_value = arg_item[\"default\"]\n@@ -431,6 +459,8 @@ class OpenAILLMOptions(BindingOptions):\n     stop: List[str] = field(default_factory=list)  # Stop sequences\n     temperature: float = DEFAULT_TEMPERATURE  # Controls randomness (0.0 to 2.0)\n     top_p: float = 1.0  # Nucleus sampling parameter (0.0 to 1.0)\n+    max_tokens: int = None  # Maximum number of tokens to generate(deprecated, use max_completion_tokens instead)\n+    extra_body: dict = None  # Extra body parameters for OpenRouter of vLLM\n \n     # Help descriptions\n     _help: ClassVar[dict[str, str]] = {\n@@ -443,6 +473,8 @@ class OpenAILLMOptions(BindingOptions):\n         \"stop\": 'Stop sequences (JSON array of strings, e.g., \\'[\"</s>\", \"\\\\n\\\\n\"]\\')',\n         \"temperature\": \"Controls randomness (0.0-2.0, higher = more creative)\",\n         \"top_p\": \"Nucleus sampling parameter (0.0-1.0, lower = more focused)\",\n+        \"max_tokens\": \"Maximum number of tokens to generate (deprecated, use max_completion_tokens instead)\",\n+        \"extra_body\": 'Extra body parameters for OpenRouter of vLLM (JSON dict, e.g., \\'\"reasoning\": {\"reasoning\": {\"enabled\": false}}\\')',\n     }\n \n \n@@ -493,6 +525,8 @@ if __name__ == \"__main__\":\n                 \"1000\",\n                 \"--openai-llm-stop\",\n                 '[\"</s>\", \"\\\\n\\\\n\"]',\n+                \"--openai-llm-reasoning\",\n+                '{\"effort\": \"high\", \"max_tokens\": 2000, \"exclude\": false, \"enabled\": true}',\n             ]\n         )\n         print(\"Final args for LLM and Embedding:\")\n@@ -518,5 +552,100 @@ if __name__ == \"__main__\":\n         print(\"\\nOpenAI LLM options instance:\")\n         print(openai_options.asdict())\n \n+        # Test creating OpenAI options instance with reasoning parameter\n+        openai_options_with_reasoning = OpenAILLMOptions(\n+            temperature=0.9,\n+            max_completion_tokens=2000,\n+            reasoning={\n+                \"effort\": \"medium\",\n+                \"max_tokens\": 1500,\n+                \"exclude\": True,\n+                \"enabled\": True,\n+            },\n+        )\n+        print(\"\\nOpenAI LLM options instance with reasoning:\")\n+        print(openai_options_with_reasoning.asdict())\n+\n+        # Test dict parsing functionality\n+        print(\"\\n\" + \"=\" * 50)\n+        print(\"TESTING DICT PARSING FUNCTIONALITY\")\n+        print(\"=\" * 50)\n+\n+        # Test valid JSON dict parsing\n+        test_parser = ArgumentParser(description=\"Test dict parsing\")\n+        OpenAILLMOptions.add_args(test_parser)\n+\n+        try:\n+            test_args = test_parser.parse_args(\n+                [\"--openai-llm-reasoning\", '{\"effort\": \"low\", \"max_tokens\": 1000}']\n+            )\n+            print(\"\u2713 Valid JSON dict parsing successful:\")\n+            print(\n+                f\"  Parsed reasoning: {OpenAILLMOptions.options_dict(test_args)['reasoning']}\"\n+            )\n+        except Exception as e:\n+            print(f\"\u2717 Valid JSON dict parsing failed: {e}\")\n+\n+        # Test invalid JSON dict parsing\n+        try:\n+            test_args = test_parser.parse_args(\n+                [\n+                    \"--openai-llm-reasoning\",\n+                    '{\"effort\": \"low\", \"max_tokens\": 1000',  # Missing closing brace\n+                ]\n+            )\n+            print(\"\u2717 Invalid JSON should have failed but didn't\")\n+        except SystemExit:\n+            print(\"\u2713 Invalid JSON dict parsing correctly rejected\")\n+        except Exception as e:\n+            print(f\"\u2713 Invalid JSON dict parsing correctly rejected: {e}\")\n+\n+        # Test non-dict JSON parsing\n+        try:\n+            test_args = test_parser.parse_args(\n+                [\n+                    \"--openai-llm-reasoning\",\n+                    '[\"not\", \"a\", \"dict\"]',  # Array instead of dict\n+                ]\n+            )\n+            print(\"\u2717 Non-dict JSON should have failed but didn't\")\n+        except SystemExit:\n+            print(\"\u2713 Non-dict JSON parsing correctly rejected\")\n+        except Exception as e:\n+            print(f\"\u2713 Non-dict JSON parsing correctly rejected: {e}\")\n+\n+        print(\"\\n\" + \"=\" * 50)\n+        print(\"TESTING ENVIRONMENT VARIABLE SUPPORT\")\n+        print(\"=\" * 50)\n+\n+        # Test environment variable support for dict\n+        import os\n+\n+        os.environ[\"OPENAI_LLM_REASONING\"] = (\n+            '{\"effort\": \"high\", \"max_tokens\": 3000, \"exclude\": false}'\n+        )\n+\n+        env_parser = ArgumentParser(description=\"Test env var dict parsing\")\n+        OpenAILLMOptions.add_args(env_parser)\n+\n+        try:\n+            env_args = env_parser.parse_args(\n+                []\n+            )  # No command line args, should use env var\n+            reasoning_from_env = OpenAILLMOptions.options_dict(env_args).get(\n+                \"reasoning\"\n+            )\n+            if reasoning_from_env:\n+                print(\"\u2713 Environment variable dict parsing successful:\")\n+                print(f\"  Parsed reasoning from env: {reasoning_from_env}\")\n+            else:\n+                print(\"\u2717 Environment variable dict parsing failed: No reasoning found\")\n+        except Exception as e:\n+            print(f\"\u2717 Environment variable dict parsing failed: {e}\")\n+        finally:\n+            # Clean up environment variable\n+            if \"OPENAI_LLM_REASONING\" in os.environ:\n+                del os.environ[\"OPENAI_LLM_REASONING\"]\n+\n     else:\n         print(BindingOptions.generate_dot_env_sample())\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm/lollms.py",
            "diff": "diff --git a/lightrag/llm/lollms.py b/lightrag/llm/lollms.py\nindex 357b65bf..39b64ce3 100644\n--- a/lightrag/llm/lollms.py\n+++ b/lightrag/llm/lollms.py\n@@ -59,7 +59,7 @@ async def lollms_model_if_cache(\n         \"personality\": kwargs.get(\"personality\", -1),\n         \"n_predict\": kwargs.get(\"n_predict\", None),\n         \"stream\": stream,\n-        \"temperature\": kwargs.get(\"temperature\", 0.8),\n+        \"temperature\": kwargs.get(\"temperature\", 1.0),\n         \"top_k\": kwargs.get(\"top_k\", 50),\n         \"top_p\": kwargs.get(\"top_p\", 0.95),\n         \"repeat_penalty\": kwargs.get(\"repeat_penalty\", 0.8),\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm/ollama.py",
            "diff": "diff --git a/lightrag/llm/ollama.py b/lightrag/llm/ollama.py\nindex 1ca5504e..6423fa90 100644\n--- a/lightrag/llm/ollama.py\n+++ b/lightrag/llm/ollama.py\n@@ -51,6 +51,8 @@ async def _ollama_model_if_cache(\n     # kwargs.pop(\"response_format\", None) # allow json\n     host = kwargs.pop(\"host\", None)\n     timeout = kwargs.pop(\"timeout\", None)\n+    if timeout == 0:\n+        timeout = None\n     kwargs.pop(\"hashing_kv\", None)\n     api_key = kwargs.pop(\"api_key\", None)\n     headers = {\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/llm/openai.py",
            "diff": "diff --git a/lightrag/llm/openai.py b/lightrag/llm/openai.py\nindex 910d1812..3bd652f4 100644\n--- a/lightrag/llm/openai.py\n+++ b/lightrag/llm/openai.py\n@@ -149,18 +149,20 @@ async def openai_complete_if_cache(\n     if not VERBOSE_DEBUG and logger.level == logging.DEBUG:\n         logging.getLogger(\"openai\").setLevel(logging.INFO)\n \n+    # Remove special kwargs that shouldn't be passed to OpenAI\n+    kwargs.pop(\"hashing_kv\", None)\n+    kwargs.pop(\"keyword_extraction\", None)\n+\n     # Extract client configuration options\n     client_configs = kwargs.pop(\"openai_client_configs\", {})\n \n     # Create the OpenAI client\n     openai_async_client = create_openai_async_client(\n-        api_key=api_key, base_url=base_url, client_configs=client_configs\n+        api_key=api_key,\n+        base_url=base_url,\n+        client_configs=client_configs,\n     )\n \n-    # Remove special kwargs that shouldn't be passed to OpenAI\n-    kwargs.pop(\"hashing_kv\", None)\n-    kwargs.pop(\"keyword_extraction\", None)\n-\n     # Prepare messages\n     messages: list[dict[str, Any]] = []\n     if system_prompt:\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/operate.py",
            "diff": "diff --git a/lightrag/operate.py b/lightrag/operate.py\nindex 41836d7d..afa8205f 100644\n--- a/lightrag/operate.py\n+++ b/lightrag/operate.py\n@@ -31,6 +31,7 @@ from .utils import (\n     pick_by_vector_similarity,\n     process_chunks_unified,\n     build_file_path,\n+    sanitize_text_for_encoding,\n )\n from .base import (\n     BaseGraphStorage,\n@@ -47,6 +48,8 @@ from .constants import (\n     DEFAULT_MAX_TOTAL_TOKENS,\n     DEFAULT_RELATED_CHUNK_NUMBER,\n     DEFAULT_KG_CHUNK_PICK_METHOD,\n+    DEFAULT_ENTITY_TYPES,\n+    DEFAULT_SUMMARY_LANGUAGE,\n )\n from .kg.shared_storage import get_storage_keyed_lock\n import time\n@@ -114,48 +117,195 @@ def chunking_by_token_size(\n \n \n async def _handle_entity_relation_summary(\n+    description_type: str,\n     entity_or_relation_name: str,\n-    description: str,\n+    description_list: list[str],\n+    seperator: str,\n+    global_config: dict,\n+    llm_response_cache: BaseKVStorage | None = None,\n+) -> tuple[str, bool]:\n+    \"\"\"Handle entity relation description summary using map-reduce approach.\n+\n+    This function summarizes a list of descriptions using a map-reduce strategy:\n+    1. If total tokens < summary_context_size and len(description_list) < force_llm_summary_on_merge, no need to summarize\n+    2. If total tokens < summary_max_tokens, summarize with LLM directly\n+    3. Otherwise, split descriptions into chunks that fit within token limits\n+    4. Summarize each chunk, then recursively process the summaries\n+    5. Continue until we get a final summary within token limits or num of descriptions is less than force_llm_summary_on_merge\n+\n+    Args:\n+        entity_or_relation_name: Name of the entity or relation being summarized\n+        description_list: List of description strings to summarize\n+        global_config: Global configuration containing tokenizer and limits\n+        llm_response_cache: Optional cache for LLM responses\n+\n+    Returns:\n+        Tuple of (final_summarized_description_string, llm_was_used_boolean)\n+    \"\"\"\n+    # Handle empty input\n+    if not description_list:\n+        return \"\", False\n+\n+    # If only one description, return it directly (no need for LLM call)\n+    if len(description_list) == 1:\n+        return description_list[0], False\n+\n+    # Get configuration\n+    tokenizer: Tokenizer = global_config[\"tokenizer\"]\n+    summary_context_size = global_config[\"summary_context_size\"]\n+    summary_max_tokens = global_config[\"summary_max_tokens\"]\n+    force_llm_summary_on_merge = global_config[\"force_llm_summary_on_merge\"]\n+\n+    current_list = description_list[:]  # Copy the list to avoid modifying original\n+    llm_was_used = False  # Track whether LLM was used during the entire process\n+\n+    # Iterative map-reduce process\n+    while True:\n+        # Calculate total tokens in current list\n+        total_tokens = sum(len(tokenizer.encode(desc)) for desc in current_list)\n+\n+        # If total length is within limits, perform final summarization\n+        if total_tokens <= summary_context_size or len(current_list) <= 2:\n+            if (\n+                len(current_list) < force_llm_summary_on_merge\n+                and total_tokens < summary_max_tokens\n+            ):\n+                # no LLM needed, just join the descriptions\n+                final_description = seperator.join(current_list)\n+                return final_description if final_description else \"\", llm_was_used\n+            else:\n+                if total_tokens > summary_context_size and len(current_list) <= 2:\n+                    logger.warning(\n+                        f\"Summarizing {entity_or_relation_name}: Oversize descpriton found\"\n+                    )\n+                # Final summarization of remaining descriptions - LLM will be used\n+                final_summary = await _summarize_descriptions(\n+                    description_type,\n+                    entity_or_relation_name,\n+                    current_list,\n+                    global_config,\n+                    llm_response_cache,\n+                )\n+                return final_summary, True  # LLM was used for final summarization\n+\n+        # Need to split into chunks - Map phase\n+        # Ensure each chunk has minimum 2 descriptions to guarantee progress\n+        chunks = []\n+        current_chunk = []\n+        current_tokens = 0\n+\n+        # Currently least 3 descriptions in current_list\n+        for i, desc in enumerate(current_list):\n+            desc_tokens = len(tokenizer.encode(desc))\n+\n+            # If adding current description would exceed limit, finalize current chunk\n+            if current_tokens + desc_tokens > summary_context_size and current_chunk:\n+                # Ensure we have at least 2 descriptions in the chunk (when possible)\n+                if len(current_chunk) == 1:\n+                    # Force add one more description to ensure minimum 2 per chunk\n+                    current_chunk.append(desc)\n+                    chunks.append(current_chunk)\n+                    logger.warning(\n+                        f\"Summarizing {entity_or_relation_name}: Oversize descpriton found\"\n+                    )\n+                    current_chunk = []  # next group is empty\n+                    current_tokens = 0\n+                else:  # curren_chunk is ready for summary in reduce phase\n+                    chunks.append(current_chunk)\n+                    current_chunk = [desc]  # leave it for next group\n+                    current_tokens = desc_tokens\n+            else:\n+                current_chunk.append(desc)\n+                current_tokens += desc_tokens\n+\n+        # Add the last chunk if it exists\n+        if current_chunk:\n+            chunks.append(current_chunk)\n+\n+        logger.info(\n+            f\"   Summarizing {entity_or_relation_name}: Map {len(current_list)} descriptions into {len(chunks)} groups\"\n+        )\n+\n+        # Reduce phase: summarize each group from chunks\n+        new_summaries = []\n+        for chunk in chunks:\n+            if len(chunk) == 1:\n+                # Optimization: single description chunks don't need LLM summarization\n+                new_summaries.append(chunk[0])\n+            else:\n+                # Multiple descriptions need LLM summarization\n+                summary = await _summarize_descriptions(\n+                    description_type,\n+                    entity_or_relation_name,\n+                    chunk,\n+                    global_config,\n+                    llm_response_cache,\n+                )\n+                new_summaries.append(summary)\n+                llm_was_used = True  # Mark that LLM was used in reduce phase\n+\n+        # Update current list with new summaries for next iteration\n+        current_list = new_summaries\n+\n+\n+async def _summarize_descriptions(\n+    description_type: str,\n+    description_name: str,\n+    description_list: list[str],\n     global_config: dict,\n     llm_response_cache: BaseKVStorage | None = None,\n ) -> str:\n-    \"\"\"Handle entity relation summary\n-    For each entity or relation, input is the combined description of already existing description and new description.\n-    If too long, use LLM to summarize.\n+    \"\"\"Helper function to summarize a list of descriptions using LLM.\n+\n+    Args:\n+        entity_or_relation_name: Name of the entity or relation being summarized\n+        descriptions: List of description strings to summarize\n+        global_config: Global configuration containing LLM function and settings\n+        llm_response_cache: Optional cache for LLM responses\n+\n+    Returns:\n+        Summarized description string\n     \"\"\"\n     use_llm_func: callable = global_config[\"llm_model_func\"]\n     # Apply higher priority (8) to entity/relation summary tasks\n     use_llm_func = partial(use_llm_func, _priority=8)\n \n-    tokenizer: Tokenizer = global_config[\"tokenizer\"]\n-    llm_max_tokens = global_config[\"summary_max_tokens\"]\n+    language = global_config[\"addon_params\"].get(\"language\", DEFAULT_SUMMARY_LANGUAGE)\n \n-    language = global_config[\"addon_params\"].get(\n-        \"language\", PROMPTS[\"DEFAULT_LANGUAGE\"]\n-    )\n+    summary_length_recommended = global_config[\"summary_length_recommended\"]\n \n-    tokens = tokenizer.encode(description)\n+    prompt_template = PROMPTS[\"summarize_entity_descriptions\"]\n \n-    ### summarize is not determined here anymore (It's determined by num_fragment now)\n-    # if len(tokens) < summary_max_tokens:  # No need for summary\n-    #     return description\n+    # Join descriptions and apply token-based truncation if necessary\n+    joined_descriptions = \"\\n\\n\".join(description_list)\n+    tokenizer = global_config[\"tokenizer\"]\n+    summary_context_size = global_config[\"summary_context_size\"]\n \n-    prompt_template = PROMPTS[\"summarize_entity_descriptions\"]\n-    use_description = tokenizer.decode(tokens[:llm_max_tokens])\n+    # Token-based truncation to ensure input fits within limits\n+    tokens = tokenizer.encode(joined_descriptions)\n+    if len(tokens) > summary_context_size:\n+        truncated_tokens = tokens[:summary_context_size]\n+        joined_descriptions = tokenizer.decode(truncated_tokens)\n+\n+    # Prepare context for the prompt\n     context_base = dict(\n-        entity_name=entity_or_relation_name,\n-        description_list=use_description.split(GRAPH_FIELD_SEP),\n+        description_type=description_type,\n+        description_name=description_name,\n+        description_list=joined_descriptions,\n+        summary_length=summary_length_recommended,\n         language=language,\n     )\n     use_prompt = prompt_template.format(**context_base)\n-    logger.debug(f\"Trigger summary: {entity_or_relation_name}\")\n+\n+    logger.debug(\n+        f\"Summarizing {len(description_list)} descriptions for: {description_name}\"\n+    )\n \n     # Use LLM function with cache (higher priority for summary generation)\n     summary = await use_llm_func_with_cache(\n         use_prompt,\n         use_llm_func,\n         llm_response_cache=llm_response_cache,\n-        # max_tokens=summary_max_tokens,\n         cache_type=\"extract\",\n     )\n     return summary\n@@ -169,50 +319,62 @@ async def _handle_single_entity_extraction(\n     if len(record_attributes) < 4 or '\"entity\"' not in record_attributes[0]:\n         return None\n \n-    # Clean and validate entity name\n-    entity_name = clean_str(record_attributes[1]).strip()\n-    if not entity_name:\n-        logger.warning(\n-            f\"Entity extraction error: empty entity name in: {record_attributes}\"\n-        )\n-        return None\n+    try:\n+        # Step 1: Strict UTF-8 encoding sanitization (fail-fast approach)\n+        entity_name = sanitize_text_for_encoding(record_attributes[1])\n \n-    # Normalize entity name\n-    entity_name = normalize_extracted_info(entity_name, is_entity=True)\n+        # Step 2: HTML and control character cleaning\n+        entity_name = clean_str(entity_name).strip()\n \n-    # Check if entity name became empty after normalization\n-    if not entity_name or not entity_name.strip():\n-        logger.warning(\n-            f\"Entity extraction error: entity name became empty after normalization. Original: '{record_attributes[1]}'\"\n+        # Step 3: Business logic normalization\n+        entity_name = normalize_extracted_info(entity_name, is_entity=True)\n+\n+        # Validate entity name after all cleaning steps\n+        if not entity_name or not entity_name.strip():\n+            logger.warning(\n+                f\"Entity extraction error: entity name became empty after cleaning. Original: '{record_attributes[1]}'\"\n+            )\n+            return None\n+\n+        # Process entity type with same cleaning pipeline\n+        entity_type = sanitize_text_for_encoding(record_attributes[2])\n+        entity_type = clean_str(entity_type).strip('\"')\n+        if not entity_type.strip() or entity_type.startswith('(\"'):\n+            logger.warning(\n+                f\"Entity extraction error: invalid entity type in: {record_attributes}\"\n+            )\n+            return None\n+\n+        # Process entity description with same cleaning pipeline\n+        entity_description = sanitize_text_for_encoding(record_attributes[3])\n+        entity_description = clean_str(entity_description)\n+        entity_description = normalize_extracted_info(entity_description)\n+\n+        if not entity_description.strip():\n+            logger.warning(\n+                f\"Entity extraction error: empty description for entity '{entity_name}' of type '{entity_type}'\"\n+            )\n+            return None\n+\n+        return dict(\n+            entity_name=entity_name,\n+            entity_type=entity_type,\n+            description=entity_description,\n+            source_id=chunk_key,\n+            file_path=file_path,\n         )\n-        return None\n \n-    # Clean and validate entity type\n-    entity_type = clean_str(record_attributes[2]).strip('\"')\n-    if not entity_type.strip() or entity_type.startswith('(\"'):\n-        logger.warning(\n-            f\"Entity extraction error: invalid entity type in: {record_attributes}\"\n+    except ValueError as e:\n+        logger.error(\n+            f\"Entity extraction failed due to encoding issues in chunk {chunk_key}: {e}\"\n         )\n         return None\n-\n-    # Clean and validate description\n-    entity_description = clean_str(record_attributes[3])\n-    entity_description = normalize_extracted_info(entity_description)\n-\n-    if not entity_description.strip():\n-        logger.warning(\n-            f\"Entity extraction error: empty description for entity '{entity_name}' of type '{entity_type}'\"\n+    except Exception as e:\n+        logger.error(\n+            f\"Entity extraction failed with unexpected error in chunk {chunk_key}: {e}\"\n         )\n         return None\n \n-    return dict(\n-        entity_name=entity_name,\n-        entity_type=entity_type,\n-        description=entity_description,\n-        source_id=chunk_key,\n-        file_path=file_path,\n-    )\n-\n \n async def _handle_single_relationship_extraction(\n     record_attributes: list[str],\n@@ -221,56 +383,78 @@ async def _handle_single_relationship_extraction(\n ):\n     if len(record_attributes) < 5 or '\"relationship\"' not in record_attributes[0]:\n         return None\n-    # add this record as edge\n-    source = clean_str(record_attributes[1])\n-    target = clean_str(record_attributes[2])\n \n-    # Normalize source and target entity names\n-    source = normalize_extracted_info(source, is_entity=True)\n-    target = normalize_extracted_info(target, is_entity=True)\n+    try:\n+        # Process source and target entities with strict cleaning pipeline\n+        # Step 1: Strict UTF-8 encoding sanitization (fail-fast approach)\n+        source = sanitize_text_for_encoding(record_attributes[1])\n+        # Step 2: HTML and control character cleaning\n+        source = clean_str(source)\n+        # Step 3: Business logic normalization\n+        source = normalize_extracted_info(source, is_entity=True)\n+\n+        # Same pipeline for target entity\n+        target = sanitize_text_for_encoding(record_attributes[2])\n+        target = clean_str(target)\n+        target = normalize_extracted_info(target, is_entity=True)\n+\n+        # Validate entity names after all cleaning steps\n+        if not source or not source.strip():\n+            logger.warning(\n+                f\"Relationship extraction error: source entity became empty after cleaning. Original: '{record_attributes[1]}'\"\n+            )\n+            return None\n \n-    # Check if source or target became empty after normalization\n-    if not source or not source.strip():\n-        logger.warning(\n-            f\"Relationship extraction error: source entity became empty after normalization. Original: '{record_attributes[1]}'\"\n+        if not target or not target.strip():\n+            logger.warning(\n+                f\"Relationship extraction error: target entity became empty after cleaning. Original: '{record_attributes[2]}'\"\n+            )\n+            return None\n+\n+        if source == target:\n+            logger.debug(\n+                f\"Relationship source and target are the same in: {record_attributes}\"\n+            )\n+            return None\n+\n+        # Process relationship description with same cleaning pipeline\n+        edge_description = sanitize_text_for_encoding(record_attributes[3])\n+        edge_description = clean_str(edge_description)\n+        edge_description = normalize_extracted_info(edge_description)\n+\n+        # Process keywords with same cleaning pipeline\n+        edge_keywords = sanitize_text_for_encoding(record_attributes[4])\n+        edge_keywords = clean_str(edge_keywords)\n+        edge_keywords = normalize_extracted_info(edge_keywords, is_entity=True)\n+        edge_keywords = edge_keywords.replace(\"\uff0c\", \",\")\n+\n+        edge_source_id = chunk_key\n+        weight = (\n+            float(record_attributes[-1].strip('\"').strip(\"'\"))\n+            if is_float_regex(record_attributes[-1].strip('\"').strip(\"'\"))\n+            else 1.0\n         )\n-        return None\n \n-    if not target or not target.strip():\n-        logger.warning(\n-            f\"Relationship extraction error: target entity became empty after normalization. Original: '{record_attributes[2]}'\"\n+        return dict(\n+            src_id=source,\n+            tgt_id=target,\n+            weight=weight,\n+            description=edge_description,\n+            keywords=edge_keywords,\n+            source_id=edge_source_id,\n+            file_path=file_path,\n         )\n-        return None\n \n-    if source == target:\n-        logger.debug(\n-            f\"Relationship source and target are the same in: {record_attributes}\"\n+    except ValueError as e:\n+        logger.error(\n+            f\"Relationship extraction failed due to encoding issues in chunk {chunk_key}: {e}\"\n+        )\n+        return None\n+    except Exception as e:\n+        logger.error(\n+            f\"Relationship extraction failed with unexpected error in chunk {chunk_key}: {e}\"\n         )\n         return None\n-\n-    edge_description = clean_str(record_attributes[3])\n-    edge_description = normalize_extracted_info(edge_description)\n-\n-    edge_keywords = normalize_extracted_info(\n-        clean_str(record_attributes[4]), is_entity=True\n-    )\n-    edge_keywords = edge_keywords.replace(\"\uff0c\", \",\")\n-\n-    edge_source_id = chunk_key\n-    weight = (\n-        float(record_attributes[-1].strip('\"').strip(\"'\"))\n-        if is_float_regex(record_attributes[-1].strip('\"').strip(\"'\"))\n-        else 1.0\n-    )\n-    return dict(\n-        src_id=source,\n-        tgt_id=target,\n-        weight=weight,\n-        description=edge_description,\n-        keywords=edge_keywords,\n-        source_id=edge_source_id,\n-        file_path=file_path,\n-    )\n \n \n async def _rebuild_knowledge_from_chunks(\n@@ -413,7 +597,7 @@ async def _rebuild_knowledge_from_chunks(\n                     )\n                     rebuilt_entities_count += 1\n                     status_message = (\n-                        f\"Rebuilt entity: {entity_name} from {len(chunk_ids)} chunks\"\n+                        f\"Rebuilt `{entity_name}` from {len(chunk_ids)} chunks\"\n                     )\n                     logger.info(status_message)\n                     if pipeline_status is not None and pipeline_status_lock is not None:\n@@ -422,7 +606,7 @@ async def _rebuild_knowledge_from_chunks(\n                             pipeline_status[\"history_messages\"].append(status_message)\n                 except Exception as e:\n                     failed_entities_count += 1\n-                    status_message = f\"Failed to rebuild entity {entity_name}: {e}\"\n+                    status_message = f\"Failed to rebuild `{entity_name}`: {e}\"\n                     logger.info(status_message)  # Per requirement, change to info\n                     if pipeline_status is not None and pipeline_status_lock is not None:\n                         async with pipeline_status_lock:\n@@ -453,7 +637,9 @@ async def _rebuild_knowledge_from_chunks(\n                         global_config=global_config,\n                     )\n                     rebuilt_relationships_count += 1\n-                    status_message = f\"Rebuilt relationship: {src}->{tgt} from {len(chunk_ids)} chunks\"\n+                    status_message = (\n+                        f\"Rebuilt `{src} - {tgt}` from {len(chunk_ids)} chunks\"\n+                    )\n                     logger.info(status_message)\n                     if pipeline_status is not None and pipeline_status_lock is not None:\n                         async with pipeline_status_lock:\n@@ -461,7 +647,7 @@ async def _rebuild_knowledge_from_chunks(\n                             pipeline_status[\"history_messages\"].append(status_message)\n                 except Exception as e:\n                     failed_relationships_count += 1\n-                    status_message = f\"Failed to rebuild relationship {src}->{tgt}: {e}\"\n+                    status_message = f\"Failed to rebuild `{src} - {tgt}`: {e}\"\n                     logger.info(status_message)  # Per requirement, change to info\n                     if pipeline_status is not None and pipeline_status_lock is not None:\n                         async with pipeline_status_lock:\n@@ -525,14 +711,20 @@ async def _get_cached_extraction_results(\n ) -> dict[str, list[str]]:\n     \"\"\"Get cached extraction results for specific chunk IDs\n \n+    This function retrieves cached LLM extraction results for the given chunk IDs and returns\n+    them sorted by creation time. The results are sorted at two levels:\n+    1. Individual extraction results within each chunk are sorted by create_time (earliest first)\n+    2. Chunks themselves are sorted by the create_time of their earliest extraction result\n+\n     Args:\n         llm_response_cache: LLM response cache storage\n         chunk_ids: Set of chunk IDs to get cached results for\n-        text_chunks_data: Pre-loaded chunk data (optional, for performance)\n-        text_chunks_storage: Text chunks storage (fallback if text_chunks_data is None)\n+        text_chunks_storage: Text chunks storage for retrieving chunk data and LLM cache references\n \n     Returns:\n-        Dict mapping chunk_id -> list of extraction_result_text\n+        Dict mapping chunk_id -> list of extraction_result_text, where:\n+        - Keys (chunk_ids) are ordered by the create_time of their first extraction result\n+        - Values (extraction results) are ordered by create_time within each chunk\n     \"\"\"\n     cached_results = {}\n \n@@ -541,15 +733,13 @@ async def _get_cached_extraction_results(\n \n     # Read from storage\n     chunk_data_list = await text_chunks_storage.get_by_ids(list(chunk_ids))\n-    for chunk_id, chunk_data in zip(chunk_ids, chunk_data_list):\n+    for chunk_data in chunk_data_list:\n         if chunk_data and isinstance(chunk_data, dict):\n             llm_cache_list = chunk_data.get(\"llm_cache_list\", [])\n             if llm_cache_list:\n                 all_cache_ids.update(llm_cache_list)\n         else:\n-            logger.warning(\n-                f\"Chunk {chunk_id} data is invalid or None: {type(chunk_data)}\"\n-            )\n+            logger.warning(f\"Chunk data is invalid or None: {chunk_data}\")\n \n     if not all_cache_ids:\n         logger.warning(f\"No LLM cache IDs found for {len(chunk_ids)} chunk IDs\")\n@@ -560,7 +750,7 @@ async def _get_cached_extraction_results(\n \n     # Process cache entries and group by chunk_id\n     valid_entries = 0\n-    for cache_id, cache_entry in zip(all_cache_ids, cache_data_list):\n+    for cache_entry in cache_data_list:\n         if (\n             cache_entry is not None\n             and isinstance(cache_entry, dict)\n@@ -580,16 +770,30 @@ async def _get_cached_extraction_results(\n             # Store tuple with extraction result and creation time for sorting\n             cached_results[chunk_id].append((extraction_result, create_time))\n \n-    # Sort extraction results by create_time for each chunk\n+    # Sort extraction results by create_time for each chunk and collect earliest times\n+    chunk_earliest_times = {}\n     for chunk_id in cached_results:\n         # Sort by create_time (x[1]), then extract only extraction_result (x[0])\n         cached_results[chunk_id].sort(key=lambda x: x[1])\n+        # Store the earliest create_time for this chunk (first item after sorting)\n+        chunk_earliest_times[chunk_id] = cached_results[chunk_id][0][1]\n+        # Extract only extraction_result (x[0])\n         cached_results[chunk_id] = [item[0] for item in cached_results[chunk_id]]\n \n+    # Sort cached_results by the earliest create_time of each chunk\n+    sorted_chunk_ids = sorted(\n+        chunk_earliest_times.keys(), key=lambda chunk_id: chunk_earliest_times[chunk_id]\n+    )\n+\n+    # Rebuild cached_results in sorted order\n+    sorted_cached_results = {}\n+    for chunk_id in sorted_chunk_ids:\n+        sorted_cached_results[chunk_id] = cached_results[chunk_id]\n+\n     logger.info(\n-        f\"Found {valid_entries} valid cache entries, {len(cached_results)} chunks with results\"\n+        f\"Found {valid_entries} valid cache entries, {len(sorted_cached_results)} chunks with results\"\n     )\n-    return cached_results\n+    return sorted_cached_results\n \n \n async def _parse_extraction_result(\n@@ -690,15 +894,6 @@ async def _rebuild_single_entity(\n         # Update entity in vector database\n         entity_vdb_id = compute_mdhash_id(entity_name, prefix=\"ent-\")\n \n-        # Delete old vector record first\n-        try:\n-            await entities_vdb.delete([entity_vdb_id])\n-        except Exception as e:\n-            logger.debug(\n-                f\"Could not delete old entity vector record {entity_vdb_id}: {e}\"\n-            )\n-\n-        # Insert new vector record\n         entity_content = f\"{entity_name}\\n{final_description}\"\n         await entities_vdb.upsert(\n             {\n@@ -713,21 +908,6 @@ async def _rebuild_single_entity(\n             }\n         )\n \n-    # Helper function to generate final description with optional LLM summary\n-    async def _generate_final_description(combined_description: str) -> str:\n-        force_llm_summary_on_merge = global_config[\"force_llm_summary_on_merge\"]\n-        num_fragment = combined_description.count(GRAPH_FIELD_SEP) + 1\n-\n-        if num_fragment >= force_llm_summary_on_merge:\n-            return await _handle_entity_relation_summary(\n-                entity_name,\n-                combined_description,\n-                global_config,\n-                llm_response_cache=llm_response_cache,\n-            )\n-        else:\n-            return combined_description\n-\n     # Collect all entity data from relevant chunks\n     all_entity_data = []\n     for chunk_id in chunk_ids:\n@@ -736,13 +916,13 @@ async def _rebuild_single_entity(\n \n     if not all_entity_data:\n         logger.warning(\n-            f\"No cached entity data found for {entity_name}, trying to rebuild from relationships\"\n+            f\"No entity data found for `{entity_name}`, trying to rebuild from relationships\"\n         )\n \n         # Get all edges connected to this entity\n         edges = await knowledge_graph_inst.get_node_edges(entity_name)\n         if not edges:\n-            logger.warning(f\"No relationships found for entity {entity_name}\")\n+            logger.warning(f\"No relations attached to entity `{entity_name}`\")\n             return\n \n         # Collect relationship data to extract entity information\n@@ -760,10 +940,19 @@ async def _rebuild_single_entity(\n                     edge_file_paths = edge_data[\"file_path\"].split(GRAPH_FIELD_SEP)\n                     file_paths.update(edge_file_paths)\n \n-        # Generate description from relationships or fallback to current\n-        if relationship_descriptions:\n-            combined_description = GRAPH_FIELD_SEP.join(relationship_descriptions)\n-            final_description = await _generate_final_description(combined_description)\n+        # deduplicate descriptions\n+        description_list = list(dict.fromkeys(relationship_descriptions))\n+\n+        # Generate final description from relationships or fallback to current\n+        if description_list:\n+            final_description, _ = await _handle_entity_relation_summary(\n+                \"Entity\",\n+                entity_name,\n+                description_list,\n+                GRAPH_FIELD_SEP,\n+                global_config,\n+                llm_response_cache=llm_response_cache,\n+            )\n         else:\n             final_description = current_entity.get(\"description\", \"\")\n \n@@ -784,12 +973,9 @@ async def _rebuild_single_entity(\n         if entity_data.get(\"file_path\"):\n             file_paths.add(entity_data[\"file_path\"])\n \n-    # Combine all descriptions\n-    combined_description = (\n-        GRAPH_FIELD_SEP.join(descriptions)\n-        if descriptions\n-        else current_entity.get(\"description\", \"\")\n-    )\n+    # Remove duplicates while preserving order\n+    description_list = list(dict.fromkeys(descriptions))\n+    entity_types = list(dict.fromkeys(entity_types))\n \n     # Get most common entity type\n     entity_type = (\n@@ -798,8 +984,19 @@ async def _rebuild_single_entity(\n         else current_entity.get(\"entity_type\", \"UNKNOWN\")\n     )\n \n-    # Generate final description and update storage\n-    final_description = await _generate_final_description(combined_description)\n+    # Generate final description from entities or fallback to current\n+    if description_list:\n+        final_description, _ = await _handle_entity_relation_summary(\n+            \"Entity\",\n+            entity_name,\n+            description_list,\n+            GRAPH_FIELD_SEP,\n+            global_config,\n+            llm_response_cache=llm_response_cache,\n+        )\n+    else:\n+        final_description = current_entity.get(\"description\", \"\")\n+\n     await _update_entity_storage(final_description, entity_type, file_paths)\n \n \n@@ -836,7 +1033,7 @@ async def _rebuild_single_relationship(\n                     )\n \n     if not all_relationship_data:\n-        logger.warning(f\"No cached relationship data found for {src}-{tgt}\")\n+        logger.warning(f\"No relation data found for `{src}-{tgt}`\")\n         return\n \n     # Merge descriptions and keywords\n@@ -855,42 +1052,38 @@ async def _rebuild_single_relationship(\n         if rel_data.get(\"file_path\"):\n             file_paths.add(rel_data[\"file_path\"])\n \n-    # Combine descriptions and keywords\n-    combined_description = (\n-        GRAPH_FIELD_SEP.join(descriptions)\n-        if descriptions\n-        else current_relationship.get(\"description\", \"\")\n-    )\n+    # Remove duplicates while preserving order\n+    description_list = list(dict.fromkeys(descriptions))\n+    keywords = list(dict.fromkeys(keywords))\n+\n     combined_keywords = (\n         \", \".join(set(keywords))\n         if keywords\n         else current_relationship.get(\"keywords\", \"\")\n     )\n-    # weight = (\n-    #     sum(weights) / len(weights)\n-    #     if weights\n-    #     else current_relationship.get(\"weight\", 1.0)\n-    # )\n-    weight = sum(weights) if weights else current_relationship.get(\"weight\", 1.0)\n \n-    # Use summary if description has too many fragments\n-    force_llm_summary_on_merge = global_config[\"force_llm_summary_on_merge\"]\n-    num_fragment = combined_description.count(GRAPH_FIELD_SEP) + 1\n+    weight = sum(weights) if weights else current_relationship.get(\"weight\", 1.0)\n \n-    if num_fragment >= force_llm_summary_on_merge:\n-        final_description = await _handle_entity_relation_summary(\n+    # Generate final description from relations or fallback to current\n+    if description_list:\n+        final_description, _ = await _handle_entity_relation_summary(\n+            \"Relation\",\n             f\"{src}-{tgt}\",\n-            combined_description,\n+            description_list,\n+            GRAPH_FIELD_SEP,\n             global_config,\n             llm_response_cache=llm_response_cache,\n         )\n     else:\n-        final_description = combined_description\n+        # fallback to keep current(unchanged)\n+        final_description = current_relationship.get(\"description\", \"\")\n \n     # Update relationship in graph storage\n     updated_relationship_data = {\n         **current_relationship,\n-        \"description\": final_description,\n+        \"description\": final_description\n+        if final_description\n+        else current_relationship.get(\"description\", \"\"),\n         \"keywords\": combined_keywords,\n         \"weight\": weight,\n         \"source_id\": GRAPH_FIELD_SEP.join(chunk_ids),\n@@ -948,13 +1141,9 @@ async def _merge_nodes_then_upsert(\n     already_node = await knowledge_graph_inst.get_node(entity_name)\n     if already_node:\n         already_entity_types.append(already_node[\"entity_type\"])\n-        already_source_ids.extend(\n-            split_string_by_multi_markers(already_node[\"source_id\"], [GRAPH_FIELD_SEP])\n-        )\n-        already_file_paths.extend(\n-            split_string_by_multi_markers(already_node[\"file_path\"], [GRAPH_FIELD_SEP])\n-        )\n-        already_description.append(already_node[\"description\"])\n+        already_source_ids.extend(already_node[\"source_id\"].split(GRAPH_FIELD_SEP))\n+        already_file_paths.extend(already_node[\"file_path\"].split(GRAPH_FIELD_SEP))\n+        already_description.extend(already_node[\"description\"].split(GRAPH_FIELD_SEP))\n \n     entity_type = sorted(\n         Counter(\n@@ -962,42 +1151,54 @@ async def _merge_nodes_then_upsert(\n         ).items(),\n         key=lambda x: x[1],\n         reverse=True,\n-    )[0][0]\n-    description = GRAPH_FIELD_SEP.join(\n-        sorted(set([dp[\"description\"] for dp in nodes_data] + already_description))\n+    )[0][0]  # Get the entity type with the highest count\n+\n+    # merge and deduplicate description\n+    description_list = list(\n+        dict.fromkeys(\n+            already_description\n+            + [dp[\"description\"] for dp in nodes_data if dp.get(\"description\")]\n+        )\n     )\n+\n+    num_fragment = len(description_list)\n+    already_fragment = len(already_description)\n+    deduplicated_num = already_fragment + len(nodes_data) - num_fragment\n+    if deduplicated_num > 0:\n+        dd_message = f\"(dd:{deduplicated_num})\"\n+    else:\n+        dd_message = \"\"\n+    if num_fragment > 0:\n+        # Get summary and LLM usage status\n+        description, llm_was_used = await _handle_entity_relation_summary(\n+            \"Entity\",\n+            entity_name,\n+            description_list,\n+            GRAPH_FIELD_SEP,\n+            global_config,\n+            llm_response_cache,\n+        )\n+\n+        # Log based on actual LLM usage\n+        if llm_was_used:\n+            status_message = f\"LLMmrg: `{entity_name}` | {already_fragment}+{num_fragment-already_fragment}{dd_message}\"\n+        else:\n+            status_message = f\"Merged: `{entity_name}` | {already_fragment}+{num_fragment-already_fragment}{dd_message}\"\n+\n+        logger.info(status_message)\n+        if pipeline_status is not None and pipeline_status_lock is not None:\n+            async with pipeline_status_lock:\n+                pipeline_status[\"latest_message\"] = status_message\n+                pipeline_status[\"history_messages\"].append(status_message)\n+    else:\n+        logger.error(f\"Entity {entity_name} has no description\")\n+        description = \"(no description)\"\n+\n     source_id = GRAPH_FIELD_SEP.join(\n         set([dp[\"source_id\"] for dp in nodes_data] + already_source_ids)\n     )\n     file_path = build_file_path(already_file_paths, nodes_data, entity_name)\n \n-    force_llm_summary_on_merge = global_config[\"force_llm_summary_on_merge\"]\n-\n-    num_fragment = description.count(GRAPH_FIELD_SEP) + 1\n-    num_new_fragment = len(set([dp[\"description\"] for dp in nodes_data]))\n-\n-    if num_fragment > 1:\n-        if num_fragment >= force_llm_summary_on_merge:\n-            status_message = f\"LLM merge N: {entity_name} | {num_new_fragment}+{num_fragment-num_new_fragment}\"\n-            logger.info(status_message)\n-            if pipeline_status is not None and pipeline_status_lock is not None:\n-                async with pipeline_status_lock:\n-                    pipeline_status[\"latest_message\"] = status_message\n-                    pipeline_status[\"history_messages\"].append(status_message)\n-            description = await _handle_entity_relation_summary(\n-                entity_name,\n-                description,\n-                global_config,\n-                llm_response_cache,\n-            )\n-        else:\n-            status_message = f\"Merge N: {entity_name} | {num_new_fragment}+{num_fragment-num_new_fragment}\"\n-            logger.info(status_message)\n-            if pipeline_status is not None and pipeline_status_lock is not None:\n-                async with pipeline_status_lock:\n-                    pipeline_status[\"latest_message\"] = status_message\n-                    pipeline_status[\"history_messages\"].append(status_message)\n-\n     node_data = dict(\n         entity_id=entity_name,\n         entity_type=entity_type,\n@@ -1044,22 +1245,20 @@ async def _merge_edges_then_upsert(\n             # Get source_id with empty string default if missing or None\n             if already_edge.get(\"source_id\") is not None:\n                 already_source_ids.extend(\n-                    split_string_by_multi_markers(\n-                        already_edge[\"source_id\"], [GRAPH_FIELD_SEP]\n-                    )\n+                    already_edge[\"source_id\"].split(GRAPH_FIELD_SEP)\n                 )\n \n             # Get file_path with empty string default if missing or None\n             if already_edge.get(\"file_path\") is not None:\n                 already_file_paths.extend(\n-                    split_string_by_multi_markers(\n-                        already_edge[\"file_path\"], [GRAPH_FIELD_SEP]\n-                    )\n+                    already_edge[\"file_path\"].split(GRAPH_FIELD_SEP)\n                 )\n \n             # Get description with empty string default if missing or None\n             if already_edge.get(\"description\") is not None:\n-                already_description.append(already_edge[\"description\"])\n+                already_description.extend(\n+                    already_edge[\"description\"].split(GRAPH_FIELD_SEP)\n+                )\n \n             # Get keywords with empty string default if missing or None\n             if already_edge.get(\"keywords\") is not None:\n@@ -1071,15 +1270,47 @@ async def _merge_edges_then_upsert(\n \n     # Process edges_data with None checks\n     weight = sum([dp[\"weight\"] for dp in edges_data] + already_weights)\n-    description = GRAPH_FIELD_SEP.join(\n-        sorted(\n-            set(\n-                [dp[\"description\"] for dp in edges_data if dp.get(\"description\")]\n-                + already_description\n-            )\n+\n+    description_list = list(\n+        dict.fromkeys(\n+            already_description\n+            + [dp[\"description\"] for dp in edges_data if dp.get(\"description\")]\n         )\n     )\n \n+    num_fragment = len(description_list)\n+    already_fragment = len(already_description)\n+    deduplicated_num = already_fragment + len(edges_data) - num_fragment\n+    if deduplicated_num > 0:\n+        dd_message = f\"(dd:{deduplicated_num})\"\n+    else:\n+        dd_message = \"\"\n+    if num_fragment > 0:\n+        # Get summary and LLM usage status\n+        description, llm_was_used = await _handle_entity_relation_summary(\n+            \"Relation\",\n+            f\"({src_id}, {tgt_id})\",\n+            description_list,\n+            GRAPH_FIELD_SEP,\n+            global_config,\n+            llm_response_cache,\n+        )\n+\n+        # Log based on actual LLM usage\n+        if llm_was_used:\n+            status_message = f\"LLMmrg: `{src_id} - {tgt_id}` | {already_fragment}+{num_fragment-already_fragment}{dd_message}\"\n+        else:\n+            status_message = f\"Merged: `{src_id} - {tgt_id}` | {already_fragment}+{num_fragment-already_fragment}{dd_message}\"\n+\n+        logger.info(status_message)\n+        if pipeline_status is not None and pipeline_status_lock is not None:\n+            async with pipeline_status_lock:\n+                pipeline_status[\"latest_message\"] = status_message\n+                pipeline_status[\"history_messages\"].append(status_message)\n+    else:\n+        logger.error(f\"Edge {src_id} - {tgt_id} has no description\")\n+        description = \"(no description)\"\n+\n     # Split all existing and new keywords into individual terms, then combine and deduplicate\n     all_keywords = set()\n     # Process already_keywords (which are comma-separated)\n@@ -1127,35 +1358,6 @@ async def _merge_edges_then_upsert(\n                 }\n                 added_entities.append(entity_data)\n \n-    force_llm_summary_on_merge = global_config[\"force_llm_summary_on_merge\"]\n-\n-    num_fragment = description.count(GRAPH_FIELD_SEP) + 1\n-    num_new_fragment = len(\n-        set([dp[\"description\"] for dp in edges_data if dp.get(\"description\")])\n-    )\n-\n-    if num_fragment > 1:\n-        if num_fragment >= force_llm_summary_on_merge:\n-            status_message = f\"LLM merge E: {src_id} - {tgt_id} | {num_new_fragment}+{num_fragment-num_new_fragment}\"\n-            logger.info(status_message)\n-            if pipeline_status is not None and pipeline_status_lock is not None:\n-                async with pipeline_status_lock:\n-                    pipeline_status[\"latest_message\"] = status_message\n-                    pipeline_status[\"history_messages\"].append(status_message)\n-            description = await _handle_entity_relation_summary(\n-                f\"({src_id}, {tgt_id})\",\n-                description,\n-                global_config,\n-                llm_response_cache,\n-            )\n-        else:\n-            status_message = f\"Merge E: {src_id} - {tgt_id} | {num_new_fragment}+{num_fragment-num_new_fragment}\"\n-            logger.info(status_message)\n-            if pipeline_status is not None and pipeline_status_lock is not None:\n-                async with pipeline_status_lock:\n-                    pipeline_status[\"latest_message\"] = status_message\n-                    pipeline_status[\"history_messages\"].append(status_message)\n-\n     await knowledge_graph_inst.upsert_edge(\n         src_id,\n         tgt_id,\n@@ -1463,7 +1665,7 @@ async def merge_nodes_and_edges(\n             )\n             # Don't raise exception to avoid affecting main flow\n \n-    log_message = f\"Completed merging: {len(processed_entities)} entities, {len(all_added_entities)} added entities, {len(processed_edges)} relations\"\n+    log_message = f\"Completed merging: {len(processed_entities)} entities, {len(all_added_entities)} extra entities, {len(processed_edges)} relations\"\n     logger.info(log_message)\n     async with pipeline_status_lock:\n         pipeline_status[\"latest_message\"] = log_message\n@@ -1483,11 +1685,9 @@ async def extract_entities(\n \n     ordered_chunks = list(chunks.items())\n     # add language and example number params to prompt\n-    language = global_config[\"addon_params\"].get(\n-        \"language\", PROMPTS[\"DEFAULT_LANGUAGE\"]\n-    )\n+    language = global_config[\"addon_params\"].get(\"language\", DEFAULT_SUMMARY_LANGUAGE)\n     entity_types = global_config[\"addon_params\"].get(\n-        \"entity_types\", PROMPTS[\"DEFAULT_ENTITY_TYPES\"]\n+        \"entity_types\", DEFAULT_ENTITY_TYPES\n     )\n     example_number = global_config[\"addon_params\"].get(\"example_number\", None)\n     if example_number and example_number < len(PROMPTS[\"entity_extraction_examples\"]):\n@@ -1727,6 +1927,9 @@ async def kg_query(\n     system_prompt: str | None = None,\n     chunks_vdb: BaseVectorStorage = None,\n ) -> str | AsyncIterator[str]:\n+    if not query:\n+        return PROMPTS[\"fail_response\"]\n+\n     if query_param.model_func:\n         use_model_func = query_param.model_func\n     else:\n@@ -1763,21 +1966,16 @@ async def kg_query(\n     logger.debug(f\"Low-level  keywords: {ll_keywords}\")\n \n     # Handle empty keywords\n+    if ll_keywords == [] and query_param.mode in [\"local\", \"hybrid\", \"mix\"]:\n+        logger.warning(\"low_level_keywords is empty\")\n+    if hl_keywords == [] and query_param.mode in [\"global\", \"hybrid\", \"mix\"]:\n+        logger.warning(\"high_level_keywords is empty\")\n     if hl_keywords == [] and ll_keywords == []:\n-        logger.warning(\"low_level_keywords and high_level_keywords is empty\")\n-        return PROMPTS[\"fail_response\"]\n-    if ll_keywords == [] and query_param.mode in [\"local\", \"hybrid\"]:\n-        logger.warning(\n-            \"low_level_keywords is empty, switching from %s mode to global mode\",\n-            query_param.mode,\n-        )\n-        query_param.mode = \"global\"\n-    if hl_keywords == [] and query_param.mode in [\"global\", \"hybrid\"]:\n-        logger.warning(\n-            \"high_level_keywords is empty, switching from %s mode to local mode\",\n-            query_param.mode,\n-        )\n-        query_param.mode = \"local\"\n+        if len(query) < 50:\n+            logger.warning(f\"Forced low_level_keywords to origin query: {query}\")\n+            ll_keywords = [query]\n+        else:\n+            return PROMPTS[\"fail_response\"]\n \n     ll_keywords_str = \", \".join(ll_keywords) if ll_keywords else \"\"\n     hl_keywords_str = \", \".join(hl_keywords) if hl_keywords else \"\"\n@@ -1949,9 +2147,7 @@ async def extract_keywords_only(\n         )\n     else:\n         examples = \"\\n\".join(PROMPTS[\"keywords_extraction_examples\"])\n-    language = global_config[\"addon_params\"].get(\n-        \"language\", PROMPTS[\"DEFAULT_LANGUAGE\"]\n-    )\n+    language = global_config[\"addon_params\"].get(\"language\", DEFAULT_SUMMARY_LANGUAGE)\n \n     # 3. Process conversation history\n     # history_context = \"\"\n@@ -2038,6 +2234,7 @@ async def _get_vector_context(\n     query: str,\n     chunks_vdb: BaseVectorStorage,\n     query_param: QueryParam,\n+    query_embedding: list[float] = None,\n ) -> list[dict]:\n     \"\"\"\n     Retrieve text chunks from the vector database without reranking or truncation.\n@@ -2049,6 +2246,7 @@ async def _get_vector_context(\n         query: The query string to search for\n         chunks_vdb: Vector database containing document chunks\n         query_param: Query parameters including chunk_top_k and ids\n+        query_embedding: Optional pre-computed query embedding to avoid redundant embedding calls\n \n     Returns:\n         List of text chunks with metadata\n@@ -2057,8 +2255,11 @@ async def _get_vector_context(\n         # Use chunk_top_k if specified, otherwise fall back to top_k\n         search_top_k = query_param.chunk_top_k or query_param.top_k\n \n-        results = await chunks_vdb.query(query, top_k=search_top_k)\n+        results = await chunks_vdb.query(\n+            query, top_k=search_top_k, query_embedding=query_embedding\n+        )\n         if not results:\n+            logger.info(f\"Naive query: 0 chunks (chunk_top_k: {search_top_k})\")\n             return []\n \n         valid_chunks = []\n@@ -2094,6 +2295,10 @@ async def _build_query_context(\n     query_param: QueryParam,\n     chunks_vdb: BaseVectorStorage = None,\n ):\n+    if not query:\n+        logger.warning(\"Query is empty, skipping context building\")\n+        return \"\"\n+\n     logger.info(f\"Process {os.getpid()} building query context...\")\n \n     # Collect chunks from different sources separately\n@@ -2112,8 +2317,26 @@ async def _build_query_context(\n     # Track chunk sources and metadata for final logging\n     chunk_tracking = {}  # chunk_id -> {source, frequency, order}\n \n+    # Pre-compute query embedding once for all vector operations\n+    kg_chunk_pick_method = text_chunks_db.global_config.get(\n+        \"kg_chunk_pick_method\", DEFAULT_KG_CHUNK_PICK_METHOD\n+    )\n+    query_embedding = None\n+    if query and (kg_chunk_pick_method == \"VECTOR\" or chunks_vdb):\n+        embedding_func_config = text_chunks_db.embedding_func\n+        if embedding_func_config and embedding_func_config.func:\n+            try:\n+                query_embedding = await embedding_func_config.func([query])\n+                query_embedding = query_embedding[\n+                    0\n+                ]  # Extract first embedding from batch result\n+                logger.debug(\"Pre-computed query embedding for all vector operations\")\n+            except Exception as e:\n+                logger.warning(f\"Failed to pre-compute query embedding: {e}\")\n+                query_embedding = None\n+\n     # Handle local and global modes\n-    if query_param.mode == \"local\":\n+    if query_param.mode == \"local\" and len(ll_keywords) > 0:\n         local_entities, local_relations = await _get_node_data(\n             ll_keywords,\n             knowledge_graph_inst,\n@@ -2121,7 +2344,7 @@ async def _build_query_context(\n             query_param,\n         )\n \n-    elif query_param.mode == \"global\":\n+    elif query_param.mode == \"global\" and len(hl_keywords) > 0:\n         global_relations, global_entities = await _get_edge_data(\n             hl_keywords,\n             knowledge_graph_inst,\n@@ -2130,18 +2353,20 @@ async def _build_query_context(\n         )\n \n     else:  # hybrid or mix mode\n-        local_entities, local_relations = await _get_node_data(\n-            ll_keywords,\n-            knowledge_graph_inst,\n-            entities_vdb,\n-            query_param,\n-        )\n-        global_relations, global_entities = await _get_edge_data(\n-            hl_keywords,\n-            knowledge_graph_inst,\n-            relationships_vdb,\n-            query_param,\n-        )\n+        if len(ll_keywords) > 0:\n+            local_entities, local_relations = await _get_node_data(\n+                ll_keywords,\n+                knowledge_graph_inst,\n+                entities_vdb,\n+                query_param,\n+            )\n+        if len(hl_keywords) > 0:\n+            global_relations, global_entities = await _get_edge_data(\n+                hl_keywords,\n+                knowledge_graph_inst,\n+                relationships_vdb,\n+                query_param,\n+            )\n \n         # Get vector chunks first if in mix mode\n         if query_param.mode == \"mix\" and chunks_vdb:\n@@ -2149,6 +2374,7 @@ async def _build_query_context(\n                 query,\n                 chunks_vdb,\n                 query_param,\n+                query_embedding,\n             )\n             # Track vector chunks with source metadata\n             for i, chunk in enumerate(vector_chunks):\n@@ -2159,6 +2385,8 @@ async def _build_query_context(\n                         \"frequency\": 1,  # Vector chunks always have frequency 1\n                         \"order\": i + 1,  # 1-based order in vector search results\n                     }\n+                else:\n+                    logger.warning(f\"Vector chunk missing chunk_id: {chunk}\")\n \n     # Use round-robin merge to combine local and global data fairly\n     final_entities = []\n@@ -2372,6 +2600,7 @@ async def _build_query_context(\n             query,\n             chunks_vdb,\n             chunk_tracking=chunk_tracking,\n+            query_embedding=query_embedding,\n         )\n \n     # Find deduplcicated chunks from edge\n@@ -2385,6 +2614,7 @@ async def _build_query_context(\n             query,\n             chunks_vdb,\n             chunk_tracking=chunk_tracking,\n+            query_embedding=query_embedding,\n         )\n \n     # Round-robin merge chunks from different sources with deduplication by chunk_id\n@@ -2442,6 +2672,7 @@ async def _build_query_context(\n \n     # Apply token processing to merged chunks\n     text_units_context = []\n+    truncated_chunks = []\n     if merged_chunks:\n         # Calculate dynamic token limit for text chunks\n         entities_str = json.dumps(entities_context, ensure_ascii=False)\n@@ -2473,15 +2704,15 @@ async def _build_query_context(\n         kg_context_tokens = len(tokenizer.encode(kg_context))\n \n         # Calculate actual system prompt overhead dynamically\n-        # 1. Calculate conversation history tokens\n+        # 1. Converstion history not included in context length calculation\n         history_context = \"\"\n-        if query_param.conversation_history:\n-            history_context = get_conversation_turns(\n-                query_param.conversation_history, query_param.history_turns\n-            )\n-        history_tokens = (\n-            len(tokenizer.encode(history_context)) if history_context else 0\n-        )\n+        # if query_param.conversation_history:\n+        #     history_context = get_conversation_turns(\n+        #         query_param.conversation_history, query_param.history_turns\n+        #     )\n+        # history_tokens = (\n+        #     len(tokenizer.encode(history_context)) if history_context else 0\n+        # )\n \n         # 2. Calculate system prompt template tokens (excluding context_data)\n         user_prompt = query_param.user_prompt if query_param.user_prompt else \"\"\n@@ -2516,7 +2747,7 @@ async def _build_query_context(\n         available_chunk_tokens = max_total_tokens - used_tokens\n \n         logger.debug(\n-            f\"Token allocation - Total: {max_total_tokens}, History: {history_tokens}, SysPrompt: {sys_prompt_overhead}, KG: {kg_context_tokens}, Buffer: {buffer_tokens}, Available for chunks: {available_chunk_tokens}\"\n+            f\"Token allocation - Total: {max_total_tokens}, SysPrompt: {sys_prompt_overhead}, KG: {kg_context_tokens}, Buffer: {buffer_tokens}, Available for chunks: {available_chunk_tokens}\"\n         )\n \n         # Apply token truncation to chunks using the dynamic limit\n@@ -2606,9 +2837,7 @@ async def _get_node_data(\n         f\"Query nodes: {query}, top_k: {query_param.top_k}, cosine: {entities_vdb.cosine_better_than_threshold}\"\n     )\n \n-    results = await entities_vdb.query(\n-        query, top_k=query_param.top_k\n-    )\n+    results = await entities_vdb.query(query, top_k=query_param.top_k)\n \n     if not len(results):\n         return [], []\n@@ -2719,6 +2948,7 @@ async def _find_related_text_unit_from_entities(\n     query: str = None,\n     chunks_vdb: BaseVectorStorage = None,\n     chunk_tracking: dict = None,\n+    query_embedding=None,\n ):\n     \"\"\"\n     Find text chunks related to entities using configurable chunk selection method.\n@@ -2814,6 +3044,7 @@ async def _find_related_text_unit_from_entities(\n                         num_of_chunks=num_of_chunks,\n                         entity_info=entities_with_chunks,\n                         embedding_func=actual_embedding_func,\n+                        query_embedding=query_embedding,\n                     )\n \n                 if selected_chunk_ids == []:\n@@ -2882,9 +3113,7 @@ async def _get_edge_data(\n         f\"Query edges: {keywords}, top_k: {query_param.top_k}, cosine: {relationships_vdb.cosine_better_than_threshold}\"\n     )\n \n-    results = await relationships_vdb.query(\n-        keywords, top_k=query_param.top_k\n-    )\n+    results = await relationships_vdb.query(keywords, top_k=query_param.top_k)\n \n     if not len(results):\n         return [], []\n@@ -2971,6 +3200,7 @@ async def _find_related_text_unit_from_relations(\n     query: str = None,\n     chunks_vdb: BaseVectorStorage = None,\n     chunk_tracking: dict = None,\n+    query_embedding=None,\n ):\n     \"\"\"\n     Find text chunks related to relationships using configurable chunk selection method.\n@@ -3106,6 +3336,7 @@ async def _find_related_text_unit_from_relations(\n                         num_of_chunks=num_of_chunks,\n                         entity_info=relations_with_chunks,\n                         embedding_func=actual_embedding_func,\n+                        query_embedding=query_embedding,\n                     )\n \n                 if selected_chunk_ids == []:\n@@ -3205,7 +3436,7 @@ async def naive_query(\n \n     tokenizer: Tokenizer = global_config[\"tokenizer\"]\n \n-    chunks = await _get_vector_context(query, chunks_vdb, query_param)\n+    chunks = await _get_vector_context(query, chunks_vdb, query_param, None)\n \n     if chunks is None or len(chunks) == 0:\n         return PROMPTS[\"fail_response\"]\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/prompt.py",
            "diff": "diff --git a/lightrag/prompt.py b/lightrag/prompt.py\nindex 32666bb5..f8ea6589 100644\n--- a/lightrag/prompt.py\n+++ b/lightrag/prompt.py\n@@ -4,13 +4,10 @@ from typing import Any\n \n PROMPTS: dict[str, Any] = {}\n \n-PROMPTS[\"DEFAULT_LANGUAGE\"] = \"English\"\n PROMPTS[\"DEFAULT_TUPLE_DELIMITER\"] = \"<|>\"\n PROMPTS[\"DEFAULT_RECORD_DELIMITER\"] = \"##\"\n PROMPTS[\"DEFAULT_COMPLETION_DELIMITER\"] = \"<|COMPLETE|>\"\n \n-PROMPTS[\"DEFAULT_ENTITY_TYPES\"] = [\"organization\", \"person\", \"geo\", \"event\", \"category\"]\n-\n PROMPTS[\"DEFAULT_USER_PROMPT\"] = \"n/a\"\n \n PROMPTS[\"entity_extraction\"] = \"\"\"---Goal---\n@@ -40,22 +37,19 @@ Format the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_\n \n 5. When finished, output {completion_delimiter}\n \n-######################\n ---Examples---\n-######################\n {examples}\n \n-#############################\n ---Real Data---\n-######################\n Entity_types: [{entity_types}]\n Text:\n {input_text}\n-######################\n+\n+---Output---\n Output:\"\"\"\n \n PROMPTS[\"entity_extraction_examples\"] = [\n-    \"\"\"Example 1:\n+    \"\"\"------Example 1------\n \n Entity_types: [person, technology, mission, organization, location]\n Text:\n@@ -81,8 +75,9 @@ Output:\n (\"relationship\"{tuple_delimiter}\"Jordan\"{tuple_delimiter}\"Cruz\"{tuple_delimiter}\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"{tuple_delimiter}\"ideological conflict, rebellion\"{tuple_delimiter}5){record_delimiter}\n (\"relationship\"{tuple_delimiter}\"Taylor\"{tuple_delimiter}\"The Device\"{tuple_delimiter}\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"{tuple_delimiter}\"reverence, technological significance\"{tuple_delimiter}9){record_delimiter}\n (\"content_keywords\"{tuple_delimiter}\"power dynamics, ideological conflict, discovery, rebellion\"){completion_delimiter}\n-#############################\"\"\",\n-    \"\"\"Example 2:\n+\n+\"\"\",\n+    \"\"\"------Example 2------\n \n Entity_types: [company, index, commodity, market_trend, economic_policy, biological]\n Text:\n@@ -109,8 +104,9 @@ Output:\n (\"relationship\"{tuple_delimiter}\"Gold Futures\"{tuple_delimiter}\"Market Selloff\"{tuple_delimiter}\"Gold prices rose as investors sought safe-haven assets during the market selloff.\"{tuple_delimiter}\"market reaction, safe-haven investment\"{tuple_delimiter}10){record_delimiter}\n (\"relationship\"{tuple_delimiter}\"Federal Reserve Policy Announcement\"{tuple_delimiter}\"Market Selloff\"{tuple_delimiter}\"Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\"{tuple_delimiter}\"interest rate impact, financial regulation\"{tuple_delimiter}7){record_delimiter}\n (\"content_keywords\"{tuple_delimiter}\"market downturn, investor sentiment, commodities, Federal Reserve, stock performance\"){completion_delimiter}\n-#############################\"\"\",\n-    \"\"\"Example 3:\n+\n+\"\"\",\n+    \"\"\"------Example 3------\n \n Entity_types: [economic_policy, athlete, event, location, record, organization, equipment]\n Text:\n@@ -130,23 +126,29 @@ Output:\n (\"relationship\"{tuple_delimiter}\"Noah Carter\"{tuple_delimiter}\"Carbon-Fiber Spikes\"{tuple_delimiter}\"Noah Carter used carbon-fiber spikes to enhance performance during the race.\"{tuple_delimiter}\"athletic equipment, performance boost\"{tuple_delimiter}7){record_delimiter}\n (\"relationship\"{tuple_delimiter}\"World Athletics Federation\"{tuple_delimiter}\"100m Sprint Record\"{tuple_delimiter}\"The World Athletics Federation is responsible for validating and recognizing new sprint records.\"{tuple_delimiter}\"sports regulation, record certification\"{tuple_delimiter}9){record_delimiter}\n (\"content_keywords\"{tuple_delimiter}\"athletics, sprinting, record-breaking, sports technology, competition\"){completion_delimiter}\n-#############################\"\"\",\n+\n+\"\"\",\n ]\n \n-PROMPTS[\n-    \"summarize_entity_descriptions\"\n-] = \"\"\"You are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\n-Given one or two entities, and a list of descriptions, all related to the same entity or group of entities.\n-Please concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\n-If the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\n-Make sure it is written in third person, and include the entity names so we the have full context.\n-Use {language} as output language.\n+PROMPTS[\"summarize_entity_descriptions\"] = \"\"\"---Role---\n+You are a Knowledge Graph Specialist responsible for data curation and synthesis.\n+\n+---Task---\n+Your task is to synthesize a list of descriptions of a given entity or relation into a single, comprehensive, and cohesive summary.\n+\n+---Instructions---\n+1. **Comprehensiveness:** The summary must integrate key information from all provided descriptions. Do not omit important facts.\n+2. **Context:** The summary must explicitly mention the name of the entity or relation for full context.\n+3. **Style:** The output must be written from an objective, third-person perspective.\n+4. **Length:** Maintain depth and completeness while ensuring the summary's length not exceed {summary_length} tokens.\n+5. **Language:** The entire output must be written in {language}.\n \n-#######\n ---Data---\n-Entities: {entity_name}\n-Description List: {description_list}\n-#######\n+{description_type} Name: {description_name}\n+Description List:\n+{description_list}\n+\n+---Output---\n Output:\n \"\"\"\n \n@@ -188,8 +190,7 @@ PROMPTS[\"entity_if_loop_extraction\"] = \"\"\"\n It appears some entities may have still been missed.\n \n ---Output---\n-\n-Answer ONLY by `YES` OR `NO` if there are still entities that need to be added.\n+Output:\n \"\"\".strip()\n \n PROMPTS[\"fail_response\"] = (\n@@ -211,7 +212,7 @@ Generate a concise response based on Knowledge Base and follow Response Rules, c\n ---Knowledge Graph and Document Chunks---\n {context_data}\n \n----RESPONSE GUIDELINES---\n+---Response Guidelines---\n **1. Content & Adherence:**\n - Strictly adhere to the provided context from the Knowledge Base. Do not invent, assume, or include any information not present in the source data.\n - If the answer cannot be found in the provided context, state that you do not have enough information to answer.\n@@ -233,8 +234,8 @@ Generate a concise response based on Knowledge Base and follow Response Rules, c\n ---USER CONTEXT---\n - Additional user prompt: {user_prompt}\n \n-\n-Response:\"\"\"\n+---Response---\n+Output:\"\"\"\n \n PROMPTS[\"keywords_extraction\"] = \"\"\"---Role---\n You are an expert keyword extractor, specializing in analyzing user queries for a Retrieval-Augmented Generation (RAG) system. Your purpose is to identify both high-level and low-level keywords in the user's query that will be used for effective document retrieval.\n@@ -257,7 +258,7 @@ Given a user query, your task is to extract two distinct types of keywords:\n User Query: {query}\n \n ---Output---\n-\"\"\"\n+Output:\"\"\"\n \n PROMPTS[\"keywords_extraction_examples\"] = [\n     \"\"\"Example 1:\n@@ -327,5 +328,5 @@ Generate a concise response based on Document Chunks and follow Response Rules,\n ---USER CONTEXT---\n - Additional user prompt: {user_prompt}\n \n-\n-Response:\"\"\"\n+---Response---\n+Output:\"\"\"\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/rerank.py",
            "diff": "diff --git a/lightrag/rerank.py b/lightrag/rerank.py\nindex 5ed1ca68..35551f5a 100644\n--- a/lightrag/rerank.py\n+++ b/lightrag/rerank.py\n@@ -2,232 +2,199 @@ from __future__ import annotations\n \n import os\n import aiohttp\n-from typing import Callable, Any, List, Dict, Optional\n-from pydantic import BaseModel, Field\n-\n+from typing import Any, List, Dict, Optional\n+from tenacity import (\n+    retry,\n+    stop_after_attempt,\n+    wait_exponential,\n+    retry_if_exception_type,\n+)\n from .utils import logger\n \n+from dotenv import load_dotenv\n \n-class RerankModel(BaseModel):\n-    \"\"\"\n-    Wrapper for rerank functions that can be used with LightRAG.\n-\n-    Example usage:\n-    ```python\n-    from lightrag.rerank import RerankModel, jina_rerank\n-\n-    # Create rerank model\n-    rerank_model = RerankModel(\n-        rerank_func=jina_rerank,\n-        kwargs={\n-            \"model\": \"BAAI/bge-reranker-v2-m3\",\n-            \"api_key\": \"your_api_key_here\",\n-            \"base_url\": \"https://api.jina.ai/v1/rerank\"\n-        }\n-    )\n-\n-    # Use in LightRAG\n-    rag = LightRAG(\n-        rerank_model_func=rerank_model.rerank,\n-        # ... other configurations\n-    )\n-\n-    # Query with rerank enabled (default)\n-    result = await rag.aquery(\n-        \"your query\",\n-        param=QueryParam(enable_rerank=True)\n-    )\n-    ```\n-\n-    Or define a custom function directly:\n-    ```python\n-    async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):\n-        return await jina_rerank(\n-            query=query,\n-            documents=documents,\n-            model=\"BAAI/bge-reranker-v2-m3\",\n-            api_key=\"your_api_key_here\",\n-            top_n=top_n or 10,\n-            **kwargs\n-        )\n-\n-    rag = LightRAG(\n-        rerank_model_func=my_rerank_func,\n-        # ... other configurations\n-    )\n-\n-    # Control rerank per query\n-    result = await rag.aquery(\n-        \"your query\",\n-        param=QueryParam(enable_rerank=True)  # Enable rerank for this query\n-    )\n-    ```\n-    \"\"\"\n-\n-    rerank_func: Callable[[Any], List[Dict]]\n-    kwargs: Dict[str, Any] = Field(default_factory=dict)\n-\n-    async def rerank(\n-        self,\n-        query: str,\n-        documents: List[Dict[str, Any]],\n-        top_n: Optional[int] = None,\n-        **extra_kwargs,\n-    ) -> List[Dict[str, Any]]:\n-        \"\"\"Rerank documents using the configured model function.\"\"\"\n-        # Merge extra kwargs with model kwargs\n-        kwargs = {**self.kwargs, **extra_kwargs}\n-        return await self.rerank_func(\n-            query=query, documents=documents, top_n=top_n, **kwargs\n-        )\n-\n-\n-class MultiRerankModel(BaseModel):\n-    \"\"\"Multiple rerank models for different modes/scenarios.\"\"\"\n-\n-    # Primary rerank model (used if mode-specific models are not defined)\n-    rerank_model: Optional[RerankModel] = None\n-\n-    # Mode-specific rerank models\n-    entity_rerank_model: Optional[RerankModel] = None\n-    relation_rerank_model: Optional[RerankModel] = None\n-    chunk_rerank_model: Optional[RerankModel] = None\n-\n-    async def rerank(\n-        self,\n-        query: str,\n-        documents: List[Dict[str, Any]],\n-        mode: str = \"default\",\n-        top_n: Optional[int] = None,\n-        **kwargs,\n-    ) -> List[Dict[str, Any]]:\n-        \"\"\"Rerank using the appropriate model based on mode.\"\"\"\n-\n-        # Select model based on mode\n-        if mode == \"entity\" and self.entity_rerank_model:\n-            model = self.entity_rerank_model\n-        elif mode == \"relation\" and self.relation_rerank_model:\n-            model = self.relation_rerank_model\n-        elif mode == \"chunk\" and self.chunk_rerank_model:\n-            model = self.chunk_rerank_model\n-        elif self.rerank_model:\n-            model = self.rerank_model\n-        else:\n-            logger.warning(f\"No rerank model available for mode: {mode}\")\n-            return documents\n-\n-        return await model.rerank(query, documents, top_n, **kwargs)\n+# use the .env that is inside the current folder\n+# allows to use different .env file for each lightrag instance\n+# the OS environment variables take precedence over the .env file\n+load_dotenv(dotenv_path=\".env\", override=False)\n \n \n+@retry(\n+    stop=stop_after_attempt(3),\n+    wait=wait_exponential(multiplier=1, min=4, max=60),\n+    retry=(\n+        retry_if_exception_type(aiohttp.ClientError)\n+        | retry_if_exception_type(aiohttp.ClientResponseError)\n+    ),\n+)\n async def generic_rerank_api(\n     query: str,\n-    documents: List[Dict[str, Any]],\n+    documents: List[str],\n     model: str,\n     base_url: str,\n-    api_key: str,\n+    api_key: Optional[str],\n     top_n: Optional[int] = None,\n-    **kwargs,\n+    return_documents: Optional[bool] = None,\n+    extra_body: Optional[Dict[str, Any]] = None,\n+    response_format: str = \"standard\",  # \"standard\" (Jina/Cohere) or \"aliyun\"\n+    request_format: str = \"standard\",  # \"standard\" (Jina/Cohere) or \"aliyun\"\n ) -> List[Dict[str, Any]]:\n     \"\"\"\n-    Generic rerank function that works with Jina/Cohere compatible APIs.\n+    Generic rerank API call for Jina/Cohere/Aliyun models.\n \n     Args:\n         query: The search query\n-        documents: List of documents to rerank\n-        model: Model identifier\n+        documents: List of strings to rerank\n+        model: Model name to use\n         base_url: API endpoint URL\n-        api_key: API authentication key\n+        api_key: API key for authentication\n         top_n: Number of top results to return\n-        **kwargs: Additional API-specific parameters\n+        return_documents: Whether to return document text (Jina only)\n+        extra_body: Additional body parameters\n+        response_format: Response format type (\"standard\" for Jina/Cohere, \"aliyun\" for Aliyun)\n \n     Returns:\n-        List of reranked documents with relevance scores\n+        List of dictionary of [\"index\": int, \"relevance_score\": float]\n     \"\"\"\n-    if not api_key:\n-        logger.warning(\"No API key provided for rerank service\")\n-        return documents\n-\n-    if not documents:\n-        return documents\n-\n-    # Prepare documents for reranking - handle both text and dict formats\n-    prepared_docs = []\n-    for doc in documents:\n-        if isinstance(doc, dict):\n-            # Use 'content' field if available, otherwise use 'text' or convert to string\n-            text = doc.get(\"content\") or doc.get(\"text\") or str(doc)\n-        else:\n-            text = str(doc)\n-        prepared_docs.append(text)\n-\n-    # Prepare request\n-    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n-\n-    data = {\"model\": model, \"query\": query, \"documents\": prepared_docs, **kwargs}\n-\n-    if top_n is not None:\n-        data[\"top_n\"] = min(top_n, len(prepared_docs))\n-\n-    try:\n-        async with aiohttp.ClientSession() as session:\n-            async with session.post(base_url, headers=headers, json=data) as response:\n-                if response.status != 200:\n-                    error_text = await response.text()\n-                    logger.error(f\"Rerank API error {response.status}: {error_text}\")\n-                    return documents\n-\n-                result = await response.json()\n-\n-                # Extract reranked results\n-                if \"results\" in result:\n-                    # Standard format: results contain index and relevance_score\n-                    reranked_docs = []\n-                    for item in result[\"results\"]:\n-                        if \"index\" in item:\n-                            doc_idx = item[\"index\"]\n-                            if 0 <= doc_idx < len(documents):\n-                                reranked_doc = documents[doc_idx].copy()\n-                                if \"relevance_score\" in item:\n-                                    reranked_doc[\"rerank_score\"] = item[\n-                                        \"relevance_score\"\n-                                    ]\n-                                reranked_docs.append(reranked_doc)\n-                    return reranked_docs\n-                else:\n-                    logger.warning(\"Unexpected rerank API response format\")\n-                    return documents\n+    if not base_url:\n+        raise ValueError(\"Base URL is required\")\n+\n+    headers = {\"Content-Type\": \"application/json\"}\n+    if api_key is not None:\n+        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n+\n+    # Build request payload based on request format\n+    if request_format == \"aliyun\":\n+        # Aliyun format: nested input/parameters structure\n+        payload = {\n+            \"model\": model,\n+            \"input\": {\n+                \"query\": query,\n+                \"documents\": documents,\n+            },\n+            \"parameters\": {},\n+        }\n \n-    except Exception as e:\n-        logger.error(f\"Error during reranking: {e}\")\n-        return documents\n+        # Add optional parameters to parameters object\n+        if top_n is not None:\n+            payload[\"parameters\"][\"top_n\"] = top_n\n+\n+        if return_documents is not None:\n+            payload[\"parameters\"][\"return_documents\"] = return_documents\n+\n+        # Add extra parameters to parameters object\n+        if extra_body:\n+            payload[\"parameters\"].update(extra_body)\n+    else:\n+        # Standard format for Jina/Cohere\n+        payload = {\n+            \"model\": model,\n+            \"query\": query,\n+            \"documents\": documents,\n+        }\n \n+        # Add optional parameters\n+        if top_n is not None:\n+            payload[\"top_n\"] = top_n\n \n-async def jina_rerank(\n+        # Only Jina API supports return_documents parameter\n+        if return_documents is not None:\n+            payload[\"return_documents\"] = return_documents\n+\n+        # Add extra parameters\n+        if extra_body:\n+            payload.update(extra_body)\n+\n+    logger.debug(\n+        f\"Rerank request: {len(documents)} documents, model: {model}, format: {response_format}\"\n+    )\n+\n+    async with aiohttp.ClientSession() as session:\n+        async with session.post(base_url, headers=headers, json=payload) as response:\n+            if response.status != 200:\n+                error_text = await response.text()\n+                content_type = response.headers.get(\"content-type\", \"\").lower()\n+                is_html_error = (\n+                    error_text.strip().startswith(\"<!DOCTYPE html>\")\n+                    or \"text/html\" in content_type\n+                )\n+                if is_html_error:\n+                    if response.status == 502:\n+                        clean_error = \"Bad Gateway (502) - Rerank service temporarily unavailable. Please try again in a few minutes.\"\n+                    elif response.status == 503:\n+                        clean_error = \"Service Unavailable (503) - Rerank service is temporarily overloaded. Please try again later.\"\n+                    elif response.status == 504:\n+                        clean_error = \"Gateway Timeout (504) - Rerank service request timed out. Please try again.\"\n+                    else:\n+                        clean_error = f\"HTTP {response.status} - Rerank service error. Please try again later.\"\n+                else:\n+                    clean_error = error_text\n+                logger.error(f\"Rerank API error {response.status}: {clean_error}\")\n+                raise aiohttp.ClientResponseError(\n+                    request_info=response.request_info,\n+                    history=response.history,\n+                    status=response.status,\n+                    message=f\"Rerank API error: {clean_error}\",\n+                )\n+\n+            response_json = await response.json()\n+\n+            if response_format == \"aliyun\":\n+                # Aliyun format: {\"output\": {\"results\": [...]}}\n+                results = response_json.get(\"output\", {}).get(\"results\", [])\n+                if not isinstance(results, list):\n+                    logger.warning(\n+                        f\"Expected 'output.results' to be list, got {type(results)}: {results}\"\n+                    )\n+                    results = []\n+\n+            elif response_format == \"standard\":\n+                # Standard format: {\"results\": [...]}\n+                results = response_json.get(\"results\", [])\n+                if not isinstance(results, list):\n+                    logger.warning(\n+                        f\"Expected 'results' to be list, got {type(results)}: {results}\"\n+                    )\n+                    results = []\n+            else:\n+                raise ValueError(f\"Unsupported response format: {response_format}\")\n+            if not results:\n+                logger.warning(\"Rerank API returned empty results\")\n+                return []\n+\n+            # Standardize return format\n+            return [\n+                {\"index\": result[\"index\"], \"relevance_score\": result[\"relevance_score\"]}\n+                for result in results\n+            ]\n+\n+\n+async def cohere_rerank(\n     query: str,\n-    documents: List[Dict[str, Any]],\n-    model: str = \"BAAI/bge-reranker-v2-m3\",\n+    documents: List[str],\n     top_n: Optional[int] = None,\n-    base_url: str = \"https://api.jina.ai/v1/rerank\",\n     api_key: Optional[str] = None,\n-    **kwargs,\n+    model: str = \"rerank-v3.5\",\n+    base_url: str = \"https://api.cohere.com/v2/rerank\",\n+    extra_body: Optional[Dict[str, Any]] = None,\n ) -> List[Dict[str, Any]]:\n     \"\"\"\n-    Rerank documents using Jina AI API.\n+    Rerank documents using Cohere API.\n \n     Args:\n         query: The search query\n-        documents: List of documents to rerank\n-        model: Jina rerank model name\n+        documents: List of strings to rerank\n         top_n: Number of top results to return\n-        base_url: Jina API endpoint\n-        api_key: Jina API key\n-        **kwargs: Additional parameters\n+        api_key: API key\n+        model: rerank model name\n+        base_url: API endpoint\n+        extra_body: Additional body for http request(reserved for extra params)\n \n     Returns:\n-        List of reranked documents with relevance scores\n+        List of dictionary of [\"index\": int, \"relevance_score\": float]\n     \"\"\"\n     if api_key is None:\n-        api_key = os.getenv(\"JINA_API_KEY\") or os.getenv(\"RERANK_API_KEY\")\n+        api_key = os.getenv(\"COHERE_API_KEY\") or os.getenv(\"RERANK_BINDING_API_KEY\")\n \n     return await generic_rerank_api(\n         query=query,\n@@ -236,36 +203,38 @@ async def jina_rerank(\n         base_url=base_url,\n         api_key=api_key,\n         top_n=top_n,\n-        **kwargs,\n+        return_documents=None,  # Cohere doesn't support this parameter\n+        extra_body=extra_body,\n+        response_format=\"standard\",\n     )\n \n \n-async def cohere_rerank(\n+async def jina_rerank(\n     query: str,\n-    documents: List[Dict[str, Any]],\n-    model: str = \"rerank-english-v2.0\",\n+    documents: List[str],\n     top_n: Optional[int] = None,\n-    base_url: str = \"https://api.cohere.ai/v1/rerank\",\n     api_key: Optional[str] = None,\n-    **kwargs,\n+    model: str = \"jina-reranker-v2-base-multilingual\",\n+    base_url: str = \"https://api.jina.ai/v1/rerank\",\n+    extra_body: Optional[Dict[str, Any]] = None,\n ) -> List[Dict[str, Any]]:\n     \"\"\"\n-    Rerank documents using Cohere API.\n+    Rerank documents using Jina AI API.\n \n     Args:\n         query: The search query\n-        documents: List of documents to rerank\n-        model: Cohere rerank model name\n+        documents: List of strings to rerank\n         top_n: Number of top results to return\n-        base_url: Cohere API endpoint\n-        api_key: Cohere API key\n-        **kwargs: Additional parameters\n+        api_key: API key\n+        model: rerank model name\n+        base_url: API endpoint\n+        extra_body: Additional body for http request(reserved for extra params)\n \n     Returns:\n-        List of reranked documents with relevance scores\n+        List of dictionary of [\"index\": int, \"relevance_score\": float]\n     \"\"\"\n     if api_key is None:\n-        api_key = os.getenv(\"COHERE_API_KEY\") or os.getenv(\"RERANK_API_KEY\")\n+        api_key = os.getenv(\"JINA_API_KEY\") or os.getenv(\"RERANK_BINDING_API_KEY\")\n \n     return await generic_rerank_api(\n         query=query,\n@@ -274,24 +243,39 @@ async def cohere_rerank(\n         base_url=base_url,\n         api_key=api_key,\n         top_n=top_n,\n-        **kwargs,\n+        return_documents=False,\n+        extra_body=extra_body,\n+        response_format=\"standard\",\n     )\n \n \n-# Convenience function for custom API endpoints\n-async def custom_rerank(\n+async def ali_rerank(\n     query: str,\n-    documents: List[Dict[str, Any]],\n-    model: str,\n-    base_url: str,\n-    api_key: str,\n+    documents: List[str],\n     top_n: Optional[int] = None,\n-    **kwargs,\n+    api_key: Optional[str] = None,\n+    model: str = \"gte-rerank-v2\",\n+    base_url: str = \"https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank\",\n+    extra_body: Optional[Dict[str, Any]] = None,\n ) -> List[Dict[str, Any]]:\n     \"\"\"\n-    Rerank documents using a custom API endpoint.\n-    This is useful for self-hosted or custom rerank services.\n+    Rerank documents using Aliyun DashScope API.\n+\n+    Args:\n+        query: The search query\n+        documents: List of strings to rerank\n+        top_n: Number of top results to return\n+        api_key: Aliyun API key\n+        model: rerank model name\n+        base_url: API endpoint\n+        extra_body: Additional body for http request(reserved for extra params)\n+\n+    Returns:\n+        List of dictionary of [\"index\": int, \"relevance_score\": float]\n     \"\"\"\n+    if api_key is None:\n+        api_key = os.getenv(\"DASHSCOPE_API_KEY\") or os.getenv(\"RERANK_BINDING_API_KEY\")\n+\n     return await generic_rerank_api(\n         query=query,\n         documents=documents,\n@@ -299,26 +283,72 @@ async def custom_rerank(\n         base_url=base_url,\n         api_key=api_key,\n         top_n=top_n,\n-        **kwargs,\n+        return_documents=False,  # Aliyun doesn't need this parameter\n+        extra_body=extra_body,\n+        response_format=\"aliyun\",\n+        request_format=\"aliyun\",\n     )\n \n \n+\"\"\"Please run this test as a module:\n+python -m lightrag.rerank\n+\"\"\"\n if __name__ == \"__main__\":\n     import asyncio\n \n     async def main():\n-        # Example usage\n+        # Example usage - documents should be strings, not dictionaries\n         docs = [\n-            {\"content\": \"The capital of France is Paris.\"},\n-            {\"content\": \"Tokyo is the capital of Japan.\"},\n-            {\"content\": \"London is the capital of England.\"},\n+            \"The capital of France is Paris.\",\n+            \"Tokyo is the capital of Japan.\",\n+            \"London is the capital of England.\",\n         ]\n \n         query = \"What is the capital of France?\"\n \n-        result = await jina_rerank(\n-            query=query, documents=docs, top_n=2, api_key=\"your-api-key-here\"\n-        )\n-        print(result)\n+        # Test Jina rerank\n+        try:\n+            print(\"=== Jina Rerank ===\")\n+            result = await jina_rerank(\n+                query=query,\n+                documents=docs,\n+                top_n=2,\n+            )\n+            print(\"Results:\")\n+            for item in result:\n+                print(f\"Index: {item['index']}, Score: {item['relevance_score']:.4f}\")\n+                print(f\"Document: {docs[item['index']]}\")\n+        except Exception as e:\n+            print(f\"Jina Error: {e}\")\n+\n+        # Test Cohere rerank\n+        try:\n+            print(\"\\n=== Cohere Rerank ===\")\n+            result = await cohere_rerank(\n+                query=query,\n+                documents=docs,\n+                top_n=2,\n+            )\n+            print(\"Results:\")\n+            for item in result:\n+                print(f\"Index: {item['index']}, Score: {item['relevance_score']:.4f}\")\n+                print(f\"Document: {docs[item['index']]}\")\n+        except Exception as e:\n+            print(f\"Cohere Error: {e}\")\n+\n+        # Test Aliyun rerank\n+        try:\n+            print(\"\\n=== Aliyun Rerank ===\")\n+            result = await ali_rerank(\n+                query=query,\n+                documents=docs,\n+                top_n=2,\n+            )\n+            print(\"Results:\")\n+            for item in result:\n+                print(f\"Index: {item['index']}, Score: {item['relevance_score']:.4f}\")\n+                print(f\"Document: {docs[item['index']]}\")\n+        except Exception as e:\n+            print(f\"Aliyun Error: {e}\")\n \n     asyncio.run(main())\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/tools/check_initialization.py",
            "diff": "diff --git a/lightrag/tools/check_initialization.py b/lightrag/tools/check_initialization.py\nnew file mode 100644\nindex 00000000..6bcb17e3\n--- /dev/null\n+++ b/lightrag/tools/check_initialization.py\n@@ -0,0 +1,180 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Diagnostic tool to check LightRAG initialization status.\n+\n+This tool helps developers verify that their LightRAG instance is properly\n+initialized before use, preventing common initialization errors.\n+\n+Usage:\n+    python -m lightrag.tools.check_initialization\n+\"\"\"\n+\n+import asyncio\n+import sys\n+from pathlib import Path\n+\n+# Add parent directory to path for imports\n+sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n+\n+from lightrag import LightRAG\n+from lightrag.base import StoragesStatus\n+\n+\n+async def check_lightrag_setup(rag_instance: LightRAG, verbose: bool = False) -> bool:\n+    \"\"\"\n+    Check if a LightRAG instance is properly initialized.\n+\n+    Args:\n+        rag_instance: The LightRAG instance to check\n+        verbose: If True, print detailed diagnostic information\n+\n+    Returns:\n+        True if properly initialized, False otherwise\n+    \"\"\"\n+    issues = []\n+    warnings = []\n+\n+    print(\"\ud83d\udd0d Checking LightRAG initialization status...\\n\")\n+\n+    # Check storage initialization status\n+    if not hasattr(rag_instance, \"_storages_status\"):\n+        issues.append(\"LightRAG instance missing _storages_status attribute\")\n+    elif rag_instance._storages_status != StoragesStatus.INITIALIZED:\n+        issues.append(\n+            f\"Storages not initialized (status: {rag_instance._storages_status.name})\"\n+        )\n+    else:\n+        print(\"\u2705 Storage status: INITIALIZED\")\n+\n+    # Check individual storage components\n+    storage_components = [\n+        (\"full_docs\", \"Document storage\"),\n+        (\"text_chunks\", \"Text chunks storage\"),\n+        (\"entities_vdb\", \"Entity vector database\"),\n+        (\"relationships_vdb\", \"Relationship vector database\"),\n+        (\"chunks_vdb\", \"Chunks vector database\"),\n+        (\"doc_status\", \"Document status tracker\"),\n+        (\"llm_response_cache\", \"LLM response cache\"),\n+        (\"full_entities\", \"Entity storage\"),\n+        (\"full_relations\", \"Relation storage\"),\n+        (\"chunk_entity_relation_graph\", \"Graph storage\"),\n+    ]\n+\n+    if verbose:\n+        print(\"\\n\ud83d\udce6 Storage Components:\")\n+\n+    for component, description in storage_components:\n+        if not hasattr(rag_instance, component):\n+            issues.append(f\"Missing storage component: {component} ({description})\")\n+        else:\n+            storage = getattr(rag_instance, component)\n+            if storage is None:\n+                warnings.append(f\"Storage {component} is None (might be optional)\")\n+            elif hasattr(storage, \"_storage_lock\"):\n+                if storage._storage_lock is None:\n+                    issues.append(f\"Storage {component} not initialized (lock is None)\")\n+                elif verbose:\n+                    print(f\"  \u2705 {description}: Ready\")\n+            elif verbose:\n+                print(f\"  \u2705 {description}: Ready\")\n+\n+    # Check pipeline status\n+    try:\n+        from lightrag.kg.shared_storage import get_namespace_data\n+\n+        get_namespace_data(\"pipeline_status\")\n+        print(\"\u2705 Pipeline status: INITIALIZED\")\n+    except KeyError:\n+        issues.append(\n+            \"Pipeline status not initialized - call initialize_pipeline_status()\"\n+        )\n+    except Exception as e:\n+        issues.append(f\"Error checking pipeline status: {str(e)}\")\n+\n+    # Print results\n+    print(\"\\n\" + \"=\" * 50)\n+\n+    if issues:\n+        print(\"\u274c Issues found:\\n\")\n+        for issue in issues:\n+            print(f\"  \u2022 {issue}\")\n+\n+        print(\"\\n\ud83d\udcdd To fix, run this initialization sequence:\\n\")\n+        print(\"  await rag.initialize_storages()\")\n+        print(\"  from lightrag.kg.shared_storage import initialize_pipeline_status\")\n+        print(\"  await initialize_pipeline_status()\")\n+        print(\n+            \"\\n\ud83d\udcda Documentation: https://github.com/HKUDS/LightRAG#important-initialization-requirements\"\n+        )\n+\n+        if warnings and verbose:\n+            print(\"\\n\u26a0\ufe0f  Warnings (might be normal):\")\n+            for warning in warnings:\n+                print(f\"  \u2022 {warning}\")\n+\n+        return False\n+    else:\n+        print(\"\u2705 LightRAG is properly initialized and ready to use!\")\n+\n+        if warnings and verbose:\n+            print(\"\\n\u26a0\ufe0f  Warnings (might be normal):\")\n+            for warning in warnings:\n+                print(f\"  \u2022 {warning}\")\n+\n+        return True\n+\n+\n+async def demo():\n+    \"\"\"Demonstrate the diagnostic tool with a test instance.\"\"\"\n+    from lightrag.llm.openai import openai_embed, gpt_4o_mini_complete\n+    from lightrag.kg.shared_storage import initialize_pipeline_status\n+\n+    print(\"=\" * 50)\n+    print(\"LightRAG Initialization Diagnostic Tool\")\n+    print(\"=\" * 50)\n+\n+    # Create test instance\n+    rag = LightRAG(\n+        working_dir=\"./test_diagnostic\",\n+        embedding_func=openai_embed,\n+        llm_model_func=gpt_4o_mini_complete,\n+    )\n+\n+    print(\"\\n\ud83d\udd34 BEFORE initialization:\\n\")\n+    await check_lightrag_setup(rag, verbose=True)\n+\n+    print(\"\\n\" + \"=\" * 50)\n+    print(\"\\n\ud83d\udd04 Initializing...\\n\")\n+    await rag.initialize_storages()\n+    await initialize_pipeline_status()\n+\n+    print(\"\\n\ud83d\udfe2 AFTER initialization:\\n\")\n+    await check_lightrag_setup(rag, verbose=True)\n+\n+    # Cleanup\n+    import shutil\n+\n+    shutil.rmtree(\"./test_diagnostic\", ignore_errors=True)\n+\n+\n+if __name__ == \"__main__\":\n+    import argparse\n+\n+    parser = argparse.ArgumentParser(description=\"Check LightRAG initialization status\")\n+    parser.add_argument(\n+        \"--demo\", action=\"store_true\", help=\"Run a demonstration with a test instance\"\n+    )\n+    parser.add_argument(\n+        \"--verbose\",\n+        \"-v\",\n+        action=\"store_true\",\n+        help=\"Show detailed diagnostic information\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    if args.demo:\n+        asyncio.run(demo())\n+    else:\n+        print(\"Run with --demo to see the diagnostic tool in action\")\n+        print(\"Or import this module and use check_lightrag_setup() with your instance\")\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag/utils.py",
            "diff": "diff --git a/lightrag/utils.py b/lightrag/utils.py\nindex 979517b5..87ce5b6a 100644\n--- a/lightrag/utils.py\n+++ b/lightrag/utils.py\n@@ -27,17 +27,31 @@ from lightrag.constants import (\n     DEFAULT_MAX_FILE_PATH_LENGTH,\n )\n \n+# Initialize logger with basic configuration\n+logger = logging.getLogger(\"lightrag\")\n+logger.propagate = False  # prevent log message send to root logger\n+logger.setLevel(logging.INFO)\n+\n+# Add console handler if no handlers exist\n+if not logger.handlers:\n+    console_handler = logging.StreamHandler()\n+    console_handler.setLevel(logging.INFO)\n+    formatter = logging.Formatter(\"%(levelname)s: %(message)s\")\n+    console_handler.setFormatter(formatter)\n+    logger.addHandler(console_handler)\n+\n+# Set httpx logging level to WARNING\n+logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n+\n # Global import for pypinyin with startup-time logging\n try:\n     import pypinyin\n \n     _PYPINYIN_AVAILABLE = True\n-    logger = logging.getLogger(\"lightrag\")\n-    logger.info(\"pypinyin loaded successfully for Chinese pinyin sorting\")\n+    # logger.info(\"pypinyin loaded successfully for Chinese pinyin sorting\")\n except ImportError:\n     pypinyin = None\n     _PYPINYIN_AVAILABLE = False\n-    logger = logging.getLogger(\"lightrag\")\n     logger.warning(\n         \"pypinyin is not installed. Chinese pinyin sorting will use simple string sorting.\"\n     )\n@@ -121,15 +135,6 @@ def set_verbose_debug(enabled: bool):\n \n statistic_data = {\"llm_call\": 0, \"llm_cache\": 0, \"embed_call\": 0}\n \n-# Initialize logger\n-logger = logging.getLogger(\"lightrag\")\n-logger.propagate = False  # prevent log message send to root loggger\n-# Let the main application configure the handlers\n-logger.setLevel(logging.INFO)\n-\n-# Set httpx logging level to WARNING\n-logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n-\n \n class LightragPathFilter(logging.Filter):\n     \"\"\"Filter for lightrag logger to filter out frequent path access logs\"\"\"\n@@ -254,6 +259,18 @@ class UnlimitedSemaphore:\n         pass\n \n \n+@dataclass\n+class TaskState:\n+    \"\"\"Task state tracking for priority queue management\"\"\"\n+\n+    future: asyncio.Future\n+    start_time: float\n+    execution_start_time: float = None\n+    worker_started: bool = False\n+    cancellation_requested: bool = False\n+    cleanup_done: bool = False\n+\n+\n @dataclass\n class EmbeddingFunc:\n     embedding_dim: int\n@@ -323,20 +340,58 @@ def parse_cache_key(cache_key: str) -> tuple[str, str, str] | None:\n     return None\n \n \n-# Custom exception class\n+# Custom exception classes\n class QueueFullError(Exception):\n     \"\"\"Raised when the queue is full and the wait times out\"\"\"\n \n     pass\n \n \n-def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n+class WorkerTimeoutError(Exception):\n+    \"\"\"Worker-level timeout exception with specific timeout information\"\"\"\n+\n+    def __init__(self, timeout_value: float, timeout_type: str = \"execution\"):\n+        self.timeout_value = timeout_value\n+        self.timeout_type = timeout_type\n+        super().__init__(f\"Worker {timeout_type} timeout after {timeout_value}s\")\n+\n+\n+class HealthCheckTimeoutError(Exception):\n+    \"\"\"Health Check-level timeout exception\"\"\"\n+\n+    def __init__(self, timeout_value: float, execution_duration: float):\n+        self.timeout_value = timeout_value\n+        self.execution_duration = execution_duration\n+        super().__init__(\n+            f\"Task forcefully terminated due to execution timeout (>{timeout_value}s, actual: {execution_duration:.1f}s)\"\n+        )\n+\n+\n+def priority_limit_async_func_call(\n+    max_size: int,\n+    llm_timeout: float = None,\n+    max_execution_timeout: float = None,\n+    max_task_duration: float = None,\n+    max_queue_size: int = 1000,\n+    cleanup_timeout: float = 2.0,\n+):\n     \"\"\"\n-    Enhanced priority-limited asynchronous function call decorator\n+    Enhanced priority-limited asynchronous function call decorator with robust timeout handling\n+\n+    This decorator provides a comprehensive solution for managing concurrent LLM requests with:\n+    - Multi-layer timeout protection (LLM -> Worker -> Health Check -> User)\n+    - Task state tracking to prevent race conditions\n+    - Enhanced health check system with stuck task detection\n+    - Proper resource cleanup and error recovery\n \n     Args:\n         max_size: Maximum number of concurrent calls\n         max_queue_size: Maximum queue capacity to prevent memory overflow\n+        llm_timeout: LLM provider timeout (from global config), used to calculate other timeouts\n+        max_execution_timeout: Maximum time for worker to execute function (defaults to llm_timeout + 30s)\n+        max_task_duration: Maximum time before health check intervenes (defaults to llm_timeout + 60s)\n+        cleanup_timeout: Maximum time to wait for cleanup operations (defaults to 2.0s)\n+\n     Returns:\n         Decorator function\n     \"\"\"\n@@ -345,81 +400,173 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n         # Ensure func is callable\n         if not callable(func):\n             raise TypeError(f\"Expected a callable object, got {type(func)}\")\n+\n+        # Calculate timeout hierarchy if llm_timeout is provided (Dynamic Timeout Calculation)\n+        if llm_timeout is not None:\n+            nonlocal max_execution_timeout, max_task_duration\n+            if max_execution_timeout is None:\n+                max_execution_timeout = (\n+                    llm_timeout + 30\n+                )  # LLM timeout + 30s buffer for network delays\n+            if max_task_duration is None:\n+                max_task_duration = (\n+                    llm_timeout + 60\n+                )  # LLM timeout + 1min buffer for execution phase\n+\n         queue = asyncio.PriorityQueue(maxsize=max_queue_size)\n         tasks = set()\n         initialization_lock = asyncio.Lock()\n         counter = 0\n         shutdown_event = asyncio.Event()\n-        initialized = False  # Global initialization flag\n+        initialized = False\n         worker_health_check_task = None\n \n-        # Track active future objects for cleanup\n+        # Enhanced task state management\n+        task_states = {}  # task_id -> TaskState\n+        task_states_lock = asyncio.Lock()\n         active_futures = weakref.WeakSet()\n-        reinit_count = 0  # Reinitialization counter to track system health\n+        reinit_count = 0\n \n-        # Worker function to process tasks in the queue\n         async def worker():\n-            \"\"\"Worker that processes tasks in the priority queue\"\"\"\n+            \"\"\"Enhanced worker that processes tasks with proper timeout and state management\"\"\"\n             try:\n                 while not shutdown_event.is_set():\n                     try:\n-                        # Use timeout to get tasks, allowing periodic checking of shutdown signal\n+                        # Get task from queue with timeout for shutdown checking\n                         try:\n                             (\n                                 priority,\n                                 count,\n-                                future,\n+                                task_id,\n                                 args,\n                                 kwargs,\n                             ) = await asyncio.wait_for(queue.get(), timeout=1.0)\n                         except asyncio.TimeoutError:\n-                            # Timeout is just to check shutdown signal, continue to next iteration\n                             continue\n \n-                        # If future is cancelled, skip execution\n-                        if future.cancelled():\n+                        # Get task state and mark worker as started\n+                        async with task_states_lock:\n+                            if task_id not in task_states:\n+                                queue.task_done()\n+                                continue\n+                            task_state = task_states[task_id]\n+                            task_state.worker_started = True\n+                            # Record execution start time when worker actually begins processing\n+                            task_state.execution_start_time = (\n+                                asyncio.get_event_loop().time()\n+                            )\n+\n+                        # Check if task was cancelled before worker started\n+                        if (\n+                            task_state.cancellation_requested\n+                            or task_state.future.cancelled()\n+                        ):\n+                            async with task_states_lock:\n+                                task_states.pop(task_id, None)\n                             queue.task_done()\n                             continue\n \n                         try:\n-                            # Execute function\n-                            result = await func(*args, **kwargs)\n-                            # If future is not done, set the result\n-                            if not future.done():\n-                                future.set_result(result)\n+                            # Execute function with timeout protection\n+                            if max_execution_timeout is not None:\n+                                result = await asyncio.wait_for(\n+                                    func(*args, **kwargs), timeout=max_execution_timeout\n+                                )\n+                            else:\n+                                result = await func(*args, **kwargs)\n+\n+                            # Set result if future is still valid\n+                            if not task_state.future.done():\n+                                task_state.future.set_result(result)\n+\n+                        except asyncio.TimeoutError:\n+                            # Worker-level timeout (max_execution_timeout exceeded)\n+                            logger.warning(\n+                                f\"limit_async: Worker timeout for task {task_id} after {max_execution_timeout}s\"\n+                            )\n+                            if not task_state.future.done():\n+                                task_state.future.set_exception(\n+                                    WorkerTimeoutError(\n+                                        max_execution_timeout, \"execution\"\n+                                    )\n+                                )\n                         except asyncio.CancelledError:\n-                            if not future.done():\n-                                future.cancel()\n-                            logger.debug(\"limit_async: Task cancelled during execution\")\n+                            # Task was cancelled during execution\n+                            if not task_state.future.done():\n+                                task_state.future.cancel()\n+                            logger.debug(\n+                                f\"limit_async: Task {task_id} cancelled during execution\"\n+                            )\n                         except Exception as e:\n+                            # Function execution error\n                             logger.error(\n-                                f\"limit_async: Error in decorated function: {str(e)}\"\n+                                f\"limit_async: Error in decorated function for task {task_id}: {str(e)}\"\n                             )\n-                            if not future.done():\n-                                future.set_exception(e)\n+                            if not task_state.future.done():\n+                                task_state.future.set_exception(e)\n                         finally:\n+                            # Clean up task state\n+                            async with task_states_lock:\n+                                task_states.pop(task_id, None)\n                             queue.task_done()\n+\n                     except Exception as e:\n-                        # Catch all exceptions in worker loop to prevent worker termination\n+                        # Critical error in worker loop\n                         logger.error(f\"limit_async: Critical error in worker: {str(e)}\")\n-                        await asyncio.sleep(0.1)  # Prevent high CPU usage\n+                        await asyncio.sleep(0.1)\n             finally:\n                 logger.debug(\"limit_async: Worker exiting\")\n \n-        async def health_check():\n-            \"\"\"Periodically check worker health status and recover\"\"\"\n+        async def enhanced_health_check():\n+            \"\"\"Enhanced health check with stuck task detection and recovery\"\"\"\n             nonlocal initialized\n             try:\n                 while not shutdown_event.is_set():\n                     await asyncio.sleep(5)  # Check every 5 seconds\n \n-                    # No longer acquire lock, directly operate on task set\n-                    # Use a copy of the task set to avoid concurrent modification\n+                    current_time = asyncio.get_event_loop().time()\n+\n+                    # Detect and handle stuck tasks based on execution start time\n+                    if max_task_duration is not None:\n+                        stuck_tasks = []\n+                        async with task_states_lock:\n+                            for task_id, task_state in list(task_states.items()):\n+                                # Only check tasks that have started execution\n+                                if (\n+                                    task_state.worker_started\n+                                    and task_state.execution_start_time is not None\n+                                    and current_time - task_state.execution_start_time\n+                                    > max_task_duration\n+                                ):\n+                                    stuck_tasks.append(\n+                                        (\n+                                            task_id,\n+                                            current_time\n+                                            - task_state.execution_start_time,\n+                                        )\n+                                    )\n+\n+                        # Force cleanup of stuck tasks\n+                        for task_id, execution_duration in stuck_tasks:\n+                            logger.warning(\n+                                f\"limit_async: Detected stuck task {task_id} (execution time: {execution_duration:.1f}s), forcing cleanup\"\n+                            )\n+                            async with task_states_lock:\n+                                if task_id in task_states:\n+                                    task_state = task_states[task_id]\n+                                    if not task_state.future.done():\n+                                        task_state.future.set_exception(\n+                                            HealthCheckTimeoutError(\n+                                                max_task_duration, execution_duration\n+                                            )\n+                                        )\n+                                    task_states.pop(task_id, None)\n+\n+                    # Worker recovery logic\n                     current_tasks = set(tasks)\n                     done_tasks = {t for t in current_tasks if t.done()}\n                     tasks.difference_update(done_tasks)\n \n-                    # Calculate active tasks count\n                     active_tasks_count = len(tasks)\n                     workers_needed = max_size - active_tasks_count\n \n@@ -432,21 +579,16 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n                             task = asyncio.create_task(worker())\n                             new_tasks.add(task)\n                             task.add_done_callback(tasks.discard)\n-                        # Update task set in one operation\n                         tasks.update(new_tasks)\n+\n             except Exception as e:\n-                logger.error(f\"limit_async: Error in health check: {str(e)}\")\n+                logger.error(f\"limit_async: Error in enhanced health check: {str(e)}\")\n             finally:\n-                logger.debug(\"limit_async: Health check task exiting\")\n+                logger.debug(\"limit_async: Enhanced health check task exiting\")\n                 initialized = False\n \n         async def ensure_workers():\n-            \"\"\"Ensure worker threads and health check system are available\n-\n-            This function checks if the worker system is already initialized.\n-            If not, it performs a one-time initialization of all worker threads\n-            and starts the health check system.\n-            \"\"\"\n+            \"\"\"Ensure worker system is initialized with enhanced error handling\"\"\"\n             nonlocal initialized, worker_health_check_task, tasks, reinit_count\n \n             if initialized:\n@@ -456,45 +598,56 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n                 if initialized:\n                     return\n \n-                # Increment reinitialization counter if this is not the first initialization\n                 if reinit_count > 0:\n                     reinit_count += 1\n                     logger.warning(\n-                        f\"limit_async: Reinitializing needed (count: {reinit_count})\"\n+                        f\"limit_async: Reinitializing system (count: {reinit_count})\"\n                     )\n                 else:\n-                    reinit_count = 1  # First initialization\n+                    reinit_count = 1\n \n-                # Check for completed tasks and remove them from the task set\n+                # Clean up completed tasks\n                 current_tasks = set(tasks)\n                 done_tasks = {t for t in current_tasks if t.done()}\n                 tasks.difference_update(done_tasks)\n \n-                # Log active tasks count during reinitialization\n                 active_tasks_count = len(tasks)\n                 if active_tasks_count > 0 and reinit_count > 1:\n                     logger.warning(\n                         f\"limit_async: {active_tasks_count} tasks still running during reinitialization\"\n                     )\n \n-                # Create initial worker tasks, only adding the number needed\n+                # Create worker tasks\n                 workers_needed = max_size - active_tasks_count\n                 for _ in range(workers_needed):\n                     task = asyncio.create_task(worker())\n                     tasks.add(task)\n                     task.add_done_callback(tasks.discard)\n \n-                # Start health check\n-                worker_health_check_task = asyncio.create_task(health_check())\n+                # Start enhanced health check\n+                worker_health_check_task = asyncio.create_task(enhanced_health_check())\n \n                 initialized = True\n-                logger.info(f\"limit_async: {workers_needed} new workers initialized\")\n+                # Log dynamic timeout configuration\n+                timeout_info = []\n+                if llm_timeout is not None:\n+                    timeout_info.append(f\"Func: {llm_timeout}s\")\n+                if max_execution_timeout is not None:\n+                    timeout_info.append(f\"Worker: {max_execution_timeout}s\")\n+                if max_task_duration is not None:\n+                    timeout_info.append(f\"Health Check: {max_task_duration}s\")\n+\n+                timeout_str = (\n+                    f\" (Timeouts: {', '.join(timeout_info)})\" if timeout_info else \"\"\n+                )\n+                logger.info(\n+                    f\"limit_async: {workers_needed} new workers initialized {timeout_str}\"\n+                )\n \n         async def shutdown():\n-            \"\"\"Gracefully shut down all workers and the queue\"\"\"\n+            \"\"\"Gracefully shut down all workers and cleanup resources\"\"\"\n             logger.info(\"limit_async: Shutting down priority queue workers\")\n \n-            # Set the shutdown event\n             shutdown_event.set()\n \n             # Cancel all active futures\n@@ -502,7 +655,14 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n                 if not future.done():\n                     future.cancel()\n \n-            # Wait for the queue to empty\n+            # Cancel all pending tasks\n+            async with task_states_lock:\n+                for task_id, task_state in list(task_states.items()):\n+                    if not task_state.future.done():\n+                        task_state.future.cancel()\n+                task_states.clear()\n+\n+            # Wait for queue to empty with timeout\n             try:\n                 await asyncio.wait_for(queue.join(), timeout=5.0)\n             except asyncio.TimeoutError:\n@@ -510,7 +670,7 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n                     \"limit_async: Timeout waiting for queue to empty during shutdown\"\n                 )\n \n-            # Cancel all worker tasks\n+            # Cancel worker tasks\n             for task in list(tasks):\n                 if not task.done():\n                     task.cancel()\n@@ -519,7 +679,7 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n             if tasks:\n                 await asyncio.gather(*tasks, return_exceptions=True)\n \n-            # Cancel the health check task\n+            # Cancel health check task\n             if worker_health_check_task and not worker_health_check_task.done():\n                 worker_health_check_task.cancel()\n                 try:\n@@ -534,77 +694,113 @@ def priority_limit_async_func_call(max_size: int, max_queue_size: int = 1000):\n             *args, _priority=10, _timeout=None, _queue_timeout=None, **kwargs\n         ):\n             \"\"\"\n-            Execute the function with priority-based concurrency control\n+            Execute function with enhanced priority-based concurrency control and timeout handling\n+\n             Args:\n                 *args: Positional arguments passed to the function\n                 _priority: Call priority (lower values have higher priority)\n-                _timeout: Maximum time to wait for function completion (in seconds)\n+                _timeout: Maximum time to wait for completion (in seconds, none means determinded by max_execution_timeout of the queue)\n                 _queue_timeout: Maximum time to wait for entering the queue (in seconds)\n                 **kwargs: Keyword arguments passed to the function\n+\n             Returns:\n                 The result of the function call\n+\n             Raises:\n-                TimeoutError: If the function call times out\n+                TimeoutError: If the function call times out at any level\n                 QueueFullError: If the queue is full and waiting times out\n                 Any exception raised by the decorated function\n             \"\"\"\n-            # Ensure worker system is initialized\n             await ensure_workers()\n \n-            # Create a future for the result\n+            # Generate unique task ID\n+            task_id = f\"{id(asyncio.current_task())}_{asyncio.get_event_loop().time()}\"\n             future = asyncio.Future()\n-            active_futures.add(future)\n \n-            nonlocal counter\n-            async with initialization_lock:\n-                current_count = counter  # Use local variable to avoid race conditions\n-                counter += 1\n+            # Create task state\n+            task_state = TaskState(\n+                future=future, start_time=asyncio.get_event_loop().time()\n+            )\n \n-            # Try to put the task into the queue, supporting timeout\n             try:\n-                if _queue_timeout is not None:\n-                    # Use timeout to wait for queue space\n-                    try:\n+                # Register task state\n+                async with task_states_lock:\n+                    task_states[task_id] = task_state\n+\n+                active_futures.add(future)\n+\n+                # Get counter for FIFO ordering\n+                nonlocal counter\n+                async with initialization_lock:\n+                    current_count = counter\n+                    counter += 1\n+\n+                # Queue the task with timeout handling\n+                try:\n+                    if _queue_timeout is not None:\n                         await asyncio.wait_for(\n-                            # current_count is used to ensure FIFO order\n-                            queue.put((_priority, current_count, future, args, kwargs)),\n+                            queue.put(\n+                                (_priority, current_count, task_id, args, kwargs)\n+                            ),\n                             timeout=_queue_timeout,\n                         )\n-                    except asyncio.TimeoutError:\n-                        raise QueueFullError(\n-                            f\"Queue full, timeout after {_queue_timeout} seconds\"\n+                    else:\n+                        await queue.put(\n+                            (_priority, current_count, task_id, args, kwargs)\n                         )\n-                else:\n-                    # No timeout, may wait indefinitely\n-                    # current_count is used to ensure FIFO order\n-                    await queue.put((_priority, current_count, future, args, kwargs))\n-            except Exception as e:\n-                # Clean up the future\n-                if not future.done():\n-                    future.set_exception(e)\n-                active_futures.discard(future)\n-                raise\n+                except asyncio.TimeoutError:\n+                    raise QueueFullError(\n+                        f\"Queue full, timeout after {_queue_timeout} seconds\"\n+                    )\n+                except Exception as e:\n+                    # Clean up on queue error\n+                    if not future.done():\n+                        future.set_exception(e)\n+                    raise\n \n-            try:\n-                # Wait for the result, optional timeout\n-                if _timeout is not None:\n-                    try:\n+                # Wait for result with timeout handling\n+                try:\n+                    if _timeout is not None:\n                         return await asyncio.wait_for(future, _timeout)\n-                    except asyncio.TimeoutError:\n-                        # Cancel the future\n-                        if not future.done():\n-                            future.cancel()\n-                        raise TimeoutError(\n-                            f\"limit_async: Task timed out after {_timeout} seconds\"\n-                        )\n-                else:\n-                    # Wait for the result without timeout\n-                    return await future\n+                    else:\n+                        return await future\n+                except asyncio.TimeoutError:\n+                    # This is user-level timeout (asyncio.wait_for caused)\n+                    # Mark cancellation request\n+                    async with task_states_lock:\n+                        if task_id in task_states:\n+                            task_states[task_id].cancellation_requested = True\n+\n+                    # Cancel future\n+                    if not future.done():\n+                        future.cancel()\n+\n+                    # Wait for worker cleanup with timeout\n+                    cleanup_start = asyncio.get_event_loop().time()\n+                    while (\n+                        task_id in task_states\n+                        and asyncio.get_event_loop().time() - cleanup_start\n+                        < cleanup_timeout\n+                    ):\n+                        await asyncio.sleep(0.1)\n+\n+                    raise TimeoutError(\n+                        f\"limit_async: User timeout after {_timeout} seconds\"\n+                    )\n+                except WorkerTimeoutError as e:\n+                    # This is Worker-level timeout, directly propagate exception information\n+                    raise TimeoutError(f\"limit_async: {str(e)}\")\n+                except HealthCheckTimeoutError as e:\n+                    # This is Health Check-level timeout, directly propagate exception information\n+                    raise TimeoutError(f\"limit_async: {str(e)}\")\n+\n             finally:\n-                # Clean up the future reference\n+                # Ensure cleanup\n                 active_futures.discard(future)\n+                async with task_states_lock:\n+                    task_states.pop(task_id, None)\n \n-        # Add the shutdown method to the decorated function\n+        # Add shutdown method to decorated function\n         wait_func.shutdown = shutdown\n \n         return wait_func\n@@ -1470,7 +1666,14 @@ async def use_llm_func_with_cache(\n         if max_tokens is not None:\n             kwargs[\"max_tokens\"] = max_tokens\n \n-        res: str = await use_llm_func(safe_input_text, **kwargs)\n+        try:\n+            res: str = await use_llm_func(safe_input_text, **kwargs)\n+        except Exception as e:\n+            # Add [LLM func] prefix to error message\n+            error_msg = f\"[LLM func] {str(e)}\"\n+            # Re-raise with the same exception type but modified message\n+            raise type(e)(error_msg) from e\n+\n         res = remove_think_tags(res)\n \n         if llm_response_cache.global_config.get(\"enable_llm_cache_for_entity_extract\"):\n@@ -1498,8 +1701,14 @@ async def use_llm_func_with_cache(\n     if max_tokens is not None:\n         kwargs[\"max_tokens\"] = max_tokens\n \n-    logger.info(f\"Call LLM function with query text length: {len(safe_input_text)}\")\n-    res = await use_llm_func(safe_input_text, **kwargs)\n+    try:\n+        res = await use_llm_func(safe_input_text, **kwargs)\n+    except Exception as e:\n+        # Add [LLM func] prefix to error message\n+        error_msg = f\"[LLM func] {str(e)}\"\n+        # Re-raise with the same exception type but modified message\n+        raise type(e)(error_msg) from e\n+\n     return remove_think_tags(res)\n \n \n@@ -1577,7 +1786,7 @@ def sanitize_text_for_encoding(text: str, replacement_char: str = \"\") -> str:\n     \"\"\"Sanitize text to ensure safe UTF-8 encoding by removing or replacing problematic characters.\n \n     This function handles:\n-    - Surrogate characters (the main cause of the encoding error)\n+    - Surrogate characters (the main cause of encoding errors)\n     - Other invalid Unicode sequences\n     - Control characters that might cause issues\n     - Whitespace trimming\n@@ -1588,6 +1797,9 @@ def sanitize_text_for_encoding(text: str, replacement_char: str = \"\") -> str:\n \n     Returns:\n         Sanitized text that can be safely encoded as UTF-8\n+\n+    Raises:\n+        ValueError: When text contains uncleanable encoding issues that cannot be safely processed\n     \"\"\"\n     if not isinstance(text, str):\n         return str(text)\n@@ -1624,7 +1836,7 @@ def sanitize_text_for_encoding(text: str, replacement_char: str = \"\") -> str:\n             else:\n                 sanitized += char\n \n-        # Additional cleanup: remove null bytes  and other control characters that might cause issues\n+        # Additional cleanup: remove null bytes and other control characters that might cause issues\n         # (but preserve common whitespace like \\t, \\n, \\r)\n         sanitized = re.sub(\n             r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\", replacement_char, sanitized\n@@ -1636,34 +1848,21 @@ def sanitize_text_for_encoding(text: str, replacement_char: str = \"\") -> str:\n         return sanitized\n \n     except UnicodeEncodeError as e:\n-        logger.warning(\n-            f\"Text sanitization: UnicodeEncodeError encountered, applying aggressive cleaning: {str(e)[:100]}\"\n-        )\n-\n-        # Aggressive fallback: encode with error handling\n-        try:\n-            # Use 'replace' error handling to substitute problematic characters\n-            safe_bytes = text.encode(\"utf-8\", errors=\"replace\")\n-            sanitized = safe_bytes.decode(\"utf-8\")\n-\n-            # Additional cleanup\n-            sanitized = re.sub(\n-                r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\", replacement_char, sanitized\n-            )\n-\n-            return sanitized\n-\n-        except Exception as fallback_error:\n-            logger.error(\n-                f\"Text sanitization: Aggressive fallback failed: {str(fallback_error)}\"\n-            )\n-            # Last resort: return a safe placeholder\n-            return f\"[TEXT_ENCODING_ERROR: {len(text)} characters]\"\n+        # Critical change: Don't return placeholder, raise exception for caller to handle\n+        error_msg = f\"Text contains uncleanable UTF-8 encoding issues: {str(e)[:100]}\"\n+        logger.error(f\"Text sanitization failed: {error_msg}\")\n+        raise ValueError(error_msg) from e\n \n     except Exception as e:\n         logger.error(f\"Text sanitization: Unexpected error: {str(e)}\")\n-        # Return original text if no encoding issues detected\n-        return text\n+        # For other exceptions, if no encoding issues detected, return original text\n+        try:\n+            text.encode(\"utf-8\")\n+            return text\n+        except UnicodeEncodeError:\n+            raise ValueError(\n+                f\"Text sanitization failed with unexpected error: {str(e)}\"\n+            ) from e\n \n \n def check_storage_env_vars(storage_name: str) -> None:\n@@ -1774,6 +1973,7 @@ async def pick_by_vector_similarity(\n     num_of_chunks: int,\n     entity_info: list[dict[str, Any]],\n     embedding_func: callable,\n+    query_embedding=None,\n ) -> list[str]:\n     \"\"\"\n     Vector similarity-based text chunk selection algorithm.\n@@ -1818,11 +2018,19 @@ async def pick_by_vector_similarity(\n     all_chunk_ids = list(all_chunk_ids)\n \n     try:\n-        # Get query embedding\n-        query_embedding = await embedding_func([query])\n-        query_embedding = query_embedding[\n-            0\n-        ]  # Extract first embedding from batch result\n+        # Use pre-computed query embedding if provided, otherwise compute it\n+        if query_embedding is None:\n+            query_embedding = await embedding_func([query])\n+            query_embedding = query_embedding[\n+                0\n+            ]  # Extract first embedding from batch result\n+            logger.debug(\n+                \"Computed query embedding for vector similarity chunk selection\"\n+            )\n+        else:\n+            logger.debug(\n+                \"Using pre-computed query embedding for vector similarity chunk selection\"\n+            )\n \n         # Get chunk embeddings from vector database\n         chunk_vectors = await chunks_vdb.get_vectors_by_ids(all_chunk_ids)\n@@ -1969,17 +2177,50 @@ async def apply_rerank_if_enabled(\n         return retrieved_docs\n \n     try:\n-        # Apply reranking - let rerank_model_func handle top_k internally\n-        reranked_docs = await rerank_func(\n+        # Extract document content for reranking\n+        document_texts = []\n+        for doc in retrieved_docs:\n+            # Try multiple possible content fields\n+            content = (\n+                doc.get(\"content\")\n+                or doc.get(\"text\")\n+                or doc.get(\"chunk_content\")\n+                or doc.get(\"document\")\n+                or str(doc)\n+            )\n+            document_texts.append(content)\n+\n+        # Call the new rerank function that returns index-based results\n+        rerank_results = await rerank_func(\n             query=query,\n-            documents=retrieved_docs,\n+            documents=document_texts,\n             top_n=top_n,\n         )\n-        if reranked_docs and len(reranked_docs) > 0:\n-            if len(reranked_docs) > top_n:\n-                reranked_docs = reranked_docs[:top_n]\n-            logger.info(f\"Successfully reranked: {len(retrieved_docs)} chunks\")\n-            return reranked_docs\n+\n+        # Process rerank results based on return format\n+        if rerank_results and len(rerank_results) > 0:\n+            # Check if results are in the new index-based format\n+            if isinstance(rerank_results[0], dict) and \"index\" in rerank_results[0]:\n+                # New format: [{\"index\": 0, \"relevance_score\": 0.85}, ...]\n+                reranked_docs = []\n+                for result in rerank_results:\n+                    index = result[\"index\"]\n+                    relevance_score = result[\"relevance_score\"]\n+\n+                    # Get original document and add rerank score\n+                    if 0 <= index < len(retrieved_docs):\n+                        doc = retrieved_docs[index].copy()\n+                        doc[\"rerank_score\"] = relevance_score\n+                        reranked_docs.append(doc)\n+\n+                logger.info(\n+                    f\"Successfully reranked: {len(reranked_docs)} chunks from {len(retrieved_docs)} original chunks\"\n+                )\n+                return reranked_docs\n+            else:\n+                # Legacy format: assume it's already reranked documents\n+                logger.info(f\"Using legacy rerank format: {len(rerank_results)} chunks\")\n+                return rerank_results[:top_n] if top_n else rerank_results\n         else:\n             logger.warning(\"Rerank returned empty results, using original chunks\")\n             return retrieved_docs\n@@ -2018,13 +2259,6 @@ async def process_chunks_unified(\n \n     # 1. Apply reranking if enabled and query is provided\n     if query_param.enable_rerank and query and unique_chunks:\n-        # \u4fdd\u5b58 chunk_id \u5b57\u6bb5\uff0c\u56e0\u4e3a rerank \u53ef\u80fd\u4f1a\u4e22\u5931\u8fd9\u4e2a\u5b57\u6bb5\n-        chunk_ids = {}\n-        for chunk in unique_chunks:\n-            chunk_id = chunk.get(\"chunk_id\")\n-            if chunk_id:\n-                chunk_ids[id(chunk)] = chunk_id\n-\n         rerank_top_k = query_param.chunk_top_k or len(unique_chunks)\n         unique_chunks = await apply_rerank_if_enabled(\n             query=query,\n@@ -2034,11 +2268,6 @@ async def process_chunks_unified(\n             top_n=rerank_top_k,\n         )\n \n-        # \u6062\u590d chunk_id \u5b57\u6bb5\n-        for chunk in unique_chunks:\n-            if id(chunk) in chunk_ids:\n-                chunk[\"chunk_id\"] = chunk_ids[id(chunk)]\n-\n     # 2. Filter by minimum rerank score if reranking is enabled\n     if query_param.enable_rerank and unique_chunks:\n         min_rerank_score = global_config.get(\"min_rerank_score\", 0.5)\n@@ -2086,13 +2315,6 @@ async def process_chunks_unified(\n \n         original_count = len(unique_chunks)\n \n-        # Keep chunk_id field, cause truncate_list_by_token_size will lose it\n-        chunk_ids_map = {}\n-        for i, chunk in enumerate(unique_chunks):\n-            chunk_id = chunk.get(\"chunk_id\")\n-            if chunk_id:\n-                chunk_ids_map[i] = chunk_id\n-\n         unique_chunks = truncate_list_by_token_size(\n             unique_chunks,\n             key=lambda x: json.dumps(x, ensure_ascii=False),\n@@ -2100,11 +2322,6 @@ async def process_chunks_unified(\n             tokenizer=tokenizer,\n         )\n \n-        # restore chunk_id feiled\n-        for i, chunk in enumerate(unique_chunks):\n-            if i in chunk_ids_map:\n-                chunk[\"chunk_id\"] = chunk_ids_map[i]\n-\n         logger.debug(\n             f\"Token truncation: {len(unique_chunks)} chunks from {original_count} \"\n             f\"(chunk available tokens: {chunk_token_limit}, source: {source_type})\"\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag_webui/src/api/lightrag.ts",
            "diff": "diff --git a/lightrag_webui/src/api/lightrag.ts b/lightrag_webui/src/api/lightrag.ts\nindex 98fc59ca..265126c7 100644\n--- a/lightrag_webui/src/api/lightrag.ts\n+++ b/lightrag_webui/src/api/lightrag.ts\n@@ -35,7 +35,6 @@ export type LightragStatus = {\n     embedding_binding: string\n     embedding_binding_host: string\n     embedding_model: string\n-    max_tokens: number\n     kv_storage: string\n     doc_status_storage: string\n     graph_storage: string\n@@ -43,6 +42,7 @@ export type LightragStatus = {\n     workspace?: string\n     max_graph_nodes?: string\n     enable_rerank?: boolean\n+    rerank_binding?: string | null\n     rerank_model?: string | null\n     rerank_binding_host?: string | null\n     summary_language: string\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag_webui/src/components/status/StatusCard.tsx",
            "diff": "diff --git a/lightrag_webui/src/components/status/StatusCard.tsx b/lightrag_webui/src/components/status/StatusCard.tsx\nindex a084ea13..8eeaaf70 100644\n--- a/lightrag_webui/src/components/status/StatusCard.tsx\n+++ b/lightrag_webui/src/components/status/StatusCard.tsx\n@@ -52,7 +52,7 @@ const StatusCard = ({ status }: { status: LightragStatus | null }) => {\n             <span>{t('graphPanel.statusCard.rerankerBindingHost')}:</span>\n             <span>{status.configuration.rerank_binding_host || '-'}</span>\n             <span>{t('graphPanel.statusCard.rerankerModel')}:</span>\n-            <span>{status.configuration.rerank_model || '-'}</span>\n+            <span>{(status.configuration.rerank_binding || '-')} : {(status.configuration.rerank_model || '-')}</span>\n           </div>\n         </div>\n       )}\n"
        },
        {
            "commit": "b860ffe5103cf9bc6270707dd2a7955f74f7d3fb",
            "file_path": "lightrag_webui/src/features/DocumentManager.tsx",
            "diff": "diff --git a/lightrag_webui/src/features/DocumentManager.tsx b/lightrag_webui/src/features/DocumentManager.tsx\nindex 01f08df5..2ce3daee 100644\n--- a/lightrag_webui/src/features/DocumentManager.tsx\n+++ b/lightrag_webui/src/features/DocumentManager.tsx\n@@ -183,7 +183,7 @@ export default function DocumentManager() {\n   const setDocumentsPageSize = useSettingsStore.use.setDocumentsPageSize()\n \n   // New pagination state\n-  const [, setCurrentPageDocs] = useState<DocStatusResponse[]>([])\n+  const [currentPageDocs, setCurrentPageDocs] = useState<DocStatusResponse[]>([])\n   const [pagination, setPagination] = useState<PaginationInfo>({\n     page: 1,\n     page_size: documentsPageSize,\n@@ -292,6 +292,16 @@ export default function DocumentManager() {\n   type DocStatusWithStatus = DocStatusResponse & { status: DocStatus };\n \n   const filteredAndSortedDocs = useMemo(() => {\n+    // Use currentPageDocs directly if available (from paginated API)\n+    // This preserves the backend's sort order and prevents status grouping\n+    if (currentPageDocs && currentPageDocs.length > 0) {\n+      return currentPageDocs.map(doc => ({\n+        ...doc,\n+        status: doc.status as DocStatus\n+      })) as DocStatusWithStatus[];\n+    }\n+\n+    // Fallback to legacy docs structure for backward compatibility\n     if (!docs) return null;\n \n     // Create a flat array of documents with status information\n@@ -324,7 +334,7 @@ export default function DocumentManager() {\n     }\n \n     return allDocuments;\n-  }, [docs, sortField, sortDirection, statusFilter, sortDocuments]);\n+  }, [currentPageDocs, docs, sortField, sortDirection, statusFilter, sortDocuments]);\n \n   // Calculate current page selection state (after filteredAndSortedDocs is defined)\n   const currentPageDocIds = useMemo(() => {\n@@ -469,22 +479,42 @@ export default function DocumentManager() {\n     };\n   }, [docs]);\n \n-  // New paginated data fetching function\n-  const fetchPaginatedDocuments = useCallback(async (\n-    page: number,\n-    pageSize: number,\n-    statusFilter: StatusFilter\n+  // Utility function to update component state\n+  const updateComponentState = useCallback((response: any) => {\n+    setPagination(response.pagination);\n+    setCurrentPageDocs(response.documents);\n+    setStatusCounts(response.status_counts);\n+\n+    // Update legacy docs state for backward compatibility\n+    const legacyDocs: DocsStatusesResponse = {\n+      statuses: {\n+        processed: response.documents.filter((doc: DocStatusResponse) => doc.status === 'processed'),\n+        processing: response.documents.filter((doc: DocStatusResponse) => doc.status === 'processing'),\n+        pending: response.documents.filter((doc: DocStatusResponse) => doc.status === 'pending'),\n+        failed: response.documents.filter((doc: DocStatusResponse) => doc.status === 'failed')\n+      }\n+    };\n+\n+    setDocs(response.pagination.total_count > 0 ? legacyDocs : null);\n+  }, []);\n+\n+  // Intelligent refresh function: handles all boundary cases\n+  const handleIntelligentRefresh = useCallback(async (\n+    targetPage?: number, // Optional target page, defaults to current page\n+    resetToFirst?: boolean // Whether to force reset to first page\n   ) => {\n     try {\n       if (!isMountedRef.current) return;\n \n       setIsRefreshing(true);\n \n-      // Prepare request parameters\n+      // Determine target page\n+      const pageToFetch = resetToFirst ? 1 : (targetPage || pagination.page);\n+\n       const request: DocumentsRequest = {\n         status_filter: statusFilter === 'all' ? null : statusFilter,\n-        page,\n-        page_size: pageSize,\n+        page: pageToFetch,\n+        page_size: pagination.page_size,\n         sort_field: sortField,\n         sort_direction: sortDirection\n       };\n@@ -493,26 +523,34 @@ export default function DocumentManager() {\n \n       if (!isMountedRef.current) return;\n \n-      // Update pagination state\n-      setPagination(response.pagination);\n-      setCurrentPageDocs(response.documents);\n-      setStatusCounts(response.status_counts);\n-\n-      // Update legacy docs state for backward compatibility\n-      const legacyDocs: DocsStatusesResponse = {\n-        statuses: {\n-          processed: response.documents.filter(doc => doc.status === 'processed'),\n-          processing: response.documents.filter(doc => doc.status === 'processing'),\n-          pending: response.documents.filter(doc => doc.status === 'pending'),\n-          failed: response.documents.filter(doc => doc.status === 'failed')\n+      // Boundary case handling: if target page has no data but total count > 0\n+      if (response.documents.length === 0 && response.pagination.total_count > 0) {\n+        // Calculate last page\n+        const lastPage = Math.max(1, response.pagination.total_pages);\n+\n+        if (pageToFetch !== lastPage) {\n+          // Re-request last page\n+          const lastPageRequest: DocumentsRequest = {\n+            ...request,\n+            page: lastPage\n+          };\n+\n+          const lastPageResponse = await getDocumentsPaginated(lastPageRequest);\n+\n+          if (!isMountedRef.current) return;\n+\n+          // Update page state to last page\n+          setPageByStatus(prev => ({ ...prev, [statusFilter]: lastPage }));\n+          updateComponentState(lastPageResponse);\n+          return;\n         }\n-      };\n+      }\n \n-      if (response.pagination.total_count > 0) {\n-        setDocs(legacyDocs);\n-      } else {\n-        setDocs(null);\n+      // Normal case: update state\n+      if (pageToFetch !== pagination.page) {\n+        setPageByStatus(prev => ({ ...prev, [statusFilter]: pageToFetch }));\n       }\n+      updateComponentState(response);\n \n     } catch (err) {\n       if (isMountedRef.current) {\n@@ -523,7 +561,20 @@ export default function DocumentManager() {\n         setIsRefreshing(false);\n       }\n     }\n-  }, [sortField, sortDirection, t]);\n+  }, [statusFilter, pagination.page, pagination.page_size, sortField, sortDirection, t, updateComponentState]);\n+\n+  // New paginated data fetching function\n+  const fetchPaginatedDocuments = useCallback(async (\n+    page: number,\n+    pageSize: number,\n+    _statusFilter: StatusFilter // eslint-disable-line @typescript-eslint/no-unused-vars\n+  ) => {\n+    // Update pagination state\n+    setPagination(prev => ({ ...prev, page, page_size: pageSize }));\n+\n+    // Use intelligent refresh\n+    await handleIntelligentRefresh(page);\n+  }, [handleIntelligentRefresh]);\n \n   // Legacy fetchDocuments function for backward compatibility\n   const fetchDocuments = useCallback(async () => {\n@@ -678,9 +729,10 @@ export default function DocumentManager() {\n     if (prevPipelineBusyRef.current !== undefined && prevPipelineBusyRef.current !== pipelineBusy) {\n       // pipelineBusy state has changed, trigger immediate refresh\n       if (currentTab === 'documents' && health && isMountedRef.current) {\n-        handleManualRefresh();\n+        // Use intelligent refresh to preserve current page\n+        handleIntelligentRefresh();\n \n-        // Reset polling timer after manual refresh\n+        // Reset polling timer after intelligent refresh\n         const hasActiveDocuments = (statusCounts.processing || 0) > 0 || (statusCounts.pending || 0) > 0;\n         const pollingInterval = hasActiveDocuments ? 5000 : 30000;\n         startPollingInterval(pollingInterval);\n@@ -688,7 +740,7 @@ export default function DocumentManager() {\n     }\n     // Update the previous state\n     prevPipelineBusyRef.current = pipelineBusy;\n-  }, [pipelineBusy, currentTab, health, handleManualRefresh, statusCounts.processing, statusCounts.pending, startPollingInterval]);\n+  }, [pipelineBusy, currentTab, health, handleIntelligentRefresh, statusCounts.processing, statusCounts.pending, startPollingInterval]);\n \n   // Set up intelligent polling with dynamic interval based on document status\n   useEffect(() => {\n"
        }
    ]
}