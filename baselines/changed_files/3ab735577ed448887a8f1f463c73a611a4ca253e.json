{
    "sha_fail": "3ab735577ed448887a8f1f463c73a611a4ca253e",
    "changed_files": [
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/context/__init__.py",
            "diff": "diff --git a/cookbook/agent_concepts/context/__init__.py b/cookbook/agent_concepts/context/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/knowledge/custom/async_retriever.py",
            "diff": "diff --git a/cookbook/agent_concepts/knowledge/custom/async_retriever.py b/cookbook/agent_concepts/knowledge/custom/async_retriever.py\nnew file mode 100644\nindex 000000000..cae296818\n--- /dev/null\n+++ b/cookbook/agent_concepts/knowledge/custom/async_retriever.py\n@@ -0,0 +1,90 @@\n+import asyncio\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.vectordb.qdrant import Qdrant\n+from qdrant_client import AsyncQdrantClient\n+\n+# ---------------------------------------------------------\n+# This section loads the knowledge base. Skip if your knowledge base was populated elsewhere.\n+# Define the embedder\n+embedder = OpenAIEmbedder(id=\"text-embedding-3-small\")\n+# Initialize vector database connection\n+vector_db = Qdrant(collection=\"thai-recipes\", path=\"tmp/qdrant\", embedder=embedder)\n+# Load the knowledge base\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=vector_db,\n+)\n+\n+# Load the knowledge base\n+# knowledge_base.load(recreate=True)  # Comment out after first run\n+# Knowledge base is now loaded\n+# ---------------------------------------------------------\n+\n+\n+# Define the custom async retriever\n+# This is the function that the agent will use to retrieve documents\n+async def retriever(\n+    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs\n+) -> Optional[list[dict]]:\n+    \"\"\"\n+    Custom async retriever function to search the vector database for relevant documents.\n+\n+    Args:\n+        query (str): The search query string\n+        agent (Agent): The agent instance making the query\n+        num_documents (int): Number of documents to retrieve (default: 5)\n+        **kwargs: Additional keyword arguments\n+\n+    Returns:\n+        Optional[list[dict]]: List of retrieved documents or None if search fails\n+    \"\"\"\n+    try:\n+        qdrant_client = AsyncQdrantClient(path=\"tmp/qdrant\")\n+        query_embedding = embedder.get_embedding(query)\n+        results = await qdrant_client.query_points(\n+            collection_name=\"thai-recipes\",\n+            query=query_embedding,\n+            limit=num_documents,\n+        )\n+        results_dict = results.model_dump()\n+        if \"points\" in results_dict:\n+            return results_dict[\"points\"]\n+        else:\n+            return None\n+    except Exception as e:\n+        print(f\"Error during vector database search: {str(e)}\")\n+        return None\n+\n+\n+async def amain():\n+    \"\"\"Async main function to demonstrate agent usage.\"\"\"\n+    # Initialize agent with custom retriever\n+    # Remember to set search_knowledge=True to use agentic_rag or add_reference=True for traditional RAG\n+    # search_knowledge=True is default when you add a knowledge base but is needed here\n+    agent = Agent(\n+        retriever=retriever,\n+        knowledge=knowledge_base,\n+        search_knowledge=True,\n+        instructions=\"Search the knowledge base for information\",\n+        show_tool_calls=True,\n+    )\n+\n+    # Load the knowledge base (uncomment for first run)\n+    await knowledge_base.aload(recreate=True)\n+\n+    # Example query\n+    query = \"List down the ingredients to make Massaman Gai\"\n+    await agent.aprint_response(query, markdown=True)\n+\n+\n+def main():\n+    \"\"\"Synchronous wrapper for main function\"\"\"\n+    asyncio.run(amain())\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/knowledge/custom/retriever.py",
            "diff": "diff --git a/cookbook/agent_concepts/knowledge/custom/retriever.py b/cookbook/agent_concepts/knowledge/custom/retriever.py\nindex f15bcea1f..704e87e64 100644\n--- a/cookbook/agent_concepts/knowledge/custom/retriever.py\n+++ b/cookbook/agent_concepts/knowledge/custom/retriever.py\n@@ -19,7 +19,7 @@ knowledge_base = PDFUrlKnowledgeBase(\n )\n \n # Load the knowledge base\n-# knowledge_base.load(recreate=True)  # Comment out after first run\n+knowledge_base.load(recreate=True)  # Comment out after first run\n # Knowledge base is now loaded\n # ---------------------------------------------------------\n \n@@ -66,6 +66,7 @@ def main():\n     # search_knowledge=True is default when you add a knowledge base but is needed here\n     agent = Agent(\n         retriever=retriever,\n+        knowledge=knowledge_base,\n         search_knowledge=True,\n         instructions=\"Search the knowledge base for information\",\n         show_tool_calls=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/knowledge/embedders/aws_bedrock_embedder.py",
            "diff": "diff --git a/cookbook/agent_concepts/knowledge/embedders/aws_bedrock_embedder.py b/cookbook/agent_concepts/knowledge/embedders/aws_bedrock_embedder.py\nnew file mode 100644\nindex 000000000..f1ba45b97\n--- /dev/null\n+++ b/cookbook/agent_concepts/knowledge/embedders/aws_bedrock_embedder.py\n@@ -0,0 +1,25 @@\n+from agno.document.reader.pdf_reader import PDFUrlReader\n+from agno.embedder.aws_bedrock import AwsBedrockEmbedder\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.vectordb.pgvector import PgVector\n+\n+embeddings = AwsBedrockEmbedder().get_embedding(\n+    \"The quick brown fox jumps over the lazy dog.\"\n+)\n+# Print the embeddings and their dimensions\n+print(f\"Embeddings: {embeddings[:5]}\")\n+print(f\"Dimensions: {len(embeddings)}\")\n+\n+# Example usage:\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    reader=PDFUrlReader(\n+        chunk_size=2048\n+    ),  # Required because cohere has a fixed size of 2048\n+    vector_db=PgVector(\n+        table_name=\"recipes\",\n+        db_url=\"postgresql+psycopg://ai:ai@localhost:5532/ai\",\n+        embedder=AwsBedrockEmbedder(),\n+    ),\n+)\n+knowledge_base.load(recreate=False)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/memory/16_custom_memory_instructions.py",
            "diff": "diff --git a/cookbook/agent_concepts/memory/16_custom_memory_instructions.py b/cookbook/agent_concepts/memory/16_custom_memory_instructions.py\nindex 15c95599b..a743c5735 100644\n--- a/cookbook/agent_concepts/memory/16_custom_memory_instructions.py\n+++ b/cookbook/agent_concepts/memory/16_custom_memory_instructions.py\n@@ -13,7 +13,7 @@ from rich.pretty import pprint\n memory = Memory(\n     db=SqliteMemoryDb(table_name=\"memory\", db_file=\"tmp/memory.db\"),\n     memory_manager=MemoryManager(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         memory_capture_instructions=dedent(\"\"\"\\\n             Memories should only include details about the user's academic interests.\n             Ignore names, hobbies, and personal interests.\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/state/__init__.py",
            "diff": "diff --git a/cookbook/agent_concepts/state/__init__.py b/cookbook/agent_concepts/state/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/tool_concepts/custom_tools/__init__.py",
            "diff": "diff --git a/cookbook/agent_concepts/tool_concepts/custom_tools/__init__.py b/cookbook/agent_concepts/tool_concepts/custom_tools/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/agent_concepts/tool_concepts/toolkits/__init__.py",
            "diff": "diff --git a/cookbook/agent_concepts/tool_concepts/toolkits/__init__.py b/cookbook/agent_concepts/tool_concepts/toolkits/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/demo/agents/memory_agent.py",
            "diff": "diff --git a/cookbook/demo/agents/memory_agent.py b/cookbook/demo/agents/memory_agent.py\nindex c3cd2a517..912f073d7 100644\n--- a/cookbook/demo/agents/memory_agent.py\n+++ b/cookbook/demo/agents/memory_agent.py\n@@ -5,6 +5,7 @@ from agno.memory.v2.db.postgres import PostgresMemoryDb\n from agno.memory.v2.memory import Memory\n from agno.models.openai import OpenAIChat\n from agno.storage.agent.postgres import PostgresAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n \n # ************* Database Connection *************\n db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n@@ -45,4 +46,6 @@ def get_memory_agent(\n         add_datetime_to_instructions=True,\n         markdown=True,\n         debug_mode=debug_mode,\n+        # Add a tool to search the web\n+        tools=[DuckDuckGoTools()],\n     )\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/examples/agents/recipe_rag_image.py",
            "diff": "diff --git a/cookbook/examples/agents/recipe_rag_image.py b/cookbook/examples/agents/recipe_rag_image.py\nnew file mode 100644\nindex 000000000..c8eafba54\n--- /dev/null\n+++ b/cookbook/examples/agents/recipe_rag_image.py\n@@ -0,0 +1,54 @@\n+\"\"\"Example: Multi-Modal RAG & Image Agent\n+\n+An agent that uses Llama 4 for multi-modal RAG and OpenAITools to create a visual, step-by-step image manual for a recipe.\n+\n+Run: `pip install openai agno groq cohere` to install the dependencies\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.embedder.cohere import CohereEmbedder\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.groq import Groq\n+from agno.tools.openai import OpenAITools\n+from agno.utils.media import download_image\n+from agno.vectordb.pgvector import PgVector\n+\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=PgVector(\n+        db_url=\"postgresql+psycopg://ai:ai@localhost:5532/ai\",\n+        table_name=\"embed_vision_documents\",\n+        embedder=CohereEmbedder(\n+            id=\"embed-v4.0\",\n+        ),\n+    ),\n+)\n+\n+knowledge_base.load()\n+\n+agent = Agent(\n+    name=\"EmbedVisionRAGAgent\",\n+    model=Groq(id=\"meta-llama/llama-4-scout-17b-16e-instruct\"),\n+    tools=[OpenAITools()],\n+    knowledge=knowledge_base,\n+    instructions=[\n+        \"You are a specialized recipe assistant.\",\n+        \"When asked for a recipe:\",\n+        \"1. Search the knowledge base to retrieve the relevant recipe details.\",\n+        \"2. Analyze the retrieved recipe steps carefully.\",\n+        \"3. Use the `generate_image` tool to create a visual, step-by-step image manual for the recipe.\",\n+        \"4. Present the recipe text clearly and mention that you have generated an accompanying image manual. Add instructions while generating the image.\",\n+    ],\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+agent.print_response(\n+    \"What is the recipe for a Thai curry?\",\n+)\n+\n+response = agent.run_response\n+if response.images:\n+    download_image(response.images[0].url, Path(\"tmp/recipe_image.png\"))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/deepinfra/__init__.py",
            "diff": "diff --git a/cookbook/models/deepinfra/__init__.py b/cookbook/models/deepinfra/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/async_basic.py",
            "diff": "diff --git a/cookbook/models/google/gemini/async_basic.py b/cookbook/models/google/gemini/async_basic.py\nindex b39bf1301..aea6ae9ba 100644\n--- a/cookbook/models/google/gemini/async_basic.py\n+++ b/cookbook/models/google/gemini/async_basic.py\n@@ -5,7 +5,7 @@ from agno.models.google import Gemini\n \n agent = Agent(\n     model=Gemini(\n-        id=\"gemini-2.0-flash-exp\",\n+        id=\"gemini-2.0-flash-001\",\n         instructions=[\"You are a basic agent that writes short stories.\"],\n     ),\n     markdown=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/google/gemini/async_basic_stream.py b/cookbook/models/google/gemini/async_basic_stream.py\nindex cb4e5933b..3551a1903 100644\n--- a/cookbook/models/google/gemini/async_basic_stream.py\n+++ b/cookbook/models/google/gemini/async_basic_stream.py\n@@ -4,7 +4,7 @@ from typing import Iterator  # noqa\n from agno.agent import Agent, RunResponse  # noqa\n from agno.models.google import Gemini\n \n-agent = Agent(model=Gemini(id=\"gemini-2.0-flash-exp\"), markdown=True)\n+agent = Agent(model=Gemini(id=\"gemini-2.0-flash-001\"), markdown=True)\n \n # Get the response in a variable\n # run_response: Iterator[RunResponse] = agent.run(\"Share a 2 sentence horror story\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/google/gemini/async_tool_use.py b/cookbook/models/google/gemini/async_tool_use.py\nindex aa0db04aa..18797925f 100644\n--- a/cookbook/models/google/gemini/async_tool_use.py\n+++ b/cookbook/models/google/gemini/async_tool_use.py\n@@ -9,7 +9,7 @@ from agno.models.google import Gemini\n from agno.tools.duckduckgo import DuckDuckGoTools\n \n agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     tools=[DuckDuckGoTools()],\n     show_tool_calls=True,\n     markdown=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/basic.py",
            "diff": "diff --git a/cookbook/models/google/gemini/basic.py b/cookbook/models/google/gemini/basic.py\nindex 9015894d9..9205bbdd2 100644\n--- a/cookbook/models/google/gemini/basic.py\n+++ b/cookbook/models/google/gemini/basic.py\n@@ -1,7 +1,7 @@\n from agno.agent import Agent, RunResponse  # noqa\n from agno.models.google import Gemini\n \n-agent = Agent(model=Gemini(id=\"gemini-2.0-flash-exp\"), markdown=True)\n+agent = Agent(model=Gemini(id=\"gemini-2.0-flash-001\"), markdown=True)\n \n # Get the response in a variable\n # run: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/basic_stream.py",
            "diff": "diff --git a/cookbook/models/google/gemini/basic_stream.py b/cookbook/models/google/gemini/basic_stream.py\nindex 56b912781..1ed4b1a07 100644\n--- a/cookbook/models/google/gemini/basic_stream.py\n+++ b/cookbook/models/google/gemini/basic_stream.py\n@@ -2,7 +2,7 @@ from typing import Iterator  # noqa\n from agno.agent import Agent, RunResponse  # noqa\n from agno.models.google import Gemini\n \n-agent = Agent(model=Gemini(id=\"gemini-2.0-flash-exp\"), markdown=True)\n+agent = Agent(model=Gemini(id=\"gemini-2.0-flash-001\"), markdown=True)\n \n # Get the response in a variable\n # run_response: Iterator[RunResponse] = agent.run(\"Share a 2 sentence horror story\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/knowledge.py",
            "diff": "diff --git a/cookbook/models/google/gemini/knowledge.py b/cookbook/models/google/gemini/knowledge.py\nindex 18ab73e88..b6cf3f59d 100644\n--- a/cookbook/models/google/gemini/knowledge.py\n+++ b/cookbook/models/google/gemini/knowledge.py\n@@ -19,7 +19,7 @@ knowledge_base = PDFUrlKnowledgeBase(\n knowledge_base.load(recreate=True)  # Comment out after first run\n \n agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     knowledge=knowledge_base,\n     show_tool_calls=True,\n )\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/storage.py",
            "diff": "diff --git a/cookbook/models/google/gemini/storage.py b/cookbook/models/google/gemini/storage.py\nindex c4d2331e3..e7b9aa4a1 100644\n--- a/cookbook/models/google/gemini/storage.py\n+++ b/cookbook/models/google/gemini/storage.py\n@@ -8,7 +8,7 @@ from agno.tools.duckduckgo import DuckDuckGoTools\n db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n \n agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     storage=PostgresStorage(table_name=\"agent_sessions\", db_url=db_url),\n     tools=[DuckDuckGoTools()],\n     add_history_to_messages=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/storage_and_memory.py",
            "diff": "diff --git a/cookbook/models/google/gemini/storage_and_memory.py b/cookbook/models/google/gemini/storage_and_memory.py\nindex d74ba19fc..cf6ced86b 100644\n--- a/cookbook/models/google/gemini/storage_and_memory.py\n+++ b/cookbook/models/google/gemini/storage_and_memory.py\n@@ -18,7 +18,7 @@ knowledge_base = PDFUrlKnowledgeBase(\n knowledge_base.load(recreate=True)  # Comment out after first run\n \n agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     tools=[DuckDuckGoTools()],\n     knowledge=knowledge_base,\n     storage=PostgresStorage(table_name=\"agent_sessions\", db_url=db_url),\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/structured_output.py",
            "diff": "diff --git a/cookbook/models/google/gemini/structured_output.py b/cookbook/models/google/gemini/structured_output.py\nindex ef4a6dd8b..3209d750c 100644\n--- a/cookbook/models/google/gemini/structured_output.py\n+++ b/cookbook/models/google/gemini/structured_output.py\n@@ -26,7 +26,7 @@ class MovieScript(BaseModel):\n \n \n structured_output_agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     description=\"You help people write movie scripts.\",\n     response_model=MovieScript,\n )\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/tool_use.py",
            "diff": "diff --git a/cookbook/models/google/gemini/tool_use.py b/cookbook/models/google/gemini/tool_use.py\nindex 911e9ebca..ad009f443 100644\n--- a/cookbook/models/google/gemini/tool_use.py\n+++ b/cookbook/models/google/gemini/tool_use.py\n@@ -5,7 +5,7 @@ from agno.models.google import Gemini\n from agno.tools.duckduckgo import DuckDuckGoTools\n \n agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     tools=[DuckDuckGoTools()],\n     show_tool_calls=True,\n     markdown=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/google/gemini/tool_use_stream.py b/cookbook/models/google/gemini/tool_use_stream.py\nindex 0b3c32777..000e9d11d 100644\n--- a/cookbook/models/google/gemini/tool_use_stream.py\n+++ b/cookbook/models/google/gemini/tool_use_stream.py\n@@ -5,7 +5,7 @@ from agno.models.google import Gemini\n from agno.tools.duckduckgo import DuckDuckGoTools\n \n agent = Agent(\n-    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n     tools=[DuckDuckGoTools()],\n     show_tool_calls=True,\n     markdown=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/vertexai.py",
            "diff": "diff --git a/cookbook/models/google/gemini/vertexai.py b/cookbook/models/google/gemini/vertexai.py\nindex 04675bd94..ef87f1ee0 100644\n--- a/cookbook/models/google/gemini/vertexai.py\n+++ b/cookbook/models/google/gemini/vertexai.py\n@@ -17,7 +17,7 @@ gemini = Gemini(\n from agno.agent import Agent, RunResponse  # noqa\n from agno.models.google import Gemini\n \n-agent = Agent(model=Gemini(id=\"gemini-2.0-flash-exp\"), markdown=True)\n+agent = Agent(model=Gemini(id=\"gemini-2.0-flash-001\"), markdown=True)\n \n # Get the response in a variable\n # run: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/google/gemini/video_input_youtube.py",
            "diff": "diff --git a/cookbook/models/google/gemini/video_input_youtube.py b/cookbook/models/google/gemini/video_input_youtube.py\nnew file mode 100644\nindex 000000000..39508c733\n--- /dev/null\n+++ b/cookbook/models/google/gemini/video_input_youtube.py\n@@ -0,0 +1,22 @@\n+from agno.agent import Agent\n+from agno.media import Video\n+from agno.models.google import Gemini\n+\n+agent = Agent(\n+    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    markdown=True,\n+)\n+\n+agent.print_response(\n+    \"Tell me about this video?\",\n+    videos=[Video(url=\"https://www.youtube.com/watch?v=XinoY2LDdA0\")],\n+)\n+\n+# Video upload via URL is also supported with Vertex AI\n+\n+# agent = Agent(\n+#     model=Gemini(id=\"gemini-2.0-flash-exp\", vertexai=True),\n+#     markdown=True,\n+# )\n+\n+# agent.print_response(\"Tell me about this video?\", videos=[Video(url=\"https://www.youtube.com/watch?v=XinoY2LDdA0\")])\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/__init__.py",
            "diff": "diff --git a/cookbook/models/meta/__init__.py b/cookbook/models/meta/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/__init__.py",
            "diff": "diff --git a/cookbook/models/meta/llama/__init__.py b/cookbook/models/meta/llama/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/async_basic.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_basic.py b/cookbook/models/meta/llama/async_basic.py\nnew file mode 100644\nindex 000000000..f15b653e5\n--- /dev/null\n+++ b/cookbook/models/meta/llama/async_basic.py\n@@ -0,0 +1,17 @@\n+import asyncio\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Get the response in a variable\n+# run: RunResponse = asyncio.run(agent.arun(\"Share a 2 sentence horror story\"))\n+# print(run.content)\n+\n+# Print the response in the terminal\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\"))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_basic_stream.py b/cookbook/models/meta/llama/async_basic_stream.py\nnew file mode 100644\nindex 000000000..58b28d445\n--- /dev/null\n+++ b/cookbook/models/meta/llama/async_basic_stream.py\n@@ -0,0 +1,15 @@\n+import asyncio\n+from typing import Iterator  # noqa\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+\n+agent = Agent(model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True)\n+\n+# Get the response in a variable\n+# run_response: Iterator[RunResponse] = asyncio.run(agent.arun(\"Share a 2 sentence horror story\", stream=True))\n+# for chunk in run_response:\n+#     print(chunk.content)\n+\n+# Print the response in the terminal\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/async_knowledge.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_knowledge.py b/cookbook/models/meta/llama/async_knowledge.py\nnew file mode 100644\nindex 000000000..30f89466e\n--- /dev/null\n+++ b/cookbook/models/meta/llama/async_knowledge.py\n@@ -0,0 +1,28 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy pgvector pypdf llama-api-client` to install dependencies.\"\"\"\n+\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.meta import Llama\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=PgVector(table_name=\"recipes\", db_url=db_url),\n+)\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    knowledge=knowledge_base,\n+    show_tool_calls=True,\n+)\n+\n+if __name__ == \"__main__\":\n+    # Comment out after first run\n+    asyncio.run(knowledge_base.aload(recreate=True))\n+\n+    # Create and use the agent\n+    asyncio.run(agent.aprint_response(\"How to make Thai curry?\", markdown=True))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_tool_use.py b/cookbook/models/meta/llama/async_tool_use.py\nnew file mode 100644\nindex 000000000..a0342c619\n--- /dev/null\n+++ b/cookbook/models/meta/llama/async_tool_use.py\n@@ -0,0 +1,14 @@\n+\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    debug_mode=True,\n+)\n+asyncio.run(agent.aprint_response(\"Tell me the latest news about Llama API\"))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/async_tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama/async_tool_use_stream.py b/cookbook/models/meta/llama/async_tool_use_stream.py\nnew file mode 100644\nindex 000000000..89931951b\n--- /dev/null\n+++ b/cookbook/models/meta/llama/async_tool_use_stream.py\n@@ -0,0 +1,14 @@\n+\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+)\n+asyncio.run(agent.aprint_response(\"What's happening in France?\", stream=True))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/basic.py",
            "diff": "diff --git a/cookbook/models/meta/llama/basic.py b/cookbook/models/meta/llama/basic.py\nnew file mode 100644\nindex 000000000..83d7df80f\n--- /dev/null\n+++ b/cookbook/models/meta/llama/basic.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    markdown=True,\n+)\n+\n+# Get the response in a variable\n+# run: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+# print(run.content)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Share a 2 sentence horror story\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/basic_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama/basic_stream.py b/cookbook/models/meta/llama/basic_stream.py\nnew file mode 100644\nindex 000000000..23278c9fa\n--- /dev/null\n+++ b/cookbook/models/meta/llama/basic_stream.py\n@@ -0,0 +1,13 @@\n+from typing import Iterator  # noqa\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+\n+agent = Agent(model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True)\n+\n+# Get the response in a variable\n+# run_response: Iterator[RunResponse] = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+# for chunk in run_response:\n+#     print(chunk.content)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Share a 2 sentence horror story\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/image_input_bytes.py",
            "diff": "diff --git a/cookbook/models/meta/llama/image_input_bytes.py b/cookbook/models/meta/llama/image_input_bytes.py\nnew file mode 100644\nindex 000000000..d4f6279b5\n--- /dev/null\n+++ b/cookbook/models/meta/llama/image_input_bytes.py\n@@ -0,0 +1,31 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.meta import LlamaOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.utils.media import download_image\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    markdown=True,\n+)\n+\n+image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n+\n+download_image(\n+    url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\",\n+    output_path=str(image_path),\n+)\n+\n+# Read the image file content as bytes\n+image_bytes = image_path.read_bytes()\n+\n+agent.print_response(\n+    \"Tell me about this image and give me the latest news about it.\",\n+    images=[\n+        Image(content=image_bytes),\n+    ],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/image_input_file.py",
            "diff": "diff --git a/cookbook/models/meta/llama/image_input_file.py b/cookbook/models/meta/llama/image_input_file.py\nnew file mode 100644\nindex 000000000..40331ae4c\n--- /dev/null\n+++ b/cookbook/models/meta/llama/image_input_file.py\n@@ -0,0 +1,24 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.meta import Llama\n+from agno.utils.media import download_image\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    markdown=True,\n+)\n+\n+image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n+\n+download_image(\n+    url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\",\n+    output_path=str(image_path),\n+)\n+\n+agent.print_response(\n+    \"Tell me about this image?\",\n+    images=[Image(filepath=image_path)],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/knowledge.py",
            "diff": "diff --git a/cookbook/models/meta/llama/knowledge.py b/cookbook/models/meta/llama/knowledge.py\nnew file mode 100644\nindex 000000000..424061a2e\n--- /dev/null\n+++ b/cookbook/models/meta/llama/knowledge.py\n@@ -0,0 +1,21 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy pgvector pypdf llama-api-client` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.meta import Llama\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=PgVector(table_name=\"recipes\", db_url=db_url),\n+)\n+knowledge_base.load(recreate=True)  # Comment out after first run\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    knowledge=knowledge_base,\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"How to make Thai curry?\", markdown=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/memory.py",
            "diff": "diff --git a/cookbook/models/meta/llama/memory.py b/cookbook/models/meta/llama/memory.py\nnew file mode 100644\nindex 000000000..c1e54061b\n--- /dev/null\n+++ b/cookbook/models/meta/llama/memory.py\n@@ -0,0 +1,55 @@\n+\"\"\"\n+This recipe shows how to use personalized memories and summaries in an agent.\n+Steps:\n+1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector\n+2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies\n+3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.memory.v2.db.postgres import PostgresMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.meta import Llama\n+from agno.storage.postgres import PostgresStorage\n+from rich.pretty import pprint\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    # Store the memories and summary in a database\n+    memory=Memory(\n+        db=PostgresMemoryDb(table_name=\"agent_memory\", db_url=db_url),\n+    ),\n+    enable_user_memories=True,\n+    enable_session_summaries=True,\n+    # Store agent sessions in a database\n+    storage=PostgresStorage(table_name=\"personalized_agent_sessions\", db_url=db_url),\n+    # Show debug logs so, you can see the memory being created\n+    # debug_mode=True,\n+)\n+\n+# -*- Share personal information\n+agent.print_response(\"My name is john billings?\", stream=True)\n+# -*- Print memories\n+pprint(agent.memory.memories)\n+# -*- Print summary\n+pprint(agent.memory.summaries)\n+\n+# -*- Share personal information\n+agent.print_response(\"I live in nyc?\", stream=True)\n+# -*- Print memories\n+pprint(agent.memory.memories)\n+# -*- Print summary\n+pprint(agent.memory.summaries)\n+\n+# -*- Share personal information\n+agent.print_response(\"I'm going to a concert tomorrow?\", stream=True)\n+# -*- Print memories\n+pprint(agent.memory.memories)\n+# -*- Print summary\n+pprint(agent.memory.summaries)\n+\n+# Ask about the conversation\n+agent.print_response(\n+    \"What have we been talking about, do you know my name?\", stream=True\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/metrics.py",
            "diff": "diff --git a/cookbook/models/meta/llama/metrics.py b/cookbook/models/meta/llama/metrics.py\nnew file mode 100644\nindex 000000000..337746396\n--- /dev/null\n+++ b/cookbook/models/meta/llama/metrics.py\n@@ -0,0 +1,38 @@\n+from typing import Iterator\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.meta import Llama\n+from agno.tools.yfinance import YFinanceTools\n+from agno.utils.pprint import pprint_run_response\n+from rich.pretty import pprint\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[YFinanceTools(stock_price=True)],\n+    markdown=True,\n+    show_tool_calls=True,\n+)\n+\n+run_stream: Iterator[RunResponse] = agent.run(\n+    \"What is the stock price of NVDA\", stream=True\n+)\n+pprint_run_response(run_stream, markdown=True)\n+\n+# Print metrics per message\n+if agent.run_response.messages:\n+    for message in agent.run_response.messages:\n+        if message.role == \"assistant\":\n+            if message.content:\n+                print(f\"Message: {message.content}\")\n+            elif message.tool_calls:\n+                print(f\"Tool calls: {message.tool_calls}\")\n+            print(\"---\" * 5, \"Metrics\", \"---\" * 5)\n+            pprint(message.metrics)\n+            print(\"---\" * 20)\n+\n+# Print the metrics\n+print(\"---\" * 5, \"Collected Metrics\", \"---\" * 5)\n+pprint(agent.run_response.metrics)\n+# Print the session metrics\n+print(\"---\" * 5, \"Session Metrics\", \"---\" * 5)\n+pprint(agent.session_metrics)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/storage.py",
            "diff": "diff --git a/cookbook/models/meta/llama/storage.py b/cookbook/models/meta/llama/storage.py\nnew file mode 100644\nindex 000000000..e642fef5b\n--- /dev/null\n+++ b/cookbook/models/meta/llama/storage.py\n@@ -0,0 +1,17 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy llama-api-client` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.meta import Llama\n+from agno.storage.postgres import PostgresStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    storage=PostgresStorage(table_name=\"agent_sessions\", db_url=db_url),\n+    tools=[DuckDuckGoTools()],\n+    add_history_to_messages=True,\n+)\n+agent.print_response(\"How many people live in Canada?\")\n+agent.print_response(\"What is their national anthem called?\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/structured_output.py",
            "diff": "diff --git a/cookbook/models/meta/llama/structured_output.py b/cookbook/models/meta/llama/structured_output.py\nnew file mode 100644\nindex 000000000..f7b90328d\n--- /dev/null\n+++ b/cookbook/models/meta/llama/structured_output.py\n@@ -0,0 +1,35 @@\n+from typing import List\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+from pydantic import BaseModel, Field\n+from rich.pretty import pprint  # noqa\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+\n+\n+# Agent that uses a JSON schema output\n+json_schema_output_agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\", temperature=0.1),\n+    description=\"You are a helpful assistant. Summarize the movie script based on the location in a JSON object.\",\n+    response_model=MovieScript,\n+)\n+\n+json_schema_output_agent.print_response(\"New York\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama/tool_use.py b/cookbook/models/meta/llama/tool_use.py\nnew file mode 100644\nindex 000000000..19fa85bae\n--- /dev/null\n+++ b/cookbook/models/meta/llama/tool_use.py\n@@ -0,0 +1,11 @@\n+\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+)\n+agent.print_response(\"Tell me the latest news about Llama API\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama/tool_use_stream.py b/cookbook/models/meta/llama/tool_use_stream.py\nnew file mode 100644\nindex 000000000..4e56f81bd\n--- /dev/null\n+++ b/cookbook/models/meta/llama/tool_use_stream.py\n@@ -0,0 +1,12 @@\n+\"\"\"Run `pip install agno llama-api-client duckduckgo-search` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"Tell me the latest news about Llama API\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/__init__.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/__init__.py b/cookbook/models/meta/llama_openai/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/async_basic.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/async_basic.py b/cookbook/models/meta/llama_openai/async_basic.py\nnew file mode 100644\nindex 000000000..4f0328ec3\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/async_basic.py\n@@ -0,0 +1,17 @@\n+import asyncio\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import LlamaOpenAI\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Get the response in a variable\n+# run: RunResponse = asyncio.run(agent.arun(\"Share a 2 sentence horror story\"))\n+# print(run.content)\n+\n+# Print the response in the terminal\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\"))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/async_basic_stream.py b/cookbook/models/meta/llama_openai/async_basic_stream.py\nnew file mode 100644\nindex 000000000..4a7f77493\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/async_basic_stream.py\n@@ -0,0 +1,17 @@\n+import asyncio\n+from typing import Iterator  # noqa\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import LlamaOpenAI\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True\n+)\n+\n+# Get the response in a variable\n+# run_response: Iterator[RunResponse] = asyncio.run(agent.arun(\"Share a 2 sentence horror story\", stream=True))\n+# for chunk in run_response:\n+#     print(chunk.content)\n+\n+# Print the response in the terminal\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/async_tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/async_tool_use.py b/cookbook/models/meta/llama_openai/async_tool_use.py\nnew file mode 100644\nindex 000000000..644280080\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/async_tool_use.py\n@@ -0,0 +1,14 @@\n+\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.meta import LlamaOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+)\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\"))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/async_tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/async_tool_use_stream.py b/cookbook/models/meta/llama_openai/async_tool_use_stream.py\nnew file mode 100644\nindex 000000000..b4061c795\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/async_tool_use_stream.py\n@@ -0,0 +1,14 @@\n+\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\n+import asyncio\n+\n+from agno.agent import Agent\n+from agno.models.meta import LlamaOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+)\n+asyncio.run(agent.aprint_response(\"Whats happening in France?\", stream=True))\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/basic.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/basic.py b/cookbook/models/meta/llama_openai/basic.py\nnew file mode 100644\nindex 000000000..a2f1fa9e5\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/basic.py\n@@ -0,0 +1,15 @@\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import LlamaOpenAI\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    markdown=True,\n+    debug_mode=True,\n+)\n+\n+# Get the response in a variable\n+# run: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+# print(run.content)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Share a 2 sentence horror story\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/basic_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/basic_stream.py b/cookbook/models/meta/llama_openai/basic_stream.py\nnew file mode 100644\nindex 000000000..2b05921d2\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/basic_stream.py\n@@ -0,0 +1,15 @@\n+from typing import Iterator  # noqa\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import LlamaOpenAI\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True\n+)\n+\n+# Get the response in a variable\n+# run_response: Iterator[RunResponse] = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+# for chunk in run_response:\n+#     print(chunk.content)\n+\n+# Print the response in the terminal\n+agent.print_response(\"Share a 2 sentence horror story\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/image_input_bytes.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/image_input_bytes.py b/cookbook/models/meta/llama_openai/image_input_bytes.py\nnew file mode 100644\nindex 000000000..70359a683\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/image_input_bytes.py\n@@ -0,0 +1,31 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.utils.media import download_image\n+\n+agent = Agent(\n+    model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    markdown=True,\n+)\n+\n+image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n+\n+download_image(\n+    url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\",\n+    output_path=str(image_path),\n+)\n+\n+# Read the image file content as bytes\n+image_bytes = image_path.read_bytes()\n+\n+agent.print_response(\n+    \"Tell me about this image and give me the latest news about it.\",\n+    images=[\n+        Image(content=image_bytes),\n+    ],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/image_input_file.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/image_input_file.py b/cookbook/models/meta/llama_openai/image_input_file.py\nnew file mode 100644\nindex 000000000..737adbabe\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/image_input_file.py\n@@ -0,0 +1,24 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.meta import LlamaOpenAI\n+from agno.utils.media import download_image\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    markdown=True,\n+)\n+\n+image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n+\n+download_image(\n+    url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\",\n+    output_path=str(image_path),\n+)\n+\n+agent.print_response(\n+    \"Tell me about this image?\",\n+    images=[Image(filepath=image_path)],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/knowledge.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/knowledge.py b/cookbook/models/meta/llama_openai/knowledge.py\nnew file mode 100644\nindex 000000000..02c70a375\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/knowledge.py\n@@ -0,0 +1,21 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy pgvector pypdf openai` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.meta import LlamaOpenAI\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+knowledge_base = PDFUrlKnowledgeBase(\n+    urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n+    vector_db=PgVector(table_name=\"recipes\", db_url=db_url),\n+)\n+knowledge_base.load(recreate=True)  # Comment out after first run\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    knowledge=knowledge_base,\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"How to make Thai curry?\", markdown=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/memory.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/memory.py b/cookbook/models/meta/llama_openai/memory.py\nnew file mode 100644\nindex 000000000..f502abddf\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/memory.py\n@@ -0,0 +1,55 @@\n+\"\"\"\n+This recipe shows how to use personalized memories and summaries in an agent.\n+Steps:\n+1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector\n+2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies\n+3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.memory.v2.db.postgres import PostgresMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.meta import LlamaOpenAI\n+from agno.storage.postgres import PostgresStorage\n+from rich.pretty import pprint\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    # Store the memories and summary in a database\n+    memory=Memory(\n+        db=PostgresMemoryDb(table_name=\"agent_memory\", db_url=db_url),\n+    ),\n+    enable_user_memories=True,\n+    enable_session_summaries=True,\n+    # Store agent sessions in a database\n+    storage=PostgresStorage(table_name=\"personalized_agent_sessions\", db_url=db_url),\n+    # Show debug logs so, you can see the memory being created\n+    # debug_mode=True,\n+)\n+\n+# -*- Share personal information\n+agent.print_response(\"My name is john billings?\", stream=True)\n+# -*- Print memories\n+pprint(agent.memory.memories)\n+# -*- Print summary\n+pprint(agent.memory.summaries)\n+\n+# -*- Share personal information\n+agent.print_response(\"I live in nyc?\", stream=True)\n+# -*- Print memories\n+pprint(agent.memory.memories)\n+# -*- Print summary\n+pprint(agent.memory.summaries)\n+\n+# -*- Share personal information\n+agent.print_response(\"I'm going to a concert tomorrow?\", stream=True)\n+# -*- Print memories\n+pprint(agent.memory.memories)\n+# -*- Print summary\n+pprint(agent.memory.summaries)\n+\n+# Ask about the conversation\n+agent.print_response(\n+    \"What have we been talking about, do you know my name?\", stream=True\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/metrics.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/metrics.py b/cookbook/models/meta/llama_openai/metrics.py\nnew file mode 100644\nindex 000000000..b9f5e7e68\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/metrics.py\n@@ -0,0 +1,38 @@\n+from typing import Iterator\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.meta import LlamaOpenAI\n+from agno.tools.yfinance import YFinanceTools\n+from agno.utils.pprint import pprint_run_response\n+from rich.pretty import pprint\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[YFinanceTools(stock_price=True)],\n+    markdown=True,\n+    show_tool_calls=True,\n+)\n+\n+run_stream: Iterator[RunResponse] = agent.run(\n+    \"What is the stock price of NVDA\", stream=True\n+)\n+pprint_run_response(run_stream, markdown=True)\n+\n+# Print metrics per message\n+if agent.run_response.messages:\n+    for message in agent.run_response.messages:\n+        if message.role == \"assistant\":\n+            if message.content:\n+                print(f\"Message: {message.content}\")\n+            elif message.tool_calls:\n+                print(f\"Tool calls: {message.tool_calls}\")\n+            print(\"---\" * 5, \"Metrics\", \"---\" * 5)\n+            pprint(message.metrics)\n+            print(\"---\" * 20)\n+\n+# Print the metrics\n+print(\"---\" * 5, \"Collected Metrics\", \"---\" * 5)\n+pprint(agent.run_response.metrics)\n+# Print the session metrics\n+print(\"---\" * 5, \"Session Metrics\", \"---\" * 5)\n+pprint(agent.session_metrics)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/storage.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/storage.py b/cookbook/models/meta/llama_openai/storage.py\nnew file mode 100644\nindex 000000000..1d38f9f74\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/storage.py\n@@ -0,0 +1,17 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy openai` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.meta import LlamaOpenAI\n+from agno.storage.postgres import PostgresStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    storage=PostgresStorage(table_name=\"agent_sessions\", db_url=db_url),\n+    tools=[DuckDuckGoTools()],\n+    add_history_to_messages=True,\n+)\n+agent.print_response(\"How many people live in Canada?\")\n+agent.print_response(\"What is their national anthem called?\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/structured_output.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/structured_output.py b/cookbook/models/meta/llama_openai/structured_output.py\nnew file mode 100644\nindex 000000000..b01514646\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/structured_output.py\n@@ -0,0 +1,35 @@\n+from typing import List\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import LlamaOpenAI\n+from pydantic import BaseModel, Field\n+from rich.pretty import pprint  # noqa\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+\n+\n+# Agent that uses a JSON schema output\n+json_schema_output_agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\", temperature=0.1),\n+    description=\"You are a helpful assistant. Summarize the movie script based on the location in a JSON object.\",\n+    response_model=MovieScript,\n+)\n+\n+json_schema_output_agent.print_response(\"New York\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/tool_use.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/tool_use.py b/cookbook/models/meta/llama_openai/tool_use.py\nnew file mode 100644\nindex 000000000..af3286b85\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/tool_use.py\n@@ -0,0 +1,12 @@\n+\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.meta import LlamaOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"Whats happening in France?\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/meta/llama_openai/tool_use_stream.py",
            "diff": "diff --git a/cookbook/models/meta/llama_openai/tool_use_stream.py b/cookbook/models/meta/llama_openai/tool_use_stream.py\nnew file mode 100644\nindex 000000000..eaa3f74ab\n--- /dev/null\n+++ b/cookbook/models/meta/llama_openai/tool_use_stream.py\n@@ -0,0 +1,12 @@\n+\"\"\"Run `pip install duckduckgo-search` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.meta import LlamaOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+    tools=[DuckDuckGoTools()],\n+    show_tool_calls=True,\n+)\n+agent.print_response(\"Whats happening in France?\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/openai/chat/pdf_input_file_upload.py",
            "diff": "diff --git a/cookbook/models/openai/chat/pdf_input_file_upload.py b/cookbook/models/openai/chat/pdf_input_file_upload.py\nnew file mode 100644\nindex 000000000..c28b37eba\n--- /dev/null\n+++ b/cookbook/models/openai/chat/pdf_input_file_upload.py\n@@ -0,0 +1,23 @@\n+\"\"\"\n+In this example, we upload a PDF file to Google GenAI directly and then use it as an input to an agent.\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.openai import OpenAIChat\n+\n+pdf_path = Path(__file__).parent.joinpath(\"ThaiRecipes.pdf\")\n+\n+# Pass the local PDF file path directly; the client will inline small files or upload large files automatically\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    markdown=True,\n+    add_history_to_messages=True,\n+)\n+\n+agent.print_response(\n+    \"Suggest me a recipe from the attached file.\",\n+    files=[File(filepath=str(pdf_path))],\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/openai/chat/pdf_input_local.py",
            "diff": "diff --git a/cookbook/models/openai/chat/pdf_input_local.py b/cookbook/models/openai/chat/pdf_input_local.py\nnew file mode 100644\nindex 000000000..5b75292a4\n--- /dev/null\n+++ b/cookbook/models/openai/chat/pdf_input_local.py\n@@ -0,0 +1,24 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.openai import OpenAIChat\n+from agno.utils.media import download_file\n+\n+pdf_path = Path(__file__).parent.joinpath(\"ThaiRecipes.pdf\")\n+\n+# Download the file using the download_file function\n+download_file(\n+    \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\", str(pdf_path)\n+)\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    markdown=True,\n+    add_history_to_messages=True,\n+)\n+\n+agent.print_response(\n+    \"What is the recipe for Gaeng Som Phak Ruam? Also what are the health benefits. Refer to the attached file.\",\n+    files=[File(filepath=pdf_path)],\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/openai/chat/pdf_input_url.py",
            "diff": "diff --git a/cookbook/models/openai/chat/pdf_input_url.py b/cookbook/models/openai/chat/pdf_input_url.py\nnew file mode 100644\nindex 000000000..bfad4747f\n--- /dev/null\n+++ b/cookbook/models/openai/chat/pdf_input_url.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.openai import OpenAIChat\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    markdown=True,\n+    add_history_to_messages=True,\n+)\n+\n+agent.print_response(\n+    \"Suggest me a recipe from the attached file.\",\n+    files=[File(url=\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\")],\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/openai/chat/text_to_speech_agent.py",
            "diff": "diff --git a/cookbook/models/openai/chat/text_to_speech_agent.py b/cookbook/models/openai/chat/text_to_speech_agent.py\nnew file mode 100644\nindex 000000000..b81ab113e\n--- /dev/null\n+++ b/cookbook/models/openai/chat/text_to_speech_agent.py\n@@ -0,0 +1,33 @@\n+\"\"\"\ud83d\udd0a Example: Using the OpenAITools Toolkit for Text-to-Speech\n+\n+This script demonstrates how to use an agent to generate speech from a given text input and optionally save it to a specified audio file.\n+\n+Run `pip install openai agno` to install the necessary dependencies.\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.openai import OpenAITools\n+from agno.utils.media import save_base64_data\n+\n+output_file: str = Path(\"tmp/speech_output.mp3\")\n+\n+agent: Agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[OpenAITools()],\n+    markdown=True,\n+    show_tool_calls=True,\n+)\n+\n+# Ask the agent to generate speech, but not save it\n+response = agent.run(\n+    f'Please generate speech for the following text: \"Hello from Agno! This is a demonstration of the text-to-speech capability using OpenAI\"'\n+)\n+\n+print(f\"Agent response: {response.get_content_as_string()}\")\n+\n+if response.audio:\n+    save_base64_data(response.audio[0].base64_audio, output_file)\n+    print(f\"Successfully saved generated speech to{output_file}\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/models/openai/responses/tool_use_o3.py",
            "diff": "diff --git a/cookbook/models/openai/responses/tool_use_o3.py b/cookbook/models/openai/responses/tool_use_o3.py\nnew file mode 100644\nindex 000000000..3a3b15a01\n--- /dev/null\n+++ b/cookbook/models/openai/responses/tool_use_o3.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIResponses\n+from agno.tools.yfinance import YFinanceTools\n+\n+agent = Agent(\n+    model=OpenAIResponses(id=\"o3\"),\n+    tools=[YFinanceTools(cache_results=True)],\n+    show_tool_calls=True,\n+    markdown=True,\n+    telemetry=False,\n+    monitoring=False,\n+)\n+\n+agent.print_response(\"What is the current price of TSLA?\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/observability/__init__.py",
            "diff": "diff --git a/cookbook/observability/__init__.py b/cookbook/observability/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/observability/weave_op.py",
            "diff": "diff --git a/cookbook/observability/weave_op.py b/cookbook/observability/weave_op.py\nnew file mode 100644\nindex 000000000..ea1a5aa0d\n--- /dev/null\n+++ b/cookbook/observability/weave_op.py\n@@ -0,0 +1,29 @@\n+\"\"\"\n+This example shows how to use weave to log model calls.\n+\n+Steps to get started with weave:\n+1. Install weave: pip install weave\n+2. Add weave.init('project-name') and weave.op() decorators to your functions\n+3. Authentication:\n+ - Go to https://wandb.ai and copy your API key from https://wandb.ai/authorize\n+ - Enter your API key in terminal when prompted\n+ Or \n+ - Export your API key as an environment variable:\n+    - export WANDB_API_KEY=<your-api-key>\n+\"\"\"\n+\n+import weave\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+\n+agent = Agent(model=OpenAIChat(id=\"gpt-4o\"), markdown=True, debug_mode=True)\n+\n+weave.init(\"agno\")\n+\n+\n+@weave.op()\n+def run(content: str):\n+    return agent.run(content)\n+\n+\n+run(\"Share a 2 sentence horror story\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/reasoning/agents/default_chain_of_thought.py",
            "diff": "diff --git a/cookbook/reasoning/agents/default_chain_of_thought.py b/cookbook/reasoning/agents/default_chain_of_thought.py\nnew file mode 100644\nindex 000000000..52faaca82\n--- /dev/null\n+++ b/cookbook/reasoning/agents/default_chain_of_thought.py\n@@ -0,0 +1,34 @@\n+\"\"\"\n+This example demonstrates how it works when you pass a non-reasoning model as a reasoning model.\n+It defaults to using the default OpenAI reasoning model.\n+We recommend using the appropriate reasoning model or passing reasoning=True for the default COT.\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+\n+reasoning_agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    reasoning_model=OpenAIChat(\n+        id=\"gpt-4o\", max_tokens=1200\n+    ),  # Should default to manual COT because it is not a native reasoning model\n+    markdown=True,\n+)\n+reasoning_agent.print_response(\n+    \"Give me steps to write a python script for fibonacci series\",\n+    stream=True,\n+    show_full_reasoning=True,\n+)\n+\n+\n+# It uses the default model of the Agent\n+reasoning_agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\", max_tokens=1200),\n+    reasoning=True,\n+    markdown=True,\n+)\n+reasoning_agent.print_response(\n+    \"Give me steps to write a python script for fibonacci series\",\n+    stream=True,\n+    show_full_reasoning=True,\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/reasoning/models/azure_ai_foundry/__init__.py",
            "diff": "diff --git a/cookbook/reasoning/models/azure_ai_foundry/__init__.py b/cookbook/reasoning/models/azure_ai_foundry/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/reasoning/models/xai/__init__.py",
            "diff": "diff --git a/cookbook/reasoning/models/xai/__init__.py b/cookbook/reasoning/models/xai/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/reasoning/models/xai/reasoning_effort.py",
            "diff": "diff --git a/cookbook/reasoning/models/xai/reasoning_effort.py b/cookbook/reasoning/models/xai/reasoning_effort.py\nnew file mode 100644\nindex 000000000..fdcdf469d\n--- /dev/null\n+++ b/cookbook/reasoning/models/xai/reasoning_effort.py\n@@ -0,0 +1,19 @@\n+from agno.agent import Agent\n+from agno.models.xai import xAI\n+from agno.tools.yfinance import YFinanceTools\n+\n+agent = Agent(\n+    model=xAI(id=\"grok-3-mini-fast\", reasoning_effort=\"high\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=\"Use tables to display data.\",\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+agent.print_response(\"Write a report comparing NVDA to TSLA\", stream=True)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/storage/mongodb_storage/mongodb_storage_for_team.py",
            "diff": "diff --git a/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py b/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py\nindex 985a85c37..ed4c5d9b9 100644\n--- a/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py\n+++ b/cookbook/storage/mongodb_storage/mongodb_storage_for_team.py\n@@ -57,6 +57,7 @@ hn_team = Team(\n     markdown=True,\n     debug_mode=True,\n     show_members_responses=True,\n+    add_member_tools_to_system_message=False,\n )\n \n hn_team.print_response(\"Write an article about the top 2 stories on hackernews\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/apify_tools.py",
            "diff": "diff --git a/cookbook/tools/apify_tools.py b/cookbook/tools/apify_tools.py\nindex bc247a7cb..f68750607 100644\n--- a/cookbook/tools/apify_tools.py\n+++ b/cookbook/tools/apify_tools.py\n@@ -1,5 +1,82 @@\n from agno.agent import Agent\n from agno.tools.apify import ApifyTools\n \n-agent = Agent(tools=[ApifyTools()], show_tool_calls=True)\n-agent.print_response(\"Tell me about https://docs.agno.com/introduction\", markdown=True)\n+# Apify Tools Demonstration Script\n+\"\"\"\n+This script showcases the power of web scraping and data extraction using Apify's Actors (serverless tools). \n+The Apify ecosystem has 4000+ pre-built Actors for almost any web data extraction need!\n+\n+---\n+Configuration Instructions:\n+1. Install required dependencies:\n+   pip install agno langchain-apify apify-client\n+\n+2. Set the APIFY_API_TOKEN environment variable:\n+   Add a .env file with APIFY_API_TOKEN=your_apify_api_key\n+---\n+\n+Tip: Check out the Apify Store (https://apify.com/store) to find tools for almost any web scraping or data extraction task.\n+\"\"\"\n+\n+# Create an Apify Tools agent with versatile capabilities\n+agent = Agent(\n+    name=\"Web Insights Explorer\",\n+    instructions=[\n+        \"You are a sophisticated web research assistant capable of extracting insights from various online sources. \"\n+        \"Use the available tools for your tasks to gather accurate, well-structured information.\"\n+    ],\n+    tools=[\n+        ApifyTools(\n+            actors=[\n+                \"apify/rag-web-browser\",\n+                \"compass/crawler-google-places\",\n+                \"clockworks/free-tiktok-scraper\",\n+            ]\n+        )\n+    ],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+\n+def demonstrate_tools():\n+    print(\"Apify Tools Exploration \ud83d\udd0d\")\n+\n+    # RAG Web Search Demonstrations\n+    print(\"\\n1.1 \ud83d\udd75\ufe0f RAG Web Search Scenarios:\")\n+    prompt = \"Research the latest AI ethics guidelines from top tech companies. Compile a summary from at least 3 different sources comparing their approaches using RAG Web Browser.\"\n+    agent.print_response(prompt, show_full_reasoning=True)\n+\n+    print(\"\\n1.2 \ud83d\udd75\ufe0f RAG Web Search Scenarios:\")\n+    prompt = \"Carefully extract the key introduction details from https://docs.agno.com/introduction\"  #  Extract content from specific website\n+    agent.print_response(prompt)\n+\n+    # Google Places Demonstration\n+    print(\"\\n2. Google Places Crawler:\")\n+    prompt = \"Find the top 5 highest-rated coffee shops in San Francisco with detailed information about each location\"\n+    agent.print_response(prompt)\n+\n+    # Tiktok Scraper Demonstration\n+    print(\"\\n3. Tiktok Profile Analysis:\")\n+    prompt = \"Analyze two profiles on Tiktok that lately added #AI (hashtag AI), extracting their statistics and recent content trends\"\n+    agent.print_response(prompt)\n+\n+\n+if __name__ == \"__main__\":\n+    demonstrate_tools()\n+\n+\"\"\"\n+Want to add a new tool? It's easy!\n+- Browse Apify Store\n+- Find an Actor that matches your needs\n+- Add a new method to ApifyTools following the existing pattern\n+- Register the method in the __init__\n+\n+Examples of potential tools:\n+- YouTube video info scraper\n+- Twitter/X profile analyzer\n+- Product price trackers\n+- Job board crawlers\n+- News article extractors\n+- And SO MUCH MORE!\n+\"\"\"\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/mcp/cli.py",
            "diff": "diff --git a/cookbook/tools/mcp/cli.py b/cookbook/tools/mcp/cli.py\nnew file mode 100644\nindex 000000000..6d6c54a98\n--- /dev/null\n+++ b/cookbook/tools/mcp/cli.py\n@@ -0,0 +1,47 @@\n+\"\"\"Show how to run an interactive CLI to interact with an agent equipped with MCP tools.\n+\n+This example uses the MCP GitHub Agent. Example prompts to try:\n+- \"List open issues in the repository\"\n+- \"Show me recent pull requests\"\n+- \"What are the repository statistics?\"\n+- \"Find issues labeled as bugs\"\n+- \"Show me contributor activity\"\n+\n+Run: `pip install agno mcp openai` to install the dependencies\n+\"\"\"\n+\n+import asyncio\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.tools.mcp import MCPTools\n+\n+\n+async def run_agent(message: str) -> None:\n+    \"\"\"Run an interactive CLI for the GitHub agent with the given message.\"\"\"\n+\n+    # Create a client session to connect to the MCP server\n+    async with MCPTools(\"npx -y @modelcontextprotocol/server-github\") as mcp_tools:\n+        agent = Agent(\n+            tools=[mcp_tools],\n+            instructions=dedent(\"\"\"\\\n+                You are a GitHub assistant. Help users explore repositories and their activity.\n+\n+                - Use headings to organize your responses\n+                - Be concise and focus on relevant information\\\n+            \"\"\"),\n+            markdown=True,\n+            show_tool_calls=True,\n+        )\n+\n+        # Run an interactive command-line interface to interact with the agent.\n+        await agent.acli_app(message=message, stream=True)\n+\n+\n+if __name__ == \"__main__\":\n+    # Pull request example\n+    asyncio.run(\n+        run_agent(\n+            \"Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information.\"\n+        )\n+    )\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/mcp/include_exclude_tools.py",
            "diff": "diff --git a/cookbook/tools/mcp/include_exclude_tools.py b/cookbook/tools/mcp/include_exclude_tools.py\nindex 665b6bf96..be21991a6 100644\n--- a/cookbook/tools/mcp/include_exclude_tools.py\n+++ b/cookbook/tools/mcp/include_exclude_tools.py\n@@ -29,7 +29,7 @@ async def run_agent(message: str) -> None:\n             \"npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt\",\n             \"npx -y @modelcontextprotocol/server-google-maps\",\n         ],\n-        include_tools=[\"maps_search_places\", \"airbnb_search\"],\n+        include_tools=[\"airbnb_search\"],\n         exclude_tools=[\"maps_place_details\"],\n     ) as mcp_tools:\n         agent = Agent(\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/mcp/include_tools.py",
            "diff": "diff --git a/cookbook/tools/mcp/include_tools.py b/cookbook/tools/mcp/include_tools.py\nindex de7f0f567..f0ee6a7ee 100644\n--- a/cookbook/tools/mcp/include_tools.py\n+++ b/cookbook/tools/mcp/include_tools.py\n@@ -5,11 +5,10 @@ from textwrap import dedent\n from agno.agent import Agent\n from agno.models.groq import Groq\n from agno.tools.mcp import MCPTools\n-from mcp import StdioServerParameters\n \n \n async def run_agent(message: str) -> None:\n-    file_path = str(Path(__file__).parent.parent.parent.parent)\n+    file_path = str(Path(__file__).parents[3] / \"libs/agno\")\n \n     # Initialize the MCP server\n     async with (\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/models/__init__.py",
            "diff": "diff --git a/cookbook/tools/models/__init__.py b/cookbook/tools/models/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/models/gemini_video_generation.py",
            "diff": "diff --git a/cookbook/tools/models/gemini_video_generation.py b/cookbook/tools/models/gemini_video_generation.py\nnew file mode 100644\nindex 000000000..0ce53bafc\n--- /dev/null\n+++ b/cookbook/tools/models/gemini_video_generation.py\n@@ -0,0 +1,33 @@\n+\"\"\"\ud83d\udd27 Example: Using the GeminiTools Toolkit for Video Generation\n+\n+An Agent using the Gemini video generation tool.\n+\n+Video generation only works with Vertex AI.\n+Make sure you have set the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION environment variables.\n+\n+Example prompts to try:\n+- \"Generate a 5-second video of a kitten playing a piano\"\n+- \"Create a short looping animation of a neon city skyline at dusk\"\n+\n+Run `pip install google-genai agno` to install the necessary dependencies.\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.models.gemini import GeminiTools\n+from agno.utils.media import save_base64_data\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[GeminiTools(vertexai=True)],  # Video Generation only works on VertexAI mode\n+    show_tool_calls=True,\n+    debug_mode=True,\n+)\n+\n+agent.print_response(\n+    \"create a video of a cat driving at top speed\",\n+)\n+response = agent.run_response\n+if response.videos:\n+    for video in response.videos:\n+        save_base64_data(video.content, f\"tmp/cat_driving_{video.id}.mp4\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/tools/models/openai_tools.py",
            "diff": "diff --git a/cookbook/tools/models/openai_tools.py b/cookbook/tools/models/openai_tools.py\nnew file mode 100644\nindex 000000000..1376c3f4a\n--- /dev/null\n+++ b/cookbook/tools/models/openai_tools.py\n@@ -0,0 +1,25 @@\n+\"\"\"\n+This example demonstrates how to use the OpenAITools to transcribe an audio file.\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.tools.openai import OpenAITools\n+from agno.utils.media import download_file\n+\n+url = \"https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav\"\n+\n+\n+local_audio_path = Path(\"tmp/sample_conversation.wav\")\n+print(f\"Downloading file to local path: {local_audio_path}\")\n+download_file(url, local_audio_path)\n+\n+transcription_agent = Agent(\n+    tools=[OpenAITools(transcription_model=\"gpt-4o-transcribe\")],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+transcription_agent.print_response(\n+    f\"Transcribe the audio file for this file: {local_audio_path}\"\n+)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "cookbook/workflows/team_workflow.py",
            "diff": "diff --git a/cookbook/workflows/team_workflow.py b/cookbook/workflows/team_workflow.py\nindex e7f6152ad..8ed3c7f65 100644\n--- a/cookbook/workflows/team_workflow.py\n+++ b/cookbook/workflows/team_workflow.py\n@@ -3,6 +3,7 @@ from typing import Iterator\n \n from agno.agent import Agent, RunResponse\n from agno.models.openai import OpenAIChat\n+from agno.run.team import TeamRunResponse\n from agno.team.team import Team\n from agno.tools.duckduckgo import DuckDuckGoTools\n from agno.tools.exa import ExaTools\n@@ -87,7 +88,7 @@ class TeamWorkflow(Workflow):\n \n     def run(self) -> Iterator[RunResponse]:\n         logger.info(\"Getting top stories from HackerNews.\")\n-        discussion: RunResponse = self.agent_team.run(\n+        discussion: TeamRunResponse = self.agent_team.run(\n             \"Getting 2 top stories from HackerNews and reddit and write a brief report on them\"\n         )\n         if discussion is None or not discussion.content:\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex de025cd88..e3638ae1f 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from collections import ChainMap, deque\n+from collections import ChainMap, defaultdict, deque\n from dataclasses import asdict, dataclass\n from os import getenv\n from textwrap import dedent\n@@ -965,7 +965,9 @@ class Agent:\n             if self.session_metrics is None:\n                 self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n             else:\n-                self.session_metrics += self.calculate_session_metrics(session_id)  # Calculate metrics for the session\n+                self.session_metrics += self.calculate_metrics(\n+                    run_messages.messages\n+                )  # Calculate metrics for the session\n \n         # Yield UpdatingMemory event\n         if self.stream_intermediate_steps:\n@@ -1609,7 +1611,9 @@ class Agent:\n             if self.session_metrics is None:\n                 self.session_metrics = self.calculate_metrics(run_messages.messages)  # Calculate metrics for the run\n             else:\n-                self.session_metrics += self.calculate_session_metrics(session_id)  # Calculate metrics for the session\n+                self.session_metrics += self.calculate_metrics(\n+                    run_messages.messages\n+                )  # Calculate metrics for the session\n \n         # Yield UpdatingMemory event\n         if self.stream_intermediate_steps:\n@@ -1855,6 +1859,7 @@ class Agent:\n         session_messages: List[Message] = []\n         self.memory = cast(Memory, self.memory)\n         if self.enable_user_memories and run_messages.user_message is not None:\n+            log_debug(\"Creating user memories.\")\n             self.memory.create_user_memories(message=run_messages.user_message.get_content_string(), user_id=user_id)\n \n             # TODO: Possibly do both of these in one step\n@@ -1883,6 +1888,7 @@ class Agent:\n \n         # Update the session summary if needed\n         if self.enable_session_summaries:\n+            log_debug(\"Creating session summary.\")\n             self.memory.create_session_summary(session_id=session_id, user_id=user_id)\n \n     async def _amake_memories_and_summaries(\n@@ -1895,6 +1901,7 @@ class Agent:\n         self.memory = cast(Memory, self.memory)\n         session_messages: List[Message] = []\n         if self.enable_user_memories and run_messages.user_message is not None:\n+            log_debug(\"Creating user memories.\")\n             await self.memory.acreate_user_memories(\n                 message=run_messages.user_message.get_content_string(), user_id=user_id\n             )\n@@ -1925,6 +1932,7 @@ class Agent:\n \n         # Update the session summary if needed\n         if self.enable_session_summaries:\n+            log_debug(\"Creating session summary.\")\n             await self.memory.acreate_session_summary(session_id=session_id, user_id=user_id)\n \n     def get_tools(\n@@ -1949,6 +1957,15 @@ class Agent:\n \n         # Add tools for accessing knowledge\n         if self.knowledge is not None or self.retriever is not None:\n+            # Check if retriever is an async function but used in sync mode\n+            from inspect import iscoroutinefunction\n+\n+            if not async_mode and iscoroutinefunction(self.retriever):\n+                log_warning(\n+                    \"Async retriever function is being used with synchronous agent.run() or agent.print_response(). \"\n+                    \"It is recommended to use agent.arun() or agent.aprint_response() instead.\"\n+                )\n+\n             if self.search_knowledge:\n                 # Use async or sync search based on async_mode\n                 if async_mode:\n@@ -1979,6 +1996,8 @@ class Agent:\n                     agent_tool_names.extend([f for f in tool.functions.keys()])\n                 elif callable(tool):\n                     agent_tool_names.append(tool.__name__)\n+                else:\n+                    agent_tool_names.append(str(tool))\n \n         # Create new functions if we don't have any set on the model OR if the list of tool names is different than what is set on the model\n         existing_model_functions = model.get_functions()\n@@ -1998,7 +2017,6 @@ class Agent:\n \n                 _tools_for_model = []\n                 _functions_for_model = {}\n-\n                 for tool in agent_tools:\n                     if isinstance(tool, Dict):\n                         # If a dict is passed, it is a builtin tool\n@@ -2013,13 +2031,13 @@ class Agent:\n                             if name not in _functions_for_model:\n                                 func._agent = self\n                                 func.process_entrypoint(strict=strict)\n-                                if strict:\n+                                if strict and func.strict is None:\n                                     func.strict = True\n                                 if self.tool_hooks is not None:\n                                     func.tool_hooks = self.tool_hooks\n                                 _functions_for_model[name] = func\n                                 _tools_for_model.append({\"type\": \"function\", \"function\": func.to_dict()})\n-                                log_debug(f\"Added function {name} from {tool.name}\")\n+                                log_debug(f\"Added tool {name} from {tool.name}\")\n \n                         # Add instructions from the toolkit\n                         if tool.add_instructions and tool.instructions is not None:\n@@ -2037,7 +2055,7 @@ class Agent:\n                                 tool.tool_hooks = self.tool_hooks\n                             _functions_for_model[tool.name] = tool\n                             _tools_for_model.append({\"type\": \"function\", \"function\": tool.to_dict()})\n-                            log_debug(f\"Added function {tool.name}\")\n+                            log_debug(f\"Added tool {tool.name}\")\n \n                         # Add instructions from the Function\n                         if tool.add_instructions and tool.instructions is not None:\n@@ -2057,9 +2075,9 @@ class Agent:\n                                     func.tool_hooks = self.tool_hooks\n                                 _functions_for_model[func.name] = func\n                                 _tools_for_model.append({\"type\": \"function\", \"function\": func.to_dict()})\n-                                log_debug(f\"Added function {func.name}\")\n+                                log_debug(f\"Added tool {func.name}\")\n                         except Exception as e:\n-                            log_warning(f\"Could not add function {tool}: {e}\")\n+                            log_warning(f\"Could not add tool {tool}: {e}\")\n \n                 # Set tools on the model\n                 model.set_tools(tools=_tools_for_model)\n@@ -2083,7 +2101,9 @@ class Agent:\n                     self.model.response_format = self.response_model\n                     self.model.structured_outputs = True\n                 else:\n-                    log_debug(\"Model supports native structured outputs but not enabled. Using JSON mode instead.\")\n+                    log_debug(\n+                        \"Model supports native structured outputs but it is not enabled. Using JSON mode instead.\"\n+                    )\n                     self.model.response_format = json_response_format\n                     self.model.structured_outputs = False\n \n@@ -2106,8 +2126,6 @@ class Agent:\n                 self.model.response_format = json_response_format\n                 self.model.structured_outputs = False\n \n-            log_debug(f\"Structured outputs: {self.model.structured_outputs}\")\n-\n         # Add tools to the Model\n         self.add_tools_to_model(model=self.model, session_id=session_id, async_mode=async_mode, user_id=user_id)\n \n@@ -2214,7 +2232,7 @@ class Agent:\n             else:\n                 self.memory = cast(Memory, self.memory)\n                 # We fake the structure on storage, to maintain the interface with the legacy implementation\n-                run_responses = self.memory.runs[session_id]  # type: ignore\n+                run_responses = self.memory.runs.get(session_id, [])  # type: ignore\n                 memory_dict = self.memory.to_dict()\n                 memory_dict[\"runs\"] = [rr.to_dict() for rr in run_responses]\n         else:\n@@ -2574,13 +2592,14 @@ class Agent:\n             if self.add_state_in_messages:\n                 sys_message_content = self.format_message_with_state_variables(sys_message_content)\n \n-            # Add the JSON output prompt if response_model is provided and structured_outputs is False\n+            # Add the JSON output prompt if response_model is provided and the model does not support native structured outputs or JSON schema outputs\n+            # or if use_json_mode is True\n             if (\n-                self.response_model is not None\n-                and self.model\n-                and (\n-                    self.model.supports_native_structured_outputs\n-                    and (self.use_json_mode or self.structured_outputs is False)\n+                self.model is not None\n+                and self.response_model is not None\n+                and not (\n+                    (self.model.supports_native_structured_outputs or self.model.supports_json_schema_outputs)\n+                    and (not self.use_json_mode or self.structured_outputs is True)\n                 )\n             ):\n                 sys_message_content += f\"\\n{get_json_output_prompt(self.response_model)}\"  # type: ignore\n@@ -2781,14 +2800,15 @@ class Agent:\n                         \"You should ALWAYS prefer information from this conversation over the past summary.\\n\\n\"\n                     )\n \n-        # 3.3.12 Finally, add the system message from the Model\n+        # 3.3.12 Add the system message from the Model\n         system_message_from_model = self.model.get_system_message_for_model()\n         if system_message_from_model is not None:\n             system_message_content += system_message_from_model\n \n-        # Add the JSON output prompt if response_model is provided and structured_outputs is False (only applicable if the model supports structured outputs)\n+        # 3.3.13 Add the JSON output prompt if response_model is provided and the model does not support native structured outputs or JSON schema outputs\n+        # or if use_json_mode is True\n         if self.response_model is not None and not (\n-            self.model.supports_native_structured_outputs\n+            (self.model.supports_native_structured_outputs or self.model.supports_json_schema_outputs)\n             and (not self.use_json_mode or self.structured_outputs is True)\n         ):\n             system_message_content += f\"{get_json_output_prompt(self.response_model)}\"  # type: ignore\n@@ -3327,7 +3347,7 @@ class Agent:\n         from agno.document import Document\n \n         if self.retriever is not None and callable(self.retriever):\n-            from inspect import signature\n+            from inspect import isawaitable, signature\n \n             try:\n                 sig = signature(self.retriever)\n@@ -3335,7 +3355,12 @@ class Agent:\n                 if \"agent\" in sig.parameters:\n                     retriever_kwargs = {\"agent\": self}\n                 retriever_kwargs.update({\"query\": query, \"num_documents\": num_documents, **kwargs})\n-                return self.retriever(**retriever_kwargs)\n+                result = self.retriever(**retriever_kwargs)\n+\n+                if isawaitable(result):\n+                    result = await result\n+\n+                return result\n             except Exception as e:\n                 log_warning(f\"Retriever failed: {e}\")\n                 return None\n@@ -3471,7 +3496,7 @@ class Agent:\n             self.run_response.reasoning_content += reasoning_content\n \n     def aggregate_metrics_from_messages(self, messages: List[Message]) -> Dict[str, Any]:\n-        aggregated_metrics: Dict[str, Any] = {}\n+        aggregated_metrics: Dict[str, Any] = defaultdict(list)\n         assistant_message_role = self.model.assistant_message_role if self.model is not None else \"assistant\"\n         for m in messages:\n             if m.role == assistant_message_role and m.metrics is not None:\n@@ -3479,7 +3504,9 @@ class Agent:\n                     if k == \"timer\":\n                         continue\n                     if v is not None:\n-                        aggregated_metrics[k] = v\n+                        aggregated_metrics[k].append(v)\n+        if aggregated_metrics is not None:\n+            aggregated_metrics = dict(aggregated_metrics)\n         return aggregated_metrics\n \n     def calculate_metrics(self, messages: List[Message]) -> SessionMetrics:\n@@ -3490,16 +3517,6 @@ class Agent:\n                 session_metrics += m.metrics\n         return session_metrics\n \n-    def calculate_session_metrics(self, session_id: str) -> SessionMetrics:\n-        self.memory = cast(Memory, self.memory)\n-        runs = self.memory.get_runs(session_id=session_id)\n-        run_metrics = {}\n-        for run in runs:\n-            if run.metrics is not None:\n-                run_metrics.update(run.metrics)\n-\n-        return SessionMetrics(**run_metrics)\n-\n     def rename(self, name: str, session_id: Optional[str] = None) -> None:\n         \"\"\"Rename the Agent and save to storage\"\"\"\n \n@@ -3706,7 +3723,9 @@ class Agent:\n         reasoning_model: Optional[Model] = self.reasoning_model\n         reasoning_model_provided = reasoning_model is not None\n         if reasoning_model is None and self.model is not None:\n-            reasoning_model = self.model.__class__(id=self.model.id)\n+            from copy import deepcopy\n+\n+            reasoning_model = deepcopy(self.model)\n         if reasoning_model is None:\n             log_warning(\"Reasoning error. Reasoning model is None, continuing regular session...\")\n             return\n@@ -3915,7 +3934,9 @@ class Agent:\n         reasoning_model: Optional[Model] = self.reasoning_model\n         reasoning_model_provided = reasoning_model is not None\n         if reasoning_model is None and self.model is not None:\n-            reasoning_model = self.model.__class__(id=self.model.id)\n+            from copy import deepcopy\n+\n+            reasoning_model = deepcopy(self.model)\n         if reasoning_model is None:\n             log_warning(\"Reasoning error. Reasoning model is None, continuing regular session...\")\n             return\n@@ -5393,8 +5414,20 @@ class Agent:\n         exit_on: Optional[List[str]] = None,\n         **kwargs: Any,\n     ) -> None:\n+        \"\"\"Run an interactive command-line interface to interact with the agent.\"\"\"\n+\n+        from inspect import isawaitable\n+\n         from rich.prompt import Prompt\n \n+        # Ensuring the agent is not using our async MCP tools\n+        if self.tools is not None:\n+            for tool in self.tools:\n+                if isawaitable(tool):\n+                    raise NotImplementedError(\"Use `acli_app` to use async tools.\")\n+                if tool.__class__.__name__ in [\"MCPTools\", \"MultiMCPTools\"]:\n+                    raise NotImplementedError(\"Use `acli_app` to use MCP tools.\")\n+\n         if message:\n             self.print_response(\n                 message=message, stream=stream, markdown=markdown, user_id=user_id, session_id=session_id, **kwargs\n@@ -5409,3 +5442,36 @@ class Agent:\n             self.print_response(\n                 message=message, stream=stream, markdown=markdown, user_id=user_id, session_id=session_id, **kwargs\n             )\n+\n+    async def acli_app(\n+        self,\n+        message: Optional[str] = None,\n+        session_id: Optional[str] = None,\n+        user_id: Optional[str] = None,\n+        user: str = \"User\",\n+        emoji: str = \":sunglasses:\",\n+        stream: bool = False,\n+        markdown: bool = False,\n+        exit_on: Optional[List[str]] = None,\n+        **kwargs: Any,\n+    ) -> None:\n+        \"\"\"\n+        Run an interactive command-line interface to interact with the agent.\n+        Works with agent dependencies requiring async logic.\n+        \"\"\"\n+        from rich.prompt import Prompt\n+\n+        if message:\n+            await self.aprint_response(\n+                message=message, stream=stream, markdown=markdown, user_id=user_id, session_id=session_id, **kwargs\n+            )\n+\n+        _exit_on = exit_on or [\"exit\", \"quit\", \"bye\"]\n+        while True:\n+            message = Prompt.ask(f\"[bold] {emoji} {user} [/bold]\")\n+            if message in _exit_on:\n+                break\n+\n+            await self.aprint_response(\n+                message=message, stream=stream, markdown=markdown, user_id=user_id, session_id=session_id, **kwargs\n+            )\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/document/chunking/fixed.py",
            "diff": "diff --git a/libs/agno/agno/document/chunking/fixed.py b/libs/agno/agno/document/chunking/fixed.py\nindex 1a563cb35..cf6cae1e9 100644\n--- a/libs/agno/agno/document/chunking/fixed.py\n+++ b/libs/agno/agno/document/chunking/fixed.py\n@@ -22,7 +22,6 @@ class FixedSizeChunking(ChunkingStrategy):\n         chunked_documents: List[Document] = []\n         chunk_number = 1\n         chunk_meta_data = document.meta_data\n-\n         start = 0\n         while start + self.overlap < content_length:\n             end = min(start + self.chunk_size, content_length)\n@@ -55,5 +54,4 @@ class FixedSizeChunking(ChunkingStrategy):\n             )\n             chunk_number += 1\n             start = end - self.overlap\n-\n         return chunked_documents\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/document/reader/base.py",
            "diff": "diff --git a/libs/agno/agno/document/reader/base.py b/libs/agno/agno/document/reader/base.py\nindex 4734e2c31..6df9dd933 100644\n--- a/libs/agno/agno/document/reader/base.py\n+++ b/libs/agno/agno/document/reader/base.py\n@@ -1,6 +1,6 @@\n import asyncio\n from dataclasses import dataclass, field\n-from typing import Any, List\n+from typing import Any, List, Optional\n \n from agno.document.base import Document\n from agno.document.chunking.fixed import FixedSizeChunking\n@@ -12,9 +12,13 @@ class Reader:\n     \"\"\"Base class for reading documents\"\"\"\n \n     chunk: bool = True\n-    chunk_size: int = 3000\n+    chunk_size: int = 5000\n     separators: List[str] = field(default_factory=lambda: [\"\\n\", \"\\n\\n\", \"\\r\", \"\\r\\n\", \"\\n\\r\", \"\\t\", \" \", \"  \"])\n-    chunking_strategy: ChunkingStrategy = field(default_factory=FixedSizeChunking)\n+    chunking_strategy: Optional[ChunkingStrategy] = None\n+\n+    def __init__(self, chunk_size: int = 5000, chunking_strategy: Optional[ChunkingStrategy] = None) -> None:\n+        self.chunk_size = chunk_size\n+        self.chunking_strategy = chunking_strategy or FixedSizeChunking(chunk_size=self.chunk_size)\n \n     def read(self, obj: Any) -> List[Document]:\n         raise NotImplementedError\n@@ -23,7 +27,7 @@ class Reader:\n         raise NotImplementedError\n \n     def chunk_document(self, document: Document) -> List[Document]:\n-        return self.chunking_strategy.chunk(document)\n+        return self.chunking_strategy.chunk(document)  # type: ignore\n \n     async def chunk_documents_async(self, documents: List[Document]) -> List[Document]:\n         \"\"\"\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/document/reader/firecrawl_reader.py",
            "diff": "diff --git a/libs/agno/agno/document/reader/firecrawl_reader.py b/libs/agno/agno/document/reader/firecrawl_reader.py\nindex 6f2d38f56..dbe58c964 100644\n--- a/libs/agno/agno/document/reader/firecrawl_reader.py\n+++ b/libs/agno/agno/document/reader/firecrawl_reader.py\n@@ -18,6 +18,19 @@ class FirecrawlReader(Reader):\n     params: Optional[Dict] = None\n     mode: Literal[\"scrape\", \"crawl\"] = \"scrape\"\n \n+    def __init__(\n+        self,\n+        api_key: Optional[str] = None,\n+        params: Optional[Dict] = None,\n+        mode: Literal[\"scrape\", \"crawl\"] = \"scrape\",\n+        *args,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(*args, **kwargs)\n+        self.api_key = api_key\n+        self.params = params\n+        self.mode = mode\n+\n     def scrape(self, url: str) -> List[Document]:\n         \"\"\"\n         Scrapes a website and returns a list of documents.\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/document/reader/website_reader.py",
            "diff": "diff --git a/libs/agno/agno/document/reader/website_reader.py b/libs/agno/agno/document/reader/website_reader.py\nindex addb062d7..35093638c 100644\n--- a/libs/agno/agno/document/reader/website_reader.py\n+++ b/libs/agno/agno/document/reader/website_reader.py\n@@ -108,6 +108,10 @@ class WebsiteReader(Reader):\n         - Dict[str, str]: A dictionary where each key is a URL and the corresponding value is the main\n                           content extracted from that URL.\n \n+        Raises:\n+        - httpx.HTTPStatusError: If there's an HTTP status error.\n+        - httpx.RequestError: If there's a request-related error (connection, timeout, etc).\n+\n         Note:\n         The function focuses on extracting the main content by prioritizing content inside common HTML tags\n         like `<article>`, `<main>`, and `<div>` with class names such as \"content\", \"main-content\", etc.\n@@ -178,9 +182,29 @@ class WebsiteReader(Reader):\n                         ):\n                             self._urls_to_crawl.append((full_url_str, current_depth + 1))\n \n+            except httpx.HTTPStatusError as e:\n+                # Log HTTP status errors but continue crawling other pages\n+                logger.warning(f\"HTTP status error while crawling {current_url}: {e}\")\n+                # For the initial URL, we should raise the error\n+                if current_url == url and not crawler_result:\n+                    raise\n+            except httpx.RequestError as e:\n+                # Log request errors but continue crawling other pages\n+                logger.warning(f\"Request error while crawling {current_url}: {e}\")\n+                # For the initial URL, we should raise the error\n+                if current_url == url and not crawler_result:\n+                    raise\n             except Exception as e:\n-                logger.warning(f\"Failed to crawl: {current_url}: {e}\")\n-                pass\n+                # Log other exceptions but continue crawling other pages\n+                logger.warning(f\"Failed to crawl {current_url}: {e}\")\n+                # For the initial URL, we should raise the error\n+                if current_url == url and not crawler_result:\n+                    # Wrap non-HTTP exceptions in a RequestError\n+                    raise httpx.RequestError(f\"Failed to crawl starting URL {url}: {str(e)}\", request=None) from e\n+\n+        # If we couldn't crawl any pages, raise an error\n+        if not crawler_result:\n+            raise httpx.RequestError(f\"Failed to extract any content from {url}\", request=None)\n \n         return crawler_result\n \n@@ -195,6 +219,10 @@ class WebsiteReader(Reader):\n         Returns:\n         - Dict[str, str]: A dictionary where each key is a URL and the corresponding value is the main\n                         content extracted from that URL.\n+\n+        Raises:\n+        - httpx.HTTPStatusError: If there's an HTTP status error.\n+        - httpx.RequestError: If there's a request-related error (connection, timeout, etc).\n         \"\"\"\n         num_links = 0\n         crawler_result: Dict[str, str] = {}\n@@ -255,8 +283,31 @@ class WebsiteReader(Reader):\n                             ):\n                                 self._urls_to_crawl.append((full_url_str, current_depth + 1))\n \n+                except httpx.HTTPStatusError as e:\n+                    # Log HTTP status errors but continue crawling other pages\n+                    logger.warning(f\"HTTP status error while crawling asynchronously {current_url}: {e}\")\n+                    # For the initial URL, we should raise the error\n+                    if current_url == url and not crawler_result:\n+                        raise\n+                except httpx.RequestError as e:\n+                    # Log request errors but continue crawling other pages\n+                    logger.warning(f\"Request error while crawling asynchronously {current_url}: {e}\")\n+                    # For the initial URL, we should raise the error\n+                    if current_url == url and not crawler_result:\n+                        raise\n                 except Exception as e:\n-                    logger.warning(f\"Failed to crawl asynchronously: {current_url}: {e}\")\n+                    # Log other exceptions but continue crawling other pages\n+                    logger.warning(f\"Failed to crawl asynchronously {current_url}: {e}\")\n+                    # For the initial URL, we should raise the error\n+                    if current_url == url and not crawler_result:\n+                        # Wrap non-HTTP exceptions in a RequestError\n+                        raise httpx.RequestError(\n+                            f\"Failed to crawl starting URL {url} asynchronously: {str(e)}\", request=None\n+                        ) from e\n+\n+        # If we couldn't crawl any pages, raise an error\n+        if not crawler_result:\n+            raise httpx.RequestError(f\"Failed to extract any content from {url} asynchronously\", request=None)\n \n         return crawler_result\n \n@@ -269,30 +320,39 @@ class WebsiteReader(Reader):\n \n         :param url: The URL of the website to read.\n         :return: A list of documents.\n+        :raises httpx.HTTPStatusError: If there's an HTTP status error.\n+        :raises httpx.RequestError: If there's a request-related error.\n         \"\"\"\n \n         log_debug(f\"Reading: {url}\")\n-        crawler_result = self.crawl(url)\n-        documents = []\n-        for crawled_url, crawled_content in crawler_result.items():\n-            if self.chunk:\n-                documents.extend(\n-                    self.chunk_document(\n-                        Document(\n-                            name=url, id=str(crawled_url), meta_data={\"url\": str(crawled_url)}, content=crawled_content\n+        try:\n+            crawler_result = self.crawl(url)\n+            documents = []\n+            for crawled_url, crawled_content in crawler_result.items():\n+                if self.chunk:\n+                    documents.extend(\n+                        self.chunk_document(\n+                            Document(\n+                                name=url,\n+                                id=str(crawled_url),\n+                                meta_data={\"url\": str(crawled_url)},\n+                                content=crawled_content,\n+                            )\n                         )\n                     )\n-                )\n-            else:\n-                documents.append(\n-                    Document(\n-                        name=url,\n-                        id=str(crawled_url),\n-                        meta_data={\"url\": str(crawled_url)},\n-                        content=crawled_content,\n+                else:\n+                    documents.append(\n+                        Document(\n+                            name=url,\n+                            id=str(crawled_url),\n+                            meta_data={\"url\": str(crawled_url)},\n+                            content=crawled_content,\n+                        )\n                     )\n-                )\n-        return documents\n+            return documents\n+        except (httpx.HTTPStatusError, httpx.RequestError) as e:\n+            logger.error(f\"Error reading website {url}: {e}\")\n+            raise\n \n     async def async_read(self, url: str) -> List[Document]:\n         \"\"\"\n@@ -303,36 +363,43 @@ class WebsiteReader(Reader):\n \n         :param url: The URL of the website to read.\n         :return: A list of documents.\n+        :raises httpx.HTTPStatusError: If there's an HTTP status error.\n+        :raises httpx.RequestError: If there's a request-related error.\n         \"\"\"\n         log_debug(f\"Reading asynchronously: {url}\")\n-        crawler_result = await self.async_crawl(url)\n-        documents = []\n-\n-        # Process documents in parallel\n-        async def process_document(crawled_url, crawled_content):\n-            if self.chunk:\n-                doc = Document(\n-                    name=url, id=str(crawled_url), meta_data={\"url\": str(crawled_url)}, content=crawled_content\n-                )\n-                return self.chunk_document(doc)\n-            else:\n-                return [\n-                    Document(\n-                        name=url,\n-                        id=str(crawled_url),\n-                        meta_data={\"url\": str(crawled_url)},\n-                        content=crawled_content,\n+        try:\n+            crawler_result = await self.async_crawl(url)\n+            documents = []\n+\n+            # Process documents in parallel\n+            async def process_document(crawled_url, crawled_content):\n+                if self.chunk:\n+                    doc = Document(\n+                        name=url, id=str(crawled_url), meta_data={\"url\": str(crawled_url)}, content=crawled_content\n                     )\n-                ]\n-\n-        # Use asyncio.gather to process all documents in parallel\n-        tasks = [\n-            process_document(crawled_url, crawled_content) for crawled_url, crawled_content in crawler_result.items()\n-        ]\n-        results = await asyncio.gather(*tasks)\n-\n-        # Flatten the results\n-        for doc_list in results:\n-            documents.extend(doc_list)\n-\n-        return documents\n+                    return self.chunk_document(doc)\n+                else:\n+                    return [\n+                        Document(\n+                            name=url,\n+                            id=str(crawled_url),\n+                            meta_data={\"url\": str(crawled_url)},\n+                            content=crawled_content,\n+                        )\n+                    ]\n+\n+            # Use asyncio.gather to process all documents in parallel\n+            tasks = [\n+                process_document(crawled_url, crawled_content)\n+                for crawled_url, crawled_content in crawler_result.items()\n+            ]\n+            results = await asyncio.gather(*tasks)\n+\n+            # Flatten the results\n+            for doc_list in results:\n+                documents.extend(doc_list)\n+\n+            return documents\n+        except (httpx.HTTPStatusError, httpx.RequestError) as e:\n+            logger.error(f\"Error reading website asynchronously {url}: {e}\")\n+            raise\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/embedder/aws_bedrock.py",
            "diff": "diff --git a/libs/agno/agno/embedder/aws_bedrock.py b/libs/agno/agno/embedder/aws_bedrock.py\nnew file mode 100644\nindex 000000000..8769d2e75\n--- /dev/null\n+++ b/libs/agno/agno/embedder/aws_bedrock.py\n@@ -0,0 +1,212 @@\n+import json\n+from dataclasses import dataclass\n+from os import getenv\n+from typing import Any, Dict, List, Optional, Tuple\n+\n+from agno.embedder.base import Embedder\n+from agno.exceptions import AgnoError, ModelProviderError\n+from agno.utils.log import log_error, logger\n+\n+try:\n+    from boto3 import client as AwsClient\n+    from boto3.session import Session\n+    from botocore.exceptions import ClientError\n+except ImportError:\n+    log_error(\"`boto3` not installed. Please install it via `pip install boto3`.\")\n+    raise\n+\n+\n+@dataclass\n+class AwsBedrockEmbedder(Embedder):\n+    \"\"\"\n+    AWS Bedrock embedder.\n+\n+    To use this embedder, you need to either:\n+    1. Set the following environment variables:\n+       - AWS_ACCESS_KEY_ID\n+       - AWS_SECRET_ACCESS_KEY\n+       - AWS_REGION\n+    2. Or provide a boto3 Session object\n+\n+    Args:\n+        id (str): The model ID to use. Default is 'cohere.embed-multilingual-v3'.\n+        dimensions (Optional[int]): The dimensions of the embeddings. Default is 1024.\n+        input_type (str): Prepends special tokens to differentiate types. Options:\n+            'search_document', 'search_query', 'classification', 'clustering'. Default is 'search_query'.\n+        truncate (Optional[str]): How to handle inputs longer than the maximum token length.\n+            Options: 'NONE', 'START', 'END'. Default is 'NONE'.\n+        embedding_types (Optional[List[str]]): Types of embeddings to return. Options:\n+            'float', 'int8', 'uint8', 'binary', 'ubinary'. Default is ['float'].\n+        aws_region (Optional[str]): The AWS region to use.\n+        aws_access_key_id (Optional[str]): The AWS access key ID to use.\n+        aws_secret_access_key (Optional[str]): The AWS secret access key to use.\n+        session (Optional[Session]): A boto3 Session object to use for authentication.\n+        request_params (Optional[Dict[str, Any]]): Additional parameters to pass to the API requests.\n+        client_params (Optional[Dict[str, Any]]): Additional parameters to pass to the boto3 client.\n+    \"\"\"\n+\n+    id: str = \"cohere.embed-multilingual-v3\"\n+    dimensions: int = 1024  # Cohere models have 1024 dimensions by default\n+    input_type: str = \"search_query\"\n+    truncate: Optional[str] = None  # 'NONE', 'START', or 'END'\n+    # 'float', 'int8', 'uint8', etc.\n+    embedding_types: Optional[List[str]] = None\n+\n+    aws_region: Optional[str] = None\n+    aws_access_key_id: Optional[str] = None\n+    aws_secret_access_key: Optional[str] = None\n+    session: Optional[Session] = None\n+\n+    request_params: Optional[Dict[str, Any]] = None\n+    client_params: Optional[Dict[str, Any]] = None\n+    client: Optional[AwsClient] = None\n+\n+    def get_client(self) -> AwsClient:\n+        \"\"\"\n+        Returns an AWS Bedrock client.\n+\n+        Returns:\n+            AwsClient: An instance of the AWS Bedrock client.\n+        \"\"\"\n+        if self.client is not None:\n+            return self.client\n+\n+        if self.session:\n+            self.client = self.session.client(\"bedrock-runtime\")\n+            return self.client\n+\n+        self.aws_access_key_id = self.aws_access_key_id or getenv(\"AWS_ACCESS_KEY_ID\")\n+        self.aws_secret_access_key = self.aws_secret_access_key or getenv(\"AWS_SECRET_ACCESS_KEY\")\n+        self.aws_region = self.aws_region or getenv(\"AWS_REGION\")\n+\n+        if not self.aws_access_key_id or not self.aws_secret_access_key:\n+            raise AgnoError(\n+                message=\"AWS credentials not found. Please set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables or provide a boto3 session.\",\n+                status_code=400,\n+            )\n+\n+        self.client = AwsClient(\n+            service_name=\"bedrock-runtime\",\n+            region_name=self.aws_region,\n+            aws_access_key_id=self.aws_access_key_id,\n+            aws_secret_access_key=self.aws_secret_access_key,\n+            **(self.client_params or {}),\n+        )\n+        return self.client\n+\n+    def _format_request_body(self, text: str) -> str:\n+        \"\"\"\n+        Format the request body for the embedder.\n+\n+        Args:\n+            text (str): The text to embed.\n+\n+        Returns:\n+            str: The formatted request body as a JSON string.\n+        \"\"\"\n+        request_body = {\n+            \"texts\": [text],\n+            \"input_type\": self.input_type,\n+        }\n+\n+        if self.truncate:\n+            request_body[\"truncate\"] = self.truncate\n+\n+        if self.embedding_types:\n+            request_body[\"embedding_types\"] = self.embedding_types\n+\n+        # Add additional request parameters if provided\n+        if self.request_params:\n+            request_body.update(self.request_params)\n+\n+        return json.dumps(request_body)\n+\n+    def response(self, text: str) -> Dict[str, Any]:\n+        \"\"\"\n+        Get embeddings from AWS Bedrock for the given text.\n+\n+        Args:\n+            text (str): The text to embed.\n+\n+        Returns:\n+            Dict[str, Any]: The response from the API.\n+        \"\"\"\n+        try:\n+            body = self._format_request_body(text)\n+            response = self.get_client().invoke_model(\n+                modelId=self.id,\n+                body=body,\n+                contentType=\"application/json\",\n+                accept=\"application/json\",\n+            )\n+            response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n+            return response_body\n+        except ClientError as e:\n+            log_error(f\"Unexpected error calling Bedrock API: {str(e)}\")\n+            raise ModelProviderError(message=str(e.response), model_name=\"AwsBedrockEmbedder\", model_id=self.id) from e\n+        except Exception as e:\n+            log_error(f\"Unexpected error calling Bedrock API: {str(e)}\")\n+            raise ModelProviderError(message=str(e), model_name=\"AwsBedrockEmbedder\", model_id=self.id) from e\n+\n+    def get_embedding(self, text: str) -> List[float]:\n+        \"\"\"\n+        Get embeddings for the given text.\n+\n+        Args:\n+            text (str): The text to embed.\n+\n+        Returns:\n+            List[float]: The embedding vector.\n+        \"\"\"\n+        response = self.response(text=text)\n+        try:\n+            # Check if response contains embeddings or embeddings by type\n+            if \"embeddings\" in response:\n+                if isinstance(response[\"embeddings\"], list):\n+                    # Default 'float' embeddings response format\n+                    return response[\"embeddings\"][0]\n+                elif isinstance(response[\"embeddings\"], dict):\n+                    # If embeddings_types parameter was used, select float embeddings\n+                    if \"float\" in response[\"embeddings\"]:\n+                        return response[\"embeddings\"][\"float\"][0]\n+                    # Fallback to the first available embedding type\n+                    for embedding_type in response[\"embeddings\"]:\n+                        return response[\"embeddings\"][embedding_type][0]\n+            logger.warning(\"No embeddings found in response\")\n+            return []\n+        except Exception as e:\n+            logger.warning(f\"Error extracting embeddings: {e}\")\n+            return []\n+\n+    def get_embedding_and_usage(self, text: str) -> Tuple[List[float], Optional[Dict[str, Any]]]:\n+        \"\"\"\n+        Get embeddings and usage information for the given text.\n+\n+        Args:\n+            text (str): The text to embed.\n+\n+        Returns:\n+            Tuple[List[float], Optional[Dict[str, Any]]]: The embedding vector and usage information.\n+        \"\"\"\n+        response = self.response(text=text)\n+\n+        embedding: List[float] = []\n+        # Extract embeddings\n+        if \"embeddings\" in response:\n+            if isinstance(response[\"embeddings\"], list):\n+                embedding = response[\"embeddings\"][0]\n+            elif isinstance(response[\"embeddings\"], dict):\n+                if \"float\" in response[\"embeddings\"]:\n+                    embedding = response[\"embeddings\"][\"float\"][0]\n+                # Fallback to the first available embedding type\n+                else:\n+                    for embedding_type in response[\"embeddings\"]:\n+                        embedding = response[\"embeddings\"][embedding_type][0]\n+                        break\n+\n+        # Extract usage metrics if available\n+        usage = None\n+        if \"usage\" in response:\n+            usage = response[\"usage\"]\n+\n+        return embedding, usage\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/knowledge/agent.py",
            "diff": "diff --git a/libs/agno/agno/knowledge/agent.py b/libs/agno/agno/knowledge/agent.py\nindex d1e087ed4..715a15490 100644\n--- a/libs/agno/agno/knowledge/agent.py\n+++ b/libs/agno/agno/knowledge/agent.py\n@@ -29,7 +29,7 @@ class AgentKnowledge(BaseModel):\n \n     @model_validator(mode=\"after\")\n     def update_reader(self) -> \"AgentKnowledge\":\n-        if self.reader is not None:\n+        if self.reader is not None and self.reader.chunking_strategy is None:\n             self.reader.chunking_strategy = self.chunking_strategy\n         return self\n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/knowledge/website.py",
            "diff": "diff --git a/libs/agno/agno/knowledge/website.py b/libs/agno/agno/knowledge/website.py\nindex 8eef1b91b..859037cb9 100644\n--- a/libs/agno/agno/knowledge/website.py\n+++ b/libs/agno/agno/knowledge/website.py\n@@ -74,7 +74,6 @@ class WebsiteKnowledgeBase(AgentKnowledge):\n         self.vector_db.create()\n \n         log_info(\"Loading knowledge base\")\n-        num_documents = 0\n \n         # Given that the crawler needs to parse the URL before existence can be checked\n         # We check if the website url exists in the vector db if recreate is False\n@@ -86,11 +85,14 @@ class WebsiteKnowledgeBase(AgentKnowledge):\n                     log_debug(f\"Skipping {url} as it exists in the vector db\")\n                     urls_to_read.remove(url)\n \n+        num_documents = 0\n         for url in urls_to_read:\n             if document_list := self.reader.read(url=url):\n                 # Filter out documents which already exist in the vector db\n                 if not recreate:\n                     document_list = [document for document in document_list if not self.vector_db.doc_exists(document)]\n+                    if not document_list:\n+                        continue\n                 if upsert and self.vector_db.upsert_available():\n                     self.vector_db.upsert(documents=document_list, filters=filters)\n                 else:\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/media.py",
            "diff": "diff --git a/libs/agno/agno/media.py b/libs/agno/agno/media.py\nindex 4b51b6b56..1c97c8509 100644\n--- a/libs/agno/agno/media.py\n+++ b/libs/agno/agno/media.py\n@@ -11,7 +11,9 @@ class Media(BaseModel):\n \n \n class VideoArtifact(Media):\n-    url: str  # Remote location for file\n+    url: Optional[str] = None  # Remote location for file (if no inline content)\n+    content: Optional[Union[str, bytes]] = None  # type: ignore\n+    mime_type: Optional[str] = None  # MIME type of the video content\n     eta: Optional[str] = None\n     length: Optional[str] = None\n \n@@ -44,17 +46,19 @@ class AudioArtifact(Media):\n class Video(BaseModel):\n     filepath: Optional[Union[Path, str]] = None  # Absolute local location for video\n     content: Optional[Any] = None  # Actual video bytes content\n+    url: Optional[str] = None  # Remote location for video\n     format: Optional[str] = None  # E.g. `mp4`, `mov`, `avi`, `mkv`, `webm`, `flv`, `mpeg`, `mpg`, `wmv`, `three_gp`\n \n     @model_validator(mode=\"before\")\n     def validate_data(cls, data: Any):\n         \"\"\"\n-        Ensure that exactly one of `filepath`, or `content` is provided.\n+        Ensure that exactly one of `filepath`, or `content` or `url` is provided.\n         Also converts content to bytes if it's a string.\n         \"\"\"\n         # Extract the values from the input data\n         filepath = data.get(\"filepath\")\n         content = data.get(\"content\")\n+        url = data.get(\"url\")\n \n         # Convert and decompress content to bytes if it's a string\n         if content and isinstance(content, str):\n@@ -70,12 +74,12 @@ class Video(BaseModel):\n         data[\"content\"] = content\n \n         # Count how many fields are set (not None)\n-        count = len([field for field in [filepath, content] if field is not None])\n+        count = len([field for field in [filepath, content, url] if field is not None])\n \n         if count == 0:\n-            raise ValueError(\"One of `filepath` or `content` must be provided.\")\n+            raise ValueError(\"One of `filepath` or `content` or `url` must be provided.\")\n         elif count > 1:\n-            raise ValueError(\"Only one of `filepath` or `content` should be provided.\")\n+            raise ValueError(\"Only one of `filepath` or `content` or `url` should be provided.\")\n \n         return data\n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/memory/v2/memory.py",
            "diff": "diff --git a/libs/agno/agno/memory/v2/memory.py b/libs/agno/agno/memory/v2/memory.py\nindex 8ce44cb6c..2da4004ad 100644\n--- a/libs/agno/agno/memory/v2/memory.py\n+++ b/libs/agno/agno/memory/v2/memory.py\n@@ -374,6 +374,10 @@ class Memory:\n         if refresh_from_db:\n             self.refresh_from_db(user_id=user_id)\n \n+        if memory_id not in self.memories[user_id]:  # type: ignore\n+            log_warning(f\"Memory {memory_id} not found for user {user_id}\")\n+            return None\n+\n         del self.memories[user_id][memory_id]  # type: ignore\n         if self.db:\n             self._delete_db_memory(memory_id=memory_id)\n@@ -669,6 +673,7 @@ class Memory:\n         \"\"\"Adds a RunResponse to the runs list.\"\"\"\n         if not self.runs:\n             self.runs = {}\n+\n         self.runs.setdefault(session_id, []).append(run)\n         log_debug(\"Added RunResponse to Memory\")\n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/aws/bedrock.py",
            "diff": "diff --git a/libs/agno/agno/models/aws/bedrock.py b/libs/agno/agno/models/aws/bedrock.py\nindex c5d0e25e3..27399d5b6 100644\n--- a/libs/agno/agno/models/aws/bedrock.py\n+++ b/libs/agno/agno/models/aws/bedrock.py\n@@ -125,8 +125,7 @@ class AwsBedrock(Model):\n             \"stopSequences\": self.stop_sequences,\n         }\n \n-        request_kwargs = {k: v for k, v in request_kwargs.items() if v is not None}\n-        return request_kwargs\n+        return {k: v for k, v in request_kwargs.items() if v is not None}\n \n     def _format_messages(self, messages: List[Message]) -> Tuple[List[Dict[str, Any]], Optional[List[Dict[str, Any]]]]:\n         formatted_messages: List[Dict[str, Any]] = []\n@@ -135,8 +134,7 @@ class AwsBedrock(Model):\n             if message.role == \"system\":\n                 system_message = [{\"text\": message.content}]\n             else:\n-                formatted_message: Dict[str, Any] = {\"role\": message.role}\n-                formatted_message[\"content\"] = []\n+                formatted_message: Dict[str, Any] = {\"role\": message.role, \"content\": []}\n                 # Handle tool results\n                 if isinstance(message.content, list):\n                     formatted_message[\"content\"].extend(message.content)\n@@ -158,56 +156,53 @@ class AwsBedrock(Model):\n \n                 if message.images:\n                     for image in message.images:\n-                        # Only supported via bytes for now\n-                        if image.content and image.format:\n-                            if image.format not in [\"png\", \"jpeg\", \"webp\", \"gif\"]:\n-                                raise ValueError(f\"Unsupported image format: {image.format}\")\n-\n-                            formatted_message[\"content\"].append(\n-                                {\n-                                    \"image\": {\n-                                        \"format\": image.format,\n-                                        \"source\": {\n-                                            \"bytes\": image.content,\n-                                        },\n-                                    }\n-                                }\n-                            )\n-                        else:\n+                        if not image.content or not image.format:\n                             raise ValueError(\"Image content and format are required.\")\n \n+                        if image.format not in [\"png\", \"jpeg\", \"webp\", \"gif\"]:\n+                            raise ValueError(f\"Unsupported image format: {image.format}\")\n+\n+                        formatted_message[\"content\"].append(\n+                            {\n+                                \"image\": {\n+                                    \"format\": image.format,\n+                                    \"source\": {\n+                                        \"bytes\": image.content,\n+                                    },\n+                                }\n+                            }\n+                        )\n                 if message.audio:\n                     log_warning(\"Audio input is currently unsupported.\")\n \n                 if message.videos:\n                     for video in message.videos:\n-                        if video.content and video.format:\n-                            if video.format not in [\n-                                \"mp4\",\n-                                \"mov\",\n-                                \"mkv\",\n-                                \"webm\",\n-                                \"flv\",\n-                                \"mpeg\",\n-                                \"mpg\",\n-                                \"wmv\",\n-                                \"three_gp\",\n-                            ]:\n-                                raise ValueError(f\"Unsupported video format: {video.format}\")\n-\n-                            formatted_message[\"content\"].append(\n-                                {\n-                                    \"video\": {\n-                                        \"format\": video.format,\n-                                        \"source\": {\n-                                            \"bytes\": video.content,\n-                                        },\n-                                    }\n-                                }\n-                            )\n-                        else:\n+                        if not video.content or not video.format:\n                             raise ValueError(\"Video content and format are required.\")\n \n+                        if video.format not in [\n+                            \"mp4\",\n+                            \"mov\",\n+                            \"mkv\",\n+                            \"webm\",\n+                            \"flv\",\n+                            \"mpeg\",\n+                            \"mpg\",\n+                            \"wmv\",\n+                            \"three_gp\",\n+                        ]:\n+                            raise ValueError(f\"Unsupported video format: {video.format}\")\n+\n+                        formatted_message[\"content\"].append(\n+                            {\n+                                \"video\": {\n+                                    \"format\": video.format,\n+                                    \"source\": {\n+                                        \"bytes\": video.content,\n+                                    },\n+                                }\n+                            }\n+                        )\n                 if message.files is not None and len(message.files) > 0:\n                     log_warning(\"File input is currently unsupported.\")\n \n@@ -227,7 +222,6 @@ class AwsBedrock(Model):\n         \"\"\"\n         try:\n             formatted_messages, system_message = self._format_messages(messages)\n-            inference_config = self._get_inference_config()\n \n             tool_config = None\n             if self._functions is not None:\n@@ -236,10 +230,13 @@ class AwsBedrock(Model):\n             body = {\n                 \"system\": system_message,\n                 \"toolConfig\": tool_config,\n-                \"inferenceConfig\": inference_config,\n+                \"inferenceConfig\": self._get_inference_config(),\n             }\n             body = {k: v for k, v in body.items() if v is not None}\n \n+            if self.request_params:\n+                body.update(**self.request_params)\n+\n             return self.get_client().converse(modelId=self.id, messages=formatted_messages, **body)\n         except ClientError as e:\n             log_error(f\"Unexpected error calling Bedrock API: {str(e)}\")\n@@ -260,7 +257,6 @@ class AwsBedrock(Model):\n         \"\"\"\n         try:\n             formatted_messages, system_message = self._format_messages(messages)\n-            inference_config = self._get_inference_config()\n \n             tool_config = None\n             if self._functions is not None:\n@@ -269,10 +265,13 @@ class AwsBedrock(Model):\n             body = {\n                 \"system\": system_message,\n                 \"toolConfig\": tool_config,\n-                \"inferenceConfig\": inference_config,\n+                \"inferenceConfig\": self._get_inference_config(),\n             }\n             body = {k: v for k, v in body.items() if v is not None}\n \n+            if self.request_params:\n+                body.update(**self.request_params)\n+\n             return self.get_client().converse_stream(modelId=self.id, messages=formatted_messages, **body)[\"stream\"]\n         except ClientError as e:\n             log_error(f\"Unexpected error calling Bedrock API: {str(e)}\")\n@@ -293,7 +292,7 @@ class AwsBedrock(Model):\n             function_call_results (List[Message]): The results of the function calls.\n             tool_ids (List[str]): The tool ids.\n         \"\"\"\n-        if len(function_call_results) > 0:\n+        if function_call_results:\n             tool_result_content: List = []\n \n             for _fc_message_index, _fc_message in enumerate(function_call_results):\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/google/gemini.py",
            "diff": "diff --git a/libs/agno/agno/models/google/gemini.py b/libs/agno/agno/models/google/gemini.py\nindex fe0c73fe8..6ba84800e 100644\n--- a/libs/agno/agno/models/google/gemini.py\n+++ b/libs/agno/agno/models/google/gemini.py\n@@ -52,7 +52,7 @@ class Gemini(Model):\n     Based on https://googleapis.github.io/python-genai/\n     \"\"\"\n \n-    id: str = \"gemini-2.0-flash-exp\"\n+    id: str = \"gemini-2.0-flash-001\"\n     name: str = \"Gemini\"\n     provider: str = \"Google\"\n \n@@ -491,7 +491,7 @@ class Gemini(Model):\n             log_warning(f\"Unknown audio type: {type(audio.content)}\")\n             return None\n \n-    def _format_video_for_message(self, video: Video) -> Optional[GeminiFile]:\n+    def _format_video_for_message(self, video: Video) -> Optional[Part]:\n         # Case 1: Video is a bytes object\n         if video.content and isinstance(video.content, bytes):\n             return Part.from_bytes(\n@@ -538,6 +538,12 @@ class Gemini(Model):\n             return Part.from_uri(\n                 file_uri=video_file.uri, mime_type=f\"video/{video.format}\" if video.format else \"video/mp4\"\n             )\n+        # Case 3: Video is a URL\n+        elif video.url is not None:\n+            return Part.from_uri(\n+                file_uri=video.url,\n+                mime_type=f\"video/{video.format}\" if video.format else \"video/webm\",\n+            )\n         else:\n             log_warning(f\"Unknown video type: {type(video.content)}\")\n             return None\n@@ -658,7 +664,9 @@ class Gemini(Model):\n \n                     # Extract function call if present\n                     if hasattr(part, \"function_call\") and part.function_call is not None:\n+                        call_id = part.function_call.id if part.function_call.id else str(uuid4())\n                         tool_call = {\n+                            \"id\": call_id,\n                             \"type\": \"function\",\n                             \"function\": {\n                                 \"name\": part.function_call.name,\n@@ -722,7 +730,9 @@ class Gemini(Model):\n \n                 # Extract function call if present\n                 if hasattr(part, \"function_call\") and part.function_call is not None:\n+                    call_id = part.function_call.id if part.function_call.id else str(uuid4())\n                     tool_call = {\n+                        \"id\": call_id,\n                         \"type\": \"function\",\n                         \"function\": {\n                             \"name\": part.function_call.name,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/message.py",
            "diff": "diff --git a/libs/agno/agno/models/message.py b/libs/agno/agno/models/message.py\nindex 38dbfa5be..9173bc5f8 100644\n--- a/libs/agno/agno/models/message.py\n+++ b/libs/agno/agno/models/message.py\n@@ -330,7 +330,12 @@ class Message(BaseModel):\n                 tool_call_arguments = tool_call.get(\"function\", {}).get(\"arguments\")\n                 if tool_call_arguments:\n                     try:\n-                        arguments = \", \".join(f\"{k}: {v}\" for k, v in json.loads(tool_call_arguments).items())\n+                        tool_call_args: dict = (\n+                            tool_call_arguments\n+                            if isinstance(tool_call_arguments, dict)\n+                            else json.loads(tool_call_arguments)\n+                        )\n+                        arguments = \", \".join(f\"{k}: {v}\" for k, v in tool_call_args.items())\n                         tool_calls_list.append(f\"    Arguments: '{arguments}'\")\n                     except json.JSONDecodeError:\n                         tool_calls_list.append(\"    Arguments: 'Invalid JSON format'\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/meta/__init__.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/__init__.py b/libs/agno/agno/models/meta/__init__.py\nnew file mode 100644\nindex 000000000..dfbea4261\n--- /dev/null\n+++ b/libs/agno/agno/models/meta/__init__.py\n@@ -0,0 +1,12 @@\n+from agno.models.meta.llama import Llama\n+\n+try:\n+    from agno.models.meta.llama_openai import LlamaOpenAI\n+except ImportError:\n+\n+    class LlamaOpenAI:  # type: ignore\n+        def __init__(self, *args, **kwargs):\n+            raise ImportError(\"`openai` not installed. Please install it via `pip install openai`\")\n+\n+\n+__all__ = [\"Llama\", \"LlamaOpenAI\"]\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/meta/llama.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/llama.py b/libs/agno/agno/models/meta/llama.py\nnew file mode 100644\nindex 000000000..cd607d5b5\n--- /dev/null\n+++ b/libs/agno/agno/models/meta/llama.py\n@@ -0,0 +1,450 @@\n+from collections.abc import AsyncIterator\n+from dataclasses import dataclass\n+from os import getenv\n+from typing import Any, Dict, Iterator, List, Optional, Union\n+\n+import httpx\n+from pydantic import BaseModel\n+\n+from agno.exceptions import ModelProviderError\n+from agno.models.base import Model\n+from agno.models.message import Message\n+from agno.models.response import ModelResponse\n+from agno.utils.log import log_error, log_warning\n+from agno.utils.models.llama import format_message\n+\n+try:\n+    from llama_api_client import AsyncLlamaAPIClient, LlamaAPIClient\n+    from llama_api_client.types.create_chat_completion_response import CreateChatCompletionResponse\n+    from llama_api_client.types.create_chat_completion_response_stream_chunk import (\n+        CreateChatCompletionResponseStreamChunk,\n+        EventDeltaTextDelta,\n+        EventDeltaToolCallDelta,\n+        EventDeltaToolCallDeltaFunction,\n+    )\n+    from llama_api_client.types.message_text_content_item import MessageTextContentItem\n+except (ImportError, ModuleNotFoundError):\n+    raise ImportError(\"`llama-api-client` not installed. Please install using `pip install llama-api-client`\")\n+\n+\n+@dataclass\n+class Llama(Model):\n+    \"\"\"\n+    A class for interacting with Llama models using the Llama API using the Llama SDK.\n+    \"\"\"\n+\n+    id: str = \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n+    name: str = \"Llama\"\n+    provider: str = \"Llama\"\n+    supports_native_structured_outputs: bool = False\n+    supports_json_schema_outputs: bool = True\n+\n+    # Request parameters\n+    max_completion_tokens: Optional[int] = None\n+    repetition_penalty: Optional[float] = None\n+    temperature: Optional[float] = None\n+    top_p: Optional[float] = None\n+    top_k: Optional[int] = None\n+    extra_headers: Optional[Any] = None\n+    extra_query: Optional[Any] = None\n+    extra_body: Optional[Any] = None\n+    request_params: Optional[Dict[str, Any]] = None\n+\n+    # Client parameters\n+    api_key: Optional[str] = None\n+    base_url: Optional[Union[str, httpx.URL]] = None\n+    timeout: Optional[float] = None\n+    max_retries: Optional[int] = None\n+    default_headers: Optional[Any] = None\n+    default_query: Optional[Any] = None\n+    http_client: Optional[httpx.Client] = None\n+    client_params: Optional[Dict[str, Any]] = None\n+\n+    # OpenAI clients\n+    client: Optional[LlamaAPIClient] = None\n+    async_client: Optional[AsyncLlamaAPIClient] = None\n+\n+    def _get_client_params(self) -> Dict[str, Any]:\n+        # Fetch API key from env if not already set\n+        if not self.api_key:\n+            self.api_key = getenv(\"LLAMA_API_KEY\")\n+            if not self.api_key:\n+                log_error(\"LLAMA_API_KEY not set. Please set the LLAMA_API_KEY environment variable.\")\n+\n+        # Define base client params\n+        base_params = {\n+            \"api_key\": self.api_key,\n+            \"base_url\": self.base_url,\n+            \"timeout\": self.timeout,\n+            \"max_retries\": self.max_retries,\n+            \"default_headers\": self.default_headers,\n+            \"default_query\": self.default_query,\n+        }\n+\n+        # Create client_params dict with non-None values\n+        client_params = {k: v for k, v in base_params.items() if v is not None}\n+\n+        # Add additional client params if provided\n+        if self.client_params:\n+            client_params.update(self.client_params)\n+        return client_params\n+\n+    def get_client(self) -> LlamaAPIClient:\n+        \"\"\"\n+        Returns an Llama client.\n+\n+        Returns:\n+            LlamaAPIClient: An instance of the Llama client.\n+        \"\"\"\n+        if self.client:\n+            return self.client\n+\n+        client_params: Dict[str, Any] = self._get_client_params()\n+        if self.http_client is not None:\n+            client_params[\"http_client\"] = self.http_client\n+        self.client = LlamaAPIClient(**client_params)\n+        return self.client\n+\n+    def get_async_client(self) -> AsyncLlamaAPIClient:\n+        \"\"\"\n+        Returns an asynchronous Llama client.\n+\n+        Returns:\n+            AsyncLlamaAPIClient: An instance of the asynchronous Llama client.\n+        \"\"\"\n+        if self.async_client:\n+            return self.async_client\n+\n+        client_params: Dict[str, Any] = self._get_client_params()\n+        if self.http_client:\n+            client_params[\"http_client\"] = self.http_client\n+        else:\n+            # Create a new async HTTP client with custom limits\n+            client_params[\"http_client\"] = httpx.AsyncClient(\n+                limits=httpx.Limits(max_connections=1000, max_keepalive_connections=100)\n+            )\n+        return AsyncLlamaAPIClient(**client_params)\n+\n+    @property\n+    def request_kwargs(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Returns keyword arguments for API requests.\n+\n+        Returns:\n+            Dict[str, Any]: A dictionary of keyword arguments for API requests.\n+        \"\"\"\n+        # Define base request parameters\n+        base_params = {\n+            \"max_completion_tokens\": self.max_completion_tokens,\n+            \"repetition_penalty\": self.repetition_penalty,\n+            \"temperature\": self.temperature,\n+            \"top_p\": self.top_p,\n+            \"top_k\": self.top_k,\n+            \"extra_headers\": self.extra_headers,\n+            \"extra_query\": self.extra_query,\n+            \"extra_body\": self.extra_body,\n+            \"request_params\": self.request_params,\n+        }\n+\n+        # Filter out None values\n+        request_params = {k: v for k, v in base_params.items() if v is not None}\n+\n+        # Add tools\n+        if self._tools is not None and len(self._tools) > 0:\n+            request_params[\"tools\"] = self._tools\n+\n+            # Fix optional parameters where the \"type\" is [<type>, null]\n+            for tool in request_params[\"tools\"]:  # type: ignore\n+                if \"parameters\" in tool[\"function\"] and \"properties\" in tool[\"function\"][\"parameters\"]:  # type: ignore\n+                    for _, obj in tool[\"function\"][\"parameters\"].get(\"properties\", {}).items():  # type: ignore\n+                        if isinstance(obj[\"type\"], list):\n+                            obj[\"type\"] = obj[\"type\"][0]\n+\n+        if self.response_format is not None:\n+            request_params[\"response_format\"] = self.response_format\n+\n+        # Add additional request params if provided\n+        if self.request_params:\n+            request_params.update(self.request_params)\n+\n+        return request_params\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Convert the model to a dictionary.\n+\n+        Returns:\n+            Dict[str, Any]: The dictionary representation of the model.\n+        \"\"\"\n+        model_dict = super().to_dict()\n+        model_dict.update(\n+            {\n+                \"max_completion_tokens\": self.max_completion_tokens,\n+                \"repetition_penalty\": self.repetition_penalty,\n+                \"temperature\": self.temperature,\n+                \"top_p\": self.top_p,\n+                \"top_k\": self.top_k,\n+                \"extra_headers\": self.extra_headers,\n+                \"extra_query\": self.extra_query,\n+                \"extra_body\": self.extra_body,\n+                \"request_params\": self.request_params,\n+            }\n+        )\n+        if self._tools is not None:\n+            model_dict[\"tools\"] = self._tools\n+        cleaned_dict = {k: v for k, v in model_dict.items() if v is not None}\n+        return cleaned_dict\n+\n+    def invoke(self, messages: List[Message]) -> CreateChatCompletionResponse:\n+        \"\"\"\n+        Send a chat completion request to the Llama API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            CreateChatCompletionResponse: The chat completion response from the API.\n+        \"\"\"\n+        return self.get_client().chat.completions.create(\n+            model=self.id,\n+            messages=[format_message(m) for m in messages],  # type: ignore\n+            **self.request_kwargs,\n+        )\n+\n+    async def ainvoke(self, messages: List[Message]) -> CreateChatCompletionResponse:\n+        \"\"\"\n+        Sends an asynchronous chat completion request to the Llama API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            CreateChatCompletionResponse: The chat completion response from the API.\n+        \"\"\"\n+\n+        return await self.get_async_client().chat.completions.create(\n+            model=self.id,\n+            messages=[format_message(m) for m in messages],  # type: ignore\n+            **self.request_kwargs,\n+        )\n+\n+    def invoke_stream(self, messages: List[Message]) -> Iterator[CreateChatCompletionResponseStreamChunk]:\n+        \"\"\"\n+        Send a streaming chat completion request to the Llama API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            Iterator[CreateChatCompletionResponseStreamChunk]: An iterator of chat completion chunks.\n+        \"\"\"\n+\n+        try:\n+            yield from self.get_client().chat.completions.create(\n+                model=self.id,\n+                messages=[format_message(m) for m in messages],  # type: ignore\n+                stream=True,\n+                **self.request_kwargs,\n+            )  # type: ignore\n+        except Exception as e:\n+            log_error(f\"Error from Llama API: {e}\")\n+            raise ModelProviderError(message=str(e), model_name=self.name, model_id=self.id) from e\n+\n+    async def ainvoke_stream(self, messages: List[Message]) -> AsyncIterator[CreateChatCompletionResponseStreamChunk]:\n+        \"\"\"\n+        Sends an asynchronous streaming chat completion request to the Llama API.\n+\n+        Args:\n+            messages (List[Message]): A list of messages to send to the model.\n+\n+        Returns:\n+            AsyncIterator[CreateChatCompletionResponseStreamChunk]: An asynchronous iterator of chat completion chunks.\n+        \"\"\"\n+\n+        try:\n+            async_stream = await self.get_async_client().chat.completions.create(\n+                model=self.id,\n+                messages=[format_message(m) for m in messages],  # type: ignore\n+                stream=True,\n+                **self.request_kwargs,\n+            )\n+            async for chunk in async_stream:  # type: ignore\n+                yield chunk  # type: ignore\n+        except Exception as e:\n+            log_error(f\"Error from Llama API: {e}\")\n+            raise ModelProviderError(message=str(e), model_name=self.name, model_id=self.id) from e\n+\n+    @staticmethod\n+    def parse_tool_calls(tool_calls_data: List[EventDeltaToolCallDeltaFunction]) -> List[Dict[str, Any]]:\n+        \"\"\"\n+        Parse the tool calls from the Llama API.\n+\n+        Args:\n+            tool_calls_data (List[Tuple[str, Any]]): The tool calls data.\n+\n+        Returns:\n+            List[Dict[str, Any]]: The parsed tool calls.\n+        \"\"\"\n+        tool_calls: List[Dict[str, Any]] = []\n+\n+        _tool_call_id: Optional[str] = None\n+        _function_name_parts: List[str] = []\n+        _function_arguments_parts: List[str] = []\n+\n+        def _create_tool_call():\n+            nonlocal _tool_call_id\n+            if _tool_call_id and (_function_name_parts or _function_arguments_parts):\n+                tool_calls.append(\n+                    {\n+                        \"id\": _tool_call_id,\n+                        \"type\": \"function\",\n+                        \"function\": {\n+                            \"name\": \"\".join(_function_name_parts),\n+                            \"arguments\": \"\".join(_function_arguments_parts),\n+                        },\n+                    }\n+                )\n+            _tool_call_id = None\n+            _function_name_parts.clear()\n+            _function_arguments_parts.clear()\n+\n+        for _field, _value in tool_calls_data:\n+            if _field == \"function\" and isinstance(_value, EventDeltaToolCallDeltaFunction):\n+                if _value.name and (_tool_call_id or _function_name_parts or _function_arguments_parts):\n+                    _create_tool_call()\n+                if _value.name:\n+                    _function_name_parts.append(_value.name)\n+                if _value.arguments:\n+                    _function_arguments_parts.append(_value.arguments)\n+\n+            elif _field == \"id\":\n+                if _value and _tool_call_id:\n+                    _create_tool_call()\n+                if _value:\n+                    _tool_call_id = _value  # type: ignore\n+\n+        _create_tool_call()\n+\n+        return tool_calls\n+\n+    def parse_provider_response(self, response: CreateChatCompletionResponse) -> ModelResponse:\n+        \"\"\"\n+        Parse the Llama response into a ModelResponse.\n+\n+        Args:\n+            response: Response from invoke() method\n+\n+        Returns:\n+            ModelResponse: Parsed response data\n+        \"\"\"\n+        model_response = ModelResponse()\n+\n+        # Get response message\n+        response_message = response.completion_message\n+\n+        # Parse structured outputs if enabled\n+        try:\n+            if (\n+                self.response_format is not None\n+                and self.structured_outputs\n+                and issubclass(self.response_format, BaseModel)\n+            ):\n+                parsed_object = response_message.content  # type: ignore\n+                if parsed_object is not None:\n+                    model_response.parsed = parsed_object\n+        except Exception as e:\n+            log_warning(f\"Error retrieving structured outputs: {e}\")\n+\n+        # Add role\n+        if response_message.role is not None:\n+            model_response.role = response_message.role\n+\n+        # Add content\n+        if response_message.content is not None:\n+            if isinstance(response_message.content, MessageTextContentItem):\n+                model_response.content = response_message.content.text\n+            else:\n+                model_response.content = response_message.content\n+\n+        # Add tool calls\n+        if response_message.tool_calls is not None and len(response_message.tool_calls) > 0:\n+            try:\n+                for tool_call in response_message.tool_calls:\n+                    tool_name = tool_call.function.name\n+                    tool_input = tool_call.function.arguments\n+\n+                    function_def = {\"name\": tool_name}\n+                    if tool_input:\n+                        function_def[\"arguments\"] = tool_input\n+\n+                    model_response.tool_calls.append(\n+                        {\n+                            \"id\": tool_call.id,\n+                            \"type\": \"function\",\n+                            \"function\": function_def,\n+                        }\n+                    )\n+\n+            except Exception as e:\n+                log_warning(f\"Error processing tool calls: {e}\")\n+\n+        # Add metrics from the metrics list\n+        if hasattr(response, \"metrics\") and response.metrics is not None:\n+            usage_data = {}\n+            metric_map = {\n+                \"num_prompt_tokens\": \"input_tokens\",\n+                \"num_completion_tokens\": \"output_tokens\",\n+                \"num_total_tokens\": \"total_tokens\",\n+            }\n+\n+            for metric in response.metrics:\n+                key = metric_map.get(metric.metric)\n+                if key:\n+                    value = int(metric.value)\n+                    usage_data[key] = value\n+\n+                if usage_data:\n+                    model_response.response_usage = usage_data\n+\n+        return model_response\n+\n+    def parse_provider_response_delta(self, response_delta: CreateChatCompletionResponseStreamChunk) -> ModelResponse:\n+        \"\"\"\n+        Parse the Llama streaming response into a ModelResponse.\n+\n+        Args:\n+            response_delta: Raw response chunk from the Llama API\n+\n+        Returns:\n+            ModelResponse: Parsed response data\n+        \"\"\"\n+        model_response = ModelResponse()\n+\n+        if response_delta is not None:\n+            delta = response_delta.event\n+\n+            # Capture metrics event\n+            if delta.event_type == \"metrics\" and delta.metrics is not None:\n+                usage_data = {}\n+                metric_map = {\n+                    \"num_prompt_tokens\": \"input_tokens\",\n+                    \"num_completion_tokens\": \"output_tokens\",\n+                    \"num_total_tokens\": \"total_tokens\",\n+                }\n+\n+                for metric in delta.metrics:\n+                    key = metric_map.get(metric.metric)\n+                    if key:\n+                        usage_data[key] = int(metric.value)\n+\n+                if usage_data:\n+                    model_response.response_usage = usage_data\n+\n+            if isinstance(delta.delta, EventDeltaTextDelta):\n+                model_response.content = delta.delta.text\n+\n+            # Add tool calls\n+            if isinstance(delta.delta, EventDeltaToolCallDelta):\n+                model_response.tool_calls = delta.delta  # type: ignore\n+\n+        return model_response\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/meta/llama_openai.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/llama_openai.py b/libs/agno/agno/models/meta/llama_openai.py\nnew file mode 100644\nindex 000000000..3194e5c6d\n--- /dev/null\n+++ b/libs/agno/agno/models/meta/llama_openai.py\n@@ -0,0 +1,122 @@\n+from dataclasses import dataclass\n+from os import getenv\n+from typing import Any, Dict, Optional\n+\n+import httpx\n+\n+try:\n+    from openai import AsyncOpenAI as AsyncOpenAIClient\n+except (ModuleNotFoundError, ImportError):\n+    raise ImportError(\"`openai` not installed. Please install using `pip install openai`\")\n+\n+from agno.models.meta.llama import Message\n+from agno.models.openai.like import OpenAILike\n+from agno.utils.models.llama import format_message\n+\n+\n+@dataclass\n+class LlamaOpenAI(OpenAILike):\n+    \"\"\"\n+    Class for interacting with the Llama API via OpenAI-like interface.\n+\n+    Attributes:\n+        id (str): The ID of the language model.\n+        name (str): The name of the API.\n+        provider (str): The provider of the API.\n+        api_key (Optional[str]): The API key for the xAI API.\n+        base_url (Optional[str]): The base URL for the xAI API.\n+    \"\"\"\n+\n+    id: str = \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n+    name: str = \"LlamaOpenAI\"\n+    provider: str = \"LlamaOpenAI\"\n+\n+    api_key: Optional[str] = getenv(\"LLAMA_API_KEY\")\n+    base_url: Optional[str] = \"https://api.llama.com/compat/v1/\"\n+\n+    # Request parameters\n+    max_completion_tokens: Optional[int] = None\n+    repetition_penalty: Optional[float] = None\n+    temperature: Optional[float] = None\n+    top_p: Optional[float] = None\n+    top_k: Optional[int] = None\n+    extra_headers: Optional[Any] = None\n+    extra_query: Optional[Any] = None\n+    extra_body: Optional[Any] = None\n+    request_params: Optional[Dict[str, Any]] = None\n+\n+    supports_native_structured_outputs: bool = False\n+    supports_json_schema_outputs: bool = True\n+\n+    @property\n+    def request_kwargs(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Returns keyword arguments for API requests.\n+\n+        Returns:\n+            Dict[str, Any]: A dictionary of keyword arguments for API requests.\n+        \"\"\"\n+        # Define base request parameters\n+        base_params = {\n+            \"max_completion_tokens\": self.max_completion_tokens,\n+            \"repetition_penalty\": self.repetition_penalty,\n+            \"temperature\": self.temperature,\n+            \"top_p\": self.top_p,\n+            \"top_k\": self.top_k,\n+            \"extra_headers\": self.extra_headers,\n+            \"extra_query\": self.extra_query,\n+            \"extra_body\": self.extra_body,\n+            \"request_params\": self.request_params,\n+        }\n+\n+        # Filter out None values\n+        request_params = {k: v for k, v in base_params.items() if v is not None}\n+\n+        # Add tools\n+        if self._tools is not None and len(self._tools) > 0:\n+            request_params[\"tools\"] = self._tools\n+\n+            # Fix optional parameters where the \"type\" is [<type>, null]\n+            for tool in request_params[\"tools\"]:  # type: ignore\n+                if \"parameters\" in tool[\"function\"] and \"properties\" in tool[\"function\"][\"parameters\"]:  # type: ignore\n+                    for _, obj in tool[\"function\"][\"parameters\"].get(\"properties\", {}).items():  # type: ignore\n+                        if isinstance(obj[\"type\"], list):\n+                            obj[\"type\"] = obj[\"type\"][0]\n+\n+        if self.response_format is not None:\n+            request_params[\"response_format\"] = self.response_format\n+\n+        # Add additional request params if provided\n+        if self.request_params:\n+            request_params.update(self.request_params)\n+\n+        return request_params\n+\n+    def _format_message(self, message: Message) -> Dict[str, Any]:\n+        \"\"\"\n+        Format a message into the format expected by Llama API.\n+\n+        Args:\n+            message (Message): The message to format.\n+\n+        Returns:\n+            Dict[str, Any]: The formatted message.\n+        \"\"\"\n+        return format_message(message, openai_like=True)\n+\n+    def get_async_client(self):\n+        \"\"\"Override to provide custom httpx client that properly handles redirects\"\"\"\n+        if self.async_client:\n+            return self.async_client\n+\n+        client_params = self._get_client_params()\n+\n+        # Llama gives a 307 redirect error, so we need to set up a custom client to allow redirects\n+        client_params[\"http_client\"] = httpx.AsyncClient(\n+            limits=httpx.Limits(max_connections=1000, max_keepalive_connections=100),\n+            follow_redirects=True,\n+            timeout=httpx.Timeout(30.0),\n+        )\n+\n+        self.async_client = AsyncOpenAIClient(**client_params)\n+        return self.async_client\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/openai/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/chat.py b/libs/agno/agno/models/openai/chat.py\nindex 15e421fe5..740d67059 100644\n--- a/libs/agno/agno/models/openai/chat.py\n+++ b/libs/agno/agno/models/openai/chat.py\n@@ -12,7 +12,7 @@ from agno.models.base import Model\n from agno.models.message import Message\n from agno.models.response import ModelResponse\n from agno.utils.log import log_error, log_warning\n-from agno.utils.openai import audio_to_message, images_to_message\n+from agno.utils.openai import _format_file_for_message, audio_to_message, images_to_message\n \n try:\n     from openai import APIConnectionError, APIStatusError, RateLimitError\n@@ -291,6 +291,20 @@ class OpenAIChat(Model):\n         if message.tool_calls is not None and len(message.tool_calls) == 0:\n             message_dict[\"tool_calls\"] = None\n \n+        if message.files is not None:\n+            # Ensure content is a list of parts\n+            content = message_dict.get(\"content\")\n+            if isinstance(content, str):  # wrap existing text\n+                text = content\n+                message_dict[\"content\"] = [{\"type\": \"text\", \"text\": text}]\n+            elif content is None:\n+                message_dict[\"content\"] = []\n+            # Insert each file part before text parts\n+            for file in message.files:\n+                file_part = _format_file_for_message(file)\n+                if file_part:\n+                    message_dict[\"content\"].insert(0, file_part)\n+\n         # Manually add the content field even if it is None\n         if message.content is None:\n             message_dict[\"content\"] = None\n@@ -372,7 +386,6 @@ class OpenAIChat(Model):\n         Returns:\n             ChatCompletion: The chat completion response from the API.\n         \"\"\"\n-\n         try:\n             if self.response_format is not None and self.structured_outputs:\n                 if isinstance(self.response_format, type) and issubclass(self.response_format, BaseModel):\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/models/openai/responses.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/responses.py b/libs/agno/agno/models/openai/responses.py\nindex 223a37275..2c0808a51 100644\n--- a/libs/agno/agno/models/openai/responses.py\n+++ b/libs/agno/agno/models/openai/responses.py\n@@ -10,7 +10,7 @@ from agno.media import File\n from agno.models.base import MessageData, Model\n from agno.models.message import Citations, Message, UrlCitation\n from agno.models.response import ModelResponse\n-from agno.utils.log import log_error, log_warning\n+from agno.utils.log import log_debug, log_error, log_warning\n from agno.utils.models.openai_responses import images_to_message, sanitize_response_schema\n \n try:\n@@ -151,7 +151,7 @@ class OpenAIResponses(Model):\n         self.async_client = AsyncOpenAI(**client_params)\n         return self.async_client\n \n-    def get_request_params(self) -> Dict[str, Any]:\n+    def get_request_params(self, messages: List[Message]) -> Dict[str, Any]:\n         \"\"\"\n         Returns keyword arguments for API requests.\n \n@@ -193,9 +193,32 @@ class OpenAIResponses(Model):\n         # Filter out None values\n         request_params: Dict[str, Any] = {k: v for k, v in base_params.items() if v is not None}\n \n+        if self._tools:\n+            request_params[\"tools\"] = self._format_tool_params(messages=messages)\n+\n         if self.tool_choice is not None:\n             request_params[\"tool_choice\"] = self.tool_choice\n \n+        # Handle reasoning tools for o3 and o4-mini models\n+        if self.id.startswith(\"o3\") or self.id.startswith(\"o4-mini\"):\n+            request_params[\"store\"] = True\n+\n+            # Check if the last assistant message has a previous_response_id to continue from\n+            previous_response_id = None\n+            for msg in reversed(messages):\n+                if (\n+                    msg.role == \"assistant\"\n+                    and hasattr(msg, \"provider_data\")\n+                    and msg.provider_data\n+                    and \"response_id\" in msg.provider_data\n+                ):\n+                    previous_response_id = msg.provider_data[\"response_id\"]\n+                    log_debug(f\"Using previous_response_id: {previous_response_id}\")\n+                    break\n+\n+            if previous_response_id:\n+                request_params[\"previous_response_id\"] = previous_response_id\n+\n         # Add additional request params if provided\n         if self.request_params:\n             request_params.update(self.request_params)\n@@ -329,6 +352,13 @@ class OpenAIResponses(Model):\n \n                 formatted_messages.append(message_dict)\n \n+            if self.id.startswith((\"o3\", \"o4-mini\")):\n+                if message.role == \"tool\":\n+                    if message.tool_call_id and message.content is not None:\n+                        formatted_messages.append(\n+                            {\"type\": \"function_call_output\", \"call_id\": message.tool_call_id, \"output\": message.content}\n+                        )\n+\n             else:\n                 # OpenAI expects the tool_calls to be None if empty, not an empty list\n                 if message.tool_calls is not None and len(message.tool_calls) > 0:\n@@ -361,9 +391,7 @@ class OpenAIResponses(Model):\n             Response: The response from the API.\n         \"\"\"\n         try:\n-            request_params = self.get_request_params()\n-            if self._tools:\n-                request_params[\"tools\"] = self._format_tool_params(messages=messages)\n+            request_params = self.get_request_params(messages=messages)\n \n             return self.get_client().responses.create(\n                 model=self.id,\n@@ -416,9 +444,8 @@ class OpenAIResponses(Model):\n             Response: The response from the API.\n         \"\"\"\n         try:\n-            request_params = self.get_request_params()\n-            if self._tools:\n-                request_params[\"tools\"] = self._format_tool_params(messages=messages)\n+            request_params = self.get_request_params(messages=messages)\n+\n             return await self.get_async_client().responses.create(\n                 model=self.id,\n                 input=self._format_messages(messages),  # type: ignore\n@@ -470,9 +497,8 @@ class OpenAIResponses(Model):\n             Iterator[ResponseStreamEvent]: An iterator of response stream events.\n         \"\"\"\n         try:\n-            request_params = self.get_request_params()\n-            if self._tools:\n-                request_params[\"tools\"] = self._format_tool_params(messages=messages)\n+            request_params = self.get_request_params(messages=messages)\n+\n             yield from self.get_client().responses.create(\n                 model=self.id,\n                 input=self._format_messages(messages),  # type: ignore\n@@ -525,9 +551,7 @@ class OpenAIResponses(Model):\n             Any: An asynchronous iterator of chat completion chunks.\n         \"\"\"\n         try:\n-            request_params = self.get_request_params()\n-            if self._tools:\n-                request_params[\"tools\"] = self._format_tool_params(messages=messages)\n+            request_params = self.get_request_params(messages=messages)\n             async_stream = await self.get_async_client().responses.create(\n                 model=self.id,\n                 input=self._format_messages(messages),  # type: ignore\n@@ -606,6 +630,12 @@ class OpenAIResponses(Model):\n                 model_id=self.id,\n             )\n \n+        # Store the response ID for continuity\n+        if response.id:\n+            if model_response.provider_data is None:\n+                model_response.provider_data = {}\n+            model_response.provider_data[\"response_id\"] = response.id\n+\n         # Add role\n         model_response.role = \"assistant\"\n         for output in response.output:\n@@ -673,6 +703,12 @@ class OpenAIResponses(Model):\n         model_response = None\n \n         if stream_event.type == \"response.created\":\n+            model_response = ModelResponse()\n+            # Store the response ID for continuity\n+            if stream_event.response.id:\n+                if stream_data.response_provider_data is None:\n+                    stream_data.response_provider_data = {}\n+                stream_data.response_provider_data[\"response_id\"] = stream_event.response.id\n             # Update metrics\n             if not assistant_message.metrics.time_to_first_token:\n                 assistant_message.metrics.set_time_to_first_token()\n@@ -755,6 +791,7 @@ class OpenAIResponses(Model):\n                 stream_data=stream_data,\n                 tool_use=tool_use,\n             )\n+\n             if model_response is not None:\n                 yield model_response\n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/team/team.py",
            "diff": "diff --git a/libs/agno/agno/team/team.py b/libs/agno/agno/team/team.py\nindex f4bd792b0..34ce10780 100644\n--- a/libs/agno/agno/team/team.py\n+++ b/libs/agno/agno/team/team.py\n@@ -475,9 +475,6 @@ class Team:\n         # Set debug mode\n         self._set_debug()\n \n-        # Make sure for the team, we are using the team logger\n-        use_team_logger()\n-\n         # Set monitoring and telemetry\n         self._set_monitoring()\n \n@@ -493,6 +490,9 @@ class Team:\n         for member in self.members:\n             self._initialize_member(member, session_id=session_id)\n \n+        # Make sure for the team, we are using the team logger\n+        use_team_logger()\n+\n     @property\n     def is_streamable(self) -> bool:\n         return self.response_model is None\n@@ -898,7 +898,7 @@ class Team:\n             self._make_memories_and_summaries(run_messages, session_id, user_id)\n \n             session_messages: List[Message] = []\n-            for run in self.memory.runs[session_id]:  # type: ignore\n+            for run in self.memory.runs.get(session_id, []):  # type: ignore\n                 if run.messages is not None:\n                     for m in run.messages:\n                         session_messages.append(m)\n@@ -1219,7 +1219,7 @@ class Team:\n             self._make_memories_and_summaries(run_messages, session_id, user_id)\n \n             session_messages: List[Message] = []\n-            for run in self.memory.runs[session_id]:  # type: ignore\n+            for run in self.memory.runs.get(session_id, []):  # type: ignore\n                 if run.messages is not None:\n                     for m in run.messages:\n                         session_messages.append(m)\n@@ -1638,7 +1638,7 @@ class Team:\n             await self._amake_memories_and_summaries(run_messages, session_id, user_id)\n \n             session_messages: List[Message] = []\n-            for run in self.memory.runs[session_id]:\n+            for run in self.memory.runs.get(session_id, []):\n                 for m in run.messages:\n                     session_messages.append(m)\n \n@@ -1966,7 +1966,7 @@ class Team:\n             await self._amake_memories_and_summaries(run_messages, session_id, user_id)\n \n             session_messages: List[Message] = []\n-            for run in self.memory.runs[session_id]:  # type: ignore\n+            for run in self.memory.runs.get(session_id, []):  # type: ignore\n                 if run.messages is not None:\n                     for m in run.messages:\n                         session_messages.append(m)\n@@ -3746,7 +3746,6 @@ class Team:\n     def _calculate_full_team_session_metrics(self, messages: List[Message], session_id: str) -> SessionMetrics:\n         current_session_metrics = self.session_metrics or self._calculate_session_metrics(messages)\n         current_session_metrics = replace(current_session_metrics)\n-\n         assistant_message_role = self.model.assistant_message_role if self.model is not None else \"assistant\"\n \n         # Get metrics of the team-agent's messages\n@@ -3822,7 +3821,9 @@ class Team:\n         reasoning_model: Optional[Model] = self.reasoning_model\n         reasoning_model_provided = reasoning_model is not None\n         if reasoning_model is None and self.model is not None:\n-            reasoning_model = self.model.__class__(id=self.model.id)  # type: ignore\n+            from copy import deepcopy\n+\n+            reasoning_model = deepcopy(self.model)\n         if reasoning_model is None:\n             log_warning(\"Reasoning error. Reasoning model is None, continuing regular session...\")\n             return\n@@ -4001,7 +4002,9 @@ class Team:\n         reasoning_model: Optional[Model] = self.reasoning_model\n         reasoning_model_provided = reasoning_model is not None\n         if reasoning_model is None and self.model is not None:\n-            reasoning_model = self.model.__class__(id=self.model.id)  # type: ignore\n+            from copy import deepcopy\n+\n+            reasoning_model = deepcopy(self.model)\n         if reasoning_model is None:\n             log_warning(\"Reasoning error. Reasoning model is None, continuing regular session...\")\n             return\n@@ -4336,7 +4339,7 @@ class Team:\n                                 func.tool_hooks = self.tool_hooks\n                             _functions_for_model[name] = func\n                             _tools_for_model.append({\"type\": \"function\", \"function\": func.to_dict()})\n-                            log_debug(f\"Added function {name} from {tool.name}\")\n+                            log_debug(f\"Added tool {name} from {tool.name}\")\n \n                     # Add instructions from the toolkit\n                     if tool.add_instructions and tool.instructions is not None:\n@@ -4355,7 +4358,7 @@ class Team:\n                             tool.tool_hooks = self.tool_hooks\n                         _functions_for_model[tool.name] = tool\n                         _tools_for_model.append({\"type\": \"function\", \"function\": tool.to_dict()})\n-                        log_debug(f\"Added function {tool.name}\")\n+                        log_debug(f\"Added tool {tool.name}\")\n \n                     # Add instructions from the Function\n                     if tool.add_instructions and tool.instructions is not None:\n@@ -4375,9 +4378,9 @@ class Team:\n                             func.tool_hooks = self.tool_hooks\n                         _functions_for_model[func.name] = func\n                         _tools_for_model.append({\"type\": \"function\", \"function\": func.to_dict()})\n-                        log_debug(f\"Added function {func.name}\")\n+                        log_debug(f\"Added tool {func.name}\")\n                     except Exception as e:\n-                        log_warning(f\"Could not add function {tool}: {e}\")\n+                        log_warning(f\"Could not add tool {tool}: {e}\")\n \n             # Set tools on the model\n             model.set_tools(tools=_tools_for_model)\n@@ -6039,11 +6042,9 @@ class Team:\n             if isinstance(self.memory, dict) and \"create_user_memories\" in self.memory:\n                 # Convert dict to TeamMemory\n                 self.memory = TeamMemory(**self.memory)\n-            elif isinstance(self.memory, dict):\n-                # Convert dict to Memory\n-                self.memory = Memory(**self.memory)\n             else:\n-                raise TypeError(f\"Expected memory to be a dict or TeamMemory, but got {type(self.memory)}\")\n+                # Default to base memory\n+                self.memory = Memory()\n \n         if session.memory is not None:\n             if isinstance(self.memory, TeamMemory):\n@@ -6083,13 +6084,13 @@ class Team:\n                     try:\n                         if self.memory.runs is None:\n                             self.memory.runs = {}\n+                        self.memory.runs[session.session_id] = []\n                         for run in session.memory[\"runs\"]:\n-                            session_id = run[\"session_id\"]\n-                            self.memory.runs[session_id] = []\n+                            run_session_id = run[\"session_id\"]\n                             if \"team_id\" in run:\n-                                self.memory.runs[session_id].append(TeamRunResponse.from_dict(run))\n+                                self.memory.runs[run_session_id].append(TeamRunResponse.from_dict(run))\n                             else:\n-                                self.memory.runs[session_id].append(RunResponse.from_dict(run))\n+                                self.memory.runs[run_session_id].append(RunResponse.from_dict(run))\n                     except Exception as e:\n                         log_warning(f\"Failed to load runs from memory: {e}\")\n                 if \"team_context\" in session.memory:\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/apify.py",
            "diff": "diff --git a/libs/agno/agno/tools/apify.py b/libs/agno/agno/tools/apify.py\nindex ac515e46c..d3e4acae6 100644\n--- a/libs/agno/agno/tools/apify.py\n+++ b/libs/agno/agno/tools/apify.py\n@@ -1,8 +1,12 @@\n-from os import getenv\n-from typing import List, Optional\n+import json\n+import os\n+import string\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import requests\n \n from agno.tools import Toolkit\n-from agno.utils.log import log_debug, logger\n+from agno.utils.log import log_info, logger\n \n try:\n     from apify_client import ApifyClient\n@@ -11,112 +15,341 @@ except ImportError:\n \n \n class ApifyTools(Toolkit):\n-    def __init__(\n-        self,\n-        api_key: Optional[str] = None,\n-        website_content_crawler: bool = True,\n-        web_scraper: bool = False,\n-        **kwargs,\n-    ):\n-        super().__init__(name=\"apify_tools\", **kwargs)\n-\n-        self.api_key = api_key or getenv(\"MY_APIFY_TOKEN\")\n-        if not self.api_key:\n-            logger.error(\"No Apify API key provided\")\n-\n-        if website_content_crawler:\n-            self.register(self.website_content_crawler)\n-        if web_scraper:\n-            self.register(self.web_scrapper)\n-\n-    def website_content_crawler(self, urls: List[str], timeout: Optional[int] = 60) -> str:\n+    def __init__(self, actors: Optional[Union[str, List[str]]] = None, apify_api_token: Optional[str] = None):\n+        \"\"\"Initialize ApifyTools with specific Actors.\n+\n+        Args:\n+            actors (Union[str, List[str]]): Single Actor ID as string or list of Actor IDs to register as individual tools\n+            apify_api_token (Optional[str]): Apify API token (defaults to APIFY_API_TOKEN env variable)\n+\n+        Examples:\n+            Configuration Instructions:\n+            1. Install required dependencies:\n+            pip install agno apify-client\n+\n+            2. Set the APIFY_API_TOKEN environment variable:\n+            Add a .env file with APIFY_API_TOKEN=your_apify_api_key\n+\n+            Import necessary components:\n+\n+            from agno.agent import Agent\n+            from agno.tools.apify import ApifyTools\n+\n+            # Create an agent with ApifyTools\n+            agent = Agent(\n+                tools=[\n+                    ApifyTools(actors=[\"apify/rag-web-browser\"])\n+                ],\n+                markdown=True\n+            )\n+\n+            # Ask the agent to process web content\n+            agent.print_response(\"Summarize the content from https://docs.agno.com/introduction\", markdown=True)\n+\n+            # Using multiple actors with the agent\n+            agent = Agent(\n+                tools=[\n+                    ApifyTools(actors=[\n+                        \"apify/rag-web-browser\",\n+                        \"compass/crawler-google-places\"\n+                    ])\n+                ],\n+                show_tool_calls=True\n+            )\n+            agent.print_response(\n+                '''\n+                I'm traveling to Tokyo next month.\n+                1. Research the best time to visit and major attractions\n+                2. Find one good rated sushi restaurant near Shinjuku\n+                Compile a comprehensive travel guide with this information.\n+                ''',\n+                markdown=True\n+            )\n         \"\"\"\n-        Crawls a website using Apify's website-content-crawler actor.\n+        super().__init__(name=\"ApifyTools\")\n+\n+        # Get API token from args or environment\n+        self.apify_api_token = apify_api_token or os.getenv(\"APIFY_API_TOKEN\")\n+        if not self.apify_api_token:\n+            raise ValueError(\"APIFY_API_TOKEN environment variable or apify_api_token parameter must be set\")\n+\n+        self.client = create_apify_client(self.apify_api_token)\n \n-        :param urls: The URLs to crawl.\n-        :param timeout: The timeout for the crawling.\n+        # Register specific Actors if provided\n+        if actors:\n+            actor_list = [actors] if isinstance(actors, str) else actors\n+            for actor_id in actor_list:\n+                self.register_actor(actor_id)\n \n-        :return: The results of the crawling.\n+    def register_actor(self, actor_id: str) -> None:\n+        \"\"\"Register an Apify Actor as a function in the toolkit.\n+\n+        Args:\n+            actor_id (str): ID of the Apify Actor to register (e.g., 'apify/web-scraper')\n         \"\"\"\n-        if self.api_key is None:\n-            return \"No API key provided\"\n+        try:\n+            # Get actor metadata\n+            build = get_actor_latest_build(self.client, actor_id)\n+            tool_name = actor_id_to_tool_name(actor_id)\n+\n+            # Get actor description\n+            actor_description = build.get(\"actorDefinition\", {}).get(\"description\", \"\")\n+            if len(actor_description) > MAX_DESCRIPTION_LEN:\n+                actor_description = actor_description[:MAX_DESCRIPTION_LEN] + \"...(TRUNCATED, TOO LONG)\"\n+\n+            # Get input schema\n+            actor_input = build.get(\"actorDefinition\", {}).get(\"input\")\n+            if not actor_input:\n+                raise ValueError(f\"Input schema not found in the Actor build for Actor: {actor_id}\")\n+\n+            properties, required = prune_actor_input_schema(actor_input)\n+\n+            # Create a wrapper function that calls the Actor\n+            def actor_function(**kwargs) -> str:\n+                \"\"\"Actor function wrapper.\"\"\"\n+                try:\n+                    # Params are nested under 'kwargs' key\n+                    if len(kwargs) == 1 and \"kwargs\" in kwargs:\n+                        kwargs = kwargs[\"kwargs\"]\n+\n+                    log_info(f\"Running Apify Actor {actor_id} with parameters: {kwargs}\")\n+\n+                    # Run the Actor directly with the client\n+                    details = self.client.actor(actor_id=actor_id).call(run_input=kwargs)\n+                    if details is None:\n+                        error_msg = (\n+                            f\"Actor: {actor_id} was not started properly and details about the run were not returned\"\n+                        )\n+                        raise ValueError(error_msg)\n+\n+                    run_id = details.get(\"id\")\n+                    if run_id is None:\n+                        error_msg = f\"Run ID not found in the run details for Actor: {actor_id}\"\n+                        raise ValueError(error_msg)\n+\n+                    # Get the results\n+                    run = self.client.run(run_id=run_id)\n+                    results = run.dataset().list_items(clean=True).items\n+\n+                    return json.dumps(results)\n+\n+                except Exception as e:\n+                    error_msg = f\"Error running Apify Actor {actor_id}: {str(e)}\"\n+                    logger.error(error_msg)\n+                    return json.dumps([{\"error\": error_msg}])\n+\n+            docstring = f\"{actor_description}\\n\\nArgs:\\n\"\n+\n+            for param_name, param_info in properties.items():\n+                param_type = param_info.get(\"type\", \"any\")\n+                param_desc = param_info.get(\"description\", \"No description available\")\n+                required_marker = \"(required)\" if param_name in required else \"(optional)\"\n+                docstring += f\"    {param_name} ({param_type}): {required_marker} {param_desc}\\n\"\n+\n+            docstring += \"\"\"\n+Returns:\n+    str: JSON string containing the Actor's output dataset\n+\"\"\"\n+\n+            # Update function metadata\n+            actor_function.__name__ = tool_name\n+            actor_function.__doc__ = docstring\n+\n+            # Register the function with the toolkit\n+            self.register(actor_function, sanitize_arguments=False)\n+            # Fix params schema\n+            self.functions[tool_name].parameters = props_to_json_schema(properties, required)\n+            log_info(f\"Registered Apify Actor '{actor_id}' as function '{tool_name}'\")\n+\n+        except Exception as e:\n+            logger.error(f\"Failed to register Apify Actor '{actor_id}': {str(e)}\")\n+\n+\n+# Constants\n+MAX_DESCRIPTION_LEN = 350\n+REQUESTS_TIMEOUT_SECS = 300\n+APIFY_API_ENDPOINT_GET_DEFAULT_BUILD = \"https://api.apify.com/v2/acts/{actor_id}/builds/default\"\n+\n+\n+# Utility functions\n+def props_to_json_schema(input_dict, required_fields=None):\n+    schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": required_fields or []}\n+\n+    def infer_array_item_type(prop):\n+        type_map = {\n+            \"string\": \"string\",\n+            \"int\": \"number\",\n+            \"float\": \"number\",\n+            \"dict\": \"object\",\n+            \"list\": \"array\",\n+            \"bool\": \"boolean\",\n+            \"none\": \"null\",\n+        }\n+        if prop.get(\"items\", {}).get(\"type\"):\n+            return prop[\"items\"][\"type\"]\n+        if \"prefill\" in prop and prop[\"prefill\"] and len(prop[\"prefill\"]) > 0:\n+            return type_map.get(type(prop[\"prefill\"][0]).__name__.lower(), \"string\")\n+        if \"default\" in prop and prop[\"default\"] and len(prop[\"default\"]) > 0:\n+            return type_map.get(type(prop[\"default\"][0]).__name__.lower(), \"string\")\n+        return \"string\"  # Fallback for arrays like searchStringsArray\n+\n+    for key, value in input_dict.items():\n+        prop_schema: Dict[str, Any] = {}\n+        prop_type = value.get(\"type\")\n+\n+        if \"enum\" in value:\n+            prop_schema[\"enum\"] = value[\"enum\"]\n+\n+        if \"default\" in value:\n+            prop_schema[\"default\"] = value[\"default\"]\n+        elif \"prefill\" in value:\n+            prop_schema[\"default\"] = value[\"prefill\"]\n+\n+        if \"description\" in value:\n+            prop_schema[\"description\"] = value[\"description\"]\n+\n+        # Handle Apify special types first\n+        if prop_type == \"object\" and value.get(\"editor\") == \"proxy\":\n+            prop_schema[\"type\"] = \"object\"\n+            prop_schema[\"properties\"] = {\n+                \"useApifyProxy\": {\n+                    \"title\": \"Use Apify Proxy\",\n+                    \"type\": \"boolean\",\n+                    \"description\": \"Whether to use Apify Proxy - ALWAYS SET TO TRUE.\",\n+                    \"default\": True,\n+                    \"examples\": [True],\n+                }\n+            }\n+            prop_schema[\"required\"] = [\"useApifyProxy\"]\n+            if \"default\" in value:\n+                prop_schema[\"default\"] = value[\"default\"]\n+\n+        elif prop_type == \"array\" and value.get(\"editor\") == \"requestListSources\":\n+            prop_schema[\"type\"] = \"array\"\n+            prop_schema[\"items\"] = {\n+                \"type\": \"object\",\n+                \"title\": \"Request list source\",\n+                \"description\": \"Request list source\",\n+                \"properties\": {\n+                    \"url\": {\"title\": \"URL\", \"type\": \"string\", \"description\": \"URL of the request list source\"}\n+                },\n+            }\n \n-        if urls is None:\n-            return \"No URLs provided\"\n+        elif prop_type == \"array\":\n+            prop_schema[\"type\"] = \"array\"\n+            prop_schema[\"items\"] = {\n+                \"type\": infer_array_item_type(value),\n+                \"title\": value.get(\"title\", \"Item\"),\n+                \"description\": \"Item\",\n+            }\n \n-        client = ApifyClient(self.api_key)\n+        elif prop_type == \"object\":\n+            prop_schema[\"type\"] = \"object\"\n+            if \"default\" in value:\n+                prop_schema[\"default\"] = value[\"default\"]\n+                prop_schema[\"properties\"] = {}\n+                for k, v in value.get(\"properties\", value[\"default\"]).items():\n+                    prop_type = v[\"type\"] if isinstance(v, dict) else type(v).__name__.lower()\n+                    if prop_type == \"bool\":\n+                        prop_type = \"boolean\"\n+                    prop_schema[\"properties\"][k] = {\"type\": prop_type}\n \n-        log_debug(f\"Crawling URLs: {urls}\")\n+        else:\n+            prop_schema[\"type\"] = prop_type\n \n-        formatted_urls = [{\"url\": url} for url in urls]\n+        schema[\"properties\"][key] = prop_schema\n \n-        run_input = {\"startUrls\": formatted_urls}\n+    return schema\n \n-        run = client.actor(\"apify/website-content-crawler\").call(run_input=run_input, timeout_secs=timeout)\n \n-        results: str = \"\"\n+def create_apify_client(token: str) -> ApifyClient:\n+    \"\"\"Create an Apify client instance with a custom user-agent.\n \n-        for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n-            results += \"Results for URL: \" + item.get(\"url\") + \"\\n\"\n-            results += item.get(\"text\") + \"\\n\"\n+    Args:\n+        token (str): API token\n \n-        return results\n+    Returns:\n+        ApifyClient: Apify client instance\n \n-    def web_scrapper(self, urls: List[str], timeout: Optional[int] = 60) -> str:\n-        \"\"\"\n-        Scrapes a website using Apify's web-scraper actor.\n+    Raises:\n+        ValueError: If the API token is not provided\n+    \"\"\"\n+    if not token:\n+        raise ValueError(\"API token is required to create an Apify client.\")\n \n-        :param urls: The URLs to scrape.\n-        :param timeout: The timeout for the scraping.\n+    client = ApifyClient(token)\n+    if http_client := getattr(client.http_client, \"httpx_client\", None):\n+        http_client.headers[\"user-agent\"] += \"; Origin/agno\"\n+    return client\n \n-        :return: The results of the scraping.\n-        \"\"\"\n-        if self.api_key is None:\n-            return \"No API key provided\"\n \n-        if urls is None:\n-            return \"No URLs provided\"\n+def actor_id_to_tool_name(actor_id: str) -> str:\n+    \"\"\"Turn actor_id into a valid tool name.\n \n-        client = ApifyClient(self.api_key)\n+    Args:\n+        actor_id (str): Actor ID from Apify store\n \n-        log_debug(f\"Scrapping URLs: {urls}\")\n+    Returns:\n+        str: A valid tool name\n+    \"\"\"\n+    valid_chars = string.ascii_letters + string.digits + \"_-\"\n+    return \"apify_actor_\" + \"\".join(char if char in valid_chars else \"_\" for char in actor_id)\n \n-        formatted_urls = [{\"url\": url} for url in urls]\n \n-        page_function_string = \"\"\"\n-            async function pageFunction(context) {\n-                const $ = context.jQuery;\n-                const pageTitle = $('title').first().text();\n-                const h1 = $('h1').first().text();\n-                const first_h2 = $('h2').first().text();\n-                const random_text_from_the_page = $('p').first().text();\n+def get_actor_latest_build(apify_client: ApifyClient, actor_id: str) -> Dict[str, Any]:\n+    \"\"\"Get the latest build of an Actor from the default build tag.\n \n-                context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n+    Args:\n+        apify_client (ApifyClient): An instance of the ApifyClient class\n+        actor_id (str): Actor name from Apify store to run\n \n-                return {\n-                    url: context.request.url,\n-                    pageTitle,\n-                    h1,\n-                    first_h2,\n-                    random_text_from_the_page\n-                };\n-            }\n-        \"\"\"\n+    Returns:\n+        Dict[str, Any]: The latest build of the Actor\n \n-        run_input = {\n-            \"pageFunction\": page_function_string,\n-            \"startUrls\": formatted_urls,\n-        }\n+    Raises:\n+        ValueError: If the Actor is not found or the build data is not found\n+        TypeError: If the build is not a dictionary\n+    \"\"\"\n+    if not (actor := apify_client.actor(actor_id).get()):\n+        raise ValueError(f\"Actor {actor_id} not found.\")\n+\n+    if not (actor_obj_id := actor.get(\"id\")):\n+        raise ValueError(f\"Failed to get the Actor object ID for {actor_id}.\")\n+\n+    url = APIFY_API_ENDPOINT_GET_DEFAULT_BUILD.format(actor_id=actor_obj_id)\n+    response = requests.request(\"GET\", url, timeout=REQUESTS_TIMEOUT_SECS)\n+\n+    build = response.json()\n+    if not isinstance(build, dict):\n+        raise TypeError(f\"Failed to get the latest build of the Actor {actor_id}.\")\n+\n+    if (data := build.get(\"data\")) is None:\n+        raise ValueError(f\"Failed to get the latest build data of the Actor {actor_id}.\")\n+\n+    return data\n+\n+\n+def prune_actor_input_schema(input_schema: Dict[str, Any]) -> Tuple[Dict[str, Any], List[str]]:\n+    \"\"\"Get the input schema from the Actor build and trim descriptions.\n \n-        run = client.actor(\"apify/web-scraper\").call(run_input=run_input, timeout_secs=timeout)\n+    Args:\n+        input_schema (Dict[str, Any]): The input schema from the Actor build\n \n-        results: str = \"\"\n+    Returns:\n+        Tuple[Dict[str, Any], List[str]]: A tuple containing the pruned properties and required fields\n+    \"\"\"\n+    properties = input_schema.get(\"properties\", {})\n+    required = input_schema.get(\"required\", [])\n \n-        for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n-            results += \"Results for URL: \" + item.get(\"url\") + \"\\n\"\n-            results += item.get(\"pageTitle\") + \"\\n\"\n-            results += item.get(\"h1\") + \"\\n\"\n-            results += item.get(\"first_h2\") + \"\\n\"\n-            results += item.get(\"random_text_from_the_page\") + \"\\n\"\n+    properties_out: Dict[str, Any] = {}\n+    for item, meta in properties.items():\n+        properties_out[item] = {}\n+        if desc := meta.get(\"description\"):\n+            properties_out[item][\"description\"] = (\n+                desc[:MAX_DESCRIPTION_LEN] + \"...\" if len(desc) > MAX_DESCRIPTION_LEN else desc\n+            )\n+        for key_name in (\"type\", \"default\", \"prefill\", \"enum\"):\n+            if value := meta.get(key_name):\n+                properties_out[item][key_name] = value\n \n-        return results\n+    return properties_out, required\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/function.py",
            "diff": "diff --git a/libs/agno/agno/tools/function.py b/libs/agno/agno/tools/function.py\nindex 4407903a9..ec7f6186a 100644\n--- a/libs/agno/agno/tools/function.py\n+++ b/libs/agno/agno/tools/function.py\n@@ -169,6 +169,8 @@ class Function(BaseModel):\n         from agno.utils.json_schema import get_json_schema\n \n         if self.skip_entrypoint_processing:\n+            if strict:\n+                self.process_schema_for_strict()\n             return\n \n         if self.entrypoint is None:\n@@ -260,6 +262,10 @@ class Function(BaseModel):\n         except Exception as e:\n             log_warning(f\"Failed to add validate decorator to entrypoint: {e}\")\n \n+    def process_schema_for_strict(self):\n+        self.parameters[\"additionalProperties\"] = False\n+        self.parameters[\"required\"] = [name for name in self.parameters[\"properties\"] if name not in [\"agent\", \"team\"]]\n+\n     def get_type_name(self, t: Type[T]):\n         name = str(t)\n         if \"list\" in name or \"dict\" in name:\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/mcp.py",
            "diff": "diff --git a/libs/agno/agno/tools/mcp.py b/libs/agno/agno/tools/mcp.py\nindex 160a4b949..d8c1dd6f3 100644\n--- a/libs/agno/agno/tools/mcp.py\n+++ b/libs/agno/agno/tools/mcp.py\n@@ -69,7 +69,12 @@ class MCPTools(Toolkit):\n             exclude_tools: Optional list of tool names to exclude (if None, excludes none)\n             transport: The transport protocol to use, either \"stdio\" or \"sse\"\n         \"\"\"\n-        super().__init__(name=\"MCPToolkit\", include_tools=include_tools, exclude_tools=exclude_tools, **kwargs)\n+        super().__init__(name=\"MCPTools\", **kwargs)\n+\n+        # Set these after `__init__` to bypass the `_check_tools_filters`\n+        # beacuse tools are not available until `initialize()` is called.\n+        self.include_tools = include_tools\n+        self.exclude_tools = exclude_tools\n \n         if session is None and server_params is None:\n             if transport == \"sse\" and url is None:\n@@ -179,6 +184,12 @@ class MCPTools(Toolkit):\n             # Get the list of tools from the MCP server\n             available_tools = await self.session.list_tools()\n \n+            self._check_tools_filters(\n+                available_tools=[tool.name for tool in available_tools.tools],\n+                include_tools=self.include_tools,\n+                exclude_tools=self.exclude_tools,\n+            )\n+\n             # Filter tools based on include/exclude lists\n             filtered_tools = []\n             for tool in available_tools.tools:\n@@ -192,7 +203,6 @@ class MCPTools(Toolkit):\n                 try:\n                     # Get an entrypoint for the tool\n                     entrypoint = get_entrypoint_for_tool(tool, self.session)\n-\n                     # Create a Function for the tool\n                     f = Function(\n                         name=tool.name,\n@@ -253,7 +263,12 @@ class MultiMCPTools(Toolkit):\n             include_tools: Optional list of tool names to include (if None, includes all).\n             exclude_tools: Optional list of tool names to exclude (if None, excludes none).\n         \"\"\"\n-        super().__init__(name=\"MultiMCPToolkit\", include_tools=include_tools, exclude_tools=exclude_tools, **kwargs)\n+        super().__init__(name=\"MultiMCPTools\", **kwargs)\n+\n+        # Set these after `__init__` to bypass the `_check_tools_filters`\n+        # beacuse tools are not available until `initialize()` is called.\n+        self.include_tools = include_tools\n+        self.exclude_tools = exclude_tools\n \n         if server_params_list is None and commands is None and urls is None:\n             raise ValueError(\"Either server_params_list or commands or urls must be provided\")\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/models/gemini.py",
            "diff": "diff --git a/libs/agno/agno/tools/models/gemini.py b/libs/agno/agno/tools/models/gemini.py\nindex 0b6016f8b..841a6261b 100644\n--- a/libs/agno/agno/tools/models/gemini.py\n+++ b/libs/agno/agno/tools/models/gemini.py\n@@ -1,44 +1,67 @@\n import base64\n+import time\n from os import getenv\n-from typing import Optional\n+from typing import Any, Optional\n from uuid import uuid4\n \n from agno.agent import Agent\n-from agno.media import ImageArtifact\n+from agno.media import ImageArtifact, VideoArtifact\n from agno.tools import Toolkit\n-from agno.utils.log import log_debug, log_error\n+from agno.utils.log import log_debug, log_error, log_info\n \n try:\n     from google.genai import Client\n+    from google.genai.types import GenerateImagesResponse, GenerateVideosOperation\n except (ModuleNotFoundError, ImportError):\n     raise ImportError(\"`google-genai` not installed. Please install using `pip install google-genai`\")\n \n \n class GeminiTools(Toolkit):\n-    \"\"\"Tools for interacting with Google Gemini API (including Imagen for images)\"\"\"\n+    \"\"\"Tools for interacting with Google Gemini API\"\"\"\n \n     def __init__(\n         self,\n         api_key: Optional[str] = None,\n+        vertexai: bool = False,\n+        project_id: Optional[str] = None,\n+        location: Optional[str] = None,\n         image_generation_model: str = \"imagen-3.0-generate-002\",\n+        video_generation_model: str = \"veo-2.0-generate-001\",\n         **kwargs,\n     ):\n-        super().__init__(name=\"gemini_tools\", tools=[self.generate_image], **kwargs)\n+        super().__init__(name=\"gemini_tools\", tools=[self.generate_image, self.generate_video], **kwargs)\n \n+        # Set mode and credentials: use only provided vertexai parameter\n+        self.vertexai = vertexai\n+        self.project_id = project_id\n+        self.location = location\n+\n+        # Load API key from argument or environment\n         self.api_key = api_key or getenv(\"GOOGLE_API_KEY\")\n-        if not self.api_key:\n-            raise ValueError(\n-                \"GOOGLE_API_KEY not set. Please provide api_key or set the GOOGLE_API_KEY environment variable.\"\n-            )\n+        if not self.vertexai and not self.api_key:\n+            log_error(\"GOOGLE_API_KEY not set. Please set the GOOGLE_API_KEY environment variable.\")\n+            raise ValueError(\"GOOGLE_API_KEY not set. Please provide api_key or set the environment variable.\")\n+\n+        # Prepare client parameters\n+        client_params: dict[str, Any] = {}\n+        if self.vertexai:\n+            log_info(\"Using Vertex AI API\")\n+            client_params[\"vertexai\"] = True\n+            client_params[\"project\"] = self.project_id or getenv(\"GOOGLE_CLOUD_PROJECT\")\n+            client_params[\"location\"] = self.location or getenv(\"GOOGLE_CLOUD_LOCATION\")\n+        else:\n+            log_info(\"Using Gemini API\")\n+            client_params[\"api_key\"] = self.api_key\n \n         try:\n-            self.client = Client(api_key=self.api_key)\n+            self.client = Client(**client_params)\n             log_debug(\"Google GenAI Client created successfully.\")\n         except Exception as e:\n             log_error(f\"Failed to create Google GenAI Client: {e}\", exc_info=True)\n             raise ValueError(f\"Failed to create Google GenAI Client. Error: {e}\")\n \n         self.image_model = image_generation_model\n+        self.video_model = video_generation_model\n \n     def generate_image(\n         self,\n@@ -54,40 +77,89 @@ class GeminiTools(Toolkit):\n         \"\"\"\n \n         try:\n-            response = self.client.models.generate_images(\n+            response: GenerateImagesResponse = self.client.models.generate_images(\n                 model=self.image_model,\n                 prompt=prompt,\n             )\n \n             log_debug(\"DEBUG: Raw Gemini API response\")\n \n-            image_bytes = None\n-            actual_mime_type = \"image/png\"\n+            # Extract image bytes\n+            image_bytes = response.generated_images[0].image.image_bytes\n+            for generated_image in response.generated_images:\n+                image_bytes = generated_image.image.image_bytes\n+                if not image_bytes:\n+                    log_error(\"No valid image data extracted.\")\n+                    return \"Failed to generate image: No valid image data extracted.\"\n+                base64_encoded_image_bytes = base64.b64encode(image_bytes)\n+                actual_mime_type = \"image/png\"\n+\n+                media_id = str(uuid4())\n+                agent.add_image(\n+                    ImageArtifact(\n+                        id=media_id,\n+                        content=base64_encoded_image_bytes,\n+                        original_prompt=prompt,\n+                        mime_type=actual_mime_type,\n+                    )\n+                )\n+                log_debug(f\"Successfully generated image {media_id} with model {self.image_model}\")\n+            return \"Image generated successfully\"\n \n-            if response.generated_images and response.generated_images[0].image.image_bytes:\n-                image_bytes = response.generated_images[0].image.image_bytes\n-            else:\n-                log_error(\"No image data found in the response structure.\")\n-                return \"Failed to generate image: No valid image data extracted.\"\n+        except Exception as e:\n+            log_error(f\"Failed to generate image: Client or method not available ({e})\")\n+            return f\"Failed to generate image: Client or method not available ({e})\"\n \n-            if image_bytes is None:\n-                log_error(\"image_bytes is None after extraction.\")\n-                return \"Failed to generate image: No valid image data extracted.\"\n+    def generate_video(\n+        self,\n+        agent: Agent,\n+        prompt: str,\n+    ) -> str:\n+        \"\"\"Generate a video based on a text prompt.\n+        Args:\n+            prompt (str): The text prompt to generate the video from.\n+        Returns:\n+            str: A message indicating success or failure.\n+        \"\"\"\n+        # Video generation requires Vertex AI mode.\n+        if not self.vertexai:\n+            log_error(\"Video generation requires Vertex AI mode. Please enable Vertex AI mode.\")\n+            return (\n+                \"Video generation requires Vertex AI mode. \"\n+                \"Please set `vertexai=True` or environment variable `GOOGLE_GENAI_USE_VERTEXAI=true`.\"\n+            )\n \n-            base64_encoded_image_bytes = base64.b64encode(image_bytes)\n+        from google.genai.types import GenerateVideosConfig\n \n-            media_id = str(uuid4())\n-            agent.add_image(\n-                ImageArtifact(\n-                    id=media_id,\n-                    content=base64_encoded_image_bytes,\n-                    original_prompt=prompt,\n-                    mime_type=actual_mime_type,\n-                )\n+        try:\n+            operation: GenerateVideosOperation = self.client.models.generate_videos(\n+                model=self.video_model,\n+                prompt=prompt,\n+                config=GenerateVideosConfig(\n+                    enhance_prompt=True,\n+                ),\n             )\n-            log_debug(f\"Successfully generated image {media_id} with model {self.image_model}\")\n-            return f\"Image generated successfully with ID: {media_id}\"\n \n+            while not operation.done:\n+                time.sleep(5)\n+                operation = self.client.operations.get(operation=operation)\n+\n+            for video in operation.result.generated_videos:\n+                generated_video = video.video\n+\n+                media_id = str(uuid4())\n+                encoded_video = base64.b64encode(generated_video.video_bytes).decode(\"utf-8\")\n+\n+                agent.add_video(\n+                    VideoArtifact(\n+                        id=media_id,\n+                        content=encoded_video,\n+                        original_prompt=prompt,\n+                        mime_type=generated_video.mime_type,\n+                    )\n+                )\n+                log_debug(f\"Successfully generated video {media_id} with model {self.video_model}\")\n+            return \"Video generated successfully\"\n         except Exception as e:\n-            log_error(f\"Failed to generate image: Client or method not available ({e})\")\n-            return f\"Failed to generate image: Client or method not available ({e})\"\n+            log_error(f\"Failed to generate video: {e}\")\n+            return f\"Failed to generate video: {e}\"\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/openai.py",
            "diff": "diff --git a/libs/agno/agno/tools/openai.py b/libs/agno/agno/tools/openai.py\nindex 3355fc7a7..2fc54035a 100644\n--- a/libs/agno/agno/tools/openai.py\n+++ b/libs/agno/agno/tools/openai.py\n@@ -27,6 +27,7 @@ class OpenAITools(Toolkit):\n         enable_transcription: bool = True,\n         enable_image_generation: bool = True,\n         enable_speech_generation: bool = True,\n+        transcription_model: str = \"whisper-1\",\n         text_to_speech_voice: OpenAIVoice = \"alloy\",\n         text_to_speech_model: OpenAITTSModel = \"tts-1\",\n         text_to_speech_format: OpenAITTSFormat = \"mp3\",\n@@ -39,6 +40,7 @@ class OpenAITools(Toolkit):\n         if not self.api_key:\n             raise ValueError(\"OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.\")\n \n+        self.transcription_model = transcription_model\n         # Store TTS defaults\n         self.tts_voice = text_to_speech_voice\n         self.tts_model = text_to_speech_model\n@@ -56,15 +58,16 @@ class OpenAITools(Toolkit):\n         \"\"\"Transcribe audio file using OpenAI's Whisper API\n         Args:\n             audio_path: Path to the audio file\n-        Returns:\n-            str: Transcribed text\n         \"\"\"\n         log_debug(f\"Transcribing audio from {audio_path}\")\n         try:\n-            with open(audio_path, \"rb\") as audio_file:\n-                transcript = OpenAIClient().audio.transcriptions.create(\n-                    model=\"whisper-1\", file=audio_file, response_format=\"srt\"\n-                )\n+            audio_file = open(audio_path, \"rb\")\n+\n+            transcript = OpenAIClient().audio.transcriptions.create(\n+                model=self.transcription_model,\n+                file=audio_file,\n+                response_format=\"text\",\n+            )\n         except Exception as e:  # type: ignore[return]\n             log_error(f\"Failed to transcribe audio: {str(e)}\")\n             return f\"Failed to transcribe audio: {str(e)}\"\n@@ -80,8 +83,6 @@ class OpenAITools(Toolkit):\n         \"\"\"Generate images based on a text prompt.\n         Args:\n             prompt (str): The text prompt to generate the image from.\n-        Returns:\n-            str: Return the result of the model.\n         \"\"\"\n         try:\n             response = OpenAIClient().images.generate(\n@@ -97,8 +98,6 @@ class OpenAITools(Toolkit):\n                     ImageArtifact(\n                         id=media_id,\n                         url=image_url,\n-                        prompt=prompt,\n-                        model=self.image_model,\n                     )\n                 )\n                 return f\"Image generated successfully: {image_url}\"\n@@ -118,8 +117,6 @@ class OpenAITools(Toolkit):\n         \"\"\"Generate speech from text using OpenAI's Text-to-Speech API.\n         Args:\n             text_input (str): The text to synthesize into speech.\n-        Returns:\n-            str: Return the result of the model.\n         \"\"\"\n         try:\n             import base64\n@@ -143,9 +140,6 @@ class OpenAITools(Toolkit):\n                 AudioArtifact(\n                     id=media_id,\n                     base64_audio=base64_encoded_audio,\n-                    format=self.tts_format,\n-                    model=self.tts_model,\n-                    voice=self.tts_voice,\n                 )\n             )\n             return f\"Speech generated successfully with ID: {media_id}\"\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/sql.py",
            "diff": "diff --git a/libs/agno/agno/tools/sql.py b/libs/agno/agno/tools/sql.py\nindex a02c4aa2e..325a8bd1d 100644\n--- a/libs/agno/agno/tools/sql.py\n+++ b/libs/agno/agno/tools/sql.py\n@@ -49,6 +49,8 @@ class SQLTools(Toolkit):\n         self.db_engine: Engine = _engine\n         self.Session: sessionmaker[Session] = sessionmaker(bind=self.db_engine)\n \n+        self.schema = schema\n+\n         # Tables this toolkit can access\n         self.tables: Optional[Dict[str, Any]] = tables\n \n@@ -71,7 +73,11 @@ class SQLTools(Toolkit):\n \n         try:\n             log_debug(\"listing tables in the database\")\n-            table_names = inspect(self.db_engine).get_table_names()\n+            inspector = inspect(self.db_engine)\n+            if self.schema:\n+                table_names = inspector.get_table_names(schema=self.schema)\n+            else:\n+                table_names = inspector.get_table_names()\n             log_debug(f\"table_names: {table_names}\")\n             return json.dumps(table_names)\n         except Exception as e:\n@@ -90,9 +96,14 @@ class SQLTools(Toolkit):\n \n         try:\n             log_debug(f\"Describing table: {table_name}\")\n-            table_names = inspect(self.db_engine)\n-            table_schema = table_names.get_columns(table_name)\n-            return json.dumps([str(column) for column in table_schema])\n+            inspector = inspect(self.db_engine)\n+            table_schema = inspector.get_columns(table_name, schema=self.schema)\n+            return json.dumps(\n+                [\n+                    {\"name\": column[\"name\"], \"type\": str(column[\"type\"]), \"nullable\": column[\"nullable\"]}\n+                    for column in table_schema\n+                ]\n+            )\n         except Exception as e:\n             logger.error(f\"Error getting table schema: {e}\")\n             return f\"Error getting table schema: {e}\"\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/tools/toolkit.py",
            "diff": "diff --git a/libs/agno/agno/tools/toolkit.py b/libs/agno/agno/tools/toolkit.py\nindex 8bd3aef04..b629940b3 100644\n--- a/libs/agno/agno/tools/toolkit.py\n+++ b/libs/agno/agno/tools/toolkit.py\n@@ -39,30 +39,39 @@ class Toolkit:\n         self.instructions: Optional[str] = instructions\n         self.add_instructions: bool = add_instructions\n \n+        self._check_tools_filters(\n+            available_tools=[tool.__name__ for tool in tools], include_tools=include_tools, exclude_tools=exclude_tools\n+        )\n+\n         self.include_tools = include_tools\n         self.exclude_tools = exclude_tools\n \n-        if include_tools or exclude_tools:\n-            available_tools = {tool.__name__ for tool in tools}\n+        self.cache_results: bool = cache_results\n+        self.cache_ttl: int = cache_ttl\n+        self.cache_dir: Optional[str] = cache_dir\n \n+        # Automatically register all methods if auto_register is True\n+        if auto_register and self.tools:\n+            self._register_tools()\n+\n+    def _check_tools_filters(\n+        self,\n+        available_tools: List[str],\n+        include_tools: Optional[list[str]] = None,\n+        exclude_tools: Optional[list[str]] = None,\n+    ) -> None:\n+        \"\"\"Check if `include_tools` and `exclude_tools` are valid\"\"\"\n+        if include_tools or exclude_tools:\n             if include_tools:\n-                missing_includes = set(include_tools) - available_tools\n+                missing_includes = set(include_tools) - set(available_tools)\n                 if missing_includes:\n                     raise ValueError(f\"Included tool(s) not present in the toolkit: {', '.join(missing_includes)}\")\n \n             if exclude_tools:\n-                missing_excludes = set(exclude_tools) - available_tools\n+                missing_excludes = set(exclude_tools) - set(available_tools)\n                 if missing_excludes:\n                     raise ValueError(f\"Excluded tool(s) not present in the toolkit: {', '.join(missing_excludes)}\")\n \n-        self.cache_results: bool = cache_results\n-        self.cache_ttl: int = cache_ttl\n-        self.cache_dir: Optional[str] = cache_dir\n-\n-        # Automatically register all methods if auto_register is True\n-        if auto_register and self.tools:\n-            self._register_tools()\n-\n     def _register_tools(self) -> None:\n         \"\"\"Register all tools.\"\"\"\n         for tool in self.tools:\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/utils/mcp.py",
            "diff": "diff --git a/libs/agno/agno/utils/mcp.py b/libs/agno/agno/utils/mcp.py\nindex b25c2fb66..2b72916a9 100644\n--- a/libs/agno/agno/utils/mcp.py\n+++ b/libs/agno/agno/utils/mcp.py\n@@ -1,7 +1,6 @@\n from functools import partial\n from uuid import uuid4\n \n-from agno.agent import Agent\n from agno.utils.log import log_debug, log_exception\n \n try:\n@@ -26,6 +25,7 @@ def get_entrypoint_for_tool(tool: MCPTool, session: ClientSession):\n     Returns:\n         Callable: The entrypoint function for the tool\n     \"\"\"\n+    from agno.agent import Agent\n \n     async def call_tool(agent: Agent, tool_name: str, **kwargs) -> str:\n         try:\n@@ -63,4 +63,4 @@ def get_entrypoint_for_tool(tool: MCPTool, session: ClientSession):\n             log_exception(f\"Failed to call MCP tool '{tool_name}': {e}\")\n             return f\"Error: {e}\"\n \n-    return partial(call_tool, tool_name=tool.name, tool_description=tool.description)\n+    return partial(call_tool, tool_name=tool.name)\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/utils/models/llama.py",
            "diff": "diff --git a/libs/agno/agno/utils/models/llama.py b/libs/agno/agno/utils/models/llama.py\nnew file mode 100644\nindex 000000000..df29f5b02\n--- /dev/null\n+++ b/libs/agno/agno/utils/models/llama.py\n@@ -0,0 +1,67 @@\n+from typing import Any, Dict\n+\n+from agno.agent import Message\n+from agno.utils.log import log_warning\n+from agno.utils.openai import process_image\n+\n+\n+def format_message(message: Message, openai_like: bool = False) -> Dict[str, Any]:\n+    \"\"\"\n+    Format a message into the format expected by Llama API.\n+\n+    Args:\n+        message (Message): The message to format.\n+        openai_like (bool): Whether to format the message as an OpenAI-like message.\n+\n+    Returns:\n+        Dict[str, Any]: The formatted message.\n+    \"\"\"\n+    message_dict: Dict[str, Any] = {\n+        \"role\": message.role,\n+        \"content\": [{\"type\": \"text\", \"text\": message.content or \" \"}],\n+        \"name\": message.name,\n+        \"tool_call_id\": message.tool_call_id,\n+        \"tool_calls\": message.tool_calls,\n+    }\n+    message_dict = {k: v for k, v in message_dict.items() if v is not None}\n+\n+    if message.images is not None and len(message.images) > 0:\n+        for image in message.images:\n+            image_payload = process_image(image)\n+            if image_payload:\n+                message_dict[\"content\"].append(image_payload)\n+\n+    if message.videos is not None and len(message.videos) > 0:\n+        log_warning(\"Video input is currently unsupported.\")\n+\n+    if message.audio is not None and len(message.audio) > 0:\n+        log_warning(\"Audio input is currently unsupported.\")\n+\n+    if message.role == \"tool\":\n+        message_dict = {\n+            \"role\": \"tool\",\n+            \"tool_call_id\": message.tool_call_id,\n+            \"content\": message.content,\n+        }\n+\n+    if message.role == \"assistant\":\n+        if message.tool_calls is not None and len(message.tool_calls) > 0:\n+            message_dict = {\n+                \"content\": {\n+                    \"type\": \"text\",\n+                    \"text\": message.content or \" \",\n+                },\n+                \"role\": \"assistant\",\n+                \"tool_calls\": message.tool_calls,\n+                \"stop_reason\": \"tool_calls\",\n+            }\n+        else:\n+            text_content = {\"type\": \"text\", \"text\": message.content or \" \"}\n+\n+            message_dict = {\n+                \"role\": \"assistant\",\n+                \"content\": [text_content] if openai_like else text_content,\n+                \"stop_reason\": \"stop\",\n+            }\n+\n+    return message_dict\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/utils/openai.py",
            "diff": "diff --git a/libs/agno/agno/utils/openai.py b/libs/agno/agno/utils/openai.py\nindex b52871b8a..77d067124 100644\n--- a/libs/agno/agno/utils/openai.py\n+++ b/libs/agno/agno/utils/openai.py\n@@ -3,7 +3,7 @@ import mimetypes\n from pathlib import Path\n from typing import Any, Dict, List, Optional, Sequence, Union\n \n-from agno.media import Audio, Image\n+from agno.media import Audio, File, Image\n from agno.utils.log import log_error, log_warning\n \n # Ensure .webp is recognized\n@@ -130,7 +130,7 @@ def _process_image_url(image_url: str) -> Dict[str, Any]:\n         raise ValueError(\"Image URL must start with 'data:image' or 'http(s)://'.\")\n \n \n-def _process_image(image: Image) -> Optional[Dict[str, Any]]:\n+def process_image(image: Image) -> Optional[Dict[str, Any]]:\n     \"\"\"Process an image based on the format.\"\"\"\n     image_payload: Optional[Dict[str, Any]] = None  # Initialize\n     try:\n@@ -185,7 +185,7 @@ def images_to_message(images: Sequence[Image]) -> List[Dict[str, Any]]:\n     # Add images to the message content\n     for image in images:\n         try:\n-            image_data = _process_image(image)\n+            image_data = process_image(image)\n             if image_data:\n                 image_messages.append(image_data)\n         except Exception as e:\n@@ -193,3 +193,50 @@ def images_to_message(images: Sequence[Image]) -> List[Dict[str, Any]]:\n             continue\n \n     return image_messages\n+\n+\n+def _format_file_for_message(file: File) -> Optional[Dict[str, Any]]:\n+    \"\"\"\n+    Add a document url, base64 encoded content or OpenAI file to a message.\n+    \"\"\"\n+    import base64\n+    import mimetypes\n+    from pathlib import Path\n+\n+    # Case 1: Document is a URL\n+    if file.url is not None:\n+        from urllib.parse import urlparse\n+\n+        result = file.file_url_content\n+        if not result:\n+            log_error(f\"Failed to fetch file from URL: {file.url}\")\n+            return None\n+        content_bytes, mime_type = result\n+        name = Path(urlparse(file.url).path).name or \"file\"\n+        _mime = mime_type or file.mime_type or mimetypes.guess_type(name)[0] or \"application/pdf\"\n+        _encoded = base64.b64encode(content_bytes).decode(\"utf-8\")\n+        _data_url = f\"data:{_mime};base64,{_encoded}\"\n+        return {\"type\": \"file\", \"file\": {\"filename\": name, \"file_data\": _data_url}}\n+\n+    # Case 2: Document is a local file path\n+    if file.filepath is not None:\n+        path = Path(file.filepath)\n+        if not path.is_file():\n+            log_error(f\"File not found: {path}\")\n+            return None\n+        data = path.read_bytes()\n+\n+        _mime = file.mime_type or mimetypes.guess_type(path.name)[0] or \"application/pdf\"\n+        _encoded = base64.b64encode(data).decode(\"utf-8\")\n+        _data_url = f\"data:{_mime};base64,{_encoded}\"\n+        return {\"type\": \"file\", \"file\": {\"filename\": path.name, \"file_data\": _data_url}}\n+\n+    # Case 3: Document is bytes content\n+    if file.content is not None:\n+        name = getattr(file, \"filename\", \"file\")\n+        _mime = file.mime_type or mimetypes.guess_type(name)[0] or \"application/pdf\"\n+        _encoded = base64.b64encode(file.content).decode(\"utf-8\")\n+        _data_url = f\"data:{_mime};base64,{_encoded}\"\n+        return {\"type\": \"file\", \"file\": {\"filename\": name, \"file_data\": _data_url}}\n+\n+    return None\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/vectordb/pineconedb/pineconedb.py",
            "diff": "diff --git a/libs/agno/agno/vectordb/pineconedb/pineconedb.py b/libs/agno/agno/vectordb/pineconedb/pineconedb.py\nindex 5c6fbc473..17bdb03a8 100644\n--- a/libs/agno/agno/vectordb/pineconedb/pineconedb.py\n+++ b/libs/agno/agno/vectordb/pineconedb/pineconedb.py\n@@ -39,6 +39,7 @@ class PineconeDb(VectorDb):\n         metric (Optional[str], optional): The metric used for similarity search. Defaults to \"cosine\".\n         additional_headers (Optional[Dict[str, str]], optional): Additional headers to pass to the Pinecone client. Defaults to {}.\n         pool_threads (Optional[int], optional): The number of threads to use for the Pinecone client. Defaults to 1.\n+        namespace: (Optional[str], optional): The namespace partition within the index that will be used. Defaults to None.\n         timeout (Optional[int], optional): The timeout for Pinecone operations. Defaults to None.\n         index_api (Optional[Any], optional): The Index API object. Defaults to None.\n         api_key (Optional[str], optional): The Pinecone API key. Defaults to None.\n@@ -204,7 +205,7 @@ class PineconeDb(VectorDb):\n             bool: True if the document exists, False otherwise.\n \n         \"\"\"\n-        response = self.index.fetch(ids=[document.id])\n+        response = self.index.fetch(ids=[document.id], namespace=self.namespace)\n         return len(response.vectors) > 0\n \n     async def async_doc_exists(self, document: Document) -> bool:\n@@ -265,7 +266,7 @@ class PineconeDb(VectorDb):\n \n         self.index.upsert(\n             vectors=vectors,\n-            namespace=namespace,\n+            namespace=namespace or self.namespace,\n             batch_size=batch_size,\n             show_progress=show_progress,\n         )\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/agno/workflow/workflow.py",
            "diff": "diff --git a/libs/agno/agno/workflow/workflow.py b/libs/agno/agno/workflow/workflow.py\nindex c7c70600d..c1512f746 100644\n--- a/libs/agno/agno/workflow/workflow.py\n+++ b/libs/agno/agno/workflow/workflow.py\n@@ -18,7 +18,7 @@ from agno.run.response import RunEvent, RunResponse  # noqa: F401\n from agno.storage.base import Storage\n from agno.storage.session.workflow import WorkflowSession\n from agno.utils.common import nested_model_dump\n-from agno.utils.log import log_debug, logger, set_log_level_to_debug, set_log_level_to_info\n+from agno.utils.log import log_debug, log_warning, logger, set_log_level_to_debug, set_log_level_to_info\n from agno.utils.merge_dict import merge_dictionaries\n \n \n@@ -358,11 +358,29 @@ class Workflow:\n         self.memory = cast(WorkflowMemory, self.memory)\n         self.session_id = cast(str, self.session_id)\n         self.workflow_id = cast(str, self.workflow_id)\n+        if self.memory is not None:\n+            if isinstance(self.memory, WorkflowMemory):\n+                self.memory = cast(WorkflowMemory, self.memory)\n+                memory_dict = self.memory.to_dict()\n+                # We only persist the runs for the current session ID (not all runs in memory)\n+                memory_dict[\"runs\"] = [\n+                    agent_run.model_dump()\n+                    for agent_run in self.memory.runs\n+                    if agent_run.response is not None and agent_run.response.session_id == self.session_id\n+                ]\n+            else:\n+                self.memory = cast(Memory, self.memory)\n+                # We fake the structure on storage, to maintain the interface with the legacy implementation\n+                run_responses = self.memory.runs[self.session_id]  # type: ignore\n+                memory_dict = self.memory.to_dict()\n+                memory_dict[\"runs\"] = [rr.to_dict() for rr in run_responses]\n+        else:\n+            memory_dict = None\n         return WorkflowSession(\n             session_id=self.session_id,\n             workflow_id=self.workflow_id,\n             user_id=self.user_id,\n-            memory=self.memory.to_dict() if self.memory is not None else None,\n+            memory=memory_dict,\n             workflow_data=self.get_workflow_data(),\n             session_data=self.get_session_data(),\n             extra_data=self.extra_data,\n@@ -437,16 +455,28 @@ class Workflow:\n \n         if session.memory is not None:\n             if self.memory is None:\n-                self.memory = WorkflowMemory()\n+                self.memory = Memory()\n+\n+            if isinstance(self.memory, Memory):\n+                try:\n+                    if self.memory.runs is None:\n+                        self.memory.runs = {}\n+                    self.memory.runs[session.session_id] = []\n+                    for run in session.memory[\"runs\"]:\n+                        run_session_id = run[\"session_id\"]\n+                        self.memory.runs[run_session_id].append(RunResponse.from_dict(run))\n+                except Exception as e:\n+                    log_warning(f\"Failed to load runs from memory: {e}\")\n+            else:\n+                try:\n+                    if \"runs\" in session.memory:\n+                        try:\n+                            self.memory.runs = [WorkflowRun(**m) for m in session.memory[\"runs\"]]\n+                        except Exception as e:\n+                            logger.warning(f\"Failed to load runs from memory: {e}\")\n+                except Exception as e:\n+                    logger.warning(f\"Failed to load WorkflowMemory: {e}\")\n \n-            try:\n-                if \"runs\" in session.memory:\n-                    try:\n-                        self.memory.runs = [WorkflowRun(**m) for m in session.memory[\"runs\"]]\n-                    except Exception as e:\n-                        logger.warning(f\"Failed to load runs from memory: {e}\")\n-            except Exception as e:\n-                logger.warning(f\"Failed to load WorkflowMemory: {e}\")\n         log_debug(f\"-*- WorkflowSession loaded: {session.session_id}\")\n \n     def read_from_storage(self) -> Optional[WorkflowSession]:\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/agent/test_metrics.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_metrics.py b/libs/agno/tests/integration/agent/test_metrics.py\nnew file mode 100644\nindex 000000000..0c458f825\n--- /dev/null\n+++ b/libs/agno/tests/integration/agent/test_metrics.py\n@@ -0,0 +1,42 @@\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.openai import OpenAIChat\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+\n+def test_session_metrics():\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[DuckDuckGoTools(cache_results=True)],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Hi, my name is John\")\n+\n+    input_tokens = sum(response.metrics.get(\"input_tokens\", []))\n+    output_tokens = sum(response.metrics.get(\"output_tokens\", []))\n+    total_tokens = sum(response.metrics.get(\"total_tokens\", []))\n+\n+    assert input_tokens > 0\n+    assert output_tokens > 0\n+    assert total_tokens > 0\n+    assert total_tokens == input_tokens + output_tokens\n+\n+    assert agent.session_metrics.input_tokens == input_tokens\n+    assert agent.session_metrics.output_tokens == output_tokens\n+    assert agent.session_metrics.total_tokens == total_tokens\n+\n+    response = agent.run(\"What is current news in France?\")\n+\n+    input_tokens_list = response.metrics.get(\"input_tokens\", [])\n+    assert len(input_tokens_list) == 2  # Should be 2 assistant messages\n+\n+    input_tokens += sum(response.metrics.get(\"input_tokens\", []))\n+    output_tokens += sum(response.metrics.get(\"output_tokens\", []))\n+    total_tokens += sum(response.metrics.get(\"total_tokens\", []))\n+\n+    assert agent.session_metrics.input_tokens == input_tokens\n+    assert agent.session_metrics.output_tokens == output_tokens\n+    assert agent.session_metrics.total_tokens == total_tokens\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py b/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py\nindex 8615436e5..4494d6e2e 100644\n--- a/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py\n+++ b/libs/agno/tests/integration/agent/test_reasoning_content_knowledge_tools.py\n@@ -62,7 +62,7 @@ def test_knowledge_tools_non_streaming(knowledge_base):\n         instructions=dedent(\"\"\"\\\n             You are an expert problem-solving assistant with strong analytical skills! \ud83e\udde0\n             Use step-by-step reasoning to solve the problem.\n-            Make sure to use the knowledge tools to search for information.\n+            Make sure to use the knowledge tools to think and search for information.\n             \\\n         \"\"\"),\n     )\n@@ -70,16 +70,21 @@ def test_knowledge_tools_non_streaming(knowledge_base):\n     # Run the agent in non-streaming mode\n     response = agent.run(\"What does Paul Graham explain about reading in his essay?\", stream=False)\n \n-    # Print the reasoning_content when received\n-    if hasattr(response, \"reasoning_content\") and response.reasoning_content:\n-        print(\"\\n=== KnowledgeTools (non-streaming) reasoning_content ===\")\n-        print(response.reasoning_content)\n-        print(\"=========================================================\\n\")\n+    think_called = False\n+    for tool_call in response.formatted_tool_calls:\n+        if \"think\" in tool_call:\n+            think_called = True\n+    if think_called:\n+        # Print the reasoning_content when received\n+        if hasattr(response, \"reasoning_content\") and response.reasoning_content:\n+            print(\"\\n=== KnowledgeTools (non-streaming) reasoning_content ===\")\n+            print(response.reasoning_content)\n+            print(\"=========================================================\\n\")\n \n-    # Assert that reasoning_content exists and is populated\n-    assert hasattr(response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n-    assert response.reasoning_content is not None, \"reasoning_content should not be None\"\n-    assert len(response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n+        # Assert that reasoning_content exists and is populated\n+        assert hasattr(response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n+        assert response.reasoning_content is not None, \"reasoning_content should not be None\"\n+        assert len(response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n \n \n @pytest.mark.integration\n@@ -92,7 +97,7 @@ def test_knowledge_tools_streaming(knowledge_base):\n         instructions=dedent(\"\"\"\\\n             You are an expert problem-solving assistant with strong analytical skills! \ud83e\udde0\n             Use step-by-step reasoning to solve the problem.\n-            Make sure to use the knowledge tools to search for information.\n+            Make sure to use the knowledge tools to think and search for information.\n             \\\n         \"\"\"),\n     )\n@@ -102,20 +107,25 @@ def test_knowledge_tools_streaming(knowledge_base):\n         agent.run(\"What are Paul Graham's suggestions on what to read?\", stream=True, stream_intermediate_steps=True)\n     )\n \n-    # Print the reasoning_content when received\n-    if (\n-        hasattr(agent, \"run_response\")\n-        and agent.run_response\n-        and hasattr(agent.run_response, \"reasoning_content\")\n-        and agent.run_response.reasoning_content\n-    ):\n-        print(\"\\n=== KnowledgeTools (streaming) reasoning_content ===\")\n-        print(agent.run_response.reasoning_content)\n-        print(\"====================================================\\n\")\n-\n-    # Check the agent's run_response directly after streaming is complete\n-    assert hasattr(agent, \"run_response\"), \"Agent should have run_response after streaming\"\n-    assert agent.run_response is not None, \"Agent's run_response should not be None\"\n-    assert hasattr(agent.run_response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n-    assert agent.run_response.reasoning_content is not None, \"reasoning_content should not be None\"\n-    assert len(agent.run_response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n+    think_called = False\n+    for tool_call in agent.run_response.formatted_tool_calls:\n+        if \"think\" in tool_call:\n+            think_called = True\n+    if think_called:\n+        # Print the reasoning_content when received\n+        if (\n+            hasattr(agent, \"run_response\")\n+            and agent.run_response\n+            and hasattr(agent.run_response, \"reasoning_content\")\n+            and agent.run_response.reasoning_content\n+        ):\n+            print(\"\\n=== KnowledgeTools (streaming) reasoning_content ===\")\n+            print(agent.run_response.reasoning_content)\n+            print(\"====================================================\\n\")\n+\n+        # Check the agent's run_response directly after streaming is complete\n+        assert hasattr(agent, \"run_response\"), \"Agent should have run_response after streaming\"\n+        assert agent.run_response is not None, \"Agent's run_response should not be None\"\n+        assert hasattr(agent.run_response, \"reasoning_content\"), \"Response should have reasoning_content attribute\"\n+        assert agent.run_response.reasoning_content is not None, \"reasoning_content should not be None\"\n+        assert len(agent.run_response.reasoning_content) > 0, \"reasoning_content should not be empty\"\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/google/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/google/test_multimodal.py b/libs/agno/tests/integration/models/google/test_multimodal.py\nindex b7da753c7..fc2d35bda 100644\n--- a/libs/agno/tests/integration/models/google/test_multimodal.py\n+++ b/libs/agno/tests/integration/models/google/test_multimodal.py\n@@ -10,7 +10,7 @@ from agno.models.google import Gemini\n \n def test_image_input():\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         markdown=True,\n@@ -35,7 +35,7 @@ def test_audio_input_bytes():\n \n     # Provide the agent with the audio file and get result as text\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         markdown=True,\n@@ -49,7 +49,7 @@ def test_audio_input_bytes():\n \n def test_audio_input_url():\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         markdown=True,\n@@ -67,7 +67,7 @@ def test_audio_input_url():\n \n def test_video_input_bytes():\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         markdown=True,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/google/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/google/test_tool_use.py b/libs/agno/tests/integration/models/google/test_tool_use.py\nindex 25923fbd3..ddaf9ce92 100644\n--- a/libs/agno/tests/integration/models/google/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/google/test_tool_use.py\n@@ -114,7 +114,7 @@ def test_tool_use_with_native_structured_outputs():\n         currency: str = Field(..., description=\"The currency of the stock\")\n \n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         markdown=True,\n@@ -137,7 +137,7 @@ def test_tool_use_with_json_structured_outputs():\n         currency: str = Field(..., description=\"The currency of the stock\")\n \n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n         tools=[YFinanceTools(cache_results=True)],\n         exponential_backoff=True,\n         delay_between_retries=5,\n@@ -206,7 +206,7 @@ def test_multiple_tool_calls():\n \n def test_grounding():\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\", grounding=True),\n+        model=Gemini(id=\"gemini-2.0-flash-001\", grounding=True),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         telemetry=False,\n@@ -224,7 +224,7 @@ def test_grounding():\n \n def test_grounding_stream():\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\", grounding=True),\n+        model=Gemini(id=\"gemini-2.0-flash-001\", grounding=True),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         telemetry=False,\n@@ -248,7 +248,7 @@ def test_grounding_stream():\n \n def test_search_stream():\n     agent = Agent(\n-        model=Gemini(id=\"gemini-2.0-flash-exp\", search=True),\n+        model=Gemini(id=\"gemini-2.0-flash-001\", search=True),\n         exponential_backoff=True,\n         delay_between_retries=5,\n         telemetry=False,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/__init__.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/__init__.py b/libs/agno/tests/integration/models/meta/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama/__init__.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama/__init__.py b/libs/agno/tests/integration/models/meta/llama/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama/test_basic.py b/libs/agno/tests/integration/models/meta/llama/test_basic.py\nnew file mode 100644\nindex 000000000..ad45a570e\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/meta/llama/test_basic.py\n@@ -0,0 +1,169 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.meta.llama import Llama\n+from agno.storage.sqlite import SqliteStorage\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        add_history_to_messages=True,\n+        num_history_runs=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_structured_output():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_json_response_mode():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        response_model=MovieScript,\n+        use_json_mode=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_history():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        storage=SqliteStorage(table_name=\"agent_sessions\", db_file=\"tmp/agent_storage.db\"),\n+        add_history_to_messages=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    agent.run(\"Hello\")\n+    assert len(agent.run_response.messages) == 2\n+    agent.run(\"Hello 2\")\n+    assert len(agent.run_response.messages) == 4\n+    agent.run(\"Hello 3\")\n+    assert len(agent.run_response.messages) == 6\n+    agent.run(\"Hello 4\")\n+    assert len(agent.run_response.messages) == 8\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama/test_multimodal.py b/libs/agno/tests/integration/models/meta/llama/test_multimodal.py\nnew file mode 100644\nindex 000000000..2fd80af37\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/meta/llama/test_multimodal.py\n@@ -0,0 +1,35 @@\n+from pathlib import Path\n+\n+from agno.agent.agent import Agent\n+from agno.media import Image\n+from agno.models.meta import Llama\n+\n+image_path = Path(__file__).parent.parent.parent.joinpath(\"sample_image.jpg\")\n+\n+\n+def test_image_input_file():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response = agent.run(\n+        \"Tell me about this image?\",\n+        images=[Image(filepath=image_path)],\n+    )\n+\n+    assert \"golden\" in response.content.lower()\n+\n+\n+def test_image_input_bytes():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    image_bytes = image_path.read_bytes()\n+\n+    response = agent.run(\n+        \"Tell me about this image?\",\n+        images=[Image(content=image_bytes)],\n+    )\n+\n+    assert \"golden\" in response.content.lower()\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama/test_tool_use.py b/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\nnew file mode 100644\nindex 000000000..6b657ed6c\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/meta/llama/test_tool_use.py\n@@ -0,0 +1,209 @@\n+from typing import Optional\n+\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What is the current price of TSLA?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"$\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What is the current price of TSLA?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\"What is the current price of TSLA?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"$\" in r.content for r in responses if r.content)\n+\n+\n+def test_tool_use_with_content():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA? What does the ticker stand for?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+    assert \"Tesla\" in response.content\n+\n+\n+def test_parallel_tool_calls():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA and AAPL?\")\n+\n+    # Verify tool usage\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    assert len([call for call in tool_calls if call.get(\"type\", \"\") == \"function\"]) == 2  # Total of 2 tool calls made\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content and \"AAPL\" in response.content\n+\n+\n+def test_multiple_tool_calls():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True), DuckDuckGoTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA and what is the latest news about it?\")\n+\n+    # Verify tool usage\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    assert len([call for call in tool_calls if call.get(\"type\", \"\") == \"function\"]) == 2  # Total of 2 tool calls made\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+@pytest.mark.skip(\"Llama models do not call tools without any parameters, instead they return tool call as a string\")\n+def test_tool_call_custom_tool_no_parameters():\n+    def get_the_weather_in_tokyo():\n+        \"\"\"\n+        Get the weather in Tokyo\n+        \"\"\"\n+        return \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[get_the_weather_in_tokyo],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo? Use the tool get_the_weather_in_tokyo\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"Tokyo\" in response.content\n+\n+\n+def test_tool_call_custom_tool_optional_parameters():\n+    def get_the_weather(city: Optional[str] = None):\n+        \"\"\"\n+        Get the weather in a city\n+\n+        Args:\n+            city: The city to get the weather for\n+        \"\"\"\n+        if city is None:\n+            return \"It is currently 70 degrees and cloudy in Tokyo\"\n+        else:\n+            return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Paris?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"70\" in response.content\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/__init__.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/__init__.py b/libs/agno/tests/integration/models/meta/llama_openai/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/test_basic.py b/libs/agno/tests/integration/models/meta/llama_openai/test_basic.py\nnew file mode 100644\nindex 000000000..ac2445d7b\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/meta/llama_openai/test_basic.py\n@@ -0,0 +1,169 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.meta.llama_openai import LlamaOpenAI\n+from agno.storage.sqlite import SqliteStorage\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        add_history_to_messages=True,\n+        num_history_runs=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_structured_output():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_json_response_mode():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        response_model=MovieScript,\n+        use_json_mode=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_history():\n+    agent = Agent(\n+        model=LlamaOpenAI(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        storage=SqliteStorage(table_name=\"agent_sessions\", db_file=\"tmp/agent_storage.db\", auto_upgrade_schema=True),\n+        add_history_to_messages=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    agent.run(\"Hello\")\n+    assert len(agent.run_response.messages) == 2\n+    agent.run(\"Hello 2\")\n+    assert len(agent.run_response.messages) == 4\n+    agent.run(\"Hello 3\")\n+    assert len(agent.run_response.messages) == 6\n+    agent.run(\"Hello 4\")\n+    assert len(agent.run_response.messages) == 8\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py b/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py\nnew file mode 100644\nindex 000000000..6bb5b6436\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/meta/llama_openai/test_multimodal.py\n@@ -0,0 +1,43 @@\n+from pathlib import Path\n+\n+from agno.agent.agent import Agent\n+from agno.media import Image\n+from agno.models.meta import Llama\n+from agno.utils.media import download_image\n+\n+image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n+\n+download_image(\n+    url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\",\n+    output_path=str(image_path),\n+)\n+\n+\n+def test_image_input_file():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response = agent.run(\n+        \"Tell me about this image?\",\n+        images=[Image(filepath=image_path)],\n+    )\n+\n+    assert \"golden\" in response.content.lower()\n+    assert \"bridge\" in response.content.lower()\n+\n+\n+def test_image_input_bytes():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    image_bytes = image_path.read_bytes()\n+\n+    response = agent.run(\n+        \"Tell me about this image?\",\n+        images=[Image(content=image_bytes)],\n+    )\n+\n+    assert \"golden\" in response.content.lower()\n+    assert \"bridge\" in response.content.lower()\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py b/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\nnew file mode 100644\nindex 000000000..6b657ed6c\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/meta/llama_openai/test_tool_use.py\n@@ -0,0 +1,209 @@\n+from typing import Optional\n+\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.meta import Llama\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What is the current price of TSLA?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"$\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What is the current price of TSLA?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\"What is the current price of TSLA?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"$\" in r.content for r in responses if r.content)\n+\n+\n+def test_tool_use_with_content():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA? What does the ticker stand for?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+    assert \"Tesla\" in response.content\n+\n+\n+def test_parallel_tool_calls():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA and AAPL?\")\n+\n+    # Verify tool usage\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    assert len([call for call in tool_calls if call.get(\"type\", \"\") == \"function\"]) == 2  # Total of 2 tool calls made\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content and \"AAPL\" in response.content\n+\n+\n+def test_multiple_tool_calls():\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[YFinanceTools(cache_results=True), DuckDuckGoTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the current price of TSLA and what is the latest news about it?\")\n+\n+    # Verify tool usage\n+    tool_calls = []\n+    for msg in response.messages:\n+        if msg.tool_calls:\n+            tool_calls.extend(msg.tool_calls)\n+    assert len([call for call in tool_calls if call.get(\"type\", \"\") == \"function\"]) == 2  # Total of 2 tool calls made\n+    assert response.content is not None\n+    assert \"TSLA\" in response.content\n+\n+\n+@pytest.mark.skip(\"Llama models do not call tools without any parameters, instead they return tool call as a string\")\n+def test_tool_call_custom_tool_no_parameters():\n+    def get_the_weather_in_tokyo():\n+        \"\"\"\n+        Get the weather in Tokyo\n+        \"\"\"\n+        return \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[get_the_weather_in_tokyo],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo? Use the tool get_the_weather_in_tokyo\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"Tokyo\" in response.content\n+\n+\n+def test_tool_call_custom_tool_optional_parameters():\n+    def get_the_weather(city: Optional[str] = None):\n+        \"\"\"\n+        Get the weather in a city\n+\n+        Args:\n+            city: The city to get the weather for\n+        \"\"\"\n+        if city is None:\n+            return \"It is currently 70 degrees and cloudy in Tokyo\"\n+        else:\n+            return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=Llama(id=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Paris?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"70\" in response.content\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/openai/responses/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/openai/responses/test_tool_use.py b/libs/agno/tests/integration/models/openai/responses/test_tool_use.py\nindex ec843d856..09a809557 100644\n--- a/libs/agno/tests/integration/models/openai/responses/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/openai/responses/test_tool_use.py\n@@ -292,5 +292,4 @@ def test_web_search_built_in_tool_with_other_tools():\n     tool_calls = [msg.tool_calls for msg in response.messages if msg.tool_calls]\n     assert len(tool_calls) >= 1  # At least one message has tool calls\n     assert response.content is not None\n-    assert \"TSLA\" in response.content\n-    assert \"news\" in response.content.lower()\n+    assert \"TSLA\" in response.content or \"tesla\" in response.content.lower()\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/xai/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/xai/test_basic.py b/libs/agno/tests/integration/models/xai/test_basic.py\nindex 906a702aa..39a162d11 100644\n--- a/libs/agno/tests/integration/models/xai/test_basic.py\n+++ b/libs/agno/tests/integration/models/xai/test_basic.py\n@@ -18,7 +18,7 @@ def _assert_metrics(response: RunResponse):\n \n \n def test_basic():\n-    agent = Agent(model=xAI(id=\"grok-beta\"), markdown=True, telemetry=False, monitoring=False)\n+    agent = Agent(model=xAI(id=\"grok-3-mini-fast\"), markdown=True, telemetry=False, monitoring=False)\n \n     response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n \n@@ -30,7 +30,7 @@ def test_basic():\n \n \n def test_basic_stream():\n-    agent = Agent(model=xAI(id=\"grok-beta\"), markdown=True, telemetry=False, monitoring=False)\n+    agent = Agent(model=xAI(id=\"grok-3-mini-fast\"), markdown=True, telemetry=False, monitoring=False)\n \n     response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n \n@@ -48,7 +48,7 @@ def test_basic_stream():\n \n @pytest.mark.asyncio\n async def test_async_basic():\n-    agent = Agent(model=xAI(id=\"grok-beta\"), markdown=True, telemetry=False, monitoring=False)\n+    agent = Agent(model=xAI(id=\"grok-3-mini-fast\"), markdown=True, telemetry=False, monitoring=False)\n \n     response = await agent.arun(\"Share a 2 sentence horror story\")\n \n@@ -60,7 +60,7 @@ async def test_async_basic():\n \n @pytest.mark.asyncio\n async def test_async_basic_stream():\n-    agent = Agent(model=xAI(id=\"grok-beta\"), markdown=True, telemetry=False, monitoring=False)\n+    agent = Agent(model=xAI(id=\"grok-3-mini-fast\"), markdown=True, telemetry=False, monitoring=False)\n \n     response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n \n@@ -73,7 +73,7 @@ async def test_async_basic_stream():\n \n def test_with_memory():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         add_history_to_messages=True,\n         markdown=True,\n         telemetry=False,\n@@ -168,7 +168,7 @@ def test_structured_outputs_deprecated():\n \n def test_history():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         storage=SqliteStorage(table_name=\"agent_sessions\", db_file=\"tmp/agent_storage.db\"),\n         add_history_to_messages=True,\n         telemetry=False,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/xai/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/xai/test_multimodal.py b/libs/agno/tests/integration/models/xai/test_multimodal.py\nindex ddc13a105..b2c0d4d98 100644\n--- a/libs/agno/tests/integration/models/xai/test_multimodal.py\n+++ b/libs/agno/tests/integration/models/xai/test_multimodal.py\n@@ -6,7 +6,7 @@ from agno.tools.duckduckgo import DuckDuckGoTools\n \n def test_image_input():\n     agent = Agent(\n-        model=xAI(id=\"grok-2-vision-latest\"),\n+        model=xAI(id=\"grok-2-vision-1212\"),\n         tools=[DuckDuckGoTools(cache_results=True)],\n         markdown=True,\n         telemetry=False,\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/models/xai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/xai/test_tool_use.py b/libs/agno/tests/integration/models/xai/test_tool_use.py\nindex 3efb78238..2f86ec305 100644\n--- a/libs/agno/tests/integration/models/xai/test_tool_use.py\n+++ b/libs/agno/tests/integration/models/xai/test_tool_use.py\n@@ -11,7 +11,7 @@ from agno.tools.yfinance import YFinanceTools\n \n def test_tool_use():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         markdown=True,\n@@ -29,7 +29,7 @@ def test_tool_use():\n \n def test_tool_use_stream():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         markdown=True,\n@@ -60,7 +60,7 @@ def test_tool_use_stream():\n @pytest.mark.asyncio\n async def test_async_tool_use():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         markdown=True,\n@@ -79,7 +79,7 @@ async def test_async_tool_use():\n @pytest.mark.asyncio\n async def test_async_tool_use_stream():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[YFinanceTools(cache_results=True)],\n         show_tool_calls=True,\n         markdown=True,\n@@ -143,7 +143,7 @@ def test_tool_call_custom_tool_no_parameters():\n         return \"It is currently 70 degrees and cloudy in Tokyo\"\n \n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[get_the_weather_in_tokyo],\n         show_tool_calls=True,\n         markdown=True,\n@@ -173,7 +173,7 @@ def test_tool_call_custom_tool_optional_parameters():\n             return f\"It is currently 70 degrees and cloudy in {city}\"\n \n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[get_the_weather],\n         show_tool_calls=True,\n         markdown=True,\n@@ -191,7 +191,7 @@ def test_tool_call_custom_tool_optional_parameters():\n \n def test_tool_call_list_parameters():\n     agent = Agent(\n-        model=xAI(id=\"grok-beta\"),\n+        model=xAI(id=\"grok-3-mini-fast\"),\n         tools=[ExaTools()],\n         instructions=\"Use a single tool call if possible\",\n         show_tool_calls=True,\n@@ -212,5 +212,5 @@ def test_tool_call_list_parameters():\n             tool_calls.extend(msg.tool_calls)\n     for call in tool_calls:\n         if call.get(\"type\", \"\") == \"function\":\n-            assert call[\"function\"][\"name\"] in [\"get_contents\", \"exa_answer\"]\n+            assert call[\"function\"][\"name\"] in [\"search_exa\", \"get_contents\", \"exa_answer\"]\n     assert response.content is not None\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/storage/test_json_storage_workflow.py",
            "diff": "diff --git a/libs/agno/tests/integration/storage/test_json_storage_workflow.py b/libs/agno/tests/integration/storage/test_json_storage_workflow.py\nindex 327c5719c..674658ce8 100644\n--- a/libs/agno/tests/integration/storage/test_json_storage_workflow.py\n+++ b/libs/agno/tests/integration/storage/test_json_storage_workflow.py\n@@ -93,7 +93,7 @@ def test_multiple_interactions(workflow_with_storage, workflow_storage):\n     stored_session = workflow_storage.read(session_id)\n     assert stored_session is not None\n     assert \"runs\" in stored_session.memory\n-    runs = stored_session.memory[\"runs\"][session_id]\n+    runs = stored_session.memory[\"runs\"]\n     assert len(runs) == 2  # Should have 2 runs\n \n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/storage/test_sqlite_storage_workflow.py",
            "diff": "diff --git a/libs/agno/tests/integration/storage/test_sqlite_storage_workflow.py b/libs/agno/tests/integration/storage/test_sqlite_storage_workflow.py\nindex 289cb278a..590e803cf 100644\n--- a/libs/agno/tests/integration/storage/test_sqlite_storage_workflow.py\n+++ b/libs/agno/tests/integration/storage/test_sqlite_storage_workflow.py\n@@ -97,7 +97,7 @@ def test_multiple_interactions(workflow_with_storage, workflow_storage):\n     stored_session = workflow_storage.read(session_id)\n     assert stored_session is not None\n     assert \"runs\" in stored_session.memory\n-    runs = stored_session.memory[\"runs\"][session_id]\n+    runs = stored_session.memory[\"runs\"]\n     assert len(runs) == 2  # Should have 2 runs\n \n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/storage/test_yaml_storage_workflow.py",
            "diff": "diff --git a/libs/agno/tests/integration/storage/test_yaml_storage_workflow.py b/libs/agno/tests/integration/storage/test_yaml_storage_workflow.py\nindex 978d486fe..bfc72b454 100644\n--- a/libs/agno/tests/integration/storage/test_yaml_storage_workflow.py\n+++ b/libs/agno/tests/integration/storage/test_yaml_storage_workflow.py\n@@ -93,7 +93,7 @@ def test_multiple_interactions(workflow_with_storage, workflow_storage):\n     stored_session = workflow_storage.read(session_id)\n     assert stored_session is not None\n     assert \"runs\" in stored_session.memory\n-    runs = stored_session.memory[\"runs\"][session_id]\n+    runs = stored_session.memory[\"runs\"]\n     assert len(runs) == 2  # Should have 2 runs\n \n \n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/teams/test_team_metrics.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_metrics.py b/libs/agno/tests/integration/teams/test_team_metrics.py\nindex 4eac20f1b..8843a6df5 100644\n--- a/libs/agno/tests/integration/teams/test_team_metrics.py\n+++ b/libs/agno/tests/integration/teams/test_team_metrics.py\n@@ -52,13 +52,6 @@ def test_team_metrics_basic():\n     assert team.session_metrics.output_tokens is not None\n     assert team.session_metrics.total_tokens is not None\n \n-    # Check full team session metrics\n-    assert team.full_team_session_metrics is not None\n-    assert isinstance(team.full_team_session_metrics, SessionMetrics)\n-    assert team.full_team_session_metrics.input_tokens is not None\n-    assert team.full_team_session_metrics.output_tokens is not None\n-    assert team.full_team_session_metrics.total_tokens is not None\n-\n \n def test_team_metrics_streaming():\n     \"\"\"Test team metrics with streaming.\"\"\"\n@@ -94,10 +87,6 @@ def test_team_metrics_streaming():\n     assert team.run_response.metrics[\"output_tokens\"] is not None\n     assert team.run_response.metrics[\"total_tokens\"] is not None\n \n-    # Verify session metrics updated after streaming\n-    assert team.full_team_session_metrics is not None\n-    assert team.full_team_session_metrics.total_tokens > 0\n-\n \n def test_team_metrics_multiple_runs():\n     \"\"\"Test team metrics across multiple runs.\"\"\"\n@@ -128,6 +117,3 @@ def test_team_metrics_multiple_runs():\n \n     # Verify metrics have been updated after second run\n     assert team.session_metrics.total_tokens > metrics_run1.total_tokens\n-\n-    # Verify member metrics are tracked in full team metrics\n-    assert team.full_team_session_metrics.total_tokens >= team.session_metrics.total_tokens\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\nnew file mode 100644\nindex 000000000..312598de6\n--- /dev/null\n+++ b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\n@@ -0,0 +1,178 @@\n+import os\n+import tempfile\n+import uuid\n+\n+import pytest\n+\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.anthropic.claude import Claude\n+from agno.models.openai.chat import OpenAIChat\n+from agno.storage.sqlite import SqliteStorage\n+from agno.team.team import Team\n+\n+\n+@pytest.fixture\n+def temp_storage_db_file():\n+    \"\"\"Create a temporary SQLite database file for team storage testing.\"\"\"\n+    with tempfile.NamedTemporaryFile(suffix=\".db\", delete=False) as temp_file:\n+        db_path = temp_file.name\n+\n+    yield db_path\n+\n+    # Clean up the temporary file after the test\n+    if os.path.exists(db_path):\n+        os.unlink(db_path)\n+\n+\n+@pytest.fixture\n+def temp_memory_db_file():\n+    \"\"\"Create a temporary SQLite database file for memory testing.\"\"\"\n+    with tempfile.NamedTemporaryFile(suffix=\".db\", delete=False) as temp_file:\n+        db_path = temp_file.name\n+\n+    yield db_path\n+\n+    # Clean up the temporary file after the test\n+    if os.path.exists(db_path):\n+        os.unlink(db_path)\n+\n+\n+@pytest.fixture\n+def team_storage(temp_storage_db_file):\n+    \"\"\"Create a SQLite storage for team sessions.\"\"\"\n+    # Use a unique table name for each test run\n+    table_name = f\"team_sessions_{uuid.uuid4().hex[:8]}\"\n+    storage = SqliteStorage(table_name=table_name, db_file=temp_storage_db_file, mode=\"team\")\n+    storage.create()\n+    return storage\n+\n+\n+@pytest.fixture\n+def memory_db(temp_memory_db_file):\n+    \"\"\"Create a SQLite memory database for testing.\"\"\"\n+    db = SqliteMemoryDb(db_file=temp_memory_db_file)\n+    db.create()\n+    return db\n+\n+\n+@pytest.fixture\n+def memory(memory_db):\n+    \"\"\"Create a Memory instance for testing.\"\"\"\n+    return Memory(model=Claude(id=\"claude-3-5-sonnet-20241022\"), db=memory_db)\n+\n+\n+@pytest.fixture\n+def route_team(team_storage, memory):\n+    \"\"\"Create a route team with storage and memory for testing.\"\"\"\n+    return Team(\n+        name=\"Route Team\",\n+        mode=\"route\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        members=[],\n+        storage=team_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_run_history_persistence(route_team, team_storage, memory):\n+    \"\"\"Test that all runs within a session are persisted in storage.\"\"\"\n+    user_id = \"john@example.com\"\n+    session_id = \"session_123\"\n+    num_turns = 5\n+\n+    # Clear memory for this specific test case\n+    memory.clear()\n+\n+    # Perform multiple turns\n+    conversation_messages = [\n+        \"What's the weather like today?\",\n+        \"What about tomorrow?\",\n+        \"Any recommendations for indoor activities?\",\n+        \"Search for nearby museums.\",\n+        \"Which one has the best reviews?\",\n+    ]\n+\n+    assert len(conversation_messages) == num_turns\n+\n+    for msg in conversation_messages:\n+        await route_team.arun(msg, user_id=user_id, session_id=session_id)\n+\n+    # Verify the stored session data after all turns\n+    team_session = team_storage.read(session_id=session_id)\n+\n+    stored_memory_data = team_session.memory\n+    assert stored_memory_data is not None, \"Memory data not found in stored session.\"\n+\n+    stored_runs = stored_memory_data[\"runs\"]\n+    assert isinstance(stored_runs, list), \"Stored runs data is not a list.\"\n+\n+    first_user_message_content = stored_runs[0][\"messages\"][1][\"content\"]\n+    assert first_user_message_content == conversation_messages[0]\n+\n+\n+@pytest.mark.asyncio\n+async def test_multi_user_multi_session_route_team(route_team, team_storage, memory):\n+    \"\"\"Test multi-user multi-session route team with storage and memory.\"\"\"\n+    # Define user and session IDs\n+    user_1_id = \"user_1@example.com\"\n+    user_2_id = \"user_2@example.com\"\n+    user_3_id = \"user_3@example.com\"\n+\n+    user_1_session_1_id = \"user_1_session_1\"\n+    user_1_session_2_id = \"user_1_session_2\"\n+    user_2_session_1_id = \"user_2_session_1\"\n+    user_3_session_1_id = \"user_3_session_1\"\n+\n+    # Clear memory for this test\n+    memory.clear()\n+\n+    # Team interaction with user 1 - Session 1\n+    await route_team.arun(\"What is the current stock price of AAPL?\", user_id=user_1_id, session_id=user_1_session_1_id)\n+    await route_team.arun(\"What are the latest news about Apple?\", user_id=user_1_id, session_id=user_1_session_1_id)\n+\n+    # Team interaction with user 1 - Session 2\n+    await route_team.arun(\n+        \"Compare the stock performance of AAPL with recent tech industry news\",\n+        user_id=user_1_id,\n+        session_id=user_1_session_2_id,\n+    )\n+\n+    # Team interaction with user 2\n+    await route_team.arun(\"What is the current stock price of MSFT?\", user_id=user_2_id, session_id=user_2_session_1_id)\n+    await route_team.arun(\n+        \"What are the latest news about Microsoft?\", user_id=user_2_id, session_id=user_2_session_1_id\n+    )\n+\n+    # Team interaction with user 3\n+    await route_team.arun(\n+        \"What is the current stock price of GOOGL?\", user_id=user_3_id, session_id=user_3_session_1_id\n+    )\n+    await route_team.arun(\"What are the latest news about Google?\", user_id=user_3_id, session_id=user_3_session_1_id)\n+\n+    # Continue the conversation with user 1\n+    await route_team.arun(\n+        \"Based on the information you have, what stock would you recommend investing in?\",\n+        user_id=user_1_id,\n+        session_id=user_1_session_1_id,\n+    )\n+\n+    # Verify storage DB has the right sessions\n+    all_session_ids = team_storage.get_all_session_ids()\n+    assert len(all_session_ids) == 4  # 4 sessions total\n+\n+    # Check that each user has the expected sessions\n+    user_1_sessions = team_storage.get_all_sessions(user_id=user_1_id)\n+    assert len(user_1_sessions) == 2\n+    assert user_1_session_1_id in [session.session_id for session in user_1_sessions]\n+    assert user_1_session_2_id in [session.session_id for session in user_1_sessions]\n+\n+    user_2_sessions = team_storage.get_all_sessions(user_id=user_2_id)\n+    assert len(user_2_sessions) == 1\n+    assert user_2_session_1_id in [session.session_id for session in user_2_sessions]\n+\n+    user_3_sessions = team_storage.get_all_sessions(user_id=user_3_id)\n+    assert len(user_3_sessions) == 1\n+    assert user_3_session_1_id in [session.session_id for session in user_3_sessions]\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/reader/test_firecrawl_reader.py",
            "diff": "diff --git a/libs/agno/tests/unit/reader/test_firecrawl_reader.py b/libs/agno/tests/unit/reader/test_firecrawl_reader.py\nindex 163716b10..ca37c2348 100644\n--- a/libs/agno/tests/unit/reader/test_firecrawl_reader.py\n+++ b/libs/agno/tests/unit/reader/test_firecrawl_reader.py\n@@ -119,7 +119,7 @@ def test_scrape_with_chunking(mock_scrape_response):\n         # Create reader with chunking enabled\n         reader = FirecrawlReader()\n         reader.chunk = True\n-        reader.chunk_size = 10  # Small chunk size to ensure multiple chunks\n+        reader.chunking_strategy.chunk_size = 10  # Small chunk size to ensure multiple chunks\n \n         # Create a patch for chunk_document\n         def mock_chunk_document(doc):\n@@ -209,7 +209,7 @@ def test_crawl_with_chunking(mock_crawl_response):\n         # Create reader with chunking enabled\n         reader = FirecrawlReader(mode=\"crawl\")\n         reader.chunk = True\n-        reader.chunk_size = 10  # Small chunk size to ensure multiple chunks\n+        reader.chunking_strategy.chunk_size = 10  # Small chunk size to ensure multiple chunks\n \n         def mock_chunk_document(doc):\n             # Simple mock that splits into 2 chunks\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/reader/test_url_reader.py",
            "diff": "diff --git a/libs/agno/tests/unit/reader/test_url_reader.py b/libs/agno/tests/unit/reader/test_url_reader.py\nindex b0bc69605..0db0e6958 100644\n--- a/libs/agno/tests/unit/reader/test_url_reader.py\n+++ b/libs/agno/tests/unit/reader/test_url_reader.py\n@@ -97,7 +97,7 @@ def test_chunking(mock_response):\n     with patch(\"httpx.get\", return_value=mock_response):\n         reader = URLReader()\n         reader.chunk = True\n-        reader.chunk_size = 100\n+        reader.chunking_strategy.chunk_size = 100\n         documents = reader.read(url)\n \n         assert len(documents) > 1\n@@ -198,7 +198,7 @@ async def test_async_chunking():\n     with patch(\"httpx.AsyncClient\", return_value=mock_client):\n         reader = URLReader()\n         reader.chunk = True\n-        reader.chunk_size = 100\n+        reader.chunking_strategy.chunk_size = 100\n         documents = await reader.async_read(url)\n \n         assert len(documents) > 1\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/tools/models/test_gemini.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/models/test_gemini.py b/libs/agno/tests/unit/tools/models/test_gemini.py\nindex ade365023..283645245 100644\n--- a/libs/agno/tests/unit/tools/models/test_gemini.py\n+++ b/libs/agno/tests/unit/tools/models/test_gemini.py\n@@ -6,7 +6,7 @@ from uuid import UUID\n import pytest\n \n from agno.agent import Agent\n-from agno.media import ImageArtifact\n+from agno.media import ImageArtifact, VideoArtifact\n from agno.tools.models.gemini import GeminiTools\n \n \n@@ -122,7 +122,7 @@ def test_generate_image_success(mock_gemini_tools, mock_agent, mock_successful_r\n         result = mock_gemini_tools.generate_image(mock_agent, prompt)\n \n         expected_media_id = \"12345678-1234-5678-1234-567812345678\"\n-        assert result == f\"Image generated successfully with ID: {expected_media_id}\"\n+        assert result == \"Image generated successfully\"\n         mock_gemini_tools.client.models.generate_images.assert_called_once_with(model=image_model, prompt=prompt)\n \n         # Verify agent.add_image was called with the correct ImageArtifact\n@@ -173,3 +173,63 @@ def test_generate_image_no_image_bytes(mock_gemini_tools, mock_agent, mock_faile\n         prompt=prompt,\n     )\n     mock_agent.add_image.assert_not_called()\n+\n+\n+# Tests for generate_video method\n+def test_generate_video_requires_vertexai(mock_gemini_tools, mock_agent):\n+    \"\"\"Test video generation when vertexai mode is disabled.\"\"\"\n+    prompt = \"A sample video prompt\"\n+    result = mock_gemini_tools.generate_video(mock_agent, prompt)\n+    expected = (\n+        \"Video generation requires Vertex AI mode. \"\n+        \"Please set `vertexai=True` or environment variable `GOOGLE_GENAI_USE_VERTEXAI=true`.\"\n+    )\n+    assert result == expected\n+    mock_agent.add_video.assert_not_called()\n+\n+\n+@pytest.fixture\n+def mock_video_operation():\n+    \"\"\"Fixture for a completed video generation operation.\"\"\"\n+    op = MagicMock()\n+    op.done = True\n+    video = MagicMock()\n+    video.video_bytes = b\"fake_video_bytes\"\n+    video.mime_type = \"video/mp4\"\n+    op.result = MagicMock(generated_videos=[MagicMock(video=video)])\n+    return op\n+\n+\n+def test_generate_video_success(mock_gemini_tools, mock_agent, mock_video_operation):\n+    \"\"\"Test successful video generation.\"\"\"\n+    mock_gemini_tools.vertexai = True\n+    mock_gemini_tools.client.models.generate_videos.return_value = mock_video_operation\n+    prompt = \"A sample video prompt\"\n+    with patch(\"agno.tools.models.gemini.uuid4\", return_value=UUID(\"87654321-4321-8765-4321-876543214321\")):\n+        result = mock_gemini_tools.generate_video(mock_agent, prompt)\n+        expected_id = \"87654321-4321-8765-4321-876543214321\"\n+        assert result == \"Video generated successfully\"\n+        assert mock_gemini_tools.client.models.generate_videos.called\n+        call_args = mock_gemini_tools.client.models.generate_videos.call_args\n+        assert call_args.kwargs[\"model\"] == mock_gemini_tools.video_model\n+        assert call_args.kwargs[\"prompt\"] == prompt\n+        mock_agent.add_video.assert_called_once()\n+        added = mock_agent.add_video.call_args[0][0]\n+        assert isinstance(added, VideoArtifact)\n+        assert added.id == expected_id\n+        assert added.original_prompt == prompt\n+        assert added.mime_type == \"video/mp4\"\n+        import base64\n+\n+        expected_content = base64.b64encode(b\"fake_video_bytes\").decode(\"utf-8\")\n+        assert added.content == expected_content\n+\n+\n+def test_generate_video_exception(mock_gemini_tools, mock_agent):\n+    \"\"\"Test video generation when API raises exception.\"\"\"\n+    mock_gemini_tools.vertexai = True\n+    mock_gemini_tools.client.models.generate_videos.side_effect = Exception(\"API error\")\n+    prompt = \"A sample video prompt\"\n+    result = mock_gemini_tools.generate_video(mock_agent, prompt)\n+    assert result == \"Failed to generate video: API error\"\n+    mock_agent.add_video.assert_not_called()\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/tools/test_github.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_github.py b/libs/agno/tests/unit/tools/test_github.py\nindex bf69b9a29..2cb58760e 100644\n--- a/libs/agno/tests/unit/tools/test_github.py\n+++ b/libs/agno/tests/unit/tools/test_github.py\n@@ -526,6 +526,7 @@ def test_get_repository_stars(mock_github):\n     assert \"error\" in result_data\n     assert \"Repository not found\" in result_data[\"error\"]\n \n+\n def test_get_pull_request_comments(mock_github):\n     \"\"\"Test getting comments on a pull request.\"\"\"\n     mock_client, mock_repo = mock_github\n@@ -699,6 +700,7 @@ def test_edit_pull_request_comment(mock_github):\n         assert \"error\" in result_data\n         assert \"Permission denied\" in result_data[\"error\"]\n \n+\n def test_create_repository(mock_github):\n     \"\"\"Test creating a new repository.\"\"\"\n     mock_client, _ = mock_github\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/tools/test_mcp.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_mcp.py b/libs/agno/tests/unit/tools/test_mcp.py\nindex 175bdde14..8d3f12c7d 100644\n--- a/libs/agno/tests/unit/tools/test_mcp.py\n+++ b/libs/agno/tests/unit/tools/test_mcp.py\n@@ -1,4 +1,4 @@\n-from unittest.mock import patch\n+from unittest.mock import AsyncMock, patch\n \n import pytest\n \n@@ -43,3 +43,27 @@ def test_multimcp_empty_command_string():\n         # Mock shlex.split to return an empty list\n         with patch(\"shlex.split\", return_value=[]):\n             MultiMCPTools(commands=[\"\"])\n+\n+\n+@pytest.mark.asyncio\n+@pytest.mark.parametrize(\n+    \"mcp_tools,kwargs\",\n+    (\n+        (MCPTools, {\"command\": \"echo foo\", \"include_tools\": [\"foo\"]}),\n+        (MCPTools, {\"command\": \"echo foo\", \"exclude_tools\": [\"foo\"]}),\n+    ),\n+)\n+async def test_mcp_include_exclude_tools_bad_values(mcp_tools, kwargs):\n+    \"\"\"Test that _check_tools_filters raises ValueError during initialize\"\"\"\n+    session_mock = AsyncMock()\n+    tool_mock = AsyncMock()\n+    tool_mock.__name__ = \"baz\"\n+    tools = AsyncMock()\n+    tools.tools = [tool_mock]\n+    session_mock.list_tools.return_value = tools\n+\n+    # _check_tools_filters should be bypassed during __init__\n+    tools = mcp_tools(**kwargs)\n+    with pytest.raises(ValueError, match=\"not present in the toolkit\"):\n+        tools.session = session_mock\n+        await tools.initialize()\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/utils/test_openai.py",
            "diff": "diff --git a/libs/agno/tests/unit/utils/test_openai.py b/libs/agno/tests/unit/utils/test_openai.py\nindex 04cba0b45..ee9369273 100644\n--- a/libs/agno/tests/unit/utils/test_openai.py\n+++ b/libs/agno/tests/unit/utils/test_openai.py\n@@ -6,8 +6,8 @@ from pathlib import Path\n \n import pytest\n \n-from agno.media import Audio, Image\n-from agno.utils.openai import audio_to_message, images_to_message\n+from agno.media import Audio, File, Image\n+from agno.utils.openai import _format_file_for_message, audio_to_message, images_to_message\n \n \n # Helper function to create dummy file\n@@ -324,3 +324,65 @@ def test_images_to_message_mixed(tmp_png_file, dummy_image_bytes):\n     assert result[0][\"image_url\"][\"url\"].startswith(\"data:image/jpeg;base64,\")\n     assert result[1][\"image_url\"][\"url\"].startswith(\"data:image/png;base64,\")\n     assert result[2][\"image_url\"][\"url\"] == \"https://example.com/image.webp\"\n+\n+\n+# --- Tests for _format_file_for_message --- #\n+def test_format_file_external():\n+    f = File(external=\"file123\")\n+    msg = _format_file_for_message(f)\n+    # External-file branch was removed, unsupported externals return None\n+    assert msg is None\n+\n+\n+def test_format_file_url_inline(mocker):\n+    f = File(url=\"http://example.com/doc.pdf\")\n+    mock_data = (b\"PDF_CONTENT\", \"application/pdf\")\n+    mocker.patch.object(File, \"file_url_content\", new_callable=mocker.PropertyMock, return_value=mock_data)\n+    msg = _format_file_for_message(f)\n+    assert msg[\"type\"] == \"file\"\n+    assert msg[\"file\"][\"filename\"] == \"doc.pdf\"\n+    data_url = msg[\"file\"][\"file_data\"]\n+    assert data_url.startswith(\"data:application/pdf;base64,\")\n+    assert base64.b64decode(data_url.split(\",\", 1)[1]) == mock_data[0]\n+\n+\n+def test_format_file_path_inline(tmp_path):\n+    content = b\"HELLO\"\n+    p = tmp_path / \"test.txt\"\n+    p.write_bytes(content)\n+    f = File(filepath=str(p))\n+    msg = _format_file_for_message(f)\n+    assert msg[\"type\"] == \"file\"\n+    assert msg[\"file\"][\"filename\"] == \"test.txt\"\n+    data_url = msg[\"file\"][\"file_data\"]\n+    assert data_url.startswith(\"data:text/plain;base64,\")\n+    assert base64.b64decode(data_url.split(\",\", 1)[1]) == content\n+\n+\n+def test_format_file_path_persistent(tmp_path):\n+    # Large files are inlined under the same logic\n+    content = b\"PERSIST\"\n+    p = tmp_path / \"big.bin\"\n+    p.write_bytes(content)\n+    f = File(filepath=str(p))\n+    msg = _format_file_for_message(f)\n+    assert msg[\"type\"] == \"file\"\n+    assert msg[\"file\"][\"filename\"] == \"big.bin\"\n+    data_url = msg[\"file\"][\"file_data\"]\n+    # It should be a data URL with base64 payload\n+    assert data_url.startswith(\"data:\")\n+    assert \";base64,\" in data_url\n+    # The base64 payload should decode back to the original content\n+    payload = data_url.split(\",\", 1)[1]\n+    assert base64.b64decode(payload) == content\n+\n+\n+def test_format_file_raw_bytes():\n+    content = b\"RAWBYTES\"\n+    f = File(content=content)\n+    msg = _format_file_for_message(f)\n+    assert msg[\"type\"] == \"file\"\n+    assert msg[\"file\"][\"filename\"] == \"file\"\n+    data_url = msg[\"file\"][\"file_data\"]\n+    assert data_url.startswith(\"data:application/pdf;base64,\")\n+    assert base64.b64decode(data_url.split(\",\", 1)[1]) == content\n"
        },
        {
            "commit": "3ab735577ed448887a8f1f463c73a611a4ca253e",
            "file_path": "libs/agno/tests/unit/vectordb/test_pineconedb.py",
            "diff": "diff --git a/libs/agno/tests/unit/vectordb/test_pineconedb.py b/libs/agno/tests/unit/vectordb/test_pineconedb.py\nindex 69d7eb9f0..0f9e76a54 100644\n--- a/libs/agno/tests/unit/vectordb/test_pineconedb.py\n+++ b/libs/agno/tests/unit/vectordb/test_pineconedb.py\n@@ -9,6 +9,7 @@ from agno.vectordb.pineconedb import PineconeDb\n # Configuration for tests\n TEST_INDEX_NAME = f\"test_index_{uuid.uuid4().hex[:8]}\"\n TEST_DIMENSION = 1024\n+TEST_NAMESPACE = \"test_namespace\"\n \n \n @pytest.fixture\n@@ -54,6 +55,7 @@ def mock_pinecone_db(mock_pinecone_client, mock_pinecone_index, mock_embedder):\n         db = PineconeDb(\n             name=TEST_INDEX_NAME,\n             dimension=TEST_DIMENSION,\n+            namespace=TEST_NAMESPACE,\n             spec={\"serverless\": {\"cloud\": \"aws\", \"region\": \"us-west-2\"}},\n             embedder=mock_embedder,\n             api_key=\"fake-api-key\",\n@@ -86,12 +88,14 @@ def test_initialization():\n         db = PineconeDb(\n             name=TEST_INDEX_NAME,\n             dimension=TEST_DIMENSION,\n+            namespace=TEST_NAMESPACE,\n             spec={\"serverless\": {\"cloud\": \"aws\", \"region\": \"us-west-2\"}},\n             api_key=\"fake-api-key\",\n         )\n \n         assert db.name == TEST_INDEX_NAME\n         assert db.dimension == TEST_DIMENSION\n+        assert db.namespace == TEST_NAMESPACE\n         assert db.api_key == \"fake-api-key\"\n         assert db._client is None\n         assert db._index is None\n@@ -184,7 +188,7 @@ def test_doc_exists(mock_pinecone_db):\n     mock_pinecone_db.index.fetch.return_value.vectors = {}\n \n     assert mock_pinecone_db.doc_exists(doc) is False\n-    mock_pinecone_db.index.fetch.assert_called_with(ids=[doc.id])\n+    mock_pinecone_db.index.fetch.assert_called_with(ids=[doc.id], namespace=TEST_NAMESPACE)\n \n     # Test when document exists\n     mock_pinecone_db.index.fetch.return_value.vectors = {doc.id: {\"id\": doc.id}}\n@@ -251,7 +255,7 @@ def test_search(mock_pinecone_db, mock_embedder):\n \n     # Check that index.query was called with the right arguments\n     mock_pinecone_db.index.query.assert_called_with(\n-        vector=[0.1] * 1024, top_k=2, namespace=None, filter=None, include_values=None, include_metadata=True\n+        vector=[0.1] * 1024, top_k=2, namespace=TEST_NAMESPACE, filter=None, include_values=None, include_metadata=True\n     )\n \n     # Check the results\n"
        }
    ]
}