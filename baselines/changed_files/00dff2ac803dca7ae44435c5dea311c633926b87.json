{
    "sha_fail": "00dff2ac803dca7ae44435c5dea311c633926b87",
    "changed_files": [
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "docs/api/autogluon.tabular.models.rst",
            "diff": "diff --git a/docs/api/autogluon.tabular.models.rst b/docs/api/autogluon.tabular.models.rst\nindex 230b9fb..f728130 100644\n--- a/docs/api/autogluon.tabular.models.rst\n+++ b/docs/api/autogluon.tabular.models.rst\n@@ -56,6 +56,7 @@ Here is the mapping of keys to models:\n         \"NN_TORCH\": TabularNeuralNetTorchModel,\n         \"LR\": LinearModel,\n         \"FASTAI\": NNFastAiTabularModel,\n+        \"TABM\": TabMModel,\n         \"AG_TEXT_NN\": TextPredictorModel,\n         \"AG_IMAGE_NN\": ImagePredictorModel,\n         \"AG_AUTOMM\": MultiModalPredictorModel,\n@@ -89,6 +90,7 @@ Here is the mapping of model types to their default names when trained:\n         TabularNeuralNetTorchModel: 'NeuralNetTorch',\n         LinearModel: 'LinearModel',\n         NNFastAiTabularModel: 'NeuralNetFastAI',\n+        TabMModel: 'TabM',\n         TextPredictorModel: 'TextPredictor',\n         ImagePredictorModel: 'ImagePredictor',\n         MultiModalPredictorModel: 'MultiModalPredictor',\n@@ -161,6 +163,7 @@ Models\n    LinearModel\n    TabularNeuralNetTorchModel\n    NNFastAiTabularModel\n+   TabMModel\n    MultiModalPredictorModel\n    TextPredictorModel\n    ImagePredictorModel\n@@ -231,6 +234,12 @@ Models\n .. autoclass:: NNFastAiTabularModel\n    :members: init\n \n+:hidden:`TabMModel`\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. autoclass:: TabMModel\n+   :members: init\n+\n :hidden:`MultiModalPredictorModel`\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/setup.py",
            "diff": "diff --git a/tabular/setup.py b/tabular/setup.py\nindex 91fc702..13809a3 100644\n--- a/tabular/setup.py\n+++ b/tabular/setup.py\n@@ -48,6 +48,9 @@ extras_require = {\n         \"torch\",  # version range defined in `core/_setup_utils.py`\n         \"fastai>=2.3.1,<2.9\",  # <{N+1} upper cap, where N is the latest released minor version\n     ],\n+    \"tabm\": [\n+        \"torch\",  # version range defined in `core/_setup_utils.py`\n+    ],\n     \"tabpfn\": [\n         # versions below 0.1.11 are yanked, not compatible with >=2.0.0 yet\n         \"tabpfn>=0.1.11,<2.0\",  # after v2 compatibility is ensured, should be <{N+1} upper cap, where N is the latest released minor version\n@@ -94,7 +97,7 @@ else:\n # TODO: v1.0: Rename `all` to `core`, make `all` contain everything.\n all_requires = []\n # TODO: Consider adding 'skex' to 'all'\n-for extra_package in [\"lightgbm\", \"catboost\", \"xgboost\", \"fastai\", \"tabpfnmix\", \"ray\"]:\n+for extra_package in [\"lightgbm\", \"catboost\", \"xgboost\", \"fastai\", \"tabm\", \"tabpfnmix\", \"ray\"]:\n     all_requires += extras_require[extra_package]\n all_requires = list(set(all_requires))\n extras_require[\"all\"] = all_requires\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/models/__init__.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/models/__init__.py b/tabular/src/autogluon/tabular/models/__init__.py\nindex e23bb1f..d1cd53d 100644\n--- a/tabular/src/autogluon/tabular/models/__init__.py\n+++ b/tabular/src/autogluon/tabular/models/__init__.py\n@@ -18,6 +18,7 @@ from .knn.knn_model import KNNModel\n from .lgb.lgb_model import LGBModel\n from .lr.lr_model import LinearModel\n from .rf.rf_model import RFModel\n+from .tabm.tabm_model import TabMModel\n from .tabpfn.tabpfn_model import TabPFNModel\n from .tabpfnmix.tabpfnmix_model import TabPFNMixModel\n from .tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/models/tabm/__init__.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/models/tabm/__init__.py b/tabular/src/autogluon/tabular/models/tabm/__init__.py\nnew file mode 100644\nindex 0000000..e69de29\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/models/tabm/_tabm_internal.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/models/tabm/_tabm_internal.py b/tabular/src/autogluon/tabular/models/tabm/_tabm_internal.py\nnew file mode 100644\nindex 0000000..c4f329f\n--- /dev/null\n+++ b/tabular/src/autogluon/tabular/models/tabm/_tabm_internal.py\n@@ -0,0 +1,541 @@\n+\"\"\"Partially adapted from pytabkit's TabM implementation.\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import math\n+import random\n+import time\n+from typing import TYPE_CHECKING, Any, Literal\n+\n+import numpy as np\n+import pandas as pd\n+import scipy\n+import torch\n+from autogluon.core.metrics import compute_metric\n+from sklearn.base import BaseEstimator, TransformerMixin\n+from sklearn.impute import SimpleImputer\n+from sklearn.pipeline import Pipeline\n+from sklearn.preprocessing import OrdinalEncoder, QuantileTransformer\n+from sklearn.utils.validation import check_is_fitted\n+\n+from . import rtdl_num_embeddings, tabm_reference\n+from .tabm_reference import make_parameter_groups\n+\n+if TYPE_CHECKING:\n+    from autogluon.core.metrics import Scorer\n+\n+TaskType = Literal[\"regression\", \"binclass\", \"multiclass\"]\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def get_tabm_auto_batch_size(n_train: int) -> int:\n+    # by Yury Gorishniy, inferred from the choices in the TabM paper.\n+    if n_train < 2_800:\n+        return 32\n+    if n_train < 4_500:\n+        return 64\n+    if n_train < 6_400:\n+        return 128\n+    if n_train < 32_000:\n+        return 256\n+    if n_train < 108_000:\n+        return 512\n+    return 1024\n+\n+\n+class RTDLQuantileTransformer(BaseEstimator, TransformerMixin):\n+    # adapted from pytabkit\n+    def __init__(\n+        self,\n+        noise=1e-5,\n+        random_state=None,\n+        n_quantiles=1000,\n+        subsample=1_000_000_000,\n+        output_distribution=\"normal\",\n+    ):\n+        self.noise = noise\n+        self.random_state = random_state\n+        self.n_quantiles = n_quantiles\n+        self.subsample = subsample\n+        self.output_distribution = output_distribution\n+\n+    def fit(self, X, y=None):\n+        # Calculate the number of quantiles based on data size\n+        n_quantiles = max(min(X.shape[0] // 30, self.n_quantiles), 10)\n+\n+        # Initialize QuantileTransformer\n+        normalizer = QuantileTransformer(\n+            output_distribution=self.output_distribution,\n+            n_quantiles=n_quantiles,\n+            subsample=self.subsample,\n+            random_state=self.random_state,\n+        )\n+\n+        # Add noise if required\n+        X_modified = self._add_noise(X) if self.noise > 0 else X\n+\n+        # Fit the normalizer\n+        normalizer.fit(X_modified)\n+        # show that it's fitted\n+        self.normalizer_ = normalizer\n+\n+        return self\n+\n+    def transform(self, X, y=None):\n+        check_is_fitted(self)\n+        return self.normalizer_.transform(X)\n+\n+    def _add_noise(self, X):\n+        return X + np.random.default_rng(self.random_state).normal(0.0, 1e-5, X.shape).astype(X.dtype)\n+\n+\n+class TabMOrdinalEncoder(BaseEstimator, TransformerMixin):\n+    # encodes missing and unknown values to a value one larger than the known values\n+    def __init__(self):\n+        # No fitted attributes here \u2014 only parameters\n+        pass\n+\n+    def fit(self, X, y=None):\n+        X = pd.DataFrame(X)\n+\n+        # Fit internal OrdinalEncoder with NaNs preserved for now\n+        self.encoder_ = OrdinalEncoder(\n+            handle_unknown=\"use_encoded_value\",\n+            unknown_value=np.nan,\n+            encoded_missing_value=np.nan,\n+        )\n+        self.encoder_.fit(X)\n+\n+        # Cardinalities = number of known categories per column\n+        self.cardinalities_ = [len(cats) for cats in self.encoder_.categories_]\n+\n+        return self\n+\n+    def transform(self, X):\n+        check_is_fitted(self, [\"encoder_\", \"cardinalities_\"])\n+\n+        X = pd.DataFrame(X)\n+        X_enc = self.encoder_.transform(X)\n+\n+        # Replace np.nan (unknown or missing) with cardinality value\n+        for col_idx, cardinality in enumerate(self.cardinalities_):\n+            mask = np.isnan(X_enc[:, col_idx])\n+            X_enc[mask, col_idx] = cardinality\n+\n+        return X_enc.astype(int)\n+\n+    def get_cardinalities(self):\n+        check_is_fitted(self, [\"cardinalities_\"])\n+        return self.cardinalities_\n+\n+\n+class TabMImplementation:\n+    def __init__(self, early_stopping_metric: Scorer, **config):\n+        self.config = config\n+        self.early_stopping_metric = early_stopping_metric\n+\n+        self.ord_enc_ = None\n+        self.num_prep_ = None\n+        self.cat_col_names_ = None\n+        self.n_classes_ = None\n+        self.task_type_ = None\n+        self.device_ = None\n+        self.has_num_cols = None\n+\n+    def fit(\n+        self,\n+        X_train: pd.DataFrame,\n+        y_train: pd.Series,\n+        X_val: pd.DataFrame,\n+        y_val: pd.Series,\n+        cat_col_names: list[Any],\n+        time_to_fit_in_seconds: float | None = None,\n+    ):\n+        start_time = time.time()\n+\n+        if X_val is None or len(X_val) == 0:\n+            raise ValueError(\"Training without validation set is currently not implemented\")\n+        seed: int | None = self.config.get(\"random_state\", None)\n+        if seed is not None:\n+            torch.manual_seed(seed)\n+            np.random.seed(seed)\n+            random.seed(seed)\n+        if \"n_threads\" in self.config:\n+            torch.set_num_threads(self.config[\"n_threads\"])\n+\n+        # -- Meta parameters\n+        problem_type = self.config[\"problem_type\"]\n+        task_type: TaskType = \"binclass\" if problem_type == \"binary\" else problem_type\n+        n_train = len(X_train)\n+        n_classes = None\n+        device = self.config[\"device\"]\n+        device = torch.device(device)\n+        self.task_type_ = task_type\n+        self.device_ = device\n+        self.cat_col_names_ = cat_col_names\n+\n+        # -- Hyperparameters\n+        arch_type = self.config.get(\"arch_type\", \"tabm-mini\")\n+        num_emb_type = self.config.get(\"num_emb_type\", \"pwl\")\n+        n_epochs = self.config.get(\"n_epochs\", 1_000_000_000)\n+        patience = self.config.get(\"patience\", 16)\n+        batch_size = self.config.get(\"batch_size\", \"auto\")\n+        compile_model = self.config.get(\"compile_model\", False)\n+        lr = self.config.get(\"lr\", 2e-3)\n+        d_embedding = self.config.get(\"d_embedding\", 16)\n+        d_block = self.config.get(\"d_block\", 512)\n+        dropout = self.config.get(\"dropout\", 0.1)\n+        tabm_k = self.config.get(\"tabm_k\", 32)\n+        allow_amp = self.config.get(\"allow_amp\", False)\n+        n_blocks = self.config.get(\"n_blocks\", \"auto\")\n+        num_emb_n_bins = self.config.get(\"num_emb_n_bins\", 48)\n+        eval_batch_size = self.config.get(\"eval_batch_size\", 1024)\n+        share_training_batches = self.config.get(\"share_training_batches\", False)\n+        weight_decay = self.config.get(\"weight_decay\", 3e-4)\n+        # this is the search space default but not the example default (which is 'none')\n+        gradient_clipping_norm = self.config.get(\"gradient_clipping_norm\", 1.0)\n+\n+        # -- Verify HPs\n+        num_emb_n_bins = min(num_emb_n_bins, n_train - 1)\n+        if n_train <= 2:\n+            num_emb_type = \"none\"  # there is no valid number of bins for piecewise linear embeddings\n+        if batch_size == \"auto\":\n+            batch_size = get_tabm_auto_batch_size(n_train=n_train)\n+\n+        # -- Preprocessing\n+        ds_parts = dict()\n+        self.ord_enc_ = (\n+            TabMOrdinalEncoder()\n+        )  # Unique ordinal encoder -> replaces nan and missing values with the cardinality\n+        self.ord_enc_.fit(X_train[self.cat_col_names_])\n+        # TODO: fix transformer to be able to work with empty input data like the sklearn default\n+        self.num_prep_ = Pipeline(steps=[(\"qt\", RTDLQuantileTransformer()), (\"imp\", SimpleImputer(add_indicator=True))])\n+        self.has_num_cols = bool(set(X_train.columns) - set(cat_col_names))\n+        for part, X, y in [(\"train\", X_train, y_train), (\"val\", X_val, y_val)]:\n+            tensors = dict()\n+\n+            tensors[\"x_cat\"] = torch.as_tensor(self.ord_enc_.transform(X[cat_col_names]), dtype=torch.long)\n+\n+            if self.has_num_cols:\n+                x_cont_np = X.drop(columns=cat_col_names).to_numpy(dtype=np.float32)\n+                if part == \"train\":\n+                    self.num_prep_.fit(x_cont_np)\n+                tensors[\"x_cont\"] = torch.as_tensor(self.num_prep_.transform(x_cont_np))\n+            else:\n+                tensors[\"x_cont\"] = torch.empty((len(X), 0), dtype=torch.float32)\n+\n+            if task_type == \"regression\":\n+                tensors[\"y\"] = torch.as_tensor(y.to_numpy(np.float32))\n+                if part == \"train\":\n+                    n_classes = 0\n+            else:\n+                tensors[\"y\"] = torch.as_tensor(y.to_numpy(np.int32), dtype=torch.long)\n+                if part == \"train\":\n+                    n_classes = tensors[\"y\"].max().item() + 1\n+\n+            ds_parts[part] = tensors\n+\n+        part_names = [\"train\", \"val\"]\n+        cat_cardinalities = self.ord_enc_.get_cardinalities()\n+        self.n_classes_ = n_classes\n+\n+        # filter out numerical columns with only a single value\n+        #  -> AG also does this already but preprocessing might create constant columns again\n+        x_cont_train = ds_parts[\"train\"][\"x_cont\"]\n+        self.num_col_mask_ = ~torch.all(x_cont_train == x_cont_train[0:1, :], dim=0)\n+        for part in part_names:\n+            ds_parts[part][\"x_cont\"] = ds_parts[part][\"x_cont\"][:, self.num_col_mask_]\n+            # tensor infos are not correct anymore, but might not be used either\n+        for part in part_names:\n+            for tens_name in ds_parts[part]:\n+                ds_parts[part][tens_name] = ds_parts[part][tens_name].to(device)\n+\n+        # update\n+        n_cont_features = ds_parts[\"train\"][\"x_cont\"].shape[1]\n+\n+        Y_train = ds_parts[\"train\"][\"y\"].clone()\n+        if task_type == \"regression\":\n+            self.y_mean_ = ds_parts[\"train\"][\"y\"].mean().item()\n+            self.y_std_ = ds_parts[\"train\"][\"y\"].std(correction=0).item()\n+\n+            Y_train = (Y_train - self.y_mean_) / (self.y_std_ + 1e-30)\n+\n+        # the | operator joins dicts (like update() but not in-place)\n+        data = {\n+            part: dict(x_cont=ds_parts[part][\"x_cont\"], y=ds_parts[part][\"y\"])\n+            | (dict(x_cat=ds_parts[part][\"x_cat\"]) if ds_parts[part][\"x_cat\"].shape[1] > 0 else dict())\n+            for part in part_names\n+        }\n+\n+        # adapted from https://github.com/yandex-research/tabm/blob/main/example.ipynb\n+\n+        # Automatic mixed precision (AMP)\n+        # torch.float16 is implemented for completeness,\n+        # but it was not tested in the project,\n+        # so torch.bfloat16 is used by default.\n+        amp_dtype = (\n+            torch.bfloat16\n+            if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n+            else torch.float16\n+            if torch.cuda.is_available()\n+            else None\n+        )\n+        # Changing False to True will result in faster training on compatible hardware.\n+        amp_enabled = allow_amp and amp_dtype is not None\n+        grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n+\n+        # fmt: off\n+        logger.log(15, f\"Device:        {device.type.upper()}\"\n+                    f\"\\nAMP:           {amp_enabled} (dtype: {amp_dtype})\"\n+                    f\"\\ntorch.compile: {compile_model}\",\n+                    )\n+        # fmt: on\n+\n+        bins = (\n+            None\n+            if num_emb_type != \"pwl\" or n_cont_features == 0\n+            else rtdl_num_embeddings.compute_bins(data[\"train\"][\"x_cont\"], n_bins=num_emb_n_bins)\n+        )\n+\n+        model = tabm_reference.Model(\n+            n_num_features=n_cont_features,\n+            cat_cardinalities=cat_cardinalities,\n+            n_classes=n_classes if n_classes > 0 else None,\n+            backbone={\n+                \"type\": \"MLP\",\n+                \"n_blocks\": n_blocks if n_blocks != \"auto\" else (3 if bins is None else 2),\n+                \"d_block\": d_block,\n+                \"dropout\": dropout,\n+            },\n+            bins=bins,\n+            num_embeddings=(\n+                None\n+                if bins is None\n+                else {\n+                    \"type\": \"PiecewiseLinearEmbeddings\",\n+                    \"d_embedding\": d_embedding,\n+                    \"activation\": False,\n+                    \"version\": \"B\",\n+                }\n+            ),\n+            arch_type=arch_type,\n+            k=tabm_k,\n+            share_training_batches=share_training_batches,\n+        ).to(device)\n+        optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=lr, weight_decay=weight_decay)\n+\n+        if compile_model:\n+            # NOTE\n+            # `torch.compile` is intentionally called without the `mode` argument\n+            # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n+            model = torch.compile(model)\n+            evaluation_mode = torch.no_grad\n+        else:\n+            evaluation_mode = torch.inference_mode\n+\n+        @torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n+        def apply_model(part: str, idx: torch.Tensor) -> torch.Tensor:\n+            return (\n+                model(\n+                    data[part][\"x_cont\"][idx],\n+                    data[part][\"x_cat\"][idx] if \"x_cat\" in data[part] else None,\n+                )\n+                .squeeze(-1)  # Remove the last dimension for regression tasks.\n+                .float()\n+            )\n+\n+        # TODO: use BCELoss for binary classification\n+        base_loss_fn = torch.nn.functional.mse_loss if task_type == \"regression\" else torch.nn.functional.cross_entropy\n+\n+        def loss_fn(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n+            # TabM produces k predictions per object. Each of them must be trained separately.\n+            # (regression)     y_pred.shape == (batch_size, k)\n+            # (classification) y_pred.shape == (batch_size, k, n_classes)\n+            k = y_pred.shape[1]\n+            return base_loss_fn(\n+                y_pred.flatten(0, 1),\n+                y_true.repeat_interleave(k) if model.share_training_batches else y_true,\n+            )\n+\n+        @evaluation_mode()\n+        def evaluate(part: str) -> float:\n+            model.eval()\n+\n+            # When using torch.compile, you may need to reduce the evaluation batch size.\n+            y_pred: np.ndarray = (\n+                torch.cat(\n+                    [\n+                        apply_model(part, idx)\n+                        for idx in torch.arange(len(data[part][\"y\"]), device=device).split(\n+                            eval_batch_size,\n+                        )\n+                    ],\n+                )\n+                .cpu()\n+                .numpy()\n+            )\n+            if task_type == \"regression\":\n+                # Transform the predictions back to the original label space.\n+                y_pred = y_pred * self.y_std_ + self.y_mean_\n+\n+            # Compute the mean of the k predictions.\n+            average_logits = self.config.get(\"average_logits\", False)\n+            if average_logits:\n+                y_pred = y_pred.mean(1)\n+            if task_type != \"regression\":\n+                # For classification, the mean must be computed in the probability space.\n+                y_pred = scipy.special.softmax(y_pred, axis=-1)\n+            if not average_logits:\n+                y_pred = y_pred.mean(1)\n+\n+            return compute_metric(\n+                y=data[part][\"y\"].cpu().numpy(),\n+                metric=self.early_stopping_metric,\n+                y_pred=y_pred if task_type == \"regression\" else y_pred.argmax(1),\n+                y_pred_proba=y_pred[:, 1] if task_type == \"binclass\" else y_pred,\n+                silent=True,\n+            )\n+\n+        math.ceil(n_train / batch_size)\n+        best = {\n+            \"val\": -math.inf,\n+            # 'test': -math.inf,\n+            \"epoch\": -1,\n+        }\n+        best_params = [p.clone() for p in model.parameters()]\n+        # Early stopping: the training stops when\n+        # there are more than `patience` consecutive bad updates.\n+        remaining_patience = patience\n+\n+        try:\n+            if self.config.get(\"verbosity\", 0) >= 1:\n+                from tqdm.std import tqdm\n+            else:\n+                tqdm = lambda arr, desc: arr\n+        except ImportError:\n+            tqdm = lambda arr, desc: arr\n+\n+        logger.log(15, \"-\" * 88 + \"\\n\")\n+        for epoch in range(n_epochs):\n+            # check time limit\n+            if epoch > 0 and time_to_fit_in_seconds is not None:\n+                pred_time_after_next_epoch = (epoch + 1) / epoch * (time.time() - start_time)\n+                if pred_time_after_next_epoch >= time_to_fit_in_seconds:\n+                    break\n+\n+            batches = (\n+                torch.randperm(n_train, device=device).split(batch_size)\n+                if model.share_training_batches\n+                else [\n+                    x.transpose(0, 1).flatten()\n+                    for x in torch.rand((model.k, n_train), device=device).argsort(dim=1).split(batch_size, dim=1)\n+                ]\n+            )\n+\n+            for batch_idx in tqdm(batches, desc=f\"Epoch {epoch}\"):\n+                model.train()\n+                optimizer.zero_grad()\n+                loss = loss_fn(apply_model(\"train\", batch_idx), Y_train[batch_idx])\n+\n+                # added from https://github.com/yandex-research/tabm/blob/main/bin/model.py\n+                if gradient_clipping_norm is not None and gradient_clipping_norm != \"none\":\n+                    if grad_scaler is not None:\n+                        grad_scaler.unscale_(optimizer)\n+                    torch.nn.utils.clip_grad.clip_grad_norm_(\n+                        model.parameters(),\n+                        gradient_clipping_norm,\n+                    )\n+\n+                if grad_scaler is None:\n+                    loss.backward()\n+                    optimizer.step()\n+                else:\n+                    grad_scaler.scale(loss).backward()  # type: ignore\n+                    grad_scaler.step(optimizer)  # Ignores grad scaler might skip steps; should not break anything\n+                    grad_scaler.update()\n+\n+            val_score = evaluate(\"val\")\n+            logger.log(15, f\"(val) {val_score:.4f}\")\n+\n+            if val_score > best[\"val\"]:\n+                logger.log(15, \"\ud83c\udf38 New best epoch! \ud83c\udf38\")\n+                # best = {'val': val_score, 'test': test_score, 'epoch': epoch}\n+                best = {\"val\": val_score, \"epoch\": epoch}\n+                remaining_patience = patience\n+                with torch.no_grad():\n+                    for bp, p in zip(best_params, model.parameters(), strict=False):\n+                        bp.copy_(p)\n+            else:\n+                remaining_patience -= 1\n+\n+            if remaining_patience < 0:\n+                break\n+\n+        logger.log(15, \"\\n\\nResult:\")\n+        logger.log(15, str(best))\n+\n+        logger.log(15, \"Restoring best model\")\n+        with torch.no_grad():\n+            for bp, p in zip(best_params, model.parameters(), strict=False):\n+                p.copy_(bp)\n+\n+        self.model_ = model\n+\n+    def predict_raw(self, X: pd.DataFrame) -> torch.Tensor:\n+        self.model_.eval()\n+\n+        tensors = dict()\n+        tensors[\"x_cat\"] = torch.as_tensor(self.ord_enc_.transform(X[self.cat_col_names_]), dtype=torch.long).to(\n+            self.device_,\n+        )\n+        tensors[\"x_cont\"] = torch.as_tensor(\n+            self.num_prep_.transform(X.drop(columns=X[self.cat_col_names_]).to_numpy(dtype=np.float32))\n+            if self.has_num_cols\n+            else np.empty((len(X), 0), dtype=np.float32),\n+        ).to(self.device_)\n+\n+        tensors[\"x_cont\"] = tensors[\"x_cont\"][:, self.num_col_mask_]\n+\n+        eval_batch_size = self.config.get(\"eval_batch_size\", 1024)\n+        with torch.no_grad():\n+            y_pred: torch.Tensor = torch.cat(\n+                [\n+                    self.model_(\n+                        tensors[\"x_cont\"][idx],\n+                        tensors[\"x_cat\"][idx] if tensors[\"x_cat\"].numel() != 0 else None,\n+                    )\n+                    .squeeze(-1)  # Remove the last dimension for regression tasks.\n+                    .float()\n+                    for idx in torch.arange(tensors[\"x_cont\"].shape[0], device=self.device_).split(\n+                        eval_batch_size,\n+                    )\n+                ],\n+            )\n+        if self.task_type_ == \"regression\":\n+            # Transform the predictions back to the original label space.\n+            y_pred = y_pred * self.y_std_ + self.y_mean_\n+            y_pred = y_pred.mean(1)\n+            # y_pred = y_pred.unsqueeze(-1)  # add extra \"features\" dimension\n+        else:\n+            average_logits = self.config.get(\"average_logits\", False)\n+            if average_logits:\n+                y_pred = y_pred.mean(1)\n+            else:\n+                # For classification, the mean must be computed in the probability space.\n+                y_pred = torch.log(torch.softmax(y_pred, dim=-1).mean(1) + 1e-30)\n+\n+        return y_pred.cpu()\n+\n+    def predict(self, X: pd.DataFrame) -> np.ndarray:\n+        y_pred = self.predict_raw(X)\n+        if self.task_type_ == \"regression\":\n+            return y_pred.numpy()\n+        return y_pred.argmax(dim=-1).numpy()\n+\n+    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n+        probas = torch.softmax(self.predict_raw(X), dim=-1).numpy()\n+        if probas.shape[1] == 2:\n+            probas = probas[:, 1]\n+        return probas\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/models/tabm/rtdl_num_embeddings.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/models/tabm/rtdl_num_embeddings.py b/tabular/src/autogluon/tabular/models/tabm/rtdl_num_embeddings.py\nnew file mode 100644\nindex 0000000..2b57003\n--- /dev/null\n+++ b/tabular/src/autogluon/tabular/models/tabm/rtdl_num_embeddings.py\n@@ -0,0 +1,807 @@\n+# taken from https://github.com/yandex-research/rtdl-num-embeddings/blob/main/package/rtdl_num_embeddings.py\n+\"\"\"On Embeddings for Numerical Features in Tabular Deep Learning.\"\"\"\n+\n+__version__ = '0.0.12'\n+\n+__all__ = [\n+    'LinearEmbeddings',\n+    'LinearReLUEmbeddings',\n+    'PeriodicEmbeddings',\n+    'PiecewiseLinearEmbeddings',\n+    'PiecewiseLinearEncoding',\n+    'compute_bins',\n+]\n+\n+import math\n+import warnings\n+from typing import Any, Literal, Optional, Union\n+\n+try:\n+    import sklearn.tree as sklearn_tree\n+except ImportError:\n+    sklearn_tree = None\n+\n+import torch\n+import torch.nn as nn\n+from torch import Tensor\n+from torch.nn.parameter import Parameter\n+\n+try:\n+    from tqdm import tqdm\n+except ImportError:\n+    tqdm = None\n+\n+\n+def _check_input_shape(x: Tensor, expected_n_features: int) -> None:\n+    if x.ndim < 1:\n+        raise ValueError(\n+            f'The input must have at least one dimension, however: {x.ndim=}'\n+        )\n+    if x.shape[-1] != expected_n_features:\n+        raise ValueError(\n+            'The last dimension of the input was expected to be'\n+            f' {expected_n_features}, however, {x.shape[-1]=}'\n+        )\n+\n+\n+class LinearEmbeddings(nn.Module):\n+    \"\"\"Linear embeddings for continuous features.\n+\n+    **Shape**\n+\n+    - Input: `(*, n_features)`\n+    - Output: `(*, n_features, d_embedding)`\n+\n+    **Examples**\n+\n+    >>> batch_size = 2\n+    >>> n_cont_features = 3\n+    >>> x = torch.randn(batch_size, n_cont_features)\n+    >>> d_embedding = 4\n+    >>> m = LinearEmbeddings(n_cont_features, d_embedding)\n+    >>> m.get_output_shape()\n+    torch.Size([3, 4])\n+    >>> m(x).shape\n+    torch.Size([2, 3, 4])\n+    \"\"\"\n+\n+    def __init__(self, n_features: int, d_embedding: int) -> None:\n+        \"\"\"\n+        Args:\n+            n_features: the number of continuous features.\n+            d_embedding: the embedding size.\n+        \"\"\"\n+        if n_features <= 0:\n+            raise ValueError(f'n_features must be positive, however: {n_features=}')\n+        if d_embedding <= 0:\n+            raise ValueError(f'd_embedding must be positive, however: {d_embedding=}')\n+\n+        super().__init__()\n+        self.weight = Parameter(torch.empty(n_features, d_embedding))\n+        self.bias = Parameter(torch.empty(n_features, d_embedding))\n+        self.reset_parameters()\n+\n+    def reset_parameters(self) -> None:\n+        d_rqsrt = self.weight.shape[1] ** -0.5\n+        nn.init.uniform_(self.weight, -d_rqsrt, d_rqsrt)\n+        nn.init.uniform_(self.bias, -d_rqsrt, d_rqsrt)\n+\n+    def get_output_shape(self) -> torch.Size:\n+        \"\"\"Get the output shape without the batch dimensions.\"\"\"\n+        return self.weight.shape\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        _check_input_shape(x, self.weight.shape[0])\n+        return torch.addcmul(self.bias, self.weight, x[..., None])\n+\n+\n+class LinearReLUEmbeddings(nn.Module):\n+    \"\"\"Simple non-linear embeddings for continuous features.\n+\n+    **Shape**\n+\n+    - Input: `(*, n_features)`\n+    - Output: `(*, n_features, d_embedding)`\n+\n+    **Examples**\n+\n+    >>> batch_size = 2\n+    >>> n_cont_features = 3\n+    >>> x = torch.randn(batch_size, n_cont_features)\n+    >>>\n+    >>> d_embedding = 32\n+    >>> m = LinearReLUEmbeddings(n_cont_features, d_embedding)\n+    >>> m.get_output_shape()\n+    torch.Size([3, 32])\n+    >>> m(x).shape\n+    torch.Size([2, 3, 32])\n+    \"\"\"\n+\n+    def __init__(self, n_features: int, d_embedding: int = 32) -> None:\n+        \"\"\"\n+        Args:\n+            n_features: the number of continuous features.\n+            d_embedding: the embedding size.\n+        \"\"\"\n+        super().__init__()\n+        self.linear = LinearEmbeddings(n_features, d_embedding)\n+        self.activation = nn.ReLU()\n+\n+    def get_output_shape(self) -> torch.Size:\n+        \"\"\"Get the output shape without the batch dimensions.\"\"\"\n+        return self.linear.weight.shape\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        x = self.linear(x)\n+        x = self.activation(x)\n+        return x\n+\n+\n+class _Periodic(nn.Module):\n+    \"\"\"\n+    NOTE: THIS MODULE SHOULD NOT BE USED DIRECTLY.\n+\n+    Technically, this is a linear embedding without bias followed by\n+    the periodic activations. The scale of the initialization\n+    (defined by the `sigma` argument) plays an important role.\n+    \"\"\"\n+\n+    def __init__(self, n_features: int, k: int, sigma: float) -> None:\n+        if sigma <= 0.0:\n+            raise ValueError(f'sigma must be positive, however: {sigma=}')\n+\n+        super().__init__()\n+        self._sigma = sigma\n+        self.weight = Parameter(torch.empty(n_features, k))\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        \"\"\"Reset the parameters.\"\"\"\n+        # NOTE[DIFF]\n+        # Here, extreme values (~0.3% probability) are explicitly avoided just in case.\n+        # In the paper, there was no protection from extreme values.\n+        bound = self._sigma * 3\n+        nn.init.trunc_normal_(self.weight, 0.0, self._sigma, a=-bound, b=bound)\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        _check_input_shape(x, self.weight.shape[0])\n+        x = 2 * math.pi * self.weight * x[..., None]\n+        x = torch.cat([torch.cos(x), torch.sin(x)], -1)\n+        return x\n+\n+\n+# _NLinear is a simplified copy of delu.nn.NLinear:\n+# https://yura52.github.io/delu/stable/api/generated/delu.nn.NLinear.html\n+class _NLinear(nn.Module):\n+    \"\"\"N *separate* linear layers for N feature embeddings.\n+\n+    In other words,\n+    each feature embedding is transformed by its own dedicated linear layer.\n+    \"\"\"\n+\n+    def __init__(\n+        self, n: int, in_features: int, out_features: int, bias: bool = True\n+    ) -> None:\n+        super().__init__()\n+        self.weight = Parameter(torch.empty(n, in_features, out_features))\n+        self.bias = Parameter(torch.empty(n, out_features)) if bias else None\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        \"\"\"Reset the parameters.\"\"\"\n+        d_in_rsqrt = self.weight.shape[-2] ** -0.5\n+        nn.init.uniform_(self.weight, -d_in_rsqrt, d_in_rsqrt)\n+        if self.bias is not None:\n+            nn.init.uniform_(self.bias, -d_in_rsqrt, d_in_rsqrt)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        if x.ndim != 3:\n+            raise ValueError(\n+                '_NLinear supports only inputs with exactly one batch dimension,'\n+                ' so `x` must have a shape like (BATCH_SIZE, N_FEATURES, D_EMBEDDING).'\n+            )\n+        assert x.shape[-(self.weight.ndim - 1) :] == self.weight.shape[:-1]\n+\n+        x = x.transpose(0, 1)\n+        x = x @ self.weight\n+        x = x.transpose(0, 1)\n+        if self.bias is not None:\n+            x = x + self.bias\n+        return x\n+\n+\n+class PeriodicEmbeddings(nn.Module):\n+    \"\"\"Embeddings for continuous features based on periodic activations.\n+\n+    See README for details.\n+\n+    **Shape**\n+\n+    - Input: `(*, n_features)`\n+    - Output: `(*, n_features, d_embedding)`\n+\n+    **Examples**\n+\n+    >>> batch_size = 2\n+    >>> n_cont_features = 3\n+    >>> x = torch.randn(batch_size, n_cont_features)\n+    >>>\n+    >>> d_embedding = 24\n+    >>> m = PeriodicEmbeddings(n_cont_features, d_embedding, lite=False)\n+    >>> m.get_output_shape()\n+    torch.Size([3, 24])\n+    >>> m(x).shape\n+    torch.Size([2, 3, 24])\n+    >>>\n+    >>> m = PeriodicEmbeddings(n_cont_features, d_embedding, lite=True)\n+    >>> m.get_output_shape()\n+    torch.Size([3, 24])\n+    >>> m(x).shape\n+    torch.Size([2, 3, 24])\n+    >>>\n+    >>> # PL embeddings.\n+    >>> m = PeriodicEmbeddings(n_cont_features, d_embedding=8, activation=False, lite=False)\n+    >>> m.get_output_shape()\n+    torch.Size([3, 8])\n+    >>> m(x).shape\n+    torch.Size([2, 3, 8])\n+    \"\"\"  # noqa: E501\n+\n+    def __init__(\n+        self,\n+        n_features: int,\n+        d_embedding: int = 24,\n+        *,\n+        n_frequencies: int = 48,\n+        frequency_init_scale: float = 0.01,\n+        activation: bool = True,\n+        lite: bool,\n+    ) -> None:\n+        \"\"\"\n+        Args:\n+            n_features: the number of features.\n+            d_embedding: the embedding size.\n+            n_frequencies: the number of frequencies for each feature.\n+                (denoted as \"k\" in Section 3.3 in the paper).\n+            frequency_init_scale: the initialization scale for the first linear layer\n+                (denoted as \"sigma\" in Section 3.3 in the paper).\n+                **This is an important hyperparameter**, see README for details.\n+            activation: if `False`, the ReLU activation is not applied.\n+                Must be `True` if ``lite=True``.\n+            lite: if True, the outer linear layer is shared between all features.\n+                See README for details.\n+        \"\"\"\n+        super().__init__()\n+        self.periodic = _Periodic(n_features, n_frequencies, frequency_init_scale)\n+        self.linear: Union[nn.Linear, _NLinear]\n+        if lite:\n+            # NOTE[DIFF]\n+            # The lite variation was introduced in a different paper\n+            # (about the TabR model).\n+            if not activation:\n+                raise ValueError('lite=True is allowed only when activation=True')\n+            self.linear = nn.Linear(2 * n_frequencies, d_embedding)\n+        else:\n+            self.linear = _NLinear(n_features, 2 * n_frequencies, d_embedding)\n+        self.activation = nn.ReLU() if activation else None\n+\n+    def get_output_shape(self) -> torch.Size:\n+        \"\"\"Get the output shape without the batch dimensions.\"\"\"\n+        n_features = self.periodic.weight.shape[0]\n+        d_embedding = (\n+            self.linear.weight.shape[0]\n+            if isinstance(self.linear, nn.Linear)\n+            else self.linear.weight.shape[-1]\n+        )\n+        return torch.Size((n_features, d_embedding))\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        x = self.periodic(x)\n+        x = self.linear(x)\n+        if self.activation is not None:\n+            x = self.activation(x)\n+        return x\n+\n+\n+def _check_bins(bins: list[Tensor]) -> None:\n+    if not bins:\n+        raise ValueError('The list of bins must not be empty')\n+    for i, feature_bins in enumerate(bins):\n+        if not isinstance(feature_bins, Tensor):\n+            raise ValueError(\n+                'bins must be a list of PyTorch tensors. '\n+                f'However, for {i=}: {type(bins[i])=}'\n+            )\n+        if feature_bins.ndim != 1:\n+            raise ValueError(\n+                'Each item of the bin list must have exactly one dimension.'\n+                f' However, for {i=}: {bins[i].ndim=}'\n+            )\n+        if len(feature_bins) < 2:\n+            raise ValueError(\n+                'All features must have at least two bin edges.'\n+                f' However, for {i=}: {len(bins[i])=}'\n+            )\n+        if not feature_bins.isfinite().all():\n+            raise ValueError(\n+                'Bin edges must not contain nan/inf/-inf.'\n+                f' However, this is not true for the {i}-th feature'\n+            )\n+        if (feature_bins[:-1] >= feature_bins[1:]).any():\n+            raise ValueError(\n+                'Bin edges must be sorted.'\n+                f' However, the for the {i}-th feature, the bin edges are not sorted'\n+            )\n+        # Commented out due to spaming warnings.\n+        # if len(feature_bins) == 2:\n+        #     warnings.warn(\n+        #         f'The {i}-th feature has just two bin edges, which means only one bin.'\n+        #         ' Strictly speaking, using a single bin for the'\n+        #         ' piecewise-linear encoding should not break anything,'\n+        #         ' but it is the same as using sklearn.preprocessing.MinMaxScaler'\n+        #     )\n+\n+\n+def compute_bins(\n+    X: torch.Tensor,\n+    n_bins: int = 48,\n+    *,\n+    tree_kwargs: Optional[dict[str, Any]] = None,\n+    y: Optional[Tensor] = None,\n+    regression: Optional[bool] = None,\n+    verbose: bool = False,\n+) -> list[Tensor]:\n+    \"\"\"Compute the bin boundaries for `PiecewiseLinearEncoding` and `PiecewiseLinearEmbeddings`.\n+\n+    **Usage**\n+\n+    Compute bins using quantiles (Section 3.2.1 in the paper):\n+\n+    >>> X_train = torch.randn(10000, 2)\n+    >>> bins = compute_bins(X_train)\n+\n+    Compute bins using decision trees (Section 3.2.2 in the paper):\n+\n+    >>> X_train = torch.randn(10000, 2)\n+    >>> y_train = torch.randn(len(X_train))\n+    >>> bins = compute_bins(\n+    ...     X_train,\n+    ...     y=y_train,\n+    ...     regression=True,\n+    ...     tree_kwargs={'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4},\n+    ... )\n+\n+    Args:\n+        X: the training features.\n+        n_bins: the number of bins.\n+        tree_kwargs: keyword arguments for `sklearn.tree.DecisionTreeRegressor`\n+            (if ``regression=True``) or `sklearn.tree.DecisionTreeClassifier`\n+            (if ``regression=False``).\n+            NOTE: requires ``scikit-learn>=1.0,>2`` to be installed.\n+        y: the training labels (must be provided if ``tree`` is not None).\n+        regression: whether the labels are regression labels\n+            (must be provided if ``tree`` is not None).\n+        verbose: if True and ``tree_kwargs`` is not None, than ``tqdm``\n+            (must be installed) will report the progress while fitting trees.\n+\n+    Returns:\n+        A list of bin edges for all features. For one feature:\n+\n+        - the maximum possible number of bin edges is ``n_bins + 1``.\n+        - the minimum possible number of bin edges is ``1``.\n+    \"\"\"  # noqa: E501\n+    if not isinstance(X, Tensor):\n+        raise ValueError(f'X must be a PyTorch tensor, however: {type(X)=}')\n+    if X.ndim != 2:\n+        raise ValueError(f'X must have exactly two dimensions, however: {X.ndim=}')\n+    if X.shape[0] < 2:\n+        raise ValueError(f'X must have at least two rows, however: {X.shape[0]=}')\n+    if X.shape[1] < 1:\n+        raise ValueError(f'X must have at least one column, however: {X.shape[1]=}')\n+    if not X.isfinite().all():\n+        raise ValueError('X must not contain nan/inf/-inf.')\n+    if (X == X[0]).all(dim=0).any():\n+        raise ValueError(\n+            'All columns of X must have at least two distinct values.'\n+            ' However, X contains columns with just one distinct value.'\n+        )\n+    if n_bins <= 1 or n_bins >= len(X):\n+        raise ValueError(\n+            'n_bins must be more than 1, but less than len(X), however:'\n+            f' {n_bins=}, {len(X)=}'\n+        )\n+\n+    if tree_kwargs is None:\n+        if y is not None or regression is not None or verbose:\n+            raise ValueError(\n+                'If tree_kwargs is None, then y must be None, regression must be None'\n+                ' and verbose must be False'\n+            )\n+\n+        _upper = 2**24  # 16_777_216\n+        if len(X) > _upper:\n+            warnings.warn(\n+                f'Computing quantile-based bins for more than {_upper} million objects'\n+                ' may not be possible due to the limitation of PyTorch'\n+                ' (for details, see https://github.com/pytorch/pytorch/issues/64947;'\n+                ' if that issue is successfully resolved, this warning may be irrelevant).'  # noqa\n+                ' As a workaround, subsample the data, i.e. instead of'\n+                '\\ncompute_bins(X, ...)'\n+                '\\ndo'\n+                '\\ncompute_bins(X[torch.randperm(len(X), device=X.device)[:16_777_216]], ...)'  # noqa\n+                '\\nOn CUDA, the computation can still fail with OOM even after'\n+                ' subsampling. If this is the case, try passing features by groups:'\n+                '\\nbins = sum('\n+                '\\n    compute_bins(X[:, idx], ...)'\n+                '\\n    for idx in torch.arange(len(X), device=X.device).split(group_size),'  # noqa\n+                '\\n    start=[]'\n+                '\\n)'\n+                '\\nAnother option is to perform the computation on CPU:'\n+                '\\ncompute_bins(X.cpu(), ...)'\n+            )\n+        del _upper\n+\n+        # NOTE[DIFF]\n+        # The code below is more correct than the original implementation,\n+        # because the original implementation contains an unintentional divergence\n+        # from what is written in the paper. That divergence affected only the\n+        # quantile-based embeddings, but not the tree-based embeddings.\n+        # For historical reference, here is the original, less correct, implementation:\n+        # https://github.com/yandex-research/tabular-dl-num-embeddings/blob/c1d9eb63c0685b51d7e1bc081cdce6ffdb8886a8/bin/train4.py#L612C30-L612C30\n+        # (explanation: limiting the number of quantiles by the number of distinct\n+        #  values is NOT the same as removing identical quantiles after computing them).\n+        bins = [\n+            q.unique()\n+            for q in torch.quantile(\n+                X, torch.linspace(0.0, 1.0, n_bins + 1).to(X), dim=0\n+            ).T\n+        ]\n+        _check_bins(bins)\n+        return bins\n+\n+    else:\n+        if sklearn_tree is None:\n+            raise RuntimeError(\n+                'The scikit-learn package is missing.'\n+                ' See README.md for installation instructions'\n+            )\n+        if y is None or regression is None:\n+            raise ValueError(\n+                'If tree_kwargs is not None, then y and regression must not be None'\n+            )\n+        if y.ndim != 1:\n+            raise ValueError(f'y must have exactly one dimension, however: {y.ndim=}')\n+        if len(y) != len(X):\n+            raise ValueError(\n+                f'len(y) must be equal to len(X), however: {len(y)=}, {len(X)=}'\n+            )\n+        if y is None or regression is None:\n+            raise ValueError(\n+                'If tree_kwargs is not None, then y and regression must not be None'\n+            )\n+        if 'max_leaf_nodes' in tree_kwargs:\n+            raise ValueError(\n+                'tree_kwargs must not contain the key \"max_leaf_nodes\"'\n+                ' (it will be set to n_bins automatically).'\n+            )\n+\n+        if verbose:\n+            if tqdm is None:\n+                raise ImportError('If verbose is True, tqdm must be installed')\n+            tqdm_ = tqdm\n+        else:\n+            tqdm_ = lambda x: x  # noqa: E731\n+\n+        if X.device.type != 'cpu' or y.device.type != 'cpu':\n+            warnings.warn(\n+                'Computing tree-based bins involves the conversion of the input PyTorch'\n+                ' tensors to NumPy arrays. The provided PyTorch tensors are not'\n+                ' located on CPU, so the conversion has some overhead.',\n+                UserWarning,\n+            )\n+        X_numpy = X.cpu().numpy()\n+        y_numpy = y.cpu().numpy()\n+        bins = []\n+        for column in tqdm_(X_numpy.T):\n+            feature_bin_edges = [float(column.min()), float(column.max())]\n+            tree = (\n+                (\n+                    sklearn_tree.DecisionTreeRegressor\n+                    if regression\n+                    else sklearn_tree.DecisionTreeClassifier\n+                )(max_leaf_nodes=n_bins, **tree_kwargs)\n+                .fit(column.reshape(-1, 1), y_numpy)\n+                .tree_\n+            )\n+            for node_id in range(tree.node_count):\n+                # The following condition is True only for split nodes. Source:\n+                # https://scikit-learn.org/1.0/auto_examples/tree/plot_unveil_tree_structure.html#tree-structure\n+                if tree.children_left[node_id] != tree.children_right[node_id]:\n+                    feature_bin_edges.append(float(tree.threshold[node_id]))\n+            bins.append(torch.as_tensor(feature_bin_edges).unique())\n+        _check_bins(bins)\n+        return [x.to(device=X.device, dtype=X.dtype) for x in bins]\n+\n+\n+class _PiecewiseLinearEncodingImpl(nn.Module):\n+    \"\"\"Piecewise-linear encoding.\n+\n+    NOTE: THIS CLASS SHOULD NOT BE USED DIRECTLY.\n+    In particular, this class does *not* add any positional information\n+    to feature encodings. Thus, for Transformer-like models,\n+    `PiecewiseLinearEmbeddings` is the only valid option.\n+\n+    Note:\n+        This is the *encoding* module, not the *embedding* module,\n+        so it only implements Equation 1 (Figure 1) from the paper,\n+        and does not have trainable parameters.\n+\n+    **Shape**\n+\n+    * Input: ``(*, n_features)``\n+    * Output: ``(*, n_features, max_n_bins)``,\n+      where ``max_n_bins`` is the maximum number of bins over all features:\n+      ``max_n_bins = max(len(b) - 1 for b in bins)``.\n+\n+    To understand the output structure,\n+    consider a feature with the number of bins ``n_bins``.\n+    Formally, its piecewise-linear encoding is a vector of the size ``n_bins``\n+    that looks as follows::\n+\n+        x_ple = [1, ..., 1, (x - this_bin_left_edge) / this_bin_width, 0, ..., 0]\n+\n+    However, this class will instead produce a vector of the size ``max_n_bins``::\n+\n+        x_ple_actual = [*x_ple[:-1], *zeros(max_n_bins - n_bins), x_ple[-1]]\n+\n+    In other words:\n+\n+    * The last encoding component is **always** located in the end,\n+      even if ``n_bins == 1`` (i.e. even if it is the only component).\n+    * The leading ``n_bins - 1`` components are located in the beginning.\n+    * Everything in-between is always set to zeros (like \"padding\", but in the middle).\n+\n+    This implementation is *significantly* faster than the original one.\n+    It relies on two key observations:\n+\n+    * The piecewise-linear encoding is just\n+      a non-trainable linear transformation followed by a clamp-based activation.\n+      Pseudocode: `PiecewiseLinearEncoding(x) = Activation(Linear(x))`.\n+      The parameters of the linear transformation are defined by the bin edges.\n+    * Aligning the *last* encoding channel across all features\n+      allows applying the aforementioned activation simultaneously to all features\n+      without the loop over features.\n+    \"\"\"\n+\n+    weight: Tensor\n+    \"\"\"The weight of the linear transformation mentioned in the class docstring.\"\"\"\n+\n+    bias: Tensor\n+    \"\"\"The bias of the linear transformation mentioned in the class docstring.\"\"\"\n+\n+    single_bin_mask: Optional[Tensor]\n+    \"\"\"The indicators of the features with only one bin.\"\"\"\n+\n+    mask: Optional[Tensor]\n+    \"\"\"The indicators of the \"valid\" (i.e. \"non-padding\") part of the encoding.\"\"\"\n+\n+    def __init__(self, bins: list[Tensor]) -> None:\n+        \"\"\"\n+        Args:\n+            bins: the bins computed by `compute_bins`.\n+        \"\"\"\n+        assert len(bins) > 0\n+        super().__init__()\n+\n+        n_features = len(bins)\n+        n_bins = [len(x) - 1 for x in bins]\n+        max_n_bins = max(n_bins)\n+\n+        self.register_buffer('weight', torch.zeros(n_features, max_n_bins))\n+        self.register_buffer('bias', torch.zeros(n_features, max_n_bins))\n+\n+        single_bin_mask = torch.tensor(n_bins) == 1\n+        self.register_buffer(\n+            'single_bin_mask', single_bin_mask if single_bin_mask.any() else None\n+        )\n+\n+        self.register_buffer(\n+            'mask',\n+            # The mask is needed if features have different number of bins.\n+            None\n+            if all(len(x) == len(bins[0]) for x in bins)\n+            else torch.row_stack(\n+                [\n+                    torch.cat(\n+                        [\n+                            # The number of bins for this feature, minus 1:\n+                            torch.ones((len(x) - 1) - 1, dtype=torch.bool),\n+                            # Unused components (always zeros):\n+                            torch.zeros(max_n_bins - (len(x) - 1), dtype=torch.bool),\n+                            # The last bin:\n+                            torch.ones(1, dtype=torch.bool),\n+                        ]\n+                    )\n+                    # x is a tensor containing the bin bounds for a given feature.\n+                    for x in bins\n+                ]\n+            ),\n+        )\n+\n+        for i, bin_edges in enumerate(bins):\n+            # Formally, the piecewise-linear encoding of one feature looks as follows:\n+            # `[1, ..., 1, (x - this_bin_left_edge) / this_bin_width, 0, ..., 0]`\n+            # The linear transformation based on the weight and bias defined below\n+            # implements the expression in the middle before the clipping to [0, 1].\n+            # Note that the actual encoding layout produced by this class\n+            # is slightly different. See the docstring of this class for details.\n+            bin_width = bin_edges.diff()\n+            w = 1.0 / bin_width\n+            b = -bin_edges[:-1] / bin_width\n+            # The last encoding component:\n+            self.weight[i, -1] = w[-1]\n+            self.bias[i, -1] = b[-1]\n+            # The leading encoding components:\n+            self.weight[i, : n_bins[i] - 1] = w[:-1]\n+            self.bias[i, : n_bins[i] - 1] = b[:-1]\n+            # All in-between components will always be zeros,\n+            # because the weight and bias are initialized with zeros.\n+\n+    def get_max_n_bins(self) -> int:\n+        return self.weight.shape[-1]\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        x = torch.addcmul(self.bias, self.weight, x[..., None])\n+        if x.shape[-1] > 1:\n+            x = torch.cat(\n+                [\n+                    x[..., :1].clamp_max(1.0),\n+                    x[..., 1:-1].clamp(0.0, 1.0),\n+                    (\n+                        x[..., -1:].clamp_min(0.0)\n+                        if self.single_bin_mask is None\n+                        else torch.where(\n+                            # For features with only one bin,\n+                            # the whole \"piecewise-linear\" encoding effectively behaves\n+                            # like mix-max scaling\n+                            # (assuming that the edges of the single bin\n+                            #  are the minimum and maximum feature values).\n+                            self.single_bin_mask[..., None],\n+                            x[..., -1:],\n+                            x[..., -1:].clamp_min(0.0),\n+                        )\n+                    ),\n+                ],\n+                dim=-1,\n+            )\n+        return x\n+\n+\n+class PiecewiseLinearEncoding(nn.Module):\n+    \"\"\"Piecewise-linear encoding.\n+\n+    See README for detailed explanation.\n+\n+    **Shape**\n+\n+    - Input: ``(*, n_features)``\n+    - Output: ``(*, total_n_bins)``,\n+      where ``total_n_bins`` is the total number of bins for all features:\n+      ``total_n_bins = sum(len(b) - 1 for b in bins)``.\n+\n+    Technically, the output of this module is the flattened output\n+    of `_PiecewiseLinearEncoding` with all \"padding\" values removed.\n+    \"\"\"\n+\n+    def __init__(self, bins: list[Tensor]) -> None:\n+        \"\"\"\n+        Args:\n+            bins: the bins computed by `compute_bins`.\n+        \"\"\"\n+        super().__init__()\n+        self.impl = _PiecewiseLinearEncodingImpl(bins)\n+\n+    def get_output_shape(self) -> torch.Size:\n+        \"\"\"Get the output shape without the batch dimensions.\"\"\"\n+        total_n_bins = (\n+            self.impl.weight.shape.numel()\n+            if self.impl.mask is None\n+            else int(self.impl.mask.long().sum().cpu().item())\n+        )\n+        return torch.Size((total_n_bins,))\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        x = self.impl(x)\n+        return x.flatten(-2) if self.impl.mask is None else x[:, self.impl.mask]\n+\n+\n+class PiecewiseLinearEmbeddings(nn.Module):\n+    \"\"\"Piecewise-linear embeddings.\n+\n+    **Shape**\n+\n+    - Input: ``(batch_size, n_features)``\n+    - Output: ``(batch_size, n_features, d_embedding)``\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        bins: list[Tensor],\n+        d_embedding: int,\n+        *,\n+        activation: bool,\n+        version: Literal[None, 'A', 'B'] = None,\n+    ) -> None:\n+        \"\"\"\n+        Args:\n+            bins: the bins computed by `compute_bins`.\n+            d_embedding: the embedding size.\n+            activation: if True, the ReLU activation is additionally applied in the end.\n+            version: the preset for various implementation details, such as\n+                parametrization and initialization. See README for details.\n+        \"\"\"\n+        if d_embedding <= 0:\n+            raise ValueError(\n+                f'd_embedding must be a positive integer, however: {d_embedding=}'\n+            )\n+        _check_bins(bins)\n+        if version is None:\n+            warnings.warn(\n+                'The `version` argument is not provided, so version=\"A\" will be used'\n+                ' for backward compatibility.'\n+                ' See README for recommendations regarding `version`.'\n+                ' In future, omitting this argument will result in an exception.'\n+            )\n+            version = 'A'\n+\n+        super().__init__()\n+        n_features = len(bins)\n+        # NOTE[DIFF]\n+        # version=\"B\" was introduced in a different paper (about the TabM model).\n+        is_version_B = version == 'B'\n+\n+        self.linear0 = (\n+            LinearEmbeddings(n_features, d_embedding) if is_version_B else None\n+        )\n+        self.impl = _PiecewiseLinearEncodingImpl(bins)\n+        self.linear = _NLinear(\n+            len(bins),\n+            self.impl.get_max_n_bins(),\n+            d_embedding,\n+            # For the version \"B\", the bias is already presented in self.linear0.\n+            bias=not is_version_B,\n+        )\n+        if is_version_B:\n+            # Because of the following line, at initialization,\n+            # the whole embedding behaves like a linear embedding.\n+            # The piecewise-linear component is incrementally learnt during training.\n+            nn.init.zeros_(self.linear.weight)\n+        self.activation = nn.ReLU() if activation else None\n+\n+    def get_output_shape(self) -> torch.Size:\n+        \"\"\"Get the output shape without the batch dimensions.\"\"\"\n+        n_features = self.linear.weight.shape[0]\n+        d_embedding = self.linear.weight.shape[2]\n+        return torch.Size((n_features, d_embedding))\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        \"\"\"Do the forward pass.\"\"\"\n+        if x.ndim != 2:\n+            raise ValueError(\n+                'For now, only inputs with exactly one batch dimension are supported.'\n+            )\n+\n+        x_linear = None if self.linear0 is None else self.linear0(x)\n+\n+        x_ple = self.impl(x)\n+        x_ple = self.linear(x_ple)\n+        if self.activation is not None:\n+            x_ple = self.activation(x_ple)\n+        return x_ple if x_linear is None else x_linear + x_ple\n\\ No newline at end of file\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/models/tabm/tabm_model.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/models/tabm/tabm_model.py b/tabular/src/autogluon/tabular/models/tabm/tabm_model.py\nnew file mode 100644\nindex 0000000..af6e89f\n--- /dev/null\n+++ b/tabular/src/autogluon/tabular/models/tabm/tabm_model.py\n@@ -0,0 +1,275 @@\n+\"\"\"\n+Code Adapted from TabArena: https://github.com/autogluon/tabrepo/blob/main/tabrepo/benchmark/models/ag/tabm/tabm_model.py\n+Note: This is a custom implementation of TabM based on TabArena. Because the AutoGluon 1.4 release occurred at nearly\n+the same time as TabM became available on PyPi, we chose to use TabArena's implementation\n+for the AutoGluon 1.4 release as it has already been benchmarked.\n+\n+Model: TabM\n+Paper: TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling\n+Authors: Yury Gorishniy, Akim Kotelnikov, Artem Babenko\n+Codebase: https://github.com/yandex-research/tabm\n+License: Apache-2.0\n+\n+Partially adapted from pytabkit's TabM implementation.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import time\n+\n+import pandas as pd\n+from autogluon.common.utils.resource_utils import ResourceManager\n+from autogluon.core.models import AbstractModel\n+from autogluon.tabular import __version__\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class TabMModel(AbstractModel):\n+    ag_key = \"TABM\"\n+    ag_name = \"TabM\"\n+    ag_priority = 85\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self._imputer = None\n+        self._features_to_impute = None\n+        self._features_to_keep = None\n+        self._indicator_columns = None\n+        self._features_bool = None\n+        self._bool_to_cat = None\n+\n+    def _fit(\n+        self,\n+        X: pd.DataFrame,\n+        y: pd.Series,\n+        X_val: pd.DataFrame = None,\n+        y_val: pd.Series = None,\n+        time_limit: float | None = None,\n+        num_cpus: int = 1,\n+        num_gpus: float = 0,\n+        **kwargs,\n+    ):\n+        start_time = time.time()\n+\n+        try:\n+            # imports various dependencies such as torch\n+            from ._tabm_internal import TabMImplementation\n+            from torch.cuda import is_available\n+        except ImportError as err:\n+            logger.log(\n+                40,\n+                f\"\\tFailed to import tabm! To use the TabM model, \"\n+                f\"do: `pip install autogluon.tabular[tabm]=={__version__}`.\",\n+            )\n+            raise err\n+\n+        device = \"cpu\" if num_gpus == 0 else \"cuda\"\n+        if (device == \"cuda\") and (not is_available()):\n+            # FIXME: warn instead and switch to CPU.\n+            raise AssertionError(\n+                \"Fit specified to use GPU, but CUDA is not available on this machine. \"\n+                \"Please switch to CPU usage instead.\",\n+            )\n+\n+        if X_val is None:\n+            from autogluon.core.utils import generate_train_test_split\n+\n+            X_train, X_val, y_train, y_val = generate_train_test_split(\n+                X=X,\n+                y=y,\n+                problem_type=self.problem_type,\n+                test_size=0.2,\n+                random_state=0,\n+            )\n+\n+        hyp = self._get_model_params()\n+        bool_to_cat = hyp.pop(\"bool_to_cat\", True)\n+\n+        X = self.preprocess(X, is_train=True, bool_to_cat=bool_to_cat)\n+        if X_val is not None:\n+            X_val = self.preprocess(X_val)\n+\n+        self.model = TabMImplementation(\n+            n_threads=num_cpus,\n+            device=device,\n+            problem_type=self.problem_type,\n+            early_stopping_metric=self.stopping_metric,\n+            **hyp,\n+        )\n+\n+        self.model.fit(\n+            X_train=X,\n+            y_train=y,\n+            X_val=X_val,\n+            y_val=y_val,\n+            cat_col_names=X.select_dtypes(include=\"category\").columns.tolist(),\n+            time_to_fit_in_seconds=time_limit - (time.time() - start_time) if time_limit is not None else None,\n+        )\n+\n+    # FIXME: bool_to_cat is a hack: Maybe move to abstract model?\n+    def _preprocess(\n+        self,\n+        X: pd.DataFrame,\n+        is_train: bool = False,\n+        bool_to_cat: bool = False,\n+        **kwargs,\n+    ) -> pd.DataFrame:\n+        \"\"\"Imputes missing values via the mean and adds indicator columns for numerical features.\n+        Converts indicator columns to categorical features to avoid them being treated as numerical by RealMLP.\n+        \"\"\"\n+        X = super()._preprocess(X, **kwargs)\n+\n+        if is_train:\n+            self._bool_to_cat = bool_to_cat\n+            self._features_bool = self._feature_metadata.get_features(required_special_types=[\"bool\"])\n+        if self._bool_to_cat and self._features_bool:\n+            # FIXME: Use CategoryFeatureGenerator? Or tell the model which is category\n+            X = X.copy(deep=True)\n+            X[self._features_bool] = X[self._features_bool].astype(\"category\")\n+\n+        return X\n+\n+    def _set_default_params(self):\n+        default_params = dict(\n+            random_state=0,\n+        )\n+        for param, val in default_params.items():\n+            self._set_default_param_value(param, val)\n+\n+    @classmethod\n+    def supported_problem_types(cls) -> list[str] | None:\n+        return [\"binary\", \"multiclass\", \"regression\"]\n+\n+    def _get_default_stopping_metric(self):\n+        return self.eval_metric\n+\n+    def _get_default_resources(self) -> tuple[int, int]:\n+        # logical=False is faster in training\n+        num_cpus = ResourceManager.get_cpu_count_psutil(logical=False)\n+        num_gpus = min(ResourceManager.get_gpu_count_torch(), 1)\n+        return num_cpus, num_gpus\n+\n+    def _estimate_memory_usage(self, X: pd.DataFrame, **kwargs) -> int:\n+        hyperparameters = self._get_model_params()\n+        return self.estimate_memory_usage_static(\n+            X=X,\n+            problem_type=self.problem_type,\n+            num_classes=self.num_classes,\n+            hyperparameters=hyperparameters,\n+            **kwargs,\n+        )\n+\n+    @classmethod\n+    def _estimate_memory_usage_static(\n+        cls,\n+        *,\n+        X: pd.DataFrame,\n+        hyperparameters: dict = None,\n+        num_classes: int | None = 1,\n+        **kwargs,\n+    ) -> int:\n+        \"\"\"\n+        Heuristic memory estimate that correlates strongly with RealMLP\n+        \"\"\"\n+        if num_classes is None:\n+            num_classes = 1\n+        if hyperparameters is None:\n+            hyperparameters = {}\n+\n+        cat_sizes = []\n+        for col in X.select_dtypes(include=[\"category\", \"object\"]):\n+            if isinstance(X[col], pd.CategoricalDtype):\n+                # Use .cat.codes for category dtype\n+                unique_codes = X[col].cat.codes.unique()\n+            else:\n+                # For object dtype, treat unique strings as codes\n+                unique_codes = X[col].astype(\"category\").cat.codes.unique()\n+            cat_sizes.append(len(unique_codes))\n+\n+        n_numerical = len(X.select_dtypes(include=[\"number\"]).columns)\n+\n+        # TODO: This estimates very high memory usage,\n+        #  we probably need to adjust batch size automatically to compensate\n+        mem_estimate_bytes = cls._estimate_tabm_ram(\n+            hyperparameters=hyperparameters,\n+            n_numerical=n_numerical,\n+            cat_sizes=cat_sizes,\n+            n_classes=num_classes,\n+            n_samples=len(X),\n+        )\n+\n+        return mem_estimate_bytes\n+\n+    @classmethod\n+    def _estimate_tabm_ram(\n+        cls,\n+        hyperparameters: dict,\n+        n_numerical: int,\n+        cat_sizes: list[int],\n+        n_classes: int,\n+        n_samples: int,\n+    ) -> int:\n+        num_emb_n_bins = hyperparameters.get(\"num_emb_n_bins\", 48)\n+        d_embedding = hyperparameters.get(\"d_embedding\", 16)\n+        d_block = hyperparameters.get(\"d_block\", 512)\n+        # not completely sure if this is hidden blocks or all blocks, taking the safe option below\n+        n_blocks = hyperparameters.get(\"n_blocks\", \"auto\")\n+        if isinstance(n_blocks, str) and n_blocks == \"auto\":\n+            n_blocks = 3\n+        batch_size = hyperparameters.get(\"batch_size\", \"auto\")\n+        if isinstance(batch_size, str) and batch_size == \"auto\":\n+            batch_size = cls.get_tabm_auto_batch_size(n_samples=n_samples)\n+        tabm_k = hyperparameters.get(\"tabm_k\", 32)\n+        predict_batch_size = hyperparameters.get(\"eval_batch_size\", 1024)\n+\n+        # not completely sure\n+        n_params_num_emb = n_numerical * (num_emb_n_bins + 1) * d_embedding\n+        n_params_mlp = (n_numerical + sum(cat_sizes)) * d_embedding * (d_block + tabm_k) \\\n+                       + (n_blocks - 1) * d_block ** 2 \\\n+                       + n_blocks * d_block + d_block * (1 + max(1, n_classes))\n+        # 4 bytes per float, up to 5 copies of parameters (1 standard, 1 .grad, 2 adam, 1 best_epoch)\n+        mem_params = 4 * 5 * (n_params_num_emb + n_params_mlp)\n+\n+        # compute number of floats in forward pass (per batch element)\n+        # todo: numerical embedding layer (not sure if this is entirely correct)\n+        n_floats_forward = n_numerical * (num_emb_n_bins + d_embedding)\n+        # before and after scale\n+        n_floats_forward += 2 * (sum(cat_sizes) + n_numerical * d_embedding)\n+        # 2 for pre-act, post-act\n+        n_floats_forward += n_blocks * 2 * d_block + 2 * max(1, n_classes)\n+        # 2 for forward and backward, 4 bytes per float\n+        mem_forward_backward = 4 * max(batch_size * 2, predict_batch_size) * n_floats_forward * tabm_k\n+        # * 8 is pessimistic for the long tensors in the forward pass, 4 would probably suffice\n+\n+        mem_ds = n_samples * (4 * n_numerical + 8 * len(cat_sizes))\n+\n+        # some safety constants and offsets (the 5 is probably excessive)\n+        mem_total = 5 * mem_ds + 1.2 * mem_forward_backward + 1.2 * mem_params + 0.3 * (1024 ** 3)\n+\n+        return mem_total\n+\n+    @classmethod\n+    def get_tabm_auto_batch_size(cls, n_samples: int) -> int:\n+        # by Yury Gorishniy, inferred from the choices in the TabM paper.\n+        if n_samples < 2_800:\n+            return 32\n+        if n_samples < 4_500:\n+            return 64\n+        if n_samples < 6_400:\n+            return 128\n+        if n_samples < 32_000:\n+            return 256\n+        if n_samples < 108_000:\n+            return 512\n+        return 1024\n+\n+    @classmethod\n+    def _class_tags(cls):\n+        return {\"can_estimate_memory_usage_static\": True}\n+\n+    def _more_tags(self) -> dict:\n+        # TODO: Need to add train params support, track best epoch\n+        #  How to force stopping at a specific epoch?\n+        return {\"can_refit_full\": False}\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/models/tabm/tabm_reference.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/models/tabm/tabm_reference.py b/tabular/src/autogluon/tabular/models/tabm/tabm_reference.py\nnew file mode 100644\nindex 0000000..feb52bf\n--- /dev/null\n+++ b/tabular/src/autogluon/tabular/models/tabm/tabm_reference.py\n@@ -0,0 +1,628 @@\n+# License: https://github.com/yandex-research/tabm/blob/main/LICENSE\n+\n+# NOTE\n+# The minimum required versions of the dependencies are specified in README.md.\n+\n+import itertools\n+from typing import Any, Literal\n+\n+import torch\n+import torch.nn as nn\n+from torch import Tensor\n+\n+from . import rtdl_num_embeddings\n+\n+\n+# ======================================================================================\n+# Initialization\n+# ======================================================================================\n+def init_rsqrt_uniform_(x: Tensor, d: int) -> Tensor:\n+    assert d > 0\n+    d_rsqrt = d**-0.5\n+    return nn.init.uniform_(x, -d_rsqrt, d_rsqrt)\n+\n+\n+@torch.inference_mode()\n+def init_random_signs_(x: Tensor) -> Tensor:\n+    return x.bernoulli_(0.5).mul_(2).add_(-1)\n+\n+\n+# ======================================================================================\n+# Modules\n+# ======================================================================================\n+class NLinear(nn.Module):\n+    \"\"\"N linear layers applied in parallel to N disjoint parts of the input.\n+\n+    **Shape**\n+\n+    - Input: ``(B, N, in_features)``\n+    - Output: ``(B, N, out_features)``\n+\n+    The i-th linear layer is applied to the i-th matrix of the shape (B, in_features).\n+\n+    Technically, this is a simplified version of delu.nn.NLinear:\n+    https://yura52.github.io/delu/stable/api/generated/delu.nn.NLinear.html.\n+    The difference is that this layer supports only 3D inputs\n+    with exactly one batch dimension. By contrast, delu.nn.NLinear supports\n+    any number of batch dimensions.\n+    \"\"\"\n+\n+    def __init__(\n+        self, n: int, in_features: int, out_features: int, bias: bool = True\n+    ) -> None:\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.empty(n, in_features, out_features))\n+        self.bias = nn.Parameter(torch.empty(n, out_features)) if bias else None\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        d = self.weight.shape[-2]\n+        init_rsqrt_uniform_(self.weight, d)\n+        if self.bias is not None:\n+            init_rsqrt_uniform_(self.bias, d)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        assert x.ndim == 3\n+        assert x.shape[-(self.weight.ndim - 1) :] == self.weight.shape[:-1]\n+\n+        x = x.transpose(0, 1)\n+        x = x @ self.weight\n+        x = x.transpose(0, 1)\n+        if self.bias is not None:\n+            x = x + self.bias\n+        return x\n+\n+\n+class OneHotEncoding0d(nn.Module):\n+    # Input:  (*, n_cat_features=len(cardinalities))\n+    # Output: (*, sum(cardinalities))\n+\n+    def __init__(self, cardinalities: list[int]) -> None:\n+        super().__init__()\n+        self._cardinalities = cardinalities\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        assert x.ndim >= 1\n+        assert x.shape[-1] == len(self._cardinalities)\n+\n+        return torch.cat(\n+            [\n+                # NOTE\n+                # This is a quick hack to support out-of-vocabulary categories.\n+                #\n+                # Recall that lib.data.transform_cat encodes categorical features\n+                # as follows:\n+                # - In-vocabulary values receive indices from `range(cardinality)`.\n+                # - All out-of-vocabulary values (i.e. new categories in validation\n+                #   and test data that are not presented in the training data)\n+                #   receive the index `cardinality`.\n+                #\n+                # As such, the line below will produce the standard one-hot encoding for\n+                # known categories, and the all-zeros encoding for unknown categories.\n+                # This may not be the best approach to deal with unknown values,\n+                # but should be enough for our purposes.\n+                nn.functional.one_hot(x[..., i], cardinality + 1)[..., :-1]\n+                for i, cardinality in enumerate(self._cardinalities)\n+            ],\n+            -1,\n+        )\n+\n+\n+class ScaleEnsemble(nn.Module):\n+    def __init__(\n+        self,\n+        k: int,\n+        d: int,\n+        *,\n+        init: Literal['ones', 'normal', 'random-signs'],\n+    ) -> None:\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.empty(k, d))\n+        self._weight_init = init\n+        self.reset_parameters()\n+\n+    def reset_parameters(self) -> None:\n+        if self._weight_init == 'ones':\n+            nn.init.ones_(self.weight)\n+        elif self._weight_init == 'normal':\n+            nn.init.normal_(self.weight)\n+        elif self._weight_init == 'random-signs':\n+            init_random_signs_(self.weight)\n+        else:\n+            raise ValueError(f'Unknown weight_init: {self._weight_init}')\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        assert x.ndim >= 2\n+        return x * self.weight\n+\n+\n+class LinearEfficientEnsemble(nn.Module):\n+    \"\"\"\n+    This layer is a more configurable version of the \"BatchEnsemble\" layer\n+    from the paper\n+    \"BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning\"\n+    (link: https://arxiv.org/abs/2002.06715).\n+\n+    First, this layer allows to select only some of the \"ensembled\" parts:\n+    - the input scaling  (r_i in the BatchEnsemble paper)\n+    - the output scaling (s_i in the BatchEnsemble paper)\n+    - the output bias    (not mentioned in the BatchEnsemble paper,\n+                          but is presented in public implementations)\n+\n+    Second, the initialization of the scaling weights is configurable\n+    through the `scaling_init` argument.\n+\n+    NOTE\n+    The term \"adapter\" is used in the TabM paper only to tell the story.\n+    The original BatchEnsemble paper does NOT use this term. So this class also\n+    avoids the term \"adapter\".\n+    \"\"\"\n+\n+    r: None | Tensor\n+    s: None | Tensor\n+    bias: None | Tensor\n+\n+    def __init__(\n+        self,\n+        in_features: int,\n+        out_features: int,\n+        bias: bool = True,\n+        *,\n+        k: int,\n+        ensemble_scaling_in: bool,\n+        ensemble_scaling_out: bool,\n+        ensemble_bias: bool,\n+        scaling_init: Literal['ones', 'random-signs'],\n+    ):\n+        assert k > 0\n+        if ensemble_bias:\n+            assert bias\n+        super().__init__()\n+\n+        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n+        self.register_parameter(\n+            'r',\n+            (\n+                nn.Parameter(torch.empty(k, in_features))\n+                if ensemble_scaling_in\n+                else None\n+            ),  # type: ignore[code]\n+        )\n+        self.register_parameter(\n+            's',\n+            (\n+                nn.Parameter(torch.empty(k, out_features))\n+                if ensemble_scaling_out\n+                else None\n+            ),  # type: ignore[code]\n+        )\n+        self.register_parameter(\n+            'bias',\n+            (\n+                nn.Parameter(torch.empty(out_features))  # type: ignore[code]\n+                if bias and not ensemble_bias\n+                else nn.Parameter(torch.empty(k, out_features))\n+                if ensemble_bias\n+                else None\n+            ),\n+        )\n+\n+        self.in_features = in_features\n+        self.out_features = out_features\n+        self.k = k\n+        self.scaling_init = scaling_init\n+\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        init_rsqrt_uniform_(self.weight, self.in_features)\n+        scaling_init_fn = {'ones': nn.init.ones_, 'random-signs': init_random_signs_}[\n+            self.scaling_init\n+        ]\n+        if self.r is not None:\n+            scaling_init_fn(self.r)\n+        if self.s is not None:\n+            scaling_init_fn(self.s)\n+        if self.bias is not None:\n+            bias_init = torch.empty(\n+                # NOTE: the shape of bias_init is (out_features,) not (k, out_features).\n+                # It means that all biases have the same initialization.\n+                # This is similar to having one shared bias plus\n+                # k zero-initialized non-shared biases.\n+                self.out_features,\n+                dtype=self.weight.dtype,\n+                device=self.weight.device,\n+            )\n+            bias_init = init_rsqrt_uniform_(bias_init, self.in_features)\n+            with torch.inference_mode():\n+                self.bias.copy_(bias_init)\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        # x.shape == (B, K, D)\n+        assert x.ndim == 3\n+\n+        # >>> The equation (5) from the BatchEnsemble paper (arXiv v2).\n+        if self.r is not None:\n+            x = x * self.r\n+        x = x @ self.weight.T\n+        if self.s is not None:\n+            x = x * self.s\n+        # <<<\n+\n+        if self.bias is not None:\n+            x = x + self.bias\n+        return x\n+\n+\n+class MLP(nn.Module):\n+    def __init__(\n+        self,\n+        *,\n+        d_in: None | int = None,\n+        d_out: None | int = None,\n+        n_blocks: int,\n+        d_block: int,\n+        dropout: float,\n+        activation: str = 'ReLU',\n+    ) -> None:\n+        super().__init__()\n+\n+        d_first = d_block if d_in is None else d_in\n+        self.blocks = nn.ModuleList(\n+            [\n+                nn.Sequential(\n+                    nn.Linear(d_first if i == 0 else d_block, d_block),\n+                    getattr(nn, activation)(),\n+                    nn.Dropout(dropout),\n+                )\n+                for i in range(n_blocks)\n+            ]\n+        )\n+        self.output = None if d_out is None else nn.Linear(d_block, d_out)\n+\n+    def forward(self, x: Tensor) -> Tensor:\n+        for block in self.blocks:\n+            x = block(x)\n+        if self.output is not None:\n+            x = self.output(x)\n+        return x\n+\n+\n+def make_efficient_ensemble(module: nn.Module, EnsembleLayer, **kwargs) -> None:\n+    \"\"\"Replace linear layers with efficient ensembles of linear layers.\n+\n+    NOTE\n+    In the paper, there are no experiments with networks with normalization layers.\n+    Perhaps, their trainable weights (the affine transformations) also need\n+    \"ensemblification\" as in the paper about \"FiLM-Ensemble\".\n+    Additional experiments are required to make conclusions.\n+    \"\"\"\n+    for name, submodule in list(module.named_children()):\n+        if isinstance(submodule, nn.Linear):\n+            module.add_module(\n+                name,\n+                EnsembleLayer(\n+                    in_features=submodule.in_features,\n+                    out_features=submodule.out_features,\n+                    bias=submodule.bias is not None,\n+                    **kwargs,\n+                ),\n+            )\n+        else:\n+            make_efficient_ensemble(submodule, EnsembleLayer, **kwargs)\n+\n+\n+def _get_first_ensemble_layer(backbone: MLP) -> LinearEfficientEnsemble:\n+    if isinstance(backbone, MLP):\n+        return backbone.blocks[0][0]  # type: ignore[code]\n+    else:\n+        raise RuntimeError(f'Unsupported backbone: {backbone}')\n+\n+\n+@torch.inference_mode()\n+def _init_first_adapter(\n+    weight: Tensor,\n+    distribution: Literal['normal', 'random-signs'],\n+    init_sections: list[int],\n+) -> None:\n+    \"\"\"Initialize the first adapter.\n+\n+    NOTE\n+    The `init_sections` argument is a historical artifact that accidentally leaked\n+    from irrelevant experiments to the final models. Perhaps, the code related\n+    to `init_sections` can be simply removed, but this was not tested.\n+    \"\"\"\n+    assert weight.ndim == 2\n+    assert weight.shape[1] == sum(init_sections)\n+\n+    if distribution == 'normal':\n+        init_fn_ = nn.init.normal_\n+    elif distribution == 'random-signs':\n+        init_fn_ = init_random_signs_\n+    else:\n+        raise ValueError(f'Unknown distribution: {distribution}')\n+\n+    section_bounds = [0, *torch.tensor(init_sections).cumsum(0).tolist()]\n+    for i in range(len(init_sections)):\n+        # NOTE\n+        # As noted above, this section-based initialization is an arbitrary historical\n+        # artifact. Consider the first adapter of one ensemble member.\n+        # This adapter vector is implicitly split into \"sections\",\n+        # where one section corresponds to one feature. The code below ensures that\n+        # the adapter weights in one section are initialized with the same random value\n+        # from the given distribution.\n+        w = torch.empty((len(weight), 1), dtype=weight.dtype, device=weight.device)\n+        init_fn_(w)\n+        weight[:, section_bounds[i] : section_bounds[i + 1]] = w\n+\n+\n+_CUSTOM_MODULES = {\n+    # https://docs.python.org/3/library/stdtypes.html#definition.__name__\n+    CustomModule.__name__: CustomModule\n+    for CustomModule in [\n+        rtdl_num_embeddings.LinearEmbeddings,\n+        rtdl_num_embeddings.LinearReLUEmbeddings,\n+        rtdl_num_embeddings.PeriodicEmbeddings,\n+        rtdl_num_embeddings.PiecewiseLinearEmbeddings,\n+        MLP,\n+    ]\n+}\n+\n+\n+def make_module(type: str, *args, **kwargs) -> nn.Module:\n+    Module = getattr(nn, type, None)\n+    if Module is None:\n+        Module = _CUSTOM_MODULES[type]\n+    return Module(*args, **kwargs)\n+\n+\n+# ======================================================================================\n+# Optimization\n+# ======================================================================================\n+def default_zero_weight_decay_condition(\n+    module_name: str, module: nn.Module, parameter_name: str, parameter: nn.Parameter\n+):\n+    from tabrepo.benchmark.models.ag.tabm.rtdl_num_embeddings import _Periodic\n+\n+    del module_name, parameter\n+    return parameter_name.endswith('bias') or isinstance(\n+        module,\n+        nn.BatchNorm1d\n+        | nn.LayerNorm\n+        | nn.InstanceNorm1d\n+        | rtdl_num_embeddings.LinearEmbeddings\n+        | rtdl_num_embeddings.LinearReLUEmbeddings\n+        | _Periodic,\n+    )\n+\n+\n+def make_parameter_groups(\n+    module: nn.Module,\n+    zero_weight_decay_condition=default_zero_weight_decay_condition,\n+    custom_groups: None | list[dict[str, Any]] = None,\n+) -> list[dict[str, Any]]:\n+    if custom_groups is None:\n+        custom_groups = []\n+    custom_params = frozenset(\n+        itertools.chain.from_iterable(group['params'] for group in custom_groups)\n+    )\n+    assert len(custom_params) == sum(\n+        len(group['params']) for group in custom_groups\n+    ), 'Parameters in custom_groups must not intersect'\n+    zero_wd_params = frozenset(\n+        p\n+        for mn, m in module.named_modules()\n+        for pn, p in m.named_parameters()\n+        if p not in custom_params and zero_weight_decay_condition(mn, m, pn, p)\n+    )\n+    default_group = {\n+        'params': [\n+            p\n+            for p in module.parameters()\n+            if p not in custom_params and p not in zero_wd_params\n+        ]\n+    }\n+    return [\n+        default_group,\n+        {'params': list(zero_wd_params), 'weight_decay': 0.0},\n+        *custom_groups,\n+    ]\n+\n+\n+# ======================================================================================\n+# The model\n+# ======================================================================================\n+class Model(nn.Module):\n+    \"\"\"MLP & TabM.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        n_num_features: int,\n+        cat_cardinalities: list[int],\n+        n_classes: None | int,\n+        backbone: dict,\n+        bins: None | list[Tensor],  # For piecewise-linear encoding/embeddings.\n+        num_embeddings: None | dict = None,\n+        arch_type: Literal[\n+            # Plain feed-forward network without any kind of ensembling.\n+            'plain',\n+            #\n+            # TabM\n+            'tabm',\n+            #\n+            # TabM-mini\n+            'tabm-mini',\n+            #\n+            # TabM-packed\n+            'tabm-packed',\n+            #\n+            # TabM. The first adapter is initialized from the normal distribution.\n+            # This variant was not used in the paper, but it may be useful in practice.\n+            'tabm-normal',\n+            #\n+            # TabM-mini. The adapter is initialized from the normal distribution.\n+            # This variant was not used in the paper.\n+            'tabm-mini-normal',\n+        ],\n+        k: None | int = None,\n+        share_training_batches: bool = True,\n+    ) -> None:\n+        # >>> Validate arguments.\n+        assert n_num_features >= 0\n+        assert n_num_features or cat_cardinalities\n+        if arch_type == 'plain':\n+            assert k is None\n+            assert (\n+                share_training_batches\n+            ), 'If `arch_type` is set to \"plain\", then `simple` must remain True'\n+        else:\n+            assert k is not None\n+            assert k > 0\n+\n+        super().__init__()\n+\n+        # >>> Continuous (numerical) features\n+        first_adapter_sections = []  # See the comment in `_init_first_adapter`.\n+\n+        if n_num_features == 0:\n+            assert bins is None\n+            self.num_module = None\n+            d_num = 0\n+\n+        elif num_embeddings is None:\n+            assert bins is None\n+            self.num_module = None\n+            d_num = n_num_features\n+            first_adapter_sections.extend(1 for _ in range(n_num_features))\n+\n+        else:\n+            if bins is None:\n+                self.num_module = make_module(\n+                    **num_embeddings, n_features=n_num_features\n+                )\n+            else:\n+                assert num_embeddings['type'].startswith('PiecewiseLinearEmbeddings')\n+                self.num_module = make_module(**num_embeddings, bins=bins)\n+            d_num = n_num_features * num_embeddings['d_embedding']\n+            first_adapter_sections.extend(\n+                num_embeddings['d_embedding'] for _ in range(n_num_features)\n+            )\n+\n+        # >>> Categorical features\n+        self.cat_module = (\n+            OneHotEncoding0d(cat_cardinalities) if cat_cardinalities else None\n+        )\n+        first_adapter_sections.extend(cat_cardinalities)\n+        d_cat = sum(cat_cardinalities)\n+\n+        # >>> Backbone\n+        d_flat = d_num + d_cat\n+        self.minimal_ensemble_adapter = None\n+        # Any backbone can be here but we provide only MLP\n+        self.backbone = make_module(d_in=d_flat, **backbone)\n+\n+        if arch_type != 'plain':\n+            assert k is not None\n+            first_adapter_init = (\n+                None\n+                if arch_type == 'tabm-packed'\n+                else 'normal'\n+                if arch_type in ('tabm-mini-normal', 'tabm-normal')\n+                # For other arch_types, the initialization depends\n+                # on the presense of num_embeddings.\n+                else 'random-signs'\n+                if num_embeddings is None\n+                else 'normal'\n+            )\n+\n+            if arch_type in ('tabm', 'tabm-normal'):\n+                # Like BatchEnsemble, but all multiplicative adapters,\n+                # except for the very first one, are initialized with ones.\n+                assert first_adapter_init is not None\n+                make_efficient_ensemble(\n+                    self.backbone,\n+                    LinearEfficientEnsemble,\n+                    k=k,\n+                    ensemble_scaling_in=True,\n+                    ensemble_scaling_out=True,\n+                    ensemble_bias=True,\n+                    scaling_init='ones',\n+                )\n+                _init_first_adapter(\n+                    _get_first_ensemble_layer(self.backbone).r,  # type: ignore[code]\n+                    first_adapter_init,\n+                    first_adapter_sections,\n+                )\n+\n+            elif arch_type in ('tabm-mini', 'tabm-mini-normal'):\n+                # MiniEnsemble\n+                assert first_adapter_init is not None\n+                self.minimal_ensemble_adapter = ScaleEnsemble(\n+                    k,\n+                    d_flat,\n+                    init='random-signs' if num_embeddings is None else 'normal',\n+                )\n+                _init_first_adapter(\n+                    self.minimal_ensemble_adapter.weight,  # type: ignore[code]\n+                    first_adapter_init,\n+                    first_adapter_sections,\n+                )\n+\n+            elif arch_type == 'tabm-packed':\n+                # Packed ensemble.\n+                # In terms of the Packed Ensembles paper by Laurent et al.,\n+                # TabM-packed is PackedEnsemble(alpha=k, M=k, gamma=1).\n+                assert first_adapter_init is None\n+                make_efficient_ensemble(self.backbone, NLinear, n=k)\n+\n+            else:\n+                raise ValueError(f'Unknown arch_type: {arch_type}')\n+\n+        # >>> Output\n+        d_block = backbone['d_block']\n+        d_out = 1 if n_classes is None else n_classes\n+        self.output = (\n+            nn.Linear(d_block, d_out)\n+            if arch_type == 'plain'\n+            else NLinear(k, d_block, d_out)  # type: ignore[code]\n+        )\n+\n+        # >>>\n+        self.arch_type = arch_type\n+        self.k = k\n+        self.share_training_batches = share_training_batches\n+\n+    def forward(\n+        self, x_num: None | Tensor = None, x_cat: None | Tensor = None\n+    ) -> Tensor:\n+        x = []\n+        if x_num is not None:\n+            x.append(x_num if self.num_module is None else self.num_module(x_num))\n+        if x_cat is None:\n+            assert self.cat_module is None\n+        else:\n+            assert self.cat_module is not None\n+            x.append(self.cat_module(x_cat).float())\n+        x = torch.column_stack([x_.flatten(1, -1) for x_ in x])\n+\n+        if self.k is not None:\n+            if self.share_training_batches or not self.training:\n+                # (B, D) -> (B, K, D)\n+                x = x[:, None].expand(-1, self.k, -1)\n+            else:\n+                # (B * K, D) -> (B, K, D)\n+                x = x.reshape(len(x) // self.k, self.k, *x.shape[1:])\n+            if self.minimal_ensemble_adapter is not None:\n+                x = self.minimal_ensemble_adapter(x)\n+        else:\n+            assert self.minimal_ensemble_adapter is None\n+\n+        x = self.backbone(x)\n+        x = self.output(x)\n+        if self.k is None:\n+            # Adjust the output shape for plain networks to make them compatible\n+            # with the rest of the script (loss, metrics, predictions, ...).\n+            # (B, D_OUT) -> (B, 1, D_OUT)\n+            x = x[:, None]\n+        return x\n\\ No newline at end of file\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/src/autogluon/tabular/registry/_ag_model_registry.py",
            "diff": "diff --git a/tabular/src/autogluon/tabular/registry/_ag_model_registry.py b/tabular/src/autogluon/tabular/registry/_ag_model_registry.py\nindex 3772b2a..4b2dd05 100644\n--- a/tabular/src/autogluon/tabular/registry/_ag_model_registry.py\n+++ b/tabular/src/autogluon/tabular/registry/_ag_model_registry.py\n@@ -21,6 +21,7 @@ from ..models import (\n     NNFastAiTabularModel,\n     RFModel,\n     RuleFitModel,\n+    TabMModel,\n     TabPFNMixModel,\n     TabPFNModel,\n     TabularNeuralNetTorchModel,\n@@ -45,6 +46,7 @@ REGISTERED_MODEL_CLS_LST = [\n     ImagePredictorModel,\n     MultiModalPredictorModel,\n     FTTransformerModel,\n+    TabMModel,\n     TabPFNModel,\n     TabPFNMixModel,\n     FastTextModel,\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/tests/unittests/models/test_tabm.py",
            "diff": "diff --git a/tabular/tests/unittests/models/test_tabm.py b/tabular/tests/unittests/models/test_tabm.py\nnew file mode 100644\nindex 0000000..544b6ac\n--- /dev/null\n+++ b/tabular/tests/unittests/models/test_tabm.py\n@@ -0,0 +1,11 @@\n+from autogluon.tabular.models.tabm.tabm_model import TabMModel\n+from autogluon.tabular.testing import FitHelper\n+\n+toy_model_params = {}\n+\n+\n+def test_tabm():\n+    model_cls = TabMModel\n+    model_hyperparameters = toy_model_params\n+\n+    FitHelper.verify_model(model_cls=model_cls, model_hyperparameters=model_hyperparameters)\n"
        },
        {
            "commit": "00dff2ac803dca7ae44435c5dea311c633926b87",
            "file_path": "tabular/tests/unittests/registry/test_model_registry.py",
            "diff": "diff --git a/tabular/tests/unittests/registry/test_model_registry.py b/tabular/tests/unittests/registry/test_model_registry.py\nindex e89c7cd..37d4593 100644\n--- a/tabular/tests/unittests/registry/test_model_registry.py\n+++ b/tabular/tests/unittests/registry/test_model_registry.py\n@@ -29,6 +29,7 @@ from autogluon.tabular.models import (\n     NNFastAiTabularModel,\n     RFModel,\n     RuleFitModel,\n+    TabMModel,\n     TabPFNMixModel,\n     TabPFNModel,\n     TabularNeuralNetTorchModel,\n@@ -52,6 +53,7 @@ EXPECTED_MODEL_KEYS = {\n     ImagePredictorModel: \"AG_IMAGE_NN\",\n     MultiModalPredictorModel: \"AG_AUTOMM\",\n     FTTransformerModel: \"FT_TRANSFORMER\",\n+    TabMModel: \"TABM\",\n     TabPFNModel: \"TABPFN\",\n     TabPFNMixModel: \"TABPFNMIX\",\n     FastTextModel: \"FASTTEXT\",\n@@ -79,6 +81,7 @@ EXPECTED_MODEL_NAMES = {\n     ImagePredictorModel: \"ImagePredictor\",\n     MultiModalPredictorModel: \"MultiModalPredictor\",\n     FTTransformerModel: \"FTTransformer\",\n+    TabMModel: \"TabM\",\n     TabPFNModel: \"TabPFN\",\n     TabPFNMixModel: \"TabPFNMix\",\n     FastTextModel: \"FastText\",\n@@ -107,6 +110,7 @@ EXPECTED_MODEL_PRIORITY = {\n     ImagePredictorModel: 0,\n     MultiModalPredictorModel: 0,\n     FTTransformerModel: 0,\n+    TabMModel: 85,\n     TabPFNModel: 110,\n     TabPFNMixModel: 45,\n     FastTextModel: 0,\n"
        }
    ]
}