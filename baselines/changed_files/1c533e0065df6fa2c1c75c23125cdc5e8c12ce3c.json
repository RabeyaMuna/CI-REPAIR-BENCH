{
    "sha_fail": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
    "changed_files": [
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "cookbook/agent_concepts/agentic_search/lightrag/agentic_rag_with_lightrag.py",
            "diff": "diff --git a/cookbook/agent_concepts/agentic_search/lightrag/agentic_rag_with_lightrag.py b/cookbook/agent_concepts/agentic_search/lightrag/agentic_rag_with_lightrag.py\nnew file mode 100644\nindex 000000000..e6eea15a0\n--- /dev/null\n+++ b/cookbook/agent_concepts/agentic_search/lightrag/agentic_rag_with_lightrag.py\n@@ -0,0 +1,39 @@\n+from agno.agent import Agent\n+from agno.knowledge.light_rag import LightRagKnowledgeBase, lightrag_retriever\n+from agno.models.anthropic import Claude\n+import asyncio\n+\n+# Create a knowledge base, loaded with documents from a URL\n+knowledge_base = LightRagKnowledgeBase(\n+    lightrag_server_url=\"http://localhost:9621\",\n+    path=\"tmp/\", # Load documents from a local directory\n+    urls=[\"https://docs.agno.com/introduction/agents.md\"], # Load documents from a URL\n+)\n+\n+# Load the knowledge base with the documents from the local directory and the URL\n+asyncio.run(knowledge_base.load())\n+\n+# Load the knowledge base with the text\n+asyncio.run(knowledge_base.load_text(text=\"Dogs and cats are not pets, they are friends.\"))\n+\n+\n+\n+# Create an agent with the knowledge base and the retriever\n+agent = Agent(\n+    model=Claude(id=\"claude-3-7-sonnet-latest\"),\n+    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.\n+    knowledge=knowledge_base,\n+    retriever=lightrag_retriever,\n+    # search_knowledge=True gives the Agent the ability to search on demand\n+    # search_knowledge is True by default\n+    search_knowledge=True,\n+    instructions=[\n+        \"Include sources in your response.\",\n+        \"Always search your knowledge before answering the question.\",\n+        \"Use the async_search method to search the knowledge base.\",\n+    ],\n+    markdown=True)\n+\n+\n+asyncio.run(agent.aprint_response(\"Which candidates are available for the role of a Senior Software Engineer?\"))\n+asyncio.run(agent.aprint_response(\"What are Agno Agents?\"))\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "cookbook/apps/fastapi/advanced.py",
            "diff": "diff --git a/cookbook/apps/fastapi/advanced.py b/cookbook/apps/fastapi/advanced.py\nnew file mode 100644\nindex 000000000..08eac3c31\n--- /dev/null\n+++ b/cookbook/apps/fastapi/advanced.py\n@@ -0,0 +1,149 @@\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.app.fastapi.app import FastAPIApp\n+from agno.memory.v2 import Memory\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.models.openai import OpenAIChat\n+from agno.storage.sqlite import SqliteStorage\n+from agno.team.team import Team\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+agent_storage_file: str = \"tmp/agents.db\"\n+memory_storage_file: str = \"tmp/memory.db\"\n+\n+memory_db = SqliteMemoryDb(table_name=\"memory\", db_file=memory_storage_file)\n+\n+# No need to set the model, it gets set by the agent to the agent's model\n+memory = Memory(db=memory_db)\n+\n+simple_agent = Agent(\n+    name=\"Simple Agent\",\n+    role=\"Answer basic questions\",\n+    agent_id=\"simple-agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    storage=SqliteStorage(\n+        table_name=\"simple_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    agent_id=\"web-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[DuckDuckGoTools()],\n+    instructions=[\n+        \"Break down the users request into 2-3 different searches.\",\n+        \"Always include sources\",\n+    ],\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=[\"Always use tables to display data\"],\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+research_agent = Agent(\n+    name=\"Research Agent\",\n+    role=\"Research agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    instructions=[\"You are a research agent\"],\n+    tools=[DuckDuckGoTools(), ExaTools()],\n+    agent_id=\"research_agent\",\n+    memory=memory,\n+    storage=SqliteStorage(\n+        table_name=\"research_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    enable_user_memories=True,\n+)\n+\n+research_team = Team(\n+    name=\"Research Team\",\n+    description=\"A team of agents that research the web\",\n+    members=[research_agent, simple_agent],\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    mode=\"coordinate\",\n+    team_id=\"research-team\",\n+    success_criteria=dedent(\"\"\"\\\n+        A comprehensive research report with clear sections and data-driven insights.\n+    \"\"\"),\n+    instructions=[\n+        \"You are the lead researcher of a research team! \ud83d\udd0d\",\n+    ],\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+    enable_agentic_context=True,\n+    storage=SqliteStorage(\n+        table_name=\"research_team\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+        mode=\"team\",\n+    ),\n+)\n+\n+\n+fastapi_app = FastAPIApp(\n+    agents=[\n+        simple_agent,\n+        web_agent,\n+        finance_agent,\n+    ],\n+    teams=[research_team],\n+    app_id=\"advanced-app\",\n+    name=\"Advanced FastAPI App\",\n+    description=\"A FastAPI app for advanced agents\",\n+)\n+app = fastapi_app.get_app()\n+\n+if __name__ == \"__main__\":\n+    \"\"\"\n+    Now you can reach your agents/teams with the following URLs:\n+    - http://localhost:8001/runs?agent_id=simple-agent\n+    - http://localhost:8001/runs?agent_id=web-agent\n+    - http://localhost:8001/runs?agent_id=finance-agent\n+    - http://localhost:8001/runs?team_id=research-team\n+    \"\"\"\n+    fastapi_app.serve(app=\"advanced:app\", port=8001, reload=True)\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "cookbook/scripts/lightrag-init/example.env",
            "diff": "diff --git a/cookbook/scripts/lightrag-init/example.env b/cookbook/scripts/lightrag-init/example.env\nnew file mode 100644\nindex 000000000..b68a17b01\n--- /dev/null\n+++ b/cookbook/scripts/lightrag-init/example.env\n@@ -0,0 +1,166 @@\n+### This is sample file of .env\n+\n+\n+### Server Configuration\n+HOST=0.0.0.0\n+PORT=9621\n+WEBUI_TITLE='My Graph KB'\n+WEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\n+OLLAMA_EMULATING_MODEL_TAG=latest\n+# WORKERS=2\n+# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\n+\n+### Login Configuration\n+# AUTH_ACCOUNTS='admin:admin123,user1:pass456'\n+# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server\n+# TOKEN_EXPIRE_HOURS=48\n+# GUEST_TOKEN_EXPIRE_HOURS=24\n+# JWT_ALGORITHM=HS256\n+\n+### API-Key to access LightRAG Server API\n+# LIGHTRAG_API_KEY=your-secure-api-key-here\n+# WHITELIST_PATHS=/health,/api/*\n+\n+### Optional SSL Configuration\n+# SSL=true\n+# SSL_CERTFILE=/path/to/cert.pem\n+# SSL_KEYFILE=/path/to/key.pem\n+\n+### Directory Configuration (defaults to current working directory)\n+### Should not be set if deploy by docker (Set by Dockerfile instead of .env)\n+### Default value is ./inputs and ./rag_storage\n+# INPUT_DIR=<absolute_path_for_doc_input_dir>\n+# WORKING_DIR=<absolute_path_for_working_dir>\n+\n+### Max nodes return from grap retrieval\n+# MAX_GRAPH_NODES=1000\n+\n+### Logging level\n+# LOG_LEVEL=INFO\n+# VERBOSE=False\n+# LOG_MAX_BYTES=10485760\n+# LOG_BACKUP_COUNT=5\n+### Logfile location (defaults to current working directory)\n+# LOG_DIR=/path/to/log/directory\n+\n+### Settings for RAG query\n+# HISTORY_TURNS=3\n+# COSINE_THRESHOLD=0.2\n+# TOP_K=60\n+# MAX_TOKEN_TEXT_CHUNK=4000\n+# MAX_TOKEN_RELATION_DESC=4000\n+# MAX_TOKEN_ENTITY_DESC=4000\n+\n+### Entity and ralation summarization configuration\n+### Language: English, Chinese, French, German ...\n+SUMMARY_LANGUAGE=English\n+### Number of duplicated entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)\n+# FORCE_LLM_SUMMARY_ON_MERGE=6\n+### Max tokens for entity/relations description after merge\n+# MAX_TOKEN_SUMMARY=500\n+\n+### Number of parallel processing documents(Less than MAX_ASYNC/2 is recommended)\n+# MAX_PARALLEL_INSERT=2\n+### Chunk size for document splitting, 500~1500 is recommended\n+# CHUNK_SIZE=1200\n+# CHUNK_OVERLAP_SIZE=100\n+\n+### LLM Configuration\n+ENABLE_LLM_CACHE=true\n+ENABLE_LLM_CACHE_FOR_EXTRACT=true\n+### Time out in seconds for LLM, None for infinite timeout\n+TIMEOUT=240\n+### Some models like o1-mini require temperature to be set to 1\n+TEMPERATURE=0\n+### Max concurrency requests of LLM\n+MAX_ASYNC=4\n+### MAX_TOKENS: max tokens send to LLM for entity relation summaries (less than context size of the model)\n+### MAX_TOKENS: set as num_ctx option for Ollama by API Server\n+MAX_TOKENS=32768\n+### LLM Binding type: openai, ollama, lollms, azure_openai\n+LLM_BINDING=openai\n+LLM_MODEL=gpt-4o\n+LLM_BINDING_HOST=https://api.openai.com/v1\n+LLM_BINDING_API_KEY=sk-proj-***\n+### Optional for Azure\n+# AZURE_OPENAI_API_VERSION=2024-08-01-preview\n+# AZURE_OPENAI_DEPLOYMENT=gpt-4o\n+\n+### Embedding Configuration\n+### Embedding Binding type: openai, ollama, lollms, azure_openai\n+EMBEDDING_BINDING=openai\n+EMBEDDING_MODEL=text-embedding-3-large\n+EMBEDDING_DIM=3072\n+EMBEDDING_BINDING_API_KEY=sk-proj-***\n+EMBEDDING_BINDING_HOST=https://api.openai.com/v1\n+### Num of chunks send to Embedding in single request\n+# EMBEDDING_BATCH_NUM=32\n+### Max concurrency requests for Embedding\n+# EMBEDDING_FUNC_MAX_ASYNC=16\n+### Maximum tokens sent to Embedding for each chunk (no longer in use?)\n+# MAX_EMBED_TOKENS=8192\n+### Optional for Azure\n+# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large\n+# AZURE_EMBEDDING_API_VERSION=2023-05-15\n+\n+### Data storage selection\n+# LIGHTRAG_KV_STORAGE=PGKVStorage\n+# LIGHTRAG_VECTOR_STORAGE=PGVectorStorage\n+# LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n+# LIGHTRAG_GRAPH_STORAGE=Neo4JStorage\n+\n+### TiDB Configuration (Deprecated)\n+# TIDB_HOST=localhost\n+# TIDB_PORT=4000\n+# TIDB_USER=your_username\n+# TIDB_PASSWORD='your_password'\n+# TIDB_DATABASE=your_database\n+### separating all data from difference Lightrag instances(deprecating)\n+# TIDB_WORKSPACE=default\n+\n+### PostgreSQL Configuration\n+POSTGRES_HOST=localhost\n+POSTGRES_PORT=5432\n+POSTGRES_USER=ai\n+POSTGRES_PASSWORD='ai'\n+POSTGRES_DATABASE=ai\n+POSTGRES_MAX_CONNECTIONS=12\n+### separating all data from difference Lightrag instances(deprecating)\n+# POSTGRES_WORKSPACE=default\n+\n+### Neo4j Configuration\n+#NEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io\n+NEO4J_URI=bolt://neo4j:7687\n+NEO4J_USERNAME=neo4j\n+NEO4J_PASSWORD='testpassword'\n+\n+### Independent AGM Configuration(not for AMG embedded in PostreSQL)\n+# AGE_POSTGRES_DB=\n+# AGE_POSTGRES_USER=\n+# AGE_POSTGRES_PASSWORD=\n+# AGE_POSTGRES_HOST=\n+# AGE_POSTGRES_PORT=8529\n+\n+# AGE Graph Name(apply to PostgreSQL and independent AGM)\n+### AGE_GRAPH_NAME is precated\n+# AGE_GRAPH_NAME=lightrag\n+\n+### MongoDB Configuration\n+MONGO_URI=mongodb://root:root@localhost:27017/\n+MONGO_DATABASE=LightRAG\n+### separating all data from difference Lightrag instances(deprecating)\n+# MONGODB_GRAPH=false\n+\n+### Milvus Configuration\n+MILVUS_URI=http://localhost:19530\n+MILVUS_DB_NAME=lightrag\n+# MILVUS_USER=root\n+# MILVUS_PASSWORD=your_password\n+# MILVUS_TOKEN=your_token\n+\n+### Qdrant\n+QDRANT_URL=http://localhost:16333\n+# QDRANT_API_KEY=your-api-key\n+\n+### Redis\n+REDIS_URI=redis://localhost:6379\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "cookbook/scripts/lightrag-init/run_lightrag.sh",
            "diff": "diff --git a/cookbook/scripts/lightrag-init/run_lightrag.sh b/cookbook/scripts/lightrag-init/run_lightrag.sh\nnew file mode 100755\nindex 000000000..f5fd9c7ef\n--- /dev/null\n+++ b/cookbook/scripts/lightrag-init/run_lightrag.sh\n@@ -0,0 +1 @@\n+docker-compose -f cookbook/scripts/lightrag-init/docker-compose.yml up -d\n\\ No newline at end of file\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "cookbook/scripts/lightrag-init/stop_lightrag.sh",
            "diff": "diff --git a/cookbook/scripts/lightrag-init/stop_lightrag.sh b/cookbook/scripts/lightrag-init/stop_lightrag.sh\nnew file mode 100755\nindex 000000000..d84571249\n--- /dev/null\n+++ b/cookbook/scripts/lightrag-init/stop_lightrag.sh\n@@ -0,0 +1 @@\n+docker-compose -f cookbook/scripts/lightrag-init/docker-compose.yml down\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "cookbook/tools/gmail_tools.py",
            "diff": "diff --git a/cookbook/tools/gmail_tools.py b/cookbook/tools/gmail_tools.py\nindex f5e06283b..dc94cdccb 100644\n--- a/cookbook/tools/gmail_tools.py\n+++ b/cookbook/tools/gmail_tools.py\n@@ -27,12 +27,14 @@ agent = Agent(\n         \"Based on user query, you can read, draft and send emails using Gmail.\",\n         \"While showing email contents, you can summarize the email contents, extract key details and dates.\",\n         \"Show the email contents in a structured markdown format.\",\n+        \"Attachments can be added to the email\",\n     ],\n     markdown=True,\n     show_tool_calls=False,\n     response_model=FindEmailOutput,\n )\n \n+# Example 1: Find the last email from a specific sender\n email = \"<replace_with_email_address>\"\n response: FindEmailOutput = agent.run(\n     f\"Find the last email from {email} along with the message id, references and in-reply-to\",\n@@ -47,3 +49,11 @@ agent.print_response(\n     markdown=True,\n     stream=True,\n )\n+\n+# Example 2: Send a new email with attachments\n+# agent.print_response(\n+#     \"\"\"Send an email to user@example.com with subject 'Subject' \n+#     and body 'Body' and Attach the file 'tmp/attachment.pdf'\"\"\",\n+#     markdown=True,\n+#     stream=True,\n+# )\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/app/fastapi/app.py",
            "diff": "diff --git a/libs/agno/agno/app/fastapi/app.py b/libs/agno/agno/app/fastapi/app.py\nindex da55f0549..cf153da54 100644\n--- a/libs/agno/agno/app/fastapi/app.py\n+++ b/libs/agno/agno/app/fastapi/app.py\n@@ -1,10 +1,19 @@\n import logging\n+from typing import List, Optional, Union\n \n+import uvicorn\n+from fastapi import FastAPI\n from fastapi.routing import APIRouter\n \n+from agno.agent.agent import Agent\n from agno.app.base import BaseAPIApp\n from agno.app.fastapi.async_router import get_async_router\n from agno.app.fastapi.sync_router import get_sync_router\n+from agno.app.settings import APIAppSettings\n+from agno.app.utils import generate_id\n+from agno.team.team import Team\n+from agno.utils.log import log_info\n+from agno.workflow.workflow import Workflow\n \n logger = logging.getLogger(__name__)\n \n@@ -12,8 +21,93 @@ logger = logging.getLogger(__name__)\n class FastAPIApp(BaseAPIApp):\n     type = \"fastapi\"\n \n+    def __init__(\n+        self,\n+        agents: Optional[List[Agent]] = None,\n+        teams: Optional[List[Team]] = None,\n+        workflows: Optional[List[Workflow]] = None,\n+        settings: Optional[APIAppSettings] = None,\n+        api_app: Optional[FastAPI] = None,\n+        router: Optional[APIRouter] = None,\n+        app_id: Optional[str] = None,\n+        name: Optional[str] = None,\n+        description: Optional[str] = None,\n+        monitoring: bool = True,\n+    ):\n+        if not agents and not teams and not workflows:\n+            raise ValueError(\"Either agents, teams or workflows must be provided.\")\n+\n+        self.agents: Optional[List[Agent]] = agents\n+        self.teams: Optional[List[Team]] = teams\n+        self.workflows: Optional[List[Workflow]] = workflows\n+\n+        self.settings: APIAppSettings = settings or APIAppSettings()\n+        self.api_app: Optional[FastAPI] = api_app\n+        self.router: Optional[APIRouter] = router\n+\n+        self.app_id: Optional[str] = app_id\n+        self.name: Optional[str] = name\n+        self.monitoring = monitoring\n+        self.description = description\n+        self.set_app_id()\n+\n+        if self.agents:\n+            for agent in self.agents:\n+                if not agent.app_id:\n+                    agent.app_id = self.app_id\n+                agent.initialize_agent()\n+\n+        if self.teams:\n+            for team in self.teams:\n+                if not team.app_id:\n+                    team.app_id = self.app_id\n+                team.initialize_team()\n+                for member in team.members:\n+                    if isinstance(member, Agent):\n+                        if not member.app_id:\n+                            member.app_id = self.app_id\n+\n+                        member.team_id = None\n+                        member.initialize_agent()\n+                    elif isinstance(member, Team):\n+                        member.initialize_team()\n+\n+        if self.workflows:\n+            for workflow in self.workflows:\n+                if not workflow.app_id:\n+                    workflow.app_id = self.app_id\n+                if not workflow.workflow_id:\n+                    workflow.workflow_id = generate_id(workflow.name)\n+\n     def get_router(self) -> APIRouter:\n-        return get_sync_router(agent=self.agent, team=self.team)\n+        return get_sync_router(agents=self.agents, teams=self.teams, workflows=self.workflows)\n \n     def get_async_router(self) -> APIRouter:\n-        return get_async_router(agent=self.agent, team=self.team)\n+        return get_async_router(agents=self.agents, teams=self.teams, workflows=self.workflows)\n+\n+    def serve(\n+        self,\n+        app: Union[str, FastAPI],\n+        *,\n+        host: str = \"localhost\",\n+        port: int = 7777,\n+        reload: bool = False,\n+        **kwargs,\n+    ):\n+        self.set_app_id()\n+        self.register_app_on_platform()\n+\n+        if self.agents:\n+            for agent in self.agents:\n+                agent.register_agent()\n+\n+        if self.teams:\n+            for team in self.teams:\n+                team.register_team()\n+\n+        if self.workflows:\n+            for workflow in self.workflows:\n+                workflow.register_workflow()\n+        log_info(f\"Starting API on {host}:{port}\")\n+\n+        uvicorn.run(app=app, host=host, port=port, reload=reload, **kwargs)\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/app/fastapi/async_router.py",
            "diff": "diff --git a/libs/agno/agno/app/fastapi/async_router.py b/libs/agno/agno/app/fastapi/async_router.py\nindex 3c4b7a0ce..1d42463f4 100644\n--- a/libs/agno/agno/app/fastapi/async_router.py\n+++ b/libs/agno/agno/app/fastapi/async_router.py\n@@ -1,8 +1,10 @@\n+import json\n+from dataclasses import asdict\n from io import BytesIO\n-from typing import AsyncGenerator, List, Optional, cast\n+from typing import Any, AsyncGenerator, Dict, List, Optional, cast\n from uuid import uuid4\n \n-from fastapi import APIRouter, File, Form, HTTPException, UploadFile\n+from fastapi import APIRouter, File, Form, HTTPException, Query, UploadFile\n from fastapi.responses import StreamingResponse\n \n from agno.agent.agent import Agent, RunResponse\n@@ -13,6 +15,7 @@ from agno.run.response import RunEvent\n from agno.run.team import TeamRunResponse\n from agno.team.team import Team\n from agno.utils.log import logger\n+from agno.workflow.workflow import Workflow\n \n \n async def agent_chat_response_streamer(\n@@ -81,11 +84,13 @@ async def team_chat_response_streamer(\n         return\n \n \n-def get_async_router(agent: Optional[Agent] = None, team: Optional[Team] = None) -> APIRouter:\n+def get_async_router(\n+    agents: Optional[List[Agent]] = None, teams: Optional[List[Team]] = None, workflows: Optional[List[Workflow]] = None\n+) -> APIRouter:\n     router = APIRouter()\n \n-    if agent is None and team is None:\n-        raise ValueError(\"Either agent or team must be provided.\")\n+    if agents is None and teams is None and workflows is None:\n+        raise ValueError(\"Either agents, teams or workflows must be provided.\")\n \n     @router.get(\"/status\")\n     async def status():\n@@ -246,13 +251,17 @@ def get_async_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n         return base64_images, base64_audios, base64_videos, document_files\n \n     @router.post(\"/runs\")\n-    async def run_agent_or_team(\n-        message: str = Form(...),\n+    async def run_agent_or_team_or_workflow(\n+        message: str = Form(None),\n         stream: bool = Form(False),\n         monitor: bool = Form(False),\n         session_id: Optional[str] = Form(None),\n         user_id: Optional[str] = Form(None),\n         files: Optional[List[UploadFile]] = File(None),\n+        agent_id: Optional[str] = Query(None),\n+        team_id: Optional[str] = Query(None),\n+        workflow_id: Optional[str] = Query(None),\n+        workflow_input: Optional[Dict[str, Any]] = Form(None),\n     ):\n         if session_id is not None and session_id != \"\":\n             logger.debug(f\"Continuing session: {session_id}\")\n@@ -260,16 +269,42 @@ def get_async_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n             logger.debug(\"Creating new session\")\n             session_id = str(uuid4())\n \n+        agent = None\n+        team = None\n+        workflow = None\n+\n+        # Only one of agent_id, team_id or workflow_id can be provided\n+        if agent_id and team_id or agent_id and workflow_id or team_id and workflow_id:\n+            raise HTTPException(status_code=400, detail=\"Only one of agent_id, team_id or workflow_id can be provided\")\n+\n+        if not agent_id and not team_id and not workflow_id:\n+            raise HTTPException(status_code=400, detail=\"One of agent_id, team_id or workflow_id must be provided\")\n+\n+        if agent_id and agents:\n+            agent = next((agent for agent in agents if agent.agent_id == agent_id), None)\n+            if agent is None:\n+                raise HTTPException(status_code=404, detail=\"Agent not found\")\n+            if not message:\n+                raise HTTPException(status_code=400, detail=\"Message is required\")\n+        if team_id and teams:\n+            team = next((team for team in teams if team.team_id == team_id), None)\n+            if team is None:\n+                raise HTTPException(status_code=404, detail=\"Team not found\")\n+            if not message:\n+                raise HTTPException(status_code=400, detail=\"Message is required\")\n+        if workflow_id and workflows:\n+            workflow = next((workflow for workflow in workflows if workflow.workflow_id == workflow_id), None)\n+            if workflow is None:\n+                raise HTTPException(status_code=404, detail=\"Workflow not found\")\n+            if not workflow_input:\n+                raise HTTPException(status_code=400, detail=\"Workflow input is required\")\n+\n         if agent:\n-            if monitor:\n-                agent.monitoring = True\n-            else:\n-                agent.monitoring = False\n+            agent.monitoring = bool(monitor)\n         elif team:\n-            if monitor:\n-                team.monitoring = True\n-            else:\n-                team.monitoring = False\n+            team.monitoring = bool(monitor)\n+        elif workflow:\n+            workflow.monitoring = bool(monitor)\n \n         base64_images: List[Image] = []\n         base64_audios: List[Audio] = []\n@@ -309,6 +344,14 @@ def get_async_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n                     ),\n                     media_type=\"text/event-stream\",\n                 )\n+            elif workflow:\n+                workflow_instance = workflow.deep_copy(update={\"workflow_id\": workflow_id})\n+                workflow_instance.user_id = user_id\n+                workflow_instance.session_name = None\n+                return StreamingResponse(\n+                    (json.dumps(asdict(result)) for result in await workflow_instance.arun(**(workflow_input or {}))),\n+                    media_type=\"text/event-stream\",\n+                )\n         else:\n             if agent:\n                 run_response = cast(\n@@ -336,5 +379,10 @@ def get_async_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n                     stream=False,\n                 )\n                 return team_run_response.to_dict()\n+            elif workflow:\n+                workflow_instance = workflow.deep_copy(update={\"workflow_id\": workflow_id})\n+                workflow_instance.user_id = user_id\n+                workflow_instance.session_name = None\n+                return (await workflow_instance.arun(**(workflow_input or {}))).to_dict()\n \n     return router\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/app/fastapi/sync_router.py",
            "diff": "diff --git a/libs/agno/agno/app/fastapi/sync_router.py b/libs/agno/agno/app/fastapi/sync_router.py\nindex 2398cd27c..f74437c7a 100644\n--- a/libs/agno/agno/app/fastapi/sync_router.py\n+++ b/libs/agno/agno/app/fastapi/sync_router.py\n@@ -1,8 +1,10 @@\n+import json\n+from dataclasses import asdict\n from io import BytesIO\n-from typing import Generator, List, Optional, cast\n+from typing import Any, Dict, Generator, List, Optional, cast\n from uuid import uuid4\n \n-from fastapi import APIRouter, File, Form, HTTPException, UploadFile\n+from fastapi import APIRouter, File, Form, HTTPException, Query, UploadFile\n from fastapi.responses import StreamingResponse\n \n from agno.agent.agent import Agent, RunResponse\n@@ -13,6 +15,7 @@ from agno.run.response import RunEvent\n from agno.run.team import TeamRunResponse\n from agno.team.team import Team\n from agno.utils.log import logger\n+from agno.workflow.workflow import Workflow\n \n \n def agent_chat_response_streamer(\n@@ -81,11 +84,13 @@ def team_chat_response_streamer(\n         return\n \n \n-def get_sync_router(agent: Optional[Agent] = None, team: Optional[Team] = None) -> APIRouter:\n+def get_sync_router(\n+    agents: Optional[List[Agent]] = None, teams: Optional[List[Team]] = None, workflows: Optional[List[Workflow]] = None\n+) -> APIRouter:\n     router = APIRouter()\n \n-    if agent is None and team is None:\n-        raise ValueError(\"Either agent or team must be provided.\")\n+    if agents is None and teams is None and workflows is None:\n+        raise ValueError(\"Either agents, teams or workflows must be provided.\")\n \n     @router.get(\"/status\")\n     def status():\n@@ -246,10 +251,14 @@ def get_sync_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n         return base64_images, base64_audios, base64_videos, document_files\n \n     @router.post(\"/runs\")\n-    def run_agent_or_team(\n-        message: str = Form(...),\n+    def run_agent_or_team_or_workflow(\n+        message: str = Form(None),\n         stream: bool = Form(True),\n         monitor: bool = Form(False),\n+        agent_id: Optional[str] = Query(None),\n+        team_id: Optional[str] = Query(None),\n+        workflow_id: Optional[str] = Query(None),\n+        workflow_input: Optional[Dict[str, Any]] = Form(None),\n         session_id: Optional[str] = Form(None),\n         user_id: Optional[str] = Form(None),\n         files: Optional[List[UploadFile]] = File(None),\n@@ -260,16 +269,42 @@ def get_sync_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n             logger.debug(\"Creating new session\")\n             session_id = str(uuid4())\n \n+        # Only one of agent_id, team_id or workflow_id can be provided\n+        if agent_id and team_id or agent_id and workflow_id or team_id and workflow_id:\n+            raise HTTPException(status_code=400, detail=\"Only one of agent_id, team_id or workflow_id can be provided\")\n+\n+        if not agent_id and not team_id and not workflow_id:\n+            raise HTTPException(status_code=400, detail=\"One of agent_id, team_id or workflow_id must be provided\")\n+\n+        agent = None\n+        team = None\n+        workflow = None\n+\n+        if agent_id and agents:\n+            agent = next((agent for agent in agents if agent.agent_id == agent_id), None)\n+            if agent is None:\n+                raise HTTPException(status_code=404, detail=\"Agent not found\")\n+            if not message:\n+                raise HTTPException(status_code=400, detail=\"Message is required\")\n+        if team_id and teams:\n+            team = next((team for team in teams if team.team_id == team_id), None)\n+            if team is None:\n+                raise HTTPException(status_code=404, detail=\"Team not found\")\n+            if not message:\n+                raise HTTPException(status_code=400, detail=\"Message is required\")\n+        if workflow_id and workflows:\n+            workflow = next((workflow for workflow in workflows if workflow.workflow_id == workflow_id), None)\n+            if workflow is None:\n+                raise HTTPException(status_code=404, detail=\"Workflow not found\")\n+            if not workflow_input:\n+                raise HTTPException(status_code=400, detail=\"Workflow input is required\")\n+\n         if agent:\n-            if monitor:\n-                agent.monitoring = True\n-            else:\n-                agent.monitoring = False\n+            agent.monitoring = bool(monitor)\n         elif team:\n-            if monitor:\n-                team.monitoring = True\n-            else:\n-                team.monitoring = False\n+            team.monitoring = bool(monitor)\n+        elif workflow:\n+            workflow.monitoring = bool(monitor)\n \n         if files:\n             if agent:\n@@ -305,6 +340,14 @@ def get_sync_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n                     ),\n                     media_type=\"text/event-stream\",\n                 )\n+            elif workflow:\n+                workflow_instance = workflow.deep_copy(update={\"workflow_id\": workflow_id})\n+                workflow_instance.user_id = user_id\n+                workflow_instance.session_name = None\n+                return StreamingResponse(\n+                    (json.dumps(asdict(result)) for result in workflow_instance.run(**(workflow_input or {}))),\n+                    media_type=\"text/event-stream\",\n+                )\n         else:\n             if agent:\n                 run_response = cast(\n@@ -332,5 +375,10 @@ def get_sync_router(agent: Optional[Agent] = None, team: Optional[Team] = None)\n                     stream=False,\n                 )\n                 return team_run_response.to_dict()\n+            elif workflow:\n+                workflow_instance = workflow.deep_copy(update={\"workflow_id\": workflow_id})\n+                workflow_instance.user_id = user_id\n+                workflow_instance.session_name = None\n+                return workflow_instance.run(**(workflow_input or {})).to_dict()\n \n     return router\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/app/playground/app.py",
            "diff": "diff --git a/libs/agno/agno/app/playground/app.py b/libs/agno/agno/app/playground/app.py\nindex 461cb02e9..4dc72a11c 100644\n--- a/libs/agno/agno/app/playground/app.py\n+++ b/libs/agno/agno/app/playground/app.py\n@@ -15,6 +15,7 @@ from agno.agent.agent import Agent\n from agno.api.playground import PlaygroundEndpointCreate\n from agno.app.playground.async_router import get_async_playground_router\n from agno.app.playground.sync_router import get_sync_playground_router\n+from agno.app.utils import generate_id\n from agno.cli.console import console\n from agno.cli.settings import agno_cli_settings\n from agno.playground.settings import PlaygroundSettings\n@@ -43,10 +44,13 @@ class Playground:\n         self.agents: Optional[List[Agent]] = agents\n         self.workflows: Optional[List[Workflow]] = workflows\n         self.teams: Optional[List[Team]] = teams\n+\n         self.settings: PlaygroundSettings = settings or PlaygroundSettings()\n         self.api_app: Optional[FastAPI] = api_app\n         self.router: Optional[APIRouter] = router\n+\n         self.endpoints_created: Optional[PlaygroundEndpointCreate] = None\n+\n         self.app_id: Optional[str] = app_id\n         self.name: Optional[str] = name\n         self.monitoring = monitoring\n@@ -237,10 +241,3 @@ class Playground:\n         }\n         payload = {k: v for k, v in payload.items() if v is not None}\n         return payload\n-\n-\n-def generate_id(name: Optional[str] = None) -> str:\n-    if name:\n-        return name.lower().replace(\" \", \"-\").replace(\"_\", \"-\")\n-    else:\n-        return str(uuid4())\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/document/reader/url_reader.py",
            "diff": "diff --git a/libs/agno/agno/document/reader/url_reader.py b/libs/agno/agno/document/reader/url_reader.py\nindex 2b49e5a8a..8a607bad4 100644\n--- a/libs/agno/agno/document/reader/url_reader.py\n+++ b/libs/agno/agno/document/reader/url_reader.py\n@@ -1,5 +1,4 @@\n-import asyncio\n-from time import sleep\n+\n from typing import List, Optional\n from urllib.parse import urlparse\n \n@@ -7,7 +6,8 @@ import httpx\n \n from agno.document.base import Document\n from agno.document.reader.base import Reader\n-from agno.utils.log import log_debug, log_info, logger\n+from agno.utils.http import async_fetch_with_retry, fetch_with_retry\n+from agno.utils.log import log_debug\n \n \n class URLReader(Reader):\n@@ -21,31 +21,9 @@ class URLReader(Reader):\n         if not url:\n             raise ValueError(\"No url provided\")\n \n-        log_info(f\"Reading: {url}\")\n+        log_debug(f\"Reading: {url}\")\n         # Retry the request up to 3 times with exponential backoff\n-        for attempt in range(3):\n-            try:\n-                response = httpx.get(url, proxy=self.proxy) if self.proxy else httpx.get(url)\n-                break\n-            except httpx.RequestError as e:\n-                if attempt == 2:  # Last attempt\n-                    logger.error(f\"Failed to fetch URL after 3 attempts: {e}\")\n-                    raise\n-                wait_time = 2**attempt  # Exponential backoff: 1, 2, 4 seconds\n-                logger.warning(f\"Request failed, retrying in {wait_time} seconds...\")\n-                sleep(wait_time)\n-\n-        try:\n-            log_debug(f\"Status: {response.status_code}\")\n-            log_debug(f\"Content size: {len(response.content)} bytes\")\n-        except Exception:\n-            pass\n-\n-        try:\n-            response.raise_for_status()\n-        except httpx.HTTPStatusError as e:\n-            logger.error(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n-            raise\n+        response = fetch_with_retry(url, proxy=self.proxy)\n \n         document = self._create_document(url, response.text)\n         if self.chunk:\n@@ -57,37 +35,15 @@ class URLReader(Reader):\n         if not url:\n             raise ValueError(\"No url provided\")\n \n-        log_info(f\"Reading async: {url}\")\n+        log_debug(f\"Reading async: {url}\")\n         client_args = {\"proxy\": self.proxy} if self.proxy else {}\n         async with httpx.AsyncClient(**client_args) as client:  # type: ignore\n-            for attempt in range(3):\n-                try:\n-                    response = await client.get(url)\n-                    break\n-                except httpx.RequestError as e:\n-                    if attempt == 2:  # Last attempt\n-                        logger.error(f\"Failed to fetch URL after 3 attempts: {e}\")\n-                        raise\n-                    wait_time = 2**attempt\n-                    logger.warning(f\"Request failed, retrying in {wait_time} seconds...\")\n-                    await asyncio.sleep(wait_time)\n-\n-            try:\n-                log_debug(f\"Status: {response.status_code}\")\n-                log_debug(f\"Content size: {len(response.content)} bytes\")\n-            except Exception:\n-                pass\n+            response = await async_fetch_with_retry(url, client=client)\n \n-            try:\n-                response.raise_for_status()\n-            except httpx.HTTPStatusError as e:\n-                logger.error(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n-                raise\n-\n-            document = self._create_document(url, response.text)\n-            if self.chunk:\n-                return await self.chunk_documents_async([document])\n-            return [document]\n+        document = self._create_document(url, response.text)\n+        if self.chunk:\n+            return await self.chunk_documents_async([document])\n+        return [document]\n \n     def _create_document(self, url: str, content: str) -> Document:\n         \"\"\"Helper method to create a document from URL content\"\"\"\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/knowledge/light_rag.py",
            "diff": "diff --git a/libs/agno/agno/knowledge/light_rag.py b/libs/agno/agno/knowledge/light_rag.py\nnew file mode 100644\nindex 000000000..18767a488\n--- /dev/null\n+++ b/libs/agno/agno/knowledge/light_rag.py\n@@ -0,0 +1,294 @@\n+from __future__ import annotations\n+\n+from pathlib import Path\n+from typing import Any, ClassVar, Dict, Iterator, List, Optional, Union\n+import textract\n+\n+from pydantic import Field\n+\n+from agno.document import Document\n+from agno.document.reader.markdown_reader import MarkdownReader\n+from agno.document.reader.pdf_reader import PDFUrlReader\n+from agno.document.reader.url_reader import URLReader\n+from agno.knowledge.agent import AgentKnowledge\n+from agno.utils.log import log_debug, log_info, logger\n+\n+\n+class LightRagKnowledgeBase(AgentKnowledge):\n+    \"\"\"LightRAG-based knowledge base for document processing and retrieval.\"\"\"\n+    \n+    # Constants\n+    DEFAULT_SERVER_URL: ClassVar[str] = \"http://localhost:9621\"\n+    SUPPORTED_EXTENSIONS: ClassVar[List[str]] = [\".pdf\", \".md\", \".txt\"]\n+    \n+    lightrag_server_url: str = DEFAULT_SERVER_URL\n+    path: Optional[Union[str, Path, List[Dict[str, Union[str, Dict[str, Any]]]]]] = None\n+    urls: Optional[Union[List[str], List[Dict[str, Union[str, Dict[str, Any]]]]]] = None\n+    exclude_files: List[str] = Field(default_factory=list)\n+\n+    pdf_url_reader: PDFUrlReader = PDFUrlReader()\n+    markdown_reader: MarkdownReader = MarkdownReader()\n+    url_reader: URLReader = URLReader()\n+    \n+    @property\n+    def document_lists(self) -> Iterator[List[Document]]:\n+        \"\"\"Iterate over documents and yield lists of Document objects.\"\"\"\n+        # Convert text lists to Document objects to match parent class signature\n+        for text_list in self._text_document_lists():\n+            documents = [Document(content=text) for text in text_list]\n+            yield documents\n+\n+    def _text_document_lists(self) -> Iterator[List[str]]:\n+        \"\"\"Internal method to iterate over documents and yield lists of text content.\"\"\"\n+        if self.path is not None:\n+            yield from self._process_paths()\n+        \n+        if self.urls is not None:\n+            yield from self._process_urls()\n+\n+        if self.urls is None and self.path is None:\n+            raise ValueError(\"Path or URLs are not set\")\n+\n+    def _process_paths(self) -> Iterator[List[str]]:\n+        \"\"\"Process path-based documents.\"\"\"\n+        if self.path is None:\n+            return\n+            \n+        if isinstance(self.path, list):\n+            for item in self.path:\n+                if isinstance(item, dict) and \"path\" in item:\n+                    path_value = item[\"path\"]\n+                    if isinstance(path_value, (str, Path)):\n+                        yield from self._process_single_path(Path(path_value))\n+        else:\n+            yield from self._process_single_path(Path(self.path))\n+\n+    def _process_single_path(self, path: Path) -> Iterator[List[str]]:\n+        \"\"\"Process a single path (file or directory).\"\"\"\n+        if path.is_dir():\n+            for file_path in path.glob(\"**/*\"):\n+                if file_path.is_file():\n+                    text_str = textract.process(str(file_path)).decode('utf-8')\n+                    yield [text_str]\n+        elif path.exists() and path.is_file():\n+            if path.suffix == '.md':\n+                documents = self.markdown_reader.read(file=path)\n+                text_contents = [doc.content for doc in documents]\n+                yield text_contents\n+            elif path.suffix == '.pdf':\n+                text_str = textract.process(str(path)).decode('utf-8')\n+                yield [text_str]\n+            else:\n+                text_str = textract.process(str(path)).decode('utf-8')\n+                yield [text_str]\n+\n+    def _process_urls(self) -> Iterator[List[str]]:\n+        \"\"\"Process URL-based documents.\"\"\"\n+        if self.urls is None:\n+            return\n+            \n+        log_info(f\"Processing URLs: {self.urls}\")\n+        for item in self.urls:\n+            if isinstance(item, dict) and \"url\" in item:\n+                url = item[\"url\"]\n+                config = item.get(\"metadata\", {})\n+                if isinstance(url, str) and isinstance(config, dict):\n+                    yield from self._process_url_with_metadata(url, config)\n+            elif isinstance(item, str):\n+                yield from self._process_simple_url(item)\n+\n+    def _process_url_with_metadata(self, url: str, config: Dict[str, Any]) -> Iterator[List[str]]:\n+        \"\"\"Process URL with metadata configuration.\"\"\"\n+        log_debug(f\"Processing URL with metadata - URL: {url}, Config: {config}\")\n+        if self._is_valid_url(url):\n+            if url.endswith(\".pdf\"):\n+                log_info(f\"READING PDF URL: {url}\")\n+                documents = self.pdf_url_reader.read(url=url)\n+                text_contents = [doc.content for doc in documents]\n+                yield text_contents\n+            else:\n+                log_debug(f\"URL is valid, reading documents from: {url}\")\n+                documents = self.url_reader.read(url=url)\n+                text_contents = []\n+                for doc in documents:\n+                    if config:\n+                        log_debug(f\"Adding metadata {config} to document from URL: {url}\")\n+                        doc.meta_data.update(config)\n+                    text_contents.append(doc.content)\n+                yield text_contents\n+\n+    def _process_simple_url(self, url: str) -> Iterator[List[str]]:\n+        \"\"\"Process a simple URL without metadata.\"\"\"\n+        log_info(f\"Processing simple URL: {url}\")\n+        if self._is_valid_url(url):\n+            log_info(f\"Simple URL is valid, reading documents from: {url}\")\n+            if url.endswith(\".pdf\"):\n+                documents = self.pdf_url_reader.read(url=url)\n+                text_contents = [doc.content for doc in documents]\n+                yield text_contents\n+            else:\n+                documents = self.url_reader.read(url=url)\n+                text_contents = [doc.content for doc in documents]\n+                yield text_contents\n+\n+    async def load(\n+        self,\n+        recreate: bool = False,\n+        upsert: bool = False, \n+        skip_existing: bool = True,\n+    ) -> None:\n+        \"\"\"Load the knowledge base to the LightRAG server asynchronously.\n+        \n+        Note: The LightRAG implementation is inherently async.\n+        \"\"\"\n+        logger.debug(\"Loading LightRagKnowledgeBase\")\n+        for text_list in self._text_document_lists():\n+            for text in text_list:\n+                await self._insert_text(text)\n+        \n+    async def aload(\n+        self,\n+        recreate: bool = False,\n+        upsert: bool = False,\n+        skip_existing: bool = True,\n+    ) -> None:\n+        \"\"\"Load all documents into the LightRAG server asynchronously.\"\"\"\n+        # Delegate to load() since both are async for LightRAG\n+        await self.load(recreate=recreate, upsert=upsert, skip_existing=skip_existing)\n+        \n+    async def load_text(\n+        self, \n+        text: str, \n+        upsert: bool = False, \n+        skip_existing: bool = True, \n+        filters: Optional[Dict[str, Any]] = None\n+    ) -> None:\n+        \"\"\"Load a single text into the LightRAG server asynchronously.\"\"\"\n+        await self._insert_text(text)\n+        \n+    async def async_search(\n+        self, \n+        query: str, \n+        num_documents: Optional[int] = None, \n+        filters: Optional[Dict[str, Any]] = None\n+    ) -> List[Document]:\n+        \"\"\"Override the async_search method from AgentKnowledge to query the LightRAG server.\"\"\"\n+        import httpx\n+        \n+        logger.info(f\"Querying LightRAG server with query: {query}\")\n+        mode = \"hybrid\"  # Default mode, can be \"local\", \"global\", or \"hybrid\"\n+        \n+        async with httpx.AsyncClient() as client:\n+            response = await client.post(\n+                f\"{self.lightrag_server_url}/query\",\n+                json={\"query\": query, \"mode\": mode},\n+                headers={\"Content-Type\": \"application/json\"}\n+            )\n+            response.raise_for_status()\n+            result = response.json()\n+            logger.info(f\"Query result: {result}\")\n+            \n+            # Convert result to Document objects to match parent class signature\n+            if isinstance(result, dict) and 'response' in result:\n+                return [Document(content=result['response'], meta_data={\"query\": query, \"mode\": mode})]\n+            elif isinstance(result, list):\n+                return [Document(content=str(item), meta_data={\"query\": query, \"mode\": mode}) for item in result]\n+            else:\n+                return [Document(content=str(result), meta_data={\"query\": query, \"mode\": mode})]\n+    \n+    async def _insert_text(self, text: str) -> Dict[str, Any]:\n+        \"\"\"Insert text into the LightRAG server.\"\"\"\n+        import httpx\n+        \n+        async with httpx.AsyncClient() as client:\n+            response = await client.post(\n+                f\"{self.lightrag_server_url}/documents/text\",\n+                json={\"text\": text},\n+                headers={\"Content-Type\": \"application/json\"}\n+            )\n+            response.raise_for_status()\n+            result = response.json()\n+            logger.debug(f\"Text insertion result: {result}\")\n+            return result\n+        \n+    def _is_valid_url(self, url: str) -> bool:\n+        \"\"\"Helper to check if URL is valid.\"\"\"\n+        if not any(url.endswith(ext) for ext in self.SUPPORTED_EXTENSIONS):\n+            logger.error(\n+                f\"Unsupported URL: {url}. \"\n+                f\"Supported file types: {', '.join(self.SUPPORTED_EXTENSIONS)}\"\n+            )\n+            return False\n+        return True\n+\n+\n+async def lightrag_retriever(\n+    query: str,\n+    num_documents: int = 5,\n+    mode: str = \"hybrid\",  # Default mode, can be \"local\", \"global\", or \"hybrid\"\n+    lightrag_server_url: str = \"http://localhost:9621\"\n+) -> Optional[List[Dict[str, Any]]]:\n+    \"\"\"\n+    Custom retriever function to search the LightRAG server for relevant documents.\n+\n+    Args:\n+        query: The search query string\n+        num_documents: Number of documents to retrieve (currently unused by LightRAG)\n+        mode: Query mode - \"local\", \"global\", or \"hybrid\"\n+        lightrag_server_url: URL of the LightRAG server\n+\n+    Returns:\n+        List of retrieved documents or None if search fails\n+    \"\"\"\n+    try:\n+        import httpx\n+        \n+        async with httpx.AsyncClient(timeout=30.0) as client:\n+            response = await client.post(\n+                f\"{lightrag_server_url}/query\",\n+                json={\"query\": query, \"mode\": mode},\n+                headers={\"Content-Type\": \"application/json\"}\n+            )\n+            \n+            response.raise_for_status()\n+            result = response.json()\n+            \n+            return _format_lightrag_response(result, query, mode)\n+                \n+    except httpx.RequestError as e:\n+        logger.error(f\"HTTP Request Error: {type(e).__name__}: {str(e)}\")\n+        return None\n+    except httpx.HTTPStatusError as e:\n+        logger.error(f\"HTTP Status Error: {e.response.status_code} - {e.response.text}\")\n+        return None\n+    except Exception as e:\n+        logger.error(f\"Unexpected error during LightRAG server search: {type(e).__name__}: {str(e)}\")\n+        import traceback\n+        logger.error(f\"Full traceback: {traceback.format_exc()}\")\n+        return None\n+\n+\n+def _format_lightrag_response(\n+    result: Any, \n+    query: str, \n+    mode: str\n+) -> List[Dict[str, Any]]:\n+    \"\"\"Format LightRAG server response to expected document format.\"\"\"\n+    # LightRAG server returns a dict with 'response' key, but we expect a list of documents\n+    # Convert the response to the expected format\n+    if isinstance(result, dict) and 'response' in result:\n+        # Wrap the response in a document-like structure\n+        return [{\n+            \"content\": result['response'],\n+            \"source\": \"lightrag\",\n+            \"metadata\": {\"query\": query, \"mode\": mode}\n+        }]\n+    elif isinstance(result, list):\n+        return result\n+    else:\n+        # If it's a string or other format, wrap it\n+        return [{\n+            \"content\": str(result),\n+            \"source\": \"lightrag\", \n+            \"metadata\": {\"query\": query, \"mode\": mode}\n+        }]\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/agno/tools/gmail.py",
            "diff": "diff --git a/libs/agno/agno/tools/gmail.py b/libs/agno/agno/tools/gmail.py\nindex e4bf3613a..dabb9390a 100644\n--- a/libs/agno/agno/tools/gmail.py\n+++ b/libs/agno/agno/tools/gmail.py\n@@ -26,14 +26,17 @@ How to Get These Credentials:\n      * Client ID (GOOGLE_CLIENT_ID)\n      * Client Secret (GOOGLE_CLIENT_SECRET)\n    - The Project ID (GOOGLE_PROJECT_ID) is visible in the project dropdown at the top of the page\n+   \n+5. Add auth redirect URI:\n+   - Go to https://console.cloud.google.com/auth/clients and add the redirect URI as http://127.0.0.1/\n \n-5. Set up environment variables:\n+6. Set up environment variables:\n    Create a .envrc file in your project root with:\n    ```\n    export GOOGLE_CLIENT_ID=your_client_id_here\n    export GOOGLE_CLIENT_SECRET=your_client_secret_here\n    export GOOGLE_PROJECT_ID=your_project_id_here\n-   export GOOGLE_REDIRECT_URI=http://localhost  # Default value\n+   export GOOGLE_REDIRECT_URI=http://127.0.0.1/  # Default value\n    ```\n \n Note: The first time you run the application, it will open a browser window for OAuth authentication.\n@@ -41,16 +44,19 @@ A token.json file will be created to store the authentication credentials for fu\n \"\"\"\n \n import base64\n+import mimetypes\n import re\n from datetime import datetime, timedelta\n from functools import wraps\n from os import getenv\n from pathlib import Path\n-from typing import Any, List, Optional\n+from typing import Any, List, Optional, Union\n \n from agno.tools import Toolkit\n \n try:\n+    from email.mime.application import MIMEApplication\n+    from email.mime.multipart import MIMEMultipart\n     from email.mime.text import MIMEText\n \n     from google.auth.transport.requests import Request\n@@ -397,7 +403,14 @@ class GmailTools(Toolkit):\n             return f\"Unexpected error retrieving emails by date: {type(error).__name__}: {error}\"\n \n     @authenticate\n-    def create_draft_email(self, to: str, subject: str, body: str, cc: Optional[str] = None) -> str:\n+    def create_draft_email(\n+        self,\n+        to: str,\n+        subject: str,\n+        body: str,\n+        cc: Optional[str] = None,\n+        attachments: Optional[Union[str, List[str]]] = None,\n+    ) -> str:\n         \"\"\"\n         Create and save a draft email. to and cc are comma separated string of email ids\n         Args:\n@@ -405,18 +418,42 @@ class GmailTools(Toolkit):\n             subject (str): Email subject\n             body (str): Email body content\n             cc (Optional[str]): Comma separated string of CC email addresses (optional)\n+            attachments (Optional[Union[str, List[str]]]): File path(s) for attachments (optional)\n \n         Returns:\n             str: Stringified dictionary containing draft email details including id\n         \"\"\"\n         self._validate_email_params(to, subject, body)\n-        message = self._create_message(to.split(\",\"), subject, body, cc.split(\",\") if cc else None)\n+\n+        # Process attachments\n+        attachment_files = []\n+        if attachments:\n+            if isinstance(attachments, str):\n+                attachment_files = [attachments]\n+            else:\n+                attachment_files = attachments\n+\n+            # Validate attachment files\n+            for file_path in attachment_files:\n+                if not Path(file_path).exists():\n+                    raise ValueError(f\"Attachment file not found: {file_path}\")\n+\n+        message = self._create_message(\n+            to.split(\",\"), subject, body, cc.split(\",\") if cc else None, attachments=attachment_files\n+        )\n         draft = {\"message\": message}\n         draft = self.service.users().drafts().create(userId=\"me\", body=draft).execute()  # type: ignore\n         return str(draft)\n \n     @authenticate\n-    def send_email(self, to: str, subject: str, body: str, cc: Optional[str] = None) -> str:\n+    def send_email(\n+        self,\n+        to: str,\n+        subject: str,\n+        body: str,\n+        cc: Optional[str] = None,\n+        attachments: Optional[Union[str, List[str]]] = None,\n+    ) -> str:\n         \"\"\"\n         Send an email immediately. to and cc are comma separated string of email ids\n         Args:\n@@ -424,19 +461,43 @@ class GmailTools(Toolkit):\n             subject (str): Email subject\n             body (str): Email body content\n             cc (Optional[str]): Comma separated string of CC email addresses (optional)\n+            attachments (Optional[Union[str, List[str]]]): File path(s) for attachments (optional)\n \n         Returns:\n             str: Stringified dictionary containing sent email details including id\n         \"\"\"\n         self._validate_email_params(to, subject, body)\n+\n+        # Process attachments\n+        attachment_files = []\n+        if attachments:\n+            if isinstance(attachments, str):\n+                attachment_files = [attachments]\n+            else:\n+                attachment_files = attachments\n+\n+            # Validate attachment files\n+            for file_path in attachment_files:\n+                if not Path(file_path).exists():\n+                    raise ValueError(f\"Attachment file not found: {file_path}\")\n+\n         body = body.replace(\"\\n\", \"<br>\")\n-        message = self._create_message(to.split(\",\"), subject, body, cc.split(\",\") if cc else None)\n+        message = self._create_message(\n+            to.split(\",\"), subject, body, cc.split(\",\") if cc else None, attachments=attachment_files\n+        )\n         message = self.service.users().messages().send(userId=\"me\", body=message).execute()  # type: ignore\n         return str(message)\n \n     @authenticate\n     def send_email_reply(\n-        self, thread_id: str, message_id: str, to: str, subject: str, body: str, cc: Optional[str] = None\n+        self,\n+        thread_id: str,\n+        message_id: str,\n+        to: str,\n+        subject: str,\n+        body: str,\n+        cc: Optional[str] = None,\n+        attachments: Optional[Union[str, List[str]]] = None,\n     ) -> str:\n         \"\"\"\n         Respond to an existing email thread.\n@@ -448,6 +509,7 @@ class GmailTools(Toolkit):\n             subject (str): Email subject (prefixed with \"Re:\" if not already).\n             body (str): Email body content.\n             cc (Optional[str]): Comma-separated CC email addresses (optional).\n+            attachments (Optional[Union[str, List[str]]]): File path(s) for attachments (optional)\n \n         Returns:\n             str: Stringified dictionary containing sent email details including id.\n@@ -458,9 +520,28 @@ class GmailTools(Toolkit):\n         if not subject.lower().startswith(\"re:\"):\n             subject = f\"Re: {subject}\"\n \n+        # Process attachments\n+        attachment_files = []\n+        if attachments:\n+            if isinstance(attachments, str):\n+                attachment_files = [attachments]\n+            else:\n+                attachment_files = attachments\n+\n+            # Validate attachment files\n+            for file_path in attachment_files:\n+                if not Path(file_path).exists():\n+                    raise ValueError(f\"Attachment file not found: {file_path}\")\n+\n         body = body.replace(\"\\n\", \"<br>\")\n         message = self._create_message(\n-            to.split(\",\"), subject, body, cc.split(\",\") if cc else None, thread_id, message_id\n+            to.split(\",\"),\n+            subject,\n+            body,\n+            cc.split(\",\") if cc else None,\n+            thread_id,\n+            message_id,\n+            attachments=attachment_files,\n         )\n         message = self.service.users().messages().send(userId=\"me\", body=message).execute()  # type: ignore\n         return str(message)\n@@ -511,9 +592,43 @@ class GmailTools(Toolkit):\n         cc: Optional[List[str]] = None,\n         thread_id: Optional[str] = None,\n         message_id: Optional[str] = None,\n+        attachments: Optional[List[str]] = None,\n     ) -> dict:\n         body = body.replace(\"\\\\n\", \"\\n\")\n-        message = MIMEText(body, \"html\")\n+\n+        # Create multipart message if attachments exist, otherwise simple text message\n+        message: Union[MIMEMultipart, MIMEText]\n+        if attachments:\n+            message = MIMEMultipart()\n+\n+            # Add the text body\n+            text_part = MIMEText(body, \"html\")\n+            message.attach(text_part)\n+\n+            # Add attachments\n+            for file_path in attachments:\n+                file_path_obj = Path(file_path)\n+                if not file_path_obj.exists():\n+                    continue\n+\n+                # Guess the content type based on the file extension\n+                content_type, encoding = mimetypes.guess_type(file_path)\n+                if content_type is None or encoding is not None:\n+                    content_type = \"application/octet-stream\"\n+\n+                main_type, sub_type = content_type.split(\"/\", 1)\n+\n+                # Read file and create attachment\n+                with open(file_path, \"rb\") as file:\n+                    attachment_data = file.read()\n+\n+                attachment = MIMEApplication(attachment_data, _subtype=sub_type)\n+                attachment.add_header(\"Content-Disposition\", \"attachment\", filename=file_path_obj.name)\n+                message.attach(attachment)\n+        else:\n+            message = MIMEText(body, \"html\")\n+\n+        # Set headers\n         message[\"to\"] = \", \".join(to)\n         message[\"from\"] = \"me\"\n         message[\"subject\"] = subject\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/tests/unit/reader/test_url_reader.py",
            "diff": "diff --git a/libs/agno/tests/unit/reader/test_url_reader.py b/libs/agno/tests/unit/reader/test_url_reader.py\nindex 655daf7eb..ef2c6ab85 100644\n--- a/libs/agno/tests/unit/reader/test_url_reader.py\n+++ b/libs/agno/tests/unit/reader/test_url_reader.py\n@@ -21,7 +21,7 @@ def test_read_url_success(mock_response):\n     \"\"\"Test successful URL reading\"\"\"\n     url = \"https://example.com\"\n \n-    with patch(\"httpx.get\", return_value=mock_response):\n+    with patch(\"agno.document.reader.url_reader.fetch_with_retry\", return_value=mock_response):\n         reader = URLReader()\n         reader.chunk = False\n         documents = reader.read(url)\n@@ -36,7 +36,7 @@ def test_read_url_with_path(mock_response):\n     \"\"\"Test URL reading with path components\"\"\"\n     url = \"https://example.com/blog/post-1\"\n \n-    with patch(\"httpx.get\", return_value=mock_response):\n+    with patch(\"agno.document.reader.url_reader.fetch_with_retry\", return_value=mock_response):\n         reader = URLReader()\n         reader.chunk = False\n         documents = reader.read(url)\n@@ -53,38 +53,38 @@ def test_read_empty_url():\n         reader.read(\"\")\n \n \n-def test_read_url_with_retry(mock_response):\n-    \"\"\"Test URL reading with retry mechanism\"\"\"\n+def test_read_url_with_proxy(mock_response):\n+    \"\"\"Test URL reading with proxy\"\"\"\n     url = \"https://example.com\"\n+    proxy = \"http://proxy.example.com:8080\"\n \n-    with patch(\"httpx.get\", side_effect=[httpx.RequestError(\"Connection error\"), mock_response]):\n-        reader = URLReader()\n+    with patch(\"agno.document.reader.url_reader.fetch_with_retry\", return_value=mock_response) as mock_fetch:\n+        reader = URLReader(proxy=proxy)\n         reader.chunk = False\n         documents = reader.read(url)\n \n+        # Verify the proxy was passed to fetch_with_retry\n+        mock_fetch.assert_called_once_with(url, proxy=proxy)\n         assert len(documents) == 1\n         assert documents[0].content == \"Hello, World!\"\n \n \n-def test_read_url_max_retries():\n-    \"\"\"Test URL reading with max retries exceeded\"\"\"\n+def test_read_url_request_error():\n+    \"\"\"Test URL reading when fetch_with_retry raises RequestError\"\"\"\n     url = \"https://example.com\"\n \n-    with patch(\"httpx.get\", side_effect=httpx.RequestError(\"Connection error\")):\n+    with patch(\"agno.document.reader.url_reader.fetch_with_retry\", side_effect=httpx.RequestError(\"Connection failed\")):\n         reader = URLReader()\n         with pytest.raises(httpx.RequestError):\n             reader.read(url)\n \n \n-def test_read_url_http_error(mock_response):\n-    \"\"\"Test URL reading with HTTP error\"\"\"\n+def test_read_url_http_error():\n+    \"\"\"Test URL reading when fetch_with_retry raises HTTPStatusError\"\"\"\n     url = \"https://example.com\"\n-    mock_response.status_code = 404\n-    mock_response.raise_for_status.side_effect = httpx.HTTPStatusError(\n-        \"404 Not Found\", request=Mock(), response=mock_response\n-    )\n-\n-    with patch(\"httpx.get\", return_value=mock_response):\n+    \n+    with patch(\"agno.document.reader.url_reader.fetch_with_retry\", \n+               side_effect=httpx.HTTPStatusError(\"404 Not Found\", request=Mock(), response=Mock())):\n         reader = URLReader()\n         with pytest.raises(httpx.HTTPStatusError):\n             reader.read(url)\n@@ -95,7 +95,7 @@ def test_chunking(mock_response):\n     url = \"https://example.com\"\n     mock_response.text = \"Hello, world! \" * 1000\n \n-    with patch(\"httpx.get\", return_value=mock_response):\n+    with patch(\"agno.document.reader.url_reader.fetch_with_retry\", return_value=mock_response):\n         reader = URLReader()\n         reader.chunk = True\n         reader.chunking_strategy = FixedSizeChunking(chunk_size=100)\n@@ -107,18 +107,11 @@ def test_chunking(mock_response):\n \n \n @pytest.mark.asyncio\n-async def test_async_read_url_success():\n+async def test_async_read_url_success(mock_response):\n     \"\"\"Test successful async URL reading\"\"\"\n     url = \"https://example.com\"\n \n-    mock_response = Mock(spec=httpx.Response)\n-    mock_response.status_code = 200\n-    mock_response.text = \"Hello, World!\"\n-\n-    mock_client = AsyncMock(spec=httpx.AsyncClient)\n-    mock_client.__aenter__.return_value.get.return_value = mock_response\n-\n-    with patch(\"httpx.AsyncClient\", return_value=mock_client):\n+    with patch(\"agno.document.reader.url_reader.async_fetch_with_retry\", return_value=mock_response):\n         reader = URLReader()\n         reader.chunk = False  # Disable chunking for this test\n         documents = await reader.async_read(url)\n@@ -130,35 +123,41 @@ async def test_async_read_url_success():\n \n \n @pytest.mark.asyncio\n-async def test_async_read_url_with_retry():\n-    \"\"\"Test async URL reading with retry mechanism\"\"\"\n-    url = \"https://example.com\"\n+async def test_async_read_empty_url():\n+    \"\"\"Test async reading with empty URL\"\"\"\n+    reader = URLReader()\n+    with pytest.raises(ValueError, match=\"No url provided\"):\n+        await reader.async_read(\"\")\n \n-    mock_response = Mock(spec=httpx.Response)\n-    mock_response.status_code = 200\n-    mock_response.text = \"Hello, World!\"\n \n-    mock_client = AsyncMock(spec=httpx.AsyncClient)\n-    mock_client.__aenter__.return_value.get.side_effect = [httpx.RequestError(\"Connection error\"), mock_response]\n+@pytest.mark.asyncio\n+async def test_async_read_url_with_proxy(mock_response):\n+    \"\"\"Test async URL reading with proxy\"\"\"\n+    url = \"https://example.com\"\n+    proxy = \"http://proxy.example.com:8080\"\n \n-    with patch(\"httpx.AsyncClient\", return_value=mock_client):\n-        reader = URLReader()\n-        reader.chunk = False  # Disable chunking for this test\n+    with patch(\"agno.document.reader.url_reader.async_fetch_with_retry\", return_value=mock_response) as mock_fetch:\n+        reader = URLReader(proxy=proxy)\n+        reader.chunk = False\n         documents = await reader.async_read(url)\n \n+        # Verify the client was passed to async_fetch_with_retry\n+        mock_fetch.assert_called_once()\n+        call_args = mock_fetch.call_args\n+        assert call_args[0][0] == url  # First positional arg is url\n+        assert \"client\" in call_args[1]  # client should be in kwargs\n+        \n         assert len(documents) == 1\n         assert documents[0].content == \"Hello, World!\"\n \n \n @pytest.mark.asyncio\n-async def test_async_read_url_max_retries():\n-    \"\"\"Test async URL reading with max retries exceeded\"\"\"\n+async def test_async_read_url_request_error():\n+    \"\"\"Test async URL reading when async_fetch_with_retry raises RequestError\"\"\"\n     url = \"https://example.com\"\n \n-    mock_client = AsyncMock(spec=httpx.AsyncClient)\n-    mock_client.__aenter__.return_value.get.side_effect = httpx.RequestError(\"Connection error\")\n-\n-    with patch(\"httpx.AsyncClient\", return_value=mock_client):\n+    with patch(\"agno.document.reader.url_reader.async_fetch_with_retry\", \n+               side_effect=httpx.RequestError(\"Connection failed\")):\n         reader = URLReader()\n         with pytest.raises(httpx.RequestError):\n             await reader.async_read(url)\n@@ -166,37 +165,23 @@ async def test_async_read_url_max_retries():\n \n @pytest.mark.asyncio\n async def test_async_read_url_http_error():\n-    \"\"\"Test async URL reading with HTTP error\"\"\"\n+    \"\"\"Test async URL reading when async_fetch_with_retry raises HTTPStatusError\"\"\"\n     url = \"https://example.com\"\n \n-    mock_response = Mock(spec=httpx.Response)\n-    mock_response.status_code = 404\n-    mock_response.raise_for_status.side_effect = httpx.HTTPStatusError(\n-        \"404 Not Found\", request=Mock(), response=mock_response\n-    )\n-\n-    mock_client = AsyncMock(spec=httpx.AsyncClient)\n-    mock_client.__aenter__.return_value.get.return_value = mock_response\n-\n-    with patch(\"httpx.AsyncClient\", return_value=mock_client):\n+    with patch(\"agno.document.reader.url_reader.async_fetch_with_retry\",\n+               side_effect=httpx.HTTPStatusError(\"404 Not Found\", request=Mock(), response=Mock())):\n         reader = URLReader()\n         with pytest.raises(httpx.HTTPStatusError):\n             await reader.async_read(url)\n \n \n @pytest.mark.asyncio\n-async def test_async_chunking():\n+async def test_async_chunking(mock_response):\n     \"\"\"Test async document chunking functionality\"\"\"\n     url = \"https://example.com\"\n-\n-    mock_response = Mock(spec=httpx.Response)\n-    mock_response.status_code = 200\n     mock_response.text = \"Hello, world! \" * 1000\n \n-    mock_client = AsyncMock(spec=httpx.AsyncClient)\n-    mock_client.__aenter__.return_value.get.return_value = mock_response\n-\n-    with patch(\"httpx.AsyncClient\", return_value=mock_client):\n+    with patch(\"agno.document.reader.url_reader.async_fetch_with_retry\", return_value=mock_response):\n         reader = URLReader()\n         reader.chunk = True\n         reader.chunking_strategy = FixedSizeChunking(chunk_size=100)\n"
        },
        {
            "commit": "1c533e0065df6fa2c1c75c23125cdc5e8c12ce3c",
            "file_path": "libs/agno/tests/unit/tools/test_gmail_tools.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_gmail_tools.py b/libs/agno/tests/unit/tools/test_gmail_tools.py\nindex c686ce1ee..38fd73419 100644\n--- a/libs/agno/tests/unit/tools/test_gmail_tools.py\n+++ b/libs/agno/tests/unit/tools/test_gmail_tools.py\n@@ -3,7 +3,7 @@\n import base64\n from datetime import datetime\n from typing import Any, Dict\n-from unittest.mock import MagicMock, Mock, patch\n+from unittest.mock import MagicMock, Mock, mock_open, patch\n \n import pytest\n from google.oauth2.credentials import Credentials\n@@ -488,3 +488,228 @@ def test_service_initialization():\n         with patch.object(tools, \"_auth\"):\n             tools.get_latest_emails(count=1)\n             mock_build.assert_called_once_with(\"gmail\", \"v1\", credentials=mock_creds)\n+\n+\n+def test_send_email_with_single_attachment(gmail_tools, mock_gmail_service):\n+    \"\"\"Test sending email with a single attachment.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(\"application/pdf\", None)):\n+                result = gmail_tools.send_email(\n+                    to=\"recipient@test.com\",\n+                    subject=\"With Attachment\",\n+                    body=\"Email with attachment\",\n+                    attachments=\"test.pdf\",\n+                )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n+\n+\n+def test_send_email_with_multiple_attachments(gmail_tools, mock_gmail_service):\n+    \"\"\"Test sending email with multiple attachments.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(\"application/pdf\", None)):\n+                result = gmail_tools.send_email(\n+                    to=\"recipient@test.com\",\n+                    subject=\"Multiple Attachments\",\n+                    body=\"Email with multiple attachments\",\n+                    attachments=[\"test1.pdf\", \"test2.pdf\"],\n+                )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n+\n+\n+def test_create_draft_with_attachment(gmail_tools, mock_gmail_service):\n+    \"\"\"Test creating draft email with attachment.\"\"\"\n+    mock_draft_response = {\"id\": \"draft123\", \"message\": {\"id\": \"msg123\"}}\n+    mock_gmail_service.users().drafts().create().execute.return_value = mock_draft_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(\"text/plain\", None)):\n+                result = gmail_tools.create_draft_email(\n+                    to=\"recipient@test.com\",\n+                    subject=\"Draft with Attachment\",\n+                    body=\"Draft with attachment\",\n+                    attachments=\"document.txt\",\n+                )\n+\n+    assert \"draft123\" in result\n+    mock_gmail_service.users().drafts().create.assert_called_once()\n+\n+\n+def test_send_email_reply_with_attachment(gmail_tools, mock_gmail_service):\n+    \"\"\"Test sending email reply with attachment.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(\"image/jpeg\", None)):\n+                result = gmail_tools.send_email_reply(\n+                    thread_id=\"thread123\",\n+                    message_id=\"msg456\",\n+                    to=\"recipient@test.com\",\n+                    subject=\"Reply with Attachment\",\n+                    body=\"Reply with attachment\",\n+                    attachments=\"image.jpg\",\n+                )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n+\n+\n+def test_send_email_attachment_file_not_found(gmail_tools, mock_gmail_service):\n+    \"\"\"Test error handling when attachment file doesn't exist.\"\"\"\n+    with patch(\"pathlib.Path.exists\", return_value=False):\n+        with pytest.raises(ValueError, match=\"Attachment file not found\"):\n+            gmail_tools.send_email(\n+                to=\"recipient@test.com\", subject=\"Test\", body=\"Test body\", attachments=\"nonexistent.pdf\"\n+            )\n+\n+\n+def test_create_draft_attachment_file_not_found(gmail_tools, mock_gmail_service):\n+    \"\"\"Test error handling when draft attachment file doesn't exist.\"\"\"\n+    with patch(\"pathlib.Path.exists\", return_value=False):\n+        with pytest.raises(ValueError, match=\"Attachment file not found\"):\n+            gmail_tools.create_draft_email(\n+                to=\"recipient@test.com\", subject=\"Test\", body=\"Test body\", attachments=\"nonexistent.pdf\"\n+            )\n+\n+\n+def test_send_reply_attachment_file_not_found(gmail_tools, mock_gmail_service):\n+    \"\"\"Test error handling when reply attachment file doesn't exist.\"\"\"\n+    with patch(\"pathlib.Path.exists\", return_value=False):\n+        with pytest.raises(ValueError, match=\"Attachment file not found\"):\n+            gmail_tools.send_email_reply(\n+                thread_id=\"thread123\",\n+                message_id=\"msg456\",\n+                to=\"recipient@test.com\",\n+                subject=\"Test\",\n+                body=\"Test body\",\n+                attachments=\"nonexistent.pdf\",\n+            )\n+\n+\n+def test_send_email_mixed_attachment_existence(gmail_tools, mock_gmail_service):\n+    \"\"\"Test error handling when some attachments exist and others don't.\"\"\"\n+\n+    # Create a mock Path class\n+    class MockPath:\n+        def __init__(self, path):\n+            self.path = str(path)\n+        \n+        def exists(self):\n+            return self.path.endswith(\"exists.pdf\")\n+\n+    with patch(\"agno.tools.gmail.Path\", MockPath):\n+        with pytest.raises(ValueError, match=\"Attachment file not found\"):\n+            gmail_tools.send_email(\n+                to=\"recipient@test.com\", subject=\"Test\", body=\"Test body\", attachments=[\"exists.pdf\", \"missing.pdf\"]\n+            )\n+\n+\n+def test_attachment_mime_type_guessing(gmail_tools, mock_gmail_service):\n+    \"\"\"Test MIME type guessing for different file types.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    # Test with unknown MIME type (should default to application/octet-stream)\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(None, None)):\n+                result = gmail_tools.send_email(\n+                    to=\"recipient@test.com\",\n+                    subject=\"Unknown File Type\",\n+                    body=\"Email with unknown file type\",\n+                    attachments=\"unknown.xyz\",\n+                )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n+\n+\n+def test_attachment_with_encoding(gmail_tools, mock_gmail_service):\n+    \"\"\"Test attachment handling when MIME type has encoding.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    # Test with encoding present (should default to application/octet-stream)\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(\"text/plain\", \"gzip\")):\n+                result = gmail_tools.send_email(\n+                    to=\"recipient@test.com\",\n+                    subject=\"Encoded File\",\n+                    body=\"Email with encoded file\",\n+                    attachments=\"file.txt.gz\",\n+                )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n+\n+\n+def test_empty_attachments_list(gmail_tools, mock_gmail_service):\n+    \"\"\"Test sending email with empty attachments list.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    result = gmail_tools.send_email(\n+        to=\"recipient@test.com\", subject=\"No Attachments\", body=\"Email without attachments\", attachments=[]\n+    )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n+\n+\n+def test_attachment_filename_extraction(gmail_tools, mock_gmail_service):\n+    \"\"\"Test that attachment filenames are properly extracted from paths.\"\"\"\n+    mock_send_response = {\"id\": \"msg123\", \"labelIds\": [\"SENT\"]}\n+    mock_gmail_service.users().messages().send().execute.return_value = mock_send_response\n+    \n+    # Reset mock to clear any setup calls\n+    mock_gmail_service.reset_mock()\n+\n+    with patch(\"pathlib.Path.exists\", return_value=True):\n+        with patch(\"builtins.open\", mock_open(read_data=b\"fake file content\")):\n+            with patch(\"mimetypes.guess_type\", return_value=(\"application/pdf\", None)):\n+                # Test with full path - should extract just the filename\n+                result = gmail_tools.send_email(\n+                    to=\"recipient@test.com\",\n+                    subject=\"Path Test\",\n+                    body=\"Email with full path attachment\",\n+                    attachments=\"/full/path/to/document.pdf\",\n+                )\n+\n+    assert \"msg123\" in result\n+    mock_gmail_service.users().messages().send.assert_called_once()\n"
        }
    ]
}