{
    "sha_fail": "331dcf050910618b9bb5e3028da4597770199f72",
    "changed_files": [
        {
            "commit": "331dcf050910618b9bb5e3028da4597770199f72",
            "file_path": "lightrag/operate.py",
            "diff": "diff --git a/lightrag/operate.py b/lightrag/operate.py\nindex dd8d54be..7fb150f7 100644\n--- a/lightrag/operate.py\n+++ b/lightrag/operate.py\n@@ -1747,7 +1747,7 @@ async def kg_query(\n         query_param.user_prompt or \"\",\n         query_param.enable_rerank,\n     )\n-    cached_response, quantized, min_val, max_val = await handle_cache(\n+    cached_response = await handle_cache(\n         hashing_kv, args_hash, query, query_param.mode, cache_type=\"query\"\n     )\n     if cached_response is not None:\n@@ -1922,18 +1922,10 @@ async def extract_keywords_only(\n     args_hash = compute_args_hash(\n         param.mode,\n         text,\n-        param.response_type,\n-        param.top_k,\n-        param.chunk_top_k,\n-        param.max_entity_tokens,\n-        param.max_relation_tokens,\n-        param.max_total_tokens,\n         param.hl_keywords or [],\n         param.ll_keywords or [],\n-        param.user_prompt or \"\",\n-        param.enable_rerank,\n     )\n-    cached_response, quantized, min_val, max_val = await handle_cache(\n+    cached_response = await handle_cache(\n         hashing_kv, args_hash, text, param.mode, cache_type=\"keywords\"\n     )\n     if cached_response is not None:\n@@ -3020,7 +3012,7 @@ async def naive_query(\n         query_param.user_prompt or \"\",\n         query_param.enable_rerank,\n     )\n-    cached_response, quantized, min_val, max_val = await handle_cache(\n+    cached_response = await handle_cache(\n         hashing_kv, args_hash, query, query_param.mode, cache_type=\"query\"\n     )\n     if cached_response is not None:\n"
        },
        {
            "commit": "331dcf050910618b9bb5e3028da4597770199f72",
            "file_path": "lightrag/utils.py",
            "diff": "diff --git a/lightrag/utils.py b/lightrag/utils.py\nindex 96b7bdc3..5216fac1 100644\n--- a/lightrag/utils.py\n+++ b/lightrag/utils.py\n@@ -762,27 +762,27 @@ async def handle_cache(\n     prompt,\n     mode=\"default\",\n     cache_type=None,\n-):\n+) -> str|None:\n     \"\"\"Generic cache handling function with flattened cache keys\"\"\"\n     if hashing_kv is None:\n-        return None, None, None, None\n+        return None\n \n     if mode != \"default\":  # handle cache for all type of query\n         if not hashing_kv.global_config.get(\"enable_llm_cache\"):\n-            return None, None, None, None\n+            return None\n     else:  # handle cache for entity extraction\n         if not hashing_kv.global_config.get(\"enable_llm_cache_for_entity_extract\"):\n-            return None, None, None, None\n+            return None\n \n     # Use flattened cache key format: {mode}:{cache_type}:{hash}\n     flattened_key = generate_cache_key(mode, cache_type, args_hash)\n     cache_entry = await hashing_kv.get_by_id(flattened_key)\n     if cache_entry:\n         logger.debug(f\"Flattened cache hit(key:{flattened_key})\")\n-        return cache_entry[\"return\"], None, None, None\n+        return cache_entry[\"return\"]\n \n     logger.debug(f\"Cache missed(mode:{mode} type:{cache_type})\")\n-    return None, None, None, None\n+    return None\n \n \n @dataclass\n@@ -1409,7 +1409,7 @@ async def use_llm_func_with_cache(\n         # Generate cache key for this LLM call\n         cache_key = generate_cache_key(\"default\", cache_type, arg_hash)\n \n-        cached_return, _1, _2, _3 = await handle_cache(\n+        cached_return = await handle_cache(\n             llm_response_cache,\n             arg_hash,\n             _prompt,\n"
        }
    ]
}