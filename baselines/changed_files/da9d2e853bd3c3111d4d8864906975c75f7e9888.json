{
    "sha_fail": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
    "changed_files": [
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/audio_input_agent.py",
            "diff": "diff --git a/cookbook/models/litellm/audio_input_agent.py b/cookbook/models/litellm/audio_input_agent.py\nnew file mode 100644\nindex 000000000..1e4bb5c2a\n--- /dev/null\n+++ b/cookbook/models/litellm/audio_input_agent.py\n@@ -0,0 +1,21 @@\n+import requests\n+from agno.agent import Agent\n+from agno.media import Audio\n+from agno.models.litellm import LiteLLM\n+\n+# Fetch the QA audio file and convert it to a base64 encoded string\n+url = \"https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3\"\n+response = requests.get(url)\n+response.raise_for_status()\n+mp3_data = response.content\n+\n+# Audio input requires specific audio-enabled models like gpt-4o-audio-preview\n+agent = Agent(\n+    model=LiteLLM(id=\"gpt-4o-audio-preview\"),\n+    markdown=True,\n+)\n+agent.print_response(\n+    \"What's the audio about?\",\n+    audio=[Audio(content=mp3_data, format=\"mp3\")],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/basic.py",
            "diff": "diff --git a/cookbook/models/litellm/basic.py b/cookbook/models/litellm/basic.py\nnew file mode 100644\nindex 000000000..8d0f0de44\n--- /dev/null\n+++ b/cookbook/models/litellm/basic.py\n@@ -0,0 +1,12 @@\n+from agno.agent import Agent\n+from agno.models.litellm import LiteLLM\n+\n+openai_agent = Agent(\n+    model=LiteLLM(\n+        id=\"huggingface/mistralai/Mistral-7B-Instruct-v0.2\",\n+        top_p=0.95,\n+    ),\n+    markdown=True,\n+)\n+\n+openai_agent.print_response(\"Whats happening in France?\")\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/image_agent.py",
            "diff": "diff --git a/cookbook/models/litellm/image_agent.py b/cookbook/models/litellm/image_agent.py\nnew file mode 100644\nindex 000000000..d189cc342\n--- /dev/null\n+++ b/cookbook/models/litellm/image_agent.py\n@@ -0,0 +1,20 @@\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.litellm import LiteLLM\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+agent = Agent(\n+    model=LiteLLM(id=\"gpt-4o\"),\n+    tools=[DuckDuckGoTools()],\n+    markdown=True,\n+)\n+\n+agent.print_response(\n+    \"Tell me about this image and give me the latest news about it.\",\n+    images=[\n+        Image(\n+            url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\"\n+        )\n+    ],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/image_agent_bytes.py",
            "diff": "diff --git a/cookbook/models/litellm/image_agent_bytes.py b/cookbook/models/litellm/image_agent_bytes.py\nnew file mode 100644\nindex 000000000..5fafc72d6\n--- /dev/null\n+++ b/cookbook/models/litellm/image_agent_bytes.py\n@@ -0,0 +1,31 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.litellm import LiteLLM\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.utils.media import download_image\n+\n+agent = Agent(\n+    model=LiteLLM(id=\"gpt-4o\"),\n+    tools=[DuckDuckGoTools()],\n+    markdown=True,\n+)\n+\n+image_path = Path(__file__).parent.joinpath(\"sample.jpg\")\n+\n+download_image(\n+    url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\",\n+    output_path=str(image_path),\n+)\n+\n+# Read the image file content as bytes\n+image_bytes = image_path.read_bytes()\n+\n+agent.print_response(\n+    \"Tell me about this image and give me the latest news about it.\",\n+    images=[\n+        Image(content=image_bytes),\n+    ],\n+    stream=True,\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/pdf_input_bytes.py",
            "diff": "diff --git a/cookbook/models/litellm/pdf_input_bytes.py b/cookbook/models/litellm/pdf_input_bytes.py\nnew file mode 100644\nindex 000000000..a17ce46a9\n--- /dev/null\n+++ b/cookbook/models/litellm/pdf_input_bytes.py\n@@ -0,0 +1,26 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.litellm import LiteLLM\n+from agno.utils.media import download_file\n+\n+pdf_path = Path(__file__).parent.joinpath(\"ThaiRecipes.pdf\")\n+\n+download_file(\n+    \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\", str(pdf_path)\n+)\n+\n+agent = Agent(\n+    model=LiteLLM(id=\"openai/gpt-4o\"),\n+    markdown=True,\n+)\n+\n+agent.print_response(\n+    \"Summarize the contents of the attached file.\",\n+    files=[\n+        File(\n+            content=pdf_path.read_bytes(),\n+        ),\n+    ],\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/pdf_input_local.py",
            "diff": "diff --git a/cookbook/models/litellm/pdf_input_local.py b/cookbook/models/litellm/pdf_input_local.py\nnew file mode 100644\nindex 000000000..835e08d69\n--- /dev/null\n+++ b/cookbook/models/litellm/pdf_input_local.py\n@@ -0,0 +1,24 @@\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.litellm import LiteLLM\n+from agno.utils.media import download_file\n+\n+pdf_path = Path(__file__).parent.joinpath(\"ThaiRecipes.pdf\")\n+\n+# Download the file using the download_file function\n+download_file(\n+    \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\", str(pdf_path)\n+)\n+\n+agent = Agent(\n+    model=LiteLLM(id=\"gpt-4o\"),\n+    markdown=True,\n+    add_history_to_messages=True,\n+)\n+\n+agent.print_response(\n+    \"What is the recipe for Gaeng Som Phak Ruam? Also what are the health benefits. Refer to the attached file.\",\n+    files=[File(filepath=pdf_path)],\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm/pdf_input_url.py",
            "diff": "diff --git a/cookbook/models/litellm/pdf_input_url.py b/cookbook/models/litellm/pdf_input_url.py\nnew file mode 100644\nindex 000000000..8c0fc54ad\n--- /dev/null\n+++ b/cookbook/models/litellm/pdf_input_url.py\n@@ -0,0 +1,14 @@\n+from agno.agent import Agent\n+from agno.media import File\n+from agno.models.litellm import LiteLLM\n+\n+agent = Agent(\n+    model=LiteLLM(id=\"gpt-4o\"),\n+    markdown=True,\n+    add_history_to_messages=True,\n+)\n+\n+agent.print_response(\n+    \"Suggest me a recipe from the attached file.\",\n+    files=[File(url=\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\")],\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/models/litellm_openai/audio_input_agent.py",
            "diff": "diff --git a/cookbook/models/litellm_openai/audio_input_agent.py b/cookbook/models/litellm_openai/audio_input_agent.py\nnew file mode 100644\nindex 000000000..b6d931797\n--- /dev/null\n+++ b/cookbook/models/litellm_openai/audio_input_agent.py\n@@ -0,0 +1,28 @@\n+\"\"\"\n+Please first install litellm[proxy] by running: pip install 'litellm[proxy]'\n+\n+Before running this script, you need to start the LiteLLM server:\n+\n+litellm --model gpt-4o-audio-preview --host 127.0.0.1 --port 4000\n+\"\"\"\n+\n+import requests\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.media import Audio\n+from agno.models.litellm import LiteLLMOpenAI\n+\n+# Fetch the QA audio file and convert it to a base64 encoded string\n+url = \"https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3\"\n+response = requests.get(url)\n+response.raise_for_status()\n+mp3_data = response.content\n+\n+# Provide the agent with the audio file and get result as text\n+# Note: Audio input requires specific audio-enabled models like gpt-4o-audio-preview\n+agent = Agent(\n+    model=LiteLLMOpenAI(id=\"gpt-4o-audio-preview\"),\n+    markdown=True,\n+)\n+agent.print_response(\n+    \"What is in this audio?\", audio=[Audio(content=mp3_data, format=\"mp3\")], stream=True\n+)\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/observability/langwatch_op.py",
            "diff": "diff --git a/cookbook/observability/langwatch_op.py b/cookbook/observability/langwatch_op.py\nnew file mode 100644\nindex 000000000..91a71f7e5\n--- /dev/null\n+++ b/cookbook/observability/langwatch_op.py\n@@ -0,0 +1,31 @@\n+\"\"\"\n+This example shows how to instrument your agno agent and send traces to LangWatch.\n+\n+1. Install dependencies: pip install openai langwatch openinference-instrumentation-agno\n+2. Sign up for an account at https://app.langwatch.ai/\n+3. Set your LangWatch API key as an environment variables:\n+  - export LANGWATCH_API_KEY=<your-key>\n+\"\"\"\n+import os\n+import langwatch\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.yfinance import YFinanceTools\n+from openinference.instrumentation.agno import AgnoInstrumentor\n+\n+# Initialize LangWatch and instrument Agno\n+langwatch.setup(\n+    instrumentors=[AgnoInstrumentor()]\n+)\n+\n+# Create and configure your Agno agent\n+agent = Agent(\n+    name=\"Stock Price Agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[YFinanceTools()],\n+    instructions=\"You are a stock price agent. Answer questions in the style of a stock analyst.\",\n+    debug_mode=True,\n+)\n+\n+agent.print_response(\"What is the current price of Tesla?\")\n\\ No newline at end of file\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "cookbook/workflows_2/sync/01_basic_workflows/basic_workflow.py",
            "diff": "diff --git a/cookbook/workflows_2/sync/01_basic_workflows/basic_workflow.py b/cookbook/workflows_2/sync/01_basic_workflows/basic_workflow.py\nnew file mode 100644\nindex 000000000..755b9e217\n--- /dev/null\n+++ b/cookbook/workflows_2/sync/01_basic_workflows/basic_workflow.py\n@@ -0,0 +1,72 @@\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.storage.postgres import PostgresStorage\n+from agno.workflow.v2.step import Step\n+from agno.workflow.v2.workflow import Workflow\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+# Define response format\n+class Source(BaseModel):\n+    name: str = Field(description=\"The name of the source\")\n+    page_number: int = Field(description=\"The page number of the source\")\n+\n+class Response(BaseModel):\n+    response: str = Field(description=\"The response to the user's query\")\n+    sources: list[Source] = Field(description=\"The sources used to generate the response\")\n+\n+# Define agents\n+information_agent = Agent(\n+    name=\"Information Agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    role=\"Gathers sufficient context from the user regarding their query\",\n+)\n+\n+knowledge_search_agent = Agent(\n+    name=\"Knowledge Search Agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    role=\"Searches the knowledge base for relevant information to answer the user's query\",\n+    # knowledge=knowledge, # Commented out for now, but will have its own knowledge base\n+)\n+\n+response_agent = Agent(\n+    name=\"Response Agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    role=\"Respond to the user's query based on the provided information and sources\",\n+    response_model=Response,\n+)\n+\n+# Define steps\n+information_gather_step = Step(\n+    name=\"Research Step\",\n+    agent=information_agent,\n+)\n+\n+knowledge_search_step = Step(\n+    name=\"Knowledge Search Step\",\n+    agent=knowledge_search_agent,\n+)\n+\n+response_step = Step(\n+    name=\"Response Step\",\n+    agent=response_agent,\n+)\n+\n+# Create and use workflow\n+if __name__ == \"__main__\":\n+    rag_workflow = Workflow(\n+        name=\"RAG Workflow\",\n+        description=\"A RAG workflow tasked with answering user queries based on a provided knowledge base.\",\n+        storage=PostgresStorage(\n+            table_name=\"workflow_v2\",\n+            db_url=db_url,\n+            mode=\"workflow_v2\",\n+        ),\n+        steps=[information_gather_step, knowledge_search_step, response_step],\n+    )\n+    rag_workflow.print_response(\n+        message=\"What is the latest news in AI?\",\n+        markdown=True,\n+    )\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/agno/models/google/gemini.py",
            "diff": "diff --git a/libs/agno/agno/models/google/gemini.py b/libs/agno/agno/models/google/gemini.py\nindex 83b876988..91506cccb 100644\n--- a/libs/agno/agno/models/google/gemini.py\n+++ b/libs/agno/agno/models/google/gemini.py\n@@ -713,15 +713,29 @@ class Gemini(Model):\n                     if isinstance(text_content, str):\n                         # Check if this is a thought summary\n                         if hasattr(part, \"thought\") and part.thought:\n-                            model_response.reasoning_content = text_content\n+                            # Add all parts as single message\n+                            if model_response.reasoning_content is None:\n+                                model_response.reasoning_content = text_content\n+                            else:\n+                                model_response.reasoning_content += text_content\n                         else:\n-                            model_response.content = text_content\n+                            if model_response.content is None:\n+                                model_response.content = text_content\n+                            else:\n+                                model_response.content += text_content\n                     else:\n                         content_str = str(text_content) if text_content is not None else \"\"\n                         if hasattr(part, \"thought\") and part.thought:\n-                            model_response.reasoning_content = content_str\n+                            # Add all parts as single message\n+                            if model_response.reasoning_content is None:\n+                                model_response.reasoning_content = content_str\n+                            else:\n+                                model_response.reasoning_content += content_str\n                         else:\n-                            model_response.content = content_str\n+                            if model_response.content is None:\n+                                model_response.content = content_str\n+                            else:\n+                                model_response.content += content_str\n \n                 if hasattr(part, \"inline_data\") and part.inline_data is not None:\n                     model_response.image = ImageArtifact(\n@@ -803,9 +817,15 @@ class Gemini(Model):\n                         text_content = str(part.text) if part.text is not None else \"\"\n                         # Check if this is a thought summary\n                         if hasattr(part, \"thought\") and part.thought:\n-                            model_response.reasoning_content = text_content\n+                            if model_response.reasoning_content is None:\n+                                model_response.reasoning_content = text_content\n+                            else:\n+                                model_response.reasoning_content += text_content\n                         else:\n-                            model_response.content = text_content\n+                            if model_response.content is None:\n+                                model_response.content = text_content\n+                            else:\n+                                model_response.content += text_content\n \n                     if hasattr(part, \"inline_data\") and part.inline_data is not None:\n                         model_response.image = ImageArtifact(\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/agno/models/litellm/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/litellm/chat.py b/libs/agno/agno/models/litellm/chat.py\nindex a95535bbf..8491019f1 100644\n--- a/libs/agno/agno/models/litellm/chat.py\n+++ b/libs/agno/agno/models/litellm/chat.py\n@@ -9,6 +9,7 @@ from agno.models.base import Model\n from agno.models.message import Message\n from agno.models.response import ModelResponse\n from agno.utils.log import log_debug, log_error, log_warning\n+from agno.utils.openai import _format_file_for_message, audio_to_message, images_to_message\n \n try:\n     import litellm\n@@ -73,6 +74,31 @@ class LiteLLM(Model):\n         for m in messages:\n             msg = {\"role\": m.role, \"content\": m.content if m.content is not None else \"\"}\n \n+            # Handle media\n+            if (m.images is not None and len(m.images) > 0) or (m.audio is not None and len(m.audio) > 0):\n+                if isinstance(m.content, str):\n+                    content_list = [{\"type\": \"text\", \"text\": m.content}]\n+                    if m.images is not None:\n+                        content_list.extend(images_to_message(images=m.images))\n+                    if m.audio is not None:\n+                        content_list.extend(audio_to_message(audio=m.audio))\n+                    msg[\"content\"] = content_list\n+\n+            if m.videos is not None and len(m.videos) > 0:\n+                log_warning(\"Video input is currently unsupported by LLM providers.\")\n+\n+            # Handle files\n+            if m.files is not None:\n+                if isinstance(msg[\"content\"], str):\n+                    content_list = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n+                else:\n+                    content_list = msg[\"content\"]\n+                for file in m.files:\n+                    file_part = _format_file_for_message(file)\n+                    if file_part:\n+                        content_list.append(file_part)\n+                msg[\"content\"] = content_list\n+\n             # Handle tool calls in assistant messages\n             if m.role == \"assistant\" and m.tool_calls:\n                 msg[\"tool_calls\"] = [\n@@ -95,12 +121,8 @@ class LiteLLM(Model):\n                 if m.images is not None and len(m.images) > 0:\n                     log_warning(\"Image input is currently unsupported.\")\n \n-                if m.files is not None and len(m.files) > 0:\n-                    log_warning(\"File input is currently unsupported.\")\n-\n                 if m.videos is not None and len(m.videos) > 0:\n                     log_warning(\"Video input is currently unsupported.\")\n-\n             formatted_messages.append(msg)\n \n         return formatted_messages\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/agno/storage/mysql.py",
            "diff": "diff --git a/libs/agno/agno/storage/mysql.py b/libs/agno/agno/storage/mysql.py\nindex 9d80d77fb..e489254e7 100644\n--- a/libs/agno/agno/storage/mysql.py\n+++ b/libs/agno/agno/storage/mysql.py\n@@ -131,6 +131,7 @@ class MySQLStorage(Storage):\n         elif self.mode == \"workflow_v2\":\n             specific_columns = [\n                 Column(\"workflow_id\", String(255), index=True),\n+                Column(\"workflow_name\", String(255), index=True),\n                 Column(\"workflow_data\", JSON),\n                 Column(\"runs\", JSON),\n             ]\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/agno/storage/postgres.py",
            "diff": "diff --git a/libs/agno/agno/storage/postgres.py b/libs/agno/agno/storage/postgres.py\nindex ea54ff472..908ace52f 100644\n--- a/libs/agno/agno/storage/postgres.py\n+++ b/libs/agno/agno/storage/postgres.py\n@@ -30,7 +30,7 @@ class PostgresStorage(Storage):\n         db_engine: Optional[Engine] = None,\n         schema_version: int = 1,\n         auto_upgrade_schema: bool = False,\n-        mode: Optional[Literal[\"agent\", \"team\", \"workflow\"]] = \"agent\",\n+        mode: Optional[Literal[\"agent\", \"team\", \"workflow\", \"workflow_v2\"]] = \"agent\",\n     ):\n         \"\"\"\n         This class provides agent storage using a PostgreSQL table.\n@@ -131,6 +131,7 @@ class PostgresStorage(Storage):\n         elif self.mode == \"workflow_v2\":\n             specific_columns = [\n                 Column(\"workflow_id\", String, index=True),\n+                Column(\"workflow_name\", String, index=True),\n                 Column(\"workflow_data\", postgresql.JSONB),\n                 Column(\"runs\", postgresql.JSONB),\n             ]\n@@ -570,9 +571,9 @@ class PostgresStorage(Storage):\n                     stmt = postgresql.insert(self.table).values(\n                         session_id=session.session_id,\n                         workflow_id=session.workflow_id,  # type: ignore\n-                        workflow_name=session.workflow_name,  # type: ignore\n                         user_id=session.user_id,\n                         runs=session_dict.get(\"runs\"),\n+                        workflow_name=session.workflow_name,  # type: ignore\n                         workflow_data=session.workflow_data,  # type: ignore\n                         session_data=session.session_data,\n                         extra_data=session.extra_data,\n@@ -583,9 +584,9 @@ class PostgresStorage(Storage):\n                         index_elements=[\"session_id\"],\n                         set_=dict(\n                             workflow_id=session.workflow_id,  # type: ignore\n-                            workflow_name=session.workflow_name,  # type: ignore\n                             user_id=session.user_id,\n                             runs=session_dict.get(\"runs\"),\n+                            workflow_name=session.workflow_name,  # type: ignore\n                             workflow_data=session.workflow_data,  # type: ignore\n                             session_data=session.session_data,\n                             extra_data=session.extra_data,\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/agno/workflow/v2/workflow.py",
            "diff": "diff --git a/libs/agno/agno/workflow/v2/workflow.py b/libs/agno/agno/workflow/v2/workflow.py\nindex 5f1fc29e6..8c336c2d0 100644\n--- a/libs/agno/agno/workflow/v2/workflow.py\n+++ b/libs/agno/agno/workflow/v2/workflow.py\n@@ -1631,7 +1631,7 @@ class Workflow:\n         videos: Optional[List[Video]] = None,\n         stream: bool = False,\n         stream_intermediate_steps: bool = False,\n-        markdown: bool = True,\n+        markdown: bool = False,\n         show_time: bool = True,\n         show_step_details: bool = True,\n         console: Optional[Any] = None,\n@@ -1699,7 +1699,7 @@ class Workflow:\n         audio: Optional[List[Audio]] = None,\n         images: Optional[List[Image]] = None,\n         videos: Optional[List[Video]] = None,\n-        markdown: bool = True,\n+        markdown: bool = False,\n         show_time: bool = True,\n         show_step_details: bool = True,\n         console: Optional[Any] = None,\n@@ -1856,7 +1856,7 @@ class Workflow:\n         images: Optional[List[Image]] = None,\n         videos: Optional[List[Video]] = None,\n         stream_intermediate_steps: bool = False,\n-        markdown: bool = True,\n+        markdown: bool = False,\n         show_time: bool = True,\n         show_step_details: bool = True,\n         console: Optional[Any] = None,\n@@ -2407,7 +2407,7 @@ class Workflow:\n         videos: Optional[List[Video]] = None,\n         stream: bool = False,\n         stream_intermediate_steps: bool = False,\n-        markdown: bool = True,\n+        markdown: bool = False,\n         show_time: bool = True,\n         show_step_details: bool = True,\n         console: Optional[Any] = None,\n@@ -2471,7 +2471,7 @@ class Workflow:\n         audio: Optional[List[Audio]] = None,\n         images: Optional[List[Image]] = None,\n         videos: Optional[List[Video]] = None,\n-        markdown: bool = True,\n+        markdown: bool = False,\n         show_time: bool = True,\n         show_step_details: bool = True,\n         console: Optional[Any] = None,\n@@ -2629,7 +2629,7 @@ class Workflow:\n         images: Optional[List[Image]] = None,\n         videos: Optional[List[Video]] = None,\n         stream_intermediate_steps: bool = False,\n-        markdown: bool = True,\n+        markdown: bool = False,\n         show_time: bool = True,\n         show_step_details: bool = True,\n         console: Optional[Any] = None,\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/tests/integration/models/litellm/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/litellm/test_multimodal.py b/libs/agno/tests/integration/models/litellm/test_multimodal.py\nnew file mode 100644\nindex 000000000..095bf3fe0\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/litellm/test_multimodal.py\n@@ -0,0 +1,87 @@\n+from typing import Any\n+\n+import requests\n+\n+from agno.agent.agent import Agent\n+from agno.media import Audio, Image\n+from agno.models.litellm import LiteLLM\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+\n+def _get_audio_input() -> bytes | Any:\n+    \"\"\"Fetch an example audio file and return it as base64 encoded string\"\"\"\n+    url = \"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\"\n+    response = requests.get(url)\n+    response.raise_for_status()\n+    return response.content\n+\n+\n+def test_image_input():\n+    \"\"\"Test LiteLLM with image input\"\"\"\n+    agent = Agent(\n+        model=LiteLLM(id=\"gpt-4o\"),\n+        tools=[DuckDuckGoTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"Tell me about this image and give me the latest news about it.\",\n+        images=[Image(url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\")],\n+    )\n+\n+    assert \"golden\" in response.content.lower()\n+\n+\n+def test_audio_input_bytes():\n+    \"\"\"Test LiteLLM with audio input from bytes\"\"\"\n+    wav_data = _get_audio_input()\n+\n+    # Provide the agent with the audio file and get result as text\n+    agent = Agent(\n+        model=LiteLLM(id=\"gpt-4o-audio-preview\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    response = agent.run(\"What is in this audio?\", audio=[Audio(content=wav_data, format=\"wav\")])\n+\n+    assert response.content is not None\n+\n+\n+def test_audio_input_url():\n+    \"\"\"Test LiteLLM with audio input from URL\"\"\"\n+    agent = Agent(\n+        model=LiteLLM(id=\"gpt-4o-audio-preview\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"What is in this audio?\",\n+        audio=[Audio(url=\"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\")],\n+    )\n+\n+    assert response.content is not None\n+\n+\n+def test_single_image_simple():\n+    \"\"\"Test LiteLLM with a simple image\"\"\"\n+    agent = Agent(\n+        model=LiteLLM(id=\"gpt-4o\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"What do you see in this image?\",\n+        images=[\n+            Image(url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\"),\n+        ],\n+    )\n+\n+    assert response.content is not None\n+    assert len(response.content) > 0\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "libs/agno/tests/integration/models/litellm_openai/test_multimodal.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/litellm_openai/test_multimodal.py b/libs/agno/tests/integration/models/litellm_openai/test_multimodal.py\nnew file mode 100644\nindex 000000000..97d325a8a\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/litellm_openai/test_multimodal.py\n@@ -0,0 +1,87 @@\n+from typing import Any\n+\n+import requests\n+\n+from agno.agent.agent import Agent\n+from agno.media import Audio, Image\n+from agno.models.litellm import LiteLLMOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+\n+def _get_audio_input() -> bytes | Any:\n+    \"\"\"Fetch an example audio file and return it as base64 encoded string\"\"\"\n+    url = \"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\"\n+    response = requests.get(url)\n+    response.raise_for_status()\n+    return response.content\n+\n+\n+def test_image_input():\n+    \"\"\"Test LiteLLMOpenAI with image input\"\"\"\n+    agent = Agent(\n+        model=LiteLLMOpenAI(id=\"gpt-4o\"),\n+        tools=[DuckDuckGoTools(cache_results=True)],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"Tell me about this image and give me the latest news about it.\",\n+        images=[Image(url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\")],\n+    )\n+\n+    assert \"golden\" in response.content.lower()\n+\n+\n+def test_audio_input_bytes():\n+    \"\"\"Test LiteLLMOpenAI with audio input from bytes\"\"\"\n+    wav_data = _get_audio_input()\n+\n+    # Provide the agent with the audio file and get result as text\n+    agent = Agent(\n+        model=LiteLLMOpenAI(id=\"gpt-4o-audio-preview\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    response = agent.run(\"What is in this audio?\", audio=[Audio(content=wav_data, format=\"wav\")])\n+\n+    assert response.content is not None\n+\n+\n+def test_audio_input_url():\n+    \"\"\"Test LiteLLMOpenAI with audio input from URL\"\"\"\n+    agent = Agent(\n+        model=LiteLLMOpenAI(id=\"gpt-4o-audio-preview\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"What is in this audio?\",\n+        audio=[Audio(url=\"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\")],\n+    )\n+\n+    assert response.content is not None\n+\n+\n+def test_single_image_simple():\n+    \"\"\"Test LiteLLMOpenAI with a simple image\"\"\"\n+    agent = Agent(\n+        model=LiteLLMOpenAI(id=\"gpt-4o\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\n+        \"What do you see in this image?\",\n+        images=[\n+            Image(url=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\"),\n+        ],\n+    )\n+\n+    assert response.content is not None\n+    assert len(response.content) > 0\n"
        },
        {
            "commit": "da9d2e853bd3c3111d4d8864906975c75f7e9888",
            "file_path": "scripts/run_model_tests.sh",
            "diff": "diff --git a/scripts/run_model_tests.sh b/scripts/run_model_tests.sh\nindex cf7cdbe96..785bef54b 100755\n--- a/scripts/run_model_tests.sh\n+++ b/scripts/run_model_tests.sh\n@@ -30,6 +30,7 @@ if [ -z \"$1\" ]; then\n     echo \"- google\"\n     echo \"- groq\"\n     echo \"- ibm\"\n+    echo \"- litellm\"\n     echo \"- mistral\"\n     echo \"- nvidia\"\n     echo \"- openai\"\n@@ -201,6 +202,14 @@ case $MODEL_NAME in\n             exit 1\n         fi\n         ;;\n+    \"litellm\")\n+        if [ -z \"${LITELLM_API_KEY}\" ] && [ -z \"${OPENAI_API_KEY}\" ]; then\n+            print_heading \"Error: LITELLM_API_KEY or OPENAI_API_KEY environment variable is not set\"\n+            exit 1\n+        fi\n+        print_info \"Installing litellm package -- required in tests\"\n+        pip install litellm\n+        ;;\n     \"cerebras\")\n         if [ -z \"${CEREBRAS_API_KEY}\" ]; then\n             print_heading \"Error: CEREBRAS_API_KEY environment variable is not set\"\n"
        }
    ]
}