{
    "sha_fail": "0b806e8ab76c7c89d77ba91d827fbe0d52ec97b0",
    "changed_files": [
        {
            "commit": "0b806e8ab76c7c89d77ba91d827fbe0d52ec97b0",
            "file_path": "faster_whisper/__init__.py",
            "diff": "diff --git a/faster_whisper/__init__.py b/faster_whisper/__init__.py\nindex ad69277..9c44381 100644\n--- a/faster_whisper/__init__.py\n+++ b/faster_whisper/__init__.py\n@@ -1,5 +1,9 @@\n from faster_whisper.audio import decode_audio\n-from faster_whisper.transcribe import BatchedInferencePipeline, WhisperModel\n+from faster_whisper.transcribe import (\n+    AsyncBatchedInferencePipeline,\n+    BatchedInferencePipeline,\n+    WhisperModel,\n+)\n from faster_whisper.utils import available_models, download_model, format_timestamp\n from faster_whisper.version import __version__\n \n@@ -8,7 +12,8 @@ __all__ = [\n     \"decode_audio\",\n     \"WhisperModel\",\n     \"BatchedInferencePipeline\",\n+    \"AsyncBatchedInferencePipeline\",\n     \"download_model\",\n     \"format_timestamp\",\n     \"__version__\",\n-]\n+]\n\\ No newline at end of file\n"
        },
        {
            "commit": "0b806e8ab76c7c89d77ba91d827fbe0d52ec97b0",
            "file_path": "faster_whisper/transcribe.py",
            "diff": "diff --git a/faster_whisper/transcribe.py b/faster_whisper/transcribe.py\nindex a73b31b..8b83dc9 100644\n--- a/faster_whisper/transcribe.py\n+++ b/faster_whisper/transcribe.py\n@@ -1,3 +1,4 @@\n+import asyncio\n import itertools\n import json\n import logging\n@@ -7,14 +8,16 @@ import zlib\n from dataclasses import asdict, dataclass\n from inspect import signature\n from math import ceil\n-from typing import BinaryIO, Iterable, List, Optional, Tuple, Union\n+from typing import AsyncGenerator, BinaryIO, Iterable, List, Optional, Tuple, Union\n from warnings import warn\n \n import ctranslate2\n import numpy as np\n import tokenizers\n \n+from ctranslate2._ext import WhisperGenerationResultAsync\n from tqdm import tqdm\n+from tqdm.asyncio import tqdm as atqdm\n \n from faster_whisper.audio import decode_audio, pad_or_trim\n from faster_whisper.feature_extractor import FeatureExtractor\n@@ -603,6 +606,523 @@ class BatchedInferencePipeline:\n         self.last_speech_timestamp = 0.0\n \n \n+class AsyncBatchedInferencePipeline:\n+    def __init__(\n+        self,\n+        model,\n+    ):\n+        self.model: WhisperModel = model\n+        self.last_speech_timestamp = 0.0\n+\n+    async def forward(self, features, tokenizer, chunks_metadata, options):\n+        encoder_output, outputs = await self.generate_segment_batched(\n+            features, tokenizer, options\n+        )\n+\n+        segmented_outputs = []\n+        segment_sizes = []\n+        for chunk_metadata, output in zip(chunks_metadata, outputs):\n+            duration = chunk_metadata[\"duration\"]\n+            segment_size = int(ceil(duration) * self.model.frames_per_second)\n+            segment_sizes.append(segment_size)\n+            (\n+                subsegments,\n+                seek,\n+                single_timestamp_ending,\n+            ) = self.model._split_segments_by_timestamps(\n+                tokenizer=tokenizer,\n+                tokens=output[\"tokens\"],\n+                time_offset=chunk_metadata[\"offset\"],\n+                segment_size=segment_size,\n+                segment_duration=duration,\n+                seek=0,\n+            )\n+            segmented_outputs.append(\n+                [\n+                    dict(\n+                        text=tokenizer.decode(subsegment[\"tokens\"]),\n+                        avg_logprob=output[\"avg_logprob\"],\n+                        no_speech_prob=output[\"no_speech_prob\"],\n+                        tokens=subsegment[\"tokens\"],\n+                        start=subsegment[\"start\"],\n+                        end=subsegment[\"end\"],\n+                        compression_ratio=get_compression_ratio(\n+                            tokenizer.decode(subsegment[\"tokens\"])\n+                        ),\n+                        seek=int(\n+                            chunk_metadata[\"offset\"] * self.model.frames_per_second\n+                        ),\n+                    )\n+                    for subsegment in subsegments\n+                ]\n+            )\n+        if options.word_timestamps:\n+            self.last_speech_timestamp = self.model.add_word_timestamps(\n+                segmented_outputs,\n+                tokenizer,\n+                encoder_output,\n+                segment_sizes,\n+                options.prepend_punctuations,\n+                options.append_punctuations,\n+                self.last_speech_timestamp,\n+            )\n+\n+        return segmented_outputs\n+\n+    async def generate_segment_batched(\n+        self,\n+        features: np.ndarray,\n+        tokenizer: Tokenizer,\n+        options: TranscriptionOptions,\n+    ):\n+        batch_size = features.shape[0]\n+\n+        prompt = self.model.get_prompt(\n+            tokenizer,\n+            previous_tokens=(\n+                tokenizer.encode(options.initial_prompt)\n+                if options.initial_prompt is not None\n+                else []\n+            ),\n+            without_timestamps=options.without_timestamps,\n+            hotwords=options.hotwords,\n+        )\n+\n+        if options.max_new_tokens is not None:\n+            max_length = len(prompt) + options.max_new_tokens\n+        else:\n+            max_length = self.model.max_length\n+\n+        if max_length > self.model.max_length:\n+            raise ValueError(\n+                f\"The length of the prompt is {len(prompt)}, and the `max_new_tokens` \"\n+                f\"{max_length - len(prompt)}. Thus, the combined length of the prompt \"\n+                f\"and `max_new_tokens` is: {max_length}. This exceeds the \"\n+                f\"`max_length` of the Whisper model: {self.model.max_length}. \"\n+                \"You should either reduce the length of your prompt, or \"\n+                \"reduce the value of `max_new_tokens`, \"\n+                f\"so that their combined length is less that {self.model.max_length}.\"\n+            )\n+\n+        encoder_output = self.model.encode(features)\n+        prompts = [prompt.copy() for _ in range(batch_size)]\n+\n+        if options.multilingual:\n+            language_tokens = [\n+                tokenizer.tokenizer.token_to_id(segment_langs[0][0])\n+                for segment_langs in self.model.model.detect_language(encoder_output)\n+            ]\n+            language_token_index = prompt.index(tokenizer.language)\n+\n+            for i, language_token in enumerate(language_tokens):\n+                prompts[i][language_token_index] = language_token\n+\n+        futures: List[WhisperGenerationResultAsync] = self.model.model.generate(\n+            encoder_output,\n+            prompts,\n+            beam_size=options.beam_size,\n+            patience=options.patience,\n+            length_penalty=options.length_penalty,\n+            max_length=max_length,\n+            suppress_blank=options.suppress_blank,\n+            suppress_tokens=options.suppress_tokens,\n+            return_scores=True,\n+            return_no_speech_prob=True,\n+            sampling_temperature=options.temperatures[0],\n+            repetition_penalty=options.repetition_penalty,\n+            no_repeat_ngram_size=options.no_repeat_ngram_size,\n+            asynchronous=True,\n+        )\n+\n+        async def await_result(future):\n+            while not future.done():\n+                await asyncio.sleep(0.001)\n+            return future.result()\n+\n+        results = await asyncio.gather(*[await_result(future) for future in futures])\n+\n+        output = []\n+        for result in results:\n+            # return scores\n+            seq_len = len(result.sequences_ids[0])\n+            cum_logprob = result.scores[0] * (seq_len**options.length_penalty)\n+\n+            output.append(\n+                dict(\n+                    avg_logprob=cum_logprob / (seq_len + 1),\n+                    no_speech_prob=result.no_speech_prob,\n+                    tokens=result.sequences_ids[0],\n+                )\n+            )\n+\n+        return encoder_output, output\n+\n+    async def transcribe(\n+        self,\n+        audio: Union[str, BinaryIO, np.ndarray],\n+        language: Optional[str] = None,\n+        task: str = \"transcribe\",\n+        log_progress: bool = False,\n+        beam_size: int = 5,\n+        best_of: int = 5,\n+        patience: float = 1,\n+        length_penalty: float = 1,\n+        repetition_penalty: float = 1,\n+        no_repeat_ngram_size: int = 0,\n+        temperature: Union[float, List[float], Tuple[float, ...]] = [\n+            0.0,\n+            0.2,\n+            0.4,\n+            0.6,\n+            0.8,\n+            1.0,\n+        ],\n+        compression_ratio_threshold: Optional[float] = 2.4,\n+        log_prob_threshold: Optional[float] = -1.0,\n+        no_speech_threshold: Optional[float] = 0.6,\n+        condition_on_previous_text: bool = True,\n+        prompt_reset_on_temperature: float = 0.5,\n+        initial_prompt: Optional[Union[str, Iterable[int]]] = None,\n+        prefix: Optional[str] = None,\n+        suppress_blank: bool = True,\n+        suppress_tokens: Optional[List[int]] = [-1],\n+        without_timestamps: bool = True,\n+        max_initial_timestamp: float = 1.0,\n+        word_timestamps: bool = False,\n+        prepend_punctuations: str = \"\\\"'\u201c\u00bf([{-\",\n+        append_punctuations: str = \"\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\",\n+        multilingual: bool = False,\n+        vad_filter: bool = True,\n+        vad_parameters: Optional[Union[dict, VadOptions]] = None,\n+        max_new_tokens: Optional[int] = None,\n+        chunk_length: Optional[int] = None,\n+        clip_timestamps: Optional[List[dict]] = None,\n+        hallucination_silence_threshold: Optional[float] = None,\n+        batch_size: int = 8,\n+        hotwords: Optional[str] = None,\n+        language_detection_threshold: Optional[float] = 0.5,\n+        language_detection_segments: int = 1,\n+    ) -> Tuple[AsyncGenerator[Segment, None], TranscriptionInfo]:\n+        \"\"\"transcribe audio in chunks in batched fashion and return with language info.\n+\n+        Arguments:\n+            audio: Path to the input file (or a file-like object), or the audio waveform.\n+            language: The language spoken in the audio. It should be a language code such\n+                as \"en\" or \"fr\". If not set, the language will be detected in the first 30 seconds\n+                of audio.\n+            task: Task to execute (transcribe or translate).\n+            log_progress: whether to show progress bar or not.\n+            beam_size: Beam size to use for decoding.\n+            best_of: Number of candidates when sampling with non-zero temperature.\n+            patience: Beam search patience factor.\n+            length_penalty: Exponential length penalty constant.\n+            repetition_penalty: Penalty applied to the score of previously generated tokens\n+                (set > 1 to penalize).\n+            no_repeat_ngram_size: Prevent repetitions of ngrams with this size (set 0 to disable).\n+            temperature: Temperature for sampling. If a list or tuple is passed,\n+                only the first value is used.\n+            initial_prompt: Optional text string or iterable of token ids to provide as a\n+                prompt for the each window.\n+            suppress_blank: Suppress blank outputs at the beginning of the sampling.\n+            suppress_tokens: List of token IDs to suppress. -1 will suppress a default set\n+                of symbols as defined in `tokenizer.non_speech_tokens()`.\n+            without_timestamps: Only sample text tokens.\n+            word_timestamps: Extract word-level timestamps using the cross-attention pattern\n+                and dynamic time warping, and include the timestamps for each word in each segment.\n+                Set as False.\n+            prepend_punctuations: If word_timestamps is True, merge these punctuation symbols\n+                with the next word\n+            append_punctuations: If word_timestamps is True, merge these punctuation symbols\n+                with the previous word\n+            multilingual: Perform language detection on every segment.\n+            vad_filter: Enable the voice activity detection (VAD) to filter out parts of the audio\n+                without speech. This step is using the Silero VAD model\n+                https://github.com/snakers4/silero-vad.\n+            vad_parameters: Dictionary of Silero VAD parameters or VadOptions class (see available\n+                parameters and default values in the class `VadOptions`).\n+            max_new_tokens: Maximum number of new tokens to generate per-chunk. If not set,\n+                the maximum will be set by the default max_length.\n+            chunk_length: The length of audio segments. If it is not None, it will overwrite the\n+                default chunk_length of the FeatureExtractor.\n+            clip_timestamps: Optionally provide list of dictionaries each containing \"start\" and\n+                \"end\" keys that specify the start and end of the voiced region within\n+                `chunk_length` boundary. vad_filter will be ignored if clip_timestamps is used.\n+            batch_size: the maximum number of parallel requests to model for decoding.\n+            hotwords:\n+                Hotwords/hint phrases to the model. Has no effect if prefix is not None.\n+            language_detection_threshold: If the maximum probability of the language tokens is\n+                higher than this value, the language is detected.\n+            language_detection_segments: Number of segments to consider for the language detection.\n+\n+        Unused Arguments\n+            compression_ratio_threshold: If the gzip compression ratio is above this value,\n+                treat as failed.\n+            log_prob_threshold: If the average log probability over sampled tokens is\n+                below this value, treat as failed.\n+            no_speech_threshold: If the no_speech probability is higher than this value AND\n+                the average log probability over sampled tokens is below `log_prob_threshold`,\n+                consider the segment as silent.\n+            condition_on_previous_text: If True, the previous output of the model is provided\n+                as a prompt for the next window; disabling may make the text inconsistent across\n+                windows, but the model becomes less prone to getting stuck in a failure loop,\n+                such as repetition looping or timestamps going out of sync. Set as False\n+            prompt_reset_on_temperature: Resets prompt if temperature is above this value.\n+                Arg has effect only if condition_on_previous_text is True. Set at 0.5\n+            prefix: Optional text to provide as a prefix at the beginning of each window.\n+            max_initial_timestamp: The initial timestamp cannot be later than this, set at 0.0.\n+            hallucination_silence_threshold: Optional[float]\n+                When word_timestamps is True, skip silent periods longer than this threshold\n+                (in seconds) when a possible hallucination is detected. set as None.\n+        Returns:\n+          A tuple with:\n+\n+            - a generator over transcribed segments\n+            - an instance of TranscriptionInfo\n+        \"\"\"\n+\n+        sampling_rate = self.model.feature_extractor.sampling_rate\n+\n+        if multilingual and not self.model.model.is_multilingual:\n+            self.model.logger.warning(\n+                \"The current model is English-only but the multilingual parameter is set to\"\n+                \"True; setting to False instead.\"\n+            )\n+            multilingual = False\n+\n+        if not isinstance(audio, np.ndarray):\n+            audio = decode_audio(audio, sampling_rate=sampling_rate)\n+        duration = audio.shape[0] / sampling_rate\n+\n+        self.model.logger.info(\n+            \"Processing audio with duration %s\", format_timestamp(duration)\n+        )\n+\n+        chunk_length = chunk_length or self.model.feature_extractor.chunk_length\n+        # if no segment split is provided, use vad_model and generate segments\n+        if not clip_timestamps:\n+            if vad_filter:\n+                if vad_parameters is None:\n+                    vad_parameters = VadOptions(\n+                        max_speech_duration_s=chunk_length,\n+                        min_silence_duration_ms=160,\n+                    )\n+                elif isinstance(vad_parameters, dict):\n+                    if \"max_speech_duration_s\" in vad_parameters.keys():\n+                        vad_parameters.pop(\"max_speech_duration_s\")\n+\n+                    vad_parameters = VadOptions(\n+                        **vad_parameters, max_speech_duration_s=chunk_length\n+                    )\n+\n+                clip_timestamps = get_speech_timestamps(audio, vad_parameters)\n+            # run the audio if it is less than 30 sec even without clip_timestamps\n+            elif duration < chunk_length:\n+                clip_timestamps = [{\"start\": 0, \"end\": audio.shape[0]}]\n+            else:\n+                raise RuntimeError(\n+                    \"No clip timestamps found. \"\n+                    \"Set 'vad_filter' to True or provide 'clip_timestamps'.\"\n+                )\n+\n+            audio_chunks, chunks_metadata = collect_chunks(\n+                audio, clip_timestamps, max_duration=chunk_length\n+            )\n+\n+        else:\n+            clip_timestamps = [\n+                {k: int(v * sampling_rate) for k, v in segment.items()}\n+                for segment in clip_timestamps\n+            ]\n+\n+            audio_chunks, chunks_metadata = [], []\n+            for clip in clip_timestamps:\n+                audio_chunks.append(audio[clip[\"start\"] : clip[\"end\"]])\n+                chunks_metadata.append(\n+                    {\n+                        \"offset\": clip[\"start\"] / sampling_rate,\n+                        \"duration\": (clip[\"end\"] - clip[\"start\"]) / sampling_rate,\n+                        \"segments\": [clip],\n+                    }\n+                )\n+\n+        duration_after_vad = (\n+            sum((segment[\"end\"] - segment[\"start\"]) for segment in clip_timestamps)\n+            / sampling_rate\n+        )\n+\n+        self.model.logger.info(\n+            \"VAD filter removed %s of audio\",\n+            format_timestamp(duration - duration_after_vad),\n+        )\n+\n+        features = (\n+            [self.model.feature_extractor(chunk)[..., :-1] for chunk in audio_chunks]\n+            if duration_after_vad\n+            else []\n+        )\n+\n+        all_language_probs = None\n+        # detecting the language if not provided\n+        if language is None:\n+            if not self.model.model.is_multilingual:\n+                language = \"en\"\n+                language_probability = 1\n+            else:\n+                (\n+                    language,\n+                    language_probability,\n+                    all_language_probs,\n+                ) = self.model.detect_language(\n+                    features=np.concatenate(\n+                        features\n+                        + [\n+                            np.full((self.model.model.n_mels, 1), -1.5, dtype=\"float32\")\n+                        ],\n+                        axis=1,\n+                    ),  # add a dummy feature to account for empty audio\n+                    language_detection_segments=language_detection_segments,\n+                    language_detection_threshold=language_detection_threshold,\n+                )\n+\n+                self.model.logger.info(\n+                    \"Detected language '%s' with probability %.2f\",\n+                    language,\n+                    language_probability,\n+                )\n+        else:\n+            if not self.model.model.is_multilingual and language != \"en\":\n+                self.model.logger.warning(\n+                    \"The current model is English-only but the language parameter is set to '%s'; \"\n+                    \"using 'en' instead.\" % language\n+                )\n+                language = \"en\"\n+\n+            language_probability = 1\n+\n+        tokenizer = Tokenizer(\n+            self.model.hf_tokenizer,\n+            self.model.model.is_multilingual,\n+            task=task,\n+            language=language,\n+        )\n+\n+        features = (\n+            np.stack([pad_or_trim(feature) for feature in features]) if features else []\n+        )\n+\n+        options = TranscriptionOptions(\n+            beam_size=beam_size,\n+            best_of=best_of,\n+            patience=patience,\n+            length_penalty=length_penalty,\n+            repetition_penalty=repetition_penalty,\n+            no_repeat_ngram_size=no_repeat_ngram_size,\n+            log_prob_threshold=log_prob_threshold,\n+            no_speech_threshold=no_speech_threshold,\n+            compression_ratio_threshold=compression_ratio_threshold,\n+            temperatures=(\n+                temperature[:1]\n+                if isinstance(temperature, (list, tuple))\n+                else [temperature]\n+            ),\n+            initial_prompt=initial_prompt,\n+            prefix=prefix,\n+            suppress_blank=suppress_blank,\n+            suppress_tokens=(\n+                get_suppressed_tokens(tokenizer, suppress_tokens)\n+                if suppress_tokens\n+                else suppress_tokens\n+            ),\n+            prepend_punctuations=prepend_punctuations,\n+            append_punctuations=append_punctuations,\n+            max_new_tokens=max_new_tokens,\n+            hotwords=hotwords,\n+            word_timestamps=word_timestamps,\n+            hallucination_silence_threshold=None,\n+            condition_on_previous_text=False,\n+            clip_timestamps=clip_timestamps,\n+            prompt_reset_on_temperature=0.5,\n+            multilingual=multilingual,\n+            without_timestamps=without_timestamps,\n+            max_initial_timestamp=0.0,\n+        )\n+\n+        info = TranscriptionInfo(\n+            language=language,\n+            language_probability=language_probability,\n+            duration=duration,\n+            duration_after_vad=duration_after_vad,\n+            transcription_options=options,\n+            vad_options=vad_parameters,\n+            all_language_probs=all_language_probs,\n+        )\n+\n+        segments = self._batched_segments_generator(\n+            features,\n+            tokenizer,\n+            chunks_metadata,\n+            batch_size,\n+            options,\n+            log_progress,\n+        )\n+        segments = restore_speech_timestamps(segments, clip_timestamps, sampling_rate)\n+\n+        return segments, info\n+\n+    async def _batched_segments_generator(\n+        self, features, tokenizer, chunks_metadata, batch_size, options, log_progress\n+    ):\n+        \"\"\"\n+        Asynchronous generator for batch processing of transcription segments.\n+\n+        Args:\n+            features: Extracted audio features.\n+            tokenizer: Tokenizer for decoding tokens.\n+            chunks_metadata: Audio chunks metadata.\n+            batch_size: Batch size for processing.\n+            options: Transcription options.\n+            log_progress: Flag to display progress.\n+\n+        Yields:\n+            Segment: Transcription segment objects.\n+        \"\"\"\n+        pbar = atqdm(total=len(features), disable=not log_progress, position=0)\n+        seg_idx = 0\n+        try:\n+            for i in range(0, len(features), batch_size):\n+                results = await self.forward(\n+                    features[i : i + batch_size],\n+                    tokenizer,\n+                    chunks_metadata[i : i + batch_size],\n+                    options,\n+                )\n+\n+                for result in results:\n+                    for segment in result:\n+                        seg_idx += 1\n+                        yield Segment(\n+                            seek=segment[\"seek\"],\n+                            id=seg_idx,\n+                            text=segment[\"text\"],\n+                            start=round(segment[\"start\"], 3),\n+                            end=round(segment[\"end\"], 3),\n+                            words=(\n+                                None\n+                                if not options.word_timestamps\n+                                else [Word(**word) for word in segment[\"words\"]]\n+                            ),\n+                            tokens=segment[\"tokens\"],\n+                            avg_logprob=segment[\"avg_logprob\"],\n+                            no_speech_prob=segment[\"no_speech_prob\"],\n+                            compression_ratio=segment[\"compression_ratio\"],\n+                            temperature=options.temperatures[0],\n+                        )\n+\n+                    pbar.update(1)\n+        finally:\n+            pbar.close()\n+            self.last_speech_timestamp = 0.0\n+\n class WhisperModel:\n     def __init__(\n         self,\n"
        },
        {
            "commit": "0b806e8ab76c7c89d77ba91d827fbe0d52ec97b0",
            "file_path": "setup.py",
            "diff": "diff --git a/setup.py b/setup.py\nindex e63e46a..b1e7e26 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -60,6 +60,7 @@ setup(\n             \"flake8==6.*\",\n             \"isort==5.*\",\n             \"pytest==7.*\",\n+            \"pytest-asyncio==0.21.*\",\n         ],\n     },\n     packages=find_packages(),\n"
        },
        {
            "commit": "0b806e8ab76c7c89d77ba91d827fbe0d52ec97b0",
            "file_path": "tests/test_transcribe.py",
            "diff": "diff --git a/tests/test_transcribe.py b/tests/test_transcribe.py\nindex 48b409e..9641a7e 100644\n--- a/tests/test_transcribe.py\n+++ b/tests/test_transcribe.py\n@@ -2,8 +2,14 @@ import inspect\n import os\n \n import numpy as np\n+import pytest\n \n-from faster_whisper import BatchedInferencePipeline, WhisperModel, decode_audio\n+from faster_whisper import (\n+    AsyncBatchedInferencePipeline,\n+    BatchedInferencePipeline,\n+    WhisperModel,\n+    decode_audio,\n+)\n \n \n def test_supported_languages():\n@@ -87,6 +93,34 @@ def test_batched_transcribe(physcisworks_path):\n         )\n     assert len(segments) > 7\n \n+@pytest.mark.asyncio\n+async def test_async_batched_transcribe(physcisworks_path):\n+    model = WhisperModel(\"tiny\")\n+    batched_model = AsyncBatchedInferencePipeline(model=model)\n+    result, info = await batched_model.transcribe(physcisworks_path, batch_size=16)\n+    assert info.language == \"en\"\n+    assert info.language_probability > 0.7\n+    segments = []\n+    async for segment in result:\n+        segments.append(\n+            {\"start\": segment.start, \"end\": segment.end, \"text\": segment.text}\n+        )\n+    # number of near 30 sec segments\n+    assert len(segments) == 6\n+\n+    result, info = await batched_model.transcribe(\n+        physcisworks_path,\n+        batch_size=16,\n+        without_timestamps=False,\n+        word_timestamps=True,\n+    )\n+    segments = []\n+    async for segment in result:\n+        assert segment.words is not None\n+        segments.append(\n+            {\"start\": segment.start, \"end\": segment.end, \"text\": segment.text}\n+        )\n+    assert len(segments) > 7\n \n def test_empty_audio():\n     audio = np.asarray([], dtype=\"float32\")\n"
        }
    ]
}