{
    "sha_fail": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
    "changed_files": [
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "examples/community/rerender_a_video.py",
            "diff": "diff --git a/examples/community/rerender_a_video.py b/examples/community/rerender_a_video.py\nnew file mode 100644\nindex 00000000..b28145ae\n--- /dev/null\n+++ b/examples/community/rerender_a_video.py\n@@ -0,0 +1,1178 @@\n+# Copyright 2023 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import sys\n+from dataclasses import dataclass\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import PIL.Image\n+import torch\n+import torch.nn.functional as F\n+import torchvision.transforms as T\n+from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer\n+\n+from diffusers.image_processor import VaeImageProcessor\n+from diffusers.models import AutoencoderKL, ControlNetModel, UNet2DConditionModel\n+from diffusers.models.attention_processor import Attention, AttnProcessor\n+from diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\n+from diffusers.pipelines.controlnet.pipeline_controlnet_img2img import StableDiffusionControlNetImg2ImgPipeline\n+from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n+from diffusers.schedulers import KarrasDiffusionSchedulers\n+from diffusers.utils import BaseOutput, deprecate, logging\n+from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n+\n+\n+gmflow_dir = \"/path/to/gmflow\"\n+sys.path.insert(0, gmflow_dir)\n+from gmflow.gmflow import GMFlow  # noqa: E402\n+\n+from utils.utils import InputPadder  # noqa: E402\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+def coords_grid(b, h, w, homogeneous=False, device=None):\n+    y, x = torch.meshgrid(torch.arange(h), torch.arange(w))  # [H, W]\n+\n+    stacks = [x, y]\n+\n+    if homogeneous:\n+        ones = torch.ones_like(x)  # [H, W]\n+        stacks.append(ones)\n+\n+    grid = torch.stack(stacks, dim=0).float()  # [2, H, W] or [3, H, W]\n+\n+    grid = grid[None].repeat(b, 1, 1, 1)  # [B, 2, H, W] or [B, 3, H, W]\n+\n+    if device is not None:\n+        grid = grid.to(device)\n+\n+    return grid\n+\n+\n+def bilinear_sample(img, sample_coords, mode=\"bilinear\", padding_mode=\"zeros\", return_mask=False):\n+    # img: [B, C, H, W]\n+    # sample_coords: [B, 2, H, W] in image scale\n+    if sample_coords.size(1) != 2:  # [B, H, W, 2]\n+        sample_coords = sample_coords.permute(0, 3, 1, 2)\n+\n+    b, _, h, w = sample_coords.shape\n+\n+    # Normalize to [-1, 1]\n+    x_grid = 2 * sample_coords[:, 0] / (w - 1) - 1\n+    y_grid = 2 * sample_coords[:, 1] / (h - 1) - 1\n+\n+    grid = torch.stack([x_grid, y_grid], dim=-1)  # [B, H, W, 2]\n+\n+    img = F.grid_sample(img, grid, mode=mode, padding_mode=padding_mode, align_corners=True)\n+\n+    if return_mask:\n+        mask = (x_grid >= -1) & (y_grid >= -1) & (x_grid <= 1) & (y_grid <= 1)  # [B, H, W]\n+\n+        return img, mask\n+\n+    return img\n+\n+\n+def flow_warp(feature, flow, mask=False, mode=\"bilinear\", padding_mode=\"zeros\"):\n+    b, c, h, w = feature.size()\n+    assert flow.size(1) == 2\n+\n+    grid = coords_grid(b, h, w).to(flow.device) + flow  # [B, 2, H, W]\n+    grid = grid.to(feature.dtype)\n+    return bilinear_sample(feature, grid, mode=mode, padding_mode=padding_mode, return_mask=mask)\n+\n+\n+def forward_backward_consistency_check(fwd_flow, bwd_flow, alpha=0.01, beta=0.5):\n+    # fwd_flow, bwd_flow: [B, 2, H, W]\n+    # alpha and beta values are following UnFlow\n+    # (https://arxiv.org/abs/1711.07837)\n+    assert fwd_flow.dim() == 4 and bwd_flow.dim() == 4\n+    assert fwd_flow.size(1) == 2 and bwd_flow.size(1) == 2\n+    flow_mag = torch.norm(fwd_flow, dim=1) + torch.norm(bwd_flow, dim=1)  # [B, H, W]\n+\n+    warped_bwd_flow = flow_warp(bwd_flow, fwd_flow)  # [B, 2, H, W]\n+    warped_fwd_flow = flow_warp(fwd_flow, bwd_flow)  # [B, 2, H, W]\n+\n+    diff_fwd = torch.norm(fwd_flow + warped_bwd_flow, dim=1)  # [B, H, W]\n+    diff_bwd = torch.norm(bwd_flow + warped_fwd_flow, dim=1)\n+\n+    threshold = alpha * flow_mag + beta\n+\n+    fwd_occ = (diff_fwd > threshold).float()  # [B, H, W]\n+    bwd_occ = (diff_bwd > threshold).float()\n+\n+    return fwd_occ, bwd_occ\n+\n+\n+@torch.no_grad()\n+def get_warped_and_mask(flow_model, image1, image2, image3=None, pixel_consistency=False):\n+    if image3 is None:\n+        image3 = image1\n+    padder = InputPadder(image1.shape, padding_factor=8)\n+    image1, image2 = padder.pad(image1[None].cuda(), image2[None].cuda())\n+    results_dict = flow_model(\n+        image1, image2, attn_splits_list=[2], corr_radius_list=[-1], prop_radius_list=[-1], pred_bidir_flow=True\n+    )\n+    flow_pr = results_dict[\"flow_preds\"][-1]  # [B, 2, H, W]\n+    fwd_flow = padder.unpad(flow_pr[0]).unsqueeze(0)  # [1, 2, H, W]\n+    bwd_flow = padder.unpad(flow_pr[1]).unsqueeze(0)  # [1, 2, H, W]\n+    fwd_occ, bwd_occ = forward_backward_consistency_check(fwd_flow, bwd_flow)  # [1, H, W] float\n+    if pixel_consistency:\n+        warped_image1 = flow_warp(image1, bwd_flow)\n+        bwd_occ = torch.clamp(\n+            bwd_occ + (abs(image2 - warped_image1).mean(dim=1) > 255 * 0.25).float(), 0, 1\n+        ).unsqueeze(0)\n+    warped_results = flow_warp(image3, bwd_flow)\n+    return warped_results, bwd_occ, bwd_flow\n+\n+\n+blur = T.GaussianBlur(kernel_size=(9, 9), sigma=(18, 18))\n+\n+\n+@dataclass\n+class TextToVideoSDPipelineOutput(BaseOutput):\n+    \"\"\"\n+    Output class for text-to-video pipelines.\n+\n+    Args:\n+        frames (`List[np.ndarray]` or `torch.FloatTensor`)\n+            List of denoised frames (essentially images) as NumPy arrays of shape `(height, width, num_channels)` or as\n+            a `torch` tensor. The length of the list denotes the video length (the number of frames).\n+    \"\"\"\n+\n+    frames: Union[List[np.ndarray], torch.FloatTensor]\n+\n+\n+@torch.no_grad()\n+def find_flat_region(mask):\n+    device = mask.device\n+    kernel_x = torch.Tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n+    kernel_y = torch.Tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]).unsqueeze(0).unsqueeze(0).to(device)\n+    mask_ = F.pad(mask.unsqueeze(0), (1, 1, 1, 1), mode=\"replicate\")\n+\n+    grad_x = torch.nn.functional.conv2d(mask_, kernel_x)\n+    grad_y = torch.nn.functional.conv2d(mask_, kernel_y)\n+    return ((abs(grad_x) + abs(grad_y)) == 0).float()[0]\n+\n+\n+class AttnState:\n+    STORE = 0\n+    LOAD = 1\n+    LOAD_AND_STORE_PREV = 2\n+\n+    def __init__(self):\n+        self.reset()\n+\n+    @property\n+    def state(self):\n+        return self.__state\n+\n+    @property\n+    def timestep(self):\n+        return self.__timestep\n+\n+    def set_timestep(self, t):\n+        self.__timestep = t\n+\n+    def reset(self):\n+        self.__state = AttnState.STORE\n+        self.__timestep = 0\n+\n+    def to_load(self):\n+        self.__state = AttnState.LOAD\n+\n+    def to_load_and_store_prev(self):\n+        self.__state = AttnState.LOAD_AND_STORE_PREV\n+\n+\n+class CrossFrameAttnProcessor(AttnProcessor):\n+    \"\"\"\n+    Cross frame attention processor. Each frame attends the first frame and previous frame.\n+\n+    Args:\n+        attn_state: Whether the model is processing the first frame or an intermediate frame\n+    \"\"\"\n+\n+    def __init__(self, attn_state: AttnState):\n+        super().__init__()\n+        self.attn_state = attn_state\n+        self.first_maps = {}\n+        self.prev_maps = {}\n+\n+    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):\n+        # Is self attention\n+        if encoder_hidden_states is None:\n+            t = self.attn_state.timestep\n+            if self.attn_state.state == AttnState.STORE:\n+                self.first_maps[t] = hidden_states.detach()\n+                self.prev_maps[t] = hidden_states.detach()\n+                res = super().__call__(attn, hidden_states, encoder_hidden_states, attention_mask, temb)\n+            else:\n+                if self.attn_state.state == AttnState.LOAD_AND_STORE_PREV:\n+                    tmp = hidden_states.detach()\n+                cross_map = torch.cat((self.first_maps[t], self.prev_maps[t]), dim=1)\n+                res = super().__call__(attn, hidden_states, cross_map, attention_mask, temb)\n+                if self.attn_state.state == AttnState.LOAD_AND_STORE_PREV:\n+                    self.prev_maps[t] = tmp\n+        else:\n+            res = super().__call__(attn, hidden_states, encoder_hidden_states, attention_mask, temb)\n+\n+        return res\n+\n+\n+def prepare_image(image):\n+    if isinstance(image, torch.Tensor):\n+        # Batch single image\n+        if image.ndim == 3:\n+            image = image.unsqueeze(0)\n+\n+        image = image.to(dtype=torch.float32)\n+    else:\n+        # preprocess image\n+        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n+            image = [image]\n+\n+        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n+            image = [np.array(i.convert(\"RGB\"))[None, :] for i in image]\n+            image = np.concatenate(image, axis=0)\n+        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n+            image = np.concatenate([i[None, :] for i in image], axis=0)\n+\n+        image = image.transpose(0, 3, 1, 2)\n+        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n+\n+    return image\n+\n+\n+class RerenderAVideoPipeline(StableDiffusionControlNetImg2ImgPipeline):\n+    r\"\"\"\n+    Pipeline for video-to-video translation using Stable Diffusion with Rerender Algorithm.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n+    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n+\n+    In addition the pipeline inherits the following loading methods:\n+        - *Textual-Inversion*: [`loaders.TextualInversionLoaderMixin.load_textual_inversion`]\n+\n+    Args:\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`CLIPTextModel`]):\n+            Frozen text-encoder. Stable Diffusion uses the text portion of\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n+            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n+        tokenizer (`CLIPTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n+        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n+        controlnet ([`ControlNetModel`] or `List[ControlNetModel]`):\n+            Provides additional conditioning to the unet during the denoising process. If you set multiple ControlNets\n+            as a list, the outputs from each ControlNet are added together to create one combined additional\n+            conditioning.\n+        scheduler ([`SchedulerMixin`]):\n+            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n+            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n+        safety_checker ([`StableDiffusionSafetyChecker`]):\n+            Classification module that estimates whether generated images could be considered offensive or harmful.\n+            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n+        feature_extractor ([`CLIPImageProcessor`]):\n+            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n+    \"\"\"\n+\n+    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n+\n+    def __init__(\n+        self,\n+        vae: AutoencoderKL,\n+        text_encoder: CLIPTextModel,\n+        tokenizer: CLIPTokenizer,\n+        unet: UNet2DConditionModel,\n+        controlnet: Union[ControlNetModel, List[ControlNetModel], Tuple[ControlNetModel], MultiControlNetModel],\n+        scheduler: KarrasDiffusionSchedulers,\n+        safety_checker: StableDiffusionSafetyChecker,\n+        feature_extractor: CLIPImageProcessor,\n+        image_encoder=None,\n+        requires_safety_checker: bool = True,\n+    ):\n+        super().__init__(\n+            vae,\n+            text_encoder,\n+            tokenizer,\n+            unet,\n+            controlnet,\n+            scheduler,\n+            safety_checker,\n+            feature_extractor,\n+            image_encoder,\n+            requires_safety_checker,\n+        )\n+\n+        if safety_checker is None and requires_safety_checker:\n+            logger.warning(\n+                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n+                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n+                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n+                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n+                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n+                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n+            )\n+\n+        if safety_checker is not None and feature_extractor is None:\n+            raise ValueError(\n+                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n+                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n+            )\n+\n+        if isinstance(controlnet, (list, tuple)):\n+            controlnet = MultiControlNetModel(controlnet)\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            unet=unet,\n+            controlnet=controlnet,\n+            scheduler=scheduler,\n+            safety_checker=safety_checker,\n+            feature_extractor=feature_extractor,\n+        )\n+        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor, do_convert_rgb=True)\n+        self.control_image_processor = VaeImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor, do_convert_rgb=True, do_normalize=False\n+        )\n+        self.register_to_config(requires_safety_checker=requires_safety_checker)\n+        self.attn_state = AttnState()\n+        attn_processor_dict = {}\n+        for k in unet.attn_processors.keys():\n+            if k.startswith(\"up\"):\n+                attn_processor_dict[k] = CrossFrameAttnProcessor(self.attn_state)\n+            else:\n+                attn_processor_dict[k] = AttnProcessor()\n+\n+        self.unet.set_attn_processor(attn_processor_dict)\n+\n+        flow_model = GMFlow(\n+            feature_channels=128,\n+            num_scales=1,\n+            upsample_factor=8,\n+            num_head=1,\n+            attention_type=\"swin\",\n+            ffn_dim_expansion=4,\n+            num_transformer_layers=6,\n+        ).to(\"cuda\")\n+\n+        checkpoint = torch.utils.model_zoo.load_url(\n+            \"https://huggingface.co/Anonymous-sub/Rerender/resolve/main/models/gmflow_sintel-0c07dcb3.pth\",\n+            map_location=lambda storage, loc: storage,\n+        )\n+        weights = checkpoint[\"model\"] if \"model\" in checkpoint else checkpoint\n+        flow_model.load_state_dict(weights, strict=False)\n+        flow_model.eval()\n+        self.flow_model = flow_model\n+\n+    # Modified from src/diffusers/pipelines/controlnet/pipeline_controlnet.StableDiffusionControlNetImg2ImgPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        callback_steps,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        controlnet_conditioning_scale=1.0,\n+        control_guidance_start=0.0,\n+        control_guidance_end=1.0,\n+    ):\n+        if (callback_steps is None) or (\n+            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n+        ):\n+            raise ValueError(\n+                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n+                f\" {type(callback_steps)}.\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and negative_prompt_embeds is not None:\n+            if prompt_embeds.shape != negative_prompt_embeds.shape:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n+                    f\" {negative_prompt_embeds.shape}.\"\n+                )\n+\n+        # `prompt` needs more sophisticated handling when there are multiple\n+        # conditionings.\n+        if isinstance(self.controlnet, MultiControlNetModel):\n+            if isinstance(prompt, list):\n+                logger.warning(\n+                    f\"You have {len(self.controlnet.nets)} ControlNets and you have passed {len(prompt)}\"\n+                    \" prompts. The conditionings will be fixed across the prompts.\"\n+                )\n+\n+        is_compiled = hasattr(F, \"scaled_dot_product_attention\") and isinstance(\n+            self.controlnet, torch._dynamo.eval_frame.OptimizedModule\n+        )\n+\n+        # Check `controlnet_conditioning_scale`\n+        if (\n+            isinstance(self.controlnet, ControlNetModel)\n+            or is_compiled\n+            and isinstance(self.controlnet._orig_mod, ControlNetModel)\n+        ):\n+            if not isinstance(controlnet_conditioning_scale, float):\n+                raise TypeError(\"For single controlnet: `controlnet_conditioning_scale` must be type `float`.\")\n+        elif (\n+            isinstance(self.controlnet, MultiControlNetModel)\n+            or is_compiled\n+            and isinstance(self.controlnet._orig_mod, MultiControlNetModel)\n+        ):\n+            if isinstance(controlnet_conditioning_scale, list):\n+                if any(isinstance(i, list) for i in controlnet_conditioning_scale):\n+                    raise ValueError(\"A single batch of multiple conditionings are supported at the moment.\")\n+            elif isinstance(controlnet_conditioning_scale, list) and len(controlnet_conditioning_scale) != len(\n+                self.controlnet.nets\n+            ):\n+                raise ValueError(\n+                    \"For multiple controlnets: When `controlnet_conditioning_scale` is specified as `list`, it must have\"\n+                    \" the same length as the number of controlnets\"\n+                )\n+        else:\n+            assert False\n+\n+        if len(control_guidance_start) != len(control_guidance_end):\n+            raise ValueError(\n+                f\"`control_guidance_start` has {len(control_guidance_start)} elements, but `control_guidance_end` has {len(control_guidance_end)} elements. Make sure to provide the same number of elements to each list.\"\n+            )\n+\n+        if isinstance(self.controlnet, MultiControlNetModel):\n+            if len(control_guidance_start) != len(self.controlnet.nets):\n+                raise ValueError(\n+                    f\"`control_guidance_start`: {control_guidance_start} has {len(control_guidance_start)} elements but there are {len(self.controlnet.nets)} controlnets available. Make sure to provide {len(self.controlnet.nets)}.\"\n+                )\n+\n+        for start, end in zip(control_guidance_start, control_guidance_end):\n+            if start >= end:\n+                raise ValueError(\n+                    f\"control guidance start: {start} cannot be larger or equal to control guidance end: {end}.\"\n+                )\n+            if start < 0.0:\n+                raise ValueError(f\"control guidance start: {start} can't be smaller than 0.\")\n+            if end > 1.0:\n+                raise ValueError(f\"control guidance end: {end} can't be larger than 1.0.\")\n+\n+    # Copied from diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline.prepare_image\n+    def prepare_control_image(\n+        self,\n+        image,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        image = self.control_image_processor.preprocess(image, height=height, width=width).to(dtype=torch.float32)\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+\n+        image = image.to(device=device, dtype=dtype)\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            image = torch.cat([image] * 2)\n+\n+        return image\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline.get_timesteps\n+    def get_timesteps(self, num_inference_steps, strength, device):\n+        # get the original timestep using init_timestep\n+        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n+\n+        t_start = max(num_inference_steps - init_timestep, 0)\n+        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]\n+\n+        return timesteps, num_inference_steps - t_start\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline.prepare_latents\n+    def prepare_latents(self, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n+        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n+            raise ValueError(\n+                f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n+            )\n+\n+        image = image.to(device=device, dtype=dtype)\n+\n+        batch_size = batch_size * num_images_per_prompt\n+\n+        if image.shape[1] == 4:\n+            init_latents = image\n+\n+        else:\n+            if isinstance(generator, list) and len(generator) != batch_size:\n+                raise ValueError(\n+                    f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                    f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+                )\n+\n+            elif isinstance(generator, list):\n+                init_latents = [\n+                    self.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n+                ]\n+                init_latents = torch.cat(init_latents, dim=0)\n+            else:\n+                init_latents = self.vae.encode(image).latent_dist.sample(generator)\n+\n+            init_latents = self.vae.config.scaling_factor * init_latents\n+\n+        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n+            # expand init_latents for batch_size\n+            deprecation_message = (\n+                f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n+                \" images (`image`). Initial images are now duplicating to match the number of text prompts. Note\"\n+                \" that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update\"\n+                \" your script to pass as many initial images as text prompts to suppress this warning.\"\n+            )\n+            deprecate(\"len(prompt) != len(image)\", \"1.0.0\", deprecation_message, standard_warn=False)\n+            additional_image_per_prompt = batch_size // init_latents.shape[0]\n+            init_latents = torch.cat([init_latents] * additional_image_per_prompt, dim=0)\n+        elif batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] != 0:\n+            raise ValueError(\n+                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n+            )\n+        else:\n+            init_latents = torch.cat([init_latents], dim=0)\n+\n+        shape = init_latents.shape\n+        noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+\n+        # get latents\n+        init_latents = self.scheduler.add_noise(init_latents, noise, timestep)\n+        latents = init_latents\n+\n+        return latents\n+\n+    @torch.no_grad()\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        frames: Union[List[np.ndarray], torch.FloatTensor] = None,\n+        control_frames: Union[List[np.ndarray], torch.FloatTensor] = None,\n+        strength: float = 0.8,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 7.5,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        eta: float = 0.0,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.FloatTensor] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n+        callback_steps: int = 1,\n+        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n+        guess_mode: bool = False,\n+        control_guidance_start: Union[float, List[float]] = 0.0,\n+        control_guidance_end: Union[float, List[float]] = 1.0,\n+        warp_start: Union[float, List[float]] = 0.0,\n+        warp_end: Union[float, List[float]] = 0.3,\n+        mask_start: Union[float, List[float]] = 0.5,\n+        mask_end: Union[float, List[float]] = 0.8,\n+        smooth_boundary: bool = True,\n+        mask_strength: Union[float, List[float]] = 0.5,\n+        inner_strength: Union[float, List[float]] = 0.9,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            frames (`List[np.ndarray]` or `torch.FloatTensor`): The input images to be used as the starting point for the image generation process.\n+            control_frames (`List[np.ndarray]` or `torch.FloatTensor`): The ControlNet input images condition to provide guidance to the `unet` for generation.\n+            strength ('float'): SDEdit strength.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, *optional*, defaults to 7.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            eta (`float`, *optional*, defaults to 0.0):\n+                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n+                [`schedulers.DDIMScheduler`], will be ignored for others.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.FloatTensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will ge generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n+                plain tuple.\n+            callback (`Callable`, *optional*):\n+                A function that will be called every `callback_steps` steps during inference. The function will be\n+                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n+            callback_steps (`int`, *optional*, defaults to 1):\n+                The frequency at which the `callback` function will be called. If not specified, the callback will be\n+                called at every step.\n+            cross_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            controlnet_conditioning_scale (`float` or `List[float]`, *optional*, defaults to 1.0):\n+                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\n+                to the residual in the original unet. If multiple ControlNets are specified in init, you can set the\n+                corresponding scale as a list. Note that by default, we use a smaller conditioning scale for inpainting\n+                than for [`~StableDiffusionControlNetPipeline.__call__`].\n+            guess_mode (`bool`, *optional*, defaults to `False`):\n+                In this mode, the ControlNet encoder will try best to recognize the content of the input image even if\n+                you remove all prompts. The `guidance_scale` between 3.0 and 5.0 is recommended.\n+            control_guidance_start (`float` or `List[float]`, *optional*, defaults to 0.0):\n+                The percentage of total steps at which the controlnet starts applying.\n+            control_guidance_end (`float` or `List[float]`, *optional*, defaults to 1.0):\n+                The percentage of total steps at which the controlnet stops applying.\n+            warp_start (`float`): Shape-aware fusion start timestep.\n+            warp_end (`float`): Shape-aware fusion end timestep.\n+            mask_start (`float`): Pixel-aware fusion start timestep.\n+            mask_end (`float`):Pixel-aware fusion end timestep.\n+            smooth_boundary (`bool`): Smooth fusion boundary. Set `True` to prevent artifacts at boundary.\n+            mask_strength (`float`): Pixel-aware fusion strength.\n+            inner_strength (`float`): Pixel-aware fusion detail level.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n+            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n+            When returning a tuple, the first element is a list with the generated images, and the second element is a\n+            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n+            (nsfw) content, according to the `safety_checker`.\n+        \"\"\"\n+        controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n+\n+        # align format for control guidance\n+        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n+            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n+        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n+            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n+        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n+            mult = len(controlnet.nets) if isinstance(controlnet, MultiControlNetModel) else 1\n+            control_guidance_start, control_guidance_end = (\n+                mult * [control_guidance_start],\n+                mult * [control_guidance_end],\n+            )\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            callback_steps,\n+            negative_prompt,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            controlnet_conditioning_scale,\n+            control_guidance_start,\n+            control_guidance_end,\n+        )\n+\n+        # 2. Define call parameters\n+        # Currently we only support 1 prompt\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            assert False\n+        else:\n+            assert False\n+        num_images_per_prompt = 1\n+\n+        device = self._execution_device\n+        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n+        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n+        # corresponds to doing no classifier free guidance.\n+        do_classifier_free_guidance = guidance_scale > 1.0\n+\n+        if isinstance(controlnet, MultiControlNetModel) and isinstance(controlnet_conditioning_scale, float):\n+            controlnet_conditioning_scale = [controlnet_conditioning_scale] * len(controlnet.nets)\n+\n+        global_pool_conditions = (\n+            controlnet.config.global_pool_conditions\n+            if isinstance(controlnet, ControlNetModel)\n+            else controlnet.nets[0].config.global_pool_conditions\n+        )\n+        guess_mode = guess_mode or global_pool_conditions\n+\n+        # 3. Encode input prompt\n+        text_encoder_lora_scale = (\n+            cross_attention_kwargs.get(\"scale\", None) if cross_attention_kwargs is not None else None\n+        )\n+        prompt_embeds = self._encode_prompt(\n+            prompt,\n+            device,\n+            num_images_per_prompt,\n+            do_classifier_free_guidance,\n+            negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            lora_scale=text_encoder_lora_scale,\n+        )\n+\n+        # 4. Process the first frame\n+        height, width = None, None\n+        output_frames = []\n+        self.attn_state.reset()\n+\n+        # 4.1 prepare frames\n+        image = self.image_processor.preprocess(frames[0]).to(dtype=torch.float32)\n+        first_image = image[0]  # C, H, W\n+\n+        # 4.2 Prepare controlnet_conditioning_image\n+        # Currently we only support single control\n+        if isinstance(controlnet, ControlNetModel):\n+            control_image = self.prepare_control_image(\n+                image=control_frames[0],\n+                width=width,\n+                height=height,\n+                batch_size=batch_size,\n+                num_images_per_prompt=1,\n+                device=device,\n+                dtype=controlnet.dtype,\n+                do_classifier_free_guidance=do_classifier_free_guidance,\n+                guess_mode=guess_mode,\n+            )\n+        else:\n+            assert False\n+\n+        # 4.3 Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps, cur_num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n+        latent_timestep = timesteps[:1].repeat(batch_size)\n+\n+        # 4.4 Prepare latent variables\n+        latents = self.prepare_latents(\n+            image,\n+            latent_timestep,\n+            batch_size,\n+            num_images_per_prompt,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+        )\n+\n+        # 4.5 Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n+        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n+\n+        # 4.6 Create tensor stating which controlnets to keep\n+        controlnet_keep = []\n+        for i in range(len(timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n+                for s, e in zip(control_guidance_start, control_guidance_end)\n+            ]\n+            controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n+\n+        first_x0_list = []\n+\n+        # 4.7 Denoising loop\n+        num_warmup_steps = len(timesteps) - cur_num_inference_steps * self.scheduler.order\n+        with self.progress_bar(total=cur_num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                self.attn_state.set_timestep(t.item())\n+\n+                # expand the latents if we are doing classifier free guidance\n+                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n+                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n+\n+                # controlnet(s) inference\n+                if guess_mode and do_classifier_free_guidance:\n+                    # Infer ControlNet only for the conditional batch.\n+                    control_model_input = latents\n+                    control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n+                    controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n+                else:\n+                    control_model_input = latent_model_input\n+                    controlnet_prompt_embeds = prompt_embeds\n+\n+                if isinstance(controlnet_keep[i], list):\n+                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n+                else:\n+                    controlnet_cond_scale = controlnet_conditioning_scale\n+                    if isinstance(controlnet_cond_scale, list):\n+                        controlnet_cond_scale = controlnet_cond_scale[0]\n+                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n+\n+                down_block_res_samples, mid_block_res_sample = self.controlnet(\n+                    control_model_input,\n+                    t,\n+                    encoder_hidden_states=controlnet_prompt_embeds,\n+                    controlnet_cond=control_image,\n+                    conditioning_scale=cond_scale,\n+                    guess_mode=guess_mode,\n+                    return_dict=False,\n+                )\n+\n+                if guess_mode and do_classifier_free_guidance:\n+                    # Infered ControlNet only for the conditional batch.\n+                    # To apply the output of ControlNet to both the unconditional and conditional batches,\n+                    # add 0 to the unconditional batch to keep it unchanged.\n+                    down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n+                    mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n+\n+                # predict the noise residual\n+                noise_pred = self.unet(\n+                    latent_model_input,\n+                    t,\n+                    encoder_hidden_states=prompt_embeds,\n+                    cross_attention_kwargs=cross_attention_kwargs,\n+                    down_block_additional_residuals=down_block_res_samples,\n+                    mid_block_additional_residual=mid_block_res_sample,\n+                    return_dict=False,\n+                )[0]\n+\n+                # perform guidance\n+                if do_classifier_free_guidance:\n+                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n+\n+                alpha_prod_t = self.scheduler.alphas_cumprod[t]\n+                beta_prod_t = 1 - alpha_prod_t\n+                pred_x0 = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n+                first_x0 = pred_x0.detach()\n+                first_x0_list.append(first_x0)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+                    if callback is not None and i % callback_steps == 0:\n+                        callback(i, t, latents)\n+\n+        if not output_type == \"latent\":\n+            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n+        else:\n+            image = latents\n+\n+        first_result = image\n+        prev_result = image\n+        do_denormalize = [True] * image.shape[0]\n+        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n+\n+        output_frames.append(image[0])\n+\n+        # 5. Process each frame\n+        for idx in range(1, len(frames)):\n+            image = frames[idx]\n+            prev_image = frames[idx - 1]\n+            control_image = control_frames[idx]\n+            # 5.1 prepare frames\n+            image = self.image_processor.preprocess(image).to(dtype=torch.float32)\n+            prev_image = self.image_processor.preprocess(prev_image).to(dtype=torch.float32)\n+\n+            warped_0, bwd_occ_0, bwd_flow_0 = get_warped_and_mask(\n+                self.flow_model, first_image, image[0], first_result, False\n+            )\n+            blend_mask_0 = blur(F.max_pool2d(bwd_occ_0, kernel_size=9, stride=1, padding=4))\n+            blend_mask_0 = torch.clamp(blend_mask_0 + bwd_occ_0, 0, 1)\n+\n+            warped_pre, bwd_occ_pre, bwd_flow_pre = get_warped_and_mask(\n+                self.flow_model, prev_image[0], image[0], prev_result, False\n+            )\n+            blend_mask_pre = blur(F.max_pool2d(bwd_occ_pre, kernel_size=9, stride=1, padding=4))\n+            blend_mask_pre = torch.clamp(blend_mask_pre + bwd_occ_pre, 0, 1)\n+\n+            warp_mask = 1 - F.max_pool2d(blend_mask_0, kernel_size=8)\n+            warp_flow = F.interpolate(bwd_flow_0 / 8.0, scale_factor=1.0 / 8, mode=\"bilinear\")\n+\n+            # 5.2 Prepare controlnet_conditioning_image\n+            # Currently we only support single control\n+            if isinstance(controlnet, ControlNetModel):\n+                control_image = self.prepare_control_image(\n+                    image=control_image,\n+                    width=width,\n+                    height=height,\n+                    batch_size=batch_size,\n+                    num_images_per_prompt=1,\n+                    device=device,\n+                    dtype=controlnet.dtype,\n+                    do_classifier_free_guidance=do_classifier_free_guidance,\n+                    guess_mode=guess_mode,\n+                )\n+            else:\n+                assert False\n+\n+            # 5.3 Prepare timesteps\n+            self.scheduler.set_timesteps(num_inference_steps, device=device)\n+            timesteps, cur_num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n+            latent_timestep = timesteps[:1].repeat(batch_size)\n+\n+            skip_t = int(num_inference_steps * (1 - strength))\n+            warp_start_t = int(warp_start * num_inference_steps)\n+            warp_end_t = int(warp_end * num_inference_steps)\n+            mask_start_t = int(mask_start * num_inference_steps)\n+            mask_end_t = int(mask_end * num_inference_steps)\n+\n+            # 5.4 Prepare latent variables\n+            init_latents = self.prepare_latents(\n+                image,\n+                latent_timestep,\n+                batch_size,\n+                num_images_per_prompt,\n+                prompt_embeds.dtype,\n+                device,\n+                generator,\n+            )\n+\n+            # 5.5 Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n+            extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n+\n+            # 5.6 Create tensor stating which controlnets to keep\n+            controlnet_keep = []\n+            for i in range(len(timesteps)):\n+                keeps = [\n+                    1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n+                    for s, e in zip(control_guidance_start, control_guidance_end)\n+                ]\n+                controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n+\n+            # 5.7 Denoising loop\n+            num_warmup_steps = len(timesteps) - cur_num_inference_steps * self.scheduler.order\n+\n+            def denoising_loop(latents, mask=None, xtrg=None, noise_rescale=None):\n+                dir_xt = 0\n+                latents_dtype = latents.dtype\n+                with self.progress_bar(total=cur_num_inference_steps) as progress_bar:\n+                    for i, t in enumerate(timesteps):\n+                        self.attn_state.set_timestep(t.item())\n+                        if i + skip_t >= mask_start_t and i + skip_t <= mask_end_t and xtrg is not None:\n+                            rescale = torch.maximum(1.0 - mask, (1 - mask**2) ** 0.5 * inner_strength)\n+                            if noise_rescale is not None:\n+                                rescale = (1.0 - mask) * (1 - noise_rescale) + rescale * noise_rescale\n+                            noise = randn_tensor(xtrg.shape, generator=generator, device=device, dtype=xtrg.dtype)\n+                            latents_ref = self.scheduler.add_noise(xtrg, noise, t)\n+                            latents = latents_ref * mask + (1.0 - mask) * (latents - dir_xt) + rescale * dir_xt\n+                            latents = latents.to(latents_dtype)\n+\n+                        # expand the latents if we are doing classifier free guidance\n+                        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n+                        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n+\n+                        # controlnet(s) inference\n+                        if guess_mode and do_classifier_free_guidance:\n+                            # Infer ControlNet only for the conditional batch.\n+                            control_model_input = latents\n+                            control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n+                            controlnet_prompt_embeds = prompt_embeds.chunk(2)[1]\n+                        else:\n+                            control_model_input = latent_model_input\n+                            controlnet_prompt_embeds = prompt_embeds\n+\n+                        if isinstance(controlnet_keep[i], list):\n+                            cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n+                        else:\n+                            controlnet_cond_scale = controlnet_conditioning_scale\n+                            if isinstance(controlnet_cond_scale, list):\n+                                controlnet_cond_scale = controlnet_cond_scale[0]\n+                            cond_scale = controlnet_cond_scale * controlnet_keep[i]\n+                        down_block_res_samples, mid_block_res_sample = self.controlnet(\n+                            control_model_input,\n+                            t,\n+                            encoder_hidden_states=controlnet_prompt_embeds,\n+                            controlnet_cond=control_image,\n+                            conditioning_scale=cond_scale,\n+                            guess_mode=guess_mode,\n+                            return_dict=False,\n+                        )\n+\n+                        if guess_mode and do_classifier_free_guidance:\n+                            # Infered ControlNet only for the conditional batch.\n+                            # To apply the output of ControlNet to both the unconditional and conditional batches,\n+                            # add 0 to the unconditional batch to keep it unchanged.\n+                            down_block_res_samples = [\n+                                torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples\n+                            ]\n+                            mid_block_res_sample = torch.cat(\n+                                [torch.zeros_like(mid_block_res_sample), mid_block_res_sample]\n+                            )\n+\n+                        # predict the noise residual\n+                        noise_pred = self.unet(\n+                            latent_model_input,\n+                            t,\n+                            encoder_hidden_states=prompt_embeds,\n+                            cross_attention_kwargs=cross_attention_kwargs,\n+                            down_block_additional_residuals=down_block_res_samples,\n+                            mid_block_additional_residual=mid_block_res_sample,\n+                            return_dict=False,\n+                        )[0]\n+\n+                        # perform guidance\n+                        if do_classifier_free_guidance:\n+                            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+                            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n+\n+                        # Get pred_x0 from scheduler\n+                        alpha_prod_t = self.scheduler.alphas_cumprod[t]\n+                        beta_prod_t = 1 - alpha_prod_t\n+                        pred_x0 = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n+\n+                        if i + skip_t >= warp_start_t and i + skip_t <= warp_end_t:\n+                            # warp x_0\n+                            pred_x0 = (\n+                                flow_warp(first_x0_list[i], warp_flow, mode=\"nearest\") * warp_mask\n+                                + (1 - warp_mask) * pred_x0\n+                            )\n+\n+                            # get x_t from x_0\n+                            latents = self.scheduler.add_noise(pred_x0, noise_pred, t).to(latents_dtype)\n+\n+                        prev_t = t - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n+                        if i == len(timesteps) - 1:\n+                            alpha_t_prev = 1.0\n+                        else:\n+                            alpha_t_prev = self.scheduler.alphas_cumprod[prev_t]\n+\n+                        dir_xt = (1.0 - alpha_t_prev) ** 0.5 * noise_pred\n+\n+                        # compute the previous noisy sample x_t -> x_t-1\n+                        latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[\n+                            0\n+                        ]\n+\n+                        # call the callback, if provided\n+                        if i == len(timesteps) - 1 or (\n+                            (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n+                        ):\n+                            progress_bar.update()\n+                            if callback is not None and i % callback_steps == 0:\n+                                callback(i, t, latents)\n+\n+                    return latents\n+\n+            if mask_start_t <= mask_end_t:\n+                self.attn_state.to_load()\n+            else:\n+                self.attn_state.to_load_and_store_prev()\n+            latents = denoising_loop(init_latents)\n+\n+            if mask_start_t <= mask_end_t:\n+                direct_result = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n+\n+                blend_results = (1 - blend_mask_pre) * warped_pre + blend_mask_pre * direct_result\n+                blend_results = (1 - blend_mask_0) * warped_0 + blend_mask_0 * blend_results\n+\n+                bwd_occ = 1 - torch.clamp(1 - bwd_occ_pre + 1 - bwd_occ_0, 0, 1)\n+                blend_mask = blur(F.max_pool2d(bwd_occ, kernel_size=9, stride=1, padding=4))\n+                blend_mask = 1 - torch.clamp(blend_mask + bwd_occ, 0, 1)\n+\n+                blend_results = blend_results.to(latents.dtype)\n+                xtrg = self.vae.encode(blend_results).latent_dist.sample(generator)\n+                xtrg = self.vae.config.scaling_factor * xtrg\n+                blend_results_rec = self.vae.decode(xtrg / self.vae.config.scaling_factor, return_dict=False)[0]\n+                xtrg_rec = self.vae.encode(blend_results_rec).latent_dist.sample(generator)\n+                xtrg_rec = self.vae.config.scaling_factor * xtrg_rec\n+                xtrg_ = xtrg + (xtrg - xtrg_rec)\n+                blend_results_rec_new = self.vae.decode(xtrg_ / self.vae.config.scaling_factor, return_dict=False)[0]\n+                tmp = (abs(blend_results_rec_new - blend_results).mean(dim=1, keepdims=True) > 0.25).float()\n+\n+                mask_x = F.max_pool2d(\n+                    (F.interpolate(tmp, scale_factor=1 / 8.0, mode=\"bilinear\") > 0).float(),\n+                    kernel_size=3,\n+                    stride=1,\n+                    padding=1,\n+                )\n+\n+                mask = 1 - F.max_pool2d(1 - blend_mask, kernel_size=8)  # * (1-mask_x)\n+\n+                if smooth_boundary:\n+                    noise_rescale = find_flat_region(mask)\n+                else:\n+                    noise_rescale = torch.ones_like(mask)\n+\n+                xtrg = (xtrg + (1 - mask_x) * (xtrg - xtrg_rec)) * mask\n+                xtrg = xtrg.to(latents.dtype)\n+\n+                self.scheduler.set_timesteps(num_inference_steps, device=device)\n+                timesteps, cur_num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n+\n+                self.attn_state.to_load_and_store_prev()\n+                latents = denoising_loop(init_latents, mask * mask_strength, xtrg, noise_rescale)\n+\n+            if not output_type == \"latent\":\n+                image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n+            else:\n+                image = latents\n+\n+            prev_result = image\n+\n+            do_denormalize = [True] * image.shape[0]\n+            image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n+\n+            output_frames.append(image[0])\n+\n+        # Offload last model to CPU\n+        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n+            self.final_offload_hook.offload()\n+\n+        if not return_dict:\n+            return output_frames\n+\n+        return TextToVideoSDPipelineOutput(frames=output_frames)\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "examples/dreambooth/train_dreambooth_lora_sdxl.py",
            "diff": "diff --git a/examples/dreambooth/train_dreambooth_lora_sdxl.py b/examples/dreambooth/train_dreambooth_lora_sdxl.py\nindex 0b7bd64e..122af238 100644\n--- a/examples/dreambooth/train_dreambooth_lora_sdxl.py\n+++ b/examples/dreambooth/train_dreambooth_lora_sdxl.py\n@@ -780,13 +780,12 @@ def encode_prompt(text_encoders, tokenizers, prompt, text_input_ids_list=None):\n             text_input_ids = text_input_ids_list[i]\n \n         prompt_embeds = text_encoder(\n-            text_input_ids.to(text_encoder.device),\n-            output_hidden_states=True,\n+            text_input_ids.to(text_encoder.device), output_hidden_states=True, return_dict=False\n         )\n \n         # We are only ALWAYS interested in the pooled output of the final text encoder\n         pooled_prompt_embeds = prompt_embeds[0]\n-        prompt_embeds = prompt_embeds.hidden_states[-2]\n+        prompt_embeds = prompt_embeds[-1][-2]\n         bs_embed, seq_len, _ = prompt_embeds.shape\n         prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)\n         prompt_embeds_list.append(prompt_embeds)\n@@ -1429,7 +1428,8 @@ def main(args):\n                         timesteps,\n                         prompt_embeds_input,\n                         added_cond_kwargs=unet_added_conditions,\n-                    ).sample\n+                        return_dict=False,\n+                    )[0]\n                 else:\n                     unet_added_conditions = {\"time_ids\": add_time_ids.repeat(elems_to_repeat_time_ids, 1)}\n                     prompt_embeds, pooled_prompt_embeds = encode_prompt(\n@@ -1443,8 +1443,12 @@ def main(args):\n                     )\n                     prompt_embeds_input = prompt_embeds.repeat(elems_to_repeat_text_embeds, 1, 1)\n                     model_pred = unet(\n-                        noisy_model_input, timesteps, prompt_embeds_input, added_cond_kwargs=unet_added_conditions\n-                    ).sample\n+                        noisy_model_input,\n+                        timesteps,\n+                        prompt_embeds_input,\n+                        added_cond_kwargs=unet_added_conditions,\n+                        return_dict=False,\n+                    )[0]\n \n                 # Get the target for loss depending on the prediction type\n                 if noise_scheduler.config.prediction_type == \"epsilon\":\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "examples/research_projects/instructpix2pix_lora/train_instruct_pix2pix_lora.py",
            "diff": "diff --git a/examples/research_projects/instructpix2pix_lora/train_instruct_pix2pix_lora.py b/examples/research_projects/instructpix2pix_lora/train_instruct_pix2pix_lora.py\nnew file mode 100644\nindex 00000000..3437be1a\n--- /dev/null\n+++ b/examples/research_projects/instructpix2pix_lora/train_instruct_pix2pix_lora.py\n@@ -0,0 +1,1049 @@\n+#!/usr/bin/env python\n+# coding=utf-8\n+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Script to fine-tune Stable Diffusion for InstructPix2Pix.\"\"\"\n+\n+import argparse\n+import logging\n+import math\n+import os\n+import shutil\n+from pathlib import Path\n+\n+import accelerate\n+import datasets\n+import numpy as np\n+import PIL\n+import requests\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+import transformers\n+from accelerate import Accelerator\n+from accelerate.logging import get_logger\n+from accelerate.utils import ProjectConfiguration, set_seed\n+from datasets import load_dataset\n+from huggingface_hub import create_repo, upload_folder\n+from packaging import version\n+from torchvision import transforms\n+from tqdm.auto import tqdm\n+from transformers import CLIPTextModel, CLIPTokenizer\n+\n+import diffusers\n+from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionInstructPix2PixPipeline, UNet2DConditionModel\n+from diffusers.models.lora import LoRALinearLayer\n+from diffusers.optimization import get_scheduler\n+from diffusers.training_utils import EMAModel\n+from diffusers.utils import check_min_version, deprecate, is_wandb_available\n+from diffusers.utils.import_utils import is_xformers_available\n+\n+\n+# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n+check_min_version(\"0.26.0.dev0\")\n+\n+logger = get_logger(__name__, log_level=\"INFO\")\n+\n+DATASET_NAME_MAPPING = {\n+    \"fusing/instructpix2pix-1000-samples\": (\"input_image\", \"edit_prompt\", \"edited_image\"),\n+}\n+WANDB_TABLE_COL_NAMES = [\"original_image\", \"edited_image\", \"edit_prompt\"]\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(description=\"Simple example of a training script for InstructPix2Pix.\")\n+    parser.add_argument(\n+        \"--pretrained_model_name_or_path\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n+    )\n+    parser.add_argument(\n+        \"--revision\",\n+        type=str,\n+        default=None,\n+        required=False,\n+        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n+    )\n+    parser.add_argument(\n+        \"--variant\",\n+        type=str,\n+        default=None,\n+        help=\"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\",\n+    )\n+    parser.add_argument(\n+        \"--dataset_name\",\n+        type=str,\n+        default=None,\n+        help=(\n+            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n+            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n+            \" or to a folder containing files that \ud83e\udd17 Datasets can understand.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--dataset_config_name\",\n+        type=str,\n+        default=None,\n+        help=\"The config of the Dataset, leave as None if there's only one config.\",\n+    )\n+    parser.add_argument(\n+        \"--train_data_dir\",\n+        type=str,\n+        default=None,\n+        help=(\n+            \"A folder containing the training data. Folder contents must follow the structure described in\"\n+            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n+            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--original_image_column\",\n+        type=str,\n+        default=\"input_image\",\n+        help=\"The column of the dataset containing the original image on which edits where made.\",\n+    )\n+    parser.add_argument(\n+        \"--edited_image_column\",\n+        type=str,\n+        default=\"edited_image\",\n+        help=\"The column of the dataset containing the edited image.\",\n+    )\n+    parser.add_argument(\n+        \"--edit_prompt_column\",\n+        type=str,\n+        default=\"edit_prompt\",\n+        help=\"The column of the dataset containing the edit instruction.\",\n+    )\n+    parser.add_argument(\n+        \"--val_image_url\",\n+        type=str,\n+        default=None,\n+        help=\"URL to the original image that you would like to edit (used during inference for debugging purposes).\",\n+    )\n+    parser.add_argument(\n+        \"--validation_prompt\", type=str, default=None, help=\"A prompt that is sampled during training for inference.\"\n+    )\n+    parser.add_argument(\n+        \"--num_validation_images\",\n+        type=int,\n+        default=4,\n+        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n+    )\n+    parser.add_argument(\n+        \"--validation_epochs\",\n+        type=int,\n+        default=1,\n+        help=(\n+            \"Run fine-tuning validation every X epochs. The validation process consists of running the prompt\"\n+            \" `args.validation_prompt` multiple times: `args.num_validation_images`.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--max_train_samples\",\n+        type=int,\n+        default=None,\n+        help=(\n+            \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n+            \"value if set.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        type=str,\n+        default=\"instruct-pix2pix-model\",\n+        help=\"The output directory where the model predictions and checkpoints will be written.\",\n+    )\n+    parser.add_argument(\n+        \"--cache_dir\",\n+        type=str,\n+        default=None,\n+        help=\"The directory where the downloaded models and datasets will be stored.\",\n+    )\n+    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n+    parser.add_argument(\n+        \"--resolution\",\n+        type=int,\n+        default=256,\n+        help=(\n+            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n+            \" resolution\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--center_crop\",\n+        default=False,\n+        action=\"store_true\",\n+        help=(\n+            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n+            \" cropped. The images will be resized to the resolution first before cropping.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--random_flip\",\n+        action=\"store_true\",\n+        help=\"whether to randomly flip images horizontally\",\n+    )\n+    parser.add_argument(\n+        \"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\"\n+    )\n+    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n+    parser.add_argument(\n+        \"--max_train_steps\",\n+        type=int,\n+        default=None,\n+        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n+    )\n+    parser.add_argument(\n+        \"--gradient_accumulation_steps\",\n+        type=int,\n+        default=1,\n+        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n+    )\n+    parser.add_argument(\n+        \"--gradient_checkpointing\",\n+        action=\"store_true\",\n+        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n+    )\n+    parser.add_argument(\n+        \"--learning_rate\",\n+        type=float,\n+        default=1e-4,\n+        help=\"Initial learning rate (after the potential warmup period) to use.\",\n+    )\n+    parser.add_argument(\n+        \"--scale_lr\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n+    )\n+    parser.add_argument(\n+        \"--lr_scheduler\",\n+        type=str,\n+        default=\"constant\",\n+        help=(\n+            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n+            ' \"constant\", \"constant_with_warmup\"]'\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n+    )\n+    parser.add_argument(\n+        \"--conditioning_dropout_prob\",\n+        type=float,\n+        default=None,\n+        help=\"Conditioning dropout probability. Drops out the conditionings (image and edit prompt) used in training InstructPix2Pix. See section 3.2.1 in the paper: https://arxiv.org/abs/2211.09800.\",\n+    )\n+    parser.add_argument(\n+        \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n+    )\n+    parser.add_argument(\n+        \"--allow_tf32\",\n+        action=\"store_true\",\n+        help=(\n+            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n+            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n+        ),\n+    )\n+    parser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use EMA model.\")\n+    parser.add_argument(\n+        \"--non_ema_revision\",\n+        type=str,\n+        default=None,\n+        required=False,\n+        help=(\n+            \"Revision of pretrained non-ema model identifier. Must be a branch, tag or git identifier of the local or\"\n+            \" remote repository specified with --pretrained_model_name_or_path.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--dataloader_num_workers\",\n+        type=int,\n+        default=0,\n+        help=(\n+            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n+        ),\n+    )\n+    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n+    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n+    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n+    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n+    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n+    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n+    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n+    parser.add_argument(\n+        \"--hub_model_id\",\n+        type=str,\n+        default=None,\n+        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n+    )\n+    parser.add_argument(\n+        \"--logging_dir\",\n+        type=str,\n+        default=\"logs\",\n+        help=(\n+            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n+            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--mixed_precision\",\n+        type=str,\n+        default=None,\n+        choices=[\"no\", \"fp16\", \"bf16\"],\n+        help=(\n+            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n+            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n+            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--report_to\",\n+        type=str,\n+        default=\"tensorboard\",\n+        help=(\n+            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n+            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n+        ),\n+    )\n+    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n+    parser.add_argument(\n+        \"--checkpointing_steps\",\n+        type=int,\n+        default=500,\n+        help=(\n+            \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\"\n+            \" training using `--resume_from_checkpoint`.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--checkpoints_total_limit\",\n+        type=int,\n+        default=None,\n+        help=(\"Max number of checkpoints to store.\"),\n+    )\n+    parser.add_argument(\n+        \"--resume_from_checkpoint\",\n+        type=str,\n+        default=None,\n+        help=(\n+            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n+            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\"\n+    )\n+    parser.add_argument(\n+        \"--rank\",\n+        type=int,\n+        default=4,\n+        help=(\"The dimension of the LoRA update matrices.\"),\n+    )\n+\n+    args = parser.parse_args()\n+    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n+    if env_local_rank != -1 and env_local_rank != args.local_rank:\n+        args.local_rank = env_local_rank\n+\n+    # Sanity checks\n+    if args.dataset_name is None and args.train_data_dir is None:\n+        raise ValueError(\"Need either a dataset name or a training folder.\")\n+\n+    # default to using the same revision for the non-ema model if not specified\n+    if args.non_ema_revision is None:\n+        args.non_ema_revision = args.revision\n+\n+    return args\n+\n+\n+def convert_to_np(image, resolution):\n+    image = image.convert(\"RGB\").resize((resolution, resolution))\n+    return np.array(image).transpose(2, 0, 1)\n+\n+\n+def download_image(url):\n+    image = PIL.Image.open(requests.get(url, stream=True).raw)\n+    image = PIL.ImageOps.exif_transpose(image)\n+    image = image.convert(\"RGB\")\n+    return image\n+\n+\n+def main():\n+    args = parse_args()\n+\n+    if args.non_ema_revision is not None:\n+        deprecate(\n+            \"non_ema_revision!=None\",\n+            \"0.15.0\",\n+            message=(\n+                \"Downloading 'non_ema' weights from revision branches of the Hub is deprecated. Please make sure to\"\n+                \" use `--variant=non_ema` instead.\"\n+            ),\n+        )\n+    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n+    accelerator = Accelerator(\n+        gradient_accumulation_steps=args.gradient_accumulation_steps,\n+        mixed_precision=args.mixed_precision,\n+        log_with=args.report_to,\n+        project_config=accelerator_project_config,\n+    )\n+\n+    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n+\n+    if args.report_to == \"wandb\":\n+        if not is_wandb_available():\n+            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n+        import wandb\n+\n+    # Make one log on every process with the configuration for debugging.\n+    logging.basicConfig(\n+        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n+        datefmt=\"%m/%d/%Y %H:%M:%S\",\n+        level=logging.INFO,\n+    )\n+    logger.info(accelerator.state, main_process_only=False)\n+    if accelerator.is_local_main_process:\n+        datasets.utils.logging.set_verbosity_warning()\n+        transformers.utils.logging.set_verbosity_warning()\n+        diffusers.utils.logging.set_verbosity_info()\n+    else:\n+        datasets.utils.logging.set_verbosity_error()\n+        transformers.utils.logging.set_verbosity_error()\n+        diffusers.utils.logging.set_verbosity_error()\n+\n+    # If passed along, set the training seed now.\n+    if args.seed is not None:\n+        set_seed(args.seed)\n+\n+    # Handle the repository creation\n+    if accelerator.is_main_process:\n+        if args.output_dir is not None:\n+            os.makedirs(args.output_dir, exist_ok=True)\n+\n+        if args.push_to_hub:\n+            repo_id = create_repo(\n+                repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n+            ).repo_id\n+\n+    # Load scheduler, tokenizer and models.\n+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n+    tokenizer = CLIPTokenizer.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n+    )\n+    text_encoder = CLIPTextModel.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision, variant=args.variant\n+    )\n+    vae = AutoencoderKL.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision, variant=args.variant\n+    )\n+    unet = UNet2DConditionModel.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.non_ema_revision\n+    )\n+\n+    # Freeze vae, text_encoder and unet\n+    vae.requires_grad_(False)\n+    text_encoder.requires_grad_(False)\n+    unet.requires_grad_(False)\n+\n+    # referred to https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py\n+    unet_lora_parameters = []\n+    for attn_processor_name, attn_processor in unet.attn_processors.items():\n+        # Parse the attention module.\n+        attn_module = unet\n+        for n in attn_processor_name.split(\".\")[:-1]:\n+            attn_module = getattr(attn_module, n)\n+\n+        # Set the `lora_layer` attribute of the attention-related matrices.\n+        attn_module.to_q.set_lora_layer(\n+            LoRALinearLayer(\n+                in_features=attn_module.to_q.in_features, out_features=attn_module.to_q.out_features, rank=args.rank\n+            )\n+        )\n+        attn_module.to_k.set_lora_layer(\n+            LoRALinearLayer(\n+                in_features=attn_module.to_k.in_features, out_features=attn_module.to_k.out_features, rank=args.rank\n+            )\n+        )\n+\n+        attn_module.to_v.set_lora_layer(\n+            LoRALinearLayer(\n+                in_features=attn_module.to_v.in_features, out_features=attn_module.to_v.out_features, rank=args.rank\n+            )\n+        )\n+        attn_module.to_out[0].set_lora_layer(\n+            LoRALinearLayer(\n+                in_features=attn_module.to_out[0].in_features,\n+                out_features=attn_module.to_out[0].out_features,\n+                rank=args.rank,\n+            )\n+        )\n+\n+        # Accumulate the LoRA params to optimize.\n+        unet_lora_parameters.extend(attn_module.to_q.lora_layer.parameters())\n+        unet_lora_parameters.extend(attn_module.to_k.lora_layer.parameters())\n+        unet_lora_parameters.extend(attn_module.to_v.lora_layer.parameters())\n+        unet_lora_parameters.extend(attn_module.to_out[0].lora_layer.parameters())\n+\n+    # Create EMA for the unet.\n+    if args.use_ema:\n+        ema_unet = EMAModel(unet.parameters(), model_cls=UNet2DConditionModel, model_config=unet.config)\n+\n+    if args.enable_xformers_memory_efficient_attention:\n+        if is_xformers_available():\n+            import xformers\n+\n+            xformers_version = version.parse(xformers.__version__)\n+            if xformers_version == version.parse(\"0.0.16\"):\n+                logger.warn(\n+                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n+                )\n+            unet.enable_xformers_memory_efficient_attention()\n+        else:\n+            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n+\n+    # `accelerate` 0.16.0 will have better support for customized saving\n+    if version.parse(accelerate.__version__) >= version.parse(\"0.16.0\"):\n+        # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n+        def save_model_hook(models, weights, output_dir):\n+            if accelerator.is_main_process:\n+                if args.use_ema:\n+                    ema_unet.save_pretrained(os.path.join(output_dir, \"unet_ema\"))\n+\n+                for i, model in enumerate(models):\n+                    model.save_pretrained(os.path.join(output_dir, \"unet\"))\n+\n+                    # make sure to pop weight so that corresponding model is not saved again\n+                    weights.pop()\n+\n+        def load_model_hook(models, input_dir):\n+            if args.use_ema:\n+                load_model = EMAModel.from_pretrained(os.path.join(input_dir, \"unet_ema\"), UNet2DConditionModel)\n+                ema_unet.load_state_dict(load_model.state_dict())\n+                ema_unet.to(accelerator.device)\n+                del load_model\n+\n+            for i in range(len(models)):\n+                # pop models so that they are not loaded again\n+                model = models.pop()\n+\n+                # load diffusers style into model\n+                load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n+                model.register_to_config(**load_model.config)\n+\n+                model.load_state_dict(load_model.state_dict())\n+                del load_model\n+\n+        accelerator.register_save_state_pre_hook(save_model_hook)\n+        accelerator.register_load_state_pre_hook(load_model_hook)\n+\n+    if args.gradient_checkpointing:\n+        unet.enable_gradient_checkpointing()\n+\n+    # Enable TF32 for faster training on Ampere GPUs,\n+    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n+    if args.allow_tf32:\n+        torch.backends.cuda.matmul.allow_tf32 = True\n+\n+    if args.scale_lr:\n+        args.learning_rate = (\n+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n+        )\n+\n+    # Initialize the optimizer\n+    if args.use_8bit_adam:\n+        try:\n+            import bitsandbytes as bnb\n+        except ImportError:\n+            raise ImportError(\n+                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n+            )\n+\n+        optimizer_cls = bnb.optim.AdamW8bit\n+    else:\n+        optimizer_cls = torch.optim.AdamW\n+\n+    # train on only unet_lora_parameters\n+    optimizer = optimizer_cls(\n+        unet_lora_parameters,\n+        lr=args.learning_rate,\n+        betas=(args.adam_beta1, args.adam_beta2),\n+        weight_decay=args.adam_weight_decay,\n+        eps=args.adam_epsilon,\n+    )\n+\n+    # Get the datasets: you can either provide your own training and evaluation files (see below)\n+    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n+\n+    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n+    # download the dataset.\n+    if args.dataset_name is not None:\n+        # Downloading and loading a dataset from the hub.\n+        dataset = load_dataset(\n+            args.dataset_name,\n+            args.dataset_config_name,\n+            cache_dir=args.cache_dir,\n+        )\n+    else:\n+        data_files = {}\n+        if args.train_data_dir is not None:\n+            data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n+        dataset = load_dataset(\n+            \"imagefolder\",\n+            data_files=data_files,\n+            cache_dir=args.cache_dir,\n+        )\n+        # See more about loading custom images at\n+        # https://huggingface.co/docs/datasets/main/en/image_load#imagefolder\n+\n+    # Preprocessing the datasets.\n+    # We need to tokenize inputs and targets.\n+    column_names = dataset[\"train\"].column_names\n+\n+    # 6. Get the column names for input/target.\n+    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\n+    if args.original_image_column is None:\n+        original_image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n+    else:\n+        original_image_column = args.original_image_column\n+        if original_image_column not in column_names:\n+            raise ValueError(\n+                f\"--original_image_column' value '{args.original_image_column}' needs to be one of: {', '.join(column_names)}\"\n+            )\n+    if args.edit_prompt_column is None:\n+        edit_prompt_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n+    else:\n+        edit_prompt_column = args.edit_prompt_column\n+        if edit_prompt_column not in column_names:\n+            raise ValueError(\n+                f\"--edit_prompt_column' value '{args.edit_prompt_column}' needs to be one of: {', '.join(column_names)}\"\n+            )\n+    if args.edited_image_column is None:\n+        edited_image_column = dataset_columns[2] if dataset_columns is not None else column_names[2]\n+    else:\n+        edited_image_column = args.edited_image_column\n+        if edited_image_column not in column_names:\n+            raise ValueError(\n+                f\"--edited_image_column' value '{args.edited_image_column}' needs to be one of: {', '.join(column_names)}\"\n+            )\n+\n+    # Preprocessing the datasets.\n+    # We need to tokenize input captions and transform the images.\n+    def tokenize_captions(captions):\n+        inputs = tokenizer(\n+            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n+        )\n+        return inputs.input_ids\n+\n+    # Preprocessing the datasets.\n+    train_transforms = transforms.Compose(\n+        [\n+            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n+            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n+        ]\n+    )\n+\n+    def preprocess_images(examples):\n+        original_images = np.concatenate(\n+            [convert_to_np(image, args.resolution) for image in examples[original_image_column]]\n+        )\n+        edited_images = np.concatenate(\n+            [convert_to_np(image, args.resolution) for image in examples[edited_image_column]]\n+        )\n+        # We need to ensure that the original and the edited images undergo the same\n+        # augmentation transforms.\n+        images = np.concatenate([original_images, edited_images])\n+        images = torch.tensor(images)\n+        images = 2 * (images / 255) - 1\n+        return train_transforms(images)\n+\n+    def preprocess_train(examples):\n+        # Preprocess images.\n+        preprocessed_images = preprocess_images(examples)\n+        # Since the original and edited images were concatenated before\n+        # applying the transformations, we need to separate them and reshape\n+        # them accordingly.\n+        original_images, edited_images = preprocessed_images.chunk(2)\n+        original_images = original_images.reshape(-1, 3, args.resolution, args.resolution)\n+        edited_images = edited_images.reshape(-1, 3, args.resolution, args.resolution)\n+\n+        # Collate the preprocessed images into the `examples`.\n+        examples[\"original_pixel_values\"] = original_images\n+        examples[\"edited_pixel_values\"] = edited_images\n+\n+        # Preprocess the captions.\n+        captions = list(examples[edit_prompt_column])\n+        examples[\"input_ids\"] = tokenize_captions(captions)\n+        return examples\n+\n+    with accelerator.main_process_first():\n+        if args.max_train_samples is not None:\n+            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n+        # Set the training transforms\n+        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n+\n+    def collate_fn(examples):\n+        original_pixel_values = torch.stack([example[\"original_pixel_values\"] for example in examples])\n+        original_pixel_values = original_pixel_values.to(memory_format=torch.contiguous_format).float()\n+        edited_pixel_values = torch.stack([example[\"edited_pixel_values\"] for example in examples])\n+        edited_pixel_values = edited_pixel_values.to(memory_format=torch.contiguous_format).float()\n+        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n+        return {\n+            \"original_pixel_values\": original_pixel_values,\n+            \"edited_pixel_values\": edited_pixel_values,\n+            \"input_ids\": input_ids,\n+        }\n+\n+    # DataLoaders creation:\n+    train_dataloader = torch.utils.data.DataLoader(\n+        train_dataset,\n+        shuffle=True,\n+        collate_fn=collate_fn,\n+        batch_size=args.train_batch_size,\n+        num_workers=args.dataloader_num_workers,\n+    )\n+\n+    # Scheduler and math around the number of training steps.\n+    overrode_max_train_steps = False\n+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n+    if args.max_train_steps is None:\n+        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n+        overrode_max_train_steps = True\n+\n+    lr_scheduler = get_scheduler(\n+        args.lr_scheduler,\n+        optimizer=optimizer,\n+        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n+        num_training_steps=args.max_train_steps * accelerator.num_processes,\n+    )\n+\n+    # Prepare everything with our `accelerator`.\n+    unet, unet_lora_parameters, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n+        unet, unet_lora_parameters, optimizer, train_dataloader, lr_scheduler\n+    )\n+\n+    if args.use_ema:\n+        ema_unet.to(accelerator.device)\n+\n+    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n+    # as these models are only used for inference, keeping weights in full precision is not required.\n+    weight_dtype = torch.float32\n+    if accelerator.mixed_precision == \"fp16\":\n+        weight_dtype = torch.float16\n+    elif accelerator.mixed_precision == \"bf16\":\n+        weight_dtype = torch.bfloat16\n+\n+    # Move text_encode and vae to gpu and cast to weight_dtype\n+    text_encoder.to(accelerator.device, dtype=weight_dtype)\n+    vae.to(accelerator.device, dtype=weight_dtype)\n+\n+    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n+    if overrode_max_train_steps:\n+        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n+    # Afterwards we recalculate our number of training epochs\n+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n+\n+    # We need to initialize the trackers we use, and also store our configuration.\n+    # The trackers initializes automatically on the main process.\n+    if accelerator.is_main_process:\n+        accelerator.init_trackers(\"instruct-pix2pix\", config=vars(args))\n+\n+    # Train!\n+    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n+\n+    logger.info(\"***** Running training *****\")\n+    logger.info(f\"  Num examples = {len(train_dataset)}\")\n+    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n+    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n+    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n+    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n+    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n+    global_step = 0\n+    first_epoch = 0\n+\n+    # Potentially load in the weights and states from a previous save\n+    if args.resume_from_checkpoint:\n+        if args.resume_from_checkpoint != \"latest\":\n+            path = os.path.basename(args.resume_from_checkpoint)\n+        else:\n+            # Get the most recent checkpoint\n+            dirs = os.listdir(args.output_dir)\n+            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n+            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n+            path = dirs[-1] if len(dirs) > 0 else None\n+\n+        if path is None:\n+            accelerator.print(\n+                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n+            )\n+            args.resume_from_checkpoint = None\n+        else:\n+            accelerator.print(f\"Resuming from checkpoint {path}\")\n+            accelerator.load_state(os.path.join(args.output_dir, path))\n+            global_step = int(path.split(\"-\")[1])\n+\n+            resume_global_step = global_step * args.gradient_accumulation_steps\n+            first_epoch = global_step // num_update_steps_per_epoch\n+            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n+\n+    # Only show the progress bar once on each machine.\n+    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\n+    progress_bar.set_description(\"Steps\")\n+\n+    for epoch in range(first_epoch, args.num_train_epochs):\n+        unet.train()\n+        train_loss = 0.0\n+        for step, batch in enumerate(train_dataloader):\n+            # Skip steps until we reach the resumed step\n+            if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n+                if step % args.gradient_accumulation_steps == 0:\n+                    progress_bar.update(1)\n+                continue\n+\n+            with accelerator.accumulate(unet):\n+                # We want to learn the denoising process w.r.t the edited images which\n+                # are conditioned on the original image (which was edited) and the edit instruction.\n+                # So, first, convert images to latent space.\n+                latents = vae.encode(batch[\"edited_pixel_values\"].to(weight_dtype)).latent_dist.sample()\n+                latents = latents * vae.config.scaling_factor\n+\n+                # Sample noise that we'll add to the latents\n+                noise = torch.randn_like(latents)\n+                bsz = latents.shape[0]\n+                # Sample a random timestep for each image\n+                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n+                timesteps = timesteps.long()\n+\n+                # Add noise to the latents according to the noise magnitude at each timestep\n+                # (this is the forward diffusion process)\n+                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n+\n+                # Get the text embedding for conditioning.\n+                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n+\n+                # Get the additional image embedding for conditioning.\n+                # Instead of getting a diagonal Gaussian here, we simply take the mode.\n+                original_image_embeds = vae.encode(batch[\"original_pixel_values\"].to(weight_dtype)).latent_dist.mode()\n+\n+                # Conditioning dropout to support classifier-free guidance during inference. For more details\n+                # check out the section 3.2.1 of the original paper https://arxiv.org/abs/2211.09800.\n+                if args.conditioning_dropout_prob is not None:\n+                    random_p = torch.rand(bsz, device=latents.device, generator=generator)\n+                    # Sample masks for the edit prompts.\n+                    prompt_mask = random_p < 2 * args.conditioning_dropout_prob\n+                    prompt_mask = prompt_mask.reshape(bsz, 1, 1)\n+                    # Final text conditioning.\n+                    null_conditioning = text_encoder(tokenize_captions([\"\"]).to(accelerator.device))[0]\n+                    encoder_hidden_states = torch.where(prompt_mask, null_conditioning, encoder_hidden_states)\n+\n+                    # Sample masks for the original images.\n+                    image_mask_dtype = original_image_embeds.dtype\n+                    image_mask = 1 - (\n+                        (random_p >= args.conditioning_dropout_prob).to(image_mask_dtype)\n+                        * (random_p < 3 * args.conditioning_dropout_prob).to(image_mask_dtype)\n+                    )\n+                    image_mask = image_mask.reshape(bsz, 1, 1, 1)\n+                    # Final image conditioning.\n+                    original_image_embeds = image_mask * original_image_embeds\n+\n+                # Concatenate the `original_image_embeds` with the `noisy_latents`.\n+                concatenated_noisy_latents = torch.cat([noisy_latents, original_image_embeds], dim=1)\n+\n+                # Get the target for loss depending on the prediction type\n+                if noise_scheduler.config.prediction_type == \"epsilon\":\n+                    target = noise\n+                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n+                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n+                else:\n+                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n+\n+                # Predict the noise residual and compute loss\n+                model_pred = unet(concatenated_noisy_latents, timesteps, encoder_hidden_states).sample\n+                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n+\n+                # Gather the losses across all processes for logging (if we use distributed training).\n+                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n+                train_loss += avg_loss.item() / args.gradient_accumulation_steps\n+\n+                # Backpropagate\n+                accelerator.backward(loss)\n+                if accelerator.sync_gradients:\n+                    accelerator.clip_grad_norm_(unet_lora_parameters, args.max_grad_norm)\n+                optimizer.step()\n+                lr_scheduler.step()\n+                optimizer.zero_grad()\n+\n+            # Checks if the accelerator has performed an optimization step behind the scenes\n+            if accelerator.sync_gradients:\n+                if args.use_ema:\n+                    ema_unet.step(unet_lora_parameters)\n+                progress_bar.update(1)\n+                global_step += 1\n+                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n+                train_loss = 0.0\n+\n+                if global_step % args.checkpointing_steps == 0:\n+                    if accelerator.is_main_process:\n+                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n+                        if args.checkpoints_total_limit is not None:\n+                            checkpoints = os.listdir(args.output_dir)\n+                            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n+\n+                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n+                            if len(checkpoints) >= args.checkpoints_total_limit:\n+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n+                                removing_checkpoints = checkpoints[0:num_to_remove]\n+\n+                                logger.info(\n+                                    f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n+                                )\n+                                logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n+\n+                                for removing_checkpoint in removing_checkpoints:\n+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n+                                    shutil.rmtree(removing_checkpoint)\n+\n+                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n+                        accelerator.save_state(save_path)\n+                        logger.info(f\"Saved state to {save_path}\")\n+\n+            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n+            progress_bar.set_postfix(**logs)\n+\n+            if global_step >= args.max_train_steps:\n+                break\n+\n+        if accelerator.is_main_process:\n+            if (\n+                (args.val_image_url is not None)\n+                and (args.validation_prompt is not None)\n+                and (epoch % args.validation_epochs == 0)\n+            ):\n+                logger.info(\n+                    f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n+                    f\" {args.validation_prompt}.\"\n+                )\n+                # create pipeline\n+                if args.use_ema:\n+                    # Store the UNet parameters temporarily and load the EMA parameters to perform inference.\n+                    ema_unet.store(unet.parameters())\n+                    ema_unet.copy_to(unet.parameters())\n+                # The models need unwrapping because for compatibility in distributed training mode.\n+                pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n+                    args.pretrained_model_name_or_path,\n+                    unet=accelerator.unwrap_model(unet),\n+                    text_encoder=accelerator.unwrap_model(text_encoder),\n+                    vae=accelerator.unwrap_model(vae),\n+                    revision=args.revision,\n+                    variant=args.variant,\n+                    torch_dtype=weight_dtype,\n+                )\n+                pipeline = pipeline.to(accelerator.device)\n+                pipeline.set_progress_bar_config(disable=True)\n+\n+                # run inference\n+                original_image = download_image(args.val_image_url)\n+                edited_images = []\n+                with torch.autocast(\n+                    str(accelerator.device).replace(\":0\", \"\"), enabled=accelerator.mixed_precision == \"fp16\"\n+                ):\n+                    for _ in range(args.num_validation_images):\n+                        edited_images.append(\n+                            pipeline(\n+                                args.validation_prompt,\n+                                image=original_image,\n+                                num_inference_steps=20,\n+                                image_guidance_scale=1.5,\n+                                guidance_scale=7,\n+                                generator=generator,\n+                            ).images[0]\n+                        )\n+\n+                for tracker in accelerator.trackers:\n+                    if tracker.name == \"wandb\":\n+                        wandb_table = wandb.Table(columns=WANDB_TABLE_COL_NAMES)\n+                        for edited_image in edited_images:\n+                            wandb_table.add_data(\n+                                wandb.Image(original_image), wandb.Image(edited_image), args.validation_prompt\n+                            )\n+                        tracker.log({\"validation\": wandb_table})\n+                if args.use_ema:\n+                    # Switch back to the original UNet parameters.\n+                    ema_unet.restore(unet.parameters())\n+\n+                del pipeline\n+                torch.cuda.empty_cache()\n+\n+    # Create the pipeline using the trained modules and save it.\n+    accelerator.wait_for_everyone()\n+    if accelerator.is_main_process:\n+        unet = accelerator.unwrap_model(unet)\n+        if args.use_ema:\n+            ema_unet.copy_to(unet.parameters())\n+\n+        pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n+            args.pretrained_model_name_or_path,\n+            text_encoder=accelerator.unwrap_model(text_encoder),\n+            vae=accelerator.unwrap_model(vae),\n+            unet=unet,\n+            revision=args.revision,\n+            variant=args.variant,\n+        )\n+        # store only LORA layers\n+        unet.save_attn_procs(args.output_dir)\n+\n+        if args.push_to_hub:\n+            upload_folder(\n+                repo_id=repo_id,\n+                folder_path=args.output_dir,\n+                commit_message=\"End of training\",\n+                ignore_patterns=[\"step_*\", \"epoch_*\"],\n+            )\n+\n+        if args.validation_prompt is not None:\n+            edited_images = []\n+            pipeline = pipeline.to(accelerator.device)\n+            with torch.autocast(str(accelerator.device).replace(\":0\", \"\")):\n+                for _ in range(args.num_validation_images):\n+                    edited_images.append(\n+                        pipeline(\n+                            args.validation_prompt,\n+                            image=original_image,\n+                            num_inference_steps=20,\n+                            image_guidance_scale=1.5,\n+                            guidance_scale=7,\n+                            generator=generator,\n+                        ).images[0]\n+                    )\n+\n+            for tracker in accelerator.trackers:\n+                if tracker.name == \"wandb\":\n+                    wandb_table = wandb.Table(columns=WANDB_TABLE_COL_NAMES)\n+                    for edited_image in edited_images:\n+                        wandb_table.add_data(\n+                            wandb.Image(original_image), wandb.Image(edited_image), args.validation_prompt\n+                        )\n+                    tracker.log({\"test\": wandb_table})\n+\n+    accelerator.end_training()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "examples/textual_inversion/test_textual_inversion_sdxl.py",
            "diff": "diff --git a/examples/textual_inversion/test_textual_inversion_sdxl.py b/examples/textual_inversion/test_textual_inversion_sdxl.py\nnew file mode 100644\nindex 00000000..5c8f949f\n--- /dev/null\n+++ b/examples/textual_inversion/test_textual_inversion_sdxl.py\n@@ -0,0 +1,152 @@\n+# coding=utf-8\n+# Copyright 2023 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import logging\n+import os\n+import sys\n+import tempfile\n+\n+\n+sys.path.append(\"..\")\n+from test_examples_utils import ExamplesTestsAccelerate, run_command  # noqa: E402\n+\n+\n+logging.basicConfig(level=logging.DEBUG)\n+\n+logger = logging.getLogger()\n+stream_handler = logging.StreamHandler(sys.stdout)\n+logger.addHandler(stream_handler)\n+\n+\n+class TextualInversionSdxl(ExamplesTestsAccelerate):\n+    def test_textual_inversion_sdxl(self):\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            test_args = f\"\"\"\n+                examples/textual_inversion/textual_inversion_sdxl.py\n+                --pretrained_model_name_or_path hf-internal-testing/tiny-sdxl-pipe\n+                --train_data_dir docs/source/en/imgs\n+                --learnable_property object\n+                --placeholder_token <cat-toy>\n+                --initializer_token a\n+                --save_steps 1\n+                --num_vectors 2\n+                --resolution 64\n+                --train_batch_size 1\n+                --gradient_accumulation_steps 1\n+                --max_train_steps 2\n+                --learning_rate 5.0e-04\n+                --scale_lr\n+                --lr_scheduler constant\n+                --lr_warmup_steps 0\n+                --output_dir {tmpdir}\n+                \"\"\".split()\n+\n+            run_command(self._launch_args + test_args)\n+            # save_pretrained smoke test\n+            self.assertTrue(os.path.isfile(os.path.join(tmpdir, \"learned_embeds.safetensors\")))\n+\n+    def test_textual_inversion_sdxl_checkpointing(self):\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            test_args = f\"\"\"\n+                examples/textual_inversion/textual_inversion_sdxl.py\n+                --pretrained_model_name_or_path hf-internal-testing/tiny-sdxl-pipe\n+                --train_data_dir docs/source/en/imgs\n+                --learnable_property object\n+                --placeholder_token <cat-toy>\n+                --initializer_token a\n+                --save_steps 1\n+                --num_vectors 2\n+                --resolution 64\n+                --train_batch_size 1\n+                --gradient_accumulation_steps 1\n+                --max_train_steps 3\n+                --learning_rate 5.0e-04\n+                --scale_lr\n+                --lr_scheduler constant\n+                --lr_warmup_steps 0\n+                --output_dir {tmpdir}\n+                --checkpointing_steps=1\n+                --checkpoints_total_limit=2\n+                \"\"\".split()\n+\n+            run_command(self._launch_args + test_args)\n+\n+            # check checkpoint directories exist\n+            self.assertEqual(\n+                {x for x in os.listdir(tmpdir) if \"checkpoint\" in x},\n+                {\"checkpoint-2\", \"checkpoint-3\"},\n+            )\n+\n+    def test_textual_inversion_sdxl_checkpointing_checkpoints_total_limit_removes_multiple_checkpoints(self):\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            test_args = f\"\"\"\n+                examples/textual_inversion/textual_inversion_sdxl.py\n+                --pretrained_model_name_or_path hf-internal-testing/tiny-sdxl-pipe\n+                --train_data_dir docs/source/en/imgs\n+                --learnable_property object\n+                --placeholder_token <cat-toy>\n+                --initializer_token a\n+                --save_steps 1\n+                --num_vectors 2\n+                --resolution 64\n+                --train_batch_size 1\n+                --gradient_accumulation_steps 1\n+                --max_train_steps 2\n+                --learning_rate 5.0e-04\n+                --scale_lr\n+                --lr_scheduler constant\n+                --lr_warmup_steps 0\n+                --output_dir {tmpdir}\n+                --checkpointing_steps=1\n+                \"\"\".split()\n+\n+            run_command(self._launch_args + test_args)\n+\n+            # check checkpoint directories exist\n+            self.assertEqual(\n+                {x for x in os.listdir(tmpdir) if \"checkpoint\" in x},\n+                {\"checkpoint-1\", \"checkpoint-2\"},\n+            )\n+\n+            resume_run_args = f\"\"\"\n+                examples/textual_inversion/textual_inversion_sdxl.py\n+                --pretrained_model_name_or_path hf-internal-testing/tiny-sdxl-pipe\n+                --train_data_dir docs/source/en/imgs\n+                --learnable_property object\n+                --placeholder_token <cat-toy>\n+                --initializer_token a\n+                --save_steps 1\n+                --num_vectors 2\n+                --resolution 64\n+                --train_batch_size 1\n+                --gradient_accumulation_steps 1\n+                --max_train_steps 2\n+                --learning_rate 5.0e-04\n+                --scale_lr\n+                --lr_scheduler constant\n+                --lr_warmup_steps 0\n+                --output_dir {tmpdir}\n+                --checkpointing_steps=1\n+                --resume_from_checkpoint=checkpoint-2\n+                --checkpoints_total_limit=2\n+                \"\"\".split()\n+\n+            run_command(self._launch_args + resume_run_args)\n+\n+            # check checkpoint directories exist\n+            self.assertEqual(\n+                {x for x in os.listdir(tmpdir) if \"checkpoint\" in x},\n+                {\"checkpoint-2\", \"checkpoint-3\"},\n+            )\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "examples/textual_inversion/textual_inversion_sdxl.py",
            "diff": "diff --git a/examples/textual_inversion/textual_inversion_sdxl.py b/examples/textual_inversion/textual_inversion_sdxl.py\nnew file mode 100644\nindex 00000000..6eba281e\n--- /dev/null\n+++ b/examples/textual_inversion/textual_inversion_sdxl.py\n@@ -0,0 +1,1058 @@\n+#!/usr/bin/env python\n+# coding=utf-8\n+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+\n+import argparse\n+import logging\n+import math\n+import os\n+import random\n+import shutil\n+from pathlib import Path\n+\n+import numpy as np\n+import PIL\n+import safetensors\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+import transformers\n+from accelerate import Accelerator\n+from accelerate.logging import get_logger\n+from accelerate.utils import ProjectConfiguration, set_seed\n+from huggingface_hub import create_repo, upload_folder\n+\n+# TODO: remove and import from diffusers.utils when the new version of diffusers is released\n+from packaging import version\n+from PIL import Image\n+from torch.utils.data import Dataset\n+from torchvision import transforms\n+from tqdm.auto import tqdm\n+from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer\n+\n+import diffusers\n+from diffusers import (\n+    AutoencoderKL,\n+    DDPMScheduler,\n+    DiffusionPipeline,\n+    DPMSolverMultistepScheduler,\n+    UNet2DConditionModel,\n+)\n+from diffusers.optimization import get_scheduler\n+from diffusers.utils import check_min_version, is_wandb_available\n+from diffusers.utils.import_utils import is_xformers_available\n+\n+\n+if is_wandb_available():\n+    import wandb\n+\n+if version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n+    PIL_INTERPOLATION = {\n+        \"linear\": PIL.Image.Resampling.BILINEAR,\n+        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n+        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n+        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n+        \"nearest\": PIL.Image.Resampling.NEAREST,\n+    }\n+else:\n+    PIL_INTERPOLATION = {\n+        \"linear\": PIL.Image.LINEAR,\n+        \"bilinear\": PIL.Image.BILINEAR,\n+        \"bicubic\": PIL.Image.BICUBIC,\n+        \"lanczos\": PIL.Image.LANCZOS,\n+        \"nearest\": PIL.Image.NEAREST,\n+    }\n+# ------------------------------------------------------------------------------\n+\n+\n+# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n+check_min_version(\"0.25.0.dev0\")\n+\n+logger = get_logger(__name__)\n+\n+\n+def save_model_card(repo_id: str, images=None, base_model=str, repo_folder=None):\n+    img_str = \"\"\n+    for i, image in enumerate(images):\n+        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n+        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n+\n+    yaml = f\"\"\"\n+---\n+license: creativeml-openrail-m\n+base_model: {base_model}\n+tags:\n+- stable-diffusion\n+- stable-diffusion-diffusers\n+- text-to-image\n+- diffusers\n+- textual_inversion\n+inference: true\n+---\n+    \"\"\"\n+    model_card = f\"\"\"\n+# Textual inversion text2image fine-tuning - {repo_id}\n+These are textual inversion adaption weights for {base_model}. You can find some example images in the following. \\n\n+{img_str}\n+\"\"\"\n+    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n+        f.write(yaml + model_card)\n+\n+\n+def log_validation(\n+    text_encoder_1,\n+    text_encoder_2,\n+    tokenizer_1,\n+    tokenizer_2,\n+    unet,\n+    vae,\n+    args,\n+    accelerator,\n+    weight_dtype,\n+    epoch,\n+    is_final_validation=False,\n+):\n+    logger.info(\n+        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n+        f\" {args.validation_prompt}.\"\n+    )\n+    pipeline = DiffusionPipeline.from_pretrained(\n+        args.pretrained_model_name_or_path,\n+        text_encoder=accelerator.unwrap_model(text_encoder_1),\n+        text_encoder_2=text_encoder_2,\n+        tokenizer=tokenizer_1,\n+        tokenizer_2=tokenizer_2,\n+        unet=unet,\n+        vae=vae,\n+        safety_checker=None,\n+        revision=args.revision,\n+        variant=args.variant,\n+        torch_dtype=weight_dtype,\n+    )\n+    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n+    pipeline = pipeline.to(accelerator.device)\n+    pipeline.set_progress_bar_config(disable=True)\n+\n+    # run inference\n+    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n+    images = []\n+    for _ in range(args.num_validation_images):\n+        image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\n+        images.append(image)\n+\n+    tracker_key = \"test\" if is_final_validation else \"validation\"\n+    for tracker in accelerator.trackers:\n+        if tracker.name == \"tensorboard\":\n+            np_images = np.stack([np.asarray(img) for img in images])\n+            tracker.writer.add_images(tracker_key, np_images, epoch, dataformats=\"NHWC\")\n+        if tracker.name == \"wandb\":\n+            tracker.log(\n+                {\n+                    tracker_key: [\n+                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images)\n+                    ]\n+                }\n+            )\n+\n+    del pipeline\n+    torch.cuda.empty_cache()\n+    return images\n+\n+\n+def save_progress(text_encoder, placeholder_token_ids, accelerator, args, save_path, safe_serialization=True):\n+    logger.info(\"Saving embeddings\")\n+    learned_embeds = (\n+        accelerator.unwrap_model(text_encoder)\n+        .get_input_embeddings()\n+        .weight[min(placeholder_token_ids) : max(placeholder_token_ids) + 1]\n+    )\n+    learned_embeds_dict = {args.placeholder_token: learned_embeds.detach().cpu()}\n+\n+    if safe_serialization:\n+        safetensors.torch.save_file(learned_embeds_dict, save_path, metadata={\"format\": \"pt\"})\n+    else:\n+        torch.save(learned_embeds_dict, save_path)\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n+    parser.add_argument(\n+        \"--save_steps\",\n+        type=int,\n+        default=500,\n+        help=\"Save learned_embeds.bin every X updates steps.\",\n+    )\n+    parser.add_argument(\n+        \"--save_as_full_pipeline\",\n+        action=\"store_true\",\n+        help=\"Save the complete stable diffusion pipeline.\",\n+    )\n+    parser.add_argument(\n+        \"--num_vectors\",\n+        type=int,\n+        default=1,\n+        help=\"How many textual inversion vectors shall be used to learn the concept.\",\n+    )\n+    parser.add_argument(\n+        \"--pretrained_model_name_or_path\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n+    )\n+    parser.add_argument(\n+        \"--revision\",\n+        type=str,\n+        default=None,\n+        required=False,\n+        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n+    )\n+    parser.add_argument(\n+        \"--variant\",\n+        type=str,\n+        default=None,\n+        help=\"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\",\n+    )\n+    parser.add_argument(\n+        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n+    )\n+    parser.add_argument(\n+        \"--placeholder_token\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=\"A token to use as a placeholder for the concept.\",\n+    )\n+    parser.add_argument(\n+        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n+    )\n+    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", help=\"Choose between 'object' and 'style'\")\n+    parser.add_argument(\"--repeats\", type=int, default=100, help=\"How many times to repeat the training data.\")\n+    parser.add_argument(\n+        \"--output_dir\",\n+        type=str,\n+        default=\"text-inversion-model\",\n+        help=\"The output directory where the model predictions and checkpoints will be written.\",\n+    )\n+    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n+    parser.add_argument(\n+        \"--resolution\",\n+        type=int,\n+        default=512,\n+        help=(\n+            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n+            \" resolution\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--center_crop\", action=\"store_true\", help=\"Whether to center crop images before resizing to resolution.\"\n+    )\n+    parser.add_argument(\n+        \"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\"\n+    )\n+    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n+    parser.add_argument(\n+        \"--max_train_steps\",\n+        type=int,\n+        default=5000,\n+        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n+    )\n+    parser.add_argument(\n+        \"--gradient_accumulation_steps\",\n+        type=int,\n+        default=1,\n+        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n+    )\n+    parser.add_argument(\n+        \"--gradient_checkpointing\",\n+        action=\"store_true\",\n+        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n+    )\n+    parser.add_argument(\n+        \"--learning_rate\",\n+        type=float,\n+        default=1e-4,\n+        help=\"Initial learning rate (after the potential warmup period) to use.\",\n+    )\n+    parser.add_argument(\n+        \"--scale_lr\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n+    )\n+    parser.add_argument(\n+        \"--lr_scheduler\",\n+        type=str,\n+        default=\"constant\",\n+        help=(\n+            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n+            ' \"constant\", \"constant_with_warmup\"]'\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n+    )\n+    parser.add_argument(\n+        \"--lr_num_cycles\",\n+        type=int,\n+        default=1,\n+        help=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\",\n+    )\n+    parser.add_argument(\n+        \"--dataloader_num_workers\",\n+        type=int,\n+        default=0,\n+        help=(\n+            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n+    )\n+    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n+    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n+    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n+    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n+    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n+    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n+    parser.add_argument(\n+        \"--hub_model_id\",\n+        type=str,\n+        default=None,\n+        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n+    )\n+    parser.add_argument(\n+        \"--logging_dir\",\n+        type=str,\n+        default=\"logs\",\n+        help=(\n+            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n+            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--mixed_precision\",\n+        type=str,\n+        default=\"no\",\n+        choices=[\"no\", \"fp16\", \"bf16\"],\n+        help=(\n+            \"Whether to use mixed precision. Choose\"\n+            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n+            \"and an Nvidia Ampere GPU.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--allow_tf32\",\n+        action=\"store_true\",\n+        help=(\n+            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n+            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--report_to\",\n+        type=str,\n+        default=\"tensorboard\",\n+        help=(\n+            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n+            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--validation_prompt\",\n+        type=str,\n+        default=\"A <cat-toy> backpack\",\n+        help=\"A prompt that is used during validation to verify that the model is learning.\",\n+    )\n+    parser.add_argument(\n+        \"--num_validation_images\",\n+        type=int,\n+        default=4,\n+        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n+    )\n+    parser.add_argument(\n+        \"--validation_steps\",\n+        type=int,\n+        default=100,\n+        help=(\n+            \"Run validation every X steps. Validation consists of running the prompt\"\n+            \" `args.validation_prompt` multiple times: `args.num_validation_images`\"\n+            \" and logging the images.\"\n+        ),\n+    )\n+    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n+    parser.add_argument(\n+        \"--checkpointing_steps\",\n+        type=int,\n+        default=500,\n+        help=(\n+            \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\"\n+            \" training using `--resume_from_checkpoint`.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--checkpoints_total_limit\",\n+        type=int,\n+        default=None,\n+        help=(\"Max number of checkpoints to store.\"),\n+    )\n+    parser.add_argument(\n+        \"--resume_from_checkpoint\",\n+        type=str,\n+        default=None,\n+        help=(\n+            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n+            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\"\n+    )\n+\n+    args = parser.parse_args()\n+    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n+    if env_local_rank != -1 and env_local_rank != args.local_rank:\n+        args.local_rank = env_local_rank\n+\n+    if args.train_data_dir is None:\n+        raise ValueError(\"You must specify a train data directory.\")\n+\n+    return args\n+\n+\n+imagenet_templates_small = [\n+    \"a photo of a {}\",\n+    \"a rendering of a {}\",\n+    \"a cropped photo of the {}\",\n+    \"the photo of a {}\",\n+    \"a photo of a clean {}\",\n+    \"a photo of a dirty {}\",\n+    \"a dark photo of the {}\",\n+    \"a photo of my {}\",\n+    \"a photo of the cool {}\",\n+    \"a close-up photo of a {}\",\n+    \"a bright photo of the {}\",\n+    \"a cropped photo of a {}\",\n+    \"a photo of the {}\",\n+    \"a good photo of the {}\",\n+    \"a photo of one {}\",\n+    \"a close-up photo of the {}\",\n+    \"a rendition of the {}\",\n+    \"a photo of the clean {}\",\n+    \"a rendition of a {}\",\n+    \"a photo of a nice {}\",\n+    \"a good photo of a {}\",\n+    \"a photo of the nice {}\",\n+    \"a photo of the small {}\",\n+    \"a photo of the weird {}\",\n+    \"a photo of the large {}\",\n+    \"a photo of a cool {}\",\n+    \"a photo of a small {}\",\n+]\n+\n+imagenet_style_templates_small = [\n+    \"a painting in the style of {}\",\n+    \"a rendering in the style of {}\",\n+    \"a cropped painting in the style of {}\",\n+    \"the painting in the style of {}\",\n+    \"a clean painting in the style of {}\",\n+    \"a dirty painting in the style of {}\",\n+    \"a dark painting in the style of {}\",\n+    \"a picture in the style of {}\",\n+    \"a cool painting in the style of {}\",\n+    \"a close-up painting in the style of {}\",\n+    \"a bright painting in the style of {}\",\n+    \"a cropped painting in the style of {}\",\n+    \"a good painting in the style of {}\",\n+    \"a close-up painting in the style of {}\",\n+    \"a rendition in the style of {}\",\n+    \"a nice painting in the style of {}\",\n+    \"a small painting in the style of {}\",\n+    \"a weird painting in the style of {}\",\n+    \"a large painting in the style of {}\",\n+]\n+\n+\n+class TextualInversionDataset(Dataset):\n+    def __init__(\n+        self,\n+        data_root,\n+        tokenizer_1,\n+        tokenizer_2,\n+        learnable_property=\"object\",  # [object, style]\n+        size=512,\n+        repeats=100,\n+        interpolation=\"bicubic\",\n+        flip_p=0.5,\n+        set=\"train\",\n+        placeholder_token=\"*\",\n+        center_crop=False,\n+    ):\n+        self.data_root = data_root\n+        self.tokenizer_1 = tokenizer_1\n+        self.tokenizer_2 = tokenizer_2\n+        self.learnable_property = learnable_property\n+        self.size = size\n+        self.placeholder_token = placeholder_token\n+        self.center_crop = center_crop\n+        self.flip_p = flip_p\n+\n+        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n+\n+        self.num_images = len(self.image_paths)\n+        self._length = self.num_images\n+\n+        if set == \"train\":\n+            self._length = self.num_images * repeats\n+\n+        self.interpolation = {\n+            \"linear\": PIL_INTERPOLATION[\"linear\"],\n+            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n+            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n+            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n+        }[interpolation]\n+\n+        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n+        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n+        self.crop = transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size)\n+\n+    def __len__(self):\n+        return self._length\n+\n+    def __getitem__(self, i):\n+        example = {}\n+        image = Image.open(self.image_paths[i % self.num_images])\n+\n+        if not image.mode == \"RGB\":\n+            image = image.convert(\"RGB\")\n+\n+        placeholder_string = self.placeholder_token\n+        text = random.choice(self.templates).format(placeholder_string)\n+\n+        example[\"original_size\"] = (image.height, image.width)\n+\n+        if self.center_crop:\n+            y1 = max(0, int(round((image.height - self.size) / 2.0)))\n+            x1 = max(0, int(round((image.width - self.size) / 2.0)))\n+            image = self.crop(image)\n+        else:\n+            y1, x1, h, w = self.crop.get_params(image, (self.size, self.size))\n+            image = transforms.functional.crop(image, y1, x1, h, w)\n+\n+        example[\"crop_top_left\"] = (y1, x1)\n+\n+        example[\"input_ids_1\"] = self.tokenizer_1(\n+            text,\n+            padding=\"max_length\",\n+            truncation=True,\n+            max_length=self.tokenizer_1.model_max_length,\n+            return_tensors=\"pt\",\n+        ).input_ids[0]\n+\n+        example[\"input_ids_2\"] = self.tokenizer_2(\n+            text,\n+            padding=\"max_length\",\n+            truncation=True,\n+            max_length=self.tokenizer_2.model_max_length,\n+            return_tensors=\"pt\",\n+        ).input_ids[0]\n+\n+        # default to score-sde preprocessing\n+        img = np.array(image).astype(np.uint8)\n+\n+        image = Image.fromarray(img)\n+        image = image.resize((self.size, self.size), resample=self.interpolation)\n+\n+        image = self.flip_transform(image)\n+        image = np.array(image).astype(np.uint8)\n+        image = (image / 127.5 - 1.0).astype(np.float32)\n+\n+        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n+        return example\n+\n+\n+def main():\n+    args = parse_args()\n+    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n+    accelerator = Accelerator(\n+        gradient_accumulation_steps=args.gradient_accumulation_steps,\n+        mixed_precision=args.mixed_precision,\n+        log_with=args.report_to,\n+        project_config=accelerator_project_config,\n+    )\n+\n+    if args.report_to == \"wandb\":\n+        if not is_wandb_available():\n+            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n+\n+    # Make one log on every process with the configuration for debugging.\n+    logging.basicConfig(\n+        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n+        datefmt=\"%m/%d/%Y %H:%M:%S\",\n+        level=logging.INFO,\n+    )\n+    logger.info(accelerator.state, main_process_only=False)\n+    if accelerator.is_local_main_process:\n+        transformers.utils.logging.set_verbosity_warning()\n+        diffusers.utils.logging.set_verbosity_info()\n+    else:\n+        transformers.utils.logging.set_verbosity_error()\n+        diffusers.utils.logging.set_verbosity_error()\n+\n+    # If passed along, set the training seed now.\n+    if args.seed is not None:\n+        set_seed(args.seed)\n+\n+    # Handle the repository creation\n+    if accelerator.is_main_process:\n+        if args.output_dir is not None:\n+            os.makedirs(args.output_dir, exist_ok=True)\n+\n+        if args.push_to_hub:\n+            repo_id = create_repo(\n+                repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n+            ).repo_id\n+\n+    # Load tokenizer\n+    tokenizer_1 = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n+    tokenizer_2 = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer_2\")\n+\n+    # Load scheduler and models\n+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n+    text_encoder_1 = CLIPTextModel.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n+    )\n+    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"text_encoder_2\", revision=args.revision\n+    )\n+    vae = AutoencoderKL.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision, variant=args.variant\n+    )\n+    unet = UNet2DConditionModel.from_pretrained(\n+        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision, variant=args.variant\n+    )\n+\n+    # Add the placeholder token in tokenizer_1\n+    placeholder_tokens = [args.placeholder_token]\n+\n+    if args.num_vectors < 1:\n+        raise ValueError(f\"--num_vectors has to be larger or equal to 1, but is {args.num_vectors}\")\n+\n+    # add dummy tokens for multi-vector\n+    additional_tokens = []\n+    for i in range(1, args.num_vectors):\n+        additional_tokens.append(f\"{args.placeholder_token}_{i}\")\n+    placeholder_tokens += additional_tokens\n+\n+    num_added_tokens = tokenizer_1.add_tokens(placeholder_tokens)\n+    if num_added_tokens != args.num_vectors:\n+        raise ValueError(\n+            f\"The tokenizer already contains the token {args.placeholder_token}. Please pass a different\"\n+            \" `placeholder_token` that is not already in the tokenizer.\"\n+        )\n+\n+    # Convert the initializer_token, placeholder_token to ids\n+    token_ids = tokenizer_1.encode(args.initializer_token, add_special_tokens=False)\n+    # Check if initializer_token is a single token or a sequence of tokens\n+    if len(token_ids) > 1:\n+        raise ValueError(\"The initializer token must be a single token.\")\n+\n+    initializer_token_id = token_ids[0]\n+    placeholder_token_ids = tokenizer_1.convert_tokens_to_ids(placeholder_tokens)\n+\n+    # Resize the token embeddings as we are adding new special tokens to the tokenizer\n+    text_encoder_1.resize_token_embeddings(len(tokenizer_1))\n+\n+    # Initialise the newly added placeholder token with the embeddings of the initializer token\n+    token_embeds = text_encoder_1.get_input_embeddings().weight.data\n+    with torch.no_grad():\n+        for token_id in placeholder_token_ids:\n+            token_embeds[token_id] = token_embeds[initializer_token_id].clone()\n+\n+    # Freeze vae and unet\n+    vae.requires_grad_(False)\n+    unet.requires_grad_(False)\n+    text_encoder_2.requires_grad_(False)\n+    # Freeze all parameters except for the token embeddings in text encoder\n+    text_encoder_1.text_model.encoder.requires_grad_(False)\n+    text_encoder_1.text_model.final_layer_norm.requires_grad_(False)\n+    text_encoder_1.text_model.embeddings.position_embedding.requires_grad_(False)\n+\n+    if args.gradient_checkpointing:\n+        text_encoder_1.gradient_checkpointing_enable()\n+\n+    if args.enable_xformers_memory_efficient_attention:\n+        if is_xformers_available():\n+            import xformers\n+\n+            xformers_version = version.parse(xformers.__version__)\n+            if xformers_version == version.parse(\"0.0.16\"):\n+                logger.warn(\n+                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n+                )\n+            unet.enable_xformers_memory_efficient_attention()\n+        else:\n+            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n+\n+    # Enable TF32 for faster training on Ampere GPUs,\n+    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n+    if args.allow_tf32:\n+        torch.backends.cuda.matmul.allow_tf32 = True\n+\n+    if args.scale_lr:\n+        args.learning_rate = (\n+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n+        )\n+\n+    # Initialize the optimizer\n+    if args.use_8bit_adam:\n+        try:\n+            import bitsandbytes as bnb\n+        except ImportError:\n+            raise ImportError(\n+                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n+            )\n+\n+        optimizer_class = bnb.optim.AdamW8bit\n+    else:\n+        optimizer_class = torch.optim.AdamW\n+\n+    optimizer = optimizer_class(\n+        text_encoder_1.get_input_embeddings().parameters(),  # only optimize the embeddings\n+        lr=args.learning_rate,\n+        betas=(args.adam_beta1, args.adam_beta2),\n+        weight_decay=args.adam_weight_decay,\n+        eps=args.adam_epsilon,\n+    )\n+\n+    placeholder_token = \" \".join(tokenizer_1.convert_ids_to_tokens(placeholder_token_ids))\n+    # Dataset and DataLoaders creation:\n+    train_dataset = TextualInversionDataset(\n+        data_root=args.train_data_dir,\n+        tokenizer_1=tokenizer_1,\n+        tokenizer_2=tokenizer_2,\n+        size=args.resolution,\n+        placeholder_token=placeholder_token,\n+        repeats=args.repeats,\n+        learnable_property=args.learnable_property,\n+        center_crop=args.center_crop,\n+        set=\"train\",\n+    )\n+    train_dataloader = torch.utils.data.DataLoader(\n+        train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers\n+    )\n+\n+    # Scheduler and math around the number of training steps.\n+    overrode_max_train_steps = False\n+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n+    if args.max_train_steps is None:\n+        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n+        overrode_max_train_steps = True\n+\n+    lr_scheduler = get_scheduler(\n+        args.lr_scheduler,\n+        optimizer=optimizer,\n+        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n+        num_training_steps=args.max_train_steps * accelerator.num_processes,\n+        num_cycles=args.lr_num_cycles,\n+    )\n+\n+    text_encoder_1.train()\n+    # Prepare everything with our `accelerator`.\n+    text_encoder_1, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n+        text_encoder_1, optimizer, train_dataloader, lr_scheduler\n+    )\n+\n+    # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision\n+    # as these weights are only used for inference, keeping weights in full precision is not required.\n+    weight_dtype = torch.float32\n+    if accelerator.mixed_precision == \"fp16\":\n+        weight_dtype = torch.float16\n+    elif accelerator.mixed_precision == \"bf16\":\n+        weight_dtype = torch.bfloat16\n+\n+    # Move vae and unet and text_encoder_2 to device and cast to weight_dtype\n+    unet.to(accelerator.device, dtype=weight_dtype)\n+    vae.to(accelerator.device, dtype=weight_dtype)\n+    text_encoder_2.to(accelerator.device, dtype=weight_dtype)\n+\n+    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n+    if overrode_max_train_steps:\n+        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n+    # Afterwards we recalculate our number of training epochs\n+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n+\n+    # We need to initialize the trackers we use, and also store our configuration.\n+    # The trackers initializes automatically on the main process.\n+    if accelerator.is_main_process:\n+        accelerator.init_trackers(\"textual_inversion\", config=vars(args))\n+\n+    # Train!\n+    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n+\n+    logger.info(\"***** Running training *****\")\n+    logger.info(f\"  Num examples = {len(train_dataset)}\")\n+    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n+    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n+    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n+    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n+    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n+    global_step = 0\n+    first_epoch = 0\n+    # Potentially load in the weights and states from a previous save\n+    if args.resume_from_checkpoint:\n+        if args.resume_from_checkpoint != \"latest\":\n+            path = os.path.basename(args.resume_from_checkpoint)\n+        else:\n+            # Get the most recent checkpoint\n+            dirs = os.listdir(args.output_dir)\n+            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n+            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n+            path = dirs[-1] if len(dirs) > 0 else None\n+\n+        if path is None:\n+            accelerator.print(\n+                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n+            )\n+            args.resume_from_checkpoint = None\n+            initial_global_step = 0\n+        else:\n+            accelerator.print(f\"Resuming from checkpoint {path}\")\n+            accelerator.load_state(os.path.join(args.output_dir, path))\n+            global_step = int(path.split(\"-\")[1])\n+\n+            initial_global_step = global_step\n+            first_epoch = global_step // num_update_steps_per_epoch\n+\n+    else:\n+        initial_global_step = 0\n+\n+    progress_bar = tqdm(\n+        range(0, args.max_train_steps),\n+        initial=initial_global_step,\n+        desc=\"Steps\",\n+        # Only show the progress bar once on each machine.\n+        disable=not accelerator.is_local_main_process,\n+    )\n+\n+    # keep original embeddings as reference\n+    orig_embeds_params = accelerator.unwrap_model(text_encoder_1).get_input_embeddings().weight.data.clone()\n+\n+    for epoch in range(first_epoch, args.num_train_epochs):\n+        text_encoder_1.train()\n+        for step, batch in enumerate(train_dataloader):\n+            with accelerator.accumulate(text_encoder_1):\n+                # Convert images to latent space\n+                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample().detach()\n+                latents = latents * vae.config.scaling_factor\n+\n+                # Sample noise that we'll add to the latents\n+                noise = torch.randn_like(latents)\n+                bsz = latents.shape[0]\n+                # Sample a random timestep for each image\n+                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n+                timesteps = timesteps.long()\n+\n+                # Add noise to the latents according to the noise magnitude at each timestep\n+                # (this is the forward diffusion process)\n+                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n+\n+                # Get the text embedding for conditioning\n+                encoder_hidden_states_1 = (\n+                    text_encoder_1(batch[\"input_ids_1\"], output_hidden_states=True)\n+                    .hidden_states[-2]\n+                    .to(dtype=weight_dtype)\n+                )\n+                encoder_output_2 = text_encoder_2(\n+                    batch[\"input_ids_2\"].reshape(batch[\"input_ids_1\"].shape[0], -1), output_hidden_states=True\n+                )\n+                encoder_hidden_states_2 = encoder_output_2.hidden_states[-2].to(dtype=weight_dtype)\n+                original_size = [\n+                    (batch[\"original_size\"][0][i].item(), batch[\"original_size\"][1][i].item())\n+                    for i in range(args.train_batch_size)\n+                ]\n+                crop_top_left = [\n+                    (batch[\"crop_top_left\"][0][i].item(), batch[\"crop_top_left\"][1][i].item())\n+                    for i in range(args.train_batch_size)\n+                ]\n+                target_size = (args.resolution, args.resolution)\n+                add_time_ids = torch.cat(\n+                    [\n+                        torch.tensor(original_size[i] + crop_top_left[i] + target_size)\n+                        for i in range(args.train_batch_size)\n+                    ]\n+                ).to(accelerator.device, dtype=weight_dtype)\n+                added_cond_kwargs = {\"text_embeds\": encoder_output_2[0], \"time_ids\": add_time_ids}\n+                encoder_hidden_states = torch.cat([encoder_hidden_states_1, encoder_hidden_states_2], dim=-1)\n+\n+                # Predict the noise residual\n+                model_pred = unet(\n+                    noisy_latents, timesteps, encoder_hidden_states, added_cond_kwargs=added_cond_kwargs\n+                ).sample\n+\n+                # Get the target for loss depending on the prediction type\n+                if noise_scheduler.config.prediction_type == \"epsilon\":\n+                    target = noise\n+                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n+                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n+                else:\n+                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n+\n+                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n+\n+                accelerator.backward(loss)\n+\n+                optimizer.step()\n+                lr_scheduler.step()\n+                optimizer.zero_grad()\n+\n+                # Let's make sure we don't update any embedding weights besides the newly added token\n+                index_no_updates = torch.ones((len(tokenizer_1),), dtype=torch.bool)\n+                index_no_updates[min(placeholder_token_ids) : max(placeholder_token_ids) + 1] = False\n+\n+                with torch.no_grad():\n+                    accelerator.unwrap_model(text_encoder_1).get_input_embeddings().weight[\n+                        index_no_updates\n+                    ] = orig_embeds_params[index_no_updates]\n+\n+            # Checks if the accelerator has performed an optimization step behind the scenes\n+            if accelerator.sync_gradients:\n+                images = []\n+                progress_bar.update(1)\n+                global_step += 1\n+                if global_step % args.save_steps == 0:\n+                    weight_name = f\"learned_embeds-steps-{global_step}.safetensors\"\n+                    save_path = os.path.join(args.output_dir, weight_name)\n+                    save_progress(\n+                        text_encoder_1,\n+                        placeholder_token_ids,\n+                        accelerator,\n+                        args,\n+                        save_path,\n+                        safe_serialization=True,\n+                    )\n+\n+                if accelerator.is_main_process:\n+                    if global_step % args.checkpointing_steps == 0:\n+                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n+                        if args.checkpoints_total_limit is not None:\n+                            checkpoints = os.listdir(args.output_dir)\n+                            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n+\n+                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n+                            if len(checkpoints) >= args.checkpoints_total_limit:\n+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n+                                removing_checkpoints = checkpoints[0:num_to_remove]\n+\n+                                logger.info(\n+                                    f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n+                                )\n+                                logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n+\n+                                for removing_checkpoint in removing_checkpoints:\n+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n+                                    shutil.rmtree(removing_checkpoint)\n+\n+                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n+                        accelerator.save_state(save_path)\n+                        logger.info(f\"Saved state to {save_path}\")\n+\n+                    if args.validation_prompt is not None and global_step % args.validation_steps == 0:\n+                        images = log_validation(\n+                            text_encoder_1,\n+                            text_encoder_2,\n+                            tokenizer_1,\n+                            tokenizer_2,\n+                            unet,\n+                            vae,\n+                            args,\n+                            accelerator,\n+                            weight_dtype,\n+                            epoch,\n+                        )\n+\n+            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n+            progress_bar.set_postfix(**logs)\n+            accelerator.log(logs, step=global_step)\n+\n+            if global_step >= args.max_train_steps:\n+                break\n+    # Create the pipeline using the trained modules and save it.\n+    accelerator.wait_for_everyone()\n+    if accelerator.is_main_process:\n+        if args.validation_prompt:\n+            images = log_validation(\n+                text_encoder_1,\n+                text_encoder_2,\n+                tokenizer_1,\n+                tokenizer_2,\n+                unet,\n+                vae,\n+                args,\n+                accelerator,\n+                weight_dtype,\n+                epoch,\n+                is_final_validation=True,\n+            )\n+\n+        if args.push_to_hub and not args.save_as_full_pipeline:\n+            logger.warn(\"Enabling full model saving because --push_to_hub=True was specified.\")\n+            save_full_model = True\n+        else:\n+            save_full_model = args.save_as_full_pipeline\n+        if save_full_model:\n+            pipeline = DiffusionPipeline.from_pretrained(\n+                args.pretrained_model_name_or_path,\n+                text_encoder=accelerator.unwrap_model(text_encoder_1),\n+                text_encoder_2=text_encoder_2,\n+                vae=vae,\n+                unet=unet,\n+                tokenizer=tokenizer_1,\n+                tokenizer_2=tokenizer_2,\n+            )\n+            pipeline.save_pretrained(args.output_dir)\n+        # Save the newly trained embeddings\n+        weight_name = \"learned_embeds.safetensors\"\n+        save_path = os.path.join(args.output_dir, weight_name)\n+        save_progress(\n+            text_encoder_1,\n+            placeholder_token_ids,\n+            accelerator,\n+            args,\n+            save_path,\n+            safe_serialization=True,\n+        )\n+\n+        if args.push_to_hub:\n+            save_model_card(\n+                repo_id,\n+                images=images,\n+                base_model=args.pretrained_model_name_or_path,\n+                repo_folder=args.output_dir,\n+            )\n+            upload_folder(\n+                repo_id=repo_id,\n+                folder_path=args.output_dir,\n+                commit_message=\"End of training\",\n+                ignore_patterns=[\"step_*\", \"epoch_*\"],\n+            )\n+\n+    accelerator.end_training()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/__init__.py",
            "diff": "diff --git a/src/diffusers/__init__.py b/src/diffusers/__init__.py\nindex 3f0d0211..00b660a6 100644\n--- a/src/diffusers/__init__.py\n+++ b/src/diffusers/__init__.py\n@@ -316,7 +316,7 @@ except OptionalDependencyNotAvailable:\n     ]\n \n else:\n-    _import_structure[\"pipelines\"].extend([\"StableDiffusionKDiffusionPipeline\"])\n+    _import_structure[\"pipelines\"].extend([\"StableDiffusionKDiffusionPipeline\", \"StableDiffusionXLKDiffusionPipeline\"])\n \n try:\n     if not (is_torch_available() and is_transformers_available() and is_onnx_available()):\n@@ -668,7 +668,7 @@ if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     except OptionalDependencyNotAvailable:\n         from .utils.dummy_torch_and_transformers_and_k_diffusion_objects import *  # noqa F403\n     else:\n-        from .pipelines import StableDiffusionKDiffusionPipeline\n+        from .pipelines import StableDiffusionKDiffusionPipeline, StableDiffusionXLKDiffusionPipeline\n \n     try:\n         if not (is_torch_available() and is_transformers_available() and is_onnx_available()):\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/loaders/peft.py",
            "diff": "diff --git a/src/diffusers/loaders/peft.py b/src/diffusers/loaders/peft.py\nindex a7f1e1c9..f89bc5a6 100644\n--- a/src/diffusers/loaders/peft.py\n+++ b/src/diffusers/loaders/peft.py\n@@ -20,15 +20,13 @@ from ..utils import MIN_PEFT_VERSION, check_peft_version, is_peft_available\n class PeftAdapterMixin:\n     \"\"\"\n     A class containing all functions for loading and using adapters weights that are supported in PEFT library. For\n-    more details about adapters and injecting them on a transformer-based model, check out the documentation of PEFT\n-    library: https://huggingface.co/docs/peft/index.\n+    more details about adapters and injecting them in a transformer-based model, check out the PEFT [documentation](https://huggingface.co/docs/peft/index).\n \n-\n-    With this mixin, if the correct PEFT version is installed, it is possible to:\n+    Install the latest version of PEFT, and use this mixin to:\n \n     - Attach new adapters in the model.\n-    - Attach multiple adapters and iteratively activate / deactivate them.\n-    - Activate / deactivate all adapters from the model.\n+    - Attach multiple adapters and iteratively activate/deactivate them.\n+    - Activate/deactivate all adapters from the model.\n     - Get a list of the active adapters.\n     \"\"\"\n \n@@ -77,11 +75,11 @@ class PeftAdapterMixin:\n         Sets a specific adapter by forcing the model to only use that adapter and disables the other adapters.\n \n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n-        official documentation: https://huggingface.co/docs/peft\n+        [documentation](https://huggingface.co/docs/peft).\n \n         Args:\n             adapter_name (Union[str, List[str]])):\n-                The list of adapters to set or the adapter name in case of single adapter.\n+                The list of adapters to set or the adapter name in the case of a single adapter.\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n@@ -126,7 +124,7 @@ class PeftAdapterMixin:\n         Disable all adapters attached to the model and fallback to inference with the base model only.\n \n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n-        official documentation: https://huggingface.co/docs/peft\n+        [documentation](https://huggingface.co/docs/peft).\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n@@ -145,11 +143,11 @@ class PeftAdapterMixin:\n \n     def enable_adapters(self) -> None:\n         \"\"\"\n-        Enable adapters that are attached to the model. The model will use `self.active_adapters()` to retrieve the\n+        Enable adapters that are attached to the model. The model uses `self.active_adapters()` to retrieve the\n         list of adapters to enable.\n \n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n-        official documentation: https://huggingface.co/docs/peft\n+        [documentation](https://huggingface.co/docs/peft).\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n@@ -171,7 +169,7 @@ class PeftAdapterMixin:\n         Gets the current list of active adapters of the model.\n \n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n-        official documentation: https://huggingface.co/docs/peft\n+        [documentation](https://huggingface.co/docs/peft).\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/models/resnet.py",
            "diff": "diff --git a/src/diffusers/models/resnet.py b/src/diffusers/models/resnet.py\nindex bbfb71ca..3b8718f3 100644\n--- a/src/diffusers/models/resnet.py\n+++ b/src/diffusers/models/resnet.py\n@@ -42,6 +42,156 @@ from .upsampling import (  # noqa\n )\n \n \n+class ResnetBlockCondNorm2D(nn.Module):\n+    r\"\"\"\n+    A Resnet block that use normalization layer that incorporate conditioning information.\n+\n+    Parameters:\n+        in_channels (`int`): The number of channels in the input.\n+        out_channels (`int`, *optional*, default to be `None`):\n+            The number of output channels for the first conv2d layer. If None, same as `in_channels`.\n+        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.\n+        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.\n+        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.\n+        groups_out (`int`, *optional*, default to None):\n+            The number of groups to use for the second normalization layer. if set to None, same as `groups`.\n+        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n+        non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n+        time_embedding_norm (`str`, *optional*, default to `\"ada_group\"` ):\n+            The normalization layer for time embedding `temb`. Currently only support \"ada_group\" or \"spatial\".\n+        kernel (`torch.FloatTensor`, optional, default to None): FIR filter, see\n+            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n+        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n+        use_in_shortcut (`bool`, *optional*, default to `True`):\n+            If `True`, add a 1x1 nn.conv2d layer for skip-connection.\n+        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.\n+        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.\n+        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the\n+            `conv_shortcut` output.\n+        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.\n+            If None, same as `out_channels`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        in_channels: int,\n+        out_channels: Optional[int] = None,\n+        conv_shortcut: bool = False,\n+        dropout: float = 0.0,\n+        temb_channels: int = 512,\n+        groups: int = 32,\n+        groups_out: Optional[int] = None,\n+        eps: float = 1e-6,\n+        non_linearity: str = \"swish\",\n+        time_embedding_norm: str = \"ada_group\",  # ada_group, spatial\n+        output_scale_factor: float = 1.0,\n+        use_in_shortcut: Optional[bool] = None,\n+        up: bool = False,\n+        down: bool = False,\n+        conv_shortcut_bias: bool = True,\n+        conv_2d_out_channels: Optional[int] = None,\n+    ):\n+        super().__init__()\n+        self.in_channels = in_channels\n+        out_channels = in_channels if out_channels is None else out_channels\n+        self.out_channels = out_channels\n+        self.use_conv_shortcut = conv_shortcut\n+        self.up = up\n+        self.down = down\n+        self.output_scale_factor = output_scale_factor\n+        self.time_embedding_norm = time_embedding_norm\n+\n+        conv_cls = nn.Conv2d if USE_PEFT_BACKEND else LoRACompatibleConv\n+\n+        if groups_out is None:\n+            groups_out = groups\n+\n+        if self.time_embedding_norm == \"ada_group\":  # ada_group\n+            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)\n+        elif self.time_embedding_norm == \"spatial\":\n+            self.norm1 = SpatialNorm(in_channels, temb_channels)\n+        else:\n+            raise ValueError(f\" unsupported time_embedding_norm: {self.time_embedding_norm}\")\n+\n+        self.conv1 = conv_cls(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n+\n+        if self.time_embedding_norm == \"ada_group\":  # ada_group\n+            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)\n+        elif self.time_embedding_norm == \"spatial\":  # spatial\n+            self.norm2 = SpatialNorm(out_channels, temb_channels)\n+        else:\n+            raise ValueError(f\" unsupported time_embedding_norm: {self.time_embedding_norm}\")\n+\n+        self.dropout = torch.nn.Dropout(dropout)\n+\n+        conv_2d_out_channels = conv_2d_out_channels or out_channels\n+        self.conv2 = conv_cls(out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1)\n+\n+        self.nonlinearity = get_activation(non_linearity)\n+\n+        self.upsample = self.downsample = None\n+        if self.up:\n+            self.upsample = Upsample2D(in_channels, use_conv=False)\n+        elif self.down:\n+            self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name=\"op\")\n+\n+        self.use_in_shortcut = self.in_channels != conv_2d_out_channels if use_in_shortcut is None else use_in_shortcut\n+\n+        self.conv_shortcut = None\n+        if self.use_in_shortcut:\n+            self.conv_shortcut = conv_cls(\n+                in_channels,\n+                conv_2d_out_channels,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                bias=conv_shortcut_bias,\n+            )\n+\n+    def forward(\n+        self,\n+        input_tensor: torch.FloatTensor,\n+        temb: torch.FloatTensor,\n+        scale: float = 1.0,\n+    ) -> torch.FloatTensor:\n+        hidden_states = input_tensor\n+\n+        hidden_states = self.norm1(hidden_states, temb)\n+\n+        hidden_states = self.nonlinearity(hidden_states)\n+\n+        if self.upsample is not None:\n+            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n+            if hidden_states.shape[0] >= 64:\n+                input_tensor = input_tensor.contiguous()\n+                hidden_states = hidden_states.contiguous()\n+            input_tensor = self.upsample(input_tensor, scale=scale)\n+            hidden_states = self.upsample(hidden_states, scale=scale)\n+\n+        elif self.downsample is not None:\n+            input_tensor = self.downsample(input_tensor, scale=scale)\n+            hidden_states = self.downsample(hidden_states, scale=scale)\n+\n+        hidden_states = self.conv1(hidden_states, scale) if not USE_PEFT_BACKEND else self.conv1(hidden_states)\n+\n+        hidden_states = self.norm2(hidden_states, temb)\n+\n+        hidden_states = self.nonlinearity(hidden_states)\n+\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.conv2(hidden_states, scale) if not USE_PEFT_BACKEND else self.conv2(hidden_states)\n+\n+        if self.conv_shortcut is not None:\n+            input_tensor = (\n+                self.conv_shortcut(input_tensor, scale) if not USE_PEFT_BACKEND else self.conv_shortcut(input_tensor)\n+            )\n+\n+        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n+\n+        return output_tensor\n+\n+\n class ResnetBlock2D(nn.Module):\n     r\"\"\"\n     A Resnet block.\n@@ -58,8 +208,8 @@ class ResnetBlock2D(nn.Module):\n         eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n         non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n         time_embedding_norm (`str`, *optional*, default to `\"default\"` ): Time scale shift config.\n-            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose \"scale_shift\" or\n-            \"ada_group\" for a stronger conditioning with scale and shift.\n+            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose \"scale_shift\"\n+            for a stronger conditioning with scale and shift.\n         kernel (`torch.FloatTensor`, optional, default to None): FIR filter, see\n             [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n         output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n@@ -87,7 +237,7 @@ class ResnetBlock2D(nn.Module):\n         eps: float = 1e-6,\n         non_linearity: str = \"swish\",\n         skip_time_act: bool = False,\n-        time_embedding_norm: str = \"default\",  # default, scale_shift, ada_group, spatial\n+        time_embedding_norm: str = \"default\",  # default, scale_shift,\n         kernel: Optional[torch.FloatTensor] = None,\n         output_scale_factor: float = 1.0,\n         use_in_shortcut: Optional[bool] = None,\n@@ -97,7 +247,15 @@ class ResnetBlock2D(nn.Module):\n         conv_2d_out_channels: Optional[int] = None,\n     ):\n         super().__init__()\n-        self.pre_norm = pre_norm\n+        if time_embedding_norm == \"ada_group\":\n+            raise ValueError(\n+                \"This class cannot be used with `time_embedding_norm==ada_group`, please use `ResnetBlockCondNorm2D` instead\",\n+            )\n+        if time_embedding_norm == \"spatial\":\n+            raise ValueError(\n+                \"This class cannot be used with `time_embedding_norm==spatial`, please use `ResnetBlockCondNorm2D` instead\",\n+            )\n+\n         self.pre_norm = True\n         self.in_channels = in_channels\n         out_channels = in_channels if out_channels is None else out_channels\n@@ -115,12 +273,7 @@ class ResnetBlock2D(nn.Module):\n         if groups_out is None:\n             groups_out = groups\n \n-        if self.time_embedding_norm == \"ada_group\":\n-            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)\n-        elif self.time_embedding_norm == \"spatial\":\n-            self.norm1 = SpatialNorm(in_channels, temb_channels)\n-        else:\n-            self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n+        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n \n         self.conv1 = conv_cls(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n \n@@ -129,19 +282,12 @@ class ResnetBlock2D(nn.Module):\n                 self.time_emb_proj = linear_cls(temb_channels, out_channels)\n             elif self.time_embedding_norm == \"scale_shift\":\n                 self.time_emb_proj = linear_cls(temb_channels, 2 * out_channels)\n-            elif self.time_embedding_norm == \"ada_group\" or self.time_embedding_norm == \"spatial\":\n-                self.time_emb_proj = None\n             else:\n                 raise ValueError(f\"unknown time_embedding_norm : {self.time_embedding_norm} \")\n         else:\n             self.time_emb_proj = None\n \n-        if self.time_embedding_norm == \"ada_group\":\n-            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)\n-        elif self.time_embedding_norm == \"spatial\":\n-            self.norm2 = SpatialNorm(out_channels, temb_channels)\n-        else:\n-            self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n+        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n \n         self.dropout = torch.nn.Dropout(dropout)\n         conv_2d_out_channels = conv_2d_out_channels or out_channels\n@@ -188,11 +334,7 @@ class ResnetBlock2D(nn.Module):\n     ) -> torch.FloatTensor:\n         hidden_states = input_tensor\n \n-        if self.time_embedding_norm == \"ada_group\" or self.time_embedding_norm == \"spatial\":\n-            hidden_states = self.norm1(hidden_states, temb)\n-        else:\n-            hidden_states = self.norm1(hidden_states)\n-\n+        hidden_states = self.norm1(hidden_states)\n         hidden_states = self.nonlinearity(hidden_states)\n \n         if self.upsample is not None:\n@@ -233,17 +375,20 @@ class ResnetBlock2D(nn.Module):\n                 else self.time_emb_proj(temb)[:, :, None, None]\n             )\n \n-        if temb is not None and self.time_embedding_norm == \"default\":\n-            hidden_states = hidden_states + temb\n-\n-        if self.time_embedding_norm == \"ada_group\" or self.time_embedding_norm == \"spatial\":\n-            hidden_states = self.norm2(hidden_states, temb)\n-        else:\n+        if self.time_embedding_norm == \"default\":\n+            if temb is not None:\n+                hidden_states = hidden_states + temb\n             hidden_states = self.norm2(hidden_states)\n-\n-        if temb is not None and self.time_embedding_norm == \"scale_shift\":\n+        elif self.time_embedding_norm == \"scale_shift\":\n+            if temb is None:\n+                raise ValueError(\n+                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n+                )\n             scale, shift = torch.chunk(temb, 2, dim=1)\n+            hidden_states = self.norm2(hidden_states)\n             hidden_states = hidden_states * (1 + scale) + shift\n+        else:\n+            hidden_states = self.norm2(hidden_states)\n \n         hidden_states = self.nonlinearity(hidden_states)\n \n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/models/unet_2d_blocks.py",
            "diff": "diff --git a/src/diffusers/models/unet_2d_blocks.py b/src/diffusers/models/unet_2d_blocks.py\nindex e404cef2..470a0211 100644\n--- a/src/diffusers/models/unet_2d_blocks.py\n+++ b/src/diffusers/models/unet_2d_blocks.py\n@@ -24,7 +24,16 @@ from .activations import get_activation\n from .attention_processor import Attention, AttnAddedKVProcessor, AttnAddedKVProcessor2_0\n from .dual_transformer_2d import DualTransformer2DModel\n from .normalization import AdaGroupNorm\n-from .resnet import Downsample2D, FirDownsample2D, FirUpsample2D, KDownsample2D, KUpsample2D, ResnetBlock2D, Upsample2D\n+from .resnet import (\n+    Downsample2D,\n+    FirDownsample2D,\n+    FirUpsample2D,\n+    KDownsample2D,\n+    KUpsample2D,\n+    ResnetBlock2D,\n+    ResnetBlockCondNorm2D,\n+    Upsample2D,\n+)\n from .transformer_2d import Transformer2DModel\n \n \n@@ -557,20 +566,35 @@ class UNetMidBlock2D(nn.Module):\n             attn_groups = resnet_groups if resnet_time_scale_shift == \"default\" else None\n \n         # there is always at least one resnet\n-        resnets = [\n-            ResnetBlock2D(\n-                in_channels=in_channels,\n-                out_channels=in_channels,\n-                temb_channels=temb_channels,\n-                eps=resnet_eps,\n-                groups=resnet_groups,\n-                dropout=dropout,\n-                time_embedding_norm=resnet_time_scale_shift,\n-                non_linearity=resnet_act_fn,\n-                output_scale_factor=output_scale_factor,\n-                pre_norm=resnet_pre_norm,\n-            )\n-        ]\n+        if resnet_time_scale_shift == \"spatial\":\n+            resnets = [\n+                ResnetBlockCondNorm2D(\n+                    in_channels=in_channels,\n+                    out_channels=in_channels,\n+                    temb_channels=temb_channels,\n+                    eps=resnet_eps,\n+                    groups=resnet_groups,\n+                    dropout=dropout,\n+                    time_embedding_norm=\"spatial\",\n+                    non_linearity=resnet_act_fn,\n+                    output_scale_factor=output_scale_factor,\n+                )\n+            ]\n+        else:\n+            resnets = [\n+                ResnetBlock2D(\n+                    in_channels=in_channels,\n+                    out_channels=in_channels,\n+                    temb_channels=temb_channels,\n+                    eps=resnet_eps,\n+                    groups=resnet_groups,\n+                    dropout=dropout,\n+                    time_embedding_norm=resnet_time_scale_shift,\n+                    non_linearity=resnet_act_fn,\n+                    output_scale_factor=output_scale_factor,\n+                    pre_norm=resnet_pre_norm,\n+                )\n+            ]\n         attentions = []\n \n         if attention_head_dim is None:\n@@ -599,20 +623,35 @@ class UNetMidBlock2D(nn.Module):\n             else:\n                 attentions.append(None)\n \n-            resnets.append(\n-                ResnetBlock2D(\n-                    in_channels=in_channels,\n-                    out_channels=in_channels,\n-                    temb_channels=temb_channels,\n-                    eps=resnet_eps,\n-                    groups=resnet_groups,\n-                    dropout=dropout,\n-                    time_embedding_norm=resnet_time_scale_shift,\n-                    non_linearity=resnet_act_fn,\n-                    output_scale_factor=output_scale_factor,\n-                    pre_norm=resnet_pre_norm,\n+            if resnet_time_scale_shift == \"spatial\":\n+                resnets.append(\n+                    ResnetBlockCondNorm2D(\n+                        in_channels=in_channels,\n+                        out_channels=in_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=\"spatial\",\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                    )\n+                )\n+            else:\n+                resnets.append(\n+                    ResnetBlock2D(\n+                        in_channels=in_channels,\n+                        out_channels=in_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=resnet_time_scale_shift,\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                        pre_norm=resnet_pre_norm,\n+                    )\n                 )\n-            )\n \n         self.attentions = nn.ModuleList(attentions)\n         self.resnets = nn.ModuleList(resnets)\n@@ -1290,20 +1329,35 @@ class DownEncoderBlock2D(nn.Module):\n \n         for i in range(num_layers):\n             in_channels = in_channels if i == 0 else out_channels\n-            resnets.append(\n-                ResnetBlock2D(\n-                    in_channels=in_channels,\n-                    out_channels=out_channels,\n-                    temb_channels=None,\n-                    eps=resnet_eps,\n-                    groups=resnet_groups,\n-                    dropout=dropout,\n-                    time_embedding_norm=resnet_time_scale_shift,\n-                    non_linearity=resnet_act_fn,\n-                    output_scale_factor=output_scale_factor,\n-                    pre_norm=resnet_pre_norm,\n+            if resnet_time_scale_shift == \"spatial\":\n+                resnets.append(\n+                    ResnetBlockCondNorm2D(\n+                        in_channels=in_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=None,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=\"spatial\",\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                    )\n+                )\n+            else:\n+                resnets.append(\n+                    ResnetBlock2D(\n+                        in_channels=in_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=None,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=resnet_time_scale_shift,\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                        pre_norm=resnet_pre_norm,\n+                    )\n                 )\n-            )\n \n         self.resnets = nn.ModuleList(resnets)\n \n@@ -1358,20 +1412,35 @@ class AttnDownEncoderBlock2D(nn.Module):\n \n         for i in range(num_layers):\n             in_channels = in_channels if i == 0 else out_channels\n-            resnets.append(\n-                ResnetBlock2D(\n-                    in_channels=in_channels,\n-                    out_channels=out_channels,\n-                    temb_channels=None,\n-                    eps=resnet_eps,\n-                    groups=resnet_groups,\n-                    dropout=dropout,\n-                    time_embedding_norm=resnet_time_scale_shift,\n-                    non_linearity=resnet_act_fn,\n-                    output_scale_factor=output_scale_factor,\n-                    pre_norm=resnet_pre_norm,\n+            if resnet_time_scale_shift == \"spatial\":\n+                resnets.append(\n+                    ResnetBlockCondNorm2D(\n+                        in_channels=in_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=None,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=\"spatial\",\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                    )\n+                )\n+            else:\n+                resnets.append(\n+                    ResnetBlock2D(\n+                        in_channels=in_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=None,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=resnet_time_scale_shift,\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                        pre_norm=resnet_pre_norm,\n+                    )\n                 )\n-            )\n             attentions.append(\n                 Attention(\n                     out_channels,\n@@ -1889,7 +1958,7 @@ class KDownBlock2D(nn.Module):\n             groups_out = out_channels // resnet_group_size\n \n             resnets.append(\n-                ResnetBlock2D(\n+                ResnetBlockCondNorm2D(\n                     in_channels=in_channels,\n                     out_channels=out_channels,\n                     dropout=dropout,\n@@ -1975,7 +2044,7 @@ class KCrossAttnDownBlock2D(nn.Module):\n             groups_out = out_channels // resnet_group_size\n \n             resnets.append(\n-                ResnetBlock2D(\n+                ResnetBlockCondNorm2D(\n                     in_channels=in_channels,\n                     out_channels=out_channels,\n                     dropout=dropout,\n@@ -2500,20 +2569,35 @@ class UpDecoderBlock2D(nn.Module):\n         for i in range(num_layers):\n             input_channels = in_channels if i == 0 else out_channels\n \n-            resnets.append(\n-                ResnetBlock2D(\n-                    in_channels=input_channels,\n-                    out_channels=out_channels,\n-                    temb_channels=temb_channels,\n-                    eps=resnet_eps,\n-                    groups=resnet_groups,\n-                    dropout=dropout,\n-                    time_embedding_norm=resnet_time_scale_shift,\n-                    non_linearity=resnet_act_fn,\n-                    output_scale_factor=output_scale_factor,\n-                    pre_norm=resnet_pre_norm,\n+            if resnet_time_scale_shift == \"spatial\":\n+                resnets.append(\n+                    ResnetBlockCondNorm2D(\n+                        in_channels=input_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=\"spatial\",\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                    )\n+                )\n+            else:\n+                resnets.append(\n+                    ResnetBlock2D(\n+                        in_channels=input_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=resnet_time_scale_shift,\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                        pre_norm=resnet_pre_norm,\n+                    )\n                 )\n-            )\n \n         self.resnets = nn.ModuleList(resnets)\n \n@@ -2568,20 +2652,36 @@ class AttnUpDecoderBlock2D(nn.Module):\n         for i in range(num_layers):\n             input_channels = in_channels if i == 0 else out_channels\n \n-            resnets.append(\n-                ResnetBlock2D(\n-                    in_channels=input_channels,\n-                    out_channels=out_channels,\n-                    temb_channels=temb_channels,\n-                    eps=resnet_eps,\n-                    groups=resnet_groups,\n-                    dropout=dropout,\n-                    time_embedding_norm=resnet_time_scale_shift,\n-                    non_linearity=resnet_act_fn,\n-                    output_scale_factor=output_scale_factor,\n-                    pre_norm=resnet_pre_norm,\n+            if resnet_time_scale_shift == \"spatial\":\n+                resnets.append(\n+                    ResnetBlockCondNorm2D(\n+                        in_channels=input_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=\"spatial\",\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                    )\n                 )\n-            )\n+            else:\n+                resnets.append(\n+                    ResnetBlock2D(\n+                        in_channels=input_channels,\n+                        out_channels=out_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=resnet_time_scale_shift,\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                        pre_norm=resnet_pre_norm,\n+                    )\n+                )\n+\n             attentions.append(\n                 Attention(\n                     out_channels,\n@@ -3159,7 +3259,7 @@ class KUpBlock2D(nn.Module):\n             groups_out = out_channels // resnet_group_size\n \n             resnets.append(\n-                ResnetBlock2D(\n+                ResnetBlockCondNorm2D(\n                     in_channels=in_channels,\n                     out_channels=k_out_channels if (i == num_layers - 1) else out_channels,\n                     temb_channels=temb_channels,\n@@ -3267,7 +3367,7 @@ class KCrossAttnUpBlock2D(nn.Module):\n                 conv_2d_out_channels = None\n \n             resnets.append(\n-                ResnetBlock2D(\n+                ResnetBlockCondNorm2D(\n                     in_channels=in_channels,\n                     out_channels=out_channels,\n                     conv_2d_out_channels=conv_2d_out_channels,\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/__init__.py",
            "diff": "diff --git a/src/diffusers/pipelines/__init__.py b/src/diffusers/pipelines/__init__.py\nindex 2b456f4c..13163f96 100644\n--- a/src/diffusers/pipelines/__init__.py\n+++ b/src/diffusers/pipelines/__init__.py\n@@ -265,7 +265,10 @@ except OptionalDependencyNotAvailable:\n \n     _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_and_k_diffusion_objects))\n else:\n-    _import_structure[\"stable_diffusion_k_diffusion\"] = [\"StableDiffusionKDiffusionPipeline\"]\n+    _import_structure[\"stable_diffusion_k_diffusion\"] = [\n+        \"StableDiffusionKDiffusionPipeline\",\n+        \"StableDiffusionXLKDiffusionPipeline\",\n+    ]\n try:\n     if not is_flax_available():\n         raise OptionalDependencyNotAvailable()\n@@ -491,7 +494,10 @@ if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n         except OptionalDependencyNotAvailable:\n             from ..utils.dummy_torch_and_transformers_and_k_diffusion_objects import *\n         else:\n-            from .stable_diffusion_k_diffusion import StableDiffusionKDiffusionPipeline\n+            from .stable_diffusion_k_diffusion import (\n+                StableDiffusionKDiffusionPipeline,\n+                StableDiffusionXLKDiffusionPipeline,\n+            )\n \n         try:\n             if not is_flax_available():\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py",
            "diff": "diff --git a/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py b/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py\nindex cbe39f78..12ff9bbb 100644\n--- a/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py\n+++ b/src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py\n@@ -20,13 +20,23 @@ import numpy as np\n import PIL.Image\n import torch\n import torch.nn.functional as F\n-from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer\n+from transformers import (\n+    CLIPImageProcessor,\n+    CLIPTextModel,\n+    CLIPTextModelWithProjection,\n+    CLIPTokenizer,\n+    CLIPVisionModelWithProjection,\n+)\n \n from diffusers.utils.import_utils import is_invisible_watermark_available\n \n from ...image_processor import PipelineImageInput, VaeImageProcessor\n-from ...loaders import StableDiffusionXLLoraLoaderMixin, TextualInversionLoaderMixin\n-from ...models import AutoencoderKL, ControlNetModel, UNet2DConditionModel\n+from ...loaders import (\n+    IPAdapterMixin,\n+    StableDiffusionXLLoraLoaderMixin,\n+    TextualInversionLoaderMixin,\n+)\n+from ...models import AutoencoderKL, ControlNetModel, ImageProjection, UNet2DConditionModel\n from ...models.attention_processor import (\n     AttnProcessor2_0,\n     LoRAAttnProcessor2_0,\n@@ -147,7 +157,7 @@ def retrieve_latents(\n \n \n class StableDiffusionXLControlNetImg2ImgPipeline(\n-    DiffusionPipeline, TextualInversionLoaderMixin, StableDiffusionXLLoraLoaderMixin\n+    DiffusionPipeline, TextualInversionLoaderMixin, StableDiffusionXLLoraLoaderMixin, IPAdapterMixin\n ):\n     r\"\"\"\n     Pipeline for image-to-image generation using Stable Diffusion XL with ControlNet guidance.\n@@ -159,6 +169,7 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n         - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings\n         - [`~loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n         - [`~loaders.StableDiffusionXLLoraLoaderMixin.save_lora_weights`] for saving LoRA weights\n+        - [`~loaders.IPAdapterMixin.load_ip_adapter`] for loading IP Adapters\n \n     Args:\n         vae ([`AutoencoderKL`]):\n@@ -197,10 +208,19 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n             Whether to use the [invisible_watermark library](https://github.com/ShieldMnt/invisible-watermark/) to\n             watermark output images. If not defined, it will default to True if the package is installed, otherwise no\n             watermarker will be used.\n+        feature_extractor ([`~transformers.CLIPImageProcessor`]):\n+            A `CLIPImageProcessor` to extract features from generated images; used as inputs to the `safety_checker`.\n     \"\"\"\n \n-    model_cpu_offload_seq = \"text_encoder->text_encoder_2->unet->vae\"\n-    _optional_components = [\"tokenizer\", \"tokenizer_2\", \"text_encoder\", \"text_encoder_2\"]\n+    model_cpu_offload_seq = \"text_encoder->text_encoder_2->image_encoder->unet->vae\"\n+    _optional_components = [\n+        \"tokenizer\",\n+        \"tokenizer_2\",\n+        \"text_encoder\",\n+        \"text_encoder_2\",\n+        \"feature_extractor\",\n+        \"image_encoder\",\n+    ]\n     _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n \n     def __init__(\n@@ -216,6 +236,8 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n         requires_aesthetics_score: bool = False,\n         force_zeros_for_empty_prompt: bool = True,\n         add_watermarker: Optional[bool] = None,\n+        feature_extractor: CLIPImageProcessor = None,\n+        image_encoder: CLIPVisionModelWithProjection = None,\n     ):\n         super().__init__()\n \n@@ -231,6 +253,8 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n             unet=unet,\n             controlnet=controlnet,\n             scheduler=scheduler,\n+            feature_extractor=feature_extractor,\n+            image_encoder=image_encoder,\n         )\n         self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n         self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor, do_convert_rgb=True)\n@@ -515,6 +539,31 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n \n         return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds\n \n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.encode_image\n+    def encode_image(self, image, device, num_images_per_prompt, output_hidden_states=None):\n+        dtype = next(self.image_encoder.parameters()).dtype\n+\n+        if not isinstance(image, torch.Tensor):\n+            image = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n+\n+        image = image.to(device=device, dtype=dtype)\n+        if output_hidden_states:\n+            image_enc_hidden_states = self.image_encoder(image, output_hidden_states=True).hidden_states[-2]\n+            image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n+            uncond_image_enc_hidden_states = self.image_encoder(\n+                torch.zeros_like(image), output_hidden_states=True\n+            ).hidden_states[-2]\n+            uncond_image_enc_hidden_states = uncond_image_enc_hidden_states.repeat_interleave(\n+                num_images_per_prompt, dim=0\n+            )\n+            return image_enc_hidden_states, uncond_image_enc_hidden_states\n+        else:\n+            image_embeds = self.image_encoder(image).image_embeds\n+            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n+            uncond_image_embeds = torch.zeros_like(image_embeds)\n+\n+            return image_embeds, uncond_image_embeds\n+\n     # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n     def prepare_extra_step_kwargs(self, generator, eta):\n         # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n@@ -1011,6 +1060,7 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n         negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n         pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n         negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        ip_adapter_image: Optional[PipelineImageInput] = None,\n         output_type: Optional[str] = \"pil\",\n         return_dict: bool = True,\n         cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n@@ -1109,6 +1159,7 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n                 Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                 weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n                 input argument.\n+            ip_adapter_image: (`PipelineImageInput`, *optional*): Optional image input to work with IP Adapters.\n             output_type (`str`, *optional*, defaults to `\"pil\"`):\n                 The output format of the generate image. Choose between\n                 [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n@@ -1262,7 +1313,7 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n         )\n         guess_mode = guess_mode or global_pool_conditions\n \n-        # 3. Encode input prompt\n+        # 3.1. Encode input prompt\n         text_encoder_lora_scale = (\n             self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n         )\n@@ -1287,6 +1338,15 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n             clip_skip=self.clip_skip,\n         )\n \n+        # 3.2 Encode ip_adapter_image\n+        if ip_adapter_image is not None:\n+            output_hidden_state = False if isinstance(self.unet.encoder_hid_proj, ImageProjection) else True\n+            image_embeds, negative_image_embeds = self.encode_image(\n+                ip_adapter_image, device, num_images_per_prompt, output_hidden_state\n+            )\n+            if self.do_classifier_free_guidance:\n+                image_embeds = torch.cat([negative_image_embeds, image_embeds])\n+\n         # 4. Prepare image and controlnet_conditioning_image\n         image = self.image_processor.preprocess(image, height=height, width=width).to(dtype=torch.float32)\n \n@@ -1449,6 +1509,9 @@ class StableDiffusionXLControlNetImg2ImgPipeline(\n                     down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n                     mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n \n+                if ip_adapter_image is not None:\n+                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n+\n                 # predict the noise residual\n                 noise_pred = self.unet(\n                     latent_model_input,\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py",
            "diff": "diff --git a/src/diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py b/src/diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py\nindex 86e3cfef..504dfa40 100644\n--- a/src/diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py\n+++ b/src/diffusers/pipelines/deprecated/versatile_diffusion/modeling_text_unet.py\n@@ -31,6 +31,7 @@ from ....models.embeddings import (\n     TimestepEmbedding,\n     Timesteps,\n )\n+from ....models.resnet import ResnetBlockCondNorm2D\n from ....models.transformer_2d import Transformer2DModel\n from ....models.unet_2d_condition import UNet2DConditionOutput\n from ....utils import USE_PEFT_BACKEND, is_torch_version, logging, scale_lora_layers, unscale_lora_layers\n@@ -2126,20 +2127,35 @@ class UNetMidBlockFlat(nn.Module):\n             attn_groups = resnet_groups if resnet_time_scale_shift == \"default\" else None\n \n         # there is always at least one resnet\n-        resnets = [\n-            ResnetBlockFlat(\n-                in_channels=in_channels,\n-                out_channels=in_channels,\n-                temb_channels=temb_channels,\n-                eps=resnet_eps,\n-                groups=resnet_groups,\n-                dropout=dropout,\n-                time_embedding_norm=resnet_time_scale_shift,\n-                non_linearity=resnet_act_fn,\n-                output_scale_factor=output_scale_factor,\n-                pre_norm=resnet_pre_norm,\n-            )\n-        ]\n+        if resnet_time_scale_shift == \"spatial\":\n+            resnets = [\n+                ResnetBlockCondNorm2D(\n+                    in_channels=in_channels,\n+                    out_channels=in_channels,\n+                    temb_channels=temb_channels,\n+                    eps=resnet_eps,\n+                    groups=resnet_groups,\n+                    dropout=dropout,\n+                    time_embedding_norm=\"spatial\",\n+                    non_linearity=resnet_act_fn,\n+                    output_scale_factor=output_scale_factor,\n+                )\n+            ]\n+        else:\n+            resnets = [\n+                ResnetBlockFlat(\n+                    in_channels=in_channels,\n+                    out_channels=in_channels,\n+                    temb_channels=temb_channels,\n+                    eps=resnet_eps,\n+                    groups=resnet_groups,\n+                    dropout=dropout,\n+                    time_embedding_norm=resnet_time_scale_shift,\n+                    non_linearity=resnet_act_fn,\n+                    output_scale_factor=output_scale_factor,\n+                    pre_norm=resnet_pre_norm,\n+                )\n+            ]\n         attentions = []\n \n         if attention_head_dim is None:\n@@ -2168,20 +2184,35 @@ class UNetMidBlockFlat(nn.Module):\n             else:\n                 attentions.append(None)\n \n-            resnets.append(\n-                ResnetBlockFlat(\n-                    in_channels=in_channels,\n-                    out_channels=in_channels,\n-                    temb_channels=temb_channels,\n-                    eps=resnet_eps,\n-                    groups=resnet_groups,\n-                    dropout=dropout,\n-                    time_embedding_norm=resnet_time_scale_shift,\n-                    non_linearity=resnet_act_fn,\n-                    output_scale_factor=output_scale_factor,\n-                    pre_norm=resnet_pre_norm,\n+            if resnet_time_scale_shift == \"spatial\":\n+                resnets.append(\n+                    ResnetBlockCondNorm2D(\n+                        in_channels=in_channels,\n+                        out_channels=in_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=\"spatial\",\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                    )\n+                )\n+            else:\n+                resnets.append(\n+                    ResnetBlockFlat(\n+                        in_channels=in_channels,\n+                        out_channels=in_channels,\n+                        temb_channels=temb_channels,\n+                        eps=resnet_eps,\n+                        groups=resnet_groups,\n+                        dropout=dropout,\n+                        time_embedding_norm=resnet_time_scale_shift,\n+                        non_linearity=resnet_act_fn,\n+                        output_scale_factor=output_scale_factor,\n+                        pre_norm=resnet_pre_norm,\n+                    )\n                 )\n-            )\n \n         self.attentions = nn.ModuleList(attentions)\n         self.resnets = nn.ModuleList(resnets)\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py",
            "diff": "diff --git a/src/diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py b/src/diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py\nindex 6c4bd004..7eb5bf8c 100644\n--- a/src/diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py\n+++ b/src/diffusers/pipelines/stable_diffusion_k_diffusion/__init__.py\n@@ -30,6 +30,7 @@ except OptionalDependencyNotAvailable:\n     _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_and_k_diffusion_objects))\n else:\n     _import_structure[\"pipeline_stable_diffusion_k_diffusion\"] = [\"StableDiffusionKDiffusionPipeline\"]\n+    _import_structure[\"pipeline_stable_diffusion_xl_k_diffusion\"] = [\"StableDiffusionXLKDiffusionPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -45,6 +46,7 @@ if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n         from ...utils.dummy_torch_and_transformers_and_k_diffusion_objects import *\n     else:\n         from .pipeline_stable_diffusion_k_diffusion import StableDiffusionKDiffusionPipeline\n+        from .pipeline_stable_diffusion_xl_k_diffusion import StableDiffusionXLKDiffusionPipeline\n \n else:\n     import sys\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py",
            "diff": "diff --git a/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py b/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py\nindex 53e5a34a..71b2ef27 100755\n--- a/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py\n+++ b/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py\n@@ -134,7 +134,15 @@ class StableDiffusionKDiffusionPipeline(DiffusionPipeline, TextualInversionLoade\n     def set_scheduler(self, scheduler_type: str):\n         library = importlib.import_module(\"k_diffusion\")\n         sampling = getattr(library, \"sampling\")\n-        self.sampler = getattr(sampling, scheduler_type)\n+        try:\n+            self.sampler = getattr(sampling, scheduler_type)\n+        except Exception:\n+            valid_samplers = []\n+            for s in dir(sampling):\n+                if \"sample_\" in s:\n+                    valid_samplers.append(s)\n+\n+            raise ValueError(f\"Invalid scheduler type {scheduler_type}. Please choose one of {valid_samplers}.\")\n \n     # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._encode_prompt\n     def _encode_prompt(\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py",
            "diff": "diff --git a/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py b/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py\nnew file mode 100644\nindex 00000000..39f7f6f7\n--- /dev/null\n+++ b/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py\n@@ -0,0 +1,1012 @@\n+# Copyright 2023 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import importlib\n+import inspect\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from k_diffusion.external import CompVisDenoiser, CompVisVDenoiser\n+from k_diffusion.sampling import BrownianTreeNoiseSampler, get_sigmas_karras\n+from transformers import (\n+    CLIPTextModel,\n+    CLIPTextModelWithProjection,\n+    CLIPTokenizer,\n+)\n+\n+from ...image_processor import VaeImageProcessor\n+from ...loaders import (\n+    FromSingleFileMixin,\n+    IPAdapterMixin,\n+    StableDiffusionXLLoraLoaderMixin,\n+    TextualInversionLoaderMixin,\n+)\n+from ...models import AutoencoderKL, UNet2DConditionModel\n+from ...models.attention_processor import (\n+    AttnProcessor2_0,\n+    FusedAttnProcessor2_0,\n+    LoRAAttnProcessor2_0,\n+    LoRAXFormersAttnProcessor,\n+    XFormersAttnProcessor,\n+)\n+from ...models.lora import adjust_lora_scale_text_encoder\n+from ...schedulers import KarrasDiffusionSchedulers, LMSDiscreteScheduler\n+from ...utils import (\n+    USE_PEFT_BACKEND,\n+    logging,\n+    replace_example_docstring,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from ..stable_diffusion_xl.pipeline_output import StableDiffusionXLPipelineOutput\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers import StableDiffusionXLKDiffusionPipeline\n+\n+        >>> pipe = StableDiffusionXLKDiffusionPipeline.from_pretrained(\n+        ...     \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n+        ... )\n+        >>> pipe = pipe.to(\"cuda\")\n+        >>> pipe.set_scheduler(\"sample_dpmpp_2m_sde\")\n+\n+        >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n+        >>> image = pipe(prompt).images[0]\n+        ```\n+\"\"\"\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion_k_diffusion.pipeline_stable_diffusion_k_diffusion.ModelWrapper\n+class ModelWrapper:\n+    def __init__(self, model, alphas_cumprod):\n+        self.model = model\n+        self.alphas_cumprod = alphas_cumprod\n+\n+    def apply_model(self, *args, **kwargs):\n+        if len(args) == 3:\n+            encoder_hidden_states = args[-1]\n+            args = args[:2]\n+        if kwargs.get(\"cond\", None) is not None:\n+            encoder_hidden_states = kwargs.pop(\"cond\")\n+        return self.model(*args, encoder_hidden_states=encoder_hidden_states, **kwargs).sample\n+\n+\n+class StableDiffusionXLKDiffusionPipeline(\n+    DiffusionPipeline,\n+    FromSingleFileMixin,\n+    StableDiffusionXLLoraLoaderMixin,\n+    TextualInversionLoaderMixin,\n+    IPAdapterMixin,\n+):\n+    r\"\"\"\n+    Pipeline for text-to-image generation using Stable Diffusion XL and k-diffusion.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n+    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n+\n+    The pipeline also inherits the following loading methods:\n+        - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings\n+        - [`~loaders.FromSingleFileMixin.from_single_file`] for loading `.ckpt` files\n+        - [`~loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n+        - [`~loaders.StableDiffusionXLLoraLoaderMixin.save_lora_weights`] for saving LoRA weights\n+        - [`~loaders.IPAdapterMixin.load_ip_adapter`] for loading IP Adapters\n+\n+    Args:\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`CLIPTextModel`]):\n+            Frozen text-encoder. Stable Diffusion XL uses the text portion of\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n+            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n+        text_encoder_2 ([` CLIPTextModelWithProjection`]):\n+            Second frozen text-encoder. Stable Diffusion XL uses the text and pool portion of\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n+            specifically the\n+            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)\n+            variant.\n+        tokenizer (`CLIPTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n+        tokenizer_2 (`CLIPTokenizer`):\n+            Second Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n+        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n+        scheduler ([`SchedulerMixin`]):\n+            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n+            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n+        force_zeros_for_empty_prompt (`bool`, *optional*, defaults to `\"True\"`):\n+            Whether the negative prompt embeddings shall be forced to always be set to 0. Also see the config of\n+            `stabilityai/stable-diffusion-xl-base-1-0`.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->text_encoder_2->unet->vae\"\n+    _optional_components = [\n+        \"tokenizer\",\n+        \"tokenizer_2\",\n+        \"text_encoder\",\n+        \"text_encoder_2\",\n+        \"feature_extractor\",\n+    ]\n+\n+    def __init__(\n+        self,\n+        vae: AutoencoderKL,\n+        text_encoder: CLIPTextModel,\n+        text_encoder_2: CLIPTextModelWithProjection,\n+        tokenizer: CLIPTokenizer,\n+        tokenizer_2: CLIPTokenizer,\n+        unet: UNet2DConditionModel,\n+        scheduler: KarrasDiffusionSchedulers,\n+        force_zeros_for_empty_prompt: bool = True,\n+    ):\n+        super().__init__()\n+\n+        # get correct sigmas from LMS\n+        scheduler = LMSDiscreteScheduler.from_config(scheduler.config)\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            text_encoder_2=text_encoder_2,\n+            tokenizer=tokenizer,\n+            tokenizer_2=tokenizer_2,\n+            unet=unet,\n+            scheduler=scheduler,\n+        )\n+        self.register_to_config(force_zeros_for_empty_prompt=force_zeros_for_empty_prompt)\n+        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n+\n+        self.default_sample_size = self.unet.config.sample_size\n+\n+        model = ModelWrapper(unet, scheduler.alphas_cumprod)\n+        if scheduler.config.prediction_type == \"v_prediction\":\n+            self.k_diffusion_model = CompVisVDenoiser(model)\n+        else:\n+            self.k_diffusion_model = CompVisDenoiser(model)\n+\n+    # Copied from diffusers.pipelines.stable_diffusion_k_diffusion.pipeline_stable_diffusion_k_diffusion.StableDiffusionKDiffusionPipeline.set_scheduler\n+    def set_scheduler(self, scheduler_type: str):\n+        library = importlib.import_module(\"k_diffusion\")\n+        sampling = getattr(library, \"sampling\")\n+        try:\n+            self.sampler = getattr(sampling, scheduler_type)\n+        except Exception:\n+            valid_samplers = []\n+            for s in dir(sampling):\n+                if \"sample_\" in s:\n+                    valid_samplers.append(s)\n+\n+            raise ValueError(f\"Invalid scheduler type {scheduler_type}. Please choose one of {valid_samplers}.\")\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.enable_vae_slicing\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.disable_vae_slicing\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.enable_vae_tiling\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.disable_vae_tiling\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: str,\n+        prompt_2: Optional[str] = None,\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: Optional[str] = None,\n+        negative_prompt_2: Optional[str] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        lora_scale: Optional[float] = None,\n+        clip_skip: Optional[int] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            prompt_2 (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n+                used in both text-encoders\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            do_classifier_free_guidance (`bool`):\n+                whether to use classifier free guidance or not\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            negative_prompt_2 (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n+                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n+                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n+            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n+                input argument.\n+            lora_scale (`float`, *optional*):\n+                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n+            clip_skip (`int`, *optional*):\n+                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n+                the output of the pre-final layer will be used for computing the prompt embeddings.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        # set lora scale so that monkey patched LoRA\n+        # function of text encoder can correctly access it\n+        if lora_scale is not None and isinstance(self, StableDiffusionXLLoraLoaderMixin):\n+            self._lora_scale = lora_scale\n+\n+            # dynamically adjust the LoRA scale\n+            if self.text_encoder is not None:\n+                if not USE_PEFT_BACKEND:\n+                    adjust_lora_scale_text_encoder(self.text_encoder, lora_scale)\n+                else:\n+                    scale_lora_layers(self.text_encoder, lora_scale)\n+\n+            if self.text_encoder_2 is not None:\n+                if not USE_PEFT_BACKEND:\n+                    adjust_lora_scale_text_encoder(self.text_encoder_2, lora_scale)\n+                else:\n+                    scale_lora_layers(self.text_encoder_2, lora_scale)\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # Define tokenizers and text encoders\n+        tokenizers = [self.tokenizer, self.tokenizer_2] if self.tokenizer is not None else [self.tokenizer_2]\n+        text_encoders = (\n+            [self.text_encoder, self.text_encoder_2] if self.text_encoder is not None else [self.text_encoder_2]\n+        )\n+\n+        if prompt_embeds is None:\n+            prompt_2 = prompt_2 or prompt\n+            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2\n+\n+            # textual inversion: procecss multi-vector tokens if necessary\n+            prompt_embeds_list = []\n+            prompts = [prompt, prompt_2]\n+            for prompt, tokenizer, text_encoder in zip(prompts, tokenizers, text_encoders):\n+                if isinstance(self, TextualInversionLoaderMixin):\n+                    prompt = self.maybe_convert_prompt(prompt, tokenizer)\n+\n+                text_inputs = tokenizer(\n+                    prompt,\n+                    padding=\"max_length\",\n+                    max_length=tokenizer.model_max_length,\n+                    truncation=True,\n+                    return_tensors=\"pt\",\n+                )\n+\n+                text_input_ids = text_inputs.input_ids\n+                untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n+\n+                if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n+                    text_input_ids, untruncated_ids\n+                ):\n+                    removed_text = tokenizer.batch_decode(untruncated_ids[:, tokenizer.model_max_length - 1 : -1])\n+                    logger.warning(\n+                        \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n+                        f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n+                    )\n+\n+                prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)\n+\n+                # We are only ALWAYS interested in the pooled output of the final text encoder\n+                pooled_prompt_embeds = prompt_embeds[0]\n+                if clip_skip is None:\n+                    prompt_embeds = prompt_embeds.hidden_states[-2]\n+                else:\n+                    # \"2\" because SDXL always indexes from the penultimate layer.\n+                    prompt_embeds = prompt_embeds.hidden_states[-(clip_skip + 2)]\n+\n+                prompt_embeds_list.append(prompt_embeds)\n+\n+            prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n+\n+        # get unconditional embeddings for classifier free guidance\n+        zero_out_negative_prompt = negative_prompt is None and self.config.force_zeros_for_empty_prompt\n+        if do_classifier_free_guidance and negative_prompt_embeds is None and zero_out_negative_prompt:\n+            negative_prompt_embeds = torch.zeros_like(prompt_embeds)\n+            negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)\n+        elif do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt_2 = negative_prompt_2 or negative_prompt\n+\n+            # normalize str to list\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+            negative_prompt_2 = (\n+                batch_size * [negative_prompt_2] if isinstance(negative_prompt_2, str) else negative_prompt_2\n+            )\n+\n+            uncond_tokens: List[str]\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+            else:\n+                uncond_tokens = [negative_prompt, negative_prompt_2]\n+\n+            negative_prompt_embeds_list = []\n+            for negative_prompt, tokenizer, text_encoder in zip(uncond_tokens, tokenizers, text_encoders):\n+                if isinstance(self, TextualInversionLoaderMixin):\n+                    negative_prompt = self.maybe_convert_prompt(negative_prompt, tokenizer)\n+\n+                max_length = prompt_embeds.shape[1]\n+                uncond_input = tokenizer(\n+                    negative_prompt,\n+                    padding=\"max_length\",\n+                    max_length=max_length,\n+                    truncation=True,\n+                    return_tensors=\"pt\",\n+                )\n+\n+                negative_prompt_embeds = text_encoder(\n+                    uncond_input.input_ids.to(device),\n+                    output_hidden_states=True,\n+                )\n+                # We are only ALWAYS interested in the pooled output of the final text encoder\n+                negative_pooled_prompt_embeds = negative_prompt_embeds[0]\n+                negative_prompt_embeds = negative_prompt_embeds.hidden_states[-2]\n+\n+                negative_prompt_embeds_list.append(negative_prompt_embeds)\n+\n+            negative_prompt_embeds = torch.concat(negative_prompt_embeds_list, dim=-1)\n+\n+        if self.text_encoder_2 is not None:\n+            prompt_embeds = prompt_embeds.to(dtype=self.text_encoder_2.dtype, device=device)\n+        else:\n+            prompt_embeds = prompt_embeds.to(dtype=self.unet.dtype, device=device)\n+\n+        bs_embed, seq_len, _ = prompt_embeds.shape\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n+\n+        if do_classifier_free_guidance:\n+            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n+            seq_len = negative_prompt_embeds.shape[1]\n+\n+            if self.text_encoder_2 is not None:\n+                negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder_2.dtype, device=device)\n+            else:\n+                negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.unet.dtype, device=device)\n+\n+            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+\n+        pooled_prompt_embeds = pooled_prompt_embeds.repeat(1, num_images_per_prompt).view(\n+            bs_embed * num_images_per_prompt, -1\n+        )\n+        if do_classifier_free_guidance:\n+            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(1, num_images_per_prompt).view(\n+                bs_embed * num_images_per_prompt, -1\n+            )\n+\n+        if self.text_encoder is not None:\n+            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:\n+                # Retrieve the original scale by scaling back the LoRA layers\n+                unscale_lora_layers(self.text_encoder, lora_scale)\n+\n+        if self.text_encoder_2 is not None:\n+            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:\n+                # Retrieve the original scale by scaling back the LoRA layers\n+                unscale_lora_layers(self.text_encoder_2, lora_scale)\n+\n+        return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        prompt_2,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        negative_prompt_2=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        pooled_prompt_embeds=None,\n+        negative_pooled_prompt_embeds=None,\n+    ):\n+        if height % 8 != 0 or width % 8 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt_2 is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif prompt_2 is not None and (not isinstance(prompt_2, str) and not isinstance(prompt_2, list)):\n+            raise ValueError(f\"`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+        elif negative_prompt_2 is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt_2`: {negative_prompt_2} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and negative_prompt_embeds is not None:\n+            if prompt_embeds.shape != negative_prompt_embeds.shape:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n+                    f\" {negative_prompt_embeds.shape}.\"\n+                )\n+\n+        if prompt_embeds is not None and pooled_prompt_embeds is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+\n+        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n+        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline._get_add_time_ids\n+    def _get_add_time_ids(\n+        self, original_size, crops_coords_top_left, target_size, dtype, text_encoder_projection_dim=None\n+    ):\n+        add_time_ids = list(original_size + crops_coords_top_left + target_size)\n+\n+        passed_add_embed_dim = (\n+            self.unet.config.addition_time_embed_dim * len(add_time_ids) + text_encoder_projection_dim\n+        )\n+        expected_add_embed_dim = self.unet.add_embedding.linear_1.in_features\n+\n+        if expected_add_embed_dim != passed_add_embed_dim:\n+            raise ValueError(\n+                f\"Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`.\"\n+            )\n+\n+        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n+        return add_time_ids\n+\n+    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline.upcast_vae\n+    def upcast_vae(self):\n+        dtype = self.vae.dtype\n+        self.vae.to(dtype=torch.float32)\n+        use_torch_2_0_or_xformers = isinstance(\n+            self.vae.decoder.mid_block.attentions[0].processor,\n+            (\n+                AttnProcessor2_0,\n+                XFormersAttnProcessor,\n+                LoRAXFormersAttnProcessor,\n+                LoRAAttnProcessor2_0,\n+                FusedAttnProcessor2_0,\n+            ),\n+        )\n+        # if xformers or torch_2_0 is used attention block does not need\n+        # to be in float32 which can save lots of memory\n+        if use_torch_2_0_or_xformers:\n+            self.vae.post_quant_conv.to(dtype)\n+            self.vae.decoder.conv_in.to(dtype)\n+            self.vae.decoder.mid_block.to(dtype)\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.enable_freeu\n+    def enable_freeu(self, s1: float, s2: float, b1: float, b2: float):\n+        r\"\"\"Enables the FreeU mechanism as in https://arxiv.org/abs/2309.11497.\n+\n+        The suffixes after the scaling factors represent the stages where they are being applied.\n+\n+        Please refer to the [official repository](https://github.com/ChenyangSi/FreeU) for combinations of the values\n+        that are known to work well for different pipelines such as Stable Diffusion v1, v2, and Stable Diffusion XL.\n+\n+        Args:\n+            s1 (`float`):\n+                Scaling factor for stage 1 to attenuate the contributions of the skip features. This is done to\n+                mitigate \"oversmoothing effect\" in the enhanced denoising process.\n+            s2 (`float`):\n+                Scaling factor for stage 2 to attenuate the contributions of the skip features. This is done to\n+                mitigate \"oversmoothing effect\" in the enhanced denoising process.\n+            b1 (`float`): Scaling factor for stage 1 to amplify the contributions of backbone features.\n+            b2 (`float`): Scaling factor for stage 2 to amplify the contributions of backbone features.\n+        \"\"\"\n+        if not hasattr(self, \"unet\"):\n+            raise ValueError(\"The pipeline must have `unet` for using FreeU.\")\n+        self.unet.enable_freeu(s1=s1, s2=s2, b1=b1, b2=b2)\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.disable_freeu\n+    def disable_freeu(self):\n+        \"\"\"Disables the FreeU mechanism if enabled.\"\"\"\n+        self.unet.disable_freeu()\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.fuse_qkv_projections\n+    def fuse_qkv_projections(self, unet: bool = True, vae: bool = True):\n+        \"\"\"\n+        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query,\n+        key, value) are fused. For cross-attention modules, key and value projection matrices are fused.\n+\n+        <Tip warning={true}>\n+\n+        This API is \ud83e\uddea experimental.\n+\n+        </Tip>\n+\n+        Args:\n+            unet (`bool`, defaults to `True`): To apply fusion on the UNet.\n+            vae (`bool`, defaults to `True`): To apply fusion on the VAE.\n+        \"\"\"\n+        self.fusing_unet = False\n+        self.fusing_vae = False\n+\n+        if unet:\n+            self.fusing_unet = True\n+            self.unet.fuse_qkv_projections()\n+            self.unet.set_attn_processor(FusedAttnProcessor2_0())\n+\n+        if vae:\n+            if not isinstance(self.vae, AutoencoderKL):\n+                raise ValueError(\"`fuse_qkv_projections()` is only supported for the VAE of type `AutoencoderKL`.\")\n+\n+            self.fusing_vae = True\n+            self.vae.fuse_qkv_projections()\n+            self.vae.set_attn_processor(FusedAttnProcessor2_0())\n+\n+    def unfuse_qkv_projections(self, unet: bool = True, vae: bool = True):\n+        \"\"\"Disable QKV projection fusion if enabled.\n+\n+        <Tip warning={true}>\n+\n+        This API is \ud83e\uddea experimental.\n+\n+        </Tip>\n+\n+        Args:\n+            unet (`bool`, defaults to `True`): To apply fusion on the UNet.\n+            vae (`bool`, defaults to `True`): To apply fusion on the VAE.\n+\n+        \"\"\"\n+        if unet:\n+            if not self.fusing_unet:\n+                logger.warning(\"The UNet was not initially fused for QKV projections. Doing nothing.\")\n+            else:\n+                self.unet.unfuse_qkv_projections()\n+                self.fusing_unet = False\n+\n+        if vae:\n+            if not self.fusing_vae:\n+                logger.warning(\"The VAE was not initially fused for QKV projections. Doing nothing.\")\n+            else:\n+                self.vae.unfuse_qkv_projections()\n+                self.fusing_vae = False\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def clip_skip(self):\n+        return self._clip_skip\n+\n+    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n+    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n+    # corresponds to doing no classifier free guidance.\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1 and self.unet.config.time_cond_proj_dim is None\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        prompt_2: Optional[Union[str, List[str]]] = None,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n+        num_images_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.FloatTensor] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        original_size: Optional[Tuple[int, int]] = None,\n+        crops_coords_top_left: Tuple[int, int] = (0, 0),\n+        target_size: Optional[Tuple[int, int]] = None,\n+        negative_original_size: Optional[Tuple[int, int]] = None,\n+        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n+        negative_target_size: Optional[Tuple[int, int]] = None,\n+        use_karras_sigmas: Optional[bool] = False,\n+        noise_sampler_seed: Optional[int] = None,\n+        clip_skip: Optional[int] = None,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            prompt_2 (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n+                used in both text-encoders\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+                Anything below 512 pixels won't work well for\n+                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n+                and checkpoints that are not specifically fine-tuned on low resolutions.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+                Anything below 512 pixels won't work well for\n+                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n+                and checkpoints that are not specifically fine-tuned on low resolutions.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, *optional*, defaults to 5.0):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            negative_prompt_2 (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n+                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.FloatTensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will ge generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n+                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n+            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n+                input argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n+                of a plain tuple.\n+            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n+                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.\n+                `original_size` defaults to `(height, width)` if not specified. Part of SDXL's micro-conditioning as\n+                explained in section 2.2 of\n+                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n+            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n+                `crops_coords_top_left` can be used to generate an image that appears to be \"cropped\" from the position\n+                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting\n+                `crops_coords_top_left` to (0, 0). Part of SDXL's micro-conditioning as explained in section 2.2 of\n+                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n+            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n+                For most cases, `target_size` should be set to the desired height and width of the generated image. If\n+                not specified it will default to `(height, width)`. Part of SDXL's micro-conditioning as explained in\n+                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n+            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n+                To negatively condition the generation process based on a specific image resolution. Part of SDXL's\n+                micro-conditioning as explained in section 2.2 of\n+                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n+                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n+            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n+                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL's\n+                micro-conditioning as explained in section 2.2 of\n+                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n+                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n+            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n+                To negatively condition the generation process based on a target image resolution. It should be as same\n+                as the `target_size` for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n+                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n+                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] or `tuple`:\n+            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] if `return_dict` is True, otherwise a\n+            `tuple`. When returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+\n+        # 0. Default height and width to unet\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        original_size = original_size or (height, width)\n+        target_size = target_size or (height, width)\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            prompt_2,\n+            height,\n+            width,\n+            negative_prompt,\n+            negative_prompt_2,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            pooled_prompt_embeds,\n+            negative_pooled_prompt_embeds,\n+        )\n+\n+        if guidance_scale <= 1.0:\n+            raise ValueError(\"has to use guidance_scale\")\n+\n+        self._guidance_scale = guidance_scale\n+        self._clip_skip = clip_skip\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        # 3. Encode input prompt\n+        lora_scale = None\n+\n+        (\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            pooled_prompt_embeds,\n+            negative_pooled_prompt_embeds,\n+        ) = self.encode_prompt(\n+            prompt=prompt,\n+            prompt_2=prompt_2,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            negative_prompt=negative_prompt,\n+            negative_prompt_2=negative_prompt_2,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            pooled_prompt_embeds=pooled_prompt_embeds,\n+            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n+            lora_scale=lora_scale,\n+            clip_skip=self.clip_skip,\n+        )\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=prompt_embeds.device)\n+\n+        # 5. Prepare sigmas\n+        if use_karras_sigmas:\n+            sigma_min: float = self.k_diffusion_model.sigmas[0].item()\n+            sigma_max: float = self.k_diffusion_model.sigmas[-1].item()\n+            sigmas = get_sigmas_karras(n=num_inference_steps, sigma_min=sigma_min, sigma_max=sigma_max)\n+            sigmas = sigmas.to(device)\n+        else:\n+            sigmas = self.scheduler.sigmas\n+        sigmas = sigmas.to(prompt_embeds.dtype)\n+\n+        # 6. Prepare latent variables\n+        num_channels_latents = self.unet.config.in_channels\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        latents = latents * sigmas[0]\n+\n+        self.k_diffusion_model.sigmas = self.k_diffusion_model.sigmas.to(latents.device)\n+        self.k_diffusion_model.log_sigmas = self.k_diffusion_model.log_sigmas.to(latents.device)\n+\n+        # 7. Prepare added time ids & embeddings\n+        add_text_embeds = pooled_prompt_embeds\n+        if self.text_encoder_2 is None:\n+            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n+        else:\n+            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n+\n+        add_time_ids = self._get_add_time_ids(\n+            original_size,\n+            crops_coords_top_left,\n+            target_size,\n+            dtype=prompt_embeds.dtype,\n+            text_encoder_projection_dim=text_encoder_projection_dim,\n+        )\n+        if negative_original_size is not None and negative_target_size is not None:\n+            negative_add_time_ids = self._get_add_time_ids(\n+                negative_original_size,\n+                negative_crops_coords_top_left,\n+                negative_target_size,\n+                dtype=prompt_embeds.dtype,\n+                text_encoder_projection_dim=text_encoder_projection_dim,\n+            )\n+        else:\n+            negative_add_time_ids = add_time_ids\n+\n+        if self.do_classifier_free_guidance:\n+            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n+            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n+            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n+\n+        prompt_embeds = prompt_embeds.to(device)\n+        add_text_embeds = add_text_embeds.to(device)\n+        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n+\n+        added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n+\n+        # 8. Optionally get Guidance Scale Embedding\n+        timestep_cond = None\n+        if self.unet.config.time_cond_proj_dim is not None:\n+            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n+            timestep_cond = self.get_guidance_scale_embedding(\n+                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n+            ).to(device=device, dtype=latents.dtype)\n+\n+        # 9. Define model function\n+        def model_fn(x, t):\n+            latent_model_input = torch.cat([x] * 2)\n+            t = torch.cat([t] * 2)\n+\n+            noise_pred = self.k_diffusion_model(\n+                latent_model_input,\n+                t,\n+                cond=prompt_embeds,\n+                timestep_cond=timestep_cond,\n+                added_cond_kwargs=added_cond_kwargs,\n+            )\n+\n+            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n+            return noise_pred\n+\n+        # 10. Run k-diffusion solver\n+        sampler_kwargs = {}\n+\n+        if \"noise_sampler\" in inspect.signature(self.sampler).parameters:\n+            min_sigma, max_sigma = sigmas[sigmas > 0].min(), sigmas.max()\n+            noise_sampler = BrownianTreeNoiseSampler(latents, min_sigma, max_sigma, noise_sampler_seed)\n+            sampler_kwargs[\"noise_sampler\"] = noise_sampler\n+\n+        if \"generator\" in inspect.signature(self.sampler).parameters:\n+            sampler_kwargs[\"generator\"] = generator\n+\n+        latents = self.sampler(model_fn, latents, sigmas, **sampler_kwargs)\n+\n+        if not output_type == \"latent\":\n+            # make sure the VAE is in float32 mode, as it overflows in float16\n+            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n+\n+            if needs_upcasting:\n+                self.upcast_vae()\n+                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n+\n+            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n+\n+            # cast back to fp16 if needed\n+            if needs_upcasting:\n+                self.vae.to(dtype=torch.float16)\n+        else:\n+            image = latents\n+\n+        if not output_type == \"latent\":\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return StableDiffusionXLPipelineOutput(images=image)\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py",
            "diff": "diff --git a/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py b/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py\nindex 36a0a956..4a253643 100644\n--- a/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py\n+++ b/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py\n@@ -681,6 +681,11 @@ class StableDiffusionSAGPipeline(DiffusionPipeline, TextualInversionLoaderMixin,\n         self.scheduler.set_timesteps(num_inference_steps, device=device)\n         timesteps = self.scheduler.timesteps\n \n+        if timesteps.dtype not in [torch.int16, torch.int32, torch.int64]:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} does not support using a scheduler of type {self.scheduler.__class__.__name__}. Please make sure to use one of 'DDIMScheduler, PNDMScheduler, DDPMScheduler, DEISMultistepScheduler, UniPCMultistepScheduler, DPMSolverMultistepScheduler, DPMSolverSinlgestepScheduler'.\"\n+            )\n+\n         # 5. Prepare latent variables\n         num_channels_latents = self.unet.config.in_channels\n         latents = self.prepare_latents(\n@@ -830,14 +835,14 @@ class StableDiffusionSAGPipeline(DiffusionPipeline, TextualInversionLoaderMixin,\n         degraded_latents = degraded_latents * attn_mask + original_latents * (1 - attn_mask)\n \n         # Noise it again to match the noise level\n-        degraded_latents = self.scheduler.add_noise(degraded_latents, noise=eps, timesteps=t)\n+        degraded_latents = self.scheduler.add_noise(degraded_latents, noise=eps, timesteps=t[None])\n \n         return degraded_latents\n \n     # Modified from diffusers.schedulers.scheduling_ddim.DDIMScheduler.step\n     # Note: there are some schedulers that clip or do not return x_0 (PNDMScheduler, DDIMScheduler, etc.)\n     def pred_x0(self, sample, model_output, timestep):\n-        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n+        alpha_prod_t = self.scheduler.alphas_cumprod[timestep].to(sample.device)\n \n         beta_prod_t = 1 - alpha_prod_t\n         if self.scheduler.config.prediction_type == \"epsilon\":\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py",
            "diff": "diff --git a/src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py b/src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py\nindex 4cc0905a..fa96f41c 100644\n--- a/src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py\n+++ b/src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py\n@@ -77,7 +77,7 @@ class StableVideoDiffusionPipeline(DiffusionPipeline):\n     implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n \n     Args:\n-        vae ([`AutoencoderKL`]):\n+        vae ([`AutoencoderKLTemporalDecoder`]):\n             Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.\n         image_encoder ([`~transformers.CLIPVisionModelWithProjection`]):\n             Frozen CLIP image-encoder ([laion/CLIP-ViT-H-14-laion2B-s32B-b79K](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K)).\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py",
            "diff": "diff --git a/src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py b/src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py\nindex 56836f0b..2ab00c54 100644\n--- a/src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py\n+++ b/src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py\n@@ -15,3 +15,18 @@ class StableDiffusionKDiffusionPipeline(metaclass=DummyObject):\n     @classmethod\n     def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\", \"k_diffusion\"])\n+\n+\n+class StableDiffusionXLKDiffusionPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\", \"k_diffusion\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\", \"k_diffusion\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\", \"k_diffusion\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\", \"k_diffusion\"])\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "tests/pipelines/controlnet/test_controlnet_sdxl_img2img.py",
            "diff": "diff --git a/tests/pipelines/controlnet/test_controlnet_sdxl_img2img.py b/tests/pipelines/controlnet/test_controlnet_sdxl_img2img.py\nindex ee8c479b..ff09693d 100644\n--- a/tests/pipelines/controlnet/test_controlnet_sdxl_img2img.py\n+++ b/tests/pipelines/controlnet/test_controlnet_sdxl_img2img.py\n@@ -136,6 +136,8 @@ class ControlNetPipelineSDXLImg2ImgFastTests(\n             \"tokenizer\": tokenizer if not skip_first_text_encoder else None,\n             \"text_encoder_2\": text_encoder_2,\n             \"tokenizer_2\": tokenizer_2,\n+            \"image_encoder\": None,\n+            \"feature_extractor\": None,\n         }\n         return components\n \n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py",
            "diff": "diff --git a/tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py b/tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py\nindex 8b789408..ef695af5 100644\n--- a/tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py\n+++ b/tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py\n@@ -23,6 +23,9 @@ from transformers import CLIPTextConfig, CLIPTextModel, CLIPTokenizer\n from diffusers import (\n     AutoencoderKL,\n     DDIMScheduler,\n+    DEISMultistepScheduler,\n+    DPMSolverMultistepScheduler,\n+    EulerDiscreteScheduler,\n     StableDiffusionSAGPipeline,\n     UNet2DConditionModel,\n )\n@@ -45,14 +48,15 @@ class StableDiffusionSAGPipelineFastTests(PipelineLatentTesterMixin, PipelineTes\n     def get_dummy_components(self):\n         torch.manual_seed(0)\n         unet = UNet2DConditionModel(\n-            block_out_channels=(32, 64),\n+            block_out_channels=(4, 8),\n             layers_per_block=2,\n-            sample_size=32,\n+            sample_size=8,\n+            norm_num_groups=1,\n             in_channels=4,\n             out_channels=4,\n             down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n             up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n-            cross_attention_dim=32,\n+            cross_attention_dim=8,\n         )\n         scheduler = DDIMScheduler(\n             beta_start=0.00085,\n@@ -63,7 +67,8 @@ class StableDiffusionSAGPipelineFastTests(PipelineLatentTesterMixin, PipelineTes\n         )\n         torch.manual_seed(0)\n         vae = AutoencoderKL(\n-            block_out_channels=[32, 64],\n+            block_out_channels=[4, 8],\n+            norm_num_groups=1,\n             in_channels=3,\n             out_channels=3,\n             down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n@@ -74,11 +79,11 @@ class StableDiffusionSAGPipelineFastTests(PipelineLatentTesterMixin, PipelineTes\n         text_encoder_config = CLIPTextConfig(\n             bos_token_id=0,\n             eos_token_id=2,\n-            hidden_size=32,\n+            hidden_size=8,\n+            num_hidden_layers=2,\n             intermediate_size=37,\n             layer_norm_eps=1e-05,\n             num_attention_heads=4,\n-            num_hidden_layers=5,\n             pad_token_id=1,\n             vocab_size=1000,\n         )\n@@ -108,13 +113,35 @@ class StableDiffusionSAGPipelineFastTests(PipelineLatentTesterMixin, PipelineTes\n             \"num_inference_steps\": 2,\n             \"guidance_scale\": 1.0,\n             \"sag_scale\": 1.0,\n-            \"output_type\": \"numpy\",\n+            \"output_type\": \"np\",\n         }\n         return inputs\n \n     def test_inference_batch_single_identical(self):\n         super().test_inference_batch_single_identical(expected_max_diff=3e-3)\n \n+    @unittest.skip(\"Not necessary to test here.\")\n+    def test_xformers_attention_forwardGenerator_pass(self):\n+        pass\n+\n+    def test_pipeline_different_schedulers(self):\n+        pipeline = self.pipeline_class(**self.get_dummy_components())\n+        inputs = self.get_dummy_inputs(\"cpu\")\n+\n+        expected_image_size = (16, 16, 3)\n+        for scheduler_cls in [DDIMScheduler, DEISMultistepScheduler, DPMSolverMultistepScheduler]:\n+            pipeline.scheduler = scheduler_cls.from_config(pipeline.scheduler.config)\n+            image = pipeline(**inputs).images[0]\n+\n+            shape = image.shape\n+            assert shape == expected_image_size\n+\n+        pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n+\n+        with self.assertRaises(ValueError):\n+            # Karras schedulers are not supported\n+            image = pipeline(**inputs).images[0]\n+\n \n @nightly\n @require_torch_gpu\n"
        },
        {
            "commit": "2c06ffa4c9d2c37846c60ad75899b4d72f214ff9",
            "file_path": "tests/pipelines/stable_diffusion_xl/test_stable_diffusion_xl_k_diffusion.py",
            "diff": "diff --git a/tests/pipelines/stable_diffusion_xl/test_stable_diffusion_xl_k_diffusion.py b/tests/pipelines/stable_diffusion_xl/test_stable_diffusion_xl_k_diffusion.py\nnew file mode 100644\nindex 00000000..158385db\n--- /dev/null\n+++ b/tests/pipelines/stable_diffusion_xl/test_stable_diffusion_xl_k_diffusion.py\n@@ -0,0 +1,138 @@\n+# coding=utf-8\n+# Copyright 2023 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import unittest\n+\n+import numpy as np\n+import torch\n+\n+from diffusers import StableDiffusionXLKDiffusionPipeline\n+from diffusers.utils.testing_utils import enable_full_determinism, require_torch_gpu, slow, torch_device\n+\n+\n+enable_full_determinism()\n+\n+\n+@slow\n+@require_torch_gpu\n+class StableDiffusionXLKPipelineIntegrationTests(unittest.TestCase):\n+    dtype = torch.float16\n+\n+    def tearDown(self):\n+        # clean up the VRAM after each test\n+        super().tearDown()\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+\n+    def test_stable_diffusion_xl(self):\n+        sd_pipe = StableDiffusionXLKDiffusionPipeline.from_pretrained(\n+            \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=self.dtype\n+        )\n+        sd_pipe = sd_pipe.to(torch_device)\n+        sd_pipe.set_progress_bar_config(disable=None)\n+\n+        sd_pipe.set_scheduler(\"sample_euler\")\n+\n+        prompt = \"A painting of a squirrel eating a burger\"\n+        generator = torch.manual_seed(0)\n+        output = sd_pipe(\n+            [prompt],\n+            generator=generator,\n+            guidance_scale=9.0,\n+            num_inference_steps=20,\n+            height=512,\n+            width=512,\n+            output_type=\"np\",\n+        )\n+\n+        image = output.images\n+\n+        image_slice = image[0, -3:, -3:, -1]\n+\n+        assert image.shape == (1, 512, 512, 3)\n+        expected_slice = np.array(\n+            [0.79804534, 0.7981539, 0.8019961, 0.7936565, 0.7892033, 0.7914713, 0.7792827, 0.77754563, 0.7836789]\n+        )\n+\n+        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n+\n+    def test_stable_diffusion_karras_sigmas(self):\n+        sd_pipe = StableDiffusionXLKDiffusionPipeline.from_pretrained(\n+            \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=self.dtype\n+        )\n+        sd_pipe = sd_pipe.to(torch_device)\n+        sd_pipe.set_progress_bar_config(disable=None)\n+\n+        sd_pipe.set_scheduler(\"sample_dpmpp_2m\")\n+\n+        prompt = \"A painting of a squirrel eating a burger\"\n+        generator = torch.manual_seed(0)\n+        output = sd_pipe(\n+            [prompt],\n+            generator=generator,\n+            guidance_scale=7.5,\n+            num_inference_steps=15,\n+            output_type=\"np\",\n+            use_karras_sigmas=True,\n+            height=512,\n+            width=512,\n+        )\n+\n+        image = output.images\n+\n+        image_slice = image[0, -3:, -3:, -1]\n+\n+        assert image.shape == (1, 512, 512, 3)\n+        expected_slice = np.array(\n+            [0.9704869, 0.9714559, 0.9693254, 0.96892524, 0.9685236, 0.9659081, 0.9666761, 0.9619067, 0.961759]\n+        )\n+\n+        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n+\n+    def test_stable_diffusion_noise_sampler_seed(self):\n+        sd_pipe = StableDiffusionXLKDiffusionPipeline.from_pretrained(\n+            \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=self.dtype\n+        )\n+        sd_pipe = sd_pipe.to(torch_device)\n+        sd_pipe.set_progress_bar_config(disable=None)\n+\n+        sd_pipe.set_scheduler(\"sample_dpmpp_sde\")\n+\n+        prompt = \"A painting of a squirrel eating a burger\"\n+        seed = 0\n+        images1 = sd_pipe(\n+            [prompt],\n+            generator=torch.manual_seed(seed),\n+            noise_sampler_seed=seed,\n+            guidance_scale=9.0,\n+            num_inference_steps=20,\n+            output_type=\"np\",\n+            height=512,\n+            width=512,\n+        ).images\n+        images2 = sd_pipe(\n+            [prompt],\n+            generator=torch.manual_seed(seed),\n+            noise_sampler_seed=seed,\n+            guidance_scale=9.0,\n+            num_inference_steps=20,\n+            output_type=\"np\",\n+            height=512,\n+            width=512,\n+        ).images\n+        assert images1.shape == (1, 512, 512, 3)\n+        assert images2.shape == (1, 512, 512, 3)\n+        assert np.abs(images1.flatten() - images2.flatten()).max() < 1e-2\n"
        }
    ]
}